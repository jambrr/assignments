<doc id="4822" url="https://en.wikipedia.org/wiki?curid=4822" title="Board of directors">
Board of directors

A board of directors is a body of elected or appointed members who jointly oversee the activities of a company or organization. Other names include board of governors, board of managers, board of regents, board of trustees, and board of visitors. It may also be called "the executive board" and is often simply referred to as "the board".
A board's activities are determined by the powers, duties, and responsibilities delegated to it or conferred on it by an authority outside itself. These matters are typically detailed in the organization's bylaws. The bylaws commonly also specify the number of members of the board, how they are to be chosen, and when they are to meet. However, these bylaws rarely address a board's powers when faced with a corporate turnaround or restructuring, where board members need to act as agents of change in addition to their traditional fiduciary responsibilities.
In an organization with voting members, the board acts on behalf of, and is subordinate to, the organization's full group, which usually chooses the members of the board. In a stock corporation, the board is elected by the shareholders and is the highest authority in the management of the corporation. In a non-stock corporation with no general voting membership, the board is the supreme governing body of the institution; its members are sometimes chosen by the board itself.
Typical duties of boards of directors include:
The legal responsibilities of boards and board members vary with the nature of the organization, and with the jurisdiction within which it operates. For companies with publicly trading stock, these responsibilities are typically much more rigorous and complex than for those of other types.
Typically the board chooses one of its members to be the "chairman", who holds whatever title is specified in the bylaws or articles of association. However, in membership organizations, the members elect the president of the organization and the president becomes the chairman of the board, unless the bylaws say otherwise.
Directors.
The directors of an organization are the persons who are members of its board. Several specific terms categorize directors by the presence or absence of their other relationships to the organization.
Inside director.
An inside director is a director who is also an employee, officer, major shareholder, or someone similarly connected to the organization. Inside directors represent the interests of the entity's stakeholders, and often have special knowledge of its inner workings, its financial or market position, and so on.
Typical inside directors are:
An inside director who is employed as a manager or executive of the organization is sometimes referred to as an executive director (not to be confused with the title executive director sometimes used for the CEO position). Executive directors often have a specified area of responsibility in the organization, such as finance, marketing, human resources, or production.
Outside director.
An outside director is a member of the board who is not otherwise employed by or engaged with the organization, and does not represent any of its stakeholders. A typical example is a director who is president of a firm in a different industry. Outside directors are not employees of the company or affiliated with it in any other way.
Outside directors bring outside experience and perspective to the board. They keep a watchful eye on the inside directors and on the way the organization is run. Outside directors are often useful in handling disputes between inside directors, or between shareholders and the board. They are thought to be advantageous because they can be objective and present little risk of conflict of interest. On the other hand, they might lack familiarity with the specific issues connected to the organization's governance.
Terminology.
Individual directors often serve on more than one board. This practice results in an interlocking directorate, where a relatively small number of individuals have significant influence over a large number of important entities. This situation can have important corporate, social, economic, and legal consequences, and has been the subject of significant research.
Process.
The process for running a board, sometimes called the board process, includes the selection of board members, the setting of clear board objectives, the dissemination of documents or board package to the board members, the collaborative creation of an agenda for the meeting, the creation and follow-up of assigned action items, and the assessment of the board process through standardized assessments of board members, owners, and CEOs. The science of this process has been slow to develop due to the secretive nature of the way most companies run their boards, however some standardization is beginning to develop. Some who are pushing for this standardization in the USA are the National Association of Corporate Directors, McKinsey Consulting and The Board Group.
Board meetings.
A board of directors conducts its meetings according to the rules and procedures contained in its governing documents. These procedures may allow the board to conduct its business by conference call or other electronic means. They may also specify how a quorum is to be determined.
Most organizations have adopted "Robert's Rules of Order" as its guide to supplement its own rules. In this book, the rules for conducting board meetings may be less formal if there is no more than about a dozen board members present. An example of the informality is that motions are not required if it's clear what is being discussed.
Non-corporate boards.
The role and responsibilities of a board of directors vary depending on the nature and type of business entity and the laws applying to the entity (see types of business entity). For example, the nature of the business entity may be one that is traded on a public market (public company), not traded on a public market (a private, limited or closely held company), owned by family members (a family business), or exempt from income taxes (a non-profit, not for profit, or tax-exempt entity). There are numerous types of business entities available throughout the world such as a corporation, limited liability company, cooperative, business trust, partnership, private limited company, and public limited company.
Much of what has been written about boards of directors relates to boards of directors of business entities actively traded on public markets. More recently, however, material is becoming available for boards of private and closely held businesses including family businesses.
A board-only organization is one whose board is self-appointed, rather than being accountable to a base of members through elections; or in which the powers of the membership are extremely limited.
Membership organizations.
In membership organizations, such as a society made up of members of a certain profession or one advocating a certain cause, a board of directors may have the responsibility of running the organization in between meetings of the membership, especially if the membership meets infrequently, such as only at an annual general meeting. The amount of powers and authority delegated to the board depend on the bylaws and rules of the particular organization. Some organizations place matters exclusively in the board's control while in others, the general membership retains full power and the board can only make recommendations.
The setup of a board of directors vary widely across organizations and may include provisions that are applicable to corporations, in which the "shareholders" are the members of the organization. A difference may be that the membership elects the officers of the organization, such as the president and the secretary, and the officers become members of the board in addition to the directors and retain those duties on the board. The directors may also be classified as officers in this situation. There may also be ex-officio members of the board, or persons who are members due to another position that they hold. These ex-officio members have all the same rights as the other board members.
Members of the board may be removed before their term is complete. Details on how they can be removed are usually provided in the bylaws. If the bylaws do not contain such details, the section on disciplinary procedures in "Robert's Rules of Order" may be used.
Corporations.
In a publicly held company, directors are elected to represent and are legally obligated as fiduciaries to represent owners of the company—the shareholders/stockholders. In this capacity they establish policies and make decisions on issues such as whether there is dividend and how much it is, stock options distributed to employees, and the hiring/firing and compensation of upper management.
Governance.
Theoretically, the control of a company is divided between two bodies: the board of directors, and the shareholders in general meeting. In practice, the amount of power exercised by the board varies with the type of company. In small private companies, the directors and the shareholders are normally the same people, and thus there is no real division of power. In large public companies, the board tends to exercise more of a supervisory role, and individual responsibility and management tends to be delegated downward to individual professional executives (such as a finance director or a marketing director) who deal with particular areas of the company's affairs.
Another feature of boards of directors in large public companies is that the board tends to have more de facto power. Many shareholders grant proxies to the directors to vote their shares at general meetings and accept all recommendations of the board rather than try to get involved in management, since each shareholder's power, as well as interest and information is so small. Larger institutional investors also grant the board proxies. The large number of shareholders also makes it hard for them to organize. However, there have been moves recently to try to increase shareholder activism among both institutional investors and individuals with small shareholdings.
A contrasting view is that in large public companies it is upper management and not boards that wield practical power, because boards delegate nearly all of their power to the top executive employees, adopting their recommendations almost without fail. As a practical matter, executives even choose the directors, with shareholders normally following management recommendations and voting for them.
In most cases, serving on a board is not a career unto itself, but board members often receive remunerations amounting to hundreds of thousands of dollars per year since they often sit on the boards of several companies. Inside directors are usually not paid for sitting on a board, but the duty is instead considered part of their larger job description. Outside directors are usually paid for their services. These remunerations vary between corporations, but usually consist of a yearly or monthly salary, additional compensation for each meeting attended, stock options, and various other benefits. Tiffany & Co., for example, pays directors an annual retainer of $46,500, an additional annual retainer of $2,500 if the director is also a chairperson of a committee, a per-meeting-attended fee of $2,000 for meetings attended in person, a $500 fee for each meeting attended via telephone, in addition to stock options and retirement benefits.
Two-tier system.
In some European and Asian countries, there are two separate boards, an executive board for day-to-day business and a supervisory board (elected by the shareholders and employees) for supervising the executive board. In these countries, the CEO (chief executive or managing director) presides over the executive board and the chairman presides over the supervisory board, and these two roles will always be held by different people. This ensures a distinction between management by the executive board and governance by the supervisory board and allows for clear lines of authority. The aim is to prevent a conflict of interest and too much power being concentrated in the hands of one person. There is a strong parallel here with the structure of government, which tends to separate the political cabinet from the management civil service. In the United States, the board of directors (elected by the shareholders) is often equivalent to the supervisory board, while the executive board may often be known as the executive committee (operating committee or executive council), composed of the CEO and their direct reports (other C-level officers, division/subsidiary heads).
History.
The development of a separate board of directors to manage/govern/oversee a company has occurred incrementally and indefinitely over legal history. Until the end of the 19th century, it seems to have been generally assumed that the general meeting (of all shareholders) was the supreme organ of a company, and that the board of directors merely acted as an agent of the company subject to the control of the shareholders in general meeting.
However, by 1906, the English Court of Appeal had made it clear in the decision of "Automatic Self-Cleansing Filter Syndicate Co Ltd v Cuninghame" 2 Ch 34 that the division of powers between the board and the shareholders in general meaning depended on the construction of the articles of association and that, where the powers of management were vested in the board, the general meeting could not interfere with their lawful exercise. The articles were held to constitute a contract by which the members had agreed that "the directors and the directors alone shall manage."
The new approach did not secure immediate approval, but it was endorsed by the House of Lords in "Quin & Axtens v Salmon" AC 442 and has since received general acceptance. Under English law, successive versions of Table A have reinforced the norm that, unless the directors are acting contrary to the law or the provisions of the Articles, the powers of conducting the management and affairs of the company are vested in them.
The modern doctrine was expressed in "John Shaw & Sons (Salford) Ltd v Shaw" 2 KB 113 by Greer LJ as follows:
A company is an entity distinct alike from its shareholders and its directors. Some of its powers may, according to its articles, be exercised by directors, certain other powers may be reserved for the shareholders in general meeting. If powers of management are vested in the directors, they and they alone can exercise these powers. The only way in which the general body of shareholders can control the exercise of powers by the articles in the directors is by altering the articles, or, if opportunity arises under the articles, by refusing to re-elect the directors of whose actions they disapprove. They cannot themselves usurp the powers which by the articles are vested in the directors any more than the directors can usurp the powers vested by the articles in the general body of shareholders.
It has been remarked that this development in the law was somewhat surprising at the time, as the relevant provisions in Table A (as it was then) seemed to contradict this approach rather than to endorse it.
Election and removal.
In most legal systems, the appointment and removal of directors is voted upon by the shareholders in general meeting or through a proxy statement. For publicly traded companies in the U.S., the directors which are available to vote on are largely selected by either the board as a whole or a nominating committee. Although in 2002 the New York Stock Exchange and the NASDAQ required that nominating committees consist of independent directors as a condition of listing, nomination committees have historically received input from management in their selections even when the CEO does not have a position on the board. Shareholder nominations can only occur at the general meeting itself or through the prohibitively expensive process of mailing out ballots separately; in May 2009 the SEC proposed a new rule allowing shareholders meeting certain criteria to add nominees to the proxy statement. In practice for publicly traded companies, the managers (inside directors) who are purportedly accountable to the board of directors have historically played a major role in selecting and nominating the directors who are voted on by the shareholders, in which case more "gray outsider directors" (independent directors with conflicts of interest) are nominated and elected.
Directors may also leave office by resignation or death. In some legal systems, directors may also be removed by a resolution of the remaining directors (in some countries they may only do so "with cause"; in others the power is unrestricted).
Some jurisdictions also permit the board of directors to appoint directors, either to fill a vacancy which arises on resignation or death, or as an addition to the existing directors.
In practice, it can be quite difficult to remove a director by a resolution in general meeting. In many legal systems, the director has a right to receive special notice of any resolution to remove him or her; the company must often supply a copy of the proposal to the director, who is usually entitled to be heard by the meeting. The director may require the company to circulate any representations that he wishes to make. Furthermore, the director's contract of service will usually entitle him to compensation if he is removed, and may often include a generous "golden parachute" which also acts as a deterrent to removal.
A recent study examines how corporate shareholders voted in director elections in the United States. It found that directors received fewer votes from shareholders when their companies performed poorly, had excess CEO compensation, or had poor shareholder protection. Also, directors received fewer votes when they did not regularly attend board meetings or received negative recommendations from a proxy advisory firm. The study also shows that companies often improve their corporate governance by removing poison pills or classified boards and by reducing excessive CEO pay after their directors receive low shareholder support.
Board accountability to shareholders is a recurring issue. In 2010, the "New York Times" noted that several directors who had overseen companies which had failed in the financial crisis of 2007–2010 had found new positions as directors. The SEC sometimes imposes a ban (a "D&O bar") on serving on a board as part of its fraud cases, and one of these was upheld in 2013.
Exercise of powers.
The exercise by the board of directors of its powers usually occurs in board meetings. Most legal systems require sufficient notice to be given to all directors of these meetings, and that a quorum must be present before any business may be conducted. Usually, a meeting which is held without notice having been given is still valid if all of the directors attend, but it has been held that a failure to give notice may negate resolutions passed at a meeting, because the persuasive oratory of a minority of directors might have persuaded the majority to change their minds and vote otherwise.
In most common law countries, the powers of the board are vested in the board as a whole, and not in the individual directors. However, in instances an individual director may still bind the company by his acts by virtue of his ostensible authority (see also: the rule in "Turquand's Case").
Duties.
Because directors exercise control and management over the organization, but organizations are (in theory) run for the benefit of the shareholders, the law imposes strict duties on directors in relation to the exercise of their duties. The duties imposed on directors are fiduciary duties, similar to those that the law imposes on those in similar positions of trust: agents and trustees.
The duties apply to each director separately, while the powers apply to the board jointly. Also, the duties are owed to the company itself, and not to any other entity. This does not mean that directors can never stand in a fiduciary relationship to the individual shareholders; they may well have such a duty in certain circumstances.
"Proper purpose".
Directors must exercise their powers for a proper purpose. While in many instances an improper purpose is readily evident, such as a director looking to feather his or her own nest or divert an investment opportunity to a relative, such breaches usually involve a breach of the director's duty to act in good faith. Greater difficulties arise where the director, while acting in good faith, is serving a purpose that is not regarded by the law as proper.
The seminal authority in relation to what amounts to a proper purpose is the Supreme Court decision in . The case concerned the powers of directors under the articles of association of the company to disenfranchise voting rights attached to shares for failure to properly comply with notice served on the shareholders. Prior to that case the leading authority was "Howard Smith Ltd v Ampol Ltd" AC 821. The case concerned the power of the directors to issue new shares. It was alleged that the directors had issued a large number of new shares purely to deprive a particular shareholder of his voting majority. An argument that the power to issue shares could only be properly exercised to raise new capital was rejected as too narrow, and it was held that it would be a proper exercise of the director's powers to issue shares to a larger company to ensure the financial stability of the company, or as part of an agreement to exploit mineral rights owned by the company. If so, the mere fact that an incidental result (even if it was a desired consequence) was that a shareholder lost his majority, or a takeover bid was defeated, this would not itself make the share issue improper. But if the sole purpose was to destroy a voting majority, or block a takeover bid, that would be an improper purpose.
Not all jurisdictions recognised the "proper purpose" duty as separate from the "good faith" duty however.
"Unfettered discretion".
Directors cannot, without the consent of the company, fetter their discretion in relation to the exercise of their powers, and cannot bind themselves to vote in a particular way at future board meetings. This is so even if there is no improper motive or purpose, and no personal advantage to the director.
This does not mean, however, that the board cannot agree to the company entering into a contract which binds the company to a certain course, even if certain actions in that course will require further board approval. The company remains bound, but the directors retain the discretion to vote against taking the future actions (although that may involve a breach by the company of the contract that the board previously approved).
"Conflict of duty and interest".
As fiduciaries, the directors may not put themselves in a position where their interests and duties conflict with the duties that they owe to the company. The law takes the view that good faith must not only be done, but must be manifestly seen to be done, and zealously patrols the conduct of directors in this regard; and will not allow directors to escape liability by asserting that his decision was in fact well founded. Traditionally, the law has divided conflicts of duty and interest into three sub-categories.
Transactions with the company.
By definition, where a director enters into a transaction with a company, there is a conflict between the director's interest (to do well for himself out of the transaction) and his duty to the company (to ensure that the company gets as much as it can out of the transaction). This rule is so strictly enforced that, even where the conflict of interest or conflict of duty is purely hypothetical, the directors can be forced to disgorge all personal gains arising from it. In "Aberdeen Ry v Blaikie" (1854) 1 Macq HL 461 Lord Cranworth stated in his judgment that:
However, in many jurisdictions the members of the company are permitted to ratify transactions which would otherwise fall foul of this principle. It is also largely accepted in most jurisdictions that this principle can be overridden in the company's constitution.
In many countries, there is also a statutory duty to declare interests in relation to any transactions, and the director can be fined for failing to make disclosure.
Use of corporate property, opportunity, or information.
Directors must not, without the informed consent of the company, use for their own profit the company's assets, opportunities, or information. This prohibition is much less flexible than the prohibition against the transactions with the company, and attempts to circumvent it using provisions in the articles have met with limited success.
In "Regal (Hastings) Ltd v Gulliver" All ER 378 the House of Lords, in upholding what was regarded as a wholly unmeritorious claim by the shareholders, held that:
And accordingly, the directors were required to disgorge the profits that they made, and the shareholders received their windfall.
The decision has been followed in several subsequent cases, and is now regarded as settled law.
Competing with the company.
Directors cannot compete directly with the company without a conflict of interest arising. Similarly, they should not act as directors of competing companies, as their duties to each company would then conflict with each other.
Common law duties of care and skill.
Traditionally, the level of care and skill which has to be demonstrated by a director has been framed largely with reference to the non-executive director. In "Re City Equitable Fire Insurance Co" Ch 407, it was expressed in purely subjective terms, where the court held that:
However, this decision was based firmly in the older notions (see above) that prevailed at the time as to the mode of corporate decision making, and effective control residing in the shareholders; if they elected and put up with an incompetent decision maker, they should not have recourse to complain.
However, a more modern approach has since developed, and in "Dorchester Finance Co Ltd v Stebbing" BCLC 498 the court held that the rule in "Equitable Fire" related only to skill, and not to diligence. With respect to diligence, what was required was:
This was a dual subjective and objective test, and one deliberately pitched at a higher level.
More recently, it has been suggested that both the tests of skill and diligence should be assessed objectively and subjectively; in the United Kingdom, the statutory provisions relating to directors' duties in the new Companies Act 2006 have been codified on this basis.
Remedies for breach of duty.
In most jurisdictions, the law provides for a variety of remedies in the event of a breach by the directors of their duties:
The future.
Historically, directors' duties have been owed almost exclusively to the company and its members, and the board was expected to exercise its powers for the financial benefit of the company. However, more recently there have been attempts to "soften" the position, and provide for more scope for directors to act as good corporate citizens. For example, in the United Kingdom, the Companies Act 2006 requires directors of companies "to promote the success of the company for the benefit of its members as a whole" and sets out the following six factors regarding a director's duty to promote success:
This represents a considerable departure from the traditional notion that directors' duties are owed only to the company. Previously in the United Kingdom, under the Companies Act 1985, protections for non-member stakeholders were considerably more limited (see for example, s.309 which permitted directors to take into account the interests of employees but which could only be enforced by the shareholders and not by the employees themselves). The changes have therefore been the subject of some criticism.
United States.
Sarbanes–Oxley Act.
The Sarbanes–Oxley Act of 2002 has introduced new standards of accountability on boards of U.S. companies or companies listed on U.S. stock exchanges. Under the Act, directors risk large fines and prison sentences in the case of accounting crimes. Internal control is now the direct responsibility of directors. The vast majority of companies covered by the Act have hired internal auditors to ensure that the company adheres to required standards of internal control. The internal auditors are required by law to report directly to an audit board, consisting of directors more than half of whom are outside directors, one of whom is a "financial expert."
The law requires companies listed on the major stock exchanges (NYSE, NASDAQ) to have a majority of independent directors—directors who are not otherwise employed by the firm or in a business relationship with it.
Size.
According to the Corporate Library's study, the average size of publicly traded company's board is 9.2 members, and most boards range from 3 to 31 members. According to Investopedia, some analysts think the ideal size is seven. State law may specify a minimum number of directors, maximum number of directors, and qualifications for directors (e.g. whether board members must be individuals or may be business entities).
Committees.
While a board may have several committees, two—the compensation committee and audit committee—are critical and must be made up of at least three independent directors and no inside directors. Other common committees in boards are nominating and governance.
Compensation.
Directors of Fortune 500 companies received median pay of $234,000 in 2011. Directorship is a part-time job. A recent National Association of Corporate Directors study found directors averaging just 4.3 hours a week on board work.
Criticism.
According to John Gillespie, a former investment banker and co-author of a book critical of boards, "Far too much of their time has been for check-the-box and cover-your-behind activities rather than real monitoring of executives and providing strategic advice on behalf of shareholders". At the same time, scholars have found that individual directors have a large effect on major corporate initiatives such as mergers and acquisitions and cross-border investments.

</doc>
<doc id="4823" url="https://en.wikipedia.org/wiki?curid=4823" title="Balkan Wars">
Balkan Wars

The Balkan Wars (, literally "the Balkan Wars" or "Balkan Faciası", meaning "the Balkan Tragedy") consisted of two conflicts that took place in the Balkan Peninsula in south-eastern Europe in 1912 and 1913. Four Balkan states defeated the Ottoman Empire in the first war; one of the four, Bulgaria, suffered defeat in the second war. The Ottoman Empire lost the bulk of its territory in Europe. Austria-Hungary, although not a combatant, became relatively weaker as a much enlarged Serbia pushed for union of the South Slavic peoples. The war set the stage for the Balkan crisis of 1914 and thus served as a "prelude to the First World War".
By the early 20th century, Bulgaria, Greece, Montenegro and Serbia had achieved independence from the Ottoman Empire, but large elements of their ethnic populations remained under Ottoman rule. In 1912 these countries formed the Balkan League. The First Balkan War had three main causes:
The Ottoman Empire lost all its European territories to the west of the River Maritsa as a result of the two Balkan Wars, which thus delineated present-day Turkey's western border. A large influx of Turks started to flee into the Ottoman heartland from the lost lands. By 1914, the remaining core region of the Ottoman Empire had experienced a population increase of around 2.5 million because of the flood of immigration from the Balkans.
Citizens of Turkey regard the Balkan Wars as a major disaster ("Balkan harbi faciası") in the nation's history. The unexpected fall and sudden relinquishing of Turkish-dominated European territories created a psycho-traumatic event amongst the Turks that is said to have triggered the ultimate collapse of the empire itself within five years. Nazım Pasha, Chief of Staff of the Ottoman army, was held responsible for the failure and was assassinated on 23 January 1913 during the 1913 Ottoman coup d'état carried out by the "Young Turks".
The First Balkan War broke out when the League member states attacked the Ottoman Empire on 8 October 1912 and ended seven months later with the signing of the Treaty of London on 30 May 1913. The Second Balkan War broke out on 16 June 1913. Both Serbia and Greece, utilizing the argument that the war had been prolonged, repudiated important particulars of the pre-war treaty and retained occupation of all the conquered districts in their possession which were to be divided according to specific predefined boundaries. Seeing the treaty as trampled, Bulgaria was dissatisfied over the division of the spoils in Macedonia (made in secret by its former allies, Serbia and Greece) and commenced military action against them. The more numerous combined Serbian and Greek armies repelled the Bulgarian offensive and counter-attacked into Bulgaria. Romania, who having taken no part in the conflict, had intact armies to strike with, invaded Bulgaria from the north in violation of a peace treaty between the two states. The Ottoman Empire also attacked Bulgaria and advanced in Thrace regaining Adrianople. In the resulting Treaty of Bucharest, Bulgaria lost most of the territories it had gained in the First Balkan War in addition to being forced to cede the ex-Ottoman south-third of Dobroudja province to Romania.
Background.
The background to the wars lies in the incomplete emergence of nation-states on the European territory of the Ottoman Empire during the second half of the 19th century. Serbia had gained substantial territory during the Russo-Turkish War, 1877–1878, while Greece acquired Thessaly in 1881 (although it lost a small area back to the Ottoman Empire in 1897) and Bulgaria (an autonomous principality since 1878) incorporated the formerly distinct province of Eastern Rumelia (1885). All three countries, as well as Montenegro, sought additional territories within the large Ottoman-ruled region known as Rumelia, comprising Eastern Rumelia, Albania, Macedonia, and Thrace.
Policies of the Great Powers.
Throughout the 19th century, the Great Powers shared different aims over the "Eastern Question" and the integrity of the Ottoman Empire. Russia wanted access to the "warm waters" of the Mediterranean from the Black Sea; it pursued a pan-Slavic foreign policy and therefore supported Bulgaria and Serbia. Britain wished to deny Russia access to the "warm waters" and supported the integrity of the Ottoman Empire, although it also supported a limited expansion of Greece as a backup plan in case integrity of the Empire was no longer possible. France wished to strengthen its position in the region, especially in the Levant (today's Lebanon, Syria, the Palestinian territories and Israel).
Habsburg-ruled Austria-Hungary wished for a continuation of the existence of the Ottoman Empire, since both were troubled multinational entities and thus the collapse of the one might weaken the other. The Habsburgs also saw a strong Ottoman presence in the area as a counterweight to the Serbian nationalistic call to their own Serb subjects in Bosnia, Vojvodina and other parts of the empire. Italy, it has been argued, wished to recreate the Roman empire, though its primary aim at the time seems to have been the denial of access to the Adriatic Sea to another major sea power. The German Empire, in turn, under the "Drang nach Osten" policy, aspired to turn the Ottoman Empire into its own de facto colony, and thus supported its integrity.
In the late 19th and early 20th century, Bulgaria and Greece contended for Ottoman Macedonia and Thrace. Ethnic Greeks sought the forced "Hellenization" of ethnic Bulgars, who sought "Bulgarization" of Greeks. Both nations sent armed irregulars into Ottoman territory to protect and assist their ethnic kindred. From 1904, there was low intensity warfare in Macedonia between the Greek and Bulgarian bands and the Ottoman army (the Struggle for Macedonia). After the Young Turk revolution of July 1908, the situation changed drastically.
Young Turk Revolution.
The 1908 Young Turk Revolution saw the reinstatement of constitutional monarchy in the Ottoman Empire and the start of the Second Constitutional Era. When the revolt broke out, it was supported by intellectuals, the army, and almost all the ethnic minorities of the Empire, and forced Sultan Abdul Hamid II to re-adopt the long defunct Ottoman constitution of 1876 and parliament. Hopes were raised among the Balkan ethnicities of reforms and autonomy, and elections were held to form a representative, multi-ethnic, Ottoman parliament. However, following the Sultan's attempted counter-coup, the liberal element of the Young Turks was sidelined and the nationalist element became dominant.
At the same time, in October 1908, Austria-Hungary seized the opportunity of the Ottoman political upheaval to annex the "de jure" Ottoman province of Bosnia and Herzegovina, which it had occupied since 1878 (see "Bosnian Crisis"). Bulgaria declared independence as it had done in 1878, but this time the independence was internationally recognised. The Greeks of the autonomous Cretan State proclaimed unification with Greece, though the opposition of the Great Powers prevented the latter action from taking practical effect. It has large influence in the consequent world order.
Reaction in the Balkan States.
Serbia was frustrated in the north by Austria-Hungary's incorporation of Bosnia. In March 1909, Serbia was forced to accept the annexation and restrain anti-Habsburg agitation by Serbian nationalists. Instead, the Serbian government looked to formerly Serb territories in the south, notably "Old Serbia" (the Sanjak of Novi Pazar and the province of Kosovo).
On 15 August 1909 the Military League, a group of Greek officers, took action against the government to reform their country's national government and reorganize the army. The Military League found itself unable to create a new political system, until the League summoned the Cretan politician Eleutherios Venizelos to Athens as its political adviser. Venizelos persuaded the king to revise the constitution and asked the League to disband in favor of a National Assembly. In March 1910 the Military League dissolved itself.
Bulgaria, which had secured Ottoman recognition of her independence in April 1909 and enjoyed the friendship of Russia, also looked to annex districts of Ottoman Thrace and Macedonia. In August 1910 Montenegro followed Bulgaria's precedent by becoming a kingdom.
Balkan League.
Following Italy's victory in the Italo-Turkish War of 1911–1912, the Young Turks fell from power after a coup. The Balkan countries saw this as an opportunity to attack the Ottoman Empire and fulfill their desires of expansion.
With the initial encouragement of Russian agents, a series of agreements was concluded between Serbia and Bulgaria in March 1912. Military victory against the Ottoman Empire would not be possible while it could bring reinforcements from Asia. The condition of the Ottoman railways of the time was not advanced, so most reinforcements would have to come by sea through the Aegean Sea. Greece was the only Balkan country with a navy powerful enough to deny use of the Aegean to the Ottoman Empire, thus a treaty between Greece and Bulgaria became necessary; it was signed in May 1912.
Montenegro concluded agreements between Serbia and Bulgaria later that year. Bulgaria signed treaties with Serbia to divide the territory of northern Macedonia.
This alliance between Greece, Serbia, Bulgaria, and Montenegro became known as the Balkan League; its existence was undesirable for all the Great Powers. The League was loose at best, though secret liaison officers were exchanged between the Greek and the Serbian army after the war began. Greece delayed the start of the war several times in the summer of 1912, to better prepare her navy, but Montenegro declared war on 8 October (25 September O.S.). Following an ultimatum to the Ottoman Empire, the remaining members of the alliance entered the conflict on 17 October.
First Balkan War.
The three Slavic allies (Bulgaria, Serbia and Montenegro) had laid out extensive plans to coordinate their war efforts, in continuation of their secret prewar settlements and under close Russian supervision (Greece was not included). Serbia and Montenegro would attack in the theater of Sandjak, Bulgaria and Serbia in Macedonia and Thrace.
The Ottoman Empire's situation was difficult. Its population of about 26 million people provided a massive pool of manpower, but ¾ of the population and nearly all of the Muslim component lived in the Asian part of the Empire. Reinforcements had to come from Asia mainly by sea, which depended on the result of battles between the Turkish and Greek navies in the Aegean.
With the outbreak of the war, the Ottoman Empire activated three Army HQs: the Thracian HQ in Constantinople, the Western HQ in Salonika, and the Vardar HQ in Skopje, against the Bulgarians, the Greeks and the Serbians respectively. Most of their available forces were allocated to these fronts. Smaller independent units were allocated elsewhere, mostly around heavily fortified cities.
Montenegro was the first that declared war on 8 October (25 September O.S.). Its main thrust was towards Shkodra, with secondary operations in the Novi Pazar area. The rest of the Allies, after giving a common ultimatum, declared war a week later. Bulgaria attacked towards Eastern Thrace, being stopped only at the outskirts of Constantinople at the Çatalca line and the isthmus of the Gallipoli peninsula, while secondary forces captured Western Thrace and Eastern Macedonia. Serbia attacked south towards Skopje and Monastir and then turned west to present-day Albania, reaching the Adriatic, while a second Army captured Kosovo and linked with the Montenegrin forces. Greece's main forces attacked from Thessaly into Macedonia through the Sarantaporo strait and after capturing Thessaloniki on 12 November (on 26 October 1912, O.S.) expanded its occupied area and linked up with the Serbian army to the northwest, while its main forces turned east towards Kavala, reaching the Bulgarians. Another Greek army attacked into Epirus towards Ioannina.
In the naval front the Ottoman fleet twice exited the Dardanelles and was twice defeated by the Greek Navy, in the battles of Elli and Lemnos. Greek dominance on the Aegean Sea made it impossible for the Ottomans to transfer the planned troops from the Middle East to the Thracian (against the Bulgarian) and to the Macedonian (against the Greeks and Serbians) fronts. According to the E.J. Erickson the Greek Navy also played a crucial, albeit indirect role, in the Thracian campaign by neutralizing no less than three Thracian Corps (see First Balkan War, the Bulgarian theater of operations), a significant portion of the Ottoman Army there, in the all-important opening round of the war. After the defeating of the Ottoman fleet the Greek Navy was also free to liberate the islands of the Aegean. General Nikola Ivanov identified the activity of the Greek Navy as the chief factor in the general success of the allies.
In January, after a successful coup by young army officers, the Ottoman Empire decided to continue the war. After a failed Ottoman counter-attack in the Western-Thracian front, Bulgarian forces, with the help of the Serbian Army, managed to conquer Adrianople while Greek forces managed to take Ioannina after defeating the Ottomans in the battle of Bizani. In the joint Serbian-Montenegrin theater of operation, the Montenegrin army besieged and captured the Shkodra, ending the Ottoman presence in Europe west of the Çatalca line after nearly 500 years. The war ended officially with the Treaty of London on 30(17) May 1913.
Second Balkan War.
Though the Balkan allies had fought together against the common enemy, that was not enough to overcome their mutual rivalries. In the original document for the Balkans league, Serbia promised Bulgaria most of Macedonia. But before the first war come to an end, Serbia (in violation of the previous agreement) and Greece, revealed their plan to keep possession of the territories that their forces had occupied. This act prompted the tsar of Bulgaria to invade his allies. The Second Balkan War broke out on 29(16) June 1913 when Bulgaria attacked its erstwhile allies in the First Balkan War, Serbia and Greece, while Montenegro and the Ottoman Empire intervened later against Bulgaria, with Romania attacking Bulgaria from the north. When the Greek army entered Thessaloniki in the First Balkan War ahead of the Bulgarian 7th division by only a day, they were asked to allow a Bulgarian battalion to enter the city. Greece accepted in exchange for allowing a Greek unit to enter the city of Serres.
The Bulgarian unit that entered Thessaloniki turned out to be a 18,000-strong division instead of the battalion, something which caused concern among the Greeks, who viewed it as a Bulgarian attempt to establish a condominium over the city. In the event, due to the urgently needed reinforcements in the Thracian front, Bulgarian Headquarters was soon forced to remove its troops from the city (while the Greeks agreed by mutual treaty to remove their units based in Serres) and transport them to Dedeağaç (modern Alexandroupolis), but still it left behind a battalion that started fortifying its positions.
Greece had also allowed the Bulgarians to control the stretch of the Thessaloniki-Constantinople railroad that lay in Greek-occupied territory, since Bulgaria controlled the largest part of this railroad towards Thrace. After the end of the operations in Thrace—and confirming Greek concerns—Bulgaria was not satisfied with the territory it controlled in Macedonia and immediately asked Greece to relinquish its control over Thessaloniki and the land north of Pieria, effectively handing over all Aegean Macedonia. These unacceptable demands together with the Bulgarian refusal to demobilize its army after the Treaty of London had ended the common war against the Ottomans and alarmed Greece, which decided also to maintain its army's mobilization.
Similarly, in northern Macedonia, the tension between Serbia and Bulgaria due to later aspirations over Vardar Macedonia generated many incidents between the nearby armies, prompting Serbia to maintain its army's mobilization. Serbia and Greece proposed that each of the three countries reduce its army by one fourth, as a first step to facilitate a peaceful solution, but Bulgaria rejected it.
Seeing the omens, Greece and Serbia started a series of negotiations and signed a treaty on 1 June(19 May) 1913. With this treaty, a mutual border was agreed between the two countries, together with an agreement for mutual military and diplomatic support in case of a Bulgarian or/and Austro-Hungarian attack. Tsar Nicholas II of Russia, being well informed, tried to stop the upcoming conflict on 8 June, by sending an identical personal message to the Kings of Bulgaria and Serbia, offering to act as arbitrator according to the provisions of the 1912 Serbo-Bulgarian treaty. But Bulgaria, by making the acceptance of Russian arbitration conditional, in effect denied any discussion and caused Russia to repudiate its alliance with Bulgaria (see Russo-Bulgarian military convention signed 31 May 1902).
The Serbs and the Greeks had a military advantage on the eve of the war because their armies confronted comparatively weak Ottoman forces in the First Balkan War and suffered relatively light casualties while the Bulgarians were involved in heavy fighting in Thrace. The Serbs and the Greeks had time to fortify their positions in Macedonia. The Bulgarians also held some advantages, controlling internal communication and supply lines.
On 29(16) June 1913 General Savov, under direct orders of Tsar Ferdinand I, issued attacking orders against both Greece and Serbia without consulting the Bulgarian government and without any official declaration of war. During the night of 30(17) June 1913 they attacked the Serbian army at Bregalnica river and then the Greek army in Nigrita. The Serbian army resisted the sudden night attack, while most of soldiers did not even know who they were fighting with, as Bulgarian camps were located next to Serbs and were considered allies. Montenegro's forces were just a few kilometers away and also rushed to the battle. The Bulgarian attack was halted.
The Greek army was also successful. It retreated according to plan for two days while Thessaloniki was cleared of the remaining Bulgarian regiment. Then the Greek army counter-attacked and defeated the Bulgarians at Kilkis-Lahanas (Kukush), after which the mostly Bulgarian town was destroyed and its population expelled. The Greek army destroyed altogether 161 Bulgarian villages and massacred thousands of inhabitants. Following the capture of Kilkis, the Greek army's pace was not quick enough to prevent the destruction of Nigrita, Serres, and Doxato and massacres of non-combatant Greek inhabitants at Demir Hisar and Doxato by the Bulgarian army. The Greek army then divided its forces and advanced in two directions. Part proceeded east and occupied Western Thrace. The rest of the Greek army advanced up to the Struma River valley, defeating the Bulgarian army in the battles of Doiran and Mt. Beles, and continued its advance to the north towards Sofia. In the Kresna straits the Greeks were ambushed by the Bulgarian 2nd and 1st Army newly arrived from the Serbian front that had already taken defensive positions there following the Bulgarian victory at Kalimanci.
By 30 July the Greek army was outnumbered by the counter-attacking Bulgarian army, which attempted to encircle the Greeks in a Cannae-type battle, by applying pressure on their flanks. The Greek army was exhausted and faced logistical difficulties. The battle was continued for 11 days, between 29 July and 9 August over 20 km of a maze of forests and mountains with no conclusion. The Greek King, seeing that the units he fought were from the Serbian front, tried to convince the Serbs to renew their attack, as the front ahead of them was now thinner, but the Serbs rejected it. By then, news came of the Romanian advance toward Sofia and its imminent fall. Facing the danger of encirclement, Constantine realized that his army could no longer continue hostilities, agreed to Eleftherios Venizelos' proposal and accepted the Bulgarian request for armistice as this had been communicated through Romania.
Romania had raised an army and declared war on Bulgaria on 10 July(27 June) as it had from 28(15) June officially warned Bulgaria that it would not remain neutral in a new Balkan war, due to Bulgaria's refusal to cede the fortress of Silistra as promised before the First Balkan war in exchange for Romanian neutrality. Its forces encountered little resistance and by the time the Greeks accepted the Bulgarian request for armistice they had reached Vrazhdebna, 7 miles from the center of Sofia.
Seeing the military position of the Bulgarian army the Ottomans decided to intervene. They attacked and finding no opposition, managed to recover eastern Thrace with its fortified city of Adrianople, regaining an area in Europe which was only slightly larger than the present-day European territory of the Republic of Turkey.
Reactions among the Great Powers during the wars.
The developments that led to the First Balkan War did not go unnoticed by the Great Powers, but although there was an official consensus between the European Powers over the territorial integrity of the Ottoman Empire, which led to a stern warning to the Balkan states, unofficially each of them took a different diplomatic approach due to their conflicting interests in the area. As a result, any possible preventive effect of the common official warning was cancelled by the mixed unofficial signals, and failed to prevent or to stop the war:
The Second Balkan war was a catastrophic blow to Russian policies in the Balkans, where Russia had focused its interests for access to the "warm seas" for centuries. First, it marked the end of the Balkan League, a vital arm of the Russian system of defense against Austria-Hungary. Second, the clearly pro-Serbian position Russia had been forced to take in the conflict, mainly due to Bulgaria's uncompromising aggressiveness, caused a permanent break-up between the two countries. Accordingly, Bulgaria reverted its policy to one closer to the Central Powers' understanding over an anti-Serbian front, due to its new national aspirations, now expressed mainly against Serbia. As a result, Serbia was isolated militarily against its rival Austria-Hungary, a development that eventually doomed Serbia in the coming war a year later. But most damaging, the new situation effectively trapped Russian foreign policy: After 1913, Russia could not afford losing its last ally in this crucial area and thus had no alternatives but to unconditionally support Serbia when the crisis between Serbia and Austria broke out in 1914. This was a position that inevitably drew her, although unwillingly, into a World War with devastating results for her, since she was less prepared (both militarily and socially) for that event than any other Great Power.
Austria-Hungary took alarm at the great increase in Serbia's territory at the expense of its national aspirations in the region, as well as Serbia's rising status, especially to Austria-Hungary's Slavic populations. This concern was shared by Germany, which saw Serbia as a satellite of Russia. This contributed significantly to the two Central Powers' willingness to go to war as soon as possible.
Finally, when a Serbian backed organization assassinated the heir of the Austro-Hungarian throne, causing the 1914 July Crisis, nobody could stop the conflict and the First World War broke out.
Aftermath.
Soviet demographer Boris Urlanis estimated in "Voini I Narodo-Nacelenie Europi" (1960) that in the first and second Balkan wars there were 122,000 killed in action, 20,000 dead of wounds, and 82,000 dead of disease.
See also.
Since the area has been referred to as the Balkans, notable conflicts have included the following:

</doc>
<doc id="4824" url="https://en.wikipedia.org/wiki?curid=4824" title="Buffalo">
Buffalo

Buffalo (or buffaloe) may refer to:

</doc>
<doc id="4825" url="https://en.wikipedia.org/wiki?curid=4825" title="BeBox">
BeBox

The BeBox is a dual CPU personal computer, briefly sold by Be Inc. to run the company's own operating system, BeOS. Notable aspects of the system include its CPU configuration, I/O board with "GeekPort", and "Blinkenlights" on the front bezel.
The BeBox made its debut in October 1995 (BeBox Dual603-66). The processors were upgraded to 133 MHz in August 1996 (BeBox Dual603e-133). Production was halted in January 1997, following the port of BeOS to the Macintosh, in order for the company to concentrate on software. Be sold around 1000 66 MHz BeBoxes and 800 133 MHz BeBoxes.
CPU configuration.
Initial prototypes are equipped with two AT&T Hobbit processors and three AT&T 9308S DSPs.
Production models use two PowerPC 603 processors running at 66 or 133 MHz to power the BeBox. Prototypes having dual 200 MHz CPUs or four CPUs exist, but these were never publicly available.
"Blinkenlights".
Two yellow/green vertical LED arrays, dubbed the "blinkenlights", are built into the front bezel to illustrate the CPU load. The bottommost LED on the right side indicates hard disk activity.

</doc>
<doc id="4827" url="https://en.wikipedia.org/wiki?curid=4827" title="Biomedical engineering">
Biomedical engineering

Biomedical engineering (BME) is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g. diagnostic or therapeutic). This field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical and biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy.
Biomedical engineering has only recently emerged as its own study, compared to many other engineering fields. Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EEGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.
History.
Biomedical engineering has existed for centuries, perhaps even thousands of years. Researchers said the wear on the bottom surface suggests that it could be the oldest known limb prosthesis.
Origin.
Biomedical engineering originated during World War II. Biologists were needed to do work involving advances on radar technology, which led them to the electronic developments in medicine. Due to these developments, the next generation of biologists could not benefit from this technology, as they couldn’t understand it. A bridge was needed to fill the gap between technical knowledge and biology. Doctors and biologists who were interested in engineering and electrical engineers interested in biology became the first bioengineers. Those primarily concerned with medicine became the first biomedical engineers. The unique mix of engineering, medicine and science in biomedical engineering emerged alongside biophysics and medical physics early this century.
Major milestones.
Biomedical engineering achievements range from early devices, such as crutches, platform shoes, and wooden teeth to more modern equipment, including pacemakers, heart-lung machine, dialysis machines, diagnostic equipment, imaging technologies of every kind, and artificial organs, medical implants and advanced prosthetics.
Bioinformatics.
Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data.
Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.
Biomechanics.
See Biomechanics.
Biomaterial.
A biomaterial is any matter, surface, or construct that interacts with living systems. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science or biomaterials engineering. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.
Tissue engineering.
Tissue engineering, like genetic engineering (see below), is a major segment of biotechnology - which overlaps significantly with BME.
One of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. Biomedical engineers are currently researching methods of creating such organs. Researchers have grown solid jawbones and tracheas from human stem cells towards this end. Several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients. Bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.
Genetic engineering.
Genetic engineering, recombinant DNA technology, genetic modification/manipulation (GM) and gene splicing are terms that apply to the direct manipulation of an organism's genes. Unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. Genetic engineering techniques have found success in numerous applications. Some examples include the improvement of crop technology ("not a medical application", but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.
Neural engineering.
Neural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. Neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.
Pharmaceutical engineering.
Pharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of Chemical Engineering, and Pharmaceutical Analysis. It may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment. The ISPE is an international body that certifies this now rapidly emerging interdisciplinary science.
Medical devices.
This is an "extremely broad category"—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism.
A medical device is intended for use in:
Some examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.
Stereolithography is a practical example of "medical modeling" being used to create physical objects. Beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies, treatments, patient monitoring, of complex diseases.
Medical devices are regulated and classified (in the US) as follows (see also "Regulation"):
Medical imaging.
Medical/biomedical imaging is a major segment of medical devices. This area deals with enabling clinicians to directly or indirectly "view" things not visible in plain sight (such as due to their size, and/or location). This can involve utilizing ultrasound, magnetism, UV, radiology, and other means.
Imaging technologies are often essential to medical diagnosis, and are typically the most complex equipment found in a hospital including: fluoroscopy, magnetic resonance imaging (MRI), nuclear medicine, positron emission tomography (PET), PET-CT scans, projection radiography such as X-rays and CT scans, tomography, ultrasound, optical microscopy, and electron microscopy.
Implants.
An implant is a kind of medical device made to replace and act as a missing biological structure (as compared with a transplant, which indicates transplanted biomedical tissue). The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone or apatite depending on what is the most functional. In some cases, implants contain electronics, e.g. artificial pacemakers and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.
Bionics.
Artificial body part replacements are one of the many applications of bionics. Concerned with the intricate and thorough study of the properties and function of human body systems, bionics may be applied to solve some engineering problems. Careful study of the different functions and processes of the eyes, ears, and other organs paved the way for improved cameras, television, radio transmitters and receivers, and many other useful tools. These developments have indeed made our lives better, but the best contribution that bionics has made is in the field of biomedical engineering (the building of useful replacements for various parts of the human body). Modern hospitals now have available spare parts to replace body parts badly damaged by injury or disease. Biomedical engineers work hand in hand with doctors to build these artificial body parts.
Clinical engineering.
Clinical engineering is the branch of biomedical engineering dealing with the actual implementation of medical equipment and technologies in hospitals or other clinical settings. Major roles of clinical engineers include training and supervising biomedical equipment technicians (BMETs), selecting technological products/services and logistically managing their implementation, working with governmental regulators on inspections/audits, and serving as technological consultants for other hospital staff (e.g. physicians, administrators, I.T., etc.). Clinical engineers also advise and collaborate with medical device producers regarding prospective design improvements based on clinical experiences, as well as monitor the progression of the state of the art so as to redirect procurement patterns accordingly.
Their inherent focus on "practical" implementation of technology has tended to keep them oriented more towards "incremental"-level redesigns and reconfigurations, as opposed to revolutionary research & development or ideas that would be many years from clinical adoption; however, there is a growing effort to expand this time-horizon over which clinical engineers can influence the trajectory of biomedical innovation. In their various roles, they form a "bridge" between the primary designers and the end-users, by combining the perspectives of being both 1) close to the point-of-use, while 2) trained in product and process engineering. Clinical Engineering departments will sometimes hire not just biomedical engineers, but also industrial/systems engineers to help address operations research/optimization, human factors, cost analysis, etc. Also see safety engineering for a discussion of the procedures used to design safe systems.
Rehabilitation engineering.
Rehabilitation engineering is the systematic application of engineering sciences to design, develop, adapt, test, evaluate, apply, and distribute technological solutions to problems confronted by individuals with disabilities. Functional areas addressed through rehabilitation engineering may include mobility, communications, hearing, vision, and cognition, and activities associated with employment, independent living, education, and integration into the community.
While some rehabilitation engineers have master's degrees in rehabilitation engineering, usually a subspecialty of Biomedical engineering, most rehabilitation engineers have undergraduate or graduate degrees in biomedical engineering, mechanical engineering, or electrical engineering. A Portuguese university provides an undergraduate degree and a master's degree in Rehabilitation Engineering and Accessibility. Qualification to become a Rehab' Engineer in the UK is possible via a University BSc Honours Degree course such as Health Design & Technology Institute, Coventry University.
The rehabilitation process for people with disabilities often entails the design of assistive devices such as Walking aids intended to promote inclusion of their users into the mainstream of society, commerce, and recreation.
Regulatory issues.
Regulatory issues have been constantly increased in the last decades to respond to the many incidents caused by devices to patients. For example, from 2008 to 2011, in US, there were 119 FDA recalls of medical devices classified as class I. According to U.S. Food and Drug Administration (FDA), Class I recall is associated to “a" situation in which there is a reasonable probability that the use of, or exposure to, a product will cause serious adverse health consequences or death"“ 
Regardless of the country-specific legislation, the main regulatory objectives coincide worldwide. For example, in the medical device regulations, a product must be: 1) safe "and" 2) effective and 3) for all the manufactured devices
A product is safe if patients, users and third parties do not run unacceptable risks of physical hazards (death, injuries, …) in its intended use. Protective measures have to be introduced on the devices to reduce residual risks at acceptable level if compared with the benefit derived from the use of it.
A product is effective if it performs as specified by the manufacturer in the intended use. Effectiveness is achieved through clinical evaluation, compliance to performance standards or demonstrations of substantial equivalence with an already marketed device.
The previous features have to be ensured for all the manufactured items of the medical device. This requires that a quality system shall be in place for all the relevant entities and processes that may impact safety and effectiveness over the whole medical device lifecyle.
The medical device engineering area is among the most heavily regulated fields of engineering, and practicing biomedical engineers must routinely consult and cooperate with regulatory law attorneys and other experts. The Food and Drug Administration (FDA) is the principal healthcare regulatory authority in the United States, having jurisdiction over medical "devices, drugs, biologics, and combination" products. The paramount objectives driving policy decisions by the FDA are safety and effectiveness of healthcare products that have to be assured through a quality system in place as specified under 21 CFR 829 regulation. In addition, because biomedical engineers often develop devices and technologies for "consumer" use, such as physical therapy devices (which are also "medical" devices), these may also be governed in some respects by the Consumer Product Safety Commission. The greatest hurdles tend to be 510K "clearance" (typically for Class 2 devices) or pre-market "approval" (typically for drugs and class 3 devices).
In the European context, safety effectiveness and quality is ensured through the "Conformity Assessment" that is defined as "the method by which a manufacturer demonstrates that its device complies with the requirements of the European Medical Device Directive". The directive specifies different procedures according to the class of the device ranging from the simple Declaration of Conformity (Annex VII) for Class I devices to EC verification (Annex IV), Production quality assurance (Annex V), Product quality assurance (Annex VI) and Full quality assurance (Annex II). The Medical Device Directive specifies detailed procedures for Certification. In general terms, these procedures include tests and verifications that are to be contained in specific deliveries such as the risk management file, the technical file and the quality system deliveries. The risk management file is the first deliverable that conditions the following design and manufacturing steps. Risk management stage shall drive the product so that product risks are reduced at an acceptable level with respect to the benefits expected for the patients for the use of the device. The technical file contains all the documentation data and records supporting medical device certification. FDA technical file has similar content although organized in different structure. The Quality System deliverables usually includes procedures that ensure quality throughout all product life cycle. The same standard (ISO EN 13485) is usually applied for quality management systems in US and worldwide.
In the European Union, there are certifying entities named "Notified Bodies", accredited by European Member States. The Notified Bodies must ensure the effectiveness of the certification process for all medical devices apart from the class I devices where a declaration of conformity produced by the manufacturer is sufficient for marketing. Once a product has passed all the steps required by the Medical Device Directive, the device is entitled to bear a CE marking, indicating that the device is believed to be safe and effective when used as intended, and, therefore, it can be marketed within the European Union area.
The different regulatory arrangements sometimes result in particular technologies being developed first for either the U.S. or in Europe depending on the more favorable form of regulation. While nations often strive for substantive harmony to facilitate cross-national distribution, philosophical differences about the "optimal extent" of regulation can be a hindrance; more restrictive regulations seem appealing on an intuitive level, but critics decry the tradeoff cost in terms of slowing access to life-saving developments.
RoHS II.
Directive 2011/65/EU, better known as RoHS 2 is a recast of legislation originally introduced in 2002. The original EU legislation “Restrictions of Certain Hazardous Substances in Electrical and Electronics Devices” (RoHS Directive 2002/95/EC) was replaced and superseded by 2011/65/EU published in July 2011 and commonly known as RoHS 2.
RoHS seeks to limit the dangerous substances in circulation in electronics products, in particular toxins and heavy metals, which are subsequently released into the environment when such devices are recycled.
The scope of RoHS 2 is widened to include products previously excluded, such as medical devices and industrial equipment. In addition, manufacturers are now obliged to provide conformity risk assessments and test reports – or explain why they are lacking. For the first time, not only manufacturers, but also importers and distributors share a responsibility to ensure Electrical and Electronic Equipment within the scope of RoHS comply with the hazardous substances limits and have a CE mark on their products.
IEC 60601.
The new International Standard IEC 60601 for home healthcare electro-medical devices defining the requirements for devices used in the home healthcare environment. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series.
The mandatory date for implementation of the EN European version of the standard is June 1, 2013. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.
Training and certification.
Education.
Biomedical engineers require considerable knowledge of both engineering and biology, and typically have a Master's (M.S., M.Tech, M.S.E., or M.Eng.) or a Doctoral (Ph.D.) degree in BME (Biomedical Engineering) or another branch of engineering with considerable potential for BME overlap. As interest in BME increases, many engineering colleges now have a Biomedical Engineering Department or Program, with offerings ranging from the undergraduate (B.Tech,B.S., B.Eng or B.S.E.) to doctoral levels. Biomedical engineering has only recently been emerging as "its own discipline" rather than a cross-disciplinary hybrid specialization of other disciplines; and BME programs at all levels are becoming more widespread, including the Bachelor of Science in Biomedical Engineering which actually includes so much biological science content that many students use it as a "pre-med" major in preparation for medical school. The number of biomedical engineers is expected to rise as both a cause and effect of improvements in medical technology.
In the U.S., an increasing number of undergraduate programs are also becoming recognized by ABET as accredited bioengineering/biomedical engineering programs. Over 65 programs are currently accredited by ABET. Case Western Reserve created the first MD/PhD program in 1969 to develop engineers with a physician’s perspective.
In Canada and Australia, accredited graduate programs in Biomedical Engineering are common, for example in Universities such as McMaster University, and the first Canadian undergraduate BME program at Ryerson University offering a four-year B.Eng program. The Polytechnique in Montreal is also offering a bachelors's degree in biomedical engineering.
As with many degrees, the reputation and ranking of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees are also linked to the institution's graduate or research programs, which have some tangible factors for rating, such as research funding and volume, publications and citations. With BME specifically, the ranking of a university's hospital and medical school can also be a significant factor in the perceived prestige of its BME department/program.
Graduate education is a particularly important aspect in BME. While many engineering fields (such as mechanical or electrical engineering) do not need graduate-level training to obtain an entry-level job in their field, the majority of BME positions do prefer or even require them. Since most BME-related professions involve scientific research, such as in pharmaceutical and medical device development, graduate education is almost a requirement (as undergraduate degrees typically do not involve sufficient research training and experience). This can be either a Masters or Doctoral level degree; while in certain specialties a Ph.D. is notably more common than in others, it is hardly ever the majority (except in academia). In fact, the perceived need for some kind of graduate credential is so strong that some undergraduate BME programs will actively discourage students from majoring in BME without an expressed intention to also obtain a master's degree or apply to medical school afterwards.
Graduate programs in BME, like in other scientific fields, are highly varied, and particular programs may emphasize certain aspects within the field. They may also feature extensive collaborative efforts with programs in other fields (such as the University's Medical School or other engineering divisions), owing again to the interdisciplinary nature of BME. M.S. and Ph.D. programs will typically require applicants to have an undergraduate degree in BME, or "another engineering" discipline (plus certain life science coursework), or "life science" (plus certain engineering coursework).
Education in BME also varies greatly around the world. By virtue of its extensive biotechnology sector, its numerous major universities, and relatively few internal barriers, the U.S. has progressed a great deal in its development of BME education and training opportunities. Europe, which also has a large biotechnology sector and an impressive education system, has encountered trouble in creating uniform standards as the European community attempts to supplant some of the national jurisdictional barriers that still exist. Recently, initiatives such as BIOMEDEA have sprung up to develop BME-related education and professional standards. Other countries, such as Australia, are recognizing and moving to correct deficiencies in their BME education. Also, as high technology endeavors are usually marks of developed nations, some areas of the world are prone to slower development in education, including in BME.
Licensure/certification.
Engineering licensure in the US is largely optional, and rarely specified by branch/discipline. As with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered Professional Engineer (PE), but in practice such a license is not required to practice in the majority of situations (due to an exception known as the private industry exemption, which effectively applies to the vast majority of American engineers). This is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine.
Biomedical engineering is regulated in some countries, such as Australia, but registration is typically only recommended and not required.
In the UK, mechanical engineers working in the areas of Medical Engineering, Bioengineering or Biomedical engineering can gain Chartered Engineer status through the Institution of Mechanical Engineers. The Institution also runs the Engineering in Medicine and Health Division. The Institute of Physics and Engineering in Medicine (IPEM) has a panel for the accreditation of MSc courses in Biomedical Engineering and Chartered Engineering status can also be sought through IPEM.
The Fundamentals of Engineering exam – the first (and more general) of two licensure examinations for most U.S. jurisdictions—does now cover biology (although technically not BME). For the second exam, called the Principles and Practices, Part 2, or the Professional Engineering exam, candidates may select a particular engineering discipline's content to be tested on; there is currently not an option for BME with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). However, the Biomedical Engineering Society (BMES) is, as of 2009, exploring the possibility of seeking to implement a BME-specific version of this exam to facilitate biomedical engineers pursuing licensure.
Beyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. One such example is the Certified Clinical Engineer (CCE) certification for Clinical engineers.
Career prospects.
In 2012 there were about 19,400 biomedical engineers employed in the US, and the field was predicted to grow by 27% (Much faster than average) from 2012-2022. Biomedical engineering has the highest percentage of women engineers compared to other common engineering professions.

</doc>
<doc id="4829" url="https://en.wikipedia.org/wiki?curid=4829" title="Balkans">
Balkans

The Balkan Peninsula and the Balkans is a peninsula and a cultural area in Southeast Europe with different and disputed borders. The region takes its name from the Balkan Mountains that stretch from the east of Serbia to the Black Sea at the east of Bulgaria.
The Balkans meet the Adriatic Sea on the northwest, Ionian Sea on the southwest, the Mediterranean and Aegean Sea on the south and southeast, and the Black Sea on the east and northeast. The highest point of the Balkans is Mount Musala on the Rila mountain range in Bulgaria.
Name.
Antiquity and early Middle Ages.
From Antiquity through the Middle Ages, the Balkan Mountains had been called by the local Thracian name "Haemus". According to Greek mythology, the Thracian king Haemus was turned into a mountain by Zeus as a punishment and the mountain has remained with his name. A reverse name scheme has also been suggested. D. Dechev considers that Haemus (Αἷμος) is derived from a Thracian word "*saimon", 'mountain ridge'. A third possibility is that "Haemus" () derives from the Greek word "haema" () meaning 'blood'. The myth relates to a fight between Zeus and the monster/titan Typhon. Zeus injured Typhon with a thunder bolt and Typhon's blood fell on the mountains from which they got their name.
Late Middle Ages and Ottoman period.
The earliest mention of the name appears in an early 14th-century Arab map, in which the Haemus mountains are referred to as "Balkan". The first attested time the name "Balkan" was used in the West for the mountain range in Bulgaria was in a letter sent in 1490 to Pope Innocent VIII by Buonaccorsi Callimaco, an Italian humanist, writer and diplomat. The Ottomans first mention it in a document dated from 1565. There has been no other documented usage of the word to refer to the region before that, despite the fact that other Turkic tribes had already settled earlier or were passing through the Peninsula. There exists also a claim about an earlier Bulgar Turkic origin of the word popular in Bulgaria, however it is only an unscholary assertion. The word was used by the Ottomans in Rumelia in its general meaning of mountain, as in "Kod̲j̲a-Balkan", "Čatal-Balkan", and "Ungurus-Balkani̊", but especially it was applied to the Haemus mountain. In Turkish Balkan means "a chain of wooded mountains" (""), while in Bulgarian language the word "balkan" (балкан) means "mountain". Another possibility to its etymology is related to Persian "bālk" meaning "mud", and the Turkish suffix "an", i.e. "swampy forest". The name is still preserved in Central Asia with the Balkan Daglary (Balkan Mountains) and the Balkan Province of Turkmenistan. A less popular hypothesis regarding its etymology is that it derived from the Persian "Balā-Khān"a meaning "big, high, house". English traveler John Morritt introduced this term into the English literature at the end of the 18th-century, and other authors started applying the name to the wider area between the Adriatic and the Black Sea. The concept of the "Balkans" was created by the German geographer August Zeune in 1808. During the 1820s, "Balkan became the preferred although not yet exclusive term alongside Haemus among British travelers... Among Russian travelers not so burdened by classical toponymy, Balkan was the preferred term."
Evolution of meaning.
As time passed, the term gradually acquired political connotations far from its initial geographic meaning, arising from political changes from the late 19th-century to the creation of post–World War I Yugoslavia (initially the Kingdom of Serbs, Croats and Slovenes). Zeune's goal was to have a geographical parallel term to the Italic and Iberian Peninsula, and seemingly nothing more. The gradually acquired political connotations are newer and, to a large extent, due to oscillating political circumstances.
After the dissolution of Yugoslavia beginning in June 1991, the term "Balkans" again received a negative meaning, especially in Croatia and Slovenia, even in casual usage (see Balkanization).
Southeast Europe.
In part due to the historical and political connotations of the term "Balkans", especially since the military conflicts of the 1990s, the term "Southeast Europe" is becoming increasingly popular even though it literally refers to a much larger area and thus isn't as precise. A European Union initiative of 1999 is called the "Stability Pact for South Eastern Europe", and the online newspaper "Balkan Times" renamed itself "Southeast European Times" in 2003.
Current.
In the languages of the region, the peninsula is known as:
Definitions and boundaries.
The Balkan Peninsula.
The Balkan Peninsula is an area of southeastern Europe surrounded by water on three sides: the Adriatic Sea to the west, the Mediterranean Sea (including the Ionian and Aegean seas) and the Marmara Sea to the south and the Black Sea to the east. Its northern boundary is often given as the Danube, Sava and Kupa Rivers. The Balkan Peninsula has a combined area of about (slightly smaller than Spain). Note: Balkan peninsula largely coincides with Southeastern Europe.
As of 1920 until World War II, Italy included Istria and some Dalmatian areas (like "Zara", known as Zadar) that are within the general definition of the Balkan peninsula. The current territory of Italy includes only the small area around Trieste inside the Balkan Peninsula. However, the regions of Trieste and Istria are not usually considered part of the Balkans by Italian geographers, due to a definition of the Balkans that limits its western border to the Kupa River.
Share of land area within the Balkan Peninsula by country by the Danube-Sava definition:
Entirely within the Balkans:
Mostly or partially Balkan states:
The Balkans.
The abstract term "The Balkans", unlike the geographical borders of the Peninsula, is defined by the political borders of the states comprising it. The term is used to describe areas beyond the Balkan Peninsula, or inversely in the case of the part of Italy in the Peninsula, which is always excluded from the Balkans and as a totality is generally accepted as part of Western Europe and the Apennines.
According to the "Encyclopaedia Britannica", the Balkans are usually said to comprise Albania, Bosnia and Herzegovina, Bulgaria, Croatia, Kosovo, the Republic of Macedonia, Montenegro, Romania, Serbia, Slovenia, while Greece and Turkey are often included (depending on the definition), and its total area is usually given as 666,700 square km (257,400 square miles) and the population as 59,297,000 (est. 2002).
According to an earlier version of the "Britannica", the Balkans comprise the territories of the states of Albania, Bosnia and Herzegovina, Bulgaria, Croatia, Greece, Kosovo, the Republic of Macedonia, Montenegro, Romania, Serbia, Slovenia and the European part of Turkey; it notes Turkey as a non-Balkan state and the inclusion of Slovenia and the Transylvanian part of Romania in the region as dubious.
Inclusion of Balkan states in other regions:
Western Balkans.
The institutions of the European Union and its member states have defined the "Western Balkans" as the south-east European area that includes countries that are not members of the European Union, while others refer to the geographical aspects.
Nature and natural resources.
Most of the area is covered by mountain ranges running from the northwest to southeast. The main ranges are the Balkan mountains, running from the Black Sea coast in Bulgaria to its border with Serbia, the Rhodope mountains in southern Bulgaria and northern Greece, the Dinaric Alps in Bosnia and Herzegovina, Croatia and Montenegro, the Šar massif which spreads from Albania to Macedonia, and the Pindus range, spanning from southern Albania into central Greece and the Albanian Alps. The highest mountain of the region is Rila in Bulgaria, with Musala at 2925 m, Mount Olympus in Greece, the throne of Zeus, being second at 2917 m and Vihren in Bulgaria being the third at 2914 m. The karst field or polje is a common feature of the landscape.
On the Adriatic and Aegean coasts the climate is Mediterranean, on the Black Sea coast the climate is humid subtropical and oceanic, and inland it is humid continental. In the northern part of the peninsula and on the mountains, winters are frosty and snowy, while summers are hot and dry. In the southern part winters are milder. The humid continental climate is predominant in Bosnia and Herzegovina, northern Croatia, Bulgaria, Kosovo, Macedonia, northern Montenegro, the interior of Albania, Romania and Serbia, while the other, less common climates, the humid subtropical and oceanic climates, are seen on the Black Sea coast of Bulgaria and Turkey; and the Mediterranean climate is seen on the coast of Albania, the coast of Croatia, Greece, southern Montenegro and the Aegean coast of Turkey.
Over the centuries many woods have been cut down and replaced with bush. In the southern part and on the coast there is evergreen vegetation. Inland there are woods typical of Central Europe (oak and beech, and in the mountains, spruce, fir and pine). The tree line in the mountains lies at the height of 1800–2300 m. The land provides habitats for numerous endemic species, including extraordinarily abundant insects and reptiles that serve as food for a variety of birds of prey and rare vultures.
The soils are generally poor, except on the plains, where areas with natural grass, fertile soils and warm summers provide an opportunity for tillage. Elsewhere, land cultivation is mostly unsuccessful because of the mountains, hot summers and poor soils, although certain cultures such as olive and grape flourish.
Resources of energy are scarce, except in the territory of Kosovo, where considerable coal, lead, zinc, chromium and silver deposits are located. Other deposits of coal, especially in Romania, Bulgaria, Serbia and Bosnia, also exist. Lignite deposits are widespread in Greece. Petroleum is most notably present in Romania, although scarce reserves exist in Greece, Serbia and Albania. Natural gas deposits are scarce. Hydropower is in wide use, from over 1,000 dams. The often relentless bora wind is also being harnessed for power generation.
Metal ores are more usual than other raw materials. Iron ore is rare, but in some countries there is a considerable amount of copper, zinc, tin, chromite, manganese, magnesite and bauxite. Some metals are exported.
The time zones are situated as follows:
History and geopolitical significance.
Antiquity.
The Balkan region was the first area in Europe to experience the arrival of farming cultures in the Neolithic era. The Balkans have been inhabited since the Paleolithic and are the route by which farming from the Middle East spread to Europe during the Neolithic (7th millennium BC). The practices of growing grain and raising livestock arrived in the Balkans from the Fertile Crescent by way of Anatolia and spread west and north into Pannonia and Central Europe. Two early culture-complexes have developed in the region, Starčevo culture and Vinča culture. The Balkans are also the location of the first advanced civilizations. Vinča culture developed a form of proto-writing before the Sumerians and Minoans, known as the Old European script, while the bulk of the symbols had been created in the period between 4500 and 4000 BC, with the ones on the Tărtăria clay tablets even dating back to around 5300 BC.
The identity of the Balkans is dominated by its geographical position; historically the area was known as a crossroads of cultures. It has been a juncture between the Latin and Greek bodies of the Roman Empire, the destination of a massive influx of pagan Bulgars and Slavs, an area where Orthodox and Catholic Christianity met, as well as the meeting point between Islam and Christianity.
In pre-classical and classical antiquity, this region was home to Greeks, Illyrians, Paeonians, Thracians, Dacians, and other ancient groups. The Achaemenid Persian Empire incorporated parts of the Balkans comprising Macedonia, Thrace, Bulgaria, and the Black Sea coastal region of Romania between the late 6th and the first half of the 5th-century BC into its territories. Later the Roman Empire conquered most of the region and spread Roman culture and the Latin language, but significant parts still remained under classical Greek influence. The Romans considered the Rhodope Mountains to be the northern limit of the Peninsula of Haemus and the same limit applied approximately to the border between Greek and Latin use in the region (later called the Jireček Line). The Bulgars and Slavs arrived in the 6th-century and began assimilating and displacing already-assimilated (through Romanization and Hellenization) older inhabitants of the northern and central Balkans, forming the Bulgarian Empire. During the Middle Ages, the Balkans became the stage for a series of wars between the Byzantine Roman and the Bulgarian Empires.
Early modern period.
By the end of the 16th-century, the Ottoman Empire had become the controlling force in the region after expanding from Anatolia through Thrace to the Balkans. Many people in the Balkans place their greatest folk heroes in the era of either the onslaught or the retreat of the Ottoman Empire. As examples, for Greeks, Constantine XI Palaiologos and Kolokotronis; and for Serbs, Miloš Obilić and Tzar Lazar; for Montenegrins, Đurađ I Balšić and Ivan Crnojević; for Albanians, George Kastrioti Skanderbeg; for ethnic Macedonians, Nikola Karev and Goce Delčev; for Bulgarians, Vasil Levski, Georgi Sava Rakovski and Hristo Botev and for Croats, Nikola Šubić Zrinjski.
In the past several centuries, because of the frequent Ottoman wars in Europe fought in and around the Balkans and the comparative Ottoman isolation from the mainstream of economic advance (reflecting the shift of Europe's commercial and political centre of gravity towards the Atlantic), the Balkans has been the least developed part of Europe. According to Suraiya Faroqhi and Donald Quataert, "The population of the Balkans, according to one estimate, fell from a high of 8 million in the late 16th-century to only 3 million by the mid-eighteenth. This estimate is in harmony with the first findings based on Ottoman documentary evidence."
Most of the Balkan nation-states emerged during the 19th and early 20th centuries as they gained independence from the Ottoman Empire or the Austro-Hungarian empire (Greece in 1821, Serbia, Montenegro and Romania in 1878, Bulgaria in 1908, Albania in 1912).
Recent history.
World wars.
In 1912–1913 the First Balkan War broke out when the nation-states of Bulgaria, Serbia, Greece and Montenegro united in an alliance against the Ottoman Empire. As a result of the war, almost all remaining European territories of the Ottoman Empire were captured and partitioned among the allies. Ensuing events also led to the creation of an independent Albanian state. Bulgaria insisted on its status quo territorial integrity, divided and shared by the Great Powers next to the Russo-Turkish War (1877–78) in other boundaries and on the pre-war Bulgarian-Serbian agreement. Provoked by the backstage deals between its former allies Serbia and Greece on allocation the spoils at the end of the First Balkan War, while it fights at the main Thracian Front, Bulgaria marks the beginning of Second Balkan War when attacked them. The Serbs and the Greeks repulse single attacks, but when the Greek army invaded Bulgaria together with an unprovoked Romanian intervention in the back, regardless of the single won battles, Bulgaria collapsed. The Ottoman Empire also used the opportunity to recapture Eastern Thrace, establishing its new western borders that still stand today.
The First World War was sparked in the Balkans in 1914 when members of Mlada Bosna, a revolutionary organization with predominately Serbian and pro-Yugoslav members, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand of Austria in Bosnia and Herzegovina's capital, Sarajevo. That caused a war between the two countries which—through the existing chains of alliances—led to the First World War. The Ottoman Empire soon joined the Central Powers becoming one of the three empires participating in that alliance. The next year Bulgaria joined the Central Powers attacking Serbia, which was successfully fighting Austro-Hungary to the north for a year. That led to Serbia's defeat and the intervention of the Entente in the Balkans which sent an expeditionary force to establish a new front, the third one of that war, which soon also became static. The participation of Greece in the war three years later, in 1918, on the part of the Entente finally altered the balance between the opponents leading to the collapse of the common German-Bulgarian front there, which caused the exit of Bulgaria from the war, and in turn the collapse of the Austro-Hungarian Empire, ending the First World War.
With the start of the Second World War all Balkan countries, with the exception of Greece, were allies of Nazi Germany, having bilateral military agreements or being part of the Axis Pact. Fascist Italy expanded the war in the Balkans by using its protectorate Albania to invade Greece. After repelling the attack, the Greeks counterattacked, invading Italy-held Albania and causing Nazi Germany's intervention in the Balkans to help its ally. Days before the German invasion a successful coup d'état in Belgrade by neutral military personnel seized power.
Although the new government reaffirmed Serbia's intentions to fulfill its obligations as member of the Axis, Germany, using its other two allied countries in the region, Bulgaria and Hungary, invaded both Greece and Yugoslavia. Yugoslavia immediately disintegrated when those loyal to the Serbian King and the Croatian units mutinied. Greece resisted, but, after two months of fighting, collapsed and was occupied. The two countries were partitioned between the three Axis allies, Bulgaria, Germany and Italy, and the Independent State of Croatia, a puppet state of Italy and Germany.
During the occupation the population suffered considerable hardship due to repression and starvation, to which the population reacted by creating a mass resistance movement. Together with the early and extremely heavy winter of that year (which caused hundreds of thousands deaths among the poorly fed population), the German invasion had disastrous effects in the timetable of the planned invasion in Russia causing a significant delay, which had major consequences during the course of the war.
Finally, at the end of 1944, the Soviets entered Romania and Bulgaria forcing the Germans out of the Balkans. They left behind a region largely ruined as a result of wartime exploitation.
Cold War.
During the Cold War, most of the countries on the Balkans were governed by communist governments. Greece became the first battleground of the emerging Cold War. The Truman Doctrine was the US response to the civil war, which raged from 1944 to 1949. This civil war, unleashed by the Communist Party of Greece, backed by communist volunteers from neighboring countries (Albania, Bulgaria and Yugoslavia), led to massive American assistance for the non-communist Greek government. With this backing, Greece managed to defeat the partisans and, ultimately, remained the only non-communist country in the region.
However, despite being under communist governments, Yugoslavia (1948) and Albania (1961) fell out with the Soviet Union. Yugoslavia, led by Marshal Josip Broz Tito (1892–1980), first propped up then rejected the idea of merging with Bulgaria and instead sought closer relations with the West, later even spearheaded, together with India and Egypt the Non-Aligned Movement. Albania on the other hand gravitated toward Communist China, later adopting an isolationist position.
As the only non-communist countries, Greece and Turkey were (and still are) part of NATO composing the southeastern wing of the alliance.
Post–Cold War.
In the 1990s, the transition of the regions' ex-Soviet bloc countries towards democratic free-market societies went peacefully with the exception of Yugoslavia. Wars between the former Yugoslav republics broke out after Slovenia and Croatia held free elections and their people voted for independence on their respective countries' referenda. Serbia in turn declared the dissolution of the union as unconstitutional and the Yugoslavian army unsuccessfully tried to maintain status quo. Slovenia and Croatia declared independence on 25 June 1991, followed by the Ten-Day War in Slovenia. Till October 1991, the Army withdrew from Slovenia, and in Croatia, the Croatian War of Independence would continue until 1995. In the ensuing 10 years armed confrontation, gradually all the other Republics declared independence, with Bosnia being the most affected by the fighting. The long lasting wars resulted in a United Nations intervention and NATO ground and air forces took action against Serb forces in Bosnia and Herzegovina and Serbia.
From the dissolution of Yugoslavia six republics achieved international recognition as sovereign republics, but these are traditionally included in Balkans: Croatia, Bosnia and Herzegovina, Macedonia, Montenegro and Serbia. In 2008, while under UN administration, Kosovo declared independence (according to the official Serbian policy, Kosovo is still an internal autonomous region). In July 2010, the International Court of Justice, ruled that the declaration of independence was legal. Most UN member states recognise Kosovo. After the end of the wars a revolution broke in Serbia and Slobodan Milošević, the Serbian communist leader (elected president between 1989 and 2000), was overthrown and handed for trial to the International Criminal Tribunal for crimes against the International Humanitarian Law during the Yugoslav wars. Milošević died of a heart attack in 2006 before a verdict could have been released. Ιn 2001 an Albanian uprising in Macedonia forced the country to give local autonomy to the ethnic Albanians in the areas where they predominate.
With the dissolution of Yugoslavia an issue emerged over the name under which the former (federated) republic of Macedonia would internationally be recognized, between the new country and Greece. Being the Macedonian part of Yugoslavia (see Vardar Macedonia), the federated Republic under the Yugoslav identity had the name Republic of Macedonia on which it declared its sovereignty in 1991. Greece, having a large region (see Macedonia) also under the same name opposed to the usage of this name as an indication of a nationality. The issue is currently under negotiations after a UN initiation.
Balkan countries control the direct land routes between Western Europe and South West Asia (Asia Minor and the Middle East). Since 2000, all Balkan countries are friendly towards the EU and the USA.
Greece has been the member of the European Union since 1981 while Slovenia is a member since 2004, Bulgaria and Romania are members since 2007, and Croatia is a member since 2013. In 2005, the European Union decided to start accession negotiations with candidate countries; Turkey, and Macedonia were accepted as candidates for EU membership. In 2012, Montenegro started accession negotiations with the EU. In 2014, Albania is an official candidate for accession to the EU. In 2015, Serbia is expected to start accession negotiations with the EU.
Greece has been a member of the NATO since 1952. In March 2004, Bulgaria, Romania and Slovenia have become members of NATO. As of April 2009, Albania and Croatia are members of NATO.
All other countries have expressed a desire to join the EU and NATO at some point in the future.
Politics and economy.
Currently all of the states are republics, but until World War II all except Turkey were monarchies. Most of the republics are parliamentary, excluding Romania and Bosnia which are semi-presidential. All the states have open market economies, most of which are in the upper-middle income range ($4,000 – $12,000 p.c.), however, Greece has high income economies (over $12,000 p.c.), and is also classified with very high HDI in contrast to the remaining states which are classified with high HDI. The states from the former Eastern Bloc that formerly had planned economy system and Turkey mark gradual economic growth each year, only the economy of Greece drops for 2012 and meanwhile it was expected to grow in 2013. The Gross domestic product (Purchasing power parity) per capita is highest in Greece (over $25,000), followed by Turkey, Bulgaria, Romania, Montenegro, Serbia, Macedonia ($10,000 – $15,000), Bosnia, Albania and Kosovo (below $10,000). The Gini coefficient, which indicates the level of difference by monetary welfare of the layers, is on the second level at the highest monetary equality in Albania, Bulgaria and Serbia, on the third level in Greece, Montenegro and Romania, on the fourth level in Macedonia, on the fifth level in Turkey, and the most unequal by Gini coefficient is Bosnia at the eighth level which is the penultimate level and one of the highest in the world. The unemployment is lowest in Romania (below 10%), followed by Bulgaria, Turkey, Albania (10 – 15%), Greece (15 – 20%), Montenegro, Serbia, Bosnia (20 – 30%), Macedonia (over 30%) and Kosovo (over 40%).
Regional organizations.
See also the Black Sea Regional organizations
Demographics.
The region is inhabited by Albanians, Aromanians, Bulgarians, Bosniaks, Croats, Gorani, Greeks, Macedonians, Montenegrins, Serbs, Slovenes, Romanians, Turks, and other ethnic groups which present minorities in certain countries like the Romani and Ashkali.
Religion.
The region is a meeting point of Orthodox Christianity, Islam and Roman Catholic Christianity. Eastern Orthodoxy is the majority religion in both the Balkan peninsula and the Balkan region. A variety of different traditions of each faith are practiced, with each of the Eastern Orthodox countries having its own national church. A part of the population in the Balkans defines itself as irreligious.
The Jewish communities of the Balkans were some of the oldest in Europe and date back to ancient times. These communities were Sephardi Jews, except in Romania where the Jewish communities were Ashkenazi Jews. In Bosnia and Herzegovina, the small and close-knit Jewish community is 90% Sephardic, and Ladino is still spoken among the elderly. The Sephardi Jewish cemetery in Sarajevo has tombstones of a unique shape and inscribed in ancient Ladino. Sephardi Jews used to have a large presence in the city of Thessaloniki, and by 1900, some 80,000, or more than half of the population, were Jews. The Jewish communities in the Balkans suffered immensely during World War II, and the vast majority were killed during the Holocaust. An exception were the Bulgarian Jews, most of whom were saved by Boris III of Bulgaria, who resisted Adolf Hitler, opposing their deportation to Nazi concentration camps. Almost all of the few survivors have emigrated to the (then) newly founded state of Israel and elsewhere. No Balkan country today has a significant Jewish minority.
Languages.
The Balkan region today is a very diverse ethno-linguistic region, being home to multiple Slavic, Romance, and Turkic languages, as well as Greek, Albanian, and others. Romani is spoken by a large portion of the Romanis living throughout the Balkan countries. Throughout history many other ethnic groups with their own languages lived in the area, among them Thracians, Illyrians, Romans, Celts and various Germanic tribes. All of the aforementioned languages from the present and from the past belong to the wider Indo-European language family, with the exception of the Turkic languages (e.g., Turkish and Gagauz).
Urbanization.
Most of the states in the Balkans are predominantly urbanized; the countries in which the rural population is the majority are Bosnia and Herzegovina and Kosovo each being about 50% rural and 50% urban.
A list of largest cities:

</doc>
<doc id="4831" url="https://en.wikipedia.org/wiki?curid=4831" title="Bohr model">
Bohr model

In atomic physics, the Rutherford–Bohr model or Bohr model, introduced by Niels Bohr in 1913, depicts the atom as a small, positively charged nucleus surrounded by electrons that travel in circular orbits around the nucleus—similar in structure to the solar system, but with attraction provided by electrostatic forces rather than gravity. After the cubic model (1902), the plum-pudding model (1904), the Saturnian model (1904), and the Rutherford model (1911) came the Rutherford–Bohr model or just "Bohr model" for short (1913). The improvement to the Rutherford model is mostly a quantum physical interpretation of it. The Bohr model has been superseded, but the quantum theory remains sound.
The model's key success lay in explaining the Rydberg formula for the spectral emission lines of atomic hydrogen. While the Rydberg formula had been known experimentally, it did not gain a theoretical underpinning until the Bohr model was introduced. Not only did the Bohr model explain the reason for the structure of the Rydberg formula, it also provided a justification for its empirical results in terms of fundamental physical constants.
The Bohr model is a relatively primitive model of the hydrogen atom, compared to the valence shell atom. As a theory, it can be derived as a first-order approximation of the hydrogen atom using the broader and much more accurate quantum mechanics and thus may be considered to be an obsolete scientific theory. However, because of its simplicity, and its correct results for selected systems (see below for application), the Bohr model is still commonly taught to introduce students to quantum mechanics or energy level diagrams before moving on to the more accurate, but more complex, valence shell atom. A related model was originally proposed by Arthur Erich Haas in 1910, but was rejected. The quantum theory of the period between Planck's discovery of the quantum (1900) and the advent of a full-blown quantum mechanics (1925) is often referred to as the old quantum theory.
Origin.
In the early 20th century, experiments by Ernest Rutherford established that atoms consisted of a diffuse cloud of negatively charged electrons surrounding a small, dense, positively charged nucleus. Given this experimental data, Rutherford naturally considered a planetary-model atom, the Rutherford model of 1911 – electrons orbiting a solar nucleus – however, said planetary-model atom has a technical difficulty. The laws of classical mechanics (i.e. the Larmor formula), predict that the electron will release electromagnetic radiation while orbiting a nucleus. Because the electron would lose energy, it would rapidly spiral inwards, collapsing into the nucleus on a timescale of around 16 picoseconds. This atom model is disastrous, because it predicts that all atoms are unstable.
Also, as the electron spirals inward, the emission would rapidly increase in frequency as the orbit got smaller and faster. This would produce a continuous smear, in frequency, of electromagnetic radiation. However, late 19th century experiments with electric discharges have shown that atoms will only emit light (that is, electromagnetic radiation) at certain discrete frequencies.
To overcome this difficulty, Niels Bohr proposed, in 1913, what is now called the "Bohr model of the atom". He suggested that electrons could only have certain "classical" motions:
The significance of the Bohr model is that the laws of classical mechanics apply to the motion of the electron about the nucleus "only when restricted by a quantum rule". Although Rule 3 is not completely well defined for small orbits, because the emission process involves two orbits with two different periods, Bohr could determine the energy spacing between levels using Rule 3 and come to an exactly correct quantum rule: the angular momentum "L" is restricted to be an integer multiple of a fixed unit:
where "n" = 1, 2, 3, ... is called the principal quantum number, and . The lowest value of "n" is 1; this gives a smallest possible orbital radius of 0.0529 nm known as the Bohr radius. Once an electron is in this lowest orbit, it can get no closer to the proton. Starting from the angular momentum quantum rule, Bohr was able to calculate the energies of the allowed orbits of the hydrogen atom and other hydrogen-like atoms and ions.
Other points are:
Bohr's condition, that the angular momentum is an integer multiple of "ħ" was later reinterpreted in 1924 by de Broglie as a standing wave condition: the electron is described by a wave and a whole number of wavelengths must fit along the circumference of the electron's orbit:
Substituting de Broglie's wavelength of reproduces Bohr's rule. In 1913, however, Bohr justified his rule by appealing to the correspondence principle, without providing any sort of wave interpretation. In 1913, the wave behavior of matter particles such as the electron (i.e., matter waves) was not suspected.
In 1925 a new kind of mechanics was proposed, quantum mechanics, in which Bohr's model of electrons traveling in quantized orbits was extended into a more accurate model of electron motion. The new theory was proposed by Werner Heisenberg. Another form of the same theory, wave mechanics, was discovered by the Austrian physicist Erwin Schrödinger independently, and by different reasoning. Schrödinger employed de Broglie's matter waves, but sought wave solutions of a three-dimensional wave equation describing electrons that were constrained to move about the nucleus of a hydrogen-like atom, by being trapped by the potential of the positive nuclear charge.
Electron energy levels.
The Bohr model gives almost exact results only for a system where two charged points orbit each other at speeds much less than that of light. This not only includes one-electron systems such as the hydrogen atom, singly ionized helium, doubly ionized lithium, but it includes positronium and Rydberg states of any atom where one electron is far away from everything else. It can be used for K-line X-ray transition calculations if other assumptions are added (see Moseley's law below). In high energy physics, it can be used to calculate the masses of heavy quark mesons.
Calculation of the orbits requires two assumptions.
An electron in the lowest energy level of hydrogen () therefore has about 13.6 eV less energy than a motionless electron infinitely far from the nucleus. The next energy level () is −3.4 eV. The third ("n" = 3) is −1.51 eV, and so on. For larger values of "n", these are also the binding energies of a highly excited atom with one electron in a large circular orbit around the rest of the atom. The hydrogen formula also coincides with the Wallis product.
The combination of natural constants in the energy formula is called the Rydberg energy ("R"E):
This expression is clarified by interpreting it in combinations that form more natural units:
Since this derivation is with the assumption that the nucleus is orbited by one electron, we can generalize this result by letting the nucleus have a charge "q" = "Z e" where "Z" is the atomic number. This will now give us energy levels for hydrogenic atoms, which can serve as a rough order-of-magnitude approximation of the actual energy levels. So for nuclei with "Z" protons, the energy levels are (to a rough approximation):
The actual energy levels cannot be solved analytically for more than one electron (see "n"-body problem) because the electrons are not only affected by the nucleus but also interact with each other via the Coulomb Force.
When "Z" = 1/"α" (Z ≈ 137), the motion becomes highly relativistic, and "Z"2 cancels the "α"2 in "R"; the orbit energy begins to be comparable to rest energy. Sufficiently large nuclei, if they were stable, would reduce their charge by creating a bound electron from the vacuum, ejecting the positron to infinity. This is the theoretical phenomenon of electromagnetic charge screening which predicts a maximum nuclear charge. Emission of such positrons has been observed in the collisions of heavy ions to create temporary super-heavy nuclei.
The Bohr formula properly uses the reduced mass of electron and proton in all situations, instead of the mass of the electron, 
However, these numbers are very nearly the same, due to the much larger mass of the proton, about 1836.1 times the mass of the electron, so that the reduced mass in the system is the mass of the electron multiplied by the constant 1836.1/(1+1836.1) = 0.99946. This fact was historically important in convincing Rutherford of the importance of Bohr's model, for it explained the fact that the frequencies of lines in the spectra for singly ionized helium do not differ from those of hydrogen by a factor of exactly 4, but rather by 4 times the ratio of the reduced mass for the hydrogen vs. the helium systems, which was much closer to the experimental ratio than exactly 4.
For positronium, the formula uses the reduced mass also, but in this case, it is exactly the electron mass divided by 2. For any value of the radius, the electron and the positron are each moving at half the speed around their common center of mass, and each has only one fourth the kinetic energy. The total kinetic energy is half what it would be for a single electron moving around a heavy nucleus.
Rydberg formula.
The Rydberg formula, which was known empirically before Bohr's formula, is seen in Bohr's theory as describing the energies of transitions or quantum jumps between one orbital energy levels. Bohr's formula gives the numerical value of the already-known and measured Rydberg's constant, but in terms of more fundamental constants of nature, including the electron's charge and Planck's constant.
When the electron gets moved from its original energy level to a higher one, it then jumps back each level until it comes to the original position, which results in a photon being emitted. Using the derived formula for the different energy levels of hydrogen one may determine the wavelengths of light that a hydrogen atom can emit.
The energy of a photon emitted by a hydrogen atom is given by the difference of two hydrogen energy levels:
where is the final energy level, and is the initial energy level.
Since the energy of a photon is
the wavelength of the photon given off is given by
This is known as the Rydberg formula, and the Rydberg constant is , or in natural units. This formula was known in the nineteenth century to scientists studying spectroscopy, but there was no theoretical explanation for this form or a theoretical prediction for the value of , until Bohr. In fact, Bohr's derivation of the Rydberg constant, as well as the concomitant agreement of Bohr's formula with experimentally observed spectral lines of the Lyman ( =1), Balmer ( =2), and Paschen ( =3) series, and successful theoretical prediction of other lines not yet observed, was one reason that his model was immediately accepted.
To apply to atoms with more than one electron, the Rydberg formula can be modified by replacing with or with where is constant representing a screening effect due to the inner-shell and other electrons (see Electron shell and the later discussion of the "Shell Model of the Atom" below). This was established empirically before Bohr presented his model.
Shell model of heavier atoms.
Bohr extended the model of hydrogen to give an approximate model for heavier atoms. This gave a physical picture that reproduced many known atomic properties for the first time.
Heavier atoms have more protons in the nucleus, and more electrons to cancel the charge. Bohr's idea was that each discrete orbit could only hold a certain number of electrons. After that orbit is full, the next level would have to be used. This gives the atom a shell structure, in which each shell corresponds to a Bohr orbit.
This model is even more approximate than the model of hydrogen, because it treats the electrons in each shell as non-interacting. But the repulsions of electrons are taken into account somewhat by the phenomenon of screening. The electrons in outer orbits do not only orbit the nucleus, but they also move around the inner electrons, so the effective charge Z that they feel is reduced by the number of the electrons in the inner orbit.
For example, the lithium atom has two electrons in the lowest 1s orbit, and these orbit at Z=2. Each one sees the nuclear charge of Z=3 minus the screening effect of the other, which crudely reduces the nuclear charge by 1 unit. This means that the innermost electrons orbit at approximately 1/4 the Bohr radius. The outermost electron in lithium orbits at roughly Z=1, since the two inner electrons reduce the nuclear charge by 2. This outer electron should be at nearly one Bohr radius from the nucleus. Because the electrons strongly repel each other, the effective charge description is very approximate; the effective charge Z doesn't usually come out to be an integer. But Moseley's law experimentally probes the innermost pair of electrons, and shows that they do see a nuclear charge of approximately Z−1, while the outermost electron in an atom or ion with only one electron in the outermost shell orbits a core with effective charge Z−k where k is the total number of electrons in the inner shells.
The shell model was able to qualitatively explain many of the mysterious properties of atoms which became codified in the late 19th century in the periodic table of the elements. One property was the size of atoms, which could be determined approximately by measuring the viscosity of gases and density of pure crystalline solids. Atoms tend to get smaller toward the right in the periodic table, and become much larger at the next line of the table. Atoms to the right of the table tend to gain electrons, while atoms to the left tend to lose them. Every element on the last column of the table is chemically inert (noble gas).
In the shell model, this phenomenon is explained by shell-filling. Successive atoms become smaller because they are filling orbits of the same size, until the orbit is full, at which point the next atom in the table has a loosely bound outer electron, causing it to expand. The first Bohr orbit is filled when it has two electrons, which explains why helium is inert. The second orbit allows eight electrons, and when it is full the atom is neon, again inert. The third orbital contains eight again, except that in the more correct Sommerfeld treatment (reproduced in modern quantum mechanics) there are extra "d" electrons. The third orbit may hold an extra 10 d electrons, but these positions are not filled until a few more orbitals from the next level are filled (filling the n=3 d orbitals produces the 10 transition elements). The irregular filling pattern is an effect of interactions between electrons, which are not taken into account in either the Bohr or Sommerfeld models and which are difficult to calculate even in the modern treatment.
Moseley's law and calculation of K-alpha X-ray emission lines.
Niels Bohr said in 1962, "You see actually the Rutherford work nuclear atom was not taken seriously. We cannot understand today, but it was not taken seriously at all. There was no mention of it any place. The great change came from Moseley."
In 1913 Henry Moseley found an empirical relationship between the strongest X-ray line emitted by atoms under electron bombardment (then known as the K-alpha line), and their atomic number . Moseley's empiric formula was found to be derivable from Rydberg and Bohr's formula (Moseley actually mentions only Ernest Rutherford and Antonius Van den Broek in terms of models). The two additional assumptions that this X-ray line came from a transition between energy levels with quantum numbers 1 and 2, and [2, that the atomic number when used in the formula for atoms heavier than hydrogen, should be diminished by 1, to .
Moseley wrote to Bohr, puzzled about his results, but Bohr was not able to help. At that time, he thought that the postulated innermost "K" shell of electrons should have at least four electrons, not the two which would have neatly explained the result. So Moseley published his results without a theoretical explanation.
Later, people realized that the effect was caused by charge screening, with an inner shell containing only 2 electrons. In the experiment, one of the innermost electrons in the atom is knocked out, leaving a vacancy in the lowest Bohr orbit, which contains a single remaining electron. This vacancy is then filled by an electron from the next orbit, which has n=2. But the n=2 electrons see an effective charge of Z−1, which is the value appropriate for the charge of the nucleus, when a single electron remains in the lowest Bohr orbit to screen the nuclear charge +Z, and lower it by −1 (due to the electron's negative charge screening the nuclear positive charge). The energy gained by an electron dropping from the second shell to the first gives Moseley's law for K-alpha lines,
or
Here, "R"v = "R"E/"h" is the Rydberg constant, in terms of frequency equal to 3.28 x 1015 Hz. For values of Z between 11 and 31 this latter relationship had been empirically derived by Moseley, in a simple (linear) plot of the square root of X-ray frequency against atomic number (however, for silver, Z = 47, the experimentally obtained screening term should be replaced by 0.4). Notwithstanding its restricted validity, Moseley's law not only established the objective meaning of atomic number (see Henry Moseley for detail) but, as Bohr noted, it also did more than the Rydberg derivation to establish the validity of the Rutherford/Van den Broek/Bohr nuclear model of the atom, with atomic number (place on the periodic table) standing for whole units of nuclear charge.
The K-alpha line of Moseley's time is now known to be a pair of close lines, written as (Kα1 and Kα2) in Siegbahn notation.
Shortcomings.
The Bohr model gives an incorrect value for the ground state orbital angular momentum: The angular momentum in the true ground state is known to be zero from experiment. Although mental pictures fail somewhat at these levels of scale, an electron in the lowest modern "orbital" with no orbital momentum, may be thought of as not to rotate "around" the nucleus at all, but merely to go tightly around it in an ellipse with zero area (this may be pictured as "back and forth", without striking or interacting with the nucleus). This is only reproduced in a more sophisticated semiclassical treatment like Sommerfeld's. Still, even the most sophisticated semiclassical model fails to explain the fact that the lowest energy state is spherically symmetric - it doesn't point in any particular direction.
Nevertheless, in the modern "fully quantum treatment in phase space", the proper deformation ( careful full extension) of the semi-classical result adjusts the angular momentum value to the correct effective one. As a consequence, the physical ground state expression is obtained through a shift of the vanishing quantum angular momentum expression, which corresponds to spherical symmetry.
In modern quantum mechanics, the electron in hydrogen is a spherical cloud of probability that grows denser near the nucleus. The rate-constant of probability-decay in hydrogen is equal to the inverse of the Bohr radius, but since Bohr worked with circular orbits, not zero area ellipses, the fact that these two numbers exactly agree is considered a "coincidence". (However, many such coincidental agreements are found between the semiclassical vs. full quantum mechanical treatment of the atom; these include identical energy levels in the hydrogen atom and the derivation of a fine structure constant, which arises from the relativistic Bohr–Sommerfeld model (see below) and which happens to be equal to an entirely different concept, in full modern quantum mechanics).
The Bohr model also has difficulty with, or else fails to explain:
Refinements.
Several enhancements to the Bohr model were proposed, most notably the Sommerfeld model or Bohr–Sommerfeld model, which suggested that electrons travel in elliptical orbits around a nucleus instead of the Bohr model's circular orbits. This model supplemented the quantized angular momentum condition of the Bohr model with an additional radial quantization condition, the Sommerfeld–Wilson quantization condition
where "pr" is the radial momentum canonically conjugate to the coordinate "q" which is the radial position and "T" is one full orbital period. The integral is the action of action-angle coordinates. This condition, suggested by the correspondence principle, is the only one possible, since the quantum numbers are adiabatic invariants.
The Bohr–Sommerfeld model was fundamentally inconsistent and led to many paradoxes. The magnetic quantum number measured the tilt of the orbital plane relative to the "xy"-plane, and it could only take a few discrete values. This contradicted the obvious fact that an atom could be turned this way and that relative to the coordinates without restriction. The Sommerfeld quantization can be performed in different canonical coordinates and sometimes gives different answers. The incorporation of radiation corrections was difficult, because it required finding action-angle coordinates for a combined radiation/atom system, which is difficult when the radiation is allowed to escape. The whole theory did not extend to non-integrable motions, which meant that many systems could not be treated even in principle. In the end, the model was replaced by the modern quantum mechanical treatment of the hydrogen atom, which was first given by Wolfgang Pauli in 1925, using Heisenberg's matrix mechanics. The current picture of the hydrogen atom is based on the atomic orbitals of wave mechanics which Erwin Schrödinger developed in 1926.
However, this is not to say that the Bohr model was without its successes. Calculations based on the Bohr–Sommerfeld model were able to accurately explain a number of more complex atomic spectral effects. For example, up to first-order perturbations, the Bohr model and quantum mechanics make the same predictions for the spectral line splitting in the Stark effect. At higher-order perturbations, however, the Bohr model and quantum mechanics differ, and measurements of the Stark effect under high field strengths helped confirm the correctness of quantum mechanics over the Bohr model. The prevailing theory behind this difference lies in the shapes of the orbitals of the electrons, which vary according to the energy state of the electron.
The Bohr–Sommerfeld quantization conditions lead to questions in modern mathematics. Consistent semiclassical quantization condition requires a certain type of structure on the phase space, which places topological limitations on the types of symplectic manifolds which can be quantized. In particular, the symplectic form should be the curvature form of a connection of a Hermitian line bundle, which is called a prequantization.

</doc>
<doc id="4832" url="https://en.wikipedia.org/wiki?curid=4832" title="Bombay Sapphire">
Bombay Sapphire

Bombay Sapphire is a brand of gin that was first launched in 1987 by IDV. In 1997 Diageo sold the brand to Bacardi. Its name originates from gin's popularity in India during the British Raj and the sapphire in question is the Star of Bombay on display at the Smithsonian Institution. Bombay Sapphire is marketed in a flat-sided, sapphire-coloured bottle that bears a picture of Queen Victoria on the label.
The flavouring of the drink comes from a recipe of ten ingredients: almond, lemon peel, liquorice, juniper berries, orris root, angelica, coriander, cassia, cubeb, and grains of paradise. The spirit is triple distilled using a carterhead still, and the alcohol vapours are passed through a mesh/basket containing the ten botanicals, in order to gain flavour and aroma. This is felt to give the gin a lighter, more floral taste compared to those gins that are distilled using a copper pot still. Water from Lake Vyrnwy is added to bring the strength of Bombay Sapphire down to 40.0% (UK, Canada, Australia).
In 2011, plans were announced to move the distillation process to a new facility in Laverstoke, Hampshire, including the restoration of the former Portal's paper mill at the proposed site, and the construction of a visitor centre. Planning permission was granted in February 2012, and the centre opened to the public in the autumn of 2014.
Production and bottling of the drink is contracted out by Bacardi to G&J Greenall.
Varieties.
Bacardi also markets Bombay Original London Dry Gin (or Bombay Original Dry). Eight botanical ingredients are used in the production of the Original Dry Gin variety as opposed to the ten in Bombay Sapphire. "Wine Enthusiast" preferred Bombay Sapphire.
In September 2011, Bombay Sapphire East was launched in test markets in New York and Las Vegas. This variety has another two botanicals, lemongrass and black peppercorns, in addition to the original ten. It is bottled at 42% and was designed to counteract the sweetness of American tonic water.
A Special edition of Bombay Gin was produced in 2015 for the UK Market. It is called Bombay Star and was named best Gin available to consumers by "The Independent" newspaper. It is bottled at 47.5% and is distilled from grain.
Design connection.
The brand started a series of design collaborations. Their first step into the design world was a series of advertisements featuring work from currently popular designers. Their works, varying from martini glasses to tiles and cloth patterns, are labelled as “Inspired by Bombay Sapphire”. The campaign featured designers such as Marcel Wanders, Yves Behar, Karim Rashid, Ulla Darni, and Dror Benshetrit and performance artist Jurgen Hahn.
From the success of this campaign, the company began a series of events and sponsored locations. The best known is the Bombay Sapphire Designer Glass Competition, held each year, where design students from all over the world can participate by designing their own “inspired” martini cocktail glass. The finalists (one from each participating country) are then invited to the yearly Salone del Mobile, an international design fair in Milano, where the winner is chosen.
Bombay Sapphire also endorses glass artists and designers with the Bombay Sapphire Prize, which is awarded every year to an outstanding design which features glass. Bombay Sapphire also showcases the designers' work in the Bombay Sapphire endorsed blue room, which is actually a design exhibition touring the world each year.
From 2008 the Bombay Sapphire Designer Glass Competition final will be held at 100% Design in London, UK and the Bombay Sapphire Prize will take place in Milan at the Salone Del Mobile.
Evaluation.
Bombay Sapphire has been reviewed by several outside spirit ratings organizations to mixed success. Recently, it was awarded a score of 92 (on a 100-point scale) from the Beverage Testing Institute. Ratings aggregator Proof66.com categorizes the Sapphire as a Tier 2 spirit, indicating highly favourable "expert" reviews.

</doc>
<doc id="4833" url="https://en.wikipedia.org/wiki?curid=4833" title="Bob Wills">
Bob Wills

James Robert "Bob" Wills (March 6, 1905 – May 13, 1975) was an American Western swing musician, songwriter, and bandleader. Considered by music authorities as the co-founder of Western swing, he was universally known as the King of Western Swing (after the death of Spade Cooley who used the moniker "King Of Western Swing" from 1942 to 1969).
Wills formed several bands and played radio stations around the South and West until he formed the Texas Playboys in 1934 with Wills on fiddle, Tommy Duncan on piano and vocals, rhythm guitarist June Whalin, tenor banjoist Johnnie Lee Wills, and Kermit Whalin, who played steel guitar and bass. The band played regularly on a Tulsa, Oklahoma radio station and added Leon McAuliffe on steel guitar, pianist Al Stricklin, drummer Smokey Dacus, and a horn section that expanded the band's sound. Wills favored jazz-like arrangements and the band found national popularity into the 1940s with such hits as "Steel Guitar Rag", "New San Antonio Rose", "Smoke On The Water", "Stars And Stripes On Iwo Jima", and "New Spanish Two Step".
Wills and the Texas Playboys recorded with several publishers and companies, including Vocalion, Okeh, Columbia, and MGM, frequently moving. In 1950, he had two Top 10 hits, "Ida Red Likes The Boogie" and "Faded Love", which were his last hits for a decade. Throughout the 1950s, he struggled with poor health and tenuous finances, but continued to perform frequently despite the decline in popularity of his earlier music as rock and roll took over. Wills had a heart attack in 1962 and a second one the next year, which forced him to disband the Playboys although Wills continued to perform solo.
The Country Music Hall of Fame inducted Wills in 1968 and the Texas State Legislature honored him for his contribution to American music. In 1972, Wills accepted a citation from the American Society of Composers, Authors and Publishers in Nashville. He was recording an album with fan Merle Haggard in 1973 when a stroke left him comatose until his death in 1975. The Rock and Roll Hall of Fame inducted Wills and the Texas Playboys in 1999.
Biography.
Early years.
He was born on a farm near Kosse, Texas, in Limestone County near Groesbeck, to Emma Lee Foley and John Tompkins Wills. His father was a statewide champion fiddle player and the Wills family was either playing music, or someone was "always wanting us to play for them", in addition to raising cotton on their farm.
In addition to picking cotton, the young Jim Bob learned to play the fiddle and the mandolin. Both a sister and several brothers played musical instruments, while another sister played piano. The Wills family frequently held country dances in their home, and there was dancing in all four rooms. While living in Hall County, Texas, they also played at 'ranch dances' which were popular in both North Texas and eastern New Mexico.
Wills not only learned traditional music from his family, he learned some Negro songs directly from African Americans in the cotton fields near Lakeview, Texas, and said that he did not play with many white children other than his siblings, until he was seven or eight years old. African Americans were his playmates, and his father enjoyed watching him jig dance with black children.
"I don't know whether they made them up as they moved down the cotton rows or not," Wills once told Charles Townsend, author of "San Antonio Rose: The Life And Times Of Bob Wills", "but they sang blues you never heard before."
New Mexico and Texas.
The family moved to Hall County in the Texas Panhandle in 1913, and in 1919 they bought a farm between the towns of Lakeview and Turkey. At the age of 16, Wills left the family and hopped a freight train. "Jim Rob", as he became known, drifted for several years, traveling from town to town to try to earn a living, at one point almost losing his life when he nearly fell from a moving train, and later being chased by railroad police. In his 20s he attended barber school, got married, and moved first to Roy, New Mexico, then returned to Turkey in Hall County (now considered his home town) to work as a barber at Hamm's Barber Shop. He alternated barbering and fiddling even when he moved to Fort Worth after leaving Hall County in 1929. There he played in minstrel and medicine shows, and, as with other Texas musicians such as Ocie Stockard, continued to earn money as a barber. He wore blackface makeup to appear in comedy routines, something that was common at the time. "He was playing his violin and singing." There were two guitars and a banjo player with him. "Bob was in blackface and was the comic; he cracked jokes, sang, and did an amazing jig dance." Since there was already a "Jim" on the show, the manager began calling him "Bob". However, it was as "Jim Rob Wills", paired with Herman Arnspiger, that he made his first commercial (though unissued) recordings in November 1929 for Brunswick/Vocalion.
Wills was known for his hollering and wisecracking. One source for this was when, as a very young boy, he would hear his father, grandfather, and cowboys give out loud cries when the music moved them. When asked if his wisecracking and talking on the bandstand came from his medicine show experience, he said it did not. Rather, he said that it came directly from playing and living close to Negroes, and that he never did it necessarily as show, but more as a way to express his feelings.
While in Fort Worth, Wills added the "rowdy city blues" of Bessie Smith and Emmett Miller to a repertoire of mainly waltzes and breakdowns he had learned from his father, and patterned his vocal style after that of Miller and other performers such as Al Bernard. Wills acknowledged that he idolized Miller. Furthermore, his 1935 version of "St. Louis Blues" is nearly a word-for-word copy of Al Bernard's patter on his 1928 recording of the same song.
The fact that Wills made his professional debut in blackface was commented on by Wills' daughter, Rosetta: "He had a lot of respect for the musicians and music of his black friends," Rosetta is quoted as saying on the Bob Wills and the Texas Playboys Web site. She remembers that her father was such a fan of Bessie Smith stating, "He once rode fifty miles on horseback just to see her perform live." (Wills is quoted as saying, "I rode horseback from the place between the rivers to Childress to see Bessie Smith ... She was about the greatest thing I had ever heard. In fact, there was no doubt about it. She "was" the greatest thing I ever heard."
In Fort Worth, Wills met Herman Arnspinger and formed The Wills Fiddle Band. In 1930 Milton Brown joined the group as lead vocalist and brought a sense of innovation and experimentation to the band, now called the Light Crust Doughboys due to radio sponsorship by the makers of Light Crust Flour. Brown left the band in 1932 to form the Musical Brownies, the first true Western swing band. Brown added twin fiddles, tenor banjo and slap bass, pointing the music in the direction of swing, which they played on local radio and at dancehalls.
Wills remained with the Doughboys and replaced Brown with new singer Tommy Duncan in 1932. He found himself unable to get along with future Texas Governor W. Lee "Pappy" O'Daniel, the authoritarian host of the Light Crust Doughboy radio show. O'Daniel had parlayed the show's popularity into growing power within Light Crust Flour's parent company, Burrus Mill and Elevator Company, and wound up as General Manager, though he despised what he considered "hillbilly music". Wills and Duncan left the Doughboys in 1933 after Wills had missed one show too many due to his sporadic drinking.
Wills recalled the early days of what became known as Western swing music in a 1949 interview. "Here's the way I figure it. We sure not tryin' to take credit for swingin' it." Speaking of Milt Brown and himself working with songs done by Jimmie Davis, the Skillet Lickers, Jimmie Rodgers, and others, and songs he'd learned from his father, he said that "We'd pull these tunes down an set 'em in a dance category. It wouldn't be a runaway, and just lay a real nice beat behind it an the people would get to really like it. It was nobody intended to start anything in the world. We was just tryin' to find enough tunes to keep 'em dancin' to not have to repeat so much."
As stated in the Texas Playboys standard "Time Changes Everything" (written by Tommy Duncan), "You can change the name of an old song, rearrange it and make it a swing." "One Star Rag", "Rat Cheese Under The Hill", "Take Me Back To Tulsa", "Basin Street Blues", "Steel Guitar Rag", and "Trouble In Mind" were some of the songs in his extensive repertory.
The Texas Playboys.
After forming a new band, The Playboys, and relocating to Waco, Wills found enough popularity there to decide on a bigger market. They left Waco in January 1934 for Oklahoma City. Wills soon settled the renamed Texas Playboys in Tulsa, Oklahoma, and began broadcasting noontime shows over the 50,000 watt KVOO radio station. Their 12:30–1:15 p.m. Monday–Friday broadcasts became a veritable institution in the region. Nearly all of the daily (except Sunday) shows originated from the stage of Cain's Ballroom. In addition, they played dances in the evenings, including regular ones at the ballroom on Thursdays and Saturdays.
Wills added a trumpet to the band inadvertently when he hired Everet Stover as an announcer, not knowing that he had played with the New Orleans symphony and had directed the governor's band in Austin. Stover, thinking he had been hired as a trumpeter, began playing with the band with no comment from Wills. Young sax player Zeb McNally was allowed to play with the band, although Wills initially discouraged it. With two horns in the band, Wills realized he would have to add a drummer to balance things and create a fuller sound. He hired the young, "modern style musician" Smokey Ducas. By 1935, Wills had added horn and reed players as well as drums to the Playboys. The addition of steel guitar whiz Leon McAuliffe in March 1935 added not only a formidable instrumentalist but a second engaging vocalist. Wills himself largely sang blues and sentimental ballads. Wills and the Texas Playboys did their first recordings on September 23–25, 1935 in Dallas, Texas, being produced by Don Law and Art Satherley of the American Record Corporation. There is strong evidence that the 1935 sessions took place at 508 Park Avenue along with sessions in 1937 and 1938.
With its jazz sophistication, pop music and blues influence, plus improvised scat and wisecrack commentary by Wills, the band became the first superstars of the genre. Milton Brown's death in 1936 had cleared the way for the Playboys.
Session rosters from 1938 show both "lead guitar" and "electric guitar" in addition to guitar and steel guitar in the Texas Playboys recordings. Wills' 1938 recording of "Ida Red" served as a model for Chuck Berry's decades later version of the same song, "Maybellene".
About this time, Wills purchased and performed with an old Guadagnini violin that had once fetched $7,600 for $1,600, the equivalent of about $24,000 in 2009.
In 1940, "New San Antonio Rose" sold a million records and became the signature song of The Texas Playboys. The song's title referred to the fact that Wills had recorded it as a fiddle instrumental in 1938 as "San Antonio Rose". By then, the Texas Playboys were virtually two bands: one a fiddle-guitar-steel band with rhythm section and the second a first-rate big band able to play the day's swing and pop hits as well as Dixieland.
The "front line" of Wills' orchestra consisted of either fiddles or guitars after 1944.
Film career.
In 1940, Wills, along with the Texas Playboys, co-starred with Tex Ritter in "Take Me Back To Oklahoma". Other films would follow. In December 1942, after several band members had left the group, and as World War II raged, Wills joined the Army at the age of 37, but he received a medical discharge in 1943.
Wills also appeared in "The Lone Prairie" (1942), "Riders Of The Northwest Mounted" (1943), "Saddles And Sagebrush" (1943), "The Vigilantes Ride" (1943), "The Last Horseman" (1944), "Rhythm Round-Up" (1945), "Blazing The Western Trail" (1945), and "Lawless Empire" (1945). According to one source, he appeared in a total of 19 films.
Swing era.
After leaving the Army in 1943, Wills moved to Hollywood, moving into a rented house in September, and began to reorganize the Texas Playboys. He became an enormous draw in Los Angeles, where many of his Texas, Oklahoma and regional fans had also relocated during the Great Depression and World War II in search of jobs. Monday through Friday, the band broadcast from 12:01 to 1:00 p.m. PT over KMTR-AM (now KLAC) in Los Angeles. They also played regularly every Friday, Saturday, and Sunday night at the Mission Beach Ballroom in San Diego.
He commanded enormous fees playing dances there, and began to make more creative use of electric guitars to replace the big horn sections the Tulsa band had boasted. For a very brief period in 1944, the Wills band included 23 members, and around mid-year he toured Northern California and the Pacific Northwest with 21 pieces in the orchestra. "Billboard" reported that Wills out-grossed Harry James, Benny Goodman, "both Dorsies, et al." at Civic Auditorium in Oakland, California, in January 1944.
Wills and His Texas Playboys began their first cross-country tour in November 1944, and appeared at the Grand Ole Opry on December 30, 1944. According to the Opry, drums and horns were not considered to be part of country music. Wills' band at the time consisted of two fiddlers, two bass fiddles, two electric guitars, an amplified electric steel guitar, and a trumpet, as well as the noted drums, which belonged to Wills' then drummer, who played in the Dixieland style.
In 1945, Wills' dances were outdrawing those of Tommy Dorsey and Benny Goodman, and he moved to Fresno, California. Then in 1947 he opened the Wills Point nightclub in Sacramento and continued touring the Southwest and Pacific Northwest from Texas to Washington State. While based in Sacramento, his radio broadcasts over 50,000-watt KFBK were heard all over the West.
Famous swing orchestras in California realized that many of their followers were leaving to dance to Bob Will's Western swing. Because he was in such demand, some places booked Wills any time he had an opening, regardless of how undesirable the date. The manager of a popular auditorium in the LA Basin town of Wilmington, California: "Although Monday night dancing is frankly an experiment it was the only night of the week on which this outstanding band could be secured."
During the postwar period, KGO radio in San Francisco syndicated a Bob Wills and His Texas Playboys show recorded at the Fairmont Hotel. Many of these recordings survive today as the Tiffany Transcriptions and are available on CD. They show off the band's strengths significantly, in part because the group was not confined to the three-minute limits of 78 RPM discs. They featured superb instrumental work from fiddlers Joe Holley and Louis Tierney, steel guitarists Roy Honeycutt, Noel Boggs and Herb Remington, guitarists Eldon Shamblin and Junior Barnard and electric mandolinist-fiddler Tiny Moore. The original recorded version of Wills' "Faded Love" appeared on the Tiffanys as a fairly swinging instrumental unlike the ballad it became when lyrics were added in 1950.
On April 3, 1948, Wills and the Texas Playboys appeared for the inaugural broadcast of the "Louisiana Hayride" on KWKH, broadcasting from the Municipal Auditorium in Shreveport, Louisiana.
Wills and the Texas Playboys played dances throughout the West to more than 10,000 people every week. They held dance attendance records at Jantzen Beach in Portland, Oregon; in Santa Monica, California, and at the Oakland (California) Auditorium, where they drew 19,000 people in two nights. Wills also broke an attendance record of 2,100 previously held by Jan Garber at the Armory in Klamath Falls, Oregon, by attracting 2,514 dancers. Wills and the Playboys also played small towns on the West Coast. Actor Clint Eastwood recalled seeing Wills when he was 18 or 19 (1948 or 1949) and working at a pulp mill in Springfield, Oregon.
Appearances at the Bostonia Ballroom in San Diego continued throughout the 1950s.
Still a binge drinker, Wills became increasingly unreliable in the late 1940s, causing a rift with Tommy Duncan (who bore the brunt of audience anger when Wills's binges prevented him from appearing). It ended when he fired Duncan in the fall of 1948.
Later years.
Having lived a lavish lifestyle in California, Wills moved back to Oklahoma City in 1949, then went back on the road to maintain his payroll and Wills Point. He opened a second club, the Bob Wills Ranch House in Dallas, Texas. Turning the club over to managers later revealed to be dishonest left Wills in desperate financial straits with heavy debts to the IRS for back taxes that caused him to sell many assets including, mistakenly, the rights to "New San Antonio Rose". It wrecked him financially.
In 1950, Wills had two Top 10 hits, "Ida Red Likes The Boogie" and "Faded Love". After 1950, radio stations began to increasingly specialize in one form or another of commercially popular music. Wills did not fit into the popular Nashville country and western stations, although he was usually labeled "country and western". Neither did he fit into the pop or middle of the road stations, although he played a good deal of pop music, and was not accepted in the pop music world.
He continued to tour and record through the 1950s into the early 1960s, despite the fact that Western swing's popularity, even in the Southwest, had greatly diminished. Bob could draw "a thousand people on Monday night between 1950 and 1952, but he could not do that by 1956. Entertainment habits had changed."
On Wills' return to Tulsa late in 1957, Jim Downing of the "Tulsa Tribune" wrote an article headlined "Wills Brothers Together Again: Bob Back With Heavy Beat". The article quotes Wills as saying, "Rock and Roll? Why, man, that's the same kind of music we've been playin' since 1928! ... We didn't call it rock and roll back when we introduced it as our style back in 1928, and we don't call it rock and roll the way we play it now. But it's just basic rhythm and has gone by a lot of different names in my time. It's the same, whether you just follow a drum beat like in Africa or surround it with a lot of instruments. The rhythm's what's important." The use of amplified guitars accentuates Wills's claim; some Bob Wills recordings from the 1930s and 1940s sound similar to rock and roll records of the 1950s.
Even a 1958 return to KVOO, where his younger brother Johnnie Lee Wills had maintained the family's presence, did not produce the success he hoped for. He appeared twice on ABC-TV's "Jubilee USA" and kept the band on the road into the 1960s. After two heart attacks, in 1965 he dissolved the Texas Playboys (who briefly continued as an independent unit) to perform solo with house bands. While he did well in Las Vegas and other areas, and made records for the Kapp Records label, he was largely a forgotten figure—even though inducted into the Country Music Hall of Fame in 1968. A 1969 stroke left his right side paralyzed, ending his active career.
May 26, 1975 issue of "TIME" (Milestones section) read: "Died. Bob Wills, 70, "Western Swing" bandleader-composer; of pneumonia; in Fort Worth. Wills turned out dance tunes that are now called country rock, introducing with his Texas Playboys such C & W classics as Take Me Back to Tulsa and New San Antonio Rose".
Legacy.
Wills' style influenced performers Buck Owens and Merle Haggard and helped to spawn a style of music now known as the Bakersfield Sound. (Bakersfield, California was one of Wills' regular stops in his heyday). A 1970 tribute album by Haggard directed a wider audience to Wills' music, as did the appearance of younger "revival" bands like Asleep at the Wheel and the growing popularity of longtime Wills disciple and fan Willie Nelson. By 1971, Wills recovered sufficiently to travel occasionally and appear at tribute concerts. In 1973 he participated in a final reunion session with members of some the Texas Playboys from the 1930s to the 1960s. Merle Haggard was invited to play at this reunion. The session, scheduled for two days, took place in December 1973, with the album to be titled "For The Last Time". Wills, speaking or attempting to holler, appeared on a couple tracks from the first day's session but suffered a stroke overnight. He had a more severe one a few days later. The musicians completed the album without him. Wills by then was comatose. He lingered until his death on May 13, 1975.
In addition to being inducted into the Country Music Hall of Fame in 1968, Wills was inducted into the Nashville Songwriters Hall of Fame in 1970, the Rock and Roll Hall of Fame in the Early Influence category along with the Texas Playboys in 1999, and received the Grammy Lifetime Achievement Award in 2007.
From the 1970s until his 2002 death, Waylon Jennings performed a song called "Bob Wills Is Still The King". In addition, The Rolling Stones performed this song live in Austin, Texas at Zilker Park for their DVD "The Biggest Bang". In a 1968 issue of "Guitar Player", rock guitarist Jimi Hendrix said of Wills and the Playboys: "I dig them. The Grand Ole Opry used to come on, and I used to watch that. They used to have some pretty heavy cats, some heavy guitar players."
Wills ranked #27 in "CMT's 40 Greatest Men In Country Music" in 2003.
Fats Domino once remarked that he patterned his 1960 rhythm section after that of Bob Wills.
During the 49th Grammy Awards in 2007, Carrie Underwood performed his song "San Antonio Rose". Today, George Strait performs Wills' music on concert tours and also records songs influenced by Wills and his Texas-style swing.
The Austin-based Western swing band Asleep at the Wheel have honored Wills' music since the band's inception, mostly notably with their continuing performances of the musical drama "A Ride With Bob", which debuted in Austin in March 2005 to coincide with celebrations of Wills' 100th birthday.
The Bob Wills Birthday Celebration is held every year in March at the Cain's Ballroom in Tulsa, Oklahoma with a Western swing concert and dance.
In 2004, a documentary film about his life and music, entitled "Fiddlin' Man: The Life And Times Of Bob Wills", was released by VIEW Inc.
On October 26, 2006, The Rolling Stones performed the Waylon Jennings-penned "Bob Wills Is Still the King" at Zilker Park in Austin, Texas
In 2011, Proper Records released an album by Hot Club of Cowtown titled "What Makes Bob Holler: A Tribute To Bob Wills And His Texas Playboys".
In 2011, the Texas Legislature adopted a resolution designating western swing as the official "State Music Of Texas".
On February 9, 2014, the 80th Anniversary of Bob Wills' first performance at the Cain's Ballroom in Tulsa, Oklahoma, the Oklahoma Historical Society and the Oklahoma Museum of Popular Culture (OKPOP) announced plans to create a feature-length documentary about the life and music of Bob Wills. The documentary will be titled "Still the King. Bob Wills: The Man. The Music." 

</doc>
<doc id="4834" url="https://en.wikipedia.org/wiki?curid=4834" title="Badtrans">
Badtrans

BadTrans is a malicious Microsoft Windows computer worm distributed by e-mail. Because of a known vulnerability in older versions of Internet Explorer, some e-mail programs, such as Microsoft's Outlook Express and Microsoft Outlook programs, may install and execute the worm as soon as the e-mail message is viewed.
Once executed, the worm replicates by sending copies of itself to other e-mail addresses found on the host's machine, and installs a keystroke logger, which then captures everything typed on the affected computer. Badtrans then transmits the data to one of several e-mail addresses. 
Among the e-mail addresses that received the keyloggers were free addresses at Excite, Yahoo, and IJustGotFired.com. IJustGotFired is a free service of MonkeyBrains, a San Francisco-based Internet service provider.
The target address at IJustGotFired began receiving e-mails at 3:23pm on November 24, 2001. Once the account exceeded its quotas, it was automatically disabled, but the messages were still saved as they arrived. The address received over 100,000 keylogs in the first day alone.
In mid-December, the FBI contacted Rudy Rucker, Jr., owner of MonkeyBrains, and requested a copy of the keylogged data. All of that data was stolen from the victims of the worm; it includes no information about the creator of Badtrans.
Instead of complying with the FBI request, MonkeyBrains published a database website http://badtrans.monkeybrains.net for the public to determine if a given address has been compromised. The database does not reveal the actual passwords or keylogged data.

</doc>
<doc id="4836" url="https://en.wikipedia.org/wiki?curid=4836" title="Barış Manço">
Barış Manço

Mehmet Barış Manço (born Tosun Yusuf Mehmet Barış Manço; 2 January 1943 – 1 February 1999), known by his stage name Barış Manço, was a Turkish rock musician, singer, songwriter, composer, and television producer. Beginning his musical career while attending Galatasaray High School, he was a pioneer of rock music in Turkey and one of the founders of the Anatolian rock genre. Manço composed around 200 songs and is among the best-selling and most awarded Turkish artists to date. Many of his songs were translated into a variety of languages including English, French, Japanese, Greek, Italian, Bulgarian, Romanian, Persian, Hebrew, Urdu, Arabic, and German, among others. Through his TV program, "7'den 77'ye" (""From 7 to 77""), Manço traveled the world and visited most countries on the globe. He remains one of the most popular public figures of Turkey.
Early life and career.
Barış Manço was born in Üsküdar, Istanbul, Turkey on 2 January 1943. His mother, Rikkat Uyanık, was a famous singer in the early 1940s. His older brother, who was born during World War II, was named Savaş ("war" in Turkish) while he was named Barış ("peace" in Turkish) by his parents to celebrate the end of the war. At birth, he was additionally named Tosun Yusuf after his deceased uncle Yusuf called Tosun (literally: Joseph the Sturdy). However, this name was erased just before he attended the primary school.
In primary school his head was shaven to prevent head lice, a serious threat back then, which he cited among reasons for his later signature long hair.
During his highschool days in Galatasaray High School (and later in Şişli Terakki High School) he formed his first band, Kafadarlar ("The Buddies"), allegedly upon seeing Erkin Koray's band performing, all students of Deutsche Schule Istanbul ("İstanbul Alman Lisesi"), a nearby highschool. Prof. Dr. Asaf Savaş Akat, a famous economist in Turkey, played saxophone, and guitarist Ender Enön made his own guitar because it was difficult to find a real one on the market in those years.
In 1962 and 1963, with his next band, Harmoniler ("The Harmonies"), he recorded cover versions of some of popular American twist songs and rearrangements of Turkish folk songs in rock and roll form, marking the beginning of the Anatolian rock movement, a synthesis of Turkish folk music and rock. In this period, his key visual and musical influence was Elvis Presley.
After graduating from high school in 1963, he moved to Europe, travelling around Paris and Liège, where he formed bands with local musicians and recorded some singles mainly in English and in French but also in Turkish. Then, in 1964, Barış Manço continued his studies at the Royal Academy of Fine Arts in Liège, Belgium. He toured with his band Les Mistigris (not related to Mistigris) in Germany, Belgium, France and Turkey until 1967.
In 1967, he suffered a serious car accident, after which he started to grow his signature mustache to disguise his scar.
Frustrated by the difficulties of working with musicians from different nationalities, he formed Kaygısızlar (The Carefrees), featuring Mazhar Alanson and Fuat Güner, future members of the band MFÖ. He recorded several singles and toured with the band, both domestically and internationally, until the band members revealed that they did not want to live abroad.
In 1970, he formed Barış Manço Ve ... ("Barış Manço and ...") again with foreign musicians, to record his first hit single, both in Turkey and in Belgium, "Dağlar Dağlar" (Mountains, Mountains!), selling over 700,000 copies. Today, the song remains one of his most popular works.
1970s.
After the success of "Dağlar Dağlar", Manço recorded a couple of singles with Moğollar (The Mongols), another influential Turkish Anatolian rock band. He then decided to return to Turkey where he recorded with the reformed Kaygısızlar for a short period. In 1971, his early works were compiled under his first full-length album "Dünden Bugüne", today commonly referred as "Dağlar Dağlar".
In 1972, he formed Kurtalan Ekspres, a legend by itself, the band that would accompany him until his death. In 1975 until when he continued to release singles, he released his first non-compilation LP "2023", a concept album that includes many instrumental songs.
As a last attempt to reach international success, he released the LP titled "Baris Mancho" (1976), a strange transcription of his name, mostly with George Hayes Orchestra under CBS Records label, in Europe and South Africa. Although the album did not bring the fame he was expecting, it did reach the top of the charts in Romania and Morocco. The following year, the album was released in Turkey under the title "Nick the Chopper".
From 1977 to 1980, he released three more albums in Turkey, partly consisting of compilations of older singles, namely "Sakla Samanı Gelir Zamanı" (1977), "Yeni Bir Gün" (1979) and "20. Sanat Yılı Disko Manço" (1980), all following a similar sound with "2023". All these albums are now rarity items, but most of the material from the era are available in later compilations "Ben Bilirim" and "Sarı Çizmeli Mehmet Ağa".
1980s.
In 1981, Manço released "Sözüm Meclisten Dışarı" with Kurtalan Ekspres, containing many hit songs including "Alla Beni Pulla Beni", "Arkadaşım Eşek", "Gülpembe", "Halhal" and "Dönence" among others. The album remains as one of their most popular works and launched a boost of popularity for Barış Manço during the 1980s.
"Arkadaşım Eşek" ("My Friend Donkey"), quickly grew very popular among children (the song is about rural nostalgia and was not initially intended as a children's song). Throughout his career, he went on to write many other songs primarily for children to achieve an iconic acceptance among Turkish children of the 1980s and 1990s.
On the other hand "Gülpembe", composed by Kurtalan Ekspres bassist Ahmet Güvenç, a requiem for Manço's grandmother, caught older audiences and probably is the artist's most popular song, competing perhaps only with "Dağlar Dağlar".
In 1983, "Estağfurullah, Ne Haddimize" was released. It contained hit songs "Halil İbrahim Sofrası" and "Kol Düğmeleri", a new version of the artist's first song. "Halil İbrahim Sofrası" exemplified Manço's signature moral themed lyrics, a rare feature in Turkish pop music.
In 1985, "24 Ayar Manço" which included "Gibi Gibi" and a long conceptual song "Lahburger" was released. It also marked the beginning of the shift in Manço's sound characterized with the heavy use of synthesizers and drum machine in contrast with his older works consisting of a group oriented rock based sound. In subsequent years, Manço released "Değmesin Yağlı Boya" (1986), "Sahibinden İhtiyaçtan" (1988) and "Darısı Başınıza" (1989), all containing a couple of hit songs and demonstrating his new sound.
"7'den 77'ye" and 1990s.
In 1988, "7'den 77'ye" ("From 7 to 77"), a TV show directed and presented by Manço, began to run on TRT 1, the Turkish state television channel. It was a combined music, talk show, and documentary program which was a major hit during the eight years it stayed on air. Manço traveled to almost 150 countries for the show. "Adam Olacak Çocuk", a part of the show, strengthened Manço's acceptance among children.
Although his popularity continued mostly due to the TV show, his musical works in the 1990s were not well received. The albums "Mega Manço" (1992) and "Müsadenizle Çocuklar" (1995) were considered as the weakest efforts of his career, despite the limited success of 1992 children hit "Ayı" (The Bear). On the other hand, in 1995 he toured in Japan with Kurtalan Ekspres, leading to "Live In Japan" (1996), his only live album. He released two albums in that country with some recognition as "the man who writes songs about vegetables", referring to "Domates, Biber, Patlıcan" ("Tomato, Pepper, Aubergine") and "Nane, Limon Kabuğu" (Mint, Lemon Rind), two of his hit songs from the 1980s.
Death.
On 1 February 1999, Barış Manço died of a sudden heart attack before the release of his just finished last work "Mançoloji" ("Mançology" or "Manchology") (1999), a double album containing the new recordings of his hit songs along with an unfinished instrumental song "40. Yıl" ("The 40th Anniversary"), celebrating his 40th year in music. His sudden death caused an almost unanimous shock in Turkey with millions of people mourning and tens of thousands of people attending his funeral.
Legacy.
Barış Manço was one of the most influential Turkish musicians. In his early career he and his bands contributed to the Turkish rock movement by combining traditional Turkish music with rock influences, which is still one of the main trends of Turkish popular music.
His visual image, characterized by his long hair, mustache and big rings, softened the reaction of the otherwise conservative Turkish public opinion.
Manço pioneered the progressive rock-influenced Anatolian rock movement in the 1970s. His experimentation with electronic instruments in the late 1980s contributed to the 1990s sound of Turkish popular music.
His lyrics with diverse themes, mostly following a somewhat modernized version of the "aşık" (wandering folk poets) tradition were heavily marginal in the popular music scene of the 1980s which was mostly dominated by love-themed lyrics.
In 2002, a tribute album was released under the name "Yüreğimdeki Barış Şarkıları" ("Songs of Barış (Peace) In My Heart"), featuring 15 extremely popular Turkish artists of such diverse genres as arabesque, pop and rock (both Anatolian and western style) demonstrating his wide range of influence.
Discography.
Singles.
With Harmoniler
With Jacques Denjean Orchestra
With Les Mistigris
With Kaygısızlar
With "Barış Manço Ve'
With Moğollar
With Moğollar / Kaygısızlar
With Kaygısızlar / Les Mistigris
With Kurtalan Ekspres
With George Hayes Orchestra / Kurtalan Ekspres
With Kurtalan Ekspres

</doc>
<doc id="4840" url="https://en.wikipedia.org/wiki?curid=4840" title="Blitz BASIC">
Blitz BASIC

Blitz BASIC refers to the programming language dialect that was interpreted by the first Blitz compilers, devised by New Zealand-based developer Mark Sibly. Being derived from BASIC, Blitz syntax was designed to be easy to pick up for beginners first learning to program. The languages are game-programming oriented but are often found general-purpose enough to be used for most types of application. The Blitz language evolved as new products were released, with recent incarnations offering support for more advanced programming techniques such as object-orientation and multi-threading. This led to the languages losing their BASIC moniker in later years.
History.
The first iteration of the Blitz language was created for the Amiga platform and published by the Australian firm Memory and Storage Technology. Returning to New Zealand, Blitz BASIC 2 was published several years later by Acid Software (a local Amiga game publisher). Since then, Blitz compilers have been released on several platforms. Following the demise of the Amiga as a commercially viable platform, the Blitz BASIC 2 source code was released to the Amiga community. Development continues to this day under the name AmiBlitz.
BlitzBasic.
Idigicon published BlitzBasic for Microsoft Windows in October 2000. The language included a built-in API for performing basic 2D graphics and audio operations. Following the release of Blitz3D, BlitzBasic is often synonymously referred to as Blitz2D.
Recognition of BlitzBasic increased when a limited range of "free" versions were distributed in popular UK computer magazines such as PC Format. This resulted in a legal dispute between the developer and publisher which was eventually resolved amicably.
Versions.
Blitz3D.
Blitz3D was released for Microsoft Windows in September 2001, competing with other similar PC game-development languages of the time (such as Dark Basic). Blitz3D extended BlitzBasic's command-set with the inclusion of an API for a DirectX 7-based 3D engine.
Although originally Blitz3D's distribution rights were owned by Idigicon, Blitz Research Ltd. later signed a deal with the firm so as to allow Blitz Research Ltd. to distribute Blitz3D themselves. In return, Idigicon were granted full rights to distribute BlitzBasic and to clear any outstanding stock copies of Blitz3D.
Blitz3D was released as Open Source on 4 August 2014.
BlitzPlus.
In February 2003, Blitz Research Ltd. released BlitzPlus also for Microsoft Windows. It lacked the 3D engine of Blitz3D, but did bring new features to the 2D side of the language by implementing limited Microsoft Windows control support for creating native GUIs. Backwards compatibility of the 2D engine was also extended, allowing compiled BlitzPlus games and applications to run on systems that might only have DirectX 1.
BlitzPlus was released as Open Source on 28 April 2014
BlitzMax.
The first BlitzMax compiler was released in December 2004 for Mac OS X. This made it the first Blitz dialect that could be compiled on *nix platforms. Compilers for Microsoft Windows and Linux were subsequently released in May 2005. BlitzMax brought the largest change of language structure to the modern range of Blitz products by extending the type system to include object-oriented concepts and modifying the graphics API to better suit OpenGL. BlitzMax was also the first of the Blitz languages to represent strings internally using UCS2, allowing native-support for string literals composed of non-ASCII characters.
BlitzMax's platform-agnostic command-set allows developers to compile and run source code on multiple platforms. However the official compiler and build chain will only generate binaries for the platform that it is executing on. Unofficially, users have been able to get Linux and Mac OS X to cross-compile to the Windows platform.
BlitzMax is also the first modular version of the Blitz languages, improving the extensibility of the command-set. In addition, all of the standard modules shipped with the compiler are open-source and so can be tweaked and recompiled by the programmer if necessary. The official BlitzMax cross-platform GUI module (known as MaxGUI) allows developers to write GUI interfaces for their applications on Linux (FLTK), Mac (Cocoa) and Windows. Various user-contributed modules extend the use of the language by wrapping such libraries as wxWidgets, Cairo, and Fontconfig as well as a selection of database modules. There are also a selection of third-party 3D modules available namely MiniB3D - an open-source OpenGL engine which can be compiled and used on all three of BlitzMax's supported platforms.
In October 2007, BlitzMax 1.26 was released which included the addition of a reflection module. BlitzMax 1.32 shipped new threading and Lua scripting modules and most of the standard library functions have been updated so that they are unicode friendly.
BlitzMax was released as Open Source on 21 September 2015
Blitz3D SDK.
Blitz3D SDK is a 3D graphics engine based on the engine in Blitz3D. It was marketed for use with C++, C#, BlitzMax and PureBasic, however it could also be used with other languages that follow compatible calling conventions. As of January 2011, Blitz3D SDK is no longer listed for sale on the official Blitz website.
Max3D module.
In 2008, the source code to Max3D - a C++-based cross-platform 3D engine - was released under a BSD license. This engine focused on OpenGL but had an abstract backend for other graphics drivers (such as DirectX) and made use of several open-source libraries, namely Assimp, Boost and ODE.
Despite the excitement in the Blitz community of Max3D being the eagerly awaited successor to Blitz3D, interest and support died off soon after the source code was released and eventually development came to a halt. There is no indication that Blitz Research will pick up the project again.
Monkey and Mojo.
In 2011, BRL released a new cross-platform programming language called Monkey and its first official module called Mojo. Monkey has a similar syntax to BlitzMax, but instead of compiling direct to assembly code, it translates Monkey source files directly into source code for a chosen language, framework or platform e.g. Windows, Mac OS X, iOS, Android, HTML5, and Flash.
Sample code.
The following code creates a windowed application that shows the current time in binary and decimal format. This code is written in BlitzBasic, but will compile and run in both Blitz3D and BlitzPlus. See below for the same example written in BlitzMax.
BlitzMax version of the above clock:

</doc>
<doc id="4842" url="https://en.wikipedia.org/wiki?curid=4842" title="Bliss bibliographic classification">
Bliss bibliographic classification

The Bliss bibliographic classification (BC) is a library classification system that was created by Henry E. Bliss (1870–1955) and published in four volumes between 1940 and 1953. Although originally devised in the United States, it was more commonly adopted by British libraries. A second edition of the system (BC2) has been developed in Britain since 1977.
Origins of the system.
Henry E. Bliss began working on the Bliss Classification system while working at the City College of New York Library as Assistant Librarian. He was a critic of Melvil Dewey's work with the Dewey Decimal System and believed that organization of titles needed to be done with an intellectual mind frame. Being overly pragmatic or simply alphabetical, would be inadequate. In fact, Bliss is the only theorist who created an organizational scheme based on societal needs. Bliss wanted a classification system that would provide distinct rules yet still be adaptable to whatever kind of collection a library might have, as different libraries have different needs. His solution was the concept of “alternative location,” in which a particular subject could be put in more than one place, as long as the library made a specific choice and used it consistently.
Bliss discusses his theories and basis of organization for the Bliss Classification for the first time in his 1910 article, "A Modern Classification for Libraries, with Simple Notation, Mnemonics, and Alternatives". This publication followed his 1908 reclassification of the City College collection. His work, "Organization of Knowledge and the System of the Sciences" was published in four volumes between 1940 and 1953.
The four broad underlying policies of the BC system are: 
Bliss deliberately avoided the use of the decimal point because of his objection to Dewey's system. Instead he used capital and lower-case letters, numerals, and every typographical symbol available on his extensive and somewhat eccentric typewriter.
Adoption and change to BC2.
In 1967 the Bliss Classification Association was formed. Its first publication was the Abridged Bliss Classification (ABC), intended for school libraries. In 1977 it began to publish and maintain a revised version of Bliss’s system, the Bliss Bibliographic Classification (Second Edition) or BC2. This retains only the broad outlines of Bliss's scheme, replacing most of the detailed notation with a new scheme based on the principles of faceted classification. 15 of approximately 28 volumes of schedules have so far been published. A revision of this nature has been considered by some to be a completely new system.
The City College library continued to use Bliss’s system until 1967, when it switched to the Library of Congress system. It had become too expensive to train new staff members to use BC, and too expensive to maintain in general. Much of the Bliss stacks remain, however, as no-one has re-cataloged the books.
The case was different, however, in Britain. BC proved more popular there and also spread to other English-speaking countries. Part of the reason for its success was that libraries in teachers’ colleges liked the way Bliss had organized the subject areas on teaching and education. By the mid-1950s the system was being used in at least sixty British libraries and in a hundred by the 1970s The Bliss Classification system has been found to be successful in academic, specialty, government, and law libraries. It has also found success in libraries outside of the United States of America, as many of these libraries do not have a history of using either the Dewey Decimal, or the Library of Congress classification system.
The Bliss Classification system has been found to be successful in academic, specialty, government, and law libraries. It has also found success in libraries outside of the United States of America, as many of these libraries do not have a history of using either the Dewey Decimal, or the Library of Congress classification system.
The general organizational pattern for classifying titles in the BC2 method are:
Classifications (BC2).
The Class Schedule is:

</doc>
<doc id="4845" url="https://en.wikipedia.org/wiki?curid=4845" title="Blood alcohol content">
Blood alcohol content

Blood alcohol content (BAC), also called blood alcohol concentration, blood ethanol concentration, or blood alcohol level is most commonly used as a metric of alcohol intoxication for legal or medical purposes. Blood Alcohol Content is the legal name for BAC but Blood Alcohol Concentration is sometimes used for simpler description.
Blood alcohol content is usually expressed as a percentage of ethanol in the blood in units of mass of alcohol per volume of blood or mass of alcohol per mass of blood, depending on the country. For instance, in North America a BAC of 0.1 (0.1% or one tenth of one percent) means that there are 0.10 g of alcohol for every dL of blood.
Estimated blood alcohol content by intake.
To calculate estimated peak blood alcohol concentration (EBAC), a variation, including drinking period in hours, of the Widmark formula was used. The formula is:
where :
Regarding metabolism (MR) in the formula; Females demonstrated a higher average rate of elimination (mean, 0.017; range, 0.014-0.021 g/210 L) than males (mean, 0.015; range, 0.013-0.017 g/210 L). Female subjects on average had a higher percentage of body fat (mean, 26.0; range, 16.7-36.8%) than males (mean, 18.0; range, 10.2-25.3%). Additionally, men are, on average, heavier than women but it is not strictly accurate to say that the water content of a person alone is responsible for the dissolution of alcohol within the body, because alcohol does dissolve in fatty tissue as well. When it does, a certain amount of alcohol is temporarily taken out of the blood and briefly stored in the fat. For this reason, most calculations of alcohol to body mass simply use the weight of the individual, and not specifically his/her water content. Finally, it is speculated that the bubbles in sparkling wine may speed up alcohol intoxication by helping the alcohol to reach the bloodstream faster. A study conducted at the University of Surrey in the United Kingdom gave subjects equal amounts of flat and sparkling Champagne which contained the same levels of alcohol. After 5 minutes following consumption, the group that had the sparkling wine had 54 milligrams of alcohol in their blood while the group that had the same sparkling wine, only flat, had 39 milligrams.
Examples:
Binge drinking.
The National Institute on Alcohol Abuse and Alcoholism (NIAAA) define the term "binge drinking" as a pattern of drinking that brings a person’s blood alcohol concentration (BAC) to 0.08 grams percent or above. This typically happens when men consume 5 or more drinks, and when women consume 4 or more drinks, in about 2 hours.
Units of measurement.
There are several different units in use around the world for defining blood alcohol concentration. Each is defined as either a mass of alcohol per volume of blood or a mass of alcohol per mass of blood (never a volume per volume). 1 milliliter of blood has a mass of approximately 1.06 grams. Because of this, units by volume are similar but not identical to units by mass. In the U.S. the concentration unit 1% w/v (percent mass/volume, equivalent to 10 g/l or 1 g per 100 ml) is in use. This is not to be confused with the amount of alcohol measured on the breath, as with a breathalyzer. The amount of alcohol measured on the breath is generally accepted as proportional to the amount of alcohol present in the blood at a rate of 1:2100. Therefore, a breathalyzer measurement of 0.10 mg/L of breath alcohol converts to 0.0001×2100 g/10dL, or 0.021 g/dL of blood alcohol (the units of the BAC in the United States). While a variety of units (or sometimes lack thereof) is used throughout the world, many countries use the g/L unit, which does not create confusion as percentages do. Usual units are highlighted in the table below.
Legal limits.
For purposes of law enforcement, blood alcohol content is used to define intoxication and provides a rough measure of impairment. Although the degree of impairment may vary among individuals with the same blood alcohol content, it can be measured objectively and is therefore legally useful and difficult to contest in court. Most countries disallow operation of motor vehicles and heavy machinery above prescribed levels of blood alcohol content. Operation of boats and aircraft are also regulated.
The alcohol level at which a person is considered legally impaired varies by country. The list below gives limits by country. These are typically blood alcohol content limits for the operation of a vehicle.
It is illegal to have any measurable alcohol in the blood while driving in these countries. Most jurisdictions have a tolerance slightly higher than zero to account for false positives and naturally occurring alcohol in the body. Some of the following jurisdictions have a general prohibition of alcohol.
Limits by country (BrAC: breath alcohol content).
In certain countries, alcohol limits are determined by the breath alcohol content (BrAC), not to be confused with blood alcohol content (BAC).
Test assumptions.
Blood alcohol tests assume the individual being tested is average in various ways. For example, on average the ratio of blood alcohol content to breath alcohol content (the "partition ratio") is 2100 to 1. In other words, there are 2100 parts of alcohol in the blood for every part in the breath. However, the actual ratio in any given individual can vary from 1300:1 to 3100:1, or even more widely. This ratio varies not only from person to person, but within one person from moment to moment. Thus a person with a true blood alcohol level of .08% but a partition ratio of 1700:1 at the time of testing would have a .10 reading on a Breathalyzer calibrated for the average 2100:1 ratio.
A similar assumption is made in urinalysis. When urine is analyzed for alcohol, the assumption is that there are 1.3 parts of alcohol in the urine for every 1 part in the blood, even though the actual ratio can vary greatly.
Breath alcohol testing further assumes that the test is "post-absorptive"—that is, that the absorption of alcohol in the subject's body is complete. If the subject is still actively absorbing alcohol, their body has not reached a state of "equilibrium" where the concentration of alcohol is uniform throughout the body. Most forensic alcohol experts reject test results during this period as the amounts of alcohol in the breath will not accurately reflect a true concentration in the blood.
Metabolism and excretion.
Alcohol is absorbed throughout the gastrointestinal tract, but more slowly in the stomach than in the small or large intestine. For this reason, alcohol consumed with food is absorbed more slowly, because it spends a longer time in the stomach. Furthermore, alcohol dehydrogenase is present in the stomach lining. After absorption, the alcohol passes to the liver through the hepatic portal vein, where it undergoes a first pass of metabolism before entering the general bloodstream.
Alcohol is removed from the bloodstream by a combination of metabolism, excretion, and evaporation. The relative proportion disposed of in each way varies from person to person, but typically about 95% is metabolized by the liver. The remainder of the alcohol is eliminated through excretion in breath, urine, sweat, feces, milk and saliva. Excretion into urine typically begins after about 40 minutes, whereas metabolisation commences as soon as the alcohol is absorbed, and even before alcohol levels have risen in the brain.
Alcohol is metabolized mainly by the group of six enzymes collectively called alcohol dehydrogenase. These convert the ethanol into acetaldehyde (an intermediate more toxic than ethanol). The enzyme acetaldehyde dehydrogenase then converts the acetaldehyde into non-toxic Acetic acid.
Many physiologically active materials are removed from the bloodstream (whether by metabolism or excretion) at a rate proportional to the current concentration, so that they exhibit exponential decay with a characteristic halflife (see pharmacokinetics). This is not true for alcohol, however. Typical doses of alcohol actually saturate the enzymes' capacity, so that alcohol is removed from the bloodstream at an approximately constant rate. This rate varies considerably between individuals. Another sex based difference is in the elimination of alcohol. People under 25, women or with liver disease may process alcohol more slowly. False High (BAC) readings are related to patients with proteinuria and hematuria, due to kidney-liver metabolism and failure (for example, Hematuria 1+ protenuria 1+ )
Such persons have impaired acetaldehyde dehydrogenase, which causes acetaldehyde levels to peak higher, producing more severe hangovers and other effects such as flushing and tachycardia. Conversely, members of certain ethnicities that traditionally did not use alcoholic beverages have lower levels of alcohol dehydrogenases and thus "sober up" very slowly, but reach lower aldehyde concentrations and have milder hangovers. Rate of detoxification of alcohol can also be slowed by certain drugs which interfere with the action of alcohol dehydrogenases, notably aspirin, furfural (which may be found in fusel alcohol), fumes of certain solvents, many heavy metals, and some pyrazole compounds. Also suspected of having this effect are cimetidine (Tagamet), ranitidine (Zantac), and acetaminophen (Tylenol) (paracetamol).
Currently, the only known substance that can increase the rate of metabolism of alcohol is fructose. The effect can vary significantly from person to person, but a 100 g dose of fructose has been shown to increase alcohol metabolism by an average of 80%. Fructose also increases false positives of high BAC ratio readings in anyone with proteinuria and hematuria, due to kidney-liver metabolism.
Alcohol absorption can be slowed by ingesting alcohol on a full stomach. Spreading the total absorption of alcohol over a greater period of time decreases the maximum alcohol level, decreasing the hangover effect. Thus, drinking on a full stomach or drinking while ingesting drugs which slow the breakdown of ethanol into acetaldehyde will reduce the maximum blood levels of this substance and thus decrease the hangover. 
Carbonated beverages.
Alcohol in carbonated beverages is absorbed faster than alcohol in non-carbonated drinks. Another study also confirmed this, conducted at the University of Surrey in the United Kingdom gave subjects equal amounts of flat and sparkling champagne which contained the same levels of alcohol. After 5 minutes following consumption, the group that had the sparkling wine had 54 milligrams of alcohol in their blood while the group that had the same wine, only flat, had 39 milligrams.
Retrograde extrapolation.
Retrograde extrapolation is the mathematical process by which someone's blood alcohol concentration at the time of driving is estimated by projecting backwards from a later chemical test. This involves estimating the absorption and elimination of alcohol in the interim between driving and testing. The rate of elimination in the average person is commonly estimated at .015 to .020 grams per deciliter per hour (g/dl/h), although again this can vary from person to person and in a given person from one moment to another. Metabolism can be affected by numerous factors, including such things as body temperature, the type of alcoholic beverage consumed, and the amount and type of food consumed.
In an increasing number of states, laws have been enacted to facilitate this speculative task: the blood alcohol content at the time of driving is legally presumed to be the same as when later tested. There are usually time limits put on this presumption, commonly two or three hours, and the defendant is permitted to offer evidence to rebut this presumption.
Forward extrapolation can also be attempted. If the amount of alcohol consumed is known, along with such variables as the weight and sex of the subject and period and rate of consumption, the blood alcohol level can be estimated by extrapolating forward. Although subject to the same infirmities as retrograde extrapolation—guessing based upon averages and unknown variables—this can be relevant in estimating BAC when driving and/or corroborating or contradicting the results of a later chemical test.
Highest recorded blood alcohol level/content.
There have been reported cases of blood alcohol content higher than 1%:

</doc>
<doc id="4848" url="https://en.wikipedia.org/wiki?curid=4848" title="Barrister">
Barrister

A barrister (also known as barrister-at-law or Bar-at-law) is a type of lawyer in common law jurisdictions who works at higher levels of court. Barristers mostly specialise in courtroom advocacy and litigation. Their tasks include taking cases in superior courts and tribunals, drafting legal pleadings, researching the philosophy, hypothesis and history of law, and giving expert legal opinions. Often, barristers are also recognised as "legal scholars".
Barristers are distinguished from solicitors, who have more direct access to clients, and may do transactional-type legal work. It is mainly barristers who are appointed as judges, and they are rarely hired by clients directly. In some legal systems, including those of Scotland, South Africa, Scandinavia, Pakistan, India, Bangladesh, the British Crown dependencies of Jersey, Guernsey and the Isle of Man, the word barrister is also regarded as an "honorific title".
In a few jurisdictions, barristers are usually forbidden from "conducting" litigation, and can only act on the instructions of a senior solicitor, who performs tasks such as corresponding with parties and the court, and drafting court documents. In England and Wales, barristers may seek authorisation from the Bar Standards Board to conduct litigation. This allows a barrister to practise in a 'dual capacity', fulfilling the role of both barrister and solicitor.
In some countries with common law legal systems, such as New Zealand and some regions of Australia, lawyers are entitled to practise both as barristers and solicitors, but it remains a separate system of qualification to practise exclusively as a barrister.
Differences between barristers and other lawyers.
Differences.
A barrister, who can be considered as a jurist, is a lawyer who represents a litigant as advocate before a court of appropriate jurisdiction. A barrister speaks in court and presents the case before a judge or jury. In some jurisdictions, a barrister receives additional training in evidence law, ethics, and court practice and procedure. In contrast, a solicitor generally meets with clients, does preparatory and administrative work and provides legal advice. In this role, he or she may draft and review legal documents, interact with the client as necessary, prepare evidence, and generally manage the day-to-day administration of a lawsuit. A solicitor can provide a crucial support role to a barrister when in court, such as managing large volumes of documents in the case or even negotiating a settlement outside the courtroom while the trial continues inside.
There are other essential differences. A barrister will usually have rights of audience in the higher courts, whereas other legal professionals will often have more limited access, or will need to acquire additional qualifications to have such access. In countries where there is a split between the roles of barrister and solicitor; whereas, the barrister in civil law jurisdictions is responsible for appearing in trials or pleading cases before the courts.
Barristers usually have particular knowledge of case law, precedent, and the skills to "build" a case. When a solicitor in general practice is confronted with an unusual point of law, they may seek the "opinion of counsel" on the issue.
In most countries, barristers operate as sole practitioners, and are prohibited from forming partnerships or from working as a barrister as part of a corporation. (In 2009, the Clementi Report recommended the abolition of this restriction in England and Wales.) However, barristers normally band together into "chambers" to share clerks (administrators) and operating expenses. Some chambers grow to be large and sophisticated, and have a distinctly corporate feel. In some jurisdictions, barristers may be employed by firms of solicitors, banks, or corporations as in-house legal advisers.
In contrast, solicitors and attorneys work directly with the clients and are responsible for engaging a barrister with the appropriate expertise for the case. Barristers generally have little or no direct contact with their 'lay clients', particularly without the presence or involvement of the solicitor. All correspondence, inquiries, invoices, and so on, will be addressed to the solicitor, who is primarily responsible for the barrister's fees.
In court, barristers are often visibly distinguished from solicitors by their apparel. For example, in Ireland, England, and Wales, a barrister usually wears a horsehair wig, stiff collar, bands, and a gown. Since January 2008, solicitor advocates have also been entitled to wear wigs, but wear different gowns.
In many countries the traditional divisions between barristers and solicitors are breaking down. Barristers once enjoyed a monopoly on appearances before the higher courts, but in Great Britain this has now been abolished, and solicitor advocates can generally appear for clients at trial. Increasingly, firms of solicitors are keeping even the most advanced advisory and litigation work in-house for economic and client relationship reasons. Similarly, the prohibition on barristers taking instructions directly from the public has also been widely abolished. But, in practice, direct instruction is still a rarity in most jurisdictions, partly because barristers with narrow specializations, or who are only really trained for advocacy, are not prepared to provide general advice to members of the public.
Historically, barristers have had a major role in trial preparation, including drafting pleadings and reviewing evidence. In some areas of law, that is still the case. In other areas, it is relatively common for the barrister to receive the brief from the instructing solicitor to represent a client at trial only a day or two before the proceeding. Part of the reason for this is cost. A barrister is entitled to a 'brief fee' when a brief is delivered, and this represents the bulk of her/his fee in relation to any trial. They are then usually entitled to a 'refresher' for each day of the trial after the first. But if a case is settled before the trial, the barrister is not needed and the brief fee would be wasted. Some solicitors avoid this by delaying delivery of the brief until it is certain the case will go to trial.
Justification for a split profession.
Some benefits of maintaining the split include:
Some disadvantages of the split include:
A detailed examination of the justifications for a split legal profession and of the arguments in favour of a fused profession can be found in English solicitor Peter Reeve’s 1986 book, "Are Two Legal Professions Necessary?"
Regulation.
Barristers are regulated by the Bar for the jurisdiction where they practise, and in some countries, by the Inn of Court to which they belong. In some countries, there is external regulation.
Inns of Court, where they exist, regulate admission to the profession. Inns of Court are independent societies that are titularly responsible for the training, admission (calling), and discipline of barristers. Where they exist, a person may only be called to the Bar by an Inn, of which they must first be a member. In fact, historically, call to and success at the Bar, to a large degree, depended upon social connections made early in life.
A Bar collectively describes all members of the profession of barrister within a given jurisdiction. While as a minimum the Bar is an association embracing all its members, it is usually the case, either "de facto" or "de jure", that the Bar is invested with regulatory powers over the manner in which barristers practise.
Barristers around the world.
In the common law tradition, the respective roles of a lawyer – that is as legal adviser and advocate – were formally split into two separate, regulated sub-professions, the other being the office of solicitor. Historically, the distinction was absolute, but in the modern legal age, some countries that had a split legal profession now have a fused profession – anyone entitled to practise as a barrister may also practise as a solicitor, and vice versa. In practice, the distinction may be non-existent, minor, or marked, depending on the jurisdiction. In some jurisdictions, such Australia, Scotland and Ireland, there is little overlap.
Australia.
In the Australian states of New South Wales and Queensland, there is a split profession. Nevertheless, subject to conditions, barristers can accept direct access work from clients. Each state Bar Association regulates the profession and essentially has the functions of the English Inns of Court. In the states of South Australia, Victoria, and Western Australia, as well as the Australian Capital Territory, the professions of barrister and solicitor are fused, but an independent bar nonetheless exists, regulated by the Legal Practice Board of the state or territory. In Tasmania and the Northern Territory, the profession is fused, although a very small number of practitioners operate as an independent bar.
Generally counsel dress in the traditional English manner (wig, gown, and jabot) before superior courts, although they no longer robe for appearances in lower jurisdictions. Wigs are no longer worn in the highest civil court in New South Wales, the Court of Appeal. Wigs are still worn in the Supreme Court, while only robes without wigs are worn in the District Courts in civil matters. Robes and wigs are worn in all criminal cases. In Western Australia, wigs are no longer worn in any court.
Each year, the Bar Association appoints certain barristers of seniority and eminence to the rank of "Senior Counsel" (in most States and Territories) or "Queen's Counsel" (in the Northern Territory, Queensland, and Victoria). Such barristers carry the title "SC" or "QC" after their name. The appointments are made after a process of consultation with members of the profession and the judiciary. Senior Counsel appear in particularly complex or difficult cases. They make up about 14 per cent of the bar in New South Wales.
Canada.
In Canada (except Quebec), the professions of barrister and solicitor are fused, and many lawyers refer to themselves with both names, even if they do not practise in both areas. In colloquial parlance within the Canadian legal profession, lawyers often term themselves as "litigators" (or "barristers"), or as "solicitors", depending on the nature of their law practice though some may in effect practise as both litigators and solicitors. However, "litigators" would generally perform all litigation functions traditionally performed by barristers and solicitors; in contrast, those terming themselves "solicitors" would generally limit themselves to legal work not involving practice before the courts (not even in a preparatory manner as performed by solicitors in England), though some might practise before chambers judges. As is the practice in many other Commonwealth jurisdictions such as Australia, Canadian litigators are "gowned", but without a wig, when appearing before courts of "superior jurisdiction".
The situation is somewhat different in Quebec as a result of its civil law tradition. The profession of solicitor, or "avoué", never took hold in colonial Quebec, so attorneys ("avocats") have traditionally been a fused profession, arguing and preparing cases in contentious matters, whereas Quebec's other type of lawyer, civil-law notaries ("notaires"), handle out-of-court non-contentious matters. However, a number of areas of non-contentious private law are not monopolized by notaries so that attorneys often specialise in handling either trials, cases, advising, or non-trial matters. The only disadvantage is that attorneys cannot draw up public instruments that have the same force of law as notarial acts. Most large law firms in Quebec offer the full range of legal services of law firms in common-law provinces. Intending Quebec attorneys must earn a bachelor's degree in civil law, pass the provincial bar examination, and successfully complete a legal internship to be admitted to practice. Attorneys are regulated by the Quebec Law Society ("Barreau du Québec").
France.
In France, "avocats", or attorneys, were, until the 20th century, the equivalent of barristers. The profession included several grades ranked by seniority: "avocat-stagiaire" (trainee, who was already qualified but needed to complete 2 years (or more, depending on the period) of training alongside seasoned lawyers), "avocat", and "avocat honoraire" (senior barrister). Since the 14th century and during the course of the 19th and 20th in particular, French barristers competed in territorial battles over respective areas of legal practice against the "conseil juridique" (legal advisor, transactional solicitor) and "avoué" (procedural solicitor), and expanded to become the generalist legal practitioner, with the notable exception of "notaires" (notaries), who are ministry appointed lawyers (with a separate qualification) and who retain exclusivity over conveyancing and probate. After the 1971 and 1990 legal reforms, the "avocat" was fused with the "avoué" and the "conseil juridique", making the "avocat" (or, if female, "avocate") an all-purpose lawyer for matters of contentious jurisdiction, analogous to an American attorney. French attorneys usually do not (although it they are entitled to) act both as litigators (trial lawyers) and legal consultants (advising lawyers), known respectively as "avocat plaidant" and "avocat-conseil". This distinction is however purely informal and does not correspond to any difference in qualification or admission to the roll. All intending attorneys must pass an examination to be able to enrol in one of the "Centre régional de formation à la profession d'avocat (CRFPA)" (Regional centre for the training of lawyers). The "CRFPA" course has a duration of two years and is a mix between classroom teachings and internships. Its culmination is the "stage final" (final training), where the intending attorney spends 6 months in a law firm (generally in his/her favoured field of practice and in a firm in which he/she hopes to be recruited afterwards). The intending attorney then needs to pass the "Certificat d'Aptitude à la Profession d'Avocat (CAPA)", which is the last professional examination allowing him/her to join a court's bar ("barreau"). It is generally recognised that the first examination is much more difficult than the CAPA and is dreaded by most law students. Each bar is regulated by a Bar Council ("Ordre du barreau").
Germany.
In Germany, no distinction is made and lawyers may plead at all courts with the exception of the Federal Court of Justice ("Bundesgerichtshof") to which fewer than fifty lawyers are admitted as of 25 September 2007. See the list of lawyers admitted to the "Bundesgerichtshof". Those lawyers may not plead at other courts, almost only deal with litigation, and are usually instructed by a lawyer who represented the client in the lower courts. However, these restrictions do not apply to criminal cases, nor to pleadings at courts of the other court systems (labour, administrative, taxation, and social courts, as well as the EU court system).
Hong Kong.
The legal profession in Hong Kong is also divided into two branches: barristers and solicitors.
In the High Court and the Court of Final Appeal, as a general rule, only barristers and solicitor-advocates are allowed to speak on behalf of any party in open court. This means that solicitors are restricted from doing so. In these two courts, barristers dress in the traditional English manner, as do the judges and other lawyers.
In Hong Kong, the rank of Queen's Counsel was granted prior to the transfer of the sovereignty of Hong Kong from the United Kingdom to China in 1997. After the handover, the rank has been replaced by Senior Counsel post-nominal letters: SC. Senior Counsels may still, however, style themselves as silks, like their British counterparts.
Ireland.
In the Republic of Ireland, admission to the Bar by the Chief Justice of Ireland is restricted to those on whom a Barrister-at-Law degree (B.L.) has first been conferred. The Honorable Society of King's Inns is the only educational establishment which runs vocational courses for barristers in the Republic and degrees of Barrister-at-Law can only be conferred by King's Inns. King's Inns are also the only body with the capacity to call individuals to the bar and to disbar them.
Most Irish barristers choose to be governed thereafter by the Bar Council of Ireland, a quasi-private entity. Senior members of the profession may be selected for elevation to the Inner Bar, when they may describe themselves as Senior Counsel ("S.C."). Admission to the Inner Bar is made by declaration before the Supreme Court, patents of precedence having been granted by the Government. Irish barristers are sole practitioners and may not form chambers or partnerships if they wish to remain members of the Bar Council's Law Library.
To practise under the Bar Council of Ireland's rules, a newly qualified barrister is apprenticed to an experienced barrister of at least seven years' experience. This apprenticeship is known as pupillage or devilling. Devilling is compulsory for those barristers who wish to be members of the Law Library and lasts for one legal year. It is common to devil for a second year in a less formal arrangement but this is not compulsory.
Israel.
In Israel there is no distinction between barristers and solicitors, even though the judicial system is based mostly on English common law, from when Britain administered what was then Mandatory Palestine from 1920 to 1948.
Japan.
Japan adopts a unified system. However, there are certain classes of qualified professionals who are allowed to practise in certain limited areas of law, such as scriveners (""shiho shoshi"", qualified to handle title registration, deposit, and certain petite court proceedings with additional certification), tax accountants (""zeirishi"", qualified to prepare tax returns, provide advice on tax computation and represent a client in administrative tax appeals) and patent agents (""benrishi"", qualified to practise patent registration and represent a client in administrative patent appeals). Only the lawyers (""bengoshi"") can appear before court and are qualified to practise in any areas of law, including, but not limited to, areas that those qualified law-related professionals above are allowed to practise. Most attorneys still focus primarily on court practice and still a very small number of attorneys give sophisticated and expertised legal advice on a day-to-day basis to large corporations.
Netherlands.
The Netherlands used to have a semi-separated legal profession comprising the lawyer and the "procureur", the latter resembling, to some extent, the profession of barrister. Under that system, lawyers were entitled to represent their clients in law, but were only able to file cases before the court at which they were registered. Cases falling under the jurisdiction of another court had to be filed by a "procureur" registered at that court, in practice often another lawyer exercising both functions. Questions were raised on the necessity of the separation, given the fact that its main purpose – the preservation of the quality of the legal profession and observance of local court rules and customs – had become obsolete. For that reason, the "procureur" as a separate profession was abolished and its functions merged with the legal profession in 2008. Currently, lawyers can file cases before any court, regardless of where they are registered. The only notable exception concerns cases brought before the Supreme Court, which have to be handled by lawyers registered in the district of South Holland, mainly for qualitative reasons.
New Zealand.
In New Zealand, the professions are not formally fused but practitioners are enrolled in the High Court as "Barristers and Solicitors". They may choose, however, to practise as barristers sole. About 15% practise solely as barristers, mainly in the larger cities and usually in "chambers" (following the British terminology). They receive "instructions" from other practitioners, at least nominally. They usually conduct the proceedings in their entirety.
Any lawyer may apply to become a Queen's Counsel (QC) to recognize long standing contribution to the legal profession but this status is only conferred on those practising as solicitors in exceptional circumstances. This step, referred to as "being called to the inner bar" or "taking silk", is considered highly prestigious and has been a step in the career of many New Zealand judges.
Nigeria.
In Nigeria, there is no formal distinction between barristers and solicitors. All students who pass the bar examinations – offered exclusively by the Nigerian Law School – are called to the Nigerian bar, by the Body of Benchers. Lawyers may argue in any Federal trial or appellate court as well as any of the courts in Nigeria's 36 states and the Federal Capital Territory. The Legal Practitioner's Act, refers to Nigerian lawyers as Legal Practitioners, and following their call to the Bar, Nigerian lawyers enter their names in the register or Roll of Legal Practitioners kept at the Supreme Court. Perhaps for this reason, a Nigerian lawyer is also often referred to as a Barrister and Solicitor of the Supreme Court of Nigeria, and many Nigerian lawyers term themselves Barrister-at-Law complete with the postnominal initials "B.L.".
The vast majority of Nigerian lawyers combine contentious and non-contentious work, although there is a growing tendency for practitioners in the bigger practices to specialise in one or the other. In colloquial parlance within the Nigerian legal profession, lawyers may for this reason be referred to as "litigators" or as "solicitors".
Consistent with the practice in England and elsewhere in the Commonwealth, senior members of the profession may be selected for elevation to the Inner Bar by conferment of the rank of Senior Advocate of Nigeria (SAN).
Poland.
In Poland, there are two main types of legal professions: advocate and legal advisor. Both are regulated and these professions are restricted only for people who graduated five-year law studies, have at least 3 years of experience and passed 5 difficult national exams (civil law, crime law, business law, administrative law and ethic) or have a doctor of law degree. Before 2015, the only difference was that advocates have a right to represent clients before the court in all cases and the legal advisors could not represent clients before the court in criminal cases. Presently, the legal advisors can also represent clients in criminal cases so currently, the differences between this professions are only historical significance.
South Asia.
The role of barristers in South Asia generally have been difficult to identify and it has been considered as controversial.
In India, the law relating to the Barrister is the Advocates Act, 1961 introduced and thought up by Ashoke Kumar Sen, the then law minister of India, which is a law passed by the Parliament and is administered and enforced by the Bar Council of India. Under the Act, the Bar Council of India is the supreme regulatory body to regulate the legal profession in India and also to ensure the compliance of the laws and maintenance of professional standards by the legal profession in the country. For this purpose, the Bar Council of India is authorized to pass regulations and make orders in individual cases and also generally.
Each State has a Bar Council of its own whose function is to enroll the Barristers willing to practise predominately within the territorial confines of that State and to perform the functions of the Bar Council of India within the territory assigned to them. Therefore, each law degree holder must be enrolled with a (single) State Bar Council to practise in India. However, enrollment with any State Bar Council does not restrict the Barrister from appearing before any court in India, even though it is beyond the territorial jurisdiction of the State Bar Council which he is enrolled in.
The advantage with having the State Bar Councils is that the work load of the Bar Council of India can be divided into these various State Bar Councils and also that matters can be dealt with locally and in an expedited manner. However, for all practical and legal purposes, the Bar Council of India retains with it, the final power to take decisions in any and all matters related to the legal profession on the whole or with respect to any 
The process for being entitled to practise in India is twofold. First, the applicant must be a holder of a law degree from a recognized institution in India (or from one of the four recognised Universities in the United Kingdom) and second, must pass the enrollment qualifications of the Bar Council of the state where he/she seeks to be enrolled. For this purpose, the Bar Council of India has an internal Committee whose function is to supervise and examine the various institutions conferring law degrees and to grant recognition to these institutions once they meet the required standards. In this manner the Bar Council of India also ensures the standard of education required for practising in India are met with. As regards the qualification for enrollment with the State Bar Council, while the actual formalities may vary from one State to another, yet predominately they ensure that the application has not been a bankrupt /criminal and is generally fit to practise before courts of India.
Enrollment with a Bar Council also means that the law degree holder is recognized as a Barrister and is required to maintain a standards of conduct and professional demeanor at all times, both on and off the profession. The Bar Council of India also prescribes "Rules of Conduct" to be observed by the Barristers in the courts, while interacting with clients and even otherwise.
In Pakistan a graduated can qualify as a Barrister after completion of a Master of Laws degree, six months pupillage under a senior Barrister in his chambers and afterwards to go for Bar admission test, the Bar Council of the relevant province examine him that he is fit or not to become as an Barrister and is not convicted. After passing the multiple choice question examination and interview conducted by provincial Bar Council, the Bar Council will issue him the license for appearing before the Courts.
In Bangladesh, graduate lawyers have to seat for and pass the Bar Council Exam to become professional barristers.
By passing the Bar Council Exam, barristers are eligible to practise in the Supreme Court of Bangladesh and other courts. In Bangladesh there is an association called Barristers' Association of Bangladesh that represents the barrister. A license is obtained after successful completion of two year's practice in the lower courts by applicant, which is reviewed by a body of the relevant provincial Bar Council. Most applications after successful completion of the requirement, are accepted.
South Africa.
In South Africa the employment and practise of advocates (as barristers are known in South Africa) is consistent with the rest of the Commonwealth. Advocates carry the rank of Junior or Senior Counsel (SC), and are mostly briefed and paid by solicitors (known as attorneys). They are usually employed in the higher courts, particularly in the Appeal Courts where they often appear as specialist counsel. South African solicitors (attorneys) follow a practice of referring cases to Counsel for an opinion before proceeding with a case, when Counsel in question practises as a specialist in the case law at stake. Aspiring advocates currently spend one year in pupillage (formerly only six months) before being admitted to the bar in their respective provincial or judicial jurisdictions. The term 'Advocate' is sometimes used in South Africa as a title, e. g. 'Advocate John Doe, SC' ('Advokaat' in Afrikaans) in the same fashion as 'Dr. John Doe' for a medical doctor.
South Korea.
There is no distinction between two branches. A person who passed the national bar exam, after 2 years of national education, would be able to become a judge, a prosecutor or a "lawyer" in accordance of their grades upon graduation. However, Korea has accommodated Law School System either. Now, there are two(2) ways to be a lawyer, the traditional but time limited way until 2017 and the other way to take a course on a law school and passing the bar exam. Under the current legal system, to be a judge or a prosecutor, lawyers need to practise their legal knowledge. A "lawyer" does not have any limitation of practice.
Spain.
Spain has a division that somewhat corresponds to the division in Britain between barristers/advocates and solicitors. "Procuradores" represent the litigant procedurally in court, generally under the authority of a power of attorney executed by a civil law notary, while "abogados" represent the substantive claims of the litigant through trial advocacy. Essentially, Procuradores are court agents and their practice is confined to the locality of the court to which they are admitted. Procuradores are regulated by Royal Decree 2046 of 1982, which approved the General Statute of the Procuradores, and the Organic Law no.6 of 1985. The General Statute regulates the qualifications and conduct of the procuradores. Thus, obligations to act "pro bono" are laid down by Article 13.
United States.
The United States does not draw a distinction between lawyers as pleaders (barristers) and lawyers as agents (or solicitors). All lawyers who have passed a bar examination and have been admitted to practice may prosecute or defend in the courts of the state where they are admitted. Historically, a distinction was made, and a separate label for barristers (called "counselors," hence the expression "attorney "and" counselor at law") existed in certain states, though both professions have long since been fused into the all-purpose attorney. Attorneys specializing in court procedure, combining advocacy and case preparation, are called trial attorneys or litigators.
South Carolina still requires attorneys to be licensed separately to plead in a courtroom. Additionally, some state appellate courts require attorneys to obtain a separate certificate of admission to plead and practise in the appellate court. Federal courts require specific admission to that court's bar to practise before it. At the state appellate level and in Federal courts, there is generally no separate examination process, although some U.S. district courts require an examination on practices and procedures in their specific courts. Unless an examination is required, admission is usually granted as a matter of course to any licensed attorney in the state where the court is located. Some federal courts will grant admission to any attorney licensed in any U.S. jurisdiction.
United Kingdom.
England and Wales.
Although with somewhat different laws, England and Wales are considered within the United Kingdom a single united and unified legal jurisdiction for the purposes of both civil and criminal law, alongside Scotland and Northern Ireland, the other two legal jurisdictions within the United Kingdom. England and Wales are covered by a common bar (an organisation of barristers) and a single law society (an organisation of solicitors).
The profession of barrister in England and Wales is a separate profession from that of solicitor. It is, however, possible to hold the qualification of both barrister and solicitor at the same time. It is not necessary to leave the bar to qualify as a solicitor.
Barristers are regulated by the Bar Standards Board, a division of the General Council of the Bar. A barrister must be a member of one of the Inns of Court, which traditionally educated and regulated barristers. There are four Inns of Court: The Honourable Society of Lincoln's Inn, The Honourable Society of Gray's Inn, The Honourable Society of the Middle Temple, and The Honourable Society of the Inner Temple. All are situated in central London, near the Royal Courts of Justice. They perform scholastic and social roles, and in all cases, provide financial aid to student barristers (subject to merit) through scholarships. It is the Inns that actually "call" the student to the Bar at a ceremony similar to a graduation. Social functions include dining with other members and guests and hosting other events.
Law graduates wishing to work and be known as barristers must take the Bar Professional Training Course (BPTC - previously Bar Vocational Course or BVC) at one of the institutions authorised by the Bar Council to offer the BPTC. On successful completion of the BPTC student barristers are "called" to the bar by their respective inns and are elevated to the degree of "Barrister". However, before they can practise independently they must first undertake 12 months of pupillage. The first six months of this period is spent shadowing more senior practitioners, after which pupil barristers may begin to undertake some court work of their own. Following successful completion of this stage, most barristers then join a set of Chambers, a group of counsel who share the costs of premises and support staff whilst remaining individually self-employed.
In December 2014 there were just over 15,500 barristers in independent practice, of whom about ten percent are Queen's Counsel and the remainder are junior barristers. Many barristers (about 2,800) are employed in companies as 'in-house' counsel, or by local or national government or in academic institutions.
Certain barristers in England and Wales are now instructed directly by members of the public. Members of the public may engage the services of the barrister directly within the framework of the Public Access Scheme; a solicitor is not involved at any stage. Barristers undertaking public access work can provide legal advice and representation in court in almost all areas of law (see the Public Access Information on the Bar Council website) and are entitled to represent clients in any court or tribunal in England and Wales. Once instructions from a client are accepted, it is the barrister (rather than the solicitor) who advises and guides the client through the relevant legal procedure or litigation.
Before a barrister can undertake Public Access work, they must have completed a special course. At present, about one in 20 barristers has so qualified. There is also a separate scheme called 'Licensed Access', available to certain nominated classes of professional client; it is not open to the general public. Public access work is experiencing a huge surge at the bar, with barristers taking advantage of the new opportunity for the bar to make profit in the face of legal aid cuts elsewhere in the profession.
The ability of barristers to accept such instructions is a recent development; it results from a change in the rules set down by the General Council of the Bar in July 2004. The Public Access Scheme has been introduced as part of the drive to open up the legal system to the public and to make it easier and cheaper to obtain access to legal advice. It further reduces the distinction between solicitors and barristers. The distinction remains however because there are certain aspects of a solicitor's role that a barrister is not able to undertake.
Some honorific suffixes to signify notable barristers may be "Esquire". Even though the term "barrister-at-law" is sometimes seen, and was once very common, it has never been formally correct in England and Wales. Barrister is the only correct nomenclature.
Northern Ireland.
In April 2003 there were 554 barristers in independent practice in Northern Ireland. 66 were Queen's Counsel (QCs), barristers who have earned a high reputation and are appointed by the Queen on the recommendation of the Lord Chancellor as senior advocates and advisers.
Those barristers who are not QCs are called Junior Counsel and are styled "BL" or "Barrister-at-Law". The term "junior" is often misleading since many members of the Junior Bar are experienced barristers with considerable expertise.
Benchers are, and have been for centuries, the governing bodies of the four Inns of Court in London and King's Inns, Dublin. The Benchers of the Inn of Court of Northern Ireland governed the Inn until the enactment of the Constitution of the Inn in 1983, which provides that the government of the Inn is shared between the Benchers, the Executive Council of the Inn and members of the Inn assembled in General Meeting.
The Executive Council (through its Education Committee) is responsible for considering Memorials submitted by applicants for admission as students of the Inn and by Bar students of the Inn for admission to the degree of Barrister-at-Law and making recommendations to the Benchers. The final decisions on these Memorials are taken by the Benchers. The Benchers also have the exclusive power of expelling or suspending a Bar student and of disbarring a barrister or suspending a barrister from practice.
The Executive Council is also involved with: education; fees of students; calling counsel to the Bar, although call to the Bar is performed by the Lord Chief Justice of Northern Ireland on the invitation of the Benchers; administration of the Bar Library (to which all practising members of the Bar belong); and liaising with corresponding bodies in other countries.
The Bar Council is responsible for the maintenance of the standards, honour and independence of the Bar and, through its Professional Conduct Committee, receives and investigates complaints against members of the Bar in their professional capacity.
Scotland.
In Scotland, an advocate is, in all respects except name, a barrister, but there are significant differences in professional practice.
In Scotland, admission to and the conduct of the profession is regulated by the Faculty of Advocates (as opposed to an Inn).
Crown dependencies & UK Overseas Territories.
Isle of Man, Jersey and Guernsey.
In the Bailiwick of Jersey, there are solicitors (called "ecrivains") and advocates (French "avocat"). In the Bailiwicks of Jersey and Guernsey and on the Isle of Man, Advocates perform the combined functions of both solicitors and barristers.
Gibraltar.
Gibraltar is a British Overseas Territory boasting a legal profession based on the common law. The legal profession includes both barristers and solicitors with most barristers also acting as solicitors. Admission and Disciplinary matters in Gibraltar are dealt with by the Bar Council of Gibraltar and the Supreme Court of Gibraltar. In order for Barristers and/or Solicitors to be admitted as a practising lawyer in Gibraltar they must comply with the Supreme Court Act 1930 as amended by the Supreme Court Amendment Act 2015 which requires, amongst other things, for all newly admitted lawyers as of the 1 July 2015 to undertake a year's course in Gibraltar Law at the University of Gibraltar. Solicitors also have right of audience in Gibraltar's courts.

</doc>
<doc id="4849" url="https://en.wikipedia.org/wiki?curid=4849" title="Battle of Gettysburg">
Battle of Gettysburg

The Battle of Gettysburg (, with an sound) was fought July 1–3, 1863, in and around the town of Gettysburg, Pennsylvania, by Union and Confederate forces during the American Civil War. The battle involved the largest number of casualties of the entire war and is often described as the war's turning point. Union Maj. Gen. George Meade's Army of the Potomac defeated attacks by Confederate Gen. Robert E. Lee's Army of Northern Virginia, ending Lee's attempt to invade the North.
After his success at Chancellorsville in Virginia in May 1863, Lee led his army through the Shenandoah Valley to begin his second invasion of the North—the Gettysburg Campaign. With his army in high spirits, Lee intended to shift the focus of the summer campaign from war-ravaged northern Virginia and hoped to influence Northern politicians to give up their prosecution of the war by penetrating as far as Harrisburg, Pennsylvania, or even Philadelphia. Prodded by President Abraham Lincoln, Maj. Gen. Joseph Hooker moved his army in pursuit, but was relieved of command just three days before the battle and replaced by Meade.
Elements of the two armies initially collided at Gettysburg on July 1, 1863, as Lee urgently concentrated his forces there, his objective being to engage the Union army and destroy it. Low ridges to the northwest of town were defended initially by a Union cavalry division under Brig. Gen. John Buford, and soon reinforced with two corps of Union infantry. However, two large Confederate corps assaulted them from the northwest and north, collapsing the hastily developed Union lines, sending the defenders retreating through the streets of town to the hills just to the south.
On the second day of battle, most of both armies had assembled. The Union line was laid out in a defensive formation resembling a fishhook. In the late afternoon of July 2, Lee launched a heavy assault on the Union left flank, and fierce fighting raged at Little Round Top, the Wheatfield, Devil's Den, and the Peach Orchard. On the Union right, Confederate demonstrations escalated into full-scale assaults on Culp's Hill and Cemetery Hill. All across the battlefield, despite significant losses, the Union defenders held their lines.
On the third day of battle, fighting resumed on Culp's Hill, and cavalry battles raged to the east and south, but the main event was a dramatic infantry assault by 12,500 Confederates against the center of the Union line on Cemetery Ridge, known as Pickett's Charge. The charge was repulsed by Union rifle and artillery fire, at great loss to the Confederate army.
Lee led his army on a torturous retreat back to Virginia. Between 46,000 and 51,000 soldiers from both armies were casualties in the three-day battle.
On November 19, President Lincoln used the dedication ceremony for the Gettysburg National Cemetery to honor the fallen Union soldiers and redefine the purpose of the war in his historic Gettysburg Address.
Background.
Military situation.
Shortly after the Army of Northern Virginia won a major victory over the Army of the Potomac at the Battle of Chancellorsville (April 30 – May 6, 1863), Robert E. Lee decided upon a second invasion of the North (the first was the unsuccessful Maryland Campaign of September 1862, which ended in the bloody Battle of Antietam). Such a move would upset U.S. plans for the summer campaigning season and possibly reduce the pressure on the besieged Confederate garrison at Vicksburg. The invasion would allow the Confederates to live off the bounty of the rich Northern farms while giving war-ravaged Virginia a much-needed rest. In addition, Lee's 72,000-man army could threaten Philadelphia, Baltimore, and Washington, and possibly strengthen the growing peace movement in the North.
Initial movements to battle.
Thus, on June 3, Lee's army began to shift northward from Fredericksburg, Virginia. Following the death of Thomas J. "Stonewall" Jackson, Lee reorganized his two large corps into three new corps, commanded by Lt. Gen. James Longstreet (First Corps), Lt. Gen. Richard S. Ewell (Second), and Lt. Gen. A.P. Hill (Third); both Ewell and Hill, who had formerly reported to Jackson as division commanders, were new to this level of responsibility. The Cavalry Division remained under the command of Maj. Gen. J.E.B. Stuart.
The Union Army of the Potomac, under Maj. Gen. Joseph Hooker, consisted of seven infantry corps, a cavalry corps, and an Artillery Reserve, for a combined strength of more than 100,000 men.
The first major action of the campaign took place on June 9 between cavalry forces at Brandy Station, near Culpeper, Virginia. The 9,500 Confederate cavalrymen under Stuart were surprised by Maj. Gen. Alfred Pleasonton's combined arms force of two cavalry divisions (8,000 troopers) and 3,000 infantry, but Stuart eventually repulsed the Union attack. The inconclusive battle, the largest predominantly cavalry engagement of the war, proved for the first time that the Union horse soldier was equal to his Southern counterpart.
By mid-June, the Army of Northern Virginia was poised to cross the Potomac River and enter Maryland. After defeating the U.S. garrisons at Winchester and Martinsburg, Ewell's Second Corps began crossing the river on June 15. Hill's and Longstreet's corps followed on June 24 and 25. Hooker's army pursued, keeping between the U.S. capital and Lee's army. The U.S. crossed the Potomac from June 25 to 27.
Lee gave strict orders for his army to minimize any negative impacts on the civilian population. Food, horses, and other supplies were generally not seized outright, although quartermasters reimbursing Northern farmers and merchants with Confederate money were not well received. Various towns, most notably York, Pennsylvania, were required to pay indemnities in lieu of supplies, under threat of destruction. During the invasion, the Confederates seized some 40 northern African Americans. A few of them were escaped fugitive slaves, but most were freemen; all were sent south into slavery under guard.
On June 26, elements of Maj. Gen. Jubal Early's division of Ewell's Corps occupied the town of Gettysburg after chasing off newly raised Pennsylvania militia in a series of minor skirmishes. Early laid the borough under tribute but did not collect any significant supplies. Soldiers burned several railroad cars and a covered bridge, and destroyed nearby rails and telegraph lines. The following morning, Early departed for adjacent York County.
Meanwhile, in a controversial move, Lee allowed Jeb Stuart to take a portion of the army's cavalry and ride around the east flank of the Union army. Lee's orders gave Stuart much latitude, and both generals share the blame for the long absence of Stuart's cavalry, as well as for the failure to assign a more active role to the cavalry left with the army. Stuart and his three best brigades were absent from the army during the crucial phase of the approach to Gettysburg and the first two days of battle. By June 29, Lee's army was strung out in an arc from Chambersburg (28 miles (45 km) northwest of Gettysburg) to Carlisle (30 miles (48 km) north of Gettysburg) to near Harrisburg and Wrightsville on the Susquehanna River.
In a dispute over the use of the forces defending the Harpers Ferry garrison, Hooker offered his resignation, and Abraham Lincoln and General-in-Chief Henry W. Halleck, who were looking for an excuse to get rid of him, immediately accepted. They replaced Hooker early on the morning of June 28 with Maj. Gen. George Gordon Meade, then commander of the V Corps.
On June 29, when Lee learned that the Army of the Potomac had crossed the Potomac River, he ordered a concentration of his forces around Cashtown, located at the eastern base of South Mountain and eight miles (13 km) west of Gettysburg. On June 30, while part of Hill's Corps was in Cashtown, one of Hill's brigades, North Carolinians under Brig. Gen. J. Johnston Pettigrew, ventured toward Gettysburg. In his memoirs, Maj. Gen. Henry Heth, Pettigrew's division commander, claimed that he sent Pettigrew to search for supplies in town—especially shoes.
When Pettigrew's troops approached Gettysburg on June 30, they noticed Union cavalry under Brig. Gen. John Buford arriving south of town, and Pettigrew returned to Cashtown without engaging them. When Pettigrew told Hill and Heth what he had seen, neither general believed that there was a substantial U.S. force in or near the town, suspecting that it had been only Pennsylvania militia. Despite General Lee's order to avoid a general engagement until his entire army was concentrated, Hill decided to mount a significant reconnaissance in force the following morning to determine the size and strength of the enemy force in his front. Around 5 a.m. on Wednesday, July 1, two brigades of Heth's division advanced to Gettysburg.
Opposing forces.
Union.
The Army of the Potomac, initially under Maj. Gen. Joseph Hooker (Maj. Gen. George G. Meade replaced Hooker in command on June 28), consisted of more than 100,000 men in the following organization:
During the advance on Gettysburg, Maj. Gen. Reynolds was in operational command of the left, or advanced, wing of the Army, consisting of the I, III, and XI Corps. Note that many other Union units (not part of the Army of the Potomac) were actively involved in the Gettysburg Campaign, but not directly involved in the Battle of Gettysburg. These included portions of the Union IV Corps, the militia and state troops of the Department of the Susquehanna, and various garrisons, including that at Harpers Ferry.
Confederate.
In reaction to the death of Lt. Gen. Thomas J. "Stonewall" Jackson after Chancellorsville, Lee reorganized his Army of Northern Virginia (75,000 men) from two infantry corps into three.
First day of battle.
Herr Ridge, McPherson Ridge and Seminary Ridge.
Anticipating that the Confederates would march on Gettysburg from the west on the morning of July 1, Buford laid out his defenses on three ridges west of the town: Herr Ridge, McPherson Ridge and Seminary Ridge. These were appropriate terrain for a delaying action by his small cavalry division against superior Confederate infantry forces, meant to buy time awaiting the arrival of Union infantrymen who could occupy the strong defensive positions south of town at Cemetery Hill, Cemetery Ridge, and Culp's Hill. Buford understood that if the Confederates could gain control of these heights, Meade's army would have difficulty dislodging them.
Heth's division advanced with two brigades forward, commanded by Brig. Gens. James J. Archer and Joseph R. Davis. They proceeded easterly in columns along the Chambersburg Pike. Three miles (5 km) west of town, about 7:30 a.m. on July 1, the two brigades met light resistance from vedettes of Union cavalry, and deployed into line. According to lore, the Union soldier to fire the first shot of the battle was Lt. Marcellus Jones. In 1886 Lt. Jones returned to Gettysburg to mark the spot where he fired the first shot with a monument. Eventually, Heth's men reached dismounted troopers of Col. William Gamble's cavalry brigade, who raised determined resistance and delaying tactics from behind fence posts with fire from their breechloading carbines. Still, by 10:20 a.m., the Confederates had pushed the Union cavalrymen east to McPherson Ridge, when the vanguard of the I Corps (Maj. Gen. John F. Reynolds) finally arrived.
North of the pike, Davis gained a temporary success against Brig. Gen. Lysander Cutler's brigade but was repulsed with heavy losses in an action around an unfinished railroad bed cut in the ridge. South of the pike, Archer's brigade assaulted through Herbst (also known as McPherson's) Woods. The U.S. Iron Brigade under Brig. Gen. Solomon Meredith enjoyed initial success against Archer, capturing several hundred men, including Archer himself.
General Reynolds was shot and killed early in the fighting while directing troop and artillery placements just to the east of the woods. Shelby Foote wrote that the Union cause lost a man considered by many to be "the best general in the army." Maj. Gen. Abner Doubleday assumed command. Fighting in the Chambersburg Pike area lasted until about 12:30 p.m. It resumed around 2:30 p.m., when Heth's entire division engaged, adding the brigades of Pettigrew and Col. John M. Brockenbrough.
As Pettigrew's North Carolina Brigade came on line, they flanked the 19th Indiana and drove the Iron Brigade back. The 26th North Carolina (the largest regiment in the army with 839 men) lost heavily, leaving the first day's fight with around 212 men. By the end of the three-day battle, they had about 152 men standing, the highest casualty percentage for one battle of any regiment, North or South. Slowly the Iron Brigade was pushed out of the woods toward Seminary Ridge. Hill added Maj. Gen. William Dorsey Pender's division to the assault, and the I Corps was driven back through the grounds of the Lutheran Seminary and Gettysburg streets.
As the fighting to the west proceeded, two divisions of Ewell's Second Corps, marching west toward Cashtown in accordance with Lee's order for the army to concentrate in that vicinity, turned south on the Carlisle and Harrisburg roads toward Gettysburg, while the Union XI Corps (Maj. Gen. Oliver O. Howard) raced north on the Baltimore Pike and Taneytown Road. By early afternoon, the U.S. line ran in a semicircle west, north, and northeast of Gettysburg.
However, the U.S. did not have enough troops; Cutler, who was deployed north of the Chambersburg Pike, had his right flank in the air. The leftmost division of the XI Corps was unable to deploy in time to strengthen the line, so Doubleday was forced to throw in reserve brigades to salvage his line.
Around 2 p.m., the Confederate Second Corps divisions of Maj. Gens. Robert E. Rodes and Jubal Early assaulted and out-flanked the Union I and XI Corps positions north and northwest of town. The Confederate brigades of Col. Edward A. O'Neal and Brig. Gen. Alfred Iverson suffered severe losses assaulting the I Corps division of Brig. Gen. John C. Robinson south of Oak Hill. Early's division profited from a blunder by Brig. Gen. Francis C. Barlow, when he advanced his XI Corps division to Blocher's Knoll (directly north of town and now known as Barlow's Knoll); this represented a salient in the corps line, susceptible to attack from multiple sides, and Early's troops overran Barlow's division, which constituted the right flank of the Union Army's position. Barlow was wounded and captured in the attack.
As U.S. positions collapsed both north and west of town, Gen. Howard ordered a retreat to the high ground south of town at Cemetery Hill, where he had left the division of Brig. Gen. Adolph von Steinwehr in reserve. Maj. Gen. Winfield S. Hancock assumed command of the battlefield, sent by Meade when he heard that Reynolds had been killed. Hancock, commander of the II Corps and Meade's most trusted subordinate, was ordered to take command of the field and to determine whether Gettysburg was an appropriate place for a major battle. Hancock told Howard, "I think this the strongest position by nature upon which to fight a battle that I ever saw." When Howard agreed, Hancock concluded the discussion: "Very well, sir, I select this as the battle-field." Hancock's determination had a morale-boosting effect on the retreating Union soldiers, but he played no direct tactical role on the first day.
General Lee understood the defensive potential to the Union if they held this high ground. He sent orders to Ewell that Cemetery Hill be taken "if practicable." Ewell, who had previously served under Stonewall Jackson, a general well known for issuing peremptory orders, determined such an assault was not practicable and, thus, did not attempt it; this decision is considered by historians to be a great missed opportunity.
The first day at Gettysburg, more significant than simply a prelude to the bloody second and third days, ranks as the 23rd biggest battle of the war by number of troops engaged. About one quarter of Meade's army (22,000 men) and one third of Lee's army (27,000) were engaged.
Second day of battle.
Plans and movement to battle.
Throughout the evening of July 1 and morning of July 2, most of the remaining infantry of both armies arrived on the field, including the Union II, III, V, VI, and XII Corps. Longstreet's third division, commanded by Maj. Gen. George Pickett, had begun the march from Chambersburg early in the morning; it did not arrive until late on July 2.
The Union line ran from Culp's Hill southeast of the town, northwest to Cemetery Hill just south of town, then south for nearly two miles (3 km) along Cemetery Ridge, terminating just north of Little Round Top. Most of the XII Corps was on Culp's Hill; the remnants of I and XI Corps defended Cemetery Hill; II Corps covered most of the northern half of Cemetery Ridge; and III Corps was ordered to take up a position to its flank. The shape of the Union line is popularly described as a "fishhook" formation. The Confederate line paralleled the Union line about a mile (1,600 m) to the west on Seminary Ridge, ran east through the town, then curved southeast to a point opposite Culp's Hill. Thus, the Union army had interior lines, while the Confederate line was nearly five miles (8 km) long.
Lee's battle plan for July 2 called for Longstreet's First Corps to position itself stealthily to attack the Union left flank, facing northeast astraddle the Emmitsburg Road, and to roll up the U.S.line. The attack sequence was to begin with Maj. Gens. John Bell Hood's and Lafayette McLaws's divisions, followed by Maj. Gen. Richard H. Anderson's division of Hill's Third Corps. The progressive "en echelon" sequence of this attack would prevent Meade from shifting troops from his center to bolster his left. At the same time, Maj. Gen. Edward "Allegheny" Johnson's and Jubal Early's Second Corps divisions were to make a demonstration against Culp's and Cemetery Hills (again, to prevent the shifting of U.S. troops), and to turn the demonstration into a full-scale attack if a favorable opportunity presented itself.
Lee's plan, however, was based on faulty intelligence, exacerbated by Stuart's continued absence from the battlefield. Instead of moving beyond the U.S. left and attacking their flank, Longstreet's left division, under McLaws, would face Maj. Gen. Daniel Sickles's III Corps directly in their path. Sickles had been dissatisfied with the position assigned him on the southern end of Cemetery Ridge. Seeing higher ground more favorable to artillery positions a half mile (800 m) to the west, he advanced his corps—without orders—to the slightly higher ground along the Emmitsburg Road. The new line ran from Devil's Den, northwest to the Sherfy farm's Peach Orchard, then northeast along the Emmitsburg Road to south of the Codori farm. This created an untenable salient at the Peach Orchard; Brig. Gen. Andrew A. Humphreys's division (in position along the Emmitsburg Road) and Maj. Gen. David B. Birney's division (to the south) were subject to attacks from two sides and were spread out over a longer front than their small corps could defend effectively.
Longstreet's attack was to be made as early as practicable; however, Longstreet got permission from Lee to await the arrival of one of his brigades, and while marching to the assigned position, his men came within sight of a Union signal station on Little Round Top. Countermarching to avoid detection wasted much time, and Hood's and McLaws's divisions did not launch their attacks until just after 4 p.m. and 5 p.m., respectively.
Attacks on the Union left flank.
As Longstreet's divisions slammed into the Union III Corps, Meade was forced to send 20,000 reinforcements in the form of the entire V Corps, Brig. Gen. John C. Caldwell's division of the II Corps, most of the XII Corps, and small portions of the newly arrived VI Corps. The Confederate assault deviated from Lee's plan since Hood's division moved more to the east than intended, losing its alignment with the Emmitsburg Road, attacking Devil's Den and Little Round Top. McLaws, coming in on Hood's left, drove multiple attacks into the thinly stretched III Corps in the Wheatfield and overwhelmed them in Sherfy's Peach Orchard. McLaws's attack eventually reached Plum Run Valley (the "Valley of Death") before being beaten back by the Pennsylvania Reserves division of the V Corps, moving down from Little Round Top. The III Corps was virtually destroyed as a combat unit in this battle, and Sickles's leg was amputated after it was shattered by a cannonball. Caldwell's division was destroyed piecemeal in the Wheatfield. Anderson's division, coming from McLaws's left and starting forward around 6 p.m., reached the crest of Cemetery Ridge, but it could not hold the position in the face of counterattacks from the II Corps, including an almost suicidal bayonet charge by the small 1st Minnesota regiment against a Confederate brigade, ordered in desperation by Hancock to buy time for reinforcements to arrive.
As fighting raged in the Wheatfield and Devil's Den, Col. Strong Vincent of V Corps had a precarious hold on Little Round Top, an important hill at the extreme left of the Union line. His brigade of four relatively small regiments was able to resist repeated assaults by Brig. Gen. Evander M. Law's brigade of Hood's division. Meade's chief engineer, Brig. Gen. Gouverneur K. Warren, had realized the importance of this position, and dispatched Vincent's brigade, an artillery battery, and the 140th New York to occupy Little Round Top mere minutes before Hood's troops arrived. The defense of Little Round Top with a bayonet charge by the 20th Maine, initiated by Lt. Holman S. Melcher, was one of the most fabled episodes in the Civil War and propelled Col. Joshua L. Chamberlain into prominence after the war.
Attacks on the Union right flank.
About 7:00 p.m., the Second Corps' attack by Johnson's division on Culp's Hill got off to a late start. Most of the hill's defenders, the Union XII Corps, had been sent to the left to defend against Longstreet's attacks, and the only portion of the corps remaining on the hill was a brigade of New Yorkers under Brig. Gen. George S. Greene. Because of Greene's insistence on constructing strong defensive works, and with reinforcements from the I and XI Corps, Greene's men held off the Confederate attackers, although the Southerners did capture a portion of the abandoned U.S. works on the lower part of Culp's Hill.
Just at dark, two of Jubal Early's brigades attacked the Union XI Corps positions on East Cemetery Hill where Col. Andrew L. Harris of the 2nd Brigade, 1st Division, came under a withering attack, losing half his men; however, Early failed to support his brigades in their attack, and Ewell's remaining division, that of Maj. Gen. Robert E. Rodes, failed to aid Early's attack by moving against Cemetery Hill from the west. The Union army's interior lines enabled its commanders to shift troops quickly to critical areas, and with reinforcements from II Corps, the U.S. troops retained possession of East Cemetery Hill, and Early's brigades were forced to withdraw.
Jeb Stuart and his three cavalry brigades arrived in Gettysburg around noon but had no role in the second day's battle. Brig. Gen. Wade Hampton's brigade fought a minor engagement with newly promoted 23-year-old Brig. Gen. George Armstrong Custer's Michigan cavalry near Hunterstown to the northeast of Gettysburg.
Third day of battle.
Lee's plan.
General Lee wished to renew the attack on Friday, July 3, using the same basic plan as the previous day: Longstreet would attack the U.S. left, while Ewell attacked Culp's Hill. However, before Longstreet was ready, Union XII Corps troops started a dawn artillery bombardment against the Confederates on Culp's Hill in an effort to regain a portion of their lost works. The Confederates attacked, and the second fight for Culp's Hill ended around 11 a.m. Harry Pfanz judged that, after some seven hours of bitter combat, "the Union line was intact and held more strongly than before."
Lee was forced to change his plans. Longstreet would command Pickett's Virginia division of his own First Corps, plus six brigades from Hill's Corps, in an attack on the U.S. II Corps position at the right center of the Union line on Cemetery Ridge. Prior to the attack, all the artillery the Confederacy could bring to bear on the U.S. positions would bombard and weaken the enemy's line.
The largest artillery bombardment of the war.
Around 1 p.m., from 150 to 170 Confederate guns began an artillery bombardment that was probably the largest of the war. In order to save valuable ammunition for the infantry attack that they knew would follow, the Army of the Potomac's artillery, under the command of Brig. Gen. Henry Jackson Hunt, at first did not return the enemy's fire. After waiting about 15 minutes, about 80 U.S. cannons added to the din. The Army of Northern Virginia was critically low on artillery ammunition, and the cannonade did not significantly affect the Union position.
Pickett's Charge.
Around 3 p.m., the cannon fire subsided, and 12,500 Southern soldiers stepped from the ridgeline and advanced the three-quarters of a mile (1,200 m) to Cemetery Ridge in what is known to history as "Pickett's Charge". As the Confederates approached, there was fierce flanking artillery fire from Union positions on Cemetery Hill and north of Little Round Top, and musket and canister fire from Hancock's II Corps. In the Union center, the commander of artillery had held fire during the Confederate bombardment (in order to save it for the infantry assault, which Meade had correctly predicted the day before), leading Southern commanders to believe the Northern cannon batteries had been knocked out. However, they opened fire on the Confederate infantry during their approach with devastating results. Nearly one half of the attackers did not return to their own lines. Although the U.S. line wavered and broke temporarily at a jog called the "Angle" in a low stone fence, just north of a patch of vegetation called the Copse of Trees, reinforcements rushed into the breach, and the Confederate attack was repulsed. The farthest advance of Brig. Gen. Lewis A. Armistead's brigade of Maj. Gen. George Pickett's division at the Angle is referred to as the "High-water mark of the Confederacy", arguably representing the closest the South ever came to its goal of achieving independence from the Union via military victory. Union and Confederate soldiers locked in hand-to-hand combat, attacking with their rifles, bayonets, rocks and even their bare hands. Armistead ordered his Confederates to turn two captured cannons against Union troops, but discovered that there was no ammunition left, the last double canister shots having been used against the charging Confederates. Armistead was shortly after wounded three times.
There were two significant cavalry engagements on July 3. Stuart was sent to guard the Confederate left flank and was to be prepared to exploit any success the infantry might achieve on Cemetery Hill by flanking the U.S. right and hitting their trains and lines of communications. Three miles (5 km) east of Gettysburg, in what is now called "East Cavalry Field" (not shown on the accompanying map, but between the York and Hanover Roads), Stuart's forces collided with U.S. cavalry: Brig. Gen. David McMurtrie Gregg's division and Brig. Gen. Custer's brigade. A lengthy mounted battle, including hand-to-hand sabre combat, ensued. Custer's charge, leading the 1st Michigan Cavalry, blunted the attack by Wade Hampton's brigade, blocking Stuart from achieving his objectives in the U.S. rear. Meanwhile, after hearing news of the day's victory, Brig. Gen. Judson Kilpatrick launched a cavalry attack against the infantry positions of Longstreet's Corps southwest of Big Round Top. Brig. Gen. Elon J. Farnsworth protested against the futility of such a move, but obeyed orders. Farnsworth was killed in the attack, and his brigade suffered significant losses.
Aftermath.
Casualties.
The two armies suffered between 46,000 and 51,000 casualties. Union casualties were 23,055 (3,155 killed, 14,531 wounded, 5,369 captured or missing), while Confederate casualties are more difficult to estimate. Many authors have referred to as many as "28,000 Confederate casualties", and Busey and Martin's more recent 2005 work, "Regimental Strengths and Losses at Gettysburg", documents 23,231 (4,708 killed, 12,693 wounded, 5,830 captured or missing). Nearly a third of Lee's general officers were killed, wounded, or captured. The casualties for both sides during the entire campaign were 57,225.
The following tables summarize casualties by corps for the Union and Confederate forces during the three-day battle.
Bruce Catton wrote, "The town of Gettysburg looked as if some universal moving day had been interrupted by catastrophe." But there was only one documented civilian death during the battle: Ginnie Wade (also widely known as Jennie), 20 years old, was hit by a stray bullet that passed through her kitchen in town while she was making bread. Nearly 8,000 had been killed outright; these bodies, lying in the hot summer sun, needed to be buried quickly. Over 3,000 horse carcasses were burned in a series of piles south of town; townsfolk became violently ill from the stench.
Confederate retreat.
The armies stared at one another in a heavy rain across the bloody fields on July 4, the same day that the Vicksburg garrison surrendered to Maj. Gen. Ulysses S. Grant. Lee had reformed his lines into a defensive position on Seminary Ridge the night of July 3, evacuating the town of Gettysburg. The Confederates remained on the battlefield, hoping that Meade would attack, but the cautious Union commander decided against the risk, a decision for which he would later be criticized. Both armies began to collect their remaining wounded and bury some of the dead. A proposal by Lee for a prisoner exchange was rejected by Meade.
Lee started his Army of Northern Virginia in motion late the evening of July 4 towards Fairfield and Chambersburg. Cavalry under Brig. Gen. John D. Imboden was entrusted to escort the miles-long wagon train of supplies and wounded men that Lee wanted to take back to Virginia with him, using the route through Cashtown and Hagerstown to Williamsport, Maryland. Meade's army followed, although the pursuit was half-spirited. The recently rain-swollen Potomac trapped Lee's army on the north bank of the river for a time, but when the Union troops finally caught up, the Confederates had forded the river. The rear-guard action at Falling Waters on July 14 added some more names to the long casualty lists, including General Pettigrew, who was mortally wounded.
In a brief letter to Maj. Gen. Henry W. Halleck written on July 7, Lincoln remarked on the two major Union victories at Gettysburg and Vicksburg. He continued:
Halleck then relayed the contents of Lincoln's letter to Meade in a telegram. Despite repeated pleas from Lincoln and Halleck, which continued over the next week, Meade did not pursue Lee's army aggressively enough to destroy it before it crossed back over the Potomac River to safety in the South. The campaign continued into Virginia with light engagements until July 23, in the minor Battle of Manassas Gap, after which Meade abandoned any attempts at pursuit and the two armies took up positions across from each other on the Rappahannock River.
Union reaction to the news of the victory.
The news of the Union victory electrified the North. A headline in "The Philadelphia Inquirer" proclaimed "VICTORY! WATERLOO ECLIPSED!" New York diarist George Templeton Strong wrote:
However, the Union enthusiasm soon dissipated as the public realized that Lee's army had escaped destruction and the war would continue. Lincoln complained to Secretary of the Navy Gideon Welles that "Our army held the war in the hollow of their hand and they would not close it!" Brig. Gen. Alexander S. Webb wrote to his father on July 17, stating that such Washington politicians as "Chase, Seward and others," disgusted with Meade, "write to me that Lee really won that Battle!"
Effect on the Confederacy.
In fact, the Confederates had lost militarily and also politically. During the final hours of the battle, Confederate Vice President Alexander Stephens was approaching the Union lines at Norfolk, Virginia, under a flag of truce. Although his formal instructions from Confederate President Jefferson Davis had limited his powers to negotiate on prisoner exchanges and other procedural matters, historian James M. McPherson speculates that he had informal goals of presenting peace overtures. Davis had hoped that Stephens would reach Washington from the south while Lee's victorious army was marching toward it from the north. President Lincoln, upon hearing of the Gettysburg results, refused Stephens's request to pass through the lines. Furthermore, when the news reached London, any lingering hopes of European recognition of the Confederacy were finally abandoned. Henry Adams wrote, "The disasters of the rebels are unredeemed by even any hope of success. It is now conceded that all idea of intervention is at an end."
The immediate reaction of the Southern military and public sectors was that Gettysburg was a setback, not a disaster. The sentiment was that Lee had been successful on July 1 and had fought a valiant battle on July 2–3, but could not dislodge the Union Army from the strong defensive position to which it fled. The Confederates successfully stood their ground on July 4 and withdrew only after they realized Meade would not attack them. The withdrawal to the Potomac that could have been a disaster was handled masterfully. Furthermore, the Army of the Potomac had been kept away from Virginia farmlands for the summer and all predicted that Meade would be too timid to threaten them for the rest of the year. Lee himself had a positive view of the campaign, writing to his wife that the army had returned "rather sooner than I had originally contemplated, but having accomplished what I proposed on leaving the Rappahannock, viz., relieving the Valley of the presence of the enemy and drawing his Army north of the Potomac." He was quoted as saying to Maj. John Seddon, brother of the Confederate secretary of war, "Sir, we did whip them at Gettysburg, and it will be seen for the next six months that "that army" will be as quiet as a sucking dove." Some Southern publications, such as the "Charleston Mercury", criticized Lee's actions in the campaign and on August 8 he offered his resignation to President Davis, who quickly rejected it.
Gettysburg became a postbellum focus of the "Lost Cause", a movement by writers such as Edward A. Pollard and Jubal Early to explain the reasons for the Confederate defeat in the war. A fundamental premise of their argument was that the South was doomed because of the overwhelming advantage in manpower and industrial might possessed by the North. However, they claim it also suffered because Robert E. Lee, who up until this time had been almost invincible, was betrayed by the failures of some of his key subordinates at Gettysburg: Ewell, for failing to seize Cemetery Hill on July 1; Stuart, for depriving the army of cavalry intelligence for a key part of the campaign; and especially Longstreet, for failing to attack on July 2 as early and as forcefully as Lee had originally intended. In this view, Gettysburg was seen as a great lost opportunity, in which a decisive victory by Lee could have meant the end of the war in the Confederacy's favor.
Gettysburg Address.
The ravages of war were still evident in Gettysburg more than four months later when, on November 19, the Soldiers' National Cemetery was dedicated. During this ceremony, President Abraham Lincoln honored the fallen and redefined the purpose of the war in his historic Gettysburg Address.
Historical assessment.
Decisive victory controversies.
The nature of the result of the Battle of Gettysburg has been the subject of controversy for years. Although not seen as overwhelmingly significant at the time, particularly since the war continued for almost two years, in retrospect it has often been cited as the "turning point", usually in combination with the fall of Vicksburg the following day. This is based on the observation that after Gettysburg Lee's army conducted no more strategic offensives—his army merely reacted to the initiative of Ulysses S. Grant in 1864 and 1865—and by the speculative viewpoint of the Lost Cause writers that a Confederate victory at Gettysburg might have resulted in the end of the war.
It is currently a widely held view that Gettysburg was a decisive victory for the Union, but the term is considered imprecise. It is inarguable that Lee's offensive on July 3 was turned back decisively and his campaign in Pennsylvania was terminated prematurely (although the Confederates at the time argued that this was a temporary setback and that the goals of the campaign were largely met). However, when the more common definition of "decisive victory" is intended—an indisputable military victory of a battle that determines or significantly influences the ultimate result of a conflict—historians are divided. For example, David J. Eicher called Gettysburg a "strategic loss for the Confederacy" and James M. McPherson wrote that "Lee and his men would go on to earn further laurels. But they never again possessed the power and reputation they carried into Pennsylvania those palmy summer days of 1863."
However, Herman Hattaway and Archer Jones wrote that the "strategic impact of the Battle of Gettysburg was ... fairly limited." Steven E. Woodworth wrote that "Gettysburg proved only the near impossibility of decisive action in the Eastern theater." Edwin Coddington pointed out the heavy toll on the Army of the Potomac and that "after the battle Meade no longer possessed a truly effective instrument for the accomplishments of his task. The army needed a thorough reorganization with new commanders and fresh troops, but these changes were not made until Grant appeared on the scene in March 1864." Joseph T. Glatthaar wrote that "Lost opportunities and near successes plagued the Army of Northern Virginia during its Northern invasion," yet after Gettysburg, "without the distractions of duty as an invading force, without the breakdown of discipline, the Army of Northern Virginia an extremely formidable force." Ed Bearss wrote, "Lee's invasion of the North had been a costly failure. Nevertheless, at best the Army of the Potomac had simply preserved the strategic stalemate in the Eastern Theater ..."
Peter Carmichael refers to the military context for the armies, the "horrendous losses at Chancellorsville and Gettysburg, which effectively destroyed Lee's offensive capacity," implying that these cumulative losses were not the result of a single battle. Thomas Goss, writing in the U.S. Army's "Military Review" journal on the definition of "decisive" and the application of that description to Gettysburg, concludes: "For all that was decided and accomplished, the Battle of Gettysburg fails to earn the label 'decisive battle'." The military historian John Keegan agrees. Gettysburg was a landmark battle, the largest of the war and it would not be surpassed. The Union had restored to it the belief in certain victory, and the loss dispirited the Confederacy. If "not exactly a decisive battle", Gettysburg was the end of Confederate use of Northern Virginia as a military buffer zone, the setting for Grant's Overland Campaign.
Lee vs. Meade.
Prior to Gettysburg, Robert E. Lee had established a reputation as an almost invincible general, achieving stunning victories against superior numbers—although usually at the cost of high casualties to his army—during the Seven Days, the Northern Virginia Campaign (including the Second Battle of Bull Run), Fredericksburg, and Chancellorsville. Only the Maryland Campaign, with its tactically inconclusive Battle of Antietam, had been less than successful. Therefore, historians have attempted to explain how Lee's winning streak was interrupted so dramatically at Gettysburg. Although the issue is tainted by attempts to portray history and Lee's reputation in a manner supporting different partisan goals, the major factors in Lee's loss arguably can be attributed to: (1) his overconfidence in the invincibility of his men; (2) the performance of his subordinates, and his management thereof; (3) his failing health, and (4) the performance of his opponent, George G. Meade, and the Army of the Potomac.
Throughout the campaign, Lee was influenced by the belief that his men were invincible; most of Lee's experiences with the Army of Northern Virginia had convinced him of this, including the great victory at Chancellorsville in early May and the rout of the Union troops at Gettysburg on July 1. Since morale plays an important role in military victory when other factors are equal, Lee did not want to dampen his army's desire to fight and resisted suggestions, principally by Longstreet, to withdraw from the recently captured Gettysburg to select a ground more favorable to his army. War correspondent Peter W. Alexander wrote that Lee "acted, probably, under the impression that his troops were able to carry any position however formidable. If such was the case, he committed an error, such however as the ablest commanders will sometimes fall into." Lee himself concurred with this judgment, writing to President Davis, "No blame can be attached to the army for its failure to accomplish what was projected by me, nor should it be censured for the unreasonable expectations of the public—I am alone to blame, in perhaps expecting too much of its prowess and valor."
The most controversial assessments of the battle involve the performance of Lee's subordinates. The dominant theme of the Lost Cause writers and many other historians is that Lee's senior generals failed him in crucial ways, directly causing the loss of the battle; the alternative viewpoint is that Lee did not manage his subordinates adequately, and did not thereby compensate for their shortcomings. Two of his corps commanders—Richard S. Ewell and A.P. Hill—had only recently been promoted and were not fully accustomed to Lee's style of command, in which he provided only general objectives and guidance to their former commander, Stonewall Jackson; Jackson translated these into detailed, specific orders to his division commanders. All four of Lee's principal commanders received criticism during the campaign and battle:
In addition to Hill's illness, Lee's performance was affected by heart troubles, which would eventually lead to his death in 1870; he had been diagnosed with pericarditis by his staff physicians in March 1863, though modern doctors believe he had in fact suffered a heart attack. He wrote to Jefferson Davis that his physical condition prevented him from offering full supervision in the field, and said, "I am so dull that in making use of the eyes of others I am frequently misled."
As a final factor, Lee faced a new and formidable opponent in George G. Meade, and the Army of the Potomac fought well on its home territory. Although new to his army command, Meade deployed his forces relatively effectively; relied on strong subordinates such as Winfield S. Hancock to make decisions where and when they were needed; took great advantage of defensive positions; nimbly shifted defensive resources on interior lines to parry strong threats; and, unlike some of his predecessors, stood his ground throughout the battle in the face of fierce Confederate attacks. Lee was quoted before the battle as saying Meade "would commit no blunders on my front and if I make one ... will make haste to take advantage of it." That prediction proved to be correct at Gettysburg. Stephen Sears wrote, "The fact of the matter is that George G. Meade, unexpectedly and against all odds, thoroughly outgeneraled Robert E. Lee at Gettysburg." Edwin B. Coddington wrote that the soldiers of the Army of the Potomac received a "sense of triumph which grew into an imperishable faith in . The men knew what they could do under an extremely competent general; one of lesser ability and courage could well have lost the battle."
Meade had his own detractors as well. Similar to the situation with Lee, Meade suffered partisan attacks about his performance at Gettysburg, but he had the misfortune of experiencing them in person. Supporters of his predecessor, Maj. Gen. Joseph Hooker, lambasted Meade before the U.S. Congress's Joint Committee on the Conduct of the War, where Radical Republicans suspected that Meade was a Copperhead and tried in vain to relieve him from command. Daniel E. Sickles and Daniel Butterfield accused Meade of planning to retreat from Gettysburg during the battle. Most politicians, including Lincoln, criticized Meade for what they considered to be his half-hearted pursuit of Lee after the battle. A number of Meade's most competent subordinates—Winfield S. Hancock, John Gibbon, Gouverneur K. Warren, and Henry J. Hunt, all heroes of the battle—defended Meade in print, but Meade was embittered by the overall experience.
Battlefield preservation.
Today, the Gettysburg National Cemetery and Gettysburg National Military Park are maintained by the U.S. National Park Service as two of the nation's most revered historical landmarks. Although Gettysburg is one of the best known of all Civil War battlefields, it too faces threats to its preservation and interpretation. Many historically significant locations on the battlefield lie outside the boundaries of Gettysburg National Military Park and are vulnerable to residential or commercial development.
On July 20, 2009, a Comfort Inn and Suites opened on Cemetery Hill, adjacent to Evergreen Cemetery, just one of many modern edifices infringing on the historic field. The Baltimore Pike corridor attracts development that concerns preservationists.
Some preservation successes have emerged in recent years. Two proposals to open a casino at Gettysburg were defeated in 2006 and most recently in 2011, when public pressure forced the Pennsylvania Gaming Control Board to reject the proposed gambling hub at the intersection of Routes 15 and 30, near East Cavalry Field. The Civil War Trust also successfully purchased and transferred 95 acres at the former site of the Gettysburg Country Club to the control of the U.S. Department of the Interior in 2011.
Less than half of the over 11,500 acres on the old Gettysburg Battlefield have been preserved for posterity thus far. The Civil War Trust has preserved 815 acres around the site, some of which is now part of the 4,998 acres of Gettysburg National Military Park.
Commemoration in U.S. postage and coinage.
During the Civil War Centennial, the U.S. Post Office issued five postage stamps commemorating the 100th anniversaries of famous battles, as they occurred over a four-year period, beginning with the Battle of Fort Sumter Centennial issue of 1961. The Battle of Shiloh commemorative stamp was issued in 1962, the Battle of Gettysburg in 1963, the Battle of the Wilderness in 1964, and the Appomattox Centennial commemorative stamp in 1965.
A commemorative half dollar for the battle was produced in 1936. As was typical for the period, mintage was very low, just 26,928. On January 24, 2011, the America the Beautiful quarters released a 25-cent coin commemorating Gettysburg National Military Park and the Battle of Gettysburg. The reverse side of the coin depicts the monument on Cemetery Ridge to the 72nd Pennsylvania Infantry.
In popular culture.
Film records survive of two Gettysburg reunions, held on the battlefield. At the 50th anniversary (1913), veterans re-enacted Pickett's Charge in a spirit of reconciliation, a meeting that carried great emotional force for both sides. At the 75th anniversary (1938), 2500 veterans attended, and there was a ceremonial mass hand-shake across a stone wall. This was recorded on sound film, and some Confederates can be heard giving the Rebel Yell.
Iced Earth's three-part song cycle "Gettysburg", published in 2004, dramatizes the battle.
The Battle of Gettysburg was depicted in the 1993 film "Gettysburg", based on Michael Shaara's 1974 novel "The Killer Angels". The film and novel focused primarily on the actions of Joshua Lawrence Chamberlain, John Buford, Robert E. Lee, and James Longstreet during the battle. The first day focused on Buford's cavalry defense, the second day on Chamberlain's defense at Little Round Top, and the third day on Pickett's Charge.
The south winning the Battle of Gettysburg is a popular premise for a point of divergence in American Civil War alternate histories. Here are some examples which either depict or make significant reference to an alternate Battle of Gettysburg (sometimes simply inserting fantasy or sci-fi elements in an account of the battle):

</doc>
<doc id="4851" url="https://en.wikipedia.org/wiki?curid=4851" title="Budweiser">
Budweiser

Budweiser () is an American-style pale lager produced by Belgian multinational corporation Anheuser–Busch InBev.
Introduced in 1876 by Carl Conrad & Co. of St. Louis, Missouri, it has grown to become one of the highest selling beers in the United States, and is available in over 80 markets worldwidethough, due to a trademark dispute, cannot necessarily do so under the Budweiser name. It is made with up to 30% rice in addition to hops and barley malt. Produced in various breweries around the world, Budweiser is a filtered beer available in draught and packaged forms.
History.
Adolphus Busch left Germany for the United States in 1857. He settled in St. Louis, Missouri, where he eventually established his own brewing supply house. In St. Louis, Busch also met and married Lilly Anheuser. Lilly’s father, Eberhard Anheuser, owned a small brewery that had been making lager beer for some time. In 1864 Busch partnered with his father in-law to form what would eventually become the Anheuser-Busch Company.
In the mid-1800s most Americans preferred robust, dark ales. Busch traveled extensively in Europe to observe and study the latest brewing techniques for light lagers, learning the desired formula in České Budějovice in Southern Bohemia, which he introduced under the brand name Budweiser.
In the 1870s Anheuser-Busch became the first American brewery to implement pasteurization, which greatly improved the shelf-life and transportability of its beers.
In 1913-1933 August Anheuser Busch, Sr. (1865-1934) became president of Anheuser-Busch. The onset of Prohibition in 1920 made them have to retool their line of products to survive, selling a low alcohol "near beer" made of brewer's yeast, malt extract, and other raw materials that were permitted for home brewers.
When Prohibition ended in 1933, Anheuser-Busch began brewing Budweiser again, introducing the Budweiser Clydesdales to the public on April 7, 1933. August A. Busch, Sr.'s son Adolphus Busch III (1891-1946) became president of Anheuser-Busch in 1933-1946. During Prohibition the palate of the beer consumer had changed due to the popularity of sweeter homemade and bootlegged brews, so the company dared consumers to drink Budweiser for five days, and if on the sixth day they still preferred the taste of other beers, they could go back, which often worked. Growth was limited by economic conditions in the Great Depression, but thanks in part to the introduction of the metal can in 1936, Budweiser’s sales began to climb again.
During World War Two the company diverted several resources to support the war effort and relinquished its West Coast markets to conserve rail car space. After the war, Anheuser-Busch entered into an era of rapid growth.
In 1946-1975 Adolphus Busch III's brother August Anheuser "Gussie" Busch Jr. (1899-1989) became CEO, beginning the creation of a national network of breweries. A new brewery was opened in Newark, New Jersey in 1951, and was the first of nine to open over the next 25 years. In 1957 Budweiser became the top-selling beer in the U.S.
Budweiser is available in over 80 markets.
After November 18, 2008 InBev takeover, several cost-cutting measures that were implemented have, according to some sources, negatively affected the flavor of the beer. Whole rice grains have now been replaced by broken ones, and the high quality Hallertauer Mittelfrüh hop has been phased out. A former top AB InBev executive says the company saved about $55 million a year substituting cheaper hops in Budweiser and other U.S. beers. The famous Budweiser sign on Chicago's Kennedy Expressway has been changing since 1998 as the original "Bow-Tie" type logo.
Name origin and dispute.
Anheuser–Busch has been involved in a trademark dispute with European beer companies, in particular Budweiser Budvar Brewery, over the trademark rights to the name "Budweiser".
Beer has been brewed in Budweis since it was founded by king Ottokar II of Bohemia in 1245. The name Budweiser is a derivative adjective, meaning "of Budweis". In 1876, Adolphus Busch and his friend Carl Conrad, a liquor importer, developed a "Bohemian-style" lager, inspired after a trip to the region.
In the European Union, excluding the United Kingdom, Ireland, Finland and Spain, the American beer is marketed as "Bud", as the Budweiser trademark name is owned solely by the Czech beer maker, Budweiser Budvar.
Anheuser-Busch has a market share in the United States of 50.9% for all beers sold. This is primarily composed of Budweiser brands. In 2008 the Belgian-Brazilian beer giant InBev bought the majority of Anheuser-Busch stocks at $70 per share in an all-cash agreement, to create the largest brewing company in the world.
Marketing.
The Budweiser from Budějovice has been called "The Beer of Kings" since the 16th century. Adolphus Busch changed this slogan to "The King Of Beers." The Czech Budweiser is sold in some countries as "Budejovicky Budvar" but is known as Budweiser in many other countries throughout Europe.
Anheuser-Busch is known for its sport sponsorship, videogame sponsorship ("Tapper," 1983), and humorous advertisements, some of which have entered the popular culture in the United States. They include a long line of TV advertisements in the 1990s featuring three frogs named "Bud," "Weis," and "Er;" lizards impersonating the "Bud-weis-er" frogs, the Budweiser Ants;, a campaign built around the phrase "Whassup?", and a team of Clydesdale horses commonly known as "The Budweiser Clydesdales."
The Budweiser Clydesdale has become an iconic horse. In the "Clydesdale Donkey" commercial the donkey dreams of becoming a Clydesdale horse. The donkey does everything he can think of to try and become one of these horses, and finally when the day comes for his audition, he makes the cut and becomes what he has always dreamed. This commercial ended up being in the top five for the most memorable Budweiser commercials.
The Budweiser brand is promoted in motorsports, from Bernie Little's Miss Budweiser hydroplane boat to sponsorship of the Budweiser King Top Fuel Dragster driven by Brandon Bernstein. Anheuser-Busch has sponsored the CART championship. It is the "Official Beer of NHRA" and it was the "Official Beer of NASCAR" from 1998 to 2007. It has sponsored motorsport events such as the NASCAR Busch Series, Budweiser Speedweeks, Budweiser Shootout, Budweiser Duel, Budweiser Pole Award, Budweiser 500, Budweiser 400, Budweiser 300, Budweiser 250, Budweiser 200, and Carolina Pride / Budweiser 200.
Budweiser has been sponsor of NASCAR teams such as Junior Johnson, Hendrick Motorsports, DEI, and Stewart-Haas Racing. Drivers who have sponsored Budweiser include Terry Labonte (1983, 1987–89), Neil Bonnett (1984-1985), Darrell Waltrip (1984-1986), Geoffrey Bodine (1990-1991), Bill Elliott (1992-1994), Ken Schrader (1995-1996); Dale Earnhardt, Jr. (1999-2007); Kasey Kahne (2008-2010), and Kevin Harvick (2011–2015). In IndyCar, Budweiser sponsored Mario Andretti (1983-1984), Bobby Rahal (1985-1988), Scott Pruett (1989-1992), Roberto Guerrero (1993), Scott Goodyear (1994), Paul Tracy (1995), Christian Fittipaldi (1996-1997), and Richie Hearn (1998-1999). Andretti was the 1984 IndyCar champion, whereas Rahal won the 1987 championship and the 1986 Indianapolis 500.
Anheuser-Busch also sponsored many races under the Budweiser brand, including the Budweiser Shootout, the Bud at the Glen, Budweiser 500, and Budweiser 400. However, starting in 2016, the focus of A-B's NASCAR sponsorship became its Busch Beer brand.
Anheuser-Busch has placed Budweiser as an official partner and sponsor of Major League Soccer and Los Angeles Galaxy and was the headline sponsor of the British Basketball League in the 1990s, taking over from rival company Carlsberg. Anheuser-Busch has also placed Budweiser as an official sponsor of the Premier League and the presenting sponsor of the FA Cup. Budweiser was a proud sponsor and avid advertiser during the celebration of the 1996 Atlanta Olympic Games. For the Centennial Summer Olympics, Budweiser produced many commemorative collectibles and souvenirs that were very popular among crowds at the Games. A primary Budweiser collectible was a 28-ounce Budweiser Atlanta Olympic Games Beer Stein, which featured the Olympic Flame as a decoration on the stein's handle.
In the early 20th century, the company commissioned a play-on-words song called "Under the Anheuser Bush," which was recorded by several early phonograph companies. Popular music continues to be used in advertisements for Budweiser. Some commercials feature the song "Galvanize" (2005), by The Chemical Brothers.
In 2009, Anheuser-Busch partnered with popular Chinese video-sharing site, Tudou.com for a user-generated online video contest. The contest encourages users to suggest ideas that include ants for a Bud TV spot set to run in February 2010 during the Chinese New Year.
In 2010, Budweiser launched an international entertainment property called Bud United. Bud United’s first efforts were centered around the 2010 FIFA World Cup in South Africa. The brand launched an online reality TV series, called "Bud House," that followed the lives of 32 international football fans (one representing each nation in the World Cup) living together in a house in South Africa. Bud United’s next project is another reality series called "The Big Time;" each episode focuses on a different vertical market (baseball, soccer, cooking, etc...) and features contestants competing for a chance to live their dreams. The show was cast through the Bud United Facebook page and aired in Q1 2012.
On November 5, 2012, Anheuser-Busch asked Paramount Pictures to obscure or remove the Budweiser logo from the film "Flight" (2012), directed by Robert Zemeckis and starring Denzel Washington.
In an advertisement titled "Brewed the Hard Way" that aired during Super Bowl XLIX, large on-screen text stated that Budweiser is "Proudly A Macro Beer" - distinguishing it from smaller production craft beers.
In 2016, Beer Park by Budweiser opened on the Las Vegas Strip.
Containers and packaging.
Containers.
Over the years, Budweiser has been distributed in many sizes and containers. Until the early 1950s Budweiser was primarily distributed in three packages: kegs, 12-ounce bottles and quart bottles. Cans were first introduced in 1936, which helped sales to climb. In 1955 August Busch Jr. made a strategic move to expand Budweiser's national brand and distributor presence. Along with this expansion came advances in bottling automation, new bottling materials and more efficient distribution methods. These advances brought to market many new containers and package designs. Budweiser is distributed in four large container volumes: half-barrel (15.5 US gallons), quarter-barrel, 1/6 barrel and beer balls (5.2 gallons); and in smaller 7, 8, 10, 12, 16, 18, 22, 24, 32 and 40 US ounce containers. Smaller containers may be made of glass, aluminum or plastic. On August 3, 2011, Anheuser-Busch announced its twelfth can design since 1936, one which emphasizes the bowtie.
Packages are sometimes tailored to local customs and traditions. In St. Mary's County, Maryland, ten ounce cans are the preferred package. Budweiser drinkers in the western stretches of Ottawa County, Michigan prefer the eight ounce can. This Ottawa County preference for the eight ounce can may stem from a long-standing blue law held in many Western Michigan cities that prohibit sale of beer and wine on Sundays. In response to this blue law, brewers and distributors presented the eight ounce can as a smaller alternative.
Anheuser-Busch has introduced many can designs with co-branding and sports marketing promotional packaging. Today, most of these promotional programs are represented only on the 16 ounce aluminum bottle container. However, many MLB and NFL teams in the United States also promote 24 ounce cans marked with team logos.
Bottle.
The Budweiser bottle has remained relatively unchanged since its introduction in 1876. The top label is red and currently reads "Budweiser". The top of the main label is red with a white banner with a pledge on it, which has changed three times. Below the banner is a coat of arms of sorts, which features an Anheuser-Busch stylization. Below that is a large white box.
Cans.
During the Prohibition Era, Budweiser encountered its first major obstacle to profit and growth. As alcohol became illegal to sell and produce, all alcohol companies, including Budweiser, struggled to remain profitable. Budweiser began producing non-alcoholic beverages during Prohibition to counter its ill effects. Prohibition began in 1920, and lasted into the middle of the Great Depression in 1933. These are two major setbacks that this company experienced. Just as laws regarding prohibition were repealed, Budweiser was faced with more serious economic struggles, which made their success (like all other companies) very unlikely. In attempt to re-stimulate interest in their beer, Budweiser executed a hugely successful marketing strategy of introducing beer cans for the first time in 1936. This new packaging led to an increase in sales which lasted until the start of World War II in 1939.
Over the years, Budweiser has undergone various design changes to its can which are discussed in the table below. Many of these changes are in response to market conditions and consumer tastes. Since 1936, 12 major can design changes have occurred, not including the temporary special edition designs. The table below describes each of the 12 major can designs.
2011 can design marketing strategy.
Traditional Budweiser cans embodied a strong sense of American patriotism. They were red, white, and blue in color, and had images of eagles near the label. These cans also had a very classic appearance due to the cursive font and the Anheuser-Busch logo surrounded by wheat and barley. Red, white, and blue, being the colors of the American flag, identified the cans strongly with the American market for this product. Moreover, the eagles remind viewers of the bald eagle, America's national bird. These facts strongly suggest that Budweiser was targeting a primarily American market. In contrast to this previous design, the newest can takes on a much more modern appearance and eliminates blue as a main color in the design. Red, white, and gold are the predominant colors on this new can, which makes the can look much less traditionally American, and much more modern. On this new can, there are aspects that continue to resonate with an American market, such as the red and white colors, as these are two of the nation's primary colors. The presence of the yellowish-gold color is not typically American, though, and is surprising to see in the can design of the self-proclaimed "Great American Lager."
This change in can design was very shocking for a brand that had remained relatively static since its beginning. The new design was largely in response to the huge sales decline in recent years that is threatening to lose Budweiser its title as the best-selling beer in America. One cause of this sales decline is the unemployment struggles that many Americans have been facing recently. A large group of individuals affected by unemployment are young men, who are the main demographic of Budweiser's target market. In order to regain the domestic market share that Budweiser has lost, the company is trying to update its appearance by giving the can a more contemporary look that is more appealing to this demographic. The company hopes that the new design will offset the effects that unemployment had on its sales.
Although the more modern design is intended for young male Americans, the new design, according to the vice president of Anheuser-Busch Frank Abernante, "is one of many steps in our quest to reinforce Budweiser's role as a truly global beer brand." This statement means that the new design was intended for foreign markets as well. In fact, Budweiser began selling its beer in Russia in 2010, and is currently expanding its operations in China. However, it has not released the new 2011 design can in China as of 2015.
Statistics show that China is the world's leading consumer of beer in terms of volume, which suggests that Budweiser is trying to capitalize on this market's potential in order to recover from these losses. Currently, of the 15 Anheuser-Busch breweries outside of the United States, 14 of them are positioned in China, which shows how interested the company is in this growing market. Budweiser is already the fourth leading brand in the Chinese beer market, and the company hopes that this new design change will push them further up the list as one of China's most dominant brands. Budweiser's attempt to target China is not sudden, though. There have actually been several advertising campaigns geared toward Chinese consumers. One such campaign was in 2009 when Budweiser sponsored a competition for the creation of a Budweiser commercial involving the Budweiser ants. Here, Chinese consumers were prompted to compete in this process of submitting their ideas online for the chance to have their idea become the newest Budweiser commercial.
The beer.
Budweiser is brewed using barley malt, rice, water, hops and yeast. It is lagered with beechwood chips in the aging vessel which, according to Anheuser-Busch, creates a smoother taste. While beechwood chips are used in the maturation tank, there is little to no flavor contribution from the wood, mainly because they are boiled in sodium bicarbonate soda for seven hours for the very purpose of removing any flavor from the wood. The maturation tanks that Anheuser-Busch uses are horizontal and, as such, flocculation of the yeast occurs much more quickly. Anheuser-Busch refers to this process as a secondary fermentation, with the idea being that the chips give the yeast more surface area to rest on. This is also combined with a krausening procedure that re-introduces wort into the chip tank, therefore reactivating the fermentation process. By placing the beechwood chips at the bottom of the tank, the yeast remains in suspension longer, giving it more time to reabsorb and process green beer flavors, such as acetaldehyde and diacetyl, that Anheuser-Busch believes are off-flavors which detract from overall drinkability.
Some drinkers prefer the lightness of beers like Budweiser and consume it as a refreshment or for its inebriating effects. Several beer writers consider it to be bland. The beer is light-bodied with faint sweet notes and negligible bitterness, leading to reviews characterizing it as a "...beer of underwhelming blandness." Even Adolphus Busch disliked the beer he marketed in the United States. But based upon sales alone, it became the second most popular American brewed pale lager among North American beer consumers.
Budweiser and "Bud Light" are sometimes advertised as vegan beers, in that their ingredients and conditioning do not use animal by-products. Some may object to the inclusion of genetically engineered rice and animal products used in the brewing process. In July 2006, Anheuser-Busch brewed a version of Budweiser with organic rice, for sale in Mexico. It has yet to extend this practice to any other countries.
Anheuser-Busch was one of the few breweries during Prohibition that had the resources and wherewithal to convert to "cereal beer" production—malt beverage made with non-fermentables such as rice and unmalted barley and rye, and able to stay under the 0.5% limit established by the Volstead Act. Following the repeal of Prohibition in 1933, the major breweries continued to use unmalted cereal grains to provide the full body and mouthfeel of a "real" beer while keeping the alcohol content low.
Budweiser brands.
In addition to the regular Budweiser, Anheuser-Busch brews several different beers under the Budweiser brand, including "Bud Light" and "Bud Ice".
In July 2010 Anheuser-Busch launched Budweiser 66 in the United Kingdom. Budweiser Brew No.66 has 4% alcohol by volume, and is brewed and distributed in the UK by Inbev UK Limited.

</doc>
<doc id="4854" url="https://en.wikipedia.org/wiki?curid=4854" title="Bermuda Triangle">
Bermuda Triangle

The Bermuda Triangle, also known as the Devil's Triangle, is a loosely defined region in the western part of the North Atlantic Ocean, where a number of aircraft and ships are said to have disappeared under mysterious circumstances. According to the US Navy, the triangle does not exist, and the name is not recognized by the US Board on Geographic Names. Popular culture has attributed various disappearances to the paranormal or activity by extraterrestrial beings. Documented evidence indicates that a significant percentage of the incidents were spurious, inaccurately reported, or embellished by later authors.
Triangle area.
The first written boundaries date from an article by Vincent Gaddis in a 1964 issue of the pulp magazine "Argosy", where the triangle's three vertices are in Miami, Florida peninsula; in San Juan, Puerto Rico; and in the mid-Atlantic island of Bermuda. But subsequent writers did not follow this definition. Some writers give different boundaries and vertices to the triangle, with the total area varying from . Consequently, the determination of which accidents have occurred inside the triangle depends on which writer reports them. The United States Board on Geographic Names does not recognize this name, and it is not delimited in any map drawn by US government agencies.
The area is one of the most heavily traveled shipping lanes in the world, with ships crossing through it daily for ports in the Americas, Europe, and the Caribbean Islands. Cruise ships are also plentiful, and pleasure craft regularly go back and forth between Florida and the islands. It is also a heavily flown route for commercial and private aircraft heading towards Florida, the Caribbean, and South America from points north.
Origins.
The earliest allegation of unusual disappearances in the Bermuda area appeared in a September 17, 1950 article published in "The Miami Herald" (Associated Press) by Edward Van Winkle Jones. Two years later, "Fate" magazine published "Sea Mystery at Our Back Door", a short article by George X. Sand covering the loss of several planes and ships, including the loss of Flight 19, a group of five U.S. Navy TBM Avenger bombers on a training mission. Sand's article was the first to lay out the now-familiar triangular area where the losses took place. Flight 19 alone would be covered again in the April 1962 issue of "American Legion" magazine. In it, author Allan W. Eckert wrote that the flight leader had been heard saying, "We are entering white water, nothing seems right. We don't know where we are, the water is green, no white." He also wrote that officials at the Navy board of inquiry stated that the planes "flew off to Mars." Sand's article was the first to suggest a supernatural element to the Flight 19 incident. In the February 1964 issue of "Argosy", Vincent Gaddis' article "The Deadly Bermuda Triangle" argued that Flight 19 and other disappearances were part of a pattern of strange events in the region. The next year, Gaddis expanded this article into a book, "Invisible Horizons".
Others would follow with their own works, elaborating on Gaddis' ideas: John Wallace Spencer ("Limbo of the Lost", 1969, repr. 1973); Charles Berlitz ("The Bermuda Triangle", 1974); Richard Winer ("The Devil's Triangle", 1974), and many others, all keeping to some of the same supernatural elements outlined by Eckert.
Criticism of the concept.
Larry Kusche.
Lawrence David Kusche, a research librarian from Arizona State University and author of "The Bermuda Triangle Mystery: Solved" (1975) argued that many claims of Gaddis and subsequent writers were often exaggerated, dubious or unverifiable. Kusche's research revealed a number of inaccuracies and inconsistencies between Berlitz's accounts and statements from eyewitnesses, participants, and others involved in the initial incidents. Kusche noted cases where pertinent information went unreported, such as the disappearance of round-the-world yachtsman Donald Crowhurst, which Berlitz had presented as a mystery, despite clear evidence to the contrary. Another example was the ore-carrier recounted by Berlitz as lost without trace three days out of an "Atlantic" port when it had been lost three days out of a port with the same name in the "Pacific" Ocean. Kusche also argued that a large percentage of the incidents that sparked allegations of the Triangle's mysterious influence actually occurred well outside it. Often his research was simple: he would review period newspapers of the dates of reported incidents and find reports on possibly relevant events like unusual weather, that were never mentioned in the disappearance stories.
Kusche concluded that:
Further responses.
When the UK Channel 4 television program "The Bermuda Triangle" (1992) was being produced by John Simmons of Geofilms for the "Equinox" series, the marine insurance market Lloyd's of London was asked if an unusually large number of ships had sunk in the Bermuda Triangle area. Lloyd's determined that large numbers of ships had not sunk there. Lloyd's does not charge higher rates for passing through this area. United States Coast Guard records confirm their conclusion. In fact, the number of supposed disappearances is relatively insignificant considering the number of ships and aircraft that pass through on a regular basis.
The Coast Guard is also officially skeptical of the Triangle, noting that they collect and publish, through their inquiries, much documentation contradicting many of the incidents written about by the Triangle authors. In one such incident involving the 1972 explosion and sinking of the tanker , the Coast Guard photographed the wreck and recovered several bodies, in contrast with one Triangle author's claim that all the bodies had vanished, with the exception of the captain, who was found sitting in his cabin at his desk, clutching a coffee cup. In addition, "V. A. Fogg" sank off the coast of Texas, nowhere near the commonly accepted boundaries of the Triangle.
The NOVA/Horizon episode "The Case of the Bermuda Triangle", aired on June 27, 1976, was highly critical, stating that "When we've gone back to the original sources or the people involved, the mystery evaporates. Science does not have to answer questions about the Triangle because those questions are not valid in the first place ... Ships and planes behave in the Triangle the same way they behave everywhere else in the world."
David Kusche pointed out a common problem with many of the Bermuda Triangle stories and theories: "Say I claim that a parrot has been kidnapped to teach aliens human language and I challenge you to prove that is "not" true. You can even use Einstein's Theory of Relativity if you like. There is simply no way to prove such a claim untrue. The burden of proof should be on the people who make these statements, to show where they got their information from, to see if their conclusions and interpretations are valid, and if they have left anything out." Skeptical researchers, such as Ernest Taves and Barry Singer, have noted how mysteries and the paranormal are very popular and profitable. This has led to the production of vast amounts of material on topics such as the Bermuda Triangle. They were able to show that some of the pro-paranormal material is often misleading or inaccurate, but its producers continue to market it. Accordingly, they have claimed that the market is biased in favor of books, TV specials, and other media that support the Triangle mystery, and against well-researched material if it espouses a skeptical viewpoint.
Explanation attempts.
Persons accepting the Bermuda Triangle as a real phenomenon have offered a number of explanatory approaches.
Paranormal explanations.
Triangle writers have used a number of supernatural concepts to explain the events. One explanation pins the blame on leftover technology from the mythical lost continent of Atlantis. Sometimes connected to the Atlantis story is the submerged rock formation known as the Bimini Road off the island of Bimini in the Bahamas, which is in the Triangle by some definitions. Followers of the purported psychic Edgar Cayce take his prediction that evidence of Atlantis would be found in 1968 as referring to the discovery of the Bimini Road. Believers describe the formation as a road, wall, or other structure, but the Bimini Road is of natural origin.
Other writers attribute the events to UFOs. This idea was used by Steven Spielberg for his science fiction film "Close Encounters of the Third Kind", which features the lost Flight 19 aircrews as alien abductees.
Charles Berlitz, author of various books on anomalous phenomena, lists several theories attributing the losses in the Triangle to anomalous or unexplained forces.
Natural explanations.
Compass variations.
Compass problems are one of the cited phrases in many Triangle incidents. While some have theorized that unusual local magnetic anomalies may exist in the area, such anomalies have not been found. Compasses have natural magnetic variations in relation to the magnetic poles, a fact which navigators have known for centuries. Magnetic (compass) north and geographic (true) north are only exactly the same for a small number of places – for example, as of 2000, in the United States, only those places on a line running from Wisconsin to the Gulf of Mexico. But the public may not be as informed, and think there is something mysterious about a compass "changing" across an area as large as the Triangle, which it naturally will.
Gulf Stream.
The Gulf Stream is a major surface current, primarily driven by thermohaline circulation that originates in the Gulf of Mexico and then flows through the Straits of Florida into the North Atlantic. In essence, it is a river within an ocean, and, like a river, it can and does carry floating objects. It has a surface velocity of up to about 2.5 metres per second (5.6 mi/h). A small plane making a water landing or a boat having engine trouble can be carried away from its reported position by the current.
Human error.
One of the most cited explanations in official inquiries as to the loss of any aircraft or vessel is human error. Human stubbornness may have caused businessman Harvey Conover to lose his sailing yacht, the "Revonoc", as he sailed into the teeth of a storm south of Florida on January 1, 1958.
Violent weather.
Tropical cyclones are powerful storms, which form in tropical waters and have historically cost thousands of lives lost and caused billions of dollars in damage. The sinking of Francisco de Bobadilla's Spanish fleet in 1502 was the first recorded instance of a destructive hurricane. These storms have in the past caused a number of incidents related to the Triangle.
A powerful downdraft of cold air was suspected to be a cause in the sinking of the "Pride of Baltimore" on May 14, 1986. The crew of the sunken vessel noted the wind suddenly shifted and increased velocity from to . A National Hurricane Center satellite specialist, James Lushine, stated "during very unstable weather conditions the downburst of cold air from aloft can hit the surface like a bomb, exploding outward like a giant squall line of wind and water." A similar event occurred to the "Concordia" in 2010 off the coast of Brazil.
Methane hydrates.
An explanation for some of the disappearances has focused on the presence of large fields of methane hydrates (a form of natural gas) on the continental shelves. Laboratory experiments carried out in Australia have proven that bubbles can, indeed, sink a scale model ship by decreasing the density of the water; any wreckage consequently rising to the surface would be rapidly dispersed by the Gulf Stream. It has been hypothesized that periodic methane eruptions (sometimes called "mud volcanoes") may produce regions of frothy water that are no longer capable of providing adequate buoyancy for ships. If this were the case, such an area forming around a ship could cause it to sink very rapidly and without warning.
Publications by the USGS describe large stores of undersea hydrates worldwide, including the Blake Ridge area, off the coast of the southeastern United States. However, according to the USGS, no large releases of gas hydrates are believed to have occurred in the Bermuda Triangle for the past 15,000 years.
Notable incidents.
"Ellen Austin".
The "Ellen Austin" supposedly came across a derelict ship, placed on board a prize crew, and attempted to sail with it to New York in 1881. According to the stories, the derelict disappeared; others elaborating further that the derelict reappeared minus the prize crew, then disappeared again with a second prize crew on board. A check from Lloyd's of London records proved the existence of the "Meta", built in 1854 and that in 1880 the "Meta" was renamed "Ellen Austin". There are no casualty listings for this vessel, or any vessel at that time, that would suggest a large number of missing men were placed on board a derelict that later disappeared.
USS "Cyclops".
The incident resulting in the single largest loss of life in the history of the US Navy not related to combat occurred when the collier USS "Cyclops", carrying a full load of manganese ore and with one engine out of action, went missing without a trace with a crew of 309 sometime after March 4, 1918, after departing the island of Barbados. Although there is no strong evidence for any single theory, many independent theories exist, some blaming storms, some capsizing, and some suggesting that wartime enemy activity was to blame for the loss. In addition, two of "Cyclops"s sister ships, and were subsequently lost in the North Atlantic during World War II. Both ships were transporting heavy loads of metallic ore similar to that which was loaded on "Cyclops" during her fatal voyage. In all three cases structural failure due to overloading with a much denser cargo than designed is considered the most likely cause of sinking.
"Carroll A. Deering".
A five-masted schooner built in 1919, the "Carroll A. Deering" was found hard aground and abandoned at Diamond Shoals, near Cape Hatteras, North Carolina, on January 31, 1921. Rumors and more at the time indicated the "Deering" was a victim of piracy, possibly connected with the illegal rum-running trade during Prohibition, and possibly involving another ship, , which disappeared at roughly the same time. Just hours later, an unknown steamer sailed near the lightship along the track of the "Deering", and ignored all signals from the lightship. It is speculated that "Hewitt" may have been this mystery ship, and possibly involved in the "Deering" crew's disappearance.
Flight 19.
Flight 19 was a training flight of five TBM Avenger torpedo bombers that disappeared on December 5, 1945, while over the Atlantic. The squadron's flight plan was scheduled to take them due east from Fort Lauderdale for 141 miles, north for 73 miles, and then back over a final 140-mile leg to complete the exercise. The flight never returned to base. The disappearance is attributed by Navy investigators to navigational error leading to the aircraft running out of fuel.
One of the search and rescue aircraft deployed to look for them, a PBM Mariner with a 13-man crew, also disappeared. A tanker off the coast of Florida reported seeing an explosion and observing a widespread oil slick when fruitlessly searching for survivors. The weather was becoming stormy by the end of the incident. According to contemporaneous sources the Mariner had a history of explosions due to vapour leaks when heavily loaded with fuel, as for a potentially long search and rescue operation.
"Star Tiger" and "Star Ariel".
G-AHNP "Star Tiger" disappeared on January 30, 1948, on a flight from the Azores to Bermuda; G-AGRE "Star Ariel" disappeared on January 17, 1949, on a flight from Bermuda to Kingston, Jamaica. Both were Avro Tudor IV passenger aircraft operated by British South American Airways. Both planes were operating at the very limits of their range and the slightest error or fault in the equipment could keep them from reaching the small island. One plane was not heard from long before it would have entered the Triangle.
Douglas DC-3.
On December 28, 1948, a Douglas DC-3 aircraft, number NC16002, disappeared while on a flight from San Juan, Puerto Rico, to Miami. No trace of the aircraft or the 32 people on board was ever found. A Civil Aeronautics Board investigation found there was insufficient information available on which to determine probable cause of the disappearance.
KC-135 Stratotankers.
On August 28, 1963, a pair of US Air Force KC-135 Stratotanker aircraft collided and crashed into the Atlantic. The Triangle version (Winer, Berlitz, Gaddis) of this story specifies that they did collide and crash, but there were two distinct crash sites, separated by over of water. However, Kusche's research showed that the unclassified version of the Air Force investigation report stated that the debris field defining the second "crash site" was examined by a search and rescue ship, and found to be a mass of seaweed and driftwood tangled in an old buoy.
"Connemara IV".
A pleasure yacht was found adrift in the Atlantic south of Bermuda on September 26, 1955; it is usually stated in the stories (Berlitz, Winer) that the crew vanished while the yacht survived being at sea during three hurricanes. The 1955 Atlantic hurricane season shows Hurricane Ione passing nearby between the 14th and 18th of that month, with Bermuda being affected by winds of almost gale force. In his second book on the Bermuda Triangle, Winer quoted from a letter he had received from Mr J.E. Challenor of Barbados:
On the morning of September 22 "Connemara IV" was lying to a heavy mooring in the open roadstead of Carlisle Bay. Because of the approaching hurricane, the owner strengthened the mooring ropes and put out two additional anchors. There was little else he could do, as the exposed mooring was the only available anchorage. ... In Carlisle Bay, the sea in the wake of Hurricane Janet was awe-inspiring and dangerous. The owner of "Connemara IV" observed that she had disappeared. An investigation revealed that she had dragged her moorings and gone to sea.
References.
Notes
Bibliography<br>
The incidents cited above, apart from the official documentation, come from the following works. Some incidents mentioned as having taken place within the Triangle are found "only" in these sources:
Further reading
ProQuest has newspaper source material for many incidents, archived in Portable Document Format (PDF). The newspapers include "The New York Times", "The Washington Post", and "The Atlanta Constitution". To access this website, registration is required, usually through a library connected to a college or university.
Flight 19
SS "Cotopaxi"
USS "Cyclops" (AC-4)
Carroll A. Deering
Wreckers
S.S. "Suduffco"
"Star Tiger" and "Star Ariel"
DC-3 Airliner NC16002 disappearance
Harvey Conover and "Revonoc"
KC-135 Stratotankers
B-52 Bomber ("Pogo 22")
Charter vessel "Sno'Boy"
SS "Marine Sulphur Queen"
SS "Sylvia L. Ossa"
The following websites have either online material that supports the popular version of the Bermuda Triangle, or documents published from official sources as part of hearings or inquiries, such as those conducted by the United States Navy or United States Coast Guard. Copies of some inquiries are not online and may have to be ordered; for example, the losses of Flight 19 or USS Cyclops can be ordered direct from the United States Naval Historical Center.
Most of the works listed here are largely out of print. Copies may be obtained at your local library, or purchased used at bookstores, or through eBay or Amazon.com. These books are often the "only" source material for some of the incidents that have taken place within the Triangle.

</doc>
<doc id="4856" url="https://en.wikipedia.org/wiki?curid=4856" title="Borough">
Borough

A borough is an administrative division in various countries. In principle, the term "borough" designates a self-governing walled town, although in practice, official use of the term varies widely.
The word "borough" derives from common Germanic "*Burg", meaning "fort": compare with "bury", "burgh" and "brough" (England), "burgh" (Scotland), "Burg" (Germany), "borg" (Scandinavia), "burcht" (Dutch), "boarch" (West Frisian), and the Germanic borrowing present in neighbouring Indo-european languages such as "borgo" (Italian), "bourg" (French), "burgo" (Spanish and Portuguese), "burg" (Romanian), "purg" (Kajkavian) and "durg" (दर्ग) (Hindi) and "arg" (ارگ) (Persian). The incidence of these words as suffixes to place names (for example, Aldeburgh, Bamburgh, Tilbury, Tilburg, Strasbourg (Strossburi in the local dialect), Luxembourg, Edinburgh, Grundisburgh, Hamburg, Gothenburg) usually indicates that they were once fortified settlements.
In the Middle Ages, boroughs were settlements in England that were granted some self-government; burghs were the Scottish equivalent. In medieval England, boroughs were also entitled to elect members of parliament. The use of the word "borough" probably derives from the burghal system of Alfred the Great. Alfred set up a system of defensive strong points (Burhs); in order to maintain these settlements, he granted them a degree of autonomy. After the Norman Conquest, when certain towns were granted self-governance, the concept of the burh/borough seems to have been reused to mean a self-governing settlement.
The concept of the borough has been used repeatedly (and often differently) throughout the world. Often, a borough is a single town with its own local government. However, in some cities it is a subdivision of the city (for example, New York City, London, and Montreal). In such cases, the borough will normally have either limited powers delegated to it by the city's local government, or no powers at all. In other places, such as the U.S. state of Alaska, "borough" designates a whole region; Alaska's largest borough, the North Slope Borough, is comparable in area to the entire United Kingdom, although its population is less than that of Swanage. In Australia, a "borough" was once a self-governing small town, but this designation has all but vanished, except for the only remaining borough in the country, which is the Borough of Queenscliffe.
Boroughs as administrative units are to be found in Ireland and the United Kingdom, more specifically in England and Northern Ireland. Boroughs also exist in the Canadian province of Quebec and formerly in Ontario, in some states of the United States, in Israel, formerly in New Zealand and only one left in Australia.
Etymology.
The word "borough" derives from the Old English word "burh", meaning a fortified settlement. Other English derivatives of "burh" include "bury", "brough" and "burgh". There are obvious cognates in other Indo-European languages. For example; "burgh" in Scots and Middle English; "burg" in German and Old English, "borg" in Scandinavian languages; "parcus" in Latin and "pyrgos" in Greek, "برج" (borj) in Persian.
A number of other European languages have cognate words that were borrowed from the Germanic languages during the Middle Ages, including "brog" in Irish, "bwr" or "bwrc", meaning "wall, rampart" in Welsh, "bourg" in French, "burg" in Catalan (in Catalonia there is a town named "Burg"), "borgo" in Italian, and "burgo" in Spanish (hence the place-name Burgos).
The 'burg' element is often confused with 'berg' meaning hill or mountain (cf. iceberg). Hence the 'berg' element in Bergen relates to a hill, rather than a fort. In some cases, the 'berg' element in place names has converged towards burg/borough; for instance Farnborough, from "fernaberga" (fern-hill).
Pronunciation.
In many parts of England, "borough" is pronounced as an independent word, and as when a suffix of a place-name. As a suffix, it is sometimes spelled "-brough".
In the United States, "borough" is pronounced or . When appearing as the suffix "-burg(h)" in place-names, it is pronounced .
Uses of "borough".
England and Wales.
Ancient and municipal boroughs.
During the medieval period many towns were granted self-governance by the Crown, at which point they became referred to as boroughs. The formal status of borough came to be conferred by Royal Charter. These boroughs were generally governed by a self-selecting corporation (i.e., when a member died or resigned his replacement would be by co-option). Sometimes boroughs were governed by bailiffs or headboroughs.
Debates on the Reform Bill (eventually the Reform Act 1832) had highlighted the variations in systems of governance of towns, and a Royal Commission was set up to investigate the issue. This resulted in a regularisation of municipal government (Municipal Corporations Act 1835). 178 of the ancient boroughs were reformed as "municipal boroughs", with all municipal corporations to be elected according to a standard franchise based on property ownership. The unreformed boroughs either lapsed in borough status, or were reformed (or abolished) at a later time. Several new municipal boroughs were formed in the new industrial cities after the bill enacted, according to the provisions of the bill.
As part of a large-scale reform of local government in England and Wales in 1974, municipal boroughs were finally abolished (having become increasingly irrelevant). However, the civic traditions of many boroughs were continued by the grant of a charter to their successor district councils. In smaller boroughs, a town council was formed for the area of the abolished borough, while charter trustees were formed in other former boroughs. In each case, the new body was allowed to use the regalia of the old corporation, and appoint ceremonial office holders such as sword and mace bearers as provided in their original charters. The council or trustees may apply for an Order in Council or Royal Licence to use the former borough coat of arms.
Parliamentary boroughs.
From 1265, two burgesses from each borough were summoned to the Parliament of England, alongside two knights from each county. Thus parliamentary constituencies were derived from the ancient boroughs. Representation in the House of Commons was decided by the House itself, which resulted in boroughs being established in some small settlements for the purposes of parliamentary representation, despite their possessing no actual corporation.
After the Reform Act, which disenfranchised many of the rotten boroughs (boroughs that had declined in importance, had only a small population, and had only a handful of eligible voters), parliamentary constituencies began to diverge from the ancient boroughs. While many ancient boroughs remained as municipal boroughs, they were disenfranchised by the Reform Act.
County boroughs.
The Local Government Act 1888 established a new sort of borough – the county borough. These were designed to be 'counties-to-themselves'; administrative divisions to sit alongside the new administrative counties. They allowed urban areas to be administered separately from the more rural areas. They, therefore, often contained pre-existing municipal boroughs, which thereafter became part of the second tier of local government, below the administrative counties and county boroughs.
The county boroughs were, like the municipal boroughs, abolished in 1974, being reabsorbed into their parent counties for administrative purposes.
Metropolitan boroughs.
In 1899, as part of a reform of local government in the County of London, the various parishes in London were reorganised as new entities, the 'metropolitan boroughs'. These were reorganised further when Greater London was formed out of Middlesex and the County of London in 1965.
When the new metropolitan counties (Greater Manchester, Merseyside, South Yorkshire, Tyne and Wear, West Midlands, and West Yorkshire) were created in 1974, their sub-divisions also became metropolitan boroughs; in many cases these metropolitan boroughs recapitulated abolished county boroughs (for example, Stockport). The metropolitan boroughs possessed slightly more autonomy from the metropolitan county councils than the shire county districts did from their county councils.
With the abolition of the metropolitan county councils in 1986, these metropolitan boroughs became independent, and continue to be so at present.
Other current uses.
Elsewhere in England a number of districts and unitary authority areas are called "borough". Until 1974, this was a status that denoted towns with a certain type of local government (a municipal corporation). Since 1974, it has been a purely ceremonial style granted by royal charter to districts which may consist of a single town or may include a number of towns or rural areas. Borough status entitles the council chairman to bear the title of mayor. Districts may apply to the British Crown for the grant of borough status upon advice of the Privy Council of the United Kingdom.
Northern Ireland.
In Northern Ireland, local government was reorganised in 1973. Under the legislation that created the 26 districts of Northern Ireland, a district council whose area included an existing municipal borough could resolve to adopt the charter of the old municipality and thus continue to enjoy borough status. Districts that do not contain a former borough can apply for a charter in a similar manner to English districts.
Canada.
In Quebec, the term borough is generally used as the English translation of arrondissement, referring to an administrative division of a municipality. Only eight municipalities in Quebec are divided into boroughs. See List of boroughs in Quebec.
It was previously used in Metropolitan Toronto, Ontario, to denote suburban municipalities including Scarborough, York, North York, Etobicoke prior to their conversion into cities. The Borough of East York was the last Toronto municipality to hold this status, relinquishing it upon becoming part of the City of Toronto on January 1, 1998.
Colombia.
The Colombian Municipalities are subdivided into boroughs with a local executive and an administrative board for local government.
These Boroughs are divided in neighborhoods.
United States.
In the United States, a borough is a unit of local government below the level of the state. The term is currently used in seven states.
The following states use, or have used, the word with the following meanings:
Certain names of places, such as Hillsboro, OR, Greensboro, NC, Tyngsborough, MA, and Maynesborough, NH reflect the historical use of Borough as a geographical scale in the United States.
Mexico.
In Mexico as translations from English to Spanish applied to Mexico City, the word "borough" has resulted in a delegación (delegation), referring to the 16 administrative areas within the Mexican Federal District. Also the municipalities of some states are administratively subdivided into boroughs, as showed in Municipality of Mexicali#Boroughs. (see: Boroughs of Mexico and Boroughs of the Mexican Federal District)
Australia.
In Australia, the term "borough" is an occasionally used term for a local government area. Currently there is only one borough in Australia, the Borough of Queenscliffe in Victoria, although there have been more in the past. However, in some cases it can be integrated into the councils name instead of used as an official title, such as the Municipality of Kingborough in Tasmania.
Republic of Ireland.
The Local Government Reform Act 2014 replaced the urban-only second-tier local government units with new urban and rural units termed "municipal districts". The abolished units included five which were termed "boroughs", namely Clonmel, Drogheda, Kilkenny, Sligo, and Wexford. However, the municipal districts containing four of these are styled "borough districts"; the exception is Kilkenny, whose district is the "Municipal District of Kilkenny City", because of Kilkenny's city status.
Earlier Irish boroughs include the 117 parliamentary boroughs of the Irish House of Commons, of which 80 were disfranchised by the Acts of Union 1800 and all but 11 abolished under the Municipal Corporations (Ireland) Act 1840. The six largest of those eleven became county boroughs under the Local Government (Ireland) Act 1898, of which those in the Republic were reclassed as "cities" under the Local Government Act 2001. Galway was a borough from 1937 until promoted to county borough in 1985, and Dún Laoghaire was a borough from 1930 until merged into Dún Laoghaire–Rathdown county in 1993.
New Zealand.
New Zealand formerly used the term borough to designate self-governing towns of more than 1,000 people, although 19th century census records show many boroughs with populations as low as 200. A borough of more than 20,000 people could become a city by proclamation. Boroughs and cities were collectively known as municipalities, and were enclaves separate from their surrounding counties. Boroughs proliferated in the suburban areas of the larger cities: By the 1980s there were 19 boroughs and three cities in the area that is now the City of Auckland.
In the 1980s, some boroughs and cities began to be merged with their surrounding counties to form districts with a mixed urban and rural population. A nationwide reform of local government in 1989 completed the process. Counties and boroughs were abolished and all boundaries were redrawn. Under the new system, most territorial authorities cover both urban and rural land. The more populated councils are classified as cities, and the more rural councils are classified as districts. Only Kawerau District, an enclave within Whakatāne District, continues to follow the tradition of a small town council that does not include surrounding rural area.
Israel.
Under Israeli law, inherited from British Mandate municipal law, the possibility of creating a municipal borough exists. However, no borough was actually created under law until 2005–2006, when Neve Monosson and Maccabim-Re'ut, both communal settlements (Heb: yishuv kehilati) founded in 1953 and 1984, respectively, were declared to be autonomous municipal boroughs (Heb: vaad rova ironi), within their mergers with the towns of Yehud and Modi'in. Similar structures have been created under different types of legal status over the years in Israel, notably Kiryat Haim in Haifa, Jaffa in Tel Aviv-Yafo and Ramot and Gilo in Jerusalem. However, Neve Monosson is the first example of a full municipal borough actually declared under law by the Minister of the Interior, under a model subsequently adopted in Maccabim-Re'ut as well.
It is the declared intention of the Interior Ministry to use the borough mechanism in order to facilitate municipal mergers in Israel, after a 2003 wide-reaching merger plan, which, in general, ignored the sensitivities of the communal settlements, and largely failed.
Netherlands.
In the Netherlands, the municipalities of Rotterdam and Amsterdam are divided into administrative boroughs, or deelgemeenten, which have their own borough council and a borough mayor. Other large cites are usually divided into districts, or stadsdelen, for census purposes.

</doc>
<doc id="4858" url="https://en.wikipedia.org/wiki?curid=4858" title="Bodmin">
Bodmin

Bodmin () is a civil parish and major town in Cornwall, England, United Kingdom. It is situated in the centre of the county southwest of Bodmin Moor.
The extent of the civil parish corresponds fairly closely to that of the town so is mostly urban in character. It is bordered to the east by Cardinham parish, to the southeast by Lanhydrock parish, to the southwest and west by Lanivet parish, and to the north by Helland parish.
Bodmin had a population of 12,778 (2001 census). This population had increased to 14,916 at the 2011 Census. It was formerly the county town of Cornwall until the Crown Courts moved to Truro which is also the administrative centre (before 1835 the county town was Launceston). Bodmin was in the administrative North Cornwall District until local government reorganisation in 2009 abolished the District ("see also Cornwall Council"). The town is part of the North Cornwall parliamentary constituency, which is represented by Scott Mann MP.
Bodmin Town Council is made up of 16 councillors who are elected to serve a term of four years. Each year, the Council elects one of its number as Mayor to serve as the town's civic leader and to chair council meetings.
Situation and origin of the name.
Bodmin lies in the centre of Cornwall, south-west of Bodmin Moor. It has been suggested that the town's name comes from an archaic word in the Cornish "bod" (meaning a dwelling; the later word is "bos") and a contraction of "menegh" (monks). The "monks' dwelling" may refer to an early monastic settlement instituted by St. Guron, which St. Petroc took as his site. Guron is said to have departed to St Goran on the arrival of Petroc.
The hamlets of Cooksland, Dunmere and Turfdown are in the parish.
History.
St. Petroc founded a monastery in Bodmin in the sixth century and gave the town its alternative name of "Petrockstow". The monastery was deprived of some of its lands at the Norman Conquest but at the time of Domesday still held 18 manors, including Bodmin, Padstow and Rialton. Bodmin is one of the oldest towns in Cornwall, and the only large Cornish settlement recorded in the Domesday Book in 1086. In the 15th century the Norman church of St Petroc was largely rebuilt and stands as one of the largest churches in Cornwall (the largest after the cathedral at Truro). Also built at that time was an abbey of canons regular, now mostly ruined. For most of Bodmin's history, the tin industry was a mainstay of the economy.
The name of the town probably derives from the Cornish "Bod-meneghy", meaning "dwelling of or by the sanctuary of monks". Variant spellings recorded include "Botmenei" in 1100, "Bodmen" in 1253, "Bodman" in 1377 and "Bodmyn" in 1522. The "Bodman" spelling also appears in sources and maps from the sixteenth and seventeenth centuries, most notably in the celebrated map of Cornwall produced by John Speed but actually engraved by the Dutch cartographer Jodocus Hondius the Elder (1563-1612) in Amsterdam in 1610 (published in London by Sudbury and Humble in 1626). It is unclear whether the Bodman spelling signifies any historical or monastic connection with the equally ancient settlement of Bodman at the western end of the Bodensee in the German province of Baden.
An inscription on a stone built into the wall of a summer house in Lancarffe furnishes proof of a settlement in Bodmin in the early Middle Ages. It is a memorial to one "Duno[.]atus son of Me[.]cagnus" and has been dated from the sixth to eighth centuries.
The Black Death killed half of Bodmin's population in the mid 14th century (1500 people).
Rebellions.
Bodmin was the centre of three Cornish uprisings. The first was the Cornish Rebellion of 1497 when a Cornish army, led by Michael An Gof, a blacksmith from St. Keverne, and Thomas Flamank, a lawyer from Bodmin, marched to Blackheath in London where they were eventually defeated by 10,000 men of the King's army under Baron Daubeny. Then, in the Autumn of 1497, Perkin Warbeck tried to usurp the throne from Henry VII. Warbeck was proclaimed King Richard IV in Bodmin but Henry had little difficulty crushing the uprising. In 1549, Cornishmen, allied with other rebels in neighboring Devon, rose once again in rebellion when the staunchly Protestant Edward VI tried to impose a new Prayer Book. The lower classes of Cornwall and Devon were still strongly attached to the Catholic religion and again a Cornish army was formed in Bodmin which marched across the border into Devon to lay siege to Exeter. This became known as the Prayer Book Rebellion. Proposals to translate the Prayer Book into Cornish were suppressed and in total 4,000 people were killed in the rebellion.
Churches.
Parish church of St Petroc.
The existing church building is dated 1469-72 and was until the building of Truro Cathedral the largest church in Cornwall. The tower which remains from the original Norman church and stands on the north side of the church (the upper part is 15th century) was, until the loss of its spire in 1699, 150 ft high. The building underwent two Victorian restorations and another in 1930. It is now listed Grade I. There are a number of interesting monuments, most notably that of Prior Vivian which was formerly in the Priory Church (Thomas Vivian's effigy lying on a chest: black Catacleuse stone and grey marble). The font of a type common in Cornwall is of the twelfth century: large and finely carved.
Other churches.
The Chapel of St Thomas Becket is a ruin of a 14th-century building in Bodmin churchyard. The holy well of St Guron is a small stone building at the churchyard gate. The Berry Tower is all that remains of the former church of the Holy Rood and there are even fewer remains from the substantial Franciscan Friary established ca. 1240: a gateway in Fore Street and two pillars elsewhere in the town. The Roman Catholic Abbey of St Mary and St Petroc, formerly belonging to the Canons Regular of the Lateran was built in 1965 next to the already existing seminary. The Roman Catholic parish of Bodmin includes a large area of North Cornwall and there are churches also at Wadebridge, Padstow and Tintagel. In 1881 the Roman Catholic mass was celebrated in Bodmin for the first time since 1539. A church was planned in the 1930s but delayed by World War II: the Church of St Mary and St Petroc was eventually consecrated in 1965: it was built next to the already existing seminary. There are also five other churches in Bodmin, including a Methodist church.
Sites of interest.
Institutions.
Bodmin Jail, operational for over 150 years but now a semi-ruin, was built in the late 18th century, and was the first British prison to hold prisoners in separate cells (though often up to 10 at a time) rather than communally. Over fifty prisoners condemned at the Bodmin Assize Court were hanged at the prison. It was also used for temporarily holding prisoners sentenced to transportation, awaiting transfer to the prison hulks lying in the highest navigable reaches of the River Fowey. Also, during World War I the prison held some of Britain's priceless national treasures including the Domesday Book, the ring and the Crown Jewels of the United Kingdom.
Other buildings of interest include the former Shire Hall, now a tourist information centre, and Victoria Barracks, formerly depot of the now defunct Duke of Cornwall's Light Infantry and now the site of the regimental museum. It includes the history of the regiment from 1702, plus a military library. The original barracks house the regimental museum which was founded in 1925. There is a fine collection of small arms and machine guns, plus maps, uniforms and paintings on display.
Bodmin County Lunatic Asylum was designed by John Foulston and afterwards George Wightwick. William Robert Hicks the humorist was domestic superintendent in the mid-19th century.
Freemasonry.
There is a sizeable single storey Masonic Hall in St Nicholas Street, which is home to no less than seven Masonic bodies.
Other sites.
The Bodmin Beacon Local Nature Reserve is the hill overlooking the town. The reserve has 83 acres (33.6 ha) of public land and at its highest point it reaches 162 metres with the distinctive landmark at the summit. The 44-metre tall monument to Sir Walter Raleigh Gilbert was built in 1857 by the townspeople of Bodmin to honour the soldier's life and work in India.
In 1966, the ""Finn VC Estate"" was named in honour of Victoria Cross winner James Henry Finn who once lived in the town. Langdon (1896) records six crosses in the parish of which the finest is at Carminow. An ornate granite drinking bowl which serves the needs of thirsty dogs at the entrance to Bodmin’s Priory car park was donated by Prince Chula Chakrabongse of Thailand who lived at Tredethy.
Education.
There are no independent schools in the area.
Primary schools.
St. Petroc's Voluntary Aided Church of England Primary School Athelstan Park, Bodmin, Cornwall was given this title in September 1990 after the amalgamation of St. Petroc's Infant School and St. Petroc's Junior School. St. Petroc's is a large school with some 440 pupils between the ages of four and 11. Eight of its 14 governors are nominated by the Diocese of Truro or the Parochial Church Council of St. Petroc's, Bodmin.
There are a further three primary (or elementary) schools within Bodmin; Berrycoombe School in the north west corner of the town, St. Mary's Catholic Primary School and Robartes Primary Junior School, both situated west of the town centre.
Bodmin College.
Bodmin College is a large state comprehensive school for ages 11–18 on the outskirts of the town and on the edge of Bodmin Moor. Its headmaster is Mr Brett Elliott. The College is home to the nationally acclaimed "Bodmin College Jazz Orchestra", founded and run by the previous Director of Music, Adrian Evans, until 2007 and more recently, by the current Director, Ben Vincent.Bodmin College
In 1997, Systems & Control students at Bodmin College constructed Roadblock, a robot which entered and won the first series of Robot Wars and was succeeded by "The Beast of Bodmin" (presumably named after the phantom cat purported to roam Bodmin Moor).
The School also has one of the largest sixth forms in the county.
Transport.
Bodmin Parkway railway station is served by main line trains and is situated on the Cornish Main Line about 3½ miles (5½ km) south-east from the town centre. A heritage railway, the Bodmin and Wenford Railway, runs from Bodmin Parkway station via Bodmin General railway station to Boscarne Junction where there is access to the Camel Trail. The bus link to Bodmin, Wadebridge and Padstow starts from outside the main entrance of Bodmin Parkway.
Bus and coach services connect Bodmin with other districts of Cornwall and Devon.
Sport and leisure.
Bodmin has a Non-League football club Bodmin Town F.C. who play at Priory Park.
The Royal Cornwall Golf Club (now defunct) was located on Bodmin Moor. It was founded in 1889. The club disappeared following WW2.
There is an active running club: Bodmin RoadRunners.
Media.
The "Cornish Guardian" is a weekly newspaper published every Wednesday in 7 separate editions, including the Bodmin edition.
Bodmin is the home of NCB Radio, an Internet radio station which aims to bring a dedicated station to North Cornwall.
Notable people.
See also :Category:People from Bodmin
Town twinning.
Bodmin is twinned with Bederkesa in Germany; Grass Valley, in California, United States; and Le Relecq-Kerhuon (Ar Releg-Kerhuon in Brittany, France.
Official heraldry.
W. H. Pascoe’s 1979 "A Cornish Armory" gives the arms of the priory and the monastery and the seal of the borough.
Official events.
On Halgaver Moor (Goats' Moor) near Bodmin there was once an annual carnival in July which was on one occasion attended by King Charles II. Halgaver is in the parish of Lanhydrock.
Bodmin Riding, a horseback procession through the town, is a traditional annual ceremony.
'Beating the bounds' and 'hurling'.
In 1865–66 William Robert Hicks was mayor of Bodmin, when he revived the custom of Beating the bounds of the town. He was — according to the Dictionary of National Biography — a very good man of business. This still takes place more or less every five years and concludes with a game of Cornish hurling. Hurling survives as a traditional part of beating the bounds at Bodmin, commencing at the close of the 'Beat'. The game is organised by the Rotary club of Bodmin and was last played in 2015. The game is started by the Mayor of Bodmin by throwing a silver ball into a body of water known as the "Salting Pool". There are no teams and the hurl follows a set route. The aim is to carry the ball from the "Salting Pool" via the old A30, along Callywith Road, then through Castle Street, Church Square and Honey Street to finish at the Turret Clock in Fore Street. The participant carrying the ball when it reaches the turret clock will receive a £10 reward from the mayor. 
In 2015, beating of the bounds and Cornish hurling took place at Bodmin 8 April organised by the Rotary club of Bodmin.

</doc>
<doc id="4859" url="https://en.wikipedia.org/wiki?curid=4859" title="Bodmin Moor">
Bodmin Moor

Bodmin Moor () is a granite moorland in northeastern Cornwall, England, United Kingdom. It is in size, and dates from the Carboniferous period of geological history.
Bodmin Moor is one of five granite plutons in Cornwall that make up part of the Cornubian batholith (see also Geology of Cornwall).
The name Bodmin Moor is relatively recent, an Ordnance Survey invention of 1813. It was formerly known as Fowey Moor after the River Fowey, which rises within it.
Geography.
Dramatic granite tors rise from the rolling moorland: the best known are Brown Willy, the highest point in Cornwall at , and Rough Tor at . To the south-east Kilmar Tor and Caradon Hill are the most prominent hills. Considerable areas of the moor are poorly drained and form marshes (in hot summers these can dry out). The rest of the moor is mostly rough pasture or overgrown with heather and other low vegetation.
The moor contains about 500 holdings with around 10,000 beef cows, 55,000 breeding ewes and 1,000 horses and ponies. Most of the moor is a Site of Special Scientific Interest (SSSI), "Bodmin Moor, North", and has been officially designated an Area of Outstanding Natural Beauty (AONB), as part of Cornwall AONB. Almost a third of Cornwall has AONB designation, with the same status and protection as a National Park. The moor has been identified by BirdLife International as an Important Bird Area (IBA) because it supports about 260 breeding pairs of European stonechats as well as a wintering population of 10,000 Eurasian golden plovers. The moor has also been recognized as a separate natural region and designated as national character area 153 by Natural England.
Rivers and inland waters.
Bodmin Moor is the source of several of Cornwall's rivers: they are mentioned here anti-clockwise from the south.
The River Fowey rises at a height of and flows through Lostwithiel and into the Fowey estuary.
The River Tiddy rises near Pensilva and flows southeast to its confluence with the River Lynher (the Lynher flows generally south-east until it joins the Hamoaze near Plymouth). The River Inny rises near Davidstow and flows southeast to its confluence with the River Tamar.
The River Camel rises on Hendraburnick Down and flows for approximately before joining the sea at Padstow. The River Camel and its tributary the De Lank River are an important habitat for the otter and both have been proposed as Special Areas of Conservation (SAC) The De Lank River rises near Roughtor and flows along an irregular course before joining the Camel south of Wenford.
The River Warleggan rises near Temple and flows south to join the Fowey.
On the southern slopes of the moor lies Dozmary Pool. It is Cornwall's only natural inland lake and is glacial in origin. In the 20th century three reservoirs have been constructed on the moor; these are Colliford Lake, Siblyback Lake and Crowdy reservoirs which supply water for a large part of the county's population. Various species of waterfowl are resident around these waters.
Parishes.
The parishes on the moor are as follows:
History and antiquities.
Prehistoric times.
10,000 years ago, in the Mesolithic period, hunter-gatherers wandered the area when it was wooded. There are several documented cases of flint scatters being discovered by archaeologists, indicating that these hunter gatherers practised flint knapping in the region.
During the Neolithic era, from about 4,500 to 2,300 BC, people began clearing trees and farming the land. It was also in this era that the production of various megalithic monuments began, predominantly long cairns (three of which have currently been identified, at Louden, Catshole and Bearah) and stone circles (sixteen of which have been identified). It was also likely that the naturally forming tors were also viewed in a similar manner to the manmade ceremonial sites.
In the following Bronze Age, the creation of monuments increased dramatically, with the production of over 300 further cairns, and more stone circles and stone rows. More than 200 Bronze Age settlements with enclosures and field patterns have been recorded. and many prehistoric stone barrows and circles lie scattered across the moor. In a programme shown in 2007 Channel 4's "Time Team "investigated a 500-metre cairn and the site of a Bronze Age village on the slopes of Rough Tor.
King Arthur's Hall thought to be a late Neolithic or early Bronze Age ceremonial site can be found to the east of St Breward on the moor.
Medieval and modern times.
Where practicable areas of the moor were used for pasture by herdsmen from the parishes surrounding the moor. Granite boulders were also taken from the moor and used for stone posts and to a certain extent for building (such material is known as moorstone). Granite quarrying only became reasonably productive when gunpowder became available.
The moor gave its name (Foweymore) to one of the medieval districts called stannaries which administered tin mining: the boundaries of these were never defined precisely. Until the establishment of a turnpike road through the moor (the present A30) in the 1770s the size of the moorland area made travel within Cornwall very difficult.
Its Cornish name, Goen Bren, is first recorded in the 12th century.
English Heritage monographs "Bodmin Moor: An Archaeological Survey" Volume 1 and Volume 2 covering the post-medieval and modern landscape are publicly available through the Archaeology Data Service.
Monuments and ruins.
Roughtor was the site of a medieval chapel of St Michael and is now designated as a memorial to the 43rd Wessex Division of the British Army. In 1844 on Bodmin Moor the body of 18-year-old Charlotte Dymond was discovered. Local labourer Matthew Weeks was accused of the murder and at noon on 12 August 1844 he was led from Bodmin Gaol and hanged. The murder site now has a monument erected from public money and the grave is at Davidstow churchyard.
Legends and traditions.
Dozmary Pool is identified by some people with the lake in which, according to Arthurian legend, Sir Bedivere threw Excalibur to The Lady of the Lake. Another legend relating to the pool concerns Jan Tregeagle.
The Beast of Bodmin has been reported many times but never identified with certainty.

</doc>
<doc id="4860" url="https://en.wikipedia.org/wiki?curid=4860" title="Berkeley, California">
Berkeley, California

Berkeley ( ) is a city on the east shore of San Francisco Bay in northern Alameda County, California. It is named after the 18th-century Anglo-Irish bishop and philosopher George Berkeley. It borders the cities of Oakland and Emeryville to the south and the city of Albany and unincorporated community of Kensington to the north. Its eastern border with Contra Costa County generally follows the ridge of the Berkeley Hills. Its population at the 2010 census was 112,580.
Berkeley is the site of the oldest campus in the University of California system – the University of California, Berkeley – of the Lawrence Berkeley National Laboratory which is managed and operated by the university, and the Graduate Theological Union. It is one of the most politically liberal cities in the United States.
History.
Early history.
The site of today's City of Berkeley was the territory of the Chochenyo/Huchiun band of the Ohlone people when the first Europeans arrived. Evidence of their existence in the area include pits in rock formations, which they used to grind acorns, and a shellmound, now mostly leveled and covered up, along the shoreline of San Francisco Bay at the mouth of Strawberry Creek. Other artifacts were discovered in the 1950s in the downtown area during remodeling of a commercial building, near the upper course of the creek.
The first people of European descent (most of whom were born in America, and many of whom were of mixed ancestry) arrived with the De Anza Expedition in 1776. Today, this is noted by signage on Interstate 80, which runs along the San Francisco Bay shoreline of Berkeley. The De Anza Expedition led to establishment of the Spanish Presidio of San Francisco at the entrance to San Francisco Bay (the "Golden Gate)," which is due west of Berkeley. Luis Peralta was among the soldiers at the Presidio. For his services to the King of Spain, he was granted a vast stretch of land on the east shore of San Francisco Bay (the "contra costa", "opposite shore") for a ranch, including that portion that now comprises the City of Berkeley.
Luis Peralta named his holding "Rancho San Antonio." The primary activity of the ranch was raising cattle for meat and hides, but hunting and farming were also pursued. Eventually, Peralta gave portions of the ranch to each of his four sons. What is now Berkeley lies mostly in the portion that went to Peralta's son Domingo, with a little in the portion that went to another son, Vicente. No artifact survives of the Domingo or Vicente ranches, but their names survive in Berkeley street names (Vicente, Domingo, and Peralta). However, legal title to all land in the City of Berkeley remains based on the original Peralta land grant.
The Peraltas' Rancho San Antonio continued after Alta California passed from Spanish to Mexican sovereignty after the Mexican War of Independence. However, the advent of U.S. sovereignty after the Mexican–American War, and especially, the Gold Rush, saw the Peraltas' lands quickly encroached on by squatters and diminished by dubious legal proceedings. The lands of the brothers Domingo and Vicente were quickly reduced to reservations close to their respective ranch homes. The rest of the land was surveyed and parceled out to various American claimants ("See" Kellersberger's Map).
Politically, the area that became Berkeley was initially part of a vast Contra Costa County. On March 25, 1853, Alameda County was created from a division within Contra Costa County, as well as from a small portion of Santa Clara County.
The area of Berkeley was at this period mostly a mix of open land, farms and ranches, with a small though busy wharf by the bay. It was not yet "Berkeley," but merely the northern part of the "Oakland Township" subdivision of Alameda County.
Late 19th century.
In 1866, Oakland's private College of California looked for a new site. It settled on a location north of Oakland along the foot of the Contra Costa Range (later called the Berkeley Hills) astride Strawberry Creek, at an elevation about above the bay, commanding a view of the Bay Area and the Pacific Ocean through the Golden Gate.
According to the "Centennial Record of the University of California", "In 1866…at Founders' Rock, a group of College of California men watched two ships standing out to sea through the Golden Gate. One of them, Frederick Billings, thought of the lines of the Anglo-Irish Anglican Bishop George Berkeley, 'westward the course of empire takes its way,' and suggested that the town and college site be named for the eighteenth-century Anglo-Irish philosopher." Although the philosopher's name is pronounced "bark-lee," the pronunciation of the city's name has evolved to suit American English as "burk-lee."
The College of California's "College Homestead Association" planned to raise funds for the new campus by selling off adjacent parcels of land. To this end, they laid out a plat and street grid that became the basis of Berkeley's modern street plan. Their plans fell far short of their desires, and they began a collaboration with the State of California that culminated in 1868 with the creation of the public University of California.
As construction began on the new site, more residences were constructed in the vicinity of the new campus. At the same time, a settlement of residences, saloons, and various industries grew around the wharf area called "Ocean View." A horsecar ran from Temescal in Oakland to the university campus along what is now Telegraph Avenue. The first post office opened in 1872.
By the 1870s, the Transcontinental Railroad reached its terminus in Oakland. In 1876, a branch line of the Central Pacific Railroad, the Berkeley Branch Railroad, was laid from a junction with the mainline called Shellmound (now a part of Emeryville) into what is now downtown Berkeley. That same year, the mainline of the transcontinental railroad into Oakland was re-routed, putting the right-of-way along the bay shore through Ocean View.
There was a strong prohibition movement in Berkeley at this time. In 1876, the city passed the "mile limit law," which forbade sale or public consumption of alcohol within one mile (1.6 km) of the new University of California. Then, in 1899 Berkeley residents voted to make their city an alcohol-free zone. Scientists, scholars and religious leaders spoke vehemently of the dangers of alcohol.
In 1878, the people of Ocean View and the area around the university campus, together with local farmers, incorporated as the Town of Berkeley. The first elected trustees of the town were the slate of Denis Kearney's Workingman's Party, who were particularly favored in the working class area of the former Ocean View, now called "West Berkeley." The area near the university became known for a time as "East Berkeley."
The modern age came quickly to Berkeley, no doubt due to the influence of the university. Electric lights were in use by 1888. The telephone had already come to town. Electric streetcars soon replaced the horsecar. A silent film of one of these early streetcars in Berkeley can be seen at the Library of Congress website: "A Trip To Berkeley, California"
Early 20th century.
Berkeley's slow growth ended abruptly with the Great San Francisco earthquake of 1906. The town and other parts of the East Bay escaped serious damage from the massive temblor, and thousands of refugees flowed across the Bay.
In 1908, a statewide referendum that proposed moving the California state capital to Berkeley was defeated by a margin of about 33,000 votes. The city named streets around the proposed capitol grounds for California counties. They bear those names today, a legacy of the failed referendum.
In 1909, the citizens of Berkeley adopted a new charter, and the Town of Berkeley became the City of Berkeley. Rapid growth continued up to the Crash of 1929. The Great Depression hit Berkeley hard, but not as hard as many other places in the U.S., thanks in part to the university.
On September 17, 1923, a major fire swept down the hills toward the university campus and the downtown section. Around 640 structures burned before a late afternoon sea breeze stopped its progress, allowing firefighters to put it out.
The next big growth occurred with the advent of World War II, when large numbers of people moved to the Bay Area to work in the many war industries, such as the immense Kaiser Shipyards in nearby Richmond. One who moved out, but played a big role in the outcome of the War was U.C. Professor and Berkeley resident J. Robert Oppenheimer. During the war, an Army base, Camp Ashby, was temporarily sited in Berkeley.
The element berkelium was synthesized utilizing the 60-inch cyclotron at UC Berkeley, and named in 1949, recognizing the university, thus also placing the city's name in the list of elements.
1950s and 1960s.
During the 1940s, many African Americans migrated to Berkeley. In 1950, the Census Bureau reported Berkeley's population as 11.7% black and 84.6% white.
The postwar years brought moderate growth to the city, as events on the U.C. campus began to build up to the recognizable activism of the sixties. In the 1950s, McCarthyism induced the university to demand a loyalty oath from its professors, many of whom refused to sign the oath on the principle of freedom of thought. In 1960, a U.S. House committee (HUAC) came to San Francisco to investigate the influence of communists in the Bay Area. Their presence was met by protesters, including many from the university. Meanwhile, a number of U.C. students became active in the Civil Rights Movement. Finally, in 1964, the university provoked a massive student protest by banning distribution of political literature on campus. This protest became the Free Speech Movement. As the Vietnam War rapidly escalated in the ensuing years, so did student activism at the university, particularly that organized by the Vietnam Day Committee.
Berkeley is strongly identified with the rapid social changes, civic unrest, and political upheaval that characterized the late 1960s. In that period, Berkeley—especially Telegraph Avenue—became a focal point for the hippie movement, which spilled over the Bay from San Francisco. Many hippies were apolitical drop-outs, rather than students, but in the heady atmosphere of Berkeley in 1967–1969 there was considerable overlap between the hippie movement and the radical left. An iconic event in the Berkeley Sixties scene was a conflict over a parcel of university property south of the contiguous campus site that came to be called "People's Park."
The battle over the disposition of People's Park resulted in a month-long occupation of Berkeley by the National Guard on orders of then-Governor Ronald Reagan. In the end, the park remained undeveloped, and remains so today. A spin-off, "People's Park Annex," was established at the same time by activist citizens of Berkeley on a strip of land above the Bay Area Rapid Transit subway construction along Hearst Avenue northwest of the U.C. campus. The land had also been intended for development, but was turned over to the city by BART and is now Ohlone Park.
The era of large public protest in Berkeley waned considerably with the end of the Vietnam War in 1975. While the 1960s were the heyday of liberal activism in Berkeley, it remains one of the most overwhelmingly Democratic cities in the United States.
1970s to present.
The Berkeley population declined in the 1970s, partly due to an exodus to the suburbs. Some moved because of the rising cost of living throughout the Bay Area, and others because of the decline and disappearance of many industries in West Berkeley.
From the 1980s to the present, Berkeley housing costs have risen, especially since the mid-1990s. Despite a slow down in 2005–2007, median home prices remain dramatically higher than the rest of the nation.
In 1983, Berkeley's Domestic Partner Task Force was established, which in 1984 made policy recommendation to the school board, which passed domestic partner legislation. The legislation became a model for similar measures nationwide.
In the 1990s, Public Television's Frontline documentary series featured race relations at Berkeley's only public high school, Berkeley High School.
In 2006, the Berkeley Oak Grove Protest began protesting construction of a new sports center annex to Memorial Stadium at the expense of a grove of oak trees on the UC campus. The protest ended in September 2008 after a lengthy court process.
In 2007–08, Berkeley received media attention due to demonstrations against a Marine Corps recruiting office in downtown Berkeley and a series of controversial motions by Berkeley's city council regarding opposition to Marine recruiting. ("See" Berkeley Marine Corps Recruiting Center controversy.)
During the fall of 2010, the Berkeley Student Food Collective opened after many protests on the UC Berkeley campus due to the proposed opening of the fast food chain Panda Express. Students and community members worked together to open a collectively run grocery store right off of the UC Berkeley campus, where the community can buy local, seasonal, humane, and organic foods. The Berkeley Student Food Collective still operates at 2440 Bancroft Way.
In the Fall of 2011, the nationwide Occupy Wall Street movement came to two Berkeley locations: on the campus of the University of California and as an encampment in Civic Center Park.
On September 18, 2012, Berkeley became what may be the first city in the U.S. to officially proclaim a day recognizing bisexuals September 23, which is known as Celebrate Bisexuality Day.
On September 2, 2014, the city council approved a measure to provide free medical marijuana to low-income patients.
The Measure D soda tax was approved by Berkeley voters on November 4, 2014, the first such tax in the United States.
Berkeley has a higher-than-average crime rate, particularly property crime, though the crime rate has fallen significantly since 2000.
Geography.
Berkeley is located at (37.871775, −122.274603).
According to the United States Census Bureau the city's area includes of land and (40.83%) water, most of it part of San Francisco Bay.
Berkeley borders the cities of Albany, Oakland, and Emeryville and Contra Costa County, including unincorporated Kensington, as well as San Francisco Bay.
Berkeley lies within telephone area code 510 (until September 2, 1991, Berkeley was part of the 415 telephone code that now covers only San Francisco and Marin counties), and the postal ZIP codes are 94701 through 94710, 94712, and 94720 for the University of California campus.
Geology.
Most of Berkeley lies on a rolling sedimentary plain that rises gently from sea level to the base of the Berkeley Hills. East of the Hayward Fault along the base of the hills, elevation increases more rapidly. The highest peak along the ridge line above Berkeley is Grizzly Peak, elevation . A number of small creeks run from the hills to the Bay through Berkeley: Cerrito, Codornices, Schoolhouse and Strawberry Creeks are the principal streams. Most of these are largely culverted once they reach the plain west of the hills.
The Berkeley Hills are part of the Pacific Coast Ranges, and run in a northwest–southeast alignment. Exposed in the Berkeley Hills are cherts and shales of the Claremont Formation (equivalent to the Monterey Formation), conglomerate and sandstone of the Orinda Formation and lava flows of the Moraga Volcanics. Of similar age to the Moraga Volcanics (extinct), within the Northbrae neighborhood of Berkeley, are outcroppings of erosion resistant rhyolite. These rhyolite formations can be seen in several city parks and in the yards of a number of private residences. Indian Rock Park in the northeastern part of Berkeley near the Arlington/Marin Circle features a large example.
Earthquakes.
Berkeley is traversed by the Hayward Fault Zone, a major branch of the San Andreas Fault to the west. No large earthquake has occurred on the Hayward Fault near Berkeley in historic times (except possibly in 1836), but seismologists warn about the geologic record of large tremblors several times in the deeper past. The current assessment is that a Bay Area earthquake of magnitude 6.7 or greater within the next 30 years is likely, with the Hayward Fault having the highest likelihood among faults in the Bay Area of being the epicenter. Moreover, like much of the Bay Area, Berkeley has many areas of some risk to soil liquefaction, with the flat areas closer to the shore at low to high susceptibility.
The 1868 Hayward earthquake did occur on the southern segment of the Hayward Fault in the vicinity of today's city of Hayward (hence, how the fault got its name). This quake destroyed the county seat of Alameda County then located in San Leandro and it subsequently moved to Oakland. It was strongly felt in San Francisco, causing major damage, and experienced by Samuel Clemens (Mark Twain). It was regarded as the "Great San Francisco earthquake" prior to 1906. It produced a furrow in the ground along the fault line in Berkeley, across the grounds of the new State Asylum for the Deaf, Dumb and Blind then under construction, which was noted by one early University of California professor. Though no significant damage was reported to most of the few Berkeley buildings of the time, the 1868 quake did destroy the vulnerable adobe home of Domingo Peralta in north Berkeley.
Today, evidence of the Hayward Fault's "creeping" is visible at various locations in Berkeley. Cracked roadways, sharp jogs in streams, and springs mark the fault's path. However, since it cuts across the base of the hills, the creep is often concealed by or confused with slide activity. Some of the slide activity itself, however, results from movement on the Hayward Fault.
A notorious segment of the Hayward Fault runs lengthwise down the middle of Memorial Stadium at the mouth of Strawberry Canyon on the University of California campus. Photos and measurements show the movement of the fault through the stadium.
Climate.
Berkeley has a cool summer Mediterranean climate (type Csb in the Köppen climate classification), with dry summers and wet winters. The summers are cooler than a typical Mediterranean climate thanks to upwelling ocean currents along the California coast. These help produce cool and foggy nights and mornings. Berkeley's location directly opposite the Golden Gate ensures that typical eastward fog flow blankets the city more often than its neighbors.
Winter is punctuated with rainstorms of varying ferocity and duration, but also produces stretches of bright sunny days and clear cold nights. It does not normally snow, though occasionally the hilltops get a dusting. Spring and fall are transitional and intermediate, with some rainfall and variable temperature. Summer typically brings night and morning low clouds or fog, followed by sunny, warm days. The warmest and driest months are typically June through September, with the highest temperatures occurring in September. Mid-summer (July–August) is often a bit cooler due to the sea breezes and fog common then.
Average January temperatures are a maximum of and a minimum of . Average September (the warmest month) temperatures are a maximum of and a minimum of . In a year, there are an average of 2.9 days with highs of or higher, and an average of 0.8 days with lows of or lower. The highest recorded temperature was on June 15, 2000, and the lowest recorded temperature was on December 22, 1990.
January is normally the wettest month, averaging of precipitation. Average annual precipitation is , falling on an average of 63.7 days each year. The most rainfall in one month was in February 1998. The most rainfall in 24 hours was on January 4, 1982. As in most of California, the heaviest rainfall years are usually associated with warm water El Nino episodes in the Pacific (e.g. 1982-83; 1997–98) which bring in drenching "pineapple express" storms. In contrast, dry years are often associated with cold Pacific La Nina episodes. Light snow has fallen on rare occasions. Snow has generally fallen every several years on the higher peaks of the Berkeley Hills.
In the late spring and early fall, strong offshore winds of sinking air typically develop, bringing heat and dryness to the area. In the spring, this is not usually a problem as vegetation is still moist from winter rains, but extreme dryness prevails by the fall, creating a danger of wildfires. In September 1923 a major fire swept through the neighborhoods north of the University campus, stopping just short of downtown. ("See" 1923 Berkeley fire). On October 20, 1991, gusty, hot winds fanned a conflagration along the Berkeley–Oakland border, killing 25 people and injuring 150, as well as destroying 2,449 single-family dwellings and 437 apartment and condominium units. ("See" 1991 Oakland firestorm)
Demographics.
The 2010 United States Census reported that Berkeley had a population of 112,580. The population density was 10,752 people per square mile of land area (4,104/km²). The racial makeup of Berkeley was 66,996 (59.5%) White, 11,241 (10.0%) Black or African American, 479 (0.4%) Native American, 21,690 (19.3%) Asian (8.4% Chinese, 2.4% Indian, 2.1% Korean, 1.6% Japanese, 1.5% Filipino, 1.0% Vietnamese), 186 (0.2%) Pacific Islander, 4,994 (4.4%) from other races, and 6,994 (6.2%) from two or more races. Hispanic or Latino of any race were 12,209 persons (10.8%). 6.8% of the city's population was of Mexican ancestry.
The Census reported that 99,731 people (88.6% of the population) lived in households, 12,430 (11.0%) lived in non-institutionalized group quarters, and 419 (0.4%) were institutionalized.
There were 46,029 households, out of which 8,467 (18.4%) had children under the age of 18 living in them, 13,569 (29.5%) were opposite-sex married couples living together, 3,855 (8.4%) had a female householder with no husband present, 1,368 (3.0%) had a male householder with no wife present. There were 2,931 (6.4%) unmarried opposite-sex partnerships, and 961 (2.1%) same-sex married couples or partnerships. 16,904 households (36.7%) were made up of individuals and 4,578 (9.9%) had someone living alone who was 65 years of age or older. The average household size was 2.17. There were 18,792 families (40.8% of all households); the average family size was 2.81. There were 49,454 housing units at an average density of 2,794.6 per square mile (1,079.0/km²), of which 18,846 (40.9%) were owner-occupied, and 27,183 (59.1%) were occupied by renters. The homeowner vacancy rate was 1.0%; the rental vacancy rate was 4.5%. 45,096 people (40.1% of the population) lived in owner-occupied housing units and 54,635 people (48.5%) lived in rental housing units.
The population was spread out with 13,872 people (12.3%) under the age of 18, 30,295 people (26.9%) aged 18 to 24, 30,231 people (26.9%) aged 25 to 44, 25,006 people (22.2%) aged 45 to 64, and 13,176 people (11.7%) who were 65 years of age or older. The median age was 31.0 years. For every 100 females there were 95.6 males. For every 100 females age 18 and over, there were 94.2 males.
According to the 2011 American Community Survey 5-Year estimate, the median income for a household in the city was $60,908, and the median income for a family was $102,976. Males had a median income of $67,476 versus $57,319 for females. The per capita income for the city was $38,896. About 7.2% of families and 18.3% of the population were below the poverty line, including 13.2% of those under age 18 and 9.2% of those age 65 or over.
Transportation.
Berkeley is served by Amtrak (Capitol Corridor), AC Transit, BART (Ashby, Downtown Berkeley Station and North Berkeley) and bus shuttles operated by major employers including UC Berkeley and Lawrence Berkeley National Laboratory. The Eastshore Freeway (Interstate 80 and Interstate 580) runs along the bay shoreline. Each day there is an influx of thousands of cars into the city by commuting UC faculty, staff and students, making parking for more than a few hours an expensive proposition.
Berkeley has one of the highest rates of bicycle and pedestrian commuting in the nation. Berkeley is the safest city of its size in California for pedestrians and cyclists, considering the number of injuries per pedestrian and cyclist, rather than per capita.
Berkeley has modified its original grid roadway structure through use of diverters and barriers, moving most traffic out of neighborhoods and onto arterial streets (visitors often find this confusing, because the diverters are not shown on all maps). Berkeley maintains a separate grid of arterial streets for bicycles, called Bicycle Boulevards, with bike lanes and lower amounts of car traffic than the major streets they often parallel.
Berkeley hosts car sharing networks run by City CarShare, Uhaul Car Share, and Zipcar. Rather than owning (and parking) their own cars, members share a group of cars parked nearby. Web- and telephone-based reservation systems keep track of hours and charges. Several "pods" (points of departure where cars are kept) exist throughout the city, in several downtown locations, at the Ashby and North Berkeley BART stations, and at various other locations in Berkeley (and other cities in the region). Using alternative transportation is encouraged.
Berkeley has had recurring problems with parking meter vandalism. In 1999, over 2,400 Berkeley meters were jammed, smashed, or sawed apart. Starting in 2005 and continuing into 2006, Berkeley began to phase out mechanical meters in favor of more centralized electronic meters.
Transportation past.
The first commuter service to San Francisco was provided by the Central Pacific's Berkeley Branch Railroad, a standard gauge steam railroad, which terminated in downtown Berkeley, and connected in Emeryville (at a locale then known as "Shellmound") with trains to the Oakland ferry pier as well as with the Central Pacific main line starting in 1876. The Berkeley Branch line was extended from Shattuck and University to Vine Street ("Berryman's Station") in 1878. Starting in 1882, Berkeley trains ran directly to the Oakland Pier. In the 1880s, Southern Pacific assumed operations of the Berkeley Branch. In 1911, Southern Pacific electrified this line and the several others it constructed in Berkeley, creating its East Bay Electric Lines division. The huge and heavy cars specially built for these lines were called the "Red Trains" or the "Big Red Cars." The Shattuck line was extended and connected with two other Berkeley lines (the Ninth Street Line and the California Street line) at Solano and Colusa (the "Colusa Wye"). At this time, the Northbrae Tunnel and Rose Street Undercrossing were constructed, both of which still exist. (The Rose Street Undercrossing is not accessible to the public, being situated between what is now two backyards.) The fourth Berkeley line was the Ellsworth St. line to the university campus. The last Red Trains ran in July 1941.
The first electric rail service in Berkeley was provided by several small streetcar companies starting in 1891. Most of these were eventually bought up by the Key System of Francis "Borax" Smith who added lines and improved equipment. The Key System's streetcars were operated by its East Bay Street Railways division. Principal lines in Berkeley ran on Euclid, The Arlington, College, Telegraph, Shattuck, San Pablo, University, and Grove (today's Martin Luther King Jr. Way). The last streetcars ran in 1948, replaced by buses.
The first electric commuter interurban-type trains to San Francisco from Berkeley were put in operation by the Key System in 1903, several years before the Southern Pacific electrified its steam commuter lines. Like the SP, Key trains ran to a pier serviced by the Key's own fleet of ferryboats, which also docked at the Ferry Building in San Francisco. After the Bay Bridge was built, the Key trains ran to the Transbay Terminal in San Francisco, sharing tracks on the lower deck of the Bay Bridge with the SP's red trains and the Sacramento Northern Railroad. It was at this time that the Key trains acquired their letter designations, which were later preserved by Key's public successor, AC Transit. Today's F bus is the successor of the F train. Likewise, the E, G and the H. Before the Bridge, these lines were simply the Shattuck Avenue Line, the Claremont Line, the Westbrae Line, and the Sacramento Street Line, respectively.
After the Southern Pacific abandoned transbay service in 1941, the Key System acquired the rights to use its tracks and catenary on Shattuck north of Dwight Way and through the Northbrae Tunnel to The Alameda for the F-train. The SP tracks along Monterey Avenue as far as Colusa had been acquired by the Key System in 1933 for the H-train, but were abandoned in 1941. The Key System trains stopped running in April 1958. In 1963, the Northbrae Tunnel was opened to auto traffic.
Economy.
Top employers.
According to the city's 2013 Comprehensive Annual Financial Report, the top employers in the city are:
Businesses.
Berkeley is the location of a number of nationally prominent businesses, many of which have been pioneers in their areas of operation. Notable businesses include Chez Panisse, birthplace of California cuisine, Peet's Coffee's original store, the Claremont Resort, punk rock haven 924 Gilman, and Saul Zaentz's Fantasy Studios. Notable former businesses include pioneer bookseller Cody's Books, The Nature Company, and the Berkeley Co-op.
Places.
Neighborhoods.
Berkeley has a number of distinct neighborhoods.
Surrounding the University of California campus are the most densely populated parts of the city. West of the campus is Downtown Berkeley, the city's traditional commercial core; home of the civic center, the city's only public high school, the busiest BART station in Berkeley, as well as a major transfer point for AC Transit buses. South of the campus is the Southside neighborhood, mainly a student ghetto, where much of the university's student housing is located. The busiest stretch of Telegraph Avenue is in this neighborhood. North of the campus is the quieter Northside neighborhood, the location of the Graduate Theological Union.
Further from the university campus, the influence of the university quickly becomes less visible. Most of Berkeley's neighborhoods are primarily made up of detached houses, often with separate in-law units in the rear, although larger apartment buildings are also common in many neighborhoods. Commercial activities are concentrated along the major avenues and at important intersections.
In the southeastern corner of the city is the Claremont District, home to the Claremont Hotel; and the Elmwood District, with a small shopping area on College Avenue. West of Elmwood is South Berkeley, known for its weekend flea market at the Ashby Station.
West of (and including) San Pablo Avenue, a major commercial corridor, is West Berkeley, the historic commercial center of the city, and the former unincorporated town of Ocean View. West Berkeley contains the remnants of Berkeley's industrial area, much of which has been replaced by retail and office uses, as well as residential live/work loft space, with the decline of manufacturing in the United States. The areas of South and West Berkeley are in the midst of redevelopment. Some residents have opposed redevelopment in this area. Along the shoreline of San Francisco Bay at the foot of University Avenue is the Berkeley Marina. Nearby is Berkeley's Aquatic Park, featuring an artificial linear lagoon of San Francisco Bay.
North of Downtown is the North Berkeley neighborhood, which has been nicknamed the "Gourmet Ghetto" because of the concentration of well-known restaurants and other food-related businesses. West of North Berkeley is Westbrae, a small neighborhood through which part of the Ohlone Greenway runs. Meanwhile, further north of North Berkeley are Northbrae, a master-planned subdivision from the early 20th century, and Thousand Oaks. Above these last three neighborhoods, on the western slopes of the Berkeley Hills are the neighborhoods of Cragmont and La Loma Park, notable for their dramatic views, winding streets, and numerous public stairways and paths.
Parks and recreation.
The city has many parks, and promotes greenery and the environment. The city has planted trees for years and is a leader in the nationwide effort to re-tree urban areas. Tilden Regional Park, lies east of the city, occupying the upper extent of Wildcat Canyon between the Berkeley Hills and the San Pablo Ridge. The city is also heavily involved in creek restoration and wetlands restoration, including a planned "daylighting" of Strawberry Creek along Center Street. The Berkeley Marina and East Shore State Park flank its shoreline at San Francisco Bay and organizations like the Urban Creeks Council and Friends of the Five Creeks the former of which is headquartered in Berkeley support the riparian areas in the town and coastlines as well. César Chávez Park, near the Berkeley Marina, was built at the former site of the city dump.
Landmarks and historic districts.
165 buildings in Berkeley are designated as local landmarks or local structures of merit. Of these, 49 are listed in the National Register of Historic Places, including:
Historic Districts listed in the National Register of Historic Places:
See "List of Berkeley Landmarks, Structures of Merit, and Historic Districts"
Arts and culture.
Berkeley is home to the Chilean-American community's La Peña Cultural Center, the largest cultural center for this community in the United States. The Freight and Salvage is the oldest established full-time folk and traditional music venue west of the Mississippi River.
Education.
Colleges and universities.
University of California, Berkeley's main campus is in the city limits.
The Graduate Theological Union, a consortium of nine independent theological schools, is located a block north of the University of California Berkeley's main campus. The Graduate Theological Union has the largest number of students and faculty of any religious studies doctoral program in the United States. In addition to more theological schools, Zaytuna College, a newly established Muslim liberal arts college, has taken 'Holy Hill' as its new home. Wright Institute, a psychology graduate school, is located in Berkeley. Berkeley City College is a community college in the Peralta Community College District.
Primary and secondary schools.
The Berkeley Unified School District operates public schools.
The first public school in Berkeley was the Ocean View School, now the site of the Berkeley Adult School located at Virginia Street and San Pablo Avenue. The public schools today are administered by the Berkeley Unified School District. In the 1960s, Berkeley was one of the earliest US cities to voluntarily desegregate, utilizing a system of buses, still in use. The city has one public high school, Berkeley High School (BHS). Established in 1880, BHS currently has over 3,000 students. The Berkeley High campus was designated a historic district by the National Register of Historic Places on January 7, 2008. Saint Mary's College High School, a Catholic school, has its street address in Berkeley, although most of the grounds and buildings are actually in neighboring Albany. Berkeley has 11 elementary schools and three middle schools.
There is also the Bay Area Technology School, the only school in the whole Bay Area to offer a technology- and science-based curriculum, with connections to leading universities.
The school district is managed by the Berkeley Board of Education, which consists of five members (who serve four-year terms) and two student directors. The current members are:
Public libraries.
Berkeley Public Library serves as the municipal library. University of California, Berkeley Libraries operates the University of California Berkeley libraries.
Government.
Berkeley has a mayor-council government. The mayor, currently Tom Bates, is elected for a four-year term. The Berkeley City Council has eight council members who each serve four-year terms. Districts 2, 3, 5 and 6 hold their elections in years divisible by four while Districts 1, 4, 7 and 8 hold theirs in even-numbered years not divisible by four. The current councilmembers are:
Most of the University housing is located in District 7 (although Foothill and Clark Kerr are in Districts 6 and 8, respectively). Districts 4 and 7 are majority-student.
The mayor appoints a city administrator, who is subject to confirmation by the city council.
Berkeley is also part of Alameda County, for which the Government of Alameda County is defined and authorized under the California Constitution, California law, and the Charter of the County of Alameda. The county government provides countywide services, such as elections and voter registration, law enforcement, jails, vital records, property records, tax collection, public health, and social services. The county government is primarily composed of the elected five-member Board of Supervisors, other elected offices including the Sheriff/Coroner, the District Attorney, Assessor, Auditor-Controller/County Clerk/Recorder, and Treasurer/Tax Collector, and numerous county departments and entities under the supervision of the County Administrator.
Sister cities.
Berkeley has 14 sister cities:

</doc>
<doc id="4861" url="https://en.wikipedia.org/wiki?curid=4861" title="Bolventor">
Bolventor

Bolventor () is a hamlet on Bodmin Moor in Cornwall, England, United Kingdom. It is situated in Altarnun civil parish between Launceston and Bodmin.
Toponymy.
The hamlet has been said to take its name from the "Bold Venture" that it must have appeared to build a farm in this moorland, but this is probably folk etymology, as "Bol-" is a common prefix in Cornish placenames. It is much more likely that the name derives from the 'Bold Adventure' tin-working area which was in operation near Jamaica Inn during the 1840s-1850s 
Jamaica Inn.
Bolventor is the location of the famous Jamaica Inn coaching inn. It is bypassed by a dual carriageway section of the A30 trunk road; before the bypass was built the hamlet straddled the A30 road.
Daphne du Maurier, a former resident, chose Bolventor as the setting for her novel about Cornish smugglers titled "Jamaica Inn". The inn that inspired the novel, Jamaica Inn, has stood beside the main road through the village since 1547. It is now a tourist attraction in its own right and dominates the hamlet.
Church.
The small church (dedicated to the Holy Trinity) that lies to the east of the hamlet closed some years ago. A mile from Bolventor there was a chapel of St Luke (from the 13th to the early 16th century): the font is now at the church of Tideford. Bolventor parish was established in 1846 (before that date the village was in St Neot parish) but has now been merged with Altarnun.

</doc>
<doc id="4862" url="https://en.wikipedia.org/wiki?curid=4862" title="Bengal">
Bengal

Bengal is a geographical and ethno-linguistic region in the eastern part of South Asia, at the apex of the Bay of Bengal and dominated by the fertile Ganges delta. The Bengal region was politically divided in the 1947 Partition of India based on religion: predominantly Hindu West Bengal became a province (now a state) of India, while predominantly Muslim East Bengal became a province of Pakistan and later gained independence as Bangladesh. Some regions of the historical kingdoms of Bengal are now part of Nepal and the neighbouring Indian states of Assam, Tripura, Bihar, Meghalaya, Manipur, Jharkhand, and Odisha. The Bengali people ( "Bangali"), who speak the Bengali language ( "Bangla"), which is Indo-Aryan, natively inhabit the region, alongside dozens of indigenous ethnic groups who speak minority languages of the Tibeto-Burman, Austroasiatic, and Dravidian families.
Bengal is one of the most densely populated regions on Earth, with an estimated population of 250 million people and a population density exceeding 900 people per square kilometre. Most of the Bengal region lies in the low-lying Ganges Delta, the world's largest river delta. In the southern part of the delta lies the Sundarbans, the world's largest mangrove forest and home of the Bengal tiger. In the coastal southeast lies Cox's Bazar, the world's longest beach with a length of more than .
The Bengal region has a rich literary and cultural heritage and immensely influenced South Asian history through the Bengal Renaissance during the 19th and early 20th centuries and the Bengali Language Movement in the mid-20th century. The Bengal was the seat of western science education and a major industrial hub in pre and post independent India and reshaped the modern Indian culture, and the Bengali people made important contributions to the revolutionary movement for Indian independence and the overall Indian independence movement, and successfully prosecuted the Bangladesh Liberation War.
Etymology.
The exact origin of the word "Bangla" (Bengali) and "Bongo" (Bengal) is unknown, though the word is believed to be derived from the Dravidian-speaking tribe called "Bang" that settled in the area around the year 1000 BC.
It could also be derived from the word "Vanga," which was a kingdom in the Bengal region during the times of Mahabharata as mentioned in Sanskrit literature.
History.
Remnants of Copper Age settlements in the Bengal region date back 4,300 years. After the arrival of Indo-Aryans, the kingdoms of Anga, Bongo, and Magadha were formed by the 10th century BC, located in the Bihar and Bengal regions. Magadha was one of the four main kingdoms of India at the time of Buddha and consisted of several Janapadas. One of the earliest foreign references to Bengal is the mention of a land named Gangaridai by the Greeks around 100 BC, located in an area in Bengal. From the 3rd to the 6th centuries AD, the kingdom of Magadha served as the seat of the Gupta Empire.
Two kingdoms – Vanga or Samatata and Gauda – are mentioned in some texts to have appeared after the end of Gupta Empire, although details of their ruling time are uncertain. The first recorded independent king of Bengal was Shashanka who reigned in the early 7th century. After a period of anarchy, the native Buddhist Pala Empire ruled the region for four hundred years, and expanded across a large part of South Asia during the reigns of Dharmapala and Devapala. Bengal was invaded by a Hindu Emperor Rajendra Chola I of the Chola dynasty for a short period in the 11th century. The Pala dynasty was followed by the reign of the Hindu Sena dynasty. Islam started to appear in Bengal during the late 11th century, in the form of Sufism. Beginning in 1202 a military commander from the Delhi Sultanate, Bakhtiar Khilji, overran Bihar and Bengal as far east as Rangpur, Bogra, and the Brahmaputra River. Although he failed to bring Bengal under his control, the expedition managed to defeat Lakshman Sen and his two sons, who moved to Bikramapur (present-day Munshiganj District), from where they ruled over a smaller area until the late 13th century.
From the 13th century onward, several Islamic dynasties ruled parts of Bengal, often known collectively as the Sultanate of Bengal. Some rulers such as the land-lords-Baro-Bhuyans, the Deva Kingdom, and Raja Ganesha ruled parts of the region intermittently. Bengal came once more under the direct control of Delhi when the Mughals conquered it in 1576. It became a Mughal "subah" and was ruled by "subahdars" (governors). Akbar exercised progressive rule and oversaw a period of prosperity (through trade and development) in Bengal and northern India. There were several independent Hindu states established in Bengal during the Mughal period like those of Maharaja Pratap Aditya of Jessore and Raja Sitaram Ray of Burdwan.
Bengal's trade and wealth impressed the Mughals so much that emperor Aurangzeb called the region the "Paradise of the Nations." Afghans under Sher Shah Suri and his descendants ruled Bengal from 1540 to 1560. Hindu king Hem Chandra Vikramaditya (Hemu) defeated and killed Bengal ruler Muhammed Shah in 1556 and appointed Shahbaaz Khan as his governor. Administration by governors appointed by the court of the Mughal Empire court (1575–1717) gave way to four decades of semi-independence under the Nawabs of Murshidabad, who respected the nominal sovereignty of the Mughals in Delhi. The Nawabs granted permission to the French East India Company to establish a trading post at Chandernagore in 1673, and the British East India Company at Calcutta in 1690.
Around the early 1700s, the Maratha Empire led expeditions in Bengal. The leader of the expedition was Maratha Maharaja Raghuji of Nagpur. Raghoji was able to annexe Odisha and parts of Bengal permanently as he successfully exploited the chaotic conditions prevailing in the region after the death of their Governor Murshid Quli Khan in 1727. Portuguese traders arrived late in the fifteenth century, once Vasco da Gama reached India by sea in 1498. European influence grew until the British East India Company gained taxation rights in Bengal "subah", or province, following the Battle of Plassey in 1757, when Siraj ud-Daulah, the last independent nawab, was defeated by the British. The Bengal Presidency was established by 1766, eventually including all British territories north of the Central Provinces (now Madhya Pradesh), from the mouths of the Ganges and the Brahmaputra to the Himalayas and the Punjab. The Bengal famine of 1770 claimed millions of lives. Calcutta was named the capital of British India in 1772. The Bengal Renaissance and Brahmo Samaj socio-cultural reform movements had great impact on the cultural and economic life of Bengal. The failed Indian rebellion of 1857 started near Calcutta and resulted in transfer of authority to the British Crown, administered by the Viceroy of India. Between 1905 and 1911, an abortive attempt was made to divide the province of Bengal into two zones.
Bengal has played a major role in the Indian independence movement, in which revolutionary groups were dominant. Armed attempts to overthrow the British Raj reached a climax when Subhas Chandra Bose led the Indian National Army against the British. Bengal was also central in the rising political awareness of the Muslim population—the Muslim League was established in Dhaka in 1906. In spite of a last-ditch effort to form a United Bengal, when India gained independence in 1947, Bengal was partitioned along religious lines. The western part went to India (and was named West Bengal) while the eastern part joined Pakistan as a province called East Bengal (later renamed East Pakistan, giving rise to Bangladesh in 1971). The circumstances of partition were bloody, with widespread religious riots in Bengal.
In East Pakistan, starting from the Bengali Language Movement of 1952, political dissent against West Pakistani domination grew steadily. The Awami League, led by Sheikh Mujibur Rahman, emerged as the political voice of the Bengali-speaking population of East Pakistan by the 1960s. In 1971, the crisis deepened when Rahman was arrested and a sustained military assault was launched on East Pakistan. Most of the Awami League leaders fled and set up a government-in-exile in West Bengal. The guerrilla Mukti Bahini and Bengali regulars eventually received support from the Indian Armed Forces in December 1971, resulting in a decisive victory over Pakistan on 16 December in the Bangladesh Liberation War or Indo-Pakistani War of 1971. The independent nation of Bangladesh was established. However, the nation of Bangladesh, since its creation, suffered from continuous political instability and prolonged martial and autocratic rules.
West Bengal, the western part of Bengal, became a state in India. In the 1960s and 1970s, severe power shortages, strikes and a violent Marxist-Naxalite movement damaged much of the state's infrastructure, leading to a period of economic stagnation. The Bangladesh Liberation War of 1971 resulted in the influx of millions of refugees to West Bengal, causing significant strains on its infrastructure. West Bengal politics underwent a major change when the Left Front won the 1977 assembly election, defeating the incumbent Indian National Congress. The Left Front, led by Communist Party of India (Marxist) (CPI(M)) governed the state for over three decades.
Geography.
Most of the Bengal region is in the low-lying Ganges–Brahmaputra River Delta or Ganges Delta. The Ganges Delta arises from the confluence of the rivers Ganges, Brahmaputra, and Meghna rivers and their respective tributaries. The total area of Bengal is 232,752  km2—West Bengal is and Bangladesh .
Most parts of Bangladesh are within above the sea level, and it is believed that about 10% of the land would be flooded if the sea level were to rise by . Because of this low elevation, much of this region is exceptionally vulnerable to seasonal flooding due to monsoons.
The highest point in Bangladesh is in Mowdok range at in the Chittagong Hill Tracts to the southeast of the country. A major part of the coastline comprises a marshy jungle, the Sundarbans, the largest mangrove forest in the world and home to diverse flora and fauna, including the royal Bengal tiger. In 1997, this region was declared endangered.
West Bengal is on the eastern bottleneck of India, stretching from the Himalayas in the north to the Bay of Bengal in the south. The state has a total area of . The Darjeeling Himalayan hill region in the northern extreme of the state belongs to the eastern Himalaya. This region contains Sandakfu ()—the highest peak of the state. The narrow Terai region separates this region from the plains, which in turn transitions into the Ganges delta towards the south. The Rarh region intervenes between the Ganges delta in the east and the western plateau and high lands. A small coastal region is on the extreme south, while the Sundarbans mangrove forests form a remarkable geographical landmark at the Ganges delta.
At least nine districts in West Bengal and 42 districts in Bangladesh have arsenic levels in groundwater above the World Health Organization maximum permissible limit of 50 µg/L (micro gram per litre) or 50 parts per billion and the untreated water is unfit for human consumption. The water causes arsenicosis, skin cancer and various other complications in the body. Arsenic is four times as poisonous as mercury.
Major cities.
The following are the largest cities in Bengal (in terms of population):
Demographics.
According to provisional results of 2011 Bangladesh census, population of Bangladesh was 142,319,000; however, CIA's "The World Factbook" gives 163,654,860 as its population in a July 2013 estimate. According to the provisional results of the 2011 Indian national census, West Bengal has a population of 91,347,736. So, the Bengal region, as of 2011, has at least 233 million people. This figures give a population density of 1003.9/km2; making it among the most densely populated areas in the world.
Bengali is the main language spoken in Bengal. Many phonological, lexical, and structural differences from the standard variety occur in peripheral varieties of Bengali; these include Sylheti, Chittagonian, Chakma, Rangpuri/Rajbangshi, Hajong, Rohingya, and Tangchangya.
English is often used for official work alongside Bengali, and many Bengalis are also familiar with other major Indo-Aryan languages such as Hindi, Urdu, Assamese, and Nepali.
In addition, there are several minority ethnolinguistic groups native to the region. These include speakers of other Indo-Aryan languages (e.g. Bishnupriya Manipuri, Oraon Sadri, various Bihari languages), Tibeto-Burman languages (e.g. A'Tong, Chak, Koch, Garo, Megam, Meitei Manipuri, Mizo, Mru, Pangkhua, Rakhine/Marma, Kok Borok, Riang, Tippera, Usoi, various Chin languages), Austroasiatic languages (e.g. Khasi, Koda, Mundari, Pnar, Santali, War), and Dravidian languages (e.g. Kurukh, Sauria Paharia).
Life expectancy is around 70.36 years for Bangladesh and 63.4 for West Bengal. In terms of literacy, West Bengal leads with 77% literacy rate, in Bangladesh the rate is approximately 59.82%. The level of poverty and illiteracy is high, the proportion of people living below the poverty line is more than 30%.
About 20,000 people live on "chars". Chars are temporary islands formed by the deposition of sediments eroded off the banks of the Ganges in West Bengal which often disappear in the monsoon season. They are made of very fertile soil. The inhabitants of chars are not recognised by the Government of West Bengal on the grounds that it is not known whether they are Bengalis or Bangladeshi refugees. Consequently, no identification documents are issued to char-dwellers who cannot benefit from health care, barely survive because of very poor sanitation and are prevented from emigrating to the mainland to find jobs when they have turned 14. On a particular char it was reported that 13% of women died at childbirth.
Economy.
Agriculture is the leading occupation in the region. Rice is the staple food crop. At present days, a large part of Bangladeshi economy has moved to industrial sector.
Historically, Europe once regarded Bengal as "the richest country to trade with".
Bangladesh.
Bangladesh is a Next Eleven developing nation with a US$209 billion economy and a per capita income of US$1,190. The Taka is the currency of Bangladesh. The central bank is the Bangladesh Bank. The service sector accounts for 51% of GDP, the industrial sector 30% and agriculture 18%. Between 2004 and 2014, Bangladesh averaged a GDP growth rate of 6%. The economy is increasingly led by export-oriented industrialisation. The Bangladesh textile industry is the second-largest in the world. Other key sectors include pharmaceuticals, shipbuilding, ceramics, leather goods and electronics. Being situated in one of the most fertile regions on Earth, agriculture plays a crucial role, with the principal cash crops including rice, jute, tea, wheat, cotton and sugarcane. Bangladesh ranks fifth in the global production of fish and seafood. The Bangladesh telecoms industry has witnessed rapid growth over the years and is dominated by foreign investors. The government has emphasised the development of software services and hi-tech industries under the "Digital Bangladesh" scheme. Bangladesh has substantial reserves of natural gas and coal; and many international oil companies are involved in production and exploration activities in the Bay of Bengal. Regional neighbours are keen to use Bangladeshi ports and railways for transhipment. Located at the crossroads of SAARC, the ASEAN+3, BIMSTEC, and the Indian Ocean, Bangladesh has the potential to emerge as a regional economic and logistics hub.
West Bengal.
The service sector is the largest contributor to the gross domestic product of West Bengal, contributing 60% of the state domestic product compared to 25% from agriculture and 15% from industry. State industries are localised in the Kolkata region and the mineral-rich western highlands. Durgapur–Asansol colliery belt is home to a number of major steel plants.
Kolkata is becoming a major hub for the information technology (IT) industry. Owing to the boom in Kolkata's and the overall state's economy, West Bengal had the third-fastest-growing economy in India. It is also known as the cultural capital of India.
Culture.
The common Bengali language and culture anchors the shared tradition of two parts of politically divided Bengal. Bengal has a long tradition in folk literature, evidenced by the "Chôrjapôdô", "Mangalkavya", "Shreekrishna Kirtana", "Maimansingha Gitika" or "Thakurmar Jhuli". Bengali literature in the medieval age was often either religious (e.g. Chandidas), or adaptations from other languages (e.g. Alaol). During the Bengal Renaissance of the nineteenth and twentieth centuries, Bengali literature was modernised through the works of authors such as Michael Madhusudan Dutta, Bankim Chandra Chattopadhyay, Rabindranath Tagore, Ishwar Chandra Vidyasagar and Kazi Nazrul Islam.
The Baul tradition is a unique heritage of Bengali folk music. The scholar saint Sri Anirvan loved Baul music, and in fact described himself as a simple Baul. Other folk music forms include Gombhira, Bhatiali and Bhawaiya. Folk music in Bengal is often accompanied by the ektara, a one-stringed instrument. Other instruments include the dotara, dhol, flute, and tabla. The region also has an active heritage in North Indian classical music.
Bengal had also been the harbinger of modernism in Indian arts. Abanindranath Tagore, one of the important 18th century artist from Bengal is often referred to as the father of Indian modern art. He had established the first non-British art academy in India known as the Kalabhavan within the premises of Santiniketan. Santiniketan in course of time had produced many important Indian artists like Gaganendranath Tagore, Nandalal Bose, Jamini Roy, Benode Bihari Mukherjee and Ramkinkar Baij. In the post-independence era, Bengal had produced important artists like Somenath Hore, Meera Mukherjee and Ganesh Pyne.
Rice and fish are traditional favourite foods, leading to a saying that in Bengali, "mach ar bhaath bangali baanaay", that translates as "fish and rice make a Bengali". Bengal's vast repertoire of fish-based dishes includes Hilsa preparations, a favourite among Bengalis. Bengalis make distinctive sweetmeats from milk products, including "Rôshogolla", "Chômchôm", and several kinds of "Pithe".
Bengali women commonly wear the "shaŗi" and the salwar kameez, often distinctly designed according to local cultural customs. In urban areas, many women and men wear Western-style attire. Among men, European dressing has greater acceptance. Men also wear traditional costumes such as the "kurta" with "dhoti" or "pyjama", often on religious occasions. The lungi, a kind of long skirt, is widely worn by Bangladeshi men.
The greatest religious festivals are the two Eids (Eid ul-Fitr and Eid ul-Adha) for the Muslims, and the autumnal Durga Puja for Hindus. Christmas (called "Bôŗodin" (Great day) in Bengali), Buddha Purnima are other major religious festivals. Other festivities include Pohela Baishakh (the Bengali New Year), Basanta-Utsab, Nobanno, and "Poush parbon" (festival of Poush).
Around 200 dailies are published in Bangladesh, along with more than 1800 periodicals. West Bengal had 559 published newspapers in 2005, of which 430 were in Bengali. Cricket and football are popular sports in the Bengal region. Local games include sports such as Kho Kho and Kabaddi, the later being the national sport of Bangladesh. An Indo-Bangladesh "Bengali Games" has been organised among the athletes of the Bengali speaking areas of the two countries.
Intra-Bengal relations today.
Geographic, cultural, historic, and commercial ties are growing, and both countries recognise the importance of good relations. During and immediately after Bangladesh's struggle for independence from Pakistan in 1971, India assisted refugees from East Pakistan, and intervened militarily to help bring about the independence of Bangladesh. The Indo-Bangladesh border length of , West Bengal has a border length of . Despite overlapping historic, geographic and cultural ties, the relation between West Bengal and Bangladesh is still well below the potential. The pan-Bengali sentiment among the people of the two parts of Bengal was at its height during the 1971 Bangladesh Liberation War. While the government radio and national press in India might have backed the struggle out of strategic considerations, the Bengali broadcast and print media went out of its way to lend overwhelming support.
Frequent air services link Kolkata with Dhaka and Chittagong. A bus service between Kolkata and Dhaka is operational. The primary road link is the Jessore Road which crosses the border at Petrapole-Benapole about north-west of Kolkata. The train service between Kolkata and Dhaka (Moitri Express), which was stopped after the Indo-Pakistani War of 1965, was resumed in 2008.
Visa services are provided by Bangladesh's consulate at Kolkata's Bangabandhu Sheikh Mujibur Rahman Road and India's high commissions in Dhaka, Chittagong and Rajshahi. India has a liberal visa policy and nearly 500,000 visas are issued every year to Bangladeshi students, tourists, health-tourists and others who visit West Bengal and often transit to other parts of India.
Undocumented immigration of Bangladeshi workers is a controversial issue championed by right-wing nationalist parties in India but finds little sympathy in West Bengal. India has fenced the border to control this flow but immigration is still continuing.
The official land border crossing at Petrapole-Benapole is the primary conduit for the over $1 billion trade between the two-halves of Bengal. The volume of unofficial exports to Bangladesh from India is reportedly in the range of $350–500 million each year.
External links.
Geo Links for Bengal
Perry-Castañeda Library Map Collection at University of Texas at Austin Libraries

</doc>
<doc id="4864" url="https://en.wikipedia.org/wiki?curid=4864" title="Bucket argument">
Bucket argument

Isaac Newton's rotating bucket argument (also known as "Newton's bucket") was designed to demonstrate that true rotational motion cannot be defined as the relative rotation of the body with respect to the immediately surrounding bodies. It is one of five arguments from the "properties, causes, and effects" of true motion and rest that support his contention that, in general, true motion and rest cannot be defined as special instances of motion or rest relative to other bodies, but instead can be defined only by reference to absolute space. Alternatively, these experiments provide an operational definition of what is meant by "absolute rotation", and do not pretend to address the question of "rotation relative to "what"?".
Background.
These arguments, and a discussion of the distinctions between absolute and relative time, space, place and motion, appear in a General Scholium at the very beginning of Newton's work, "The Mathematical Principles of Natural Philosophy" (1687), which established the foundations of classical mechanics and introduced his law of universal gravitation, which yielded the first quantitatively adequate dynamical explanation of planetary motion.
Despite their embrace of the principle of rectilinear inertia and the recognition of the kinematical relativity of apparent motion (which underlies whether the Ptolemaic or the Copernican system is correct), natural philosophers of the seventeenth century continued to consider true motion and rest as physically separate descriptors of an individual body. The dominant view Newton opposed was devised by René Descartes, and was supported (in part) by Gottfried Leibniz. It held that empty space is a metaphysical impossibility because space is nothing other than the extension of matter, or, in other words, that when one speaks of the space between things one is actually making reference to the relationship that exists between those things and not to some entity that stands between them. Concordant with the above understanding, any assertion about the motion of a body boils down to a description over time in which the body under consideration is at t1 found in the vicinity of one group of "landmark" bodies and at some t2 is found in the vicinity of some other "landmark" body or bodies.
Descartes recognized that there would be a real difference, however, between a situation in which a body with movable parts and originally at rest with respect to a surrounding ring was itself accelerated to a certain angular velocity with respect to the ring, and another situation in which the surrounding ring was given a contrary acceleration with respect to the central object. With sole regard to the central object and the surrounding ring, the motions would be indistinguishable from each other assuming that both the central object and the surrounding ring were absolutely rigid objects. However, if neither the central object nor the surrounding ring were absolutely rigid then the parts of one or both of them would tend to fly out from the axis of rotation.
Here is an everyday experience of the basic nature of the Descartes experiment: Consider sitting in your train and noticing a train originally at rest beside you in the railway station pulling away. Initially you think it is your own train accelerating, but then notice with surprise that you feel no force. Thus, it is not your own train moving, but the neighboring train. On the other hand, you would confirm your own train is accelerating if you sensed "g"-forces from the acceleration of your own train.
For contingent reasons having to do with the Inquisition, Descartes spoke of motion as both absolute and relative. However, his real position was that motion is absolute.
A contrasting position was taken by Ernst Mach, who contended that all motion was "relative".
The argument.
Newton discusses a bucket filled with water hung by a cord. If the cord is twisted up tightly on itself and then the bucket is released, it begins to spin rapidly, not only with respect to the experimenter, but also in relation to the water it contains. (This situation would correspond to diagram B above.)
Although the relative motion at this stage is the greatest, the surface of the water remains flat, indicating that the parts of the water have no tendency to recede from the axis of relative motion, despite proximity to the pail. Eventually, as the cord continues to unwind, the surface of the water assumes a concave shape as it acquires the motion of the bucket spinning relative to the experimenter. This concave shape shows that the water is rotating, despite the fact that the water is at rest relative to the pail. In other words, it is not the relative motion of the pail and water that causes concavity of the water, contrary to the idea that motions can only be relative, and that there is no absolute motion. (This situation would correspond to diagram D.) Possibly the concavity of the water shows rotation relative to "something else": say absolute space? Newton says: "One can find out and measure the true and absolute circular motion of the water".
In the 1846 Andrew Motte translation of Newton's words:
The argument that the motion is absolute, not relative, is incomplete, as it limits the participants relevant to the experiment to only the pail and the water, a limitation that has not been established. In fact, the concavity of the water clearly involves gravitational attraction, and by implication the Earth also is a participant. Here is a critique due to Mach arguing that only relative motion is established:
All observers agree that the surface of rotating water is curved. However, the explanation of this curvature involves centrifugal force for all observers with the exception of a truly stationary observer, who finds the curvature is consistent with the rate of rotation of the water as they observe it, with no need for an additional centrifugal force. Thus, a stationary frame can be identified, and it is not necessary to ask "Stationary with respect to what?":
A supplementary thought experiment with the same objective of determining the occurrence of absolute rotation also was proposed by Newton: the example of observing two identical spheres in rotation about their center of gravity and tied together by a string. Occurrence of tension in the string is indicative of absolute rotation; see Rotating spheres.
Detailed analysis.
The historic interest of the rotating bucket experiment is its usefulness in suggesting one can detect absolute rotation by observation of the shape of the surface of the water. However, one might question just how rotation brings about this change. Below are two approaches to understanding the concavity of the surface of rotating water in a bucket.
Newton's laws of motion.
The shape of the surface of a rotating liquid in a bucket can be determined using Newton's laws for the various forces on an element of the surface. For example, see Knudsen and Hjorth. The analysis begins with the free body diagram in the co-rotating frame where the water appears stationary. The height of the water "h" = "h"("r") is a function of the radial distance "r" from the axis of rotation Ω, and the aim is to determine this function. An element of water volume on the surface is shown to be subject to three forces: the vertical force due to gravity Fg, the horizontal, radially outward centrifugal force FCfgl, and the force normal to the surface of the water Fn due to the rest of the water surrounding the selected element of surface. The force due to surrounding water is known to be normal to the surface of the water because a liquid in equilibrium cannot support shear stresses. To quote Anthony and Brackett: Moreover, because the element of water does not move, the sum of all three forces must be zero. To sum to zero, the force of the water must point oppositely to the sum of the centrifugal and gravity forces, which means the surface of the water must adjust so its normal points in this direction. (A very similar problem is the design of a , where the slope of the turn is set so a car will not slide off the road. The analogy in the case of rotating bucket is that the element of water surface will "slide" up or down the surface unless the normal to the surface aligns with the vector resultant formed by the vector addition Fg + FCfgl.)
As "r" increases, the centrifugal force increases according to the relation (the equations are written per unit mass):
where "Ω" is the constant rate of rotation of the water. The gravitational force is unchanged at
where "g" is the acceleration due to gravity. These two forces add to make a resultant at an angle "φ" from the vertical given by
which clearly becomes larger as "r" increases. To ensure that this resultant is normal to the surface of the water, and therefore can be effectively nulled by the force of the water beneath, the normal to the surface must have the same angle, that is,
leading to the ordinary differential equation for the shape of the surface:
or, integrating:
where "h"(0) is the height of the water at "r" = 0. In words, the surface of the water is parabolic in its dependence upon the radius.
Potential energy.
The shape of the water's surface can be found in a different, very intuitive way using the interesting idea of the potential energy associated with the centrifugal force in the co-rotating frame.
In a reference frame uniformly rotating at angular rate Ω, the fictitious centrifugal force is conservative and has a potential energy of the form:
where "r" is the radius from the axis of rotation. This result can be verified by taking the gradient of the potential to obtain the radially outward force:
The meaning of the potential energy is that movement of a test body from a larger radius to a smaller radius involves doing work against the centrifugal force.
The potential energy is useful, for example, in understanding the concavity of the water surface in a rotating bucket. Notice that at equilibrium the surface adopts a shape such that an element of volume at any location on its surface has the same potential energy as at any other. That being so, no element of water on the surface has any incentive to move position, because all positions are equivalent in energy. That is, equilibrium is attained. On the other hand, were surface regions with lower energy available, the water occupying surface locations of higher potential energy would move to occupy these positions of lower energy, inasmuch as there is no barrier to lateral movement in an ideal liquid.
We might imagine deliberately upsetting this equilibrium situation by somehow momentarily altering the surface shape of the water to make it different from an equal-energy surface. This change in shape would not be stable, and the water would not stay in our artificially contrived shape, but engage in a transient exploration of many shapes until non-ideal frictional forces introduced by sloshing, either against the sides of the bucket or by the non-ideal nature of the liquid, killed the oscillations and the water settled down to the equilibrium shape.
To see the principle of an equal-energy surface at work, imagine gradually increasing the rate of rotation of the bucket from zero. The water surface is flat at first, and clearly a surface of equal potential energy because all points on the surface are at the same height in the gravitational field acting upon the water. At some small angular rate of rotation, however, an element of surface water can achieve lower potential energy by moving outward under the influence of the centrifugal force. Because water is incompressible and must remain within the confines of the bucket, this outward movement increases the depth of water at the larger radius, increasing the height of the surface at larger radius, and lowering it at smaller radius. The surface of the water becomes slightly concave, with the consequence that the potential energy of the water at the greater radius is increased by the work done against gravity to achieve the greater height. As the height of water increases, movement toward the periphery becomes no longer advantageous, because the reduction in potential energy from working with the centrifugal force is balanced against the increase in energy working against gravity. Thus, at a given angular rate of rotation, a concave surface represents the stable situation, and the more rapid the rotation, the more concave this surface. If rotation is arrested, the energy stored in fashioning the concave surface must be dissipated, for example through friction, before an equilibrium flat surface is restored.
To implement a surface of constant potential energy quantitatively, let the height of the water be formula_10: then the potential energy per unit mass contributed by gravity is formula_11 and the total potential energy per unit mass on the surface is
with formula_13 the background energy level independent of "r". In a static situation (no motion of the fluid in the rotating frame), this energy is constant independent of position "r". Requiring the energy to be constant, we obtain the parabolic form:
where "h(0)" is the height at "r" = 0 (the axis). See Figures 1 and 2.
The principle of operation of the centrifuge also can be simply understood in terms of this expression for the potential energy, which shows that it is favorable energetically when the volume far from the axis of rotation is occupied by the heavier substance.

</doc>
<doc id="4865" url="https://en.wikipedia.org/wiki?curid=4865" title="Roman Breviary">
Roman Breviary

The Roman Breviary (Latin: "Breviarium Romanum") is the liturgical book of the Latin liturgical rites of the Catholic Church containing the public or canonical prayers, hymns, the Psalms, readings, and notations for everyday use, especially by bishops, priests, and deacons in the Divine Office (i.e., at the canonical hours or Liturgy of the Hours, the Christians' daily prayer). The word breviary, in general, refers to a collection of Christian orders of prayers and readings, such as contained in Anglican or Lutheran resources. It may also be used to refer to an abridged version of any text or a brief account or summary of some subject, but is primarily used to refer to a Christian liturgical book. The volume containing the daily hours of Roman Catholic prayer was published as the "Breviarium Romanum" (Roman Breviary) until the reforms of Paul VI, when it became known as the Liturgy of the Hours. However, these terms are used interchangeably to refer to the Office in all its forms. This entry deals with the Roman Breviary prior to the changes introduced by Pope Paul VI in 1974.
Origin of name.
This word breviary (Lat. Breviarium), signifies in its primary acceptation an abridgment, or a compendium. It is often employed in this sense by Christian authors, e.g. Breviarium fidei, Breviarium in psalmos, Breviarium canonum, Breviarium regularum. In liturgical language Breviary has a special meaning, indicating a book furnishing the regulations for the celebration of Mass or the canonical Office, and may be met with under the titles Breviarium Ecclesiastici Ordinis, or Breviarium Ecclesiæ Rominsæ (Romanæ). In the ninth century Alcuin uses the word to designate an office abridged or simplified for the use of the laity. Prudentius of Troyes, about the same period, composed a Breviarium Psalterii (v. inf. V. HISTORY). In an ancient inventory occurs Breviarium Antiphonarii, meaning "Extracts from the Antiphonary". In the "Vita Aldrici" occurs "sicut in plenariis et breviariis Ecclesiæ ejusdem continentur". Again, in the inventories in the catalogues, such notes as these may be met with: "Sunt et duo cursinarii et tres benedictionales Libri; ex his unus habet obsequium mortuorum et unus Breviarius", or, "Præter Breviarium quoddam quod usque ad festivitatem S. Joannis Baptistæ retinebunt", etc. Monte Cassino about A.D. 1100 obtained a book titled "Incipit Breviarium sive Ordo Officiorum per totam anni decursionem".
From such references, and from others of a like nature, Quesnel gathers that by the word Breviarium was at first designated a book furnishing the rubrics, a sort of Ordo. The title Breviary, as we employ it—that is, a book containing the entire canonical office—appears to date from the eleventh century.
St. Gregory VII having, indeed, abridged the order of prayers, and having simplified the Liturgy as performed at the Roman Court, this abridgment received the name of Breviary, which was suitable, since, according to the etymology of the word, it was an abridgment. The name has been extended to books which contain in one volume, or at least in one work, liturgical books of different kinds, such as the Psalter, the Antiphonary, the Responsoriary, the Lectionary, etc. In this connection it may be pointed out that in this sense the word, as it is used nowadays, is illogical; it should be named a Plenarium rather than a Breviarium, since, liturgically speaking, the word Plenarium exactly designates such books as contain several different compilations united under one cover. This is pointed out, however, simply to make still clearer the meaning and origin of the word; and section V will furnish a more detailed explanation of the formation of the Breviary.
History.
Early history.
The canonical hours of the Breviary owe their remote origin to the Old Covenant when God commanded the Aaronic priests to offer morning and evening sacrifices. Other inspiration may have come from David's words in the Psalms "Seven times a day I praise you" (Ps. 119:164), as well as, "the just man meditates on the law day and night" (Ps. 1:2). Regarding Daniel "Three times daily he was kneeling and offering prayers and thanks to his God" (Dan. 6:10).
In the early days of Christian worship the Sacred Scriptures furnished all that was thought necessary, containing as it did the books from which the lessons were read and the psalms that were recited. The first step in the evolution of the Breviary was the separation of the Psalter into a choir-book. At first the president of the local church (bishop) or the leader of the choir chose a particular psalm as he thought appropriate. From about the 4th century certain psalms began to be grouped together, a process that was furthered by the monastic practice of daily reciting the 150 psalms. This took so much time that the monks began to spread it over a week, dividing each day into hours, and allotting to each hour its portion of the Psalter. St Benedict in the 6th century drew up such an arrangement, probably, though not certainly, on the basis of an older Roman division which, though not so skilful, is the one in general use. Gradually there were added to these psalter choir-books additions in the form of antiphons, responses, collects or short prayers, for the use of those not skilful at improvisation and metrical compositions. Jean Beleth, a 12th-century liturgical author, gives the following list of books necessary for the right conduct of the canonical office: the Antiphonarium, the Old and New Testaments, the "Passionarius (liber)" and the "Legendarius" (dealing respectively with martyrs and saints), the "Homiliarius" (homilies on the Gospels), the "Sermologus" (collection of sermons) and the works of the Fathers, besides, of course, the "Psalterium" and the "Collectarium". To overcome the inconvenience of using such a library the Breviary came into existence and use. Already in the 8th century Prudentius, bishop of Troyes, had in a "Breviarium Psalterii" made an abridgment of the Psalter for the laity, giving a few psalms for each day, and Alcuin had rendered a similar service by including a prayer for each day and some other prayers, but no lessons or homilies. The Breviary rightly so called, however, only dates from the 11th century; the earliest MS. containing the whole canonical office is of the year 1099 and is in the Mazarin library. Gregory VII (pope 1073–1085), too, simplified the liturgy as performed at the Roman court, and gave his abridgment the name of Breviary, which thus came to denote a work which from another point of view might be called a Plenary, involving as it did the collection of several works into one. There are several extant specimens of 12th-century Breviaries, all Benedictine, but under Innocent III (pope 1198–1216) their use was extended, especially by the newly founded and active Franciscan order. These preaching friars, with the authorization of Gregory IX, adopted (with some modifications, e.g. the substitution of the "Gallican" for the "Roman" version of the Psalter) the Breviary hitherto used exclusively by the Roman court, and with it gradually swept out of Europe all the earlier partial books (Legendaries, Responsories), &c., and to some extent the local Breviaries, like that of Sarum. Finally, Nicholas III (pope 1277–1280) adopted this version both for the curia and for the basilicas of Rome, and thus made its position secure.
Local and regular breviaries.
The Benedictines and Dominicans have Breviaries of their own. The only other types that merit notice are:
Early modern reforms.
Until the council of Trent every bishop had full power to regulate the Breviary of his own diocese; and this was acted upon almost everywhere. Each monastic community, also, had one of its own. Pius V (pope 1566–1572), however, while sanctioning those which could show at least 200 years of existence, made the Roman obligatory in all other places. But the influence of the Roman rite has gradually gone much beyond this, and has superseded almost all the local uses. The Roman has thus become nearly universal, with the allowance only of additional offices for saints specially venerated in each particular diocese. The Roman Breviary has undergone several revisions: The most remarkable of these is that by Francis Quignonez, cardinal of Santa Croce in Gerusalemme (1536), which, though not accepted by Rome (it was approved by Clement VII and Paul III, and permitted as a substitute for the unrevised Breviary, until Pius V in 1568 excluded it as too short and too modern, and issued a reformed edition ("Breviarium Pianum", Pian Breviary) of the old Breviary), formed the model for the still more thorough reform made in 1549 by the Church of England, whose daily morning and evening services are but a condensation and simplification of the Breviary offices. Some parts of the prefaces at the beginning of the English Prayer-Book are free translations of those of Quignonez. The Pian Breviary was again altered by Sixtus V in 1588, who introduced the revised Vulgate, in 1602 by Clement VIII (through Baronius and Bellarmine), especially as concerns the rubrics; and by Urban VIII (1623–1644), a purist who altered the text of certain hymns.
In the 17th and 18th centuries a movement of revision took place in France, and succeeded in modifying about half the Breviaries of that country. Historically, this proceeded from the labours of Jean de Launoy (1603–1678), "le dénicheur des saints", and Louis Sébastien le Nain de Tillemont, who had shown the falsity of numerous lives of the saints; while theologically it was produced by the Port Royal school, which led men to dwell more on communion with God as contrasted with the invocation of the saints. This was mainly carried out by the adoption of a rule that all antiphons and responses should be in the exact words of Scripture, which, of course, cut out the whole class of appeals to created beings. The services were at the same time simplified and shortened, and the use of the whole Psalter every week (which had become a mere theory in the Roman Breviary, owing to its frequent supersession by saints' day services) was made a reality. These reformed French Breviaries—e.g. the Paris Breviary of 1680 by Archbishop François de Harlay (1625–1695) and that of 1736 by Archbishop Charles-Gaspard-Guillaume de Vintimille du Luc (1655–1746)—show a deep knowledge of Holy Scripture, and much careful adaptation of different texts.
Later modern reforms.
During the pontificate of Pius IX a strong Ultramontane movement arose against the French Breviaries of 1680 and 1736. This was inaugurated by Montalembert, but its literary advocates were chiefly Dom Gueranger, a learned Benedictine monk, abbot of Solesmes, and Louis Veuillot (1813–1883) of the Univers; and it succeeded in suppressing them everywhere, the last diocese to surrender being Orleans in 1875. The Jansenist and Gallican influence was also strongly felt in Italy and in Germany, where Breviaries based on the French models were published at Cologne, Münster, Mainz and other towns. Meanwhile, under the direction of Benedict XIV (pope 1740–1758), a special congregation collected much material for an official revision, but nothing was published. In 1902, under Leo XIII, a commission under the presidency of Monsignor Louis Duchesne was appointed to consider the Breviary, the Missal, the Pontifical and the Ritual.
Significant changes came in 1910 with the reform of the Roman Breviary by Pope Pius X. This revision modified the traditional psalm scheme so that, while all 150 psalms were used in the course of the week, these were said without repetition. Those assigned to the Sunday office underwent the least revision, although noticeably fewer psalms are recited at Matins, and both Lauds and Compline are slightly shorter due to psalms (or in the case of Compline the first few verses of a psalm) being removed. Pius X was probably influenced by earlier attempts to eliminate repetition in the psalter, most notably the liturgy of the Benedictine congregation of St. Maur. However, since Cardinal Quignonez's attempt to reform the Breviary employed this principle—albeit with no regard to the traditional scheme—such notions had floated around in the western Church, and can particularly be seen in the Paris Breviary.
Pope Pius XII introduced optional use of a new translation of the Psalms from the Hebrew to a more classical Latin. Most breviaries published in the late 1950s and early 1960s used this "Pian Psalter".
Pope John XXIII also revised the Breviary in 1960, introducing changes drawn up by his predecessor Pope Pius XII. The most notable alteration is the shortening of most feasts from nine to three lessons at Matins, keeping only the Scripture readings (the former lesson i, then lessons ii and iii together), followed by either the first part of the patristic reading (lesson vii) or, for most feasts, a condensed version of the former second Nocturn, which was formerly used when a feast was reduced in rank and commemorated.
Manuscripts and printed editions.
Before the rise of the mendicant orders (wandering friars) in the thirteenth century, the daily services were usually contained in a number of large volumes. The first occurrence of a single manuscript of the daily office was written by the Benedictine order at Monte Cassino in Italy in 1099. By a strange twist, the Benedictines were not a mendicant order, but a stable, monastery-based order, and single-volume breviaries are rare from this early period.
The arrangement of the Psalms in the Rule of St. Benedict had a profound impact upon the breviaries used by secular and monastic clergy alike, up until 1911 when Pope St. Pius X introduced his reform of the Roman Breviary. In many places, every diocese, order or ecclesiastical province maintained its own edition of the breviary.
However, mendicant friars travelled around a lot and needed a shortened, or abbreviated, daily office contained in one portable book, and single-volume breviaries flourished from the thirteenth century onwards.
These abbreviated volumes soon became very popular and eventually supplanted the Roman Catholic Church's Curia office, previously said by non-monastic clergy.
Before the advent of printing, breviaries were written by hand and were often richly decorated with initials and miniature illustrations telling stories in the lives of Christ or the saints, or stories from the Bible.
Later printed breviaries usually have woodcut illustrations, interesting in their own right but the poor relation of the beautifully illuminated breviaries.
The beauty and value of many of the Latin Breviaries were brought to the notice of English churchmen by one of the numbers of the Oxford "Tracts for the Times", since which time they have been much more studied, both for their own sake and for the light they throw upon the English Prayer-Book.
From a bibliographical point of view some of the early printed Breviaries are among the rarest of literary curiosities, being merely local. The copies were not spread far, and were soon worn out by the daily use made of them. Doubtless many editions have perished without leaving a trace of their existence, while others are known by unique copies. In Scotland the only one which has survived the convulsions of the 16th century is "Aberdeen Breviary", a Scottish form of the Sarum Office (the Sarum Rite was much favoured in Scotland as a kind of protest against the jurisdiction claimed by the diocese of York), revised by William Elphinstone (bishop 1483–1514), and printed at Edinburgh by Walter Chapman and Andrew Myllar in 1509–1510. Four copies have been preserved of it, of which only one is complete; but it was reprinted in facsimile in 1854 for the Bannatyne Club by the munificence of the Duke of Buccleuch. It is particularly valuable for the trustworthy notices of the early history of Scotland which are embedded in the lives of the national saints. Though enjoined by royal mandate in 1501 for general use within the realm of Scotland, it was probably never widely adopted. The new Scottish "Proprium" sanctioned for the Roman Catholic province of St Andrews in 1903 contains many of the old Aberdeen collects and antiphons.
The Sarum or Salisbury Breviary itself was very widely used. The first edition was printed at Venice in 1483 by Raynald de Novimagio in folio; the latest at Paris, 1556, 1557. While modern Breviaries are nearly always printed in four volumes, one for each season of the year, the editions of the Sarum never exceeded two parts.
Contents of the Roman Breviary.
At the beginning stands the usual introductory matter, such as the tables for determining the date of Easter, the calendar, and the general rubrics. The Breviary itself is divided into four seasonal parts—winter, spring, summer, autumn—and comprises under each part:
These parts are often published separately.
The Psalter.
This psalm book is the very backbone of the Breviary, the groundwork of the Catholic prayer-book; out of it have grown the antiphons, responsories and versicles. Until the 1911 reform, the psalms were arranged according to a disposition dating from the 8th century, as follows: Psalms i.–cviii., with some omissions, were recited at Matins, twelve each day from Monday to Saturday, and eighteen on Sunday. The omissions were said at Lauds, Prime and Compline. Psalms cix–cxlvii (except cxvii, cxviii and cxlii) were said at Vespers, five each day. Psalms cxlviii–cl were always used at Lauds, and give that hour its name. The text of this Psalter is that commonly known as the Gallican. The name is misleading, for it is simply the second revision (A.D. 392) made by Jerome of the old "Itala" version originally used in Rome. Jerome's first revision of the "Itala" (A.D. 383), known as the Roman, is still used at St Peter's in Rome, but the "Gallican", thanks especially to St Gregory of Tours, who introduced it into Gaul in the 6th century, has ousted it everywhere else. The Antiphonary of Bangor proves that Ireland accepted the Gallican version in the 7th century, and the English Church did so in the 10th.
Following the 1911 reform, Matins was reduced to nine Psalms every day, with the other psalms redistributed throughout Prime, Terce, Sext, and Compline. For Sundays and special feasts Lauds and Vespers largely remained the same, Psalm 118 remained distributed at the Little Hours and Psalms 4, 90, and 130 were kept at Compline.
The "Proprium de Tempore".
This contains the office of the seasons of the Christian year (Advent to Trinity), a conception that only gradually grew up. There is here given the whole service for every Sunday and week-day, the proper antiphons, responsories, hymns, and especially the course of daily Scripture-reading, averaging about twenty verses a day, and (roughly) arranged thus: for Advent, Isaiah; Epiphany to Septuagesima, Pauline Epistles; Lent, patristic homilies (Genesis on Sundays); Passion-tide, Jeremiah; Easter to Whitsun, Acts, Catholic epistles and Apocalypse; Whitsun to August, Samuel and Kings; August to Advent, Wisdom books, Maccabees, Prophets.
The "Proprium Sanctorum".
This contains the lessons, psalms and liturgical formularies for saints' festivals, and depends on the days of the secular month. The readings of the second Nocturn are mainly hagiological biography, with homilies or papal documents for certain major feasts, particularly those of Jesus and Mary. Some of this material has been revised by Leo XIII, in view of archaeological and other discoveries. The third Nocturn consists of a homily on the Gospel which is read at that day's Mass. Covering a great stretch of time and space, they do for the worshipper in the field of church history what the Scripture readings do in that of biblical history.
The "Commune Sanctorum".
This comprises psalms, antiphons, lessons, &c., for feasts of various groups or classes (twelve in all); e.g. apostles, martyrs, confessors, virgins, and the Blessed Virgin Mary. These offices are of very ancient date, and many of them were probably in origin proper to individual saints. They contain passages of great literary beauty. The lessons read at the third nocturn are patristic homilies on the Gospels, and together form a rough summary of theological instruction.
Extra services.
Here are found the Little Office of the Blessed Virgin Mary, the Office of the Dead (obligatory on All Souls' Day), and offices peculiar to each diocese.
Elements of the Hours.
It has already been indicated, by reference to Matins, Lauds, &c., that not only each day, but each part of the day, has its own office, the day being divided into liturgical "hours." A detailed account of these will be found in the article Canonical Hours. Each of the hours of the office is composed of the same elements, and something must be said now of the nature of these constituent parts, of which mention has here and there been already made. They are: psalms (including canticles), antiphons, responsories, hymns, lessons, little chapters, versicles and collects.
Psalms.
Before the 1911 reform, the multiplication of saints' festivals, with practically the same festal psalms, tended to repeat the about one-third of the Psalter, with a correspondingly rare recital of the remaining two-thirds. Following this reform, the entire Psalter is again generally recited each week, with the festal psalms restricted to only the highest-ranking feasts. As in the Greek usage and in the Benedictine, certain canticles like the Song of Moses (Exodus xv.), the Song of Hannah (1 Sam. ii.), the prayer of Habakkuk (iii.), the prayer of Hezekiah (Isaiah xxxviii.) and other similar Old Testament passages, and, from the New Testament, the Magnificat, the Benedictus and the Nunc dimittis, are admitted as psalms.
Antiphons.
The antiphons are short liturgical forms, sometimes of biblical, sometimes of patristic origin, used to introduce a psalm. The term originally signified a chant by alternate choirs, but has quite lost this meaning in the Breviary.
Responsories.
The responsories are similar in form to the antiphons, but come at the end of the psalm, being originally the reply of the choir or congregation to the precentor who recited the psalm.
Hymns.
The hymns are short poems going back in part to the days of Prudentius, Synesius, Gregory of Nazianzus and Ambrose (4th and 5th centuries), but mainly the work of medieval authors.
Lessons.
The lessons, as has been seen, are drawn variously from the Bible, the Acts of the Saints and the Fathers of the Church. In the primitive church, books afterwards excluded from the canon were often read, e.g. the letters of Clement of Rome and the Shepherd of Hermas. In later days the churches of Africa, having rich memorials of martyrdom, used them to supplement the reading of Scripture. Monastic influence accounts for the practice of adding to the reading of a biblical passage some patristic commentary or exposition. Books of homilies were compiled from the writings of SS. Augustine, Hilary, Athanasius, Isidore, Gregory the Great and others, and formed part of the library of which the Breviary was the ultimate compendium. In the lessons, as in the psalms, the order for special days breaks in upon the normal order of ferial offices and dislocates the scheme for consecutive reading. The lessons are read at Matins (which is subdivided into three nocturns).
Little chapters.
The little chapters are very short lessons read at the other "hours."
Versicles.
The versicles are short responsories used after the little chapters in the minor hours. They appear after the hymns in Lauds and Vespers.
Collects.
The collects come at the close of the office and are short prayers summing up the supplications of the congregation. They arise out of a primitive practice on the part of the bishop (local president), examples of which are found in the Didachē (Teaching of the Apostles) and in the letters of Clement of Rome and Cyprian. With the crystallization of church order improvisation in prayer largely gave place to set forms, and collections of prayers were made which later developed into Sacramentaries and Orationals. The collects of the Breviary are largely drawn from the Gelasian and other Sacramentaries, and they are used to sum up the dominant idea of the festival in connection with which they happen to be used.
Celebration.
Before 1910 the difficulty of harmonizing the "Proprium de Tempore" and the "Proprium Sanctorum", to which reference has been made, was only partly met in the thirty-seven chapters of general rubrics. Additional help was given by a kind of Catholic Churchman's Almanack, called the "Ordo Recitandi Divini Officii", published in different countries and dioceses, and giving, under every day, minute directions for proper reading. In 1960 John XXIII simplified the rubrics governing the Breviary in order to make it easier to use.
Every cleric in Holy Orders and many other members of religious orders must publicly join in or privately read aloud (i.e. using the lips as well as the eyes—it takes about two hours in this way) the whole of the Breviary services allotted for each day. In large churches where they were celebrated the services were usually grouped; e.g. Matins and Lauds (about 7.30 A.M.); Prime, Terce (High Mass), Sext, and None (about 10 A.M.); Vespers and Compline (4 P.M.); and from four to eight hours (depending on the amount of music and the number of high masses) are thus spent in choir. Lay use of the Breviary has varied throughout the Church's history. In some periods laymen did not use the Breviary as a manual of devotion to any great extent. The late Medieval period saw the recitation of certain hours of the Little Office of the Blessed Virgin, which was based on the Breviary in form and content, becoming popular among those who could read, and Bishop Challoner did much to popularise the hours of Sunday Vespers and Compline (albeit in English translation) in his 'Garden of the Soul' in the eighteenth century. The Liturgical Movement in the twentieth century saw renewed interest in the Offices of the Breviary and several popular editions were produced containing the vernacular as well as the Latin.
The complete pre-Pius X Roman Breviary was translated into English (by the Marquess of Bute in 1879; new ed. with a trans, of the Martyrology, 1908), French and German. Bute's version is noteworthy for its inclusion of the skilful renderings of the ancient hymns by J.H. Newman, J.M. Neale and others. Several editions of the Pius X Breviary were produced during the twentieth century, including a notable edition prepared with the assistance of the Sisters of Stanbrook Abbey in the 1950s. Two editions in English and Latin were produced in the mid-sixties, which conformed to the rubrics of 1960, published by Liturgical Press and Benziger in America. These used the Pius XII psalter. Baronius Press's revised edition of the Liturgical Press edition uses the older Gallican psalter of St. Jerome. This edition was published and released in 2012 for pre-orders only. In 2013, the publication has resumed printing and is available on Baronius web site.
Under Pope Benedict XVI's motu proprio Summorum Pontificum, Roman Catholic bishops, priests, and deacons are again permitted to use the 1962 edition of the Roman Breviary, promulgated by Pope John XXIII to satisfy their obligation to recite the Divine Office every day.
In 2008, an "i-breviary" was launched, which combines the ancient breviaries with the latest computer technology.

</doc>
<doc id="4866" url="https://en.wikipedia.org/wiki?curid=4866" title="Boomer">
Boomer

Boomer may refer to:

</doc>
<doc id="4868" url="https://en.wikipedia.org/wiki?curid=4868" title="B. F. Skinner">
B. F. Skinner

Burrhus Frederic Skinner (March 20, 1904 – August 18, 1990), commonly known as B. F. Skinner, was an American psychologist, behaviorist, author, inventor, and social philosopher. He was the Edgar Pierce Professor of Psychology at Harvard University from 1958 until his retirement in 1974.
Skinner considered free will an illusion and human action dependent on consequences of previous actions. If the consequences are bad, there is a high chance that the action will not be repeated; if the consequences are good, however, the actions that led to it will become more probable. Skinner called this the principle of reinforcement.
Skinner called the use of reinforcement to strengthen behavior operant conditioning, and he considered the rate of response to be the most effective measure of response strength. To study operant conditioning he invented the operant conditioning chamber, also known as the Skinner Box, and to measure rate he invented the "cumulative recorder". Using these tools he and C. B. Ferster produced his most influential experimental work, which appeared in the book Schedules of Reinforcement.
Skinner developed a philosophy of science that he called radical behaviorism, and founded a school of experimental research psychology—the experimental analysis of behavior. He imagined the application of his ideas to the design of a human community in his utopian novel" Walden Two", and his analysis of human behavior culminated in his work "Verbal Behavior".
Skinner was a prolific author who published 21 books and 180 articles. Contemporary academia considers Skinner a pioneer of modern behaviorism along with John B. Watson and Ivan Pavlov. A June 2002 survey listed Skinner as the most influential psychologist of the 20th century.
Biography.
Skinner was born in Susquehanna, Pennsylvania, to Grace and William Skinner. His father was a lawyer. He became an atheist after a Christian teacher tried to assuage his fear of the hell that his grandmother described. His brother Edward, two and a half years younger, died at age sixteen of a cerebral hemorrhage. He attended Hamilton College in New York with the intention of becoming a writer. He found himself at a social disadvantage at Hamilton College because of his intellectual attitude. While attending, he joined Lambda Chi Alpha Fraternity. He wrote for the school paper, but as an atheist, he was critical of the religious school he attended. After receiving his Bachelor of Arts in English literature in 1926, he attended Harvard University, where he would later research, teach, and eventually become a prestigious board member. While he was at Harvard a fellow student, Fred Keller, convinced Skinner that he could make an experimental science from the study of behavior. This led Skinner to invent his prototype for the Skinner Box and to join Keller in the creation of other tools for small experiments. After graduation, he unsuccessfully tried to write a great novel while he lived with his parents, a period that he later called the Dark Years. He became disillusioned with his literary skills despite encouragement from the renowned poet Robert Frost, concluding that he had little world experience and no strong personal perspective from which to write. His encounter with John B. Watson's "Behaviorism" led him into graduate study in psychology and to the development of his own version of behaviorism.
Skinner received a Ph.D. from Harvard in 1931, and remained there as a researcher until 1936. He then taught at the University of Minnesota at Minneapolis and later at Indiana University, where he was chair of the psychology department from 1946–1947, before returning to Harvard as a tenured professor in 1948. He remained at Harvard for the rest of his life. In 1973 Skinner was one of the signers of the Humanist Manifesto II.
In 1936, Skinner married Yvonne (Eve) Blue. The couple had two daughters, Julie (m. Vargas) and Deborah (m. Buzan). He died of leukemia on August 18, 1990, and is buried in Mount Auburn Cemetery, Cambridge, Massachusetts. Skinner continued to write and work until just before his death. Just a few days before his death, he was given a lifetime achievement award by the American Psychological Association and delivered a 15-minute address concerning his work.
A controversial figure, Skinner has been depicted in many different ways. He has been widely revered for bringing a much-needed scientific approach to the study of human behavior; he has also been vilified for attempting to apply findings based largely on animal experiments to human behavior in real-life settings.
Contributions to psychological theory.
Behaviorism.
Skinner called his approach to the study of behavior radical behaviorism. This philosophy of behavioral science assumes that behavior is a consequence of environmental histories of reinforcement, (see Applied behavior analysis). In contrast to the approach of cognitive science, behaviorism does not accept private events such as thinking, perceptions, and unobservable emotions as causes of an organism's behavior. However, in contrast to methodological behaviorism, Skinner's radical behaviorism did accept thoughts, emotions, and other "private events" as responses subject to the same rules as overt behavior. In his words: 
In this way we repair the major damage wrought by mentalism. When what a person does attributed to what is going on inside him, investigation is brought to an end. Why explain the explanation? For twenty five hundred years people have been preoccupied with feelings and mental life, but only recently has any interest been shown in a more precise analysis of the role of the environment. Ignorance of that role led in the first place to mental fictions, and it has been perpetuated by the explanatory practices to which they gave rise.
Theoretical structure.
Skinner's behavioral theory was largely set forth in his first book, "Behavior of Organisms". Here he gave a systematic description of the manner in which environmental variables control behavior. He distinguished two sorts of behavior, which are controlled in different ways. First "respondent" behaviors, which are elicited by stimuli. These may be modified through respondent conditioning, which is often called "Pavlovian conditioning" or "classical conditioning", in which a neutral stimulus is paired with an eliciting stimulus. "Operant" behaviors, in contrast, are "emitted", meaning that initially they are not induced by any particular stimulus. They are strengthened through operant conditioning, sometimes called "instrumental conditioning", in which the occurrence of a response yields a reinforcer. Respondents might be measured by their latency or strength, operants by their rate. Both of these sorts of behavior had already been studied experimentally, for example, respondents by Pavlov and operants by Thorndike. Skinner's account differed in some ways from earlier ones, and was one of the first accounts to bring them under one roof.
The idea that behavior is strengthened or weakened by its consequences raises several questions. Among the most important are these: (1) Operant responses are strengthened by reinforcement, but where do they come from in the first place? (2) Once it is in the organism's repertoire, how is a response directed or controlled? (3) How can very complex and seemingly novel behaviors be explained?
Origin of operant behavior.
Skinner's answer to the first question was very much like Darwin's answer to the question of the origin of a "new" bodily structure, namely, variation and selection. Similarly, the behavior of an individual varies from moment to moment; a variation that is followed by reinforcement is strengthened and becomes prominent in that individual's behavioral repertoire. "Shaping" was Skinner's term for the gradual modification of behavior by the reinforcement of desired variations. As discussed later in this article, Skinner believed that "superstitious" behavior can arise when a response happens to be followed by reinforcement to which it is actually unrelated.
Control of operant behavior.
The second question, "how is operant behavior controlled?" arises because, to begin with, the behavior is "emitted" without reference to any particular stimulus. Skinner answered this question by saying that a stimulus comes to control an operant if it is present when the response is reinforced and absent when it is not. For example, if lever-pressing only brings food when a light is on, a rat, or a child, will learn to press the lever only when the light is on. Skinner summarized this relationship by saying that a discriminative stimulus (e.g. light) sets the occasion for the reinforcement (food) of the operant (lever-press). This "three-term contingency" (stimulus-response-reinforcer) is one of Skinner's most important concepts, and sets his theory apart from theories that use only pair-wise associations.
Explaining complex behavior.
Most behavior of humans cannot easily be described in terms of individual responses reinforced one by one, and Skinner devoted a great deal of effort to the problem of behavioral complexity. Some complex behavior can be seen as a sequence of relatively simple responses, and here Skinner invoked the idea of "chaining". Chaining is based on the fact, experimentally demonstrated, that a discriminative stimulus not only sets the occasion for subsequent behavior, but it can also reinforce a behavior that precedes it. That is, a discriminative stimulus is also a "conditioned reinforcer". For example, the light that sets the occasion for lever pressing may be used to reinforce "turning around" in the presence of a noise. This results in the sequence "noise - turn-around - light - press lever - food". Much longer chains can be built by adding more stimuli and responses.
However, Skinner recognized that a great deal of behavior, especially human behavior, cannot be accounted for by gradual shaping or the construction of response sequences. Complex behavior often appears suddenly in its final form, as when a person first finds his way to the elevator by following instructions given at the front desk. To account for such behavior Skinner introduced the concept of rule-governed behavior. First, relatively simple behaviors come under the control of verbal stimuli: the child learns to "jump", "open the book", and so on. After a large number of responses come under such verbal control, a sequence of verbal stimuli can evoke an almost unlimited variety of complex responses.
Reinforcement.
Reinforcement, a key concept of behaviorism, is the primary process that shapes and controls behavior, and occurs in two ways, "positive" and "negative". In The Behavior of Organisms (1938), Skinner defined "negative reinforcement" to be synonymous with punishment, that is, the presentation of an aversive stimulus. Subsequently, in Science and Human Behavior (1953), Skinner redefined negative reinforcement. In what has now become the standard set of definitions, positive reinforcement is the strengthening of behavior by the occurrence of some event (e.g., praise after some behavior is performed), whereas negative reinforcement is the strengthening of behavior by the removal or avoidance of some aversive event (e.g., opening and raising an umbrella over your head on a rainy day is reinforced by the cessation of rain falling on you).
Both types of reinforcement strengthen behavior, or increase the probability of a behavior reoccurring; the difference is in whether the reinforcing event is something applied (positive reinforcement) or something removed or avoided (negative reinforcement). Punishment is the application of an aversive stimulus/event (positive punishment or punishment by contingent stimulation) or the removal of a desirable stimulus (negative punishment or punishment by contingent withdrawal). Though punishment is often used to suppress behavior, Skinner argued that this suppression is temporary and has a number of other, often unwanted, consequences. Extinction is the absence of a rewarding stimulus, which weakens behavior.
Writing in 1981, Skinner pointed out that Darwinian natural selection is, like reinforced behavior, "selection by consequences". Though, as he said, natural selection has now "made its case", he regretted that essentially the same process, "reinforcement" was less widely accepted as underlying human behavior.
Schedules of reinforcement.
Skinner recognized that behavior is typically reinforced more than once, and, together with C. B. Ferster, he did an extensive analysis of the various ways in which reinforcements could be arranged over time, which he called "schedules of reinforcement".
The most notable schedules of reinforcement studied by Skinner were continuous, interval (fixed or variable) and ratio (fixed or variable). All are methods used in operant conditioning.
Scientific inventions.
Operant conditioning chamber.
An operant conditioning chamber (also known as a "Skinner Box") is a laboratory apparatus used in the experimental analysis of animal behavior. It was invented by Skinner while he was a graduate student at Harvard University, where he received the doctorate in 1931. As used by Skinner, the box had a lever (for rats) or a disk in one wall (for pigeons). A press on this "manipulandum" could deliver food to the animal through an opening in the wall, and responses reinforced in this way increased in frequency. By controlling this reinforcement together with discriminative stimuli such as lights and tones, or punishments such as electric shocks, experimenters have used the operant box to study a wide variety of topics, including schedules of reinforcement, discriminative control, delayed response ("memory"), punishment, and so on. By channeling research in these directions, the operant conditioning chamber has had a huge influence on course of research in animal learning and its applications. It enabled great progress on problems that could be studied by the measuring the rate, probability, or force of a simple, repeatable response. However, it discouraged the study of behavioral processes not easily conceptualized in such terms - spatial learning, in particular, which is now studied in quite different ways, for example by the use of the water maze.
Cumulative recorder.
The cumulative recorder makes a pen-and-ink record of simple repeated responses. Skinner designed it for use with the Operant chamber as a convenient way to record and view the rate of responses such as a lever press or a key peck. In this device a sheet of paper gradually unrolls over a cylinder. Each response steps a small pen across the paper, starting at one edge; when the pen reaches the other edge, it quickly resets to the initial side. The slope of the resulting ink line graphically displays the rate of the response; for example rapid responses yield a steeply sloping line on the paper, slow responding yields a line of low slope. The cumulative recorder was a key tool used by Skinner in his analysis of behavior, and it was very widely adopted by other experimenters, gradually falling out of use with the advent of the laboratory computer. Skinner's major experimental exploration of response rates, presented in his book with C. B. Ferster, "Schedules of Reinforcement", is full of cumulative records produced by this device.
Air crib.
The air crib is an easily cleaned, temperature and humidity-controlled enclosure intended to replace the standard infant crib. 
Skinner invented the device to help his wife cope with the day-to-day tasks of child rearing. It was designed to make early childcare simpler (by reducing laundry, diaper rash, cradle cap, etc.), while allowing the baby to be more mobile and comfortable, and less prone to cry. Reportedly it had some success in these goals.
The air crib was a controversial invention. It was popularly mischaracterized as a cruel pen, and it was often compared to Skinner's operant conditioning chamber, commonly called the "Skinner Box". This association with laboratory animal experimentation discouraged its commercial success, though several companies attempted production.
A 2004 book by Lauren Slater, entitled "" caused a stir by mentioning the rumors that Skinner had used his baby daughter Deborah in some of his experiments and that she had subsequently committed suicide. Although Slater's book stated that the rumors were false, a reviewer in "The Observer" in March 2004 misquoted Slater's book as supporting the rumors. This review was read by Deborah Skinner (now Deborah Buzan, an artist and writer living in London) who wrote a vehement riposte in "The Guardian".
Teaching machine.
The teaching machine was a mechanical device whose purpose was to administer a curriculum of programmed learning. The machine embodies key elements of Skinner’s theory of learning and had important implications for education in general and classroom instruction in particular.
In one incarnation, the machine was a box that housed a list of questions that could be viewed one at a time through a small window. (See picture). There was also a mechanism through which the learner could respond to each question. Upon delivering a correct answer, the learner would be rewarded.
Skinner advocated the use of teaching machines for a broad range of students (e.g., preschool aged to adult) and instructional purposes (e.g., reading and music). For example, one machine that he envisioned could teach rhythm. He wrote:
The instructional potential of the teaching machine stemmed from several factors: it provided automatic, immediate and regular reinforcement without the use of aversive control; the material presented was coherent, yet varied and novel; the pace of learning could be adjusted to suit the individual. As a result, students were interested, attentive, and learned efficiently by producing the desired behavior, "learning by doing".
Teaching machines, though perhaps rudimentary, were not rigid instruments of instruction. They could be adjusted and improved based upon the students’ performance. For example, if a student made many incorrect responses, the machine could be reprogrammed to provide less advanced prompts or questions- the idea being that students acquire behaviors most efficiently if they make few errors. Multiple-choice formats were not best suited for teaching machines because they tended to increase student mistakes and the contingencies of reinforcement were relatively uncontrolled.
Not only useful in teaching explicit skills, machines could also promote the development of a repertoire of behaviors that Skinner called self-management. Effective self-management means attending to stimuli appropriate to a task, avoiding distractions, reducing the opportunity of reward for competing behaviors, and so on. For example, machines encourage students to pay attention before receiving a reward. Skinner contrasted this with the common classroom practice of initially capturing students’ attention (e.g., with a lively video) and delivering a reward (e.g., entertainment) before the students have actually performed any relevant behavior. This practice fails to reinforce correct behavior and actually counters the development of self-management.
Skinner pioneered the use of teaching machines in the classroom, especially at the primary level. Today computers run software that performs similar teaching tasks, and there has been a resurgence of interest in the topic related to the development of adaptive learning systems.
Pigeon-guided missile.
During World War II the US Navy required a weapon effective against surface ships such the German "Bismarck" class battleships. Although missile and TV technology existed, the size of the primitive guidance systems available rendered automatic guidance impractical. To solve this problem Skinner initiated Project Pigeon which was intended to provide a simple and effective guidance system. This system divided the nose cone of a missile into three compartments, putting a pigeon in each. Lenses projected an image of distant objects onto a screen in front of each bird. Thus, when the missile was launched from an aircraft within sight of an enemy ship, an image of the ship would appear on the screen. The screen was hinged such that pecks at the image of the ship would guide the missile toward the ship.
Despite an effective demonstration the project was abandoned, and eventually more conventional solutions, such as those based on radar, became available. Skinner complained that "our problem was no one would take us seriously."
It seemed that few people would trust pigeons to guide a missile no matter how reliable the system appeared to be.
Verbal summator.
Early in his career Skinner became interested in "latent speech" and experimented with a device he called the "verbal summator". This device can be thought of as an auditory version of the Rorschach inkblots. When using the device, human participants listened to incomprehensible auditory "garbage" but often read meaning into what they heard. Thus, as with the Rorschach blots, the device was intended to yield overt behavior that projected subconscious thoughts. Skinner's interest in projective testing was brief, but he later used observations with the summator in creating his theory of verbal behavior. The device also led other researchers to invent new tests such as the tautophone test, the auditory apperception test, and the Azzageddi test.
"Verbal Behavior".
Challenged by Alfred North Whitehead during a casual discussion while at Harvard to provide an account of a randomly provided piece of verbal behavior, Skinner set about attempting to extend his then-new functional, inductive, approach to the complexity of human verbal behavior. Developed over two decades, his work appeared in the book "Verbal Behavior". Although Noam Chomsky was highly critical of "Verbal Behavior", he conceded that Skinner's "S-R psychology" was worth a review. (Behavior analyists reject the "S-R" characterization: operant conditioning involves the emission of a response which then becomes more or less likely dependending upon its consequence - see above.).
"Verbal Behavior" had an uncharacteristically cool reception, partly as a result of Chomsky's review, partly because of Skinner's failure to address or rebut any of Chomsky's criticisms. Skinner's peers may have been slow to adopt the ideas presented in "Verbal Behavior" because of the absence of experimental evidence — unlike the empirical density that marked Skinner's experimental work. However, in applied settings there has been a resurgence of interest in Skinner's functional analysis of verbal behavior.
Influence on education.
Skinner's views influenced education as well as psychology. Skinner argued that education has two major purposes: (1) to teach repertoires of both verbal and nonverbal behavior; and (2) to interest students in learning. He recommended bringing students’ behavior under appropriate control by providing reinforcement only in the presence of stimuli relevant to the learning task. Because he believed that human behavior can be affected by small consequences, something as simple as "the opportunity to move forward after completing one stage of an activity" can be an effective reinforcer (Skinner, 1961, p. 380). Skinner was convinced that, to learn, a student must engage in behavior, and not just passively receive information. (Skinner, 1961, p. 389).
Skinner believed that effective teaching must be based on positive reinforcement which is, he argued, more effective at changing and establishing behavior than punishment. He suggested that the main thing people learn from being punished is how to avoid punishment. For example, if a child is forced to practice playing an instrument, the child comes to associate practicing with punishment and thus learns to hate and avoid practicing the instrument. This view had obvious implications for the then widespread practice of rote learning and punitive discipline in education. The use of educational activities as punishment may induce rebellious behavior such as vandalism or absence.
Because teachers are primarily responsible for modifying student behavior, Skinner argued that teachers must learn effective ways of teaching. In "The Technology of Teaching", Skinner has a chapter on why teachers fail (pages 93–113): He says that teachers have not been given an in-depth understanding of teaching and learning. Without knowing the science underpinning teaching, teachers fall back on procedures that work poorly or not at all, such as:
Skinner suggests that any age-appropriate skill can be taught. The steps are
Skinner's views on education are extensively presented in his book "The Technology of Teaching". They are also reflected in Fred S. Keller's "Personalized System of Instruction" and Ogden R. Lindsley's "Precision Teaching".
"Walden Two" and "Beyond Freedom and Dignity".
Skinner is popularly known mainly for his books "Walden Two" and "Beyond Freedom and Dignity" for which he made the cover of "TIME" Magazine. The former describes a fictional "experimental community" in 1940s United States. The productivity and happiness of citizens in this community is far greater than in the outside world because the residents practice scientific social planning and use operant conditioning in raising their children.
"Walden Two", like Thoreau's "Walden", champions a lifestyle that does not support war or foster competition and social strife. It encourages a lifestyle of minimal consumption, rich social relationships, personal happiness, satisfying work and leisure. In 1967, Kat Kinkade founded the Twin Oaks Community, using Walden Two as a blueprint. The community still exists and continues to use the Planner-Manager system and other aspects described in Skinner's book.
In "Beyond Freedom and Dignity", Skinner suggests that a technology of behavior could help to make a better society. We would, however, have to accept that an autonomous agent is not the driving force of our actions. Skinner offers alternatives to punishment and challenges his readers to use science and modern technology to construct a better society.
Political views.
Skinner's political writings emphasized his hopes that an effective and human science of behavioral control – a technology of human behavior – could help with problems as yet unsolved and often aggravated by advances in technology such as the atomic bomb. Indeed, one of Skinner's goals was to prevent humanity from destroying itself. He saw political activity as the use of aversive or non-aversive means to control a population. Skinner favored the use of positive reinforcement as a means of control, citing Jean-Jacques Rousseau's novel "" as an example of literature that "did not fear the power of positive reinforcement". The science of human behavior is used to eliminate poverty, sexual expression, government as we know it, create a lifestyle without that such as war.
Skinner's book, "Walden Two", presents a vision of a decentralized, localized society, which applies a practical, scientific approach and behavioral expertise to deal peacefully with social problems. (For example, his views led him to oppose corporal punishment in schools, and he wrote a letter to the California Senate that helped lead it to a ban on spanking.) Skinner's utopia is both a thought experiment and a rhetorical piece. In his book, Skinner answers the problem that exists in many utopian novels – "What is the Good Life?" In "Walden Two", the answer is a life of friendship, health, art, a healthy balance between work and leisure, a minimum of unpleasantness, and a feeling that one has made worthwhile contributions to a society in which resources are ensured, in part, by minimizing consumption. 
Skinner described his novel as "my New Atlantis", in reference to Bacon's utopia. 
Superstition in the pigeon.
One of Skinner's experiments examined the formation of superstition in one of his favorite experimental animals, the pigeon. Skinner placed a series of hungry pigeons in a cage attached to an automatic mechanism that delivered food to the pigeon "at regular intervals with no reference whatsoever to the bird's behavior". He discovered that the pigeons associated the delivery of the food with whatever chance actions they had been performing as it was delivered, and that they subsequently continued to perform these same actions.
Skinner suggested that the pigeons behaved as if they were influencing the automatic mechanism with their "rituals" and that this experiment shed light on human behavior:
Modern behavioral psychologists have disputed Skinner's "superstition" explanation for the behaviors he recorded. Subsequent research (e.g. Staddon and Simmelhag, 1971), while finding similar behavior, failed to find support for Skinner's "adventitious reinforcement" explanation for it. By looking at the timing of different behaviors within the interval, Staddon and Simmelhag were able to distinguish two classes of behavior: the "terminal response", which occurred in anticipation of food, and "interim responses", that occurred earlier in the interfood interval and were rarely contiguous with food. Terminal responses seem to reflect classical (as opposed to operant) conditioning, rather than adventitious reinforcement, guided by a process like that observed in 1968 by Brown and Jenkins in their "autoshaping" procedures. The causation of interim activities (such as the schedule-induced polydipsia seen in a similar situation with rats) also cannot be traced to adventitious reinforcement and its details are still obscure (Staddon, 1977).
This experiment was also repeated on humans, in a less controlled manner, on the popular British TV series "Trick or Treat", leading to similar conclusions to those of Skinner.
Quotations.
"I do not admire myself as a person. My successes do not override my shortcomings."
"Ethical control may survive in small groups, but the control of the population as a whole must be delegated to specialists—to police, priests, owners, teachers, therapists, and so on, with their specialized reinforcers and their codified contingencies."
"It is a mistake to suppose that the whole issue is how to free man. The issue is to improve the way in which he is controlled."
"Education is what survives when what has been learned has been forgotten."
"As the senses grow dull, the stimulating environment becomes less clear. When reinforcing consequences no longer follow, we are bored, discouraged and depressed."
Criticism.
J. E. R. Staddon.
As understood by Skinner, ascribing "dignity" to individuals involves giving them credit for their actions. To say "Skinner is brilliant" means that Skinner is an originating force. If Skinner's determinist theory is right, he is merely the focus of his environment. He is not an originating force and he had no choice in saying the things he said or doing the things he did. Skinner's environment and genetics both allowed and compelled him to write his book. Similarly, the environment and genetic potentials of the advocates of freedom and dignity cause them to resist the reality that their own activities are deterministically grounded. J. E. R. Staddon ("The New Behaviorism, 2nd Edition", 2014) has argued the compatibilist position; Skinner's determinism is not in any way contradictory to traditional notions of reward and punishment, as he believed.
Noam Chomsky.
Perhaps Skinner's best known critic, Noam Chomsky published a review of Skinner's "Verbal Behavior" two years after it was published. Chomsky argued that Skinner's attempt to use behaviorism to explain human language amounted to little more than word games. Conditioned responses could not account for a child's ability to create or understand an infinite variety of novel sentences. The 1959 review became better known than the book itself. Chomsky's review has been credited with launching the cognitive movement in psychology and other disciplines. Skinner, who rarely responded directly to critics, never formally replied to Chomsky's critique. Many years later, Kenneth MacCorquodale's reply was endorsed by Skinner.
Chomsky also reviewed Skinner's "Beyond Freedom and Dignity", using the same basic motives as his "Verbal Behavior" review. Among Chomsky's criticisms were that Skinner's laboratory work could not be extended to humans, that when it was extended to humans it represented 'scientistic' behavior attempting to emulate science but which was not scientific, that Skinner was not a scientist because he rejected the hypothetico-deductive model of theory testing, and that Skinner had no science of behavior.
Psychodynamic psychology.
Skinner has been repeatedly criticized for his supposed animosity towards Freud, psychoanalysis, and psychodynamic psychology. There is clear evidence, however, that Skinner shared several of Freud's assumptions, and that he was influenced by Freudian points of view in more than one field, among them the analysis of defense mechanisms, such as repression. To study such phenomena, Skinner even designed his own projective test, the "verbal summator" described above.
List of awards and positions.
Honorary degrees.
Skinner received honorary degrees from:
In popular culture.
Writer of "The Simpsons" Jon Vitti named Principal Skinner character after behavioral psychologist B. F. Skinner.

</doc>
<doc id="4869" url="https://en.wikipedia.org/wiki?curid=4869" title="Bill">
Bill

Bill may refer to:

</doc>
<doc id="4870" url="https://en.wikipedia.org/wiki?curid=4870" title="Bill Macy">
Bill Macy

Bill Macy (born Wolf Martin Garber; May 18, 1922) is an American television, film and stage actor, born in Revere, Massachusetts, to Mollie (née Friedopfer) and Michael Garber, a manufacturer. He was raised in Brooklyn, New York, and worked as a cab driver before pursuing acting.
Macy is best known for playing Walter Findlay, the long-suffering husband of the title character on the 1970s television situation comedy "Maude", starring Beatrice Arthur. He also was an original cast member of the long-running theatrical revue "Oh! Calcutta!" He has made more than 70 appearances on film and television, including a memorable role as the co-inventor of the 'Opti-grab' in the 1979 Steve Martin comedy "The Jerk", and as the head television writer in "My Favorite Year" (1982). His other film credits have included roles in "Death at Love House" (1976), "The Late Show" (1976), "Serial" (1980), "Movers & Shakers" (1985), "Bad Medicine" (1985), (1986) Tales from the darkside "Sibling Rivalry" (1990), "The Doctor" (1991), "Me, Myself and I" (1992), "Analyze This" (1999), "Surviving Christmas" (2004), and "The Holiday" (2006).
He appeared in the popular television movie "Perry Mason & The Case Of The Murdered Madame" (1987) as banker Richard Wilson. 
He appeared occasionally on "Seinfeld" as one of the residents of the Florida retirement community in which Jerry Seinfeld's parents lived. He also appeared on the short-lived sitcom "Back to You". He made a guest appearance as a patient on "Chicago Hope" and as an aging gambler on the series "Las Vegas". In 2006 he made an appearance on "My Name is Earl" in the second-season episode, "Van Hickey" as an elderly patient in a nursing home who claims he "once tongue-kissed a Jamaican woman."

</doc>
