<doc id="5197" url="https://en.wikipedia.org/wiki?curid=5197" title="Transportation in Canada">
Transportation in Canada

Transportation in Canada, the world's second-largest country in total area, is dedicated to having an efficient, high-capacity multimodal transport spanning often vast distances between natural resource extraction sites, agricultural and urban areas. Canada's transportation system includes more than of roads, 10 major international airports, 300 smaller airports, of functioning railway track, and more than 300 commercial ports and harbours that provide access to the Pacific, Atlantic and Arctic oceans as well as the Great Lakes and the St. Lawrence Seaway. In 2005, the transportation sector made up 4.2% of Canada's GDP, compared to 3.7% for Canada's mining and oil and gas extraction industries.
Transport Canada oversees and regulates most aspects of transportation within federal jurisdiction, including interprovincial transport. This primarily includes rail, air and maritime transportation. Transport Canada is under the direction of the federal government's Minister of Transport. The Transportation Safety Board of Canada is responsible for maintaining transportation safety in Canada by investigating accidents and making safety recommendations.
Roads.
There is a total of of roads in Canada, of which are paved, including of expressways (the third-longest in the world, behind the Interstate Highway System of the United States and the China's National Trunk Highway System). As of 2008, were unpaved.
In 2009, there were 20,706,616 road vehicles registered in Canada, of which 96% were vehicles under , 2.4% were vehicles between tonnes and 1.6% were or greater. These vehicles travelled a total of 333.29 billion kilometres, of which 303.6 billion was for vehicles under , 8.3 billion was for vehicles between and 21.4 billion was for vehicles over . For the trucks, 88.9% of vehicle-kilometres were intra-province trips, 4.9% were inter-province, 2.8% were between Canada and the US and 3.4% made outside of Canada. For trucks over , 59.1% of vehicle-kilometres were intra-province trips, 20% inter-province trips, 13.8% Canada-US trips and 7.1% trips made outside of Canada.
Canada's vehicles consumed a total of of gasoline and of diesel. Trucking generated 35% of the total GDP from transport, compared to 25% for rail, water and air combined (the remainder being generated by the industry's transit, pipeline, scenic and support activities). Hence roads are the dominant means of passenger and freight transport in Canada.
Roads and highways were managed by provincial and municipal authorities until construction of the Northwest Highway System (the Alaska Highway) and the Trans-Canada Highway project initiation. The Alaska Highway of 1942 was constructed during World War II for military purposes connecting Fort St. John, British Columbia with Fairbanks, Alaska. The transcontinental highway, a joint national and provincial expenditure, was begun in 1949 under the initiation of the Trans Canada Highway Act on December 10, 1949. The highway was completed in 1962 at a total expenditure of $1.4 billion.
Internationally, Canada has road links with both the lower 48 US states and Alaska. The Ministry of Transportation maintains the road network in Ontario and also employs Ministry of Transport Enforcement Officers for the purpose of administering the Canada Transportation Act and related regulations. The Department of Transportation in New Brunswick performs a similar task in that province as well.
Regulations enacted in regards to Canada highways are the 1971 Motor Vehicle Safety Act and the 1990 Highway Traffic Act
The safety of Canada's roads is moderately good by international standards, and is improving both in terms of accidents per head of population and per billion vehicle kilometers.
Air transport.
Air transportation made up 9% of the transport sector's GDP generation in 2005. Canada's largest air carrier and its flag carrier is Air Canada, which had 34 million customers in 2006 and, as of April 2010, operates 363 aircraft (including Air Canada Jazz). CHC Helicopter, the largest commercial helicopter operator in the world, is second with 142 aircraft and WestJet, a low-cost carrier formed in 1996, is third with 100 aircraft. Canada's airline industry saw significant change following the signing of the US-Canada open skies agreement in 1995, when the marketplace became less regulated and more competitive.
The Canadian Transportation Agency employs transportation enforcement officers to maintain aircraft safety standards, and conduct periodic aircraft inspections, of all air carriers. The Canadian Air Transport Security Authority is charged with the responsibility for the security of air traffic within Canada. In 1994 the National Airports Policy was enacted
Principal airports.
Of over 1,800 registered Canadian aerodromes, certified airports, heliports, and floatplane bases, 26 are specially designated under Canada's National Airports System (NAS): these include all airports that handle 200,000 or more passengers each year, as well as the principal airport serving each federal, provincial, and territorial capital. However, since the introduction of the policy only one, Iqaluit Airport, has been added and no airports have been removed despite dropping below 200,000 passengers. The Government of Canada, with the exception of the three territorial capitals, retains ownership of these airports and leases them to local authorities. The next tier consists of 64 regional/local airports formerly owned by the federal government, most of which have now been transferred to other owners (most often to municipalities).
Below is a table of Canada's ten biggest airports by passenger traffic in 2011.
Railways.
In 2007, Canada had a total of of freight and passenger railway, of which is electrified. While intercity passenger transportation by rail is now very limited, freight transport by rail remains common. Total revenues of rail services in 2006 was $10.4 billion, of which only 2.8% was from passenger services. The Canadian National and Canadian Pacific Railway are Canada's two major freight railway companies, each having operations throughout North America. In 2007, 357 billion tonne-kilometres of freight were transported by rail, and 4.33 million passengers travelled 1.44 billion passenger-kilometres (an almost negligible amount compared to the 491 billion passenger-kilometres made in light road vehicles). 34,281 people were employed by the rail industry in the same year.
Nationwide passenger services are provided by the federal crown corporation Via Rail. Three Canadian cities have commuter rail services: in the Montreal area by AMT, in the Toronto area by GO Transit, and in the Vancouver area by West Coast Express. Smaller railways such as Ontario Northland, Rocky Mountaineer, and Algoma Central also run passenger trains to remote rural areas.
In Canada railways are served by standard gauge, , rails. See also track gauge in Canada.
Canada has railway links with the lower 48 US States, but no connection with Alaska other than a train ferry service from Prince Rupert, British Columbia, although a line has been proposed. There are no other international rail connections.
Waterways.
In 2005, 139.2 million tonnes of cargo was loaded and unloaded at Canadian ports. The Port of Vancouver is the busiest port in Canada, moving 68 million tonnes or 15% of Canada's total in domestic and international shipping in 2003.
Transport Canada oversees most of the regulatory functions related to marine registration, safety of large vessel, and port pilotage duties. Many of Canada's port facilities are in the process of being divested from federal responsibility to other agencies or municipalities.
Inland waterways comprise , including the St. Lawrence Seaway. Transport Canada enforces acts and regulations governing water transportation and safety.
Canals.
The St. Lawrence waterway was at one time the world's greatest inland water navigation system. The main route canals of Canada are those of the St. Lawrence River and the Great Lakes. The others are subsidiary canals.
Ports and harbours.
The National Harbours Board administered Halifax, Saint John, Chicoutimi, Trois-Rivières, Churchill, and Vancouver until 1983. At one time, over 300 harbours across Canada were supervised by the Department of Transport. A program of divestiture was implemented around the turn of the millennium, and as of 2014, 493 of the 549 sites identified for divestiture in 1995 have been sold or otherwise transferred, as indicated by a DoT list. The government maintains an active divestiture programme, and after divestiture Transport Canada oversees only 17 Canada Port Authorities for the 17 largest shipping ports.
Merchant marine.
Canada's merchant marine comprised a "total" of 173 ships ( or over) or at the end of 2007.
Pipelines.
Pipelines are part of the energy extraction and transportation network of Canada and are used to transport natural gas, natural gas liquids, crude oil, synthetic crude and other petroleum based products. Canada has of pipeline for transportation of crude and refined oil, and for liquefied petroleum gas.
Public transit.
Most Canadian cities have public transport, if only a bus system. Three Canadian cities have rapid transit systems, four have light rail systems, and three have commuter rail systems (see below). In 2006, 11% of Canadians used public transportation to get to work. This compares to 80.0% that got to work using a car (72.3% by driving, 7.7% as a passenger), 6.4% that walked and 1.3% that rode a bike.
Rapid rail systems.
There are three rapid transit systems operating in Canada: the Montreal Metro, the Toronto Subway, and the Vancouver SkyTrain.
There is also an Airport Circulator, the LINK Train at Toronto Pearson International Airport. It operates 24 hours a day, 7 days a week and is wheelchair-accessible. It is free of cost.
Light rail systems.
Three cities have light rail systems—the Calgary CTrain, the Edmonton Light Rail Transit, and the O-Train in Ottawa—and Toronto has an extensive streetcar system.
Commuter train systems.
Commuter trains serve the cities and surrounding areas of Montreal, Toronto and Vancouver:
History.
The standard history covers the French regime, fur traders, the canals, and early roads, and gives extensive attention to the railways.
European contact.
Aboriginal peoples in Canada walked. They also used canoes, kayaks, umiaks and Bull Boats, in addition to the snowshoe, toboggan and sled in winter. They had no wheeled vehicles, and no animals larger than dogs.
Europeans adopted canoes as they pushed deeper into the continent's interior, and were thus able to travel via the waterways that fed from the St. Lawrence River and Hudson Bay.
In the 19th century and early 20th century transportation relied on harnessing oxen to "Red River ox carts" or horse to waggon. Maritime transportation was via manual labour such as canoe or wind on sail. Water or land travel speeds was approximately .
Settlement was along river routes. Agricultural commodities were perishable, and trade centres were within . Rural areas centred around villages, and they were approximately apart. The advent of steam railways and steamships connected resources and markets of vast distances in the late 19th century. Railways also connected city centres, in such a way that the traveller went by sleeper, railway hotel, to the cities. Crossing the country by train took four or five days, as it still does by car. People generally lived within of the downtown core thus the train could be used for inter-city travel and the tram for commuting.
The advent of the interstate or Trans-Canada Highway in Canada in 1963 established ribbon development, truck stops, and industrial corridors along throughways.
Evolution.
The Federal Department of Transport (established 2 November 1936) supervised railways, canals, harbours, marine and shipping, civil aviation, radio and meteorology. The Transportation Act of 1938 and the amended Railway Act, placed control and regulation of carriers in the hands of the Board of Transport commissioners for Canada. The Royal Commission on Transportation was formed 29 December 1948, to examine transportation services to all areas of Canada to eliminate economic or geographic disadvantages. The Commission also reviewed the Railway Act to provide uniform yet competitive freight-rates.

</doc>
<doc id="5199" url="https://en.wikipedia.org/wiki?curid=5199" title="Canada–United States relations">
Canada–United States relations

Relations between Canada and the United States of America have spanned more than two centuries. This includes a shared British cultural heritage, and the development of one of the most stable and mutually-beneficial international relationships in the world. Each is the other's chief economic partner. Tourism and migration between the two nations have increased rapport. The U.S. is ten times larger in population and has dominant cultural and economic influence. Starting with the American Revolution, a vocal element in Canada has warned against US dominance or annexation. The War of 1812 saw invasions across the border. In 1815, the war ended with the border unchanged and demilitarized, as were the Great Lakes. Furthermore, the British ceased aiding First Nation attacks on American territory, and the United States never again attempted to invade Canada. Apart from minor raids, it has remained peaceful.
As Britain decided to disengage, fears of an American takeover played a role in the formation of the Dominion of Canada (1867), and Canada's rejection of free trade (1911). Military collaboration was close during World War II and continued throughout the Cold War, bilaterally through NORAD and multilaterally through NATO. A very high volume of trade and migration continues between the two nations, as well as a heavy overlapping of popular and elite culture, a dynamic which has generated closer ties, especially after the signing of the Canada–United States Free Trade Agreement in 1988.
Canada and the United States are currently the world's largest bilateral trading partners. The two nations have the world's longest shared border (), and also have significant interoperability within the defence sphere. Recent difficulties have included repeated trade disputes, environmental concerns, Canadian concern for the future of oil exports, and issues of illegal immigration and the threat of terrorism. Nevertheless, trade has continued to expand, especially following the 1988 FTA and North American Free Trade Agreement (NAFTA) in 1994 which has further merged the two economies.
The foreign policies of the neighbours have been closely aligned since the Cold War. However, Canada has disagreed with American policies regarding the Vietnam War, the status of Cuba, the Iraq War, Missile Defense, and the War on Terrorism. A diplomatic debate has been underway in recent years on whether the Northwest Passage is in international waters or under Canadian sovereignty.
According to Gallup's annual public opinion polls, Canada has consistently been Americans' favorite nation, with 96% of Americans viewing Canada favorably in 2012. According to a 2013 BBC World Service Poll, 84% of Americans view their northern neighbor's influence positively, with only 5% expressing a negative view, the most favorable perception of Canada in the world. As of spring 2013, 64% of Canadians had a favorable view of the U.S. and 81% expressed confidence in Obama to do the right thing in international matters. According to the same poll, 30% viewed the U.S. negatively. Also, according to a 2014 BBC World Service Poll, 86% of Americans view Canada's influence positively, with only 5% expressing a negative view. However, according to the same poll, 43% of Canadians view U.S. influence positively, with 52% expressing a negative view.
History.
Colonial wars.
Before the British conquest of French Canada in 1760, there had been a series of wars between the British and the French which were fought out in the colonies as well as in Europe and the high seas. In general, the British heavily relied on American colonial militia units, while the French heavily relied on their First Nation allies. The Iroquois Nation were important allies of the British. Much of the fighting involved ambushes and small-scale warfare in the villages along the border between New England and Quebec. The New England colonies had a much larger population than Quebec, so major invasions came from south to north. The First Nation allies, only loosely controlled by the French, repeatedly raided New England villages to kidnap women and children, and torture and kill the men. Those who survived were brought up as Francophone Catholics. The tension along the border was exacerbated by religion, the French Catholics and English Protestants had a deep mutual distrust. There was a naval dimension as well, involving privateers attacking enemy merchant ships.
England seized Quebec from 1629 to 1632, and Acadia in 1613 and again from 1654 to 1670; These territories were returned to France by the peace treaties. The major wars were (to use American names), King William's War (1689–1697); Queen Anne's War (1702–1713); King George's War (1744–1748), and the French and Indian War (1755–1763). In Canada, as in Europe, this era is known as the Seven Years' War.
New England soldiers and sailors were critical to the successful British campaign to capture the French fortress of Louisbourg in 1745, and (after it had been returned by treaty) to capture it again in 1758.
Mingling of peoples.
From the 1750s to the 21st century, there has been extensive mingling of the Canadian and American populations, with large movements in both directions.
New England Yankees settled large parts of Nova Scotia before 1775, and were neutral during the American Revolution. At the end of the Revolution, about 75,000 Loyalists moved out of the new United States to Nova Scotia, New Brunswick, and the lands of Quebec, east and south of Montreal. From 1790 to 1812 many farmers moved from New York and New England into Ontario (mostly to Niagara, and the north shore of Lake Ontario). In the mid and late 19th century gold rushes attracted American prospectors, mostly to British Columbia after the Cariboo Gold Rush, Fraser Canyon Gold Rush, and later to the Yukon. In the early 20th century, the opening of land blocks in the Prairie Provinces attracted many farmers from the American Midwest. Many Mennonites immigrated from Pennsylvania and formed their own colonies. In the 1890s some Mormons went north to form communities in Alberta after The Church of Jesus Christ of Latter-day Saints rejected plural marriage. The 1960s saw the arrival of about 50,000 draft-dodgers who opposed the Vietnam War.
In the late 19th and early 20th centuries, about 900,000 French Canadians moved to the U.S., with 395,000 residents there in 1900. Two-thirds went to mill towns in New England, where they formed distinctive ethnic communities. By the late 20th century, they had abandoned the French language, but most kept the Catholic religion. About twice as many English Canadians came to the U.S., but they did not form distinctive ethnic settlements.
Canada was a way-station through which immigrants from other lands stopped for a while, ultimately heading to the U.S. In 1851–1951, 7.1 million people arrived in Canada (mostly from Continental Europe), and 6.6 million left Canada, most of them to the U.S.
American Revolutionary War.
At the outset of the American Revolutionary War, the American revolutionaries hoped the French Canadians in Quebec and the Colonists in Nova Scotia would join their rebellion and they were pre-approved for joining the United States in the Articles of Confederation. When Canada was invaded, thousands joined the American cause and formed regiments that fought during the war; however most remained neutral and some joined the British effort. Britain advised the French Canadians that the British Empire already enshrined their rights in the Quebec Act, which the American colonies had viewed as one of the Intolerable Acts. The American invasion was a fiasco and Britain tightened its grip on its northern possessions; in 1777, a major British invasion into New York led to the surrender of the entire British army at Saratoga, and led France to enter the war as an ally of the U.S. The French Canadians largely ignored France's appeals for solidarity. After the war Canada became a refuge for about 75,000 Loyalists who either wanted to leave the U.S., or were compelled by Patriot reprisals to do so.
Among the original Loyalists there were 3,500 free blacks. Most went to Nova Scotia and in 1792, 1200 migrated to Sierra Leone. About 2000 black slaves were brought in by Loyalist owners; they remained slaves in Canada until the Empire abolished slavery in 1833. Before 1860, about 30,000–40,000 blacks entered Canada; many were already free and others were escaped slaves who came through the Underground Railroad.
War of 1812.
The Treaty of Paris (1783), which ended the war, called for British forces to vacate all their forts south of the Great Lakes border. Britain refused to do so, citing failure of the United States to provide financial restitution for Loyalists who had lost property in the war. The Jay Treaty in 1795 with Great Britain resolved that lingering issue and the British departed the forts. Thomas Jefferson saw the nearby British imperial presence as a threat to the United States, and so he opposed the Jay Treaty, and it became one of the major political issues in the United States at the time. Thousands of Americans immigrated to Upper Canada (Ontario) from 1785 to 1812 to obtain cheaper land and better tax rates prevalent in that province; despite expectations that they would be loyal to the U.S. if a war broke out, in the event they were largely non-political.
Tensions mounted again after 1805, erupting into the War of 1812, when the Americans declared war on Britain. The Americans were angered by British harassment of U.S. ships on the high seas and seizure ("Impressment") of 6,000 sailors from American ships, severe restrictions against neutral American trade with France, and British support for hostile Indian tribes in Ohio and territories the U.S. had gained in 1783. American "honor" was an implicit issue. The Americans were outgunned by more than 10 to 1 by the Royal Navy, but could call on an army much larger than the British garrison in Canada, and so a land invasion of Canada was proposed as the only feasible, and most advantegous means of attacking the British Empire. Americans on the western frontier also hoped an invasion would bring an end to British support of Native American resistance to the westward expansion of the United States, typified by Tecumseh's coalition of tribes. Americans may also have wanted to annex Canada.
Once war broke out, the American strategy was to seize Canada—perhaps as a means of forcing concessions from the British Empire, or perhaps in order to annex it. There was some hope that settlers in western Canada—most of them recent immigrants from the U.S.—would welcome the chance to overthrow their British rulers. However, the American invasions were defeated primarily by British regulars with support from Native Americans and Upper Canada (Ontario) militia. Aided by the powerful Royal Navy, a series of British raids on the American coast were highly successful, culminating with an attack on Washington that resulted in the British burning of the White House, Capitol, and other public buildings. Major British invasions of New York in 1814 and Louisiana in 1814–15 were fiascoes, with the British retreating from New York and decisively defeated at the Battle of New Orleans. At the end of the war, Britain's American Indian allies had largely been defeated, and the Americans controlled a strip of Western Ontario centered on Fort Malden. However, Britain held much of Maine, and, with the support of their remaining American Indian allies, huge areas of the Old Northwest, including Wisconsin and much of Michigan and Illinois. With the surrender of Napoleon in 1814, Britain ended naval policies that angered Americans; with the defeat of the Indian tribes the threat to American expansion was ended. The upshot was both sides had asserted their honour, Canada was not annexed, and London and Washington had nothing more to fight over. The war was ended by the Treaty of Ghent, which took effect in February 1815. A series of postwar agreements further stabilized peaceful relations along the Canadian-US border. Canada reduced American immigration for fear of undue American influence, and built up the Anglican church as a counterweight to the largely American Methodist and Baptist churches.
In later years, Anglophone Canadians, especially in Ontario, viewed the War of 1812 as a heroic and successful resistance against invasion and as a victory that defined them as a people. The myth that the Canadian militia had defeated the invasion almost single-handed, known logically as the "militia myth", became highly prevalent after the war, having been propounded by John Strachan, Anglican Bishop of York. Meanwhile, the United States celebrated victory in its "Second War of Independence," and war heroes such as Andrew Jackson and William Henry Harrison headed to the White House.
Conservative reaction.
In the aftermath of the War of 1812, pro-imperial conservatives led by Anglican Bishop John Strachan took control in Ontario ("Upper Canada"), and promoted the Anglican religion as opposed to the more republican Methodist and Baptist churches. A small interlocking elite, known as the Family Compact took full political control. Democracy, as practiced in the US, was ridiculed. The policies had the desired effect of deterring immigration from United States. Revolts in favor of democracy in Ontario and Quebec ("Lower Canada") in 1837 were suppressed; many of the leaders fled to the US. The American policy was to largely ignore the rebellions, and indeed ignore Canada generally in favor of westward expansion of the American Frontier.
Alabama claims.
At the end of the American Civil War in 1865, Americans were angry at British support for the Confederacy. One result was toleration of Fenian efforts to use the U.S. as a base to attack Canada. More serious was the demand for a huge payment to cover the damages caused, on the notion that British involvement had lengthened the war. Senator Charles Sumner, the chairman of the Senate Foreign Relations Committee, originally wanted to ask for $2 billion, or alternatively the ceding of all of Canada to the United States. When American Secretary of State William H. Seward negotiated the Alaska Purchase with Russia in 1867, he intended it as the first step in a comprehensive plan to gain control of the entire northwest Pacific Coast. Seward was a firm believer in Manifest Destiny, primarily for its commercial advantages to the U.S. Seward expected British Columbia to seek annexation to the U.S. and thought Britain might accept this in exchange for the "Alabama" claims. Soon other elements endorsed annexation, Their plan was to annex British Columbia, Red River Colony (Manitoba), and Nova Scotia, in exchange for the dropping the damage claims. The idea reached a peak in the spring and summer of 1870, with American expansionists, Canadian separatists, and British anti-imperialists seemingly combining forces. The plan was dropped for multiple reasons. London continued to stall, American commercial and financial groups pressed Washington for a quick settlement of the dispute on a cash basis, growing Canadian nationalist sentiment in British Columbia called for staying inside the British Empire, Congress became preoccupied with Reconstruction, and most Americans showed little interest in territorial expansion. The "Alabama Claims" dispute went to international arbitration. In one of the first major cases of arbitration, the tribunal in 1872 supported the American claims and ordered Britain to pay $15.5 million. Britain paid and the episode ended in peaceful relations.
Dominion of Canada.
Canada became a self-governing dominion in 1867 in internal affairs while Britain controlled diplomacy and defense policy. Prior to Confederation, there was an Oregon boundary dispute in which the Americans claimed the 54th degree latitude. That issue was resolved by splitting the disputed territory; the northern half became British Columbia, and the southern half the states of Washington and Oregon. Strained relations with America continued, however, due to a series of small-scale armed incursions named the Fenian raids by Irish-American Civil War veterans across the border from 1866 to 1871 in an attempt to trade Canada for Irish independence. The American government, angry at Canadian tolerance of Confederate raiders during the American Civil War, moved very slowly to disarm the Fenians. The British government, in charge of diplomatic relations, protested cautiously, as Anglo-American relations were tense. Much of the tension was relieved as the Fenians faded away and in 1872 by the settlement of the Alabama Claims, when Britain paid the U.S. $15.5 million for war losses caused by warships built in Britain and sold to the Confederacy.
Disputes over ocean boundaries on Georges Bank and over fishing, whaling, and sealing rights in the Pacific were settled by international arbitration, setting an important precedent.
Emigration to and from the United States.
After 1850, the pace of industrialization and urbanization was much faster in the United States, drawing a wide range of immigrants from the North. By 1870, 1/6 of all the people born in Canada had moved to the United States, with the highest concentrations in New England, which was the destination of Francophone emigrants from Quebec and Anglophone emigrants from the Maritimes. It was common for people to move back and forth across the border, such as seasonal lumberjacks, entrepreneurs looking for larger markets, and families looking for jobs in the textile mills that paid much higher wages than in Canada.
The southward migration slacked off after 1890, as Canadian industry began a growth spurt. By then, the American frontier was closing, and thousands of farmers looking for fresh land moved from the United States north into the Prairie Provinces. The net result of the flows were that in 1901 there were 128,000 American-born residents in Canada (3.5% of the Canadian population) and 1.18 million Canadian-born residents in the United States (1.6% of the U.S. population).
Alaska boundary.
A short-lived controversy was the Alaska boundary dispute, settled in favor of the United States in 1903. No one cared until a gold rush brought tens of thousands of men to Canada's Yukon, and they had to arrive through American ports. Canada needed its port and claimed that it had a legal right to a port near the present American town of Haines, Alaska. It would provide an all-Canadian route to the rich goldfields. The dispute was settled by arbitration, and the British delegate voted with the Americans—to the astonishment and disgust of Canadians who suddenly realized that Britain considered its relations with the United States paramount compared to those with Canada. The arbitrartion validated the status quo, but made Canada angry at Britain.
1907 saw a minor controversy over USS "Nashville" sailing into the Great Lakes via Canada without Canadian permission. To head off future embarrassments, in 1909 the two sides signed the International Boundary Waters Treaty and the International Joint Commission was established to manage the Great Lakes and keep them disarmed. It was amended in World War II to allow the building and training of warships.
Reciprocal trade with U.S..
Anti-Americanism reached a shrill peak in 1911 in Canada. The Liberal government in 1911 negotiated a Reciprocity treaty with the U.S. that would lower trade barriers. Canadian manufacturing interests were alarmed that free trade would allow the bigger and more efficient American factories to take their markets. The Conservatives made it a central campaign issue in the 1911 election, warning that it would be a "sell out" to the United States with economic annexation a special danger. Conservative slogan was "No truck or trade with the Yankees", as they appealed to Canadian nationalism and nostalgia for the British Empire to win a major victory.
Canadian autonomy.
Canada demanded and received permission from London to send its own delegation to the Versailles Peace Talks in 1919, with the proviso that it sign the treaty under the British Empire. Canada subsequently took responsibility for its own foreign and military affairs in the 1920s. Its first ambassador to the United States, Vincent Massey, was named in 1927. The United States first ambassador to Canada was William Phillips. Canada became an active member of the British Commonwealth, the League of Nations, and the World Court, none of which included the U.S.
Relations with the United States were cordial until 1930, when Canada vehemently protested the new Smoot–Hawley Tariff Act by which the U.S. raised tariffs (taxes) on products imported from Canada. Canada retaliated with higher tariffs of its own against American products, and moved toward more trade within the British Commonwealth. U.S.–Canadian trade fell 75% as the Great Depression dragged both countries down.
Down to the 1920s the war and naval departments of both nations designed hypothetical war game scenarios with the other as an enemy. These were primarily exercises; the departments were never told to get ready for a real war. In 1921, Canada developed Defence Scheme No. 1 for an attack on American cities and for forestalling invasion by the United States until Imperial reinforcements arrived. Through the later 1920s and 1930s, the United States Army War College developed a plan for a war with the British Empire waged largely on North American territory, in War Plan Red.
Herbert Hoover meeting in 1927 with British Ambassador Sir Esme Howard agreed on the "absurdity of contemplating the possibility of war between the United States and the British Empire."
In 1938, as the roots of World War II were set in motion, U.S. President Franklin Roosevelt gave a public speech at Queens University in Kingston, Ontario, declaring that the United States would not sit idly by if another power tried to dominate Canada. Diplomats saw it as a clear warning to Germany not to attack Canada.
World War II.
The two nations cooperated closely in World War II, as both nations saw new levels of prosperity and a determination to defeat the Axis powers. Prime Minister William Lyon Mackenzie King and President Franklin D. Roosevelt were determined not to repeat the mistakes of their predecessors. They met in August 1940 at Ogdensburg, issuing a declaration calling for close cooperation, and formed the Permanent Joint Board on Defense (PJBD).
King sought to raise Canada's international visibility by hosting the August 1943 Quadrant conference in Quebec on military and political strategy; he was a gracious host but was kept out of the important meetings by Winston Churchill and Roosevelt.
Canada allowed the construction of the Alaska Highway and participated in the building of the atomic bomb. 49,000 Americans joined the RCAF (Canadian) or RAF (British) air forces through the Clayton Knight Committee, which had Roosevelt's permission to recruit in the U.S. in 1940–42.
American attempts in the mid-1930s to integrate British Columbia into a united West Coast military command had aroused Canadian opposition. Fearing a Japanese invasion of Canada's vulnerable coast, American officials urged the creation of a united military command for an eastern Pacific Ocean theater of war. Canadian leaders feared American imperialism and the loss of autonomy more than a Japanese invasion. In 1941, Canadians successfully argued within the PJBD for mutual cooperation rather than unified command for the West Coast.
Newfoundland.
The United States built large military bases in Newfoundland, at the time, a British dominion. The American involvement ended the depression and brought new prosperity; Newfoundland's business community sought closer ties with the United States as expressed by the Economic Union Party. Ottawa took notice and wanted Newfoundland to join Canada, which it did after hotly contested referenda. There was little demand in the United States for the acquisition of Newfoundland, so the United States did not protest the British decision not to allow an American option on the Newfoundland referendum.
Cold War.
Following co-operation in the two World Wars, Canada and the United States lost much of their previous animosity. As Britain's influence as a global imperial power declined, Canada and the United States became extremely close partners. Canada was a close ally of the United States during the Cold War.
Nixon Shock 1971.
The United States had become Canada's largest market, and after the war the Canadian economy became dependent on smooth trade flows with the United States so much that in 1971 when the United States enacted the "Nixon Shock" economic policies (including a 10% tariff on all imports) it put the Canadian government into a panic. This led in a large part to the articulation of Prime Minister Trudeau's "Third Option" policy of diversifying Canada's trade and downgrading the importance of Canada – United States relations. In a 1972 speech in Ottawa, Nixon declared the "special relationship" between Canada and the United States dead.
1990s.
The main issues in Canada–U.S. relations in the 1990s focused on the NAFTA agreement, which was signed in 1994. It created a common market that by 2014 was worth $19 trillion, encompassed 470 million people, and had created millions of jobs. Wilson says, "Few dispute that NAFTA has produced
large and measurable gains for Canadian consumers, workers, and businesses." However, he adds, "NAFTA has fallen well short of expectations."
Anti-Americanism.
Since the arrival of the Loyalists as refugees from the American Revolution in the 1780s, historians have identified a constant theme of Canadian fear of the United States and of "Americanization" or a cultural takeover. In the War of 1812, for example, the enthusiastic response by French militia to defend Lower Canada reflected, according to Heidler and Heidler (2004), "the fear of Americanization." Scholars have traced this attitude over time in Ontario and Quebec.
Canadian intellectuals who wrote about the U.S. in the first half of the 20th century identified America as the world center of modernity, and deplored it. Imperialists (who admired the British Empire) explained that Canadians had narrowly escaped American conquest with its rejection of tradition, its worship of "progress" and technology, and its mass culture; they explained that Canada was much better because of its commitment to orderly government and societal harmony. There were a few ardent defenders of the nation to the south, notably liberal and socialist intellectuals such as F. R. Scott and Jean-Charles Harvey (1891–1967).
Looking at television, Collins (1990) finds that it is in English Canada that fear of cultural Americanization is most powerful, for there the attractions of the U.S. are strongest. Meren (2009) argues that after 1945, the emergence of Quebec nationalism and the desire to preserve French-Canadian cultural heritage led to growing anxiety regarding American cultural imperialism and Americanization. In 2006 surveys showed that 60 percent of Quebecers had a fear of Americanization, while other surveys showed they preferred their current situation to that of the Americans in the realms of health care, quality of life as seniors, environmental quality, poverty, educational system, racism and standard of living. While agreeing that job opportunities are greater in America, 89 percent disagreed with the notion that they would rather be in the United States, and they were more likely to feel closer to English Canadians than to Americans. However, there is evidence that the elites and Quebec are much less fearful of Americanization, and much more open to economic integration than the general public.
The history has been traced in detail by a leading Canadian historian J.L. Granatstein in "Yankee Go Home: Canadians and Anti-Americanism" (1997). Current studies report the phenomenon persists. Two scholars report, "Anti-Americanism is alive and well in Canada today, strengthened by, among other things, disputes related to NAFTA, American involvement in the Middle East, and the ever-increasing Americanization of Canadian culture." Jamie Glazov writes, "More than anything else, Diefenbaker became the tragic victim of Canadian anti-Americanism, a sentiment the prime minister had fully embraced by 1962. was unable to imagine himself (or his foreign policy) without enemies." Historian J. M. Bumsted says, "In its most extreme form, Canadian suspicion of the United States has led to outbreaks of overt anti-Americanism, usually spilling over against American residents in Canada." John R. Wennersten writes, "But at the heart of Canadian anti-Americanism lies a cultural bitterness that takes an American expatriate unaware. Canadians fear the American media's influence on their culture and talk critically about how Americans are exporting a culture of violence in its television programming and movies." However Kim Nossal points out that the Canadian variety is much milder than anti-Americanism in some other countries. By contrast Americans show very little knowledge or interest one way or the other regarding Canadian affairs. Canadian historian Frank Underhill, quoting Canadian playwright Merrill Denison summed it up: "Americans are benevolently ignorant about Canada, whereas Canadians are malevolently informed about the United States."
Relations between political executives.
The executive of each country is represented differently. The President of the United States serves as both the head of state and head of government, and his "administration" is the executive, while the Prime Minister of Canada is head of government only, and his or her "government" or "ministry" directs the executive.
Mulroney and Reagan.
Relations between Brian Mulroney and Ronald Reagan were famously close. This relationship resulted in negotiations on a potential free trade agreement, and a treaty of acid-rain-causing emissions, both major policy goals of Mulroney, that would be finalized under the presidency of George H. W. Bush.
Chrétien and Clinton.
Although Jean Chrétien was wary to appearing too close to the president, personally, he and Bill Clinton were known to be golfing partners. Their governments had many small trade quarrels over magazines, softwood lumber, and so on, but on the whole were quite friendly. Both leaders had run on reforming or abolishing NAFTA, but the agreement went ahead with the addition of environmental and labor side agreements. Crucially, the Clinton administration lent rhetorical support to Canadian unity during the 1995 referendum in Quebec on independence from Canada.
Bush and Chrétien.
Relations between Chrétien and George W. Bush were strained throughout their overlapping times in office. Jean Chrétien publicly mused that U.S. foreign policy might be part of the "root causes" of terrorism shortly after September 11 attacks. Some Americans did not appreciate his "smug moralism", and Chrétien's public refusal to support the 2003 Iraq war was met with chagrin in the United States, especially among conservatives.
Bush and Harper.
Stephen Harper and George W. Bush were thought to share warm personal relations and also close ties between their administrations. Because Bush was so unpopular in Canada, however, this was rarely emphasized by the Harper government.
Shortly after being congratulated by Bush for his victory in February 2006, Harper rebuked U.S. ambassador to Canada David Wilkins for criticizing the Conservatives' plans to assert Canada's sovereignty over the Arctic Ocean waters with military force.
Harper and Obama.
President Barack Obama's first international trip was to Canada on February 19, 2009. Aside from Canadian lobbying against "Buy American" provisions in the U.S. stimulus package, relations between the two administrations were smooth.
They also held friendly bets on hockey games during the Winter Olympic season. In the 2010 Winter Olympics hosted by Canada in Vancouver, Canada defeated the US in both gold medal matches, entitling Stephen Harper to receive a case of Molson Canadian beer from Barack Obama; in reverse, if Canada had lost, Harper would have provided a case of Yuengling beer to Obama. During the 2014 Winter Olympics, alongside U.S. Secretary of State John Kerry & Minister of Foreign Affairs John Baird, Stephen Harper was given a case of Samuel Adams beer by Obama for the Canadian gold medal victory over the US in women's hockey, and the semi-final victory over the US in men's hockey.
Canada-United States Regulatory Cooperation Council (RCC) (2011).
On February 4, 2011, Harper and Obama issued a "Declaration on a Shared Vision for Perimeter Security and Economic Competitiveness" and announced the creation of the Canada–United States Regulatory Cooperation Council (RCC) "to increase regulatory transparency and coordination between the two countries."
Health Canada and the United States Food and Drug Administration (FDA) under the RCC mandate, undertook the "first of its kind" initiative by selecting "as its first area of alignment common cold indications for certain over-the-counter antihistamine ingredients (GC 2013-01-10)."
On Wednesday, December 7, Harper flew to Washington to meet with Obama and sign an agreement to implement the joint action plans that had been developed since the initial meeting in February. The plans called on both countries to spend more on border infrastructure, share more information on people who cross the border, and acknowledge more of each other's safety and security inspection on third-country traffic. An editorial in "The Globe and Mail" praised the agreement for giving Canada the ability to track whether failed refugee claimants have left Canada via the U.S. and for eliminating "duplicated baggage screenings on connecting flights". The agreement is not a legally-binding treaty, and relies on the political will and ability of the executives of both governments to implement the terms of the agreement. These types of executive agreements are routine—on both sides of the Canada–U.S. border.
Obama and Trudeau.
President Barack Obama and Prime Minister Justin Trudeau first met formally at the APEC summit meeting in Manila, Philippines in November 2015, nearly a week after the latter was sworn into the office. Both leaders expressed their eagerness for increased cooperation and coordination between the two countries during the course of Trudeau's government with Trudeau promising an "enhanced Canada–U.S. partnership".
On November 6, 2015, Obama announced the U.S. State Department's rejection of the proposed Keystone XL pipeline, the fourth phase of the Keystone oil pipeline system running between Canada and the United States, to which Trudeau expressed disappointment but said that the rejection would not damage Canada–U.S. relations and would instead provide a "fresh start" to strengthening ties through cooperation and coordination, saying that "the Canada–U.S. relationship is much bigger than any one project." Obama has since praised Trudeau's efforts to prioritize the reduction of climate change, calling it "extraordinarily helpful" to establish a worldwide consensus on addressing the issue.
Although Trudeau has told Obama his plans to withdraw Canada's McDonnell Douglas CF-18 Hornet jets assisting in the American-led intervention against ISIL, Trudeau said that Canada will still "do more than its part" in combating the terrorist group by increasing the number of Canadian special forces members training and fighting on ground in Iraq and Syria.
Trudeau visited the White House for an official visit and state dinner on March 10, 2016. Trudeau and Obama were reported to have shared warm personal relations during the visit, making humorous remarks about which country was better at hockey and which country had better beer. Obama complimented Trudeau's 2015 election campaign for its "message of hope and change" and "positive and optimistic vision". Obama and Trudeau also held "productive" discussions on climate change and relations between the two countries, and Trudeau invited Obama to speak in the Canadian parliament in Ottawa later in the year.
Military and security.
The Canadian military, like forces of other NATO countries, fought alongside the United States in most major conflicts since World War II, including the Korean War, the Gulf War, the Kosovo War, and most recently the war in Afghanistan. The main exceptions to this were the Canadian government's opposition to the Vietnam War and the Iraq War, which caused some brief diplomatic tensions. Despite these issues, military relations have remained close.
American defense arrangements with Canada are more extensive than with any other country. The Permanent Joint Board of Defense, established in 1940, provides policy-level consultation on bilateral defense matters. The United States and Canada share North Atlantic Treaty Organization (NATO) mutual security commitments. In addition, American and Canadian military forces have cooperated since 1958 on continental air defense within the framework of the North American Aerospace Defense Command (NORAD). Canadian forces have provided indirect support for the American invasion of Iraq that began in 2003. Moreover, interoperability with the American armed forces has been a guiding principle of Canadian military force structuring and doctrine since the end of the Cold War. Canadian navy frigates, for instance, integrate seamlessly into American carrier battle groups.
In commemoration of the 200th Anniversary of the War of 1812 ambassadors from Canada and the US, and naval officers from both countries gathered at the Pritzker Military Library on August 17, 2012, for a panel discussion on Canada-US relations with emphasis on national security-related matters. Also as part of the commemoration, the navies of both countries sailed together throughout the Great Lakes region.
War in Afghanistan.
Canada's elite JTF2 unit joined American special forces in Afghanistan shortly after the al-Qaida attacks on September 11, 2001. Canadian forces joined the multinational coalition in Operation Anaconda in January 2002. On April 18, 2002, an American pilot bombed Canadian forces involved in a training exercise, killing four and wounding eight Canadians. A joint American-Canadian inquiry determined the cause of the incident to be pilot error, in which the pilot interpreted ground fire as an attack; the pilot ignored orders that he felt were "second-guessing" his field tactical decision. Canadian forces assumed a six-month command rotation of the International Security Assistance Force in 2003; in 2005, Canadians assumed operational command of the multi-national Brigade in Kandahar, with 2,300 troops, and supervises the Provincial Reconstruction Team in Kandahar, where al-Qaida forces are most active. Canada has also deployed naval forces in the Persian Gulf since 1991 in support of the UN Gulf Multinational Interdiction Force.
The Canadian Embassy in Washington, DC maintains a public relations web site named CanadianAlly.com, which is intended "to give American citizens a better sense of the scope of Canada's role in North American and Global Security and the War on Terror".
The New Democratic Party and some recent Liberal leadership candidates have expressed opposition to Canada's expanded role in the Afghan conflict on the ground that it is inconsistent with Canada's historic role (since the Second World War) of peacekeeping operations.
2003 Invasion of Iraq.
According to contemporary polls, 71% of Canadians were opposed to the 2003 invasion of Iraq. Many Canadians, and the former Liberal Cabinet headed by Paul Martin (as well as many Americans such as Bill Clinton and Barack Obama), made a policy distinction between conflicts in Afghanistan and Iraq, unlike the Bush Doctrine, which linked these together in a "Global war on terror".
Trade.
Canada and the United States have the world's largest trading relationship, with huge quantities of goods and people flowing across the border each year. Since the 1987 Canada–United States Free Trade Agreement, there have been no tariffs on most goods passed between the two countries.
In the course of the softwood lumber dispute, the U.S. has placed tariffs on Canadian softwood lumber because of what it argues is an unfair Canadian government subsidy, a claim which Canada disputes. The dispute has cycled through several agreements and arbitration cases. Other notable disputes include the Canadian Wheat Board, and Canadian cultural "restrictions" on magazines and television (See CRTC, CBC, and National Film Board of Canada). Canadians have been criticized about such things as the ban on beef since a case of Mad Cow disease was discovered in 2003 in cows from the United States (and a few subsequent cases) and the high American agricultural subsidies. Concerns in Canada also run high over aspects of the North American Free Trade Agreement (NAFTA) such as Chapter 11.
Environmental issues.
A principal instrument of this cooperation is the International Joint Commission (IJC), established as part of the Boundary Waters Treaty of 1909 to resolve differences and promote international cooperation on boundary waters. The Great Lakes Water Quality Agreement of 1972 is another historic example of joint cooperation in controlling trans-border water pollution. However, there have been some disputes. Most recently, the Devil's Lake Outlet, a project instituted by North Dakota, has angered Manitobans who fear that their water may soon become polluted as a result of this project.
Beginning in 1986 the Canadian government of Brian Mulroney began pressing the Reagan administration for an "Acid Rain Treaty" in order to do something about U.S. industrial air pollution causing acid rain in Canada. The Reagan administration was hesitant, and questioned the science behind Mulroney's claims. However, Mulroney was able to prevail. The product was the signing and ratification of the Air Quality Agreement of 1991 by the first Bush administration. Under that treaty, the two governments consult semi-annually on trans-border air pollution, which has demonstrably reduced acid rain, and they have since signed an annex to the treaty dealing with ground level ozone in 2000. Despite this, trans-border air pollution remains an issue, particularly in the Great Lakes-St. Lawrence watershed during the summer. The main source of this trans-border pollution results from coal-fired power stations, most of them located in the Midwestern United States. As part of the negotiations to create NAFTA, Canada and the U.S. signed, along with Mexico, the North American Agreement On Environmental Cooperation which created the Commission for Environmental Cooperation which monitors environmental issues across the continent, publishing the North American Environmental Atlas as one aspect of its monitoring duties.
Currently neither of the countries' governments support the Kyoto Protocol, which set out time scheduled curbing of greenhouse gas emissions. Unlike the United States, Canada has ratified the agreement. Yet after ratification, due to internal political conflict within Canada, the Canadian government does not enforce the Kyoto Protocol, and has received criticism from environmental groups and from other governments for its climate change positions. In January 2011, the Canadian minister of the environment, Peter Kent, explicitly stated that the policy of his government with regards to greenhouse gas emissions reductions is to wait for the United States to act first, and then try to harmonize with that action - a position that has been condemned by environmentalists and Canadian nationalists, and as well as scientists and government think-tanks.
Newfoundland fisheries dispute.
The United States and Britain, had a long-standing dispute about the rights of Americans fishing in the waters near Newfoundland. Before 1776, there was no question that American fishermen, mostly from Massachusetts, had rights to use the waters off Newfoundland. In the peace treaty negotiations of 1783, the Americans insisted on a statement of these rights. However, France, an American ally, disputed the American position because France had its own specified rights in the area and wanted them to be exclusive. The Treaty of Paris (1783) gave the Americans not rights, but rather "liberties" to fish within the territorial waters of British North America and to dry fish on certain coasts.
After the War of 1812, the Convention of 1818 between the United States and Britain specified exactly what liberties were involved. Canadian and Newfoundland fishermen nevertheless contested these liberties in the 1830s and 1840s. The Canadian–American Reciprocity Treaty of 1854, and the Treaty of Washington of 1871 spelled-out the liberties in more detail. However the Treaty of Washington expired in 1885, and there was a continuous round of disputes over jurisdictions and liberties. Britain the United States sent the issue to the Permanent Court of Arbitration in The Hague in 1909. It produced a compromise settlement that permanently ended the problems.
Illicit drugs.
In 2003 the American government became concerned when members of the Canadian government announced plans to decriminalize marijuana. David Murray, an assistant to U.S. Drug Czar John P. Walters, said in a CBC interview that, "We would have to respond. We would be forced to respond." However the election of the Conservative Party in early 2006 halted the liberalization of marijuana laws for the foreseeable future.
A 2007 joint report by American and Canadian officials on cross-border drug smuggling indicated that, despite their best efforts, "drug trafficking still occurs in significant quantities in both directions across the border. The principal illicit substances smuggled across our shared border are MDMA ("Ecstasy"), cocaine, and marijuana." The report indicated that Canada was a major producer of "Ecstasy" and marijuana for the U.S. market, while the U.S. was a transit country for cocaine entering Canada.
Diplomacy.
Views of presidents and prime ministers.
Presidents and prime ministers typically make formal or informal statements that indicate the diplomatic policy of their administration. Diplomats and journalists at the time—and historians since—dissect the nuances and tone to detect the warmth or coolness of the relationship.
Canada's first Prime Minister also said:
Canadian public opinion on U.S. presidents.
United States President George W. Bush was "deeply disliked" by a majority of Canadians according to the "Arizona Daily Sun". A 2004 poll found that more than two thirds of Canadians favoured Democrat John Kerry over Bush in the 2004 presidential election, with Bush's lowest approval ratings in Canada being in the province of Quebec where just 11% of the population supported him. Canadian public opinion of Barack Obama was more positive. A 2012 poll found that 65% of Canadians would vote for Obama in the 2012 presidential election "if they could" while only 9% of Canadians would vote for his Republican opponent Mitt Romney. The same study found that 61% of Canadians felt that the Obama administration had been "good" for America, while only 12% felt it had been "bad". The study also found that a majority of members of all three major Canadian political parties supported Obama, and also found that Obama had slightly higher approval ratings in Canada in 2012 than he did in 2008. John Ibbitson of "The Globe and Mail" stated in 2012 that Canadians generally supported Democratic presidents over Republican candidates, citing how President Richard Nixon was "never liked" in Canada and that Canadians generally did not approve of Prime Minister Brian Mulroney's friendship with President Ronald Reagan.
Territorial disputes.
These include maritime boundary disputes:
Territorial land disputes:
and disputes over the international status of the:
Arctic disputes.
A long-simmering dispute between Canada and the U.S. involves the issue of Canadian sovereignty over the Northwest Passage (the sea passages in the Arctic). Canada’s assertion that the Northwest Passage represents internal (territorial) waters has been challenged by other countries, especially the U.S., which argue that these waters constitute an international strait (international waters). Canadians were alarmed when Americans drove the reinforced oil tanker through the Northwest Passage in 1969, followed by the icebreaker Polar Sea in 1985, which actually resulted in a minor diplomatic incident. In 1970, the Canadian parliament enacted the Arctic Waters Pollution Prevention Act, which asserts Canadian regulatory control over pollution within a 100-mile zone. In response, the United States in 1970 stated, "We cannot accept the assertion of a Canadian claim that the Arctic waters are internal waters of Canada. ... Such acceptance would jeopardize the freedom of navigation essential for United States naval activities worldwide." A compromise of sorts was reached in 1988, by an agreement on "Arctic Cooperation," which pledges that voyages of American icebreakers "will be undertaken with the consent of the Government of Canada." However the agreement did not alter either country's basic legal position. Paul Cellucci, the American ambassador to Canada, in 2005 suggested to Washington that it should recognize the straits as belonging to Canada. His advice was rejected and Harper took opposite positions. The U.S. opposes Harper's proposed plan to deploy military icebreakers in the Arctic to detect interlopers and assert Canadian sovereignty over those waters.
Common memberships.
Canada and the United States both hold membership in a number of multinational organizations such as:
Diplomatic missions.
Canadian missions in the United States.
Canada's chief diplomatic mission to the United States is the Canadian Embassy in Washington, D.C.. It is further supported by many consulates located through United States.
The Canadian Government maintains consulates-general in several major U.S. cities including: Atlanta, Boston, Chicago, Dallas, Denver, Detroit, Los Angeles, Miami, Minneapolis, New York City, San Francisco and Seattle. Canadian consular services are also available in Honolulu at the consulate of Australia through the Canada–Australia Consular Services Sharing Agreement.
There are also Canadian trade offices located in Houston, Palo Alto and San Diego.
U.S. missions in Canada.
The United States's chief diplomatic mission to Canada is the United States Embassy in Ottawa. It is further supported by many consulates located throughout Canada.
The U.S government maintains consulates-general in several major Canadian cities including:
Calgary, Halifax, Montreal, Quebec City, Toronto, Vancouver and Winnipeg.
The United States also maintains Virtual Presence Posts (VPP) in the: Northwest Territories, Nunavut, Southwestern Ontario and Yukon.

</doc>
<doc id="5211" url="https://en.wikipedia.org/wiki?curid=5211" title="Christianity">
Christianity

Christianity is an Abrahamic monotheistic religion based on the life and teachings of Jesus Christ as presented in the New Testament. Christianity is the world's largest religion, with over 2.4 billion adherents, known as Christians. Christians believe that Jesus is the Son of God and the savior of humanity whose coming as Christ or the Messiah was prophesied in the Old Testament.
Christian theology is expressed in ecumenical creeds. These professions of faith state that Jesus suffered, died, was entombed, and was resurrected from the dead, in order to grant eternal life to those who believe in him and trust in him for the remission of their sins. The creeds further maintain that Jesus bodily ascended into heaven, where he reigns with God the Father, and that he will return to judge the living and dead and grant eternal life to his followers. His ministry, crucifixion and resurrection are often referred to as "the gospel", meaning "good news". The term "gospel" also refers to written accounts of Jesus's life and teaching, four of which—Matthew, Mark, Luke, and John—are considered canonical and included in the Christian Bible.
Christianity began as a Second Temple Judaic sect in the mid-1st century. Originating in Judea, it quickly spread to Europe, Syria, Mesopotamia, Asia Minor, Egypt, Ethiopia, and India and by the end of the 4th century had become the official state church of the Roman Empire. Following the Age of Discovery, Christianity spread to the Americas, Australasia, sub-Saharan Africa, and the rest of the world through missionary work and colonization. Christianity has played a prominent role in the shaping of Western civilization.
Throughout its history, the religion has weathered schisms and theological disputes that have resulted in many distinct Churches and denominations. Worldwide, the three largest branches of Christianity are the Roman Catholic Church, the Eastern Orthodox Church and the various denominations of Protestantism. The Roman Catholic and Eastern Orthodox Churches broke communion with each other in the schism of the 11th century; Protestantism came into existence in the Reformation of the 16th century, splitting from the Roman Catholic Church.
Beliefs.
There are many important differences of interpretation and opinion of the Bible on which Christianity is based. Because of these irreconcilable differences in theology and a lack of consensus on the core tenets of what defines Christianity, Protestants, Catholics, Orthodox Church members, and theologians often deny that members of other branches are Christians.
Creeds.
Concise doctrinal statements or confessions of religious beliefs are known as creeds (from Latin "credo", meaning "I believe"). They began as baptismal formulae and were later expanded during the Christological controversies of the 4th and 5th centuries to become statements of faith.
Many evangelical Protestants reject creeds as definitive statements of faith, even while agreeing with some or all of the substance of the creeds. The Baptists have been non-creedal "in that they have not sought to establish binding authoritative confessions of faith on one another." Also rejecting creeds are groups with roots in the Restoration Movement, such as the Christian Church (Disciples of Christ), the Evangelical Christian Church in Canada and the Churches of Christ.
The Apostles' Creed remains the most popular statement of the articles of Christian faith which are generally acceptable to most Christian denominations that are creedal. It is widely used by a number of Christian denominations for both liturgical and catechetical purposes, most visibly by liturgical Churches of Western Christian tradition, including the Latin Church of the Catholic Church, Lutheranism, Anglicanism, and Western Rite Orthodoxy. It is also used by Presbyterians, Methodists, and Congregationalists. This particular creed was developed between the 2nd and 9th centuries. Its central doctrines are those of the Trinity and God the Creator. Each of the doctrines found in this creed can be traced to statements current in the apostolic period. The creed was apparently used as a summary of Christian doctrine for baptismal candidates in the churches of Rome.
Its main points include:
The Nicene Creed, largely a response to Arianism, was formulated at the Councils of Nicaea and Constantinople in 325 and 381 respectively and ratified as the universal creed of Christendom by the First Council of Ephesus in 431.
The Chalcedonian Definition, or Creed of Chalcedon, developed at the Council of Chalcedon in 451, though rejected by the Oriental Orthodox Churches, taught Christ "to be acknowledged in two natures, inconfusedly, unchangeably, indivisibly, inseparably": one divine and one human, and that both natures, while perfect in themselves, are nevertheless also perfectly united into one person.
The Athanasian Creed, received in the Western Church as having the same status as the Nicene and Chalcedonian, says: "We worship one God in Trinity, and Trinity in Unity; neither confounding the Persons nor dividing the Substance."
Most Christians (Roman Catholics, Eastern Orthodox, Oriental Orthodox and Protestants alike) accept the use of creeds, and subscribe to at least one of the creeds mentioned above.
Jesus.
The central tenet of Christianity is the belief in Jesus as the Son of God and the Messiah (Christ). Christians believe that Jesus, as the Messiah, was anointed by God as savior of humanity, and hold that Jesus' coming was the fulfillment of messianic prophecies of the Old Testament. The Christian concept of the Messiah differs significantly from the contemporary Jewish concept. The core Christian belief is that through belief in and acceptance of the death and resurrection of Jesus, sinful humans can be reconciled to God and thereby are offered salvation and the promise of eternal life.
While there have been many theological disputes over the nature of Jesus over the earliest centuries of Christian history, generally Christians believe that Jesus is God incarnate and "true God and true man" (or both fully divine and fully human). Jesus, having become fully human, suffered the pains and temptations of a mortal man, but did not sin. As fully God, he rose to life again. According to the Bible,God raised him from the dead, he ascended to heaven, is seated at the right hand of the Father and will ultimately return to fulfill the rest of Messianic prophecy such as the Resurrection of the dead, the Last Judgment and final establishment of the Kingdom of God.
According to the canonical gospels of Matthew and Luke, Jesus was conceived by the Holy Spirit and born from the Virgin Mary. Little of Jesus' childhood is recorded in the canonical Gospels, however infancy Gospels were popular in antiquity. In comparison, his adulthood, especially the week before his death, is well documented in the Gospels contained within the New Testament, because that part of his life was believed to be most important. The Biblical accounts of Jesus' ministry include: his baptism, miracles, preaching, teaching, and deeds.
Death and resurrection.
Christians consider the resurrection of Jesus to be the cornerstone of their faith (see 1 Corinthians 15) and the most important event in history. Among Christian beliefs, the death and resurrection of Jesus are two core events on which much of Christian doctrine and theology is based. According to the New Testament Jesus was crucified, died a physical death, was buried within a tomb, and rose from the dead three days later. 
The New Testament mentions several resurrection appearances of Jesus on different occasions to his twelve apostles and disciples, including "more than five hundred brethren at once", before Jesus' Ascension to heaven. Jesus' death and resurrection are commemorated by Christians in all worship services, with special emphasis during Holy Week which includes Good Friday and Easter Sunday.
The death and resurrection of Jesus are usually considered the most important events in Christian theology, partly because they demonstrate that Jesus has power over life and death and therefore has the authority and power to give people eternal life.
Christian churches accept and teach the New Testament account of the resurrection of Jesus with very few exceptions. Some modern scholars use the belief of Jesus' followers in the resurrection as a point of departure for establishing the continuity of the historical Jesus and the proclamation of the early church. Some liberal Christians do not accept a literal bodily resurrection, seeing the story as richly symbolic and spiritually nourishing myth. Arguments over death and resurrection claims occur at many religious debates and interfaith dialogues. Paul the Apostle, an early Christian convert and missionary, wrote, "If Christ was not raised, then all our preaching is useless, and your trust in God is useless." 
Salvation.
Paul the Apostle, like Jews and Roman pagans of his time, believed that sacrifice can bring about new kinship ties, purity, and eternal life. For Paul the necessary sacrifice was the death of Jesus: Gentiles who are "Christ's" are, like Israel, descendants of Abraham and "heirs according to the promise". The God who raised Jesus from the dead would also give new life to the "mortal bodies" of Gentile Christians, who had become with Israel the "children of God" and were therefore no longer "in the flesh". 
Modern Christian churches tend to be much more concerned with how humanity can be saved from a universal condition of sin and death than the question of how both Jews and Gentiles can be in God's family. According to both Catholic and Protestant doctrine, salvation comes by Jesus' substitutionary death and resurrection. The Catholic Church teaches that salvation does not occur without faithfulness on the part of Christians; converts must live in accordance with principles of love and ordinarily must be baptized. Martin Luther taught that baptism was necessary for salvation, but modern Lutherans and other Protestants tend to teach that salvation is a gift that comes to an individual by God's grace, sometimes defined as "unmerited favor", even apart from baptism.
Christians differ in their views on the extent to which individuals' salvation is pre-ordained by God. Reformed theology places distinctive emphasis on grace by teaching that individuals are completely incapable of self-redemption, but that sanctifying grace is irresistible.Spurgeon, "A Defense of Calvinism".</ref> In contrast Catholics, Orthodox Christians and Arminian Protestants believe that the exercise of free will is necessary to have faith in Jesus.
Trinity.
"Trinity" refers to the teaching that the one God comprises three distinct, eternally co-existing persons; the "Father", the "Son" (incarnate in Jesus Christ), and the "Holy Spirit". Together, these three persons are sometimes called the Godhead, although there is no single term in use in Scripture to denote the unified Godhead. In the words of the Athanasian Creed, an early statement of Christian belief, "the Father is God, the Son is God, and the Holy Spirit is God, and yet there are not three Gods but one God". They are distinct from another: the Father has no source, the Son is begotten of the Father, and the Spirit proceeds from the Father. Though distinct, the three persons cannot be divided from one another in being or in operation.
The Trinity is an essential doctrine of mainstream Christianity. From earlier than the times of the Nicene Creed, 325 CE, Christianity advocated the triune mystery-nature of God as a normative profession of faith. According to Roger E. Olson and Christopher Hall, through prayer, meditation, study and practice, the Christian community concluded "that God must exist as both a unity and trinity", codifying this in ecumenical council at the end of the 4th century.
According to this doctrine, God is not divided in the sense that each person has a third of the whole; rather, each person is considered to be fully God (see Perichoresis). The distinction lies in their relations, the Father being unbegotten; the Son being begotten of the Father; and the Holy Spirit proceeding from the Father and (in Western Christian theology) from the Son. Regardless of this apparent difference, the three "persons" are each eternal and omnipotent. Other Christian religions including Unitarian Universalism, Jehovah's Witnesses, Mormonism and others do not share those views on the Trinity.
The word "trias", from which "trinity" is derived, is first seen in the works of Theophilus of Antioch. He wrote of "the Trinity of God (the Father), His Word (the Son) and His Wisdom (Holy Spirit)". The term may have been in use before this time. Afterwards it appears in Tertullian. In the following century the word was in general use. It is found in many passages of Origen.
Trinitarians.
"Trinitarianism" denotes those Christians who believe in the concept of the "Trinity". Almost all Christian denominations and Churches hold Trinitarian beliefs. Although the words "Trinity" and "Triune" do not appear in the Bible, theologians beginning in the 3rd century developed the term and concept to facilitate comprehension of the New Testament teachings of God as Father, God as Jesus the Son, and God as the Holy Spirit. Since that time, Christian theologians have been careful to emphasize that Trinity does not imply three gods, nor that each member of the Trinity is one-third of an infinite God; Trinity is defined as one God in three Persons. 
Nontrinitarians.
"Nontrinitarianism" refers to theology that rejects the doctrine of the Trinity. Various nontrinitarian views, such as adoptionism or modalism, existed in early Christianity, leading to the disputes about Christology. Nontrinitarianism later appeared again in the Gnosticism of the Cathars in the 11th through 13th centuries, and by groups with Unitarian theology in the Protestant Reformation of the 16th century, and in the Age of Enlightenment of the 18th century, and in some groups arising during the Second Great Awakening of the 19th century.
Scriptures.
Christianity, like other religions, has adherents whose beliefs and biblical interpretations vary. Christianity regards the biblical canon, the Old Testament and the New Testament, as the inspired word of God. The traditional view of inspiration is that God worked through human authors so that what they produced was what God wished to communicate. The Greek word referring to inspiration in is "theopneustos", which literally means "God-breathed".
Some believe that divine inspiration makes our present Bibles inerrant. Others claim inerrancy for the Bible in its original manuscripts, although none of those are extant. Still others maintain that only a particular translation is inerrant, such as the King James Version. Another closely related view is Biblical infallibility or limited inerrancy, which affirms that the Bible is free of error as a guide to salvation, but may include errors on matters such as history, geography or science.
The books of the Bible accepted by the Orthodox, Catholic and Protestant churches vary somewhat, with Jews accepting only the Hebrew Bible as canonical; there is however substantial overlap. These variations are a reflection of the range of traditions, and of the councils that have convened on the subject. Every version of the Old Testament always includes the books of the Tanakh, the canon of the Hebrew Bible. The Catholic and Orthodox canons, in addition to the Tanakh, also include the Deuterocanonical Books as part of the Old Testament. These books appear in the Septuagint, but are regarded by Protestants to be apocryphal. However, they are considered to be important historical documents which help to inform the understanding of words, grammar and syntax used in the historical period of their conception. Some versions of the Bible include a separate Apocrypha section between the Old Testament and the New Testament. The New Testament, originally written in Koine Greek, contains 27 books which are agreed upon by all churches.
Modern scholarship has raised many issues with the Bible. While the Authorized King James Version is held to by many because of its striking English prose, in fact it was translated from the Erasmus Greek Bible which in turn "was based on a single 12th Century manuscript that is one of the worst manuscripts we have available to us". Much scholarship in the past several hundred years has gone into comparing different manuscripts in order to reconstruct the original text. Another issue is that several books are considered to be forgeries. The injunction that women "be silent and submissive" in 1 Timothy 12 is thought by many to be a forgery by a follower of Paul, a similar phrase in 1 Corinthians 14, which is thought to be by Paul, appears in different places in different manuscripts and is thought to originally be a margin note by a copyist. Other verses in 1 Corinthians, such as 1 Corinthians 11:2–16 where women are instructed to wear a covering over their hair "when they pray or prophesies", contradict this verse.
A final issue with the Bible is the way in which books were selected for inclusion in the New Testament. Other Gospels have now been recovered, such as those found near Nag Hammadi in 1945, and while some of these texts are quite different from what Christians have been used to, it should be understood that some of this newly recovered Gospel material is quite possibly contemporaneous with, or even earlier than, the New Testament Gospels. The core of the Gospel of Thomas, in particular, may date from as early as 50 AD, and if so would provide an insight into the earliest gospel texts that underlie the canonical Gospels, texts that are mentioned in Luke 1:1–2. The Gospel of Thomas contains much that is familiar from the canonical Gospels—verse 113, for example ("The Father's Kingdom is spread out upon the earth, but people do not see it"), is reminiscent of Luke 17:20–21—and the Gospel of John, with a terminology and approach that is suggestive of what was later termed "Gnosticism", has recently been seen as a possible response to the Gospel of Thomas, a text that is commonly labelled "proto-Gnostic". Scholarship, then, is currently exploring the relationship in the Early Church between mystical speculation and experience on the one hand and the search for church order on the other, by analyzing new-found texts, by subjecting canonical texts to further scrutiny, and by an examination of the passage of New Testament texts to canonical status.
Catholic and Orthodox interpretations.
In antiquity, two schools of exegesis developed in Alexandria and Antioch. Alexandrine interpretation, exemplified by Origen, tended to read Scripture allegorically, while Antiochene interpretation adhered to the literal sense, holding that other meanings (called "theoria") could only be accepted if based on the literal meaning.
Catholic theology distinguishes two senses of scripture: the literal and the spiritual.
The "literal" sense of understanding scripture is the meaning conveyed by the words of Scripture. The "spiritual" sense is further subdivided into:
Regarding exegesis, following the rules of sound interpretation, Catholic theology holds:
Protestant interpretation.
Some Protestant interpreters make use of typology.
Eschatology.
The end of things, whether the end of an individual life, the end of the age, or the end of the world, broadly speaking is Christian eschatology; the study of the destiny of humans as it is revealed in the Bible. The major issues in Christian eschatology are the Tribulation, death and the afterlife, the Rapture, the Second Coming of Jesus, Resurrection of the Dead, Heaven and Hell, Millennialism, the Last Judgment, the end of the world, and the New Heavens and New Earth.
Christians believe that the second coming of Christ will occur at the end of time after a period of severe persecution (the Great Tribulation). All who have died will be resurrected bodily from the dead for the Last Judgment. Jesus will fully establish the Kingdom of God in fulfillment of scriptural prophecies.
Death and afterlife.
Most Christians believe that human beings experience divine judgment and are rewarded either with eternal life or eternal damnation. This includes the general judgement at the resurrection of the dead as well as the belief (held by Roman Catholics, Orthodox and most Protestants) in a judgment particular to the individual soul upon physical death.
In Roman Catholicism, those who die in a state of grace, i.e., without any mortal sin separating them from God, but are still imperfectly purified from the effects of sin, undergo purification through the intermediate state of purgatory to achieve the holiness necessary for entrance into God's presence. Those who have attained this goal are called "saints" (Latin "sanctus", "holy").
Some Christian groups, such as Seventh-day Adventists, hold to mortalism, the belief that the human soul is not naturally immortal, and is unconscious during the intermediate state between bodily death and resurrection. These Christians also hold to Annihilationism, the belief that subsequent to the final judgement, the wicked will cease to exist rather than suffer everlasting torment. Jehovah's Witnesses hold to a similar view.
Worship.
Justin Martyr described 2nd-century Christian liturgy in his "First Apology" (c. 150) to Emperor Antoninus Pius, and his description remains relevant to the basic structure of Christian liturgical worship:
Thus, as Justin described, Christians assemble for communal worship on Sunday, the day of the resurrection, though other liturgical practices often occur outside this setting. Scripture readings are drawn from the Old and New Testaments, but especially the gospel accounts. Often these are arranged on an annual cycle, using a book called a lectionary. Instruction is given based on these readings, called a sermon, or homily. There are a variety of congregational prayers, including thanksgiving, confession, and intercession, which occur throughout the service and take a variety of forms including recited, responsive, silent, or sung. The Lord's Prayer, or Our Father, is regularly prayed.
Some groups depart from this traditional liturgical structure. A division is often made between "High" church services, characterized by greater solemnity and ritual, and "Low" services, but even within these two categories there is great diversity in forms of worship. Seventh-day Adventists meet on Saturday, while others do not meet on a weekly basis. Charismatic or Pentecostal congregations may spontaneously feel led by the Holy Spirit to action rather than follow a formal order of service, including spontaneous prayer. Quakers sit quietly until moved by the Holy Spirit to speak.
Some evangelical services resemble concerts with rock and pop music, dancing, and use of multimedia. For groups which do not recognize a priesthood distinct from ordinary believers the services are generally led by a minister, preacher, or pastor. Still others may lack any formal leaders, either in principle or by local necessity. Some churches use only a cappella music, either on principle (for example, many Churches of Christ object to the use of instruments in worship) or by tradition (as in Orthodoxy).
Nearly all forms of churchmanship celebrate the Eucharist (Holy Communion), which consists of a consecrated meal. It is reenacted in accordance with Jesus' instruction at the Last Supper that his followers do in remembrance of him as when he gave his disciples bread, saying, "This is my body", and gave them wine saying, "This is my blood". Some Christian denominations practice closed communion. They offer communion to those who are already united in that denomination or sometimes individual church. Catholics restrict participation to their members who are not in a state of mortal sin. Most other churches practice open communion since they view communion as a means to unity, rather than an end, and invite all believing Christians to participate.
Worship can be varied for special events like baptisms or weddings in the service or significant feast days. In the early church, Christians and those yet to complete initiation would separate for the Eucharistic part of the worship. In many churches today, adults and children will separate for all or some of the service to receive age-appropriate teaching. Such children's worship is often called Sunday school or Sabbath school (Sunday schools are often held before rather than during services).
Sacraments.
In Christian belief and practice, a "sacrament" is a rite, instituted by Christ, that mediates grace, constituting a sacred mystery. The term is derived from the Latin word "sacramentum", which was used to translate the Greek word for "mystery". Views concerning both what rites are sacramental, and what it means for an act to be a sacrament vary among Christian denominations and traditions.
The most conventional functional definition of a sacrament is that it is an outward sign, instituted by Christ, that conveys an inward, spiritual grace through Christ. The two most widely accepted sacraments are Baptism and the Eucharist (or Holy Communion), however, the majority of Christians also recognize five additional sacraments: Confirmation (Chrismation in the Orthodox tradition), Holy orders (ordination), Penance (or Confession), Anointing of the Sick, and Matrimony (see Christian views on marriage).
Taken together, these are the Seven Sacraments as recognized by churches in the High Church tradition—notably Roman Catholic, Eastern Orthodox, Oriental Orthodox, Independent Catholic, Old Catholic, many Anglicans, and some Lutherans. Most other denominations and traditions typically affirm only Baptism and Eucharist as sacraments, while some Protestant groups, such as the Quakers, reject sacramental theology. Christian denominations, such as Baptists, which believe these rites do not communicate grace, prefer to call Baptism and Holy Communion "ordinances" rather than sacraments.
Liturgical calendar.
Roman Catholics, Anglicans, Eastern Christians, and traditional Protestant communities frame worship around the liturgical year. The liturgical cycle divides the year into a series of seasons, each with their theological emphases, and modes of prayer, which can be signified by different ways of decorating churches, colours of paraments and vestments for clergy, scriptural readings, themes for preaching and even different traditions and practices often observed personally or in the home.
Western Christian liturgical calendars are based on the cycle of the Roman Rite of the Catholic Church, and Eastern Christians use analogous calendars based on the cycle of their respective rites. Calendars set aside holy days, such as solemnities which commemorate an event in the life of Jesus or Mary, the saints, periods of fasting such as Lent, and other pious events such as memoria or lesser festivals commemorating saints. Christian groups that do not follow a liturgical tradition often retain certain celebrations, such as Christmas, Easter and Pentecost: these are the celebrations of Christ's birth, resurrection and the descent of the Holy Spirit upon the Church, respectively. A few denominations make no use of a liturgical calendar.
Symbols.
Christianity has not generally practiced aniconism, or the avoidance or prohibition of types of images, even if the early Jewish Christians sects, as well as some modern denominations, preferred to some extent not to use figures in their symbols, by invoking the Decalogue's prohibition of idolatry.
The cross, which is today one of the most widely recognized symbols in the world, was used as a Christian symbol from the earliest times. Tertullian, in his book "De Corona", tells how it was already a tradition for Christians to trace repeatedly on their foreheads the sign of the cross. Although the cross was known to the early Christians, the crucifix did not appear in use until the 5th century.
Among the symbols employed by the primitive Christians, that of the fish or Ichthys seems to have ranked first in importance. From monumental sources such as tombs it is known that the symbolic fish was familiar to Christians from the earliest times. The fish was depicted as a Christian symbol in the first decades of the 2nd century. Its popularity among Christians was due principally, it would seem, to the famous acrostic consisting of the initial letters of five Greek words forming the word for fish (Ichthys), which words briefly but clearly described the character of Christ and the claim to worship of believers: "Iesous Christos Theou Yios Soter" (Ίησοῦς Χριστός, Θεοῦ Υἱός, Σωτήρ), meaning, "Jesus Christ, Son of God, Savior".
Other major Christian symbols include the chi-rho monogram, the dove (symbolic of the Holy Spirit), the sacrificial lamb (symbolic of Christ's sacrifice), the vine (symbolizing the necessary connectedness of the Christian with Christ) and many others. These all derive from writings found in the New Testament.
Baptism.
Baptism is the ritual act, with the use of water, by which a person is admitted to membership of the Church. Beliefs on baptism vary among denominations. Differences occur firstly, on whether the act has any spiritual significance, some churches hold to the doctrine of Baptismal Regeneration, which affirms that baptism creates or strengthens a person's faith, and is intimately linked to salvation, this view is held by Catholic and Eastern Orthodox churches as well as Lutherans and Anglicans, while others simply acknowledge it as a purely symbolic act, an external public declaration of the inward change which has taken place in the person. Secondly, there are differences of opinion on the methodology of the act. These methods being: Baptism by Immersion; if immersion is total, Baptism by Submersion; and Baptism by Affusion (pouring) and Baptism by Aspersion (sprinkling). Those who hold the first view may also adhere to the tradition of Infant Baptism;"As an initiatory rite into membership of the Family of God, baptismal candidates are symbolically purified or washed as their sins have been forgiven and washed away" (William H. Brackney, Doing Baptism Baptist Style Believer's Baptism)</ref> the Orthodox Churches all practice infant baptism and always baptize by total immersion repeated three times in the name of the Father, the Son and the Holy Spirit.
Prayer.
Jesus' teaching on prayer in the Sermon on the Mount displays a distinct lack of interest in the external aspects of prayer. A concern with the techniques of prayer is condemned as 'pagan', and instead a simple trust in God's fatherly goodness is encouraged. Elsewhere in the New Testament this same freedom of access to God is also emphasized. This confident position should be understood in light of Christian belief in the unique relationship between the believer and Christ through the indwelling of the Holy Spirit.
In subsequent Christian traditions, certain physical gestures are emphasized, including medieval gestures such as genuflection or making the sign of the cross. Kneeling, bowing and prostrations (see also poklon) are often practiced in more traditional branches of Christianity. Frequently in Western Christianity the hands are placed palms together and forward as in the feudal commendation ceremony. At other times the older orans posture may be used, with palms up and elbows in.
"Intercessory prayer" is prayer offered for the benefit of other people. There are many intercessory prayers recorded in the Bible, including prayers of the Apostle Peter on behalf of sick persons and by prophets of the Old Testament in favor of other people. In the New Testament book of James no distinction is made between the intercessory prayer offered by ordinary believers and the prominent Old Testament prophet Elijah. The effectiveness of prayer in Christianity derives from the power of God rather than the status of the one praying.
The ancient church, in both Eastern Christianity and Western Christianity, developed a tradition of asking for the intercession of (deceased) saints, and this remains the practice of most Eastern Orthodox, Oriental Orthodox, Roman Catholic, and some Anglican churches. Churches of the Protestant Reformation however rejected prayer to the saints, largely on the basis of the sole mediatorship of Christ. The reformer Huldrych Zwingli admitted that he had offered prayers to the saints until his reading of the Bible convinced him that this was idolatrous.
According to the Catechism of the Catholic Church: "Prayer is the raising of one's mind and heart to God or the requesting of good things from God." The Book of Common Prayer in the Anglican tradition is a guide which provides a set order for church services, containing set prayers, scripture readings, and hymns or sung Psalms.
History.
Early Church and Christological Councils.
Christianity began as a Jewish sect in the Levant of the middle east in the mid-1st century. Other than Second Temple Judaism, the primary religious influences of early Christianity are Zoroastrianism and Gnosticism. John Bowker states that Christian ideas such as "angels, the end of the world, a final judgment, the resurrection, and heaven and hell received form and substance from ... Zoroastrian beliefs". Its earliest development took place under the leadership of the remaining Twelve Apostles, particularly Saint Peter, and Paul the Apostle, followed by the early bishops, whom Christians consider the successors of the Apostles.
According to the Christian scriptures, Christians were from the beginning subject to persecution by some Jewish and Roman religious authorities, who disagreed with the apostles' teachings (See Split of early Christianity and Judaism). This involved punishments, including death, for Christians such as Stephen and James, son of Zebedee. Larger-scale persecutions followed at the hands of the authorities of the Roman Empire, first in the year 64, when Emperor Nero blamed them for the Great Fire of Rome. According to Church tradition, it was under Nero's persecution that early Church leaders Peter and Paul of Tarsus were each martyred in Rome.
Further widespread persecutions of the Church occurred under nine subsequent Roman emperors, most intensely under Decius and Diocletian. From the year 150, Christian teachers began to produce theological and apologetic works aimed at defending the faith. These authors are known as the Church Fathers, and study of them is called Patristics. Notable early Fathers include Ignatius of Antioch, Polycarp, Justin Martyr, Irenaeus, Tertullian, Clement of Alexandria, and Origen. However, Armenia is considered the first nation to accept Christianity in 301 AD.
King Trdat IV made Christianity the state religion in Armenia between 301 and 314, it was not an entirely new religion in Armenia. It penetrated into the country from at least the third century, but may have been present even earlier.
End of Roman persecution under Emperor Constantine (313 AD).
State persecution ceased in the 4th century, when Constantine I issued an edict of toleration in 313. On 27 February 380, Emperor Theodosius I enacted a law establishing Nicene Christianity as the state church of the Roman Empire. From at least the 4th century, Christianity has played a prominent role in the shaping of Western civilization.
Constantine was also instrumental in the convocation of the First Council of Nicaea in 325, which sought to address the Arian heresy and formulated the Nicene Creed, which is still used by the Catholic Church, Eastern Orthodoxy, Anglican Communion, and many Protestant churches. Nicaea was the first of a series of Ecumenical (worldwide) Councils which formally defined critical elements of the theology of the Church, notably concerning Christology. The Assyrian Church of the East did not accept the third and following Ecumenical Councils, and are still separate today.
The presence of Christianity in Africa began in the middle of the 1st century in Egypt, and by the end of the 2nd century in the region around Carthage. Mark the Evangelist started the Coptic Orthodox Church of Alexandria in about 43 AD. Important Africans who influenced the early development of Christianity includes Tertullian, Clement of Alexandria, Origen of Alexandria, Cyprian, Athanasius and Augustine of Hippo. The later rise of Islam in North Africa reduced the size and numbers of Christian congregations, leaving only the Coptic Church in Egypt, the Ethiopian Orthodox Tewahedo Church in the Horn of Africa, and the Nubian Church in the Sudan (Nobatia, Makuria, and Alodia).
In terms of prosperity and cultural life, the Byzantine Empire was one of the peaks in Christian history and Orthodox civilization, and Constantinople remained the leading city of the Christian world in size, wealth, and culture. There was a renewed interest in classical Greek philosophy, as well as an increase in literary output in vernacular Greek. Byzantine art and literature held a pre-eminent place in Europe, and the cultural impact of Byzantine art on the west during this period was enormous and of long lasting significance.
Early Middle Ages.
With the decline and fall of the Roman Empire in the west, the papacy became a political player, first visible in Pope Leo's diplomatic dealings with Huns and Vandals. The church also entered into a long period of missionary activity and expansion among the various tribes. While Arianists instituted the death penalty for practicing pagans (see Massacre of Verden as example), Catholicism also spread among the Germanic peoples, the Celtic and Slavic peoples, the Hungarians, and the Baltic peoples. Christianity has been an important part of the shaping of Western civilization, at least since the 4th century.
Around 500, St. Benedict set out his Monastic Rule, establishing a system of regulations for the foundation and running of monasteries. Monasticism became a powerful force throughout Europe, and gave rise to many early centers of learning, most famously in Ireland, Scotland and Gaul, contributing to the Carolingian Renaissance of the 9th century.
In the 7th century Muslims conquered Syria (including Jerusalem), North Africa and Spain. Part of the Muslims' success was due to the exhaustion of the Byzantine empire in its decades long conflict with Persia. Beginning in the 8th century, with the rise of Carolingian leaders, the papacy began to find greater political support in the Frankish Kingdom.
The Middle Ages brought about major changes within the church. Pope Gregory the Great dramatically reformed ecclesiastical structure and administration. In the early 8th century, iconoclasm became a divisive issue, when it was sponsored by the Byzantine emperors. The Second Ecumenical Council of Nicaea (787) finally pronounced in favor of icons. In the early 10th century, Western Christian monasticism was further rejuvenated through the leadership of the great Benedictine monastery of Cluny.
Hebraism, like Hellenism, has been an all-important factor in the development of Western Civilization; Judaism, as the precursor of Christianity, has indirectly had had much to do with shaping the ideals and morality of western nations since the Christian era.
High and Late Middle Ages.
In the west, from the 11th century onward, older cathedral schools developed into universities (see University of Oxford, University of Paris, and University of Bologna.) The traditional medieval universities—evolved from Catholic and Protestant church schools—then established specialized academic structures for properly educating greater numbers of students as professionals. Prof. Walter Rüegg, editor of "A History of the University in Europe", reports that universities then only trained students to become clerics, lawyers, civil servants, and physicians.
Originally teaching only theology, universities steadily added subjects including medicine, philosophy and law, becoming the direct ancestors of modern institutions of learning.
The university is generally regarded as an institution that has its origin in the Medieval Christian setting. Prior to the establishment of universities, European higher education took place for hundreds of years in Christian cathedral schools or monastic schools ("Scholae monasticae"), in which monks and nuns taught classes; evidence of these immediate forerunners of the later university at many places dates back to the 6th century AD.
Accompanying the rise of the "new towns" throughout Europe, mendicant orders were founded, bringing the consecrated religious life out of the monastery and into the new urban setting. The two principal mendicant movements were the Franciscans and the Dominicans founded by St. Francis and St. Dominic respectively. Both orders made significant contributions to the development of the great universities of Europe. Another new order were the Cistercians, whose large isolated monasteries spearheaded the settlement of former wilderness areas. In this period church building and ecclesiastical architecture reached new heights, culminating in the orders of Romanesque and Gothic architecture and the building of the great European cathedrals.
From 1095 under the pontificate of Urban II, the Crusades were launched. These were a series of military campaigns in the Holy Land and elsewhere, initiated in response to pleas from the Byzantine Emperor Alexios I for aid against Turkish expansion. The Crusades ultimately failed to stifle Islamic aggression and even contributed to Christian enmity with the sacking of Constantinople during the Fourth Crusade.
Over a period stretching from the 7th to the 13th century, the Christian Church underwent gradual alienation, resulting in a schism dividing it into a so-called Latin or Western Christian branch, the Roman Catholic Church, and an Eastern, largely Greek, branch, the Orthodox Church. These two churches disagree on a number of administrative, liturgical, and doctrinal issues, most notably papal primacy of jurisdiction. The Second Council of Lyon (1274) and the Council of Florence (1439) attempted to reunite the churches, but in both cases the Eastern Orthodox refused to implement the decisions and the two principal churches remain in schism to the present day. However, the Roman Catholic Church has achieved union with various smaller eastern churches.
Beginning around 1184, following the crusade against the Cathar heresy, various institutions, broadly referred to as the Inquisition, were established with the aim of suppressing heresy and securing religious and doctrinal unity within Christianity through conversion and prosecution.
Protestant Reformation and Counter-Reformation.
The 15th-century Renaissance brought about a renewed interest in ancient and classical learning. Another major schism, the Reformation, resulted in the splintering of the Western Christendom into several branches. Martin Luther in 1517 protested against the sale of indulgences and soon moved on to deny several key points of Roman Catholic doctrine.
Other reformers like Zwingli, Calvin, Knox and Arminius further criticized Roman Catholic teaching and worship. These challenges developed into the movement called Protestantism, which repudiated the primacy of the pope, the role of tradition, the seven sacraments, and other doctrines and practices. The Reformation in England began in 1534, when King Henry VIII had himself declared head of the Church of England. Beginning in 1536, the monasteries throughout England, Wales and Ireland were dissolved.
Thomas Müntzer, Andreas Karlstadt and other theologians perceived both the Roman Catholic Church and the confessions of the Magisterial Reformation as corrupted. Their activity brought about the Radical Reformation, which gave birth to various Anabaptist denominations.
Partly in response to the Protestant Reformation, the Roman Catholic Church engaged in a substantial process of reform and renewal, known as the Counter-Reformation or Catholic Reform. The Council of Trent clarified and reasserted Roman Catholic doctrine. During the following centuries, competition between Roman Catholicism and Protestantism became deeply entangled with political struggles among European states.
Meanwhile, the discovery of America by Christopher Columbus in 1492 brought about a new wave of missionary activity. Partly from missionary zeal, but under the impetus of colonial expansion by the European powers, Christianity spread to the Americas, Oceania, East Asia, and sub-Saharan Africa.
Throughout Europe, the divides caused by the Reformation led to outbreaks of religious violence and the establishment of separate state churches in Europe. Lutheranism spread into northern, central and eastern parts of present-day Germany, Livonia and Scandinavia. Anglicanism was established in England in 1534. Calvinism and its varieties (such as Presbyterianism) were introduced in Scotland, the Netherlands, Hungary, Switzerland and France. Arminianism gained followers in the Netherlands and Frisia. Ultimately, these differences led to the outbreak of conflicts in which religion played a key factor. The Thirty Years' War, the English Civil War, and the French Wars of Religion are prominent examples. These events intensified the Christian debate on persecution and toleration.
Post-Enlightenment.
In the era known as the Great Divergence, when in the West the Age of Enlightenment and the Scientific revolution brought about great societal changes, Christianity was confronted with various forms of skepticism and with certain modern political ideologies such as versions of socialism and liberalism. Events ranged from mere anti-clericalism to violent outbursts against Christianity such as the Dechristianisation during the French Revolution, the Spanish Civil War, and general hostility of Marxist movements, especially the Russian Revolution.
Especially pressing in Europe was the formation of nation states after the Napoleonic era. In all European countries, different Christian denominations found themselves in competition, to greater or lesser extents, with each other and with the state. Variables are the relative sizes of the denominations and the religious, political, and ideological orientation of the state. Urs Altermatt of the University of Fribourg, looking specifically at Catholicisms in Europe, identifies four models for the European nations. In traditionally Catholic countries such as Belgium, Spain, and to some extent Austria, religious and national communities are more or less identical. Cultural symbiosis and separation are found in Poland, Ireland, and Switzerland, all countries with competing denominations. Competition is found in Germany, the Netherlands, and again Switzerland, all countries with minority Catholic populations who to a greater or lesser extent did identify with the nation. Finally, separation between religion (again, specifically Catholicism) and the state is found to a great degree in France and Italy, countries where the state actively opposed itself to the authority of the Catholic Church.
The combined factors of the formation of nation states and ultramontanism, especially in Germany and the Netherlands but also in England (to a much lesser extent), often forced Catholic churches, organizations, and believers to choose between the national demands of the state and the authority of the Church, specifically the papacy. This conflict came to a head in the First Vatican Council, and in Germany would lead directly to the Kulturkampf, where liberals and Protestants under the leadership of Bismarck managed to severely restrict Catholic expression and organization.
Christian commitment in Europe dropped as modernity and secularism came into their own in Europe, particularly in the Czech Republic and Estonia, while religious commitments in America have been generally high in comparison to Europe. The late 20th century has shown the shift of Christian adherence to the Third World and southern hemisphere in general, with the western civilization no longer the chief standard bearer of Christianity.
Some Europeans (including diaspora), Indigenous peoples of the Americas, and natives of other continents have revived their respective peoples' historical folk religions. Approximately 7.1 to 10% of Arabs are Christians most prevalent in Egypt, Syria and Lebanon.
Demographics.
With around 2.4 billion adherents, split into three main branches of Catholic, Protestant and Eastern Orthodox, Christianity is the world's largest religion. The Christian share of the world's population has stood at around 33% for the last hundred years, which says that one in three persons on earth are Christians. This masks a major shift in the demographics of Christianity; large increases in the developing world have been accompanied by substantial declines in the developed world, mainly in Europe and North America.
Christianity is the predominant religion in Europe, the Americas and Southern Africa. In Asia, it is the dominant religion in Georgia, Armenia, East Timor and the Philippines. However, it is declining in many areas including the Northern and Western United States, Oceania (Australia and New Zealand), northern Europe (including Great Britain, Scandinavia and other places), France, Germany, the Canadian provinces of Ontario, British Columbia, and Quebec, and parts of Asia (especially the Middle East – due to the Christian emigration, South Korea, Taiwan, and Macau).
The Christian population is not decreasing in Brazil, the Southern United States and the province of Alberta, Canada, but the percentage is decreasing. In countries such as Australia and New Zealand, the Christian population are declining in both numbers and percentage.
Despite the declining numbers, Christianity remains the dominant religion in the Western World, where 70% are Christians. A 2011 Pew Research Center survey found that 76.2% of Europeans, 73.3% in Oceania, and about 86.0% in the Americas (90% in Latin America and 77.4% in North America) described themselves as Christians.
However, there are many charismatic movements that have become well established over large parts of the world, especially Africa, Latin America and Asia. A leading Saudi Arabian Muslim leader Sheikh Ahmad al Qatanni reported on Al Jazeera that every day 16,000 African Muslims convert to Christianity. He claimed that Islam was losing 6 million African Muslims a year to becoming Christians, St. Mary's University study estimated about 10.2 million Muslim convert to Christianity in 2015. as well a significant numbers of Muslims converts to Christianity in Afghanistan, Albania, Azerbaijan Algeria, France, Iran, India, Morocco, Russia, Tunisia, Turkey, Kosovo, and Central Asia. It is also reported that Christianity is popular among people of different backgrounds in India (mostly Hindus), and Malaysia, Mongolia, Nigeria, Vietnam, Singapore, Indonesia, China, Japan, and South Korea.
In most countries in the developed world, church attendance among people who continue to identify themselves as Christians has been falling over the last few decades. Some sources view this simply as part of a drift away from traditional membership institutions, while others link it to signs of a decline in belief in the importance of religion in general.
Christianity, in one form or another, is the sole state religion of the following nations: Argentina (Roman Catholic), Tuvalu (Reformed), Tonga (Methodist), Norway (Lutheran), Costa Rica (Roman Catholic), Kingdom of Denmark (Lutheran), England (Anglican), Georgia (Georgian Orthodox), Greece (Greek Orthodox), Iceland (Lutheran), Liechtenstein (Roman Catholic), Malta (Roman Catholic), Monaco (Roman Catholic), and Vatican City (Roman Catholic).
There are numerous other countries, such as Cyprus, which although do not have an established church, still give official recognition and support to a specific Christian denomination.
Major denominations.
The three primary divisions of Christianity are Roman Catholicism, Eastern Orthodoxy, and Protestantism. There are other Christian groups that do not fit neatly into one of these primary categories. The Nicene Creed is "accepted as authoritative by the Roman Catholic, Eastern Orthodox, Anglican, and major Protestant churches."
There is a diversity of doctrines and practices among groups calling themselves Christian. These groups are sometimes classified under denominations, though for theological reasons many groups reject this classification system. A broader distinction that is sometimes drawn is between Eastern Christianity and Western Christianity, which has its origins in the East–West Schism (Great Schism) of the 11th century.
In addition to the Lutheran and Reformed (or Calvinist) branches of the Reformation, there is Anglicanism after the English Reformation. The Anabaptist tradition was largely ostracized by the other Protestant parties at the time, but has achieved a measure of affirmation in more recent history. Adventist, Baptist, Methodist, Pentecostal and other Protestant confessions arose in the following centuries.
As well as these modern divisions, there were many diverse Christian communities with wildly different Christologies, eschatologies, soteriologies, and cosmologies that existed alongside the "Early Church" which is itself a projected concept to indicate which communities were "proto-orthodox", in that their views would become dominate. In many ways, the first three centuries of Christianity was significantly more diverse than the modern Church.
Catholic.
The Catholic Church comprises those particular Churches, headed by bishops, in communion with the Pope, the Bishop of Rome, as its highest authority in matters of faith, morality and Church governance. Like the Eastern Orthodox, the Roman Catholic Church through apostolic succession traces its origins to the Christian community founded by Jesus Christ. Catholics maintain that the "one, holy, catholic and apostolic church" founded by Jesus subsists fully in the Roman Catholic Church, but also acknowledges other Christian churches and communities and works towards reconciliation among all Christians. The Catholic faith is detailed in the "Catechism of the Catholic Church".
The 2,834 sees are grouped into 24 particular rites, the largest being the Latin Church, each with distinct traditions regarding the liturgy and the administering the sacraments. With more than 1.1 billion baptized members, the Catholic Church is the largest church representing over half of all Christians and one sixth of the world's population.
Various smaller communities, such as the Old Catholic and Independent Catholic Churches, include the word "Catholic" in their title, and share much in common with Roman Catholicism but are no longer in communion with the See of Rome.
Orthodox.
Eastern Orthodoxy comprises those churches in communion with the Patriarchal Sees of the East, such as the Ecumenical Patriarch of Constantinople. Like the Roman Catholic Church, the Eastern Orthodox Church also traces its heritage to the foundation of Christianity through apostolic succession and has an episcopal structure, though the autonomy of its component parts is emphasized, and most of them are national churches. A number of conflicts with Western Christianity over questions of doctrine and authority culminated in the Great Schism. Eastern Orthodoxy is the second largest single denomination in Christianity, with an estimated 225–300 million adherents.
The Oriental Orthodox Churches (also called "Old Oriental Churches") are those eastern churches that recognize the first three ecumenical councils—Nicaea, Constantinople and Ephesus—but reject the dogmatic definitions of the Council of Chalcedon and instead espouse a Miaphysite christology. The Oriental Orthodox communion comprises six groups: Syriac Orthodox, Coptic Orthodox, Ethiopian Orthodox, Eritrean Orthodox, Malankara Orthodox Syrian Church (India) and Armenian Apostolic churches. These six churches, while being in communion with each other are completely independent hierarchically. These churches are generally not in communion with Eastern Orthodox Churches with whom they are in dialogue for erecting a communion.
Protestant.
In the 16th century, Martin Luther, and subsequently Huldrych Zwingli and John Calvin, inaugurated what has come to be called Protestantism. Luther's primary theological heirs are known as Lutherans. Zwingli and Calvin's heirs are far broader denominationally, and are broadly referred to as the Reformed tradition. The oldest Protestant groups separated from the Catholic Church in the Protestant Reformation, often followed by further divisions.
In the 18th century, for example, Methodism grew out of Anglican minister John Wesley's evangelical and revival movement. Several Pentecostal and non-denominational churches, which emphasize the cleansing power of the Holy Spirit, in turn grew out of Methodism. Because Methodists, Pentecostals, and other evangelicals stress "accepting Jesus as your personal Lord and Savior", which comes from Wesley's emphasis of the New Birth, they often refer to themselves as being born-again.
Estimates of the total number of Protestants are very uncertain, but it seems clear that Protestantism is the second largest major group of Christians after Roman Catholicism in number of followers (although the Eastern Orthodox Church is larger than any single Protestant denomination). Often that number is put at more than 800 million, corresponding to nearly 40% of world's Christians. The majority of Protestants are members of just a handful of denominational families, i.e. Adventists, Anglicans, Baptists, Reformed (Calvinists), Lutherans, Methodists and Pentecostals. Nondenominational, evangelical, charismatic, neo-charismatic, independent and other churches are on the rise, and constitute a significant part of Protestant Christianity.
A special grouping are the Anglican churches descended from the Church of England and organised in the Anglican Communion. Some Anglican churches consider themselves both Protestant and Catholic. Some Anglicans consider their church a branch of the "One Holy Catholic Church" alongside of the Roman Catholic and Eastern Orthodox Churches, a concept rejected by the Roman Catholic Church and some Eastern Orthodox.
While Anglicans, Lutherans and the Reformed branches of Protestantism originated in the Magisterial Reformation, other Protestant groups such as the Anabaptists (mostly made-up of Amish, Mennonites, Hutterites and Schwarzenau Brethren/German Baptist groups), originated in the Radical Reformation and are distinguished by their belief in credobaptism.
Some groups of individuals who hold basic Protestant tenets identify themselves simply as "Christians" or "born-again Christians". They typically distance themselves from the confessionalism and/or creedalism of other Christian communities by calling themselves "non-denominational" or "evangelical". Often founded by individual pastors, they have little affiliation with historic denominations.
Restorationists and others.
The Second Great Awakening, a period of religious revival that occurred in the United States during the early 1800s, saw the development of a number of unrelated churches. They generally saw themselves as restoring the original church of Jesus Christ rather than reforming one of the existing churches. A common belief held by Restorationists was that the other divisions of Christianity had introduced doctrinal defects into Christianity, which was known as the Great Apostasy. In Asia, Iglesia ni Cristo is a known restorationist religion that was established during the early 1900s.
Some of the churches originating during this period are historically connected to early 19th-century camp meetings in the Midwest and Upstate New York. American Millennialism and Adventism, which arose from Evangelical Protestantism, influenced the Jehovah's Witnesses movement and, as a reaction specifically to William Miller, the Seventh-day Adventists. Others, including the Christian Church (Disciples of Christ), Evangelical Christian Church in Canada, Churches of Christ, and the Christian churches and churches of Christ, have their roots in the contemporaneous Stone-Campbell Restoration Movement, which was centered in Kentucky and Tennessee. Other groups originating in this time period include the Christadelphians and Latter Day Saint movement. While the churches originating in the Second Great Awakening have some superficial similarities, their doctrine and practices vary significantly.
Esoteric Christians regard Christianity as a mystery religion, and profess the existence and possession of certain esoteric doctrines or practices, hidden from the public but accessible only to a narrow circle of "enlightened", "initiated", or highly educated people. Some of the esoteric Christian institutions include the Rosicrucian Fellowship, the Anthroposophical Society and the Martinism.
Messianic Judaism (or Messianic Movement) is the name of a Christian movement comprising a number of streams, whose members may consider themselves Jewish. It blends elements of religious Jewish practice with evangelical Christianity. Messianic Judaism affirms Christian creeds such as the messiahship and divinity of "Yeshua" (the Hebrew name of Jesus) and the Triune Nature of God, while also adhering to some Jewish dietary laws and customs.
Christian culture.
Western culture, throughout most of its history, has been nearly equivalent to Christian culture, and many of the population of the Western hemisphere could broadly be described as cultural Christians. The notion of "Europe" and the "Western World" has been intimately connected with the concept of "Christianity and Christendom" many even attribute Christianity for being the link that created a unified European identity.
Though Western culture contained several polytheistic religions during its early years under the Greek and Roman empires, as the centralized Roman power waned, the dominance of the Catholic Church was the only consistent force in Europe. Until the Age of Enlightenment, Christian culture guided the course of philosophy, literature, art, music and science. Christian disciplines of the respective arts have subsequently developed into Christian philosophy, Christian art, Christian music, Christian literature etc.
Christianity had a significant impact on education and science and medicine as the church created the bases of the Western system of education, and was the sponsor of founding universities in the Western world as the university is generally regarded as an institution that has its origin in the Medieval Christian setting. Many clerics throughout history have made significant contributions to science and Jesuits in particular have made numerous significant contributions to the development of science. The Civilizing influence of Christianity includes social welfare, founding hospitals, economics (as the Protestant work ethic), politics, architecture, literature and family life.
Eastern Christians (particularly Nestorian Christians) contributed to the Arab Islamic Civilization during the Ummayad and the Abbasid periods by translating works of Greek philosophers to Syriac and afterwards to Arabic. They also excelled in philosophy, science, theology and medicine.
Christians have made a myriad contributions in a broad and diverse range of fields, including the sciences, arts, politics, literatures and business. According to "100 Years of Nobel Prizes" a review of Nobel prizes award between 1901 and 2000 reveals that (65.4%) of Nobel Prizes Laureates, have identified Christianity in its various forms as their religious preference.
"Postchristianity" is the term for the decline of Christianity, particularly in Europe, Canada, Australia and to a minor degree the Southern Cone, in the 20th and 21st centuries, considered in terms of postmodernism. It refers to the loss of Christianity's monopoly on values and world view in historically Christian societies.
Cultural Christians are secular people with a Christian heritage who may not believe in the religious claims of Christianity, but who retain an affinity for the popular culture, art, music, and so on related to it. Another frequent application of the term is to distinguish political groups in areas of mixed religious backgrounds.
Ecumenism.
Christian groups and denominations have long expressed ideals of being reconciled, and in the 20th century, Christian ecumenism advanced in two ways. One way was greater cooperation between groups, such as the Edinburgh Missionary Conference of Protestants in 1910, the Justice, Peace and Creation Commission of the World Council of Churches founded in 1948 by Protestant and Orthodox churches, and similar national councils like the National Council of Churches in Australia which includes Roman Catholics.
The other way was institutional union with new United and uniting churches. Congregationalist, Methodist and Presbyterian churches united in 1925 to form the United Church of Canada, and in 1977 to form the Uniting Church in Australia. The Church of South India was formed in 1947 by the union of Anglican, Baptist, Methodist, Congregationalist, and Presbyterian churches.
The ecumenical, monastic Taizé Community is notable for being composed of more than one hundred brothers from Protestant and Catholic traditions. The community emphasizes the reconciliation of all denominations and its main church, located in Taizé, Saône-et-Loire, France, is named the "Church of Reconciliation". The community is internationally known, attracting over 100,000 young pilgrims annually.
Steps towards reconciliation on a global level were taken in 1965 by the Roman Catholic and Orthodox churches mutually revoking the excommunications that marked their Great Schism in 1054; the Anglican Roman Catholic International Commission (ARCIC) working towards full communion between those churches since 1970; and some Lutheran and Roman Catholic churches signing the Joint Declaration on the Doctrine of Justification in 1999 to address conflicts at the root of the Protestant Reformation. In 2006, the World Methodist Council, representing all Methodist denominations, adopted the declaration.
Criticism and apologetics.
Criticism of Christianity and Christians goes back to the Apostolic age, with the New Testament recording friction between the followers of Jesus and the Pharisees and scribes (e.g. and ). In the 2nd century, Christianity was criticized by the Jews on various grounds, e.g. that the prophecies of the Hebrew Bible could not have been fulfilled by Jesus, given that he did not have a successful life. By the 3rd century, criticism of Christianity had mounted, partly as a defense against it, and the 15-volume "Adversus Christianos" by Porphyry was written as a comprehensive attack on Christianity, in part building on the pre-Christian concepts of Plotinus.
By the 12th century, the Mishneh Torah (i.e., Rabbi Moses Maimonides) was criticizing Christianity on the grounds of idol worship, in that Christians attributed divinity to Jesus who had a physical body. In the 19th century, Nietzsche began to write a series of polemics on the "unnatural" teachings of Christianity (e.g. sexual abstinence), and continued his criticism of Christianity to the end of his life. In the 20th century, the philosopher Bertrand Russell expressed his criticism of Christianity in "Why I Am Not a Christian", formulating his rejection of Christianity in the setting of logical arguments.
Criticism of Christianity continues to date, e.g. Jewish and Muslim theologians criticize the doctrine of the Trinity held by most Christians, stating that this doctrine in effect assumes that there are three Gods, running against the basic tenet of monotheism. New Testament scholar Robert M. Price has outlined the possibility that some Bible stories are based partly on myth in "The Christ Myth Theory and its problems".
Christian apologetics aims to present a rational basis for Christianity. The word "apologetic" comes from the Greek word "apologeomai", meaning "in defense of". Christian apologetics has taken many forms over the centuries, starting with Paul the Apostle. The philosopher Thomas Aquinas presented five arguments for God's existence in the "Summa Theologica", while his "Summa contra Gentiles" was a major apologetic work. Another famous apologist, G. K. Chesterton, wrote in the early twentieth century about the benefits of religion and, specifically, Christianity. Famous for his use of paradox, Chesterton explained that while Christianity had the most mysteries, it was the most practical religion. He pointed to the advance of Christian civilizations as proof of its practicality. The physicist and priest John Polkinghorne, in his "Questions of Truth" discusses the subject of religion and science, a topic that other Christian apologists such as Ravi Zacharias, John Lennox and William Lane Craig have engaged, with the latter two men opining that the inflationary Big Bang model is evidence for the existence of God.

</doc>
<doc id="5213" url="https://en.wikipedia.org/wiki?curid=5213" title="Computing">
Computing

Computing is any goal-oriented activity requiring, benefiting from, or creating a mathematical sequence of steps known as an algorithm — e.g. through computers. Computing includes designing, developing and building hardware and software systems; processing, structuring, and managing various kinds of information; doing scientific research on and with computers; making computer systems behave intelligently; and creating and using communications and entertainment media. The field of computing includes computer engineering, software engineering, computer science, information systems, and information technology.
Definitions.
The ACM "Computing Curricula 2005" defined "computing" as follows:
"In a general way, we can define computing to mean any goal-oriented activity requiring, benefiting from, or creating computers. Thus, computing includes designing and building hardware and software systems for a wide range of purposes; processing, structuring, and managing various kinds of information; doing scientific studies using computers; making computer systems behave intelligently; creating and using communications and entertainment media; finding and gathering information relevant to any particular purpose, and so on. The list is virtually endless, and the possibilities are vast."
and it defines five sub-disciplines of the "computing" field: Computer Science, Computer Engineering, Information Systems, Information Technology, and Software Engineering.
However, "Computing Curricula 2005" also recognizes that the meaning of "computing" depends on the context:
"Computing also has other meanings that are more specific, based on the context in which the term is used. For example, an information systems specialist will view computing somewhat differently from a software engineer. Regardless of the context, doing computing well can be complicated and difficult. Because society needs people to do computing well, we must think of computing not only as a profession but also as a discipline."
The term "computing" has sometimes been narrowly defined, as in a 1989 ACM report on "Computing as a Discipline":
"The discipline of computing is the systematic study of algorithmic
processes that describe and transform information: their theory, analysis, design, efficiency, implementation, and application. The fundamental question underlying all computing is "What can be (efficiently) automated?"
The term "computing" is also synonymous with counting and calculating. In earlier times, it was used in reference to the action performed by mechanical computing machines, and before that, to human computers.
History of computing.
The history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables.
Computing is intimately tied to the representation of numbers. But long before abstractions like "the number" arose, there were mathematical concepts to serve the purposes of civilization. These concepts include one-to-one correspondence (the basis of counting), comparison to a standard (used for measurement), and the "3-4-5" right triangle (a device for assuring a "right angle").
The earliest known tool for use in computation was the abacus, and it was thought to have been invented in Babylon circa 2400 BC. Its original style of usage was by lines drawn in sand with pebbles. Abaci, of a more modern design, are still used as calculation tools today. This was the first known computer and most advanced system of calculation known to date - preceding Greek methods by 2,000 years.
Computer.
A computer is a machine that manipulates data according to a set of instructions called a computer program. The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop a sequence of steps known as an algorithm. Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the central processing unit type.
The execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions.
Computer software and hardware.
Computer software or just "software", is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer for some purposes. In other words, software is a set of "programs, procedures, algorithms" and its "documentation" concerned with the operation of a data processing system. Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term hardware (meaning physical devices). In contrast to hardware, software is intangible. Software is also sometimes used in a more narrow sense, meaning application software only.
Application software.
Application software, also known as an "application" or an "app", is a computer software designed to help the user to perform specific tasks. Examples include enterprise software, accounting software, office suites, graphics software and media players. Many application programs deal principally with documents. Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install one.
Application software is contrasted with system software and middleware, which manage and integrate a computer's capabilities, but typically do not directly apply them in the performance of tasks that benefit the user. The system software serves the application, which in turn serves the user.
Application software applies the power of a particular computing platform or system software to a particular purpose. Some apps such as Microsoft Office are available in versions for several different platforms; others have narrower requirements and are thus called, for example, a Geography application for Windows or an Android application for education or Linux gaming. Sometimes a new and popular application arises that only runs on one platform, increasing the desirability of that platform. This is called a killer application.
System software.
System software, or systems software, is computer software designed to operate and control the computer hardware and to provide a platform for running application software. System software includes operating systems, utility software, device drivers, window systems, and firmware. Frequently development tools such as compilers, linkers, and debuggers are classified as system software.
Computer network.
A computer network, often simply referred to as a network, is a collection of hardware components and computers interconnected by communication channels that allow sharing of resources and information. Where at least one process in one device is able to send/receive data to/from at least one process residing in a remote device, then the two devices are said to be in a network.
Networks may be classified according to a wide variety of characteristics such as the medium used to transport the data, communications protocol used, scale, topology, and organizational scope.
Communications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for network programming. Well-known communications protocols are Ethernet, a hardware and Link Layer standard that is ubiquitous in local area networks, and the Internet Protocol Suite, which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, as well as host-to-host data transfer, and application-specific data transmission formats.
Computer networking is sometimes considered a sub-discipline of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of these disciplines.
Internet.
The Internet is a global system of interconnected computer networks that use the standard Internet protocol suite (TCP/IP) to serve billions of users that consists of millions of private, public, academic, business, and government networks, of local to global scope, that are linked by a broad array of electronic, wireless and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web (WWW) and the infrastructure to support email.
Computer user.
A user is an agent, either a human agent (end-user) or software agent, who uses a computer or network service. A user often has a user account and is identified by a username (also user name), screen name (also screenname), nickname (also nick), or handle, which derives from the identical Citizen's Band radio term.
In hacker-related terminology, users are divided into "lusers" and "power users".
In projects where the system actor is another system or a software agent, there may be no end-user. In that case, the end-users for the system is indirect end-users.
End-user.
The term end-user refers to the ultimate operator of a piece of software, but it is also a concept in software engineering, referring to an abstraction of that group of end users of computers (i.e. the expected user or target-user). The term is used to distinguish those who only operate the software from the developer of the system, who knows a programming language and uses it to create new functions for end-users.
Computer programming.
Computer programming in general is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a programming language, which is an artificial language often more restrictive or demanding than natural languages, but easily translated by the computer. The purpose of programming is to invoke the desired behavior (customization) from the machine. The process of writing high quality source code requires knowledge of both the application's domain "and" the computer science domain. The highest-quality software is thus developed by a team of various domain experts, each person a specialist in some area of development. But the term "programmer" may apply to a range of program quality, from hacker to open source contributor to professional. And a single programmer could do most or all of the computer programming needed to generate the proof of concept to launch a new "killer" application.
Computer programmer.
A programmer, computer programmer, or coder is a person who writes computer software. The term "computer programmer" can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (C, C++, Java, Lisp, Python, etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with "web". The term "programmer" can be used to refer to a software developer, software engineer, computer scientist, or software analyst. However, members of these professions typically possess other software engineering skills, beyond programming.
Computer industry.
The computer industry is made up of all of the businesses involved in developing computer software, designing computer hardware and computer networking infrastructures, the manufacture of computer components and the provision of information technology services including system administration and maintenance.
Software industry.
The software industry includes businesses engaged in development, maintenance and publication of software. The industry also includes software services, such as training, documentation, and consulting.
Sub-disciplines of computing.
Computer engineering.
Computer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, but also how they integrate into the larger picture.
Software engineering.
Software engineering (SE) is the application of a systematic, disciplined, quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software. In layman's terms, it is the act of using insights to conceive, model and scale a solution to a problem. The first reference to the term is the 1968 NATO Software Engineering Conference and was meant to provoke thought regarding the perceived "software crisis" at the time. "Software development", a much used and more generic term, does not necessarily subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the Software Engineering Body of Knowledge (SWEBOK). The SWEBOK has become an internationally accepted standard ISO/IEC TR 19759:2005.
Computer science.
Computer science or computing science (abbreviated CS or Comp Sci) is the scientific and practical approach to computation and its applications. A computer scientist specializes in the theory of computation and the design of computational systems.
Its subfields can be divided into practical techniques for its implementation and application in computer systems and purely theoretical areas. Some, such as computational complexity theory, which studies fundamental properties of computational problems, are highly abstract, while others, such as computer graphics, emphasize real-world applications. Still others focus on the challenges in implementing computations. For example, programming language theory studies approaches to description of computations, while the study of computer programming itself investigates various aspects of the use of programming languages and complex systems, and human–computer interaction focuses on the challenges in making computers and computations useful, usable, and universally accessible to humans.
Information systems.
"Information systems (IS)" is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute data. Computing Careers says on their website that "A majority of IS programs are located in business schools; however, they may have different names such as management information systems, computer information systems, or business information systems. All IS degrees combine business and computing topics, but the emphasis between technical and organizational issues varies among programs. For example, programs differ substantially in the amount of programming required."
The study bridges business and computer science using the theoretical foundations of information and computation to study various business models and related algorithmic processes within a computer science discipline. Computer Information System(s) (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society while IS emphasizes functionality over design.
Information technology.
Information technology (IT) is the application of computers and telecommunications equipment to store, retrieve, transmit and manipulate data, often in the context of a business or other enterprise. The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, such as computer hardware, software, electronics, semiconductors, internet, telecom equipment, e-commerce and computer services.
Systems administration.
A system administrator, IT systems administrator, systems administrator, or sysadmin is a person employed to maintain and operate a computer system and/or network. The duties of a system administrator are wide-ranging, and vary widely from one organization to another. Sysadmins are usually charged with installing, supporting and maintaining servers or other computer systems, and planning for and responding to service outages and other problems. Other duties may include scripting or light programming, project management for systems-related projects, supervising or training computer operators, and being the consultant for computer problems beyond the knowledge of technical support staff.
Research and emerging technologies.
DNA-based computing and quantum computing are areas of active research in both hardware and software (such the development of quantum algorithms). Potential infrastructure for future technologies includes DNA origami on photolithography and quantum antennae for transferring information between ion traps. By 2011, researchers had entangled 14 qubits. Fast digital circuits (including those based on Josephson junctions and rapid single flux quantum technology) are becoming more nearly realizable with the discovery of nanoscale superconductors.
Fiber-optic and photonic (optical) devices, which already have been used to transport data over long distances, have started being used by data centers, side by side with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. IBM has created an integrated circuit with both electronic and optical information processing in one chip. This is denoted "CMOS-integrated nanophotonics" or (CINP). One benefit of optical interconnects is that motherboards which formerly required a certain kind of system on a chip (SoC) can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs.

</doc>
<doc id="5215" url="https://en.wikipedia.org/wiki?curid=5215" title="Casino">
Casino

In modern English, a casino is a facility which houses and accommodates certain types of gambling activities. The industry that deals in casinos is called the gaming industry. Casinos are most commonly built near or combined with hotels, restaurants, retail shopping, cruise ships or other tourist attractions. There is much debate over whether or not the social and economic consequences of casino gambling outweigh the initial revenue that may be generated. In the United States, some states that have high unemployment and budget deficits have turned to legalizing casinos, often in places that are not tourist destinations. Some casinos are also known for hosting live entertainment events, such as stand-up comedy, concerts, and sporting events.
Etymology and usage.
The term "casino" is a confusing linguistic false friend for translators.
"Casino" is of Italian origin; the root "casa" (house) originally meant a small country villa, summerhouse, or social club. During the 19th century, the term "casino" came to include other public buildings where pleasurable activities took place; such edifices were usually built on the grounds of a larger Italian villa or palazzo, and were used to host civic town functions, including dancing, gambling, music listening, and sports; examples in Italy include Villa Farnese and Villa Giulia, and in the US the Newport Casino in Newport, Rhode Island. In modern-day Italian, the term "casino" designates a bordello (also called "casa chiusa", literally "closed house"), while the gambling house is spelled "casinò" with an accent.
Not all casinos were used for gaming. The Catalina Casino, a famous landmark overlooking Avalon Harbor on Santa Catalina Island, California, has never been used for traditional games of chance, which were already outlawed in California by the time it was built. The Copenhagen Casino was a theatre, known for the mass public meetings often held in its hall during the 1848 Revolution, which made Denmark a constitutional monarchy. Until 1937, it was a well-known Danish theatre. The Hanko Casino in Hanko, Finland—one of that town's most conspicuous landmarks—was never used for gambling. Rather, it was a banquet hall for the Russian nobility which frequented this spa resort in the late 19th century and is now used as a restaurant.
In military and non-military usage in German and Spanish, a "casino" or "kasino" is an officers' mess. In Italian—the source-language of the word—a "casino" is either a brothel, a mess, or a noisy environment, while a gaming house is called a "casinò".
History of gambling houses.
The precise origin of gambling is unknown. It is generally believed that gambling in some form or another has been seen in almost every society in history. From the Ancient Greeks and Romans to Napoleon's France and Elizabethan England, much of history is filled with stories of entertainment based on games of chance.
The first known European gambling house, not called a casino although meeting the modern definition, was the Ridotto, established in Venice, Italy in 1638 by the Great Council of Venice to provide controlled gambling during the carnival season. It was closed in 1770 as the city government felt it was impoverishing the local gentry.
In American history, early gambling establishments were known as saloons. The creation and importance of saloons was greatly influenced by four major cities: New Orleans, St. Louis, Chicago and San Francisco. It was in the saloons that travelers could find people to talk to, drink with, and often gamble with. During the early 20th century in America, gambling became outlawed and banned by state legislation and social reformers of the time. However, in 1931, gambling was legalized throughout the state of Nevada. America's first legalized casinos were set up in those places. In 1978 New Jersey allowed gambling in Atlantic City, now America's second largest gambling city.
Gambling in casinos.
Most jurisdictions worldwide have a minimum gambling age (16 to 21 years of age in most countries which permit the operation of casinos).
Customers gamble by playing games of chance, in some cases with an element of skill, such as craps, roulette, baccarat, blackjack, and video poker. Most games played have mathematically determined odds that ensure the house has at all times an overall advantage over the players. This can be expressed more precisely by the notion of expected value, which is uniformly negative (from the player's perspective). This advantage is called the "house edge". In games such as poker where players play against each other, the house takes a commission called the rake. Casinos sometimes give out complimentary items or comps to gamblers.
"Payout" is the percentage of funds ("winnings") returned to players.
Casinos in the United States say that a player staking money won from the casino is "playing with the house's money".
Video Lottery Machines (slot machines) have become one of the most popular forms of gambling in casinos. investigative reports have started calling into question whether the modern-day slot-machine is addictive.
Design.
Casino design—regarded as a psychological exercise—is an intricate process that involves optimising floor plan, décor and atmospherics to encourage consumer gambling.
Factors influencing consumer gambling tendencies include sound, odour and lighting. Natasha Dow Schüll, an anthropologist at the Massachusetts Institute of Technology, highlights the audio directors at Silicon Gaming’s decision to make its slot machines resonate in, “the universally pleasant tone of C, sampling existing casino soundscapes to create a sound that would please but not clash”.
Dr Alan Hirsch, founder of the Smell & Taste Treatment and Research Foundation in Chicago, studied the impact of certain scents on gamblers, discerning that a pleasant albeit unidentifiable odour released by Las Vegas slots machines generated approximately 50% more in daily revenue. He suggested that the scent acted as an aphrodisiac, facilitating a more aggressive form of gambling.
Casino designer Roger Thomas is credited with implementing a successful, disruptive design for the Las Vegas Wynn Resorts’ casinos in 2008. He broke casino design convention by introducing natural sunlight and flora to appeal to a female demographic. Thomas inserted skylights and antique clocks, defying the commonplace notion that a casino should be a timeless space.
Markets.
The following lists major casino markets in the world with casino revenue of over $1 billion USD as published in PricewaterhouseCoopers's report on 
the outlook for the global casino market:
By company.
According to Bloomberg, accumulated revenue of biggest casino operator companies worldwide amounted almost 55 billion US dollars as per 2011. SJM Holdings ltd. was the leading company in this field and earned 9.7 billion in 2011, followed by Las Vegas Sands Corp. (7.4 bn). The third biggest casino operator company (based on revenue) was Caesars Entertainment with revenue of 6.2 bn US dollar.
Significant sites.
While there are casinos in many places, a few places have become well-known specifically for gambling. Perhaps the place almost defined by its casino is Monte Carlo, but other places are known as gambling centers.
Monte Carlo, Monaco.
Monte Carlo has a famous casino popular with well-off visitors and is a tourist attraction in its own right. A song and a film named "The Man Who Broke the Bank at Monte Carlo" need no explanation—they clearly refer to the casino.
"Monte Carlo"'s Casino has also been depicted in many books including Ben Mezrich's Busting Vegas, where a group of Massachusetts Institute of Technology students beat the casino out of nearly $1 000 000. This book is based on real people and events; however, many of those events are contested by main character Semyon Dukach.
The casino has made Monte Carlo so well known for games of chance that mathematical methods for solving various problems using many quasi-random numbers—numbers with the statistical distribution of numbers generated by chance—are formally known as Monte Carlo methods. Monte Carlo was part of the plot in a few James Bond novels and films.
Macau.
The former Portuguese colony of Macau, a special administrative region of China since 1999, is a popular destination for visitors who wish to gamble. This started in Portuguese times, when Macau was popular with visitors from nearby British Hong Kong where gambling was more closely regulated. The Venetian Macao is currently the largest casino in the world. Macau also surpassed Las Vegas as the largest gambling market in the world.
Singapore.
Singapore is an up-and-coming destination for visitors wanting to gamble. Although there are currently only two casinos(both foreign owned), in Singapore. The Marina Bay Sands is the most expensive standalone casino in the world, at a price of US$8 Billion, and is among the worlds top ten most expensive buildings. The Resorts World Sentosa has the worlds largest oceanarium.
United States.
With currently over 900 casinos, the United States has the largest number of casinos in the world. The number continues to steadily grow as more states seek to legalize casinos. 38 states now have some form of casino gambling. Relatively small places such as Las Vegas are best known for gambling; larger cities such as Chicago are not defined by their casinos in spite of the large turnover.
The Las Vegas Valley has the largest concentration of casinos in the United States. Based on revenue, Atlantic City, New Jersey ranks second, and the Chicago region third.
Top American casino markets by revenue (2009 annual revenues):
The Nevada Gaming Control Board divides Clark County, which is coextensive with the Las Vegas metropolitan area, into seven regions for reporting purposes.
Indian gaming has been responsible for a rise in the number of casinos outside of Las Vegas and Atlantic City.
Security.
Given the large amounts of currency handled within a casino, both patrons and staff may be tempted to cheat and steal, in collusion or independently; most casinos have security measures to prevent this. Security cameras located throughout the casino are the most basic measure.
Modern casino security is usually divided between a physical security force and a specialized surveillance department. The physical security force usually patrols the casino and responds to calls for assistance and reports of suspicious or definite criminal activity. A specialized surveillance department operates the casino's closed circuit television system, known in the industry as the eye in the sky. Both of these specialized casino security departments work very closely with each other to ensure the safety of both guests and the casino's assets, and have been quite successful in preventing crime. Some casinos also have catwalks in the ceiling above the casino floor, which allow surveillance personnel to look directly down, through one way glass, on the activities at the tables and slot machines.
When it opened in 1989, The Mirage was the first casino to use cameras full-time on all table games.
In addition to cameras and other technological measures, casinos also enforce security through rules of conduct and behavior; for example, players at card games are required to keep the cards they are holding in their hands visible at all times.
Business practices.
Over the past few decades, casinos have developed many different marketing techniques for attracting and maintaining loyal patrons. Many casinos use a loyalty rewards program used to track players' spending habits and target their patrons more effectively, by sending mailings with free slot play and other promotions.
Crime.
One area of controversy surrounding casinos is their relationship to crime rates. Economic studies that show a positive relationship between casinos and crime usually fail to consider the visiting population at risk when they calculate the crime rate in casino areas. Such studies thus count the crimes committed by visitors, but do not count visitors in the population measure, and this overstates the crime rates in casino areas. Part of the reason this methodology is used, despite it leading to an overstatement of crime rates is that reliable data on tourist count are often not available.
In a 2004 report by the US Department of Justice, researchers interviewed people who had been arrested in Las Vegas and Des Moines and found that the percentage of problem or pathological gamblers among the arrestees was three to five times higher than in the general population. According to some police reports, incidences of reported crime often double and triple in communities within three years of a casino opening.

</doc>
<doc id="5216" url="https://en.wikipedia.org/wiki?curid=5216" title="Khmer language">
Khmer language

Khmer or Cambodian (natively , or more formally ) is the language of the Khmer people and the official language of Cambodia. With approximately 16 million speakers, it is the second most widely spoken Austroasiatic language (after Vietnamese). Khmer has been influenced considerably by Sanskrit and Pali, especially in the royal and religious registers, through Hinduism and Buddhism. The more colloquial registers have influenced, and have been influenced by, Thai, Lao, Vietnamese, and Cham, all of which, due to geographical proximity and long-term cultural contact, form a sprachbund in peninsular Southeast Asia. It is also the earliest recorded and earliest written language of the Mon–Khmer family, predating Mon and by a significant margin Vietnamese, due to Old Khmer being the language of the historical empires of Chenla, Angkor and, presumably, their earlier predecessor state, Funan.
The vast majority of Khmer speakers speak Central Khmer, the dialect of the central plain where the Khmer are most heavily concentrated. Within Cambodia, regional accents exist in remote areas but these are regarded varieties of Central Khmer. Two exceptions are the speech of the capital, Phnom Penh, and that of the Khmer Khe in Stung Treng province, both of which differ sufficiently enough from Central Khmer to be considered separate dialects of Khmer. Outside of Cambodia, three distinct dialects are spoken by ethnic Khmers native to areas that were historically part of the Khmer Empire. The Northern Khmer dialect is spoken by over a million Khmers in the southern regions of Northeast Thailand and is treated by some linguists as a separate language. Khmer Krom, or Southern Khmer, is the first language of the Khmer of Vietnam while the Khmer living in the remote Cardamom mountains speak a very conservative dialect that still displays features of the Middle Khmer language.
Khmer is primarily an analytic, isolating language. There are no inflections, conjugations or case endings. Instead, particles and auxiliary words are used to indicate grammatical relationships. General word order is subject–verb–object, and modifiers follow the word they modify. Classifiers appear after numbers when used to count nouns, though not always so consistently as in languages like Chinese. In spoken Khmer, topic-comment structure is common and the perceived social relation between participants determines which sets of vocabulary, such as pronouns and honorifics, are proper.
Khmer differs from neighboring languages such as Thai, Burmese, Lao and Vietnamese in that it is not a tonal language. Words are stressed on the final syllable, hence many words conform to the typical Mon–Khmer pattern of a stressed syllable preceded by a minor syllable. The language has been written in the Khmer script, an abugida descended from the Brahmi script via the southern Indian Pallava script, since at least the seventh century. The script's form and use has evolved over the centuries; its modern features include subscripted versions of consonants used to write clusters and a division of consonants into two series with different inherent vowels. Approximately 79% of Cambodians are able to read Khmer.
Classification.
Khmer is a member of the Austroasiatic language family, the autochthonous family in an area that stretches from the Malay Peninsula through Southeast Asia to East India. Austroasiatic, which also includes Mon, Vietnamese and Munda, has been studied since 1856 and was first proposed as a language family in 1907. Despite the amount of research, there is still doubt about the internal relationship of the languages of Austroasiatic. Diffloth places Khmer in an eastern branch of the Mon-Khmer languages. In these classification schemes Khmer's closest genetic relatives are the Bahnaric and Pearic languages. More recent classifications doubt the validity of the Mon-Khmer sub-grouping and place the Khmer language as its own branch of Austroasiatic equidistant from the other 12 branches of the family.
Geographic distribution and dialects.
Khmer is spoken by some 13 million people in Cambodia, where it is the official language. It is also a second language for most of the minority groups and indigenous hill tribes there. Additionally there are a million speakers of Khmer native to southern Vietnam (1999 census) and 1.4 million in northeast Thailand (2006).
Khmer dialects, although mutually intelligible, are sometimes quite marked. Notable variations are found in speakers from Phnom Penh (Cambodia's capital city), the rural Battambang area, the areas of Northeast Thailand adjacent to Cambodia such as Surin province, the Cardamom Mountains, and southern Vietnam. The dialects form a continuum running roughly north to south. Standard Cambodian Khmer is mutually intelligible with the others but a Khmer Krom speaker from Vietnam, for instance, may have great difficulty communicating with a Khmer native to Sisaket Province in Thailand.
The following is a classification scheme showing the development of the modern Khmer dialects.
Standard Khmer, or Central Khmer, the language as taught in Cambodian schools and used by the media, is based on the dialect spoken throughout the Central Plain, a region encompassed by the northwest and central provinces.
Northern Khmer (called "Khmer Surin" in Khmer) refers to the dialects spoken by many in several border provinces of present-day northeast Thailand. After the fall of the Khmer Empire in the early 15th century, the Dongrek Mountains served as a natural border leaving the Khmer north of the mountains under the sphere of influence of the Kingdom of Lan Xang. The conquests of Cambodia by Naresuan the Great for Ayutthaya furthered their political and economic isolation from Cambodia proper, leading to a dialect that developed relatively independently from the midpoint of the Middle Khmer period. This has resulted in a distinct accent influenced by the surrounding tonal languages Lao and Thai, lexical differences, and phonemic differences in both vowels and distribution of consonants. Syllable-final , which has become silent in other dialects of Khmer, is still pronounced in Northern Khmer. Some linguists classify Northern Khmer as a separate but closely related language rather than a dialect.
Western Khmer, also called Cardamom Khmer or Chanthaburi Khmer, is spoken by a very small, isolated population in the Cardamom mountain range extending from western Cambodia into eastern Central Thailand. Although little studied, this variety is unique in that it maintains a definite system of vocal register that has all but disappeared in other dialects of modern Khmer.
Phnom Penh Khmer is spoken in the capital and surrounding areas. This dialect is characterized by merging or complete elision of syllables, considered by speakers from other regions to be a "relaxed" pronunciation. For instance, "Phnom Penh" will sometimes be shortened to "m'Penh". Another characteristic of Phnom Penh speech is observed in words with an "r" either as an initial consonant or as the second member of a consonant cluster (as in the English word "bread"). The "r", trilled or flapped in other dialects, is either pronounced as a uvular trill or not pronounced at all. This alters the quality of any preceding consonant, causing a harder, more emphasized pronunciation. Another unique result is that the syllable is spoken with a low-rising or "dipping" tone much like the "hỏi" tone in Vietnamese. For example, some people pronounce ('fish') as : the is dropped and the vowel begins by dipping much lower in tone than standard speech and then rises, effectively doubling its length. Another example is the word ('study'), which is pronounced , with the uvular "r" and the same intonation described above.
Khmer Krom or Southern Khmer is spoken by the indigenous Khmer population of the Mekong Delta, formerly controlled by the Khmer Empire but part of Vietnam since 1698. Khmers are persecuted by the Vietnamese government for using their native language and, since the 1950s, have been forced to take Vietnamese names. Consequently, very little research has been published regarding this dialect. It has been generally influenced by Vietnamese for three centuries and accordingly displays a pronounced accent, tendency toward monosyllablic words and lexical differences from Standard Khmer.
Khmer Khe is spoken in the Se San, Srepok and Sekong river valleys of Sesan and Siem Pang districts in Stung Treng Province. Following the decline of Angkor, the Khmer abandoned their northern territories which were then settled by the Lao. In the 17th century, Chey Chetha XI led a Khmer force into Stung Treng to retake the area. The Khmer Khe living in this area of Stung Treng in modern times are presumed to be the descendants of this group. Their dialect is thought to resemble that of pre-modern Siem Reap.
Historical periods.
Linguistic study of the Khmer language divides its history into four periods one of which, the Old Khmer period, is subdivided into pre-Angkorian and Angkorian. Pre-Angkorian Khmer, the Old Khmer language from 600 CE through 800, is only known from words and phrases in Sanskrit texts of the era. Old Khmer (or Angkorian Khmer) is the language as it was spoken in the Khmer Empire from the 9th century until the weakening of the empire sometime in the 13th century. Old Khmer is attested by many primary sources and has been studied in depth by a few scholars, most notably Saveros Pou, Phillip Jenner and Heinz-Jürgen Pinnow. Following the end of the Khmer Empire the language lost the standardizing influence of being the language of government and accordingly underwent a turbulent period of change in morphology, phonology and lexicon. The language of this transition period, from about the 14th to 18th centuries, is referred to as Middle Khmer and saw borrowing from Thai, Lao and, to a lesser extent, Vietnamese. The changes during this period are so profound that the rules of Modern Khmer can not be applied to correctly understand Old Khmer. The language became recognizable as Modern Khmer, spoken from the 19th century till today.
The following table shows the conventionally accepted historical stages of Khmer.
Just as modern Khmer was emerging from the transitional period represented by Middle Khmer, Cambodia fell under the influence of French colonialism. Thailand, which had for centuries claimed suzerainty over Cambodia and controlled succession to the Cambodian throne, began losing its influence on the language. In 1887 Cambodia was fully integrated into French Indochina which brought in a French-speaking aristocracy. This led to French becoming the language of higher education and the intellectual class. By 1907, the French had wrested over half of modern-day Cambodia, including the north and northwest where Thai had been the prestige language, from back Thai control and reintegrated it into the country.
Many native scholars in the early 20th century, led by a monk named Chuon Nath, resisted the French and Thai influences on their language. Forming the government sponsored Cultural Committee to define and standardize the modern language, they championed Khmerization, purging of foreign elements, reviving affixation, and the use of Old Khmer roots and historical Pali and Sanskrit to coin new words for modern ideas. Opponents, led by Keng Vannsak, who embraced "total Khmerization" by denouncing the reversion to classical languages and favoring the use of contemporary colloquial Khmer for neologisms, and Ieu Koeus, who favored borrowing from Thai, were also influential. Koeus later joined the Cultural Committee and supported Nath. Nath's views and prolific work won out and he is credited with cultivating modern Khmer-language identity and culture, overseeing the translation of the entire Pali Buddhist canon into Khmer. He also created the modern Khmer language dictionary that is still in use today, thereby ensuring that Khmer would survive, and indeed flourish, during the French colonial period.
Phonology.
The phonological system described here is the inventory of sounds of the standard spoken language, represented using appropriate symbols from the International Phonetic Alphabet (IPA).
Consonants.
The voiceless plosives may occur with or without aspiration (as vs. , etc.); this difference is contrastive before a vowel. However, the aspirated sounds in that position may be analyzed as sequences of two phonemes: . This analysis is supported by the fact that infixes can be inserted between the stop and the aspiration; for example ('big') becomes ('size') with a nominalizing infix. When one of these plosives occurs initially before another consonant, aspiration is no longer contrastive and can be regarded as mere phonetic detail: slight aspiration is expected when the following consonant is not one of (or /ŋ/ if the initial plosive is /k/).
The voiced plosives are pronounced as implosives by most speakers, but this feature is weak in educated speech, where they become .
In syllable-final position, and approach and respectively. The stops are unaspirated and have no audible release when occurring as syllable finals.
In addition, the consonants , , and occur occasionally in recent loan words in the speech of Cambodians familiar with French and other languages.
Vowels.
Various authors have proposed slightly different analyses of the Khmer vowel system. This may be in part because of the wide degree of variation in pronunciation between individual speakers, even within a dialectal region. The description below follows Huffman (1970). The number of vowel nuclei and their values vary between dialects; differences exist even between the Standard Khmer system and that of the Battambang dialect on which the standard is based.
In addition, there are diphthongs and triphthongs which are analyzed as a vowel nucleus plus a semivowel (/j/ or /w/) coda because they can not be followed by a final consonant. These include: (with short monophthongs) , , , , ; (with long monophthongs) , ; (with long diphthongs) , , , , and .
Syllable structure.
A Khmer syllable begins with a single consonant, or else with a cluster of two, or rarely three, consonants. The only possible clusters of three consonants at the start of a syllable are , and (with aspirated consonants analyzed as two-consonant sequences) . There are 85 possible two-consonant clusters (including [pʰ] etc. analyzed as /ph/ etc.). All the clusters are shown in the following table, phonetically, i.e. superscript ʰ can mark either contrastive or non-contrastive aspiration (see above).
Slight vowel epenthesis occurs in the clusters consisting of a plosive followed by /ʔ/, /b/, /d/, in those beginning /ʔ/, /m/, /l/, and in the cluster /kŋ-/.
After the initial consonant or consonant cluster comes the syllabic nucleus, which is one of the vowels listed above. This vowel may end the syllable or may be followed by a coda, which is a single consonant. If the syllable is stressed and the vowel is short, there must be a final consonant. All consonant sounds except and the aspirates can appear as the coda (although final /r/ is heard in some dialects, most notably in Northern Khmer).
A minor syllable (unstressed syllable preceding the main syllable of a word) has a structure of CV-, CrV-, CVN- or CrVN- (where C is a consonant, V a vowel, and N a nasal consonant). The vowels in such syllables are usually short; in conversation they may be reduced to , although in careful or formal speech, including on television and radio, they are clearly articulated. An example of such a word is ('person'), pronounced , or more casually .
Stress.
Stress in Khmer falls on the final syllable of a word. Because of this predictable pattern, stress is non-phonemic in Khmer (it does not distinguish different meanings).
Most Khmer words consist of either one or two syllables. In most native disyllabic words, the first syllable is a minor (fully unstressed) syllable. Such words have been described as "sesquisyllabic" (i.e. as having one-and-a-half syllables). There are also some disyllabic words in which the first syllable does not behave as a minor syllable, but takes secondary stress. Most such words are compounds, but some are single morphemes (generally loanwords). An example is ('language'), pronounced .
Words with three or more syllables, if they are not compounds, are mostly loanwords, usually derived from Pali, Sanskrit, or more recently, French. They are nonetheless adapted to Khmer stress patterns. Primary stress falls on the final syllable, with secondary stress on every second syllable from the end. Thus in a three-syllable word, the first syllable has secondary stress; in a four-syllable word, the second syllable has secondary stress; in a five-syllable word, the first and third syllables have secondary stress, and so on. Long polysyllables are not often used in conversation.
Compounds, however, preserve the stress patterns of the constituent words. Thus , the name of a kind of cookie (literally 'bird's nest'), is pronounced , with secondary stress on the second rather than the first syllable, because it is composed of the words ('nest') and ('bird').
Phonation and tone.
Khmer once had a phonation distinction in its vowels, but this now survives only in the most archaic dialect (Western Khmer). The distinction arose historically when vowels after Old Khmer voiced consonants became breathy voiced and diphthongized; for example became . When consonant voicing was lost, the distinction was maintained by the vowel (); later the phonation disappeared as well (). These processes explain the origin of what are now called a-series and o-series consonants in the Khmer script.
Although most Cambodian dialects are not tonal, colloquial Phnom Penh dialect has developed a tonal contrast (level versus peaking tone) to compensate for the elision of .
Intonation.
Intonation often conveys semantic context in Khmer, as in distinguishing declarative statements, questions and exclamations. The available grammatical means of making such distinctions are not always used, or may be ambiguous; for example, the final interrogative particle can also serve as an emphasizing (or in some cases negating) particle.
The intonation pattern of a typical Khmer declarative phrase is a steady rise throughout followed by an abrupt drop on the last syllable.
Other intonation contours signify a different type of phrase such as the "full doubt" interrogative, similar to yes-no questions in English. Full doubt interrogatives remain fairly even in tone throughout, but rise sharply towards the end.
Exclamatory phrases follow the typical steadily rising pattern, but rise sharply on the last syllable instead of falling.
Grammar.
Khmer is primarily an analytic language with no inflection. Syntactic relations are mainly determined by word order. Old and Middle Khmer used particles to mark grammatical categories and many of these have survived in Modern Khmer but are used sparingly, mostly in literary or formal language. Khmer makes extensive use of auxiliary verbs, "directionals" and serial verb construction. Colloquial Khmer is a zero copula language, instead preferring predicative adjectives (and even predicative nouns) unless using a copula for emphasis or to avoid ambiguity in more complex sentences. Basic word order is subject–verb–object (SVO), although subjects are often dropped; prepositions are used rather than postpositions. Topic-Comment constructions are common and the language is generally head-initial (modifiers follow the words they modify). Some grammatical process are still not fully understood by western scholars. For example, it's not clear if certain features of Khmer grammar, such as actor nominalization, should be treated as a morphological process or a purely syntactic device, and some derivational morphology seems to be "purely decorative" and performs no known syntactic work.
Lexical categories have been hard to define in Khmer. Henri Maspero, an early scholar of Khmer, claimed the language had no parts of speech, while a later scholar, Judith Jacob, posited four parts of speech and innumerable particles. John Haiman, on the other hand, identifies "a couple dozen" parts of speech in Khmer with the caveat that Khmer words have the freedom to perform a variety of syntactic functions depending on such factors as word order, relevant particles, location within a clause, intonation and context. Some of the more important lexical categories and their function are demonstrated in the following example sentence taken from a hospital brochure:
Morphology.
Modern Khmer is an isolating language which means that it uses little productive morphology. There is some derivation by means of prefixes and infixes, but this is a remnant of Old Khmer not always productive in the modern language. Khmer morphology is evidence of a historical process through which the language was, at some point in the past, changed from being an agglutinative language to adopting an isolating typology. Affixed forms are lexicalized and cannot be used productively to form new words. Below are some of the most common affixes with examples as given by Huffman.
Compounding in Khmer is a common derivational process that takes two forms, coordinate compounds and repetitive compounds. Coordinate compounds join two unbound morphemes (independent words) of similar meaning to form a compound signifying a concept more general than either word alone. Coordinate compounds join either two nouns or two verbs. Repetitive compounds, one of the most productive derivational features of Khmer, use reduplication of an entire word to derive words whose meaning will depend on the class of the reduplicated word. A repetitive compound of a noun indicates plurality or generality while that of an adjectival verb could mean either an intensification or plurality.
Coordinate compounds:
Repetitive compounds:
Nouns and pronouns.
Khmer nouns do not inflect for grammatical gender or singular/plural. There are no articles, but indefiniteness is often expressed by the word for "one" (, ) following the noun as in ( "a dog"). Plurality can be marked by postnominal particles, numerals, or reduplication of a following adjective, which, although similar to intensification, is usually not ambiguous due to context.
Classifying particles are used after numerals, but are not always obligatory as they are in Thai or Chinese, for example, and are often dropped in colloquial speech. Khmer nouns are divided into two groups: mass nouns, those which take classifiers, and specific nouns, which do not. The overwhelming majority are mass nouns.
Possession is colloquially expressed by word order. The possessor is placed after that which is possessed. Alternatively, in more complex sentences or when emphasis is required, a possessive construction using the word (, "property, object") may be employed. In formal and literary contexts, the possessive particle () is used:
Pronouns are subject to a complicated system of social register, the choice of pronoun depending on the perceived relationships between speaker, audience and referent (see Social registers below). Kinship terms, nicknames and proper names are often used as pronouns (including for the first person) among intimates. Subject pronouns are frequently dropped in colloquial conversation.
Adjectives, verbs and verb phrases may be made into nouns by the use of nominalization particles. Three of the more common particles used to create nouns are /kaː/, /sec kdəj/, and /pʰiəp/. These particles are prefixed most often to verbs in order to form abstract nouns. The latter, derived from Sanskrit, also occurs as a suffix in fixed forms borrowed from Sanskrit and Pali such as /sokʰa.pʰiəp/ ("health") from /sok/ ("to be healthy").
Adjectives and adverbs.
Adjectives, demonstratives and numerals follow the noun they modify. Adverbs likewise follow the verb. Morphologically, adjectives and adverbs are not distinguished, with many words often serving either function. Adjectives are also employed as verbs as Khmer sentences rarely use a copula.
Degrees of comparison are constructed syntactically. Comparatives are expressed using the word : "A X (A is more X [than B). The most common way to express superlatives is with : "A X " (A is the most X). Intensity is also expressed syntactically, similar to other languages of the region, by reduplication or with the use of intensifiers.
Verbs.
As is typical of most East Asian languages, Khmer verbs do not inflect at all; tense, aspect and mood can be expressed using auxiliary verbs, particles (such as កំពុង , placed before a verb to express continuous aspect) and adverbs (such as "yesterday", "earlier", "tomorrow"), or may be understood from context. Serial verb construction is quite common.
Khmer verbs are a relatively open class and can be divided into two types, main verbs and auxiliary verbs. Huffman defined a Khmer verb as "any word that can be (negated)", and further divided main verbs into three classes.
Transitive verbs are verbs which may be followed by a direct object:
Intransitive verbs are verbs which can not be followed by an object:
Adjectival verbs are a word class that has no equivalent in English. When modifying a noun or verb, they function as adjectives or adverbs, respectively, but they may also be used as main verbs equivalent to English "be + "adjective"".
Syntax.
Syntax is the rules and processes that describe how sentences are formed in a particular language, how words relate to each other within clauses or phrases and how those phrases relate to each other within a sentence to convey meaning. Khmer syntax is very analytic. Relationships between words and phrases are signified primarily by word order supplemented with auxiliary verbs and, particularly in formal and literary registers, grammatical marking particles. Grammatical phenomena such as negation and aspect are marked by particles while interrogative sentences are marked either by particles or interrogative words equivalent to English "wh-words".
A complete Khmer sentence consists of four basic elements which include an optional topic, an optional subject, an obligatory predicate and various adverbials and particles. The topic and subject are noun phrases, predicates are verb phrases and another noun phrase acting as an object or verbal attribute often follows the predicate.
Basic constituent order.
When combining these noun and verb phrases into a sentence the order is typically SVO:
When both a direct object and indirect object are present without any grammatical markers, the preferred order is SV(DO)(IO). In such a case, if the direct object phrase contains multiple components, the indirect object immediately follows the noun of the direct object phrase and the direct object's modifiers follow the indirect object:
This ordering of objects can be changed and the meaning clarified with the inclusion of particles. The word /dɑl/, which normally means "to arrive" or "towards", can be used as a preposition meaning "to":
Alternatively, the indirect object could precede the direct object if the object marking preposition /nəw/ were used:
However, in spoken discourse OSV is possible when emphasizing the object in a topic-comment-like structure.
Noun phrase.
The noun phrase in Khmer typically has the following structure:
The elements in parentheses are optional. Honorifics are a class of words that serve to index the social status of the referent. Honorifics can be kinship terms or personal names, both of which are often used as first and second person pronouns, or specialized words such as /preah/ ('god') before royal and religious objects. The most common demonstratives are /nih/ ('this, these') and /nuh/ ('that, those'). The word /ae nuh/ ('those over there') has a more distal or vague connotation. If the noun phrase contains a possessive adjective, it follows the noun and precedes the numeral. If a descriptive attribute co-occurs with a possessive, the possessive construction (/rɔbɑh/) is expected.
Some examples of typical Khmer noun phrases are:
The Khmer particle /dɑː/ marked attributes in Old Khmer noun phrases and is used in formal and literary language to signify that what precedes is the noun and what follows is the attribute. Modern usage may carry the connotation of mild intensity.
Verb phrase.
Khmer verbs are completely uninflected, and once a subject or topic has been introduced or is clear from context the noun phrase may be dropped. Thus, the simplest possible sentence in Khmer consists of a single verb. For example, /tɨw/ can mean "I'm going.", "He went.", "They've gone.", "Let's go.", etc. This also results in long strings of verbs such as:
Khmer uses three verbs for what translates into English as the copula. The general copula is /ciə/; it is used to convey identity with nominal predicates. For locative predicates, the copula is /nɨw/. The verb /miən/ is the "existential" copula meaning "there is" or "there exists".
Negation is achieved by putting មិន before the verb and the particle ទេ at the end of the sentence or clause. In colloquial speech, verbs can also be negated without the need for a final particle, by placing ឥត before them.
Past tense can be conveyed by adverbs, such as "yesterday" or by the use of perfective particles such as /haəj/
Different senses of future action can also be expressed by the use of adverbs like "tomorrow" or by the future tense marker /nɨŋ/, which is placed immediately before the verb, or both:
Imperatives are often unmarked. For example, in addition to the meanings given above, the "sentence" /tɨw/ can also mean "Go!". Various words and particles may be added to the verb to soften the command to varying degrees, including to the point of politeness (jussives):
Prohibitives take the form "/kom/ + V" and also are often softened by the addition of the particle /ʔəj/ to the end of the phrase.
Questions.
There are three basic types of questions in Khmer. Questions requesting specific information use question words. Polar questions are indicated with interrogative particles, most commonly /teː/ a homonym of the negation particle. Tag questions are indicated with various particles and rising inflection. The SVO word order is generally not inverted for questions.
In more formal contexts and in polite speech, questions are also marked at their beginning by the particle /taə/.
Passive voice.
Khmer does not have a passive voice, but there is a construction utilizing the main verb /trəw/ ("to hit", "to be correct", "to affect") as an auxiliary verb meaning "to be subject to" or "to undergo" which results in sentences that are translated to English using the passive voice.
Clause syntax.
Complex sentences are formed in Khmer by the addition of one or more clauses to the main clause. The various types of clauses in Khmer include the coordinate clause, the relative clause and the subordinate clause. Word order in clauses is the same for that of the basic sentences described above. Coordinate clauses do not necessarily have to be marked; they can simply follow one another. When explicitly marked, they are joined by words similar to English conjunctions such as /nɨŋ/ ("and") and /haəj/ ("and then") or by clause-final conjunction-like adverbs /dae/ and /pʰɑːŋ/, both of which can mean "also" or "and also"; disjunction is indicated by /rɨː/ ("or"). Relative clauses can be introduced by /deal/ ("that") but, similar to coordinate clauses, often simply follow the main clause. For example, both phrases below can mean "the hospital bed that has wheels".
Relative clauses are more likely to be introduced with /deal/ if they do not immediately follow the head noun. Khmer subordinate conjunctions always precede a subordinate clause. Subordinate conjunctions include words such as /prŭəh/ ("because"), /hak bəj/ ("seems as if") and /daəmbəj/ ("in order to").
Numerals.
Counting in Khmer is based on a biquinary system (the numbers from 6 to 9 have the form "five one", "five two", etc.) However, the words for multiples of ten from 30 to 90 are not related to the basic Khmer numbers, but are probably borrowed from Thai. The Khmer script has its own versions of the Arabic numerals.
The principal number words are listed in the following table, which gives Western and Khmer digits, Khmer spelling and IPA transcription.
Intermediate numbers are formed by compounding the above elements. Powers of ten are denoted by loan words: (100), (1,000), (10,000), (100,000) and (1,000,000) from Thai and (10,000,000) from Sanskrit.
Ordinal numbers are formed by placing the particle before the corresponding cardinal number.
Social registers.
Khmer employs a system of registers in which the speaker must always be conscious of the social status of the person spoken to. The different registers, which include those used for common speech, polite speech, speaking to or about royals and speaking to or about monks, employ alternate verbs, names of body parts and pronouns. This results in what appears to foreigners as separate languages and, in fact, isolated villagers often are unsure how to speak with royals and royals raised completely within the court do not feel comfortable speaking the common register. As an example, the word for "to eat" used between intimates or in reference to animals is . Used in polite reference to commoners, it's . When used of those of higher social status, it's or . For monks the word is and for royals, . Another result is that the pronominal system is complex and full of honorific variations, just a few of which are shown in the table below.
Writing system.
Khmer is written with the Khmer script, an abugida developed from the Pallava script of India before the 7th century when the first known inscription appeared. Written left-to-right with vowel signs that can be placed after, before, above or below the consonant they follow, the Khmer script is similar in appearance and usage to Thai and Lao, both of which were based on the Khmer system. The Khmer script is also distantly related to the Mon script, the ancestor of the modern Burmese script. Khmer numerals, which were inherited from Indian numerals, are used more widely than Hindu-Arabic numerals. Within Cambodia, literacy in the Khmer alphabet is estimated at 77.6%.
Consonant symbols in Khmer are divided into two groups, or series. The first series carries the inherent vowel while the second series carries the inherent vowel . The Khmer names of the series, ('voiceless') and ('voiced'), respectively, indicate that the second series consonants were used to represent the voiced phonemes of Old Khmer. As the voicing of stops was lost, however, the contrast shifted to the phonation of the attached vowels which, in turn, evolved into a simple difference of vowel quality, often by diphthongization. This process has resulted in the Khmer alphabet having two symbols for most consonant phonemes and each vowel symbol having two possible readings, depending on the series of the initial consonant:

</doc>
<doc id="5218" url="https://en.wikipedia.org/wiki?curid=5218" title="Central processing unit">
Central processing unit

A central processing unit (CPU) is the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control and input/output (I/O) operations specified by the instructions. The term has been used in the computer industry at least since the early 1960s. Traditionally, the term "CPU" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.
The form, design and implementation of CPUs have changed over the course of their history, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that fetches instructions from memory and "executes" them by directing the coordinated operations of the ALU, registers and other components.
Most modern CPUs are microprocessors, meaning they are contained on a single integrated circuit (IC) chip. An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC). Some computers employ a multi-core processor, which is a single chip containing two or more CPUs called "cores"; in that context, single chips are sometimes referred to as "sockets". Array processors or vector processors have multiple processors that operate in parallel, with no unit considered central.
History.
Computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called "fixed-program computers". Since the term "CPU" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.
The idea of a stored-program computer was already present in the design of J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner. On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed the paper entitled "First Draft of a Report on the EDVAC". It was the outline of a stored-program computer that would eventually be completed in August 1949. EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC, however, was not the first stored-program computer; the Manchester Small-Scale Experimental Machine, a small prototype stored-program computer, ran its first program on 21 June 1948 and the Manchester Mark 1 ran its first program during the night of 16–17 June 1949.
Early CPUs were custom designs used as part of a larger and sometimes distinctive computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles to cellphones, and sometimes even in toys.
While von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as the von Neumann architecture, others before him, such as Konrad Zuse, had suggested and implemented similar ideas. The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also utilized a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard architecture processors.
Relays and vacuum tubes (thermionic tubes) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Tube computers like EDVAC tended to average eight hours between failures, whereas relay computers like the (slower, but earlier) Harvard Mark I failed very rarely. In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4 MHz were very common at this time, limited largely by the speed of the switching devices they were built with.
Transistor CPUs.
The design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements like vacuum tubes and relays. With this improvement more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.
In 1964, IBM introduced its System/360 computer architecture that was used in a series of computers capable of running the same programs with different speed and performance. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM utilized the concept of a microprogram (often called "microcode"), which still sees widespread usage in modern CPUs. The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the IBM zSeries. In 1965, Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets, the PDP-8.
Transistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. Thanks to both the increased reliability as well as the dramatically increased speed of the switching elements (which were almost exclusively transistors by this time), CPU clock rates in the tens of megahertz were obtained during this period. Additionally while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like SIMD (Single Instruction Multiple Data) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc and Fujitsu Ltd.
Small-scale integration CPUs.
During this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or "chip". At first only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based upon these "building block" ICs are generally referred to as "small-scale integration" (SSI) devices. SSI ICs, such as the ones used in the Apollo guidance computer, usually contained up to a few score transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.
IBM's System/370 follow-on to the System/360 used SSI ICs rather than Solid Logic Technology discrete-transistor modules. DEC's PDP-8/I and KI10 PDP-10 also switched from the individual transistors used by the PDP-8 and PDP-10 to SSI ICs, and their extremely popular PDP-11 line was originally built with SSI ICs but was eventually implemented with LSI components once these became practical.
Large-scale integration CPUs.
Lee Boysel published influential articles, including a 1967 "manifesto", which described how to build the equivalent of a 32-bit mainframe computer from a relatively small number of large-scale integration circuits (LSI). At the time, the only way to build LSI chips, which are chips with a hundred or more gates, was to build them using a MOS process (i.e., PMOS logic, NMOS logic, or CMOS logic). However, some companies continued to build processors out of bipolar chips because bipolar junction transistors were so much faster than MOS chips; for example, Datapoint built processors out of TTL chips until the early 1980s.
People building high-speed computers wanted them to be fast, so in the 1970s they built the CPUs from small-scale integration (SSI) and medium-scale integration (MSI) 7400 series TTL gates. At the time, MOS ICs were so slow that they were considered useful only in a few niche applications that required low power.
As the microelectronic technology advanced, an increasing number of transistors were placed on ICs, decreasing the quantity of individual ICs needed for a complete CPU. MSI and LSI ICs increased transistor counts to hundreds, and then thousands. By 1968, the number of ICs required to build a complete CPU had been reduced to 24 ICs of eight different types, with each IC containing roughly 1000 MOSFETs. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits.
Microprocessors.
In the 1970s, the fundamental inventions by Federico Faggin (Silicon Gate MOS ICs with self-aligned gates along with his new random logic design methodology) changed the design and implementation of CPUs forever. Since the introduction of the first commercially available microprocessor (the Intel 4004) in 1970, and the first widely used microprocessor (the Intel 8080) in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term "CPU" is now applied almost exclusively to microprocessors. Several CPUs (denoted "cores") can be combined in a single processing chip.
Previous generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size, as a result of being implemented on a single die, means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, as the ability to construct exceedingly small transistors on an IC has increased, the complexity and number of transistors in a single CPU has increased many fold. This widely observed trend is described by Moore's law, which has proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity.
While the complexity, size, construction, and general form of CPUs have changed enormously since 1950, it is notable that the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines. As the aforementioned Moore's law continues to hold true, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model.
Operation.
The fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions that is called a program. The instructions to be executed are kept in some kind of computer memory. Nearly all CPUs follow the fetch, decode and execute steps in their operation, which are collectively known as the instruction cycle.
After the execution of an instruction, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If a jump instruction was executed, the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally. In more complex CPUs, multiple instructions can be fetched, decoded, and executed simultaneously. This section describes what is generally referred to as the "classic RISC pipeline", which is quite common among the simple CPUs used in many electronic devices (often called microcontroller). It largely ignores the important role of CPU cache, and therefore the access stage of the pipeline.
Some instructions manipulate the program counter rather than producing result data directly; such instructions are generally called "jumps" and facilitate program behavior like loops, conditional program execution (through the use of a conditional jump), and existence of functions. In some processors, some other instructions change the state of bits in a "flags" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, in such processors a "compare" instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal; one of these flags could then be used by a later jump instruction to determine program flow.
Fetch.
The first step, fetch, involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. The instruction's location (address) in program memory is determined by a program counter (PC), which stores a number that identifies the address of the next instruction to be fetched. After an instruction is fetched, the PC is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence. Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below).
Decode.
The instruction that the CPU fetches from memory determines what the CPU will do. In the decode step, performed by the circuitry known as the instruction decoder, the instruction is converted into signals that control other parts of the CPU.
The way in which the instruction is interpreted is defined by the CPU's instruction set architecture (ISA). Often, one group of bits (that is, a "field") within the instruction, called the opcode, indicates which operation is to be performed, while the remaining fields usually provide supplemental information required for the operation, such as the operands. Those operands may be specified as a constant value (called an immediate value), or as the location of a value that may be a processor register or a memory address, as determined by some addressing mode.
In some CPU designs the instruction decoder is implemented as a hardwired, unchangeable circuit. In others, a microprogram is used to translate instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses. In some cases the memory that stores the microprogram is rewritable, making it possible to change the way in which the CPU decodes instructions.
Execute.
After the fetch and decode steps, the execute step is performed. Depending on the CPU architecture, this may consist of a single action or a sequence of actions. During each action, various parts of the CPU are electrically connected so they can perform all or part of the desired operation and then the action is completed, typically in response to a clock pulse. Very often the results are written to an internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but less expensive and higher capacity main memory.
For example, if an addition instruction is to be executed, the arithmetic logic unit (ALU) inputs are connected to a pair of operand sources (numbers to be summed), the ALU is configured to perform an addition operation so that the sum of its operand inputs will appear at its output, and the ALU output is connected to storage (e.g., a register or memory) that will receive the sum. When the clock pulse occurs, the sum will be transferred to storage and, if the resulting sum is too large (i.e., it is larger than the ALU's output word size), an arithmetic overflow flag will be set.
Structure and implementation.
Hardwired into a CPU's circuitry is a set of basic operations it can perform, called an instruction set. Such operations may involve, for example, adding or subtracting two numbers, comparing two numbers, or jumping to a different part of a program. Each basic operation is represented by a particular combination of bits, known as the machine language opcode; while executing instructions in a machine language program, the CPU decides which operation to perform by "decoding" the opcode. A complete machine language instruction consists of an opcode and, in many cases, additional bits that specify arguments for the operation (for example, the numbers to be summed in the case of an addition operation). Going up the complexity scale, a machine language program is a collection of machine language instructions that the CPU executes.
The actual mathematical operation for each instruction is performed by a combinational logic circuit within the CPU's processor known as the arithmetic logic unit or ALU. In general, a CPU executes an instruction by fetching it from memory, using its ALU to perform an operation, and then storing the result to memory. Beside the instructions for integer mathematics and logic operations, various other machine instructions exist, such as those for loading data from memory and storing it back, branching operations, and mathematical operations on floating-point numbers performed by the CPU's floating-point unit (FPU).
Control unit.
The control unit of the CPU contains circuitry that uses electrical signals to direct the entire computer system to carry out stored program instructions. The control unit does not execute program instructions; rather, it directs other parts of the system to do so. The control unit communicates with both the ALU and memory.
Arithmetic logic unit.
The arithmetic logic unit (ALU) is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations. The inputs to the ALU are the data words to be operated on (called operands), status information from previous operations, and a code from the control unit indicating which operation to perform. Depending on the instruction being executed, the operands may come from internal CPU registers or external memory, or they may be constants generated by the ALU itself.
When all input signals have settled and propagated through the ALU circuitry, the result of the performed operation appears at the ALU's outputs. The result consists of both a data word, which may be stored in a register or memory, and status information that is typically stored in a special, internal CPU register reserved for this purpose.
Memory management unit.
Most high-end microprocessors (in desktop, laptop, server computers) have a memory management unit, translating logical addresses into physical RAM addresses, providing memory protection and paging abilities, useful for virtual memory. Simpler processors, especially microcontrollers usually don't include an MMU.
Integer range.
Every CPU represents numerical values in a specific way. For example, some early digital computers represented numbers as familiar decimal (base 10) numeral system values, and others have employed more unusual representations such as ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a "high" or "low" voltage.
Related to numeric representation is the size and precision of integer numbers that a CPU can represent. In the case of a binary CPU, this is measured by the number of bits (significant digits of a binary encoded integer) that the CPU can process in one operation, which is commonly called "word size", "bit width", "data path width", "integer precision", or "integer size". A CPU's integer size determines the range of integer values it can directly operate on. For example, an 8-bit CPU can directly manipulate integers represented by eight bits, which have a range of 256 (28) discrete integer values.
Integer range can also affect the number of memory locations the CPU can directly address (an address is an integer value representing a specific memory location). For example, if a binary CPU uses 32 bits to represent a memory address then it can directly address 232 memory locations. To circumvent this limitation and for various other reasons, some CPUs use mechanisms (such as bank switching) that allow additional memory to be addressed.
CPUs with larger word sizes require more circuitry and consequently are physically larger, cost more, and consume more power (and therefore generate more heat). As a result, smaller 4- or 8-bit microcontrollers are commonly used in modern applications even though CPUs with much larger word sizes (such as 16, 32, 64, even 128-bit) are available. When higher performance is required, however, the benefits of a larger word size (larger data ranges and address spaces) may outweigh the disadvantages. A CPU can have internal data paths shorter than the word size to reduce size and cost. For example, even though the IBM System/360 instruction set was a 32-bit instruction set, the System/360 Model 30 and Model 40 had 8-bit data paths in the arithmetic logical unit, so that a 32-bit add required four cycles, one for each 8 bits of the operands, and, even though the Motorola 68k instruction set was a 32-bit instruction set, the Motorola 68000 and Motorola 68010 had 16-bit data paths in the arithmetic logical unit, so that a 32-bit add required two cycles.
To gain some of the advantages afforded by both lower and higher bit lengths, many instruction sets have different bit widths for integer and floating-point data, allowing CPUs implementing that instruction set to have different bit widths for different portions of the device. For example, the IBM System/360 instruction set was primarily 32 bit, but supported 64-bit floating point values to facilitate greater accuracy and range in floating point numbers. The System/360 Model 65 had an 8-bit adder for decimal and fixed-point binary arithmetic and a 60-bit adder for floating-point arithmetic. |Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose usage where a reasonable balance of integer and floating point capability is required.
Clock rate.
Most CPUs are synchronous circuits, which means they employ a clock signal to pace their sequential operations. The clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions and, consequently, the faster the clock, the more instructions the CPU will execute each second.
To ensure proper operation of the CPU, the clock period is longer than the maximum time needed for all signals to propagate (move) through the CPU. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the "edges" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).
However, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue, as clock rates increase dramatically, is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does energy consumption, causing the CPU to require more heat dissipation in the form of CPU cooling solutions.
One method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable recent CPU design that uses extensive clock gating is the IBM PowerPC-based Xenon used in the Xbox 360; that way, power requirements of the Xbox 360 are greatly reduced. Another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire asynchronous CPUs have been built without utilizing a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS.
Rather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers.
Parallelism.
The description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as "subscalar", operates on and executes one instruction on one or two pieces of data at a time, that is less than one instruction per clock cycle ().
This process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets "hung up" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach "scalar" performance (one instruction per clock cycle, ). However, the performance is nearly always subscalar (less than one instruction per clock cycle, ).
Attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques:
Each methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.
Instruction-level parallelism.
One of the simplest methods used to accomplish increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is the simplest form of a technique known as instruction pipelining, and is utilized in almost all modern general-purpose CPUs. Pipelining allows more than one instruction to be executed at any given time by breaking down the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.
Pipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. To cope with this, additional care must be taken to check for these sorts of conditions and delay a portion of the instruction pipeline if this occurs. Naturally, accomplishing this requires additional circuitry, so pipelined processors are more complex than subscalar ones (though not very significantly so). A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).
Further improvement upon the idea of instruction pipelining led to the development of a method that decreases the idle time of CPU components even further. Designs that are said to be "superscalar" include a long instruction pipeline and multiple identical execution units. In a superscalar pipeline, multiple instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so they are dispatched to available execution units, resulting in the ability for several instructions to be executed simultaneously. In general, the more instructions a superscalar CPU is able to dispatch simultaneously to waiting execution units, the more instructions will be completed in a given cycle.
Most of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly and correctly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and gives rise to the need in superscalar architectures for significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, and out-of-order execution crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of single instruction stream, multiple data stream—a case when a lot of data from the same type has to be processed—, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.
In the case where a portion of the CPU is superscalar and part is not, the part which is not suffers a performance penalty due to scheduling stalls. The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock cycle each, but its FPU could not accept one instruction per clock cycle. Thus the P5 was integer superscalar but not floating point superscalar. Intel's successor to the P5 architecture, P6, added superscalar capabilities to its floating point features, and therefore afforded a significant increase in floating point instruction performance.
Both simple pipelining and superscalar design increase a CPU's ILP by allowing a single processor to complete execution of instructions at rates surpassing one instruction per clock cycle. Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or ISA. The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the amount of work the CPU must perform to boost ILP and thereby reducing the design's complexity.
Task-level parallelism.
Another strategy of achieving performance is to execute multiple threads or processes in parallel. This area of research is known as parallel computing. In Flynn's taxonomy, this strategy is known as multiple instruction stream, multiple data stream (MIMD).
One technology used for this purpose was multiprocessing (MP). The initial flavor of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single chip, the technology is known as chip-level multiprocessing (CMP) and the single chip as a multi-core processor.
It was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as multi-threading (MT). This approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU is replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as temporal multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly context switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the UltraSPARC Technology. Another type of MT is known as simultaneous multithreading, where instructions of multiple threads are executed in parallel within one CPU clock cycle.
For several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.
CPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or process.
This reversal of emphasis is evidenced by the proliferation of dual and more core processor designs and notably, Intel's newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360's triple-core PowerPC design, and the PS3's 7-core Cell microprocessor.
Data parallelism.
A less common but increasingly important paradigm of processors (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn's taxonomy, these two schemes of dealing with data are generally referred to as single instruction stream, multiple data stream (SIMD) and single instruction stream, single data stream (SISD), respectively. The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks are multimedia applications (images, video, and sound), as well as many types of scientific and engineering tasks. Whereas a scalar processor must complete the entire process of fetching, decoding, and executing each instruction and value in a set of data, a vector processor can perform a single operation on a comparatively large set of data with one instruction. Of course, this is only possible when the application tends to require many steps which apply one operation to a large set of data.
Most early vector processors, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose processors has become significant. Shortly after inclusion of floating-point units started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose processors. Some of these early SIMD specifications like HP's Multimedia Acceleration eXtensions (MAX) and Intel's MMX were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating-point numbers. Progressively, these early designs were refined and remade into some of the common, modern SIMD specifications, which are usually associated with one ISA. Some notable modern examples are Intel's SSE and the PowerPC-related AltiVec (also known as VMX).
Performance.
The "performance" or "speed" of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform.
Many reported IPS values have represented "peak" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in MIPS calculations. Because of these problems, various standardized tests, often called "benchmarks" for this purposesuch as SPECinthave been developed to attempt to measure the real effective performance in commonly used applications.
Processing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called "cores" in this sense) into one integrated circuit. Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, the performance gain is far smaller, only about 50%, due to imperfect software algorithms and implementation. Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information.
Due to specific capabilities of modern CPUs, such as hyper-threading and uncore, which involve sharing of actual CPU resources while aiming at increased utilization, monitoring performance levels and hardware utilization gradually became a more complex task. As a response, some CPUs implement additional hardware logic that monitors actual utilization of various parts of a CPU and provides various counters accessible to software; an example is Intel's "Performance Counter Monitor" technology.

</doc>
<doc id="5221" url="https://en.wikipedia.org/wiki?curid=5221" title="Carnivora">
Carnivora

Carnivora (; from Latin "carō" (stem "carn-") "flesh", + "vorāre" "to devour") is a diverse order that includes over 280 species of placental mammals. Its members are formally referred to as carnivorans, whereas the word "carnivore" (often popularly applied to members of this group) can refer to any meat-eating organism. Carnivorans are the most diverse in size of any mammalian order, ranging from the least weasel ("Mustela nivalis"), at as little as and , to the polar bear ("Ursus maritimus"), which can weigh up to , to the southern elephant seal ("Mirounga leonina"), whose adult males weigh up to and measure up to in length.
The first carnivoran was a carnivore, and nearly all carnivorans primarily eat meat. Some, such as cats and pinnipeds, depend entirely on meat for their nutrition. Others, such as raccoons and bears, depending on the local habitat, are more omnivorous: the giant panda is almost exclusively a herbivore, but will take fish, eggs and insects, while the polar bear subsists mainly on seals. Carnivorans have teeth and claws adapted for catching and eating other animals. Many hunt in packs and are social animals, giving them an advantage over larger prey.
Carnivorans apparently evolved in North America out of members of the family Miacidae (miacids) about 42 million years ago. They soon split into cat-like and dog-like forms (Feliformia and Caniformia). Their molecular phylogeny shows the extant Carnivora are a monophyletic group, the crown group of the Carnivoramorpha.
Distinguishing features.
Most carnivorans are terrestrial; they usually have strong, sharp claws, with never fewer than four toes on each foot, and well-developed, prominent canine teeth, cheek teeth (premolars, and molars) that generally have cutting edges. The last premolar of the upper jaw and first molar of the lower are termed the carnassials or sectorial teeth. These blade-like teeth occlude (close) with a scissor-like action for shearing and shredding meat. Carnassials are most highly developed in the Felidae and the least developed in the Ursidae. Carnivorans have six incisors and two conical canines in each jaw. The only two exceptions to this are the sea otter ("Enhydra lutris"), which has four incisors in the lower jaw, and the sloth bear ("Melursus ursinus"), which has four incisors in the upper jaw. The number of molars and premolars is variable between carnivoran species, but all teeth are deeply rooted and are diphyodont. Incisors are retained by carnivorans and the third incisor is commonly large and sharp (canine-like). Carnivorans have either four or five digits on each foot, with the first digit on the forepaws, also known as the dew claw, being vestigial in most species and absent in some.
The superfamily Canoidea (or suborder Caniformia) – Canidae (wolves, dogs and foxes), Mephitidae (skunks and stink badgers), Mustelidae (weasels, badgers, and otters), Procyonidae (raccoons), Ursidae (bears), Ailuridae (red panda), Otariidae (eared seals), Odobenidae (walrus), and Phocidae (earless seals) (the last three families formerly classified in the superfamily Pinnipedia) and the extinct family Amphicyonidae (bear-dogs) – are characterized by having nonchambered or partially chambered auditory bullae, nonretractable claws, and a well-developed baculum. Most species are rather plain in coloration, lacking the flashy spotted or rosetted coats like many species of felids and viverrids have. This is because Canoidea tend to range in the temperate and subarctic biomes, although Mustelidae and Procyonidae have a few tropical species. Most are terrestrial, although a few species, like procyonids, are arboreal. All families except the Canidae and a few species of Mustelidae are plantigrade. Diet is varied and most tend to be omnivorous to some degree, and thus the carnassial teeth are less specialized. Canoidea have more premolars and molars in an elongated skull.
The superfamily Feloidea (or suborder Feliformia)– Felidae (cats), Prionodontidae (Asiatic linsangs), Herpestidae (mongooses), Hyaenidae (hyenas), Viverridae (civets), and Eupleridae (Malagasy carnivorans), as well as the extinct family Nimravidae (paleofelids) – often have spotted, rosetted or striped coats, and tend to be more brilliantly colored than their Canoidean counterparts. This is because these species tend to range in tropical habitats, although a few species do inhabit temperate and subarctic habitats. Many are arboreal or semiarboreal, and the majority are digitigrade. Diet tends to be more strictly carnivorous, especially in the family Felidae. They have fewer teeth and shorter skulls, with much more specialized carnassials meant for shearing meat. Feliformia claws are often retractile, or rarely, semiretractile. The terminal phalanx, with the claw attached, folds back in the forefoot into a sheath by the outer side of the middle phalanx of the digit, and is retained in this position when at rest by a strong elastic ligament. In the hindfoot, the terminal joint or phalanx is retracted on to the top, and not the side of the middle phalanx. Deep flexor muscles straighten the terminal phalanges, so the claws protrude from their sheaths, and the soft "velvety" paw becomes suddenly converted into a formidable weapon. The habitual retraction of the claws preserves their points from wear.
The superfamily Pinnipedia (walruses, seals, and sea lions), now considered to be part of Caniformia, are medium to large (to 6.5 m) aquatic mammals. Being homeothermic (warm-blooded) marine mammals, pinnipeds need a low surface area to body mass ratio. Otherwise, they would suffer from excessive heat loss due to water's high capacity for heat conduction. The body is usually insulated with a thick layer of fat called blubber and typically covered with hair. The digits are not separate, but connected by a thick web that forms flippers for swimming; thus, the forelimbs and hindlimbs are transformed into paddles. This enables them to dive at extreme depths (600 m for the Weddell seal). They can remain underwater for long periods of time, sometimes an hour or more, but most dives are usually short. The facial region of skull is relatively small, with pinnae very small or lacking, and the vibrissae are well developed. The molariform teeth are mostly homodont and the canines are well developed. The tail is very short or absent, the ears are small or absent as well, and the external genitalia are hidden in slits or depressions in the body.
Skull structure.
Members of Carnivora have a characteristic skull shape with relatively large brains encased in a heavy skull. The skull has a highly developed zygomatic arch just behind the maxilla (common to all mammals and their cynodont forebears), and they have ossified external auditory bullae. Feloidea have a two-chambered auditory bulla. In addition to allowing extra room for the passage of muscles to work the lower jaw, the zygomatic arch also allows for differentiation of separate muscle groups to be involved in biting and chewing. Masseters attach from the dentary (specifically, the masseteric fossa) to the zygomatic arch and onto the maxilla in front of the arch, providing crushing force. The temporalis attaches from the dentary (specifically, the coronoid process) to the side of the braincase, providing torque about the axis of jaw articulation. In comparing the skulls of carnivores and herbivores, it can be seen that the shearing force of the temporalis is somewhat more important to carnivores, which have more room on the braincase (this is not unrelated to carnivoran intelligence) and commonly develop a sagittal crest (running from posterior to anterior on the skull), providing yet additional room for temporalis attachment. Carnivoran jaws can only move on a vertical axis, in an up-and-down motion, and cannot move from side-to-side. The jaw joint in carnivores tends to lie within the plane of tooth occlusion, an arrangement that further emphasizes shearing (as in a pair of scissors). In herbivores, the crushing force of the masseters is relatively more important than is shearing. The jaw joint is generally well above the plane of tooth occlusion, allowing extra room for masseteric attachment on the dentary and causing the rotation of the lower jaw to be translated into straight-ahead crushing force between the teeth of the upper and lower jaws.
Physiology.
Carnivora have a simple stomach adapted to digest primarily meat, as compared to the elaborate digestive systems of herbivorous animals, which are necessary to break down tough, complex plant fibers. The caecum is either absent or short and simple, and the colon is not sacculated or much wider than the small intestine. Most species of Carnivora are, to some degree, omnivorous, except the Felidae and Pinnipedia, which are obligate carnivores. Most have highly developed senses, especially vision and hearing, and often a highly acute sense of smell in many species, such as in the Canoidea. They are excellent runners: some are long-distance runners, but more commonly are sprinters. Even bears and raccoons, although seemingly slow and clumsy, are capable of remarkable bursts of speed.
Diet specializations.
Carnivorans include carnivores, omnivores, and even a few primarily herbivorous species, such as the giant panda and the binturong. Important teeth for carnivorans are the large, slightly recurved canines, used to dispatch prey, and the carnassial complex, used to rend meat from bone and slice it into digestible pieces. Dogs have molar teeth behind the carnassials for crushing bones, but cats have only a greatly reduced, functionless molar behind the carnassial in the upper jaw. Cats will strip bones clean but will not crush them to get the marrow inside. Omnivores, such as bears and raccoons, have developed blunt, molar-like carnassials. Carnassials are a key adaptation for terrestrial vertebrate predation; all other placental orders are primarily herbivores, insectivores, or aquatic.
Reproductive system.
Carnivorans tend to produce a single litter annually, but some produce multiple litters a year, and larger carnivorans, like bears, have gaps of 2–3 yr between litters. The average gestation period lies between 50 and 115 days, although the ursids and mustelids have delayed implantation, thus extending the gestation period six to 9 months beyond the normal period. Litter sizes are usually small, ranging from one to 13 young, which are born with underdeveloped eyes and ears. In most species, the mother has exclusive or at least primary care of the offspring. Many species of carnivorans are solitary, but a few are gregarious.
Phylogeny.
Carnivorans evolved from members of the family Miacidae (miacids, now recognized as paraphyletic). The transition from Miacidae to Carnivora was a general trend in the middle and late Eocene, with taxa from both North America and Eurasia involved. The divergence of carnivorans from other miacids, as well as the divergence of the two clades within Carnivora, Caniformia and Feliformia, is now inferred to have happened in the middle Eocene, about 42 million years ago (mya).
Traditionally, the extinct family Viverravidae (viverravids) had been thought to be the earliest carnivorans, with fossil records first appearing in the Paleocene of North America about 60 mya, but recently described evidence from cranial morphology now places them outside the order Carnivora.
The Miacidae are not a monophyletic group, but a paraphyletic array of stem taxa. Today, Carnivora is restricted to the crown group, Carnivora and miacoids are grouped in the clade Carnivoramorpha, and the miacoids are regarded as basal carnivoramorphs. Based on dental features and braincase sizes, it is now known that Carnivora must have evolved from a form even more primitive than Creodonta, and thus these two orders may not even be sister groups. The Carnivora, Creodonta, Pholidota, and a few other extinct orders are informally grouped together in the clade Ferae. Older classification schemes divided the order into two suborders: Fissipedia (which included the families of primarily land Carnivora) and Pinnipedia (which included the true seals, eared seals, and walrus). However, it is now recognized that the Fissipedia is a paraphyletic group and that the pinnipeds were not the sister group to the fissipeds but rather had arisen from among them.
Carnivora are generally divided into the suborders Feliformia (cat-like) and Caniformia (dog-like), the latter of which includes the pinnipeds. The pinnipeds are part of a clade, known as the Arctoidea, which also includes the Ursidae (bears) and the superfamily Musteloidea. The Musteloidea in turn consists of the Mustelidae (mustelids: weasels), Procyonidae (procyonids: raccoons), Mephitidae (skunks) and "Ailurus" (red panda). The oldest caniforms are the "Miacis" species "Miacis cognitus", the Amphicyonidae (bear-dogs) such as "Daphoenus", and "Hesperocyon" (of the family Canidae, subfamily Hesperocyoninae). Hesperocyonine canids first appeared in North America, and the earliest species is currently dated at 39.74 mya, but they were not represented in Europe until well into the Miocene, and not into Asia and Africa until the Pliocene. "Miacis" and Amphicyonidae were the first of the caniforms to split from the others and are sometimes considered to be sister groups to Ursidae, but the exact closeness of Amphicyonidae and Ursidae, as well as Arctoidae to Ursidae, is still uncertain. The Canidae (wolves, coyotes, jackals, foxes and dogs) are generally considered to be the sister group to Arctoidea. The Ursidae first occur in North America in the Late Eocene (ca. 38 mya) as the very small and graceful "Parictis" that had a skull only 7 cm long. Like the canids, this family does not appear in Eurasia and Africa until the Miocene. The other caniform families Amphicyonidae, Mustelidae and Procyonidae occur in both the Old World and the New World by the Late Eocene and Early Oligocene.
The ancestor of all Feliformia evolved from the Caniformia-Feliformia split. "Nandinia", the African palm civet, seems to be the most primitive of all the feliforms and the very first to split from the others. The Asiatic linsangs of the genus "Prionodon" (traditionally placed in the Viverridae) form a family of their own, as some recent studies indicate that "Prionodon" is actually the closest living relative to the cats. The Nimravidae are sometimes seen as the most basal of all feliforms and the first to split from the others, but there is a possibility that Nimravidae might not even belong within the order, and therefore its position as a clade within Carnivora is currently unstable. Other studies indicate that the barbourofelids form a separate family, which is closely related to the true felids instead of being related to the nimravids. Recognizable nimravid fossils date from the late Eocene (37 mya), from the Chadronian White River Carnivora Formation at Flagstaff Rim, Wyoming. Nimravid diversity appears to have peaked about 28 mya. The hypercarnivorous (strictly meat-eating) nimravid feliforms were extinct in North America after 26 mya and felids did not arrive in North America until the early middle Miocene (16 mya).
It has been suggested that canids evolved hypercarnivorous morphologies because feliforms were absent during this period (the "cat-gap", 26-18.5 mya), however recent data do not support this hypothesis. Hypercarnivore feliforms (felids and nimravids) occupied an area that canids did not and where felids, nimravids, and hypercarnivorous creodonts are found. Hypercarnivorous canids were present before the disappearance of the nimravids, and all became extinct before the appearance of felids. Following the extinction of nimravids, only three taxa originated, two of which were relatively small in body size. Disparity increased during the "cat-gap" even with the extinction of the hypercarnivorous extremes. This was due to the extinction of morphological intermediates, and because carnivorans began to occupy hypocarnivorous (nonmeat-specialist) morphospace for the first time in North America. Procyonids did not arrive in North America until the early Miocene, and "modern" ursids (e.g., Ursinae), did not arrive until the late Miocene. Extinct lineages of Ursidae were present in North America from the late Eocene through the Miocene and amphicyonids (bear-dogs) were present during this period as well, but occupied a morphospace generally shared with canids and not in close proximity to ursids. A large question remains as to why there was a progressive decline in hypercarnivorous carnivoramorphans during the late Oligocene/early Miocene. During this period all hypercarnivorous forms disappeared from the fossil record, including hypercarnivorous feliforms, canids, and mustelids. One possible explanation is climate change. Earth was gradually cooling after the late Paleocene, and over a period spanning the Eocene/Oligocene boundary, a dramatic climatic cooling event occurred.
A recent study has finally resolved the exact position of "Ailurus": the red panda is neither a procyonid nor an ursid, but forms a monotypic family, with the other musteloids as its closest living relatives. The same study also showed that the mustelids are not a primitive family, as was once thought. Their small body size is a secondary trait—the primitive body form of the arctoids was large, not small. Recent molecular studies also suggest that the endemic Carnivora of Madagascar, including three genera usually classed with the civets and four genera of mongooses classed with the Herpestidae, are all descended from a single ancestor. They form a single sister taxon to the Herpestidae. The hyenas are also closely related to this clade.
Classification.
When order Carnivora was first discovered and named, there were only 5 families
The most common modern classification scheme divides the Carnivora into sixteen living and a number of extinct families, as follows:

</doc>
<doc id="5222" url="https://en.wikipedia.org/wiki?curid=5222" title="Colombia">
Colombia

Colombia ( or ; ), officially the Republic of Colombia (), is a country situated in the northwest of South America, bordered to the northwest by Panama; to the east by Venezuela and Brazil; to the south by Ecuador and Peru; and it shares maritime limits with Costa Rica, Nicaragua, Honduras, Jamaica, Dominican Republic and Haiti. It is a unitary, constitutional republic comprising thirty-two departments.
The territory of what is now Colombia was originally inhabited by indigenous peoples including the Muisca, Quimbaya, and Tairona. The Spanish arrived in 1499 and initiated a period of conquest and colonization ultimately creating the Viceroyalty of New Granada, with its capital at Bogotá. Independence from Spain was won in 1819, but by 1830 "Gran Colombia" had collapsed with the secession of Venezuela and Ecuador. What is now Colombia and Panama emerged as the Republic of New Granada. The new nation experimented with federalism as the Granadine Confederation (1858), and then the United States of Colombia (1863), before the Republic of Colombia was finally declared in 1886. Panama seceded in 1903.
Since the 1960s, the country has suffered from an asymmetric low-intensity armed conflict, which escalated in the 1990s, but then decreased from 2005 onward.
Colombia is ethnically diverse, its people descending from the original native inhabitants, Spanish colonists, Africans originally brought to the country as slaves, and 20th-century immigrants from Europe and the Middle East, all contributing to a diverse cultural heritage. This has also been influenced by Colombia's varied geography, and the imposing landscape of the country has resulted in the development of very strong regional identities. The majority of the urban centres are located in the highlands of the Andes mountains, but Colombian territory also encompasses Amazon rainforest, tropical grassland and both Caribbean and Pacific coastlines.
Ecologically, Colombia is considered one of the world's 17 megadiverse countries, and of these, the most biodiverse per square kilometer.
Colombia is a middle power and a regional actor with the fourth largest economy in Latin America, is part of the CIVETS group of six leading emerging markets and is an accessing member to the OECD. Colombia has a diversified economy with macroeconomic stability and favorable growth prospects in the long run.
Etymology.
The name "Colombia" is derived from the last name of Christopher Columbus (, ). It was conceived by the Venezuelan revolutionary Francisco de Miranda as a reference to all the New World, but especially to those under the Spanish and Portuguese rule. The name was later adopted by the Republic of Colombia of 1819, formed out of the territories of the old Viceroyalty of New Granada (modern-day Colombia, Panama, Venezuela, Ecuador, and northwest Brazil).
When Venezuela and Ecuador parted ways, the Cundinamarca region that remained became a new country – the Republic of New Granada. In 1858 New Granada officially changed its name to the Granadine Confederation, then in 1863 the United States of Colombia, before finally adopting its present name – the Republic of Colombia – in 1886.
To refer to this country, the Colombian government uses the terms "Colombia" and "República de Colombia".
History.
Pre-Columbian era.
Due to its location, the present territory of Colombia was a corridor of early human migration from Mesoamerica and the Caribbean to the Andes and Amazon. The oldest archaeological finds are from the Pubenza and El Totumo sites in the Magdalena Valley 100 km southwest of Bogotá. These sites date from the Paleoindian period (18,000–8000 BCE). At Puerto Hormiga and other sites, traces from the Archaic Period (~8000–2000 BCE) have been found. Vestiges indicate that there was also early occupation in the regions of El Abra and Tequendama in Cundinamarca. The oldest pottery discovered in the Americas, found at San Jacinto, dates to 5000 – 4000 BCE.
By 10,500 BCE, the territory of what is now Colombia was inhabited by aboriginal people. Nomadic hunter-gatherer tribes existed near present-day Bogotá (at El Abra and Tequendama sites) which traded with one another and with cultures living in the Magdalena River Valley. Between 5000 and 1000 BCE, hunter-gatherer tribes transitioned to agrarian societies; fixed settlements were established, and pottery appeared. Beginning in the 1st millennium BCE, groups of Amerindians including the Muisca, Quimbaya, and Tairona developed the political system of "cacicazgos" with a pyramidal structure of power headed by caciques. The Muisca inhabited mainly the area of what is now the Departments of Boyacá and Cundinamarca high plateau ("Altiplano Cundiboyacense") where they formed the Muisca Confederation. They farmed maize, potato, quinoa and cotton, and traded gold, emeralds, blankets, ceramic handicrafts, coca and salt with neighboring nations. The Taironas inhabited northern Colombia in the isolated Andes mountain range of Sierra Nevada de Santa Marta. The Quimbayas inhabited regions of the Cauca River Valley between the Occidental and Central cordilleras. The Incas expanded their empire on the southwest part of the country.
Spanish rule.
Alonso de Ojeda (who had sailed with Columbus) reached the Guajira Peninsula in 1499. Spanish explorers, led by Rodrigo de Bastidas, made the first exploration of the Caribbean littoral in 1500. Christopher Columbus navigated near the Caribbean in 1502. In 1508, Vasco Núñez de Balboa accompanied an expedition to the territory through the region of Gulf of Urabá and they founded the town of Santa María la Antigua del Darién in 1510, the first stable settlement on the continent. 
Santa Marta was founded in 1525, and Cartagena in 1533. Spanish conquistador Gonzalo Jiménez de Quesada led an expedition to the interior in April, 1536, and christened the districts through which he passed "New Kingdom of Granada". In August, 1538, he founded provisionally its capital near the Muisca cacicazgo of Bacatá, and named it "Santa Fe". The name soon acquired a suffix and was called Santa Fe de Bogotá. Two other notable journeys by early conquistadors to the interior took place in the same period. Sebastián de Belalcázar, conqueror of Quito, traveled north and founded Cali, in 1536, and Popayán, in 1537; from 1536–1539, German conquistador Nikolaus Federmann crossed the Llanos Orientales and went over the Cordillera Oriental in a search for El Dorado, the "city of gold". The legend and the gold would play a pivotal role in luring the Spanish and other Europeans to New Granada during the 16th and 17th centuries.
In 1542, the region of New Granada, along with all other Spanish possessions in South America, became part of the Viceroyalty of Peru, with its capital at Lima. In 1547, New Granada became the Captaincy-General of New Granada within the viceroyalty.
In 1549, the Royal Audiencia was created by a royal decree, and New Granada was ruled by the Royal Audience of Santa Fe de Bogotá, which at that time comprised the provinces of Santa Marta, Rio de San Juan, Popayán, Guayana and Cartagena. But important decisions were taken from the colony to Spain by the Council of the Indies.
Indigenous peoples in New Granada experienced a decline in population due to conquest by the Spanish as well as Eurasian diseases, such as smallpox, to which they had no immunity. With the risk that the land was deserted, the Spanish Crown sold properties to the governors, conquerors and their descendants creating large farms and possession of mines. In the 16th century, Europeans began to bring slaves from Africa. Also there were heroes who defended human rights and freedom of oppressed peoples. To protect and exploit the indigenous peoples, several forms of land ownership and regulation were established: "resguardos", "encomiendas" and "haciendas". Repopulation was achieved by allowing colonization by farmers and their families who came from Spain.
In 1717 the Viceroyalty of New Granada was originally created, and then it was temporarily removed, to finally be reestablished in 1739. The Viceroyalty had Santa Fé de Bogotá as its capital. This Viceroyalty included some other provinces of northwestern South America which had previously been under the jurisdiction of the Viceroyalties of New Spain or Peru and correspond mainly to today's Venezuela, Ecuador and Panama. So, Bogotá became one of the principal administrative centers of the Spanish possessions in the New World, along with Lima and Mexico City, though it remained somewhat backward compared to those two cities in several economic and logistical ways.
After Great Britain declared war on Spain in 1739, Cartagena quickly became the British forces’ top target but an upset Spanish victory during the War of Jenkins’ Ear, a war with Great Britain for economic control of the Caribbean cemented Spanish dominance in the Caribbean until the Seven Years’ War.
The 18th-century priest, botanist and mathematician José Celestino Mutis was delegated by Viceroy Antonio Caballero y Góngora to conduct an inventory of the nature of the New Granada. Started in 1783, this became known as the Royal Botanical Expedition to New Granada which classified plants, wildlife and founded the first astronomical observatory in the city of Santa Fe de Bogotá. In July 1801 the Prussian scientist Alexander von Humboldt reached Santa Fe de Bogotá where he met with Mutis. In addition, historical figures in the process of independence in New Granada emerged from the expedition as the astronomer Francisco José de Caldas, the scientist Francisco Antonio Zea, the zoologist Jorge Tadeo Lozano and the painter Salvador Rizo.
Independence.
Since the beginning of the periods of conquest and colonization, there were several rebel movements against Spanish rule, but most were either crushed or remained too weak to change the overall situation. The last one that sought outright independence from Spain sprang up around 1810, following the independence of St. Domingue (present-day Haiti) in 1804, which provided some support to the eventual leaders of this rebellion: Simón Bolívar and Francisco de Paula Santander.
A movement was initiated by Antonio Nariño, who opposed Spanish centralism and led the opposition against the viceroyalty. Cartagena became independent in November 1811. Took place the formation of two independent governments which fought a civil war – a period known as the Foolish Fatherland. In 1811 the United Provinces of New Granada were proclaimed, headed by Camilo Torres Tenorio. Despite the successes of the rebellion, the emergence of two distinct ideological currents among the liberators (federalism and centralism) gave rise to an internal clash which contributed to the reconquest of territory by the Spanish. The viceroyalty was restored under the command of Juan Sámano, whose regime punished those who participated in the uprisings. The retribution stoked renewed rebellion, which, combined with a weakened Spain, made possible a successful rebellion led by the Venezuelan-born Simón Bolívar, who finally proclaimed independence in 1819. The pro-Spanish resistance was defeated in 1822 in the present territory of Colombia and in 1823 in Venezuela.
The territory of the Viceroyalty of New Granada became the Republic of Colombia, organized as a union of the current territories of Colombia, Panama, Ecuador, Venezuela, parts of Guyana and Brazil and north of Marañón River. The Congress of Cúcuta in 1821 adopted a constitution for the new Republic. Simón Bolívar became the first President of Colombia, and Francisco de Paula Santander was made Vice President. However, the new republic was unstable and ended with the rupture of Venezuela and Ecuador in 1830.
Colombia was the first constitutional government in South America, and the Liberal and Conservative parties, founded in 1848 and 1849 respectively, are two of the oldest surviving political parties in the Americas. Slavery was abolished in the country in 1851.
Internal political and territorial divisions led to the secession of Venezuela and Ecuador in 1830. The so-called "Department of Cundinamarca" adopted the name "Nueva Granada", which it kept until 1858 when it became the "Confederación Granadina" (Granadine Confederation). After a two-year civil war in 1863, the "United States of Colombia" was created, lasting until 1886, when the country finally became known as the Republic of Colombia. Internal divisions remained between the bipartisan political forces, occasionally igniting very bloody civil wars, the most significant being the Thousand Days' War (1899–1902).
20th century.
The United States of America's intentions to influence the area (especially the Panama Canal construction and control) led to the separation of the Department of Panama in 1903 and the establishment of it as a nation. The United States paid Colombia $25,000,000 in 1921, seven years after completion of the canal, for redress of President Roosevelt's role in the creation of Panama, and Colombia recognized Panama under the terms of the Thomson–Urrutia Treaty. Colombia was engulfed in the war with Peru over a territorial dispute involving the Amazonas department and its capital Leticia.
Soon after, Colombia achieved some degree of political stability, which was interrupted by a bloody conflict that took place between the late 1940s and the early 1950s, a period known as "La Violencia" ("The Violence"). Its cause was mainly mounting tensions between the two leading political parties, which subsequently ignited after the assassination of the Liberal presidential candidate Jorge Eliécer Gaitán on 9 April 1948. The ensuing riots in Bogotá, known as El Bogotazo, spread throughout the country and claimed the lives of at least 180,000 Colombians.
Colombia entered the Korean War when Laureano Gómez was elected as President. It was the only Latin American country to join the war in a direct military role as an ally of the United States. Particularly important was the resistance of the Colombian troops at Old Baldy.
From 1953 to 1964 the violence between the two political parties decreased first when Gustavo Rojas deposed the President of Colombia in a coup d'état and negotiated with the guerrillas, and then under the military junta of General Gabriel París Gordillo.
After Rojas' deposition, the Colombian Conservative Party and Colombian Liberal Party agreed to create the "National Front", a coalition which would jointly govern the country. Under the deal, the presidency would alternate between conservatives and liberals every 4 years for 16 years; the two parties would have parity in all other elective offices. The National Front ended "La Violencia", and National Front administrations attempted to institute far-reaching social and economic reforms in cooperation with the Alliance for Progress. In the end, the contradictions between each successive Liberal and Conservative administration made the results decidedly mixed. Despite the progress in certain sectors, many social and political problems continued, and guerrilla groups were formally created such as the FARC, ELN, EPL, MAQL, PRT and M-19 to fight the government and political apparatus.
Since the 1960s, the country has suffered from an asymmetric low-intensity armed conflict between the government forces, left-wing guerrilla groups and right-wing paramilitaries. The conflict escalated in the 1990s. The conflict in Colombia takes place mainly in remote rural areas or marginalized sectors of very difficult access. Since the beginning of the armed conflict, human rights defenders have staged heroic acts that shows us the importance of standing up against injustice and fight for the respect for human rights, despite staggering opposition.
The United States has been heavily involved in the conflict since its beginnings, when in the early 1960s the U.S. government encouraged the Colombian military to attack leftist militias in rural Colombia. This was part of the U.S. fight against communism.
On 4 July 1991, a new Constitution was promulgated. The changes generated by the new constitution are viewed as positive by Colombian society.
Recent history.
During the presidency of Álvaro Uribe, the government applied more military pressure on the FARC and other outlawed groups. After the offensive, many security indicators improved. Since 2005 the violence has decreased significantly, with some paramilitary groups demobilizing as part of a controversial peace process. As a result, the guerrillas lost control of much of the territory they had once dominated. Colombia achieved a great decrease in cocaine production, leading White House drug czar R. Gil Kerlikowske to announce that Colombia is no longer the world's biggest producer of cocaine.
In February 2008, millions of Colombians demonstrated against FARC. 26,648 FARC and ELN fighters have decided to demobilize since 2002. During these years the military forces of the Republic of Colombia managed to be strengthened.
The Peace process in Colombia, 2012 refers to the dialogue between the Colombian government and guerrilla of FARC-EP with the aim to find a political solution to the armed conflict. The Colombian government and rebel groups meet in Cuba. As of December 2015, the talks have led to significant breakthroughs. The Government also began a process of assistance and reparation for victims of conflict.
Colombia shows modest progress in the struggle to defend human rights, as expressed by HRW. In terms of international relations, Colombia has moved from a period of tension and animosity with Venezuela, towards a positive outlook and a spirit of cooperation. Colombia has also won a seat on the Security Council of the UN.
Today Colombia is the third largest oil producer in South America and at the end of 2012 it was estimated Colombia was producing a million barrels a day.
In 2015, the National Administrative Department of Statistics (DANE) reported that 27.8% of the population were living below the poverty line, of which 7.9% in "extreme poverty". 171,000 people have been lifted out of poverty. The Government has also been developing a process of financial inclusion within the country's most vulnerable population.
Recent economic growth has led to a considerable increase of new millionaires, including the new entrepreneurs, Colombians with a net worth exceeding US $1 billion.
Geography.
The geography of Colombia is characterized by its six main natural regions that present their own unique characteristics, from the Andes mountain range region shared with Ecuador and Venezuela; the Pacific coastal region shared with Panama and Ecuador; the Caribbean coastal region shared with Venezuela and Panama; the "Llanos" (plains) shared with Venezuela; the Amazon Rainforest region shared with Venezuela, Brazil, Peru and Ecuador; to the insular area, comprising islands in both the Atlantic and Pacific oceans.
Colombia is bordered to the northwest by Panama; to the east by Venezuela and Brazil; to the south by Ecuador and Peru; it established its maritime boundaries with neighboring countries through seven agreements on the Caribbean Sea and three on the Pacific Ocean. It lies between latitudes 12°N and 4°S, and longitudes 67° and 79°W.
Part of the Ring of Fire, a region of the world subject to earthquakes and volcanic eruptions, Colombia is dominated by the Andes (which contain the majority of the country's urban centres). Beyond the Colombian Massif (in the south-western departments of Cauca and Nariño) these are divided into three branches known as "cordilleras" (mountain ranges): the Cordillera Occidental, running adjacent to the Pacific coast and including the city of Cali; the Cordillera Central, running between the Cauca and Magdalena River valleys (to the west and east respectively) and including the cities of Medellín, Manizales, Pereira and Armenia; and the Cordillera Oriental, extending north east to the Guajira Peninsula and including Bogotá, Bucaramanga and Cúcuta.
Peaks in the Cordillera Occidental exceed , and in the Cordillera Central and Cordillera Oriental they reach . At , Bogotá is the highest city of its size in the world.
East of the Andes lies the savanna of the "Llanos", part of the Orinoco River basin, and, in the far south east, the jungle of the Amazon rainforest. Together these lowlands comprise over half Colombia's territory, but they contain less than 3% of the population. To the north the Caribbean coast, home to 20% of the population and the location of the major port cities of Barranquilla and Cartagena, generally consists of low-lying plains, but it also contains the Sierra Nevada de Santa Marta mountain range, which includes the country's tallest peaks (Pico Cristóbal Colón and Pico Simón Bolívar), and the La Guajira Desert. By contrast the narrow and discontinuous Pacific coastal lowlands, backed by the Serranía de Baudó mountains, are sparsely populated and covered in dense vegetation. The principal Pacific port is Buenaventura.
The main rivers of Colombia are Magdalena, Cauca, Guaviare, Atrato, Meta, Putumayo and Caquetá. Colombia has four main drainage systems: the Pacific drain, the Caribbean drain, the Orinoco Basin and the Amazon Basin. The Orinoco and Amazon Rivers mark limits with Colombia to Venezuela and Peru respectively.
Protected areas and the "National Park System" cover an area of about and account for 12.77% of the Colombian territory. Compared to neighboring countries, rates of deforestation in Colombia are still relatively low. Colombia is the sixth country in the world by magnitude of total renewable freshwater supply, and still has large reserves of freshwater.
Climate.
Colombians customarily describe their country in terms of the climatic zones. Below in elevation is the tierra caliente (hot land), where temperatures are above . About 82.5% of the country's total area lies in the tierra caliente.
The majority of the population can be found in the tierra templada (temperate land, between ), where temperatures vary between and the tierra fría (cold land, ).
In the tierra fría mean temperatures range between . Beyond the tierra fría lie the alpine conditions of the forested zone and then the treeless grasslands of the páramos. Above , where temperatures are below freezing, is the tierra helada, a zone of permanent snow and ice.
Biodiversity.
Colombia is one of the megadiverse countries in biodiversity, ranking first in bird species. As for plants, the country has between 40,000 and 45,000 plant species, equivalent to 10 or 20% of total global species, this is even more remarkable given that Colombia is considered a country of intermediate size. Colombia is the second most biodiverse country in the world, lagging only after Brazil which is approximately 7 times bigger.
Colombia is the country in the planet more characterized by a high biodiversity, with the highest rate of species by area unit worldwide and it has the largest number of endemisms (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth live in Colombia, including over 1,900 species of bird, more than in Europe and North America combined, Colombia has 10% of the world’s mammals species, 14% of the amphibian species and 18% of the bird species of the world.
Colombia has about 2,000 species of marine fish and is the second most diverse country in freshwater fish. Colombia is the country with more endemic species of butterflies, number 1 in terms of orchid species and approximately 7,000 species of beetles. Colombia is second in the number of amphibian species and is the third most diverse country in reptiles and palms. There are about 2,900 species of mollusks and according to estimates there are about 300,000 species of invertebrates in the country. In Colombia there are 32 terrestrial biomes and 314 types of ecosystems.
Government and politics.
The government of Colombia takes place within the framework of a presidential participatory democratic republic as established in the Constitution of 1991. In accordance with the principle of separation of powers, government is divided into three branches: the executive branch, the legislative branch and the judicial branch.
As the head of the executive branch, the President of Colombia serves as both head of state and head of government, followed by the Vice President and the Council of Ministers. The president is elected by popular vote to serve four-year term (In 2015 the Colombia’s Congress approved the repeal of a 2004 constitutional amendment that eliminated the one-term limit for presidents). At the provincial level executive power is vested in department governors, municipal mayors and local administrators for smaller administrative subdivisions, such as "corregimientos" or "comunas". All regional elections are held one year and five months after the presidential election.
The legislative branch of government is represented nationally by the Congress, a bicameral institution comprising a 166-seat Chamber of Representatives and a 102-seat Senate. The Senate is elected nationally and the Chamber of Representatives is elected in electoral districts. Members of both houses are elected to serve four-year terms two months before the president, also by popular vote. 
The judicial branch is headed by four high courts, consisting of the Supreme Court which deals with penal and civil matters, the Council of State, which has special responsibility for administrative law and also provides legal advice to the executive, the Constitutional Court, responsible for assuring the integrity of the Colombian constitution, and the Superior Council of Judicature, responsible for auditing the judicial branch. Colombia operates a system of civil law, which since 2005 has been applied through an adversarial system.
Despite a number of controversies, the democratic security policy has ensured that former President Uribe remained popular among Colombian people, with his approval rating peaking at 76%, according to a poll in 2009. However, having served two terms, he was constitutionally barred from seeking re-election in 2010. In the run-off elections on 20 June 2010 the former Minister of defense Juan Manuel Santos won with 69% of the vote against the second most popular candidate, Antanas Mockus. A second round was required since no candidate received over the 50% winning threshold of votes. Santos won nearly 51% of the vote in second-round elections on 15 June 2014, beating right-wing rival Óscar Iván Zuluaga, who won 45%. His term as Colombia's president runs for four years beginning 7 August 2014.
Foreign affairs.
The foreign affairs of Colombia are headed by the President, as head of state, and managed by the Minister of Foreign Affairs. Colombia has diplomatic missions in all continents.
Colombia was one of the 4 founding members of the Pacific Alliance, which is a political, economic and co-operative integration mechanism that promotes the free circulation of goods, services, capital and persons between the members, as well as a common stock exchange and joint embassies in several countries. Colombia is also a member of the United Nations, the Organization of American States, the Organization of Ibero-American States, the Union of South American Nations and the Andean Community of Nations.
Military.
The executive branch of government is responsible for managing the defense of Colombia, with the President commander-in-chief of the armed forces. The Ministry of Defence exercises day-to-day control of the military and the Colombian National Police. Colombia has 466,713 active military personnel. And in 2013 3.4% of the country's GDP went towards military expenditure, placing it 18th in the world. Colombia's armed forces are the largest in Latin America, and it is the second largest spender on its military after Brazil.
The Colombian military is divided into three branches: the National Army of Colombia; the Colombian Air Force; and the Colombian Navy. The National Police functions as a gendarmerie, operating independently from the military as the law enforcement agency for the entire country. Each of these operates with their own intelligence apparatus separate from the national intelligence agency (ANIC, in Spanish).
The National Army is formed by divisions, brigades, special brigades and special units; the Colombian Navy by the Naval Infantry, the Naval Force of the Caribbean, the Naval Force of the Pacific, the Naval Force of the South, the Naval Force of the East, Colombia Coast Guards, Naval Aviation and the Specific Command of San Andres y Providencia; and the Air Force by 15 air units. The National Police has a presence in all municipalities.
Administrative divisions.
Colombia is divided into 32 departments and one capital district, which is treated as a department (Bogotá also serves as the capital of the department of Cundinamarca). Departments are subdivided into municipalities, each of which is assigned a municipal seat, and municipalities are in turn subdivided into "corregimientos" in rural areas and into "comunas" in urban areas. Each department has a local government with a governor and assembly directly elected to four-year terms, and each municipality is headed by a mayor and council. There is a popularly elected local administrative board in each of the "corregimientos" or "comunas".
In addition to the capital four other cities have been designated districts (in effect special municipalities), on the basis of special distinguishing features. These are Barranquilla, Cartagena, Santa Marta and Buenaventura. Some departments have local administrative subdivisions, where towns have a large concentration of population and municipalities are near each other (for example in Antioquia and Cundinamarca). Where departments have a low population (for example Amazonas, Vaupés and Vichada), special administrative divisions are employed, such as "department "corregimientos"", which are a hybrid of a municipality and a "corregimiento".
Click on a department on the map below to go to its article.
Economy.
Historically an agrarian economy, Colombia urbanised rapidly in the 20th century, by the end of which just 17% of the workforce were employed in agriculture, generating just 6.1% of GDP; 21% of the workforce were employed in industry and 62% in services, responsible for 37.3% and 56.6% of GDP respectively.
Colombia's market economy grew steadily in the latter part of the 20th century, with gross domestic product (GDP) increasing at an average rate of over 4% per year between 1970 and 1998. The country suffered a recession in 1999 (the first full year of negative growth since the Great Depression), and the recovery from that recession was long and painful. However, in recent years growth has been impressive, reaching 6.9% in 2007, one of the highest rates of growth in Latin America. According to International Monetary Fund estimates, in 2012 Colombia's GDP (PPP) was US$500 billion (28th in the world and third in South America).
Total government expenditures account for 28.3 percent of the domestic economy. Public debt equals 32 percent of gross domestic product. A strong fiscal climate was reaffirmed by a boost in bond ratings. Annual inflation closed 2015 at 6.77% YoY (vs. 3.66% YoY in 2014). The average national unemployment rate in 2015 was 8.9%, although the informality is the biggest problem facing the labour market (the income of formal workers climbed 24.8% in 5 years while labor incomes of informal workers rose only 9%). Colombia has Free trade Zone (FTZ), such as Zona Franca del Pacifico, located in the Valle del Cauca, one of the most striking areas for foreign investment.
Colombia is rich in natural resources, and its main exports include mineral fuels, oils, distillation products, precious stones, forest products, pulp and paper, coffee, meat, cereals and vegetable oils, cotton, oilseed, sugars and sugar confectionery, fruit and other agricultural products, food processing, processed fish products, beverages, machinery, electronics, military products, aircraft, ships, motor vehicles, metal products, ferro-alloys, home and office material, chemicals and health related products, petrochemicals, agrochemicals, inorganic salts and acids, perfumery and cosmetics, medicaments, plastics, animal fibers, textile and fabrics, clothing and footwear, leather, construction equipment and materials, cement, software, among others.
Colombia is also known as an important global source of emeralds, while over 70% of cut flowers imported by the United States are Colombian. Non-traditional exports have boosted the growth of Colombian foreign sales as well as the diversification of destinations of export thanks to new free trade agreements. Principal trading partners are the United States, China, the European Union and some Latin American countries.
The electricity production in Colombia comes mainly from renewable energy sources. 70.35% is obtained from the hydroelectric generation. Colombia's commitment to renewable energy was recognized in the 2014 "Global Green Economy Index (GGEI)", ranking among the top 10 nations in the world in terms of greening efficiency sectors.
The financial sector has grown favorably due to good liquidity in the economy, the growth of credit and in general to the positive performance of the Colombian economy. The Colombian Stock Exchange through the Latin American Integrated Market (MILA) offers a regional market to trade equities. Colombia is now one of only three economies with a perfect score on the strength of legal rights index, according to the World Bank.
Tourism in Colombia is an important sector in the country's economy. Foreign tourist visits were predicted to have risen from 0.6 million in 2007 to 2.5 million in 2014.
Science and technology.
Colombia has more than 5,500 research groups in science and technology. iNNpulsa, a government body that promotes entrepreneurship and innovation in the country, provides grants to startups, in addition to other services it and institutions like Apps.co provide. Co-working spaces have arisen to serve as communities for startups large and small. Organizations such as the Corporation for biological research for the support of young people interested in scientific work has been successfully developed in Colombia. The International Center for Tropical Agriculture based in Colombia investigates the increasing challenge of global warming and food security.
Important inventions related to the medicine have been made in Colombia, such as the first external artificial pacemaker with internal electrodes, invented by the electronics engineer Jorge Reynolds Pombo, invention of great importance for those who suffer from heart failure. Also invented in Colombia were the microkeratome and keratomileusis technique, which form the fundamental basis of what now is known as LASIK (one of the most important techniques for the correction of refractive errors of vision) and the Hakim valve for the treatment of Hydrocephalus, among others. Colombia has begun to innovate in military technology for its army and other armies of the world; especially in the design and creation of personal ballistic protection products, military robots, simulators and radar.
Some leading Colombian scientists are Joseph M. Tohme, researcher recognized for his work on the genetic diversity of food, Manuel Elkin Patarroyo who is known for his groundbreaking work on synthetic vaccines for malaria, Francisco Lopera who discovered the "Paisa Mutation" or a type of early-onset Alzheimer's, Rodolfo Llinás known for his study of the intrinsic neurons properties and the theory of a syndrome that had changed the way of understanding the functioning of the brain, Jairo Quiroga Puello recognized for his studies on the characterization of synthetic substances which can be used to fight fungus, tumors, tuberculosis and even some viruses and Ángela Restrepo who established accurate diagnoses and treatments to combat the effects of a disease caused by the "Paracoccidioides brasiliensis", among other scientists.
Infrastructure.
Transportation in Colombia is regulated within the functions of the Ministry of Transport and entities such as the National Roads Institute (INVÍAS) responsible for the Highways in Colombia (13,000 km), the Aerocivil, responsible for civil aviation and airports, the National Infrastructure Agency, in charge of concessions through public–private partnerships, for the design, construction, maintenance, operation, and administration of the transport infrastructure, the General Maritime Directorate (Dimar) has the responsibility of coordinating maritime traffic control along with the Colombian Navy, among others and under the supervision of the Superintendency of Ports and Transport.
The target of Colombia’s government is to build 7,000 km of roads for the 2016–2020 period and reduce travel times by 30 per cent and transport costs by 20 per cent. A toll road concession programme will comprise 40 projects, and is part of a larger strategic goal to invest nearly $50bn in transport infrastructure, including: railway systems; making the Magdalena river navigable again; improving port facilities; as well as an expansion of Bogotá’s airport.
Demographics.
With an estimated 48 million people in 2015, Colombia is the third-most populous country in Latin America, after Brazil and Mexico. It is also home to the third-largest number of Spanish speakers in the world after Mexico and the United States. At the beginning of the 20th century, Colombia's population was approximately 4 million. The population increased at an annual rate of 1.9% between 1975 and 2005, but its rate of growth is projected to drop to 1.2% over the next decade. Colombia is projected to have a population of 50.9 million by 2020. These trends are reflected in the country's age profile. In 2005 over 30% of the population was under 15 years old, compared to just 5.1% aged 65 and over. The total fertility rate was 1.92 births per woman in 2013. 
The population is concentrated in the Andean highlands and along the Caribbean coast. The nine eastern lowland departments, comprising about 54% of Colombia's area, have less than 3% of the population and a density of less than one person per square kilometer (two persons per square mile). Traditionally a rural society, movement to urban areas was very heavy in the mid-20th century, and Colombia is now one of the most urbanized countries in Latin America. The urban population increased from 31% of the total in 1938 to 60% in 1975, and by 2014 the figure stood at 76%. The population of Bogotá alone has increased from just over 300,000 in 1938 to approximately 8 million today. In total seventy one cities now have populations of 100,000 or more (2013). Colombia has the world's largest populations of internally displaced persons (IDPs), estimated to be up to 4.9 million people.
The life expectancy is 78 years in 2012 and infant mortality is 15 per thousand in 2013. In 2013, 93.6% of adults and 98.2% of youth are literate and the government spends about 4.9% of its GDP in education.
Colombia is ranked third in the world in the Happy Planet Index.
Languages.
More than 99.2% of Colombians speak Spanish, also called Castilian; 65 Amerindian languages, two Creole languages, the Romani language and Colombian Sign Language are also spoken in the country. English has official status in the archipelago of San Andrés, Providencia and Santa Catalina.
Including Spanish, a total of 101 languages are listed for Colombia in the Ethnologue database. The specific number of spoken languages varies slightly since some authors consider as different languages what others consider are varieties or dialects of the same language. The best estimates recorded that 70 languages are spoken in the country today. Most of these belong to the Chibchan, Tucanoan, Bora–Witoto, Guajiboan, Arawakan, Cariban, Barbacoan, and Saliban language families. There are currently about 850,000 speakers of native languages.
Ethnic groups.
Colombia is ethnically diverse, its people descending from the original native inhabitants, Spanish colonists, Africans originally brought to the country as slaves, and 20th-century immigrants from Europe and the Middle East, all contributing to a diverse cultural heritage. The demographic distribution reflects a pattern that is influenced by colonial history. Whites tend to live mainly in urban centers, like Bogotá, Medellín or Cali, and the burgeoning highland cities. The populations of the major cities also include mestizos. Mestizo "campesinos" (people living in rural areas) also live in the Andean highlands where some Spanish conquerors mixed with the women of Amerindian chiefdoms. Mestizos include artisans and small tradesmen that have played a major part in the urban expansion of recent decades.
The 2005 census reported that the "non-ethnic population", consisting of whites and mestizos (those of mixed white European and Amerindian ancestry), constituted 86% of the national population. 10.6% is of African ancestry. Indigenous Amerindians comprise 3.4% of the population. 0.01% of the population are Roma. An extraofficial estimate considers that the 49% of the Colombian population is Mestizo or of mixed European and Amerindian ancestry, and that approximately 37% is White, mainly of Spanish lineage, but there is also a large population of Middle East descent; among the upper class there is a considerable input of Italian and German ancestry.
Many of the Indigenous peoples experienced a reduction in population during the Spanish rule and many others were absorbed into the mestizo population, but the remainder currently represents over eighty distinct cultures. Reserves ("resguardos") established for indigenous peoples occupy (27% of the country's total) and are inhabited by more than 800,000 people. Some of the largest indigenous groups are the Wayuu, the Paez, the Pastos, the Emberá and the Zenú. The departments of La Guajira, Cauca, Nariño, Córdoba and Sucre have the largest indigenous populations.
The Organización Nacional Indígena de Colombia (ONIC), founded at the first National Indigenous Congress in 1982, is an organization representing the indigenous peoples of Colombia. In 1991, Colombia signed and ratified the current international law concerning indigenous peoples, Indigenous and Tribal Peoples Convention, 1989.
Black Africans were brought as slaves, mostly to the coastal lowlands, beginning early in the 16th century and continuing into the 19th century. Large Afro-Colombian communities are found today on the Caribbean and Pacific coasts. The population of the department of Chocó, running along the northern portion of Colombia's Pacific coast, is over 80% black. British and Jamaicans migrated mainly to the islands of San Andres and Providencia. A number of other Europeans and North Americans migrated to the country in the late 19th and early 20th centuries, including people from the former USSR during and after the Second World War.
Many immigrant communities have settled on the Caribbean coast, in particular recent immigrants from the Middle East. Barranquilla (the largest city of the Colombian Caribbean) and other Caribbean cities have the largest populations of Lebanese, Palestinian, and other Arabs. There are also important communities of Chinese, Japanese, Romanis and Jews. There is a major migration trend of Venezuelans, due to the political and economic situation in Venezuela.
Religion.
The National Administrative Department of Statistics (DANE) does not collect religious statistics, and accurate reports are difficult to obtain. However, based on various studies and a survey, about 90% of the population adheres to Christianity, the majority of which (70.9%) are Roman Catholic, while a significant minority (16.7%) adhere to Protestantism (primarily Evangelicalism). Some 4.7% of the population is atheist or agnostic, while 3.5% claim to believe in God but do not follow a specific religion. 1.8% of Colombians adhere to Jehovah's Witnesses and Adventism and less than 1% adhere to other religions, such as Islam, Judaism, Buddhism, Mormonism, Hinduism, Indigenous religions, Hare Krishna movement, Rastafari movement, Orthodox Catholic Church, and spiritual studies. The remaining people either did not respond or replied that they did not know. In addition to the above statistics, 35.9% of Colombians reported that they did not practice their faith actively. 
While Colombia remains a mostly Roman Catholic country by baptism numbers, the 1991 Colombian constitution guarantees freedom and equality of religion.
Culture.
Colombia lies at the crossroads of Latin America and the broader American continent, and as such has been hit by a wide range of cultural influences. Native American, Spanish and other European, African, American, Caribbean, and Middle Eastern influences, as well as other Latin American cultural influences, are all present in Colombia's modern culture. Urban migration, industrialization, globalization, and other political, social and economic changes have also left an impression.
Many national symbols, both objects and themes, have arisen from Colombia's diverse cultural traditions and aim to represent what Colombia, and the Colombian people, have in common. Cultural expressions in Colombia are promoted by the government through the Ministry of Culture.
Literature.
Colombian literature dates back to pre-Columbian era; a notable example of the period is the epic poem known as the "Legend of Yurupary". In Spanish colonial times notable writers include Hernando Domínguez Camargo and his epic poem to San Ignacio de Loyola, Juan Rodríguez Freyle ("The Sheep") and the nun Francisca Josefa de Castillo, representative of mysticism.
Post-independence literature linked to Romanticism highlighted Antonio Nariño, José Fernández Madrid, Camilo Torres Tenorio and Francisco Antonio Zea. In the second half of the nineteenth century and early twentieth century the literary genre known as "costumbrismo" became popular; great writers of this period were Tomás Carrasquilla, Jorge Isaacs and Rafael Pombo (the latter of whom wrote notable works of children's literature). Within that period, authors such as José Asunción Silva, José Eustasio Rivera, León de Greiff, Porfirio Barba-Jacob and José María Vargas Vila developed the modernist movement. In 1872, Colombia established the Colombian Academy of Language, the first Spanish language academy in the Americas. Candelario Obeso wrote the groundbreaking "Cantos Populares de mi Tierra" (1877), the first book of poetry by an Afro-Colombian author.
Between 1939 and 1940 seven books of poetry were published under the name "Stone and Sky" in the city of Bogotá that significantly impacted the country; they were edited by the poet Jorge Rojas. In the following decade, Gonzalo Arango founded the movement of "nothingness" in response to the violence of the time; he was influenced by nihilism, existentialism, and the thought of another great Colombian writer: Fernando González Ochoa. During the boom in Latin American literature, successful writers emerged, led by Nobel laureate Gabriel García Márquez and his magnum opus, "One Hundred Years of Solitude", Eduardo Caballero Calderón, Manuel Mejía Vallejo, and Álvaro Mutis, a writer who was awarded the Cervantes Prize and the Prince of Asturias Award for Letters. Other leading contemporary authors are Fernando Vallejo (Rómulo Gallegos Prize) and Germán Castro Caycedo, the best-selling writer in Colombia after García Márquez.
Visual arts.
Colombian art has over 3,000 years of history. Colombian artists have captured the country's changing political and cultural backdrop using a range of styles and mediums. There is archeological evidence of ceramics being produced earlier in Colombia than anywhere else in the Americas, dating as early as 3,000 BCE.
The earliest examples of gold craftsmanship have been attributed to the Tumaco people of the Pacific coast and date to around 325 BCE. Roughly between 200 BCE and 800 CE, the San Agustín culture, masters of stonecutting, entered its “classical period". They erected raised ceremonial centres, sarcophagi, and large stone monoliths depicting anthropomorphic and zoomorphhic forms out of stone.
Colombian art has followed the trends of the time, so during the 16th to 18th centuries, Spanish Catholicism had a huge influence on Colombian art, and the popular baroque style was replaced with rococo when the Bourbons ascended to the Spanish crown. More recently, Colombian artists Pedro Nel Gómez and Santiago Martínez Delgado started the Colombian Murial Movement in the 1940s, featuring the neoclassical features of Art Deco.
Since the 1950s, the Colombian art started to have a distinctive point of view, reinventing traditional elements under the concepts of the 20th century. Examples of this are the Greiff portraits by Ignacio Gómez Jaramillo, showing what the Colombian art could do with the new techniques applied to typical Colombian themes. Carlos Correa, with his paradigmatic “Naturaleza muerta en silencio” (silent dead nature), combines geometrical abstraction and cubism. Alejandro Obregón is often considered as the father of modern Colombian painting, and one of the most influential artist in this period, due to his originality, the painting of Colombian landscapes with symbolic and expressionist use of animals, (specially the Andean condor). Fernando Botero and Omar Rayo are probably the most widely known Colombian artists in the international scene.
The Colombian sculpture from the sixteenth to 18th centuries was mostly devoted to religious depictions of ecclesiastic art, strongly influenced by the Spanish schools of sacred sculpture. During the early period of the Colombian republic, the national artists were focused in the production of sculptural portraits of politicians and public figures, in a plain neoclassicist trend. During the 20th century, the Colombian sculpture began to develop a bold and innovative work with the aim of reaching a better understanding of national sensitivity.
Photography in Colombia began with the arrival in the country of the Daguerreotype that was brought by the Baron Gros in 1841. The Piloto public library has Latin America’s largest archive of negatives, containing 1.7 million antique photographs covering Colombia 1848 until 2005.
Popular culture.
In general, Colombian music blends European-influenced guitar and song structure with large gaita flutes and percussion instruments from the indigenous population, while its percussion structure and dance forms come from Africa. Colombian music reflects a wealth of musical influences that have given birth to a dynamic musical environment. Some of the most popular music genres that have marked the Colombian music with special relevance are the cumbia, vallenato, joropo, salsa, bambuco, rock, pop and the classical music. Shakira and Juanes are two of the most well-known Colombian musicians internationally. Colombian music is promoted mainly by the support of the largest record labels, independent companies and the Government of Colombia, through the Ministry of Culture.
Colombian architecture is mainly derived of adapting European styles to local conditions, and Spanish influence, especially Andalusian, can be easily seen. The Teatro Colón in Bogotá is a lavish example of Colombian architecture from the Republican period, and the Archbishopric Cathedral also in the capital, was made in the neoclassic style in 1792, by Colombian architect Domingo de Petrés. Rogelio Salmona, whose works are noted for their use of red brick and natural shapes, is a widely renowned Colombian architect.
Theater was introduced in Colombia during the Spanish colonization in 1550 through zarzuela companies. Colombian theater is supported by the Ministry of Culture and a number of private and state owned organizations. The Ibero-American Theater Festival of Bogotá is the cultural event of the highest importance in Colombia and one of the biggest theater festivals in the world. Other important theater events are: The Festival of Puppet The Fanfare (Medellín), The Manizales Theater Festival, The Caribbean Theatre Festival (Santa Marta) and The Art Festival of Popular Culture "Cultural Invasion" (Bogotá).
Some important national circulation newspapers are "El Tiempo" and "El Espectador". Television in Colombia has two privately owned TV networks and three state-owned TV networks with national coverage, as well as six regional TV networks and dozens of local TV stations. Private channels, RCN and Caracol are the highest-rated. The regional channels and regional newspapers cover a department or more and its content is made in these particular areas.
Although the Colombian cinema is young as an industry, more recently the film industry was growing with support from the Film Act passed in 2003.
Cuisine.
Colombia's varied cuisine is influenced by its diverse fauna and flora as well as the cultural traditions of the ethnic groups. Colombian dishes and ingredients vary widely by region. Some of the most common ingredients are: cereals such as rice and maize; tubers such as potato and cassava; assorted legumes; meats, including beef, chicken, pork and goat; fish; and seafood. Colombia cuisine also features a variety of tropical fruits such as cape gooseberry, feijoa, arazá, dragon fruit, mangostino, granadilla, papaya, guava, mora (blackberry), lulo, soursop and passionfruit.
Among the most representative appetizers and soups are patacones (fried green plantains), sancocho de gallina (chicken soup with root vegetables) and ajiaco (potato and corn soup). Representative snacks and breads are pandebono, arepas (corn cakes), aborrajados (fried sweet plantains with cheese), torta de choclo, empanadas and almojábanas. Representative main courses are bandeja paisa, lechona tolimense, mamona, tamales and fish dishes (such as arroz de lisa), especially in coastal regions where suero, costeño cheese and carimañolas are also eaten. Representative side dishes are papas criollas al horno (roasted Andean potatoes), papas chorreadas (potatoes with cheese) and arroz con coco (coconut rice). Organic food is a current trend in big cities, although in general across the country the fruits and veggies are very natural and fresh.
Representative desserts are buñuelos, natillas, Maria Luisa cake, bocadillo made of guayaba (guava jelly), cocadas (coconut balls), casquitos de guayaba (candied guava peels), torta de natas, obleas, flan de arequipe, roscón, milhoja, and the tres leches cake (a sponge cake soaked in milk, covered in whipped cream, then served with condensed milk). Typical sauces (salsas) are hogao (tomato and onion sauce) and Colombian-style ají.
Some representative beverages are coffee (Tinto), champús, cholado, lulada, avena colombiana, sugarcane juice, aguapanela, aguardiente, hot chocolate and fresh fruit juices (often made with sugar and water or milk).
Sports.
Tejo is Colombia’s national sport and is a team sport that involves launching projectiles to hit a target. But of all sports in Colombia, football is the most popular. Colombia was the champion of the 2001 Copa América, in which they set a new record of being undefeated, conceding no goals and winning each match. Interestingly, Colombia has been awarded “mover of the year” twice.
Colombia is a mecca for roller skaters. The national team is a perennial powerhouse at the World Roller Speed Skating Championships. Colombia has traditionally been very good in cycling and a large number of Colombian cyclists have triumphed in major competitions of cycling.
In baseball, another sport rooted in the Caribbean Coast, Colombia was world amateur champion in 1947 and 1965.
Baseball is popular in the Caribbean, mainly in the cities Cartagena, Barranquilla and Santa Marta. Of those cities have come good players like: Orlando Cabrera, Edgar Rentería who was champion of the World Series in 1997 and 2010, and others who have played in Major League Baseball.
Boxing is one of the sports that more world champions has produced for Colombia.
Motorsports also occupies an important place in the sporting preferences of Colombians; Juan Pablo Montoya is a race car driver known for winning 7 Formula One events. Colombia also has excelled in sports such as BMX, taekwondo, shooting sport, wrestling, judo, bowling, athletics and has a long tradition in weightlifting.
Health.
Life expectancy at birth in 2000 was 74 years; the life expectancy increased to 78 years by 2012. Health standards in Colombia have improved very much since the 1980s, healthcare reforms have led to the massive improvements in the healthcare systems of the country. Although this new system has widened population coverage by the social and health security system from 21% (pre-1993) to 96% in 2012, health disparities persist, with the poor continuing to suffer less attention in their medical procedures.
Through health tourism, many people from over the world travel from their places of residence to other countries in search of medical treatment and the attractions in the countries visited. Colombia is projected as one of Latin America’s main destinations in terms of health tourism due to the quality of its health care professionals, a good number of institutions devoted to health, and an immense inventory of natural and architectural sites. Cities such as Bogotá, Cali, Medellín and Bucaramanga are the most visited in cardiology procedures, neurology, dental treatments, stem cell therapy, ENT, ophthalmology and joint replacements among others for the medical services of high quality.
A study conducted by "América Economía" magazine ranked 22 Colombian health care institutions among the top 43 in Latin America, amounting to 51 percent of the total.
Education.
The educational experience of many Colombian children begins with attendance at a preschool academy until age five ("Educación preescolar"). Basic education ("Educación básica") is compulsory by law. It has two stages: Primary basic education ("Educación básica primaria") which goes from first to fifth grade – children from six to ten years old, and Secondary basic education ("Educación básica secundaria"), which goes from sixth to ninth grade. Basic education is followed by Middle vocational education ("Educación media vocacional") that comprises the tenth and eleventh grades. It may have different vocational training modalities or specialties (academic, technical, business, and so on.) according to the curriculum adopted by each school.
After the successful completion of all the basic and middle education years, a high-school diploma is awarded. The high-school graduate is known as a "bachiller", because secondary basic school and middle education are traditionally considered together as a unit called "bachillerato" (sixth to eleventh grade). Students in their final year of middle education take the ICFES test (now renamed Saber 11) in order to gain access to higher education ("Educación superior"). This higher education includes undergraduate professional studies, technical, technological and intermediate professional education, and post-graduate studies. Technical professional institutions of Higher Education are also opened to students holder of a qualification in Arts and Business. This qualification is usually awarded by the SENA after a two years curriculum.
"Bachilleres" (high-school graduates) may enter into a professional undergraduate career program offered by a university; these programs last up to five years (or less for technical, technological and intermediate professional education, and post-graduate studies), even as much to six to seven years for some careers, such as medicine. In Colombia, there is not an institution such as college; students go directly into a career program at a university or any other educational institution to obtain a professional, technical or technological title. Once graduated from the university, people are granted a (professional, technical or technological) diploma and licensed (if required) to practice the career they have chosen. For some professional career programs, students are required to take the Saber-Pro test, in their final year of undergraduate academic education.
Public spending on education as a proportion of gross domestic product in 2013 was 4.9%. This represented 16.9% of total government expenditure. The primary and secondary gross enrolment ratios stood at 114.9% and 93% respectively. School-life expectancy was 13.5 years. A total of 93.6% of the population aged 15 and older were recorded as literate, including 98.2% of those aged 15–24.

</doc>
<doc id="5224" url="https://en.wikipedia.org/wiki?curid=5224" title="Citizen Kane">
Citizen Kane

Citizen Kane is a 1941 American drama film by Orson Welles, its producer, co-author, director and star. The picture was Welles's first feature film. Nominated for Academy Awards in nine categories, it won an Academy Award for Best Writing (Original Screenplay) by Herman J. Mankiewicz and Welles. Considered by many critics, filmmakers, and fans to be the greatest film of all time, "Citizen Kane" was voted as such in five consecutive "Sight & Sound" polls of critics, until it was displaced by "Vertigo" in the 2012 poll. It topped the American Film Institute's 100 Years ... 100 Movies list in 1998, as well as AFI's 2007 update. "Citizen Kane" is particularly praised for its cinematography, music, and narrative structure, which were innovative for its time.
The quasi-biographical film examines the life and legacy of Charles Foster Kane, played by Welles, a character based in part upon the American newspaper magnate William Randolph Hearst, Chicago tycoons Samuel Insull and Harold McCormick, and aspects of Welles's own life. Upon its release, Hearst prohibited mention of the film in any of his newspapers. Kane's career in the publishing world is born of idealistic social service, but gradually evolves into a ruthless pursuit of power. Narrated principally through flashbacks, the story is told through the research of a newsreel reporter seeking to solve the mystery of the newspaper magnate's dying word: "Rosebud".
After the Broadway successes of Welles's Mercury Theatre and the controversial 1938 radio broadcast "The War of the Worlds" on "The Mercury Theatre on the Air", Welles was courted by Hollywood. He signed a contract with RKO Pictures in 1939. Unusual for an untried director, he was given the freedom to develop his own story, to use his own cast and crew, and to have final cut privilege. Following two abortive attempts to get a project off the ground, he wrote the screenplay for "Citizen Kane", collaborating on the effort with Herman Mankiewicz. Principal photography took place in 1940 and the film received its American release in 1941.
While a critical success, "Citizen Kane" failed to recoup its costs at the box office. The film faded from view after its release but was subsequently returned to the public's attention when it was praised by such French critics as André Bazin and given an American revival in 1956. The film was released on Blu-ray Disc on September 13, 2011, for a special 70th anniversary edition.
Plot.
In a mansion in Xanadu, a vast palatial estate in Florida, the elderly Charles Foster Kane is on his deathbed. Holding a snow globe, he utters a word, "Rosebud", and dies; the globe slips from his hand and smashes on the floor. A newsreel obituary tells the life story of Kane, an enormously wealthy newspaper publisher. Kane's death becomes sensational news around the world, and the newsreel's producer tasks reporter Jerry Thompson with discovering the meaning of "Rosebud."
Thompson sets out to interview Kane's friends and associates. He approaches Kane's second wife, Susan Alexander Kane, now an alcoholic who runs her own nightclub, but she refuses to talk to him. Thompson goes to the private archive of the late banker Walter Parks Thatcher. Through Thatcher's written memoirs, Thompson learns that Kane's childhood began in poverty in Colorado.
In 1871, after a gold mine was discovered on her property, Kane's mother Mary Kane sends Charles away to live with Thatcher so that he would be properly educated. The young Kane plays happily with a sled in the snow at his parents' boarding-house and protests being sent to live with Thatcher. After gaining full control over his trust fund at the age of 25, Kane enters the newspaper business and embarks on a career of yellow journalism. He takes control of the "New York Inquirer" and begins publishing scandalous articles that attack Thatcher's business interests. After the stock market crash in 1929, Kane is forced to sell controlling interest of his newspaper empire to Thatcher.
In the present, Thompson interviews Kane's personal business manager, Mr. Bernstein. Bernstein recalls how Kane hired the best journalists available to build the "Inquirer"s circulation. Kane rose to power by successfully manipulating public opinion regarding the Spanish–American War and marrying Emily Norton, the niece of a President of the United States.
Thompson interviews Kane's estranged best friend, Jedediah Leland, in a retirement home. Leland recalls Kane's marriage to Emily disintegrates over the years, and he begins an affair with amateur singer Susan Alexander while he is running for Governor of New York. Both his wife and his political opponent discover the affair and the public scandal ends his political career. Kane marries Susan and forces her into a humiliating operatic career for which she has neither the talent nor the ambition. Susan consents to an interview with Thompson, and recalls her failed opera career. Kane finally allows her to abandon her singing career after she attempts suicide. After years spent dominated by Kane and living in isolation at Xanadu, Susan leaves Kane. Kane's butler Raymond recounts that after Susan left him Kane began violently destroying the contents of her bedroom. He suddenly calms down when he sees a snow globe and says, "Rosebud".
Back at Xanadu, Kane's belongings are being cataloged or discarded. Thompson concludes that he is unable to solve the mystery and that the meaning of Kane's last word will forever remain an enigma. As the film ends, the camera reveals that "Rosebud" is the trade name of the sled on which the eight-year-old Kane was playing on the day that he was taken from his home in Colorado. Thought to be junk by Xanadu's staff, the sled is burned in a furnace.
Pre-production.
Development.
Hollywood had shown interest in Welles as early as 1936. He turned down three scripts sent to him by Warner Bros. In 1937, he declined offers from David O. Selznick, who asked him to head his film company's story department, and William Wyler, who wanted him for a supporting role in "Wuthering Heights". "Although the possibility of making huge amounts of money in Hollywood greatly attracted him," wrote biographer Frank Brady, "he was still totally, hopelessly, insanely in love with the theater, and it is there that he had every intention of remaining to make his mark."
Following "The War of the Worlds" broadcast of his CBS radio series "The Mercury Theatre on the Air", Welles was lured to Hollywood with a remarkable contract. RKO Pictures studio head George J. Schaefer wanted to work with Welles after the notorious broadcast, believing that Welles had a gift for attracting mass attention. RKO was also uncharacteristically profitable and was entering into a series of independent production contracts that would add more artistically prestigious films to its roster. Throughout the spring and early summer of 1939, Schaefer constantly tried to lure the reluctant Welles to Hollywood. Welles was in financial trouble after failure of his plays "Five Kings" and "The Green Goddess". At first he simply wanted to spend three months in Hollywood and earn enough money to pay his debts and fund his next theatrical season. Welles first arrived on July 20, 1939 and on his first tour, he called the movie studio "the greatest electric train set a boy ever had".
Welles signed his contract with RKO on August 21. This legendary contract stipulated that Welles would act in, direct, produce and write two films. Mercury would get $100,000 for the first film by January 1, 1940, plus 20% of profits after RKO recouped $500,000, and $125,000 for a second film by January 1, 1941, plus 20% of profits after RKO recouped $500,000. The most controversial aspect of the contract was granting Welles complete artistic control of the two films so long as RKO approved both project's stories and so long as the budget did not exceed $500,000. RKO executives would not be allowed to see any footage until Welles chose to show it to them, and no cuts could be made to either film without Welles’s approval. Welles was allowed to develop the story without interference, select his own cast and crew, and have the right of final cut. Granting final cut privilege was unprecedented for a studio since it placed artistic considerations over financial investment. The contract was deeply resented in the film industry, and the Hollywood press took every opportunity to mock RKO and Welles. Schaefer remained a great supporter and saw the unprecedented contract as good publicity. Film scholar Robert L. Carringer wrote: "The simple fact seems to be that Schaefer believed Welles was going to pull off something really big almost as much as Welles did himself."
Welles spent the first five months of his RKO contract trying to get his first project going, without success. "They are laying bets over on the RKO lot that the Orson Welles deal will end up without Orson ever doing a picture there," wrote "The Hollywood Reporter". It was agreed that Welles would film "Heart of Darkness", previously adapted for "The Mercury Theatre on the Air", which would be presented entirely through a first-person camera. After elaborate pre-production and a day of test shooting with a hand-held camera—unheard of at the time—the project never reached production because Welles was unable to trim $50,000 from its budget. Schaefer told Welles that the $500,000 budget could not be exceeded; revenue was declining sharply in Europe by the fall of 1939.
He then started work on the idea that became "Citizen Kane". Knowing the script would take time to prepare, Welles suggested to RKO that while that was being done—"so the year wouldn't be lost"—he make a humorous political thriller. Welles proposed "The Smiler with a Knife", from a novel by Cecil Day-Lewis. When that project stalled in December 1939, Welles began brainstorming other story ideas with screenwriter Herman J. Mankiewicz, who had been writing Mercury radio scripts. "Arguing, inventing, discarding, these two powerful, headstrong, dazzlingly articulate personalities thrashed toward "Kane"", wrote biographer Richard Meryman.
Screenplay.
One of the long-standing controversies about "Citizen Kane" has been the authorship of the screenplay. Welles conceived the project with screenwriter Herman J. Mankiewicz, who was writing radio plays for Welles's CBS Radio series, "The Campbell Playhouse". Mankiewicz based the original outline on the life of William Randolph Hearst, whom he knew socially and came to hate after he was exiled from Hearst's circle.
In February 1940 Welles supplied Mankiewicz with 300 pages of notes and put him under contract to write the first draft screenplay under the supervision of John Houseman, Welles's former partner in the Mercury Theatre. Welles later explained, "I left him on his own finally, because we'd started to waste too much time haggling. So, after mutual agreements on storyline and character, Mank went off with Houseman and did his version, while I stayed in Hollywood and wrote mine." Taking these drafts, Welles drastically condensed and rearranged them, then added scenes of his own. The industry accused Welles of underplaying Mankiewicz's contribution to the script, but Welles countered the attacks by saying, "At the end, naturally, I was the one making the picture, after all—who had to make the decisions. I used what I wanted of Mank's and, rightly or wrongly, kept what I liked of my own."
The terms of the contract stated that Mankiewicz was to receive no credit for his work, as he was hired as a script doctor. Before he signed the contract Mankiewicz was particularly advised by his agents that all credit for his work belonged to Welles and the Mercury Theatre, the "author and creator". As the film neared release, however, Mankiewicz began threatening Welles to get credit for the film—including threats to place full-page ads in trade papers and to get his friend Ben Hecht to write an exposé for "The Saturday Evening Post". Mankiewicz also threatened to go to the Screen Writers Guild and claim full credit for writing the entire script by himself.
After lodging a protest with the Screen Writers Guild, Mankiewicz withdrew it, then vacillated. The question was resolved in January 1941 when the studio, RKO Pictures, awarded Mankiewicz credit. The guild credit form listed Welles first, Mankiewicz second. Welles's assistant Richard Wilson said that the person who circled Mankiewicz's name in pencil, then drew an arrow that put it in first place, was Welles. The official credit reads, "Screenplay by Herman J. Mankiewicz and Orson Welles". Mankiewicz's rancor toward Welles grew over the remaining 12 years of his life.
Questions over the authorship of the "Citizen Kane" screenplay were revived in 1971 by influential film critic Pauline Kael, whose controversial 50,000-word essay "Raising Kane" was commissioned as an introduction to the shooting script in "The Citizen Kane Book", published in October 1971. The book-length essay first appeared in February 1971, in two consecutive issues of "The New Yorker" magazine.
In the ensuing controversy Welles was defended by colleagues, critics, biographers and scholars, but his reputation was damaged by its charges. The essay was later discredited and Kael's own scholarship was called into question.
Any question of authorship was resolved with Carringer's 1978 essay, "The Scripts of Citizen Kane". Carringer studied the collection of script records—"almost a day-to-day record of the history of the scripting"—that was then still intact at RKO. He reviewed all seven drafts and concluded that "the full evidence reveals that Welles's contribution to the "Citizen Kane" script was not only substantial but definitive."
Sources.
Welles never confirmed a principal source for the character of Charles Foster Kane. Houseman wrote that Kane is a synthesis of different personalities, with Hearst's life used as the main source. Some events and details were invented, and Houseman and Mankiewicz also "grafted anecdotes from other giants of journalism, including Pulitzer, Northcliffe and Mank's first boss, Herbert Bayard Swope." Welles said, "Mr. Hearst was quite a bit like Kane, although Kane isn't really founded on Hearst in particular, many people sat for it so to speak". He specifically acknowledged that aspects of Kane were drawn from the lives of two business tycoons familiar from his youth in Chicago—Samuel Insull and Harold Fowler McCormick.
The character of Jedediah Leland was based on drama critic Ashton Stevens, George Stevens's uncle and Welles's close boyhood friend. Some detail came from Mankiewicz's own experience as a drama critic in New York.
The assumption that the character of Susan Alexander Kane was based on Marion Davies was a major reason Hearst tried to destroy "Citizen Kane". Welles denied that the character was based on Davies, whom he called "an extraordinary woman—nothing like the character Dorothy Comingore played in the movie." He cited Insull's building of the Chicago Opera House, and McCormick's lavish promotion of the opera career of his second wife, Ganna Walska, as direct influences on the screenplay.
The character of political boss Jim W. Gettys is based on Charles F. Murphy, a leader in New York City's infamous Tammany Hall political machine.
Welles credited "Rosebud" to Mankiewicz. Biographer Richard Meryman wrote that the symbol of Mankiewicz's own damaged childhood was a treasured bicycle, stolen while he visited the public library and not replaced by his family as punishment. He regarded it as the prototype of Charles Foster Kane's sled. In his 2015 Welles biography, Patrick McGilligan reported that Mankiewicz himself stated that the word "Rosebud" was taken from the name of a famous racehorse, Old Rosebud. Mankiewicz had a bet on the horse in the 1914 Kentucky Derby, which he won, and McGilligan wrote that "Old Rosebud symbolized his lost youth, and the break with his family". In testimony for the Lundberg suit, Mankiewicz said, "I had undergone psycho-analysis, and Rosebud, under circumstances slightly resembling the circumstances in ["Citizen Kane"], played a prominent part." Other modern assertions that the term was a nickname Hearst used for Davies' clitoris were rejected by Houseman and dismissed by Brady.
The "News on the March" sequence that begins the film satirizes the journalistic style of "The March of Time", the news documentary and dramatization series presented in movie theaters by Time Inc. From 1935 to 1938 Welles was a member of the uncredited company of actors that presented the original radio version.
Houseman claimed that banker Walter P. Thatcher was loosely based on J. P. Morgan. Bernstein was named for Dr. Maurice Bernstein, appointed Welles's guardian; Sloane's portrayal was said to be based on Bernard Herrmann. Herbert Carter, editor of "The Inquirer", was named for actor Jack Carter.
Production.
Casting.
"Citizen Kane" was a rare film in that its principal roles were played by actors new to motion pictures. Ten were billed as Mercury Actors, members of the skilled repertory company assembled by Welles for the stage and radio performances of the Mercury Theatre, an independent theater company he founded with Houseman in 1937. "He loved to use the Mercury players," wrote biographer Charles Higham, "and consequently he launched several of them on movie careers."
The film represents the feature film debuts of William Alland, Ray Collins, Joseph Cotten, Agnes Moorehead, Erskine Sanford, Everett Sloane, Paul Stewart, and Welles himself. Despite never having appeared in feature films, some of the cast members were already well known to the public. Cotten had recently become a Broadway star in the hit play "The Philadelphia Story" with Katharine Hepburn and Sloane was well known for his role on the radio show "The Goldbergs". Mercury actor George Coulouris was a star of the stage in New York and London.
Not all of the cast came from the Mercury Players. Welles cast Dorothy Comingore as Susan Alexander Kane. Comingore had never appeared in a film and was a discovery of Charlie Chaplin. Chaplin recommended Comingore to Welles, who then met Comingore at a party in Los Angeles and immediately cast her.
Welles had met stage actress Ruth Warrick while visiting New York on a break from Hollywood and remembered her as a good fit for Emily Norton Kane, later saying that she looked the part. Warrick told Carringer that she was struck by the extraordinary resemblance between herself and Welles's mother when she saw a photograph of Beatrice Ives Welles. She characterized her own personal relationship with Welles as motherly.
"He trained us for films at the same time that he was training himself," recalled Agnes Moorehead. "Orson believed in good acting, and he realized that rehearsals were needed to get the most from his actors. That was something new in Hollywood: nobody seemed interested in bringing in a group to rehearse before scenes were shot. But Orson knew it was necessary, and we rehearsed every sequence before it was shot."
When "The March of Time" narrator Westbrook Van Voorhis asked for $25,000 to narrate the "News on the March" sequence, Alland demonstrated his ability to imitate Van Voorhis and Welles cast him.
Welles later said that casting character actor Gino Corrado in the small part of the waiter at the El Rancho broke his heart. Corrado had appeared in many Hollywood films, often as a waiter, and Welles wanted all of the actors to be new to films.
Other uncredited roles went to Thomas A. Curran as Teddy Roosevelt in the faux newsreel; Richard Baer as Hillman, a man at Madison Square Garden, and a man in the "News on the March" screening room; and Alan Ladd, Arthur O'Connell and Louise Currie as reporters at Xanadu. When Kathryn Trosper Popper died on March 6, 2016, at the age of 100 she was believed to have been the film's last surviving cast member. Warrick was the last surviving member of the principal cast at the time of her death in 2005. Sonny Bupp, who played Kane's young son, was the last surviving credited cast member of "Citizen Kane" when he died in 2007.
Filming.
Production advisor Miriam Geiger quickly compiled a handmade film textbook for Welles, a practical reference book of film techniques that he studied carefully. He then taught himself filmmaking by matching its visual vocabulary to "The Cabinet of Dr. Caligari", which he ordered from the Museum of Modern Art, and films by Frank Capra, René Clair, Fritz Lang, King Vidor and Jean Renoir. The one film he genuinely studied was John Ford's "Stagecoach", which he watched 40 times. "As it turned out, the first day I ever walked onto a set was my first day as a director," Welles said. "I'd learned whatever I knew in the projection room—from Ford. After dinner every night for about a month, I'd run "Stagecoach", often with some different technician or department head from the studio, and ask questions. 'How was this done?' 'Why was this done?' It was like going to school."
Welles's cinematographer for the film was Gregg Toland, described by Welles as "just then, the number-one cameraman in the world." To Welles's astonishment, Toland visited him at his office and said, "I want you to use me on your picture." He had seen some of the Mercury stage productions (including "Caesar") and said he wanted to work with someone who had never made a movie. RKO hired Toland on loan from Samuel Goldwyn Productions in the first week of June 1940.
"And he never tried to impress us that he was doing any miracles," Welles recalled. "I was calling for things only a beginner would have been ignorant enough to think anybody could ever do, and there he was, "doing" them." Toland later explained that he wanted to work with Welles because he anticipated the first time director's inexperience and reputation for audacious experimentation in the theater would allow the cinematographer to try new and innovative camera techniques that typical Hollywood films would never have allowed him to do. Unaware of filmmaking protocol, Welles adjusted the lights on set as he was accustomed to doing in the theater; Toland quietly re-balanced them, and was angry when one of the crew informed Welles that he was infringing on Toland's responsibilities. During the first few weeks of June, Welles had lengthy discussions about the film with Toland and art director Perry Ferguson in the morning, and in the afternoon and evening he worked with actors and revised the script.
On June 29, 1940—a Saturday morning when few inquisitive studio executives would be around—Welles began filming "Citizen Kane". After the disappointment of having "Heart of Darkness" cancelled, Welles followed Ferguson's suggestion and deceived RKO into believing that he was simply shooting camera tests. "But we were shooting the "picture"," Welles said, "because we wanted to get started and be already into it before anybody knew about it."
At the time RKO executives were pressuring him to agree to direct a film called "The Men from Mars", to capitalize on "The War of the Worlds" radio broadcast. Welles said that he would consider making the project but wanted to make a different film first. At this time he did not inform them that he had already begun filming "Citizen Kane."
The early footage was called "Orson Welles Tests" on all paperwork. The first "test" shot was the "News on the March" projection room scene, economically filmed in a real studio projection room in darkness that masked many actors who appeared in other roles later in the film. "At $809 Orson did run substantially beyond the test budget of $528—to create one of the most famous scenes in movie history," wrote Barton Whaley.
The next scenes were the El Rancho nightclub scenes and the scene in which Susan attempts suicide. Welles later said that the nightclub set was available after another film had wrapped and that filming took 10 to 12 days to complete. For these scenes Welles had Comingore's throat sprayed with chemicals to give her voice a harsh, raspy tone. Other scenes shot in secret included those in which Thompson interviews Leland and Bernstein, which were also shot on sets built for other films.
During production, the film was referred to as "RKO 281". Most of the filming took place in what is now Stage 19 on the Paramount Pictures lot in Hollywood. There was some location filming at Balboa Park in San Diego and the San Diego Zoo.
In the end of July, RKO approved the film and Welles was allowed to officially begin shooting, despite having already been filming "tests" for several weeks. Welles leaked stories to newspaper reporters that the tests had been so good that there was no need to re-shoot them. The first official scene to be shot was the breakfast montage sequence between Kane and his first wife Emily. To strategically save money and appease the RKO executives who opposed him, Welles rehearsed scenes extensively before actually shooting and filmed very few takes of each shot set-up. Welles never shot master shots for any scene after Toland told him that Ford never shot them. To appease the increasingly curious press, Welles threw a cocktail party for selected reporters, promising that they could watch a scene being filmed. When the journalists arrived Welles told them they had “just finished” shooting for the day but still had the party. Welles told the press that he was ahead of schedule (without factoring in the month of "test shooting"), thus discrediting claims that after a year in Hollywood without making a film he was a failure in the film industry.
Welles usually worked 16 to 18 hours a day on the film. He often began work at 4 a.m. since the special effects make-up used to age him for certain scenes took up to four hours to apply. Welles used this time to discuss the day's shooting with Toland and other crew members. The special contact lenses used to make Welles look elderly proved very painful, and a doctor was employed to place them into Welles's eyes. Welles had difficulty seeing clearly while wearing them, which caused him to badly cut his wrist when shooting the scene in which Kane breaks up the furniture in Susan's bedroom. While shooting the scene in which Kane shouts at Gettys on the stairs of Susan Alexander's apartment building, Welles fell ten feet; an X-ray revealed two bone chips in his ankle. The injury required him to direct the film from a wheelchair for two weeks. He eventually wore a steel brace to resume performing on camera; it is visible in the low-angle scene between Kane and Leland after Kane loses the election. For the final scene, a stage at the Selznick studio was equipped with a working furnace, and multiple takes were required to show the sled being put into the fire and the word "Rosebud" consumed. Paul Stewart recalled that on the ninth take the Culver City Fire Department arrived in full gear because the furnace had grown so hot the flue caught fire. "Orson was delighted with the commotion", he said.
When "Rosebud" was burned, Welles choreographed the scene while he had composer Bernard Herrmann's cue playing on the set.
Unlike Schaefer, many members of RKO's board of governors did not like Welles or the control that his contract gave him. However such board members as Nelson Rockefeller and NBC chief David Sarnoff were sympathetic to Welles. Throughout production Welles had problems with these executives not respecting his contract’s stipulation of non-interference and several spies arrived on set to report what they saw to the executives. When the executives would sometimes arrive on set unannounced the entire cast and crew would suddenly start playing softball until they left. Before official shooting began the executives intercepted all copies of the script and delayed their delivery to Welles. They had one copy sent to their office in New York, resulting in it being leaked to press.
Principal shooting wrapped October 24. Welles then took several weeks off of the film for a lecture tour, during which he also scouted additional locations with Toland and Ferguson. Filming resumed November 15 with some re-shoots. Toland had to leave due to a commitment to shoot Howard Hughes' "The Outlaw", but Toland's camera crew continued working on the film and Toland was replaced by RKO cinematographer Harry Wild. The final day of shooting on November 30 was Kane's death scene. Welles boasted that he only went 21 days over his official shooting schedule, without factoring in the month of "camera tests." According to RKO records, the film cost $839,727. Its estimated budget had been $723,800.
Post-production.
"Citizen Kane" was edited by Robert Wise and assistant editor Mark Robson. Both would become successful film directors. Wise was hired after Welles finished shooting the "camera tests" and began officially making the film. Wise said that Welles "had an older editor assigned to him for those tests and evidently he was not too happy and asked to have somebody else. I was roughly Orson’s age and had several good credits.” Wise and Robson began editing the film while it was still shooting and said that they “could tell certainly that we were getting something very special. It was outstanding film day in and day out.” Welles gave Wise detailed instructions and was usually not present during the film's editing. The film was very well planned out and intentionally shot for such post-production techniques as slow dissolves. The lack of coverage made editing easy since Welles and Toland edited the film "in camera" by leaving few options of how it could be put together. Wise said the breakfast table sequence took weeks to edit and get the correct "timing" and "rhythm" for the whip pans and over-lapping dialogue. The "News on the March" sequence was edited by RKO's newsreel division to give it authenticity. They used stock footage from Pathé News and the General Film Library.
During post-production Welles and special effects artist Linwood G. Dunn experimented with an optical printer to improve certain scenes that Welles found unsatisfactory from the footage. Whereas Welles was often immediately pleased with Wise's work, he would require Dunn and post-production audio engineer James G. Stewart to re-do their work several times until he was satisfied.
Welles hired Bernard Herrmann to compose the film's score. Where most Hollywood film scores were written quickly, in as few as two or three weeks after filming was completed, Herrmann was given 12 weeks to write the music. He had sufficient time to do his own orchestrations and conducting, and worked on the film reel by reel as it was shot and cut. He wrote complete musical pieces for some of the montages, and Welles edited many of the scenes to match their length.
Style.
Film scholars and historians view "Citizen Kane" as Welles's attempt to create a new style of filmmaking by studying various forms of film making, and combining them all into one. However, Welles stated that his love for cinema began only when he started the work on the film. When asked where he got the confidence as a first-time director to direct a film so radically different from contemporary cinema, he responded, "Ignorance, ignorance, sheer ignorance—you know there's no confidence to equal it. It's only when you know something about a profession, I think, that you're timid or careful."
David Bordwell wrote that "The best way to understand "Citizen Kane" is to stop worshiping it as a triumph of technique." Bordwell argues that the film did not invent any of its famous techniques such as deep focus cinematography, shots of the ceilings, chiaroscuro lighting and temporal jump-cuts, and many of these stylistics had been used in German Expressionist films of the 1920s, such as "The Cabinet of Dr. Caligari". But Bordwell asserts that the film did put them all together for the first time and perfected the medium in one single film. In a 1948 interview D. W. Griffith said "I loved "Citizen Kane" and particularly loved the ideas he took from me."
Arguments against the film's cinematic innovations were made as early as 1946 when French historian Georges Sadoul wrote that "the film is an encyclopedia of old techniques." Sadoul pointed out such examples as compositions that used both the foreground and the background in the films of Auguste and Louis Lumière, special effects used in the films of Georges Méliès, shots of the ceiling in Erich von Stroheim's "Greed" and newsreel montages in the films of Dziga Vertov.
French film critic André Bazin defended the film and wrote that "In this respect, the accusation of plagiarism could very well be extended to the film's use of panchromatic film or its exploitation of the properties of gelatinous silver halide." Bazin disagreed with Sadoul's comparison to Lumière's cinematography since "Citizen Kane" used more sophisticated lenses, but acknowledged that the film had similarities to such previous works as "The 49th Parallel" and "The Power and the Glory". Bazin stated that "even if Welles did not invent the cinematic devices employed in "Citizen Kane", one should nevertheless credit him with the invention of their "meaning"." Bazin championed the techniques in the film for its depiction of heightened reality, but Bordwell believes that the film's use of special effects contradict some of Bazin's theories.
Storytelling techniques.
"Citizen Kane" eschews the traditional linear, chronological narrative, and tells Kane's story entirely in flashback using different points of view, many of them from Kane's aged and forgetful associates, the cinematic equivalent of the unreliable narrator in literature. Welles also dispenses with the idea of a single storyteller and uses multiple narrators to recount Kane's life. The use of multiple narrators was unheard of in Hollywood films. Each narrator recounts a different part of Kane's life, with each story partly overlapping. The film depicts Kane as an enigma, a complicated man who, in the end, leaves viewers with more questions than answers as to his character, such as the newsreel footage where he is attacked for being both a communist and a fascist.
The technique of using flashbacks had been used in earlier films—most notably in "The Power and the Glory" (1933)—but no film was as immersed in this technique as "Citizen Kane". The use of the reporter Thompson acts as a surrogate for the audience, questioning Kane's associates and piecing together his life.
At that time films typically had an "omniscient perspective", which Marilyn Fabe says give the audience the "illusion that we are looking with impunity into a world which is unaware of our gaze, Hollywood movies give us a feeling of power." The film begins in this fashion up until the "News on the March" sequence, after which we the audience see the film through the perspectives of others. The "News on the March" sequence gives an overview of Kane's entire life (and the film's entire story) at the beginning of the film, leaving the audience without the typical suspense of wondering how it will end. Instead the film's repetitions of events compels the audience to analyze and wonder why Kane's life happened the way that it did, under the pretext of finding out what "Rosebud" means. The film then returns to the omniscient perspective in the final scene, when only the audience discovers what "Rosebud" is.
Cinematography.
The most innovative technical aspect of "Citizen Kane" is the extended use of deep focus. In nearly every scene in the film, the foreground, background and everything in between are all in sharp focus. Cinematographer Toland did this through his experimentation with lenses and lighting. Toland described the achievement, made possible by the sensitivity of modern speed film, in an article for "Theatre Arts" magazine:
New developments in the science of motion picture photography are not abundant at this advanced stage of the game but periodically one is perfected to make this a greater art. Of these I am in an excellent position to discuss what is termed “Pan-focus”, as I have been active for two years in its development and used it for the first time in "Citizen Kane". Through its use, it is possible to photograph action from a range of eighteen inches from the camera lens to over two hundred feet away, with extreme foreground and background figures and action both recorded in sharp relief. Hitherto, the camera had to be focused either for a close or a distant shot, all efforts to encompass both at the same time resulting in one or the other being out of focus. This handicap necessitated the breaking up of a scene into long and short angles, with much consequent loss of realism. With pan-focus, the camera, like the human eye, sees an entire panorama at once, with everything clear and lifelike.
Both this article and a May 1941 "Life" magazine article with illustrated examples helped popularize deep focus cinematography and Toland's achievements on the film.
Another unorthodox method used in the film was the way low-angle shots were used to display a point of view facing upwards, thus allowing ceilings to be shown in the background of several scenes. Breaking with studio convention, every set was built with a ceiling—many constructed of fabric that ingeniously concealed microphones. Welles felt that the camera should show what the eyes see, and that it was a bad theatrical convention to pretend there was no ceiling—"a big lie in order to get all those terrible lights up there," he said. He became fascinated with the look of low angles, which made even dull interiors look interesting. One extremely low angle is used to photograph the encounter between Kane and Leland after Kane loses the election. A hole was dug for the camera, which required drilling into the concrete floor.
Welles credited Toland on the same card as himself and said "It's impossible to say how much I owe to Gregg. He was superb." He called Toland "the best director of photography that ever existed."
Sound.
"Citizen Kane"s sound was recorded by Bailey Fesler and re-recorded in post-production by audio engineer James G. Stewart, both of whom had worked in radio. Stewart said that Hollywood films never deviated from a basic pattern of how sound could be recorded or used, but with Welles "deviation from the pattern was possible because he demanded it." Although the film is known for its complex soundtrack, much of the audio is heard as it was recorded by Fesler and without manipulation.
Welles used techniques from radio like overlapping dialogue. The scene in which characters sing "Oh, Mr. Kane" was especially complicated and required mixing several soundtracks together. He also used different "sound perspectives" to create the illusion of distances, such as in scenes at Xanadu where characters speak to each other at far distances. Welles experimented with sound in post-production, creating audio montages, and chose to create all of the sound effects for the film instead of using RKO's library of sound effects.
Welles used an aural technique from radio called the "lightning-mix". Welles used this technique to link complex montage sequences via a series of related sounds or phrases. For example, Kane grows from a child into a young man in just two shots. As Thatcher hands eight-year-old Kane a sled and wishes him a Merry Christmas, the sequence suddenly jumps to a shot of Thatcher fifteen years later, completing the sentence he began in both the previous shot and the chronological past. Other radio techniques include using a number of voices, each saying a sentence or sometimes merely a fragment of a sentence, and splicing the dialogue together in quick succession, such as the projection room scene. The film's sound cost $16,996, but was originally budgeted at $7,288.
Film critic and director François Truffaut wrote that "Before "Kane", nobody in Hollywood knew how to set music properly in movies. "Kane" was the first, in fact the only, great film that uses radio techniques. … A lot of filmmakers know enough to follow Auguste Renoir's advice to fill the eyes with images at all costs, but only Orson Welles understood that the sound track had to be filled in the same way." Cedric Belfrage of "The Clipper" wrote "of all of the delectable flavours that linger on the palate after seeing "Kane", the use of sound is the strongest."
Make-up.
The make-up for "Citizen Kane" was created and applied by Maurice Seiderman (1907–1989), a junior member of the RKO make-up department. Seiderman's family came to the United States from Russia in 1920, escaping persecution. As a child Seiderman had won a drawing competition and received an apprenticeship at the Moscow Art Theatre, where his father was a wigmaker and make-up artist. In New York his uncle was a theatrical scenic painter, and he helped Seiderman get into the union. He worked on Max Reinhardt's 1924 production of "The Miracle" and with the Yiddish Art Theatre, and he studied the human figure at the Art Students League of New York. After he moved to Los Angeles he was hired first by Max Factor and then by RKO. Seiderman had not been accepted into the union, which recognized him as only an apprentice, but RKO nevertheless used him to make up principal actors.
"Apprentices were not supposed to make up any principals, only extras, and an apprentice could not be on a set without a journeyman present," wrote make-up artist Dick Smith, who became friends with Seiderman in 1979. "During his years at RKO I suspect these rules were probably overlooked often." By 1940 Seiderman's uncredited film work included "Winterset", "Gunga Din", "The Hunchback of Notre Dame", "Swiss Family Robinson" and "Abe Lincoln in Illinois". "Seiderman had gained a reputation as one of the most inventive and creatively precise up-and-coming makeup men in Hollywood," wrote biographer Frank Brady.
On an early tour of RKO, Welles met Seiderman in the small make-up lab he created for himself in an unused dressing room. "Welles fastened on to him at once," wrote biographer Charles Higham. "With his great knowledge of makeup—indeed, his obsession with it, for he hated his flat nose—Welles was fascinated … Seiderman had an intimate knowledge of anatomy and the process of aging and was acquainted with every line, wrinkle and accretion of fat in aging men and women. Impatient with most makeup methods of his era, he used casts of his subjects in order to develop makeup methods that ensured complete naturalness of expression—a naturalness unrivaled in Hollywood."
"When "Kane" came out in script form, Orson told all of us about the picture and said that the most important aspect was the makeup," Seiderman recalled. "I felt that I was being given an assignment that was unique—so I worked accordingly. And there was a lot of work to do. Straight makeups were done in the makeup department by staff, but all the trick stuff and the principal characters were my personal work; nobody else ever touched them. They could not have handled it."
Seiderman developed a thorough plan for aging the principal characters, first making a plaster cast of the face of each of the actors who aged, except Joseph Cotten who was unavailable at that time. He made a plaster mold of Welles's body down to the hips.
"My sculptural techniques for the characters' aging were handled by adding pieces of white modeling clay, which matched the plaster, onto the surface of each bust," Seiderman told visual arts historian Norman Gambill. When Seiderman achieved the desired effect he cast the clay pieces in a soft plastic material that he formulated himself. These appliances were then placed onto the plaster bust and a four-piece mold was made for each phase of aging. The castings were then fully painted and paired with the appropriate wig for evaluation.
Before the actors went before the cameras each day, the pliable pieces were applied directly to their faces to recreate Seiderman's sculptural image. Welles was allergic to Max Factor's gum, so Seiderman invented an alternative that also photographed more realistically. The facial surface was underpainted in a flexible red plastic compound; Cotten recalled being instructed to puff out his cheeks during this process. Later, seeing the results in the mirror, Cotten told Seiderman, "I am acting the part of a nice old gentleman, not a relief map of the Rocky Mountains." Seiderman replied, "You'd be surprised at what the camera doesn't see unless we place it within its view. How about some more coffee?"
The red ground resulted in a warmth of tone that was picked up by the sensitive panchromatic film. Over that was applied liquid greasepaint, and then finally a colorless translucent talcum. Seiderman created the effect of skin pores on Kane's face by stippling the surface with a negative cast he made from an orange peel.
Welles was just as heavily made up as young Kane as he was for old Kane, and he often arrived on the set at 2:30 a.m. Application of the sculptural make-up for the oldest incarnation of the character took three-and-a-half hours. The make-up included appliances to age Welles's shoulders, breast and stomach. "In the film and production photographs, you can see that Kane had a belly that overhung," Seiderman said. "That was not a costume, it was the rubber sculpture that created the image. You could see how Kane's silk shirt clung wetly to the character's body. It could not have been done any other way."
Seiderman worked with Charles Wright on the wigs. These went over a flexible skull cover that Seiderman created and sewed into place with elastic thread. When he found the wigs too full he untied one hair at a time to alter their shape. Kane's mustache was inserted into the makeup surface a few hairs at a time, to realistically vary the color and texture.
Seiderman made scleral lenses for Welles, Dorothy Comingore, George Coulouris and Everett Sloane, to dull the brightness of their young eyes. The lenses took a long time to fit properly, and Seiderman began work on them before devising any of the other makeup. "I painted them to age in phases, ending with the blood vessels and the "Aurora Senilis" of old age."
"Cotten was the only principal for whom I had not made any sculptural casts, wigs or lenses," Seiderman said. When Cotten's old-age scenes needed to be shot out of sequence due to Welles's injured ankle, Seiderman improvised with appliances made for Kane's make-up. A sun visor was chosen to conceal Cotten's low hairline and the lenses he wore—hastily supplied by a Beverly Hills ophthalmologist—were uncomfortable.
Seiderman's tour de force, the breakfast montage, was shot all in one day. "Twelve years, two years shot at each scene," he said. "Please realize, by the way, that a two-year jump in age is a bit harder to accomplish visually than one of 20 years."
As they did with art direction, the major studios gave screen credit for make-up to only the department head. When RKO make-up department head Mel Berns refused to share credit with Seiderman, who was only an apprentice, Welles told Berns that there would be no make-up credit. Welles signed a large advertisement in the Los Angeles newspaper:
THANKS TO EVERYBODY WHO GETS SCREEN CREDIT FOR "CITIZEN KANE"
"To put this event in context, remember that I was a very low man," Seiderman recalled. "I wasn't even called a make-up man. I had started their laboratory and developed their plastic appliances for make-up. But my salary was $25 a week. And I had no union card."
Seiderman told Gambill that after "Citizen Kane" was released, Welles was invited to a White House dinner where Frances Perkins was among the guests. Welles told her about the Russian immigrant who did the make-up for his film but could not join the union. Seiderman said the head of the union received a call from the Labor Department the next day, and in November 1941 he was a full union member.
Sets.
Although credited as an assistant, the film's art direction was done by Perry Ferguson. Welles and Ferguson got along during their collaboration. In the weeks before production began Welles, Toland and Ferguson met regularly to discuss the film and plan every shot, set design and prop. Ferguson would take notes during these discussions and create rough designs of the sets and story boards for individual shots. After Welles approved the rough sketches, Ferguson made miniature models for Welles and Toland to experiment on with a periscope in order to rehearse and perfect each shot. Ferguson then had detailed drawings made for the set design, including the film's lighting design. The set design was an integral part of the film's overall look and Toland's cinematography.
In the original script the Great Hall at Xanadu was modeled after the Great Hall in Hearst Castle and its design included a mixture of Renaissance and Gothic styles. "The Hearstian element is brought out in the almost perverse juxtaposition of incongruous architectural styles and motifs," wrote Carringer. Before RKO cut the film's budget, Ferguson's designs were more elaborate and resembled the production designs of early Cecil B. DeMille films and "Intolerance". The budget cuts reduced Ferguson's budget by 33 percent and his work cost $58,775 total, which was below average at that time. To save costs Ferguson and Welles re-wrote scenes in Xanadu's living room and transported them to the Great Hall. A large staircase from another film was found and used at no additional cost. When asked about the limited budget, Ferguson said "Very often—as in that much-discussed 'Xanadu' set in "Citizen Kane"—we can make a foreground piece, a background piece, and imaginative lighting suggest a great deal more on the screen than actually exists on the stage." According to the film's official budget there were 81 sets built, but Ferguson said there were between 106 and 116.
Still photographs of Oheka Castle in Huntington, New York, were used in the opening montage, representing Kane's Xanadu estate. Ferguson also designed statues from Kane's collection with styles ranging from Greek to German Gothic. The sets were also built to accommodate Toland's camera movements. Walls were built to fold and furniture could quickly be moved. The film's famous ceilings were made out of muslin fabric and camera boxes were built into the floors for low angle shots. Welles later said that he was proud that the film production value looked much more expensive than the film's budget. Although neither worked with Welles again, Toland and Ferguson collaborated in several films in the 1940s.
Special effects.
The film's special effects were supervised by RKO department head Vernon L. Walker. Welles pioneered several visual effects to cheaply shoot things like crowd scenes and large interior spaces. For example, the scene in which the camera in the opera house rises dramatically to the rafters, to show the workmen showing a lack of appreciation for Susan Alexander Kane's performance, was shot by a camera craning upwards over the performance scene, then a curtain wipe to a miniature of the upper regions of the house, and then another curtain wipe matching it again with the scene of the workmen. Other scenes effectively employed miniatures to make the film look much more expensive than it truly was, such as various shots of Xanadu.
Some shots included rear screen projection in the background, such as Thompson's interview of Leland and some of the ocean backgrounds at Xanadu. Bordwell claims that the scene where Thatcher agrees to be Kane's guardian used rear screen projection to depict young Kane in the background, despite this scene being cited as a prime example of Toland's deep focus cinematography. A special effects camera crew from Walker's department was required for the extreme close-up shots such as Kane's lips when he says "Rosebud" and the shot of the typewriter typing Susan's bad review.
Optical effects artist Dunn claimed that “up to 80 percent of some reels was optically printed.” These shots were traditionally attributed to Toland for years. The optical printer improved some of the deep focus shots. One problem with the optical printer was that it sometimes created excessive graininess, such as the optical zoom out of the snow globe. Welles decided to superimpose snow falling to mask the graininess in these shots. Toland said that he disliked the results of the optical printer, but acknowledged that "RKO special effects expert Vernon Walker, ASC, and his staff handled their part of the production—a by no means inconsiderable assignment—with ability and fine understanding."
Any time deep focus was impossible—as in the scene in which Kane finishes a negative review of Susan's opera while at the same time firing the person who began writing the review—an optical printer was used to make the whole screen appear in focus, visually layering one piece of film onto another. However, some apparently deep-focus shots were the result of in-camera effects, as in the famous scene in which Kane breaks into Susan's room after her suicide attempt. In the background, Kane and another man break into the room, while simultaneously the medicine bottle and a glass with a spoon in it are in closeup in the foreground. The shot was an in-camera matte shot. The foreground was shot first, with the background dark. Then the background was lit, the foreground darkened, the film rewound, and the scene re-shot with the background action.
Music.
The film's music was composed by Bernard Herrmann. Herrmann had composed for Welles for his Mercury Theatre radio broadcasts. Because it was Herrmann's first motion picture score, RKO wanted to pay him only a small fee, but Welles insisted he be paid at the same rate as Max Steiner.
The score established Herrmann as an important new composer of film soundtracks and eschewed the typical Hollywood practice of scoring a film with virtually non-stop music. Instead Herrmann used what he later described as '"radio scoring", musical cues typically 5–15 seconds in length that bridge the action or suggest a different emotional response. The breakfast montage sequence begins with a graceful waltz theme and gets darker with each variation on that theme as the passage of time leads to the hardening of Kane's personality and the breakdown of his first marriage.
Herrmann realized that musicians slated to play his music were hired for individual unique sessions; there was no need to write for existing ensembles. This meant that he was free to score for unusual combinations of instruments, even instruments that are not commonly heard. In the opening sequence, for example, the tour of Kane's estate Xanadu, Herrmann introduces a recurring leitmotiv played by low woodwinds, including a quartet of alto flutes.
For Susan Alexander Kane's operatic sequence, Welles suggested that Herrmann compose a witty parody of a Mary Garden vehicle, an aria from "Salammbô". "Our problem was to create something that would give the audience the feeling of the quicksand into which this simple little girl, having a charming but small voice, is suddenly thrown," Herrmann said. Writing in the style of a 19th-century French Oriental opera, Herrmann put the aria in a key that would force the singer to strain to reach the high notes, culminating in a high D, well outside the range of Susan Alexander. Soprano Jean Forward dubbed the vocal part for Comingore. Houseman claimed to have written the libretto, based on Jean Racine’s "Athalie" and "Phedre", although some confusion remains since Lucille Fletcher remembered preparing the lyrics. Fletcher, then Herrmann's wife, wrote the libretto for his opera "Wuthering Heights".
Music enthusiasts consider the scene in which Susan Alexander Kane attempts to sing the famous cavatina "Una voce poco fa" from "Il barbiere di Siviglia" by Gioachino Rossini with vocal coach Signor Matiste as especially memorable for depicting the horrors of learning music through mistakes.
In 1972, Herrmann said, "I was fortunate to start my career with a film like "Citizen Kane", it's been a downhill run ever since!" Welles loved Herrmann's score and told director Henry Jaglom that it was 50 percent responsible for the film's artistic success.
Some incidental music came from other sources. Welles heard the tune used for the publisher's theme, "Oh, Mr. Kane", in Mexico. Called "A Poco No", the song was written by Pepe Guízar and special lyrics were written by Herman Ruby.
"In a Mizz", a 1939 jazz song by Charlie Barnet and Haven Johnson, bookends Thompson's second interview of Susan Alexander Kane. "I kind of based the whole scene around that song," Welles said. "The music is by Nat Cole—it's his trio." Later—beginning with the lyrics, "It can't be love"—"In a Mizz" is performed at the Everglades picnic, framing the fight in the tent between Susan and Kane. Musicians including bandleader Cee Pee Johnson (drums), Alton Redd (vocals), Raymond Tate (trumpet), Buddy Collette (alto sax) and Buddy Banks (tenor sax) are featured.
All of the music used in the newsreel came from the RKO music library, edited at Welles's request by the newsreel department to achieve what Herrmann called "their own crazy way of cutting". The "News on the March" theme that accompanies the newsreel titles is "Belgian March" by Anthony Collins, from the film "Nurse Edith Cavell". Other examples are an excerpt from Alfred Newman's score for "Gunga Din" (the exploration of Xanadu), Roy Webb's theme for the film "Reno" (the growth of Kane's empire), and bits of Webb's score for "Five Came Back" (introducing Walter Parks Thatcher).
Editing.
One of the editing techniques used in "Citizen Kane" was the use of montage to collapse time and space, using an episodic sequence on the same set while the characters changed costume and make-up between cuts so that the scene following each cut would look as if it took place in the same location, but at a time long after the previous cut. In the breakfast montage, Welles chronicles the breakdown of Kane's first marriage in five vignettes that condense 16 years of story time into two minutes of screen time. Welles said that the idea for the breakfast scene "was stolen from "The Long Christmas Dinner" of Thornton Wilder … a one-act play, which is a long Christmas dinner that takes you through something like 60 years of a family's life." The film often uses long dissolves to signify the passage of time and its psychological effect of the characters, such as the scene in which the abandoned sled is covered with snow after the young Kane is sent away with Thatcher.
Welles was influenced by the editing theories of Sergei Eisenstein by using jarring cuts that caused "sudden graphic or associative contrasts", such as the cut from Kane's deathbed to the beginning of the "News on the March" sequence and a sudden shot of a shrieking bird at the beginning of Raymond's flashback. Although the film typically favors mise-en-scène over montage, the scene in which Kane goes to Susan Alexander's apartment after first meeting her is the only one that is primarily cut as close-ups with shots and counter shots between Kane and Susan. Fabe says that "by using a standard Hollywood technique sparingly, revitalizes its psychological expressiveness."
Themes.
Political themes.
In her 1992 monograph for the British Film Institute, critic Laura Mulvey explored the anti-fascist themes of "Citizen Kane". The "News on the March" newsreel presents Kane keeping company with Hitler and other dictators while he smugly assures the public there will be no war. Mulvey wrote that the film reflects "the battle between intervention and isolationism" then being waged in the United States; the film was released six months before the attack on Pearl Harbor, while President Franklin D. Roosevelt was laboring to win public opinion for entering World War II. "Not only was the war in Europe the burning public issue of the time," Mulvey wrote, "it was of passionate personal importance to Orson Welles … In the rhetoric of "Citizen Kane", the destiny of isolationism is realised in metaphor: in Kane's own fate, dying wealthy and lonely, surrounded by the detritus of European culture and history."
Journalist Ignacio Ramonet has cited the film as an early example of mass media manipulation of public opinion and the power that media conglomerates have on influencing the democratic process. Ramonet believes that this early example of a media mogul influencing politics is outdated and that "today Citizen Kane would be a dwarf. He owned a few papers in one country. The forces that dominate today have integrated image with text and sound and the world is their market. There are media groups with the power of a thousand Citizen Kanes." Media mogul Rupert Murdoch is sometimes labeled as a latter-day "Citizen Kane".
Reception.
Pre-release controversy.
To ensure that "Citizen Kane"'s influence from Hearst's life was a secret, Welles limited access to dailies and managed the film's publicity. A December 1940 feature story in "Stage" magazine compared the film's narrative to "Faust" and made no mention of Hearst.
The film was scheduled to premiere at RKO's flagship theater Radio City Music Hall on February 14, but in early January 1941 Welles was not finished with post-production work and told RKO that it still needed its musical score. Writers for national magazines had early deadlines and so a rough cut was previewed for a select few on January 3, 1941 for such magazines as "Life", "Look" and "Redbook". Gossip columnist Hedda Hopper (and Parsons' arch rival) showed up to the screening uninvited. Most of the critics at the preview said that they liked the film and gave it good advanced reviews. Hopper wrote negatively about it, calling the film a “viscous and irresponsible attack on a great man” and criticizing its corny writing and old fashioned photography. "Friday" magazine ran an article drawing point-by-point comparisons between Kane and Hearst and documented how Welles had led on Parsons, Hollywood correspondent for Hearst papers. Up until this Welles had been friendly with Parsons. The magazine quoted Welles as saying that he couldn’t understand why she was so nice to him and that she should “wait until the woman finds out that the picture’s about her boss.” Welles immediately denied making the statement and the editor of "Friday" admitted that it may be false. Welles apologized to Parsons and assured her that he had never made that remark.
Shortly after "Friday"'s article Hearst sent Parsons an angry letter complaining that he had learned about "Citizen Kane" from Hopper and not her. The incident made a fool of Parsons and compelled her to start attacking Welles and the film. Parsons demanded a private screening of the film and personally threatened Schaefer on Hearst’s behalf, first with a lawsuit and then with a vague threat of consequences for everyone in Hollywood. On January 10 Parsons and two lawyers working for Hearst were given a private screening of the film. James G. Stewart was present at the screening and said that she walked out of the film. Soon after, Parsons called Schaefer and threatened RKO with a lawsuit if they released "Kane". She also contacted the management of Radio City Music Hall and demanded that they not screen it. The next day, the front page headline in "Daily Variety" read, "HEARST BANS RKO FROM PAPERS." Hearst began this ban by suppressing promotion of RKO's "Kitty Foyle", but in two weeks the ban was lifted for everything except "Kane."
When Schaefer did not submit to Parsons she called other studio heads and made more threats on behalf of Hearst to expose the private lives of people throughout the entire film industry. Welles was threatened with an exposé about his romance with the married actress Delores del Rio, who wanted the affair kept secret until her divorce was finalized. In a statement to journalists Welles denied that the film was about Hearst. Hearst began preparing an injunction against the film for libel and invasion of privacy, but Welles’s lawyer told him that he doubted Hearst would proceed due to the negative publicity and requited testimony that an injunction would bring.
"The Hollywood Reporter" ran a front-page story on January 13 that Hearst papers were about to run a series of editorials attacking Hollywood's practice of hiring refugees and immigrants for jobs that could be done by Americans. The goal was to put pressure on the other studios to force RKO to shelve "Kane". Many of those immigrants had fled Europe after the rise of fascism and feared losing the safe haven of the United States. Soon afterwards, Schaefer was approached by Nicholas Schenck, head of Metro-Goldwyn-Mayer's parent company, with an offer on the behalf of Louis B. Mayer and other Hollywood executives to RKO Pictures of $805,000 to destroy all prints of the film and burn the negative. Once RKO's legal team reassured Schaefer, the studio announced on January 21 that "Kane" would be released as scheduled, and with one of the largest promotional campaigns in the studio's history. Schaefer brought Welles to New York City for a private screening of the film with the New York corporate heads of the studios and their lawyers. There was no objection to its release provided that certain changes, including the removal or softening of specific references that might offend Hearst, were made. Welles agreed and cut the running time from 122 minutes to 119 minutes. The cuts satisfied the corporate lawyers.
Hearst's response.
Hearing about "Citizen Kane" enraged Hearst so much that he banned any advertising, reviewing, or mentioning of it in his papers, and had his journalists libel Welles. Welles used Hearst's opposition as a pretext for previewing the film in several opinion-making screenings in Los Angeles, lobbying for its artistic worth against the hostile campaign that Hearst was waging. A special press screening took place in early March. Henry Luce was in attendance and reportedly wanted to buy the film from RKO for $1 million to distribute it himself. The reviews for this screening were positive. A "Hollywood Review" headline read, "Mr. Genius Comes Through; 'Kane' Astonishing Picture". The "Motion Picture Herald" reported about the screening and Welles's intention to sue RKO. "Time" magazine wrote that "The objection of Mr. Hearst, who founded a publishing empire on sensationalism, is ironic. For to most of the several hundred people who have seen the film at private screenings, "Citizen Kane" is the most sensational product of the U.S. movie industry." A second press screening occurred in April.
When Schaefer rejected Hearst's offer to suppress the film, Hearst banned every newspaper and station in his media conglomerate from reviewing—or even mentioning—the film. He also had many movie theaters ban it, and many did not show it through fear of being socially exposed by his massive newspaper empire. The Oscar-nominated documentary "The Battle Over Citizen Kane" lays the blame for the film's relative failure squarely at the feet of Hearst. The film did decent business at the box office; it went on to be the sixth highest grossing film in its year of release, a modest success its backers found acceptable. Nevertheless, the film's commercial performance fell short of its creators' expectations. Hearst's biographer David Nasaw points out that Hearst's actions were not the only reason "Kane" failed, however: the innovations Welles made with narrative, as well as the dark message at the heart of the film (that the pursuit of success is ultimately futile) meant that a popular audience could not appreciate its merits.
Hearst's attacks against Welles went beyond attempting to suppress the film. Welles said that while he was on his post-filming lecture tour a police detective approached him at a restaurant and advised him not to go back to his hotel. A 14-year-old girl had reportedly been hidden in the closet of his room, and two photographers were waiting for him to walk in. Knowing he would be jailed after the resulting publicity, Welles did not return to the hotel but waited until the train left town the following morning. "But that wasn't Hearst," Welles said, "that was a hatchet man from the local Hearst paper who thought he would advance himself by doing it."
In March 1941 Welles directed a Broadway version of Richard Wright's "Native Son" (and, for luck, used a "Rosebud" sled as a prop). "Native Son" received positive reviews, but Hearst-owned papers used the opportunity to attack Welles as a communist. The Hearst papers vociferously attacked Welles after his April 1941 radio play, "His Honor, the Mayor", produced for The Free Company radio series on CBS.
Welles described his chance encounter with Hearst in an elevator at the Fairmont Hotel on the night "Citizen Kane" opened in San Francisco. Hearst and Welles's father were acquaintances, so Welles introduced himself and asked Hearst if he would like to come to the opening. Hearst did not respond. "As he was getting off at his floor, I said, 'Charles Foster Kane would have accepted.' No reply", recalled Welles. "And Kane would have you know. That was his style—just as he finished Jed Leland's bad review of Susan as an opera singer."
In 1945 Hearst journalist Robert Shaw wrote that the film got "a full tide of insensate fury" from Hearst papers, "then it ebbed suddenly. With one brain cell working, the chief realized that such hysterical barking by the trained seals would attract too much attention to the picture. But to this day the name of Orson Welles is on the official son-of-a-bitch list of every Hearst newspaper."
Despite Hearst's attempts to destroy the film, since 1941 references to his life and career have usually included a reference to "Citizen Kane", such as the headline 'Son of Citizen Kane Dies' for the obituary of Hearst's son. In 2012 the Hearst estate agreed to screen the film at Hearst Castle in San Simeon, breaking Hearst's ban on the film.
Release.
Radio City Music Hall's management refused to screen "Citizen Kane" for its premiere. A possible factor was Parsons's threat that "The American Weekly" would run a defamatory story on the grandfather of major RKO stockholder Nelson Rockefeller. Other exhibitors feared being sued for libel by Hearst and refused to show the film. In March Welles threatened the RKO board of governors with a lawsuit if they did not release the film. Schaefer stood by Welles and opposed the board of governors. When RKO still delayed the film's release Welles offered to buy the film for $1 million and the studio finally agreed to release the film on May 1.
Schaefer managed to book a few theaters willing to show the film. Hearst papers refused to accept advertising. RKO's publicity advertisements for the film erroneously promoted it as a love story.
"Kane" opened at the RKO Palace Theatre on Broadway in New York on May 1, 1941, in Chicago on May 6, and in Los Angeles on May 8. Welles said that at the Chicago premiere that he attended the theater was almost empty. It did well in cities and larger towns but fared poorly in more remote areas. RKO still had problems getting exhibitors to show the film. For example, one chain controlling more than 500 theaters got Welles's film as part of a package but refused to play it, reportedly out of fear of Hearst. Hearst's disruption of the film's release damaged its box office performance and, as a result, it lost $160,000 during its initial run. The film earned $23,878 during its first week in New York. By the ninth week it only made $7,279. Overall it lost money in New York, Boston, Chicago, Los Angeles, San Francisco and Washington, D.C., but made a profit in Seattle.
Contemporary responses.
"Citizen Kane" received good reviews from several critics. "New York Daily News" critic Kate Cameron called it "one of the most interesting and technically superior films that has ever come out of a Hollywood studio". "New York World-Telegram" critic William Boehnel said that the film was "staggering and belongs at once among the greatest screen achievements". "Time" magazine wrote that "it has found important new techniques in picture-making and story-telling." "Life" magazine's review said that "few movies have ever come from Hollywood with such powerful narrative, such original technique, such exciting photography." John C. Mosher of "The New Yorker" called the film's style "like fresh air" and raved "Something new has come to the movie world at last." Anthony Bower of "The Nation" called it "brilliant" and praised the cinematography and performances by Welles, Comingore and Cotten. John O'Hara's "Newsweek" review called it the best picture he'd ever seen and said Welles was "the best actor in the history of acting." Welles called O'Hara's review "the greatest review that anybody ever had."
The day following the premiere of "Citizen Kane," "The New York Times" critic Bosley Crowther wrote that "... it comes close to being the most sensational film ever made in Hollywood."
Count on Mr. Welles: he doesn't do things by halves. ... Upon the screen he discovered an area large enough for his expansive whims to have free play. And the consequence is that he has made a picture of tremendous and overpowering scope, not in physical extent so much as in its rapid and graphic rotation of thoughts. Mr. Welles has put upon the screen a motion picture that really moves.
In the UK C. A. Lejeune of "The Observer" called it "The most exciting film that has come out of Hollywood in twenty-five years" and Dilys Powell of "The Sunday Times" said the film's style was made "with the ease and boldness and resource of one who controls and is not controlled by his medium." Edward Tangye Lean of "Horizon" praised the film’s technical style, calling it "perhaps a decade ahead of its contemporaries."
A few reviews were mixed. Otis Ferguson of "The New Republic" said it was "the boldest free-hand stroke in major screen production since Griffith and Bitzer were running wild to unshackle the camera", but also criticized its style, calling it a "retrogression in film technique" and stating that "it holds no great place" in film history. In a rare film review, filmmaker Erich von Stroheim criticized the film's story and non-linear structure, but praised the technical style and performances, and wrote "Whatever the truth may be about it, "Citizen Kane" is a great picture and will go down in screen history. More power to Welles!"
Some prominent critics wrote negative reviews. In his 1941 review for "Sur", Jorge Luis Borges famously called the film "a labyrinth with no center" and predicted that its legacy would be a film "whose historical value is undeniable but which no one cares to see again." "The Argus Weekend Magazine" critic Erle Cox called the film "amazing" but thought that Welles's break with Hollywood traditions was "overdone." "Tatler"'s James Agate called it "the well-intentioned, muddled, amateurish thing one expects from high-brows" and "a quite good film which tries to run the psychological essay in harness with your detective thriller, and doesn't quite succeed." Eileen Creelman of "The New York Sun" called it "a cold picture, unemotional, a puzzle rather than a drama". Other people who disliked the film were W. H. Auden and James Agee.
Awards.
"Citizen Kane" received the New York Film Critics Circle Award for Best Picture. The National Board of Review voted it Best Film of 1941, and recognized Welles and Coulouris for their performances.
"Citizen Kane" received nine nominations at the 1941 Academy Awards:
It was widely believed the film would win most of its Oscar nominations, but it received only the award for Best Writing (Original Screenplay), shared by Welles and Mankiewicz. "Variety" reported that block voting by screen extras deprived "Citizen Kane" of Academy Awards for Best Picture and Best Actor (Welles), and similar prejudices were likely to have been responsible for the film receiving no technical awards.
Legacy.
"Citizen Kane" was the only film made under Welles's original contract with RKO Pictures, which gave him complete creative control. Welles's new business manager and attorney permitted the contract to lapse. In July 1941, Welles reluctantly signed a new and less favorable deal with RKO under which he produced and directed "The Magnificent Ambersons" (1942), produced "Journey into Fear" (1943), and began "It's All True", a film he agreed to do without payment. In the new contract Welles was an employee of the studio and lost the right to final cut, which later allowed RKO to modify and re-cut "The Magnificent Ambersons" over his objections. In June 1942 Schaefer resigned the presidency of RKO Pictures and Welles's contract was terminated by his successor.
Release in Europe.
During World War II, "Citizen Kane" was not seen in most European countries. It was shown in France for the first time on July 10, 1946 at the Marbeuf theatre in Paris. Initially most French film critics were influenced by the negative reviews of Jean-Paul Sartre in 1945 and Georges Sadoul in 1946. At that time many French intellectuals and filmmakers shared Sartre's negative opinion that Hollywood filmmakers were uncultured. Sartre criticized the film's flashbacks for its nostalgic and romantic preoccupation with the past instead of the realities of the present and said that "the whole film is based on a misconception of what cinema is all about. The film is in the past tense, whereas we all know that cinema has got to be in the present tense."
André Bazin, a little-known film critic working for Sartre's "Les Temps modernes", was asked to give an impromptu speech about the film after a screening at the Colisée Theatre in the autumn of 1946 and changed the opinion of much of the audience. This speech led to Bazin's 1947 article "The Technique of Citizen Kane", which directly influenced public opinion about the film. Carringer wrote that Bazin was "the one who did the most to enhance the film’s reputation." Both Bazin's critique of the film and his theories about cinema itself centered around his strong belief in mise en scène. These theories were diametrically opposed to both the popular Soviet montage theory and the politically Marxist and anti-Hollywood beliefs of most French film critics at that time. Bazin believed that a film should depict reality without the filmmaker imposing their "will" on the spectator, which the Soviet theory supported. Bazin wrote that "Citizen Kane"'s mise en scène created a "new conception of filmmaking" and that the freedom given to the audience from the deep focus shots was innovative by changing the entire concept of the cinematic image. Bazin wrote extensively about the mise en scène in the scene where Susan Alexander attempts suicide, which was one long take while other films would have used four or five shots in the scene. Bazin wrote that the film's mise en scène "forces the spectator to participate in the meaning of the film" and creates "a psychological realism which brings the spectator back to the real conditions of perception."
In his 1950 essay "The Evolution of the Language of Cinema", Bazin placed "Citizen Kane" center stage as a work which ushered in a new period in cinema. One of the first critics to defend motion pictures as being on the same artistic level as literature or painting, Bazin often used the film as an example of cinema as an art form and wrote that "Welles has given the cinema a theoretical restoration. He has enriched his filmic repertory with new or forgotten effects that, in today’s artistic context, take on a significance we didn’t know they could have." Bazin also compared the film to Roberto Rossellini's "Paisà" for having "the same aesthetic concept of realism" and to the films of William Wyler shot by Toland (such as "The Little Foxes" and "The Best Years of Our Lives"), all of which used deep focus cinematography that Bazin called "a dialectical step forward in film language."
Bazin's praise of the film went beyond film theory and reflected his own philosophy towards life itself. His metaphysical interpretations about the film reflected humankind’s place in the universe. Bazin believed that the film examined one person's identity and search for meaning. It portrayed the world as ambiguous and full of contradictions, whereas films up until then simply portrayed people’s actions and motivations. Bazin's biographer Dudley Andrew wrote that:
The world of "Citizen Kane", that mysterious, dark, and infinitely deep world of space and memory where voices trail off into distant echoes and where meaning dissolves into interpretation, seemed to Bazin to mark the starting point from which all of us try to construct provisionally the sense of our lives.
Bazin went on to co-found "Cahiers du cinéma", whose contributors (including future film directors François Truffaut and Jean-Luc Godard) also praised the film. The popularity of Truffaut's auteur theory helped the film's and Welles's reputation.
Re-evaluation.
By 1942 "Citizen Kane" had run its course theatrically and, apart from a few showings at big city arthouse cinemas, it largely vanished and both the film's and Welles's reputation fell among American critics. In 1949 critic Richard Griffith in his overview of cinema, "The Film Till Now", dismissed "Citizen Kane" as "... tinpot if not crackpot Freud."
In the United States, it was neglected and forgotten until its revival on television in the mid-1950s. Three key events in 1956 led to its re-evaluation in the United States: first, RKO was one of the first studios to sell its library to television, and early that year "Citizen Kane" started to appear on television; second, the film was re-released theatrically to coincide with Welles's return to the New York stage, where he played "King Lear"; and third, American film critic Andrew Sarris wrote "Citizen Kane: The American Baroque" for "Film Culture", and described it as "the great American film" and "the work that influenced the cinema more profoundly than any American film since "Birth of a Nation"." Carringer considers Sarris's essay as the most important influence on the film's reputation in the US.
During Expo 58, a poll of over 100 film historians named "Kane" one of the top ten greatest films ever made (the group gave first-place honors to "The Battleship Potemkin"). When a group of young film directors announced their vote for the top six, they were booed for not including the film.
In the decades since, its critical status as the greatest film ever made has grown, with numerous essays and books on it including Peter Cowie's "The Cinema of Orson Welles", Ronald Gottesman's "Focus on Citizen Kane", a collection of significant reviews and background pieces, and most notably Kael's essay, "Raising Kane", which promoted the value of the film to a much wider audience than it had reached before. Despite its criticism of Welles, it further popularized the notion of "Citizen Kane" as the great American film. The rise of art house and film society circuits also aided in the film's rediscovery. David Thomson said that the film 'grows with every year as America comes to resemble it."
The British magazine "Sight & Sound" has produced a Top Ten list surveying film critics every decade since 1952, and is regarded as one of the most respected barometers of critical taste. "Citizen Kane" was a runner up to the top 10 in its 1952 poll but was voted as the greatest film ever made in its 1962 poll, retaining the top spot in every subsequent poll until 2012, when "Vertigo" displaced it.
The film has also ranked number one in the following film "best of" lists: Julio Castedo's "The 100 Best Films of the Century", Cahiers du cinéma's 100 films pour une cinémathèque idéale, "Kinovedcheskie Zapiski", "Time Out" magazine's Top 100 Films (Centenary), "The Village Voice"'s 100 Greatest Films, and The Royal Belgian Film Archive's Most Important and Misappreciated American Films.
Roger Ebert called "Citizen Kane" the greatest film ever made: "But people don't always ask about the greatest film. They ask, 'What's your favorite movie?' Again, I always answer with "Citizen Kane"."
In 1989, the United States Library of Congress deemed the film "culturally, historically, or aesthetically significant" and selected it for preservation in the National Film Registry. "Citizen Kane" was one of first 25 films inducted into the registry.
On February 18, 1999, the United States Postal Service honored "Citizen Kane" by including it in its Celebrate the Century series. The film was honored again February 25, 2003, in a series of U.S. postage stamps marking the 75th anniversary of the Academy of Motion Picture Arts and Sciences. Art director Perry Ferguson represents the behind-the-scenes craftsmen of filmmaking in the series; he is depicted completing a sketch for "Citizen Kane".
"Citizen Kane" was ranked number one in the American Film Institute's polls of film industry artists and leaders in 1998 and 2007. "Rosebud" was chosen as the 17th most memorable movie quotation in a 2005 AFI poll. The film's score was one of 250 nominees for the top 25 film scores in American cinema in another 2005 AFI poll.
The film currently has a 100% rating at Rotten Tomatoes, based on 70 reviews by approved critics, with an average rating of 9.4/10. The site's consensus states: "Orson Welles's epic tale of a publishing tycoon's rise and fall is entertaining, poignant, and inventive in its storytelling, earning its reputation as a landmark achievement in film."
Influence.
"Citizen Kane" has been called the most influential film of all time. Richard Corliss has asserted that Jules Dassin's 1941 film "The Tell-Tale Heart" was the first example of its influence and the first pop culture reference to the film occurred later in 1941 when the spoof comedy "Hellzapoppin"' featured a "Rosebud" sled. The film's cinematography was almost immediately influential and in 1942 "American Cinematographer" wrote "without a doubt the most immediately noticeable trend in cinematography methods during the year was the trend toward crisper definition and increased depth of field."
The cinematography influenced John Huston's "The Maltese Falcon". Cinematographer Arthur Edeson used a wider-angle lens than Toland and the film includes many long takes, low angles and shots of the ceiling, but it did not use deep focus shots on large sets to the extent that "Citizen Kane" did. Edeson and Toland are often credited together for revolutionizing cinematography in 1941. Toland's cinematography influenced his own work on "The Best Years of Our Lives". Other films influenced include "Gaslight", "Mildred Pierce" and "Jane Eyre". Cinematographer Kazuo Miyagawa said that his use of deep focus was influenced by "the camera work of Gregg Toland in "Citizen Kane"" and not by traditional Japanese art.
Its cinematography, lighting, and flashback structure influenced such film noirs of the 1940s and 1950s as "The Killers", "Keeper of the Flame", "Caught", "The Great Man" and "This Gun for Hire". David Bordwell and Kristin Thompson have written that “For over a decade thereafter American films displayed exaggerated foregrounds and somber lighting, enhanced by long takes and exaggerated camera movements.” However, by the 1960s filmmakers such as those from the French New Wave and Cinéma vérité movements favored "flatter, more shallow images with softer focus" and "Citizen Kane"'s style became less fashionable. American filmmakers in the 1970s combined these two approaches by using long takes, rapid cutting, deep focus and telephoto shots all at once. Its use of long takes influenced films such as "The Asphalt Jungle", and its use of deep focus cinematography influenced "Gun Crazy", "The Whip Hand", "The Devil's General" and "Justice Is Done". The flashback structure in which different characters have conflicting versions of past events influenced "La commare secca" and "Man of Marble".
The film's structure influenced the biographical films "Lawrence of Arabia" and ""—which begin with the subject's death and show their life in flashbacks—as well as Welles's thriller "Mr. Arkadin". Rosenbaum sees similarities in the film's plot to "Mr. Arkadin", as well as the theme of nostalgia for loss of innocence throughout Welles's career, beginning with "Citizen Kane" and including "The Magnificent Ambersons", "Mr. Arkadin" and "Chimes at Midnight". Rosenbaum also points out how the film influenced Warren Beatty's "Reds". The film depicts the life of Jack Reed through the eyes of Louise Bryant, much as Kane's life is seen through the eyes of Thompson and the people who he interviews. Rosenbaum also compared the romantic montage between Reed and Bryant with the breakfast table montage in "Citizen Kane".
Akira Kurosawa's "Rashomon" is often compared to the film due to both having complicated plot structures told by multiple characters in the film. Welles said his initial idea for the film was "Basically, the idea "Rashomon" used later on," however Kurosawa had not yet seen the film before making "Rashomon" in 1950. Nigel Andrews has compared the film's complex plot structure to "Rashomon", "Last Year at Marienbad", "Memento" and "Magnolia". Andrews also compares Charles Foster Kane to Michael Corleone in "The Godfather", Jake LaMotta in "Raging Bull" and Daniel Plainview in "There Will Be Blood" for their portrayals of "haunted megalomaniacpresiding over the shards of [their own ."
The films of Paul Thomas Anderson have been compared to it. "Variety" compared "There Will Be Blood" to the film and called it "one that rivals "Giant" and "Citizen Kane" in our popular lore as origin stories about how we came to be the people we are." "The Master" has been called "movieland’s only spiritual sequel to "Citizen Kane" that doesn’t shrivel under the hefty comparison" and the film's loose depiction of L. Ron Hubbard has been compared to "Citizen Kane"'s depiction of Hearst. "The Social Network" has been compared to the film for its depiction of a media mogul and by the character Erica Albright being similar to Rosebud. The controversy of the Sony hacking before the release of "The Interview" brought comparisons of Hearst's attempt to suppress the film. The film's plot structure and some specific shots influenced Todd Haynes's "Velvet Goldmine". Abbas Kiarostami's "The Traveler" has been called "the "Citizen Kane" of the Iranian children’s cinema." The film's use of overlapping dialogue has influenced the films of Robert Altman and Carol Reed. Reed's films "Odd Man Out", "The Third Man" (in which Welles and Cotten appeared) and "Outcast of the Islands" were also influenced by the film's cinematography.
Many directors have listed it as one of the greatest films ever made, including Woody Allen, Michael Apted, Les Blank, Kenneth Branagh, Paul Greengrass, Michel Hazanavicius, Michael Mann, Sam Mendes, Jiri Menzel, Paul Schrader, Martin Scorsese, Denys Arcand, Gillian Armstrong, John Boorman, Roger Corman, Alex Cox, Milos Forman, Norman Jewison, Richard Lester, Richard Linklater, Paul Mazursky, Ronald Neame, Sydney Pollack and Stanley Kubrick. Yasujirō Ozu said it was his favorite non-Japanese film and was impressed by its techniques. François Truffaut said that the film "has inspired more vocations to cinema throughout the world than any other" and recognized its influence in "The Barefoot Contessa", "Les Mauvaises Rencontres", "Lola Montès", and "8 1/2". Truffaut's "Day for Night" pays tribute to the film in a dream sequence depicting a childhood memory of the character played by Truffaut stealing publicity photos from the film. Numerous film directors have cited the film as influential on their own films, including Theo Angelopoulos, Luc Besson, the Coen brothers, Francis Ford Coppola, Brian De Palma, John Frankenheimer, Stephen Frears, Sergio Leone, Michael Mann, Ridley Scott, Martin Scorsese, Bryan Singer and Steven Spielberg. Ingmar Bergman disliked the film and called it "a total bore. Above all, the performances are worthless. The amount of respect that movie has is absolutely unbelievable!"
William Friedkin said that the film influenced him and called it "a veritable quarry for filmmakers, just as Joyce's "Ulysses" is a quarry for writers." The film has also influenced other art forms. Carlos Fuentes's novel "The Death of Artemio Cruz" was partially inspired by the film and the rock band The White Stripes paid unauthorized tribute to the film in the song "The Union Forever".
Film memorabilia.
In 1982, film director Steven Spielberg bought a "Rosebud" sled for $60,500; it was one of three balsa sleds used in the closing scenes and the only one that was not burned. After the Spielberg purchase, it was reported that retiree Arthur Bauer claimed to own another "Rosebud" sled. In early 1942 when Bauer was 12 he won an RKO publicity contest and selected the hardwood sled as his prize. In 1996, Bauer's estate offered the painted pine sled at auction through Christie's. Bauer's son told CBS News that his mother had once wanted to paint the sled and use it as a plant stand, but Bauer told her to "just save it and put it in the closet." The sled was sold to an anonymous bidder for $233,500.
Welles's Oscar for Best Original Screenplay was believed to be lost until it was rediscovered in 1994. It was withdrawn from a 2007 auction at Sotheby's when bidding failed to reach its estimate of $800,000 to $1.2 million. Owned by the charitable Dax Foundation, it was auctioned for $861,542 in 2011 to an anonymous buyer. Mankiewicz's Oscar was sold at least twice, in 1999 and again in 2012, the latest price being $588,455.
In 1989, Mankiewicz's personal copy of the "Citizen Kane" script was auctioned at Christie's. The leather-bound volume included the final shooting script and a carbon copy of "American" that bore handwritten annotations—purportedly made by Hearst's lawyers, who were said to have obtained it in the manner described by Kael in "Raising Kane". Estimated to bring $70,000 to $90,000, it sold for a record $231,000.
In 2007, Welles's personal copy of the last revised draft of "Citizen Kane" before the shooting script was sold at Sotheby's for $97,000. A second draft of the script titled "American", marked "Mr. Welles' working copy", was auctioned by Sotheby's in 2014 for $164,692. A collection of 24 pages from a working script found in Welles's personal possessions by his daughter Beatrice Welles was auctioned in 2014 for $15,000.
In 2014, a collection of approximately 235 "Citizen Kane" stills and production photos that had belonged to Welles was sold at auction for $7,812.
Rights and home media.
The composited camera negative of "Citizen Kane" was destroyed in a New Jersey film laboratory fire in the 1970s. Subsequent prints were derived from a master positive (a fine-grain preservation element) made in the 1940s and originally intended for use in overseas distribution. Modern techniques were used to produce a pristine print for a 50th Anniversary theatrical reissue in 1991 which Paramount released for then-owner Turner Broadcasting System, which earned $1.6 million in North America.
In 1955, RKO sold the American television rights to its film library, including "Citizen Kane", to C&C Television Corp. In 1960, television rights to the pre-1956 RKO library were acquired by United Artists. RKO kept the non-broadcast television rights to its library.
In 1976, when home video was in its infancy, entrepreneur Snuff Garrett bought cassette rights to the RKO library for what United Press International termed "a pittance." In 1978 The Nostalgia Merchant released the film through Media Home Entertainment. By 1980 the 800-title library of The Nostalgia Merchant was earning $2.3 million a year. "Nobody wanted cassettes four years ago," Garrett told UPI. "It wasn't the first time people called me crazy. It was a hobby with me which became big business." RKO Home Video released the film on VHS and Betamax in 1985.
In 1984, The Criterion Collection released the film as its first LaserDisc. It was made from a fine grain master positive provided by the UCLA Film and Television Archive. When told about the then-new concept of having an audio commentary on the disc, Welles was skeptical but said "theoretically, that’s good for teaching movies, so long as they don’t talk nonsense." In 1992 Criterion released a new 50th Anniversary Edition LaserDisc. This version had an improved transfer and additional special features, including the documentary "The Legacy of Citizen Kane" and Welles's early short "The Hearts of Age".
Turner Broadcasting System acquired broadcast television rights to the RKO library in 1986 and the full worldwide rights to the library in 1987. The RKO Home Video unit was reorganized into Turner Home Entertainment that year. In 1991 Turner released a 50th Anniversary Edition on VHS and as a collector's edition that includes the film, the documentary "Reflections On Citizen Kane", Harlan Lebo's 50th anniversary album, a poster and a copy of the original script. In 1996, Time Warner acquired Turner and Warner Home Video absorbed Turner Home Entertainment. Today, Time Warner's Warner Bros. unit has distribution rights for the film.
In 2001, Warner Home Video released a 60th Anniversary Collectors Edition DVD. The two-disc DVD included feature-length commentaries by Roger Ebert and Peter Bogdanovich, as well as "The Battle Over Citizen Kane". It was simultaneously released on VHS. The DVD was criticized for being ""too" bright, "too clean"; the dirt and grime had been cleared away, but so had a good deal of the texture, the depth, and the sense of film grain."
In 2003, Welles's daughter Beatrice Welles sued Turner Entertainment, claiming the Welles estate is the legal copyright holder of the film. She claimed that Welles's deal to terminate his contracts with RKO meant that Turner's copyright of the film was null and void. She also claimed that the estate of Orson Welles was owed 20% of the film's profits if her copyright claim was not upheld. In 2007 she was allowed to proceed with the lawsuit, overturning the 2004 decision in favor of Turner Entertainment on the issue of video rights.
In 2011, it was released on Blu-ray Disc and DVD in a 70th anniversary box set. The "San Francisco Chronicle" called it "the Blu-ray release of the year." Supplements included everything available on the 2001 Warner Home Video release, as well as "RKO 281" and packaging extras that include a hardcover booklet and a folio containing a reproduction of the original souvenir program, miniature lobby cards and other memorabilia. The Blu-ray DVD was scanned as 4K resolution from three different 35mm prints and rectified the quality issues of the 2001 DVD.
Colorization controversy.
In the 1980s, "Citizen Kane" became a catalyst in the controversy over the colorization of black-and-white films. One proponent of film colorization was Ted Turner, whose Turner Entertainment Company owned the RKO library. A Turner Entertainment spokesperson initially stated that "Citizen Kane" would not be colorized, but in July 1988 Turner said, ""Citizen Kane?" I'm thinking of colorizing it." In early 1989 it was reported that two companies were producing color tests for Turner Entertainment. Criticism increased when filmmaker Henry Jaglom stated that shortly before his death Welles had implored him "don't let Ted Turner deface my movie with his crayons."
In February 1989, Turner Entertainment president Roger Mayer announced that work to colorize the film had been stopped due to provisions in Welles's 1939 contract with RKO that "could be read to prohibit colorization without permission of the Welles estate." Mayer added that Welles's contract was "quite unusual" and "other contracts we have checked out are not like this at all." Turner had only colorized the final reel of the film before abandoning the project. In 1991 one minute of the colorized test footage was included in the BBC "Arena" documentary "The Complete Citizen Kane".
The colorization controversy was a factor in the passage of the National Film Preservation Act in 1988 which created the National Film Registry the following year. ABC News anchor Peter Jennings reported that "one major reason for doing this is to require people like the broadcaster Ted Turner, who's been adding color to some movies and re-editing others for television, to put notices on those versions saying that the movies have been altered".
External links.
Database
Other

</doc>
<doc id="5225" url="https://en.wikipedia.org/wiki?curid=5225" title="Code">
Code

In communications and information processing, code is a system of rules to convert information—such as a letter, word, sound, image, or gesture—into another form or representation, sometimes shortened or secret, for communication through a channel or storage in a medium. An early example is the invention of language, which enabled a person, through speech, to communicate what he or she saw, heard, felt, or thought to others. But speech limits the range of communication to the distance a voice can carry, and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time.
The process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands.
One reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaller or the arms of a semaphore tower encodes parts of the message, typically individual letters and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.
Theory.
In information theory and computer science, a code is usually considered as an algorithm which uniquely represents symbols from some source alphabet, by "encoded" strings, which may be in some other target alphabet. An extension of the code for representing sequences of symbols over the source alphabet is obtained by concatenating the encoded strings.
Before giving a mathematically precise definition, this is a brief example. The mapping 
is a code, whose source alphabet is the set formula_2 and whose target alphabet is the set formula_3. Using the extension of the code, the encoded string 0011001011 can be grouped into codewords as 0 011 0 01 011, and these in turn can be decoded to the sequence of source symbols "acabc".
Using terms from formal language theory, the precise mathematical definition of this concept is as follows: Let S and T be two finite sets, called the source and target alphabets, respectively. A code formula_4 is a total function mapping each symbol from S to a sequence of symbols over T, and the extension of formula_5 to a homomorphism of formula_6 into formula_7, which naturally maps each sequence of source symbols to a sequence of target symbols, is referred to as its extension.
Variable-length codes.
In this section we consider codes, which encode each source (clear text) character by a code word from some dictionary, and concatenation of such code words give us an encoded string.
Variable-length codes are especially useful when clear text characters have different probabilities; see also entropy encoding.
A "prefix code" is a code with the "prefix property": there is no valid code word in the system that is a prefix (start) of any other valid code word in the set. Huffman coding is the most known algorithm for deriving prefix codes. Prefix codes are widely referred to as "Huffman codes", even when the code was not produced by a Huffman algorithm.
Other examples of prefix codes are country calling codes, the country and publisher parts of ISBNs, and the Secondary Synchronization Codes used in the UMTS W-CDMA 3G Wireless Standard.
Kraft's inequality characterizes the sets of code word lengths that are possible in a prefix code. Virtually any uniquely decodable one-to-many code, not necessary a prefix one, must satisfy Kraft's inequality.
Error-correcting codes.
Codes may also be used to represent data in a way more resistant
to errors in transmission or storage. Such a "code" is
called an error-correcting code, and works by including carefully crafted redundancy with the stored (or transmitted) data. Examples include Hamming codes, Reed–Solomon, Reed–Muller, Walsh–Hadamard, Bose–Chaudhuri–Hochquenghem, Turbo, Golay, Goppa, low-density parity-check codes, and space–time codes.
Error detecting codes can be optimised to detect "burst errors", or "random errors".
Examples.
Codes in communication used for brevity.
A cable code replaces words (e.g., "ship" or "invoice") with shorter words, allowing the same information to be sent with fewer characters, more quickly, and most important, less expensively.
Codes can be used for brevity. When telegraph messages were the state of the art in rapid long distance communication, elaborate systems of commercial codes that encoded complete phrases into single mouths (commonly five-minute groups) were developed, so that telegraphers became conversant with such "words" as "BYOXO" ("Are you trying to weasel out of our deal?"), "LIOUY" ("Why do you not answer my question?"), "BMULD" ("You're a skunk!"), or "AYYLU" ("Not clearly coded, repeat more clearly."). Code words were chosen for various reasons: length, pronounceability, etc. Meanings were chosen to fit perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers proliferated, including one run as a front for the American Black Chamber run by Herbert Yardley between the First and Second World Wars. The purpose of most of these codes was to save on cable costs. The use of data coding for data compression predates the computer era; an early example is the telegraph Morse code where more-frequently used characters have shorter representations. Techniques such as Huffman coding are now used by computer-based algorithms to compress large data files into a more compact form for storage or transmission.
Character encodings.
Probably the most widely known data communications code so far (a.k.a. character representation) in use today is ASCII. In one or another (somewhat compatible) version, it is used by nearly all personal computers, terminals, printers, and other communication equipment. It represents 128 characters with seven-bit binary numbers—that is, as a string of seven 1s and 0s (bits). In ASCII a lowercase "a" is always 1100001, an uppercase "A" always 1000001, and so on. There are many other encodings, which represent each character by a byte (usually referred as code pages), integer code point (Unicode) or a byte sequence (UTF-8).
Genetic code.
Biological organisms contain genetic material that is used to control their function and development. This is DNA, which contains units named genes from which messenger RNA is derived. This in turn produces proteins through a code (genetic code) in which a series of triplets (codons) of four possible nucleotides can be translated into one of twenty possible amino acids. A sequence of codons results in a corresponding sequence of amino acids that form a protein molecule; a type of codon called a stop codon signals the end of the sequence.
Gödel code.
In mathematics, a Gödel code was the basis for the proof of Gödel's incompleteness theorem. Here, the idea was to map mathematical notation to a natural number (using a Gödel numbering).
Other.
There are codes using colors, like traffic lights, the color code employed to mark the nominal value of the electrical resistors or that of the trashcans devoted to specific types of garbage (paper, glass, biological, etc.)
In marketing, coupon codes can be used for a financial discount or rebate when purchasing a product from an internet retailer.
In military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry in the battlefield, etc.
Communication systems for sensory impairments, such as sign language for deaf people and braille for blind people, are based on movement or tactile codes.
Musical scores are the most common way to encode music.
Specific games, as chess, have their own code systems to record the matches called chess notation.
Cryptography.
In the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead. See code (cryptography).
Secret codes intended to obscure the real messages, ranging from serious (mainly espionage in military, diplomatic, business, etc.) to trivial (romance, games) can be any kind of imaginative encoding: flowers, game cards, clothes, fans, hats, melodies, birds, etc., in which the sole requisite is the previous agreement of the meaning by both the sender and the receiver.
Other examples.
Other examples of encoding include:
Other examples of decoding include:
Codes and acronyms.
Acronyms and abbreviations can be considered codes, and in a sense all languages and writing systems are codes for human thought.
International Air Transport Association airport codes are three-letter codes used to designate airports and used for bag tags. Station codes are similarly used on railways, but are usually national, so the same code can be used for different stations if they are in different countries.
Occasionally a code word achieves an independent existence (and meaning) while the original equivalent phrase is forgotten or at least no longer has the precise meaning attributed to the code word. For example, '30' was widely used in journalism to mean "end of story", and has been used in other contexts to signify "the end".
References.
Univercal Text Decoder

</doc>
<doc id="5228" url="https://en.wikipedia.org/wiki?curid=5228" title="Cheirogaleidae">
Cheirogaleidae

The Cheirogaleidae are the family of strepsirrhine primates containing the various dwarf and mouse lemurs. Like all other lemurs, cheirogaleids live exclusively on the island of Madagascar.
Characteristics.
Cheirogaleids are smaller than the other lemurs and, in fact, they are the smallest primates. They have soft, long fur, colored grey-brown to reddish on top, with a generally brighter underbelly. Typically, they have small ears, large, close-set eyes, and long hind legs. Like all strepsirrhines, they have fine claws at the second toe of the hind legs. They grow to a size of only 13 to 28 cm, with a tail that is very long, sometimes up to one and a half times as long as the body. They weigh no more than 500 grams, with some species weighing as little as 60 grams.
Dwarf and mouse lemurs are nocturnal and arboreal. They are excellent climbers and can also jump far, using their long tails for balance. When on the ground (a rare occurrence), they move by hopping on their hind legs. They spend the day in tree hollows or leaf nests. Cheirogaleids are typically solitary, but sometimes live together in pairs.
Their eyes possess a tapetum lucidum, a light-reflecting layer that improves their night vision. Some species, such as the lesser dwarf lemur, store fat at the hind legs and the base of the tail, and hibernate. Unlike lemurids, they have long upper incisors, although they do have the comb-like teeth typical of all strepsirhines. They have the dental formula: 
Cheirogaleids are omnivores, eating fruits, flowers and leaves (and sometimes nectar), as well as insects, spiders, and small vertebrates.
The females usually have three pairs of nipples. After a meager 60-day gestation, they will bear two to four (usually two or three) young. After five to six weeks, the young are weaned and become fully mature near the end of their first year or sometime in their second year, depending on the species. In human care, they can live for up to 15 years, although their life expectancy in the wild is probably significantly shorter.
Classification.
The five genera of cheirogaleids contain 34 species.

</doc>
<doc id="5229" url="https://en.wikipedia.org/wiki?curid=5229" title="Callitrichidae">
Callitrichidae

The Callitrichidae (also called Arctopitheci or Hapalidae) are a family of New World monkeys, including marmosets and tamarins. At times, this group of animals has been regarded as a subfamily, called Callitrichinae, of the family Cebidae.
This taxon was traditionally thought to be a primitive lineage, from which all the larger-bodied platyrrhines evolved. However, some works argue that callitrichids are actually a dwarfed lineage.
Ancestral stem-callitrichids likely were "normal-sized" ceboids that were dwarfed through evolutionary time. This may exemplify a rare example of insular dwarfing in a mainland context, with the "islands" being formed by biogeographic barriers during arid climatic periods when forest distribution became patchy, and/or by the extensive river networks in the Amazon Basin.
All callitrichids are arboreal. They are the smallest of the simian primates. They eat insects, fruit, and the sap or gum from trees; occasionally they take small vertebrates. The marmosets rely quite heavily on tree exudates, with some species (e.g. "Callithrix jacchus" and "Cebuella pygmaea") considered obligate exudativores.
Callitrichids typically live in small, territorial groups of about five or six animals. Their social organization is unique among primates and is called a "cooperative polyandrous group". This communal breeding system involves groups of multiple males and females, but only one female is reproductively active. Females mate with more than one male and each shares the responsibility of carrying the offspring.
They are the only primate group that regularly produces twins, which constitute over 80% of births in species that have been studied. Unlike other male primates, male callitrichids generally provide as much parental care as females. Parental duties may include carrying, protecting, feeding, comforting, and even engaging in play behavior with offspring. In some cases, such as in the cotton-top tamarin ("Saguinus oedipus"), males, particularly those that are paternal, will even show a greater involvement in caregiving than females. The typical social structure seems to constitute a breeding group, with several of their previous offspring living in the group and providing significant help in rearing the young.

</doc>
<doc id="5230" url="https://en.wikipedia.org/wiki?curid=5230" title="Cebidae">
Cebidae

The Cebidae are one of the five families of New World monkeys now recognised. It includes the capuchin monkeys and squirrel monkeys. These species are found throughout tropical and subtropical South and Central America.
Characteristics.
Cebid monkeys are arboreal animals that only rarely travel on the ground. They are generally small monkeys, ranging in size up to that of the brown capuchin, with a body length of 33 to 56 cm, and a weight of 2.5 to 3.9 kilograms. They are somewhat variable in form and coloration, but all have the wide, flat, noses typical of New World monkeys. They are different from marmosets as they have additional molar tooth and a prehensile tail. 
They are omnivorous, mostly eating fruit and insects, although the proportions of these foods vary greatly between species. They have the dental formula:
Females give birth to one or two young after a gestation period of between 130 and 170 days, depending on species. They are social animals, living in groups of between five and forty individuals, with the smaller species typically forming larger groups. They are generally diurnal in habit.
Classification.
Previously, New World monkeys were divided between Callitrichidae and this family. For a few recent years, marmosets, tamarins, and lion tamarins were placed as a subfamily (Callitrichinae) in Cebidae, while moving other genera from Cebidae into the families Aotidae, Pitheciidae and Atelidae. The most recent classification of New World monkeys again splits the callitrichids off, leaving only the capuchins and squirrel monkeys in this family.

</doc>
<doc id="5232" url="https://en.wikipedia.org/wiki?curid=5232" title="Chondrichthyes">
Chondrichthyes

Chondrichthyes (; from Greek χονδρ- "chondr-" 'cartilage', ἰχθύς "ichthys" 'fish') is a class that contains the cartilaginous fishes: they are jawed vertebrates with paired fins, paired nares, scales, a heart with its chambers in series, and skeletons made of cartilage rather than bone. The class is divided into two subclasses: Elasmobranchii (sharks, rays, skates, and sawfish) and Holocephali (chimaeras, sometimes called ghost sharks, which are sometimes separated into their own class).
Within the infraphylum Gnathostomata, cartilaginous fishes are distinct from all other jawed vertebrates, the extant members of which all fall into Teleostomi.
Anatomy.
Skeleton.
The skeleton is cartilaginous. The notochord, which is present in the young, is gradually replaced by cartilage. Chondrichthyans also lack ribs, so if they leave water, the larger species' own body weight would crush their internal organs long before they would suffocate.
As they do not have bone marrow, red blood cells are produced in the spleen and the epigonal organ (special tissue around the gonads, which is also thought to play a role in the immune system). They are also produced in the Leydig's organ, which is only found in certain cartilaginous fishes. The subclass Holocephali, which is a very specialized group, lacks both the Leydig's and epigonal organs.
Appendages.
Apart from electric rays, which have a thick and flabby body, with soft, loose skin, chondrichthyans have tough skin covered with dermal teeth (again, Holocephali is an exception, as the teeth are lost in adults, only kept on the clasping organ seen on the caudal ventral surface of the male), also called placoid scales (or "dermal denticles"), making it feel like sandpaper. In most species, all dermal denticles are oriented in one direction, making the skin feel very smooth if rubbed in one direction and very rough if rubbed in the other.
Originally, the pectoral and pelvic girdles, which do not contain any dermal elements, did not connect. In later forms, each pair of fins became ventrally connected in the middle when scapulocoracoid and pubioischiadic bars evolved. In rays, the pectoral fins have connected to the head and are very flexible.
One of the primary characteristics present in most sharks is the heterocercal tail, which aids in locomotion.
Body covering.
Chondrichthyans have toothlike scales called dermal denticles or placoid scales. Denticles provide two functions, protection, and in most cases streamlining. Mucous glands exist in some species as well.
It is assumed that their oral teeth evolved from dermal denticles that migrated into the mouth, but it could be the other way around as the teleost bony fish "Denticeps clupeoides" has most of its head covered by dermal teeth (as does, probably, "Atherion elymus", another bony fish). This is most likely a secondary evolved characteristic, which means there is not necessarily a connection between the teeth and the original dermal scales.
The old placoderms did not have teeth at all, but had sharp bony plates in their mouth. Thus, it is unknown whether the dermal or oral teeth evolved first. Nor is it sure how many times it has happened if it turns out to be the case. It has even been suggested that the original bony plates of all the vertebrates are gone and that the present scales are just modified teeth, even if both teeth and the body armor have a common origin a long time ago. However, there is no evidence of this at the moment.
Respiratory system.
All chondrichthyans breathe through five to seven pairs of gills, depending on the species. In general, pelagic species must keep swimming to keep oxygenated water moving through their gills, whilst demersal species can actively pump water in through their spiracles and out through their gills. However, this is only a general rule and many species differ.
A spiracle is a small hole found behind each eye. These can be tiny and circular, such as found on the nurse shark ("Ginglymostoma cirratum"), to extended and slit-like, such as found on the wobbegongs (Orectolobidae). Many larger, pelagic species, such as the mackerel sharks (Lamnidae) and the thresher sharks (Alopiidae), no longer possess them.
Immune system.
Like all other jawed vertebrates, members of Chondrichthyes have an adaptive immune system.
Reproduction.
Fertilization is internal. Development is usually live birth (ovoviviparous species) but can be through eggs (oviparous). Some rare species are viviparous. There is no parental care after birth; however, some chondrichthyans do guard their eggs.
Classification.
The class Chondrichthyes has two subclasses: the subclass Elasmobranchii (sharks, rays, skates, and sawfish) and the subclass Holocephali (chimaeras). To see the full list of the species, click here.
Evolution.
Cartilaginous fish are considered to have evolved from acanthodians. Originally assumed to be closely related to bony fish or a polyphyletic assemblage leading to both groups, the discovery of "Entelognathus" and several examinations of acanthodian characteristics indicate that bony fish evolved directly from placoderm like ancestors, while acanthodians represent a paraphyletic assemblage leading to Chondrichthyes. Some characteristics previously thought to be exclusive to acanthodians are also present in basal cartilaginous fish.
Unequivocal fossils of cartilaginous fishes first appeared in the fossil record by about 395 million years ago, during the middle Devonian. The radiation of elasmobranches in the chart on the right is divided into the taxa: Cladoselache, Eugeneodontiformes, Symmoriida, Xenacanthiformes, Ctenacanthiformes, Hybodontiformes, Galeomorphi, Squaliformes and Batoidea.
By the start of the Early Devonian, 419 mya (million years ago), jawed fishes had divided into four distinct clades: the placoderms and spiny sharks, both of which are now extinct, and the cartilaginous and bony fishes, both of which are still extant. The modern bony fishes, class Osteichthyes, appeared in the late Silurian or early Devonian, about 416 million years ago. Spiny sharks are not classified as true sharks or as cartilaginous fishes, but as a distinct group, class Acanthodii. However, both the cartilaginous and bony fishes may have arisen from either the placoderms or the spiny sharks. Cartilaginous fishes first appeared about 395 Ma. The first abundant genus of shark, "Cladoselache", appeared in the oceans during the Devonian Period.
A Bayesian analysis of molecular data suggests that the Holocephali and Elasmoblanchii diverged in the Silurian () and that the sharks and rays/skates split in the Carboniferous ().
Phylogeny.
 

</doc>
<doc id="5233" url="https://en.wikipedia.org/wiki?curid=5233" title="Carl Linnaeus">
Carl Linnaeus

Carl Linnaeus (; 23 May 1707 – 10 January 1778), also known after his ennoblement as Carl von Linné (), was a Swedish botanist, physician, and zoologist, who formalised the modern system of naming organisms called binomial nomenclature. He is known by the epithet "father of modern taxonomy". Many of his writings were in Latin, and his name is rendered in Latin as (after 1761 Carolus a Linné).
Linnaeus was born in the countryside of Småland, in southern Sweden. He received most of his higher education at Uppsala University, and began giving lectures in botany there in 1730. He lived abroad between 1735 and 1738, where he studied and also published a first edition of his "" in the Netherlands. He then returned to Sweden, where he became professor of medicine and botany at Uppsala. In the 1740s, he was sent on several journeys through Sweden to find and classify plants and animals. In the 1750s and 1760s, he continued to collect and classify animals, plants, and minerals, and published several volumes. At the time of his death, he was one of the most acclaimed scientists in Europe.
The Swiss philosopher Jean-Jacques Rousseau sent him the message: "Tell him I know no greater man on earth." The German writer Johann Wolfgang von Goethe wrote: "With the exception of Shakespeare and Spinoza, I know no one among the no longer living who has influenced me more strongly." Swedish author August Strindberg wrote: "Linnaeus was in reality a poet who happened to become a naturalist". Among other compliments, Linnaeus has been called "" (Prince of Botanists), "The Pliny of the North," and "The Second Adam". He is also considered as one of the founders of modern ecology.
In botany, the author abbreviation used to indicate Linnaeus as the authority for species' names is L. In older publications, sometimes the abbreviation "Linn." is found (for instance in: ). Linnaeus' remains comprise the type specimen for the species "Homo sapiens", following the International Code of Zoological Nomenclature, since the sole specimen he is known to have examined when writing the species description was himself.
Biography.
Childhood.
Linnæus was born in the village of Råshult in Småland, Sweden, on 23 May 1707. He was the first child of Nicolaus (Nils) Ingemarsson (who later adopted the family name Linnæus) and Christina Brodersonia. His siblings were Anna Maria Linnæa, Sofia Juliana Linnæa, Samuel Linnæus, and Emerentia Linnæa.
One of a long line of peasants and priests, Nils was an amateur botanist, a Lutheran minister, and the curate of the small village of Stenbrohult in Småland. Christina was the daughter of the rector of Stenbrohult, Samuel Brodersonius.
A year after Linnæus' birth, his grandfather Samuel Brodersonius died, and his father Nils became the rector of Stenbrohult. The family moved into the rectory from the curate's house. Carl had one brother, Samuel (who would eventually succeed their father as rector of Stenbrohult and write a manual on beekeeping). and three sisters.
Even in his early years, Linnæus seemed to have a liking for plants, flowers in particular. Whenever he was upset, he was given a flower, which immediately calmed him. Nils spent much time in his garden and often showed flowers to Linnaeus and told him their names. Soon Linnæus was given his own patch of earth where he could grow plants.
Carl's father was the first in his ancestry to adopt a permanent surname. Before that, ancestors had used the patronymic naming system of Scandinavian countries: his father was named Ingemarsson after his father Ingemar Bengtsson. When Nils was admitted to the University of Lund, he had to take on a family name. He adopted the Latinate name Linnæus after a giant linden tree (or lime tree), "" in Swedish, that grew on the family homestead. This name was spelled with the æ ligature. When Carl was born, he was named Carl Linnæus, with his father's family name. The son also always spelled it with the æ ligature, both in handwritten documents and in publications. Carl's patronymic would have been Nilsson, as in Carl Nilsson Linnæus.
Early education.
Linnaeus' father began teaching him basic Latin, religion, and geography at an early age. When Linnaeus was seven, Nils decided to hire a tutor for him. The parents picked Johan Telander, a son of a local yeoman. Linnaeus did not like him, writing in his autobiography that Telander "was better calculated to extinguish a child's talents than develop them."
Two years after his tutoring had begun, he was sent to the Lower Grammar School at Växjö in 1717. Linnaeus rarely studied, often going to the countryside to look for plants. He reached the last year of the Lower School when he was fifteen, which was taught by the headmaster, Daniel Lannerus, who was interested in botany. Lannerus noticed Linnaeus' interest in botany and gave him the run of his garden.
He also introduced him to Johan Rothman, the state doctor of Småland and a teacher at Katedralskolan (a gymnasium) in Växjö. Also a botanist, Rothman broadened Linnaeus' interest in botany and helped him develop an interest in medicine. By the age of 17, Linnaeus had become well acquainted with the existing botanical literature. He remarks in his journal "read day and night, knowing like the back of my hand, Arvidh Månsson's Rydaholm Book of Herbs, Tillandz's Flora Åboensis, Palmberg's Serta Florea Suecana, Bromelii Chloros Gothica and Rudbeckii Hortus Upsaliensis..." 
Linnaeus entered the Växjö Katedralskola in 1724, where he studied mainly Greek, Hebrew, theology and mathematics, a curriculum designed for boys preparing for the priesthood. In the last year at the gymnasium, Linnaeus' father visited to ask the professors how his son's studies were progressing; to his dismay, most said that the boy would never become a scholar. Rothman believed otherwise, suggesting Linnaeus could have a future in medicine. The doctor offered to have Linnaeus live with his family in Växjö and to teach him physiology and botany. Nils accepted this offer.
University studies.
Lund.
Rothman showed Linnaeus that botany was a serious subject. He taught Linnaeus to classify plants according to Tournefort's system. Linnaeus was also taught about the sexual reproduction of plants, according to Sébastien Vaillant. In 1727, Linnaeus, age 21, enrolled in Lund University in Skåne. He was registered as "", the Latin form of his full name, which he also used later for his Latin publications.
Professor Kilian Stobæus, natural scientist, physician and historian, offered Linnaeus tutoring and lodging, as well as the use of his library, which included many books about botany. He also gave the student free admission to his lectures. In his spare time, Linnaeus explored the flora of Skåne, together with students sharing the same interests.
Uppsala.
In August 1728, Linnaeus decided to attend Uppsala University on the advice of Rothman, who believed it would be a better choice if Linnaeus wanted to study both medicine and botany. Rothman based this recommendation on the two professors who taught at the medical faculty at Uppsala: Olof Rudbeck the Younger and Lars Roberg. Although Rudbeck and Roberg had undoubtedly been good professors, by then they were older and not so interested in teaching. Rudbeck no longer gave public lectures, and had others stand in for him. The botany, zoology, pharmacology and anatomy lectures were not in their best state. In Uppsala, Linnaeus met a new benefactor, Olof Celsius, who was a professor of theology and an amateur botanist. He received Linnaeus into his home and allowed him use of his library, which was one of the richest botanical libraries in Sweden.
In 1729, Linnaeus wrote a thesis, ' on plant sexual reproduction. This attracted the attention of Rudbeck; in May 1730, he selected Linnaeus to give lectures at the University although the young man was only a second-year student. His lectures were popular, and Linnaeus often addressed an audience of 300 people. In June, Linnaeus moved from Celsius' house to Rudbeck's to become the tutor of the three youngest of his 24 children. His friendship with Celsius did not wane and they continued their botanical expeditions. Over that winter, Linnaeus began to doubt Tournefort's system of classification and decided to create one of his own. His plan was to divide the plants by the number of stamens and pistils. He began writing several books, which would later result in, for example, ' and '. He also produced a book on the plants grown in the Uppsala Botanical Garden, '.
Rudbeck's former assistant, Nils Rosén, returned to the University in March 1731 with a degree in medicine. Rosén started giving anatomy lectures and tried to take over Linnaeus' botany lectures, but Rudbeck prevented that. Until December, Rosén gave Linnaeus private tutoring in medicine. In December, Linnaeus had a "disagreement" with Rudbeck's wife and had to move out of his mentor's house; his relationship with Rudbeck did not appear to suffer. That Christmas, Linnaeus returned home to Stenbrohult to visit his parents for the first time in about three years. His mother had disapproved of his failing to become a priest, but she was pleased to learn he was teaching at the University.
Expedition to Lapland.
During a visit with his parents, Linnaeus told them about his plan to travel to Lapland; Rudbeck had made the journey in 1695, but the detailed results of his exploration were lost in a fire seven years afterwards. Linnaeus' hope was to find new plants, animals and possibly valuable minerals. He was also curious about the customs of the native Sami people, reindeer-herding nomads who wandered Scandinavia's vast tundras. In April 1732, Linnaeus was awarded a grant from the Royal Society of Sciences in Uppsala for his journey.
Linnaeus began his expedition from Uppsala on May 12, 1732, just before he turned 25. He travelled on foot and horse, bringing with him his journal, botanical and ornithological manuscripts and sheets of paper for pressing plants. Near Gävle he found great quantities of "Campanula serpyllifolia", later known as "Linnaea borealis", the twinflower that would become his favourite. He sometimes dismounted on the way to examine a flower or rock and was particularly interested in mosses and lichens, the latter a main part of the diet of the reindeer, a common and economically important animal in Lapland.
Linnaeus travelled clockwise around the coast of the Gulf of Bothnia, making major inland incursions from Umeå, Luleå and Tornio. He returned from his six-month-long, over expedition in October, having gathered and observed many plants, birds and rocks. Although Lapland was a region with limited biodiversity, Linnaeus described about 100 previously unidentified plants. These became the basis of his book "". However, on the expedition to Lapland, Linnaeus used Latin names to describe organisms because he had not yet developed the binomial system.
In ' Linnaeus' ideas about nomenclature and classification were first used in a practical way, making this the first proto-modern Flora. The account covered 534 species, used the Linnaean classification system and included, for the described species, geographical distribution and taxonomic notes. It was Augustin Pyramus de Candolle who attributed Linnaeus with ' as the first example in the botanical genre of Flora writing. Botanical historian E. L. Greene described "" as "the most classic and delightful" of Linnaeus's works.
It was also during this expedition that Linnaeus had a flash of insight regarding the classification of mammals. Upon observing the lower jawbone of a horse at the side of a road he was traveling, Linnaeus remarked: "If I only knew how many teeth and of what kind every animal had, how many teats and where they were placed, I should perhaps be able to work out a perfectly natural system for the arrangement of all quadrupeds."
In 1734, Linnaeus led a small group of students to Dalarna. Funded by the Governor of Dalarna, the expedition was to catalogue known natural resources and discover new ones, but also to gather intelligence on Norwegian mining activities at Røros.
Doctorate.
Back in Uppsala, Linnaeus' relations with Nils Rosén worsened, and thus he gladly accepted an invitation from the student Claes Sohlberg to spend the Christmas holiday in Falun with Sohlberg's family. Sohlberg's father was a mining inspector, and let Linnaeus visit the mines near Falun. Sohlberg's father suggested to Linnaeus he should bring Sohlberg to the Dutch Republic and continue to tutor him there for an annual salary. At that time, the Dutch Republic was one of the most revered places to study natural history and a common place for Swedes to take their doctoral degree; Linnaeus, who was interested in both of these, accepted.
In April 1735, Linnaeus and Sohlberg set out for the Netherlands, with Linnaeus to take a doctoral degree in medicine at the University of Harderwijk. On the way, they stopped in Hamburg, where they met the mayor, who proudly showed them a wonder of nature which he possessed: the taxidermied remains of a seven-headed hydra. Linnaeus quickly discovered it was a fake: jaws and clawed feet from weasels and skins from snakes had been glued together. The provenance of the hydra suggested to Linnaeus it had been manufactured by monks to represent the Beast of Revelation. As much as this may have upset the mayor, Linnaeus made his observations public and the mayor's dreams of selling the hydra for an enormous sum were ruined. Fearing his wrath, Linnaeus and Sohlberg had to leave Hamburg quickly.
When Linnaeus reached Harderwijk, he began working toward a degree immediately; at the time, Harderwijk was known for awarding "instant" degrees after as little as a week. First he handed in a thesis on the cause of malaria he had written in Sweden, which he then defended in a public debate. His dissertation, submitted on 23 June, was titled "Dissertatio medica inauguralis in qua exhibetur hypothesis nova de febrium intermittentium causa" (""Inaugural thesis in medicine, in which a new hypothesis on the cause of intermittent fevers is presented""). He concluded that malaria arose only in places with clay-rich soil. He is now known to have been wrong about the cause, not having a microscope good enough to see malarial parasites, which were spread by mosquitoes breeding in the water that collected in ruts and puddles. But he was right in predicting that traditional Chinese medicine, including the use of wormwood ("Artemisia"), is a potential source of antimalarial drug. (Artemisinins, derived from wormwood, are now the principal antimalarial drugs.)
The next step was to take an oral examination and to diagnose a patient. After less than two weeks, he took his degree and became a doctor, at the age of 28. During the summer, Linnaeus met a friend from Uppsala, Peter Artedi. Before their departure from Uppsala, Artedi and Linnaeus had decided should one of them die, the survivor would finish the other's work. Ten weeks later, Artedi drowned in one of the canals of Amsterdam, and his unfinished manuscript on the classification of fish was left to Linnaeus to complete.
Publishing of "".
One of the first scientists Linnaeus met in the Netherlands was Johan Frederik Gronovius to whom Linnaeus showed one of the several manuscripts he had brought with him from Sweden. The manuscript described a new system for classifying plants. When Gronovius saw it, he was very impressed, and offered to help pay for the printing. With an additional monetary contribution by the Scottish doctor Isaac Lawson, the manuscript was published as "" (1735).
Linnaeus became acquainted with one of the most respected physicians and botanists in the Netherlands, Herman Boerhaave, who tried to convince Linnaeus to make a career there. Boerhaave offered him a journey to South Africa and America, but Linnaeus declined, stating he would not stand the heat. Instead, Boerhaave convinced Linnaeus that he should visit the botanist Johannes Burman. After his visit, Burman, impressed with his guest's knowledge, decided Linnaeus should stay with him during the winter. During his stay, Linnaeus helped Burman with his '. Burman also helped Linnaeus with the books on which he was working: ' and "".
George Clifford, Philip Miller, and Johann Jacob Dillenius.
In August, during Linnaeus' stay with Burman, he met George Clifford III, a director of the Dutch East India Company and the owner of a rich botanical garden at the estate of Hartekamp in Heemstede. Clifford was very impressed with Linnaeus' ability to classify plants, and invited him to become his physician and superintendent of his garden. Linnaeus had already agreed to stay with Burman over the winter, and could thus not accept immediately. However, Clifford offered to compensate Burman by offering him a copy of Sir Hans Sloane's "Natural History of Jamaica", a rare book, if he let Linnaeus stay with him, and Burman accepted. On 24 September 1735, Linnaeus moved to Hartekamp to become personal physician to Clifford, and curator of Clifford's herbarium. He was paid 1,000 florins a year, with free board and lodging. Though the agreement was only for a winter of that year, Linnaeus practically stayed there till 1738. It was here that he wrote a book "Hortus Cliffortianus", in the preface of which he described his experience as "the happiest time of my life." (A portion of Hartekamp was declared as public garden in April 1956 by the Heemstede local authority, and was named "Linnaeushof". It eventually became, as it is claimed, the biggest playground in Europe.)
In July 1736, Linnaeus travelled to England, at Clifford's expense. He went to London to visit Sir Hans Sloane, a collector of natural history, and to see his cabinet, as well as to visit the Chelsea Physic Garden and its keeper, Philip Miller. He taught Miller about his new system of subdividing plants, as described in "". Miller was in fact reluctant to use the new binomial nomenclature, preferring the classifications of Joseph Pitton de Tournefort and John Ray at first. Linnaeus, nevertheless, applauded Miller's "Gardeners Dictionary", The conservative Scot actually retained in his dictionary a number of pre-Linnaean binomial signifiers discarded by Linnaeus but which have been retained by modern botanists. He only fully changed to the Linnaean system in the edition of "The Gardeners Dictionary" of 1768. Miller ultimately was impressed, and from then on started to arrange the garden according to Linnaeus' system.
Linnaeus also traveled to Oxford University to visit the botanist Johann Jacob Dillenius. He failed to make Dillenius publicly fully accept his new classification system, though the two men remained in correspondence for many years afterwards. Linnaeus dedicated his Critica botanica to him, as opus botanicum quo absolutius mundus non vidit. Linnaeus would later name a genus of tropical tree Dillenia in his honor. He then returned to Hartekamp, bringing with him many specimens of rare plants. The next year, he published ', in which he described 935 genera of plants, and shortly thereafter he supplemented it with ', with another sixty ("sexaginta") genera.
His work at Hartekamp led to another book, "", a catalogue of the botanical holdings in the herbarium and botanical garden of Hartekamp. He wrote it in nine months (completed in July 1737), but it was not published until 1738. It contains the first use of the name "Nepenthes", which Linnaeus used to describe a genus of pitcher plants.
Linnaeus stayed with Clifford at Hartekamp until 18 October 1737 (new style), when he left the house to return to Sweden. Illness and the kindness of Dutch friends obliged him to stay some months longer in Holland. In May 1738, he set out for Sweden again. On the way home, he stayed in Paris for about a month, visiting botanists such as Antoine de Jussieu. After his return, Linnaeus never left Sweden again.
Return to Sweden.
When Linnaeus returned to Sweden on 28 June 1738, he went to Falun, where he entered into an engagement to Sara Elisabeth Moræa. Three months later, he moved to Stockholm to find employment as a physician, and thus to make it possible to support a family. Once again, Linnaeus found a patron; he became acquainted with Count Carl Gustav Tessin, who helped him get work as a physician at the Admiralty. During this time in Stockholm, Linnaeus helped found the Royal Swedish Academy of Science; he became the first Praeses in the academy by drawing of lots.
Because his finances had improved and were now sufficient to support a family, he received permission to marry his fiancée, Sara Elisabeth Moræa. Their wedding was held 26 June 1739. Seven months later, Sara gave birth to their first son, Carl. Two years later, a daughter, Elisabeth Christina, was born, and the subsequent year Sara gave birth to Sara Magdalena, who died when 15 days old. Sara and Linnaeus would later have four other children: Lovisa, Sara Christina, Johannes and Sophia.
In May 1741, Linnaeus was appointed Professor of Medicine at Uppsala University, first with responsibility for medicine-related matters. Soon, he changed place with the other Professor of Medicine, Nils Rosén, and thus was responsible for the Botanical Garden (which he would thoroughly reconstruct and expand), botany and natural history, instead. In October that same year, his wife and nine-year-old son followed him to live in Uppsala.
Öland and Gotland.
Ten days after he was appointed Professor, he undertook an expedition to the island provinces of Öland and Gotland with six students from the university, to look for plants useful in medicine. First, they travelled to Öland and stayed there until 21 June, when they sailed to Visby in Gotland. Linnaeus and the students stayed on Gotland for about a month, and then returned to Uppsala. During this expedition, they found 100 previously unrecorded plants. The observations from the expedition were later published in ', written in Swedish. Like ', it contained both zoological and botanical observations, as well as observations concerning the culture in Öland and Gotland.
During the summer of 1745, Linnaeus published two more books: ' and '. ' was a strictly botanical book, while ' was zoological. Anders Celsius had created the temperature scale named after him in 1742. Celsius' scale was inverted compared to today, the boiling point at 0 °C and freezing point at 100 °C. In 1745, Linnaeus inverted the scale to its present standard.
Västergötland.
In the summer of 1746, Linnaeus was once again commissioned by the Government to carry out an expedition, this time to the Swedish province of Västergötland. He set out from Uppsala on 12 June and returned on 11 August. On the expedition his primary companion was Erik Gustaf Lidbeck, a student who had accompanied him on his previous journey. Linnaeus described his findings from the expedition in the book "", published the next year. After returning from the journey the Government decided Linnaeus should take on another expedition to the southernmost province Scania. This journey was postponed, as Linnaeus felt too busy.
In 1747, Linnaeus was given the title archiater, or chief physician, by the Swedish king Adolf Frederick—a mark of great respect. The same year he was elected member of the Academy of Sciences in Berlin.
Scania.
In the spring of 1749, Linnaeus could finally journey to Scania, again commissioned by the Government. With him he brought his student, Olof Söderberg. On the way to Scania, he made his last visit to his brothers and sisters in Stenbrohult since his father had died the previous year. The expedition was similar to the previous journeys in most aspects, but this time he was also ordered to find the best place to grow walnut and Swedish whitebeam trees; these trees were used by the military to make rifles. The journey was successful, and Linnaeus' observations were published the next year in "".
Rector of Uppsala University.
In 1750, Linnaeus became rector of Uppsala University, starting a period where natural sciences were esteemed. Perhaps the most important contribution he made during his time at Uppsala was to teach; many of his students travelled to various places in the world to collect botanical samples. Linnaeus called the best of these students his "apostles". His lectures were normally very popular and were often held in the Botanical Garden. He tried to teach the students to think for themselves and not trust anybody, not even him. Even more popular than the lectures were the botanical excursions made every Saturday during summer, where Linnaeus and his students explored the flora and fauna in the vicinity of Uppsala.
"Philosophia Botanica".
Linnaeus published "Philosophia Botanica" in 1751. The book contained a complete survey of the taxonomy system he had been using in his earlier works. It also contained information of how to keep a journal on travels and how to maintain a botanical garden.
"Nutrix Noverca".
During Linnaeus' time it was normal for upper class women to have wet nurses for their babies. Linnaeus joined an ongoing campaign to end this practice in Sweden and promote breast-feeding by mothers. In 1752 Linnaeus published a thesis along with Frederick Lindberg, a physician student, based on their experiences. In the tradition of the period, this dissertation was essentially an idea of the presiding reviewer ("prases") expounded upon by the student. Linnaeus' dissertation was translated into French by J.E. Gilibert in 1770 as "La Nourrice marâtre, ou Dissertation sur les suites funestes du nourrisage mercénaire". Linnaeus suggested that children might absorb the personality of their wet nurse through the milk. He admired the child care practices of the Lapps and pointed out how healthy their babies were compared to those of Europeans who employed wet nurses. He compared the behaviour of wild animals and pointed out how none of them denied their newborns their breastmilk. It is thought that his activism played a role in his choice of the term "Mammalia" for the class of organisms.
"Species Plantarum".
Linnaeus published "Species Plantarum", the work which is now internationally accepted as the starting point of modern botanical nomenclature, in 1753. The first volume was issued on 24 May, the second volume followed on 16 August of the same year. The book contained 1,200 pages and was published in two volumes; it described over 7,300 species. The same year the king dubbed him knight of the Order of the Polar Star, the first civilian in Sweden to become a knight in this order. He was then seldom seen not wearing the order.
Ennoblement.
Linnaeus felt Uppsala was too noisy and unhealthy, so he bought two farms in 1758: Hammarby and Sävja. The next year, he bought a neighbouring farm, Edeby. He spent the summers with his family at Hammarby; initially it only had a small one-storey house, but in 1762 a new, larger main building was added. In Hammarby, Linnaeus made a garden where he could grow plants that could not be grown in the Botanical Garden in Uppsala. He began constructing a museum on a hill behind Hammarby in 1766, where he moved his library and collection of plants. A fire that destroyed about one third of Uppsala and had threatened his residence there necessitated the move.
Since the initial release of ' in 1735, the book had been expanded and reprinted several times; the tenth edition was released in 1758. This edition established itself as the starting point for zoological nomenclature, the equivalent of '.
The Swedish king Adolf Frederick granted Linnaeus nobility in 1757, but he was not ennobled until 1761. With his ennoblement, he took the name Carl von Linné (Latinized as ""), 'Linné' being a shortened and gallicised version of 'Linnæus', and the German nobiliary particle 'von' signifying his ennoblement. The noble family's coat of arms prominently features a twinflower, one of Linnaeus' favourite plants; it was given the scientific name "Linnaea borealis" in his honour by Gronovius. The shield in the coat of arms is divided into thirds: red, black and green for the three kingdoms of nature (animal, mineral and vegetable) in Linnaean classification; in the centre is an egg "to denote Nature, which is continued and perpetuated "in ovo"." At the bottom is a phrase in Latin, borrowed from the Aeneid, which reads "Famam extendere factis": we extend our fame by our deeds. Linnaeus inscribed this personal motto in books that were gifted to him by friends.
After his ennoblement, Linnaeus continued teaching and writing. His reputation had spread over the world, and he corresponded with many different people. For example, Catherine II of Russia sent him seeds from her country. He also corresponded with Giovanni Antonio Scopoli, "the Linnaeus of the Austrian Empire", who was a doctor and a botanist in Idrija, Duchy of Carniola (nowadays Slovenia). Scopoli communicated all of his research, findings, and descriptions (for example of the olm and the dormouse, two little animals hitherto unknown to Linnaeus). Linnaeus greatly respected him and showed great interest in his work. He named a solanaceous genus, "Scopolia", the source of scopolamine, after him. Because of a great distance, they never met.
Final years.
Linnaeus was relieved of his duties in the Royal Swedish Academy of Science in 1763, but continued his work there as usual for more than ten years after. He stepped down as rector at Uppsala University in December 1772, mostly due to his declining health.
Linnaeus' last years were troubled by illness. He had suffered from a disease called the Uppsala fever in 1764, but survived thanks to the care of Rosén. He developed sciatica in 1773, and the next year, he had a stroke which partially paralysed him. He suffered a second stroke in 1776, losing the use of his right side and leaving him bereft of his memory; while still able to admire his own writings, he could not recognise himself as their author.
In December 1777, he had another stroke which greatly weakened him, and eventually led to his death on 10 January 1778 in Hammarby. Despite his desire to be buried in Hammarby, he was buried in Uppsala Cathedral on 22 January.
His library and collections were left to his widow Sara and their children. Joseph Banks, an English botanist, wanted to buy the collection, but his son Carl refused and moved the collection to Uppsala. However, in 1783 Carl died and Sara inherited the collection, having outlived both her husband and son. She tried to sell it to Banks, but he was no longer interested; instead an acquaintance of his agreed to buy the collection. The acquaintance was a 24-year-old medical student, James Edward Smith, who bought the whole collection: 14,000 plants, 3,198 insects, 1,564 shells, about 3,000 letters and 1,600 books. Smith founded the Linnean Society of London five years later.
The von Linné name ended with his son Carl, who never married. His other son, Johannes, had died aged 3. There are over two hundred descendants of Linnaeus through two of his daughters.
Apostles.
During Linnaeus' time as Professor and Rector of Uppsala University, he taught many devoted students, 17 of whom he called "apostles". They were the most promising, most committed students, and all of them made botanical expeditions to various places in the world, often with his help. The amount of this help varied; sometimes he used his influence as Rector to grant his apostles a scholarship or a place on an expedition. To most of the apostles he gave instructions of what to look for on their journeys. Abroad, the apostles collected and organised new plants, animals and minerals according to Linnaeus' system. Most of them also gave some of their collection to Linnaeus when their journey was finished. Thanks to these students, the Linnaean system of taxonomy spread through the world without Linnaeus ever having to travel outside Sweden after his return from Holland. The British botanist William T. Stearn notes without Linnaeus' new system, it would not have been possible for the apostles to collect and organise so many new specimens. Many of the apostles died during their expeditions.
Early expeditions.
Christopher Tärnström, the first apostle and a 43-year-old pastor with a wife and children, made his journey in 1746. He boarded a Swedish East India Company ship headed for China. Tärnström never reached his destination, dying of a tropical fever on Côn Sơn Island the same year. Tärnström's widow blamed Linnaeus for making her children fatherless, causing Linnaeus to prefer sending out younger, unmarried students after Tärnström. Six other apostles later died on their expeditions, including Pehr Forsskål and Pehr Löfling.
Two years after Tärnström's expedition, Finnish-born Pehr Kalm set out as the second apostle to North America. There he spent two-and-a-half years studying the flora and fauna of Pennsylvania, New York, New Jersey and Canada. Linnaeus was overjoyed when Kalm returned, bringing back with him many pressed flowers and seeds. At least 90 of the 700 North American species described in "Species Plantarum" had been brought back by Kalm.
Cook expeditions and Japan.
Daniel Solander was living in Linnaeus' house during his time as a student in Uppsala. Linnaeus was very fond of him, promising Solander his oldest daughter's hand in marriage. On Linnaeus' recommendation, Solander travelled to England in 1760, where he met the English botanist Joseph Banks. With Banks, Solander joined James Cook on his expedition to Oceania on the "Endeavour" in 1768–71. Solander was not the only apostle to journey with James Cook; Anders Sparrman followed on the "Resolution" in 1772–75 bound for, among other places, Oceania and South America. Sparrman made many other expeditions, one of them to South Africa.
Perhaps the most famous and successful apostle was Carl Peter Thunberg, who embarked on a nine-year expedition in 1770. He stayed in South Africa for three years, then travelled to Japan. All foreigners in Japan were forced to stay on the island of Dejima outside Nagasaki, so it was thus hard for Thunberg to study the flora. He did, however, manage to persuade some of the translators to bring him different plants, and he also found plants in the gardens of Dejima. He returned to Sweden in 1779, one year after Linnaeus' death.
Major publications.
"Systema Naturae".
The first edition of ' was printed in the Netherlands in 1735. It was a twelve-page work. By the time it reached its 10th edition in 1758, it classified 4,400 species of animals and 7,700 species of plants. In it, the unwieldy names mostly used at the time, such as "'", were supplemented with concise and now familiar "binomials", composed of the generic name, followed by a specific epithet – in the case given, "Physalis angulata". These binomials could serve as a label to refer to the species. Higher taxa were constructed and arranged in a simple and orderly manner. Although the system, now known as binomial nomenclature, was partially developed by the Bauhin brothers (see Gaspard Bauhin and Johann Bauhin) almost 200 years earlier, Linnaeus was the first to use it consistently throughout the work, including in monospecific genera, and may be said to have popularised it within the scientific community.
After the decline in Linnaeus' health in the early 1770s, publication of editions of "Systema Naturae" went in two different directions. Another Swedish scientist, Johan Andreas Murray issued the "Regnum Vegetabile" section separately in 1774 as the "Systema Vegetabilium", rather confusingly labelled the 13th edition. Meanwhile a 13th edition of the entire "Systema" appeared in parts between 1788 and 1793. It was through the "Systema Vegetabilium" that Linnaeus' work became widely known in England, following its translation from the Latin by the Lichfield Botanical Society as "A System of Vegetables" (1783–1785).
' (or, more fully, ') was first published in 1753, as a two-volume work. Its prime importance is perhaps that it is the primary starting point of plant nomenclature as it exists today.
"" was first published in 1737, delineating plant genera. Around 10 editions were published, not all of them by Linnaeus himself; the most important is the 1754 fifth edition. In it Linnaeus divided the plant Kingdom into 24 classes. One, Cryptogamia, included all the plants with concealed reproductive parts (algae, fungi, mosses and liverworts and ferns).
' (1751) was a summary of Linnaeus' thinking on plant classification and nomenclature, and an elaboration of the work he had previously published in ' (1736) and ' (1737). Other publications forming part of his plan to reform the foundations of botany include his ' and ': all were printed in Holland (as well as ' (1737) and "" (1735)), the "Philosophia" being simultaneously released in Stockholm.
Linnaean collections.
At the end of his lifetime the Linnean collection in Uppsala was considered one of the finest collections of natural history objects in Sweden. Next to his own collection he had also built up a museum for the university of Uppsala, which was supplied by material donated by Carl Gyllenborg (in 1744–1745), crown-prince Adolf Fredrik (in 1745), Erik Petreus (in 1746), Claes Grill (in 1746), Magnus Lagerström (in 1748 and 1750) and Jonas Alströmer (in 1749). The relation between the museum and the private collection was not formalized and the steady flow of material from Linnean pupils were incorporated to the private collection rather than to the museum. Linnaeus felt his work was reflecting the harmony of nature and he said in 1754 'the earth is then nothing else but a museum of the all-wise creator's masterpieces, divided into three chambers'. He had turned his own estate into a microcosm of that 'world museum'.
In April 1766 parts of the town were destroyed by a fire and the Linnean private collection was subsequently moved to a barn outside the town, and shortly afterwards to a single-room stone building close to his countryhouse at Hammarby near Uppsala. This resulted in a physical separation between the two collections, the museum collection remained in the botanical garden of the university. Some material which needed special care (alcohol specimens) or ample storage space was moved from the private collection to the museum.
In Hammarby the Linnean private collections suffered seriously from damp and the depredations by mice and insects. Carl von Linné's son (Carl Linnaeus) inherited the collections in 1778 and retained them until his own death in 1783. Shortly after Carl von Linné's death his son confirmed that mice had caused "horrible damage" to the plants and that also moths and mould had caused considerable damage. He tried to rescue them from the neglect they had suffered during his father's later years, and also added further specimens. This last activity however reduced rather than augmented the scientific value of the original material.
In 1784 the botanist James Edward Smith purchased nearly all of the Linnean private scientific effects from the widow and daughter of Carl Linnaeus and transferred them to London. Not all material in Linné's private collection was transported to England. Thirty-three fish specimens preserved in alcohol were not sent and were later lost.
In London Smith tended to neglect the zoological parts of the collection, he added some specimens and also gave some specimens away. Over the following centuries the Linnean collection in London suffered enormously at the hands of scientists who studied the collection, and in the process disturbed the original arrangement and labels, added specimens that did not belong to the original series and withdrew precious original type material.
Much material which had been intensively studied by Linné in his scientific career belonged to the collection of Queen Lovisa Ulrika (1720–1782) (in the Linnean publications referred to as "Museum Ludovicae Ulricae" or "M. L. U."). This collection was donated by his grandson King Gustav IV Adolf (1778–1837) to the museum in Uppsala in 1804. Another important collection in this respect was that of her husband King Adolf Fredrik (1710–1771) (in the Linnean sources known as "Museum Adolphi Friderici" or "Mus. Ad. Fr."), the wet parts (alcohol collection) of which were later donated to the Royal Swedish Academy of Sciences, and is today housed in the Swedish Museum of Natural History at Stockholm. The dry material was transferred to Uppsala.
Linnaean taxonomy.
The establishment of universally accepted conventions for the naming of organisms was Linnaeus' main contribution to taxonomy—his work marks the starting point of consistent use of binomial nomenclature. During the 18th century expansion of natural history knowledge, Linnaeus also developed what became known as the "Linnaean taxonomy"; the system of scientific classification now widely used in the biological sciences. A previous zoologist Rumphius (1627-1702) had more or less approximated the Linnaean system and his material contributed to the later development of the binomial scientific classification by Linnaeus. 
The Linnaean system classified nature within a nested hierarchy, starting with three kingdoms. Kingdoms were divided into classes and they, in turn, into orders, and thence into genera ("singular:" genus), which were divided into Species ("singular:" species). Below the rank of species he sometimes recognized taxa of a lower (unnamed) rank; these have since acquired standardised names such as "variety" in botany and "subspecies" in zoology. Modern taxonomy includes a rank of family between order and genus and a rank of phylum between kingdom and class that were not present in Linnaeus' original system.
Linnaeus' groupings were based upon shared physical characteristics, and not simply upon differences. Of his higher groupings, only those for animals are still in use, and the groupings themselves have been significantly changed since their conception, as have the principles behind them. Nevertheless, Linnaeus is credited with establishing the idea of a hierarchical structure of classification which is based upon observable characteristics and intended to reflect natural relationships. While the underlying details concerning what are considered to be scientifically valid "observable characteristics" have changed with expanding knowledge (for example, DNA sequencing, unavailable in Linnaeus' time, has proven to be a tool of considerable utility for classifying living organisms and establishing their evolutionary relationships), the fundamental principle remains sound.
Influences and economic beliefs.
Linnaeus' applied science was inspired not only by the instrumental utilitarianism general to the early Enlightenment, but also by his adherence to the older economic doctrine of Cameralism. Additionally, Linnaeus was a state interventionist. He supported tariffs, levies, export bounties, quotas, embargoes, navigation acts, subsidised investment capital, ceilings on wages, cash grants, state-licensed producer monopolies, and cartels.
Views on mankind.
According to German biologist Ernst Haeckel, the question of man's origin began with Linnaeus. He helped future research in the natural history of man by describing humans just as he described any other plant or animal.
Anthropomorpha.
Linnaeus classified humans among the primates (as they were later called) beginning with the first edition of "". During his time at Hartekamp, he had the opportunity to examine several monkeys and noted similarities between them and man. He pointed out both species basically have the same anatomy; except for speech, he found no other differences. Thus he placed man and monkeys under the same category, "Anthropomorpha", meaning "manlike." This classification received criticism from other biologists such as Johan Gottschalk Wallerius, Jacob Theodor Klein and Johann Georg Gmelin on the ground that it is illogical to describe a human as 'like a man'. In a letter to Gmelin from 1747, Linnaeus replied:
"It does not please that I've placed Man among the Anthropomorpha, perhaps because of the term 'with human form', but man learns to know himself. Let's not quibble over words. It will be the same to me whatever name we apply. But I seek from you and from the whole world a generic difference between man and simian that [follows from the principles of Natural History.
"The greater number of naturalists who have taken into consideration the whole structure of man, including his mental faculties, have followed Blumenbach and Cuvier, and have placed man in a separate Order, under the title of the Bimana, and therefore on an equality with the orders of the Quadrumana, Carnivora, etc. Recently many of our best naturalists have recurred to the view first propounded by Linnaeus, so remarkable for his sagacity, and have placed man in the same Order with the Quadrumana, under the title of the Primates. The justice of this conclusion will be admitted: for in the first place, we must bear in mind the comparative insignificance for classification of the great development of the brain in man, and that the strongly marked differences between the skulls of man and the Quadrumana (lately insisted upon by Bischoff, Aeby, and others) apparently follow from their differently developed brains. In the second place, we must remember that nearly all the other and more important differences between man and the Quadrumana are manifestly adaptive in their nature, and relate chiefly to the erect position of man; such as the structure of his hand, foot, and pelvis, the curvature of his spine, and the position of his head."</ref> I absolutely know of none. If only someone might tell me a single one! If I would have called man a simian or vice versa, I would have brought together all the theologians against me. Perhaps I ought to have by virtue of the law of the discipline."
" with a division between "Homo" and "Simia"]]
The theological concerns were twofold: first, putting man at the same level as monkeys or apes would lower the spiritually higher position that man was assumed to have in the great chain of being, and second, because the Bible says man was created in the image of God (theomorphism), if monkeys/apes and humans were not distinctly and separately designed, that would mean monkeys and apes were created in the image of God as well. This was something many could not accept. The conflict between world views that was caused by asserting man was a type of animal would simmer for a century until the much greater, and still ongoing, creation–evolution controversy began in earnest with the publication of "On the Origin of Species" by Charles Darwin in 1859.
After such criticism, Linnaeus felt he needed to explain himself more clearly. The 10th edition of ' introduced new terms, including "Mammalia" and "Primates", the latter of which would replace "Anthropomorpha" as well as giving humans the full binomial "Homo sapiens". The new classification received less criticism, but many natural historians still believed he had demoted humans from their former place to rule over nature, not be a part of it. Linnaeus believed that man biologically belongs to the animal kingdom and had to be included in it. In his book ', he said, "One should not vent one's wrath on animals, Theology decree that man has a soul and that the animals are mere 'aoutomata mechanica,' but I believe they would be better advised that animals have a soul and that the difference is of nobility."
Strange people in distant lands.
Linnaeus added a second species to the genus "Homo" in "" based on a figure and description by Jacobus Bontius from a 1658 publication: "Homo troglodytes" ("caveman") and published a third in 1771: "Homo lar". Swedish historian Gunnar Broberg states that the new human species Linnaeus described were actually simians or native people clad in skins to frighten colonial settlers, whose appearance had been exaggerated in accounts to Linnaeus.
In early editions of "", many well-known legendary creatures were included such as the phoenix, dragon and manticore as well as cryptids like the satyrus, which Linnaeus collected into the catch-all category "Paradoxa". Broberg thought Linnaeus was trying to offer a natural explanation and demystify the world of superstition. Linnaeus tried to debunk some of these creatures, as he had with the hydra; regarding the purported remains of dragons, Linnaeus wrote that they were either derived from lizards or rays. For "Homo troglodytes" he asked the Swedish East India Company to search for one, but they did not find any signs of its existence. "Homo lar" has since been reclassified as "Hylobates lar", the lar gibbon.
Four races.
In the first edition of "", Linnaeus subdivided the human species into four varieties based on continent and skin colour: "Europæus albus" (white European), "Americanus rubescens" (red American), "Asiaticus fuscus" (brown Asian) and "Africanus Niger" (black African). In the tenth edition of Systema Naturae he further detailed stereotypical characteristics for each variety, based on the concept of the four temperaments from classical antiquity, and changed the description of Asians' skin tone to "luridus" (yellow). Additionally, Linnaeus created a wastebasket taxon "monstrosus" for "wild and monstrous humans, unknown groups, and more or less abnormal people".
Commemoration.
Anniversaries of Linnaeus' birth, especially in centennial years, have been marked by major celebrations. Linnaeus has appeared on numerous Swedish postage stamps and banknotes. There are numerous statues of Linnaeus in countries around the world. The Linnean Society of London has awarded the Linnean Medal for excellence in botany or zoology since 1888. Following approval by the Riksdag of Sweden, Växjö University and Kalmar College merged on 1 January 2010 to become Linnaeus University. Other things named after Linnaeus include the twinflower genus "Linnaea", the crater Linné on the Earth's moon, a street in Cambridge, Massachusetts, and the cobalt sulfide mineral Linnaeite.
Commentary on Linnaeus.
Andrew Dickson White wrote in "" (1896):
Linnaeus ... was the most eminent naturalist of his time, a wide observer, a close thinker; but the atmosphere in which he lived and moved and had his being was saturated with biblical theology, and this permeated all his thinking. ... Toward the end of his life he timidly advanced the hypothesis that all the species of one genus constituted at the creation one species; and from the last edition of his "Systema Naturæ" he quietly left out the strongly orthodox statement of the fixity of each species, which he had insisted upon in his earlier works. ... warnings came speedily both from the Catholic and Protestant sides.
The mathematical PageRank algorithm, applied to 24 multilingual Wikipedia editions in 2014, published in "PLOS ONE" in 2015, placed Carl Linnaeus at the top historical figure, above Jesus, Aristotle, Napoleon, and Adolf Hitler (in that order).

</doc>
