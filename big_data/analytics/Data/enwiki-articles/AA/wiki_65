<doc id="4609" url="https://en.wikipedia.org/wiki?curid=4609" title="Beta-lactamase">
Beta-lactamase

Beta-lactamases are enzymes () produced by bacteria(Also known as Penicillinase ) that provide resistance to β-lactam antibiotics such as penicillins, cephamycins, and carbapenems (ertapenem), although carbapenems are relatively resistant to beta-lactamase. Beta-lactamase provides antibiotic resistance by breaking the antibiotics' structure. These antibiotics all have a common element in their molecular structure: a four-atom ring known as a β-lactam. Through hydrolysis, the lactamase enzyme breaks the β-lactam ring open, deactivating the molecule's antibacterial properties.
Beta-lactam antibiotics are typically used to treat a broad spectrum of Gram-positive and Gram-negative bacteria.
Beta-lactamases produced by Gram-negative organisms are usually secreted, especially when antibiotics are present in the environment.
Structure.
The structure of a "Streptomyces" β-lactamase is given by .
Penicillinase.
Penicillinase is a specific type of β-lactamase, showing specificity for penicillins, again by hydrolysing the β-lactam ring. Molecular weights of the various penicillinases tend to cluster near 50 kiloDaltons.
Penicillinase was the first β-lactamase to be identified: It was first isolated by Abraham and Chain in 1940 from Gram-negative "E. coli" even before penicillin entered clinical use, but penicillinase production quickly spread to bacteria that previously did not produce it or produced it only rarely. Penicillinase-resistant beta-lactams such as methicillin were developed, but there is now widespread resistance to even these.
Classification.
Functional classification.
The following functional classification for beta lactamases has been proposed:
Group 2.
Group 2 are penicillinases, cephalosporinases, or both inhibited by clavulanic acid, corresponding to the molecular classes A and D reflecting the original TEM and SHV genes. Additionally, many class A TEM β-lactamases are inhibited by β-lactamase inhibitor protein (BLIP). Because of the increasing number of TEM- and SHV-derived β-lactamases, they were divided into two subclasses, 2a and 2b.
Molecular classification.
The molecular classification of β-lactamases is based on the nucleotide and amino acid sequences in these enzymes. To date, four classes are recognised (A-D), correlating with the functional classification. Classes A, C, and D act by a serine-based mechanism, whereas class B or metallo-β-lactamases need zinc for their action
"Penicillinase" was discovered in 1940 and renamed Beta-lactamase when the structure of the Beta-lactam ring was finally elucidated.
Resistance in Gram-negative bacteria.
Among Gram-negative bacteria, the emergence of resistance to expanded-spectrum cephalosporins has been a major concern. It appeared initially in a limited number of bacterial species ("E. cloacae", "C. freundii", "S. marcescens", and "P. aeruginosa") that could mutate to hyperproduce their chromosomal class C β-lactamase. A few years later, resistance appeared in bacterial species not naturally producing AmpC enzymes ("K. pneumoniae", "Salmonella" spp., "P. mirabilis") due to the production of TEM- or SHV-type ESBLs. Characteristically, such resistance has included oxyimino- (for example ceftizoxime, cefotaxime, ceftriaxone, and ceftazidime, as well as the oxyimino-monobactam aztreonam), but not 7-alpha-methoxy-cephalosporins (cephamycins; in other words, cefoxitin and cefotetan); has been blocked by inhibitors such as clavulanate, sulbactam, or tazobactam, and did not involve carbapenems and temocillin. Chromosomal-mediated AmpC β-lactamases represent a new threat, since they confer resistance to 7-alpha-methoxy-cephalosporins (cephamycins) such as cefoxitin or cefotetan are not affected by commercially available β-lactamase inhibitors, and can, in strains with loss of outer membrane porins, provide resistance to carbapenems.
Extended-spectrum beta-lactamase (ESBL).
Members of the family Sonya Burkes commonly express plasmid-encoded β-lactamases (e.g., TEM-1, TEM-2, and SHV-1). which confer resistance to penicillins but not to expanded-spectrum cephalosporins. In the mid-1980s, a new group of enzymes, the extended-spectrum β-lactamases (ESBLs), was detected (first detected in 1979). There is evidence that the prevalence of ESBL-producing bacteria have been gradually increasing in acute care hospitals, according to a 2014 study by Joseph T. Spadafino et al. ESBLs are beta-lactamases that hydrolyze extended-spectrum cephalosporins with an oxyimino side chain. These cephalosporins include cefotaxime, ceftriaxone, and ceftazidime, as well as the oxyimino-monobactam aztreonam. Thus ESBLs confer resistance to these antibiotics and related oxyimino-beta lactams. In typical circumstances, they derive from genes for TEM-1, TEM-2, or SHV-1 by mutations that alter the amino acid configuration around the active site of these β-lactamases. A broader set of β-lactam antibiotics are susceptible to hydrolysis by these enzymes. An increasing number of ESBLs not of TEM or SHV lineage have recently been described. The ESBLs are frequently plasmid encoded. Plasmids responsible for ESBL production frequently carry genes encoding resistance to other drug classes (for example, aminoglycosides). Therefore, antibiotic options in the treatment of ESBL-producing organisms are extremely limited. Carbapenems are the treatment of choice for serious infections due to ESBL-producing organisms, yet carbapenem-resistant (primarily ertapenem resistant) isolates have recently been reported. ESBL-producing organisms may appear susceptible to some extended-spectrum cephalosporins. However, treatment with such antibiotics has been associated with high failure rates.
Types.
TEM beta-lactamases (class A).
TEM-1 is the most commonly encountered beta-lactamase in Gram-negative bacteria. Up to 90% of ampicillin resistance in E. coli is due to the production of TEM-1. Also responsible for the ampicillin and penicillin resistance that is seen in "H. influenzae" and "N. gonorrhoeae" in increasing numbers. Although TEM-type beta-lactamases are most often found in "E. coli" and "K. pneumoniae", they are also found in other species of Gram-negative bacteria with increasing frequency. The amino acid substitutions responsible for the extended-specttrum beta lactamase (ESBL) phenotype cluster around the active site of the enzyme and change its configuration, allowing access to oxyimino-beta-lactam substrates. Opening the active site to beta-lactam substrates also typically enhances the susceptibility of the enzyme to b-lactamase inhibitors, such as clavulanic acid. Single amino acid substitutions at positions 104, 164, 238, and 240 produce the ESBL phenotype, but ESBLs with the broadest spectrum usually have more than a single amino acid substitution. Based upon different combinations of changes, currently 140 TEM-type enzymes have been described. TEM-10, TEM-12, and TEM-26 are among the most common in the United States.
SHV beta-lactamases (class A).
SHV-1 shares 68 percent of its amino acids with TEM-1 and has a similar overall structure. The SHV-1 beta-lactamase is most commonly found in "K. pneumoniae" and is responsible for up to 20% of the plasmid-mediated ampicillin resistance in this species. ESBLs in this family also have amino acid changes around the active site, most commonly at positions 238 or 238 and 240. More than 60 SHV varieties are known. SHV-5 and SHV-12 are among the most common.
CTX-M beta-lactamases (class A).
These enzymes were named for their greater activity against cefotaxime than other oxyimino-beta-lactam substrates (e.g., ceftazidime, ceftriaxone, or cefepime). Rather than arising by mutation, they represent examples of plasmid acquisition of beta-lactamase genes normally found on the chromosome of "Kluyvera" species, a group of rarely pathogenic commensal organisms. These enzymes are not very closely related to TEM or SHV beta-lactamases in that they show only approximately 40% identity with these two commonly isolated beta-lactamases. More than 80 CTX-M enzymes are currently known. Despite their name, a few are more active on ceftazidime than cefotaxime. They have mainly been found in strains of "Salmonella enterica" serovar "Typhimurium" and "E. coli", but have also been described in other species of Enterobacteriaceae and are the predominant ESBL type in parts of South America. (They are also seen in eastern Europe) CTX-M-14, CTX-M-3, and CTX-M-2 are the most widespread. CTX-M-15 is currently (2006) the most widespread type in "E. coli" the UK and is widely prevalent in the community. An example of beta-lactamase CTX-M-15, along with IS"Ecp1", has been found to have recently transposed onto the chromosome of "Klebsiella pneumoniae" ATCC BAA-2146.
OXA beta-lactamases (class D).
OXA beta-lactamases were long recognized as a less common but also plasmid-mediated beta-lactamase variety that could hydrolyze oxacillin and related anti-staphylococcal penicillins. These beta-lactamases differ from the TEM and SHV enzymes in that they belong to molecular class D and functional group 2d . The OXA-type beta-lactamases confer resistance to ampicillin and cephalothin and are characterized by their high hydrolytic activity against oxacillin and cloxacillin and the fact that they are poorly inhibited by clavulanic acid. Amino acid substitutions in OXA enzymes can also give the ESBL phenotype. While most ESBLs have been found in "E. coli", "K. pneumoniae", and other Enterobacteriaceae, the OXA-type ESBLs have been found mainly in "P. aeruginosa". OXA-type ESBLs have been found mainly in "Pseudomonas aeruginosa" isolates from Turkey and France. The OXA beta-lactamase family was originally created as a phenotypic rather than a genotypic group for a few beta-lactamases that had a specific hydrolysis profile. Therefore, there is as little as 20% sequence homology among some of the members of this family. However, recent additions to this family show some degree of homology to one or more of the existing members of the OXA beta-lactamase family. Some confer resistance predominantly to ceftazidime, but OXA-17 confers greater resistance to cefotaxime and cefepime than it does resistance to ceftazidime.
Others.
Other plasmid-mediated ESBLs, such as PER, VEB, GES, and IBC beta-lactamases, have been described but are uncommon and have been found mainly in "P. aeruginosa" and at a limited number of geographic sites. PER-1 in isolates in Turkey, France, and Italy; VEB-1 and VEB-2 in strains from Southeast Asia; and GES-1, GES-2, and IBC-2 in isolates from South Africa, France, and Greece. PER-1 is also common in multiresistant acinetobacter species in Korea and Turkey. Some of these enzymes are found in Enterobacteriaceae as well, whereas other uncommon ESBLs (such as BES-1, IBC-1, SFO-1, and TLA-1) have been found only in Enterobacteriaceae.
Treatment.
While ESBL-producing organisms were previously associated with hospitals and institutional care, these organisms are now increasingly found in the community. CTX-M-15-positive E. coli are a cause of community-acquired urinary infections in the UK, and tend to be resistant to all oral β-lactam antibiotics, as well as quinolones and sulfonamides. Treatment options may include nitrofurantoin, fosfomycin, mecillinam and chloramphenicol. In desperation, once-daily ertapenem or gentamicin injections may also be used.
Inhibitor-resistant β-lactamases.
Although the inhibitor-resistant β-lactamases are not ESBLs, they are often discussed with ESBLs because they are also derivatives of the classical TEM- or SHV-type enzymes. These enzymes were at first given the designation IRT for inhibitor-resistant TEM β-lactamase; however, all have subsequently been renamed with numerical TEM designations. There are at least 19 distinct inhibitor-resistant TEM β-lactamases. Inhibitor-resistant TEM β-lactamases have been found mainly in clinical isolates of "E. coli", but also some strains of "K. pneumoniae", "Klebsiella oxytoca", "P. mirabilis", and "Citrobacter freundii". Although the inhibitor-resistant TEM variants are resistant to inhibition by clavulanic acid and sulbactam, thereby showing clinical resistance to the beta-lactam—lactamase inhibitor combinations of amoxicillin-clavulanate (co-amoxiclav), ticarcillin-clavulanate (co-ticarclav), and ampicillin/sulbactam, they normally remain susceptible to inhibition by tazobactam and subsequently the combination of piperacillin/tazobactam, although resistance has been described. This is no longer a primarily European epidemiology, it is found in northern parts of America often and should be tested for with complex UTI's.
AmpC-type β-lactamases (Class C).
AmpC type β-lactamases are commonly isolated from extended-spectrum cephalosporin-resistant Gram-negative bacteria. AmpC β-lactamases (also termed class C or group 1) are typically encoded on the chromosome of many Gram-negative bacteria including "Citrobacter", "Serratia" and "Enterobacter" species where its expression is usually inducible; it may also occur on "Escherichia coli" but is not usually inducible, although it can be hyperexpressed. AmpC type β-lactamases may also be carried on plasmids. AmpC β-lactamases, in contrast to ESBLs, hydrolyse broad and extended-spectrum cephalosporins (cephamycins as well as to oxyimino-β-lactams) but are not inhibited by β-lactamase inhibitors such as clavulanic acid.
Carbapenemases.
Carbapenems are famously stable to AmpC β-lactamases and extended-spectrum-β-lactamases. Carbapenemases are a diverse group of β-lactamases that are active not only against the oxyimino-cephalosporins and cephamycins but also against the carbapenems. Aztreonam is stable to the metallo-β-lactamases,
but many IMP and VIM producers are resistant, owing to other mechanisms. Carbapenemases were formerly believed to derive only from classes A, B, and D, but a class C carbapenemase has been described.
IMP-type carbapenemases (metallo-β-lactamases) (Class B).
Plasmid-mediated IMP-type carbapenemases, 17 varieties of which are currently known, became established in Japan in the 1990s both in enteric Gram-negative organisms and in "Pseudomonas" and "Acinetobacter" species. IMP enzymes spread slowly to other countries in the Far East, were reported from Europe in 1997, and have been found in Canada and Brazil.
VIM (Verona integron-encoded metallo-β-lactamase) (Class B).
A second growing family of carbapenemases, the VIM family, was reported from Italy in 1999 and now includes 10 members, which have a wide geographic distribution in Europe, South America, and the Far East and have been found in the United States. VIM-1 was discovered in "P. aeruginosa" in Italy in 1996; since then, VIM-2 - now the predominant variant - was found repeatedly in Europe and the Far East; VIM-3 and -4 are minor variants of VIM-2 and -1, respectively. VIM enzymes occur mostly in "P. aeruginosa", also "P. putida" and, very rarely, Enterobacteriaceae.
Amino acid sequence diversity is up to 10% in the VIM family, 15% in the IMP family, and 70% between VIM and IMP. Enzymes of both the families, nevertheless, are similar. Both are integron-associated, sometimes within plasmids. Both hydrolyse all β-lactams except monobactams, and evade all β-lactam inhibitors.
OXA (oxacillinase) group of β-lactamases (Class D).
The OXA group of β-lactamases occur mainly in Acinetobacter species and are divided into two clusters. OXA carbapenemases hydrolyse carbapenems very slowly "in vitro", and the high MICs seen for some Acinetobacter hosts (>64 mg/L) may reflect secondary mechanisms. They are sometimes augmented in clinical isolates by additional resistance mechanisms, such as impermeability or efflux. OXA carbapenemases also tend to have a reduced hydrolytic efficiency towards penicillins and cephalosporins.
KPC ("K. pneumoniae" carbapenemase) (Class A).
A few class A enzymes, most noted the plasmid-mediated KPC enzymes, are effective carbapenemases as well. Ten variants, KPC-2 through KPC-11 are known, and they are distinguished by one or two amino acid substitutions (KPC-1 was re-sequenced in 2008 and found to be 100% homologous to published sequences of KPC-2). KPC-1 was found in North Carolina, KPC-2 in Baltimore and KPC-3 in New York. They have only 45% homology with SME and NMC/IMI enzymes and, unlike them, can be encoded by self-transmissible plasmids.
The class A "Klebsiella pneumoniae" carbapenemase (KPC) is currently the most common carbapenemase, which was first detected in North Carolina, US, in 1996 and has since spread worldwide. A later publication indicated that Enterobacteriaceae that produce KPC were becoming common in the United States.
CMY (Class C).
The first class C carbapenemase was described in 2006 and was isolated from a virulent strain of "Enterobacter aerogenes". It is carried on a plasmid, pYMG-1, and is therefore transmissible to other bacterial strains.
SME, IMI, NMC and CcrA.
In general, these are of little clinical significance.
CcrA (CfiA). Its gene occurs in c. 1-3% of B. fragilis isolates, but fewer produce the enzyme since expression demands appropriate migration of an insertion sequence. CcrA was known before imipenem was introduced, and producers have shown little subsequent increase.
NDM-1 (New Delhi metallo-β-lactamase) (Class B).
Originally described from New Delhi in 2009, this gene is now widespread in "Escherichia coli" and "Klebsiella pneumoniae" from India and Pakistan. As of mid-2010, NDM-1 carrying bacteria have been introduced to other countries (including the United States and UK), most probably due to the large number of tourists travelling the globe, who may have picked up the strain from the environment, as strains containing the NDM-1 gene have been found in environmental samples in India.
Treatment of ESBL/AmpC/carbapenemases.
General overview.
In general, an isolate is suspected to be an ESBL producer when it shows "in vitro" susceptibility to the second-generation cephalosporins (cefoxitin, cefotetan) but resistance to the third-generation cephalosporins and to aztreonam. Moreover, one should suspect these strains when treatment with these agents for Gram-negative infections fails despite reported "in vitro" susceptibility. Once an ESBL-producing strain is detected, the laboratory should report it as "resistant" to all penicillins, cephalosporins, and aztreonam, even if it is tested (in vitro) as susceptible. Associated resistance to aminoglycosides and trimethoprim-sulfamethoxazole, as well as high frequency of co-existence of fluoroquinolone resistance, creates problems. Beta-lactamase inhibitors such as clavulanate, sulbactam, and tazobactam "in vitro" inhibit most ESBLs, but the clinical effectiveness of beta-lactam/beta-lactamase inhibitor combinations cannot be relied on consistently for therapy. Cephamycins (cefoxitin and cefotetan) are not hydrolyzed by majority of ESBLs, but are hydrolyzed by associated AmpC-type β-lactamase. Also, β-lactam/β-lactamase inhibitor combinations may not be effective against organisms that produce AmpC-type β-lactamase. Sometimes these strains decrease the expression of outer membrane proteins, rendering them resistant to cephamycins. "In vivo" studies have yielded mixed results against ESBL-producing "K. pneumoniae". (Cefepime, a fourth-generation cephalosporin, has demonstrated "in vitro" stability in the presence of many ESBL/AmpC strains.) Currently, carbapenems are, in general, regarded as the preferred agent for treatment of infections due to ESBL-producing organisms. Carbapenems are resistant to ESBL-mediated hydrolysis and exhibit excellent "in vitro" activity against strains of Enterobacteriaceae expressing ESBLs.
According to genes.
ESBLs.
Strains producing only ESBLs are susceptible to cephamycins and carbapenems "in vitro" and show little if any inoculum effect with these agents.
For organisms producing TEM and SHV type ESBLs, apparent "in vitro" sensitivity to cefepime and to piperacillin/tazobactam is common, but both drugs show an inoculum effect, with diminished susceptibility as the size of the inoculum is increased from 105 to 107 organisms.
Strains with some CTX-M–type and OXA-type ESBLs are resistant to cefepime on testing, despite the use of a standard inoculum.
Inhibitor-Resistant β-lactamases.
Although the inhibitor-resistant TEM variants are resistant to inhibition by clavulanic acid and sulbactam, thereby showing clinical resistance to the beta-lactam—beta lactamase inhibitor combinations of amoxicillin-clavulanate (Co-amoxiclav), ticarcillin-clavulanate, and ampicillin/sulbactam, they remain susceptible to inhibition by tazobactam and subsequently the combination of piperacillin/tazobactam.
AmpC.
AmpC-producing strains are typically resistant to oxyimino-beta lactams and to cephamycins and are susceptible to carbapenems; however, diminished porin expression can make such a strain carbapenem-resistant as well.
Carbapenemases.
Strains with IMP-, VIM-, and OXA-type carbapenemases usually remain susceptible. Resistance to non-beta-lactam antibiotics is common in strains making any of these enzymes, such that alternative options for non-beta-lactam therapy need to be determined by direct susceptibility testing. Resistance to fluoroquinolones and aminoglycosides is especially high.
According to species.
"Escherichia coli" or "Klebsiella".
For infections caused by ESBL-producing "Escherichia coli" or "Klebsiella" species, treatment with imipenem or meropenem has been associated with the best outcomes in terms of survival and bacteriologic clearance. Cefepime and piperacillin/tazobactam have been less successful. Ceftriaxone, cefotaxime, and ceftazidime have failed even more often, despite the organism's susceptibility to the antibiotic "in vitro". Several reports have documented failure of cephamycin therapy as a result of resistance due to porin loss. Some patients have responded to aminoglycoside or quinolone therapy, but, in a recent comparison of ciprofloxacin and imipenem for bacteremia involving an ESBL-producing "K. pneumoniae", imipenem produced the better outcome
"Pseudomonas aeruginosa".
There have been few clinical studies to define the optimal therapy for infections caused by ESBL producing "Pseudomonas aeruginosa" strains.
Detection.
Beta-lactamase enzymatic activity can be detected using nitrocefin, a chromogenic cephalosporin substrate which changes color from yellow to red upon beta-lactamase mediated hydrolysis.
Evolution.
Beta-lactamases are ancient bacterial enzymes. The class B beta-lactamases (the metallo-beta-lactamases) are divided into three subclasses: B1, B2 and B3. Subclasses B1 and B2 are theorized to have evolved about one billion years ago and subclass B3s is theorized to have evolved before the divergence of the Gram-positive and Gram-negative eubacteria about two billion years ago.
The other three groups are serine enzymes that show little homology to each other. Structural studies have shown that groups A and D are sister taxa and that group C diverged before A and D. These serine-based enzymes, like the group B betalactamases, are of ancient origin and are theorized to have evolved about two billion years ago.
The OXA group (in class D) in particular is theorized to have evolved on chromosomes and moved to plasmids on at least two separate occasions.

</doc>
<doc id="4611" url="https://en.wikipedia.org/wiki?curid=4611" title="Burhanuddin Rabbani">
Burhanuddin Rabbani

Burhanuddin Rabbani (; 20 September 1940 – 20 September 2011) was President of the Islamic State of Afghanistan from 1992 to 1996. After the Taliban government was toppled during Operation Enduring Freedom, Rabbani returned to Kabul and served as a temporary President from November to December 20, 2001, when Hamid Karzai was chosen at the Bonn International Conference on Afghanistan. Rabbani was also the leader of Jamiat-e Islami Afghanistan (Islamic Society of Afghanistan), which has close ties to Pakistan's Jamaat-e-Islami.
He was one of the earliest founders and movement leaders of the Mujahideen in the late 1970s, right before the Soviet invasion of Afghanistan. He served as the political head of the United Islamic Front for the Salvation of Afghanistan (UIFSA), an alliance of various political groups who fought against the Taliban in Afghanistan. His government was recognized by many countries, as well as the United Nations. He later became head of Afghanistan National Front (known in the media as United National Front), the largest political opposition to Hamid Karzai's government. On 20 September 2011, Rabbani was assassinated by a suicide bomber entering his home in Kabul. As suggested by the Afghan parliament, Afghanistan's President Hamid Karzai gave him the title of "Martyr of Peace". His son Salahuddin Rabbani was chosen in April 2012 to lead efforts to forge peace in Afghanistan with the Taliban.
Early years.
Rabbani, son of Muhammed Yousuf, was born in the northern province of Badakhshan in 1940. He was a Persian-speaking ethnic Tajik. After finishing school in his native province, he went to Darul-uloom-e-Sharia (Abu-Hanifa), a religious school in Kabul. When he graduated from Abu-Hanifa, he attended Kabul University to study Islamic Law and Theology, graduating in 1963.
Soon after his graduation in 1963, he was hired as a professor at Kabul University. In order to enhance himself, Rabbani went to Egypt in 1966, and he entered the Al-Azhar University in Cairo where he developed close ties to the Muslim Brotherhood leadership. In two years, he received his masters degree in Islamic Philosophy. Rabbani was one of the first Afghans to translate the works of Sayyid Qutb into Persian. Later he returned to Egypt to complete his PhD in Islamic philosophy and his thesis was titled "The Philosophy and Teachings of Abd al-Rahman Muhammad Jami." In 2004 he received Afghanistan's highest academic and scientific title "Academician" from the Academy of Sciences of Afghanistan.
Political career.
Rabbani returned to Afghanistan in 1968, where the High Council of Jamiat-e Islami gave him the duty of organizing the University students. Due to his knowledge, reputation, and active support for the cause of Islam, in 1972, a 15-member council selected him as head of Jamiat-e Islami of Afghanistan; the founder of Jamiat-e Islami of Afghanistan, Ghulam M. Niyazi was also present. Jamiat-e Islami was primarily composed of Tajiks.
In the spring of 1974, the police came to Kabul University to arrest Rabbani for his pro-Islamic stance, but with the help of his students the police were unable to capture him, and he managed to escape to the countryside. In Pakistan, Rabbani gathered important people and established the party. Sayed Noorullah Emad, who was then a young Muslim in the University of Kabul, became the General Secretary of the party and, later, its deputy chief.
When the Soviets supported the 1979 coup, Rabbani helped lead Jamiat-e Islami in resistance to the People's Democratic Party of Afghanistan regime. Rabbani's forces were the first Mujahideen elements to enter Kabul in 1992 when the PDPA government fell from power. He took over as President from 1992 until the Taliban's conquest of Kabul in 1996. For the next five years, he and the Northern Alliance were busy fighting the Taliban until the 2001 US-led Operation Enduring Freedom in which the Taliban government was toppled. Rabbani was head of Afghanistan’s High Peace Council, which had been formed in 2010 to initiate peace talks with the Taliban and other groups in the insurgency, until his death.
Assassination.
Rabbani was killed in a suicide bombing at his home in Kabul on 20 September 2011, his 71st birthday. Two men posing as Taliban representatives approached him to offer a hug and detonated their explosives. At least one of them had hidden the explosives in his turban. The suicide bomber claimed to be a Taliban commander and said he wanted to "discuss peace" with Rabbani. Four other members of Afghanistan’s High Peace Council were also killed in the blast.
Afghan officials blamed the Quetta Shura, which is the leadership of the Afghan Taliban hiding in the affluent Satellite Town of Quetta in Pakistan. The Pakistani government confirmed that Rabbani's assassination was linked to Afghan refugees in Pakistan. A senior Pakistani official stated that over 90% of terrorist attacks in Pakistan are traced back to Afghan elements and that their presence in the country was "an important issue for Pakistan" and "a problem for Afghanistan". Pakistani foreign minister Hina Rabbani Khar said that "We are not responsible if Afghan refugees crossed the border and entered Kabul, stayed in a guest house and attacked Professor Rabbani".
In 2011, just days before he died, Rabbani was trying to persuade Islamic scholars to issue a religious edict banning suicide bombings. The former president's 29-year-old daughter said in an interview that her father died shortly after he spoke at a conference on "Islamic Awakening" in Tehran. "Right before he was assassinated, he talked about the suicide bombing issue," Fatima Rabbani told Reuters. "He called on all Islamic scholars in the conference to release a fatwa" against the tactic.
United States President Barack Obama and several NATO military leaders condemned the assassination. Japan also offered its condolences at the Sixty-sixth session of the United Nations General Assembly. Afghan President Hamid Karzai cut short his trip for the General debate of the sixty-sixth session of the United Nations General Assembly following his assassination. Rabbani's son Salahuddin then took over chairmanship of the High Peace Council from his father.

</doc>
<doc id="4614" url="https://en.wikipedia.org/wiki?curid=4614" title="Boeing 747">
Boeing 747

The Boeing 747 is a wide-body commercial jet airliner and cargo aircraft, often referred to by its original nickname, "Jumbo Jet", or "Queen of the Skies". Its distinctive "hump" upper deck along the forward part of the aircraft makes it among the world's most recognizable aircraft, and it was the first wide-body produced. Manufactured by Boeing's Commercial Airplane unit in the United States, the original version of the 747 had two and a half times greater capacity than the Boeing 707, one of the common large commercial aircraft of the 1960s. First flown commercially in 1970, the 747 held the passenger capacity record for 37 years.
The four-engine 747 uses a double deck configuration for part of its length. It is available in passenger, freighter and other versions. Boeing designed the 747's hump-like upper deck to serve as a first class lounge or extra seating, and to allow the aircraft to be easily converted to a cargo carrier by removing seats and installing a front cargo door. Boeing did so because the company expected supersonic airliners (development of which was announced in the early 1960s) to render the 747 and other subsonic airliners obsolete, while the demand for subsonic cargo aircraft would be robust well into the future. The 747 was expected to become obsolete after 400 were sold, but it exceeded critics' expectations with production passing the 1,000 mark in 1993. By March 2016, 1,520 aircraft had been built, with 23 of the 747-8 variants remaining on order.
The 747-400, the most common passenger version in service, has a high-subsonic cruise speed of Mach 0.85–0.855 (up to ) with an intercontinental range of 7,260 nautical miles (8,350 mi or 13,450 km). The 747-400 passenger version can accommodate 416 passengers in a typical three-class layout, 524 passengers in a typical two-class layout, or 660 passengers in a high density one-class configuration. The newest version of the aircraft, the 747-8, is in production and received certification in 2011. Deliveries of the 747-8F freighter version to launch customer Cargolux began in October 2011; deliveries of the 747-8I passenger version to Lufthansa began in May 2012.
Development.
Background.
In 1963, the United States Air Force started a series of study projects on a very large strategic transport aircraft. Although the C-141 Starlifter was being introduced, they believed that a much larger and more capable aircraft was needed, especially the capability to carry outsized cargo that would not fit in any existing aircraft. These studies led to initial requirements for the CX-Heavy Logistics System (CX-HLS) in March 1964 for an aircraft with a load capacity of and a speed of Mach 0.75 (), and an unrefueled range of with a payload of . The payload bay had to be wide by high and long with access through doors at the front and rear.
Featuring only four engines, the design also required new engine designs with greatly increased power and better fuel economy. On May 18, 1964, airframe proposals arrived from Boeing, Douglas, General Dynamics, Lockheed and Martin Marietta; while engine proposals were submitted by General Electric, Curtiss-Wright, and Pratt & Whitney. After a downselect, Boeing, Douglas and Lockheed were given additional study contracts for the airframe, along with General Electric and Pratt & Whitney for the engines.
All three of the airframe proposals shared a number of features. As the CX-HLS needed to be able to be loaded from the front, a door had to be included where the cockpit usually was. All of the companies solved this problem by moving the cockpit above the cargo area; Douglas had a small "pod" just forward and above the wing, Lockheed used a long "spine" running the length of the aircraft with the wing spar passing through it, while Boeing blended the two, with a longer pod that ran from just behind the nose to just behind the wing. In 1965 Lockheed's aircraft design and General Electric's engine design were selected for the new C-5 Galaxy transport, which was the largest military aircraft in the world at the time. The nose door and raised cockpit concepts would be carried over to the design of the 747.
Airliner proposal.
The 747 was conceived while air travel was increasing in the 1960s. The era of commercial jet transportation, led by the enormous popularity of the Boeing 707 and Douglas DC-8, had revolutionized long-distance travel. Even before it lost the CX-HLS contract, Boeing was pressed by Juan Trippe, president of Pan American World Airways (Pan Am), one of their most important airline customers, to build a passenger aircraft more than twice the size of the 707. During this time, airport congestion, worsened by increasing numbers of passengers carried on relatively small aircraft, became a problem that Trippe thought could be addressed by a large new aircraft.
In 1965, Joe Sutter was transferred from Boeing's 737 development team to manage the design studies for a new airliner, already assigned the model number 747. Sutter initiated a design study with Pan Am and other airlines, to better understand their requirements. At the time, it was widely thought that the 747 would eventually be superseded by supersonic transport aircraft. Boeing responded by designing the 747 so that it could be adapted easily to carry freight and remain in production even if sales of the passenger version declined. In the freighter role, the clear need was to support the containerized shipping methodologies that were being widely introduced at about the same time. Standard containers are square at the front (slightly higher due to attachment points) and available in lengths. This meant that it would be possible to support a 2-wide 2-high stack of containers two or three ranks deep with a fuselage size similar to the earlier CX-HLS project.
In April 1966, Pan Am ordered 25 747-100 aircraft for US$525 million. During the ceremonial 747 contract-signing banquet in Seattle on Boeing's 50th Anniversary, Juan Trippe predicted that the 747 would be "... a great weapon for peace, competing with intercontinental missiles for mankind's destiny". As launch customer, and because of its early involvement before placing a formal order, Pan Am was able to influence the design and development of the 747 to an extent unmatched by a single airline before or since.
Design effort.
Ultimately, the high-winged CX-HLS Boeing design was not used for the 747, although technologies developed for their bid had an influence. The original design included a full-length double-deck fuselage with eight-across seating and two aisles on the lower deck and seven-across seating and two aisles on the upper deck. However, concern over evacuation routes and limited cargo-carrying capability caused this idea to be scrapped in early 1966 in favor of a wider single deck design. The cockpit was, therefore, placed on a shortened upper deck so that a freight-loading door could be included in the nose cone; this design feature produced the 747's distinctive "bulge". In early models it was not clear what to do with the small space in the pod behind the cockpit, and this was initially specified as a "lounge" area with no permanent seating. (A different configuration that had been considered in order to keep the flight deck out of the way for freight loading had the pilots below the passengers, and was dubbed the "anteater".)
One of the principal technologies that enabled an aircraft as large as the 747 to be conceived was the high-bypass turbofan engine. The engine technology was thought to be capable of delivering double the power of the earlier turbojets while consuming a third less fuel. General Electric had pioneered the concept but was committed to developing the engine for the C-5 Galaxy and did not enter the commercial market until later. Pratt & Whitney was also working on the same principle and, by late 1966, Boeing, Pan Am and Pratt & Whitney agreed to develop a new engine, designated the JT9D to power the 747.
The project was designed with a new methodology called fault tree analysis, which allowed the effects of a failure of a single part to be studied to determine its impact on other systems. To address concerns about safety and flyability, the 747's design included structural redundancy, redundant hydraulic systems, quadruple main landing gear and dual control surfaces. Additionally, some of the most advanced high-lift devices used in the industry were included in the new design, to allow it to operate from existing airports. These included slats running almost the entire length of the wing, as well as complex three-part slotted flaps along the trailing edge of the wing. The wing's complex three-part flaps increase wing area by 21 percent and lift by 90 percent when fully deployed compared to their non-deployed configuration.
Boeing agreed to deliver the first 747 to Pan Am by the end of 1969. The delivery date left 28 months to design the aircraft, which was two-thirds of the normal time. The schedule was so fast paced that the people who worked on it were given the nickname "The Incredibles". Developing the aircraft was such a technical and financial challenge that management was said to have "bet the company" when it started the project.
Production plant.
As Boeing did not have a plant large enough to assemble the giant airliner, they chose to build a new plant. The company considered locations in about 50 cities, and eventually decided to build the new plant some north of Seattle on a site adjoining a military base at Paine Field near Everett, Washington. It bought the site in June 1966.
Developing the 747 had been a major challenge, and building its assembly plant was also a huge undertaking. Boeing president William M. Allen asked Malcolm T. Stamper, then head of the company's turbine division, to oversee construction of the Everett factory and to start production of the 747. To level the site, more than of earth had to be moved. Time was so short that the 747's full-scale mock-up was built before the factory roof above it was finished. The plant is the largest building by volume ever built, and has been substantially expanded several times to permit construction of other models of Boeing wide-body commercial jets.
Development and testing.
Before the first 747 was fully assembled, testing began on many components and systems. One important test involved the evacuation of 560 volunteers from a cabin mock-up via the aircraft's emergency chutes. The first full-scale evacuation took two and a half minutes instead of the maximum of 90 seconds mandated by the Federal Aviation Administration (FAA), and several volunteers were injured. Subsequent test evacuations achieved the 90-second goal but caused more injuries. Most problematic was evacuation from the aircraft's upper deck; instead of using a conventional slide, volunteer passengers escaped by using a harness attached to a reel. Tests also involved taxiing such a large aircraft. Boeing built an unusual training device known as "Waddell's Wagon" (named for a 747 test pilot, Jack Waddell) that consisted of a mock-up cockpit mounted on the roof of a truck. While the first 747s were still being built, the device allowed pilots to practice taxi maneuvers from a high upper-deck position.
On September 30, 1968, the first 747 was rolled out of the Everett assembly building before the world's press and representatives of the 26 airlines that had ordered the airliner. Over the following months, preparations were made for the first flight, which took place on February 9, 1969, with test pilots Jack Waddell and Brien Wygle at the controls and Jess Wallick at the flight engineer's station. Despite a minor problem with one of the flaps, the flight confirmed that the 747 handled extremely well. The 747 was found to be largely immune to "Dutch roll", a phenomenon that had been a major hazard to the early swept-wing jets.
During later stages of the flight test program, flutter testing showed that the wings suffered oscillation under certain conditions. This difficulty was partly solved by reducing the stiffness of some wing components. However, a particularly severe high-speed flutter problem was solved only by inserting depleted uranium counterweights as ballast in the outboard engine nacelles of the early 747s. This measure caused anxiety when these aircraft crashed, as did China Airlines Flight 358 at Wanli in 1991 and El Al Flight 1862 at Amsterdam in 1992 which had of uranium in the tailplane.
The flight test program was hampered by problems with the 747's JT9D engines. Difficulties included engine stalls caused by rapid throttle movements and distortion of the turbine casings after a short period of service. The problems delayed 747 deliveries for several months, up to 20 aircraft at the Everett plant were stranded while awaiting engine installation. The program was further delayed when one of the five test aircraft suffered serious damage during a landing attempt at Renton Municipal Airport, site of the company's Renton factory. On December 13, 1969 a test aircraft was being taken to have test equipment removed and a cabin installed when pilot Ralph C. Cokely undershot the airport's short runway. The 747's right, outer landing gear was torn off and two engine nacelles were damaged. However, these difficulties did not prevent Boeing from taking a test aircraft to the 28th Paris Air Show in mid-1969, where it was displayed to the public for the first time. The 747 received its FAA airworthiness certificate in December 1969, clearing it for introduction into service.
The huge cost of developing the 747 and building the Everett factory meant that Boeing had to borrow heavily from a banking syndicate. During the final months before delivery of the first aircraft, the company had to repeatedly request additional funding to complete the project. Had this been refused, Boeing's survival would have been threatened. The firm's debt exceeded $2 billion, with the $1.2 billion owed to the banks setting a record for all companies. Allen later said, "It was really too large a project for us." Ultimately, the gamble succeeded, and Boeing held a monopoly in very large passenger aircraft production for many years.
Entry into service.
On January 15, 1970, First Lady of the United States Pat Nixon christened Pan Am's first 747, at Dulles International Airport (later Washington Dulles International Airport) in the presence of Pan Am chairman Najeeb Halaby. Instead of champagne, red, white, and blue water was sprayed on the aircraft. The 747 entered service on January 22, 1970, on Pan Am's New York–London route; the flight had been planned for the evening of January 21, but engine overheating made the original aircraft unusable. Finding a substitute delayed the flight by more than six hours to the following day.
The 747 enjoyed a fairly smooth introduction into service, overcoming concerns that some airports would not be able to accommodate an aircraft that large. Although technical problems occurred, they were relatively minor and quickly solved. After the aircraft's introduction with Pan Am, other airlines that had bought the 747 to stay competitive began to put their own 747s into service. Boeing estimated that half of the early 747 sales were to airlines desiring the aircraft's long range rather than its payload capacity. While the 747 had the lowest potential operating cost per seat, this could only be achieved when the aircraft was fully loaded; costs per seat increased rapidly as occupancy declined. A moderately loaded 747, one with only 70 percent of its seats occupied, used more than 95 percent of the fuel needed by a fully occupied 747.
The recession of 1969-1970 greatly affected Boeing. For the year and a half after September 1970 it only sold two 747s in the world, and did not sell any to an American carrier for almost three years. When economic problems in the United States and other countries after the 1973 oil crisis led to reduced passenger traffic, several airlines found they did not have enough passengers to fly the 747 economically, and they replaced them with the smaller and recently introduced McDonnell Douglas DC-10 and Lockheed L-1011 TriStar trijet wide bodies (and later the 767 and A300 twinjets). Having tried replacing coach seats on its 747s with piano bars in an attempt to attract more customers, American Airlines eventually relegated its 747s to cargo service and in 1983 exchanged them with Pan Am for smaller aircraft; Delta Air Lines also removed its 747s from service after several years. Delta later reacquired 747s after it merged with Northwest Airlines.
International flights that bypassed traditional hub airports and landed at smaller cities became more common throughout the 1980s, and this eroded the 747's original market. However, many international carriers continued to use the 747 on Pacific routes. In Japan, 747s on domestic routes were configured to carry close to the maximum passenger capacity.
Improved 747 versions.
After the initial 747-100 model, Boeing developed the , a higher maximum takeoff weight (MTOW) variant, and the (Short Range), with higher passenger capacity. Increased maximum takeoff weight allows aircraft to carry more fuel and have longer range. The model followed in 1971, featuring more powerful engines and a higher MTOW. Passenger, freighter and combination passenger-freighter versions of the were produced. The shortened 747SP (special performance) with a longer range was also developed, and entered service in 1976.
The 747 line was further developed with the launch of the 747-300 in 1980. The 300 series resulted from Boeing studies to increase the seating capacity of the 747, during which modifications such as fuselage plugs and extending the upper deck over the entire length of the fuselage were rejected. The first 747-300, completed in 1983, included a stretched upper deck, increased cruise speed, and increased seating capacity. The -300 variant was previously designated 747SUD for stretched upper deck, then 747-200 SUD, followed by 747EUD, before the 747-300 designation was used. Passenger, short range and combination freighter-passenger versions of the 300 series were produced.
In 1985, development of the longer range 747-400 began. The variant had a new glass cockpit, which allowed for a cockpit crew of two instead of three, new engines, lighter construction materials, and a redesigned interior. Development cost soared, and production delays occurred as new technologies were incorporated at the request of airlines. Insufficient workforce experience and reliance on overtime contributed to early production problems on the 747-400. The -400 entered service in 1989.
In 1991, a record-breaking 1,087 passengers were airlifted aboard a 747 to Israel as part of Operation Solomon. The 747 remained the heaviest commercial aircraft in regular service until the debut of the Antonov An-124 Ruslan in 1982; variants of the 747-400 would surpass the An-124's weight in 2000. The Antonov An-225 "Mriya" cargo transport, which debuted in 1988, remains the world's largest aircraft by several measures (including the most accepted measures of maximum takeoff weight and length); one aircraft has been completed and is in service . The Hughes H-4 Hercules is the largest aircraft by wingspan, but it only completed a single flight.
Further developments.
Since the arrival of the 747-400, several stretching schemes for the 747 have been proposed. Boeing announced the larger 747-500X and preliminary designs in 1996. The new variants would have cost more than US$5 billion to develop, and interest was not sufficient to launch the program. In 2000, Boeing offered the more modest 747X and 747X stretch derivatives as alternatives to the Airbus A3XX. However, the 747X family was unable to attract enough interest to enter production. A year later, Boeing switched from the 747X studies to pursue the Sonic Cruiser, and after the Sonic Cruiser program was put on hold, the 787 Dreamliner. Some of the ideas developed for the 747X were used on the 747-400ER, a longer range variant of the 747-400.
After several variants were proposed but later abandoned, some industry observers became skeptical of new aircraft proposals from Boeing. However, in early 2004, Boeing announced tentative plans for the 747 Advanced that were eventually adopted. Similar in nature to the 747-X, the stretched 747 Advanced used technology from the 787 to modernize the design and its systems. The 747 remained the largest passenger airliner in service until the Airbus A380 began airline service in 2007.
On November 14, 2005, Boeing announced it was launching the 747 Advanced as the Boeing 747-8. The last 747-400s were completed in 2009. , most orders of the 747-8 have been for the freighter variant. On February 8, 2010, the 747-8 Freighter made its maiden flight. The first delivery of the 747-8 went to Cargolux in 2011. The 1,500th produced Boeing 747 was delivered in June 2014.
In January 2016, Boeing stated it was reducing 747-8 production to six a year beginning in September 2016, incurring a $569 million post-tax charge against its fourth-quarter 2015 profits. At the end of 2015, the company had 20 orders outstanding. On January 29, 2016, Boeing announced that it had begun the preliminary work on the modifications to a commercial 747-8 for the next Air Force One Presidential aircraft, expected to be operational by 2020.
Design.
The Boeing 747 is a large, wide-body (two-aisle) airliner with four wing-mounted engines. The wings have a high sweep angle of 37.5 degrees for a fast, efficient cruise of Mach 0.84 to 0.88, depending on the variant. The sweep also reduces the wingspan, allowing the 747 to use existing hangars. Seating capacity is more than 366 with a 3–4–3 seat arrangement (a cross section of 3 seats, an aisle, 4 seats, another aisle, and 3 seats) in economy class and a 2–3–2 arrangement in first class on the main deck. The upper deck has a 3–3 seat arrangement in economy class and a 2–2 arrangement in first class.
Raised above the main deck, the cockpit creates a hump. The raised cockpit allows front loading of cargo on freight variants. The upper deck behind the cockpit provides space for a lounge or extra seating. The "stretched upper deck" became available as an option on the 747-100B variant and later as standard on the 747-300. The 747 cockpit roof section also has an escape hatch from which crew can exit in the event of an emergency if they cannot exit through the cabin.
The 747's maximum takeoff weight ranges from 735,000 pounds (333,400 kg) for the -100 to 970,000 lb (439,985 kg) for the -8. Its range has increased from 5,300 nautical miles (6,100 mi, 9,800 km) on the -100 to 8,000 nmi (9,200 mi, 14,815 km) on the -8I.
The 747 has redundant structures along with four redundant hydraulic systems and four main landing gears with four wheels each, which provide a good spread of support on the ground and safety in case of tire blow-outs. The main gear are redundant so that landing can be performed on two opposing landing gears if the others do not function properly. In addition, the 747 has split control surfaces and was designed with sophisticated triple-slotted flaps that minimize landing speeds and allow the 747 to use standard-length runways. For transportation of spare engines, 747s can accommodate a non-functioning fifth-pod engine under the port wing of the aircraft between the inner functioning engine and the fuselage.
Variants.
The 747-100 was the original variant launched in 1966. The 747-200 soon followed, with its launch in 1968. The 747-300 was launched in 1980 and was followed by the 747-400 in 1985. Ultimately, the 747-8 was announced in 2005. Several versions of each variant have been produced, and many of the early variants were in production simultaneously. The International Civil Aviation Organization (ICAO) classifies variants using a shortened code formed by combining the model number and the variant designator (e.g. "B741" for all -100 models).
747-100.
The first 747-100s were built with six upper deck windows (three per side) to accommodate upstairs lounge areas. Later, as airlines began to use the upper deck for premium passenger seating instead of lounge space, Boeing offered a ten-window upper deck as an option. Some early -100s were retrofitted with the new configuration. The -100 was equipped with Pratt & Whitney JT9D-3A engines. No freighter version of this model was developed, but many 747-100s were converted into freighters. A total of 167 747-100s were built. Iran Air was the last airline to use the 747-100 for passenger service until 2014.
747SR.
Responding to requests from Japanese airlines for a high-capacity aircraft to serve domestic routes between major cities, Boeing developed the 747SR as a short-range version of the 747-100 with lower fuel capacity and greater payload capability. With increased economy class seating, up to 498 passengers could be carried in early versions and up to 550 in later models. The 747SR had an economic design life objective of 52,000 flights during 20 years of operation, compared to 24,600 flights in 20 years for the standard 747. The initial 747SR model, the -100SR, had a strengthened body structure and landing gear to accommodate the added stress accumulated from a greater number of takeoffs and landings. Extra structural support was built into the wings, fuselage, and the landing gear along with a 20 percent reduction in fuel capacity.
The initial order for the -100SR — four aircraft for Japan Air Lines (JAL, later Japan Airlines) — was announced on October 30, 1972; rollout occurred on August 3, 1973, and the first flight took place on August 31, 1973. The type was certified by the FAA on September 26, 1973, with the first delivery on the same day. The -100SR entered service with JAL, the type's sole customer, on October 7, 1973, and typically operated flights within Japan. Seven -100SRs were built between 1973 and 1975, each with a MTOW and Pratt & Whitney JT9D-7A engines derated to of thrust.
Following the -100SR, Boeing produced the -100BSR, a 747SR variant with increased takeoff weight capability. Debuting in 1978, the -100BSR also incorporated structural modifications for a high cycle-to-flying hour ratio; a related standard -100B model debuted in 1979. The -100BSR first flew on November 3, 1978, with first delivery to All Nippon Airways (ANA) on December 21, 1978. A total of twenty -100BSRs were produced for ANA and JAL. The -100BSR had a 600,000 lb MTOW and was powered by the same JT9D-7A or General Electric CF6-45 engines used on the -100SR. ANA operated this variant on domestic Japanese routes with 455 or 456 seats until retiring its last aircraft in March 2006.
In 1986, two -100BSR SUD models, featuring the stretched upper deck (SUD) of the -300, were produced for JAL. The type's maiden flight occurred on February 26, 1986, with FAA certification and first delivery on March 24, 1986. JAL operated the -100BSR SUD with 563 seats on domestic routes until their retirement in the third quarter of 2006. While only two -100BSR SUDs were produced, in theory, standard -100Bs can be modified to the SUD certification. Overall, twenty-nine 747SRs were built, consisting of seven -100SRs, twenty -100BSRs, and two -100BSR SUDs.
747-100B.
The 747-100B model was developed from the -100SR, using its stronger airframe and landing gear design. The type had an increased fuel capacity of , allowing for a range with a typical 452-passenger payload, and an increased MTOW of was offered. The first -100B order, one aircraft for Iran Air, was announced on June 1, 1978. This aircraft first flew on June 20, 1979, received FAA certification on August 1, 1979, and was delivered the next day. Nine -100Bs were built, one for Iran Air and eight for Saudi Arabian Airlines. Unlike the original -100, the -100B was offered with Pratt & Whitney JT9D-7A, General Electric CF6-50, or Rolls-Royce RB211-524 engines. However, only RB211-524 (Saudia) and JT9D-7A (Iran Air) engines were ordered.
747SP.
The development of the 747SP stemmed from a joint request between Pan American World Airways and Iran Air, who were looking for a high-capacity airliner with enough range to cover Pan Am's New York–Middle Eastern routes and Iran Air's planned Tehran–New York route. The Tehran–New York route, when launched, was the longest non-stop commercial flight in the world. The 747SP is shorter than the 747-100. Fuselage sections were eliminated fore and aft of the wing, and the center section of the fuselage was redesigned to fit mating fuselage sections. The SP's flaps used a simplified single-slotted configuration. The 747SP, compared to earlier variants, had a tapering of the aft upper fuselage into the empennage, a double-hinged rudder, and longer vertical and horizontal stabilizers. Power was provided by Pratt & Whitney JT9D-7(A/F/J/FW) or Rolls-Royce RB211-524 engines.
The 747SP was granted a supplemental certificate on February 4, 1976 and entered service with launch customers Pan Am and Iran Air that same year. The aircraft was chosen by airlines wishing to serve major airports with short runways. A total of 45 747SPs were built, with the 44th 747SP delivered on August 30, 1982. In 1987, Boeing re-opened the 747SP production line after five years to build one last 747SP for an order by the United Arab Emirates government. In addition to airline use, one 747SP was modified for the NASA/German Aerospace Center SOFIA experiment.
747-200.
While the 747-100 powered by Pratt & Whitney JT9D-3A engines offered enough payload and range for US domestic operations, it was marginal for long international route sectors. The demand for longer range aircraft with increased payload quickly led to the improved -200, which featured more powerful engines, increased MTOW, and greater range than the -100. A few early -200s retained the three-window configuration of the -100 on the upper deck, but most were built with a ten-window configuration on each side. The 747-200 was produced in passenger (-200B), freighter (-200F), convertible (-200C), and combi (-200M) versions.
The 747-200B was the basic passenger version, with increased fuel capacity and more powerful engines; it entered service in February 1971. In its first three years of production, the -200 was equipped with Pratt & Whitney JT9D-7 engines (initially the only engine available). Range with a full passenger load started at over and increased to with later engines. Most -200Bs had an internally stretched upper deck, allowing for up to 16 passenger seats. The freighter model, the 747-200F, could be fitted with or without a side cargo door, and had a capacity of 105 tons (95.3 tonnes) and an MTOW of up to 833,000 lb (378,000 kg). It entered service in 1972 with Lufthansa. The convertible version, the 747-200C, could be converted between a passenger and a freighter or used in mixed configurations, and featured removable seats and a nose cargo door. The -200C could also be fitted with an optional side cargo door on the main deck.
The combi model, the 747-200M, could carry freight in the rear section of the main deck via a side cargo door. A removable partition on the main deck separated the cargo area at the rear from the passengers at the front. The -200M could carry up to 238 passengers in a three-class configuration with cargo carried on the main deck. The model was also known as the 747-200 Combi. As on the -100, a stretched upper deck (SUD) modification was later offered. A total of 10 converted 747-200s were operated by KLM. Union des Transports Aériens (UTA) also had two of these aircraft converted.
After launching the -200 with Pratt & Whitney JT9D-7 engines, on August 1, 1972 Boeing announced that it had reached an agreement with General Electric to certify the 747 with CF6-50 series engines to increase the aircraft's market potential. Rolls-Royce followed 747 engine production with a launch order from British Airways for four aircraft. The option of RB211-524B engines was announced on June 17, 1975. The -200 was the first 747 to provide a choice of powerplant from the three major engine manufacturers.
A total of 393 of the 747-200 versions had been built when production ended in 1991. Of these, 225 were -200B, 73 were -200F, 13 were -200C, 78 were -200M, and 4 were military. Many 747-200s remain in operation, although most large carriers have retired them from their fleets and sold them to smaller operators. Large carriers have sped up fleet retirement following the September 11 attacks and the subsequent drop in demand for air travel, scrapping some or turning others into freighters.
747-300.
The 747-300 features a upper deck than the -200. The stretched upper deck has two emergency exit doors and is the most visible difference between the -300 and previous models. Before being made standard on the 747-300, the stretched upper deck was previously offered as a retrofit, and appeared on two Japanese 747-100SR aircraft. The 747-300 introduced a new straight stairway to the upper deck, instead of a spiral staircase on earlier variants, which creates room above and below for more seats. Minor aerodynamic changes allowed the -300's cruise speed to reach Mach 0.85 compared with Mach 0.84 on the -200 and -100 models, while retaining the same takeoff weight. The -300 could be equipped with the same Pratt & Whitney and Rolls-Royce powerplants as on the -200, as well as updated General Electric CF6-80C2B1 engines.
Swissair placed the first order for the 747-300 on June 11, 1980. The variant revived the 747-300 designation, which had been previously used on a design study that did not reach production. The 747-300 first flew on October 5, 1982, and the type's first delivery went to Swissair on March 23, 1983. Besides the passenger model, two other versions (-300M, -300SR) were produced. The 747-300M features cargo capacity on the rear portion of the main deck, similar to the -200M, but with the stretched upper deck it can carry more passengers. The 747-300SR, a short range, high-capacity domestic model, was produced for Japanese markets with a maximum seating for 584. No production freighter version of the 747-300 was built, but Boeing began modifications of used passenger -300 models into freighters in 2000.
A total of 81 747-300 series aircraft were delivered, 56 for passenger use, 21 -300M and 4 -300SR versions. In 1985, just two years after the -300 entered service, the type was superseded by the announcement of the more advanced 747-400. The last 747-300 was delivered in September 1990 to Sabena. While some -300 customers continued operating the type, several large carriers replaced their 747-300s with 747-400s. Air France, Air India, Pakistan International Airlines, and Qantas were some of the last major carriers to operate the 747-300. On December 29, 2008, Qantas flew its last scheduled 747-300 service, operating from Melbourne to Los Angeles via Auckland.
747-400.
The 747-400 is an improved model with increased range. It has wingtip extensions of and winglets of , which improve the type's fuel efficiency by four percent compared to previous 747 versions. The 747-400 introduced a new glass cockpit designed for a flight crew of two instead of three, with a reduction in the number of dials, gauges and knobs from 971 to 365 through the use of electronics. The type also features tail fuel tanks, revised engines, and a new interior. The longer range has been used by some airlines to bypass traditional fuel stops, such as Anchorage. Powerplants include the Pratt & Whitney PW4062, General Electric CF6-80C2, and Rolls-Royce RB211-524.
The was offered in passenger (-400), freighter (-400F), combi (-400M), domestic (-400D), extended range passenger (-400ER), and extended range freighter (-400ERF) versions. Passenger versions retain the same upper deck as the , while the freighter version does not have an extended upper deck. The 747-400D was built for short-range operations with maximum seating for 624. Winglets were not included, but they can be retrofitted. Cruising speed is up to Mach 0.855 on different versions of the 747-400.
The passenger version first entered service in February 1989 with launch customer Northwest Airlines on the Minneapolis to Phoenix route. The combi version entered service in September 1989 with KLM, while the freighter version entered service in November 1993 with Cargolux. The 747-400ERF entered service with Air France in October 2002, while the 747-400ER entered service with Qantas, its sole customer, in November 2002. In January 2004, Boeing and Cathay Pacific launched the Boeing 747-400 Special Freighter program, later referred to as the Boeing Converted Freighter (BCF), to modify passenger 747-400s for cargo use. The first 747-400BCF was redelivered in December 2005.
In March 2007, Boeing announced that it had no plans to produce further passenger versions of the -400. However, orders for 36 -400F and -400ERF freighters were already in place at the time of the announcement. The last passenger version of the 747-400 was delivered in April 2005 to China Airlines. Some of the last built 747-400s were delivered with Dreamliner livery along with the modern Signature interior from the Boeing 777. A total of 694 of the 747-400 series aircraft were delivered. At various times, the largest 747-400 operator has included Singapore Airlines, Japan Airlines, and British Airways with 57 .
747 LCF Dreamlifter.
The 747-400 Dreamlifter (originally called the 747 Large Cargo Freighter or LCF) is a Boeing-designed modification of existing 747-400s to a larger configuration to ferry 787 Dreamliner sub-assemblies. Evergreen Aviation Technologies Corporation of Taiwan was contracted to complete modifications of 747-400s into Dreamlifters in Taoyuan. The aircraft flew for the first time on September 9, 2006 in a test flight. Modification of four aircraft was completed by February 2010. The Dreamlifters have been placed into service transporting sub-assemblies for the 787 program to the Boeing plant in Everett, Washington, for final assembly. The aircraft is certified to carry only essential crew and not passengers.
747-8.
Boeing announced a new 747 variant, the 747-8, on November 14, 2005. Referred to as the 747 Advanced prior to its launch, the 747-8 uses the same engine and cockpit technology as the 787, hence the use of the "8". The variant is designed to be quieter, more economical, and more environmentally friendly. The 747-8's fuselage is lengthened from 232 to 251 feet (70.8 to 76.4 m), marking the first stretch variant of the aircraft. Power is supplied by General Electric GEnx-2B67 engines.
The 747-8 Freighter, or 747-8F, is derived from the 747-400ERF. The variant has 16 percent more payload capacity than its predecessor, allowing it to carry seven additional standard air cargo containers, with a maximum payload capacity of 154 tons (140 tonnes) of cargo. As on previous 747 freighters, the 747-8F features an overhead nose-door and a side-door on the main deck plus a side-door on the lower deck ("belly") to aid loading and unloading. The 747-8F made its maiden flight on February 8, 2010. The variant received its amended type certificate jointly from the FAA and the European Aviation Safety Agency (EASA) on August 19, 2011. The -8F was first delivered to Cargolux on October 12, 2011.
The passenger version, named 747-8 Intercontinental or 747-8I, is designed to carry up to 467 passengers in a 3-class configuration and fly more than at Mach 0.855. As a derivative of the already common 747-400, the 747-8 has the economic benefit of similar training and interchangeable parts. The type's first test flight occurred on March 20, 2011. At its introduction, the 747-8 surpassed the Airbus A340-600 as the world's longest airliner. The first -8I was delivered in May 2012 to Lufthansa. The 747-8 has received 125 total orders, including 74 for the -8F and 51 for the -8I, .
Undeveloped variants.
Boeing has studied a number of 747 variants that have not gone beyond the concept stage.
747 trijet.
During the late 1960s and early 1970s, Boeing studied the development of a shorter 747 with three engines, to compete with the smaller L-1011 TriStar and DC-10. The 747 trijet would have had more payload, range, and passenger capacity than the L-1011 and DC-10. The center engine would have been fitted in the tail with an S-duct intake similar to the L-1011's. However, engineering studies showed that a total redesign of the 747 wing would be necessary. Maintaining the same 747 handling characteristics would be important to minimize pilot retraining. Boeing decided instead to pursue a shortened four-engine 747, resulting in the 747SP.
747 ASB.
Boeing announced the 747 ASB ("Advanced Short Body") in 1986 as a response to the Airbus A340 and the McDonnell Douglas MD-11. This aircraft design would have combined the advanced technology used on the 747-400 with the foreshortened 747SP fuselage. The aircraft was to carry 295 passengers a range of . However, airlines were not interested in the project and it was canceled in 1988 in favor of the 777.
747-500X, -600X, and -700X.
Boeing announced the 747-500X and -600X at the 1996 Farnborough Airshow. The proposed models would have combined the 747's fuselage with a new 251 ft (77 m) span wing derived from the 777. Other changes included adding more powerful engines and increasing the number of tires from two to four on the nose landing gear and from 16 to 20 on the main landing gear.
The 747-500X concept featured an increased fuselage length of 18 ft (5.5 m) to 250 ft (76.2 m) long, and the aircraft was to carry 462 passengers over a range up to 8,700 nautical miles (10,000 mi, 16,100 km), with a gross weight of over 1.0 Mlb (450 tonnes). The 747-600X concept featured a greater stretch to 279 ft (85 m) with seating for 548 passengers, a range of up to 7,700 nmi (8,900 mi, 14,300 km), and a gross weight of 1.2 Mlb (540 tonnes). A third study concept, the 747-700X, would have combined the wing of the 747-600X with a widened fuselage, allowing it to carry 650 passengers over the same range as a 747-400. The cost of the changes from previous 747 models, in particular the new wing for the 747-500X and -600X, was estimated to be more than US$5 billion. Boeing was not able to attract enough interest to launch the aircraft.
747X and 747X Stretch.
As Airbus progressed with its A3XX study, Boeing offered a 747 derivative as an alternative in 2000; a more modest proposal than the previous -500X and -600X that retained the 747's overall wing design and add a segment at the root, increasing the span to . Power would have been supplied by either the Engine Alliance GP7172 or the Rolls-Royce Trent 600, which were also proposed for the 767-400ERX. A new flight deck based on the 777's would be used. The 747X aircraft was to carry 430 passengers over ranges of up to 8,700 nmi (10,000 mi, 16,100 km). The 747X Stretch would be extended to long, allowing it to carry 500 passengers over ranges of up to 7,800 nmi (9,000 mi, 14,500 km). Both would feature an interior based on the 777. Freighter versions of the 747X and 747X Stretch were also studied.
Like its predecessor, the 747X family was unable to garner enough interest to justify production, and it was shelved along with the 767-400ERX in March 2001, when Boeing announced the Sonic Cruiser concept. Though the 747X design was less costly than the 747-500X and -600X, it was criticized for not offering a sufficient advance from the existing 747-400. The 747X did not make it beyond the drawing board, but the 747-400X being developed concurrently moved into production to become the 747-400ER.
747-400XQLR.
After the end of the 747X program, Boeing continued to study improvements that could be made to the 747. The 747-400XQLR (Quiet Long Range) was meant to have an increased range of 7,980 nmi (9,200 mi, 14,800 km), with improvements to boost efficiency and reduce noise. Improvements studied included raked wingtips similar to those used on the 767-400ER and a sawtooth engine nacelle for noise reduction. Although the 747-400XQLR did not move to production, many of its features were used for the 747 Advanced, which has now been launched as the 747-8.
Accidents and incidents.
, the 747 has been involved in 131 accidents or incidents, including 60 hull-loss accidents, resulting in 3,718 fatalities. The 747 has been in 31 hijackings, which caused 24 fatalities.
Few crashes have been attributed to design flaws of the 747. The Tenerife airport disaster resulted from pilot error and communications failure, while the Japan Airlines Flight 123 and China Airlines Flight 611 crashes stemmed from improper aircraft repair. United Airlines Flight 811, which suffered an explosive decompression mid-flight on February 24, 1989, led the National Transportation Safety Board (NTSB) to issue a recommendation that 747-200 cargo doors similar to those on the Flight 811 aircraft be modified. Korean Air Lines Flight 007 was shot down by a Soviet fighter aircraft in 1983 after it had strayed into Soviet territory, causing U.S. President Ronald Reagan to authorize the then-strictly military global positioning system (GPS) for civilian use.
Accidents due to design errors included TWA Flight 800, where a 747-100 exploded in mid-air on July 17, 1996, probably due to sparking electricity wires inside the fuel tank; this finding led the FAA to propose a rule requiring installation of an inerting system in the center fuel tank of most large aircraft that was adopted in July 2008, after years of research into solutions. At the time, the new safety system was expected to cost US$100,000 to $450,000 per aircraft and weigh approximately . El Al Flight 1862 crashed after the fuse pins for an engine broke off shortly after take-off due to metal fatigue. Instead of dropping away from the wing, the engine knocked off the adjacent engine and damaged the wing.
Aircraft on display.
As increasing numbers of "classic" 747-100 and 747-200 series aircraft have been retired, some have found their way into museums or other uses. The "City of Everett", the first 747 and prototype, is at the Museum of Flight, Seattle, Washington, USA where it is sometimes leased to Boeing for test purposes.
Other 747s in museums include those at the Aviodrome, Lelystad, The Netherlands; the Qantas Founders Outback Museum, Longreach, Queensland, Australia; Rand Airport, Johannesburg, South Africa; Technikmuseum Speyer, Speyer, Germany; Musée de l'Air et de l'Espace, Paris, France; Tehran Aerospace Exhibition, Tehran, Iran; Jeongseok Aviation Center, Jeju, South Korea, Evergreen Aviation & Space Museum, McMinnville, Oregon, and the National Air and Space Museum, Washington, D.C.
Other uses.
Upon its retirement from service, the 747 number two in the production line was dismantled and shipped to Hopyeong, Namyangju, Gyeonggi-do, South Korea where it was re-assembled, repainted in a livery similar to that of Air Force One and converted into a restaurant. Originally flown commercially by Pan Am as N747PA, "Clipper Juan T. Trippe", and repaired for service following a tailstrike, it stayed with the airline until its bankruptcy. The restaurant closed by 2009, and the aircraft was scrapped in 2010.
A former British Airways 747-200B, G-BDXJ, is parked at the Dunsfold Aerodrome in Surrey, England and has been used as a movie set for productions such as the 2006 James Bond film, "Casino Royale". The plane also appears frequently in the BBC television series Top Gear, which is filmed at Dunsfold.
The "Jumbohostel", using a converted 747-200, opened at Arlanda Airport, Stockholm on January 15, 2009.
The wings of a 747 have been recycled as roofs of a house in Malibu, California.
Specifications.
[[File:Giant planes comparison.svg|thumb|upright=0.9|Comparison between four of the largest aircraft:
Sources: Boeing 747 specifications, 747 airport planning report, 747-8 airport brochure, Lufthansa 747-8 data Being fact sheet
Notable appearances in media.
Following its debut, the 747 rapidly achieved iconic status, appearing in numerous film productions such as the "Airport" series of disaster films, "Air Force One", and "Executive Decision". Appearing in over 300 film productions the 747 is one of the most widely depicted civilian aircraft and is considered by many as one of the most iconic in film history. The aircraft entered the cultural lexicon as the original "Jumbo Jet", a term coined by the aviation media to describe its size, and was also nicknamed "Queen of the Skies."

</doc>
<doc id="4615" url="https://en.wikipedia.org/wiki?curid=4615" title="Battle of Agincourt">
Battle of Agincourt

The Battle of Agincourt (; in French, Azincourt ) was a major English victory in the Hundred Years' War. The battle took place on Friday, 25 October 1415 (Saint Crispin's Day), near Azincourt, in northern France. Henry V's victory at Agincourt, against a numerically superior French army, crippled France and started a new period in the war during which Henry V married the French king's daughter, and their son, later Henry VI of England and Henry II of France, was made heir to the throne of France as well as of England.
Henry V led his troops into battle and participated in hand-to-hand fighting. The French king of the time, Charles VI, did not command the French army himself as he suffered from severe psychotic illnesses with moderate mental incapacitation. Instead, the French were commanded by Constable Charles d'Albret and various prominent French noblemen of the Armagnac party.
This battle is notable for the use of the English longbow in very large numbers, with English and Welsh archers forming most of Henry's army. The battle is the centrepiece of the play "Henry V" by William Shakespeare.
Contemporary accounts.
The Battle of Agincourt is well documented by at least seven contemporary accounts, three of them eyewitnesses. The approximate location of the battle has never been in dispute and the place remains relatively unaltered even after 600 years. Immediately after the battle, Henry summoned the heralds of the two armies who had watched the battle together, and with the principal French herald, Montjoie, settled on the name of the battle as Azincourt, after the nearest fortified place. Two of the most frequently cited accounts come from Burgundian sources: one from Jean Le Fevre de Saint-Remy, who was present at the battle, and the other from Enguerrand de Monstrelet. The English eyewitness account comes from the anonymous "Gesta Henrici Quinti", believed to have been written by a chaplain in the King's household, who would have been in the baggage train at the battle. A recent reappraisal of Henry's strategy of the Agincourt campaign incorporates these three accounts, and argues that war was seen as a legal due process for solving the disagreement over claims to the French throne.
Campaign.
Henry V invaded France following the failure of negotiations with the French. He claimed the title of King of France through his great-grandfather Edward III, although in practice the English kings were generally prepared to renounce this claim if the French would acknowledge the English claim on Aquitaine and other French lands (the terms of the Treaty of Brétigny). He initially called a Great Council in the spring of 1414 to discuss going to war with France, but the lords insisted that he should negotiate further and moderate his claims. In the following negotiations Henry said that he would give up his claim to the French throne if the French would pay the 1.6 million crowns outstanding from the ransom of John II (who had been captured at the Battle of Poitiers in 1356), and concede English ownership of the lands of Normandy, Touraine, Anjou, Brittany and Flanders, as well as Aquitaine. Henry would marry Princess Catherine, the young daughter of Charles VI, and receive a dowry of 2 million crowns. The French responded with what they considered the generous terms of marriage with Princess Catherine, a dowry of 600,000 crowns, and an enlarged Aquitaine. By 1415, negotiations had ground to a halt, with the English claiming that the French had mocked their claims and ridiculed Henry himself. In December 1414, the English parliament was persuaded to grant Henry a "double subsidy", a tax at twice the traditional rate, to recover his inheritance from the French. On 19 April 1415, Henry again asked the Great Council to sanction war with France, and this time they agreed.
Henry's army landed in northern France on 13 August 1415, carried by a fleet described by Shakespeare as "a city on the inconstant billows dancing / For so appears this fleet majestical", often reported to comprise 1,500 ships, but probably far smaller, and besieged the port of Harfleur with an army of about 12,000, and up to 20,000 horses. The siege took longer than expected. The town surrendered on 22 September, and the English army did not leave until 8 October. The campaign season was coming to an end, and the English army had suffered many casualties through disease. Rather than retire directly to England for the winter, with his costly expedition resulting in the capture of only one town, Henry decided to march most of his army (roughly 9,000) through Normandy to the port of Calais, the English stronghold in northern France, to demonstrate by his presence in the territory at the head of an army that his right to rule in the duchy was more than a mere abstract legal and historical claim. He also intended the manoeuvre as a deliberate provocation to battle aimed at the dauphin, who had failed to respond to Henry's personal challenge to combat at Harfleur.
The French had raised an army during the siege which assembled around Rouen. This was not strictly a feudal army, but an army paid through a system similar to the English. The French hoped to raise 9,000 troops, but the army was not ready in time to relieve Harfleur. After Henry V marched to the north, the French moved to blockade them along the River Somme. They were successful for a time, forcing Henry to move south, away from Calais, to find a ford. The English finally crossed the Somme south of Péronne, at Béthencourt and Voyennes and resumed marching north. Without the river protection, the French were hesitant to force a battle. They shadowed Henry's army while calling a "semonce des nobles", calling on local nobles to join the army. By 24 October, both armies faced each other for battle, but the French declined, hoping for the arrival of more troops. The two armies spent the night of 24 October on open ground. The next day the French initiated negotiations as a delaying tactic, but Henry ordered his army to advance and to start a battle that, given the state of his army, he would have preferred to avoid, or to fight defensively: that was how Crécy and the other famous longbow victories had been won. The English had very little food, had marched in two and a half weeks, were suffering from sickness such as dysentery, and faced much larger numbers of well equipped French men at arms. The French army blocked Henry's way to the safety of Calais, however, and delaying battle would only further weaken his tired army and allow more French troops to arrive.
Battle.
The battlefield.
The location of the battle is not precisely fixed in contemporary accounts. Most authors believe it was fought in the narrow strip of open land formed between the woods of Tramecourt and Azincourt (close to the modern village of Azincourt). However, the lack of archaeological evidence at this traditional site has led to suggestions it may have been fought to the west of Azincourt. 
English deployment.
Early on the 25th, Henry deployed his army (approximately 1,500 men-at-arms and 7,000 longbowmen) across a part of the defile. The army was organised into three "battles" or divisions: the vanguard, led by the Duke of York; the main battle led by Henry himself; and the rearguard, led by Lord Camoys. In addition, Sir Thomas Erpingham, one of Henry's most experienced household knights, had a role in marshalling the archers. It is likely that the English adopted their usual battle line of longbowmen on either flank, with men-at-arms and knights in the centre. They may also have deployed some archers in the centre of the line. The English men-at-arms in plate and mail were placed shoulder to shoulder four deep. The English and Welsh archers on the flanks drove pointed wooden stakes, or palings, into the ground at an angle to force cavalry to veer off. This use of stakes may have been inspired by the Battle of Nicopolis of 1396, where forces of the Ottoman Empire used the tactic against French cavalry.
The English made their confessions before the battle, as was customary. Henry, worried about the enemy launching surprise raids, and wanting his troops to remain focused, ordered all his men to spend the night before the battle in silence, on pain of having an ear cut off. He told his men that he would rather die in the coming battle than be captured and ransomed.
Henry made a speech emphasising the justness of his cause, and reminding his army of previous great defeats the kings of England had inflicted on the French. The Burgundian sources have him concluding the speech by telling his men that the French had boasted that they would cut off two fingers from the right hand of every archer, so that he could never draw a longbow again. Whether this was true is open to question; as previously noted, death was the normal fate of any soldier who could not be ransomed.
French deployment.
The French force was not only larger than that of the English, their noble men-at-arms would have considered themselves superior to the large number of archers in the English army, whom the French (based on their experience in recent memory of using and facing archers) considered relatively insignificant. For example, the chronicler Edmond de Dyntner stated that there were "ten French nobles against one English", ignoring the archers completely. Several French accounts emphasise that the French leaders were so eager to defeat the English (and win the ransoms of the English men-at-arms) that they insisted on being in the first line; as one of the contemporary accounts put it: "All the lords wanted to be in the vanguard, against the opinion of the constable and the experienced knights."
The French were arrayed in three lines or "battles". The first line was led by Constable d'Albret, Marshal Boucicault, and the Dukes of Orléans and Bourbon, with attached cavalry wings under the Count of Vendôme and Sir Clignet de Brebant. The second line was commanded by the Dukes of Bar and Alençon and the Count of Nevers. The third line was under the Counts of Dammartin and Fauconberg. The Burgundian chronicler, Jean de Wavrin, writes that there were 8,000 men-at-arms, 4,000 archers and 1,500 crossbowmen in the vanguard, with two wings of 600 and 800 mounted men-at-arms, and the main battle having "as many knights, esquires and archers as in the vanguard", with the rearguard containing "all of the rest of the men-at-arms". The Herald of Berry uses somewhat different figures of 4,800 men-at-arms in the first line, 3,000 men in the second line, with two "wings" containing 600 mounted men-at-arms each, and a total of "10,000 men-at-arms", but does not mention a third line.
Thousands of troops appear to have been in the rearguard, containing servants and commoners whom the French were either unable or unwilling to deploy. Wavrin gives the total French army size as 50,000: "They had plenty of archers and crossbowmen but nobody wanted to let them fire . The reason for this was that the site was so narrow that there was only enough room for the men-at-arms." A different source says that the French did not even deploy 4,000 of the best crossbowmen "on the pretext they had no need of their help".
Terrain.
The field of battle was arguably the most significant factor in deciding the outcome. The recently ploughed land hemmed in by dense woodland favoured the English, both because of its narrowness, and because of the thick mud through which the French knights had to walk. An analysis by "Battlefield Detectives" has looked at the crowd dynamics of the battlefield.
The "Battlefield Detectives" episode states that when the density reached four men per square metre, soldiers would not even be able to take full steps forward, slowing the speed of the advance by 70%. Accounts of the battle describe the French engaging the English men-at-arms before being rushed from the sides by the longbowmen as the mêlée developed. The English account in the "Gesta Henrici" says: "For when some of them, killed when battle was first joined, fall at the front, so great was the undisciplined violence and pressure of the mass of men behind them that the living fell on top of the dead, and others falling on top of the living were killed as well."
Although the French initially pushed the English back, they became so closely packed that they were described as having trouble using their weapons properly. The French monk of St. Denis says: "Their vanguard, composed of about 5,000 men, found itself at first so tightly packed that those who were in the third rank could scarcely use their swords," and the Burgundian sources have a similar passage.
As the battle was fought on a recently ploughed field, and there had recently been heavy rain leaving it very muddy, it proved very tiring to walk through in full plate armour. The French monk of St. Denis describes the French troops as "marching through the middle of the mud where they sank up to their knees. So they were already overcome with fatigue even before they advanced against the enemy". The deep, soft mud particularly favoured the English force because, once knocked to the ground, the heavily armoured French knights had a hard time getting back up to fight in the mêlée. Barker states that some knights, encumbered by their armour, actually drowned in their helmets.
Fighting.
Opening moves.
On the morning of 25 October, the French were still waiting for additional troops to arrive. The Duke of Brabant (about 2,000 men), the Duke of Anjou (about 600 men), and the Duke of Brittany (6,000 men, according to Monstrelet), were all marching to join the army.
For three hours after sunrise there was no fighting. Military textbooks of the time stated: "Everywhere and on all occasions that foot soldiers march against their enemy face to face, those who march lose and those who remain standing still and holding firm win." On top of this, the French were expecting thousands of men to join them if they waited. They were blocking Henry's retreat, and were perfectly happy to wait for as long as it took. There had even been a suggestion that the English would run away rather than give battle when they saw that they would be fighting so many French princes.
Henry's men, on the other hand, were already very weary from hunger, illness and marching. Even though Henry knew as well as the French did that his army would perform better on the defensive, he was eventually forced to take a calculated risk, and move his army farther forward to start the battle. This entailed abandoning his chosen position and pulling out, advancing, and then re-installing the long sharpened wooden stakes pointed outwards toward the enemy, which helped protect the longbowmen from cavalry charges. (The use of stakes was an innovation for the English: during the Battle of Crécy, for example, the archers had been instead protected by pits and other obstacles.)
The tightness of the terrain also seems to have restricted the planned deployment of the French forces. The French had originally drawn up a battle plan that had archers and crossbowmen in front of their men-at-arms, with a cavalry force at the rear specifically designed to "fall upon the archers, and use their force to break them," but in the event, the French archers and crossbowmen were deployed "behind" and to the sides of the men-at-arms (where they seem to have played almost no part, except possibly for an initial volley of arrows at the start of the battle). The cavalry force, which could have devastated the English line if it had attacked while they moved their stakes, charged only "after" the initial volley of arrows from the English. It is unclear whether the delay occurred because the French were hoping the English would launch a frontal assault (and were surprised when the English instead started shooting from their new defensive position), or whether the French mounted knights instead did not react quickly enough to the English advance. French chroniclers agree that when the mounted charge did come, it did not contain as many men as it should have; Gilles le Bouvier states that some had wandered off to warm themselves and others were walking or feeding their horses.
French cavalry attack.
The French cavalry, despite being somewhat disorganised and not at full numbers, charged towards the longbowmen, but it was a disaster, with the French knights unable to outflank the longbowmen (because of the encroaching woodland) and unable to charge through the forest of sharpened stakes that protected the archers. John Keegan argues that the longbows' main influence on the battle at this point was injuries to horses: armoured only on the head, many horses would have become dangerously out of control when struck in the back or flank from the high-elevation long range shots used as the charge started. The mounted charge and subsequent retreat churned up the already muddy terrain between the French and the English. Juliet Barker quotes a contemporary account by a monk of St. Denis who reports how the wounded and panicking horses galloped through the advancing infantry, scattering them and trampling them down in their headlong flight from the battlefield.
Main French assault.
The plate armour of the French men-at-arms allowed them to close the 300 yards or so to the English lines while being under what the French monk of Saint Denis described as "a terrifying hail of arrow shot". A complete coat of plate was considered such good protection that shields were generally not used, although the Burgundian contemporary sources specifically distinguish between Frenchmen who used shields and those who did not, and Rogers has suggested that the front elements of the French force may have used axes and shields. Modern historians are somewhat divided on how effective the longbow fire would have been against plate armour of the time, with some modern texts suggesting that arrows could not penetrate, especially the better quality steel armour, but others suggesting arrows could penetrate, especially the poorer quality wrought iron armour. Rogers suggests that the longbow could penetrate a wrought iron breastplate at short range and penetrate the thinner armour on the limbs even at . He considers a knight in the best quality steel armour would have been more or less invulnerable to an arrow on the breastplate or top of the helmet, but would still have been vulnerable to shots hitting the limbs, particularly at close range. In any case, to protect themselves as much as possible from the arrows, the French had to lower their visors and bend their helmeted heads to avoid being shot in the face—the eye and air-holes in their helmets were among the weakest points in the armour. This head lowered position restricted both their breathing and their vision. Then they had to walk a few hundred yards through thick mud, a press of comrades and wearing armour weighing , gathering sticky clay all the way. Increasingly they had to walk around or over fallen comrades.
The surviving French men-at-arms reached the front of the English line and pushed it back, with the longbowmen on the flanks continuing to shoot at point-blank range. When the archers ran out of arrows, they dropped their bows and using hatchets, swords and the mallets they had used to drive their stakes in, attacked the now disordered, fatigued and wounded French men-at-arms massed in front of them. The French could not cope with the thousands of lightly armoured longbowmen assailants (who were much less hindered by the mud and weight of their armour) combined with the English men-at-arms. The impact of thousands of arrows, combined with the slog in heavy armour through the mud, the heat and lack of oxygen in plate armour with the visor down, and the crush of their numbers meant the French men-at-arms could "scarcely lift their weapons" when they finally engaged the English line. The exhausted French men-at-arms are described as being knocked to the ground by the English and then unable to get back up. As the mêlée developed, the French second line also joined the attack, but they too were swallowed up, with the narrow terrain meaning the extra numbers could not be used effectively. Rogers suggests that the French at the back of their deep formation would have been attempting to push forward and quite literally add their weight to the advance, without realising that they were hindering the ability of those at the front to manoeuvre and fight, actually pushing them into the English formation of lancepoints. After the initial wave, the French would have had to fight over and on the bodies of those who had fallen before them. In such a "press" of thousands of men, Rogers finds it plausible that a significant number could have suffocated in their armour, as is described by several sources, and is also known to have happened in other battles.
The French men-at-arms were taken prisoner or killed in the thousands. The fighting lasted about three hours, but eventually the leaders of the second line were killed or captured, as those of the first line had been. The English "Gesta Henrici" describes three great heaps of the slain around the three main English standards.
According to contemporary English accounts, Henry was directly involved in the hand-to-hand fighting. Upon hearing that his youngest brother Humphrey, Duke of Gloucester had been wounded in the groin, Henry took his household guard and stood over his brother, in the front rank of the fighting, until Humphrey could be dragged to safety. The king received an axe blow to the head, which knocked off a piece of the crown that formed part of his helmet.
Attack on the English baggage train.
The only French success was an attack on the lightly protected English baggage train, with Ysembart d'Azincourt (leading a small number of men-at-arms and varlets plus about 600 peasants) seizing some of Henry's personal treasures, including a crown. Whether this was part of a deliberate French plan or an act of local brigandage is unclear from the sources. Certainly, d'Azincourt was a local knight but he may have been chosen to lead the attack because of his local knowledge and the lack of availability of a more senior soldier. In some accounts the attack happened towards the end of the battle, and led the English to think they were being attacked from the rear. Barker, following the "Gesta Henrici", believed to have been written by an English chaplain who was actually in the baggage train, concludes that the attack happened at the "start" of the battle.
Henry orders the killing of the prisoners.
Regardless of when the baggage assault happened, at some point after the initial English victory, Henry became alarmed that the French were regrouping for another attack. The "Gesta Henrici" places this after the English had overcome the onslaught of the French men-at-arms and the weary English troops were eyeing the French rearguard ("in incomparable number and still fresh"). Le Fevre and Wavrin similarly say that it was signs of the French rearguard regrouping and "marching forward in battle order" which made the English think they were still in danger. It seems it was purely a decision of Henry, since the English knights found it contrary to chivalry, and contrary to their interests to kill valuable hostages for whom it was commonplace to ask ransom. Henry threatened to hang whoever did not obey his orders.
In any event, Henry ordered the slaughter of what were perhaps several thousand French prisoners, sparing only the most high ranked (presumably those most likely to fetch a large ransom under the chivalric system of warfare). According to most chroniclers, Henry's fear was that the prisoners (who, in an unusual turn of events, actually outnumbered their captors) would realize their advantage in numbers, rearm themselves with the weapons strewn about the field and overwhelm the exhausted English forces. Contemporary chroniclers did not criticise him for it. In his study of the battle, John Keegan argued that the main aim was not to actually kill the French knights but rather to terrorise them into submission and quell any possibility they might resume the fight, which would probably have caused the uncommitted French reserve forces to join the fray, as well. Such an event would have posed a risk to the still-outnumbered English and could have easily turned a stunning victory into a mutually-destructive defeat, as the English forces were now largely intermingled with the French and would have suffered grievously from the arrows of their own longbowmen had they needed to resume shooting. Keegan also speculated that due to the relatively low number of archers actually involved in killing the French knights (roughly 200 by his estimate), together with the refusal of the English knights to assist in a duty they saw as distastefully unchivalrous and combined with the sheer difficulty of killing such a large number of prisoners in such a short space of time, the actual number of French knights killed might not have even reached the hundreds before the reserves fled the field and Henry called an end to the slaughter.
Aftermath.
The lack of reliable sources makes it impossible to give a precise figure for the French and English casualties (dead, wounded, taken prisoner). However, it is clear that though the English were outnumbered, their losses were far lower than those of the French. The French sources all give 4,000–10,000 French dead, with up to 1,600 English dead. The lowest ratio in these French sources has the French losing six times more men than the English. The English sources vary between about 1,500 and 11,000 for the French dead, with English dead put at no more than 100. Barker identifies from the available records "at least" 112 Englishmen killed in the fighting, including Edward of Norwich, 2nd Duke of York, a grandson of Edward III.
One widely used estimate puts the English casualties at 450, not an insignificant number in an army of about 8,500, but far fewer than the thousands the French lost, nearly all of whom were killed or captured. Using the lowest French estimate of their own dead of 4,000 would imply a ratio of nearly 9 to 1 in favour of the English, or over 10 to 1 if the prisoners are included.
The French suffered heavily. Three dukes, at least eight counts, a viscount, and an archbishop died, along with numerous other nobles. Of the great royal office holders, France lost her Constable, Admiral, Master of the Crossbowmen and "prévôt" of the marshals. The "baillis" of nine major northern towns were killed, often along with their sons, relatives and supporters. In the words of Juliet Barker, the battle "cut a great swath through the natural leaders of French society in Artois, Ponthieu, Normandy, Picardy." Estimates of the number of prisoners vary between 700 and 2,200, amongst them the Duke of Orléans (the famous poet Charles d'Orléans) and Jean Le Maingre (known as Boucicault) Marshal of France.
Although the victory had been militarily decisive, its impact was complex. It did not lead to further English conquests immediately as Henry's priority was to return to England, which he did on 16 November, to be received in triumph in London on the 23rd. Henry returned a conquering hero, in the eyes of his subjects and European powers outside France, blessed by God. It established the legitimacy of the Lancastrian monarchy and the future campaigns of Henry to pursue his "rights and privileges" in France. Other benefits to the English were longer term. Very quickly after the battle, the fragile truce between the Armagnac and Burgundian factions broke down. The brunt of the battle had fallen on the Armagnacs and it was they who suffered the majority of senior casualties and carried the blame for the defeat. The Burgundians seized on the opportunity and within 10 days of the battle had mustered their armies and marched on Paris. This lack of unity in France allowed Henry eighteen months to prepare militarily and politically for a renewed campaign. When that campaign took place, it was made easier by the damage done to the political and military structures of Normandy by the battle.
Notable casualties.
French.
Notable casualties (most named by Enguerrand de Monstrelet) include:
Leading officers:
Three dukes:
Seven counts (eight with d'Albret):
and some 90 bannerets and others, including:
English.
Notable casualties included: 
Prisoners.
Among the "circa" 1,500 prisoners taken by the English, were the following French notables:
Numbers at Agincourt.
Anne Curry in her 2005 book "Agincourt: A New History", argues (based on research into the surviving administrative records) that the French army was about 12,000 strong, and the English army about 9,000, giving proportions of four to three. By contrast, Juliet Barker in her book "Agincourt: The King, the Campaign, the Battle" (also published in 2005) argues the English and Welsh were outnumbered "at least four to one and possibly as much as six to one". She suggests figures of about 6,000 for the English and 36,000 for the French, based on the "Gesta Henrici"s figures of 5,000 archers and 900 men-at-arms for the English, and Jean de Wavrin's statement "that the French were six times more numerous than the English". The 2009 "Encyclopædia Britannica" uses the figures of about 6,000 for the English and 20,000 to 30,000 for the French. The 1911 "Britannica" used somewhat different figures of 6,000 archers, 1,000 men-at-arms and "a few thousands of other foot" for the English, with the French outnumbering them by "at least four times".
With one of the lowest estimates for the size of the French army and also one of the highest estimates for the size of the English army, Curry is currently in a minority in suggesting that the ratio of forces were as near equal as four to three. While not necessarily agreeing with the exact numbers Curry uses, some historians have however given support to her assertion that the French army was much smaller than traditionally thought, and the English somewhat bigger. Bertrand Schnerb, a professor of medieval history at the University of Lille, has said that he thinks the French probably had 12,000–15,000 troops. Ian Mortimer, in his 2009 book "1415: Henry V's Year of Glory", notes how Curry "minimises French numbers (by limiting her figures to those in the basic army and a few specific additional companies) and maximises English numbers (by assuming the numbers sent home from Harfleur were no greater than sick lists)", but agrees that previous estimates have exaggerated the odds, and suggests that "the most extreme imbalance which is credible is fifteen thousand French troops against 8,100 English: a ratio of about two-to-one".
However, Clifford J. Rogers, professor of history at the United States Military Academy at West Point, has recently argued that archival records are too incomplete to substantially change his view that the English were outnumbered about 4–1. Juliet Barker also disagrees with Curry's arguments in the acknowledgements section of her 2005 book on Agincourt, saying: "Surviving administrative records on both sides, but especially the French, are simply too incomplete to support [Curry's] assertion that nine thousand English were pitted against an army only twelve thousand strong. And if the differential really was as low as three to four then this makes a nonsense of the course of the battle as described by eyewitnesses and contemporaries."
Those supporting a greater imbalance have generally put more store by contemporary (and especially eyewitness) accounts. The "Gesta Henrici" gives plausible figures for the English of 5,000 archers and 900 men-at-arms, but Mortimer notes it is "wildly inaccurate" in stating the English were outnumbered 30–1, and there have also been doubts as to how much it was written as propaganda for Henry V. The proportions also seem incorrect, as from surviving records we know that Henry set out with about four times as many archers as men-at-arms, not five and a half times as many. Those who have supported the Gesta figures for the English army have generally thought that although the English army may have left Harfleur with eight or nine thousand men, it is plausible that after weeks of campaigning and disease in hostile territory they would have lost two or three thousand fighting men; however Mortimer states: "Despite the trials of the march, Henry had lost very few men to illness or death; and we have independent testimony that no more than 160 had been captured on the way." 
As Mortimer notes, the Burgundian numbers for the size of the French vanguard of 8,000 men-at-arms in the vanguard with 1,400 (or 2,400) men-at-arms in the wings correspond roughly with the figures of ten thousand men-at-arms recorded by the duke of Berry's herald. The Burgundians also recorded 4,000 archers and 1,500 crossbowmen in the "vanguard", which would suggest "fourteen or fifteen thousand fighting men". (It should be noted that the Burgundians actually give the total size of the French army as an implausible 50,000, and the numbers they use do not correspond closely to the odds they describe. Using very similar numbers, Jean Le Fevre states that the English were outnumbered 3–1, whereas Wavrin states that the English were outnumbered 6–1.)
One particular cause of confusion may have been the number of servants on both sides. Mortimer suggests that because there were a much higher proportion of men-at-arms on the French side, the number of non-combatants was much higher. Each man-at-arms could be expected to have a page, who would have ridden one of his spare horses. If the French army had an extra 10,000 mounted men (as opposed to only 1,500 extra for the English), then "the English probably did see an army about three times the size of their own fighting force".
It is open to debate whether these should all be counted as non-combatants; Rogers (for example) accepts that the French probably had about 10,000 men-at-arms, but explicitly includes one "gros valet" (an armed, armoured and mounted military servant) per French man-at-arms in his calculation of the odds.
Popular representations.
Soon after the English victory at Agincourt, a number of popular folk songs were created about the battle, the most famous being the "Agincourt Carol", produced in the first half of the 15th century. Other ballads followed, including "King Henry Fifth's Conquest of France", raising the popular prominence of particular events mentioned only in passing by the original chroniclers, such as the gift of tennis balls before the campaign.
The most famous cultural depiction of the battle today, however, is through William Shakespeare's "Henry V", written in 1599. The play focuses on the pressures of kingship, the tensions between how a king should appear – chivalric, honest and just – and how a king must sometimes act – Machiavellian and ruthless. These tensions are illustrated in the play by Shakespeare's depiction of Henry's decision to kill some of the French prisoners, whilst attempting to justify it and distance himself from the event – this moment of the battle is portrayed both as a break with the traditions of chivalry, and as key example of the paradox of kingship. Shakespeare's depiction of the battle also plays on the theme of modernity – Shakespeare contrasts the modern, English king and his army with the medieval, chivalric, older model of the French. Shakespeare's play presented Henry as leading a truly English force into battle, playing on the importance of the link between the monarch and the common soldiers in the fight. The original play does not, however, feature any scenes of the actual battle itself, leading critic Rose Zimbardo to characterise it as "full of warfare, yet empty of conflict."
The play introduced the famous St Crispin's Day Speech; Shakespeare has Henry give a moving narration to his soldiers just before the battle, urging his "band of brothers" to stand together in the forthcoming fight. One of Shakespeare's most heroic speeches, critic David Margolies describes how it "oozes honour, military glory, love of country and self-sacrifice", and it forms one of the first instances of English literature linking solidarity and comradeship to success in battle. Partially as a result, the battle was used as a metaphor at the beginning of the First World War, when the British Expeditionary Force's attempts to stop the German advances were widely likened to it.
Shakespeare's version of the battle of Agincourt has been turned into (several minor and) two major films – by Laurence Olivier in 1944, and by Kenneth Branagh in 1989. Made just prior to the invasion of Normandy, Olivier's gives the battle what Sarah Hatchuel has termed an "exhilarating and heroic" tone, with an artificial, cinematic look to the battle scenes. Branagh's version gives a longer, more Realist portrayal of the battle itself, drawing on both historical sources and images from the Vietnam and Falkland Wars. In his film adaptation, Peter Babakitis uses digital effects to exaggerate realist features during his battle scenes, producing a more avant-garde interpretation of the fighting at Agincourt. 
The battle remains an important symbol in popular culture. For example, a mock trial of Henry V for the crimes associated with the slaughter of the prisoners was held in Washington, D.C. in March 2010, drawing from both the historical record and Shakespeare's play. Participating as judges were Justices Samuel Alito and Ruth Bader Ginsburg. The trial ranged widely over whether there was just cause for war and not simply the prisoner issue. Although an audience vote was "too close to call", Henry was unanimously found guilty by the court on the basis of "evolving standards of civil society".

</doc>
<doc id="4616" url="https://en.wikipedia.org/wiki?curid=4616" title="Burgundian">
Burgundian

Burgundian can refer to any of the following:

</doc>
<doc id="4620" url="https://en.wikipedia.org/wiki?curid=4620" title="Bronze Age">
Bronze Age

The Bronze Age is a time period characterized by the use of bronze, proto-writing, and other early features of urban civilization. The Bronze Age is the second principal period of the three-age Stone-Bronze-Iron system, as proposed in modern times by Christian Jürgensen Thomsen, for classifying and studying ancient societies.
An ancient civilization is defined to be in the Bronze Age either by smelting its own copper and alloying with tin, arsenic, or other metals, or by trading for bronze from production areas elsewhere. Copper-tin ores are rare, as reflected in the fact that there were no tin bronzes in western Asia before trading in bronze began in the third millennium BC. Worldwide, the Bronze Age generally followed the Neolithic period, but in some parts of the world, the Copper Age served as a transition from the Neolithic to the Bronze Age. Although the Iron Age generally followed the Bronze Age, in some areas, the Iron Age intruded directly on the Neolithic from outside the region.
Bronze Age cultures differed in their development of the first writing. According to archaeological evidence, cultures in Mesopotamia (cuneiform) and Egypt (hieroglyphs) developed the earliest viable writing systems.
History.
The overall period is characterized by the full adoption of bronze in many regions, though the place and time of the introduction and development of bronze technology was not universally synchronous. Human-made tin bronze technology requires set production techniques. Tin must be mined (mainly as the tin ore cassiterite) and smelted separately, then added to molten copper to make bronze alloy. The Bronze Age was a time of extensive use of metals and of developing trade networks (See "Tin sources and trade in ancient times").
Near East.
The Bronze Age in the ancient Near East began with the rise of Sumer in the 4th millennium BC. Cultures in the ancient Near East (often called, one of "the cradles of civilization") practiced intensive year-round agriculture, developed a writing system, invented the potter's wheel, created a centralized government, law codes, and empires, and introduced social stratification, slavery, and organized warfare. Societies in the region laid the foundations for astronomy and mathematics.
Age sub-divisions.
The Ancient Near East Bronze Age can be divided as follows:
Mesopotamia.
In Mesopotamia, the Mesopotamian Bronze Age began about 2900 BC and ended with the Kassite period. The usual tripartite division into an Early, Middle and Late Bronze Age is not used. Instead, a division primarily based on art-historical and historical characteristics is more common. The cities of the Ancient Near East housed several tens of thousands of people. Ur in the Middle Bronze Age and Babylon in the Late Bronze Age similarly had large populations.
The earliest mention of Babylonia appears on a tablet from the reign of Sargon of Akkad in the 23rd century BC. The Amorite dynasty established the city-state of Babylon in the 19th century BC. Over 100 years later, it briefly took over the other city-states and formed the first Babylonian empire during what is also called the Old Babylonian Period. Babylonia adopted the written Semitic Akkadian language for official use. By that time, the Sumerian language was no longer spoken, but was still in religious use. The Akkadian and Sumerian traditions played a major role in later Babylonian culture, and the region, even under outside rule, remained an important cultural center throughout the Bronze and Early Iron Age.
Iranian Plateau.
Elam was an ancient civilization located to the east of Mesopotamia. In the Old Elamite period (Middle Bronze Age), Elam consisted of kingdoms on the Iranian plateau, centered in Anshan, and from the mid-2nd millennium BC, it was centered in Susa in the Khuzestan lowlands. Its culture played a crucial role in the Gutian Empire and especially during the Achaemenid dynasty that succeeded it.
The Oxus civilization was a Bronze Age Central Asian culture dated to ca. 2300–1700 BC and centered on the upper Amu Darya (Oxus). In the Early Bronze Age the culture of the Kopet Dag oases and Altyn-Depe developed a proto-urban society. This corresponds to level IV at Namazga-Depe. Altyn-Depe was a major centre even then. Pottery was wheel-turned. Grapes were grown. The height of this urban development was reached in the Middle Bronze Age c. 2300 BC, corresponding to level V at Namazga-Depe. This Bronze Age culture is called the Bactria–Margiana Archaeological Complex (BMAC).
The Kulli culture, similar to those of the Indus Valley Civilization, was located in southern Balochistan (Gedrosia) ca. 2500–2000 BC. Agriculture was the economical base of this people. At several places dams were found, providing evidence for a highly developed water management system.
Konar Sandal is associated with the hypothesized "Jiroft culture", a 3rd millennium BC culture postulated on the basis of a collection of artifacts confiscated in 2001.
Anatolia.
The Hittite Empire was established in Hattusa in northern Anatolia from the 18th century BC. In the 14th century BC, the Hittite Kingdom was at its height, encompassing central Anatolia, southwestern Syria as far as Ugarit, and upper Mesopotamia. After 1180 BC, amid general turmoil in the Levant conjectured to have been associated with the sudden arrival of the Sea Peoples, the kingdom disintegrated into several independent "Neo-Hittite" city-states, some of which survived until as late as the 8th century BC.
Arzawa in Western Anatolia during the second half of the second millennium BC likely extended along southern Anatolia in a belt that reaches from near the Turkish Lakes Region to the Aegean coast. Arzawa was the western neighbor—sometimes a rival and sometimes a vassal—of the Middle and New Hittite Kingdoms.
The Assuwa league was a confederation of states in western Anatolia that was defeated by the Hittites under an earlier Tudhaliya I, around 1400 BC. Arzawa has been associated with the much more obscure Assuwa generally located to its north. It probably bordered it, and may even be an alternative term for it (at least during some periods).
Levant.
In modern scholarship the chronology of the Bronze Age Levant is divided into Early/Proto Syrian; corresponding to the Early Bronze. Old Syrian; corresponding to the Middle Bronze. Middle Syrian; corresponding to the Late Bronze. The term Neo-Syria is used to designate the early Iron Age.
The old Syrian period was dominated by the Eblaite first kingdom, Nagar and the Mariote second kingdom. The Akkadian conquered large areas of the Levant and were followed by the Amorite kingdoms, ca. 2000–1600 BC, which arose in Mari, Yamkhad, Qatna, Assyria, From the 15th century BC onward, the term Amurru is usually applied to the region extending north of Canaan as far as Kadesh on the Orontes.
The earliest known Ugarit contact with Egypt (and the first exact dating of Ugaritic civilization) comes from a carnelian bead identified with the Middle Kingdom pharaoh Senusret I, 1971 BC–1926 BC. A stela and a statuette from the Egyptian pharaohs Senusret III and Amenemhet III have also been found. However, it is unclear at what time these monuments got to Ugarit. In the Amarna letters, messages from Ugarit ca. 1350 BC written by Ammittamru I, Niqmaddu II, and his queen, were discovered. From the 16th to the 13th century BC Ugarit remained in constant touch with Egypt and Cyprus (named Alashiya).
The Mitanni was a loosely organized state in northern Syria and south-east Anatolia from ca. 1500 BC–1300 BC. Founded by an Indo-Aryan ruling class that governed a predominately Hurrian population, Mitanni came to be a regional power after the Hittite destruction of Kassite Babylon created a power vacuum in Mesopotamia. At its beginning, Mitanni's major rival was Egypt under the Thutmosids. However, with the ascent of the Hittite empire, Mitanni and Egypt made an alliance to protect their mutual interests from the threat of Hittite domination. At the height of its power, during the 14th century BC, it had outposts centered on its capital, Washukanni, which archaeologists have located on the headwaters of the Khabur River. Eventually, Mitanni succumbed to Hittite, and later Assyrian attacks, and was reduced to a province of the Middle Assyrian Empire.
The Israelites are a Semitic people of the Ancient Near East who inhabited part of Canaan during the tribal and monarchic periods (15th to 6th centuries BC), and lived in the region in smaller numbers after the fall of the monarchy. The name Israel first appears c. 1209 BC, at the end of the Late Bronze Age and the very beginning of the Iron Age, on the Merneptah Stele raised by the Egyptian Pharaoh Merneptah.
The Aramaeans are a Northwest Semitic semi-nomadic and pastoralist people who originated in what is now modern Syria (Biblical Aram) during the Late Bronze Age and the early Iron Age. Large groups migrated to Mesopotamia, where they intermingled with the native Akkadian (Assyrian and Babylonian) population. The Aramaeans never had a unified empire; they were divided into independent kingdoms all across the Near East. After the Bronze Age collapse, their political influence was confined to a number of Syro-Hittite states, which were entirely absorbed into the Neo-Assyrian Empire by the 8th century BC.
Ancient Egypt.
Early Bronze dynasties.
In Ancient Egypt the Bronze Age begins in the Protodynastic period, c. 3150 BC. The archaic "early Bronze Age of Egypt", known as the Early Dynastic Period of Egypt, immediately follows the unification of Lower and Upper Egypt, c. 3100 BC. It is generally taken to include the First and Second Dynasties, lasting from the Protodynastic Period of Egypt until about 2686 BC, or the beginning of the Old Kingdom. With the First Dynasty, the capital moved from Abydos to Memphis with a unified Egypt ruled by an Egyptian god-king. Abydos remained the major holy land in the south. The hallmarks of ancient Egyptian civilization, such as art, architecture and many aspects of religion, took shape during the Early Dynastic period. Memphis in the Early Bronze Age was the largest city of the time.
The Old Kingdom of the regional Bronze Age is the name given to the period in the 3rd millennium BC when Egypt attained its first continuous peak of civilization in complexity and achievement – the first of three "Kingdom" periods, which mark the high points of civilization in the lower Nile Valley (the others being Middle Kingdom and the New Kingdom).
The First Intermediate Period of Egypt, often described as a "dark period" in ancient Egyptian history, spanned about 100 years after the end of the Old Kingdom from about 2181 to 2055 BC. Very little monumental evidence survives from this period, especially from the early part of it. The First Intermediate Period was a dynamic time when rule of Egypt was roughly divided between two competing power bases: Heracleopolis in Lower Egypt and Thebes in Upper Egypt. These two kingdoms would eventually come into conflict, with the Theban kings conquering the north, resulting in reunification of Egypt under a single ruler during the second part of the 11th Dynasty.
Middle Bronze dynasties.
The Middle Kingdom of Egypt lasted from 2055 to 1650 BC. During this period, the Osiris funerary cult rose to dominate Egyptian popular religion. The period comprises two phases: the 11th Dynasty, which ruled from Thebes and the 12th and 13th Dynasties centered on el-Lisht. The unified kingdom was previously considered to comprise the 11th and 12th Dynasties, but historians now at least partially consider the 13th Dynasty to belong to the Middle Kingdom.
During the Second Intermediate Period, Ancient Egypt fell into disarray for a second time, between the end of the Middle Kingdom and the start of the New Kingdom. It is best known for the Hyksos, whose reign comprised the 15th and 16th dynasties. The Hyksos first appeared in Egypt during the 11th Dynasty, began their climb to power in the 13th Dynasty, and emerged from the Second Intermediate Period in control of Avaris and the Delta. By the 15th Dynasty, they ruled lower Egypt, and they were expelled at the end of the 17th Dynasty.
Late Bronze dynasties.
The New Kingdom of Egypt, also referred to as the Egyptian Empire, lasted from the 16th to the 11th century BC. The New Kingdom followed the Second Intermediate Period and was succeeded by the Third Intermediate Period. It was Egypt's most prosperous time and marked the peak of Egypt's power. The later New Kingdom, i.e. the 19th and 20th Dynasties (1292–1069 BC), is also known as the Ramesside period, after the eleven pharaohs that took the name of Ramesses.
Central Asia.
Seima-Turbino Phenomenon.
The Altai Mountains in what is now southern Russia and central Mongolia have been identified as the point of origin of a cultural enigma termed the Seima-Turbino Phenomenon. It is conjectured that changes in climate in this region around 2000 BC and the ensuing ecological, economic and political changes triggered a rapid and massive migration westward into northeast Europe, eastward into China and southward into Vietnam and Thailand
However, recent genetic testings of sites in south Siberia and Kazakhstan (Andronovo horizon) would rather support a spreading of the bronze technology via Indo-European migrations eastwards, as this technology was well known for quite a while in western regions.
East Asia.
China.
Historians disagree about the dates of a "Bronze Age" in China. The difficulty lies in the term "Bronze Age", as it has been applied to signify a period in history when bronze tools replaced stone tools, and, later, were themselves replaced by iron ones. The medium of the new "Age" made that of the old obsolete. In China, however, any attempt to establish a definite set of dates for a Bronze Age is complicated by two factors:
The earliest bronze artifacts have been found in the Majiayao culture site (between 3100 and 2700 BC), and from then on, the society gradually grew into the Bronze Age.
Bronze metallurgy in China originated in what is referred to as the Erlitou () period, which some historians argue places it within the range of dates controlled by the Shang dynasty. Others believe the Erlitou sites belong to the preceding Xia () dynasty. The U.S. National Gallery of Art defines the Chinese Bronze Age as the "period between about 2000 BC and 771 BC," a period that begins with the Erlitou culture and ends abruptly with the disintegration of Western Zhou rule. Though this provides a concise frame of reference, it overlooks the continued importance of bronze in Chinese metallurgy and culture. Since this is significantly later than the discovery of bronze in Mesopotamia, bronze technology could have been imported rather than discovered independently in China. While there may be reason to believe that bronzework developed inside China separately from outside influence, the discovery of European mummies in Xinjiang suggests a possible route of transmission from the West.
The Shang Dynasty of the Yellow River Valley rose to power after the Xia Dynasty. While some direct information about the Shang Dynasty comes from Shang-era inscriptions on bronze artifacts, most comes from oracle bones – turtle shells, cattle scapulae, or other bones – which bear glyphs that form the first significant corpus of recorded Chinese characters.
Iron is found from the Zhou Dynasty, but its use is minimal. Chinese literature dating to the 6th century BC attests knowledge of iron smelting, yet bronze continues to occupy the seat of significance in the archaeological and historical record for some time after this. Historian W. C. White argues that iron did not supplant bronze "at any period before the end of the Zhou dynasty (256 BC)" and that bronze vessels make up the majority of metal vessels all the way through the Later Han period, or to 221 BC .
The Chinese bronze artifacts generally are either utilitarian, like spear points or adze heads, or "ritual bronzes", which are more elaborate versions in precious materials of everyday vessels, as well as tools and weapons. Examples are the numerous large sacrificial tripods known as dings in Chinese; there are many other distinct shapes. Surviving identified Chinese ritual bronzes tend to be highly decorated, often with the "taotie" motif, which involves highly stylized animal faces. These appear in three main motif types: those of demons, of symbolic animals, and of abstract symbols. Many large bronzes also bear cast inscriptions that are the great bulk of the surviving body of early Chinese writing and have helped historians and archaeologists piece together the history of China, especially during the Zhou Dynasty (1046–256 BC).
The bronzes of the Western Zhou Dynasty document large portions of history not found in the extant texts that were often composed by persons of varying rank and possibly even social class. Further, the medium of cast bronze lends the record they preserve a permanence not enjoyed by manuscripts. These inscriptions can commonly be subdivided into four parts: a reference to the date and place, the naming of the event commemorated, the list of gifts given to the artisan in exchange for the bronze, and a dedication. The relative points of reference these vessels provide have enabled historians to place most of the vessels within a certain time frame of the Western Zhou period, allowing them to trace the evolution of the vessels and the events they record.
Korea.
The beginning of the Bronze Age on the peninsula is around 900 BC – 800 BC. Although the Korean Bronze Age culture derives from the Liaoning and Manchuria, it exhibits unique typology and styles, especially in ritual objects.
The Mumun pottery period is named after the Korean name for undecorated or plain cooking and storage vessels that form a large part of the pottery assemblage over the entire length of the period, but especially 850-550 BC. The Mumun period is known for the origins of intensive agriculture and complex societies in both the Korean Peninsula and the Japanese Archipelago.
The Middle Mumun pottery period culture of the southern Korean Peninsula gradually adopted bronze production (c. 700–600? BC) after a period when Liaoning-style bronze daggers and other bronze artifacts were exchanged as far as the interior part of the Southern Peninsula (c. 900–700 BC). The bronze daggers lent prestige and authority to the personages who wielded and were buried with them in high-status megalithic burials at south-coastal centres such as the Igeum-dong site. Bronze was an important element in ceremonies and as for mortuary offerings until 100.
South Asia.
Indus Valley.
The Bronze Age on the Indian subcontinent began around 3300 BC with the beginning of the Indus Valley civilization. Inhabitants of the Indus Valley, the Harappans, developed new techniques in metallurgy and produced copper, bronze, lead and tin. The Indian Bronze Age was followed by the Iron Age Vedic Period. The Late Harappan culture, which dates from 1900 BC to 1400 BC, overlapped the transition from the Bronze Age to the Iron Age; thus it is difficult to date this transition accurately.
Southeast Asia.
Dating back to the Neolithic Age, the first bronze drums, called the Dong Son drums, have been uncovered in and around the Red River Delta regions of Vietnam and Southern China. These relate to the prehistoric Dong Son Culture of Vietnam. In Ban Chiang, Thailand, (Southeast Asia) bronze artifacts have been discovered dating to 2100 BC. However, according to the radiocarbon dating on the human and pig bones in Ban Chiang, some scholars propose that the initial Bronze Age in Ban Chiang was in late 2nd millennium. In Nyaunggan, Burma, bronze tools have been excavated along with ceramics and stone artifacts. Dating is still currently broad (3500–500 BC). Ban Non Wat, excavated by Charles Higham, was a rich site with over 640 graves excavated that gleaned many complex bronze items that may have had social value connected to them.
Ban Chiang, however, is the most thoroughly documented site while having the clearest evidence of metallurgy when it comes to Southeast Asia. With a rough date range of late third millennium BC to the first millennium AD, this site alone has various artifacts such as burial pottery (dating from 2100 BC – 1700 BC), fragments of Bronze, copper-base bangles, and much more. What's interesting about this site, however, isn't just the old age of the artifacts but the fact that this technology suggested on-site casting from the very beginning. The on-site casting supports the theory that Bronze was first introduced in Southeast Asia as fully developed which therefore shows that Bronze was actually innovated from a different country. Some scholars believe that the copper-based metallurgy was disseminated from northwest and central China via south and southwest areas such as Guangdong province and Yunnan province and finally into southeast Asia around 1,000 BC.
Archaeological research in Northern Vietnam indicates an increase in rates of infectious disease following the advent of metallurgy; skeletal fragments in sites dating to the early and mid-Bronze Age evidence a greater proportion of lesions than in sites of earlier periods. There are a few possible implications of this. One is the increase contact with bacterial and/or fungal pathogens due to increase population density and land clearing/ cultivation. The other one is decreased levels of immunocompetence in the Metal age due to changes in diet caused by agriculture. The last is that there may have been an emergence of infectious disease in the Da But period that evolved into a more virulent form in the metal period. Archaeology also suggests that Bronze Age metallurgy may not have been as significant a catalyst in social stratification and warfare in Southeast Asia as in other regions, social distribution shifting away from chiefdom-states to a heterarchical network. Data analyses of sites such as Ban Lum Khao, Ban Na Di, Non Nok Tha, Khok Phanom Di, and Nong Nor have consistently led researchers to conclude that there was no forentrenched hierarchy.
Europe.
European timeline.
A few examples of named Bronze Age cultures in Europe in roughly relative order.
The Bronze Age In Britain has been clearly identified through many sites, such as Stonehenge or the more recent Must Farm near Whittlesey.
Aegean.
The Aegean Bronze Age began around 3200 BC, when civilizations first established a far-ranging trade network. This network imported tin and charcoal to Cyprus, where copper was mined and alloyed with the tin to produce bronze. Bronze objects were then exported far and wide, and supported the trade. Isotopic analysis of tin in some Mediterranean bronze artifacts points to the fact that they may have originated from Great Britain.
Knowledge of navigation was well developed at this time, and reached a peak of skill not exceeded (except perhaps by Polynesian sailors) until 1730 when the invention of the chronometer enabled the precise determination of longitude.
The Minoan civilization based in Knossos on the island of Crete appears to have coordinated and defended its Bronze Age trade. Illyrians are also believed to have roots in the early Bronze Age. Ancient empires valued luxury goods in contrast to staple foods, leading to famine.
Aegean Collapse.
Bronze Age collapse theories have described aspects of the end of the Age in this region. At the end of the Bronze Age in the Aegean region, the Mycenaean administration of the regional trade empire followed the decline of Minoan primacy. Several Minoan client states lost much of their population to famine and/or pestilence. This would indicate that the trade network may have failed, preventing the trade that would previously have relieved such famines and prevented illness caused by malnutrition. It is also known that in this era the breadbasket of the Minoan empire, the area north of the Black Sea, also suddenly lost much of its population, and thus probably some cultivation.
The Aegean Collapse has been attributed to the exhaustion of the Cyprus forests causing the end of the bronze trade. These forests are known to have existed into later times, and experiments have shown that charcoal production on the scale necessary for the bronze production of the late Bronze Age would have exhausted them in less than fifty years.
The Aegean Collapse has also been attributed to the fact that as iron tools became more common, the main justification for the tin trade ended, and that trade network ceased to function as it did formerly. The colonies of the Minoan empire then suffered drought, famine, war, or some combination of those three, and had no access to the distant resources of an empire by which they could easily recover.
The Thera eruption occurred around the Aegean Collapse, north of Crete. Speculation include a tsunami from Thera (more commonly known today as Santorini) destroyed Cretan cities. A tsunami may have destroyed the Cretan navy in its home harbour, which then lost crucial naval battles; so that in the LMIB/LMII event (c. 1450 BC) the cities of Crete burned and the Mycenaean civilization took over Knossos. If the eruption occurred in the late 17th century BC (as most chronologists now think) then its immediate effects belong to the Middle to Late Bronze Age transition, and not to the end of the Late Bronze Age; but it could have triggered the instability that led to the collapse first of Knossos and then of Bronze Age society overall. One such theory looks to the role of Cretan expertise in administering the empire, post-Thera. If this expertise was concentrated in Crete, then the Mycenaeans may have made political and commercial mistakes in administering the Cretan empire.
Archaeological findings, including some on the island of Thera, suggest that the centre of Minoan Civilization at the time of the eruption was actually on Thera rather than on Crete . According to this theory, the catastrophic loss of the political, administrative and economic centre by the eruption as well as the damage wrought by the tsunami to the coastal towns and villages of Crete precipitated the decline of the Minoans. A weakened political entity with a reduced economic and military capability and fabled riches would have then been more vulnerable to human predators. Indeed, the Santorini Eruption is usually dated to c. 1630 BC, while the Mycenaean Greeks first enter the historical record a few decades later, c. 1600 BC. Thus, the later Mycenaean assaults on Crete (c.1450 BC) and Troy (c.1250 BC) are revealed as mere continuations of the steady encroachments of the Greeks upon the weakened Minoan world.
Central Europe.
In Central Europe, the early Bronze Age Unetice culture (1800–1600 BC) includes numerous smaller groups like the Straubing, Adlerberg and Hatvan cultures. Some very rich burials, such as the one located at Leubingen with grave gifts crafted from gold, point to an increase of social stratification already present in the Unetice culture. All in all, cemeteries of this period are rare and of small size. The Unetice culture is followed by the middle Bronze Age (1600–1200 BC) Tumulus culture, which is characterised by inhumation burials in tumuli (barrows). In the eastern Hungarian Körös tributaries, the early Bronze Age first saw the introduction of the Mako culture, followed by the Otomani and Gyulavarsand cultures.
The late Bronze Age Urnfield culture, (1300–700 BC) is characterized by cremation burials. It includes the Lusatian culture in eastern Germany and Poland (1300–500 BC) that continues into the Iron Age. The Central European Bronze Age is followed by the Iron Age Hallstatt culture (700–450 BC).
Important sites include:
The Bronze Age in Central Europe has been described in the chronological schema of German prehistorian Paul Reinecke. He described Bronze A1 (Bz A1) period (2300–2000 BC : triangular daggers, flat axes, stone wrist-guards, flint arrowheads) and Bronze A2 (Bz A2) period (1950–1700 BC : daggers with metal hilt, flanged axes, halberds, pins with perforated spherical heads, solid bracelets) and phases Hallstatt A and B (Ha A and B).
South Europe.
The Apennine culture (also called Italian Bronze Age) is a technology complex of central and southern Italy spanning the Chalcolithic and Bronze Age proper. The Camuni were an ancient people of uncertain origin (according to Pliny the Elder, they were Euganei; according to Strabo, they were Rhaetians) who lived in Val Camonica – in what is now northern Lombardy – during the Iron Age, although human groups of hunters, shepherds and farmers are known to have lived in the area since the Neolithic.
Located in Sardinia and Corsica, the Nuragic civilization lasted from the early Bronze Age (18th century BC) to the 2nd century AD, when the islands were already Romanized. They take their name from the characteristic nuragic towers, which evolved from the pre-existing megalithic culture, which built dolmens and menhirs. The nuraghe towers are unanimously considered the best preserved and largest megalithic remains in Europe. Their effective use is still debated: some scholars considered them as monumental tombs, others as Houses of the Giants, other as fortresses, ovens for metal fusion, prisons or, finally, temples for a solar cult. Around the end of the third millennium BC, Sardinia exported towards Sicily a "Culture" that built small dolmens, trilithic or polygonal shaped, that served as tombs as it has been ascertained in the Sicilian dolmen of “Cava dei Servi”. From this region they reached Malta island and other countries of Mediterranean basin.
The Terramare was an early Indo-European civilization in the area of what is now Pianura Padana (northern Italy) before the arrival of the Celts, and in other parts of Europe. They lived in square villages of wooden stilt houses. These villages were built on land, but generally near a stream, with roads that crossed each other at right angles. The whole complex denoted the nature of a fortified settlement. Terramare were widespread in the Pianura Padana (specially along the Panaro river, between Modena and Bologna) and in the rest of Europe. The civilization developed in the Middle and Late Bronze Age, between the 17th and the 13th centuries BC.
The Castellieri culture developed in Istria during the Middle Bronze Age. It lasted for more than a millennium, from the 15th century BC until the Roman conquest in the 3rd century BC. It takes its name from the fortified boroughs ("Castellieri", Friulian "cjastelir") that characterized the culture.
The Canegrate culture developed from the mid-Bronze Age (13th century BC) till the Iron Age in the Pianura Padana, in what are now western Lombardy, eastern Piedmont and Ticino. It takes its name from the township of Canegrate where, in the 20th century, some fifty tombs with ceramics and metal objects were found. The Canegrate culture migrated from the northwest part of the Alps and descended to Pianura Padana from the Swiss Alps passes and the Ticino.
The Golasecca culture developed starting from the late Bronze Age in the Po plain. It takes its name from Golasecca, a locality next to the Ticino where, in the early 19th century, abbot Giovanni Battista Giani excavated its first findings (some fifty tombs with ceramics and metal objects). Remains of the Golasecca culture span an area of c. 20,000 square kilometers south to the Alps, between the Po, Sesia and Serio rivers, dating from the 9th to the 4th century BC.
West Europe.
Atlantic Bronze Age.
The Atlantic Bronze Age is a cultural complex of the period of approximately 1300–700 BC that includes different cultures in Portugal, Andalusia, Galicia and the British Isles. It is marked by economic and cultural exchange. Commercial contacts extend to Denmark and the Mediterranean. The Atlantic Bronze Age was defined by a number of distinct regional centres of metal production, unified by a regular maritime exchange of some of their products.
Great Britain.
In Great Britain, the Bronze Age is considered to have been the period from around 2100 to 750 BC. Migration brought new people to the islands from the continent. Recent tooth enamel isotope research on bodies found in early Bronze Age graves around Stonehenge indicate that at least some of the migrants came from the area of modern Switzerland. Another example site is Must Farm, near Whittlesey, which has recently been host to the most complete Bronze Age wheel ever to be found. The Beaker culture displayed different behaviours from the earlier Neolithic people, and cultural change was significant. Integration is thought to have been peaceful, as many of the early henge sites were seemingly adopted by the newcomers. The rich Wessex culture developed in southern Britain at this time. Additionally, the climate was deteriorating; where once the weather was warm and dry it became much wetter as the Bronze Age continued, forcing the population away from easily defended sites in the hills and into the fertile valleys. Large livestock farms developed in the lowlands and appear to have contributed to economic growth and inspired increasing forest clearances. The Deverel-Rimbury culture began to emerge in the second half of the Middle Bronze Age (c. 1400–1100 BC) to exploit these conditions. Devon and Cornwall were major sources of tin for much of western Europe and copper was extracted from sites such as the Great Orme mine in northern Wales. Social groups appear to have been tribal but with growing complexity and hierarchies becoming apparent.
Burial of dead (which, until this period, had usually been communal) became more individual. For example, whereas in the Neolithic a large chambered cairn or long barrow housed the dead, Early Bronze Age people buried their dead in individual barrows (also commonly known and marked on modern British Ordnance Survey maps as tumuli), or sometimes in cists covered with cairns.
The greatest quantities of bronze objects in England were discovered in East Cambridgeshire, where the most important finds were recovered in Isleham (more than 6500 pieces).
Alloying of copper with zinc or tin to make brass or bronze was practised soon after the discovery of copper itself. One copper mine at Great Orme in North Wales, extended to a depth of 70 meters. At Alderley Edge in Cheshire, carbon dates have established mining at around 2280 to 1890 BC (at 95% probability). The earliest identified metalworking site (Sigwells, Somerset) is much later, dated by Globular Urn style pottery to approximately the 12th century BC. The identifiable sherds from over 500 mould fragments included a perfect fit of the hilt of a sword in the Wilburton style held in Somerset County Museum.
Ireland.
The Bronze Age in Ireland commenced around 2000 BC, when copper was alloyed with tin and used to manufacture Ballybeg type flat axes and associated metalwork. The preceding period is known as the Copper Age and is characterised by the production of flat axes, daggers, halberds and awls in copper. The period is divided into three phases: Early Bronze Age (2000–1500 BC), Middle Bronze Age (1500–1200 BC), and Late Bronze Age (1200 – c. 500 BC). Ireland is also known for a relatively large number of Early Bronze Age burials.
One of the characteristic types of artifact of the Early Bronze Age in Ireland is the flat axe. There are five main types of flat axes: Lough Ravel (c. 2200 BC), Ballybeg (c. 2000 BC), Killaha (c. 2000 BC), Ballyvalley (c. 2000–1600 BC), Derryniggin (c. 1600 BC), and a number of metal ingots in the shape of axes.
North Europe.
The Bronze Age in Northern Europe spans the entire 2nd millennium BC (Unetice culture, Urnfield culture, Tumulus culture, Terramare culture, Lusatian culture) lasting until c. 600 BC. The Northern Bronze Age was both a period and a Bronze Age culture in Scandinavian pre-history, c. 1700–500 BC, with sites that reached as far east as Estonia. Succeeding the Late Neolithic culture, its ethnic and linguistic affinities are unknown in the absence of written sources. It is followed by the Pre-Roman Iron Age.
Even though Northern European Bronze Age cultures were relatively late, and came in existence via trade, sites present rich and well-preserved objects made of wool, wood and imported Central European bronze and gold. Many rock carvings depict ships, and the large stone burial monuments known as stone ships suggest that shipping played an important role. Thousands of rock carvings depict ships, most probably representing sewn plank built canoes for warfare, fishing and trade. These may have a history as far back as the neolithic period and continue into the Pre-Roman Iron Age, as shown by the Hjortspring boat. There are many mounds and rock carving sites from the period. Numerous artifacts of bronze and gold are found. No written language existed in the Nordic countries during the Bronze Age. The rock carvings have been dated through comparison with depicted artifacts.
Caucasus.
Arsenical bronze artifacts of the Maykop culture in the North Caucasus have been dated around the 4th millennium BC. This innovation resulted in the circulation of arsenical bronze technology over southern and eastern Europe.
Pontic–Caspian steppe.
The Yamna culture is a Late Copper Age/Early Bronze Age culture of the Southern Bug/Dniester/Ural region (the Pontic steppe), dating to the 36th–23rd centuries BC. The name also appears in English as Pit-Grave Culture or Ochre-Grave Culture. The Catacomb culture, c. 2800–2200 BC, comprises several related Early Bronze Age cultures occupying what is presently Ukraine. The Srubna culture was a Late Bronze Age (18th–12th centuries BC) culture. It is a successor to the Yamna and the Poltavka culture.
Americas.
The Moche civilization of South America independently discovered and developed bronze smelting. Bronze technology was developed further by the Incas and used widely both for utilitarian objects and sculpture. A later appearance of limited bronze smelting in West Mexico (see Metallurgy in pre-Columbian Mesoamerica) suggests either contact of that region with Andean cultures or separate discovery of the technology. The Calchaquí people of Northwest Argentina had Bronze technology.
Outside the Bronze Age.
Japan.
The Jōmon period lasted until 300 BC and, towards the end of the period, the Japanese archipelago experienced the introduction of bronze and iron simultaneously. Bronze and iron smelting techniques spread to the Japanese archipelago through immigration and trade from the Korean peninsula and the Chinese mainland. Iron was mainly used for agricultural and other tools, whereas ritual and ceremonial artifacts were mainly made of bronze. Formerly, scholarly theories suggested that a bronze and iron using Yamato people gradually spread across the Japanese archipelago, conquering and assimilating the Jōmon people and their descendants, as well as pushing them east and north. Current archaeology suggests a more complex picture of the "Jōmon-Yayoi transition," including as regards ethnic categories; see the article on Yayoi people.
Africa.
Although North Africa was influenced to a certain extent by European Bronze Age cultures (for example, traces of the Bell beaker tradition are found in Morocco), it has long been believed that Africa did not have its own metallurgy traditions until the Phoenician colonization (ca. 1100 BC) of North Africa, and that it remained attached to the Neolithic way of life. The civilization of Ancient Egypt, whose influence did not extensively cover Africa outside of the Nile's reach, was believed to be the sole exception to this rule as regards the whole range of ancient cultures of Africa. Recently, however, some discoveries have been made that appear to contradict these views.
In the Termit region of eastern Niger, its ancient inhabitants are now thought to have become the first iron smelting people in West Africa and among the first in the world at around 1500 BC. Iron and copper working then continued to spread southward to Nigeria, and then moved elsewhere in the continent, reaching South Africa around AD 200. The widespread use of iron revolutionized the Bantu-speaking farming communities who adopted it, driving out and absorbing the rock tool using hunter-gatherer societies they encountered as they expanded to farm wider areas of savannah. The technologically superior Bantu-speakers spread across southern Africa and became wealthy and powerful, producing iron for tools and weapons in large, industrial quantities.
Trade in the Bronze Age.
Trade played a major role in the development of the ancient Bronze Age civilizations. With artefacts of the Indus Valley Civilization being found in ancient Mesopotamia and Egypt, it is clear that these civilizations were not only in touch with each other but also trading with each other. Early long distance trade was limited almost exclusively to luxury goods like spices, textiles and precious metals. Not only did this make cities with ample amounts of these products extremely rich but also led to an inter-mingling of cultures for the first time in history.
Trade routes were not only over land but also over water. The first and most extensive trade routes were over rivers such as the Nile, the Tigris and the Euphrates which led to growth of cities on the banks of these rivers. The domestication of camels at a later time also helped encourage trade routes over land, which were called caravans, and linked Indus Valley with the Mediterranean. This further led to towns sprouting up in numbers any and everywhere there was a pit-stop or caravan-to-ship port.

</doc>
<doc id="4621" url="https://en.wikipedia.org/wiki?curid=4621" title="BBC News (TV channel)">
BBC News (TV channel)

BBC News (also referred to as the BBC News Channel) is the BBC's 24-hour rolling news television network in the United Kingdom. The channel launched as BBC News 24 on 9 November 1997 at 17:30 as part of the BBC's foray into digital domestic television channels, becoming the first competitor to Sky News, which had been running since 1989. For a time, looped news, sport and weather bulletins were available to view via BBC Red Button.
On 22 February 2006, the channel was named "News Channel of the Year" at the Royal Television Society Television Journalism Awards for the first time in its history. The judges remarked that this was the year that the channel had "really come into its own."
From May 2007, UK viewers could watch the channel via the BBC News website. In April 2008, the channel was renamed "BBC News" as part of a £550,000 rebranding of the BBC's news output, complete with a new studio and presentation. Its sister service, BBC World was also renamed "BBC World News" while the national news bulletins became "BBC News at One", "BBC News at Six" and "BBC News at Ten". Across the day the channel averages about twice the audience of Sky News.
As a major part of the BBC News department, the channel is based at and broadcast from Broadcasting House in Central London.
It was named RTS "News Channel of the Year" again in 2009.
History.
BBC News 24 was originally available only to analogue cable television subscribers. To this day, it and BBC Parliament remain the only BBC "digital" channels which are made available to analogue cable subscribers. This coverage was improved in 1998 with the advent of digital television in the United Kingdom allowing satellite and digital terrestrial television viewers to also view the service. Initially it was difficult to obtain a digital satellite or terrestrial receiver without a subscription to Sky or ONdigital respectively, but now the channel forms an important part of the Freeview and Freesat channel packages.
The BBC had run the international news channel BBC World for two and a half years prior to the launch of BBC News 24 on 9 November 1997. Sky News had had a free hand with domestic news for over eight years (since 5 February 1989) and being owned by News Corporation their papers were used to criticise the BBC for extending its news output.
Sky News objected to the breaking of its monopoly, complaining about the costs associated with running a channel that only a minority could view from the licence fee. Sky News claimed that a number of British cable operators had been incentivised to carry News 24 (which, as a licence-fee funded channel was made available to such operators for free) in preference to the commercial Sky News. However, in September 1999 the European Commission ruled against a complaint made by Sky News that the publicly funded channel was unfair and illegal under EU law. The Commission ruled that the licence fee should be considered state aid but that such aid was justified due to the public service remit of the BBC and that it did not exceed actual costs.
The channel's journalistic output has been overseen by Controller of the channel, Kevin Bakhurst, since 16 December 2005. This was a return to having a dedicated Controller for the channel in the same way as the rest of the BBC's domestic television channels. At launch, Tim Orchard was Controller of News 24 from 1997 until 2000. Editorial decisions were then overseen by Rachel Atwell in her capacity as Deputy Head of television news. Her deputy Mark Popescu became responsible for editorial content in 2004, a role he continued in until the appointment of Bakhurst as Controller in 2005.
A further announcement by Head of television news Peter Horrocks came at the same time as Bakhurst's appointment in which he outlined his plan to provide more funding and resources for the channel and shift the corporation's emphasis regarding news away from the traditional BBC One bulletins and across to the rolling news channel. The introduction of simulcasts of the main bulletins on the channel was to allow the news bulletins to pool resources rather than work against each other at key times in the face of competition particularly from Sky News.
The BBC Governors' annual report for 2005/2006 reported that average audience figures for fifteen-minute periods had reached 8.6% in multichannel homes, up from 7.8% in 2004/2005. The 2004 report claimed that the channel outperformed Sky News in both weekly and monthly reach in multichannel homes for the January 2004 period, and for the first time in two years moved ahead of Sky News in being perceived as the channel best for news.
2008 rebranding.
On 21 April 2008, BBC News 24 was renamed "BBC News" on the channel itself – but is referred to as the "BBC News Channel" on other BBC services. This is part of the creative futures plan, launched in 2006, to bring all BBC News output under the single brand name.
The BBC News Channel moved from the Studio N8 set, which became home to BBC World News, to what was the home of the national news in Studio N6, allowing the Channel to share its set with the "BBC News at One" and the "BBC News at Ten" – with other bulletins moving to Studio TC7.
Move to Broadcasting House.
The channel relocated, along with the remaining BBC News services at Television Centre, to the newly refurbished Broadcasting House on 18 March 2013 at 13:00 GMT. Presentation and on-screen graphics were refreshed, with new full HD studios and a live newsroom backdrop. Moving cameras in the newsroom form part of the top of the hour title sequence and are used at the start of weather bulletins.
BBC News HD.
<br>
On 16 July 2013, the BBC announced that a high-definition (HD) simulcast of BBC News would be launched by early 2014. The channel broadcasts on the BBC's new HD multiplex on Freeview. HD output from BBC News has been simulcast on BBC One HD and BBC Two HD since the move to Broadcasting House in March 2013. The channel launched on 10 December 2013 (at an early date), though will roll-out nationwide up to June 2014 (as will BBC Four HD and CBeebies HD).
Programming.
News.
Each hour consists of headlines on each quarter-hour, extended at the top of the hour to form the main part of the daily schedule though these are interspaced with other programmes, generally at weekends. This will be often be displaced by rolling news coverage including reports and live interviews. Weather summaries are provided every half-hour by forecasters from the BBC Weather centre and the sports news from the BBC Sport centre at MediaCityUK. At 21:25 a global weather forecast is broadcast and 21:55 "Weather For The Week Ahead" is broadcast.
Breaking news.
The BBC maintains guidelines for procedures to be taken for breaking news. With domestic news, the correspondent first recorded a "generic minute" summary (for use by all stations and channels) and then priority was to report on BBC Radio 5 Live, then on the BBC News channel and any other programmes that are on air. Since 5 Live's move to Manchester, this has been reversed. For foreign news, first a "generic minute" is recorded, then reports are to World Service radio, then the reporter talks to any other programmes that are on air.
A key claim made by Lord Lambert in his report had been that the channel was slower to react to breaking news compared with its main rival Sky News. To counteract this, a new feature introduced with the 2003 relaunch was a 'breaking news sting': a globe shown briefly onscreen to direct a viewer's attention to the breaking news.
The graphics relaunch in January 2007 has since seen the globe sting replaced by a red strapline to highlight the breaking story immediately.
To complement this, a permanent live news ticker had earlier been introduced in 2006: this had previously been in use only sporadically. News statements are shown as continuously scrolling upper-case text located at the bottom of the screen; some past ambiguities noted have included spelling the plural of MPs as "MPS", together with other occasional spelling and grammatical errors. The design of this ticker was slightly altered with the 2007 graphics redesign and from June turned red to indicate breaking news, as "Newswatch" reported viewers' confusion. The ticker is removed during trails and weather forecasts.
Overnight and BBC World News simulcasts.
The BBC began simulcasting the channel overnight on terrestrial channel BBC One with the launch of the channel, ending the tradition of a closedown but at the same time effectively making the service available to many more viewers. In the early 2000s, BBC Two also started simulcasting the channel, although the weekend morning show "Weekend 24" had been simulcast on the channel in the early days. During major breaking news events, the BBC News Channel has been broadcast on BBC One; examples of special broadcasts include the 11 September 2001 attacks, 7 July 2005 London bombings, the capture of Saddam Hussein, and the death of Osama bin Laden. Coverage of major events has also been simulcast on BBC World News. Currently, overnight viewers receive 25-minute editions of BBC News every hour, and on weekdays 00:00–01:30 receive "Newsday", live from Singapore and from London 01:30–02:00 "ABC World News Tonight" 02:00–05:30 (00:00–06:00 on weekends) receive "BBC World News".
BBC One and BBC Two daytime simulcasts.
"BBC Breakfast" has been simulcast since launch (in 2000) on BBC One and BBC News, replacing the individual breakfast shows that had run on both channels. Since May 2006, the simulcast runs from 06:00 until 08:30. "Breakfast" on BBC One then continues from MediaCityUK until 09:15 with entertainment and features, whilst BBC News reverts to its traditional format.
The "BBC News at Ten" began simulcasting on the channel on 30 January 2006 as part of the "Ten O'Clock Newshour", followed by extended sport and business news updates. The bulletin was joined in being simulcast on 10 April 2006 when the "BBC News at One" (with British Sign Language in-vision signing) and "BBC News at Six" bulletins were added to the schedule following a similar format to the "News at Ten" in terms of content on the channel once each simulcast ends.
During the summer, the hour-long programme "News 24 Sunday" was broadcast both on BBC One and the BBC News Channel at 09:00, to replace "The Andrew Marr Show", which is off air. It was presented by a news presenter, and came from the main News channel studio. The programme was made up mostly of interviews focusing on current affairs, and included a full paper review, a weather summary, and a news update at 09:00, 09:30 and 10:00. "Sunday Morning Live" and alternate programming now fill this slot.
From 2013, a new programme was created for BBC Two for 11am-12pm weekdays, consisting of 30 minutes domestic and 30 minutes of BBC World News. On Wednesdays, when parliament is sitting the latter is replaced by the Daily Politics for coverage of Prime Ministers Questions. In March 2016 the channel started shown Newsnight at 23:15.
Exclusive programmes.
Pre-recorded programmes include:
Previous BBC News programming includes "Head 2 Head", "Your News", "E24", "The Record Europe", "STORYFix" and "News 24 Tonight", a weekday evening programme which ran from 2005 to 2008, providing a round up of the day's news.
2015 schedule changes.
As part of budget cuts, major changes to the channel were announced in late 2014/early 2015. This included axing some bulletins and replacing them with "Victoria Derbyshire" and "BBC Business Live" with Sally Bundock and Ben Thompson in the morning. "Outside Source with Ros Atkins" – an "interactive" show already broadcast on BBC World News – aired Mondays-Thursday at (During major stories 18:00) and 21:00 and a new edition of "World News Today" Friday-Sunday at 21:00 (During major stories 19:00/20:00 Monday-Friday) adding to the 19:00 edition on BBC Four. "HARDtalk" was moved to 20:30 in May. 00:00 edition was replaced on Sundays-Thursday with "Newsday" and on Friday-Saturday a standard edition of "BBC World News".
BBC World News shared programming.
Between 00:00-06:00 & 08:30-09:15 (weekdays) UK time, the channel simulcasts with its sister channel, BBC World News, for the first 25 minutes of each hour with world news shown all through the simulcasts. Previously UK output continued until 01:00.
Simulcasts may also happen during major or set-piece events; the News Channel presenter will join the BBC World News presenter in Studio C as it used by both channels, or a journalist will present on location. Examples include Glasgow Helicopter Crash, the election of Pope Francis and the Boston Marathon bombing and certain European elections.
On 1 October 2007, BBC World News started broadcasting "BBC World News America" and "World News Today" at 00:00 and 03:00 GMT respectively. World News Today was simulcast on the BBC News channel at 03:00 GMT. "BBC World News America" used to be aired as a reduced length, tape-delayed version at 00:30 GMT. "ABC World News Tonight with David Muir" also shown at 01:30 every Tuesday-Friday.
From 13 June 2011, the weekday editions of BBC News at 01:00, 02:00, 03:00 and 04:00 were replaced with "Newsday". The programme acts as a morning news bulletin for the Asia-Pacific region and is broadcast as a double-headed news bulletin with Rico Hizon in Singapore and Babita Sharma in London. "Asia Business Report" and "Sport Today" are aired at the back of the first three hours of "Newsday". But Newsday changed to 23:00–02:00 On BBC News a year later meaning. Mike Embley presents Tuesday-Friday "BBC World News" 23:00–02:00 with Kasia Madera on Saturdays and Daniela Ritorto 00:00–06:00 Sunday, 02:00–05:00 Friday/Monday. the slot's focus is placed on audiences watching on PBS in the U.S. In June 2015 this was change to Newsday at 00:00-02:00. 
"BBC World News" and "World Business Report" air at 05:00 this was previously known as "The World Today" on both channels and in lieu of commercials seen on the international broadcasts, the presenters give a brief update on UK news for domestic audiences.
In June 2015, they will simulcast "Outside Source with Ros Atkins" on Mondays-Thursday at (During major stories 18:00) / at 21:00 and a new edition of "World News Today" Friday-Sunday at (during major stories Monday-Friday 19:00) 21:00.
Traditionally, during simulcasts in December, care has been taken to conceal the newsroom Christmas tree for international audiences.
Sport coverage.
Since 5 March 2012, sports bulletins come from the "BBC Sport Centre" in MediaCityUK in Salford Quays, where the sports network BBC Radio 5 Live is also based.
Headlines are usually provided at 15 minutes past the hour with a full bulletin after the bottom-of-the-hour headlines. There are also extended sports bulletins per day, entitled "Sportsday" or "Sport Today" (when simulcasting with BBC World News) broadcast at 00:45, 01:45, 02:45, 03:45, 13:30, 18:30, (19:30 Weekends only), 22:30 (weekdays only). Each bulletin is read by a single sports presenter, with the exception of Saturday "Sportsday", which is double headed.
The channel's sports bulletins (internally known as Sport 24) have always had a separate, dedicated production gallery, which is also responsible for the graphics.
Bulletins during BBC Breakfast are presented by Sally Nugent or Mike Bushell, with the latter also appearing on other sports bulletins on the channel. As of March 2012 the main sports presenters on the channel are Olly Foster, Katie Gornall, Katherine Downes, Damian Johnson, Andrew Lindsay and Jenny Culshaw.
Until March 2012 bulletins came from the News Channel studio at 45 minutes past the hour. Presenters for bulletins on the channel have included: Reshmin Choudhury, Amanda Davies, Sean Fletcher, Olly Foster, Matt Gooderick, Lizzie Greenwood-Hughes, Amelia Harris, Celina Hinchcliffe, Rachael Hodges, Damian Johnson, Adnan Nawaz, George Riley and Olympic gold medallist turned journalist Matthew Pinsent.
Business.
Before BBC News moved to Broadcasting House, an hourly business update was included during the weekday schedule from the BBC Business Unit. There were two shifts, from 08:30–14:00 and 14:00–23:00, presented by Penny Haslam, Maryam Moshiri, Ben Thompson, Adam Parsons, Susannah Streeter, Joe Lynam, Sara Coburn or Sally Eden. News Channel updates were usually broadcast at 40 minutes past the hour from 08:40 until 22:40. The 21:40 round-up was often earlier and the final bulletin is an extended round-up of the day's business news. Until May 2009, the business updates on the BBC News Channel were broadcast from one of the London Stock Exchange's studios in central London. From then until March 2013 the bulletins were provided from the channel's studio at BBC Television Centre. The business updates were axed in March 2013 as part of the BBC's Delivering Quality First plan. But after complaints returned in November 2013. Stock market updates now only appear during the headlines at 45 minutes past the hour. But it was changed after complaints in November with Ben Thompson and Victoria Friz are the main presenters sharing 08:00–14:00 or 14:00–18:00 between them. with Juliette Foster, Adam Parsons, Alice Baxter, Jamie Robertson, Aaron Heslehurst and Sally Bundock offer relief. Bundock and Thompson present "Business Live" on weekdays at 08:30. Declan Curry presented "Your Money", a weekly round-up on a Saturday morning.
Rico Hizon or Sharanjit Leyl regularly present the main business stories during the early hours of the morning from Singapore during the BBC's "Asia Business Report", which is simulcast from BBC World News. Alice Baxter and Sally Bundock present "World Business Report".
News presenters.
Relief.
Aaron Heslehurst, Sally Bundock, Alice Baxter and Ben Thompson present "Business Live" and "World Business Report". Ros Atkins presents 'Outside Source'. Thomas, Patel, Wilcox, Baxter, Giannone or Coxrall present "World News Today" on Weekdays on BBC Four and weekends on the channel. Rico Hizon and Sharanjit Leyl (Reporting from Singapore), Babita Sharma and Madera are the main overnight presenters on the channel, appearing on "Newsday" and generic BBC News bulletins. These programmes are simulcast with BBC World News and either BBC One or BBC Two. Madera, Ben Bland and Mike Embley regularly present 02:00–05:00 weekdays and weekends. Nawaz and Munchetty with Bundock or Baxter present the BBC World News 05:00–hour on weekdays, which is also broadcast on the News Channel and BBC One. Komla Dumor died suddenly on 18 January 2014 of a heart attack. Martine Dennis left the BBC to join Al Jazeera English in March 2014
The simulcasting of the main national news bulletins has led to the presenters of those bulletins appearing on the channel and offer relief on the news channel including Huw Edwards, Victoria Derbyshire, Fiona Bruce, George Alagiah, Sophie Raworth, Kate Silverton and Mishal Husain. The main "Breakfast" presenters have also appeared on the channel since it was first launched as a simulcast programme in 2000, with the current presenters being Dan Walker, Louise Minchin, Charlie Stayt and Naga Munchetty.
Kasia Madera presents the BBC World News programme Reporters on the channel, while Gavin Esler presents "Dateline London". Stephen Sackur appears on Hardtalk, which is aired weeknights and at weekends, while Zeinab Badawi, Carrie Gracie and Sarah Montague provide cover for him. Spencer Kelly presents the technology news programme "Click". "Newsnight" host Evan Davis presents "The Bottom Line". Lee McKenzie presents "Inside F1" on Grand Prix weekend's. Tanya Beckett presents "Talking Business" and "Witness". Ade Adepitan, Rajan Datar, Christa Larwood, Henry Golding and Carmen Robert present "The Travel Show"
During a major news event one or more of the main news presenters may be sent to present live for the channel from the scene of the story, where they will conduct interviews with the people involved, question correspondents, introduce related reports and also give general information on the story, much as a reporter sent to cover a story would. The presenters often have expertise in the story they are sent to cover, for example channel presenters and former reporters Ben Brown and Clive Myrie were dispatched to Cairo and Tripoli during the Middle East uprisings.
Presentation.
Graphics.
The channel was criticised at launch for its style of presentation, with accusations of it being less authoritative than the BBC One news bulletins, with presenters appearing on-screen without jackets. Jenny Abramsky had originally planned to have a television version of the informal news radio channel BBC Radio 5 Live, or a TV version of Radio 4 News FM both of which she had run. The bright design of the set was also blamed for this – one insider reportedly described it as a "car crash in a shower" – and was subject to the network relaunch on 25 October 1999. The channel swapped studios with sister channel BBC World, moving to studio N8 within the newsroom, where it remained until 2008. New music and title sequences accompanied this set change, following the look of newly relaunched BBC One bulletins.
Graphics and titles were developed by the Lambie-Nairn design agency and were gradually rolled out across the whole of BBC News, including a similar design for regional news starting with "Newsroom South East" and the three 'BBC Nations' – Scotland, Wales and Northern Ireland. The similarity of main BBC News output was intended to increase the credibility of the channel as well as aiding cross-channel promotion.
A graphics relaunch in January 2007 saw the channel updated, with redesigned headline straplines, a redesigned 'digital on-screen graphic' and repositioned clock. The clock was originally placed to the left hand side of the channel name though following complaints that this could only be viewed in widescreen, it was moved to the right in February 2007. Bulletins on BBC World News and BBC One also introduced similar graphics and title sequences on the same day.
In 2008, the graphics were again relaunched, using the style introduced in 2007 and a new colour scheme.
The Lambert Report.
The Lambert Report into the channel's performance in 2002 called upon News 24 to develop a better brand of its own, to allow viewers to differentiate between itself and similar channels such as Sky News. As a direct result of this, a brand new style across all presentation for the channel launched on 8 December 2003 at 09:00. Philip Hayton and Anna Jones were the first two presenters on the set, the relaunch of which had been put back a week due to previous power disruptions at Television Centre where the channel was based. The new designs also featured a dynamic set of titles for the channel; the globe would begin spinning from where the main story was taking place, while the headline scrolled around in a ribbon; this was occasionally replaced by the BBC News logo. The titles concluded with a red globe surrounded by a red stylised clamshell and BBC News ribbons forming above the BBC News logo.
Bulletins on BBC One moved into a new set in January 2003 although retained the previous ivory Lambie-Nairn titles until February 2004. News 24 updated the title colours slightly to match those of BBC One bulletins in time for the 50th anniversary of BBC television news on 5 July 2004.
Countdown sequence.
An important part of the channel's presentation since launch has been the top of the hour countdown sequence, since there is no presentation system with continuity announcers so the countdown provides a link to the beginning of the next hour. A similar musical device is used on BBC Radio 5 Live, and mirrors the pips on BBC Radio 4.
Previous styles have included a series of fictional flags set to music between 1997 and 1999 before the major relaunch, incorporating the new contemporary music composed by David Lowe, and graphics developed by Lambie-Nairn. Various images, originally ivory numbers fully animated against a deep red background, were designed to fit the pace of the channel, and the music soon gained notoriety, and was often satirised and parodied in popular culture, perhaps most famously by comic Bill Bailey who likened the theme music to an "apocalyptic rave". Images of life around the UK were added in replacement later with the same music, together with footage of the newsroom and exterior of Television Centre. The 2003 relaunch saw a small change to this style with less of a metropolitan feel to the footage.
A new sequence was introduced on 28 March 2005, designed and created by Red Bee Media and directed by Mark Chaudoir. The full version ran for 60 seconds, though only around 30 seconds were usually shown on air. The music was revised completely but the biggest change came in the footage used – reflecting the methods and nature of newsgathering, while a strong emphasis was placed on the BBC logo itself. Satellite dishes are shown transmitting and receiving red "data streams". In production of the countdown sequence, Clive Norman filmed images around the United Kingdom, Richard Jopson in the United States, while BBC News cameramen filmed images from Iraq, Beijing (Great Wall of China), Bund of Shanghai, Africa, as well as areas affected by the 2004 Asian Tsunami and others.
The sequence has since seen several remixes to the music and a change in visuals to focus more on the well-known journalists, with less footage of camera crews and production teams. Changes have also seen the channel logo included during the sequences and at the end, as well as the fonts used for the time. The conclusion of the countdown was altered in 2008 to feature the new presentation style, rather than a data stream moving in towards the camera. Also in 2008, the graphic for the countdown changed, resembling BBC One Rhythm and Movement Idents, due to the logo being in a red square in inferior-left corner.
To coincide with the move of BBC News to Broadcasting House, on 18 March 2013 the countdown was updated again along with several other presentation elements. Three of the most striking features of the new countdown include music performed by the BBC Concert Orchestra, a redesign of the "data streams" and the ending of the sequence no longer fading to the BBC News globe and logo, but instead stopping with a time-lapse shot outside the corporation's headquarters. The countdown was also extended back to 90 seconds, of which approximately 86 were seen before the first hour from Broadcasting House.
A full three-minute version of the countdown music was made available on BBC News Online and David Lowe's own website after a remix on 16 May 2006.
An international version of the countdown was launched on BBC World News on 5 September 2005 featuring more international content and similar music. Various changes have been made to the music and visuals since then, with presentation following the style of BBC News. The visuals in the sequence were updated on 10 May 2010. In June 2011, further imagery was added relating to recent events, including the conflict in Libya and views of outside 10 Downing Street. In January 2013, as part of the relocation of BBC News to Broadcasting House in Central London, BBC World News received a new countdown in the same style as the BBC News Channel's updated countdown, with some minor differences.

</doc>
<doc id="4622" url="https://en.wikipedia.org/wiki?curid=4622" title="Bill Oddie">
Bill Oddie

William Edgar "Bill" Oddie OBE (born 7 July 1941) is an English writer, composer, musician, comedian, artist, ornithologist, conservationist and television presenter. He became famous as one of The Goodies.
A birdwatcher since his childhood in Quinton, Birmingham, Oddie has established a reputation as an ornithologist, conservationist, and television presenter on wildlife issues. Some of his books are illustrated with his own paintings and drawings. His wildlife programmes for the BBC include: "Springwatch/Autumnwatch", "How to Watch Wildlife", "Wild In Your Garden", "Birding with Bill Oddie", "Britain Goes Wild with Bill Oddie" and "Bill Oddie Goes Wild".
Early life.
Oddie's father was assistant chief accountant at the Midlands Electricity Board. His mother was diagnosed with schizophrenia, and during most of his youth lived in a hospital. He was educated at Lapal Primary School, Halesowen Grammar School (now The Earls High School, Halesowen) and King Edward's School, Birmingham, an all-boys independent school, where he captained the school's rugby union team. He then studied English Literature at Pembroke College, University of Cambridge.
Career.
Comedy.
Whilst at Cambridge University Oddie appeared in several Footlights Club productions. One of these, a revue called "A Clump of Plinths", was so successful at the Edinburgh Festival Fringe that it was renamed "Cambridge Circus" and transferred to the West End in London, then New Zealand and Broadway in September 1964. Meanwhile, still at Cambridge, Oddie wrote scripts for and appeared briefly in TV's "That Was The Week That Was".
He appeared in Bernard Braden's television series "On The Braden Beat" in 1964. Subsequently, he was a key member of the performers in the BBC radio series "I'm Sorry, I'll Read That Again" (ISIRTA; 1965), where many of his musical compositions were featured. Some were released on the album "Distinctly Oddie" (Polydor, 1967). He was one of the first performers to parody a rock song, arranging the traditional Yorkshire folk song "On Ilkla Moor Baht'at" in the style of Joe Cocker's hit rendition of the Beatles' "With a Little Help from My Friends" (released on John Peel's Dandelion Records in 1970 and featured in Peel's special box of most-treasured singles), and singing "Andy Pandy" in the style of a brassy soul number such as Wilson Pickett or Geno Washington might perform. In many shows he would do short impressions of Hughie Green.
On television Oddie was co-writer and performer in the comedy series "Twice a Fortnight" with Graeme Garden, Terry Jones, Michael Palin and Jonathan Lynn. Later, he was co-writer and performer in the comedy series "Broaden Your Mind" with Tim Brooke-Taylor and Graeme Garden, for which Oddie became a cast member for the second series.
Oddie, Brooke-Taylor and Garden then co-wrote and appeared in their television comedy series "The Goodies". The Goodies also released records, including "Father Christmas Do Not Touch Me"/"The In-Betweenies", "Funky Gibbon", and "Black Pudding Bertha", which were hit singles in 1974–75. They reformed, briefly, in 2005, for a successful 13-date tour of Australia.
Oddie, Brooke-Taylor and Garden voiced characters on the 1983 animated children's programme "Bananaman".
In the Amnesty International show "A Poke in the Eye (With a Sharp Stick)", Oddie, Brooke-Taylor and Garden sang their hit song "Funky Gibbon". They also appeared on "Top of the Pops" with the song. Together with Garden (who is a qualified doctor), Oddie co-wrote many episodes of the television comedy series "Doctor in the House", including most of the first season and all of the second season. He has occasionally appeared on the BBC Radio 4 panel game "I'm Sorry I Haven't a Clue", on which Garden and Brooke-Taylor are regular panellists. In 1982 Garden and Oddie wrote, but did not perform in, a 6-part science fiction sitcom called "Astronauts" for Central and ITV. The show was set in an international space station in the near future.
Natural history.
Oddie's first published work was an article about the birdlife of Birmingham's Bartley Reservoir in the West Midland Bird Club's 1962 Annual Report (he is first credited in the 1956 report, in which reports of his bird observations are tagged with his initials "WEO"). He has since written a number of books about birds and birdwatching, as well as articles for many specialist publications including "British Birds", "Birdwatching Magazine" and "Birdwatch".
He discussed bird song recordings with Derek Jones in an August 1973 BBC Radio 4 programme called "Sounds Natural".
In the autumn of 1976 Oddie was involved in the successful identification of Britain's first ever record of Pallas's reed bunting on Fair Isle, Shetland.
One of Oddie's first forays in the world of television natural history was as a guest on "Animal Magic" in December 1977. Another early natural history radio appearance was in October, as the guest on Radio 4's "Through My Window", discussing the birds of Hampstead Heath.
On 30 July 1985, he was the subject of a 50-minute "Nature Watch Special: Bill Oddie – Bird Watcher", in which he was interviewed by Julian Pettifer at places where he had spent time birding, including Bartley Reservoir, the Christopher Cadbury Wetland Reserve at Upton Warren, RSPB Titchwell Marsh and Blakeney Point.
Oddie has since hosted a number of successful nature programmes for the BBC, many produced by Stephen Moss, including:
On its first evening of broadcast in 2004, "Britain Goes Wild" set a record for its timeslot of 8 pm on BBC Two of 3.4 million viewers, one million more than the Channel 4 programme showing at that time. "Britain Goes Wild", renamed "Springwatch" the following year, became a wildlife broadcasting phenomenon, attracting up to 5 million viewers. 
He became president of the West Midland Bird Club in 1999, having been Vice-President since 1991, and is a former member of the council of the RSPB. Oddie is also a vice-president of the League Against Cruel Sports and a vice-president of the British Trust for Conservation Volunteers. He practised as a bird ringer, but allowed his licence to lapse.
In 2003 Oddie set up a half-marathon to raise money for various wildlife charities in his birth-town of Rochdale. Celebrities who have participated in the event include Ray Mears, Catherine Jenkins and Hugh Fearnley-Whittingstall.
In 2011, Oddie featured as an investigator in "Snares Uncovered: killers in the countryside". The film carried out an expose of snaring in Scotland and was commissioned by animal protection charity OneKind. During the investigation, Oddie discovered over 70 snare traps and several stink pits.
Music.
Oddie wrote original music at Cambridge University for the Footlights and later wrote comic songs for "I'm Sorry, I'll Read That Again". He also wrote a number of comic songs for The Goodies, most of them performed by Oddie.
In the 1960s and early 1970s, Oddie released a number of singles and at least one album. One of the former, issued in 1970 on John Peel's Dandelion Records label (Catalogue No: 4786), was "On Ilkla Moor Baht 'at", performed in the style of Joe Cocker's "With a Little Help from my Friends". The B-side, "Harry Krishna", featured the Hare Krishna chant, substituting the names of contemporary famous people called Harry, including Harry Secombe, Harry Worth, Harry Lauder, and Harry Corbett, as well as puns such as "Harry along now", "Harrystotle [Aristotle" and ending with "Harry-ly really must go now". Both tracks appear on the compilation CD "Life Too, Has Surface Noise: The Complete Dandelion Records Singles Collection 1969–1972" (2007).
He played the drums and saxophone and appeared as Cousin Kevin in a production of The Who's rock opera "Tommy" at the Rainbow Theatre, Finsbury Park, London on 9 December 1972. He has also contributed vocals to a Rick Wakeman album, "Criminal Record".
Oddie took part in the English National Opera production of the Gilbert and Sullivan comic opera "The Mikado", in which he appeared in the role of the "Lord High Executioner", taking over the role from Eric Idle. During the early 1990s, Oddie was a DJ for London-based jazz radio station 102.2 Jazz FM.
In 2007 Oddie appeared on the BBC series "Play It Again". In the episode he attempts to realise his dream of becoming a rock guitarist. Initially teacher Bridget Mermikides tries to teach him using traditional methods but he rebels: instead he turns to old friends Albert Lee, Dave Davies (of The Kinks) and Mark Knopfler for advice and strikes out on his own. He succeeds in the target of playing lead guitar for his daughter Rosie's band at her 21st birthday party, and even manages to impress his erstwhile teacher.
In November 2010, he agreed, along with fellow members of The Goodies, to re-release their 1970s hit The Funky Gibbon to raise funds for the International Primate Protection League's Save the Gibbon appeal.
Other television and audio appearances.
Oddie appeared as the hapless window cleaner in the Eric Sykes' comedy story "The Plank" in 1967. He also presented the live children's Saturday morning entertainment show "Saturday Banana" (ITV/Southern Television) during the late 1970s. In the late 1980s he was a presenter of the BBC TV show "Fax" (a show about 'facts').
In 1981 he appeared as a Telethon celebrity in New Zealand, hosted by TV1. He voices Asterix in the UK dub of the 1989 animated film "Asterix and the Big Fight" (an animated adaptation of the books "Asterix and the Big Fight" and "Asterix and the Soothsayer", novelized as "Operation Getafix").
In the 1990s he became better known as a presenter of birdwatching, and later wildlife-related, programmes such as "Springwatch". Although he remains almost unknown to US audiences, in 1992 he was a guest star in the US comedy television series "Married... with Children" for a 3-part episode set in England.
In 1997–98 he appeared on the Channel 4 archaeological programme "Time Team", as the team excavated a Roman villa site in Turkdean, Gloucestershire.
He was the compère of a daytime BBC gameshow "History Hunt" (in 2003); and has appeared in the "Doctor Who" audio drama "Doctor Who and the Pirates". In 2004, he appeared in the BBC show "Who Do You Think You Are?", in which he looked into his ancestry – he was visibly moved by its revelations. In 2005, he took part in "Rolf on Art – the big event at Trafalgar Square" and in September that year was also a celebrity guest along with Lynda Bellingham on the ITV1 programme "Who Wants to be a Millionaire".
In 2006 Oddie appeared in the BBC show "Never Mind The Buzzcocks", and also appeared on the topical quiz show "8 out of 10 Cats". He was also the voice behind many B&Q adverts throughout 2006/2007. On 25 May 2007, Oddie made a cameo appearance on Ronni Ancona's new comedy sketch show, "Ronni Ancona & Co".
He hosted the genealogy-based series "My Famous Family", broadcast on UKTV History in 2007. In 2008, Oddie was a guest on Jamie Oliver's television special "Jamie's Fowl Dinners", talking about free-range chickens. He also recorded a voice for Lionhead Studios' "Fable II".
He also appeared on "Would I Lie To You?" in 2011 where he revealed that he was saved from drowning by Freddy from popular children's series "Rainbow" and "Rod, Jane and Freddy" while on holiday in the Seychelles.
In February 2015, Oddie appeared in "The Keith Lemon Sketch Show" as the narrator of the sketch "Ed Sheeran Watch".
He appeared as a contestant on a celebrity edition of "Fifteen to One" in August 2015.
2013 Australian tour.
Oddie undertook an Australian tour during June 2013 in all of the mainland states capital cities – Brisbane, Sydney, Melbourne, Adelaide and Perth – in a series of one-off shows, ""An Oldie but a Goodie"". A video message from Tim Brooke-Taylor and Graeme Garden was shown during the performances. Oddie made personal appearances on both "The Project" and "Adam Hills Tonight" TV shows during the tour; he also filmed a guest-programming spot for the ABC-TV's all-night music video show "Rage". 
Personal life.
Family.
In 1967 Oddie married Jean Hart, and from this marriage he has two daughters, Bonnie Oddie and Kate Hardie, an actress, as well as three grandchildren, Lyle, Ella, and Gracie.
In 1983 he married Laura Beaumont-Giles, with whom he has worked on a variety of projects for children, including film scripts, drama and comedy series, puppet shows and books. They have a daughter, Rosie, born in October 1985, and live in Hampstead, North London. Rosie Oddie is a musician, also using the name Rosie Bones.
Depression and bipolar disorder.
Oddie has suffered from depression for most of his life and was diagnosed with clinical depression in 2001. On 11 March 2009 it was reported that he had been admitted to Capio Nightingale psychiatric hospital in Marylebone, to deal with his depression. His then agent David Foster said: "Bill gets these bouts every two or three years where he gets down for about two weeks and recovers. He sometimes goes into hospital or takes a break or has a change of scenery to recharge his batteries." In January 2010 Oddie spoke to the media, revealing that he had in fact had two separate stays in different hospitals, only being discharged "in time for Christmas". He said that he was dealing with depression and bipolar disorder, describing the period as "probably the worst 12 months of my life". Oddie stated that he was planning to meet with BBC executives to discuss his return to television work.
His illness meant that Oddie did not appear in the 2009 and 2010 series of "Springwatch", although he made a guest appearance in the penultimate episode of the latter. He subsequently claimed he was dismissed from "Springwatch" and that this had caused the depressive illness.
Oddie presented the BBC Radio 4 Appeal programme on 10 August 2014 on behalf of the charity Bipolar UK. He revealed that as a consequence of his bipolar disorder he had attempted suicide during one of his depressive episodes. On the UK TV program "Who Do You Think You Are?" he attributed his depression and bipolar disorder as an adult to his minimal and painful relationship with his mother.
Political views.
Oddie supports the Green Party. In October 2014, on the BBC's "Sunday Morning Live", he stated that he wanted a limit on the number of children that British families can have, saying that he was "very often ashamed" to be British, whom he called "a terrible race".
Honours.
In 2001, Oddie became the third person to decline to appear on "This Is Your Life". He changed his mind a few hours later and agreed to appear on the show. On 16 October 2003, Oddie was made an OBE for his service to Wildlife Conservation in a ceremony at Buckingham Palace. He played down the event, choosing to wear a camouflage shirt and crumpled jacket to receive his medal. In June 2004, Oddie and Johnny Morris were jointly profiled in the first of a three part BBC Two series, "The Way We Went Wild", about television wildlife presenters. In May 2005, he received the British Naturalists' Association's Peter Scott Memorial Award, from BNA president David Bellamy, "in recognition of his great contribution to our understanding of natural history and conservation." He is a recipient of the RSPB Medal.
On 30 June 2009, he was proposed for inclusion in the Birmingham Walk of Stars, with the public invited to vote.
Bibliography.
Bill Oddie also co-wrote the Springwatch & Autumnwatch book with Kate Humble and Simon King.
Co-written with the other members of The Goodies:
Co-written with Laura Beaumont:
In popular culture.
In the fictional world of comedy character Alan Partridge, Oddie is an unseen presence in Alan's life.
He has also been referenced, often humorously, by the hosts of "Top Gear". Jeremy Clarkson even used a mask with Oddie's face to escape speed cameras while racing the Nissan GT-R against the Bullet train in Japan in Episode 4 of Top Gear's 11th series. James May also raced in Finland against an Oddie lookalike who won the race.

</doc>
<doc id="4626" url="https://en.wikipedia.org/wiki?curid=4626" title="Broadway (Manhattan)">
Broadway (Manhattan)

Broadway is a road in the U.S. state of New York. Broadway runs through the borough of Manhattan and through the Bronx, exiting north from the city to run an additional through the municipalities of Yonkers, Hastings-On-Hudson, Dobbs Ferry, Irvington, and Tarrytown, and terminating north of Sleepy Hollow in Westchester County.
It is the oldest north–south main thoroughfare in New York City, dating to the first New Amsterdam settlement, although most of it did not bear its current name until the late 19th century. The name "Broadway" is the English literal translation of the Dutch name, "Breede weg". Broadway is known widely as the heart of the American theatre industry.
History.
Broadway was originally the Wickquasgeck Trail, carved into the brush of Manhattan by its Native American inhabitants. Wickquasgeck means "birch-bark country" in the Algonquian language. This trail originally snaked through swamps and rocks along the length of Manhattan Island.
Upon the arrival of the Dutch, the trail soon became the main road through the island from "Nieuw Amsterdam" at the southern tip. The Dutch explorer and entrepreneur David Pietersz. de Vries gives the first mention of it in his journal for the year 1642 ("the Wickquasgeck Road over which the Indians passed daily"). The Dutch named the road ""Heerestraat"". Although current street signs are simply labeled as "Broadway", in a 1776 map of New York City, Broadway is explicitly labeled "Broadway Street". In the mid-eighteenth century, part of Broadway in what is now lower Manhattan was known as "Great George Street".
In the 18th century, Broadway ended at the town commons north of Wall Street, where traffic continued up the East Side of the island via Eastern Post Road and the West Side via Bloomingdale Road. The western Bloomingdale Road would be widened and paved during the 19th century, and called "Western Boulevard" or "The Boulevard" north of the Grand Circle, now called Columbus Circle. On February 14, 1899, the name "Broadway" was extended to the entire Broadway/Bloomingdale/Boulevard road.
Traffic flow.
Broadway once was a two-way street for its entire length. The present status, in which it runs one-way southbound south of Columbus Circle (59th Street), came about in several stages. On 6 June 1954, Seventh Avenue became southbound and Eighth Avenue became northbound south of Broadway. None of Broadway became one-way, but the increased southbound traffic between Columbus Circle (Eighth Avenue) and Times Square (Seventh Avenue) caused the city to re-stripe that section of Broadway for four southbound and two northbound lanes. Broadway became one-way from Columbus Circle south to Herald Square (34th Street) on 10 March 1957, in conjunction with Sixth Avenue becoming one-way from Herald Square north to 59th Street and Seventh Avenue becoming one-way from 59th Street south to Times Square (where it crosses Broadway). On 3 June 1962, Broadway became one-way south of Canal Street, with Trinity Place and Church Street carrying northbound traffic. Another change was made on 10 November 1963, when Broadway became one-way southbound from Herald Square to Madison Square (23rd Street) and Union Square (14th Street) to Canal Street, and two routes – Sixth Avenue south of Herald Square and Centre Street, Lafayette Street, and Fourth Avenue south of Union Square – became one-way northbound. Finally, at the same time as Madison Avenue became one-way northbound and Fifth Avenue became one-way southbound, Broadway was made one-way southbound between Madison Square (where Fifth Avenue crosses) and Union Square on 14 January 1966, completing its conversion south of Columbus Circle.
In August 2008, two traffic lanes from 42nd to 35th Streets were taken out of service and converted to public plazas. Additionally, bike lanes were added on Broadway from 42nd Street down to Union Square.
Since May 2009, the portions of Broadway through Duffy Square, Times Square, and Herald Square have been closed entirely to automobile traffic, except for cross traffic on the Streets and Avenues, as part of a traffic and pedestrianization experiment, with the pavement reserved exclusively for walkers, cyclists, and those lounging in temporary seating placed by the city. The city decided that the experiment was successful and decided to make the change permanent in February 2010. Though the anticipated benefits to traffic flow were not as large as hoped, pedestrian injuries dropped dramatically and foot traffic increased in the designated areas; the project was popular with both residents and businesses. The current portions converted into pedestrian plazas are between West 47th Street and West 42nd Street within Times and Duffy Squares, and between West 35th Street and West 33rd Street in the Herald Square area. Additionally, portions of Broadway in the Madison Square and Union Square have been dramatically narrowed, allowing ample pedestrian plazas to exist along the side of the road.
In May 2013, the NYCDOT decided to redesign Broadway between 35th and 42nd Streets for the second time in five years, owing to poor connections between pedestrian plazas and decreased vehicular traffic. With the new redesign, the bike lane is now on the right side of the street; it was formerly on the left side adjacent to the pedestrian plazas, causing conflicts between pedestrian and bicycle traffic.
Route.
Route description.
Broadway runs the length of Manhattan Island, roughly parallel to the North River (the portion of the Hudson River bordering Manhattan), from Bowling Green at the south to Inwood at the northern tip of the island. South of Columbus Circle, it is a one-way southbound street. Since 2009, vehicular traffic has been banned at Times Square between 47th and 42nd Streets, and at Herald Square between 35th and 33rd Streets as part of a pilot program; the right-of-way is intact and reserved for cyclists and pedestrians. From the northern shore of Manhattan, Broadway crosses Spuyten Duyvil Creek via the Broadway Bridge and continues through Marble Hill (a discontinuous portion of the borough of Manhattan) and the Bronx into Westchester County. U.S. 9 continues to be known as Broadway until its junction with NY 117.
Lower Manhattan.
The section of lower Broadway from its origin at Bowling Green to City Hall Park is the historical location for the city's ticker-tape parades, and is sometimes called the "Canyon of Heroes" during such events. West of Broadway, as far as Canal Street, was the city's fashionable residential area until circa 1825; landfill has more than tripled the area, and the Hudson River shore now lies far to the west, beyond Tribeca and Battery Park City.
Broadway marks the boundary between Greenwich Village to the west and the East Village to the east, passing Astor Place. It is a short walk from there to New York University near Washington Square Park, which is at the foot of Fifth Avenue. A bend in front of Grace Church allegedly avoids an earlier tavern; from 10th Street it begins its long diagonal course across Manhattan, headed almost due north.
Midtown Manhattan.
Because Broadway preceded the grid that the Commissioners' Plan of 1811 imposed on the island, Broadway crosses midtown Manhattan diagonally, intersecting with both the east-west streets and north-south avenues. Broadway's intersections with avenues, marked by "squares" (some merely triangular slivers of open space), have induced some interesting architecture, such as the Flatiron Building.
At Union Square, Broadway crosses 14th Street, merges with 4th Avenue, and continues its diagonal uptown course from the Square's northwest corner; Union Square is the only location wherein the physical section of Broadway is discontinuous in Manhattan (other portions of Broadway in Manhattan are pedestrians-only plazas). At Madison Square, the location of the Flatiron Building, Broadway crosses Fifth Avenue at 23rd Street. At Greeley Square (West 33rd Street), Broadway crosses Sixth Avenue (Avenue of the Americas), and is discontinuous to vehicles. Macy's Herald Square department store, one block north of the vehicular discontinuity, is located on the northwest corner of Broadway and West 34th Street and southwest corner of Broadway and West 35th Street; it is one of the largest department stores in the world.
One famous stretch near Times Square, where Broadway crosses Seventh Avenue in midtown Manhattan, is the home of many Broadway theatres, housing an ever-changing array of commercial, large-scale plays, particularly musicals. This area of Manhattan is often called the Theater District or the Great White Way, a nickname originating in the headline "Found on the Great White Way" in the edition of 3 February 1902 of the "New York Evening Telegram". The journalistic nickname was inspired by the millions of lights on theater marquees and billboard advertisements that illuminate the area. After becoming the city's de facto red-light district in the 1960s and 1970s (as can be seen in the films "Taxi Driver" and "Midnight Cowboy"), since the late 1980s Times Square has emerged as a family tourist center, in effect being Disneyfied following the company's purchase and renovation of the New Amsterdam Theatre on 42nd Street in 1993. Until June 2007, "The New York Times", from which the Square gets its name, was published at offices at 239 West 43rd Street; the paper stopped printing papers there on 15 June 2007.
Upper West Side.
At the southwest corner of Central Park, Broadway crosses Eighth Avenue (called Central Park West north of 59th Street) at West 59th Street and Columbus Circle; on the site of the former New York Coliseum convention center is the new shopping center at the foot of the Time Warner Center, headquarters of Time Warner. From Columbus Circle northward, Broadway becomes a wide boulevard to 169th Street; it retains landscaped center islands that separate northbound from southbound traffic. The medians are a vestige of the central mall of "The Boulevard" that had become the spine of the Upper West Side, and many of these contain public seating.
Broadway intersects with Columbus Avenue (known as Ninth Avenue south of West 59th Street) at West 65th and 66th Streets where the Juilliard School and Lincoln Center, both well-known performing arts landmarks, as well as the Manhattan New York Temple of The Church of Jesus Christ of Latter-day Saints are located.
From West 70th to West 73rd Streets, Broadway intersects with Amsterdam Avenue (known as 10th Avenue south of West 59th Street). The wide intersection of the two thoroughfares has historically been the site of numerous traffic accidents and pedestrian casualties, partly due to the long crosswalks. Two small triangular plots of land were created at points where Broadway slices through Amsterdam Avenue. One is a tiny fenced-in patch of shrubbery and plants at West 70th Street called Sherman Square (although it and the surrounding intersection have also been known collectively as Sherman Square), and the other triangle is a lush tree-filled garden bordering Amsterdam Avenue from just above West 72nd Street to West 73rd Street. Called Verdi Square for its monument to Italian composer Giuseppe Verdi, which was erected in 1909, this triangular sliver of public space was designated a Scenic Landmark by the Landmarks Preservation Commission in 1974. In the 1960s and 1970s, the area surrounding both Verdi Square and Sherman Square was known by local drug users and dealers as "Needle Park", and was featured prominently in the gritty 1971 dramatic film "The Panic in Needle Park", directed by Jerry Schatzberg and starring Al Pacino in his second onscreen role.
The original brick and stone shelter leading to the entrance of the 72nd Street subway station, one of the first 28 subway stations in Manhattan, remains located on one of the wide islands in the center of Broadway, on the south side of West 72nd Street. For many years, all traffic on Broadway flowed on either side of this median and its subway entrance, and its uptown lanes went past it along the western edge of triangular Verdi Square. In 2001 and 2002, renovation of the historic 72nd St. station and the addition of a second subway control house and passenger shelter on an adjacent center median just north of 72nd Street, across from the original building, resulted in the creation of a public plaza with stone pavers and extensive seating, connecting the newer building with Verdi Square, and making it necessary to divert northbound traffic to Amsterdam Avenue for one block. While Broadway's southbound lanes at this intersection were unaffected by the new construction, its northbound lanes are no longer contiguous at this intersection. Drivers can either continue along Amsterdam Avenue to head uptown or turn left on West 73rd Street to resume traveling on Broadway.
Several notable apartment buildings are in close proximity to this intersection, including The Ansonia, its ornate architecture dominating the cityscape here. After the Ansonia first opened as a hotel, live seals were kept in indoor fountains inside its lobby. Later, it was home to the infamous Plato's Retreat nightclub. Immediately north of Verdi Square is the formidable Apple Bank for Savings building, formerly the Central Savings Bank, which was built in 1926 and designed to resemble the Federal Reserve Bank of New York. Broadway is also home to the Beacon Theatre at West 74th Street, designated a national landmark in 1979 and still in operation as a concert venue after its establishment in 1929 as a vaudeville and music hall, and "sister" venue to Radio City Music Hall.
At its intersection with West 78th Street, Broadway shifts direction and continues directly uptown and aligned approximately with the Commissioners' grid. Past the bend are the historic Apthorp apartment building, built in 1908, and the First Baptist Church in the City of New York, incorporated in New York in 1762, its current building on Broadway erected in 1891. The road heads north and passes historically important apartment houses such as the Belnord, the Astor Court Building, and the Art Nouveau Cornwall.
At Broadway and 95th Street, one finds Symphony Space, established in 1978 as home to avant-garde and classical music and dance performances in the former Symphony Theatre, which was originally built in 1918 as a premier "music and motion-picture house". At 99th Street, Broadway passes between the controversial skyscrapers of the Ariel East and West.
At 107th Street, Broadway intersects with West End Avenue to form Straus Park with its Titanic Memorial by Augustus Lukeman.
Northern Manhattan and the Bronx.
Broadway then passes the campus of Columbia University at 116th Street in Morningside Heights, in part on the tract that housed the Bloomingdale (Lunatic) Asylum from 1808 until it moved to Westchester County in 1894. Still in Morningside Heights, Broadway passes the handsome, park-like campus of Barnard College. Next, the beautiful Gothic quadrangle of Union Theological Seminary and the brick buildings of the Jewish Theological Seminary of America with their beautifully landscaped interior courtyards face one another across Broadway. On the next block is the Manhattan School of Music.
Broadway then runs past the proposed uptown campus of Columbia University, and the main campus of CUNY–City College near 135th Street; the beautiful Gothic buildings of the original City College campus are out of sight, a block to the east. Also to the east are the handsome brownstones of Hamilton Heights. Hamilton Place is a surviving section of Bloomingdale Road, and originally the address of Alexander Hamilton's house, The Grange, which has been moved.
Broadway achieves a verdant, park-like effect, particularly in the spring, when it runs between the uptown Trinity Church Cemetery and the former Trinity Chapel, now the Church of the Intercession near 155th Street.
NewYork-Presbyterian Hospital lies on Broadway near 166th, 167th, and 168th Streets in Washington Heights. The intersection with Saint Nicholas Avenue at 167th Street forms Mitchell Square Park. At 178th Street, U.S. 9 becomes concurrent with Broadway.
Broadway crosses the Harlem River on the Broadway Bridge to Marble Hill. Afterward, it then enters the Bronx, where it is the eastern border of Riverdale and the western border of Van Cortlandt Park. At 253rd Street, NY 9A joins with U.S. 9 and Broadway. (NY 9A splits off Broadway at Ashburton Avenue in Yonkers.)
Westchester County.
The northwestern corner of the park marks the city limit and Broadway enters Yonkers, where it is now known as South Broadway. It trends ever westward, closer to the Hudson River, remaining a busy urban commercial street. In downtown Yonkers, it drops close to the river, becomes North Broadway and 9A leaves via Ashburton Avenue. Broadway climbs to the nearby ridgetop runs parallel to the river and the railroad, a few blocks east of both as it passes St. John's Riverside Hospital. The neighborhoods become more residential and the road gently undulates along the ridgetop. In Yonkers, Broadway passes historic Philipse Manor house, which dates back to colonial America.
It remains Broadway as it leaves Yonkers for Hastings-on-Hudson, where it splits into separate north and south routes for . The trees become taller and the houses, many separated from the road by stone fences, become larger. Another National Historic Landmark, the John William Draper House, was the site of the first astrophotograph of the Moon.
In the next village, Dobbs Ferry, Broadway has various views of the Hudson River while passing through the residential section. Broadway passes by the Old Croton Aqueduct and nearby the shopping district of the village. After intersecting with Ashford Avenue, Broadway passes Mercy College, then turns left again at the center of town just past South Presbyterian Church, headed for equally comfortable Ardsley-on-Hudson and Irvington. Villa Lewaro, the home of Madam C.J. Walker, the first African-American millionaire, is along the highway here. At the north end of the village of Irvington, a memorial to writer Washington Irving, after whom the village was renamed, marks the turnoff to his home at Sunnyside. Entering into the southern portion of Tarrytown, Broadway passes by historic Lydhurst mansion, a massive mansion built along the Hudson River built in the early 1800s.
North of here, at the Kraft Foods technical center, the Tappan Zee Bridge becomes visible. After crossing under the Thruway and I-87 again, here concurrent with I-287, and then intersecting with the four-lane NY 119, where 119 splits off to the east, Broadway becomes the busy main street of Tarrytown. Christ Episcopal Church, where Irving worshiped, is along the street. Many high quality restaurants and shops are along this main road. This downtown ends at the eastern terminus of NY 448, where Broadway slopes off to the left, downhill, and four signs indicate that Broadway turns left, passing the Old Dutch Church of Sleepy Hollow, another NHL. The road then enters Sleepy Hollow (formerly North Tarrytown), passing the visitors' center for Kykuit, the National Historic Landmark that was (and partially still is) the Rockefeller family's estate. Broadway then passes the historic Sleepy Hollow Cemetery, which includes the resting place of Washington Irving and the setting for The Legend of Sleepy Hollow.
Broadway expands to four lanes at the trumpet intersection with NY 117, where it finally ends and U.S. 9 becomes Albany Post Road (and Highland Avenue) at the northern border of Sleepy Hollow, New York.
Nicknamed sections.
Canyon of Heroes.
"Canyon of Heroes" is occasionally used to refer to the section of lower Broadway in the Financial District that is the location of the city's ticker-tape parades.
The traditional route of the parade is northward from Bowling Green to City Hall Park. Most of the route is lined with tall office buildings along both sides, affording a view of the parade for thousands of office workers who create the snowstorm-like jettison of shredded paper products that characterize the parade.
While typical sports championship parades have been showered with some 50 tons of confetti and shredded paper, the V-J Day parade on August 14–15, 1945 – marking the end of World War II – was covered with 5,438 tons of paper, based on estimates provided by the New York City Department of Sanitation.
More than 200 black granite strips embedded in the sidewalks along the Canyon of Heroes list honorees of past ticker-tape parades.
The most recent parade up the Canyon of Heroes took place on July 10, 2015 for the United States women's national soccer team in honor of their 2015 FIFA Women's World Cup championship.
Great White Way.
"The Great White Way" is a nickname for a section of Broadway in Midtown Manhattan, specifically the portion that encompasses the Theater District, between 42nd and 53rd Streets, and encompassing Times Square.
In 1880, a stretch of Broadway between Union Square and Madison Square was illuminated by Brush arc lamps, making it among the first electrically lighted streets in the United States. By the 1890s, the portion from 23rd Street to 34th Street was so brightly illuminated by electrical advertising signs, that people began calling it "The Great White Way". When the theater district moved uptown, the name was transferred to the Times Square area.
The phrase "Great White Way" has been attributed to Shep Friedman, columnist for the "New York Morning Telegraph" in 1901, who lifted the term from the title of a book about the Arctic by Albert Paine. The headline "Found on the Great White Way" appeared in the February 3, 1902, edition of the "New York Evening Telegram".
A portrait of Broadway in the early part of the 20th century and "The Great White Way" late at night appeared in "Artist In Manhattan"(1940) written by the artist-historian Jerome Myers:
Transportation.
From south to north, Broadway at one point or another runs over or under various New York City Subway lines, including the IRT Lexington Avenue Line, the BMT Broadway Line, IRT Broadway – Seventh Avenue Line, and IND Eighth Avenue Line (the IND Sixth Avenue Line is the only north-south trunk line in Manhattan that does not run along Broadway).
Early street railways on Broadway included the Broadway and Seventh Avenue Railroad's Broadway and University Place Line (1864?) between Union Square (14th Street) and Times Square (42nd Street), the Ninth Avenue Railroad's Ninth and Amsterdam Avenues Line (1884) between 65th Street and 71st Street, the Forty-second Street, Manhattanville and St. Nicholas Avenue Railway's Broadway Branch Line (1885?) between Times Square and 125th Street, and the Kingsbridge Railway's Kingsbridge Line north of 169th Street. The Broadway Surface Railroad's Broadway Line, a cable car line, opened on lower Broadway (below Times Square) in 1893, and soon became the core of the Metropolitan Street Railway, with two cable branches: the Broadway and Lexington Avenue Line and Broadway and Columbus Avenue Line.
These streetcar lines were replaced with bus routes in the 1930s and 1940s. Before Broadway became one-way, the main bus routes along it were the New York City Omnibus Company's (NYCO) 6 (Broadway below Times Square), 7 (Broadway and Columbus Avenue), and 11 (Ninth and Amsterdam Avenues), and the Surface Transportation Corporation's M100 (Kingsbridge) and M104 (Broadway Branch). Additionally, the Fifth Avenue Coach Company's (FACCo) 4 and 5 used Broadway from 135th Street north to Washington Heights, and their 5 and 6 used Broadway between 57th Street and 72nd Street. With the implementation of one-way traffic, the northbound 6 and 7 were moved to Sixth Avenue.
, Broadway is now served by the M4 (ex-FACCo 4), M5 (ex-FACCo 5), M7 (ex-NYCO 7), M100, and M104. Other routes that use part of Broadway include the M10, M20, M60 Select Bus Service, Bx7, Bx9, and Bx20.
Bee-Line buses also serves Broadway within Riverdale and Westchester County. Routes 1, 2, 3, 4, 6, 13, and several others run on a portion of Broadway.
Notable buildings.
Broadway is lined with many famous and otherwise noted and historic buildings, such as:
Historic buildings on Broadway that are now demolished include:
References.
Notes
Bibliography

</doc>
<doc id="4628" url="https://en.wikipedia.org/wiki?curid=4628" title="Bilinear transform">
Bilinear transform

The bilinear transform (also known as Tustin's method) is used in digital signal processing and discrete-time control theory to transform continuous-time system representations to discrete-time and vice versa.
The bilinear transform is a special case of a conformal mapping (namely, the Möbius transformation), often used to convert a transfer function formula_1 of a linear, time-invariant (LTI) filter in the continuous-time domain (often called an analog filter) to a transfer function formula_2 of a linear, shift-invariant filter in the discrete-time domain (often called a digital filter although there are analog filters constructed with switched capacitors that are discrete-time filters). It maps positions on the formula_3 axis, formula_4, in the s-plane to the unit circle, formula_5, in the z-plane. Other bilinear transforms can be used to warp the frequency response of any discrete-time linear system (for example to approximate the non-linear frequency resolution of the human auditory system) and are implementable in the discrete domain by replacing a system's unit delays formula_6 with first order all-pass filters.
The transform preserves stability and maps every point of the frequency response of the continuous-time filter, formula_7 to a corresponding point in the frequency response of the discrete-time filter, formula_8 although to a somewhat different frequency, as shown in the Frequency warping section below. This means that for every feature that one sees in the frequency response of the analog filter, there is a corresponding feature, with identical gain and phase shift, in the frequency response of the digital filter but, perhaps, at a somewhat different frequency. This is barely noticeable at low frequencies but is quite evident at frequencies close to the Nyquist frequency.
Discrete-time approximation.
The bilinear transform is a first-order approximation of the natural logarithm function that is an exact mapping of the "z"-plane to the "s"-plane. When the Laplace transform is performed on a discrete-time signal (with each element of the discrete-time sequence attached to a correspondingly delayed unit impulse), the result is precisely the Z transform of the discrete-time sequence with the substitution of
where formula_10 is the numerical integration step size of the trapezoidal rule used in the bilinear transform derivation; or, in other words, the sampling period. The above bilinear approximation can be solved for formula_11 or a similar approximation for formula_12 can be performed.
The inverse of this mapping (and its first-order bilinear approximation) is
The bilinear transform essentially uses this first order approximation and substitutes into the continuous-time transfer function, formula_1
That is
Stability and minimum-phase property preserved.
A continuous-time causal filter is stable if the poles of its transfer function fall in the left half of the complex s-plane. A discrete-time causal filter is stable if the poles of its transfer function fall inside the unit circle in the complex z-plane. The bilinear transform maps the left half of the complex s-plane to the interior of the unit circle in the z-plane. Thus filters designed in the continuous-time domain that are stable are converted to filters in the discrete-time domain that preserve that stability.
Likewise, a continuous-time filter is minimum-phase if the zeros of its transfer function fall in the left half of the complex s-plane. A discrete-time filter is minimum-phase if the zeros of its transfer function fall inside the unit circle in the complex z-plane. Then the same mapping property assures that continuous-time filters that are minimum-phase are converted to discrete-time filters that preserve that property of being minimum-phase.
Example.
As an example take a simple low-pass RC filter. This continuous-time filter has a transfer function
If we wish to implement this filter as a digital filter, we can apply the bilinear transform by substituting for formula_18 the formula above; after some reworking, we get the following filter representation:
The coefficients of the denominator are the 'feed-backward' coefficients and the coefficients of the numerator are the 'feed-forward' coefficients used to implement a real-time digital filter.
General second-order biquad transformation.
It is possible to relate the coefficients of a continuous-time, analog filter with those of a similar discrete-time digital filter created through the bilinear transform process. Transforming a general, second-order continuous-time filter with the given transfer function
using the bilinear transform (without prewarping any frequency specification) requires the substitution of
where formula_21.
This results in a discrete-time digital biquad filter with coefficients expressed in terms of the coefficients of the original continuous time filter:
Normally the constant term in the denominator must be normalized to 1 before deriving the corresponding difference equation. This results in
The difference equation (using the Direct Form I) is
Frequency warping.
To determine the frequency response of a continuous-time filter, the transfer function formula_1 is evaluated at formula_26 which is on the formula_3 axis. Likewise, to determine the frequency response of a discrete-time filter, the transfer function formula_2 is evaluated at formula_29 which is on the unit circle, formula_5. When the actual frequency of formula_31 is input to the discrete-time filter designed by use of the bilinear transform, it is desired to know at what frequency, formula_32, for the continuous-time filter that this formula_31 is mapped to.
This shows that every point on the unit circle in the discrete-time filter z-plane, formula_29 is mapped to a point on the formula_36 axis on the continuous-time filter s-plane, formula_37. That is, the discrete-time to continuous-time frequency mapping of the bilinear transform is
and the inverse mapping is
The discrete-time filter behaves at frequency formula_40 the same way that the continuous-time filter behaves at frequency formula_41. Specifically, the gain and phase shift that the discrete-time filter has at frequency formula_40 is the same gain and phase shift that the continuous-time filter has at frequency formula_41. This means that every feature, every "bump" that is visible in the frequency response of the continuous-time filter is also visible in the discrete-time filter, but at a different frequency. For low frequencies (that is, when formula_44 or formula_45), formula_46.
One can see that the entire continuous frequency range
is mapped onto the fundamental frequency interval
The continuous-time filter frequency formula_49 corresponds to the discrete-time filter frequency formula_50 and the continuous-time filter frequency formula_51 correspond to the discrete-time filter frequency formula_52
One can also see that there is a nonlinear relationship between formula_32 and formula_54 This effect of the bilinear transform is called frequency warping. The continuous-time filter can be designed to compensate for this frequency warping by setting formula_55 for every frequency specification that the designer has control over (such as corner frequency or center frequency). This is called pre-warping the filter design.
When designing a digital filter as an approximation of a continuous time filter, the frequency response (both amplitude and phase) of the digital filter can be made to match the frequency response of the continuous filter at frequency formula_56 if the following transform is substituted into the continuous filter transfer function. This is a modified version of Tustin's transform shown above. However, note that this transform becomes the above transform as formula_57. That is to say, the above transform causes the digital filter response to match the analog filter response at DC.
The main advantage of the warping phenomenon is the absence of aliasing distortion of the frequency response characteristic, such as observed with Impulse invariance. It is necessary, however, to compensate for the frequency warping by pre-warping the given frequency specifications of the continuous-time system. These pre-warped specifications may then be used in the bilinear transform to obtain the desired discrete-time system.

</doc>
<doc id="4631" url="https://en.wikipedia.org/wiki?curid=4631" title="Brian Boitano">
Brian Boitano

Brian Anthony Boitano (born October 22, 1963) is an American figure skater from Sunnyvale, California. He is the 1988 Olympic champion, the 1986 and 1988 World Champion, and the 1985–1988 U.S. National Champion. He turned professional following the 1988 season. He returned to competition in 1993 and competed at the 1994 Winter Olympics, where he placed sixth.
Early life.
Brian Boitano was born in Mountain View, California, and as an adult has lived in San Francisco. Boitano is a graduate of Marian A. Peterson High School in Sunnyvale, California.
Figure skating career.
Early career.
Brian Boitano first made his mark on the international scene when he won the bronze medal at the 1978 World Junior Figure Skating Championships, beating future rival Brian Orser for that medal.
In 1982 Boitano became the first American to land a triple axel. In 1987 he introduced his signature jump, the 'Tano triple lutz' in which the skater raises his left arm above his head. He attempted a quadruple jump throughout the 1986–87 season and at the 1988 World Figure Skating Championships, but did not cleanly land the jump; he double-footed the landing on two occasions.
Boitano was known primarily as a jumper early in his career and he, along with several other skaters, helped push the technical envelope of men's skating. It was not until his failure to defend his World title in 1987 that he focused specifically on improving his artistry.
Boitano placed second at the 1984 United States Figure Skating Championships, earning himself a trip to the 1984 Winter Olympics. He placed 5th at the Olympics, setting the stage for his success over the next four years.
World Champion.
Following the 1984 Olympics, several skaters emerged as likely medal hopes following the retirement of Scott Hamilton.
Boitano won the 1985 United States Figure Skating Championships, the first of his four titles. At the first World Championships of the post-Hamilton era in 1985, Alexander Fadeev won, with Brian Orser finishing 2nd and Boitano 3rd. He had injured tendons in his right ankle a few weeks before the 1986 U.S. Championships but went on to win his second national title. At the 1986 World Championships, Boitano took the title, while Fadeev had a disastrous free skate despite having been in an excellent position to win; Orser finished 2nd once again.
During the 1986–87 season, Boitano had introduced two new elements to his programs: the 'Tano triple lutz and a quadruple toe loop, although he never succeeded in landing a clean quadruple jump in competition. The 1987 World Championships were held in Cincinnati, giving defending World champion Boitano a home-field advantage. The outcome of the event would set the tone for the 1988 Olympics. At Worlds, Boitano fell on his quadruple toe loop attempt and placed second.
After losing the world title to Orser at home, Boitano and his coach Linda Leaver decided that some changes needed to be made if Boitano was to become the Olympic champion. Boitano had always been good at the technical requirements ("The first mark"), but he was weak on the artistic ("the second mark"). He was a self-described "jumping robot." In order to help his growth as an artist, he hired choreographer Sandra Bezic to choreograph his programs for the 1987–1988 Olympic season.
Bezic choreographed two programs that featured clean lines and accentuated the skating abilities of the 5' 11" Boitano. The short program was based on Giacomo Meyerbeer's ballet "Les Patineurs" in which Boitano plays a cocky young man showing off his tricks, using movements dating back to the 19th century. In one famous moment, Boitano wipes ice shavings, also called snow, off his skate blade and tosses it over his shoulder after landing a triple axel combination. The free skating program was based on the film score, "Napoleon", detailing various phases of a soldier's life.
Boitano debuted his new programs at 1987 Skate Canada, held in the Saddledome in Calgary, Alberta, Canada, the same venue in which he would compete against Brian Orser for the Olympic title three months later. Boitano's new programs were received with standing ovations by the audience. Although Orser won the competition, Boitano skated clean, landing seven triple jumps, including a footwork section into a jump. Boitano, Leaver, and Bezic were so confident about the strength of Boitano's new programs that they omitted the quadruple toe loop, which if landed, could have put him a shoulder above Orser in technical merit.
The short program at the 1988 United States Figure Skating Championships proved to be a highlight. Boitano received marks of 6.0 from eight of the nine judges for presentation, the second mark. His free skate was flawed. Due to delays, he did not skate until after midnight. Still, Boitano won the competition, and went into the Olympics as the national champion (U.S.), as did Orser (Canadian).
1988 Olympics: Battle of the Brians.
Going into the Olympics, Boitano and Brian Orser each had won a World title and each had excellent, balanced repertoire, with Boitano being known as the slightly better technician and Orser as the better artist. Adding to the rivalry, Boitano and Orser were both performing military-themed programs. Boitano's was to the music of Napoleon.
The Battle of the Brians at the 1988 Winter Olympics was the highlight of Boitano's amateur career. Boitano and Orser were effectively tied going into the free skating portion of the event and whoever won that portion would win the event. Alexander Fadeev had won the compulsory figures section of the competition with Boitano second and Orser third. In the short program, Orser placed first and Boitano second. The free skating was, at the time, worth 50% of the score, and so Boitano's lead would not be enough to hold him in first place if he lost the free skate.
Boitano skated a clean, technically excellent long program, with eight triple jumps, two Axels, and a triple-triple combination. Orser made one small mistake on a jump and omitted his planned second triple axel. Boitano won the Battle in a 5–4 split. Boitano won the gold medal, wearing skates with American flag appliqués that are now part of the collections of the National Museum of American History at the Smithsonian Institution.
Following the Olympics, both Orser and Boitano went to the World Championships, which Boitano won. Boitano turned professional soon after.
Professional career and return to amateur standing.
Following the Olympics, Boitano went on to dominate competitions in the professional ranks, winning 10 straight professional competitions, including 5 consecutive World Professional Championship titles and 4 consecutive wins at the Challenge of Champions. Boitano also appeared in "Carmen on Ice", for which he won an Emmy. However, Boitano wanted to return to amateur competition and make another run at the Olympics.
Boitano's lobby proved successful and in June 1993, the International Skating Union (ISU) introduced a clause, commonly known as the "Boitano rule," which allowed professionals to reinstate as "amateur" or "eligible" skaters. This had been the result of Boitano's active involvement during the early 1990s, which saw professionals being allowed in the Olympic Games in the sports of tennis and basketball. Boitano reinstated as an amateur to compete in the 1994 Winter Olympics in Lillehammer, Norway.
Boitano competed at the 1994 United States Figure Skating Championships, led after the short program, but lost to Scott Davis in the long program in a 6–3 split decision. Boitano was named to the Olympic team. Going into the Olympics as a medal favorite in a strong field, Boitano missed his triple axel combination during the short program for the first time in his career. This mistake proved extremely costly, and knocked Boitano out of medal contention. He skated a good long program and finished 6th.
Boitano returned to the professional ranks afterwards. He was inducted into the World Figure Skating Hall of Fame and the United States Figure Skating Hall of Fame in 1996.
Personal life.
In December 2013, Boitano was named to the United States delegation to the 2014 Winter Olympics in Sochi, Russia. In conjunction with that appointment, Boitano publicly came out as gay. The Sochi games and Russia have been the targets of criticism and LGBT activism because of a Russian anti-gay "propaganda" law passed in June 2013. In January 2014, he told "Associated Press" that he had never wanted to come out until the delegation announcement.
Celebrity and popular culture career.
"South Park" song.
A caricature of Boitano as a superhero appears as a semi-recurring character in the cartoon series "South Park". The movie "" (1999), features a musical number titled "What Would Brian Boitano Do?". The name of the song is a parody of the popular evangelical Christian slogan What Would Jesus Do?.
Food Network show.
On August 23, 2009, Food Network debuted a new series entitled "What Would Brian Boitano Make?", which borrows both its name and opening musical theme from the "" song. The show features Boitano preparing meals for his friends. The series was picked up for a ten-episode second season.
Other television and film appearances.
He hosted a series on HGTV, called "The Brian Boitano Project", which premiered January 16, 2014, in which he purchased a near derelict ancestral home in Northern Italy, home to many Boitanos. During the series he gives the home in Favale di Malvaro a sympathetic restoration/renovation and shops flea markets with two nieces to find decor and furnishings. Local artisans, carpenters, masons and painters create a gem where he can live part-time and host Boitanos from afar.

</doc>
<doc id="4633" url="https://en.wikipedia.org/wiki?curid=4633" title="List of political scandals in the United Kingdom">
List of political scandals in the United Kingdom

Political scandals in the United Kingdom are commonly referred to by the press and commentators as "'sleaze".
Sleaze.
A number of political scandals in the 1980s and 1990s created the impression of what was described in the British press as "sleaze": a perception that the then Conservative government was associated with political corruption and hypocrisy. This was revived in the late 1990s due to accounts of so-called "sleaze" by the Labour government.

</doc>
<doc id="4635" url="https://en.wikipedia.org/wiki?curid=4635" title="Bombardier Inc.">
Bombardier Inc.

Bombardier Inc. () is a Canadian multinational aerospace and transportation company, founded by Joseph-Armand Bombardier as L'Auto-Neige Bombardier Limitée (loosely translated to "Bombardier Snow Car Limited") on January 29, 1942, at Valcourt in the Eastern Townships, Quebec. Starting as a maker of snow machines or snowmobiles, over the years it has grown into a large manufacturer of regional airliners, business jets, mass transportation equipment, recreational equipment and a provider of financial services. Bombardier is a Fortune Global 500 conglomerate company. Its headquarters are in Montreal, Quebec, Canada. Bombardier Inc. Corporate Headquarters are at 800 René-Lévesque Boulevard West, Montréal, Québec, Canada H3B 1Y8.
History.
Joseph-Armand Bombardier was a mechanic who dreamed of building a vehicle that could "float on snow". In 1937 he designed and produced his first snowmobile in his small repair shop in Valcourt, Quebec.
Bombardier's technological breakthrough in the design of bush vehicles came in the mid-1930s when he developed a drive system that revolutionized travel in snow and swampy conditions. In 1937 Bombardier sold 12 snowmobiles, named the B7 and, in 1942, created l'Auto-Neige Bombardier Limitée company.
The first snowmobiles were large, multipassenger vehicles designed to help people get around during the long winter months. Snowmobiles were used in rural Quebec to take children to school, carry freight, deliver mail, and as ambulances. His invention filled a very particular need in the region and soon business was booming. In 1941 Armand opened a new factory in Valcourt. Then a major setback hit the growing business: the Second World War was well underway and the Canadian government issued wartime rationing regulations. Bombardier customers had to prove that snowmobiles were essential to their livelihood in order to buy one. To keep his business going, Armand shifted his focus and developed vehicles for the military. After the war, Bombardier experienced another setback in his snowmobile business. In 1948, the Quebec government passed a law requiring all highways and local roads to be cleared of snow; the Bombardier company's sales fell by nearly half in one year. Armand Bombardier therefore decided to diversify his business, first by producing tracked snowplows sized specifically for use on municipal sidewalks (replacing horse-drawn vehicles), then by making all-terrain vehicles for the mining, oil, and forestry industries.
Of note, the machines had removable front skis that could be replaced with front wheels for use on paved or hard surfaces, thus providing greater utility to his large snowmobiles. Production of these machines evolved over time. During 1951, the wooden bodies were replaced with sheet steel and these vehicles were powered by Chrysler flathead six-cylinder engines and 3-speed manual transmissions. In the 1960s, V-8 engines began to appear and during the 1969/1970 production years, the standard round "porthole"-style windows were replaced with larger rectangular windows which allowed more interior light and made them feel less claustrophobic. Following these changes came the switchover to more reliable Chrysler Industrial 318 engines with the automatic Loadflite transmissions. Production of these machines continued into the mid-1970s.
Bombardier wanted to develop a fast, lightweight snowmobile that could carry one or two people. In the early 1950s, Armand set aside his dream to focus on developing his company's other tracked vehicles, but by the end of the decade, smaller, more efficient engines had been developed and were starting to come onto the market. Bombardier resumed his efforts to build a "miniature" snowmobile. He worked alongside his eldest son Germain, who shared his father's mechanical talents. Armand and Germain developed several prototypes of the lightweight snowmobile and finally, the first Bombardier snowmobile went on sale in 1959.
The Ski-Doo snowmobile was originally called the "Ski-Dog" because Bombardier meant it to be a practical vehicle to replace the dogsled for hunters and trappers. By an accident, a painter misinterpreted the name and painted "Ski-Doo" on the first prototype. The public soon discovered that speedy vehicles that could zoom over snow were a lot of fun. Suddenly a new winter sport was born, centred in Quebec. In the first year, Bombardier sold 225 Ski-Doos; four years later, 8,210 were sold. But Armand was reluctant to focus too much on the Ski-Doo and move resources away from his all-terrain vehicles. He vividly remembered his earlier business setbacks that forced him to diversify. Armand slowed down promotion of the Ski-Doo line to prevent it from dominating the other company products, while still allowing him to dominate the snowmobile industry. The snowmobiles produced were of exceptional quality and performance, earning a better reputation than the rival Polaris and Arctic Cat brand of motosleds. In 1975 Bombardier completed the purchase of the Moto-Ski company.
On February 18, 1964, J. Armand Bombardier died of cancer at age 56. He left behind a thriving business, but also one that had been focused on one person. Armand dominated his company, overseeing all areas of operation. He controlled the small research department, making all the drawings himself. By the time of his death sales of the company had reached C$20 million, which is the equivalent of C$160 million in 2004 dollars. The younger generation took over, led by Armand's sons and sons-in-law. The young team reorganized and decentralized the company, adopting modern business tactics. The company adopted the latest technological innovation—the computer—to handle inventory, accounts, and billing. Distribution networks were improved and increased, and an incentive program was developed for sales staff.
In 1967 L'Auto-Neige Bombardier Limitée was renamed Bombardier Limited and on January 23, 1969, the company went public, listing on the Montreal and Toronto stock exchanges.
Aerospace.
In 1986, Bombardier acquired Canadair after the Canadian government-owned aircraft manufacturing company recorded the largest corporate loss in Canadian history. Shortly thereafter, de Havilland Canada, Short Brothers and Learjet operations were absorbed by the aerospace arm, which now accounts for over half of company revenue. Bombardier's most popular aircraft currently include its Dash 8, CRJ100/200/440, and CRJ700/900/1000 lines of regional airliners. Bombardier also manufactures the CL-415 amphibious water-bomber, the Global Express and the Challenger business jet. Learjet continues to operate as a subsidiary of Bombardier, manufacturing jets under the Learjet marque. The slogan was changed in 2012 from "We Move People" to "Evolution of Mobility."
Railway technology.
In 1970 Bombardier acquired the Austrian company Lohner-Rotax, a manufacturer of snowmobile engines and tramways, and became involved with rail business.
This section started to gain importance in the mid-1990s in the renaissance of tramways or "light-rail transit." Bombardier acquired the assets and designs of Budd Company, Pullman Company, and American Locomotive Company/Montreal Locomotive Works, the latter of which continued in the locomotive business until 1985. In the Canadian market, they also acquired Hawker Siddeley Canada’s Thunder Bay facilities and UTDC (formerly of Kingston), and in Mexico the company acquired Concarril.
In 2001 Bombardier acquired Adtranz (DaimlerChrysler Rail Systems), a manufacturer of trains which were widely used throughout Germany and Great Britain. Bombardier was one of the companies that took over British Rail's Research and Development facilities after privatisation (the remainder largely being absorbed into AEA Technology and Alstom).
With the acquisition of Adtranz from DaimlerChrysler, Bombardier Transportation emerged as one of the largest manufacturers of railway rolling stock in the world.
Financial services.
Bombardier Capital (BC) was the Bombardier division in charge of financial services. From 1973, when it was based in Colchester, Vermont, it offered financial services such as lending, leasing, and asset-management throughout the Americas. In 2001, BC restricted its loan activity to existing customers. The company, which began transitioning some services to Jacksonville, Florida in 1997, ceased taking on new consumer loans in 2001, focusing instead on loans to retailers and gradually downsizing. In November 2004, Bombardier's credit evaluation was downgraded by Moody's from "moderate credit risk" (Baa3) to "questionable credit quality" (Ba2), a below investment grade rating which impacted Bombardier Capital, although the company's transportation division was unaffected. In 2005, Bombardier sold the Inventory Finance Division of BC to GE Commercial Finance.
Divestitures.
Military.
Bombardier used to be a major Canadian defence contractor. With the latest restructuring, the company sold off nearly all of its military-related work in Canada. Military Aviation Services was sold to SPAR Aerospace and land-based defence products made by Urban Transportation Development Corporation ceased operations as Bombardier moved away from non-aviation defence products.
Recreational products.
On 27 August 2003, Bombardier, Inc. announced the sale of its BRP (Recreational Products) unit to a group of investors: Bain Capital (50%), Bombardier Family (35%) and Caisse de dépôt et placement du Québec (15%) for $875 million.
As part of the deal, BRP retained rights to the sprocket logo, which it subsequently modified. Its snowcats and snowmobiles dated back to the origins of the company; current brands are Ski-Doo and Lynx. Bombardier Recreational Products has also become well known for its Sea-Doo personal watercraft division which also features jet-powered sport boats. Bombardier also makes all terrain vehicles (ATVs). In 2006, the Bombardier ATV was rebranded as Can-Am. Can-Am was the name of the line of dirt bikes it produced in the 1980s which used high-tech engines featuring Rotax Automatic Variable Exhaust (RAVE) valving to create peak power at a wider range of RPMs. The bikes were shelved but technology tweaks re-emerged in the company's Ski-Doo snowmobiles (beginning with the 1989 model year Ski-Doo Mach 1). Many of today's snowmobiles produced by the company feature proprietary engineering by BRP's Rotax brand engine production plant located in Austria. BRP's Can-Am product is among the high tech recreational vehicles which include the Can-Am Spyder, a three-wheel roadster with a rear-drive wheel and featuring a vehicle stability system (VSS), anti-lock braking system (ABS) and other safety and electronic vehicle control enhancements as certified by the National Highway Traffic Safety Administration (NHTSA).
Public transport bus.
In the late 1970s in Ireland, Córas Iompair Éireann (now Bus Éireann and Dublin Bus) commissioned a range of single and double decker buses to be designed and produced. The prototypes were devised in Germany and production was carried out in Shannon, Co. Clare, Ireland. A total of 51 express coaches (KE type) and 366 double deck buses (KD) were assembled at this facility between 1980 and 1983. They remained in service until 1997 and 2000 respectively. Some surviving examples are now exhibits at the National Transport Museum of Ireland in Howth, Co. Dublin.
Bombardier Museums.
The Bombardier Museum is a large modern museum in Valcourt, Quebec dedicated to the life of Joseph-Armand Bombardier, the snowmobile, and the industry he helped create. Opened in 1971, with substantial renovations in 1990, the museum is professionally curated and features a wide array of Ski-Doos, other industrial designs, and a selection of related books, booklets and other items of interest to enthusiasts.
Also of note at the museum is the original garage "factory" that was the genesis of the Bombardier organization. The garage was carefully removed from its original location in Valcourt and moved to its present site at the museum, which is located blocks away from the present-day Bombardier Recreational Products factory.

</doc>
<doc id="4636" url="https://en.wikipedia.org/wiki?curid=4636" title="Break key">
Break key

The Break key of a computer keyboard refers to breaking a telegraph circuit, and originated with 19th century practice. In modern usage, the key has no well-defined purpose, but while this is the case it can be used by software for miscellaneous tasks, such as to switch between multiple login sessions, to terminate a program, or to interrupt a modem connection.
Because the break function is usually combined with the pause function on one key since the introduction of the IBM Model M 101-key keyboard in 1985, the Break key is also called the Pause key. It can be used to pause some games, such as "Deus Ex" and the "Call Of Duty" series.
History.
A standard telegraph circuit connects all the keys, sounders and batteries in a single series loop. Thus the sounders actuate only when both keys are down (closed, also known as "marking" — after the ink marks made on paper tape by early printing telegraphs). So the receiving operator has to hold his key down, or close a built-in shorting switch, in order to let the other operator send. As a consequence the receiving operator could interrupt the sending operator by opening his key, breaking the circuit and forcing it into a "spacing" condition. Both sounders stop responding to the sender's keying, alerting the sender. (A physical break in the telegraph line would have the same effect.)
Teleprinter.
The teleprinter operated in a very similar fashion except that the sending station kept the loop closed (logic 1 or "marking") even during short pauses between characters. Holding down a special "break" key opened the loop, forcing it into a continuous logic 0 or "spacing" condition. When this occurred, the teleprinter mechanisms continually actuated without printing anything as the all-0's character is the non-printing "NUL" in both Baudot and ASCII. The resulting noise got the sending operator's attention.
This practice carried over to teleprinter use on time-sharing computers. A continuous spacing (logical 0) condition violates the rule that every valid character has to end with one or more logic 1 (marking) "stop" bits. The computer (specifically the UART) recognized this as a special "break" condition and generated an interrupt that typically stopped a running program or forced the operating system to prompt for a login. Although asynchronous serial telegraphy is now rare, the key once used with terminal emulators can still be used by software for similar purposes.
Sinclair.
On the Sinclair ZX80 and ZX81 computers, the Break is accessed by pressing Space. On the Sinclair ZX Spectrum it is accessed by . The Spectrum+ and later computers have a dedicated key. It does not trigger an interrupt but will halt any running BASIC program, or terminate the loading or saving of data to cassette tape. An interrupted BASIC program can usually be resumed with the codice_1 command. The Sinclair QL computer, without a key, maps the function to .
BBC Micro.
On a BBC Micro computer, the key generates an interrupt which would normally cause a warm restart of the computer. A cold restart is triggered by typing . If a DFS ROM is installed, will cause the computer to search for and load a file called codice_2 on the floppy disk in drive 0. The latter two behaviours were inherited by the successor to Acorn MOS, RISC OS.
Modern keyboards.
On many modern PCs, "Pause" interrupts screen output by BIOS until another key is pressed. This is effective during boot in text mode and in a DOS box in Windows safe mode with 50 lines. On early keyboards without a key (before the introduction of 101/102-key keyboards) the Pause function was assigned to , and the Break function to ; these key-combinations still work with most programs, even on modern PCs with modern keyboards. Pressing the dedicated key on 101/102-key keyboards sends the same scancodes as pressing , then , then releasing them in the reverse order would do; additionally, an E1hex prefix is sent which enables 101/102-key aware software to discern the two situations, while older software usually just ignores the prefix. The key is different from all other keys in that it sends no scancodes at all on release; therefore it is not possible for any software to determine whether this key is being held down.
On modern keyboards, the key is usually labeled "Pause" with "Break" below, sometimes separated by a line, or "Pause" on the top of the keycap and "Break" on the front. In most Windows environments, the key combination brings up the system properties.
Keyboards without Break key.
Compact and notebook keyboards often do not have a dedicated key. These may use the following substitutes for : 
Substitutes for : 
Apple keyboards do not have the Pause/Break key, as Mac OS X does not use it. The key may be substituted by 
Usage for breaking the program's execution.
While both and combination are commonly implemented as a way of breaking the execution of a console application, they are also used for similar effect in integrated development environments. Although these two are often considered interchangeable, compilers and execution environments usually assign different signals to these. Additionally, in some kernels (e.g. miscellaneous DOS variants) is detected only at the time OS tries reading from a keyboard buffer and only if it's the only key sequence in the buffer, while is often translated instantly (e.g. by INT 1Bh under DOS). Because of this, is usually a more effective choice under these operating systems; sensitivity for these two combinations can be enhanced by the codice_3 CONFIG.SYS statement.

</doc>
<doc id="4640" url="https://en.wikipedia.org/wiki?curid=4640" title="Bogie">
Bogie

A bogie ( ) (in some senses called a truck in American English) is a chassis or framework carrying wheels, attached to a vehicle, thus serving as a modular subassembly of wheels and axles. Bogies take various forms in various modes of transport. A bogie may remain normally attached (as on a railway carriage or locomotive, or on a semi-trailer) or be quickly detachable (as the dolly in a road train); it may contain a suspension within it (as most rail and trucking bogies do), or be solid and in turn be suspended (as most bogies of tracked vehicles are); it may be mounted on a swivel, as traditionally on a railway carriage or locomotive, additionally jointed and sprung (as in the landing gear of an airliner), or held in place by other means (centreless bogies).
While "bogie" is the preferred spelling and first-listed variant in various dictionaries, bogey and bogy are also used.
Railway.
A "bogie" in the UK, or a "railroad truck", "wheel truck", or simply "truck" in North America, is a structure underneath a train to which axles (and, hence, wheels) are attached through bearings. In Indian English, bogie may also refer to an entire railway carriage.
The first standard gauge British railway to build coaches with bogies, instead of rigidly mounted axles, was the Midland Railway in 1874.
Bogies serve a number of purposes:
Usually, two bogies are fitted to each carriage, wagon or locomotive, one at each end. An alternate configuration often is used in articulated vehicles, which places the bogies (often jacobs bogies) under the connection between the carriages or wagons.
Most bogies have two axles, but some cars designed for extremely heavy loads have been built with up to five axles per bogie, and single axle bogies can be found owing to their simplicity. Heavy-duty cars may have more than two bogies using span bolsters to equalize the load and connect the bogies to the cars.
Usually, the train floor is at a level above the bogies, but the floor of the car may be lower between bogies, such as for a double decker train to increase interior space while staying within height restrictions, or in easy-access, stepless-entry, low-floor trains.
Key components of a bogie include:
The connections of the bogie with the rail vehicle allow a certain degree of rotational movement around a vertical axis pivot (bolster), with side bearers preventing excessive movement. More modern, bolsterless bogie designs omit these features, instead taking advantage of the sideways movement of the suspension to permit rotational movement.
Examples.
BR1 bogie.
The British Railways Mark 1 coach brought into production in 1950 used the BR1 bogie, which was rated to run at . The wheels were cast as a one-piece item in a pair with their axle. The simple design involved the bogie resting on four leaf springs (one spring per wheel), which in turn were connected to the axles. The leaf springs were designed to absorb any movement or resonance and to have a damping effect to benefit ride quality.
Each spring was connected to the outermost edge of the axle by a plain bearing contained in oil-filled axle box. The oil had to be topped up at regular maintenance times to avoid the bearing running hot and seizing.
There was also a heavy-duty version designated BR2.
Commonwealth bogie.
The Commonwealth bogie, manufactured by the English Steel Corporation under licence from the Commonwealth Steel Company in Illinois, USA. Fitted with SKF or Timken bearings, it was introduced in the late 1950s for all BR Mark 1 vehicles. It was a heavy, cast-steel design weighing about , with sealed roller bearings on the axle ends, avoiding the need to maintain axle box oil levels.
The leaf springs were replaced by coil springs (one per wheel) running vertically rather than horizontally. The advanced design gave a better ride quality than the BR1, being rated for .
The side frame of the bogie was usually of bar construction, with simple horn guides attached, allowing the axle boxes vertical movements between them. The axle boxes had a cast-steel equaliser beam or bar resting on them. The bar had two steel coil springs placed on it and the bogie frame rested on the springs. The effect was to allow the bar to act as a compensating lever between the two axles and to use both springs to soften shocks from either axle. The bogie had a conventional bolster suspension with swing links carrying a spring plank.
B4 bogie.
The B4 bogie was introduced in 1963. It was a fabricated steel design versus cast iron and was lighter than the Commonwealth, weighing in at . It also had a speed rating of .
Axle to spring connection was again with fitted roller bearings. However, now two coil springs rather than one were fitted per wheel.
Only a very small number of Mark 1 stock was fitted with the B4 bogie from new, it being used on the Mark 1 only to replace worn BR1 bogies. The British Rail Mark 2 coach, however, carried the B4 bogies from new. A heavier-duty version, the B5, was standard on Southern Region Mk1-based EMUs from the 1960s onwards. Some Mark 1 catering cars had mixed bogies—a B5 under the kitchen end, and a B4 under the seating end. Some of the B4-fitted Mark 2s, as well as many B4-fitted Mark 1 BGs were allowed to run at with extra maintenance, particularly of the wheel profile, and more frequent exams.
BT10 Bogie.
The BT10 bogie was introduced on the British Rail Mark 3 coach in the 1970s. Each wheel is separately connected to the bogie by a swing-arm axle.
There is dual suspension:
Locomotives.
Diesel and electric.
Most diesel locomotives and electric locomotives are carried on bogies. Those used in the USA include AAR type A switcher truck, Blomberg B, HT-C truck and Flexicoil.
Steam.
On a steam locomotive, the leading and trailing wheels may be mounted on bogies like pony trucks or Bissel bogies. Articulated locomotives (e.g. Fairlie, Garratt or Mallet locomotives) have power bogies similar to those on diesel and electric locomotives.
Tramway.
Modern.
Tram bogies are much simpler in design because of their axle load, and the tighter curves found on tramways mean tram bogies almost never have more than two axles. Furthermore, some tramways have steeper gradients and vertical, as well as horizontal, curves, which means tram bogies often need to pivot on the horizontal axis, as well.
Some articulated trams have bogies located under articulations, a setup referred to as a Jacobs bogie. Often, low-floor trams are fitted with nonpivoting bogies and many tramway enthusiasts see this as a retrograde step, as it leads to more wear of both track and wheels and also significantly reduces the speed at which a tram can round a curve.
Historic.
In the past, many different types of bogie (truck) have been used under tramcars (e.g. Brill, Peckham, maximum traction). A maximum traction truck has one driving axle with large wheels and one nondriving axle with smaller wheels. The bogie pivot is located off-centre, so more than half the weight rests on the driving axle.
Hybrid systems.
The retractable stadium roof on Toronto's Rogers Centre used modified off-the-shelf train bogies on a circular rail. The system was chosen for its proven reliability.
Rubber-tyred metro trains use a specialised version of railway bogies. Special flanged steel wheels are behind the rubber-tired running wheels, with additional horizontal guide wheels in front of and behind the running wheels, as well. The unusually large flanges on the steel wheels guide the bogie through standard railroad switches, and in addition keep the train from derailing in case the tires deflate.
Variable gauge axles.
To overcome breaks of gauge some bogies are being fitted with variable gauge axles (VGA) so that they can operate on two different gauges. These include the SUW 2000 system from ZNTK Poznań.
Cleminson system.
The Cleminson system is not a true bogie, but serves a similar purpose. It was based on a patent of 1883 by James Cleminson, and was once popular on narrow-gauge rolling stock, e.g. on the Isle of Man and Manx Northern Railways. The vehicle would have three axles and the outer two could pivot to adapt to curvature of the track. The pivoting was controlled by levers attached to the third (centre) axle, which could slide sideways.
Tracked vehicles.
Some tanks and other tracked vehicles have bogies as external suspension components (see armoured fighting vehicle suspension). This type of bogie usually has two or more road wheels and some type of sprung suspension to smooth the ride across rough terrain. Bogie suspensions keep much of their components on the outside of the vehicle, saving internal space. Although vulnerable to antitank fire, they can often be repaired or replaced in the field.
Articulated lorries (tractor-trailers).
In trucking, a bogie is the subassembly of axles and wheels that supports a semi-trailer, whether permanently attached to the frame (as on a single trailer) or making up the dolly that can be hitched and unhitched as needed when hitching up a second or third semi-trailer (as when pulling doubles or triples).
See also.
Articles on bogies and trucks
Related topics

</doc>
<doc id="4641" url="https://en.wikipedia.org/wiki?curid=4641" title="British Steel">
British Steel

British Steel plc was a major British steel producer. It originated from the nationalised British Steel Corporation (BSC), formed in 1967 which was privatised as a public limited company, British Steel plc in 1988. It was once a constituent of the FTSE 100 Index. The company merged with Koninklijke Hoogovens to form Corus Group in 1999.
History.
Alasdair M. Blair (1997), Professor of International Relations and Head of the Department of Politics and Public Policy at De Montfort University, has explored the history of the British Steel since the Second World War to evaluate the impact of government intervention in a market economy. He suggests that entrepreneurship was lacking in the 1940s; the government could not persuade the industry to upgrade its plants. For generations the industry had followed a piecemeal growth pattern that proved inefficient in the face of world competition. The Labour Party came to power in 1945 committed to socialism. In 1946 it put the first steel development plan into practice with the aim of increasing capacity. It passed the Iron and Steel Act 1949 which meant nationalisation of the industry as the government bought out the shareholders, and created the Iron and Steel Corporation of Great Britain. American Marshall Plan aid in 1948–50 reinforced modernisation efforts and provided funding for them. However the nationalisation was reversed by the Conservative government after 1952.
The industry was again nationalised in 1967 under another Labour government, becoming British Steel Corporation (BSC). But by then twenty years of political manipulation had left companies such as British Steel with serious problems: a complacency with existing equipment, plants operating below full capacity (hence with low efficiency), poor-quality assets, outdated technology, government price controls, higher coal and oil costs, lack of funds for capital improvement, and increasing competition in the world market.
By the 1970s the Labour government's main goal for the declining industry was to keep employment high. Since British Steel was a major employer in depressed regions, it was decided to keep many mills and facilities operating at a loss. In the 1980s Conservative Prime Minister Margaret Thatcher re-privatised BSC as British Steel. Under private control the company dramatically cut its work force and underwent a radical reorganisation and massive capital investment to again become competitive in the world marketplace.
Nationalisation.
BSC was formed from the assets of former private companies which had been nationalised, largely under the Labour government of Harold Wilson, on 28 July 1967. Wilson's was the second attempt at nationalisation, Clement Attlee's Iron and Steel Corporation of Great Britain having been largely privatised by the Conservative governments of the 1950s. Only one steel company, Richard Thomas and Baldwins, remained in public ownership throughout.
BSC was established under the Iron and Steel Act 1967, which vested in the Corporation the shares of the fourteen major UK-based steel companies then in operation, being:
At the time of its formation, BSC comprised around ninety percent of the UK's steelmaking capacity; it had around 268,500 employees and around 200 wholly or partly owned subsidiaries based in the United Kingdom, Australia, New Zealand, Canada, Africa, South Asia, and South America.
Dorman Long, South Durham and Stewarts and Lloyds had merged as British Steel and Tube Ltd before vesting took place. BSC later arranged an exchange deal with Guest, Keen and Nettlefolds Ltd (GKN), the parent company of GKN Steel, under which BSC acquired Dowlais Ironworks at Merthyr Tydfil and GKN took over BSC's Brymbo Steelworks near Wrexham.
Restructuring.
According to Blair (1997) British Steel faced serious problems at the time of its formation, including obsolescent plants; plants operating under capacity and thus at low efficiency; outdated technology; price controls that reduced marketing flexibility; soaring coal and oil costs; lack of capital investment funds; and increasing competition on the world market. By the 1970s the government adopted a policy of keeping employment artificially high in the declining industry. This especially impacted BSC since it was a major employer in a number of depressed regions.
One of the arguments aired in favour of nationalisation was that it would enable steel production to be rationalised. This involved concentrating investment on major integrated plants, placed near the coast for ease of access by sea, and closing older, smaller plants, especially those that had been located inland for proximity to coal supplies.
From the mid-1970s the (now loss-making) British Steel pursued a strategy of concentrating steelmaking in five areas: South Wales, South Yorkshire, Scunthorpe, Teesside and Scotland. This policy continued following the Conservative victory in the 1979 general election. Other traditional steelmaking areas faced cutbacks. Under the Labour government of James Callaghan, a review by Lord Beswick had led to the reprieve of the so-called 'Beswick plants', for social reasons, but subsequent governments were obliged under EU rules to withdraw subsidies. Major changes resulted across Europe, including in the UK:
Privatisation.
British Steel was privatised in 1988 by the Conservative government of Margaret Thatcher. It merged with the Dutch steel producer "Koninklijke Hoogovens" to form Corus Group on 6 October 1999. Corus itself was taken over in March 2007 by the Indian steel operator Tata Steel.
British Steel is the planned future rebrand of steelworks in Scunthorpe, Lincolnshire by investment firm Greybull Capital which has acquired the site from current owners Tata.
Chairmen.
Ian MacGregor later became famous for his role as Chairman of the National Coal Board during the UK miners' strike (1984–1985). During the strike the "Battle of Orgreave" took place at British Steel's coking plant.
Sponsorships.
In 1971 British Steel sponsored Sir Chay Blyth in his record-making non-stop circumnavigation against the winds and currents, known as 'The Impossible Voyage'. In 1992 they sponsored the British Steel Challenge, the first of a series of 'wrong way' races for amateur crews.
British Steel had agreed a sponsorship deal with Middlesbrough Football Club during the 1994–95 season, with a view to British Steel-sponsored Middlesbrough shirts making their appearance the following season. But the sponsorship deal was terminated before it commenced after it was revealed that British steel only made up a tiny fraction of steel used in construction of the stadium, and that the bulk of the steel had been imported from Germany.

</doc>
<doc id="4642" url="https://en.wikipedia.org/wiki?curid=4642" title="BT Group">
BT Group

BT Group plc (trading as BT) is a holding company which owns British Telecommunications plc, a British multinational telecommunications services company with head offices in London, United Kingdom. It has operations in around 170 countries.
BT's origins date back to the founding of the Electric Telegraph Company in 1846 which developed a nationwide communications network. In 1912, the General Post Office, a government department, became the monopoly telecoms supplier in GB. The Post Office Act of 1969 led to the GPO becoming a public corporation. British Telecommunications, trading as "British Telecom", was formed in 1980, and became independent of the Post Office in 1981. British Telecommunications was privatised in 1984, becoming "British Telecommunications plc", with some 50 percent of its shares sold to investors. The Government sold its remaining stake in further share sales in 1991 and 1993. BT has a primary listing on the London Stock Exchange, a secondary listing on the New York Stock Exchange, and is a constituent of the FTSE 100 Index.
BT controls a number of large subsidiaries. BT Global Services division supplies telecoms services to corporate and government customers worldwide, and its BT Consumer division supplies telephony, broadband, and subscription television services in Great Britain to around 18 million customers. BT announced in February 2015 that it had agreed to acquire EE for £12.5 billion, and received final regulatory approval from the Competition and Markets Authority on 15 January 2016. The transaction was completed on 29 January 2016.
Most of BT's profits are generated by its Openreach subsidiary, which controls the UK's 'last mile' copper infrastructure. Since 2005, BT have been accused of abusing their control of Openreach, particularly by underinvesting in the UK's broadband infrastructure, charging high prices and providing poor customer service. 
History.
BT's origins date back to the establishment of the first telecommunications companies in Britain. Among them was the first commercial telegraph service, the Electric Telegraph Company, established in 1846. As these companies amalgamated and were taken over or collapsed, the remaining companies were transferred to state control under the Post Office in 1912. These companies were merged and rebranded as British Telecom.
1878 to 1969.
In January 1878 Alexander Graham Bell demonstrated his recently developed telephone to Queen Victoria at Osborne House on the Isle of Wight. A few days later the first telephone in Britain was installed, under licence from the General Post Office, by engineers from David Moseley and Sons, to connect the Dantzic Street premises of Manchester hardware merchant, Mr. John Hudson, with his other premises in nearby Shudehill. As the number of installed telephones across the country grew it became sensible to consider constructing telephone exchanges to allow all the telephones in each city to be connected together. The first exchange was opened in London in August 1879, closely followed by the Lancashire Telephonic Exchange in Manchester. From 1878, the telephone service in Britain was provided by private sector companies such as the National Telephone Company, and later by the General Post Office. In 1896, the National Telephone Company was taken over by the General Post Office. In 1912 it became the primary supplier of telecommunications services, after the Post Office took over the private sector telephone service in GB, except for a few local authority services. Those services all folded within a few years, the sole exception being Kingston upon Hull, where the telephone department became present day KCOM Group.
Public corporation.
Converting the Post Office into a nationalised industry, as opposed to a governmental department, was first discussed in 1932 by Lord Wolmer. In 1932 the Bridgeman Committee produced a report that was rejected. In 1961, more proposals were ignored. The Post Office remained a department of central government, with the Postmaster General sitting in Cabinet as a Secretary of State.
In March 1965, Tony Benn, the acting Postmaster General, wrote to Harold Wilson, the Prime Minister, proposing that studies be undertaken aimed at converting the Post Office into a nationalised industry. A committee was set up to look into the advantages and disadvantages of the proposal, and its findings were found to be favourable enough for the Government to re-establish a Steering Group on the Organisation of the Post Office. After some initial deliberations that the business should be divided into five divisions; Post, Telecommunications, Savings, Giro and National Data Processing Services, it was decided that there should be two: "Post" and "Telecommunications". These events finally resulted in the introduction of the Post Office Act, 1969.
On 1 October 1969, under the Post Office Act of 1969, the Post Office ceased to be a government department and it became established as a public corporation. The Act gave the Post Office the exclusive privilege of operating telecommunications systems with listed powers to authorise others to run such systems. Effectively, the General Post Office retained its telecommunications monopoly.
1969 to 1982.
In 1977, the Carter Committee Report recommended a further division of the two main services and for their relocation under two individual corporations. The findings contained in the report led to the renaming of Post Office Telecommunications as British Telecommunications (trading as British Telecom) in 1980, although it remained part of the Post Office.
The British Telecommunications Act 1981 transferred the responsibility for telecommunications services from the Post Office, creating two separate corporations, Post Office Ltd. and British Telecommunications. At this time the first steps were taken to introduce competition into British telecommunications industry. In particular, the Act empowered the Secretary of State for Trade and Industry, as well as British Telecommunications, to license other operators to run public telecommunications systems. Additionally, a framework was established which enabled the Secretary of State to set standards with the British Standards Institution (BSI) for apparatus supplied to the public by third parties, and had the effect of requiring British Telecommunications to connect approved apparatus to its systems. The Secretary of State made use of these new powers and began the process of opening up the apparatus supply market, where a phased programme of liberalisation was started in 1981. In 1982, a licence was granted to Cable & Wireless to run a public telecommunications network through its subsidiary, Mercury Communications Ltd.
1982 to 1991.
On 19 July 1982, the Government formally announced its intention to privatise British Telecommunications with the sale of up to 51% of the company's shares to private investors. This intention was confirmed by the passing of the Telecommunications Act 1984, which received Royal Assent on 12 April that year. The transfer to British Telecommunications plc of the business of British Telecommunications, the statutory corporation, took place on 6 August 1984 and, on 20 November 1984, more than 50 per cent of British Telecommunications shares were sold to the public. At the time, this was the largest share issue in the world.
The new legislation was intended to enable British Telecommunications to become more responsive to competition in GB and to expand its operations globally. Commercial freedom granted to British Telecommunications allowed it to enter into new joint ventures and, if it so decided, to engage in the manufacture of its own apparatus. The company's transfer into the private sector continued in December 1991 when the Government sold around half its remaining holding of 47.6% of shares, reducing its stake to 21.8%. Substantially all the government's remaining shares were sold in a third flotation in July 1993, raising £5 billion for the Treasury and introducing 750,000 new shareholders to the company.
The 1984 Act also abolished British Telecommunications's exclusive privilege of running telecommunications systems and established a framework to safeguard the workings of competition. This meant that British Telecommunications finally lost its monopoly in running telecommunications systems, which it had technically retained under the 1981 Act despite the Secretary of State's licensing powers. It now required a licence in the same way as any other telecommunications operator. The principal licence granted to British Telecommunications laid down strict and extensive conditions affecting the range of its activities, including those of manufacture and supply of apparatus.
In 1985, Cellnet was launched as a subsidiary of Telecom Securicor Cellular Radio Limited, a 60:40 venture between British Telecommunications and Securicor respectively. Securicor originally invested £4 million in Cellnet in 1983. In 1999, BT purchased Securicor's shares in Cellnet for £3.15 billion. The company was later rebranded as BT Cellnet, and became a part of BT Wireless, a group of subsidiary companies owned by BT.
The next major development for British Telecommunications, and a move towards a more open market in telecommunications, occurred in 1991. On 5 March, the Government's White Paper, "Competition and Choice: Telecommunications Policy" for the 1990s, was issued. In effect, it ended the duopoly which had been shared by British Telecommunications and Mercury Communications in the UK since November 1983 and the build-up to privatisation. The new policy enabled customers to acquire telecommunications services from competing providers using a variety of technologies. Independent "retail" companies were permitted to bulk-buy telecommunications capacity and sell it in packages to business and domestic users. The White Paper was endorsed by British Telecommunications, the new policy enabling the company to compete freely and more effectively by offering flexible pricing packages to meet the needs of different types of customer.
1991 to 2001.
On 2 April 1991, the company started using a new trading name, BT, and branding.
In June 1994, BT and MCI Communication Corporation, the second largest carrier of long distance telecommunications services in the United States, launched Concert Communications Services, a $1 billion joint venture company. This alliance gave BT and MCI a global network for providing end-to-end connectivity for advanced business services. Concert was the first company to provide a single-source broad portfolio of global communications services for multinational customers. On 3 November 1996, BT and MCI announced they had entered into a merger agreement to create a global telecommunications company called Concert plc, to be incorporated in GB, with headquarters in both London and Washington, D.C. As part of the alliance BT acquired a 20% holding in MCI. Nevertheless, following U.S. carrier WorldCom's rival bid for MCI on 1 October 1997, BT ultimately decided in November, to sell its stake in MCI to WorldCom for $7 billion. The deal with WorldCom resulted in a profit of more than $2 billion on BT's original investment in MCI, with an additional $465 million severance fee for the break-up of the proposed merger.
In December 2000, following modifications to BT’s licence in April 2000, BT offered local loop unbundling (LLU) to other telecommunications operators, enabling them to use BT’s copper local loops (the connection between the customer’s premises and the exchange) to connect directly with their customers.
2001 to 2006.
Following the dot com crash, the group undertook a board restructuring and asset sale to address its large debts. In May 2001, BT announced a three-for-ten rights issue to raise £5.9 billion—still GB's largest ever rights issue—and the sale of Yell Group, the international directories and associated e-commerce business, for £2.14 billion. Both activities were completed in June 2001. The group also sold its property portfolio to Telereal, a property company.
BT renamed its BT Wireless division as O2 in September 2001, and confirmed it planned to demerge the unit in November that year. Shareholders approved the demerger at a extraordinary general meeting held in Birmingham in October 2001, with 4.297 billion British Telecommunications shares voted in favour, and 0.67 million voted against. BT Wireless demerged in 2001, and was relaunched on 18 June 2002 as O2. O2 was acquired by Telefónica in 2007.
In April 2003, BT unveiled its current corporate identity, known as the "Connected World", and brand values. Reflecting the aspirations of a technologically innovative future, the connected world is designed to embody BT’s five corporate values: trustworthy, helpful, inspiring, straightforward, heart. The globe device part of the logo was originally designed by the Wolff Olins brand consultancy for BT's Concert joint venture with AT&T, and was subsequently used by BT's internet division, Openworld, prior to being adopted by the company as a whole.
The Communications Act, 2003, which came into force on 25 July 2003, introduced a new industry regulator, the Office of Communications (Ofcom), to replace the Office of Telecommunications (Oftel). It also introduced a new regulatory framework. The licensing regime was replaced by a general authorisation for companies to provide telecommunications services subject to general conditions of entitlement and, in some instances, specific conditions. Under a specific condition BT retained its universal service obligation (USO) for GB, excluding the Hull area. The USO included connecting consumers to the fixed telephone network, schemes for consumers with special social needs, and the provision of call box services.
In the summer of 2004, BT launched Consult 21, an industry consultation for BT’s 21st century network (21CN) programme. 21CN is a next-generation network transformation, that, at one time, was due for completion by the end of 2010. Using internet protocol technology, 21CN will replace the existing networks and communications from any device such as mobile phone, PC, PDA, or home phone, to any other device.
In 2004, BT was awarded the contract to deliver and manage N3, a secure and fast broadband network for the NHS National Programme for IT (NPfIT) program, on behalf of the English National Health Service (NHS).
In 2005 BT made a number of important acquisitions. In February 2005, BT acquired Infonet (now re-branded BT Infonet), a large telecoms company based in El Segundo, California, giving BT access to new geographies. It also acquired the second largest telecoms operator in the Italian business market, Albacom. Then in April 2005, it bought Radianz from Reuters (now rebranded as BT Radianz), which expanded BT's coverage and provided BT with more buying power in certain countries.
In August 2006, BT acquired online electrical retailer Dabs.com for £30.6 million. The BT Home Hub manufactured by Inventel was also launched in June 2006.
In October 2006, BT confirmed that it would be investing 75% of its total capital spending, put at £10 billion over five years, in its new Internet Protocol (IP) based 21st century network (21CN). Annual savings of £1 billion per annum were expected when the transition to the new network was to have been completed in 2010, with over 50% of its customers to have been transferred by 2008. (For actual progress see BT 21CN). That month the first customers on to 21CN was successfully tested at Adastral Park in Suffolk.
2007 to 2012.
In January 2007, BT acquired Sheffield-based ISP, PlusNet plc, adding 200,000 customers. BT stated that PlusNet will continue to operate separately out of its Sheffield head-office. On 1 February 2007, BT announced agreed terms to acquire International Network Services Inc. (INS), an international provider of IT consultancy and software. This increases BT presence in North America enhancing BT's consulting capabilities.
On 20 February 2007, Sir Michael Rake, then chairman of accountancy firm KPMG International, succeeded Sir Christopher Bland, who stepped down in September of that year. On 20 April 2007, BT acquired Comsat International which provides network services to the South American corporate market. On 1 October 2007, BT purchased Chesterfield based Lynx Technology which has been trading since 1973.
BT acquired Wire One Communications in June 2008 and folded the company into "BT Conferencing", its existing conferencing unit, as a new video business unit
In July 2008, BT acquired the online business directory firm Ufindus for £20 million in order to expand its position in the local information market in GB. On 28 July 2008, BT acquired Ribbit, of Mountain View, California, "Silicon Valley's First Phone Company". Ribbit provides Adobe Flash/Flex APIs, allowing web developers to incorporate telephony features into their software as a service (SaaS) applications.
In the early days of its fibre broadband rollout, BT said it would deliver fibre-to-the-premises (FTTP) to around 25% of the Country, with the rest catered for by the slower fibre-to-the-cabinet (FTTC), which uses copper wiring to deliver the final stretch of the connection. In 2014, with less than 0.7% of the company's fibre network being FTTP, BT dropped the 25% target, saying that it was "far less relevant today" because of improvements made to the headline speed of FTTC, which had doubled to 80Mbit/s since its fibre broadband rollout was first announced. To supplement FTTC, BT offered an 'FTTP on Demand' product. In January 2015, BT stopped taking orders for the on-demand product.
On 1 April 2009, BT Engage IT was created from the merger of two previous BT acquisitions, Lynx Technology and Basilica. Apart from the name change not much else changed in operations for another 12 months. On 14 May 2009, BT said it was cutting up to 15,000 jobs in the coming year after it announced its results for the year to 31 March 2009. Then in July 2009, BT offered workers a long holiday for an up front sum of 25% of their annual wage or a one-off payment of £1000 if they agree to go part-time.
On 6 April 2011, BT launched the first online not-for-profit fundraising service for UK charities called BT MyDonate as part of its investment to the community. The service will pass on 100% of all donations made through the site to the charity, and unlike other services which take a proportion as commission and charge charities for using their services, BT will only pass on credit/debit card charges for each donation. The service allows people to register to give money to charity or collect fundraising donations. BT developed MyDonate with the support of Cancer Research UK, Changing Faces, KidsOut, NSPCC and Women's Aid.
2013 to present.
In March 2013, BT was allocated 4G spectrum in the UK following an auction and assignment by Ofcom, after paying £201.5m.
On 29 July 2013, it was announced that BT had partnered up with Scottish Rugby Union in a four-year sponsorship deal with its two professional clubs; Edinburgh Rugby and Glasgow Warriors that will commence from August 2013. The deal involves BT Sport becoming the new shirt sponsor for both clubs as well as being promoted with BT Group at their respective home grounds; Scotstoun Stadium and Murrayfield Stadium.
On 1 August 2013, BT launched its first television channels, BT Sport, to compete with rival broadcaster Sky Sports. Plans for the channels' launch came about when it was announced in June 2012 that BT had been awarded a package of broadcast rights for the Premier League from the 2013–14 to 2015–16 season, broadcasting 38 matches from each season. In February 2013, BT acquired ESPN Inc.'s UK and Ireland TV channels, continuing its expansion into sports broadcasting. ESPN America and ESPN Classic were both closed, while ESPN continued to be operated by BT. On 9 November 2013, BT announced it had acquired exclusive rights to the Champions League and Europa League for £897m, from the 2015 season, with some free games remaining including both finals.
On 28 May 2014, it was announced that BT agreed a £20 million four-year sponsorship deal with Scottish Rugby Union which includes BT securing the naming rights for Murrayfield Stadium which becomes BT Murrayfield Stadium, become sponsor of the Scotland sevens team, become principal and exclusive sponsor of Scotland’s domestic league and cup competitions from next season, taking over the role from The Royal Bank of Scotland Group (RBS), and become sponsor of Scottish Rugby’s four new academies that aims to drive forward standards for young players who have aspirations to play professionally. The deal strengthens the existing partnership BT has with the Scottish Rugby Union having signed a four-year shirt sponsorship deal with its two professional clubs; Edinburgh Rugby and Glasgow Warriors in July 2013 under its BT Sport. BT also previously sponsored Scotland’s domestic league and cup competitions between 1999 and 2006.
On 24 November 2014, shares in BT rose considerably on the announcement that the company were in talks to buy back O2; while at the same time BT confirmed that it had been approached by EE to also buy that company. BT confirmed on 15 December 2014 that it had entered into exclusive talks to buy EE. BT confirmed on 5 February 2015 that it had agreed to buy EE for £12.5 billion, subject to regulatory approval. The deal will combine BT's 10 million retail customers and EE's 24.5 million direct mobile subscribers. Deutsche Telekom will own 12% of BT, while Orange S.A. will own 4%.
In March 2015, launched a 4G service as BT Mobile BT Group CEO Gavin Patterson announced that BT plans to migrate all of its customers onto the IP network by 2025, switching off the company's ISDN network.
In April 2015, it was announced that as part of BT's current £20 million four-year sponsorship deal with Scottish Rugby Union that was announced in May 2014, BT has completed its sponsorship portfolio following an additional investment of £3.6 million for the 3 years remaining of its sponsorship deal, to become the new shirt sponsor for the Scotland National Teams.
On 15 January 2016, BT received final unconditional approval by the Competition and Markets Authority to acquire EE. The deal was officially completed on 29 January 2016 with Deutsche Telekom now owning 12% of BT, while Orange S.A. own 4%.
On 1 February 2016, BT announced a new organisational structure that will take effect from April 2016 following the successful acquisition of EE. The EE brand, network and high street stores will be retained and will become a second consumer division, operating alongside BT Consumer. It will serve customers with mobile services, broadband and TV and will continue to deliver the Emergency Services Network contract which was awarded to EE in late 2015. There will be a new BT Business and Public Sector division that will have around £5bn of revenues and will serve small and large businesses as well as the public sector in the UK and Ireland. It will comprise the existing BT Business division along with EE's business division and those parts of BT Global Services that are UK focused. There will also be another new division; BT Wholesale and Ventures that will comprise the existing BT Wholesale division along with EE's MVNO business as well as some specialist businesses such as Fleet, Payphones and Directories. Gerry McQuade, currently Chief Sales and Marketing Officer, Business at EE, will be its CEO.
On 11 February 2016, BT announced they will be launching a new free service later in 2016 to divert nuisance calls within its network before they ring on customers' phones and will use huge computing power to root out 25 million unwanted calls a week. BT customers can currently purchase special phones that will allow them to block nuisance calls or pay to stop calls getting through. However, the new service will identify some of the 5 billion unwanted calls made each year before they arrive, which will then be diverted automatically to a junk voicemail box. BT customers will also be able to add numbers they don't want to hear from to the blacklist, for free.
Operations.
BT Group is a holding company; the majority of its businesses and assets are held by its wholly owned subsidiary British Telecommunications plc. BT's businesses are operated under special government regulation by the British telecoms regulator Ofcom (formerly Oftel). BT has been found to have significant market power in some markets following market reviews by Ofcom. In these markets, BT is required to comply with additional obligations such as meeting reasonable requests to supply services and not to discriminate.
BT runs the telephone exchanges, trunk network and local loop connections for the vast majority of British fixed-line telephones. Currently BT is responsible for approximately 28 million telephone lines in GB. Apart from KCOM Group, which serves Kingston upon Hull, BT is the only UK telecoms operator to have a "Universal Service Obligation," (USO) which means it must provide a fixed telephone line to any address in the UK. It is also obliged to provide public call boxes.
As well as continuing to provide service in those traditional areas in which BT has an obligation to provide services or is closely regulated, BT has expanded into more profitable products and services where there is less regulation. These are principally, broadband internet service and bespoke solutions in telecommunications and information technology.
Corporate affairs.
Headquarters.
BT Group's world headquarters and registered office is the BT Centre, a 10-storey office building at 81 Newgate Street in the City of London, opposite St. Paul's tube station.
Divisions.
BT Group is organised into the following divisions:
Openreach.
Openreach was established following the Telecommunications Strategic Review carried out by Ofcom. BT signed legally binding undertakings with Ofcom in September 2005 to help create a new regulatory framework for BT and the British telecoms industry generally. Openreach commenced operations on 11 January 2006, with 25,000 engineers previously employed by BT's Retail and Wholesale divisions. It provides provision and repair in the "last mile" of copper wire and is designed to ensure that other communications providers (CPs) have exactly the same operational conditions as parts of the BT Group.
Financial performance.
BT's financial results have been as follows:
Pension fund.
BT has the largest defined benefit pension plan of any UK public company. The trustees valued the scheme at £36.7 billion at the end of 2010; an actuarial valuation valued the deficit of the scheme at £9.043 billion as of 31 December 2008.
Following a change in the regulations governing inflation index linking, the deficit was estimated at £5.2 billion in November 2010.
Environmental record.
In 2004 the BT Group signed the world's largest renewable energy deal with npower and British Gas, and now all of their exchanges, satellite networks and offices are powered by renewable energy. BT is a member of the Corporate Leaders Group on Climate Change. They signed a letter urging the government to do more to tackle this problem. Janet Blake, head of global corporate social responsibility at BT, says that she would like to see incentives that find ways of rewarding those companies that focus on climate change by making investments in green business models.
BT has made it clear that it has an ambitious plan to reduce carbon dioxide emissions. Its strategy includes steps to reduce the company's carbon footprint as well as those of customers, suppliers and employees. BT has actually pledged to achieve an 80% reduction by the year 2016, which will require further efficiency improvements.
Controversies.
Abuse of monopoly position and underinvestment in infrastructure.
BT have been accused of abusing their control of Openreach, underinvesting in the UK's broadband infrastructure, charging high prices and providing poor customer service. Openreach's services receive hundreds of thousands of complaints on an annual basis.
Aborted Fibre-to-the-Home rollout.
In 2009, BT announced it would connect 2.5 million British homes to ultra-fast Fibre-to-the-Home ('FTTP') by 2012 and 25% of the UK. However, by the end of September 2015 only 250,000 homes were connected. BT has also stopped taking orders for its FTTP-on-demand product.
Undermining of National Security.
Between 2010 and 2012 the UK intelligence community initiated an investigation aimed at Huawei, the foreign supplier of BT's new fibre infrastructure with increasing urgency after the USA, Canada and Australia prevented the company from operating in their countries. Although BT had notified the UK government in 2003 of Huawei's interest in their £10b network upgrade contract, they did not raise the security implications as BT failed to explain that the Chinese company would have unfettered access to critical infrastructure. On 16 December 2012 David Cameron was supplied with a in-depth report indicating that the intelligence services had very grave doubts regarding Huawei, in that the UK governmental, military, business community and private citizen's privacy may be under serious threat. Subsequently, BT's Infinity program and other projects are now under urgent review.
On 7 June 2013, British lawmakers concluded that BT should never have allowed the Chinese company access to the UK's critical communications network without ministerial oversight, saying they were 'deeply shocked' that BT did not inform government that they were allowing Huawei and ZTE, both foreign entities with ties to the Chinese military unfettered access to critical national systems. Furthermore, ministers discovered that the agency with responsibility to ensure Chinese equipment and code, was threat-free was entirely staffed by Huawei employees. Subsequently, parliamentarians confirmed that in case of an attack on the UK there was nothing at this point that could done to stop Chinese infiltration attacking critical national infrastructure.
Another Chinese company ZTE supplying extensive network equipment and subscriber hardware to BT Infinity is also under scrutiny after the US, Canada, Australia and the European Union declared the company a security risk to its citizens.
World Wide Web hyperlink patent.
In 2001, BT discovered it owned a patent () which it believed gave it patent rights on the use of hyperlink technology on the World Wide Web. The corresponding UK patent had already expired, but the U.S. patent was valid until 2006. On 11 February 2002, BT began a court case relating to its claims in a U.S. federal court against the Internet service provider Prodigy Communications Corporation. In the case "British Telecommunications Plc. v. Prodigy", the United States District Court for the Southern District of New York ruled on 22 August 2002 that the BT patent was not applicable to web technology and granted Prodigy's request for summary judgment of non-infringement.
Behavioural targeting.
In early 2008 it was announced that BT had entered into a contract (along with Virgin Media and TalkTalk) with the spyware company Phorm (responsible under their 121Media guise for the Apropos rootkit) to intercept and analyse their users' click-stream data and sell the anonymised aggregate information as part of Phorm's OIX advertising service. The practice, known as "behavioural targeting" and condemned by critics as "data pimping", came under intense fire from various internet communities and other interested-parties who believe that the interception of data without the consent of users and web site owners is illegal under UK law (RIPA). At a more fundamental level, many have argued that the ISPs and Phorm have no right to sell a commodity (a user's data, and the copyright content of web sites) to which they have no claim of ownership. In response to questions about Phorm and the interception of data by the Webwise system Sir Tim Berners-Lee, credited as the creator of the World Wide Web protocol, indicated his disapproval of the concept and is quoted as saying of his data and web history:
Alleged complicity with drone strikes in Yemen and Somalia.
In September 2012, BT entered into a $23 million deal with the US military to provide a key communications cable connecting RAF Croughton, a US military base on UK soil, with Camp Lemonnier, a large US base in Djibouti. Camp Lemonnier is used as a base for American drone attacks in Yemen and Somalia, and has been described by The Economist as "the most important base for drone operations outside the war zone of Afghanistan."
Human rights groups including Reprieve and Amnesty International have criticised the use of armed drones outside declared war zones. Evidence produced by The Bureau of Investigative Journalism and Stanford University's International Human Rights & Conflict Resolution Clinic suggest that drone strikes have caused substantial civilian casualties, and may be illegal under international law.
In 2013, BT was the subject of a complaint by Reprieve to the Department of Business, Innovation and Skills under the OECD Guidelines for Multinational Enterprises, following their refusal to explain whether or not their infrastructure was used to facilitate drone strikes. The subsequent refusal of this complaint was appealed in May 2014, on the basis that the UK National Contact Point’s decision did not follow the OECD Guidelines. The issue of bias was also raised, due to the appointment of Lord Ian Livingston as government minister for the department which was processing the complaint: Livingston had occupied a senior position at BT when the cable between RAF Croughton and Camp Lemonnier was originally built.

</doc>
<doc id="4644" url="https://en.wikipedia.org/wiki?curid=4644" title="Balmoral Castle">
Balmoral Castle

Balmoral Castle is a large estate house in Royal Deeside, Aberdeenshire, Scotland. It is located near the village of Crathie, west of Ballater and east of Braemar.
Balmoral has been one of the residences for members of the British Royal Family since 1852, when the estate and its original castle were purchased privately by Prince Albert, consort to Queen Victoria. They remain as the private property of the royal family and are not the property of the Crown.
Soon after the estate was purchased by the royal family, the existing house was found to be too small and the current Balmoral Castle was commissioned. The architect was William Smith of Aberdeen, although his designs were amended by Prince Albert.
The castle is an example of Scots Baronial architecture, and is classified by Historic Scotland as a category A listed building. The new castle was completed in 1856 and the old castle demolished shortly thereafter.
The Balmoral Estate has been added to by successive members of the royal family, and now covers an area of approximately . It is a working estate, including grouse moors, forestry, and farmland, as well as managed herds of deer, Highland cattle, and ponies.
History.
King Robert II of Scotland (1316–1390) had a hunting lodge in the area. Historical records also indicate that a house at Balmoral was built by Sir William Drummond in 1390.
The estate is recorded in 1451 as "Bouchmorale", and later was tenanted by Alexander Gordon, second son of the 1st Earl of Huntly. A tower house was built on the estate by the Gordons. Tower houses often are referred to as "castles" because of their formidable construction.
In 1662 the estate passed to Charles Farquharson of Inverey, brother of John Farquharson, the "Black Colonel". The Farquharsons were Jacobite sympathisers, and James Farquharson of Balmoral was involved in both the 1715 and 1745 rebellions. He was wounded at the Battle of Falkirk in 1746.
The Farquharson estates were forfeit, and passed to the Farquharsons of Auchendryne. In 1798, James Duff, 2nd Earl Fife, acquired Balmoral and leased the castle.
Sir Robert Gordon, a younger brother of the 4th Earl of Aberdeen, acquired the lease in 1830. He made major alterations to the original castle at Balmoral, including baronial-style extensions that were designed by John Smith of Aberdeen.
Royal acquisition.
Queen Victoria and Prince Albert first visited Scotland in 1842, five years after her accession to the throne and two years after their marriage. During this first visit they stayed at Edinburgh, and at Taymouth Castle in Perthshire, the home of the Marquess of Breadalbane. They returned in 1844 to stay at Blair Castle and, in 1847, when they rented Ardverikie by Loch Laggan. During the latter trip they encountered weather that was extremely rainy, which led Sir James Clark, the queen's physician, to recommend Deeside instead, for its more healthy climate.
Sir Robert Gordon died in 1847 and his lease on Balmoral reverted to Lord Aberdeen. In February 1848 an arrangement was made—that Prince Albert would acquire the remaining part of the lease on Balmoral, together with its furniture and staff—without having seen the property first.
The royal couple arrived for their first visit on 8 September 1848. Victoria found the house "small but pretty", and recorded in her diary that: "All seemed to breathe freedom and peace, and to make one forget the world and its sad turmoils". The surrounding hilly landscape reminded them of Thuringia, Albert's homeland in Germany.
Quickly, the house was confirmed to be too small and, in 1848, John and William Smith were commissioned to design new offices, cottages, and other ancillary buildings. Improvements to the woodlands, gardens, and estate buildings also were being made, with the assistance of the landscape gardener, James Beattie, and possibly by the painter, James Giles.
Major additions to the old house were considered in 1849, but by then negotiations were under way to purchase the estate from the trustees of the deceased Earl Fife. After seeing a corrugated iron cottage at the Great Exhibition of 1851, Prince Albert ordered a pre-fabricated iron building for Balmoral from E. T. Bellhouse & Co., to serve as a temporary ballroom and dining room. It was in use by 1 October 1851, and would serve as a ballroom until 1856.
The sale was completed in June 1852, the price being £32,000, and Prince Albert formally took possession that autumn. The neighbouring estate of Birkhall was bought at the same time, and the lease on Abergeldie Castle secured as well. To mark the occasion, the "Purchase Cairn" was erected in the hills overlooking the castle, the first of many.
Construction of the new house.
The growing family of Victoria and Albert, the need for additional staff, and the quarters required for visiting friends and official visitors such as cabinet members, however, meant that extension of the existing structure would not be sufficient and that a larger house needed to be built. In early 1852, this was commissioned from William Smith. The son of John Smith (who designed the 1830 alterations of the original castle), William Smith was city architect of Aberdeen from 1852. On learning of the commission, William Burn sought an interview with the prince, apparently to complain that Smith previously had plagiarised his work, however, Burn was unsuccessful in depriving Smith of the appointment. William Smith's designs were amended by Prince Albert, who took a close interest in details such as turrets and windows.
Construction began during summer 1853, on a site some northwest of the original building that was considered to have a better vista. Another reason for consideration was, that whilst construction was ongoing, the family would still be able to use the old house. Queen Victoria laid the foundation stone on 28 September 1853, during her annual autumn visit. By the autumn of 1855, the royal apartments were ready for occupancy, although the tower was still under construction and the servants had to be lodged in the old house. By coincidence, shortly after their arrival at the estate that autumn, news circulated about the fall of Sevastopol, ending the Crimean War, resulting in wild celebrations by royals and locals alike. While visiting the estate shortly thereafter, Prince Frederick of Prussia asked for the hand of Princess Victoria.
The new house was completed in 1856, and the old castle subsequently was demolished. By autumn 1857, a new bridge across the Dee, designed by Isambard Kingdom Brunel linking Crathie and Balmoral was finished.
Balmoral Castle is built from granite quarried at Invergelder on the estate, It consists of two main blocks, each arranged around a courtyard. The southwestern block contains the main rooms, while the northeastern contains the service wings. At the southeast is an tall clock tower topped with turrets, one of which has a balustrade similar to a feature at Castle Fraser. Being similar in style to the demolished castle of the 1830s, the architecture of the new house is considered to be somewhat dated for its time when contrasted with the richer forms of Scots Baronial being developed by William Burn and others during the 1850s. As an exercise in Scots Baronial, it sometimes is described as being too ordered, pedantic, and even, Germanic—as a consequence of Prince Albert's influence on the design.
The purchase of a Scottish estate by Victoria and Albert and their adoption of a Scottish architectural style, however, was very influential for the ongoing revival of Highland culture. The royals decorated Balmoral with tartans and attended highland games at Braemar. Queen Victoria expressed an affinity for Scotland, even professing herself to be a Jacobite. Added to the work of Sir Walter Scott, this became a major factor in promoting the adoption of Highland culture by Lowland Scots. Historian Michael Lynch comments that "the Scottishness of Balmoral helped to give the monarchy a truly British dimension for the first time".
Victoria and Albert at Balmoral.
Even before the completion of the new house, the pattern of the life of the royal couple in the Highlands was soon established. Victoria took long walks of up to four hours daily and Albert spent many days hunting deer and game. In 1849, diarist Charles Greville described their life at Balmoral as resembling that of gentry rather than royalty. Victoria began a policy of commissioning artists to record Balmoral, its surroundings, and its staff. Over the years, numerous painters were employed at Balmoral, including Edwin and Charles Landseer, Carl Haag, William Wyld, William Henry Fisk, and many others. The royal couple took great interest in their staff. They established a lending library.
During the 1850s, new plantations were established near the house and exotic conifers were planted on the grounds. Prince Albert had an active role in these improvements, overseeing the design of parterres, the diversion of the main road north of the river via a new bridge, and plans for farm buildings. These buildings included a model dairy that he developed during 1861, the year of his death. The dairy was completed by Victoria. Subsequently, she also built several monuments to her husband on the estate. These include a pyramid-shaped cairn built a year after Albert's death, on top of "Craig Lurachain". A large statue of Albert with a dog and a gun by William Theed, was inaugurated on 15 October 1867, the twenty-eighth anniversary of their engagement.
Following Albert's death, Victoria spent increasing periods at Balmoral, staying for as long as four months a year during early summer and autumn. Few further changes were made to the grounds, with the exception of some alterations to mountain paths, the erection of various cairns and monuments, and the addition of some cottages ("Karim Cottage" and "Baile na Coille") built for senior staff. It was during this period that Victoria began to depend on her servant, John Brown. He was a local ghillie from Crathie, who became one of her closest companions during her long mourning.
In 1887, Balmoral Castle was the birthplace of Victoria Eugenie, a granddaughter of Queen Victoria. She was born to Princess Beatrice, the fifth daughter of Victoria and Albert. Victoria Eugenie would become the queen of Spain.
In September 1896, Victoria welcomed Emperor Nicholas II of Russia and Empress Alexandra to Balmoral. Four years later Victoria made her last visit to the estate, three months before her death on 22 January 1901.
After Victoria.
After Victoria's death, the royal family continued to use Balmoral during annual autumn visits. George V had substantial improvements made during the 1910s and 1920s, including formal gardens to the south of the castle.
During the Second World War, royal visits to Balmoral ceased. In addition, due to the enmity with Germany, "Danzig Shiel", a lodge built by Victoria in Ballochbuie was renamed "Garbh Allt Shiel" and the "King of Prussia's Fountain" was removed from the grounds.
Since the 1950s, Prince Philip has added herbaceous borders and a water garden. During the 1980s new staff buildings were built close to the castle.
Ownership.
Balmoral is a private property and, unlike the monarch's official residences, is not the property of the Crown. It originally was purchased personally by Prince Albert, rather than the queen, meaning that no revenues from the estate go to Parliament or to the public purse, as otherwise in accord with the Civil List Act 1760 would be the case for property owned outright by the monarch.
Along with Sandringham House in Norfolk, ownership of Balmoral was inherited by Edward VIII on his accession in 1936. When he abdicated later the same year, however, he retained ownership of them. A financial settlement was devised, under which Balmoral and Sandringham were purchased by Edward's brother and successor to the Crown, George VI.
Currently, the estate is still owned outright by the monarch, but, by Trustees under Deeds of Nomination and Appointment, it is managed by a trust.
The estate.
Current extent and operation.
Balmoral Estate is within the Cairngorms National Park and is partly within the Deeside and Lochnagar National Scenic Area. The estate contains a wide variety of landscapes, from the Dee river valley to open mountains. There are seven Munros (hills in Scotland over ) within the estate, the highest being Lochnagar at . This mountain was the setting for a children's story, "The Old Man of Lochnagar", told originally by Prince Charles to his younger brothers, Andrew and Edward. The story was published in 1980, with royalties accruing to The Prince's Trust. The estate also incorporates the 7,500-acre Delnadamph Lodge estate, bought by Elizabeth II in 1978. 
The estate extends to Loch Muick in the southeast where an old boat house and the Royal Bothy (hunting lodge) now named "Glas-allt Shiel", built by Victoria, are located.
The working estate includes grouse moors, forestry, and farmland, as well as managed herds of deer, Highland cattle, and ponies. It also offers access to the public for fishing (paid) and hiking during certain seasons.
Approximately 8,000 acres of the estate are covered by trees, with almost 3,000 acres used for forestry that yields nearly 10,000 tonnes of wood per year. "Ballochbuie Forest", one of the largest remaining areas of old Caledonian pine growth in Scotland, consists of approximately 3,000 acres. It is managed with only minimal or no intervention. The principal mammal on the estate is the red deer with a population of 2,000 to 2,500 head.
The areas of Lochnagar and Ballochbuie were designated in 1998 by the Secretary of State for Scotland as Special Protection Areas (SPA) under the European Union (EU) Birds Directive.
Bird species inhabiting the moorlands include red grouse, black grouse, ptarmigan, and the capercaillie. Ballochbuie also is protected as a Special Area of Conservation by the EU Habitats Directive, as "one of the largest remaining continuous areas of native Caledonian Forest". In addition, there are four sites of special scientific interest on the estate.
The royal family employs approximately 50 full-time and 50–100 part-time staff to maintain the working estate. A malt whisky distillery located on the Balmoral Estate produces the Royal Lochnagar Single Malt whisky.
There are approximately 150 buildings on the estate, including Birkhall, formerly home to Queen Elizabeth The Queen Mother, and used now by Prince Charles and the Duchess of Cornwall for their summer holidays. Craigowan Lodge is used regularly by the family and friends of the royal family and also has been used while Balmoral Castle was being prepared for a royal visit. Six smaller buildings on the estate are let as holiday cottages.
Public access to gardens and castle grounds.
In 1931, the castle gardens were opened to the public for the first time and they now are open daily between April and the end of July, after which Queen Elizabeth arrives for her annual stay. The ballroom is the only room in the castle that may be viewed by the public.
Craigowan Lodge.
Craigowan Lodge is a seven-bedroom stone house approximately a mile from the main castle in Balmoral. More rustic than the castle, the lodge was often the home of Charles and Diana when they visited. Currently, it is used as quarters for important guests.
In the obituary of Michael Andreevich Romanoff, the highest-ranking member of the Russian imperial family at the time of his death in 2008, it was noted that his family spent most of World War II at Craigowan Lodge.
The lodge has been in the news periodically since 2005, because Queen Elizabeth often spends the first few days of her summer holiday there. During each weekend of the summer the castle is a lucrative source of income from visiting tourists. Sometimes, the Queen arrives at Balmoral before the tourist visiting season is over.
In popular culture.
Queen Elizabeth was in residence at Balmoral at the time of the death of Diana, Princess of Wales in 1997. Her private discussions with Prime Minister Tony Blair were dramatised in the Stephen Frears film, "The Queen" (2006). The 1997 film "Mrs. Brown" also was based on events at Balmoral. In both films, however, substitute locations were used: Blairquhan Castle in "The Queen"; and Duns Castle in "Mrs Brown".
Banknote illustration.
Since 1987 an illustration of the castle has been featured on the reverse side of £100 notes issued by the Royal Bank of Scotland.

</doc>
<doc id="4647" url="https://en.wikipedia.org/wiki?curid=4647" title="Breton language">
Breton language

Breton ("Brezhoneg" or (in Morbihan)) is a Celtic language spoken in Brittany (Breton: "Breizh"; ), France.
Breton is a Brythonic language brought from Great Britain to Armorica by migrating Britons during the Early Middle Ages; it is thus an Insular Celtic language and not closely related to the Gaulish language, which had been spoken in pre-Roman Gaul. Breton is most closely related to Cornish, both being Southwestern Brittonic languages. Welsh and the extinct Cumbric are the more distantly-related Brittonic languages.
The other regional language of Brittany, Gallo, is a langue d'oïl. It is a Romance language, thus ultimately descended from Latin (unlike the similarly-named ancient Celtic language Gaulish) and consequently close to French, although not mutually intelligible.
Having declined from more than 1 million speakers around 1950 to about 200,000 in the first decade of the 21st century, Breton is classified as "severely endangered" by the UNESCO Atlas of the World's Languages in Danger. However, the number of children attending bilingual classes has risen 33% between 2006 and 2012 to 14,709.
History and status.
Breton is spoken in West Brittany ("Breizh Izel": "Lower Brittany"), roughly to the west of a line linking Plouha (west of Saint Brieuc) and La Roche-Bernard (east of Vannes). It comes from a Brittonic language community (see image) that once extended from Great Britain to Armorica (present-day Brittany) and had even established a toehold in Galicia (in present-day Spain). Old Breton is attested from the 9th century. It was the language of the upper classes until the 12th century, after which it became the language of commoners in Lower Brittany. The nobility, followed by the bourgeoisie, adopted French. The written language of the Duchy of Brittany was Latin, switching to French in the 15th century. There exists a limited tradition of Breton literature. Some Old Breton vocabulary remains in the present day as philosophical and scientific terms in Modern Breton.
The French monarchy was not concerned with the minority languages of France spoken by the lower classes, and required the use of French for government business as part of its policy of national unity. During the French Revolution, the government introduced policies favouring French over the regional languages, which it pejoratively referred to as "patois". The revolutionaries assumed that reactionary and monarchist forces preferred regional languages to try to keep the peasant masses underinformed. In 1794, Bertrand Barère submitted his "report on the "patois"" to the Committee of Public Safety in which he said that "federalism and superstition speak Breton".
Since the 19th century, under the Third, Fourth and Fifth Republics, the government has attempted to stamp out minority languages, including Breton, in state schools, in an effort to build a national culture. Teachers humiliated students for using their regional languages, and such practices prevailed until the late 1960s.
In the early 21st century, due to the political centralization of France, the influence of the media, and the increasing mobility of people, only about 200,000 people can speak Breton: a dramatic decline from more than a million in 1950. The majority of today's speakers are more than 60 years old, and Breton is now classified as an endangered language.
At the beginning of the 20th century, half of the population of Lower Brittany knew only Breton; the other half were bilingual. By 1950, there were only 100,000 monolingual Bretons, and this rapid decline has continued, with likely no monolingual speakers left today. A statistical survey in 1997 found around 300,000 speakers in "Breizh izel", of whom about 190,000 were aged 60 or older. Few 15- to 19-year-olds spoke Breton.
Revival efforts.
In 1925, Professor Roparz Hemon founded the Breton-language review "Gwalarn." During its 19-year run, "Gwalarn" tried to raise the language to the level of a great international language. Its publication encouraged the creation of original literature in all genres, and proposed Breton translations of internationally recognized foreign works. In 1946, "Al Liamm" replaced "Gwalarn". Other Breton-language periodicals have been published, which established a fairly large body of literature for a minority language.
In 1977, Diwan schools were founded to teach Breton by immersion. They taught a few thousand young people from elementary school to high school. See the education section for more information.
The "Asterix" comic series has been translated into Breton. According to the comic, the Gaulish village where Asterix lives is in the Armorica peninsula, which is now Brittany. Some other popular comics have also been translated into Breton, including "The Adventures of Tintin", "Spirou", "Titeuf", "Hägar the Horrible", "Peanuts" and "Yakari".
Some original media are created in Breton. The sitcom, "Ken Tuch", is in Breton. Radio Kerne, broadcasting from Finistère, has exclusively Breton programming. Some movies ("Lancelot du Lac", "Shakespeare in Love", "Marion du Faouet", "Sezneg") and TV series ("Columbo", "Perry Mason") have also been translated and broadcast in Breton. Poets, linguists, and writers who have written in Breton, including Yann-Ber Kalloc'h, Roparz Hemon, Anjela Duval, Pêr-Jakez Helias and Youenn Gwernig, are now known internationally.
Today, Breton is the only living Celtic language that is not recognized by national government as an official or regional language. The French State refuses to change the second article of the Constitution (added in 1994), which establishes that "the language of the Republic is French." Although Breton was long the Celtic language with the highest number of speakers, it is now endangered.
The first Breton dictionary, the "Catholicon", was also the first French dictionary. Edited by Jehan Lagadec in 1464, it was a trilingual work containing Breton, French and Latin. Today bilingual dictionaries have been published for Breton and languages including English, Dutch, German, Spanish and Welsh. A new generation is determined to gain international recognition for Breton. The monolingual dictionary, "Geriadur Brezhoneg an Here" (1995), defines Breton words in Breton. The first edition contained about 10,000 words, and the second edition of 2001 contains 20,000 words.
In the early 21st century, the "Ofis ar Brezhoneg" ("Office of the Breton language") began a campaign to encourage daily use of Breton in the region by both businesses and local communes. Efforts include installing bilingual signs and posters for regional events, as well as encouraging the use of the Spilhennig to let speakers identify each other. The office also started an Internationalization and localization policy asking Google, Firefox and SPIP to develop their interfaces in Breton. In 2004, the Breton Wikipedia started, which now counts more than 50,000 articles. On March 2007, the "Ofis ar Brezhoneg" signed a tripartite agreement with Regional Council of Brittany and Microsoft for the consideration of the Breton language in Microsoft products. In October 2014, Facebook added Breton as one of its 121 languages. after three years of talks between the "Ofis" and the American giant.
Geographic distribution and dialects.
Breton is spoken mainly in Lower Brittany, but also in a more dispersed way in Upper Brittany (where Gallo is spoken alongside Breton and French), and in areas around the world that have Breton emigrants.
The four traditional dialects of Breton correspond to medieval bishoprics rather than to linguistic divisions. They are "leoneg" ("léonard", of the county of Léon), "tregerieg" ("trégorrois", of Trégor), "kerneveg" ("cornouaillais", of Cornouaille), and "gwenedeg" ("vannetais", of Vannes). "Guérandais" was spoken up to the beginning of the 20th century in the region of Guérande and Batz-sur-Mer. There are no clear boundaries between the dialects because they form a dialect continuum, varying only slightly from one village to the next. "Gwenedeg", however, is almost mutually unintelligible with most of the other dialects.
Official status.
Nation.
As noted, only French is an official language of France. Supporters of Breton and other minority languages continue to argue for their recognition, education in public schools and place in public life.
Constitution.
In July 2008, the legislature amended the French Constitution, adding article 75-1: "les langues régionales appartiennent au patrimoine de la France" (the regional languages belong to the heritage of France). This acknowledged the significance of the languages. The government has not provided official recognition, rights or funds to support use of these languages.
On 27 October 2015, the Senate rejected a draft law on ratification of the European Charter for Regional or Minority Languages driving away the assumption of Congress for the adoption of the constitutional reform which would have given the value and legitimacy to regional languages such as Breton.
Region.
Regional and departmental authorities use Breton to a very limited extent, for example in signage. Some bilingual signage has also been installed, such as street name signs in Breton towns. One station of the Rennes metro system has signs in both French and Breton.
Under the French law known as Toubon, it is illegal for commercial signage to be in Breton alone. Signs must be bilingual or French only. Since commercial signage usually has limited physical space, most businesses have signs only in French.
"Ofis Publik ar Brezhoneg", the Breton language agency, was set up in 1999 by the Brittany region to promote and develop the daily use of Breton. It created the "Ya d'ar brezhoneg" campaign, to encourage enterprises, organisations and communes to promote the use of Breton, for example by installing bilingual signage or translating their websites into Breton.
Education.
In the late 20th century, the French government considered incorporating the independent Breton-language immersion schools (called "Diwan") into the state education system. This action was blocked by the French Constitutional Council based on the 1994 amendment to the Constitution that establishes French as the language of the republic. Therefore, no other language may be used as a language of instruction in state schools. The Toubon Law implemented the amendment, asserting that French is the language of public education.
The Diwan schools were founded in Brittany in 1977 to teach Breton by immersion. They taught a few thousand young people from elementary school to high school. They have gained fame owing to their high level of results in school exams. Breton-language schools do not receive funding from the national government, though the Brittany Region may fund them.
Another teaching method is a bilingual approach by "Div Yezh" ("Two Languages") in the State schools, created in 1979. "Dihun" ("Awakening") was created in 1990 for bilingual education in the Catholic schools.
Statistics.
In 2012, 14,709 pupils (about 1.63% of all pupils in Brittany) attended Diwan, Div Yezh and Dihun schools. Their number has increased yearly. Jean-Yves Le Drian, the president of the Regional Council, had a goal of 20,000 pupils by 2010, but is encouraged by their progress.
In 2007, some 4,500 to 5,000 adults followed a Breton language course (such as evening course, correspondence, or other). The family transmission of Breton in 1999 is estimated to be 3 percent.
Growth of the percentage of pupils in bilingual education.
Percentage of pupils in bilingual education per department.
Municipalities.
The 10 communes with the highest percentage of pupils in bilingual primary education, listed with their total population.
The 10 communes of historic Brittany with the highest total population, listed with their percentages of pupils in bilingual primary education.
Other forms of education.
In addition to bilingual education (including Breton-medium education) the region has introduced the Breton language in primary education, mainly in the department of Finistère. These "initiation" sessions are generally one to three hours per week, and consist of songs and games.
Schools in secondary education ("collèges" and "lycées") offer some courses of Breton (given as either foreign language or option, such as German or Spanish). In 2010, nearly 5,000 students in Brittany were reported to be taking this option.
Phonology.
Vowels.
Vowels in Breton may be short or long. All unstressed vowels are short; stressed vowels can be short or long (vowel lengths are not noted in usual orthographies as they are implicit in the phonology of particular dialects, and not all dialects pronounce stressed vowels as long).
All vowels can also be nasalized, which is noted by appending an 'n' letter after the base vowel, or by adding a combining tilde above the vowel, or more commonly by non-ambiguously appending an 'ñ' letter after the base vowel (this depends on the orthographic variant).
Diphthongs are .
Grammar.
Verbal aspect.
As in English and the other Celtic languages, a variety of verbal constructions are available to express grammatical aspect, for example showing a distinction between progressive and habitual actions:
"Conjugated" prepositions.
As in other modern Celtic languages, Breton pronouns are fused into preceding prepositions to produce a sort of "conjugated" preposition. Below are some examples in Breton, Cornish, Welsh, Irish, Scottish Gaelic, and Manx, along with English translations.
Note that in the examples above the Goidelic languages (Irish, Scottish Gaelic and Manx) use the preposition meaning "at" to show possession, whereas the Brittonic languages use "with". The Goidelic languages, however, do use the preposition "with" to express "belong to" (Irish "is liom an leabhar", Scottish "is leam an leabhar", Manx "s'lhiams yn lioar" The book belongs to me).
Note also that the above examples of Welsh are the formal written language. The order and preposition may differ slightly in colloquial Welsh (Formal "mae car gennym", North Wales "mae gynnon ni gar", South Wales "mae car gyda ni"). 
Initial consonant mutations.
Breton has four initial consonant mutations: though modern Breton lost the nasal mutation of Welsh, it also has a "hard" mutation, in which voiced stops become voiceless, and a "mixed" mutation, which is a mixture of hard and soft mutations.
Vocabulary.
The English words "dolmen" and "menhir" have been borrowed from French, which took them from Breton. However, this is uncertain: for instance, "menhir" is "peulvan" or "maen hir" ("long stone"), "maen sav" ("straight stone") (two words: noun + adjective) in Breton. "Dolmen" is a misconstructed word (it should be "taol-vaen"). Some studies state that these words were borrowed from Cornish. "Maen hir" can be directly translated from Welsh as "long stone" (which is exactly what a "menhir" or "maen hir" is).
The French word "baragouiner" (‘to jabber in a foreign language’) is derived from Breton "bara" 'bread' and "gwin" 'wine'. The French word "goéland" (‘large seagull’) is derived from Breton "gwelan", which shares the same root as English ‘gull’ (Welsh "gwylan", Cornish "goelann").
Orthography.
The first extant Breton texts, contained in the Leyde manuscript, were written at the end of the 8th century: 50 years prior to the Strasbourg Oaths, considered to be the earliest example of French. After centuries of orthography calqued on the French model, in the 1830s Jean-François Le Gonidec created a modern phonetic system for the language.
During the early years of the 20th century, a group of writers known as "Emglev ar Skrivanerien" elaborated and reformed Le Gonidec's system. They made it more suitable as a super-dialectal representation of the dialects of Cornouaille, Leon and Trégor (known as from "Kernev", "Leon" and "Treger" in Breton). This KLT orthography was established in 1911. At the same time writers of the more divergent Vannetais dialect developed a phonetic system also based on that of Le Gonidec.
Following proposals made during the 1920s, the KLT and Vannetais orthographies were merged in 1941 to create an orthographic system to represent all four dialects. This "" ("wholly unified") orthography was significant for the inclusion of the "zh" digraph, which represents a in Vannetais and corresponds to a in the KLT dialects.
In 1955 François Falc'hun and the group Emgleo Breiz proposed a new orthography. It was designed to use a set of graphemes closer to the conventions of French. This "Orthographie Universitaire" ("University Orthography", known in Breton as "Skolveurieg") was given official recognition by the French authorities as the "official orthography of Breton in French education." It was opposed in the region and today is used only by the magazine "Brud Nevez" and the publishing house Emgléo Breiz.
Between 1971 and 1974, a new standard orthography was devised — the "etrerannyezhel" or "interdialectale". This system is based on the derivation of the words.
Today the majority of writers continue to use the "Peurunvan orthography", and it is the version taught in most Breton-language schools.
Alphabet.
Breton is written in the Latin script. "Peurunvan", the most commonly used orthography, consists of the following letters:
The circumflex, grave accent, trema and tilde appear on some letters. These diacritics are used in the following way:
See for an introduction to the Breton alphabet and pronunciation.
Differences between "Skolveurieg" and "Peurunvan".
Both orthographies use the above alphabet, although "é" is used only in Skolveurieg.
Differences between the two systems are particularly noticeable in word endings. In Peurunvan, final obstruents, which are devoiced in absolute final position and voiced in sandhi before voiced sounds, are represented by a grapheme that indicates a voiceless sound. In OU they are written as voiced but represented as voiceless before suffixes: "braz" (big), "brasoc'h" (bigger).
In addition, Peurunvan maintains the KLT convention, which distinguishes noun/adjective pairs by nouns written with a final voiced consonant and adjectives with a voiceless one. No distinction is made in pronunciation, e.g. brezhoneg "Breton language" vs. brezhonek "Breton (adj)".
Some examples of words in the different orthographies:
Pronunciation of the Breton alphabet.
Notes:
Examples.
Words and phrases in Breton.
Visitors to Brittany may encounter words and phrases (especially on signs and posters) such as the following:

</doc>
<doc id="4648" url="https://en.wikipedia.org/wiki?curid=4648" title="Broch">
Broch

A broch () is an Iron Age drystone hollow-walled structure of a type found only in Scotland. Brochs belong to the classification "complex Atlantic Roundhouse" devised by Scottish archaeologists in the 1980s. Their origin is a matter of some controversy. The theory that they were defensive military structures (an Iron Age equivalent to the castles and tower houses of medieval Scotland) is not accepted by many modern archaeologists (see the 'general references' below), while the alternative notion that they were farmhouses is dismissed by some others. Although most stand alone in the landscape, some examples exist of brochs surrounded by clusters of smaller dwellings.
Origin and definition.
The word "broch" is derived from Lowland Scots 'brough', meaning (among other things) fort. In the mid-19th century Scottish antiquaries called brochs 'burgs', after Old Norse "borg", with the same meaning. Place names in Scandinavian Scotland such as Burgawater and Burgan show that Old Norse "borg" is the older word used for these structures in the north. Brochs are often referred to as 'duns' in the west. Antiquaries began to use the spelling 'broch' in the 1870s.
A precise definition for the word has proved elusive. Brochs are the most spectacular of a complex class of roundhouse buildings found throughout Atlantic Scotland. The Shetland Amenity Trust lists about 120 sites in Shetland as candidate brochs, while the Royal Commission on the Ancient and Historical Monuments of Scotland (RCAHMS) identifies a total of 571 candidate broch sites throughout the country. Researcher Euan MacKie has proposed a much smaller total for Scotland of 104.
The origin of brochs is a subject of continuing research. Sixty years ago most archaeologists believed that brochs, usually regarded as the 'castles' of Iron Age chieftains, were built by immigrants who had been pushed northward after being displaced first by the intrusions of Belgic tribes into what is now southeast England at the end of the second century BC and later by the Roman invasion of southern Britain beginning in AD 43. Yet there is now little doubt that the hollow-walled broch tower was purely an invention in what is now Scotland; even the kinds of pottery found inside them that most resembled south British styles were local hybrid forms. The first of the modern review articles on the subject (MacKie 1965) did not, as is commonly believed, propose that brochs were built by immigrants, but rather that a hybrid culture formed from the blending of a small number of immigrants with the native population of the Hebrides produced them in the first century BC, basing them on earlier, simpler, promontory forts. This view contrasted, for example, with that of Sir Lindsay Scott, who argued, following Childe (1935), for a wholesale migration into Atlantic Scotland of people from southwest England.
MacKie's theory has fallen from favour too, mainly because starting in the 1970s there was a general move in archaeology away from 'diffusionist' explanations towards those pointing to exclusively indigenous development. Meanwhile, the increasing number – albeit still pitifully few – of radiocarbon dates for the primary use of brochs (as opposed to their later, secondary use) still suggests that most of the towers were built in the 1st centuries BC and AD. A few may be earlier, notably the one proposed for Old Scatness Broch in Shetland, where a sheep bone dating to 390–200 BC has been reported. The other broch claimed to be substantially older than the 1st century BC is Crosskirk in Caithness, but a recent review of the evidence suggests that it cannot plausibly be assigned a date earlier than the 1st centuries BC/AD
Distribution.
The distribution of brochs is centred on northern Scotland. Caithness, Sutherland and the Northern Isles have the densest concentrations, but there are also a great many examples in the west of Scotland and the Hebrides.
Although mainly concentrated in the northern Highlands and the Islands, a few examples occur in the Borders (for example Edin's Hall Broch and Bow Castle Broch); on the west coast of Dumfries and Galloway; and near Stirling. In a c.1560 sketch there appears to be a broch by the river next to Annan Castle in Dumfries and Galloway. This small group of southern brochs has never been satisfactorily explained.
Purposes.
From the 1930s to the 1960s, archaeologists like V. Gordon Childe and later John Hamilton regarded them as castles where local landowners held sway over a subject population.
The castle theory fell from favour among Scottish archaeologists in the 1980s, due to a lack of supporting archaeological evidence. These archaeologists suggested that defensibility was never a major concern in the siting of a broch, and argued that they may have been the "stately homes" of their time, objects of prestige and very visible demonstrations of superiority for important families (Armit 2003). Once again, however, there is a lack of archaeological proof for this reconstruction, and the sheer number of brochs, sometimes in places with a lack of good land, makes it problematic.
Brochs' close groupings and profusion in many areas may indeed suggest that they had a primarily defensive or even offensive function. Some of them were sited beside precipitous cliffs and were protected by large ramparts, artificial or natural: a good example is at Burland near Gulberwick in Shetland, on a clifftop and cut off from the mainland by huge ditches. Often they are at key strategic points. In Shetland they sometimes cluster on each side of narrow stretches of water: the broch of Mousa, for instance, is directly opposite another at Burraland in Sandwick. In Orkney there are more than a dozen on the facing shores of Eynhallow Sound, and many at the exits and entrances of the great harbour of Scapa Flow. In Sutherland quite a few are placed along the sides and at the mouths of deep valleys. Writing in 1956 John Stewart suggested that brochs were forts put up by a military society to scan and protect the countryside and seas.
Finally, some archaeologists consider broch sites individually, doubting that there ever was a single common purpose for which every broch was constructed. There are differences between the various areas in which brochs are found, with regard to position, dimensions and likely status. For example, the broch 'villages' which occur at a few places in Orkney have no parallel in the Western Isles.
Structures.
Generally, brochs have a single entrance with bar-holes, door-checks and lintels. There are mural cells and there is a scarcement (ledge), perhaps for timber-framed lean-to dwellings lining the inner face of the wall. Also there is a spiral staircase winding upwards between the inner and outer wall and connecting the galleries. Brochs vary from 5 to 15 metres (16–50 ft) in internal diameter, with 3 metre (10 ft) thick walls. On average, the walls only survive to a few metres in height. There are five extant examples of towers with significantly higher walls: Dun Carloway on Lewis, Dun Telve and Dun Troddan in Glenelg, Mousa in Shetland and Dun Dornaigil in Sutherland, all of whose walls exceed 6.5 m (21 ft) in height. Mousa's walls are the best preserved and are still 13 m tall; it is not clear how many brochs originally stood this high. A frequent characteristic is that the walls are galleried: with an open space between, the outer and inner wall skins are separate but tied together with linking stone slabs; these linking slabs may in some cases have served as steps to higher floors. It is normal for there to be a cell breaking off from the passage beside the door; this is known as the guard cell. It has been found in some Shetland brochs that guard cells in entrance passageways are close to large door-check stones. Though there was much argument in the past, it is now generally accepted among archaeologists that brochs were roofed, perahps with a conical timber framed roof covered with a locally sourced thatch. The evidence for this assertion is still fairly scanty, though excavations at Dun Bharabhat, Lewis, may support it. The main difficulty with this interpretation continues to be the potential source of structural timber, though bog and driftwood may have been plentiful sources.
On the islands of Orkney and Shetland there are very few cells at ground floor. Most brochs have scarcements (ledges) which may have allowed the construction of a very sturdy wooden first floor (first spotted by the antiquary George Low in Shetland in 1774), and excavations at Loch na Berie on the Isle of Lewis show signs of a further, second floor (e.g. stairs on the first floor, which head upwards). Some brochs such as Dun Dornaigil and Culswick in Shetland have unusual triangular lintels above the entrance door.
As in the case of Old Scatness in Shetland (near Jarlshof and Burroughston on Shapinsay), brochs were sometimes located close to arable land and a source of water (some have wells or natural springs rising within their central space). Sometimes, on the other hand, they were sited in wilderness areas (e.g. Levenwick and Culswick in Shetland, Castle Cole in Sutherland). Brochs are often built beside the sea (Carn Liath, Sutherland); sometimes they are on islands in lochs (e.g. Clickimin in Shetland).
About 20 Orcadian broch sites include small settlements of stone buildings surrounding the main tower. Examples include Howe, near Stromness, Gurness Broch in the north west of Mainland, Orkney, Midhowe on Rousay and Lingro near Kirkwall (destroyed in the 1980s). There are "broch village" sites in Caithness, but elsewhere they are unknown.
Most brochs are unexcavated. Those that have been properly examined show that they continued in use for many centuries, with the interiors often modified and changed, and that they underwent many phases of habitation and abandonment. The end of the broch period seems to have come around AD 200-300.
Heritage status.
Mousa, Old Scatness and Jarlshof: The Crucible of Iron Age Shetland is a combination of three broch sites in Shetland that are on the United Kingdom "Tentative List" of possible nominations for the UNESCO World Heritage Programme list of sites of outstanding cultural or natural importance to the common heritage of humankind. This list, published in July 2010, includes sites that may be nominated for inscription over the next 5–10 years.

</doc>
<doc id="4649" url="https://en.wikipedia.org/wiki?curid=4649" title="Billy Crystal">
Billy Crystal

William Edward "Billy" Crystal (born March 14, 1948) is an American actor, writer, producer, director, comedian, and television host. He gained prominence in the 1970s for playing Jodie Dallas on the ABC sitcom "Soap" and became a Hollywood film star during the late 1980s and 1990s, appearing in the critical and box office successes "When Harry Met Sally..." (1989), "City Slickers" (1991), and "Analyze This" (1999) and providing the voice of Mike Wazowski in the "Monsters, Inc." franchise.
He has hosted the Academy Awards 9 times, beginning in 1990 and most recently in 2012; second only to that of Bob Hope, who hosted 19 times.
Early life.
Crystal was born at Doctors Hospital on the Upper East Side of Manhattan, and initially raised in The Bronx. As a toddler, he moved with his family to 549 East Park Avenue in Long Beach, New York, on Long Island. He and his older brothers Joel and Richard, nicknamed Rip, were the sons of Helen (née Gabler), a housewife, and Jack Crystal, who owned and operated the Commodore Music Store, founded by Helen's father, Julius Gabler. Jack Crystal was also a jazz promoter, a producer, and an executive for an affiliated jazz record label, Commodore Records, founded by Helen's brother, musician and songwriter Milt Gabler. Crystal is Jewish (his family emigrated from Austria and Russia). The three young brothers would entertain by reprising comedy routines from the likes of Bob Newhart, Rich Little and Sid Caesar records their father would bring. Jazz artists such as Arvell Shaw, Pee Wee Russell, Eddie Condon, and Billie Holiday were often guests in the home. With the decline of Dixieland jazz circa 1963, Crystal's father lost his business, and died later that year at the age of 54 after suffering a heart attack while bowling. His mother, Helen Crystal, died in 2001.
After graduation from Long Beach High School in 1965, Crystal attended Marshall University in Huntington, West Virginia on a baseball scholarship, having learned the game from his father, who pitched for St. John's University. Crystal never played baseball at Marshall because the program was suspended during his first year. He did not return to Marshall as a sophomore, instead deciding to stay in New York to be close to his future wife. He attended Nassau Community College with Janice, and later transferred to New York University, where he was a film and television directing major. He graduated from NYU in 1970 with a BFA from its School of Fine Arts, not yet named for the Tisch family. One of his instructors was Martin Scorsese while Oliver Stone and Christopher Guest were among his classmates.
Career.
Television.
Crystal returned to New York City and performed regularly at "The Improv" and "Catch a Rising Star". In 1976, Crystal appeared on an episode of "All in the Family". He was on the dais for The Dean Martin Celebrity Roast of Muhammad Ali on February 19, 1976, where he did impressions of both Ali and sportscaster Howard Cosell. He was scheduled to appear on the first episode of "NBC Saturday Night" (later renamed "Saturday Night Live") (October 11, 1975), but his sketch was cut. He did perform on episode 17 of that first season, doing a monologue of an old jazz man capped by the line "Can you dig it? I knew that you could." Host Ron Nessen introduced him as "Bill Crystal." Crystal was a guest on the first and the last episode of "The Tonight Show with Jay Leno", which concluded February 6, 2014 after 22 seasons on the air. Crystal also made game show appearances such as "The Hollywood Squares", "All Star Secrets" and "The $20,000 Pyramid". He holds the record for getting his contestant partner to the top of the pyramid in winner's circle in the fastest time, 26 seconds.
Crystal's earliest prominent role was as Jodie Dallas on "Soap", one of the first unambiguously homosexual characters in the cast of an American television series. He continued in the role during the series' entire 1977–1981 run.
In 1982, Billy Crystal hosted his own variety show, "The Billy Crystal Comedy Hour" on NBC. When Crystal arrived to shoot the fifth episode, he learned it had been canceled after only the first two aired. After hosting "Saturday Night Live" twice on March 17, 1984 and the show's ninth season finale on May 5, he joined the regular cast for the 1984-85 season. His most famous recurring sketch was his parody of Fernando Lamas, a smarmy talk-show host whose catchphrase, "You look... mahvelous!," became a media sensation. Crystal subsequently released an album of his stand-up material titled "Mahvelous!" in 1985, as well as the single "You Look Marvelous", which peaked at No. 58 on the "Billboard" Hot 100 in the US, and No. 17 in Canada. Also in the 1980s, Crystal starred in an episode of Shelley Duvall's "Faerie Tale Theatre" as the smartest of the three little pigs.
In 1996, Crystal was the guest star of the third episode of "Muppets Tonight" and hosted three Grammy Awards Telecasts: the 29th Grammys; the 30th Grammys; and the 31st Grammys.
In 2015, Crystal co-starred alongside Josh Gad on the FX comedy series "The Comedians", which ran for just one season before being canceled.
Acting in film and hosting the Oscars.
Crystal's first film role was in Joan Rivers' 1978 film "Rabbit Test".
Crystal appeared briefly in the Rob Reiner "rockumentary" "This Is Spinal Tap" (1984) as Morty The Mime, a waiter dressed as a mime at one of Spinal Tap's parties. He shared the scene with a then-unknown, non-speaking Dana Carvey, stating famously that "Mime is money." He later starred in the action comedy "Running Scared" (1986) and was directed by Reiner again in "The Princess Bride" (1987), in a comedic supporting role as "Miracle Max." Reiner got Crystal to accept the part by saying, "How would you like to play Mel Brooks?" Reiner also allowed Crystal to ad-lib, and his parting shot, "Have fun storming the castle!" is a frequently-quoted line.
Reiner directed Crystal for a third time in the romantic comedy "When Harry Met Sally..." (1989), in which Crystal starred alongside Meg Ryan and for which he was nominated for a Golden Globe. The film has since become an iconic classic for the genre and is Crystal's most celebrated film. Crystal then starred in the award-winning buddy comedy "City Slickers" (1991), which proved very successful both commercially and critically and for which Crystal was nominated for his second Golden Globe. The film was followed by a sequel, which was less successful. In 1992, he narrated "Dr. Seuss Video Classics: Horton Hatches the Egg".
Following the significant success of these films, Crystal wrote, directed, and starred in "Mr. Saturday Night" (1992) and "Forget Paris" (1995). In the former, Crystal played a serious role in aging makeup, as an egotistical comedian who reflects back on his career, although the character was from his "SNL" days. Though some of his subsequent films were not as well received as his earlier hits, Crystal had another success alongside Robert De Niro in Harold Ramis' mobster comedy "Analyze This" (1999). More recent performances include roles in "America's Sweethearts" (2001), the sequel "Analyze That" (2002), and "Parental Guidance" (2012).
He directed the made-for-television movie "61*" (2001) based on Roger Maris's and Mickey Mantle's race to break Babe Ruth's single-season home run record in 1961. This earned Crystal an Emmy nomination for Outstanding Directing for a Miniseries, Movie or a Special.
Crystal was originally asked to voice Buzz Lightyear in "Toy Story" (1995) but turned it down, a decision he later regretted due to the popularity of the series. Crystal later went on to provide the voice of Mike Wazowski in the blockbuster Pixar film "Monsters, Inc." (2001), and reprised his voice role in the prequel, "Monsters University", which was released in June 2013. Crystal also provided the voice of Calcifer in the English version of Hayao Miyazaki's "Howl's Moving Castle" (2004).
Crystal hosted the Academy Awards broadcast in 1990–1993, 1997, 1998, 2000, 2004 and 2012. His hosting was critically praised, resulting in two Emmy wins for hosting and writing the 63rd Academy Awards and an Emmy win for writing the 64th Academy Awards. He returned recently as the host for the 2012 Oscar ceremony, after Eddie Murphy resigned from hosting. His nine times as the M.C. is second only to Bob Hope's 18 in most ceremonies hosted. At the 83rd Academy Awards ceremony in 2011, he appeared as a presenter for a digitally inserted Bob Hope and before doing so was given a standing ovation. Film critic Roger Ebert said when Crystal came onstage about two hours into the show, he got the first laughs of the broadcast. Crystal's hosting gigs have regularly included an introductory video segment in which he comedically inserts himself into scenes of that year's nominees in addition to a song following his opening monologue.
Broadway.
Crystal won the 2005 Tony Award for Best Special Theatrical Event for "700 Sundays", a two-act, one-man play, which he conceived and wrote about his parents and his childhood growing up on Long Island. He toured throughout the US with the show in 2006 and then Australia in 2007.
Following the initial success of the play, Crystal wrote the book "700 Sundays" for Warner Books, which was published on October 31, 2005. In conjunction with the book and the play that also paid tribute to his uncle, Milt Gabler, Crystal produced two CD compilations: "Billy Crystal Presents: The Milt Gabler Story", which featured his uncle's most influential recordings from Billie Holiday's "Strange Fruit" to "Rock Around the Clock" by Bill Haley & His Comets; and "Billy Remembers Billie" featuring Crystal's favorite Holiday recordings.
In the fall of 2013, he brought the show back to Broadway for a two-month run at the Imperial Theatre. HBO filmed the January 3–4, 2014 performances for a special, which debuted on their network on April 19, 2014.
Philanthropy.
In 1986, Crystal started hosting "Comic Relief" on HBO with Robin Williams and Whoopi Goldberg. Founded by Bob Zmuda, Comic Relief raises money for homeless people in the United States.
On September 6, 2005, on "The Tonight Show with Jay Leno", Crystal and Jay Leno were the first celebrities to sign a Harley-Davidson motorcycle to be auctioned off for Gulf Coast relief.
Crystal has participated in the Simon Wiesenthal Center Museum of Tolerance in Los Angeles. Portraying himself in a video, Crystal introduces museum guests to the genealogy wing of the museum.
Sports.
On March 12, 2008, Crystal signed a one-day minor league contract to play with the New York Yankees, and was invited to the team's major league spring training. He wore uniform number 60 in honor of his upcoming 60th birthday. On March 13, in a spring training game against the Pittsburgh Pirates, Crystal led off as the designated hitter. He managed to make contact, fouling a fastball up the first base line, but was eventually struck out by Pirates pitcher Paul Maholm on six pitches and was later replaced in the batting order by Johnny Damon. He was released on March 14, his 60th birthday.
Crystal's boyhood idol was Yankee Hall of Fame legend Mickey Mantle who had signed a program for him when Crystal attended a game where Mantle had hit a home run. Years later on "The Dinah Shore Show", in one of his first television appearances, Crystal met Mantle in person and had Mantle re-sign the same program. Crystal would be good friends with Mickey Mantle until Mantle's death in 1995. He and Bob Costas together wrote the eulogy Costas read at Mantle's funeral, and George Steinbrenner then invited Crystal to emcee the unveiling of Mantle's monument at Yankee Stadium. In his 2013 memoir "Still Foolin' 'Em", Crystal writes that after the ceremony, near the Yankee clubhouse, he was punched in the stomach by Joe DiMaggio, who was angry at Crystal for not having introduced him to the crowd as the "Greatest living player."
Crystal also was well known for his impressions of Yankee Hall of Famer turned broadcaster Phil Rizzuto. Rizzuto, known for his quirks calling games, did not travel to Anaheim, California in 1996 to call the game for WPIX. Instead, Crystal joined the broadcasters in the booth and pretended to be Rizzuto for a few minutes during the August 31 game.
Although a lifelong Yankee fan, he is a part-owner of the Arizona Diamondbacks, even earning a World Series ring in 2001 when the Diamondbacks beat his beloved Yankees.
In "City Slickers", Crystal wears a New York Mets baseball cap. In the 1986 film "Running Scared", his character is an avid Chicago Cubs fan, wearing a Cubs' jersey in several scenes. In the 2012 film "Parental Guidance", his character is the announcer for the Fresno Grizzlies, a Minor League Baseball team, and aspires to announce for their Major League affiliate, the San Francisco Giants.
Crystal is also a longtime Los Angeles Clippers fan.
Personal life.
Crystal and his wife Janice (née Goldfinger) married in June 1970, have two daughters, actress Jennifer and producer Lindsay, and are grandparents.
They reside in Pacific Palisades, California.

</doc>
<doc id="4650" url="https://en.wikipedia.org/wiki?curid=4650" title="Black hole">
Black hole

A black hole is a region of spacetime exhibiting such strong gravitational effects that nothing—including particles and electromagnetic radiation such as light—can escape from inside it. The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole. The boundary of the region from which no escape is possible is called the event horizon. Although crossing the event horizon has enormous effect on the fate of the object crossing it, it appears to have no locally detectable features. In many ways a black hole acts like an ideal black body, as it reflects no light. Moreover, quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation, with the same spectrum as a black body of a temperature inversely proportional to its mass. This temperature is on the order of billionths of a kelvin for black holes of stellar mass, making it essentially impossible to observe.
Objects whose gravitational fields are too strong for light to escape were first considered in the 18th century by John Michell and Pierre-Simon Laplace. The first modern solution of general relativity that would characterize a black hole was found by Karl Schwarzschild in 1916, although its interpretation as a region of space from which nothing can escape was first published by David Finkelstein in 1958. Black holes were long considered a mathematical curiosity; it was during the 1960s that theoretical work showed they were a generic prediction of general relativity. The discovery of neutron stars sparked interest in gravitationally collapsed compact objects as a possible astrophysical reality.
Black holes of stellar mass are expected to form when very massive stars collapse at the end of their life cycle. After a black hole has formed, it can continue to grow by absorbing mass from its surroundings. By absorbing other stars and merging with other black holes, supermassive black holes of millions of solar masses () may form. There is general consensus that supermassive black holes exist in the centers of most galaxies.
Despite its invisible interior, the presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. Matter that falls onto a black hole can form an external accretion disk heated by friction, forming some of the brightest objects in the universe. If there are other stars orbiting a black hole, their orbits can be used to determine the black hole's mass and location. Such observations can be used to exclude possible alternatives such as neutron stars. In this way, astronomers have identified numerous stellar black hole candidates in binary systems, and established that the radio source known as Sagittarius A*, at the core of our own Milky Way galaxy, contains a supermassive black hole of about 4.3 million solar masses.
On 11 February 2016, the LIGO collaboration announced the first observation of gravitational waves; because these waves were generated from a black hole merger it was the first ever direct detection of a binary black hole merger.
History.
The idea of a body so massive that even light could not escape was first put forward by John Michell in a letter written in 1783 to Henry Cavendish of the Royal Society:
General relativity.
In 1915, Albert Einstein developed his theory of general relativity, having earlier shown that gravity does influence light's motion. Only a few months later, Karl Schwarzschild found a solution to the Einstein field equations, which describes the gravitational field of a point mass and a spherical mass. A few months after Schwarzschild, Johannes Droste, a student of Hendrik Lorentz, independently gave the same solution for the point mass and wrote more extensively about its properties. This solution had a peculiar behaviour at what is now called the Schwarzschild radius, where it became singular, meaning that some of the terms in the Einstein equations became infinite. The nature of this surface was not quite understood at the time. In 1924, Arthur Eddington showed that the singularity disappeared after a change of coordinates (see Eddington–Finkelstein coordinates), although it took until 1933 for Georges Lemaître to realize that this meant the singularity at the Schwarzschild radius was an unphysical coordinate singularity. Arthur Eddington did however comment on the possibility of a star with mass compressed to the Schwarzschild radius in a 1926 book, noting that Einstein's theory allows us to rule out overly large densities for visible stars like Betelgeuse because "a star of 250 million km radius could not possibly have so high a density as the sun. Firstly, the force of gravitation would be so great that light would be unable to escape from it, the rays falling back to the star like a stone to the earth. Secondly, the red shift of the spectral lines would be so great that the spectrum would be shifted out of existence. Thirdly, the mass would produce so much curvature of the space-time metric that space would close up around the star, leaving us outside (i.e., nowhere)."
In 1931, Subrahmanyan Chandrasekhar calculated, using special relativity, that a non-rotating body of electron-degenerate matter above a certain limiting mass (now called the Chandrasekhar limit at ) has no stable solutions. His arguments were opposed by many of his contemporaries like Eddington and Lev Landau, who argued that some yet unknown mechanism would stop the collapse. They were partly correct: a white dwarf slightly more massive than the Chandrasekhar limit will collapse into a neutron star, which is itself stable because of the Pauli exclusion principle. But in 1939, Robert Oppenheimer and others predicted that neutron stars above approximately (the Tolman–Oppenheimer–Volkoff limit) would collapse into black holes for the reasons presented by Chandrasekhar, and concluded that no law of physics was likely to intervene and stop at least some stars from collapsing to black holes.
Oppenheimer and his co-authors interpreted the singularity at the boundary of the Schwarzschild radius as indicating that this was the boundary of a bubble in which time stopped. This is a valid point of view for external observers, but not for infalling observers. Because of this property, the collapsed stars were called "frozen stars", because an outside observer would see the surface of the star frozen in time at the instant where its collapse takes it inside the Schwarzschild radius.
Golden age.
In 1958, David Finkelstein identified the Schwarzschild surface as an event horizon, "a perfect unidirectional membrane: causal influences can cross it in only one direction". This did not strictly contradict Oppenheimer's results, but extended them to include the point of view of infalling observers. Finkelstein's solution extended the Schwarzschild solution for the future of observers falling into a black hole. A complete extension had already been found by Martin Kruskal, who was urged to publish it.
These results came at the beginning of the golden age of general relativity, which was marked by general relativity and black holes becoming mainstream subjects of research. This process was helped by the discovery of pulsars in 1967, which, by 1969, were shown to be rapidly rotating neutron stars. Until that time, neutron stars, like black holes, were regarded as just theoretical curiosities; but the discovery of pulsars showed their physical relevance and spurred a further interest in all types of compact objects that might be formed by gravitational collapse.
In this period more general black hole solutions were found. In 1963, Roy Kerr found the exact solution for a rotating black hole. Two years later, Ezra Newman found the axisymmetric solution for a black hole that is both rotating and electrically charged. Through the work of Werner Israel, Brandon Carter, and David Robinson the no-hair theorem emerged, stating that a stationary black hole solution is completely described by the three parameters of the Kerr–Newman metric: mass, angular momentum, and electric charge.
At first, it was suspected that the strange features of the black hole solutions were pathological artifacts from the symmetry conditions imposed, and that the singularities would not appear in generic situations. This view was held in particular by Vladimir Belinsky, Isaak Khalatnikov, and Evgeny Lifshitz, who tried to prove that no singularities appear in generic solutions. However, in the late 1960s Roger Penrose and Stephen Hawking used global techniques to prove that singularities appear generically.
Work by James Bardeen, Jacob Bekenstein, Carter, and Hawking in the early 1970s led to the formulation of black hole thermodynamics. These laws describe the behaviour of a black hole in close analogy to the laws of thermodynamics by relating mass to energy, area to entropy, and surface gravity to temperature. The analogy was completed when Hawking, in 1974, showed that quantum field theory predicts that black holes should radiate like a black body with a temperature proportional to the surface gravity of the black hole.
The first use of the term "black hole" in print was by journalist Ann Ewing in her article ""'Black Holes' in Space"",
dated 18 January 1964, which was a report on a meeting of the American Association for the Advancement of Science. John Wheeler used the term "black hole" at a lecture in 1967, leading some to credit him with coining the phrase. After Wheeler's use of the term, it was quickly adopted in general use.
Properties and structure.
The no-hair theorem states that, once it achieves a stable condition after formation, a black hole has only three independent physical properties: mass, charge, and angular momentum. Any two black holes that share the same values for these properties, or parameters, are indistinguishable according to classical (i.e. non-quantum) mechanics.
These properties are special because they are visible from outside a black hole. For example, a charged black hole repels other like charges just like any other charged object. Similarly, the total mass inside a sphere containing a black hole can be found by using the gravitational analog of Gauss's law, the ADM mass, far away from the black hole. Likewise, the angular momentum can be measured from far away using frame dragging by the gravitomagnetic field.
When an object falls into a black hole, any information about the shape of the object or distribution of charge on it is evenly distributed along the horizon of the black hole, and is lost to outside observers. The behavior of the horizon in this situation is a dissipative system that is closely analogous to that of a conductive stretchy membrane with friction and electrical resistance—the membrane paradigm. This is different from other field theories such as electromagnetism, which do not have any friction or resistivity at the microscopic level, because they are time-reversible. Because a black hole eventually achieves a stable state with only three parameters, there is no way to avoid losing information about the initial conditions: the gravitational and electric fields of a black hole give very little information about what went in. The information that is lost includes every quantity that cannot be measured far away from the black hole horizon, including approximately conserved quantum numbers such as the total baryon number and lepton number. This behavior is so puzzling that it has been called the black hole information loss paradox.
Physical properties.
The simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes after Karl Schwarzschild who discovered this solution in 1916. According to Birkhoff's theorem, it is the only vacuum solution that is spherically symmetric. This means that there is no observable difference between the gravitational field of such a black hole and that of any other spherical object of the same mass. The popular notion of a black hole "sucking in everything" in its surroundings is therefore only correct near a black hole's horizon; far away, the external gravitational field is identical to that of any other body of the same mass.
Solutions describing more general black holes also exist. Non-rotating charged black holes are described by the Reissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most general stationary black hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum.
While the mass of a black hole can take any positive value, the charge and angular momentum are constrained by the mass. In Planck units, the total electric charge "Q" and the total angular momentum "J" are expected to satisfy
for a black hole of mass "M". Black holes satisfying this inequality are called extremal. Solutions of Einstein's equations that violate this inequality exist, but they do not possess an event horizon. These solutions have so-called naked singularities that can be observed from the outside, and hence are deemed "unphysical". The cosmic censorship hypothesis rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. This is supported by numerical simulations.
Due to the relatively large strength of the electromagnetic force, black holes forming from the collapse of stars are expected to retain the nearly neutral charge of the star. Rotation, however, is expected to be a common feature of compact objects. The black-hole candidate binary X-ray source GRS 1915+105 appears to have an angular momentum near the maximum allowed value.
Black holes are commonly classified according to their mass, independent of angular momentum "J" or electric charge "Q". The size of a black hole, as determined by the radius of the event horizon, or Schwarzschild radius, is roughly proportional to the mass "M" through
where "r"sh is the Schwarzschild radius and "MSun" is the mass of the Sun. This relation is exact only for black holes with zero charge and angular momentum; for more general black holes it can differ up to a factor of 2.
Event horizon.
The defining feature of a black hole is the appearance of an event horizon—a boundary in spacetime through which matter and light can only pass inward towards the mass of the black hole. Nothing, not even light, can escape from inside the event horizon. The event horizon is referred to as such because if an event occurs within the boundary, information from that event cannot reach an outside observer, making it impossible to determine if such an event occurred.
As predicted by general relativity, the presence of a mass deforms spacetime in such a way that the paths taken by particles bend towards the mass. At the event horizon of a black hole, this deformation becomes so strong that there are no paths that lead away from the black hole.
To a distant observer, clocks near a black hole appear to tick more slowly than those further away from the black hole. Due to this effect, known as gravitational time dilation, an object falling into a black hole appears to slow as it approaches the event horizon, taking an infinite time to reach it. At the same time, all processes on this object slow down, from the view point of a fixed outside observer, causing any light emitted by the object to appear redder and dimmer, an effect known as gravitational redshift. Eventually, the falling object becomes so dim that it can no longer be seen.
On the other hand, indestructible observers falling into a black hole do not notice any of these effects as they cross the event horizon. According to their own clocks, which appear to them to tick normally, they cross the event horizon after a finite time without noting any singular behaviour; it is impossible to determine the location of the event horizon from local observations.
The shape of the event horizon of a black hole is always approximately spherical. For non-rotating (static) black holes the geometry of the event horizon is precisely spherical, while for rotating black holes the sphere is oblate.
Singularity.
At the center of a black hole, as described by general relativity, lies a gravitational singularity, a region where the spacetime curvature becomes infinite. For a non-rotating black hole, this region takes the shape of a single point and for a rotating black hole, it is smeared out to form a ring singularity that lies in the plane of rotation. In both cases, the singular region has zero volume. It can also be shown that the singular region contains all the mass of the black hole solution. The singular region can thus be thought of as having infinite density.
Observers falling into a Schwarzschild black hole ("i.e.", non-rotating and not charged) cannot avoid being carried into the singularity, once they cross the event horizon. They can prolong the experience by accelerating away to slow their descent, but only up to a limit; after attaining a certain ideal velocity, it is best to free fall the rest of the way. When they reach the singularity, they are crushed to infinite density and their mass is added to the total of the black hole. Before that happens, they will have been torn apart by the growing tidal forces in a process sometimes referred to as spaghettification or the "noodle effect".
In the case of a charged (Reissner–Nordström) or rotating (Kerr) black hole, it is possible to avoid the singularity. Extending these solutions as far as possible reveals the hypothetical possibility of exiting the black hole into a different spacetime with the black hole acting as a wormhole. The possibility of traveling to another universe is however only theoretical, since any perturbation would destroy this possibility. It also appears to be possible to follow closed timelike curves (returning to one's own past) around the Kerr singularity, which lead to problems with causality like the grandfather paradox. It is expected that none of these peculiar effects would survive in a proper quantum treatment of rotating and charged black holes.
The appearance of singularities in general relativity is commonly perceived as signaling the breakdown of the theory. This breakdown, however, is expected; it occurs in a situation where quantum effects should describe these actions, due to the extremely high density and therefore particle interactions. To date, it has not been possible to combine quantum and gravitational effects into a single theory, although there exist attempts to formulate such a theory of quantum gravity. It is generally expected that such a theory will not feature any singularities.
Photon sphere.
The photon sphere is a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole. For non-rotating black holes, the photon sphere has a radius 1.5 times the Schwarzschild radius. Their orbits would be dynamically unstable, hence any small perturbation, such as a particle of infalling matter, would cause an instability that would grow over time, either setting the photon on an outward trajectory causing it to escape the black hole, or on an inward spiral where it would eventually cross the event horizon.
While light can still escape from the photon sphere, any light that crosses the photon sphere on an inbound trajectory will be captured by the black hole. Hence any light that reaches an outside observer from the photon sphere must have been emitted by objects between the photon sphere and the event horizon.
Other compact objects, such as neutron stars, can also have photon spheres. This follows from the fact that the gravitational field "external" to a spherically-symmetric object is governed by the Schwarzschild metric, which depends only on the object's mass rather than the radius of the object, hence any object whose radius shrinks to smaller than 1.5 times the Schwarzschild radius will have a photon sphere.
Ergosphere.
Rotating black holes are surrounded by a region of spacetime in which it is impossible to stand still, called the ergosphere. This is the result of a process known as frame-dragging; general relativity predicts that any rotating mass will tend to slightly "drag" along the spacetime immediately surrounding it. Any object near the rotating mass will tend to start moving in the direction of rotation. For a rotating black hole, this effect is so strong near the event horizon that an object would have to move faster than the speed of light in the opposite direction to just stand still.
The ergosphere of a black hole is a volume whose inner boundary is the black hole's event horizon and an outer boundary of an oblate spheroid, which coincides with the event horizon at the poles but noticeably wider around the equator. The outer boundary is sometimes called the "ergosurface".
Objects and radiation can escape normally from the ergosphere. Through the Penrose process, objects can emerge from the ergosphere with more energy than they entered. This energy is taken from the rotational energy of the black hole causing the latter to slow.
Innermost stable circular orbit (ISCO).
In Newtonian gravity, test particles can stably orbit at arbitrary distances from a central object. In general relativity, however, there exists an innermost stable circular orbit (often called the ISCO), inside of which, any infinitesimal perturbations to a circular orbit will lead to inspiral into the black hole. The location of the ISCO depends on the spin of the black hole, in the case of a Schwarzschild black hole (spin zero) is:
and decreases with increasing spin.
Formation and evolution.
Considering the exotic nature of black holes, it may be natural to question if such bizarre objects could exist in nature or to suggest that they are merely pathological solutions to Einstein's equations. Einstein himself wrongly thought that black holes would not form, because he held that the angular momentum of collapsing particles would stabilize their motion at some radius. This led the general relativity community to dismiss all results to the contrary for many years. However, a minority of relativists continued to contend that black holes were physical objects, and by the end of the 1960s, they had persuaded the majority of researchers in the field that there is no obstacle to the formation of an event horizon.
Once an event horizon forms, Penrose proved, a singularity will form within. Shortly afterwards, Hawking showed that many cosmological solutions that describe the Big Bang have singularities without scalar fields or other exotic matter (see "Penrose–Hawking singularity theorems"). The Kerr solution, the no-hair theorem, and the laws of black hole thermodynamics showed that the physical properties of black holes were simple and comprehensible, making them respectable subjects for research. The primary formation process for black holes is expected to be the gravitational collapse of heavy objects such as stars, but there are also more exotic processes that can lead to the production of black holes.
Gravitational collapse.
Gravitational collapse occurs when an object's internal pressure is insufficient to resist the object's own gravity. For stars this usually occurs either because a star has too little "fuel" left to maintain its temperature through stellar nucleosynthesis, or because a star that would have been stable receives extra matter in a way that does not raise its core temperature. In either case the star's temperature is no longer high enough to prevent it from collapsing under its own weight.
The collapse may be stopped by the degeneracy pressure of the star's constituents, allowing the condensation of matter into an exotic denser state. The result is one of the various types of compact star. The type of compact star formed depends on the mass of the remnant of the original star left after the outer layers have been blown away. Such explosions, from a supernova explosion or by pulsations, leads to planetary nebula. Note that this mass can be substantially less than the original star. Remnants exceeding are produced by stars that were over before the collapse.
If the mass of the remnant exceeds about (the Tolman–Oppenheimer–Volkoff limit), either because the original star was very heavy or because the remnant collected additional mass through accretion of matter, even the degeneracy pressure of neutrons is insufficient to stop the collapse. No known mechanism (except possibly quark degeneracy pressure, see quark star) is powerful enough to stop the implosion and the object will inevitably collapse to form a black hole.
The gravitational collapse of heavy stars is assumed to be responsible for the formation of stellar mass black holes. Star formation in the early universe may have resulted in very massive stars, which upon their collapse would have produced black holes of up to . These black holes could be the seeds of the supermassive black holes found in the centers of most galaxies.
While most of the energy released during gravitational collapse is emitted very quickly, an outside observer does not actually see the end of this process. Even though the collapse takes a finite amount of time from the reference frame of infalling matter, a distant observer would see the infalling material slow and halt just above the event horizon, due to gravitational time dilation. Light from the collapsing material takes longer and longer to reach the observer, with the light emitted just before the event horizon forms delayed an infinite amount of time. Thus the external observer never sees the formation of the event horizon; instead, the collapsing material seems to become dimmer and increasingly red-shifted, eventually fading away.
Primordial black holes in the Big Bang.
Gravitational collapse requires great density. In the current epoch of the universe these high densities are only found in stars, but in the early universe shortly after the big bang densities were much greater, possibly allowing for the creation of black holes. The high density alone is not enough to allow the formation of black holes since a uniform mass distribution will not allow the mass to bunch up. In order for primordial black holes to form in such a dense medium, there must be initial density perturbations that can then grow under their own gravity. Different models for the early universe vary widely in their predictions of the size of these perturbations. Various models predict the creation of black holes, ranging from a Planck mass to hundreds of thousands of solar masses. Primordial black holes could thus account for the creation of any type of black hole.
High-energy collisions.
Gravitational collapse is not the only process that could create black holes. In principle, black holes could be formed in high-energy collisions that achieve sufficient density. As of 2002, no such events have been detected, either directly or indirectly as a deficiency of the mass balance in particle accelerator experiments. This suggests that there must be a lower limit for the mass of black holes. Theoretically, this boundary is expected to lie around the Planck mass ("m"P = ≈ ≈ ), where quantum effects are expected to invalidate the predictions of general relativity. This would put the creation of black holes firmly out of reach of any high-energy process occurring on or near the Earth. However, certain developments in quantum gravity suggest that the Planck mass could be much lower: some braneworld scenarios for example put the boundary as low as . This would make it conceivable for micro black holes to be created in the high-energy collisions that occur when cosmic rays hit the Earth's atmosphere, or possibly in the Large Hadron Collider at CERN. These theories are very speculative, and the creation of black holes in these processes is deemed unlikely by many specialists. Even if micro black holes could be formed, it is expected that they would evaporate in about 10−25 seconds, posing no threat to the Earth.
Growth.
Once a black hole has formed, it can continue to grow by absorbing additional matter. Any black hole will continually absorb gas and interstellar dust from its surroundings and omnipresent cosmic background radiation. This is the primary process through which supermassive black holes seem to have grown. A similar process has been suggested for the formation of intermediate-mass black holes found in globular clusters.
Another possibility for black hole growth, is for a black hole to merge with other objects such as stars or even other black holes. Although not necessary for growth, this is thought to have been important, especially for the early development of supermassive black holes, which could have formed from the coagulation of many smaller objects. The process has also been proposed as the origin of some intermediate-mass black holes.
Evaporation.
In 1974, Hawking predicted that black holes are not entirely black but emit small amounts of thermal radiation; this effect has become known as Hawking radiation. By applying quantum field theory to a static black hole background, he determined that a black hole should emit particles that display a perfect black body spectrum. Since Hawking's publication, many others have verified the result through various approaches. If Hawking's theory of black hole radiation is correct, then black holes are expected to shrink and evaporate over time as they lose mass by the emission of photons and other particles. The temperature of this thermal spectrum (Hawking temperature) is proportional to the surface gravity of the black hole, which, for a Schwarzschild black hole, is inversely proportional to the mass. Hence, large black holes emit less radiation than small black holes.
A stellar black hole of has a Hawking temperature of about 100 nanokelvins. This is far less than the 2.7 K temperature of the cosmic microwave background radiation. Stellar-mass or larger black holes receive more mass from the cosmic microwave background than they emit through Hawking radiation and thus will grow instead of shrink. To have a Hawking temperature larger than 2.7 K (and be able to evaporate), a black hole would need a mass less than the Moon. Such a black hole would have a diameter of less than a tenth of a millimeter.
If a black hole is very small, the radiation effects are expected to become very strong. Even a black hole that is heavy compared to a human would evaporate in an instant. A black hole with the mass of a car would have a diameter of about 10−24 m and take a nanosecond to evaporate, during which time it would briefly have a luminosity of more than 200 times that of the Sun. Lower-mass black holes are expected to evaporate even faster; for example, a black hole of mass 1 TeV/"c"2 would take less than 10−88 seconds to evaporate completely. For such a small black hole, quantum gravitation effects are expected to play an important role and could hypothetically make such a small black hole stable, although current developments in quantum gravity do not indicate so.
The Hawking radiation for an astrophysical black hole is predicted to be very weak and would thus be exceedingly difficult to detect from Earth. A possible exception, however, is the burst of gamma rays emitted in the last stage of the evaporation of primordial black holes. Searches for such flashes have proven unsuccessful and provide stringent limits on the possibility of existence of low mass primordial black holes. NASA's Fermi Gamma-ray Space Telescope launched in 2008 will continue the search for these flashes.
Observational evidence.
By their very nature, black holes do not directly emit any electromagnetic radiation other than the hypothetical Hawking radiation, so astrophysicists searching for black holes must generally rely on indirect observations. For example, a black hole's existence can sometimes be inferred by observing its gravitational interactions with its surroundings. However, the Event Horizon Telescope (EHT), run by MIT's Haystack Observatory, is an attempt to directly observe the immediate environment of the event horizon of Sagittarius A*, the black hole at the centre of the Milky Way. The first image of the event horizon may appear as early as 2016. The existence of magnetic fields just outside the event horizon of Sagittarius A*, which were predicted by theoretical studies of black holes, was confirmed by the EHT in 2015.
Detection of gravitational waves from merging black holes.
On 24 September 2015 the LIGO gravitational wave observatory made the first-ever successful observation of gravitational waves. The signal was consistent with theoretical predictions for the gravitational waves produced by the merger of two black holes: one with about 36 solar masses, and the other around 29 solar masses. This observation provides the most concrete evidence for the existence of black holes to date. For instance, the gravitational wave signal suggests that the separation of the two object prior to merger was just 350 km (or roughly 4 times the Schwarzschild radius corresponding to the inferred masses). The objects must therefore have been extremely compact, leaving black holes as the most plausible interpretation.
More importantly, the signal observed by LIGO also included the start of the post-merger ringdown, the signal produced as the newly formed compact object settles down to a stationary state. Arguably, the ringdown is the most direct way of observing a black hole. From the LIGO signal it is possible to extract the frequency and damping time of the dominant mode of the ringdown. From these it is possible to infer the mass and angular momentum of the final object, which match independent predictions from numerical simulations of the merger. The frequency and decay time of the dominant mode are determined by the geometry of the photon sphere. Hence, observation of this mode confirms the presence of a photon sphere, however it cannot exclude possible exotic alternatives to black holes that are compact enough to have a photon sphere.
The observation also provides the first observational evidence for the existence of stellar-mass black hole binaries. Furthermore, it is the first observational evidence of stellar-mass black holes weighing 25 solar masses or more.
Proper motions of stars orbiting Sagittarius A*.
The proper motions of stars near the center of our own Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole. Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. By fitting their motions to Keplerian orbits, the astronomers were able to infer, in 1998, that a 2.6 million object must be contained in a volume with a radius of 0.02 light-years to cause the motions of those stars. Since then, one of the stars—called S2—has completed a full orbit. From the orbital data, astronomers were able to make refine the calculations of the mass to 4.3 million and a radius of less than 0.002 lightyears for the object causing the orbital motion of those stars. The upper limit on the object's size is still too large to test whether it is smaller than its Schwarzschild radius; nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume. Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes.
Accretion of matter.
Due to conservation of angular momentum, gas falling into the gravitational well created by a massive object will typically form a disc-like structure around the object. Artists' impressions such as the accompanying representation of a black hole with corona commonly depict the black hole as if it were a flat-space material body hiding the part of the disc just behind it, but detailed mathematical modelling shows that the image of the disc would actually be distorted by the bending of light that originated behind the black hole in such a way that the upper side of the disc would be entirely visible, while there would be a partially visible secondary image of the underside of the disk.
Within such a disc, friction would cause angular momentum to be transported outward, allowing matter to fall further inward, thus releasing potential energy and increasing the temperature of the gas.
As such, many of the universe's more energetic phenomena have been attributed to the accretion of matter on black holes. In particular, active galactic nuclei and quasars are believed to be the accretion discs of supermassive black holes. Similarly, X-ray binaries are generally accepted to be binary star systems in which one of the two stars is a compact object accreting matter from its companion. It has also been suggested that some ultraluminous X-ray sources may be the accretion disks of intermediate-mass black holes.
In November 2011 the first direct observation of a quasar accretion disk around a supermassive black hole was reported.
X-ray binaries.
X-ray binaries are binary star systems that emit a majority of their radiation in the X-ray part of the spectrum. These X-ray emissions are generally thought to result when one of the stars (compact object) accretes matter from another (regular) star. The presence of an ordinary star in such a system provides a unique opportunity for studying the central object and to determine if it might be a black hole.
If such a system emits signals that can be directly traced back to the compact object, it cannot be a black hole. The absence of such a signal does, however, not exclude the possibility that the compact object is a neutron star. By studying the companion star it is often possible to obtain the orbital parameters of the system and to obtain an estimate for the mass of the compact object. If this is much larger than the Tolman–Oppenheimer–Volkoff limit (that is, the maximum mass a neutron star can have before it collapses) then the object cannot be a neutron star and is generally expected to be a black hole.
The first strong candidate for a black hole, Cygnus X-1, was discovered in this way by Charles Thomas Bolton, Louise Webster and Paul Murdin in 1972. Some doubt, however, remained due to the uncertainties that result from the companion star being much heavier than the candidate black hole. Currently, better candidates for black holes are found in a class of X-ray binaries called soft X-ray transients. In this class of system, the companion star is of relatively low mass allowing for more accurate estimates of the black hole mass. Moreover, these systems are actively emit X-rays for only several months once every 10–50 years. During the period of low X-ray emission (called quiescence), the accretion disc is extremely faint allowing detailed observation of the companion star during this period. One of the best such candidates is V404 Cyg.
Quiescence and advection-dominated accretion flow.
The faintness of the accretion disc of an X-ray binary during quiescence is suspected to be caused by the flow of mass entering a mode called an advection-dominated accretion flow (ADAF). In this mode, almost all the energy generated by friction in the disc is swept along with the flow instead of radiated away. If this model is correct, then it forms strong qualitative evidence for the presence of an event horizon, since if the object at the center of the disc had a solid surface, it would emit large amounts of radiation as the highly energetic gas hits the surface, an effect that is observed for neutron stars in a similar state.
Quasi-periodic oscillations.
The X-ray emissions from accretion disks sometimes flicker at certain frequencies. These signals are called quasi-periodic oscillations and are thought to be caused by material moving along the inner edge of the accretion disk (the innermost stable circular orbit). As such their frequency is linked to the mass of the compact object. They can thus be used as an alternative way to determine the mass of candidate black holes.
Galactic nuclei.
Astronomers use the term "active galaxy" to describe galaxies with unusual characteristics, such as unusual spectral line emission and very strong radio emission. Theoretical and observational studies have shown that the activity in these active galactic nuclei (AGN) may be explained by the presence of supermassive black holes, which can be millions of times more massive than stellar ones. The models of these AGN consist of a central black hole that may be millions or billions of times more massive than the Sun; a disk of gas and dust called an accretion disk; and two jets perpendicular to the accretion disk.
Although supermassive black holes are expected to be found in most AGN, only some galaxies' nuclei have been more carefully studied in attempts to both identify and measure the actual masses of the central supermassive black hole candidates. Some of the most notable galaxies with supermassive black hole candidates include the Andromeda Galaxy, M32, M87, NGC 3115, NGC 3377, NGC 4258, NGC 4889, NGC 1277, OJ 287, APM 08279+5255 and the Sombrero Galaxy.
It is now widely accepted that the center of nearly every galaxy, not just active ones, contains a supermassive black hole. The close observational correlation between the mass of this hole and the velocity dispersion of the host galaxy's bulge, known as the M-sigma relation, strongly suggests a connection between the formation of the black hole and the galaxy itself.
Microlensing (proposed).
Another way that the black hole nature of an object may be tested in the future is through observation of effects caused by a strong gravitational field in their vicinity. One such effect is gravitational lensing: The deformation of spacetime around a massive object causes light rays to be deflected much as light passing through an optic lens. Observations have been made of weak gravitational lensing, in which light rays are deflected by only a few arcseconds. However, it has never been directly observed for a black hole. One possibility for observing gravitational lensing by a black hole would be to observe stars in orbit around the black hole. There are several candidates for such an observation in orbit around Sagittarius A*.
Alternatives.
The evidence for stellar black holes strongly relies on the existence of an upper limit for the mass of a neutron star. The size of this limit heavily depends on the assumptions made about the properties of dense matter. New exotic phases of matter could push up this bound. A phase of free quarks at high density might allow the existence of dense quark stars, and some supersymmetric models predict the existence of Q stars. Some extensions of the standard model posit the existence of preons as fundamental building blocks of quarks and leptons, which could hypothetically form preon stars. These hypothetical models could potentially explain a number of observations of stellar black hole candidates. However, it can be shown from arguments in general relativity that any such object will have a maximum mass.
Since the average density of a black hole inside its Schwarzschild radius is inversely proportional to the square of its mass, supermassive black holes are much less dense than stellar black holes (the average density of a black hole is comparable to that of water). Consequently, the physics of matter forming a supermassive black hole is much better understood and the possible alternative explanations for supermassive black hole observations are much more mundane. For example, a supermassive black hole could be modelled by a large cluster of very dark objects. However, such alternatives are typically not stable enough to explain the supermassive black hole candidates.
The evidence for the existence of stellar and supermassive black holes implies that in order for black holes to not form, general relativity must fail as a theory of gravity, perhaps due to the onset of quantum mechanical corrections. A much anticipated feature of a theory of quantum gravity is that it will not feature singularities or event horizons and thus black holes would not be real artifacts. In 2002, much attention has been drawn by the fuzzball model in string theory. Based on calculations for specific situations in string theory, the proposal suggests that generically the individual states of a black hole solution do not have an event horizon or singularity, but that for a classical/semi-classical observer the statistical average of such states appears just as an ordinary black hole as deduced from general relativity.
Open questions.
Entropy and thermodynamics.
In 1971, Hawking showed under general conditions that the total area of the event horizons of any collection of classical black holes can never decrease, even if they collide and merge. This result, now known as the second law of black hole mechanics, is remarkably similar to the second law of thermodynamics, which states that the total entropy of a system can never decrease. As with classical objects at absolute zero temperature, it was assumed that black holes had zero entropy. If this were the case, the second law of thermodynamics would be violated by entropy-laden matter entering a black hole, resulting in a decrease of the total entropy of the universe. Therefore, Bekenstein proposed that a black hole should have an entropy, and that it should be proportional to its horizon area.
The link with the laws of thermodynamics was further strengthened by Hawking's discovery that quantum field theory predicts that a black hole radiates blackbody radiation at a constant temperature. This seemingly causes a violation of the second law of black hole mechanics, since the radiation will carry away energy from the black hole causing it to shrink. The radiation, however also carries away entropy, and it can be proven under general assumptions that the sum of the entropy of the matter surrounding a black hole and one quarter of the area of the horizon as measured in Planck units is in fact always increasing. This allows the formulation of the first law of black hole mechanics as an analogue of the first law of thermodynamics, with the mass acting as energy, the surface gravity as temperature and the area as entropy.
One puzzling feature is that the entropy of a black hole scales with its area rather than with its volume, since entropy is normally an extensive quantity that scales linearly with the volume of the system. This odd property led Gerard 't Hooft and Leonard Susskind to propose the holographic principle, which suggests that anything that happens in a volume of spacetime can be described by data on the boundary of that volume.
Although general relativity can be used to perform a semi-classical calculation of black hole entropy, this situation is theoretically unsatisfying. In statistical mechanics, entropy is understood as counting the number of microscopic configurations of a system that have the same macroscopic qualities (such as mass, charge, pressure, etc.). Without a satisfactory theory of quantum gravity, one cannot perform such a computation for black holes. Some progress has been made in various approaches to quantum gravity. In 1995, Andrew Strominger and Cumrun Vafa showed that counting the microstates of a specific supersymmetric black hole in string theory reproduced the Bekenstein–Hawking entropy. Since then, similar results have been reported for different black holes both in string theory and in other approaches to quantum gravity like loop quantum gravity.
Information loss paradox.
Because a black hole has only a few internal parameters, most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As long as black holes were thought to persist forever this information loss is not that problematic, as the information can be thought of as existing inside the black hole, inaccessible from the outside. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information appears to be gone forever.
The question whether information is truly lost in black holes (the black hole information paradox) has divided the theoretical physics community (see Thorne–Hawking–Preskill bet). In quantum mechanics, loss of information corresponds to the violation of vital property called unitarity, which has to do with the conservation of probability. It has been argued that loss of unitarity would also imply violation of conservation of energy. Over recent years evidence has been building that indeed information and unitarity are preserved in a full quantum gravitational treatment of the problem.

</doc>
<doc id="4651" url="https://en.wikipedia.org/wiki?curid=4651" title="Beta decay">
Beta decay

In nuclear physics, beta decay ("β"-decay) is a type of radioactive decay in which a proton is transformed into a neutron, or vice versa, inside an atomic nucleus. This process allows the atom to move closer to the optimal ratio of protons and neutrons. As a result of this transformation, the nucleus emits a detectable beta particle, which is an electron or positron.
Beta decay is mediated by the weak force. There are two types of beta decay, known as "beta minus" and "beta plus". In beta minus (β−) decay a neutron is lost and a proton appears and the process produces an electron and electron antineutrino, while in beta plus (β+) decay a proton is lost and a neutron appears and the process produces a positron and electron neutrino; β+ decay is thus also known as positron emission.
An example of electron emission (β− decay) is the decay of carbon-14 into nitrogen-14:
In this form of decay, the original element becomes a new chemical element in a process known as nuclear transmutation. This new element has an unchanged mass number , but an atomic number that is increased by one. As in all nuclear decays, the decaying element (in this case ) is known as the "parent nuclide" while the resulting element (in this case ) is known as the "daughter nuclide". The emitted electron or positron is known as a beta particle.
An example of positron emission (β+ decay) is the decay of magnesium-23 into sodium-23:
In contrast to β− decay, β+ decay is accompanied by the emission of an electron neutrino and a positron. β+ decay also results in nuclear transmutation, with the resulting element having an atomic number that is decreased by one.
Electron capture is sometimes included as a type of beta decay, because the basic nuclear process, mediated by the weak force, is the same. In electron capture, an inner atomic electron is captured by a proton in the nucleus, transforming it into a neutron, and an electron neutrino is released. An example of electron capture is the decay of krypton-81 into bromine-81:
Electron capture is a competing (simultaneous) decay process for all nuclei that can undergo β+ decay. The converse, however, is not true: electron capture is the "only" type of decay that is allowed in proton-rich nuclides that do not have sufficient energy to emit a positron and neutrino.
β− decay.
In  decay, the weak interaction converts an atomic nucleus into a nucleus with atomic number increased by one, while emitting an electron () and an electron antineutrino (). generic equation is:
where and are the mass number and atomic number of the decaying nucleus, and X and X' are the initial and final elements, respectively.
Another example is when the free neutron () decays by  decay into a proton ():
At the fundamental level (as depicted in the Feynman diagram on the left), this is caused by the conversion of the negatively charged (− e) down quark to the positively charged (+ e) up quark by emission of a boson; the boson subsequently decays into an electron and an electron antineutrino:
The beta spectrum is a continuous spectrum: the total decay energy is divided between the electron and the antineutrino. In the figure to the right, this is shown, by way of example, for an electron of 0.4 MeV energy. In this example, the antineutrino then gets the remainder: 0.76 MeV, since the total decay energy is assumed to be 1.16 MeV.
 decay generally occurs in neutron-rich nuclei.
β+ decay.
In  decay, or "positron emission", the weak interaction converts an atomic nucleus into a nucleus with atomic number decreased by one, while emitting a positron () and an electron neutrino (). The generic equation is:
This may be considered as the decay of a proton inside the nucleus to a neutron
However,  decay cannot occur in an isolated proton because it requires energy due to the mass of the neutron being greater than the mass of the proton.  decay can only happen inside nuclei when the daughter nucleus has a greater binding energy (and therefore a lower total energy) than the mother nucleus. The difference between these energies goes into the reaction of converting a proton into a neutron, a positron and a neutrino and into the kinetic energy of these particles. In an opposite process to negative beta decay, the weak interaction converts a proton into a neutron by converting an up quark into a down quark by having it emit a or absorb a .
Electron capture (K-capture).
In all cases where  decay of a nucleus is allowed energetically, so is electron capture, the process in which the same nucleus captures an atomic electron with the emission of a neutrino:
The emitted neutrino is mono-energetic. In proton-rich nuclei where the energy difference between initial and final states is less than ,  decay is not energetically possible, and electron capture is the sole decay mode.
If the captured electron comes from the innermost shell of the atom, the K-shell, which has the highest probability to interact with the nucleus, the process is called K-capture. If it comes from the L-shell, the process is called L-capture, etc.
Competition of beta decay types.
Three types of beta decay in competition are illustrated by the single isotope copper-64 (29 protons, 35 neutrons), which has a half-life of about 12.7 hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay. This particular nuclide (though not all nuclides in this situation) is almost equally likely to decay through proton decay by positron emission (18%) or electron capture (43%), as through neutron decay by electron emission (39%).
Helicity (polarization) of neutrinos, electrons and positrons emitted in beta decay.
After the discovery of parity non-conservation (see history below), it was found that, in beta decay, electrons are emitted mostly with negative helicity, i.e., they move, naively speaking, like left-handed screws driven into a material (they have negative longitudinal polarization). Conversely, positrons have mostly positive helicity, i.e., they move like right-handed screws. Neutrinos (emitted in positron decay) have positive helicity, while antineutrinos (emitted in electron decay) have negative helicity.
The higher the energy of the particles, the higher their polarization.
Energy release.
The value is defined as the total energy released in a given nuclear decay. In beta decay, is therefore also the sum of the kinetic energies of the emitted beta particle, neutrino, and recoiling nucleus. (Because of the large mass of the nucleus compared to that of the beta particle and neutrino, the kinetic energy of the recoiling nucleus can generally be neglected.) Beta particles can therefore be emitted with any kinetic energy ranging from 0 to . A typical is around 1 MeV, but can range from a few keV to a few tens of MeV.
Since the rest mass of the electron is 511 keV, the most energetic beta particles are ultrarelativistic, with speeds very close to the speed of light.
β− decay.
Consider the generic equation for beta decay
The value for this decay is
where formula_2 is the mass of the nucleus of the atom, formula_3 is the mass of the electron, and formula_4 is the mass of the electron antineutrino. In other words, the total energy released is the mass energy of the initial nucleus, minus the mass energy of the final nucleus, electron, and antineutrino. The mass of the nucleus is related to the standard atomic mass by
That is, the total atomic mass is the mass of the nucleus, plus the mass of the electrons, minus the binding energy of each electron. Substituting this into our original equation, while neglecting the nearly-zero antineutrino mass and difference in electron binding energy, which is very small for high- atoms, we have
This energy is carried away as kinetic energy by the electron and neutrino.
Because the reaction will proceed only when the -value is positive, β− decay can occur when the mass of atom is greater than the mass of atom .
β+ decay.
The equations for β+ decay are similar, with the generic equation
giving
However, in this equation, the electron masses do not cancel, and we are left with
Because the reaction will proceed only when the -value is positive, β+ decay can occur when the mass of atom exceeds that of by at least twice the mass of the electron.
Electron capture.
The analogous calculation for electron capture must take into account the binding energy of the electrons. This is because the atom will be left in an excited state after capturing the electron, and the binding energy of the captured innermost electron is significant. Using the generic equation for electron capture
we have
which simplifies to
where is the binding energy of the captured electron.
Because the binding energy of the electron is much less than the mass of the electron, nuclei that can undergo β+ decay can always also undergo electron capture, but the reverse is not true.
Nuclear transmutation.
If the proton and neutron are part of an atomic nucleus, these decay processes transmute one chemical element into another. For example:
Beta decay does not change the number  of nucleons in the nucleus, but changes only its charge . Thus the set of all nuclides with the same  can be introduced; these "isobaric" nuclides may turn into each other via beta decay. Among them, several nuclides (at least one for any given mass number ) are beta stable, because they present local minima of the mass excess: if such a nucleus has numbers, the neighbour nuclei and have higher mass excess and can beta decay into , but not vice versa. For all odd mass numbers , there is only one known beta-stable isobar. For even , there are up to three different beta-stable isobars experimentally known; for example, , , and are all beta-stable. There are about 355 known beta-decay stable nuclides total.
Usually, unstable nuclides are clearly either "neutron rich" or "proton rich", with the former undergoing beta decay and the latter undergoing electron capture (or more rarely, due to the higher energy requirements, positron decay). However, in a few cases of odd-proton, odd-neutron radionuclides, it may be energetically favorable for the radionuclide to decay to an even-proton, even-neutron isobar either by undergoing beta-positive or beta-negative decay. An often-cited example is , which decays by positron emission/electron capture 61% of the time to , and 39% of the time by (negative) beta decay to .
Most naturally occurring isotopes on earth are beta stable. Those that are not have half-lives ranging from under a second to periods of time significantly greater than the age of the universe. One common example of a long-lived isotope is the odd-proton odd-neutron nuclide , which undergoes all three types of beta decay (, and electron capture) with a half-life of .
Double beta decay.
Some nuclei can undergo double beta decay (ββ decay) where the charge of the nucleus changes by two units. Double beta decay is difficult to study, as the process has an extremely long half-life. In nuclei for which both β decay and ββ decay are possible, the rarer ββ decay process is effectively impossible to observe. However, in nuclei where β decay is forbidden but ββ decay is allowed, the process can be seen and a half-life measured. Thus, ββ decay is usually studied only for beta stable nuclei. Like single beta decay, double beta decay does not change ; thus, at least one of the nuclides with some given has to be stable with regard to both single and double beta decay.
"Ordinary" double beta decay results in the emission of two electrons and two antineutrinos. If neutrinos are Majorana particles (i.e., they are their own antiparticles), then a decay known as neutrinoless double beta decay will occur. Most neutrino physicists believe that neutrinoless double beta decay has never been observed.
Bound-state β− decay.
A very small minority of free neutron decays (about four per million) are so-called "two-body decays", in which the proton, electron and antineutrino are produced, but the electron fails to gain the 13.6 eV energy necessary to escape the proton, and therefore simply remains bound to it, as a neutral hydrogen atom. In this type of beta decay, in essence all of the neutron decay energy is carried off by the antineutrino.
For fully ionized atoms (bare nuclei), it is possible in likewise manner for electrons to fail to escape the atom, and to be emitted from the nucleus into low-lying atomic bound states (orbitals). This can not occur for neutral atoms with low-lying bound states which already filled by electrons.
The phenomenon in fully ionized atoms was first observed for 163Dy66+ in 1992 by Jung et al. of the Darmstadt Heavy-Ion Research group. Although neutral 163Dy is a stable isotope, the fully ionized 163Dy66+ undergoes β decay into the K and L shells with a half-life of 47 days.
Another possibility is that a fully ionized atom undergoes greatly accelerated β decay, as observed for 187Re by Bosch et al., also at Darmstadt. Neutral 187Re does undergo β decay with a half-life of 42 × 109 years, but for fully ionized 187Re75+ this is shortened by a factor of 109 to only 32.9 years. For comparison the variation of decay rates of other nuclear processes due to chemical environment is less than 1%.
Forbidden transitions.
Beta decays can be classified according to the -value of the emitted radiation. When , the decay is referred to as "forbidden". Nuclear selection rules require high L-values to be accompanied by changes in nuclear spin () and parity (π). The selection rules for the th forbidden transitions are:
where or corresponds to no parity change or parity change, respectively. The special case of a transition between isobaric analogue states, where the structure of the final state is very similar to the structure of the initial state, is referred to as "superallowed" for beta decay, and proceeds very quickly. The following table lists the Δ and Δπ values for the first few values of :
Fermi transitions.
A Fermi transition is a beta decay in which the spins of the emitted electron (positron) and anti-neutrino (neutrino) couple to total spin formula_12, leading to an angular momentum change formula_13 between the initial and final states of the nucleus (assuming an allowed transition formula_14).
In the non-relativistic limit, the nuclear part of the operator for a Fermi transition is given by
with formula_16 the weak vector coupling constant, formula_17 the isospin raising and lowering operators, and formula_18 running over all protons and neutrons in the nucleus.
Gamow-Teller transitions.
A Gamow-Teller transition is a beta decay in which the spins of the emitted electron (positron) and anti-neutrino (neutrino) couple to total spin formula_19, leading to an angular momentum change formula_20 between the initial and final states of the nucleus (assuming an allowed transition).
In this case, the nuclear part of the operator is given by
with formula_22 the weak axial-vector coupling constant, and formula_23 the spin Pauli matrices, which can produce a spin-flip in the decaying nucleon.
Beta emission spectrum.
Beta decay can be considered as a perturbation as described in quantum mechanics, and thus Fermi's Golden Rule can be applied. This leads to an expression for the kinetic energy spectrum of emitted betas as follows:
where is the kinetic energy, is a shape function that depends on the forbiddenness of the decay (it is constant for allowed decays), is the Fermi Function (see below) with "Z" the charge of the final-state nucleus, is the total energy, is the momentum, and is the Q value of the decay. The kinetic energy of the emitted neutrino is given approximately by minus the kinetic energy of the beta.
As an example, the beta decay spectrum of 210Bi (originally called RaE) is shown to the right.
Fermi function.
The Fermi function that appears in the beta spectrum formula accounts for the Coulomb attraction / repulsion between the emitted beta and the final state nucleus. Approximating the associated wavefunctions to be spherically symmetric, the Fermi function can be analytically calculated to be:
where (α is the fine-structure constant), (+ for electrons, − for positrons), ( is the radius of the final state nucleus), and Γ is the Gamma function.
For non-relativistic betas (), this expression can be approximated by:
Other approximations can be found in the literature.
Kurie plot.
A Kurie plot (also known as a Fermi–Kurie plot) is a graph used in studying beta decay developed by Franz N. D. Kurie, in which the square root of the number of beta particles whose momenta (or energy) lie within a certain narrow range, divided by the Fermi function, is plotted against beta-particle energy. It is a straight line for allowed transitions and some forbidden transitions, in accord with the Fermi beta-decay theory. The energy-axis (x-axis) intercept of a Kurie plot corresponds to the maximum energy imparted to the electron/positron (the decay's -value). With a Kurie plot one can find the limit on the effective mass of a neutrino.
History.
Discovery and characterization of β− decay.
Radioactivity was discovered in 1896 by Henri Becquerel in uranium, and subsequently observed by Marie and Pierre Curie in thorium and in the new elements polonium and radium.
In 1899, Ernest Rutherford separated radioactive emissions into two types: alpha and beta (now beta minus), based on penetration of objects and ability to cause ionization. Alpha rays could be stopped by thin sheets of paper or aluminium, whereas beta rays could penetrate several millimetres of aluminium. (In 1900, Paul Villard identified a still more penetrating type of radiation, which Rutherford identified as a fundamentally new type in 1903, and termed gamma rays).
In 1900, Becquerel measured the mass-to-charge ratio () for beta particles by the method of J.J. Thomson used to study cathode rays and identify the electron. He found that for a beta particle is the same as for Thomson's electron, and therefore suggested that the beta particle is in fact an electron .
In 1901, Rutherford and Frederick Soddy showed that alpha and beta radioactivity involves the transmutation of atoms into atoms of other chemical elements. In 1913, after the products of more radioactive decays were known, Soddy and Kazimierz Fajans independently proposed their radioactive displacement law, which states that beta (i.e., ) emission from one element produces another element one place to the right in the periodic table, while alpha emission produces an element two places to the left.
Neutrinos in beta decay.
Historically, the study of beta decay provided the first physical evidence of the neutrino. Measurements of the beta particle (electron) kinetic energy spectrum in 1911 by Lise Meitner and Otto Hahn and in 1913 by Jean Danysz showed multiple lines on a diffuse background, offering the first hint of a continuous spectrum. In 1914, James Chadwick used a magnetic spectrometer with one of Hans Geiger's new counters to make a more accurate measurement and showed that the spectrum was continuous. This was in apparent contradiction to the law of conservation of energy, since if beta decay were simply electron emission as assumed at the time, then the energy of the emitted electron should equal the energy difference between the initial and final nuclear states and lead to a narrow energy distribution, as observed for both alpha and gamma decay. For beta decay, however, the observed broad continuous spectrum suggested that energy is lost in the beta decay process.
From 1920–1927, Charles Drummond Ellis (along with James Chadwick and colleagues) further established that the beta decay spectrum is continuous, ending all controversies. It also had an effective upper bound in energy, which was a severe blow to Bohr's suggestion that conservation of energy might be true only in a statistical sense, and might be violated in any given decay. Now the problem of how to account for the variability of energy in known beta decay products, as well as for conservation of momentum and angular momentum in the process, became acute.
A second problem related to the conservation of angular momentum. Molecular band spectra showed that the nuclear spin of nitrogen-14 is 1 (i.e. equal to the reduced Planck constant), and more generally that the spin is integral for nuclei of even mass number and half-integral for nuclei of odd mass number, as later explained by the proton-neutron model of the nucleus. Beta decay leaves the mass number unchanged, so that the change of nuclear spin must be an integer. However the electron spin is 1/2, so that angular momentum would not be conserved if beta decay were simply electron emission.
In a famous letter written in 1930, Wolfgang Pauli suggested that, in addition to electrons and protons, atomic nuclei also contained an extremely light neutral particle, which he called the neutron. He suggested that this "neutron" was also emitted during beta decay (thus accounting for the known missing energy, momentum, and angular momentum) and had simply not yet been observed. In 1931, Enrico Fermi renamed Pauli's "neutron" to neutrino and, in 1934, he published a very successful model of beta decay in which neutrinos were produced. The neutrino interaction with matter was so weak that detecting it proved a severe experimental challenge, which was finally met in 1956 in the Cowan–Reines neutrino experiment. However, the properties of neutrinos were (with a few minor modifications) as predicted by Pauli and Fermi.
Discovery of other types of beta decay.
In 1934, Frédéric and Irène Joliot-Curie bombarded aluminium with alpha particles to effect the nuclear reaction  +  →  + , and observed that the product isotope emits a positron identical to those found in cosmic rays by Carl David Anderson in 1932. This was the first example of  decay (positron emission), which they termed artificial radioactivity since is a short-lived nuclide which does not exist in nature.
The theory of electron capture was first discussed by Gian-Carlo Wick in a 1934 paper, and then developed by Hideki Yukawa and others. K-electron capture was first observed in 1937 by Luis Alvarez, in the nuclide 48V. Alvarez went on to study electron capture in 67Ga and other nuclides.
Non-conservation of parity.
In 1956, Chien-Shiung Wu and coworkers proved in the Wu experiment that parity is not conserved in beta decay. This surprising fact had been postulated shortly before in an article by Tsung-Dao Lee and Chen Ning Yang.

</doc>
<doc id="4652" url="https://en.wikipedia.org/wiki?curid=4652" title="Blitzkrieg">
Blitzkrieg

Blitzkrieg (German, "lightning war") is a method of warfare whereby an attacking force spearheaded by a dense concentration of armoured and motorised or mechanised infantry formations with close air support, breaks through the opponent's line of defence by short, fast, powerful attacks and then dislocates the defenders, using speed and surprise to encircle them. Through the employment of combined arms in manoeuvre warfare, blitzkrieg attempts to unbalance the enemy by making it difficult for it to respond to the continuously changing front and defeating it in a decisive ("battle of annihilation").
During the interwar period, aircraft and tank technologies matured and were combined with systematic application of the traditional German tactic of ("maneuver warfare"), deep penetrations and the bypassing of enemy strong points to encircle and destroy enemy forces in a ("cauldron battle"). During the Invasion of Poland, Western journalists adopted the term "blitzkrieg" to describe this form of armoured warfare. The term had appeared in 1935, in a German military periodical ("German Defense"), in connection to quick or lightning warfare. German manoeuvre operations were successful in the campaigns of 1939–1941 and by 1940 the term "blitzkrieg" was extensively used in Western media. Blitzkrieg operations capitalized on surprise penetrations (e.g., the penetration of the Ardennes forest region), general enemy unreadiness and their inability to match the pace of the German attack. During the Battle of France, the French made attempts to re-form defensive lines along rivers but were frustrated when German forces arrived first and pressed on.
Despite its ubiquity in German and British journalism during World War II, was practically never used as official military terminology of the Wehrmacht during the war. Some senior officers, including Kurt Student, Franz Halder and Johann Adolf von Kielmansegg, even disputed the idea that it was a military concept. Kielmansegg asserted that what many regarded as blitzkrieg was nothing more than "ad hoc solutions that simply popped out of the prevailing situation". Student described it as ideas that "naturally emerged from the existing circumstances" as a response to operational challenges. The Wehrmacht never officially adopt it as a concept or doctrine. In 2005, Karl-Heinz Frieser summarized blitzkrieg as simply the result of German commanders using the latest technology in the most beneficial way according to traditional military principles and employing "the right units in the right place at the right time". Modern historians now understand blitzkrieg as the outcome of the rejuvenation of the traditional German military principles, methods and doctrines of the 19th century with the latest weapon systems of the interwar period.
Despite blitzkrieg never being an official or formal doctrine, many modern historians use it casually to describe the style of manoeuvre warfare practiced by Germany during the early part of the war. In the context of the thinking of Heinz Guderian on mobile combined arms formations, blitzkrieg can be a synonym for modern manoeuvre warfare on the operational level.
Definition.
Common interpretation.
The traditional meaning of blitzkrieg is that of German tactical and operational methodology in the first half of the Second World War, that is often hailed as a new method of warfare. The word, meaning "lightning war", in its strategic sense describes a series of quick and decisive short battles to deliver a knockout blow to an enemy state before it could fully mobilize. Tactically, blitzkrieg is a coordinated military effort by tanks, motorized infantry, artillery and aircraft, to create an overwhelming local superiority in combat power, to defeat the opponent and break through its defenses. "Blitzkrieg" as used by Germany had considerable psychological, or "terror" elements, such as the "Jericho Trompete", a noise-making siren on the Junkers Ju 87 dive-bomber, to affect the morale of enemy forces. The devices were largely removed when the enemy became used to the noise after the Battle of France in 1940 and instead bombs sometimes had whistles attached. It is also common for historians and writers to include psychological warfare by using Fifth columnists to spread rumours and lies among the civilian population in the theatre of operations.
Origin of the term.
The origin of the term "blitzkrieg" is obscure. It was never used in the title of a military doctrine or handbook of the German army or air force, and no "coherent doctrine" or "unifying concept of blitzkrieg". The term seems rarely to have been used in the German military press before 1939 and recent research at the German "Militärgeschichtliches Forschungsamt" at Potsdam found it in only two military articles from the 1930s. Both used the term to mean a swift strategic knock-out, rather than a radical new military doctrine or approach to war. The first article (1935) deals primarily with supplies of food and materiel in wartime. The term "blitzkrieg" is used with reference to German efforts to win a quick victory in the First World War and is not associated with the use of armoured, mechanized or air forces. Germany must develop self-sufficiency in food, because it might again prove impossible to deal a swift knock-out to its enemies, leading to a long total war. In the second article (1938), launching a swift strategic knock-out is described as an attractive idea for Germany but difficult to achieve on land under modern conditions (especially against systems of fortification like the Maginot Line), unless an exceptionally high degree of surprise could be achieved. The author vaguely suggests that a massive strategic air attack might hold out better prospects but the topic is not explored in detail. A third relatively early use of the term in German occurs in "Die Deutsche Kriegsstärke" (German War Strength) by Fritz Sternberg, a Jewish, Marxist, political economist and refugee from the Third Reich, published in Paris in 1938, which was preceded by an English-language edition "Germany and a Lightning War". Sternberg wrote that Germany was not prepared economically for a long war but might win a lightning war. He did not go into detail regarding operations or tactics, nor did he suggest that the German armed forces had evolved a radically new operational method. His book offers scant clues as to how German lightning victories might be won.
In English and other languages, the term had been used since the 1920s. The British press used it to describe the German successes in Poland in September 1939, in the words of one scholar "as a piece of journalistic sensationalism – a buzz-word with which to label the spectacular early successes of the Germans in the Second World War" . It was later applied to the bombing of Britain, particularly London, hence "The Blitz" . The German popular press followed suit only some nine months later, after the fall of France in 1940; hence although the word had been used in German, it was first popularized by British journalism.
Heinz Guderian referred to it as a word coined by the Allies: "as a result of the successes of our rapid campaigns our enemies ... coined the word "Blitzkrieg"". After the German failure in the Soviet Union in 1941, use of the term began to be frowned upon in the Third Reich and Hitler then denied ever using the term, saying in a speech in November 1941, "I have never used the word "Blitzkrieg", because it is a very silly word"; by 1942 he dismissed it as "Italian phraseology".
Military change, 1919–1939.
Germany.
In 1914, German strategic thinking derived from the writings of Carl von Clausewitz (June 1, 1780 – November 16, 1831), Helmuth von Moltke the Elder (26 October 1800 – 24 April 1891) and Alfred von Schlieffen (28 February 1833 – 4 January 1913), who advocated manoeuvre, mass and envelopment to create the conditions for a decisive battle . During the war, generals such as Oskar von Hutier developed tactics to restore manoeuvre on the battlefield. Specialist light infantry ("Sturmtruppen" storm troops) were to exploit soft spots, to make gaps for larger infantry units to advance with heavier weapons and exploit the success, leaving isolated strong points to troops following up. Hutier tactics were combined with short hurricane artillery bombardments using massed artillery devised by Colonel Georg Bruchmüller. Attacks were to rely on speed and surprise rather than on weight of numbers. Hutier-Bruchmüller tactics met with great success in Operation Michael, the spring offensive of 1918 and restored temporarily the war of movement once the Allied trench system had been overrun. The German armies pushed on towards Amiens and then Paris, coming within before supply difficulties and Allied reinforcements halted the advance.
On the Eastern Front, the war did not bog down into trench warfare; German and Russian armies fought a war of manoeuvre over thousands of miles, which gave the German leadership unique experience which the trench-bound Western Allies did not offer. Studies of operations in the East led to the conclusion that small and coordinated forces possessed more combat power than large, uncoordinated forces. After the war, the "Reichswehr" modified Hutier tactics. The commander in chief, Hans von Seeckt, argued that there had been an excessive focus on encirclement and emphasized speed instead. Seeckt inspired a revision of "Bewegungskrieg" (maneuver warfare) thinking and its associated "Auftragstaktik" in which the commander gave his intent and subordinates had discretion to achieve it, within limits determined by the training of an elite officer-corps which made their decisions predictable to other commanders. Delegation increased the tempo of operations, which had great influence on the success of German armies in the early war period.
German operational theories were revised after the First World War. The Treaty of Versailles limited the Reichswehr to a maximum of 100,000 men, making impossible the deployment of mass armies. The German General Staff was abolished by the treaty but continued covertly as the "Truppenamt" (Troop Office), disguised as an administrative body. Committees of veteran staff officers were formed within the "Truppenamt" to evaluate 57 issues of the war. By the time of the Second World War, their reports had led to doctrinal and training publications, including H. Dv. 487, "Führung und Gefecht der verbundenen Waffen" (Command and Battle of the Combined Arms), known as das Fug (1921–23) and "Truppenführung" (1933–34), containing standard procedures for combined-arms warfare. The "Reichswehr" was influenced by its analysis of pre-war German military thought, in particular infiltration tactics, which at the end of the war had seen some breakthroughs on the Western Front and the manoeuvre warfare which dominated the Eastern Front.
Britain.
The British army took lessons from the successful infantry and artillery offensives on the Western Front in late 1918. To obtain the best co-operation between all arms, emphasis was placed on detailed planning, rigid control and adherence to orders. Mechanization of the army was considered a means to avoid mass casualties and indecisive nature of offensives, as part of a combined-arms theory of war. The four editions of "Field Service Regulations" published after 1918 held that only combined-arms operations could create enough fire power to enable mobility on a battlefield. This theory of war also emphasized consolidation, recommending caution against overconfidence and ruthless exploitation.
In the Sinai and Palestine Campaign, operations involved some aspects of what would later be called "blitzkrieg". Key elements in the "blitzkrieg warfare" at the decisive Battle of Megiddo included concentration, surprise and speed; success depended on attacking only in terrain favoring the movement of large formations around the battlefield and tactical improvements in the British artillery and infantry attack. General Edmund Allenby used infantry to attack the strong Ottoman front line in co-operation with supporting artillery, augmented by the guns of two destroyers. Through constant pressure by infantry and cavalry, two Ottoman armies in the Judean Hills were kept off-balance and virtually encircled during the Battles of Sharon and Nablus (Battle of Megiddo).
The British methods induced "strategic paralysis" among the Ottomans and led to their rapid and complete collapse. In an advance of , captures were estimated at "at least prisoners and 260 guns." Liddell Hart considered that important aspects of the operation were the extent to which Ottoman commanders were denied intelligence on the British preparations for the attack through British air superiority and air attacks on their headquarters and telephone exchanges, which paralyzed attempts to react to the rapidly deteriorating situation.
France.
French doctrine in the mid-war years was defence-oriented. Colonel Charles de Gaulle was an advocate of concentration of armour and aeroplanes. His opinions were expressed in his book "Vers l'Armée de Métier" (Towards the Professional Army 1933). Like von Seeckt, he concluded that France could no longer maintain the huge armies of conscripts and reservists with which World War I had been fought, and he sought to use tanks, mechanised forces and aircraft to allow a smaller number of highly trained soldiers to have greater impact in battle. His views little endeared him to the French high command but are claimed by some to have influenced Heinz Guderian.
Russia/USSR.
In 1916, General Alexei Brusilov had used surprise and infiltration tactics during the Brusilov Offensive. Later, Marshal Mikhail Tukhachevsky, Georgii Isserson and other members of the Red Army developed a concept of deep battle from the experience of the Polish–Soviet War. These concepts would guide Red Army doctrine throughout World War II. Realising the limitations of infantry and cavalry, Tukhachevsky was an advocate of mechanised formations and the large-scale industrialisation required. Robert Watt (2008) wrote that blitzkrieg holds little in common with Soviet deep battle. In 2002, H. P. Willmott had noted that deep battle contained two important differences: it was a doctrine of total war, not limited operations, and decisive battle was rejected in favour of several large, simultaneous offensives.
The "Reichswehr" and the Red Army began a secret collaboration in the Soviet Union to evade the Treaty of Versailles occupational agent, the Inter-Allied Commission. In 1926, War games and tests were begun at Kazan and Lipetsk. The centres were used to field test aircraft and armoured vehicles up to the battalion level and housed aerial and armoured warfare schools, through which officers were rotated.
Nazi Germany.
After becoming head of government in 1933, Adolf Hitler ignored the Versailles Treaty provisions. A command for armored forces was created within the German Wehrmacht: the "Panzerwaffe", as it came to be known later. The Luftwaffe (the German air force) was established, and development began on ground-attack aircraft and doctrines. Hitler was a strong supporter of this new strategy. He read Guderian's book "Achtung – Panzer!" and upon observing armoured field exercises at Kummersdorf he remarked, "That is what I want – and that is what I will have."
Guderian.
Guderian summarised combined-arms tactics as the way to get the mobile and motorised armoured divisions to work together and support each other to achieve decisive success. In his book, "Panzer Leader", he wrote:
Guderian believed that developments in technology were required to support the theory; especially, equipping armoured divisions—tanks
foremost–with wireless communications. Guderian insisted in 1933 to the high command that every tank in the German armoured force must be equipped with a radio. At the start of the war, only the German army was thus prepared with all tanks "radio equipped". This proved critical in early tank battles where German tank commanders exploited the organizational advantage over the Allies that radio communication gave them. Later all Allied armies would copy this innovation.
Methods of operations.
Schwerpunkt.
The Germans referred to a "Schwerpunkt" (focal point) and to a "Schwerpunktprinzip" (concentration principle) in the planning of operations. They viewed the "Schwerpunkt" as a centre of gravity or point of maximum effort, where a decisive result could be achieved. Mechanised and tactical air forces were concentrated at this point of maximum effort regardless of the sacrifices it made necessary elsewhere. By local success at the "Schwerpunkt", a small force could achieve a breakthrough and gain advantages by fighting in the enemy rear. Guderian summarised this as "Klotzen, nicht kleckern!" (approx. "Get in there and do it! Don't mess about!").
To achieve a breakthrough, armoured forces would attack an opposing defensive line frontally, supported by motorised infantry, artillery fire and aerial bombardment, to create a breach. Tanks and other motorised units could break out of the fortified zone without the encumbrance of slow-moving infantry moving on foot. Air forces began by seeking air superiority by attacking aircraft on the ground, bombing airfields and communications and seeking tactically advantageous positions from which to destroy them in the air. The principle of "Schwerpunktbildung" enabled the attacker to win numerical superiority at the point of main effort, which in turn gave the attacker tactical and operational superiority, even though the attacker may be numerically and strategically inferior in general.
Pursuit.
Having achieved a breakthrough of the enemy's line, units comprising the "Schwerpunkt" were not supposed to become decisively engaged with enemy front line units to the right and left of the breakthrough area. Units pouring through the hole were to drive upon set objectives behind the enemy front line. In World War II, German Panzer forces used motorised mobility, to paralyse the opponent's ability to react. Fast-moving mobile forces seized the initiative, exploited weaknesses and acted before opposing forces could respond. Central to this was the decision cycle (tempo). Decision-making required time to gather information, make a decision, give orders to subordinates to implement the decision. Through superior mobility and faster decision-making cycles, mobile forces could act quicker than the forces opposing them. Directive control was a fast and flexible method of command. Rather than receiving an explicit order, a commander would be told of his superior's intent and the role which his unit was to fill in this concept. The method of execution was then a matter for the discretion of the subordinate commander. Staff burden was reduced at the top and spread among tiers of command with knowledge about their situation. Delegation and the encouragement of initiative aided implementation, important decisions could be taken quickly and communicated verbally or with brief written orders.
Mopping-up.
The last part of an offensive operation was the destruction of un-subdued pockets of resistance, which had been enveloped earlier and by-passed by the fast-moving armoured and motorised spearheads. The "Kesselschlacht", (cauldron battle), was a concentric attack on such pockets. It was here that most losses were inflicted upon the enemy, primarily through the mass capture of prisoners and weapons. During Barbarossa, huge encirclements in 1941 produced nearly 3.5 million Soviet prisoners along with masses of equipment.
Air power.
Close air support was provided in the form of the dive bomber and medium bomber. They would support the focal point of attack from the air. German successes are closely related to the extent to which the German "Luftwaffe" was able to control the air war in early campaigns in Europe and the Soviet Union. However, the "Luftwaffe" was a broadly based force with no constricting central doctrine, other than its resources should be used generally to support national strategy. It was flexible and it was able to carry out both operational-tactical, and strategic bombing. Flexibility was the "Luftwaffe's" strength in 1939–1941. Paradoxically, from that period onward it became its weakness. While Allied Air Forces were tied to the support of the Army, the "Luftwaffe" deployed its resources in a more general, operational way. It switched from air superiority missions, to medium-range interdiction, to strategic strikes, to close support duties depending on the need of the ground forces. In fact, far from it being a specialist panzer spearhead arm, fewer than 15 percent of the Luftwaffe was intended for close support of the army in 1939.
Limitations and countermeasures.
Environment.
The concepts associated with the term "blitzkrieg"—deep penetrations by armour, large encirclements, and combined arms attacks—were largely dependent upon terrain and weather conditions. Where the ability for rapid movement across "tank country" was not possible, armoured penetrations were often avoided or resulted in failure. Terrain would ideally be flat, firm, unobstructed by natural barriers or fortifications, and interspersed with roads and railways. If it was instead hilly, wooded, marshy, or urban, armour would be vulnerable to infantry in close-quarters combat and unable to break out at full speed. Additionally, units could be halted by mud (thawing along the Eastern Front regularly slowed both sides) or extreme snow. Armour, motorised and aerial support was also naturally dependent on weather.
It should however be noted that the disadvantages of such terrain could be nullified if surprise was achieved over the enemy by an attack through such terrain. During the Battle of France, the German blitzkrieg-style attack on France went through the Ardennes. There is little doubt that the hilly, heavily wooded Ardennes could have been relatively easily defended by the Allies, even against the bulk of the German armoured units. However, precisely because the French thought the Ardennes unsuitable for massive troop movement, particularly for tanks, they were left with only light defences which were quickly overrun by the "Wehrmacht". The Germans quickly advanced through the forest, knocking down the trees the French thought would impede this tactic.
Air superiority.
The influence of air forces over forces on the ground changed significantly over the course of the Second World War. Early German successes were conducted when Allied aircraft could not make a significant impact on the battlefield. In May 1940, there was near parity in numbers of aircraft between the Luftwaffe and the Allies, but the Luftwaffe had been developed to support Germany's ground forces, had liaison officers with the mobile formations, and operated a higher number of sorties per aircraft. In addition, German air parity or superiority allowed the unencumbered movement of ground forces, their unhindered assembly into concentrated attack formations, aerial reconnaissance, aerial resupply of fast moving formations and close air support at the point of attack. The Allied air forces had no close air support aircraft, training or doctrine. The Allies flew 434 French and 160 British sorties a day but methods of attacking ground targets had yet to be developed; therefore Allied aircraft caused negligible damage. Against these 600 sorties the Luftwaffe on average flew 1,500 sorties a day. On May 13, "Fliegerkorps" VIII flew 1,000 sorties in support of the crossing of the Meuse. The following day the Allies made repeated attempts to destroy the German pontoon bridges, but German fighter aircraft, ground fire and Luftwaffe flak batteries with the panzer forces destroyed 56 percent of the attacking Allied aircraft while the bridges remained intact.
Allied air superiority became a significant hindrance to German operations during the later years of the war. By June 1944 the Western Allies had complete control of the air over the battlefield and their fighter-bomber aircraft were very effective at attacking ground forces. On D-Day the Allies flew 14,500 sorties over the battlefield area alone, not including sorties flown over north-western Europe. Against this on 6 June the Luftwaffe flew some 300 sorties. Though German fighter presence over Normandy increased over the next days and weeks, it never approached the numbers the Allies commanded. Fighter-bomber attacks on German formations made movement during daylight almost impossible. Subsequently shortages soon developed in food, fuel and ammunition, severely hampering the German defenders. German vehicle crews and even flak units experienced great difficulty moving during daylight. Indeed, the final German offensive operation in the west, Operation Wacht am Rhein, was planned to take place during poor weather to minimize interference by Allied aircraft. Under these conditions it was difficult for German commanders to employ the "armoured idea", if at all.
Counter-tactics.
Blitzkrieg is vulnerable to an enemy that is robust enough to weather the shock of the attack and that does not panic at the idea of enemy formations in its rear area. This is especially true if the attacking formation lacks the reserve to keep funnelling forces into the spearhead, or lacks the mobility to provide infantry, artillery and supplies into the attack. If the defender can hold the shoulders of the breach they will have the opportunity to counter-attack into the flank of the attacker, potentially cutting off the van as happened to Kampfgruppe Peiper in the Ardennes.
During the Battle of France in 1940, the 4th Armoured Division (Major-General Charles de Gaulle) and elements of the 1st Army Tank Brigade (British Expeditionary Force) made probing attacks on the German flank, pushing into the rear of the advancing armoured columns at times. This may have been a reason for Hitler to call a halt to the German advance. Those attacks combined with Maxime Weygand's Hedgehog tactic would become the major basis for responding to blitzkrieg attacks in the future: deployment in depth, permitting enemy or "shoulders" of a penetration was essential to channelling the enemy attack, and artillery, properly employed at the shoulders, could take a heavy toll of attackers. While Allied forces in 1940 lacked the experience to successfully develop these strategies, resulting in France's capitulation with heavy losses, they characterised later Allied operations. At the Battle of Kursk the Red Army employed a combination of defence in great depth, extensive minefields, and tenacious defence of breakthrough shoulders. In this way they depleted German combat power even as German forces advanced. The reverse can be seen in the Russian summer offensive of 1944. German attempts to weather the storm and fight out of encirclements failed due to the Russian ability to continue to feed armoured units into the attack, maintaining the mobility and strength of the offensive, arriving in force deep in the rear areas, faster than the Germans could regroup and resulted in the destruction of Army Group Center in Operation Bagration.
Logistics.
Although effective in quick campaigns against Poland and France, mobile operations could not be sustained by Germany in later years. Strategies based on manoeuvre have the inherent danger of the attacking force overextending its supply lines, and can be defeated by a determined foe who is willing and able to sacrifice territory for time in which to regroup and rearm, as the Soviets did on the Eastern Front (as opposed to, for example, the Dutch who had no territory to sacrifice). Tank and vehicle production was a constant problem for Germany; indeed, late in the war many panzer "divisions" had no more than a few dozen tanks. As the end of the war approached, Germany also experienced critical shortages in fuel and ammunition stocks as a result of Anglo-American strategic bombing and blockade. Although production of "Luftwaffe" fighter aircraft continued, they would be unable to fly for lack of fuel. What fuel there was went to panzer divisions, and even then they were not able to operate normally. Of those Tiger tanks lost against the United States Army, nearly half of them were abandoned for lack of fuel.
Military operations.
Spanish Civil War.
German volunteers first used armour in live field conditions during the Spanish Civil War of 1936. Armour commitment consisted of Panzer Battalion 88, a force built around three companies of Panzer I tanks that functioned as a training cadre for Nationalists. The Luftwaffe deployed squadrons of fighters, dive bombers and transport aircraft as the "Condor Legion". Guderian said that the tank deployment was "on too small a scale to allow accurate assessments to be made." The true test of his "armoured idea" would have to wait for the Second World War. However, the "Luftwaffe" also provided volunteers to Spain to test both tactics and aircraft in combat, including the first combat use of the "Stuka".
During the war, the "Condor Legion" undertook the bombing of Guernica which had a tremendous psychological effect on the populations of Europe. The results were exaggerated, and the Western Allies concluded that the "city-busting" techniques were now a part of the German way in war. The targets of the German aircraft were actually the rail lines and bridges. But lacking the ability to hit them with accuracy (only three or four Ju 87s saw action in Spain), a method of carpet bombing was chosen resulting in heavy civilian casualties.
Poland, 1939.
Despite the term "blitzkrieg" being coined by journalists during the Invasion of Poland of 1939, historians Matthew Cooper and J. P. Harris have written that German operations during it were consistent with traditional methods. The Wehrmacht strategy was more in line with "Vernichtungsgedanken" a focus on envelopment to create pockets in broad-front annihilation. Panzer forces were dispersed among the three German concentrations with little emphasis on independent use, being used to create or destroy close pockets of Polish forces and seize operational-depth terrain in support of the largely un-motorized infantry which followed.
While early German tanks, Stuka dive-bombers and concentrated forces were used in the Polish campaign, the majority of the battle was conventional infantry and artillery warfare and most Luftwaffe action was independent of the ground campaign. Matthew Cooper wrote that
John Ellis wrote that "...there is considerable justice in Matthew Cooper's assertion that the panzer divisions were not given the kind of "strategic" mission that was to characterize authentic armoured blitzkrieg, and were almost always closely subordinated to the various mass infantry armies." Steven Zaloga wrote, "Whilst Western accounts of the September campaign have stressed the shock value of the panzer and Stuka attacks, they have tended to underestimate the punishing effect of German artillery on Polish units. Mobile and available in significant quantity, artillery shattered as many units as any other branch of the Wehrmacht."
Western Europe, 1940.
The German invasion of France, with subsidiary attacks on Belgium and the Netherlands, consisted of two phases, Operation Yellow ("Fall Gelb") and Operation Red ("Fall Rot"). Yellow opened with a feint conducted against the Netherlands and Belgium by two armoured corps and paratroopers. Most of the German armoured forces were placed in Panzer Group von Kleist, which attacked through the Ardennes, a lightly defended sector that the French planned to reinforce if need be, before the Germans could bring up heavy and siege artillery. There was no time for such a reinforcement to be sent, for the Germans did not wait for siege artillery but reached the Meuse and achieved a breakthrough at the Battle of Sedan in three days.
The group raced to the English Channel, reaching the coast at Abbeville and cut off the BEF, the Belgian Army and some of the best-equipped divisions of the French Army in northern France. Armoured and motorised units under Guderian, Rommel and others, advanced far beyond the marching and horse-drawn infantry divisions and far in excess of that with which Hitler and the German high command expected or wished. When the Allies counter-attacked at Arras using the heavily armoured British Matilda I and Matilda II tanks, a brief panic was created in the German High Command. The armoured and motorised forces were halted by Hitler outside the port of Dunkirk, which was being used to evacuate the Allied forces. Hermann Göring promised that the Luftwaffe would complete the destruction of the encircled armies but aerial operations failed to prevent the evacuation of the majority of the Allied troops. In Operation Dynamo some French and British troops escaped.
Case Yellow surprised everyone, overcoming the Allies' 4,000 armoured vehicles, many of which were better than German equivalents in armour and gun-power. The French and British frequently used their tanks in the dispersed role role of infantry support rather than concentrating force at the point of attack, to create overwhelming firepower.
The French armies were much reduced in strength and the confidence of their commanders shaken. With much of their own armour and heavy equipment lost in Northern France, they lacked the means to fight a mobile war. The Germans followed their initial success with Operation Red, a triple-pronged offensive. The XV Panzer Corps attacked towards Brest, XIV Panzer Corps attacked east of Paris, towards Lyon and the XIX Panzer Corps encircled the Maginot Line. The French were hard pressed to organise any sort of counter-attack and were continually ordered to form new defensive lines and found that German forces had already by-passed them and moved on. An armoured counter-attack organised by Colonel de Gaulle could not be sustained and he had to retreat.
Prior to the German offensive in May, Winston Churchill had said "Thank God for the French Army". That same French army collapsed after barely two months of fighting. This was in shocking contrast to the four years of trench warfare they had engaged in during the First World War. The French president of the Ministerial Council, Reynaud, attributed the collapse in a speech on 21 May 1940:
The Germans had not used paratroop attacks in France and only made one big drop in the Netherlands, to capture three bridges; some small glider-landings were conducted in Belgium to tank bottle-necks on routes of advance before the arrival of the main force (the most renowned being the landing on Fort Eben-Emael in Belgium).
Eastern Front, 1941–44.
Use of armoured forces was crucial for both sides on the Eastern Front. Operation Barbarossa, the German invasion of the Soviet Union in 1941, involved a number of breakthroughs and encirclements by motorised forces. Its goal according to Führer Directive 21 (18 December 1940) was "to destroy the Russian forces deployed in the West and to prevent their escape into the wide-open spaces of Russia." The Red Army was to be destroyed west of the Dvina and Dnieper rivers, which were about east of the Soviet border, to be followed by a mopping-up operation. The surprise attack resulted in the near annihilation of the Voyenno-Vozdushnye Sily (VVS, Soviet Air Force) by simultaneous attacks on airfields, allowing the Luftwaffe to achieve total air supremacy over all the battlefields within the first week. On the ground, four German panzer groups outflanked and encircled disorganised Red Army units, while the marching infantry completed the encirclements and defeated the trapped forces. In late July, after 2nd Panzer Group (commanded by Guderian) captured the watersheds of the Dvina and Dnieper rivers near Smolensk, the panzers had to defend the encirclement, because the marching infantry divisions were still hundreds of kilometres to the west.
The Germans conquered large areas of the Soviet Union but their failure to destroy the Red Army before the winter of 1941 was a strategic failure that made German tactical superiority and territorial gains irrelevant. The Red Army had survived enormous losses and regrouped with new formations far to the rear of the front line. During the Battle of Moscow, the Red Army defeated the German Army Group Center and for the first time in the war seized the strategic initiative.
In the summer of 1942, Germany launched another offensive in the southern USSR against Stalingrad and the Caucasus, the Soviets again lost tremendous amounts of territory, only to counter-attack once more during winter. German gains were ultimately limited by Hitler diverting forces from the attack on Stalingrad and driving towards the Caucasus oilfields simultaneously. The "Wehrmacht" became overstretched, although winning operationally, it could not inflict a decisive defeat as the durability of the Soviet Union's manpower, resources, industrial base and aid from the Western Allies began to take effect.
In July 1943 the "Wehrmacht" conducted Operation (Citadel) against a salient at Kursk that was heavily defended by Soviet troops. Soviet defensive tactics were by now hugely improved, particularly in the use of artillery and air support. By April 1943, the Stavka had learned of German intentions through intelligence supplied by front line reconnaissance and Ultra intercepts. In the following months, the Red Army constructed deep defensive belts along the paths of the planned German attack. The Soviets made a concerted effort to disguise their knowledge of German plans and the extent of their own defensive preparations, and the German commanders still hoped to achieve operational surprise when the attack commenced.
The Germans did not achieve surprise and were not able to outflank or break through into enemy rear areas during the operation. Several historians assert that Operation Citadel was planned and intended to be a blitzkrieg operation. Many of the German participants who wrote about the operation after the war, including Manstein, make no mention of blitzkrieg in their accounts. In 2000, Niklas Zetterling and Anders Frankson characterised only the southern pincer of the German offensive as a "classical blitzkrieg attack". Pier Battistelli wrote that the operational planning marked a change in German offensive thinking away from blitzkrieg and that more priority was given to brute force and fire power than to speed and manoeuvre.
In 1995, David Glantz stated that for the first time, blitzkrieg was defeated in summer and the opposing Soviet forces were able to mount a successful counter-offensive. The Battle of Kursk ended with two Soviet counter-offensives and the revival of deep operations. In the summer of 1944, the Red Army destroyed Army Group Centre in Operation Bagration, using combined-arms tactics for armour, infantry and air power in a coordinated strategic assault, known as deep operations, which led to an advance of in six weeks.
Western Front, 1944–45.
Allied armies began using combined arms formations and deep penetration strategies that Germany had used in the opening years of the war. Many Allied operations in the Western Desert and on the Eastern Front, relied on firepower to establish breakthroughs by fast-moving armoured units. These artillery-based tactics were also decisive in Western Front operations after Operation Overlord and the British Commonwealth and American armies developed flexible and powerful systems for using artillery support. What the Soviets lacked in flexibility, they made up for in number of rocket launchers, guns and mortars. The Germans never achieved the kind of fire concentrations their enemies were capable of by 1944.
After the Allied landings at Normandy, the Germans began a counter-offensive to overwhelm the landing force with armoured attacks but these failed for lack of co-ordination and Allied superiority on anti-tank defence and air superiority. The most notable attempt to use deep penetration operations in Normandy was Operation Luttich at Mortain, which only hastened the Falaise Pocket and the destruction of German forces in Normandy. The Mortain counter-attack was defeated by the US 12th Army Group with little effect on its own offensive operations.
The last German offensive on the Western front, the Battle of the Bulge (Operation "Wacht am Rhein"), was an offensive launched towards the port of Antwerp in December 1944. Launched in poor weather against a thinly held Allied sector, it achieved surprise and initial success as Allied air power was grounded by cloud cover. Determined defence by US troops in places throughout the Ardennes, the lack of good roads German supply shortages caused delays. Allied forces deployed to the flanks of the German penetration and as soon as the skies cleared, Allied aircraft returned to the battlefield. Allied counter-attacks soon forced back the Germans, who abandoned much equipment for lack of fuel.
Post-war controversy.
The origins of blitzkrieg are in some doubt: if it existed, who contributed to it, whether it was part of German war strategy from 1933–1939. There has been a great deal of debate about whether it existed as a coherent military strategy. Many historians now think that blitzkrieg was not a military theory and the campaigns conducted by the Germans from 1939 to circa 1942 (with the exception of Operation Barbarossa) were improvised, rather than being based on a particular military strategy. Blitzkrieg had been called a Revolution in Military Affairs (RMA) but many writers and historians have concluded that the Germans did not invent a new form of warfare but applied new technologies to traditional ideas of "Bewegungskrieg" (manoeuvre warfare) to achieve decisive victory.
Strategy.
In 1965, Captain Robert O'Neill, Professor of the History of War at the Oxford University produced an example of the popular view. In "Doctrine and Training in the German Army 1919–1939", O'Neill wrote
other historians wrote that blitzkrieg was an operational doctrine of the German armed forces and a strategic concept on which the leadership of the "Third Reich" based its strategic and economic planning. Military planners and bureaucrats in the war economy appear rarely, if ever, to have employed the term "blitzkrieg" in official documents. That the German army had a "blitzkrieg doctrine" was rejected in the late 1970s by Matthew Cooper. The concept of a blitzkrieg "Luftwaffe" was challenged by Richard Overy in the late 1970s and by Williamson Murray in the mid-1980s. That the "Third Reich" went to war on the basis of "blitzkrieg economics" was criticised by Richard Overy in the 1980s and George Raudzens described the contradictory senses in which historians have used the word. The notion of a German blitzkrieg concept or doctrine survives in popular history and many historians still support the thesis.
Frieser wrote that after the failure of the Schlieffen Plan in 1914, the German army concluded that decisive battles were no longer possible in the changed conditions of the twentieth century. Frieser wrote that the Oberkommando der Wehrmacht (OKW), which was created in 1938 had intended to avoid the decisive battle concepts of its predecessors and planned for a long war of exhaustion ("ermattungskrieg"). It was only after the improvised plan for the Battle of France in 1940 was unexpectedly successful, that the German General Staff came to believe that "vernichtungskrieg" was still feasible. German thinking reverted to the possibility of a quick and decisive war for the Balkan Campaign and Operation Barbarossa.
Doctrine.
Most academic historians regard the notion of blitzkrieg as military doctrine to be a myth. Shimon Naveh wrote "The striking feature of the blitzkrieg concept is the complete absence of a coherent theory which should have served as the general cognitive basis for the actual conduct of operations". Naveh described it as an "ad hoc solution" to operational dangers, thrown together at the last moment. Overy disagreed with the idea that Hitler and the Nazi regime ever intended a blitzkrieg war, because the once popular belief that the Nazi state organised their economy to carry out its grand strategy in short campaigns was false. Hitler had intended for a rapid unlimited war to occur much later than 1939, but the "Third Reich's" aggressive foreign policy forced the Nazi state into war before it was ready. Hitler and the "Wehrmacht's" planning in the 1930s did not reflect a blitzkrieg method but the opposite. John Harris wrote that the Wehrmacht never used the word, and it did not appear in German army or air force field manuals; the word was coined in September 1939, by a "Times" newspaper reporter. Harris also found no evidence that German military thinking developed a blitzkrieg mentality. Karl-Heinz Frieser and Adam Tooze reached similar conclusions to Overy and Naveh, that the notions of blitzkrieg-economy and strategy were myths. Frieser wrote that surviving German economists and General Staff officers denied that Germany went to war with a blitzkrieg strategy.
Economics.
In the 1960s, Alan Milward developed a theory of blitzkrieg economics, that Germany could not fight a long war and chose to avoid comprehensive rearmament and armed in breadth, to win quick victories. Milward described an economy positioned between a full war economy and a peacetime economy. The purpose of the blitzkrieg economy was to allow the German people to enjoy high living standards in the event of hostilities and avoid the economic hardships of the First World War.
Overy wrote that blitzkrieg as a "coherent military and economic concept has proven a difficult strategy to defend in light of the evidence". Milward's theory was contrary to Hitler's and German planners' intentions. The Germans, aware of the errors of the First World War, rejected the concept of organising its economy to fight only a short war. Therefore, focus was given to the development of armament in depth for a long war, instead of armament in breadth for a short war. Hitler claimed that relying on surprise alone was "criminal" and that "we have to prepare for a long war along with surprise attack". During the winter of 1939–40, Hitler demobilised many troops from the army to return as skilled workers to factories because the war would be decided by production, not a quick "Panzer operation".
In the 1930s, Hitler had ordered rearmament programs that cannot be considered limited. In November 1937 Hitler had indicated that most of the armament projects would be completed by 1943–45. The rearmament of the "Kriegsmarine" was to have been completed in 1949 and the "Luftwaffe" rearmament program was to have matured in 1942, with a force capable of strategic bombing with heavy bombers. The construction and training of motorised forces and a full mobilisation of the rail networks would not begin until 1943 and 1944 respectively. Hitler needed to avoid war until these projects were complete but his misjudgements in 1939 forced Germany into war before rearmament was complete.
After the war, Albert Speer claimed that the German economy achieved greater armaments output, not because of diversions of capacity from civilian to military industry but through streamlining of the economy. Richard Overy pointed out some 23 percent of German output was military by 1939. Between 1937 and 1939, 70 percent of investment capital went into the rubber, synthetic fuel, aircraft and shipbuilding industries. Hermann Göring had consistently stated that the task of the Four Year Plan was to rearm Germany for total war. Hitler's correspondence with his economists also reveals that his intent was to wage war in 1943–1945, when the resources of central Europe had been absorbed into the "Third Reich".
Living standards were not high in the late 1930s. Consumption of consumer goods had fallen from 71 percent in 1928 to 59 percent in 1938. The demands of the war economy reduced the amount of spending in non-military sectors to satisfy the demand for the armed forces. On 9 September, Göring as Head of the "Reich Defence Council", called for complete "employment" of living and fighting power of the national economy for the duration of the war. Overy presents this as evidence that a "blitzkrieg economy" did not exist.
Adam Tooze wrote that the German economy was being prepared for a long war. The expenditure for this war was extensive and put the economy under severe strain. The German leadership were concerned less with how to balance the civilian economy and the needs of civilian consumption but to figure out how to best prepare the economy for total war. Once war had begun, Hitler urged his economic experts to abandon caution and expend all available resources on the war effort but the expansion plans only gradually gained momentum in 1941. Tooze wrote that the huge armament plans in the pre-war period did not indicate any clear-sighted blitzkrieg economy or strategy.
"Heer".
Frieser wrote that the () was not ready for blitzkrieg at the start of the war. A blitzkrieg method called for a young, highly skilled mechanised army. In 1939–40, 45 percent of the army was 40 years old and 50 percent of the soldiers had only a few weeks' training. The German army, contrary to the blitzkrieg legend, was not fully motorised and had only 120,000 vehicles, compared to the 300,000 of the French Army. The British also had an "enviable" contingent of motorised forces. Thus, "the image of the German 'Blitzkrieg' army is a figment of propaganda imagination". During the First World War the German army used 1.4 million horses for transport and in the Second World War used 2.7 million horses; only ten percent of the army was motorised in 1940.
Half of the German divisions available in 1940 were combat ready but less well-equipped than the British and French or the Imperial German Army of 1914. In the spring of 1940, the German army was semi-modern, in which a small number of well-equipped and "elite" divisions were offset by many second and third rate divisions". In 2003, John Mosier wrote that while the French soldiers in 1940 were better trained than German soldiers, as were the Americans later and that the German army was the least mechanised of the major armies, its leadership cadres were larger and better and that the high standard of leadership was the main reason for the successes of the German army in World War II, as it had been in World War I.
"Luftwaffe".
James Corum wrote that it was a myth that the "Luftwaffe" had a doctrine of terror bombing, in which civilians were attacked to break the will or aid the collapse of an enemy, by the "Luftwaffe" in "Blitzkrieg" operations. After the bombing of Guernica in 1937 and the Rotterdam Blitz in 1940, it was commonly assumed that terror bombing was a part of "Luftwaffe" doctrine. During the interwar period the "Luftwaffe" leadership rejected the concept of terror bombing in favour of battlefield support and interdiction operations.
Corum continues: General Walther Wever compiled a doctrine known as "The Conduct of the Aerial War". This document, which the "Luftwaffe" adopted, rejected Giulio Douhet's theory of terror bombing. Terror bombing was deemed to be "counter-productive", increasing rather than destroying the enemy's will to resist. Such bombing campaigns were regarded as diversion from the "Luftwaffe's" main operations; destruction of the enemy armed forces. The bombings of Guernica, Rotterdam and Warsaw were tactical missions in support of military operations and were not intended as strategic terror attacks.
J. P. Harris wrote that most Luftwaffe leaders from Goering through the general staff believed (as did their counterparts in Britain and the United States) that strategic bombing was the chief mission of the air force and that given such a role, the Luftwaffe would win the next war and that
The Luftwaffe did end up with an air force consisting mainly of relatively short-range aircraft, but this does not prove that the German air force was solely interested in ’tactical’ bombing. It happened because the German aircraft industry lacked the experience to build a long-range bomber fleet quickly, and because Hitler was insistent on the very rapid creation of a numerically large force. It is also significant that Germany's position in the centre of Europe to a large extent obviated the need to make a clear distinction between bombers suitable only for ’tactical’ and those necessary for strategic purposes in the early stages of a likely future war.
Fuller and Liddell Hart.
British theorists John Frederick Charles Fuller and Captain Basil Henry Liddell Hart have often been associated with the development of blitzkrieg, though this is a matter of controversy. In recent years historians have uncovered that Liddell Hart distorted and falsified facts to make it appear as if his ideas were adopted. After the war Liddell Hart imposed his own perceptions, after the event, claiming that the mobile tank warfare practised by the "Wehrmacht" was a result of his influence. By manipulation and contrivance, Liddell Hart distorted the actual circumstances of the blitzkrieg formation, and he obscured its origins. Through his indoctrinated idealisation of an ostentatious concept, he reinforced the myth of blitzkrieg. By imposing, retrospectively, his own perceptions of mobile warfare upon the shallow concept of blitzkrieg, he "created a theoretical imbroglio that has taken 40 years to unravel." Blitzkrieg was not an official doctrine and historians in recent times have come to the conclusion that it did not exist as such.
The early 1950s literature transformed blitzkrieg into a historical military doctrine, which carried the signature of Liddell Hart and Guderian. The main evidence of Liddell Hart's deceit and "tendentious" report of history can be found in his letters to Erich von Manstein, Heinz Guderian and the relatives and associates of Erwin Rommel. Liddell Hart, in letters to Guderian, "imposed his own fabricated version of blitzkrieg on the latter and compelled him to proclaim it as original formula". Kenneth Macksey found Liddell Hart's original letters to Guderian in the General's papers, requesting that Guderian give him credit for "impressing him" with his ideas of armoured warfare. When Liddell Hart was questioned about this in 1968 and the discrepancy between the English and German editions of Guderian's memoirs, "he gave a conveniently unhelpful though strictly truthful reply. ('There is nothing about the matter in my file of correspondence with Guderian himself except...that I thanked him...for what he said in that additional paragraph'.)".
During World War I, Fuller had been a staff officer attached to the new tank corps. He developed Plan 1919 for massive, independent tank operations, which he claimed were subsequently studied by the German military. It is variously argued that Fuller's wartime plans and post-war writings were an inspiration or that his readership was low and German experiences during the war received more attention. The German view of themselves as the losers of the war, may be linked to the senior and experienced officers' undertaking a thorough review, studying and rewriting of all their Army doctrine and training manuals.
Fuller and Liddell Hart were "outsiders": Liddell Hart was unable to serve as a soldier after 1916 after being gassed on the Somme and Fuller's abrasive personality resulted in his premature retirement in 1933. Their views had limited impact in the British army; the War Office permitted the formation of an Experimental Mechanized Force on 1 May 1927, composed of tanks, lorried infantry, self-propelled artillery and motorised engineers but the force was disbanded in 1928 on the grounds that it had served its purpose. A new experimental brigade was intended for the next year and became a permanent formation in 1933, during the cuts of the financial years.
Continuity.
It has been argued that blitzkrieg was not new; the Germans did not invent something called blitzkrieg in the 1920s and 1930s. Rather the German concept of wars of movement and concentrated force were seen in wars of Prussia and the German wars of unification. The first European general to introduce rapid movement, concentrated power and integrated military effort was Swedish King Gustavus Adolphus during the Thirty Years' War. The appearance of the aircraft and tank in the First World War, called an RMA, offered the German military a chance to get back to the traditional war of movement as practised by Moltke the Elder. The so-called "blitzkrieg campaigns" of 1939 – circa 1942, were well within that operational context.
At the outbreak of war, the German army had no radically new theory of war. The operational thinking of the German army had not changed significantly since the First World War or since the late 19th century. J. P. Harris and Robert M. Citino point out that the Germans had always had a marked preference for short, decisive campaigns – but were unable to achieve short-order victories in First World War conditions. The transformation from the stalemate of the First World War into tremendous initial operational and strategic success in the Second, was partly the employment of a relatively small number of mechanised divisions, most importantly the Panzer divisions, and the support of an exceptionally powerful air force.
Guderian.
Heinz Guderian is widely regarded as being highly influential in developing the military methods of warfare used by Germany's tank men at the start of the Second World War. This style of warfare brought manoeuvre back to the fore, and placed an emphasis on the offensive. This style, along with the shockingly rapid collapse in the armies that opposed it, came to be branded as blitzkrieg warfare.
Following Germany's military reforms of the 1920s, Heinz Guderian emerged as a strong proponent of mechanised forces. Within the Inspectorate of Transport Troops, Guderian and colleagues performed theoretical and field exercise work. Guderian met with opposition from some in the General Staff, who were distrustful of the new weapons and who continued to view the infantry as the primary weapon of the army. Among them, Guderian claimed, was Chief of the General Staff Ludwig Beck (1935–38), whom he alleged was sceptical that armoured forces could be decisive. This claim has been disputed by later historians. James Corum wrote:
By Guderian's account he single handedly created the German tactical and operational methodology. Between 1922 and 1928 Guderian wrote a number of articles concerning military movement. As the ideas of making use of the combustible engine in a protected encasement to bring mobility back to warfare developed in the German army, Guderian was a leading proponent of the formations that would be used for this purpose. He was later asked to write an explanatory book, which was titled "Achtung Panzer!" (1937). In it he explained the theories of the tank men and defended them.
Guderian argued that the tank would be the decisive weapon of the next war. "If the tanks succeed, then victory follows", he wrote. In an article addressed to critics of tank warfare, he wrote "until our critics can produce some new and better method of making a successful land attack other than self-massacre, we shall continue to maintain our beliefs that tanks—properly employed, needless to say—are today the best means available for land attack." Addressing the faster rate at which defenders could reinforce an area than attackers could penetrate it during the First World War, Guderian wrote that "since reserve forces will now be motorized, the building up of new defensive fronts is easier than it used to be; the chances of an offensive based on the timetable of artillery and infantry co-operation are, as a result, even slighter today than they were in the last war." He continued, "We believe that by attacking with tanks we can achieve a higher rate of movement than has been hitherto obtainable, and—what is perhaps even more important—that we can keep moving once a breakthrough has been made." Guderian additionally required that tactical radios be widely used to facilitate co-ordination and command by having one installed in all tanks.
Guderian's leadership was supported, fostered and institutionalised by his supporters in the Reichswehr General Staff system, which worked the Army to greater and greater levels of capability through massive and systematic Movement Warfare war games in the 1930s. Guderian's book incorporated the work of theorists such as Ludwig Ritter von Eimannsberger, whose book, "The Tank War" ("Der Kampfwagenkrieg") (1934) gained a wide audience in the German army. Another German theorist, Ernst Volckheim, wrote a huge amount on tank and combined arms tactics and was influential to German thinking on the use of armoured formations but his work was not acknowledged in Guderian's writings.

</doc>
