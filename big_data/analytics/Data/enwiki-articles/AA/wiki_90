<doc id="5976" url="https://en.wikipedia.org/wiki?curid=5976" title="Capoeira">
Capoeira

Capoeira (; ) is a Brazilian martial art originating in Angola, that combines elements of dance, acrobatics and music, and is usually referred to as a game. It was developed in Brazil mainly by West African descendants with native Brazilian influences, probably beginning in the 16th century. It is known for quick and complex moves, using mainly power, speed, and leverage for a wide variety of kicks, spins, and highly mobile techniques.
The most widely accepted origin of the word "capoeira" comes from the Tupi words "ka'a" ("jungle") "e pûer" ("it was"), referring to the areas of low vegetation in the Brazilian interior where fugitive slaves would hide. Practitioners of the art are called capoeiristas.
On 26 November 2014 capoeira was granted a special protected status as "intangible cultural heritage" by UNESCO.
History.
Capoeira's history begins with the beginning of African slavery in Brazil. Since the 17th century, Portuguese colonists began exporting slaves to their colonies, coming mainly from West Africa. Brazil, with its vast territory, received most of the slaves, almost 40% of all slaves sent through the Atlantic Ocean.
The early history of capoeira is still controversial, especially the period between the 16th century and the beginning of the 19th century, since historical documents were very scarce in Brazil at that time. But oral tradition, language and evidence leaves little doubt about its Brazilian roots. Different masters tend to have their own specific views on the history of Capoeira. A vast majority of masters recognize the art form as purely Brazilian, while certain masters have been researching for over 40 years to try and find any Capoeira link in Africa.
Origins.
In the 16th century, Portugal had claimed one of the largest territories of the colonial empires, but lacked people to colonize it, especially workers. In the Brazilian colony, the Portuguese, like many European colonists, chose to use slavery to supply this shortage of workers.
In its first century, the main economic activity in the colony was the production and processing of sugar cane. Portuguese colonists created large sugarcane farms called "engenhos", which depended on the labor of slaves. Slaves, living in inhumane and humiliating conditions, were forced to work hard and often suffered physical punishment for small misbehaviors. Although slaves often outnumbered colonists, rebellions were rare due to lack of weapons, harsh colonial law, disagreement between slaves coming from different African cultures and lack of knowledge about the new land and its surroundings usually discouraged the idea of a rebellion.
In this environment, capoeira was born as a simple hope of survival. It was a tool with which an escaped slave, completely unequipped, could survive in the hostile, unknown land and face the hunt of the "capitães-do-mato", the armed and mounted colonial agents who were charged with finding and capturing escapees.
Quilombos.
Soon several groups of escaping slaves would gather and establish quilombos, primitive settlements in far and hard to reach places. Some quilombos would soon increase in size, attracting more fugitive slaves, Brazilian natives and even Europeans escaping the law or Christian extremism. Some quilombos would grow to an enormous size, becoming a real independent multi-ethnic state.
Everyday life in a quilombo offered freedom and the opportunity to revive traditional cultures away from colonial oppression. In this kind of multi-ethnic community, constantly threatened by Portuguese colonial troops, capoeira evolved from a survival tool to a martial art focused on war.
The biggest quilombo, the Quilombo dos Palmares, consisted of many villages which lasted more than a century, resisting at least 24 small attacks and 18 colonial invasions. Portuguese soldiers sometimes said that it took more than one dragoon to capture a quilombo warrior, since they would defend themselves with a "strangely moving fighting technique". The provincial governor declared "it is harder to defeat a quilombo than the Dutch invaders."
Urbanization.
In 1808, the prince and future king Dom João VI, along with the Portuguese court, escaped to Brazil from the invasion of Portugal by Napoleon's troops. Formerly exploited only for its natural resources and commodity crops, the colony finally began to develop as a nation. The Portuguese monopoly effectively came to an end when Brazilian ports opened for trade with friendly foreign nations. Thus, cities grew in importance and Brazilians got permission to manufacture common products once required to be imported from Portugal, such as glass.
Registries of capoeira practices existed since the 18th century in Rio de Janeiro, Salvador and Recife. Due to city growth, more slaves were brought to cities and the increase in social life in the cities made capoeira more prominent and allowed it to be taught and practiced among more people. Because capoeira was often used against the colonial guard, in Rio the colonial government tried to suppress it and established severe physical punishments to its practice.
Ample data from police records from the 1800s shows that many slaves and free coloured people were detained for practicing capoeira:
"From 288 slaves that entered the Calabouço jail during the years 1857 and 1858, 80 (31%) were arrested for capoeira, and only 28 (10.7%) for running away. Out of 4,303 arrests in Rio police jail in 1862, 404 detainees—nearly 10%—had been arrested for capoeira."
End of slavery and prohibition of capoeira.
By the end of the 19th century, slavery was doomed in the Brazilian Empire. Reasons included growing quilombo militias raids in plantations that still used slaves, the refusal of the Brazilian army to deal with escapees and the growth of Brazilian abolitionist movements. The Empire tried to soften the problems with laws to restrict slavery, but finally Brazil would recognize the end of the institution on May 13, 1888, with a law called "Lei Áurea" (Golden Law), sanctioned by imperial parliament and signed by Princess Isabel.
However, free former slaves now felt abandoned. Most had nowhere to live, no jobs and were despised by Brazilian society, which usually viewed them as lazy workers. Also, new immigration from Europe and Asia left most former slaves with no employment.
Soon capoeiristas started to use their skills in unconventional ways. Criminals and war lords used capoeiristas as body guards and hitmen. Groups of capoeiristas, known as "maltas", raided Rio de Janeiro. In 1890, the recently proclaimed Brazilian Republic decreed the prohibition of capoeira in the whole country. Social conditions were chaotic in the Brazilian capital, and police reports identified capoeira as an advantage in fighting.
After the prohibition, any citizen caught practicing capoeira, in a fight or for any other reason, would be arrested, tortured and often mutilated by the police. Cultural practices, such as the "roda de capoeira", were conducted in remote places with sentries to warn of approaching police.
Luta Regional Baiana.
By the 1920s, capoeira repression had declined. Mestre Bimba from Salvador, a strong fighter in both legal and illegal fights, met with his future student, Cisnando Lima. Both thought capoeira was losing its martial roots due to the use of its playful side to entertain tourists. Bimba began developing the first systematic training method for capoeira, and in 1932 founded the first capoeira school. Advised by Cisnando, Bimba called his style "Luta Regional Baiana" ("regional fight from Bahia"), because capoeira was still illegal in name.
In 1937, Bimba founded the school "Centro de Cultura Física e Luta Regional", with permission from Salvador’s Secretary of Education ("Secretaria da Educação, Saúde e Assistência de Salvador"). His work was very well received, and he taught capoeira to the cultural elite of the city. By 1940, capoeira finally lost its criminal connotation and was legalized.
Bimba's Regional style overshadowed traditional capoeiristas, who were still distrusted by society. This began to change in 1941 with the founding of "Centro Esportivo de Capoeira Angola" (CECA) by Vicente Ferreira Pastinha. Located in the Salvador neighborhood of Pelourinho, this school attracted many traditional capoeiristas. With CECA's prominence, the traditional style came to be called "Capoeira Angola". The name derived from "brincar de angola" ("playing Angola"), a term used in the 19th century in some places. But it was also adopted by other masters, including some who did not follow Pastinha's style.
Today.
Capoeira nowadays is not only a martial art, but an active exporter of Brazilian culture all over the world. Since the 1970s, capoeira mestres began to emigrate and teach it in other countries. Present in many countries on every continent, every year capoeira attracts to Brazil thousands of foreign students and tourists. Foreign capoeiristas work hard to learn Portuguese to better understand and become part of the art. Renowned capoeira mestres often teach abroad and establish their own schools. Capoeira presentations, normally theatrical, acrobatic and with little martiality, are common sights around the world.
The martial art aspect is still present and still disguised, leading many non-practitioners to ignore its presence. Trickery is ever present and expert capoeiristas can even disguise an attack as a friendly gesture.
Symbol of the Brazilian culture, symbol of the ethnic amalgam that characterizes Brazil, symbol of resistance to the oppression, capoeira definitely changed its image and became a source of pride to Brazilian people. Capoeira is officially considered intangible cultural heritage of Brazil.
Techniques.
Capoeira is a fast and versatile martial art which is historically focused on fighting outnumbered or in technological disadvantage. The style emphasizes using the lower body to kick, sweep and take down and the upper body to assist those movements and occasionally attack as well. It features a series of complex positions and body postures which are meant to get chained in an uninterrupted flow, in order to strike, dodge and move without breaking motion, conferring the style with a characteristic unpredictability and versatility.
The "ginga" (literally: rocking back and forth; to swing) is the fundamental movement in capoeira, important both for attack and defense purposes. It has two main objectives. One is to keep the capoeirista in a state of constant motion, preventing him or her from being a still and easy target. The other, using also fakes and feints, is to mislead, fool, trick the opponent, leaving them open for an attack or a counter-attack.
The attacks in the capoeira should be done when opportunity arises, and though can be preceded by feints or pokes, they must be precise and decisive, like a direct kick to the head, face or a vital body part, or a strong takedown. Most capoeira attacks are made with the legs, like direct or swirling kicks, rasteiras (leg sweeps), tesouras or knee strikes. Elbow strikes, punches and other forms of takedowns complete the main list. The head strike is a very important counter-attack move.
The defense is based on the principle of non-resistance, meaning avoiding an attack using evasive moves instead of blocking it. Avoids are called "esquivas", which depend on the direction of the attack and intention of the defender, and can be done standing or with a hand leaning on the floor. A block should only be made when the "esquiva" is completely non-viable. This fighting strategy allows quick and unpredictable counterattacks, the ability to focus on more than one adversary and to face empty-handed an armed adversary.
A series of rolls and acrobatics (like the cartwheels called aú or the transitional position called negativa) allows the capoeirista to quickly overcome a takedown or a loss of balance, and to position themselves around the aggressor in order to lay up for an attack. It is this combination of attacks, defense and mobility which gives capoeira its perceived 'fluidity' and choreography-like style.
Weapons.
Through most of its history in Brazil, capoeira commonly featured weapons and weapon training, given its street fighting nature. Capoeristas usually carried knifes and bladed weapons with them, and the berimbau could be used to conceal those inside, or even to turn itself into a weapon by attaching a blade to its tip. The knife or razor was used in street "rodas" and/or against openly hostile opponents, and would be drawn fastly to stab or slash. Other hiding places for the weapons included hats and umbrellas.
Mestre Bimba included in his teachings a "curso de especialização" or specialization course in which the pupils would be taught defenses against knifes and guns, as well as the usage of knife, straight razor, scythe, club, "chanfolo" (double-edged dagger), "facão" (machete) and "tira-teima" (cane sword). Upon graduating, pupils were given a red scarf which marked their specialty. This course was scarcely used, and . A more common custom practised by Bimba and his students was handing furtively a weapon to a player before a "jogo" in order to use it to attack his opponent to Bimba's sign, being the other player's duty to disarm him.
This weapon training is almost completely absent in current capoeira teachings, but some groups still practice the use of razors for ceremonial usage in the "roda".
As a game.
Playing capoeira is both a game and a method of practicing the application of capoeira movements in simulated combat. It can be played anywhere, but it's usually done in a "roda". During the game most capoeira moves are used, but capoeiristas usually avoid using punches or elbow strikes unless it's a very aggressive game.
The game usually does not focus on knocking down or destroying the opponent, rather it emphasizes skill. Capoeiristas often prefer to rely on a takedown like a "rasteira", then allowing the opponent to recover and get back into the game. It is also very common to slow down a kick inches before hitting the target, so a capoeirista can enforce superiority without the need of injuring the opponent. If an opponent clearly cannot dodge an attack, there is no reason to complete it. However, between two high-skilled capoeiristas, the game can get much more aggressive and dangerous. Capoeiristas tend to avoid showing this kind of game in presentations or to the general public.
Roda.
The "Roda" (pronounced ) is a circle formed by capoeiristas and capoeira musical instruments, where every participant sings the typical songs and claps their hands following the music. Two "capoeiristas" enter the "roda" and play the game according to the style required by the musical instruments rhythm. The game finishes when one of the musicians holding a berimbau determine it, when one of the "capoeiristas" decide to leave or call the end of the game or when another capoeirista interrupts the game to start playing, either with one of the current players or with another "capoeirista".
In a "roda" every cultural aspect of capoeira is present, not only the martial side. Aerial acrobatics are common in a presentation "roda", while not seen as often in a more serious one. Takedowns, on the other hand, are common in a serious "roda" but rarely seen in presentations.
Batizado.
The batizado (lit. baptism) is a ceremonial "roda" where new students will get recognized as capoeiristas and earn their first graduation. Also more experienced students may go up in rank, depending on their skills and capoeira culture. In Mestre Bimba's Capoeira Regional, batizado was the first time a new student would play capoeira following the sound of the berimbau.
Students enter the "roda" against a high-ranked capoeirista (such as a teacher or master) and normally the game ends with the student being taken down. In some cases the more experienced capoeirista can judge the takedown unnecessary. Following the batizado the new graduation, generally in the form of a cord, is given.
Apelido.
Traditionally, the batizado is the moment when the new practitioner gets or formalizes his or her "apelido" (nickname). This tradition was created back when capoeira practice was considered a crime. To avoid having problems with the law, capoeiristas would present themselves in the capoeira community only by their nicknames. So if a capoeirista was captured by the police, he would be unable to identify his fellow capoeiristas, even when tortured.
"Apelidos" can come from many different things. A physical characteristic (like being tall or big), a habit (like smiling or drinking too much), place of birth, a particular skill, an animal, or trivial things.
Even though apelidos or these nicknames are not necessary anymore, the tradition is still very alive not only in capoeira but in many aspects of Brazilian culture.
Chamada.
"Chamada" means 'call' and can happen at any time during a "roda" where the rhythm "angola" is being played. It happens when one player, usually the more advanced one, calls his or her opponent to a dance-like ritual. The opponent then approaches the caller and meets him or her to walk side by side. After it both resume normal play.
While it may seem like a break time or a dance, the "chamada" is actually both a trap and a test, as the caller is just watching to see if the opponent will let his guard down so she can perform a takedown or a strike. It is a critical situation, because both players are vulnerable due to the close proximity and potential for a surprise attack. It's also a tool for experienced practitioners and masters of the art to test a student's awareness and demonstrate when the student left herself open to attack.
The use of the "chamada" can result in a highly developed sense of awareness and helps practitioners learn the subtleties of anticipating another person's hidden intentions. The "chamada" can be very simple, consisting solely of the basic elements, or the ritual can be quite elaborate including a competitive dialogue of trickery, or even theatric embellishments.
Volta ao mundo.
Volta ao mundo means "around the world".
The "volta ao mundo" takes place after an exchange of movements has reached a conclusion, or after there has been a disruption in the harmony of the game. In either of these situations, one player will begin walking around the perimeter of the circle counter-clockwise, and the other player will join the "volta ao mundo" in the opposite part of the roda, before returning to the normal game.
Malandragem and Mandinga.
"Malandragem" is a word that comes from "malandro", which means a person who possesses cunning as well as "malícia" (malice). This, however, is misleading as the meaning of "malicia" in capoeira is the capacity to understand someone's intentions. In Brazil, men who used street smarts to make a living were called "malandros". Later the meaning expanded, indicating a person who is a quick thinker in finding a solution for a problem.
In capoeira, "malandragem" is the ability to quickly understand an opponent's aggressive intentions, and during a fight or a game, fool, trick and deceive him.
Similarly capoeiristas use the concept of "mandinga". Mandinga can be translated "magic" or "spell", but in capoeira a "mandingueiro" is a clever fighter, able to trick the opponent. Mandinga is a tricky and strategic quality of the game, and even a certain esthetic, where the game is expressive and at times theatrical, particularly in the Angola style. The roots of the term "mandingueiro" would be a person who had the magic ability to avoid harm due to protection from the Orixás.
Alternately Mandinga is a way of saying Mandinka (as in the Mandinka Nation) who are known as "musical hunters". Which directly ties into the term "vadiação". Vadiação is the musical wanderer (with flute in hand), traveler, vagabond.
Music.
Music is integral to capoeira. It sets the tempo and style of game that is to be played within the "roda". Typically the music is formed by instruments and singing. Rhythm, controlled by a typical instrument called berimbau, differ from very slow to very fast, depending on the style of the "roda".
Instruments.
Capoeira instruments are disposed in a row called bateria. It is traditionally formed by three berimbaus, two pandeiros, three atabaques, one agogô and one ganzá, but this format may vary depending on the capoeira group's traditions or the "roda" style.
The berimbau is the leading instrument, determining the tempo and style of the music and game played. Two low pitch berimbaus (called "berra-boi" and "médio") form the base and a high pitch berimbau (called "viola") makes variations and improvisations. The other instruments must follow the berimbau's rhythm, free to vary and improvise a little, depending upon the capoeira group's musical style.
As the capoeiristas change their playing style significantly following the toque of the berimbau, which sets the game's speed, style and aggressiveness, it is truly the music that drives a capoeira game.
Songs.
Many of the songs are sung in a call and response format while others are in the form of a narrative. Capoeiristas sing about a wide variety of subjects. Some songs are about history or stories of famous capoeiristas. Other songs attempt to inspire players to play better. Some songs are about what is going on within the roda. Sometimes the songs are about life or love lost. Others have lighthearted and playful lyrics.
There are four basic kinds of songs in capoeira, the "Ladaínha", "Chula", "Corrido" and "Quadra". The "Ladaínha" is a narrative solo sung only at the beginning of a roda, often by a "mestre" (master) or most respected capoeirista present. The solo is followed by a "louvação", a call and response pattern that usually thanks God and one's master, among other things. Each call is usually repeated word-for-word by the responders. The Chula is a song where the singer part is much bigger than the chorus response, usually eight singer verses for one chorus response, but the proportion may vary. The Corrido is a song where the singer part and the chorus response are equal, normally two verses by two responses. Finally, the "Quadra" is a song where the same verse is repeated four times, either three singer verses followed by one chorus response, or one verse and one response.
Capoeira songs can talk about virtually anything, being it about a historical fact, a famous capoeirista, trivial life facts, hidden messages for players, anything. Improvisation is very important also, while singing a song the main singer can change the music's lyrics, telling something that's happening in or outside the roda.
Styles.
Determining styles in capoeira is difficult, since there was never a unity in the original capoeira, or a teaching method before the decade of 1920. However, a division between two styles and a sub-style is widely accepted.
Capoeira Angola.
Capoeira Angola refers to every capoeira that keeps the traditions held before the creation of the Regional style.
Existing in many parts of Brazil since colonial times, most notably in the cities of Rio de Janeiro, Salvador and Recife, it's impossible to tell where and when Capoeira Angola began taking its present form. The name "Angola" starts as early as the beginning of slavery in Brazil, when Africans, taken to Luanda to be shipped to the Americas, were called in Brazil "black people from Angola", regardless of their nationality. In some places of Brazil people would refer to capoeira as "playing Angola" and, according to Mestre Noronha, the capoeira school "Centro de Capoeira Angola Conceição da Praia", created in Bahia, already used the name "Capoeira Angola" illegally in the beginning of the 1920 decade.
The name "Angola" was finally immortalized by Mestre Pastinha at February 23, 1941, when he opened the "Centro Esportivo de capoeira Angola" (CECA). Pastinha preferred the ludic aspects of the game rather than the martial side, and was much respected by recognized capoeira masters. Soon many other masters would adopt the name "Angola", even those who would not follow Pastinha's style.
The ideal of "Capoeira Angola" is to maintain capoeira as close to its roots as possible. Characterized by being strategic, with sneaking movements executed standing or near the floor depending on the situation to face, it values the traditions of "malícia", "malandragem" and unpredictability of the original capoeira.
Typical music "bateria" formation in a "roda" of "Capoeira Angola" is three "berimbaus", two "pandeiros", one "atabaque", one "agogô" and one "ganzuá".
Capoeira Regional.
Capoeira Regional began to take form in the 1920 decade, when Mestre Bimba met his future student, José Cisnando Lima. Both believed that capoeira was losing its martial side and concluded there was a need to restructure it. Bimba created his "sequências de ensino" (teaching combinations) and created capoeira's first teaching method. Advised by Cisnando, Bimba decided to call his style "Luta Regional Baiana", as capoeira was still illegal at that time.
The base of Capoeira Regional is the original capoeira without many of the aspects that were impractical in a real fight, with less subterfuge and more objectivity. Training was mainly focused on attack, dodging and counter-attack, giving high importance to precision and discipline. Bimba also added a few moves from other arts, notably the "batuque", an old street fight game practiced by his father. Use of jumps or aerial acrobacies was kept to a minimum, since one of its foundations was always keeping at least one hand or foot firmly attached to the ground. Mestre Bimba often said, ""o chão é amigo do capoeirista"" (the floor is a friend to the capoeirista).
"Capoeira Regional" also introduced the first ranking method in capoeira. "Regional" had three levels: "calouro" (freshman), "formado" (graduated) and "formado especializado" (specialist). When a student completed a course, a special celebration ceremony was had resulting with a silk scarf being tied around the capoeirista's neck.
The traditions of "roda" and capoeira game were kept, being used to put into use what was learned during training. The disposition of musical instruments, however, was changed, being made by a single berimbau and two pandeiros.
The "Luta Regional Baiana" soon became popular, finally changing capoeira's bad image. Mestre Bimba made many presentations of his new style, but the best known was the one made at 1953 to Brazilian president Getúlio Vargas, where the president would say: ""A Capoeira é o único esporte verdadeiramente nacional"" (Capoeira is the only truly national sport).
Capoeira Contemporânea.
In the 1970s a mixed style began to take form, with practitioners taking the aspects they considered more important from both Regional and Angola. Notably more acrobatic, this sub-style is seen by some as the natural evolution of capoeira, by others as adulteration or even misinterpretation of capoeira.
Nowadays the label Contemporânea applies to any capoeira group who don't follow Regional or Angola styles, even the ones who mix capoeira with other martial arts. Some notable groups whose style cannot be described as either Angola or Regional but rather "a style of their own", include Senzala de Santos, Cordao de Ouro and Abada. In the case of Cordao de Ouro, the style may be described as "Miudinho", a low and fast paced game, while in Senzala de Santos the style may described simply as "Senzala de Santos", an elegant, playful combination of Angola and Regional. Capoeira Abada may be described as a more aggressive, less "dance" influenced style of Capoeira.
Ranks.
Because of its origin, capoeira never had unity or a general agreement. Ranking or graduating system follows the same path, as there never existed a ranking system accepted by most of the masters. That means graduation style varies depending on the group's traditions.
The most common modern system uses colored ropes, called "corda" or "cordão", tied around the waist. Some masters use different systems, or even no system at all.
There are many entities (leagues, federations and association) which have tried to unify the graduation system. The most usual is the system of the "Confederação Brasileira de Capoeira" (Brazilian Capoeira Confederation), which adopts ropes using the colors of the Brazilian flag, green, yellow, blue and white.
Even though it's widely used with many small variations, many big and influential groups still use different systems, in example, Porto da Barra Group that uses belts that tell the Brazilian slavery history. Even the "Confederação Brasileira de Capoeira" is not widely accepted as the capoeira's main representative.
In many groups (mainly of Angola school) there is no direct ranking system. There are students, treinels, professors, contra-mestres and mestre but no cordas (belts). In some groups, a capoeira is simply as it, capoeira, and therefore there is no need of cordas as "belts does not play". One of the ideas to not have a belt is that there are as many belt systems as there are groups using them. Therefore, groups can have different meanings of level for same colour belts. And then again, in origins of capoeira, there were no belts.
Related activities.
Even though those activities are strongly associated with capoeira, they have different meanings and origins.
Samba de roda.
Performed by many capoeira groups, samba de roda is a traditional Afro-Brazilian dance and musical form that has been associated with capoeira for many decades. The orchestra is composed by "pandeiro", "atabaque", "berimbau-viola" (high pitch berimbau), chocalho, accompanied by singing and clapping. "Samba de roda" is considered one of the primitive forms of modern Samba.
Maculelê.
Originally the "Maculelê" is believed to have been an indigenous armed fighting style, using two sticks or a machete. Nowadays it's a folkloric dance practiced with heavy afro-Brazilian percussion. Many capoeira groups include "Maculelê" in their presentations.
Puxada de rede.
"Puxada de Rede" is a Brazilian folkloric theatrical play, seen in many capoeira performances. It is based on a traditional Brazilian legend involving the loss of a fisherman in a seafaring accident.
Derivatives and influences.
Capoeira is cited as an influence on other martial arts and several forms of dance, including a debated status as a forerunner of breaking. Many techniques from capoeira are also present in Tricking. In the UK, capoeira has been cited as a key influence in the development of the hybrid martial art Sanjuro.
Capoeira and Sports Development.
Capoeira is currently being used as a tool in sports development (the use of sport to create positive social change) to promote psychosocial wellbeing in various youth projects around the world. Capoeira4Refugees is a UK-based NGO working with youth in conflict zones in the Middle East. Capoeira for Peace is a project based in the Democratic Republic of Congo. The Nukanti Foundation works with street children in Colombia.

</doc>
<doc id="5980" url="https://en.wikipedia.org/wiki?curid=5980" title="Carbon sink">
Carbon sink

A carbon sink is a natural or artificial reservoir that accumulates and stores some carbon-containing chemical compound for an indefinite period. The process by which carbon sinks remove carbon dioxide (CO2) from the atmosphere is known as carbon sequestration. Public awareness of the significance of CO2 sinks has grown since passage of the Kyoto Protocol, which promotes their use as a form of carbon offset. There are also different strategies used to enhance this process.
General.
The natural sinks are:
Natural sinks are typically much bigger than artificial sinks. The main artificial sinks are:
Carbon sources include:
Kyoto Protocol.
Because growing vegetation takes in carbon dioxide, the Kyoto Protocol allows Annex I countries with large areas of growing forests to issue Removal Units to recognize the sequestration of carbon. The additional units make it easier for them to achieve their target emission levels. It is estimated that forests absorb between 10 and 20 tons of carbon dioxide per hectare each year, through photosynthetic conversion into starch, cellulose, lignin, and wooden biomass. While this has been well documented for temperate forests and plantations, the fauna of the tropical forests place some limitations for such global estimates.
Some countries seek to trade emission rights in carbon emission markets, purchasing the unused carbon emission allowances of other countries. If overall limits on greenhouse gas emission are put into place, cap and trade market mechanisms are purported to find cost-effective ways to reduce emissions. There is as yet no carbon audit regime for all such markets globally, and none is specified in the Kyoto Protocol. National carbon emissions are self-declared.
In the Clean Development Mechanism, only afforestation and reforestation are eligible to produce certified emission reductions (CERs) in the first commitment period of the Kyoto Protocol (2008–2012). Forest conservation activities or activities avoiding deforestation, which would result in emission reduction through the conservation of existing carbon stocks, are not eligible at this time. Also, agricultural carbon sequestration is not possible yet.
Storage in terrestrial and marine environments.
Soils.
Soils represent a short to long-term carbon storage medium, and contains more carbon than all terrestrial vegetation and the atmosphere combined. Plant litter and other biomass including charcoal accumulates as organic matter in soils, and is degraded by chemical weathering and biological degradation. More recalcitrant organic carbon polymers such as cellulose, hemi-cellulose, lignin, aliphatic compounds, waxes and terpenoids are collectively retained as humus. Organic matter tends to accumulate in litter and soils of colder regions such as the boreal forests of North America and the Taiga of Russia. Leaf litter and humus are rapidly oxidized and poorly retained in sub-tropical and tropical climate conditions due to high temperatures and extensive leaching by rainfall. Areas where shifting cultivation or slash and burn agriculture are practiced are generally only fertile for 2–3 years before they are abandoned. These tropical jungles are similar to coral reefs in that they are highly efficient at conserving and circulating necessary nutrients, which explains their lushness in a nutrient desert. Much organic carbon retained in many agricultural areas worldwide has been severely depleted due to intensive farming practices.
Grasslands contribute to soil organic matter, stored mainly in their extensive fibrous root mats. Due in part to the climatic conditions of these regions (e.g. cooler temperatures and semi-arid to arid conditions), these soils can accumulate significant quantities of organic matter. This can vary based on rainfall, the length of the winter season, and the frequency of naturally occurring lightning-induced grass-fires. While these fires release carbon dioxide, they improve the quality of the grasslands overall, in turn increasing the amount of carbon retained in the humic material. They also deposit carbon directly to the soil in the form of char that does not significantly degrade back to carbon dioxide.
Forest fires release absorbed carbon back into the atmosphere, as does deforestation due to rapidly increased oxidation of soil organic matter.
Organic matter in peat bogs undergoes slow anaerobic decomposition below the surface. This process is slow enough that in many cases the bog grows rapidly and fixes more carbon from the atmosphere than is released. Over time, the peat grows deeper. Peat bogs inter approximately one-quarter of the carbon stored in land plants and soils.
Under some conditions, forests and peat bogs may become sources of CO2, such as when a forest is flooded by the construction of a hydroelectric dam. Unless the forests and peat are harvested before flooding, the rotting vegetation is a source of CO2 and methane comparable in magnitude to the amount of carbon released by a fossil-fuel powered plant of equivalent power.
Regenerative agriculture.
Current agricultural practices lead to carbon loss from soils. It has been suggested that improved farming practices could return the soils to being a carbon sink. Present worldwide practises of overgrazing are substantially reducing many grasslands' performance as carbon sinks. The Rodale Institute says that regenerative agriculture, if practiced on the planet’s 3.6 billion tillable acres, could sequester up to 40% of current CO2 emissions. They claim that agricultural carbon sequestration has the potential to mitigate global warming. When using biologically based regenerative practices, this dramatic benefit can be accomplished with no decrease in yields or farmer profits. Organically managed soils can convert carbon dioxide from a greenhouse gas into a food-producing asset.
In 2006, U.S. carbon dioxide emissions, largely from fossil fuel combustion, were estimated at nearly 6.5 billion tons. If a 2,000 (lb/ac)/year sequestration rate was achieved on all of cropland in the United States, nearly 1.6 billion tons of carbon dioxide would be sequestered per year, mitigating close to one quarter of the country's total fossil fuel emissions.
Oceans.
Oceans are at present CO2 sinks, and represent the largest active carbon sink on Earth, absorbing more than a quarter of the carbon dioxide that humans put into the air. The Southern Ocean absorbs about 40% annual global CO2 emissions, through winds, currents and eddies (ocean whirlpools) that transport warm and cold water around the ocean. The solubility pump is the primary mechanism responsible for the CO2 absorption by the oceans.
The biological pump plays a negligible role, because of the limitation to pump by ambient light and nutrients required by the phytoplankton that ultimately drive it. Total inorganic carbon is not believed to limit primary production in the oceans, so its increasing availability in the ocean does not directly affect production (the situation on land is different, since enhanced atmospheric levels of CO2 essentially "fertilize" land plant growth to some threshold). However, ocean acidification by invading anthropogenic CO2 may affect the biological pump by negatively impacting calcifying organisms such as coccolithophores, foraminiferans and pteropods. Climate change may also affect the biological pump in the future by warming and stratifying the surface ocean, thus reducing the supply of limiting nutrients to surface waters.
A study from 2008 claims that CO2 could increase primary productivity, particularly in eel grasses in coastal and estuarine habitats. This postulate, however, has yet to be proven.
In January 2009, the Monterey Bay Aquarium Research Institute and the National Oceanic and Atmospheric Administration announced a joint study to determine whether the ocean off the California coast was serving as a carbon source or a carbon sink. Principal instrumentation for the study will be self-contained CO2 monitors placed on buoys in the ocean. They will measure the partial pressure of CO2 in the ocean and the atmosphere just above the water surface.
In February 2009, Science Daily reported that the Southern Indian Ocean is becoming less effective at absorbing carbon dioxide due to changes to the region's climate which include higher wind speeds.
On longer timescales Oceans may be both sources and sinks – during ice ages levels decrease to ~180 ppmv, and much of this is believed to be stored in the oceans. As ice ages end, is released from the oceans and levels during previous interglacials have been around ~280 ppmv. This role as a sink for CO2 is driven by two processes, the solubility pump and the biological pump. The former is primarily a function of differential CO2 solubility in seawater and the thermohaline circulation, while the latter is the sum of a series of biological processes that transport carbon (in organic and inorganic forms) from the surface euphotic zone to the ocean's interior. A small fraction of the organic carbon transported by the biological pump to the seafloor is buried in anoxic conditions under sediments and ultimately forms fossil fuels such as oil and natural gas.
At the end of glacials with sea level rapidly rising, corals tend to grow slower due to increased ocean temperature as seen on the Showtime series "Years of Living Dangerously". The calcium carbonate from which coral skeletons are made is just over 60% carbon dioxide. If we postulate that coral reefs were eroded down to the glacial sea level, then coral reefs have grown 120m upward since the end of the recent glacial.
Enhancing natural sequestration.
Forests.
Forests are carbon stores, and they are carbon dioxide sinks when they are increasing in density or area. In Canada's boreal forests as much as 80% of the total carbon is stored in the soils as dead organic matter. A 40-year study of African, Asian, and South American tropical forests by the University of Leeds, shows tropical forests absorb about 18% of all carbon dioxide added by fossil fuels. Truly mature tropical forests, by definition, grow rapidly as each tree produces at least 10 new trees each year. Based on studies of the FAO and UNEP it has been estimated that Asian forests absorb about 5 tonnes of carbon dioxide per hectare each year. The global cooling effect of carbon sequestration by forests is partially counterbalanced in that reforestation can decrease the reflection of sunlight (albedo). Mid-to-high latitude forests have a much lower albedo during snow seasons than flat ground, thus contributing to warming. Modeling that compares the effects of albedo differences between forests and grasslands suggests that expanding the land area of forests in temperate zones offers only a temporary cooling benefit.
In the United States in 2004 (the most recent year for which EPA statistics are available), forests sequestered 10.6% (637 MegaTonnes) of the carbon dioxide released in the United States by the combustion of fossil fuels (coal, oil and natural gas; 5657 MegaTonnes). Urban trees sequestered another 1.5% (88 MegaTonnes). To further reduce U.S. carbon dioxide emissions by 7%, as stipulated by the Kyoto Protocol, would require the planting of "an area the size of Texas of the area of Brazil every 30 years". Carbon offset programs are planting millions of fast-growing trees per year to reforest tropical lands, for as little as $0.10 per tree; over their typical 40-year lifetime, one million of these trees will fix 1 to 2 MegaTonnes of carbon dioxide. In Canada, reducing timber harvesting would have very little impact on carbon dioxide emissions because of the combination of harvest and stored carbon in manufactured wood products along with the regrowth of the harvested forests. Additionally, the amount of carbon released from harvesting is small compared to the amount of carbon lost each year to forest fires and other natural disturbances.
The Intergovernmental Panel on Climate Change concluded that "a sustainable forest management strategy aimed at maintaining or increasing forest carbon stocks, while producing an annual sustained yield of timber fibre or energy from the forest, will generate the largest sustained mitigation benefit". Sustainable management practices keep forests growing at a higher rate over a potentially longer period of time, thus providing net sequestration benefits in addition to those of unmanaged forests.
Life expectancy of forests varies throughout the world, influenced by tree species, site conditions and natural disturbance patterns. In some forests carbon may be stored for centuries, while in other forests carbon is released with frequent stand replacing fires. Forests that are harvested prior to stand replacing events allow for the retention of carbon in manufactured forest products such as lumber. However, only a portion of the carbon removed from logged forests ends up as durable goods and buildings. The remainder ends up as sawmill by-products such as pulp, paper and pallets, which often end with incineration (resulting in carbon release into the atmosphere) at the end of their lifecycle. For instance, of the 1,692 MegaTonnes of carbon harvested from forests in Oregon and Washington (U.S) from 1900 to 1992, only 23% is in long-term storage in forest products.
Oceans.
One way to increase the carbon sequestration efficiency of the oceans is to add micrometre-sized iron particles in the form of either hematite (iron oxide) or melanterite (iron sulfate) to certain regions of the ocean. This has the effect of stimulating growth of plankton. Iron is an important nutrient for phytoplankton, usually made available via upwelling along the continental shelves, inflows from rivers and streams, as well as deposition of dust suspended in the atmosphere. Natural sources of ocean iron have been declining in recent decades, contributing to an overall decline in ocean productivity (NASA, 2003). Yet in the presence of iron nutrients plankton populations quickly grow, or 'bloom', expanding the base of biomass productivity throughout the region and removing significant quantities of CO2 from the atmosphere via photosynthesis. A test in 2002 in the Southern Ocean around Antarctica suggests that between 10,000 and 100,000 carbon atoms are sunk for each iron atom added to the water. More recent work in Germany (2005) suggests that any biomass carbon in the oceans, whether exported to depth or recycled in the euphotic zone, represents long-term storage of carbon. This means that application of iron nutrients in select parts of the oceans, at appropriate scales, could have the combined effect of restoring ocean productivity while at the same time mitigating the effects of human caused emissions of carbon dioxide to the atmosphere.
Because the effect of periodic small scale phytoplankton blooms on ocean ecosystems is unclear, more studies would be helpful. Phytoplankton have a complex effect on cloud formation via the release of substances such as dimethyl sulfide (DMS) that are converted to sulfate aerosols in the atmosphere, providing cloud condensation nuclei, or CCN. But the effect of small scale plankton blooms on overall DMS production is unknown.
Other nutrients such as nitrates, phosphates, and silica as well as iron may cause ocean fertilization. There has been some speculation that using pulses of fertilization (around 20 days in length) may be more effective at getting carbon to ocean floor than sustained fertilization.
There is some controversy over seeding the oceans with iron however, due to the potential for increased toxic phytoplankton growth (e.g. "red tide"), declining water quality due to overgrowth, and increasing anoxia in areas harming other sea-life such as zooplankton, fish, coral, etc.
Soils.
Since the 1850s, a large proportion of the world's grasslands have been tilled and converted to croplands, allowing the rapid oxidation of large quantities of soil organic carbon. However, in the United States in 2004 (the most recent year for which EPA statistics are available), agricultural soils including pasture land sequestered 0.8% (46 teragrams) as much carbon as was released in the United States by the combustion of fossil fuels (5988 teragrams). The annual amount of this sequestration has been gradually increasing since 1998.
Methods that significantly enhance carbon sequestration in soil include no-till farming, residue mulching, cover cropping, and crop rotation, all of which are more widely used in organic farming than in conventional farming. Because only 5% of US farmland currently uses no-till and residue mulching, there is a large potential for carbon sequestration. Conversion to pastureland, particularly with good management of grazing, can sequester even more carbon in the soil.
Terra preta, an anthropogenic, high-carbon soil, is also being investigated as a sequestration mechanism.
By pyrolysing biomass, about half of its carbon can be reduced to charcoal, which can persist in the soil for centuries, and makes a useful soil amendment, especially in tropical soils ("biochar" or "agrichar").
Savanna.
Controlled burns on far north Australian savannas can result in an overall carbon sink. One working example is the West Arnhem Fire Management Agreement, started to bring "strategic fire management across 28,000 km² of Western Arnhem Land". Deliberately starting controlled burns early in the dry season results in a mosaic of burnt and unburnt country which reduces the area of burning compared with stronger, late dry season fires. In the early dry season there are higher moisture levels, cooler temperatures, and lighter wind than later in the dry season; fires tend to go out overnight. Early controlled burns also results in a smaller proportion of the grass and tree biomass being burnt. Emission reductions of 256,000 tonnes of CO2 have been made as of 2007.
Artificial sequestration.
For carbon to be sequestered artificially (i.e. not using the natural processes of the carbon cycle) it must first be captured, "or" it must be significantly delayed or prevented from being re-released into the atmosphere (by combustion, decay, etc.) from an existing carbon-rich material, by being incorporated into an enduring usage (such as in construction). Thereafter it can be passively stored "or" remain productively utilized over time in a variety of ways.
For example, upon harvesting, wood (as a carbon-rich material) can be immediately burned or otherwise serve as a fuel, returning its carbon to the atmosphere, "or" it can be incorporated into construction or a range of other durable products, thus sequestering its carbon over years or even centuries.
Indeed, a very carefully designed and durable, energy-efficient and energy-capturing building has the potential to sequester (in its carbon-rich construction materials), as much as or more carbon than was released by the acquisition and incorporation of all its materials and than will be released by building-function "energy-imports" during the structure's (potentially multi-century) existence. Such a structure might be termed "carbon neutral" or even "carbon negative". Building construction and operation (electricity usage, heating, etc.) are estimated to contribute nearly "half" of the annual human-caused carbon additions to the atmosphere.
Natural-gas purification plants often already have to remove carbon dioxide, either to avoid dry ice clogging gas tankers or to prevent carbon-dioxide concentrations exceeding the 3% maximum permitted on the natural-gas distribution grid.
Beyond this, one of the most likely early applications of carbon capture is the capture of carbon dioxide from flue gases at power stations (in the case of coal, this is known as "clean coal"). A typical new 1000 MW coal-fired power station produces around 6 million tons of carbon dioxide annually. Adding carbon capture to existing plants can add significantly to the costs of energy production; scrubbing costs aside, a 1000 MW coal plant will require the storage of about of carbon dioxide a year. However, scrubbing is relatively affordable when added to new plants based on coal gasification technology, where it is estimated to raise energy costs for households in the United States using only coal-fired electricity sources from 10 cents per kW·h to 12 cents.
Carbon capture.
Currently, capture of carbon dioxide is performed on a large scale by absorption of carbon dioxide onto various amine-based solvents. Other techniques are currently being investigated, such as pressure swing adsorption, temperature swing adsorption, gas separation membranes, cryogenics and flue capture.
In coal-fired power stations, the main alternatives to retrofitting amine-based absorbers to existing power stations are two new technologies: coal gasification combined-cycle and oxy-fuel combustion. Gasification first produces a "syngas" primarily of hydrogen and carbon monoxide, which is burned, with carbon dioxide filtered from the flue gas. Oxy-fuel combustion burns the coal in oxygen instead of air, producing only carbon dioxide and water vapour, which are relatively easily separated. Some of the combustion products must be returned to the combustion chamber, either before or after separation, otherwise the temperatures would be too high for the turbine.
Another long-term option is carbon capture directly from the air using hydroxides. The air would literally be scrubbed of its CO2 content. This idea offers an alternative to non-carbon-based fuels for the transportation sector.
Examples of carbon sequestration at coal plants include converting carbon from smokestacks into baking soda, and algae-based carbon capture, circumventing storage by converting algae into fuel or feed.
Oceans.
Another proposed form of carbon sequestration in the ocean is direct injection. In this method, carbon dioxide is pumped directly into the water at depth, and expected to form "lakes" of liquid CO2 at the bottom. Experiments carried out in moderate to deep waters (350–3600 m) indicate that the liquid CO2 reacts to form solid CO2 clathrate hydrates, which gradually dissolve in the surrounding waters.
This method, too, has potentially dangerous environmental consequences. The carbon dioxide does react with the water to form carbonic acid, H2CO3; however, most (as much as 99%) remains as dissolved molecular CO2. The equilibrium would no doubt be quite different under the high pressure conditions in the deep ocean. In addition, if deep-sea bacterial methanogens that reduce carbon dioxide were to encounter the carbon dioxide sinks, levels of methane gas may increase, leading to the generation of an even worse greenhouse gas.
The resulting environmental effects on benthic life forms of the bathypelagic, abyssopelagic and hadopelagic zones are unknown. Even though life appears to be rather sparse in the deep ocean basins, energy and chemical effects in these deep basins could have far-reaching implications. Much more work is needed here to define the extent of the potential problems.
Carbon storage in or under oceans may not be compatible with the Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter.
An additional method of long-term ocean-based sequestration is to gather crop residue such as corn stalks or excess hay into large weighted bales of biomass and deposit it in the alluvial fan areas of the deep ocean basin. Dropping these residues in alluvial fans would cause the residues to be quickly buried in silt on the sea floor, sequestering the biomass for very long time spans. Alluvial fans exist in all of the world's oceans and seas where river deltas fall off the edge of the continental shelf such as the Mississippi alluvial fan in the gulf of Mexico and the Nile alluvial fan in the Mediterranean Sea. A downside, however, would be an increase in aerobic bacteria growth due to the introduction of biomass, leading to more competition for oxygen resources in the deep sea, similar to the oxygen minimum zone.
Geological sequestration.
The method of "geo-sequestration" or "geological storage" involves injecting carbon dioxide directly into underground geological formations. Declining oil fields, saline aquifers, and unminable coal seams have been suggested as storage sites. Caverns and old mines that are commonly used to store natural gas are not considered, because of a lack of storage safety.
CO2 has been injected into declining oil fields for more than 40 years, to increase oil recovery. This option is attractive because the storage costs are offset by the sale of additional oil that is recovered. Typically, 10–15% additional recovery of the original oil in place is possible. Further benefits are the existing infrastructure and the geophysical and geological information about the oil field that is available from the oil exploration. Another benefit of injecting CO2 into Oil fields is that CO2 is soluble in oil. Dissolving CO2 in oil lowers the viscosity of the oil and reduces its interfacial tension which increases the oils mobility. All oil fields have a geological barrier preventing upward migration of oil. As most oil and gas has been in place for millions to tens of millions of years, depleted oil and gas reservoirs can contain carbon dioxide for millennia. Identified possible problems are the many 'leak' opportunities provided by old oil wells, the need for high injection pressures and acidification which can damage the geological barrier. Other disadvantages of old oil fields are their limited geographic distribution and depths, which require high injection pressures for sequestration. Below a depth of about 1000 m, carbon dioxide is injected as a supercritical fluid, a material with the density of a liquid, but the viscosity and diffusivity of a gas.
Unminable coal seams can be used to store CO2, because CO2 absorbs to the coal surface, ensuring safe long-term storage. In the process it releases methane that was previously adsorbed to the coal surface and that may be recovered. Again the sale of the methane can be used to offset the cost of the CO2 storage. Release or burning of methane would of course at least partially offset the obtained sequestration result – except when the gas is allowed to escape into the atmosphere in significant quantities: methane has a higher global warming potential than CO2.
Saline aquifers contain highly mineralized brines and have so far been considered of no benefit to humans except in a few cases where they have been used for the storage of chemical waste. Their advantages include a large potential storage volume and relatively common occurrence reducing the distance over which CO2 has to be transported. The major disadvantage of saline aquifers is that relatively little is known about them compared to oil fields. Another disadvantage of saline aquifers is that as the salinity of the water increases, less CO2 can be dissolved into aqueous solution. To keep the cost of storage acceptable the geophysical exploration may be limited, resulting in larger uncertainty about the structure of a given aquifer. Unlike storage in oil fields or coal beds, no side product will offset the storage cost. Leakage of CO2 back into the atmosphere may be a problem in saline-aquifer storage. However, current research shows that several "trapping mechanisms" immobilize the CO2 underground, reducing the risk of leakage.
A major research project examining the geological sequestration of carbon dioxide is currently being performed at an oil field at Weyburn in south-eastern Saskatchewan. In the North Sea, Norway's Statoil natural-gas platform Sleipner strips carbon dioxide out of the natural gas with amine solvents and disposes of this carbon dioxide by geological sequestration. Sleipner reduces emissions of carbon dioxide by approximately one million tonnes a year. The cost of geological sequestration is minor relative to the overall running costs. As of April 2005, BP is considering a trial of large-scale sequestration of carbon dioxide stripped from power plant emissions in the Miller oilfield as its reserves are depleted.
In October 2007, the Bureau of Economic Geology at The University of Texas at Austin received a 10-year, $38 million subcontract to conduct the first intensively monitored, long-term project in the United States studying the feasibility of injecting a large volume of CO2 for underground storage.
Several serpentinite deposits are being investigated as potentially large scale CO2 storage sinks such as those found in NSW, Australia, where the first mineral carbonation pilot plant project is underway. Beneficial re-use of magnesium carbonate from this process could provide feedstock for new products developed for the built environment and agriculture without returning the carbon into the atmosphere and so acting as a carbon sink.
One proposed reaction is that of the olivine-rich rock dunite, or its hydrated equivalent serpentinite with carbon dioxide to form the carbonate mineral magnesite, plus silica and iron oxide (magnetite).
Serpentinite sequestration is favored because of the non-toxic and stable nature of magnesium carbonate. The ideal reactions involve the magnesium endmember components of the olivine (reaction 1) or serpentine (reaction 2), the latter derived from earlier olivine by hydration and silicification (reaction 3). The presence of iron in the olivine or serpentine reduces the efficiency of sequestration, since the iron components of these minerals break down to iron oxide and silica (reaction 4).
Serpentinite reactions.
Reaction 1
"Mg-olivine + carbon dioxide → magnesite + silica + water"
Reaction 2
"Serpentine + carbon dioxide → magnesite + silica + water"
Reaction 3
"Mg-olivine + water + silica → serpentine "
Reaction 4
"Fe-olivine + water → magnetite + silica + hydrogen "
Zeolitic imidazolate frameworks.
Zeolitic imidazolate frameworks is a metal-organic framework carbon dioxide sink which could be used to keep industrial emissions of carbon dioxide out of the atmosphere.
Trends in sink performance.
According to a report in "Nature" magazine, (November, 2009) the first year-by-year accounting of this mechanism during the industrial era, and the first time scientists have actually measured it, suggests "the oceans are struggling to keep up with rising emissions—a finding with potentially wide implications for future climate." With total world emissions from fossil fuels growing rapidly, the proportion of fossil-fuel emissions absorbed by the oceans since 2000 may have declined by as much as 10%, indicating that over time the ocean will become "a less efficient sink of manmade carbon." Samar Khatiwala, an oceanographer at Columbia University concludes that the studies suggest "we cannot count on these sinks operating in the future as they have in the past, and keep on subsidizing our ever-growing appetite for fossil fuels." However, a recent paper by Wolfgang Knorr indicates that the fraction of absorbed by carbon sinks has not changed since 1850.

</doc>
<doc id="5981" url="https://en.wikipedia.org/wiki?curid=5981" title="Charles Tupper">
Charles Tupper

Sir Charles Tupper, 1st Baronet, (July 2, 1821 – October 30, 1915) was a Canadian father of Confederation: as the Premier of Nova Scotia from 1864 to 1867, he led Nova Scotia into Confederation. He went on to serve as the sixth Prime Minister of Canada, sworn into office on May 1, 1896, seven days after parliament had been dissolved. He lost the June 23 election and resigned on July 8, 1896. His 69-day term as prime minister is currently the shortest in Canadian history.
Tupper was born in Amherst, Nova Scotia to the Rev. Charles Tupper and Miriam Lockhart. He was educated at Horton Academy, Wolfville, Nova Scotia, and studied medicine at the University of Edinburgh Medical School, graduating MD in 1843. By the age of 22 he had handled 116 obstetric cases. He practiced medicine periodically throughout his political career (and served as the first president of the Canadian Medical Association). He entered Nova Scotian politics in 1855 as a protégé of James William Johnston. During Johnston's tenure as premier of Nova Scotia in 1857–59 and 1863–64, Tupper served as provincial secretary. Tupper replaced Johnston as premier in 1864. As premier, Tupper established public education in Nova Scotia. He also worked to expand Nova Scotia's railway network in order to promote industry.
By 1860, Tupper supported a union of all the colonies of British North America. Believing that immediate union of all the colonies was impossible, in 1864, he proposed a Maritime Union. However, representatives of the Province of Canada asked to be allowed to attend the meeting in Charlottetown scheduled to discuss Maritime Union in order to present a proposal for a wider union, and the Charlottetown Conference thus became the first of the three conferences that secured Canadian Confederation. Tupper also represented Nova Scotia at the other two conferences, the Quebec Conference (1864) and the London Conference of 1866. In Nova Scotia, Tupper organized a Confederation Party to combat the activities of the Anti-Confederation Party organized by Joseph Howe and successfully led Nova Scotia into Confederation.
Following the passage of the British North America Act in 1867, Tupper resigned as premier of Nova Scotia and began a career in federal politics. He held multiple cabinet positions under Prime Minister Sir John A. Macdonald, including President of the Queen's Privy Council for Canada (1870–72), Minister of Inland Revenue (1872–73), Minister of Customs (1873–74), Minister of Public Works (1878–79), and Minister of Railways and Canals (1879–84). Initially groomed as Macdonald's successor, Tupper had a falling out with Macdonald, and by the early 1880s, he asked Macdonald to appoint him as Canadian High Commissioner to the United Kingdom. Tupper took up his post in London in 1883, and would remain High Commissioner until 1895, although in 1887–88, he served as Minister of Finance without relinquishing the High Commissionership.
In 1895, the government of Sir Mackenzie Bowell foundered over the Manitoba Schools Question; as a result, several leading members of the Conservative Party of Canada demanded the return of Tupper to serve as prime minister. Tupper accepted this invitation and returned to Canada, becoming prime minister in May 1896. An election was called, just before he was sworn in as prime minister, which his party subsequently lost to Wilfrid Laurier and the Liberals. Tupper served as Leader of the Opposition from July 1896 until 1900, at which point he returned to London, where he lived until his death in 1915.
Early life, 1821–1855.
Tupper was born in Amherst, Nova Scotia, to Charles Tupper, Sr., and Miriam Lowe, Lockhart. Charles Tupper, Sr., (1794–1881) was the co-pastor of the local Baptist church. He had been ordained as a Baptist minister in 1817, and was editor of "Baptist Magazine" 1832-1836. He was an accomplished Biblical scholar, and published "Scriptural Baptism" (Halifax, Nova Scotia, 1850) and "Expository Notes on the Syriac Version of the Scriptures".
Beginning in 1837, at age 16, Charles Tupper, Jr., attended Horton Academy in Wolfville, Nova Scotia, where he learned Latin, Greek, and some French. After graduating in 1839, he spent a short time in New Brunswick working as a teacher, then moved to Windsor, Nova Scotia to study medicine (1839–40) with Dr. Ebenezer Fitch Harding. Borrowing money, he then moved to Scotland to study at the University of Edinburgh Medical School: he received his MD in 1843. During his time in Edinburgh, Tupper's commitment to his Baptist faith faltered, and he drank Scotch whisky for the first time.
Returning to Nova Scotia in 1846, he broke off an engagement that he had contracted at age 17 with the daughter of a wealthy Halifax merchant, and instead married Frances Morse (1826–1912), the granddaughter of Col. Joseph Morse, a founder of Amherst, Nova Scotia. The Tuppers had three sons (Orin Stewart, Charles Hibbert, and William Johnston) and three daughters (Emma, Elizabeth Stewart (Lilly), and Sophy Almon). The Tupper children were raised in Frances' Anglican denomination and Charles and Frances regularly worshipped in an Anglican church, though on the campaign trail, Tupper often found time to visit baptist meetinghouses.
Tupper set himself up as a physician in Amherst, Nova Scotia and opened a drugstore.
Early years in Nova Scotia politics, 1855–1864.
The leader of the Conservative Party of Nova Scotia, James William Johnston, a fellow Baptist and family friend of the Tuppers, encouraged Charles Tupper to enter politics. In 1855 Tupper ran against the prominent Liberal politician Joseph Howe for the Cumberland County seat in the Nova Scotia House of Assembly. Joseph Howe would be Tupper's political opponent several times in years to come.
Although Tupper won his seat, the 1855 election was an overall disaster for the Nova Scotia Conservatives, with the Liberals, led by William Young, winning a large majority. Young consequently became Premier of Nova Scotia.
At a caucus meeting in January 1856, Tupper recommended a new direction for the Conservative party: they should begin actively courting Nova Scotia's Roman Catholic minority and should eagerly embrace railroad construction. Having just led his party into a disastrous election campaign, Johnston decided to basically cede control of the party to Tupper, though Johnston remained the party's leader. During 1856 Tupper led Conservative attacks on the government, leading Joseph Howe to dub Tupper "the wicked wasp of Cumberland." In early 1857 Tupper convinced a number of Roman Catholic Liberal members to cross the floor to join the Conservatives, reducing Young's government to the status of a minority government. As a result, Young was forced to resign in February 1857, and the Conservatives formed a government with Johnston as premier. Tupper became the provincial secretary.
In Tupper's first speech to the House of Assembly as provincial secretary, he set forth an ambitious plan of railroad construction. Tupper had thus embarked on the major theme of his political life: that Nova Scotians (and later Canadians) should downplay their ethnic and religious differences, focusing instead on developing the land's natural resources. He argued that with Nova Scotia's "inexhaustible mines", it could become "a vast manufacturing mart" for the east coast of North America. He quickly persuaded Johnston to end the General Mining Association's monopoly over Nova Scotia minerals.
In June 1857 Tupper initiated discussions with New Brunswick and the Province of Canada concerning an intercolonial railway. He traveled to London in 1858 to attempt to secure imperial backing for this project. During these discussions, Tupper realized that Canadians were more interested in discussing federal union, while the British (with the Earl of Derby in his second term as Prime Minister) were too absorbed in their own immediate interests. As such, nothing came of the 1858 discussions for an intercolonial railway.
Sectarian conflict played a major role in the May 1859 elections, with Catholics largely supporting the Conservatives and Protestants shifting toward the Liberals. Tupper barely retained his seat. The Conservatives were barely re-elected and lost a confidence vote later that year. Johnston asked the Governor of Nova Scotia, Lord Mulgrave, for dissolution, but Mulgrave refused and invited William Young to form a government. Tupper was outraged and petitioned the British government, asking them to recall Mulgrave.
For the next three years, Tupper was ferocious in his denunciations of the Liberal government, first Young, and then Joseph Howe, who succeeded Young in 1860. This came to a head in 1863 when the Liberals introduced legislation to restrict the Nova Scotia franchise, a move which Johnston and Tupper successfully blocked.
Tupper continued practicing medicine during this period. He established a successful medical practice in Halifax, rising to become the city medical officer. In 1863 he was elected president of the Medical Society of Nova Scotia.
In the June 1863 election, the Conservatives campaigned on a platform of railroad construction and expanded access to public education. The Conservatives won a large majority, taking 44 of the House of Assembly's 55 seats. Johnston resumed his duties as premier and Tupper again became provincial secretary. As a further sign of the Conservatives' commitment to non-sectarianism, in 1863, after a 20-year hiatus, Dalhousie College was re-opened as a non-denominational institution of higher learning.
Johnston retired from politics in May 1864 when he was appointed as a judge, and Tupper was chosen as his successor as premier of Nova Scotia.
Premier of Nova Scotia, 1864–1867.
Tupper introduced ambitious education legislation in 1864 creating a system of state-subsidized common schools. In 1865 he introduced a bill providing for compulsory local taxation to fund these schools. Although these public schools were non-denominational (which resulted in Protestants sharply criticizing Tupper), they did include a program of Christian education. However, many Protestants, particularly fellow Baptists, felt that Tupper had sold them out. To regain their trust he appointed Baptist educator Theodore Harding Rand as Nova Scotia's first superintendent of education. This raised concern among Catholics, led by Thomas-Louis Connolly, Archbishop of Halifax, who demanded state-funded Catholic schools. Tupper reached a compromise with Archbishop Connolly whereby Catholic-run schools could receive public funding, so long as they provided their religious instruction after hours.
Making good on his promise for expanded railroad construction, in 1864 Tupper appointed Sandford Fleming as the chief engineer of the Nova Scotia Railway in order to expand the line from Truro to Pictou Landing. In January 1866 he awarded Fleming a contract to complete the line after local contractors proved too slow. Though this decision was controversial, it did result in the line's being completed by May 1867. A second proposed line, from Annapolis Royal to Windsor initially faltered, but was eventually completed in 1869 by the privately owned Windsor & Annapolis Railway.
Tupper's role in securing Canadian Confederation.
In the run-up to the 1859 Nova Scotia election, Tupper had been unwilling to commit to the idea of a union with the other British North American colonies. By 1860, however, he had reconsidered his position. Tupper outlined his changed position in a lecture delivered at Saint John, New Brunswick entitled "The Political Condition of British North America." The title of the lecture was an homage to Lord Durham's 1838 "Report on the Affairs of British North America" and assessed the condition of British North America in the two decades following Lord Durham's famous report. Although Tupper was interested in the potential economic consequences of a union with the other colonies, the bulk of his lecture addressed the place of British North America within the wider British Empire. Having been convinced by his 1858 trip to London that British politicians were unwilling to pay attention to small colonies such as Nova Scotia, Tupper argued that Nova Scotia and the other Maritime colonies "could never hope to occupy a position of influence or importance except in connection with their larger sister Canada." Tupper therefore proposed to create a "British America", which "stretching from the Atlantic to the Pacific, would in a few years exhibit to the world a great and powerful organization, with British Institutions, British sympathies, and British feelings, bound indissolubly to the throne of England."
Charlottetown Conference, September 1864.
With the outbreak of the American Civil War in 1861, Tupper worried that a victorious North would turn northward and conquer the British North American provinces. This caused him to redouble his commitment to union, which he now saw as essential to protecting the British colonies against American aggression. Since he thought that full union among the British North American colonies would be unachievable for many years, on March 28, 1864, Tupper instead proposed a Maritime Union which would unite the Maritime provinces in advance of a projected future union with the Province of Canada. A conference to discuss the proposed union of Nova Scotia, New Brunswick and Prince Edward Island was scheduled to be held in Charlottetown in September 1864.
Tupper was pleasantly surprised when the Premier of the Province of Canada, John A. Macdonald, asked to be allowed to attend the Charlottetown Conference. The Conference, which was co-chaired by Tupper and New Brunswick Premier Samuel Leonard Tilley, welcomed the Canadian delegation and asked them to join the conference. The conference proved to be a smashing success, and resulted in an agreement-in-principle to form a union of the four colonies.
Quebec Conference, October 1864.
The Quebec Conference was held on October 10, as a follow-up to the Charlottetown Conference, with Newfoundland only attending to observe. Tupper headed the Nova Scotia delegation to the Quebec Conference. He supported a legislative union of the colonies (which would mean that there would be only one legislature for the united colonies). However, the French Canadian delegates to the conference, notably George-Étienne Cartier and Hector-Louis Langevin, strongly opposed the idea of a legislative union. Tupper threw his weight behind Macdonald's proposal for a federal union, which would see each colony retain its own legislature, with a central legislature in charge of common interests. Tupper argued in favour of a strong central government as a second best to a pure legislative union. He felt, however, that the local legislatures should retain the ability to levy duties on their natural resources.
Concerned that a united legislature would be dominated by the Province of Canada, Tupper pushed for regional representation in the upper house of the confederated colonies (a goal which would be achieved in the makeup of the Senate of Canada).
On the topic of which level of government would control customs in the union, Tupper ultimately agreed to accept the formula by which the federal government controlled customs in exchange for an annual subsidy of 80 cents a year for each Nova Scotian. This deal was ultimately not good for Nova Scotia, which had historically received most of its government revenue from customs, and as a result, Nova Scotia entered Confederation with a deficit.
Aftermath of the Quebec Conference.
Although Tupper had given up much at the Quebec Conference, he thought that he would be able to convince Nova Scotians that the deal he negotiated was in some good for Nova Scotia. He was therefore surprised when the deal he had negotiated at Quebec was roundly criticized by Nova Scotians: the Opposition Leader Adams George Archibald was the only member of the Liberal caucus to support Confederation. Former premier Joseph Howe now organized an Anti-Confederation Party and anti-Confederation sentiments were so strong that Tupper decided to postpone a vote of the legislature on the question of Confederation for a full year. Tupper now organized supporters of Confederation into a Confederation Party to push for the union.
In April 1866, Tupper secured a motion of the Nova Scotia legislature in favour of union by promising that he would renegotiate the Seventy-two Resolutions at the upcoming conference in London Conference.
London Conference, 1866.
Joseph Howe had begun a pamphlet campaign in the UK to turn British public opinion against the proposed union. Therefore, when Tupper arrived in the UK, he immediately initiated a campaign of pamphlets and letters to the editor designed to refute Howe's assertions.
Although Tupper did attempt to renegotiate the 72 Resolutions as he had promised, he was ineffective in securing any major changes. The only major change agreed to at the London Conference arguably did not benefit Nova Scotia - responsibility for the fisheries, which was going to be a joint federal-provincial responsibility under the Quebec agreement, became solely a federal concern.
The final push for Confederation.
Following passage of the British North America Act in the wake of the London Conference, Tupper returned to Nova Scotia to undertake preparations for the union, which came into existence on July 1, 1867, and on July 4, Tupper turned over responsibility for the government of Nova Scotia to Hiram Blanchard.
In honour of the role he had played in securing Confederation, Tupper was made a Companion in The Most Honourable Order of the Bath in 1867. He was now entitled to use the postnomial letters "CB".
Career in the Parliament of Canada, 1867–1884.
Fighting the Anti-Confederates, 1867–1869.
The first elections for the new Canadian House of Commons were held in August–September 1867. Tupper ran as a member for the new federal riding of Cumberland and won his seat. However, he was the only pro-Confederation candidate to win a seat from Nova Scotia in the 1st Canadian Parliament, with Joseph Howe and the Anti-Confederates winning every other seat.
As an ally of Sir John A. Macdonald and the Liberal-Conservative Party, it was widely believed that Tupper would have a place in the first Cabinet of Canada. However, when Macdonald ran into difficulties in organizing this cabinet, Tupper stepped aside in favour of Edward Kenny. Instead, Tupper set up a medical practice in Ottawa and was elected as the first president of the new Canadian Medical Association, a position he held until 1870.
In the November 1867 provincial elections in Nova Scotia, the pro-Confederation Hiram Blanchard was defeated by the leader of the Anti-Confederation Party, William Annand. Given the unpopularity of Confederation within Nova Scotia, Joseph Howe traveled to London in 1868 to attempt to persuade the British government (headed by the Earl of Derby, and then after February 1868 by Benjamin Disraeli) to allow Nova Scotia to secede from Confederation. Tupper followed Howe to London where he successfully lobbied British politicians against allowing Nova Scotia to secede.
Following his victory in London, Tupper proposed a reconciliation with Howe: in exchange for Howe's agreeing to stop fighting against the union, Tupper and Howe would be allies in the fight to protect Nova Scotia's interests within Confederation. Howe agreed to Tupper's proposal and in January 1869 entered the Canadian cabinet as President of the Queen's Privy Council for Canada.
With the outbreak of the Red River Rebellion in 1869, Tupper was distressed to find that his daughter Emma's husband was being held hostage by Louis Riel and the rebels. He rushed to the northwest to rescue his son-in-law.
President of the Queen's Privy Council for Canada, 1870–1872.
When Howe's health declined the next year, Tupper finally entered the 1st Canadian Ministry by becoming Privy Council president in June 1870.
The next year was dominated by a dispute with the United States regarding US access to the Atlantic fisheries. Tupper thought that the British should restrict American access to these fisheries so that they could negotiate from a position of strength. When Prime Minister Macdonald traveled to represent Canada's interests at the negotiations leading up to the Treaty of Washington (1871), Tupper served as Macdonald's liaison with the federal cabinet.
Minister of Inland Revenue, 1872–1873.
On July 2, 1872, Tupper's service as Privy Council president ended and he became Minister of Inland Revenue.
Tupper led the Nova Scotia campaign for the Liberal-Conservative party during the Canadian federal election of 1872. His efforts paid off when Nova Scotia returned not a single Anti-Confederate Member of Parliament to the 2nd Canadian Parliament, and 20 of Nova Scotia's 21 MPs were Liberal-Conservatives. (The Liberal-Conservative Party changed its name to the Conservative Party in 1873.)
Minister of Customs, 1873–1874.
In February 1873, Tupper was shifted from Inland Revenue to become Minister of Customs, and in this position he was successful in having British weights and measures adopted as the uniform standard for the united colonies.
He would not hold this post for long, however, as Macdonald's government was rocked by the Pacific Scandal throughout 1873. In November 1873, the 1st Canadian Ministry was forced to resign and was replaced by the 2nd Canadian Ministry headed by Liberal Alexander Mackenzie.
Years in Opposition, 1874–1878.
Tupper had not been involved in the Pacific Scandal, but he nevertheless continued to support Macdonald and his Conservative colleagues both before and after the 1874 election. The 1874 election was disastrous for the Conservatives, and in Nova Scotia, Tupper was one of only two Conservative MPs returned to the 3rd Canadian Parliament.
Though Macdonald stayed on as Conservative leader, Tupper now assumed a more prominent role in the Conservative Party and was widely seen as Macdonald's heir apparent. He led Conservative attacks on the Mackenzie government throughout the 3rd Parliament. The Mackenzie government attempted to negotiate a new free trade agreement with the United States to replace the Canadian–American Reciprocity Treaty which the U.S. had abrogated in 1864. When Mackenzie proved unable to achieve reciprocity, Tupper began shifting toward protectionism and became a proponent of the National Policy which became a part of the Conservative platform in 1876. The sincerity of Tupper's conversion to the protectionist cause was doubted at the time, however: according to one apocryphal story, when Tupper came to the 1876 debate on Finance Minister Richard John Cartwright's budget, he was prepared to advocate free trade if Cartwright had announced that the Liberals had shifted their position and were now supporting protectionism.
Tupper was also deeply critical of Mackenzie's approach to railways, arguing that completion of the Canadian Pacific Railway, which would link British Columbia (which entered Confederation in 1871) with the rest of Canada, should be a stronger government priority than it was for Mackenzie. This position also became an integral part of the Conservative platform.
As on previous occasions when he was not in cabinet, Tupper was active in practicing medicine during the 1874–78 stint in Opposition, though he was dedicating less and less of his time to medicine during this period.
Tupper was a councillor of the Oxford Military College in Cowley and Oxford, Oxfordshire from 1876–1896.
Minister of Public Works, 1878–1879.
During the 1878 election Tupper again led the Conservative campaign in Nova Scotia. The Conservatives under Macdonald won a resounding majority in the election, in the process capturing 16 of Nova Scotia's 21 seats in the 4th Canadian Parliament.
With the formation of the 3rd Canadian Ministry on October 17, 1878, Tupper became Minister of Public Works. His top priority was completion of the Canadian Pacific Railway, which he saw as "an Imperial Highway across the Continent of America entirely on British soil." This marked a shift in Tupper's position: although he had long argued that completion of the railway should be a major government priority, while Tupper was in Opposition, he argued that the railway should be privately constructed; he now argued that the railway ought to be completed as a public work, partly because he believed that the private sector could not complete the railroad given the recession which gripped the country throughout the 1870s.
Minister of Railways and Canals, 1879–1884.
In May 1879 Macdonald decided that completion of the railway was such a priority that he created a new ministry to focus on railways and canals, and Tupper became Canada's first Minister of Railways and Canals.
Tupper's motto as Minister of Railways and Canals was "Develop our resources." He stated "I have always supposed that the great object, in every country, and especially in a new country, was to draw as capitalists into it as possible."
Tupper traveled to London in summer 1879 to attempt to persuade the British government (then headed by the Earl of Beaconsfield in his second term as prime minister) to guarantee a bond sale to be used to construct the railway. He was not successful, though he did manage to purchase 50,000 tons of steel rails at a bargain price. Tupper's old friend Sandford Fleming oversaw the railway construction, but his inability to keep costs down led to political controversy, and Tupper was forced to remove Fleming as Chief Engineer in May 1880.
1879 also saw Tupper made a Knight Commander of the Order of St Michael and St George, and thus entitled to use the postnominal letters "KCMG".
In 1880, George Stephen approached Tupper on behalf of a syndicate and asked to be allowed to take over construction of the railway. Convinced that Stephen's syndicate was up to the task, Tupper convinced the cabinet to back the plan at a meeting in June 1880 and, together with Macdonald, negotiated a contract with the syndicate in October. The syndicate successfully created the Canadian Pacific Railway in February 1881 and assumed construction of the railway shortly thereafter.
In the following years Tupper was a vocal supporter of the CPR during its competition with the Grand Trunk Railway. In December 1883 he worked out a rescue plan for the CPR after it faced financial difficulties and persuaded his party and Parliament to accept the plan.
In addition to his support for completion of the CPR, Tupper also actively managed the existing railways in the colonies. Shortly after becoming minister in 1879, he forced the Intercolonial Railway to lower its freight rates, which had been a major grievance of Maritime business interests. He then forced the Grand Trunk Railway to sell its Rivière-du-Loup line to the Intercolonial Railway to complete a link between Halifax and the St. Lawrence Seaway. He also refused to give the CPR running rights over the Intercolonial Railway, though he did convince the CPR to build the Short Line from Halifax to Saint John.
In terms of canals, Tupper's time as Minister of Railways and Canals is notable for large expenditures on widening the Welland Canal and deepening the Saint Lawrence Seaway.
Deterioration of relationship with Macdonald and appointment as High Commissioner.
A rift developed between Tupper and Macdonald in 1879 over Sandford Fleming, whom Tupper supported but whom Macdonald wanted removed as Chief Engineer of the CPR. This rift was partially healed and Tupper and Macdonald managed to work together during the negotiations with George Stephen's syndicate in 1880, but the men were no longer close, and Tupper no longer seemed to be Macdonald's heir apparent. By early 1881 Tupper had determined that he should leave the cabinet. In March 1881 he asked Macdonald to appoint him as Canada's High Commissioner in London. Macdonald initially refused, and Alexander Tilloch Galt retained the High Commissioner's post.
During the 1882 election, Tupper campaigned only in Nova Scotia (he normally campaigned throughout the country): he was again successful, with the Conservatives winning 14 of Nova Scotia's 21 seats in the 5th Canadian Parliament. The 1882 election was personally significant for Tupper because it saw his son, Charles Hibbert Tupper, elected as MP for Pictou.
Canadian High Commissioner to the United Kingdom, 1883–1895.
Early years as High Commissioner, 1883–1887.
Tupper remained committed to leaving Ottawa, however, and in May 1883, he moved to London to become unpaid High Commissioner, though he did not surrender his ministerial position at the time. However, he soon faced criticism that the two posts were incompatible, and in May 1884 he resigned from cabinet and the House of Commons and became full-time paid High Commissioner.
During his time as High Commissioner, Tupper vigorously defended Canada's rights. Although he was not a full plenipotentiary, he represented Canada at a Paris conference in 1883, where he openly disagreed with the British delegation; and in 1884 he was allowed to conduct negotiations for a Canadian commercial treaty with Spain.
Tupper was concerned with promoting immigration to Canada and made several tours of various countries in Europe to encourage their citizens to move to Canada. A report in 1883 acknowledges the work of Sir Charles Tupper:
As directing emigration from the United Kingdom and also the Continent, his work has been greatly valuable; and especially in reference to the arrangements made by him on the Continent and in Ireland. The High Commissioner for Canada, Sir Charles Tupper, has been aided during the past year by the same Emigration Agents of the Department in the United Kingdom as in 1882, namely, Mr. John Dyke, Liverpool; Mr. Thomas Grahame, Glasgow; Mr. Charles Foy, Belfast; Mr. Thomas Connolly, Dublin, and Mr. J.W. Down, Bristol. On the European Continent, Dr. Otto Hahn, of Reutlingen, has continued to act as Agent in Germany.
In 1883 Tupper convinced William Ewart Gladstone's government to exempt Canadian cattle from the general British ban on importing American cattle by demonstrating that Canadian cattle were free of disease.
His other duties as High Commissioner included: putting Canadian exporters in contact with British importers; negotiating loans for the Canadian government and the CPR; helping to organize the Colonial and Indian Exhibition of 1886; arranging for a subsidy for the mail ship from Vancouver, British Columbia to the Orient; and lobbying on behalf of a British-Pacific cable along the lines of the transatlantic telegraph cable and for a faster transatlantic steam ship.
Tupper was present at the founding meeting of the Imperial Federation League in July 1884, where he argued against a resolution which said that the only options open to the British Empire were Imperial Federation or disintegration. Tupper believed that a form of limited federation was possible and desirable.
Interlude as Minister of Finance, 1887–1888.
1884 saw the election of Liberal William Stevens Fielding as Premier of Nova Scotia after Fielding campaigned on a platform of leading Nova Scotia out of Confederation. As such, throughout 1886, Macdonald begged Tupper to return to Canada to fight the Anti-Confederates. In January 1887 Tupper returned to Canada to rejoin the 3rd Canadian Ministry as Minister of Finance of Canada, while retaining his post as High Commissioner.
During the 1887 federal election, Tupper again presented the pro-Confederation argument to the people of Nova Scotia, and again the Conservatives won 14 of Nova Scotia's 21 seats in the 6th Canadian Parliament.
During his year as finance minister, Tupper retained the government's commitment to protectionism, even extending it to the iron and steel industry. By this time Tupper was convinced that Canada was ready to move on to its second stage of industrial development. In part, he held out the prospect of the development of a great iron industry as an inducement to keep Nova Scotia from seceding.
Tupper's unique position of being both Minister of Finance and High Commissioner to London served him well in an emerging crisis in American-Canadian relations: in 1885, the U.S. abrogated the fisheries clause of the Treaty of Washington (1871), and the Canadian government retaliated against American fishermen with a narrow reading of the Treaty of 1818. Acting as High Commissioner, Tupper pressured the British government (then led by Lord Salisbury) to stand firm in defending Canada's rights. The result was the appointment of a Joint Commission in 1887, with Tupper serving as one of the three British commissioners to negotiate with the Americans. Salisbury selected Joseph Chamberlain as one of the British commissioners. John Thompson served as the British delegation's legal counsel. During the negotiations, U.S. Secretary of State Thomas F. Bayard complained that "Mr. Chamberlain has yielded the control of the negotiations over to Sir Charles Tupper, who subjects the questions to the demands of Canadian politics." The result of the negotiations was a treaty (the Treaty of Washington of 1888) that made such concessions to Canada that it was ultimately rejected by the American Senate in February 1888. However, although the treaty was rejected, the Commission had managed to temporarily resolve the dispute.
Following the long conclusion of these negotiations, Tupper decided to return to London to become High-Commissioner full-time. Macdonald tried to persuade Tupper to stay in Ottawa: during the political crisis surrounding the 1885 North-West Rebellion, Macdonald had pledged to nominate Sir Hector-Louis Langevin as his successor; Macdonald now told Tupper that he would break this promise and nominate Tupper as his successor. Tupper was not convinced, however, and resigned as Minister of Finance on May 23, 1888, and moved back to London.
Later years as High Commissioner, 1888–1895.
For Tupper's work on the Joint Commission, Joseph Chamberlain arranged for Tupper to become a baronet of the United Kingdom, and the Tupper Baronetcy was created on September 13, 1888.
In 1889, tensions were high between the U.S. and Canada when the U.S. banned Canadians from engaging in the seal hunt in the Bering Sea as part of the ongoing Bering Sea Dispute between the U.S. and Britain. Tupper traveled to Washington, D.C. to represent Canadian interests during the negotiations and was something of an embarrassment to the British diplomats.
When, in 1890, the provincial secretary of Newfoundland, Robert Bond, negotiated a fisheries treaty with the U.S. that Tupper felt was not in Canada's interest, Tupper successfully persuaded the British government (then under Lord Salisbury's second term) to reject the treaty.
Tupper remained an active politician during his time as High Commissioner, which was controversial because diplomats are traditionally expected to be nonpartisan. (Tupper's successor as High Commissioner, Donald Smith would succeed in turning the High Commissioner's office into a nonpartisan office.) As such, Tupper returned to Canada to campaign on behalf of the Conservatives' National Policy during the 1891 election.
Tupper continued to be active in the Imperial Federation League, though after 1887, the League was split over the issue of regular colonial contribution to imperial defense. As a result, the League was dissolved in 1893, for which some people blamed Tupper.
With respect to the British Empire, Tupper advocated a system of mutual preferential trading. In a series of articles in "Nineteenth Century" in 1891 and 1892, Tupper denounced the position that Canada should unilaterally reduce its tariff on British goods. Rather, he argued that any such tariff reduction should only come as part of a wider trade agreement in which tariffs on Canadian goods would also be reduced at the same time.
Sir John A. Macdonald's death in 1891 opened the possibility of Tupper's replacing him as Prime Minister of Canada, but Tupper enjoyed life in London and decided against returning to Canada. He recommended that his son support Sir John Thompson's prime ministerial bid.
Tupper becomes prime minister, 1895–1896.
Sir John Thompson died suddenly in office in December 1894. Many observers expected the Governor General of Canada, Lord Aberdeen, to invite Tupper to return to Canada to become prime minister. However, Lord Aberdeen disliked Tupper and instead invited Sir Mackenzie Bowell to replace Thompson as prime minister. 
The greatest challenge facing Bowell as prime minister was the Manitoba Schools Question. The Conservative Party was bitterly divided on how to handle the Manitoba Schools Question, and as a result, on January 4, 1896, seven cabinet ministers resigned, demanding the return of Tupper. As a result, Bowell and Aberdeen were forced to invite Tupper to join the 6th Canadian Ministry and on January 15 Tupper became Secretary of State for Canada, with the understanding that he would become prime minister following the dissolution of the 7th Canadian Parliament.
Returning to Canada, Tupper was elected to the 7th Canadian Parliament as member for Cape Breton during a by-election held on February 4, 1896. At this point, Tupper was the "de facto" prime minister, though legally Bowell was still prime minister.
Tupper's position on the Manitoba Schools Act was that French Catholics in Manitoba had been promised the right to separate state-funded French-language Catholic schools in the Manitoba Act of 1870. Thus, even though he personally opposed French-language Catholic schools in Manitoba, he believed that the government should stand by its promise and therefore oppose Dalton McCarthy's Manitoba Schools Act. He maintained this position even after the Manitoba Schools Act was upheld by the Judicial Committee of the Privy Council.
In 1895 the Judicial Committee of the Privy Council ruled that the Canadian federal government could pass remedial legislation to overrule the Manitoba Schools Act ("see" Disallowance and reservation). Therefore, in February 1896 Tupper introduced this remedial legislation in the House of Commons. The bill was filibustered by a combination of extreme Protestants led by McCarthy and Liberals led by Wilfrid Laurier. This filibuster resulted in Tupper's abandoning the bill and asking for a dissolution.
Prime Minister, May–July 1896.
Parliament was dissolved on April 24, 1896, and the 7th Canadian Ministry with Tupper as prime minister was sworn in on May 1 making him, with John Turner, one of only three Prime Ministers to never sit in Parliament while in office as Prime Minister. Tupper remains the oldest person ever to become Canadian prime minister, at age 74.
Throughout the 1896 election campaign, Tupper argued that the real issue of the election was the future of Canadian industry, and insisted that Conservatives needed to unite to defeat the Patrons of Industry. However, the Conservatives were so bitterly divided over the Manitoba Schools Question that wherever he spoke, he was faced with a barrage of criticism, most notably at a two-hour address he gave at Massey Hall in Toronto, which was constantly interrupted by the crowd.
Wilfrid Laurier, on the other hand, modified the traditional Liberal stance on free trade and embraced aspects of the National Policy.
In the end, the Conservatives won the most votes in the 1896 election (48.2% of the votes, in comparison to 41.4% for the Liberals). However, they captured only about half of the seats in English Canada, while Laurier's Liberals won a landslide victory in Quebec, where Tupper's reputation as an ardent imperialist was a major handicap. Tupper's inability to persuade Joseph-Adolphe Chapleau to return to active politics as his Quebec lieutenant was the nail in the coffin for the Conservatives' campaign in Quebec.
Although Laurier had clearly won the election on June 24, Tupper initially refused to cede power, insisting that Laurier would be unable to form a government despite the Liberal Party's having won 55% of the seats in the House of Commons. However, when Tupper attempted to make appointments as prime minister, Lord Aberdeen refused to act on Tupper's advice. Tupper then chose to resign immediately and Aberdeen invited Laurier to form a government. Tupper maintained that Lord Aberdeen's actions were unconstitutional.
Tupper's 68 days is the shortest term of all prime ministers. His government never faced a Parliament.
Leader of the Opposition, 1896–1900.
As Leader of the Opposition during the 8th Canadian Parliament, Tupper attempted to regain the loyalty of those Conservatives who had deserted the party over the Manitoba Schools Question. He played up loyalty to the British Empire. Tupper strongly supported Canadian participation in the Second Boer War, which broke out in 1899, and criticized Laurier for not doing enough to support Britain in the war.
The 1900 election saw the Conservatives pick up 17 Ontario seats in the 9th Canadian Parliament. This was a small consolation, however, as Laurier and the Liberals won a definitive majority and had a clear mandate for a second term. Worse for Tupper was the fact he had failed to carry his own seat, losing the Cape Breton seat to Liberal Alexander Johnston. In November 1900, two weeks after the election, Tupper stepped down as leader of the Conservative Party of Canada and Leader of the Opposition - the caucus chose as his successor fellow Nova Scotian Robert Laird Borden.
Later years, 1901–1915.
Following his defeat in the 1900 election, Tupper and his wife settled with their daughter Emma in Bexleyheath in north-west Kent. He continued to make frequent trips to Canada to visit his sons Charles Hibbert Tupper and William Johnston Tupper, both of whom were Canadian politicians.
On November 9, 1907, Tupper became a member of the British Privy Council. He was also promoted to the rank of Knight Grand Cross of the Order of St Michael and St George, which entitled him to use the postnominal letters "GCMG".
Tupper remained interested in imperial politics, and particularly with promoting Canada's place within the British Empire. He sat on the executive committee of the British Empire League and advocated closer economic ties between Canada and Britain, while continuing to oppose Imperial Federation and requests for Canada to make a direct contribution to imperial defense costs (though he supported Borden's decision to voluntarily make an emergency contribution of dreadnoughts to the Royal Navy in 1912).
In his retirement, Tupper wrote his memoirs, entitled "Recollections of Sixty Years in Canada", which were published in 1914. He also gave a series of interviews to journalist W. A. Harkin which formed the basis of a second book published in 1914, entitled "Political Reminiscences of the Right Honourable Sir Charles Tupper".
Tupper's wife, Lady Tupper died in May 1912. His eldest son Orin died in April 1915. On October 30, 1915, in Bexleyheath, Tupper died of heart failure. He was the last of the original Fathers of Confederation to die, and had lived the longest life of any Canadian prime minister, at 94 years, four months. His body was returned to Canada on HMS "Blenheim" (the same vessel that had carried the body of Tupper's colleague, Sir John Thompson to Halifax when Thompson died in England in 1894) and he was buried in St. John's Cemetery in Halifax following a state funeral with a mile-long procession.
Legacy and recognition.
Tupper will be most remembered as a Father of Confederation, and his long career as a federal cabinet minister, rather than his brief time as Prime Minister. As the Premier of Nova Scotia from 1864 to 1867, he led Nova Scotia into Confederation and persuaded Joseph Howe to join the new federal government, bringing an end to the anti-Confederation movement in Nova Scotia.
In their 1999 study of the Canadian Prime Ministers through Jean Chrétien, J.L. Granatstein and Norman Hillmer included the results of a survey of Canadian historians ranking the Prime Ministers. Tupper ranked No. 16 out of the 20 up to that time, due to his extremely short tenure in which he was unable to accomplish anything of significance. Historians noted that despite Tupper's elderly age, he showed a determination and spirit during his brief time as Prime Minister that almost beat Laurier in the 1896 election.
Mount Tupper in the Canadian Rockies was named for him.

</doc>
<doc id="5985" url="https://en.wikipedia.org/wiki?curid=5985" title="Canadian Radio-television and Telecommunications Commission">
Canadian Radio-television and Telecommunications Commission

The Canadian Radio-television and Telecommunications Commission (CRTC, ) is a public organisation in Canada with mandate as a regulatory agency for broadcasting and telecommunications. It was created in 1976 when it took over responsibility for regulating telecommunication carriers. Prior to 1976, it was known as the Canadian Radio and Television Commission, which was established in 1968 by the Parliament of Canada to replace the Board of Broadcast Governors. Its headquarters is located in the Central Building (Édifice central) of Les Terrasses de la Chaudière in Gatineau, Quebec.
History.
The CRTC was originally known as the Canadian Radio-Television Commission. In 1976, jurisdiction over telecommunications services, most of which were then delivered by monopoly common carriers (for example, telephone companies), was transferred to it from the Canadian Transport Commission although the abbreviation CRTC remained the same.
On the telecom side, the CRTC originally regulated only privately held common carriers:
Other telephone companies, many of which were publicly owned and entirely within a province's borders, were regulated by provincial authorities until court rulings during the 1990s affirmed federal jurisdiction over the sector, which also included some fifty small independent incumbents, most of them in Ontario and Quebec. Notable in this group were:
Jurisdiction.
The CRTC regulates all Canadian broadcasting and telecommunications activities and enforces rules it creates to carry out the policies assigned to it; the best-known of these is probably the Canadian content rules. The CRTC reports to the Parliament of Canada through the Minister of Canadian Heritage, which is responsible for the Broadcasting Act, and has an informal relationship with Industry Canada, which is responsible for the Telecommunications Act. Provisions in these two acts, along with less-formal instructions issued by the federal cabinet known as orders-in-council, represent the bulk of the CRTC's jurisdiction.
In many cases, such as the cabinet-directed prohibition on foreign ownership for broadcasters and the legislated principle of the predominance of Canadian content, these acts and orders often leave the CRTC less room to change policy than critics sometimes suggest, and the result is that the commission is often the lightning rod for policy criticism that could arguably be better directed at the government itself.
Complaints against broadcasters, such as concerns around offensive programming, are dealt with by the Canadian Broadcast Standards Council (CBSC), an independent broadcast industry association, rather than by the CRTC, although CBSC decisions can be appealed to the CRTC if necessary. However, the CRTC is also sometimes erroneously criticized for CBSC decisions — for example, the CRTC was erroneously criticized for the CBSC's decisions pertaining to the airing of Howard Stern's terrestrial radio show in Canada in the late 1990s, as well as the CBSC's controversial ruling on the Dire Straits song "Money for Nothing".
The commission is not fully equivalent to the U.S. Federal Communications Commission, which has additional powers over technical matters, in broadcasting and other aspects of communications, in that country. In Canada, the Department of Industry is responsible for allocating frequencies and call signs, managing the broadcast spectrum, and regulating other technical issues such as interference with electronics equipment.
Regulation of broadcast distributors.
The CRTC has in the past regulated the prices cable television broadcast distributors are allowed to charge. In most major markets, however, prices are no longer regulated due to increased competition for broadcast distribution from satellite television.
The CRTC also regulates which channels broadcast distributors must or may offer. Per the Broadcasting Act the commission also gives priority to Canadian signals—many non-Canadian channels which compete with Canadian channels are thus not approved for distribution in Canada. The CRTC argues that allowing free trade in television stations would overwhelm the smaller Canadian market, preventing it from upholding its responsibility to foster a national conversation. Some people, however, consider this tantamount to censorship.
The CRTC's simultaneous substitution rules require that when a Canadian network licences a television show from a US network and shows it in the same time slot, upon request by the Canadian broadcaster, Canadian broadcast distributors must replace the show on the US channel with the broadcast of the Canadian channel, along with any overlays and commercials. As "Grey's Anatomy" is on ABC, but is carried in Canada on CTV at the same time, for instance, the cable, satellite, or other broadcast distributor must send the CTV feed over the signal of the carried ABC affiliate, even where the ABC version is somehow different, particularly commercials. (These rules are not intended to apply in case of differing "episodes" of the same series; this difference may not always be communicated to distributors, although this is rather rare.) Viewers via home antenna who receive both American and Canadian networks on their personal sets are not affected by sim-sub.
The goal of this policy is to create a market in which Canadian networks can realize revenue through advertising sales in spite of their inability to match the rates that the much larger American networks can afford to pay for syndicated programming. This policy is also why Canadian viewers do not see American advertisements during the Super Bowl, even when tuning into one of the many American networks carried on Canadian televisions.
Regulation of the Internet.
In a major May 1999 decision on "New Media", the CRTC held that under the Broadcasting Act the CRTC had jurisdiction over certain content communicated over the Internet including audio and video, but excluding content that is primarily alphanumeric such as emails and most webpages. It also issued an exemption order committing to a policy of non-interference.
In May 2011, in response to the increase presence of Over-the-Top (OTT) programming, the CRTC put a call out to the public to provide input on the impact OTT programming is having on Canadian content and existing broadcasting subscriptions through satellite and cable. On October 5, 2011 the CRTC released their findings that included consultations with stakeholders from the telecommunications industry, media producers, and cultural leaders among others. The evidence was inconclusive, suggesting that an increased availability of OTT options is not having a negative impact on the availability or diversity of Canadian content, one of the key policy mandates of the CRTC, nor are there signs that there has been a significant decline of televisions subscriptions through cable or satellite. However, given the rapid progress in the industry they are working on a more in depth study to be concluded in May 2012.
The CRTC does not "directly" regulate rates, quality of service issues, or business practices for Internet service providers. However, the CRTC does continually monitor the sector and associated trends.
Third Party ISP Access refers to a ruling forcing Cable operators (MSO) to offer Internet access to third party resellers.
Regulation of telephone service.
The commission currently has some jurisdiction over the provision of local landline telephone service in Canada. This is largely limited to the major incumbent carriers, such as Bell Canada and Telus, for traditional landline service (but not Voice over Internet Protocol (VoIP)). It has begun the gradual deregulation of such services where, in the commission's opinion, a sufficient level of competition exists.
The CRTC is sometimes blamed for the current state of the mobile phone industry in Canada, in which there are only three national mobile network operators – Bell Mobility, Telus Mobility, and Rogers Wireless – as well as a handful of MVNOs operating on these networks. In fact, the commission has very little to do with the regulation of mobile phone service, outside of "undue preference" issues (for example, a carrier offering a superior rate or service to some subscribers and not others without a good reason). It does not regulate service rates, service quality, or other business practices, and commission approval is not necessary for wireless provider sales or mergers as in the broadcasting industry. Moreover, it does not deal with the availability of spectrum for mobile phone service, which is part of the Industry Canada mandate, nor the maintenance of competition, which is largely the responsibility of The Competition Bureau.
Transfers of ownership/foreign ownership.
Any transfer of more than 30% of the ownership of a broadcasting licence (including cable/satellite distribution licences) requires advance approval of the commission. One condition normally taken into account in such a decision is the level of foreign ownership; federal regulations require that Canadian citizens ultimately own a majority of a broadcast license. Usually this takes the form of a public process, where interested parties can express their concerns and sometimes including a public hearing, followed by a commission decision.
While landline and mobile telephone providers must also be majority-owned by Canadians under the federal Telecommunications Act, the CRTC is not responsible for enforcement of this provision. In fact, the commission does not require licences at all for telephone companies, and CRTC approval is therefore not generally required for the sale of a telephone company, unless said company also owns a broadcast licence.
Controversial decisions.
Since 1987, the CRTC has been involved in several controversial decisions:
Reception of non-Canadian services.
While an exact number has not been determined, thousands of Canadians have purchased and used what they contend to be grey market radio and television services, licensed in the United States but not in Canada. Users of these unlicensed services contend that they are not directly breaking any laws by simply using the equipment. The equipment is usually purchased from an American supplier (although some merchants have attempted to set up shop in Canada) and the services are billed to an American postal address. The advent of online billing and the easy availability of credit card services has made it relatively easy for almost anyone to maintain an account in good standing, regardless of where they actually live.
Sec. 9(1)(c) of the Radiocommunication Act creates a prohibition against all decoding of encrypted programming signals, followed by an exception where authorization is received from the person holding the lawful right in Canada to transmit and authorize decoding of the signal. This means receiving the encrypted programming of DishNetwork or DirecTV, even with a grey market subscription, may be construed as unlawful (this remains an unresolved Constitutional issue).
Notwithstanding, possession of DishNetwork or DirecTV equipment is not unlawful as provided by The Radiocommuncation Act Section 4(1)(b), which states:
"No person shall, except under and in accordance with a radio authorization, install, operate or possess radio apparatus, other than (b)a radio apparatus that is capable only of the reception of broadcasting and that is not a distribution undertaking. (radio apparatus" means a device or combination of devices intended for, or capable of being used for, radiocommunication)."
Satellite radio poses a more complicated problem for the CRTC. While an unlicensed satellite dish can often be identified easily, satellite radio receivers are much more compact and can rarely be easily identified, at least not without flagrantly violating provisions against unreasonable search and seizure in the Canadian Charter of Rights and Freedoms. Some observers argued that this influenced the CRTC's June 2005 decision to ease Canadian content restrictions on satellite radio (see above).
Structure.
The CRTC is run by up to 13 full-time (including the chairman, the vice-chairman of broadcasting, and the vice-chairman of telecommunications) appointed by the Cabinet for renewable terms of up to five years. In June 2012, Jean-Pierre Blais was appointed Chairman for a five-year term.
The CRTC Interconnection Steering Committee (CISC) assists in developing information, procedures and guidelines for the CRTC's regulatory activities.

</doc>
<doc id="5986" url="https://en.wikipedia.org/wiki?curid=5986" title="Con">
Con

Con may refer to:
CON may refer to:

</doc>
<doc id="5987" url="https://en.wikipedia.org/wiki?curid=5987" title="Coal">
Coal

Coal (Old English "col"; meaning "mineral of fossilized carbon" since the thirteenth century) is a combustible black or brownish-black sedimentary rock usually occurring in rock strata in layers or veins called coal beds or coal seams. The harder forms, such as anthracite coal, can be regarded as metamorphic rock because of later exposure to elevated temperature and pressure. Coal is composed primarily of carbon along with variable quantities of other elements, chiefly hydrogen, sulfur, oxygen, and nitrogen.
Throughout history, coal has been used as an energy resource, primarily burned for the production of electricity and/or heat, and is also used for industrial purposes, such as refining metals. A fossil fuel, coal forms when dead plant matter is converted into peat, which in turn is converted into lignite, then sub-bituminous coal, after that bituminous coal, and lastly anthracite. This involves biological and geological processes that take place over a long period. The United States Energy Information Administration estimates coal reserves at short tons (860 Gt). One estimate for resources is 18,000 Gt.
Coal is the largest source of energy for the generation of electricity worldwide, as well as one of the largest worldwide anthropogenic sources of carbon dioxide releases. In 1999, world gross carbon dioxide emissions from coal usage were 8,666 million tonnes of carbon dioxide. In 2011, world gross emissions from coal usage were 14,416 million tonnes. For every megawatt-hour generated, coal-fired electric power generation emits around 2,000 pounds of carbon dioxide, which is almost double the approximately 1100 pounds of carbon dioxide released by a natural gas-fired electric plant. Because of this higher carbon efficiency of natural gas generation, as the market in the United States has changed to reduce coal and increase natural gas generation, carbon dioxide emissions may have fallen. Those measured in the first quarter of 2012 were the lowest of any recorded for the first quarter of any year since 1992. In 2013, the head of the UN climate agency advised that most of the world's coal reserves should be left in the ground to avoid catastrophic global warming.
Coal is extracted from the ground by coal mining, either underground by shaft mining, or at ground level by open pit mining extraction. Since 1983 the world top coal producer has been China. In 2011 China produced 3,520 million tonnes of coal – 49.5% of 7,695 million tonnes world coal production. In 2011 other large producers were United States (993 million tonnes), India (589), European Union (576) and Australia (416). In 2010 the largest exporters were Australia with 328 million tonnes (27.1% of world coal export) and Indonesia with 316 million tonnes (26.1%), while the largest importers were Japan with 207 million tonnes (17.5% of world coal import), China with 195 million tonnes (16.6%) and South Korea with 126 million tonnes (10.7%).
The extraction of coal, its use in energy production and its byproducts are all associated with severe environmental and health effects. Globally, the use of coal is responsible for 44 per cent of the world's total carbon dioxide emissions.
Etymology.
The word originally took the form "col" in Old English, from Proto-Germanic *"kula"("n"), which in turn is hypothesized to come from the Proto-Indo-European root *"g"("e")"u-lo-" "live coal". Germanic cognates include the Old Frisian "kole", Middle Dutch "cole", Dutch "kool", Old High German "chol", German "Kohle" and Old Norse "kol", and the Irish word "gual" is also a cognate via the Indo-European root.
The word took on the meaning "mineral consisting of fossilized carbon" in the thirteenth century.
Formation.
At various times in the geologic past, the Earth had dense forests in low-lying wetland areas. Due to natural processes such as flooding, these forests were buried underneath soil. As more and more soil deposited over them, they were compressed. The temperature also rose as they sank deeper and deeper. As the process continued the plant matter was protected from biodegradation and oxidation, usually by mud or acidic water. This trapped the carbon in immense peat bogs that were eventually covered and deeply buried by sediments. Under high pressure and high temperature, dead vegetation was slowly converted to coal. As coal contains mainly carbon, the conversion of dead vegetation into coal is called carbonization.
The wide, shallow seas of the Carboniferous Period provided ideal conditions for coal formation, although coal is known from most geological periods. The exception is the coal gap in the Permian–Triassic extinction event, where coal is rare. Coal is known from Precambrian strata, which predate land plants — this coal is presumed to have originated from residues of algae.
Ranks.
As geological processes apply pressure to dead biotic material over time, under suitable conditions, its metamorphic grade increases successively into:
The classification of coal is generally based on the content of volatiles. However, the exact classification varies between countries. According to the German classification, coal is classified as follows:
The middle six grades in the table represent a progressive transition from the English-language sub-bituminous to bituminous coal, while the last class is an approximate equivalent to anthracite, but more inclusive (US anthracite has < 6% volatiles).
Cannel coal (sometimes called "candle coal") is a variety of fine-grained, high-rank coal with significant hydrogen content. It consists primarily of "exinite" macerals, now termed "liptinite".
Hilt's law.
Hilt's law is a geological term that states that, in a small area, the deeper the coal, the higher its rank (grade). The law holds true if the thermal gradient is entirely vertical, but metamorphism may cause lateral changes of rank, irrespective of depth.
Early uses as fuel.
The earliest recognized use is from the Shenyang area of China 4000 BC where Neolithic inhabitants had begun carving ornaments from black lignite. Coal from the Fushun mine in northeastern China was used to smelt copper as early as 1000 BCE. Marco Polo, the Italian who traveled to China in the 13th century, described coal as "black stones ... which burn like logs", and said coal was so plentiful, people could take three hot baths a week. In Europe, the earliest reference to the use of coal as fuel is from the geological treatise "On stones" (Lap. 16) by the Greek scientist Theophrastus ("circa" 371–287 BC):
Outcrop coal was used in Britain during the Bronze Age (3000–2000 BC), where it has been detected as forming part of the composition of funeral pyres. In Roman Britain, with the exception of two modern fields, "the Romans were exploiting coals in all the major coalfields in England and Wales by the end of the second century AD". Evidence of trade in coal (dated to about AD 200) has been found at the Roman settlement at Heronbridge, near Chester, and in the Fenlands of East Anglia, where coal from the Midlands was transported via the Car Dyke for use in drying grain. Coal cinders have been found in the hearths of villas and Roman forts, particularly in Northumberland, dated to around AD 400. In the west of England, contemporary writers described the wonder of a permanent brazier of coal on the altar of Minerva at Aquae Sulis (modern day Bath), although in fact easily accessible surface coal from what became the Somerset coalfield was in common use in quite lowly dwellings locally. Evidence of coal's use for iron-working in the city during the Roman period has been found. In Eschweiler, Rhineland, deposits of bituminous coal were used by the Romans for the smelting of iron ore.
No evidence exists of the product being of great importance in Britain before the High Middle Ages, after about AD 1000. Mineral coal came to be referred to as "seacoal" in the 13th century; the wharf where the material arrived in London was known as Seacoal Lane, so identified in a charter of King Henry III granted in 1253. Initially, the name was given because much coal was found on the shore, having fallen from the exposed coal seams on cliffs above or washed out of underwater coal outcrops, but by the time of Henry VIII, it was understood to derive from the way it was carried to London by sea. In 1257–59, coal from Newcastle upon Tyne was shipped to London for the smiths and lime-burners building Westminster Abbey. Seacoal Lane and Newcastle Lane, where coal was unloaded at wharves along the River Fleet, are still in existence. (See Industrial processes below for modern uses of the term.)
These easily accessible sources had largely become exhausted (or could not meet the growing demand) by the 13th century, when underground extraction by shaft mining or adits was developed. The alternative name was "pitcoal", because it came from mines. It was, however, the development of the Industrial Revolution that led to the large-scale use of coal, as the steam engine took over from the water wheel. In 1700, five-sixths of the world's coal was mined in Britain. Britain would have run out of suitable sites for watermills by the 1830s if coal had not been available as a source of energy. In 1947, there were some 750,000 miners in Britain, but by 2004, this had shrunk to some 5,000 miners working in around 20 collieries.
Uses today.
Coal as fuel.
Coal is primarily used as a solid fuel to produce electricity and heat through combustion. World coal consumption was about 7.25 billion tonnes in 2010 (7.99 billion short tons) and is expected to increase 48% to 9.05 billion tonnes (9.98 billion short tons) by 2030. China produced 3.47 billion tonnes (3.83 billion short tons) in 2011. India produced about 578 million tonnes (637.1 million short tons) in 2011. 68.7% of China's electricity comes from coal. The USA consumed about 13% of the world total in 2010, i.e. 951 million tonnes (1.05 billion short tons), using 93% of it for generation of electricity. 46% of total power generated in the USA was done using coal.
When coal is used for electricity generation, it is usually pulverized and then combusted (burned) in a furnace with a boiler. The furnace heat converts boiler water to steam, which is then used to spin turbines which turn generators and create electricity. The thermodynamic efficiency of this process has been improved over time; some older coal-fired power stations have thermal efficiencies in the vicinity of 25% whereas the newest supercritical and "ultra-supercritical" steam cycle turbines, operating at temperatures over 600 °C and pressures over 27 MPa (over 3900 psi), can practically achieve thermal efficiencies in excess of 45% (LHV basis) using anthracite fuel, or around 43% (LHV basis) even when using lower-grade lignite fuel. Further thermal efficiency improvements are also achievable by improved pre-drying (especially relevant with high-moisture fuel such as lignite or biomass) and cooling technologies.
An alternative approach of using coal for electricity generation with improved efficiency is the integrated gasification combined cycle (IGCC) power plant. Instead of pulverizing the coal and burning it directly as fuel in the steam-generating boiler, the coal can be first gasified (see coal gasification) to create syngas, which is burned in a gas turbine to produce electricity (just like natural gas is burned in a turbine). Hot exhaust gases from the turbine are used to raise steam in a heat recovery steam generator which powers a supplemental steam turbine. Thermal efficiencies of current IGCC power plants range from 39-42% (HHV basis) or ~42-45% (LHV basis) for bituminous coal and assuming utilization of mainstream gasification technologies (Shell, GE Gasifier, CB&I). IGCC power plants outperform conventional pulverized coal-fueled plants in terms of pollutant emissions, and allow for relatively easy carbon capture.
At least 40% of the world's electricity comes from coal, and in 2012, about one-third of the United States' electricity came from coal, down from approximately 49% in 2008. As of 2012 in the United States, use of coal to generate electricity was declining, as plentiful supplies of natural gas obtained by hydraulic fracturing of tight shale formations became available at low prices.
In Denmark, a net electric efficiency of > 47% has been obtained at the coal-fired Nordjyllandsværket CHP Plant and an overall plant efficiency of up to 91% with cogeneration of electricity and district heating. The multifuel-fired Avedøreværket CHP Plant just outside Copenhagen can achieve a net electric efficiency as high as 49%. The overall plant efficiency with cogeneration of electricity and district heating can reach as much as 94%.
An alternative form of coal combustion is as coal-water slurry fuel (CWS), which was developed in the Soviet Union. CWS significantly reduces emissions, improving the heating value of coal. Other ways to use coal are combined heat and power cogeneration and an MHD topping cycle.
The total known deposits recoverable by current technologies, including highly polluting, low-energy content types of coal (i.e., lignite, bituminous), is sufficient for many years. However, consumption is increasing and maximal production could be reached within decades (see world coal reserves, below). On the other hand, much may have to be left in the ground to avoid climate change.
Coking coal and use of coke.
Coke is a solid carbonaceous residue derived from low-ash, low-sulfur bituminous coal (metallurgical coal), from which the volatile constituents are driven off by baking in an oven without oxygen at temperatures as high as 1,000 °C (1,832 °F), so the fixed carbon and residual ash are fused together. Metallurgical coke is used as a fuel and as a reducing agent in smelting iron ore in a blast furnace. The result is pig iron, and is too rich in dissolved carbon, so it must be treated further to make steel. The coking coal should be low in sulfur and phosphorus, so they do not migrate to the metal. Based on the ash percentage, the coking coal can be divided into various grades. These grades are:
The coke must be strong enough to resist the weight of overburden in the blast furnace, which is why coking coal is so important in making steel using the conventional route. However, the alternative route is direct reduced iron, where any carbonaceous fuel can be used to make sponge or pelletised iron. Coke from coal is grey, hard, and porous and has a heating value of 24.8 million Btu/ton (29.6 MJ/kg). Some cokemaking processes produce valuable byproducts, including coal tar, ammonia, light oils, and coal gas.
Petroleum coke is the solid residue obtained in oil refining, which resembles coke, but contains too many impurities to be useful in metallurgical applications.
Gasification.
Coal gasification can be used to produce syngas, a mixture of carbon monoxide (CO) and hydrogen (H2) gas. Often syngas is used to fire gas turbines to produce electricity, but the versatility of syngas also allows it to be converted into transportation fuels, such as gasoline and diesel, through the Fischer-Tropsch process; alternatively, syngas can be converted into methanol, which can be blended into fuel directly or converted to gasoline via the methanol to gasoline process. Gasification combined with Fischer-Tropsch technology is currently used by the Sasol chemical company of South Africa to make motor vehicle fuels from coal and natural gas. Alternatively, the hydrogen obtained from gasification can be used for various purposes, such as powering a hydrogen economy, making ammonia, or upgrading fossil fuels.
During gasification, the coal is mixed with oxygen and steam while also being heated and pressurized. During the reaction, oxygen and water molecules oxidize the coal into carbon monoxide (CO), while also releasing hydrogen gas (H2). This process has been conducted in both underground coal mines and in the production of town gas.
If the refiner wants to produce gasoline, the syngas is collected at this state and routed into a Fischer-Tropsch reaction. If hydrogen is the desired end-product, however, the syngas is fed into the water gas shift reaction, where more hydrogen is liberated.
In the past, coal was converted to make coal gas (town gas), which was piped to customers to burn for illumination, heating, and cooking.
Liquefaction.
Coal can also be converted into synthetic fuels equivalent to gasoline or diesel by several different direct processes (which do not intrinsically require gasification or indirect conversion). In the direct liquefaction processes, the coal is either hydrogenated or carbonized. Hydrogenation processes are the Bergius process, the SRC-I and SRC-II (Solvent Refined Coal) processes, the NUS Corporation hydrogenation process and several other single-stage and two-stage processes. In the process of low-temperature carbonization, coal is coked at temperatures between 360 and 750 °C (680 and 1,380 °F). These temperatures optimize the production of coal tars richer in lighter hydrocarbons than normal coal tar. The coal tar is then further processed into fuels. An overview of coal liquefaction and its future potential is available.
Coal liquefaction methods involve carbon dioxide () emissions in the conversion process. If coal liquefaction is done without employing either carbon capture and storage (CCS) technologies or biomass blending, the result is lifecycle greenhouse gas footprints that are generally greater than those released in the extraction and refinement of liquid fuel production from crude oil. If CCS technologies are employed, reductions of 5–12% can be achieved in Coal to Liquid (CTL) plants and up to a 75% reduction is achievable when co-gasifying coal with commercially demonstrated levels of biomass (30% biomass by weight) in coal/biomass-to-liquids plants. For future synthetic fuel projects, carbon dioxide sequestration is proposed to avoid releasing into the atmosphere. Sequestration adds to the cost of production.
Refined coal.
Refined coal is the product of a coal-upgrading technology that removes moisture and certain pollutants from lower-rank coals such as sub-bituminous and lignite (brown) coals. It is one form of several precombustion treatments and processes for coal that alter coal's characteristics before it is burned. The goals of precombustion coal technologies are to increase efficiency and reduce emissions when the coal is burned. Depending on the situation, precombustion technology can be used in place of or as a supplement to postcombustion technologies to control emissions from coal-fueled boilers.
Industrial processes.
Finely ground bituminous coal, known in this application as sea coal, is a constituent of foundry sand. While the molten metal is in the mould, the coal burns slowly, releasing reducing gases at pressure, and so preventing the metal from penetrating the pores of the sand. It is also contained in 'mould wash', a paste or liquid with the same function applied to the mould before casting. Sea coal can be mixed with the clay lining (the "bod") used for the bottom of a cupola furnace. When heated, the coal decomposes and the bod becomes slightly friable, easing the process of breaking open holes for tapping the molten metal.
Production of chemicals.
Coal is an important feedstock in production of a wide range of chemical fertilizers and other chemical products. The main route to these products is coal gasification to produce syngas. Primary chemicals that are produced directly from the syngas include methanol, hydrogen and carbon monoxide, which are the chemical building blocks from which a whole spectrum of derivative chemicals are manufactured, including olefins, acetic acid, formaldehyde, ammonia, urea and others. The versatility of syngas as a precursor to primary chemicals and high-value derivative products provides the option of using relatively inexpensive coal to produce a wide range of valuable commodities.
Historically, production of chemicals from coal has been used since the 1950s and has become established in the market. According to the 2010 Worldwide Gasification Database, a survey of current and planned gasifiers, from 2004 to 2007 chemical production increased its gasification product share from 37% to 45%. From 2008 to 2010, 22% of new gasifier additions were to be for chemical production.
Because the slate of chemical products that can be made via coal gasification can in general also use feedstocks derived from natural gas and petroleum, the chemical industry tends to use whatever feedstocks are most cost-effective. Therefore, interest in using coal tends to increase for higher oil and natural gas prices and during periods of high global economic growth that may strain oil and gas production. Also, production of chemicals from coal is of much higher interest in countries like South Africa, China, India and the United States where there are abundant coal resources. The abundance of coal combined with lack of natural gas resources in China is strong inducement for the coal to chemicals industry pursued there. In the United States, the best example of the industry is Eastman Chemical Company which has been successfully operating a coal-to-chemicals plant at its Kingsport, Tennessee, site since 1983. Similarly, Sasol has built and operated coal-to-chemicals facilities in South Africa.
Coal to chemical processes do require substantial quantities of water. As of 2013 much of the coal to chemical production was in the People's Republic of China where environmental regulation and water management was weak.
Coal industry.
Coal as a traded commodity.
In North America, Central Appalachian coal futures contracts are currently traded on the New York Mercantile Exchange (trading symbol "QL"). The trading unit is per contract, and is quoted in U.S. dollars and cents per ton. Since coal is the principal fuel for generating electricity in the United States, coal futures contracts provide coal producers and the electric power industry an important tool for hedging and risk management.
In addition to the NYMEX contract, the Intercontinental Exchange (ICE) has European (Rotterdam) and South African (Richards Bay) coal futures available for trading. The trading unit for these contracts is , and are also quoted in U.S. dollars and cents per ton.
The price of coal increased from around $30.00 per short ton in 2000 to around $150.00 per short ton as of September 2008. As of October 2008, the price per short ton had declined to $111.50. Prices further declined to $71.25 as of October 2010. In early 2015, it was trading near $56/ton.
Environmental and health effects.
Health effects.
The use of coal as fuel causes adverse health impact and deaths. Coal mining and coal fueling of power station and industrial processes causes major environmental damage.
The deadly London smog was caused primarily by the heavy use of coal. In the United States coal-fired power plants were estimated in 2004 to cause nearly 24,000 premature deaths every year, including 2,800 from lung cancer. Annual health costs in Europe from use of coal to generate electricity are €42.8 billion, or $55 billion. Yet the disease and mortality burden of coal use today falls most heavily upon China.
Breathing in coal dust causes coalworker's pneumoconiosis which is known colloquially as "black lung", so-called because the coal dust literally turns the lungs black from their usual pink color. In the United States alone, it is estimated that 1500 former employees of the coal industry die every year from the effects of breathing in coal mine dust.
Around 10% of coal is ash, Coal ash is hazardous and toxic to human beings and other living things. Coal ash contains the radioactive elements uranium and thorium and is more radioactive than nuclear waste. Coal ash and other solid combustion byproducts are stored locally and escape in various ways that expose those living near coal plants to radiation and environmental toxics.
Huge amounts of coal ash and other waste is produced annually. In 2013, the US alone consumed on the order of 983 million short tonnes of coal per year. Use of coal on this scale generates hundreds of millions of tons of ash and other waste products every year. These include fly ash, bottom ash, and flue-gas desulfurization sludge, that contain mercury, uranium, thorium, arsenic, and other heavy metals, along with non-metals such as selenium.
The American Lung Association, the American Nurses' Association, and the Physicians for Social Responsibility released a report in 2009 which details in depth the detrimental impact of the coal industry on human health, including workers in the mines and individuals living in communities near plants burning coal as a power source. This report provides medical information regarding damage to the lungs, heart, and nervous system of Americans caused by the burning of coal as fuel. It details how the air pollution caused by the plume of coal smokestack emissions is a cause of asthma, strokes, reduced intelligence, artery blockages, heart attacks, congestive heart failure, cardiac arrhythmias, mercury poisoning, arterial occlusion, and lung cancer.
More recently, the Chicago School of Public Health released a largely similar report, echoing many of the same findings.
Though coal burning has increasingly been supplanted by less-toxic natural gas use in recent years, a 2010 study by the Clean Air Task Force still estimated that "air pollution from coal-fired power plants accounts for more than 13,000 premature deaths, 20,000 heart attacks, and 1.6 million lost workdays in the U.S. each year." The total monetary cost of these health impacts is over $100 billion annually.
The WHO classifies coal as a "dirty fuel" and encourages the movement away from such fuels towards cleaner alternatives.
Environmental effects.
Water systems are also affected by coal. For example, mining affects with groundwater and water table levels and acidity. Spills of fly ash, such as the Kingston Fossil Plant coal fly ash slurry spill, can also contaminate land and waterways, and destroy homes. Power stations that burn coal also consume large quantities of water. This can affect the flows of rivers, and has consequential impacts on other land uses.
One of the earliest known impacts of coal on the water cycle was acid rain. Approximately 75 Tg/S per year of sulfur dioxide (SO2) is released from burning coal. After release, the sulfur dioxide is oxidized to gaseous H2SO2 which scatters solar radiation, hence its increase in the atmosphere exerts a cooling effect on climate. This beneficially masks some of the warming caused by increased greenhouse gases. However, the sulphur is precipitated out of the atmosphere as acid rain in a matter of weeks, whereas carbon dioxide remains in the atmosphere for hundreds of years. Release of SO2 also contributes to the widespread acidification of ecosystems.
Disused coal mines can also cause issues. Subsidence can occur above tunnels, causing damage to infrastructure or cropland. Coal mining can also cause long lasting fires, and it has been estimated that around 1000 coal seam fires are burning at any given time. For example, there is a coal seam fire in Germany that has been burning since 1668, and is still burning in the 21st century.
Some environmental impacts are modest, such as dust nuisance. However, perhaps the largest and most long term effect of coal use is the release of carbon dioxide, a greenhouse gas that causes climate change and global warming, according to the IPCC and the EPA. Coal is the largest contributor to the human-made increase of CO2 in the atmosphere.
The production of coke from coal produces ammonia, coal tar, and gaseous compounds as by-products which if discharged to land, air or waterways can act as environmental pollutants. The Whyalla steelworks is one example of a coke producing facility where liquid ammonia is discharged to the marine environment.
Clean coal technology.
"Clean" coal technology is a collection of technologies being developed to mitigate the environmental impact of coal energy generation. Those technologies are being developed to remove or reduce pollutant emissions to the atmosphere. Some of the techniques that would be used to accomplish this include chemically washing minerals and impurities from the coal, gasification (see also IGCC), improved technology for treating flue gases to remove pollutants to increasingly stringent levels and at higher efficiency, carbon capture and storage technologies to capture the carbon dioxide from the flue gas and dewatering lower rank coals (brown coals) to improve the calorific value, and thus the efficiency of the conversion into electricity. Figures from the United States Environmental Protection Agency show that these technologies have made today's coal-based generating fleet 77 percent cleaner on the basis of regulated emissions per unit of energy produced.
Clean coal technology usually addresses atmospheric problems resulting from burning coal. Historically, the primary focus was on SO2 and NOx, the most important gases in causation of acid rain, and particulates which cause visible air pollution and deleterious effects on human health. More recent focus has been on carbon dioxide (due to its impact on global warming) and concern over toxic species such as mercury. Concerns exist regarding the economic viability of these technologies and the timeframe of delivery, potentially high hidden economic costs in terms of social and environmental damage, and the costs and viability of disposing of removed carbon and other toxic matter.
Several different technological methods are available for the purpose of carbon capture as demanded by the clean coal concept:
The Kemper County IGCC Project, a 582 MW coal gasification-based power plant, will use pre-combustion capture of CO2 to capture 65% of the CO2 the plant produces, which will be utilized/geologically sequestered in enhanced oil recovery operations.
The Saskatchewan Government's Boundary Dam Integrated Carbon Capture and Sequestration Demonstration Project will use post-combustion, amine-based scrubber technology to capture 90% of the CO2 emitted by Unit 3 of the power plant; this CO2 will be pipelined to and utilized for enhanced oil recovery in the Weyburn oil fields.
An early example of a coal-based plant using (oxy-fuel) carbon-capture technology is Swedish company Vattenfall's Schwarze Pumpe power station located in Spremberg, Germany, built by German firm Siemens, which went on-line in September 2008. The facility captures CO2 and acid rain producing pollutants, separates them, and compresses the CO2 into a liquid. Plans are to inject the CO2 into depleted natural gas fields or other geological formations. Vattenfall opines that this technology is considered not to be a final solution for CO2 reduction in the atmosphere, but provides an achievable solution in the near term while more desirable alternative solutions to power generation can be made economically practical.
Bioremediation.
The white rot fungus Trametes versicolor can grow on and metabolize naturally occurring coal. The bacteria Diplococcus has been found to degrade coal, raising its temperature.
Economic aspects.
Coal (by liquefaction technology) is one of the backstop resources that could limit escalation of oil prices and mitigate the effects of transportation energy shortage that will occur under peak oil. This is contingent on liquefaction production capacity becoming large enough to satiate the very large and growing demand for petroleum. Estimates of the cost of producing liquid fuels from coal suggest that domestic U.S. production of fuel from coal becomes cost-competitive with oil priced at around $35 per barrel, with the $35 being the break-even cost. With oil prices as low as around $40 per barrel in the U.S. as of December 2008, liquid coal lost some of its economic allure in the U.S., but will probably be re-vitalized, similar to oil sand projects, with an oil price around $70 per barrel.
In China, due to an increasing need for liquid energy in the transportation sector, coal liquefaction projects were given high priority even during periods of oil prices below $40 per barrel. This is probably because China prefers not to be dependent on foreign oil, instead utilizing its enormous domestic coal reserves. As oil prices were increasing during the first half of 2009, the coal liquefaction projects in China were again boosted, and these projects are profitable with an oil barrel price of $40.
China is the largest producer of coal in the world. It is the world's largest energy consumer, and relies on coal to supply 69% of its energy needs. An estimated 5 million people worked in China's coal-mining industry in 2007.
Coal pollution costs the EU €43 billion each year. Measures to cut air pollution may have beneficial long-term economic impacts for individuals.
Energy density and carbon impact.
The energy density of coal, i.e. its heating value, is roughly 24 megajoules per kilogram (approximately 6.7 kilowatt-hours per kg). For a coal power plant with a 40% efficiency, it takes an estimated of coal to power a 100 W lightbulb for one year.
As of 2006, the average efficiency of electricity-generating power stations was 31%; in 2002, coal represented about 23% of total global energy supply, an equivalent of 3.4 billion tonnes of coal, of which 2.8 billion tonnes were used for electricity generation.
The US Energy Information Agency's 1999 report on CO2 emissions for energy generation quotes an emission factor of 0.963 kg CO2/kWh for coal power, compared to 0.881 kg CO2/kWh (oil), or 0.569 kg CO2/kWh (natural gas).
Underground fires.
Thousands of coal fires are burning around the world. Those burning underground can be difficult to locate and many cannot be extinguished. Fires can cause the ground above to subside, their combustion gases are dangerous to life, and breaking out to the surface can initiate surface wildfires. Coal seams can be set on fire by spontaneous combustion or contact with a mine fire or surface fire. Lightning strikes are an important source of ignition. The coal continues to burn slowly back into the seam until oxygen (air) can no longer reach the flame front. A grass fire in a coal area can set dozens of coal seams on fire. Coal fires in China burn an estimated 120 million tons of coal a year, emitting 360 million metric tons of CO2, amounting to 2–3% of the annual worldwide production of CO2 from fossil fuels. In Centralia, Pennsylvania (a borough located in the Coal Region of the United States), an exposed vein of anthracite ignited in 1962 due to a trash fire in the borough landfill, located in an abandoned anthracite strip mine pit. Attempts to extinguish the fire were unsuccessful, and it continues to burn underground to this day. The Australian Burning Mountain was originally believed to be a volcano, but the smoke and ash come from a coal fire that has been burning for some 6,000 years.
At Kuh i Malik in Yagnob Valley, Tajikistan, coal deposits have been burning for thousands of years, creating vast underground labyrinths full of unique minerals, some of them very beautiful. Local people once used this method to mine ammoniac. This place has been well-known since the time of Herodotus, but European geographers misinterpreted the Ancient Greek descriptions as the evidence of active volcanism in Turkestan (up to the 19th century, when the Russian army invaded the area).
The reddish siltstone rock that caps many ridges and buttes in the Powder River Basin in Wyoming and in western North Dakota is called "porcelanite", which resembles the coal burning waste "clinker" or volcanic "scoria". Clinker is rock that has been fused by the natural burning of coal. In the Powder River Basin approximately 27 to 54 billion tons of coal burned within the past three million years. Wild coal fires in the area were reported by the Lewis and Clark Expedition as well as explorers and settlers in the area.
Production trends.
In 2006, China was the top producer of coal with 38% share followed by the United States and India, according to the British Geological Survey. As of 2012 coal production in the United States was falling at the rate of 7% annually with many power plants using coal shut down or converted to natural gas; however, some of the reduced domestic demand was taken up by increased exports with five coal export terminals being proposed in the Pacific Northwest to export coal from the Powder River Basin to China and other Asian markets; however, as of 2013, environmental opposition was increasing. High-sulfur coal mined in Illinois which was unsaleable in the United States found a ready market in Asia as exports reached 13 million tons in 2012.
World coal reserves.
The 948 billion short tons of recoverable coal reserves estimated by the Energy Information Administration are equal to about 4,196 BBOE (billion barrels of oil equivalent). The amount of coal burned during 2007 was estimated at 7.075 billion short tons, or 133.179 quadrillion BTU's. This is an average of 18.8 million BTU per short ton. In terms of heat content, this is about of oil equivalent per day. By comparison in 2007, natural gas provided of oil equivalent per day, while oil provided per day.
British Petroleum, in its 2007 report, estimated at 2006 end that there were 147 years reserves-to-production ratio based on "proven" coal reserves worldwide. This figure only includes reserves classified as "proven"; exploration drilling programs by mining companies, particularly in under-explored areas, are continually providing new reserves. In many cases, companies are aware of coal deposits that have not been sufficiently drilled to qualify as "proven". However, some nations haven't updated their information and assume reserves remain at the same levels even with withdrawals.
Of the three fossil fuels, coal has the most widely distributed reserves; coal is mined in over 100 countries, and on all continents except Antarctica. The largest reserves are found in the United States, Russia, China, Australia and India. Note the table below.
Major coal producers.
The reserve life is an estimate based only on current production levels and proved reserves level for the countries shown, and makes no assumptions of future production or even current production trends. Countries with annual production higher than 100 million tonnes are shown. For comparison, data for the European Union is also shown.
Shares are based on data expressed in tonnes oil equivalent.
Major coal consumers.
Countries with annual consumption higher than 20 million tonnes are shown.
Major coal exporters.
Countries with annual gross export higher than 10 million tonnes are shown. In terms of net export the largest exporters are still Australia (328.1 millions tonnes), Indonesia (316.2) and Russia (100.2).
Major coal importers.
Countries with annual gross import higher than 20 million tonnes are shown. In terms of net import the largest importers are still Japan (206.0 millions tonnes), China (172.4) and South Korea (125.8).
Cultural usage.
Coal is the official state mineral of Kentucky. and the official state rock of Utah; both U.S. states have a historic link to coal mining.
Some cultures hold that children who misbehave will receive only a lump of coal from Santa Claus for Christmas in their christmas stockings instead of presents.
It is also customary and considered lucky in Scotland and the North of England to give coal as a gift on New Year's Day. This occurs as part of First-Footing and represents warmth for the year to come.

</doc>
<doc id="5992" url="https://en.wikipedia.org/wiki?curid=5992" title="Traditional Chinese medicine">
Traditional Chinese medicine

Traditional Chinese medicine (TCM; ) is a style of traditional Asian medicine informed by modern medicine but built on a foundation of more than 2,500 years of Chinese medical practice that includes various forms of herbal medicine, acupuncture, massage (tui na), exercise (qigong), and dietary therapy. It is primarily used as a complementary alternative medicine approach. TCM is widely used in China and is becoming increasingly prevalent in Europe and North America.
One of the basic tenets of TCM "holds that the body's vital energy ("chi" or "qi") circulates through channels, called "meridians", that have branches connected to bodily organs and functions." Concepts of the body and of disease used in TCM reflect its ancient origins and its emphasis on dynamic processes over material structure, similar to European humoral theory. Scientific investigation has found no histological or physiological evidence for traditional Chinese concepts such as "qi", meridians, and acupuncture points. The TCM theory and practice are not based upon scientific knowledge, and its own practitioners disagree widely on what diagnosis and treatments should be used for any given patient. The effectiveness of Chinese herbal medicine remains poorly researched and documented. There are concerns over a number of potentially toxic plants, animal parts, and mineral Chinese medicinals. A review of cost-effectiveness research for TCM found that studies had low levels of evidence, but so far have not shown benefit outcomes. Pharmaceutical research has explored the potential for creating new drugs from traditional remedies, with few successful results. A "Nature" editorial described TCM as "fraught with pseudoscience", and said that the most obvious reason why it hasn't delivered many cures is that the majority of its treatments have no logical mechanism of action. Proponents propose that research has so far missed key features of the art of TCM, such as unknown interactions between various ingredients and complex interactive biological systems.
The doctrines of Chinese medicine are rooted in books such as the "Yellow Emperor's Inner Canon" and the "Treatise on Cold Damage", as well as in cosmological notions such as yin-yang and the five phases. Starting in the 1950s, these precepts were standardized in the People's Republic of China, including attempts to integrate them with modern notions of anatomy and pathology. In the 1950s, the Chinese government promoted a systematized form of TCM.
TCM's view of the body places little emphasis on anatomical structures, but is mainly concerned with the identification of functional entities (which regulate digestion, breathing, aging etc.). While health is perceived as harmonious interaction of these entities and the outside world, disease is interpreted as a disharmony in interaction. TCM diagnosis aims to trace symptoms to patterns of an underlying disharmony, by measuring the pulse, inspecting the tongue, skin, and eyes, and looking at the eating and sleeping habits of the person as well as many other things.
History.
Traces of therapeutic activities in China date from the Shang dynasty (14th–11th centuries BCE). Though the Shang did not have a concept of "medicine" as distinct from other fields, their oracular inscriptions on bones and tortoise shells refer to illnesses that affected the Shang royal family: eye disorders, toothaches, bloated abdomen, etc., which Shang elites usually attributed to curses sent by their ancestors. There is no evidence that the Shang nobility used herbal remedies. According to a 2006 overview, the "Documentation of Chinese materia medica (CMM) dates back to around 1,100 BC when only dozens of drugs were first described. By the end of the 16th century, the number of drugs documented had reached close to 1,900. And by the end of the last century, published records of CMM have reached 12,800 drugs."
Stone and bone needles found in ancient tombs led Joseph Needham to speculate that acupuncture might have been carried out in the Shang dynasty. But most historians now make a distinction between medical lancing (or bloodletting) and acupuncture in the narrower sense of using metal needles to treat illnesses by stimulating specific points along circulation channels ("meridians") in accordance with theories related to the circulation of Qi. The earliest public evidence for acupuncture in this sense dates to the second or first century BCE.
The "Yellow Emperor's Inner Canon", the oldest received work of Chinese medical theory, was compiled around the first century BCE on the basis of shorter texts from different medical lineages. Written in the form of dialogues between the legendary Yellow Emperor and his ministers, it offers explanations on the relation between humans, their environment, and the cosmos, on the contents of the body, on human vitality and pathology, on the symptoms of illness, and on how to make diagnostic and therapeutic decisions in light of all these factors. Unlike earlier texts like "Recipes for Fifty-Two Ailments", which was excavated in the 1970s from a tomb that had been sealed in 168 BCE, the "Inner Canon" rejected the influence of spirits and the use of magic. It was also one of the first books in which the cosmological doctrines of Yinyang and the Five Phases were brought to a mature synthesis.
The "Treatise on Cold Damage Disorders and Miscellaneous Illnesses" was collated by Zhang Zhongjing sometime between 196 and 220 CE, at the end of the Han dynasty. Focusing on drug prescriptions rather than acupuncture, it was the first medical work to combine Yinyang and the Five Phases with drug therapy. This formulary was also the earliest public Chinese medical text to group symptoms into clinically useful "patterns" ("zheng" 證) that could serve as targets for therapy. Having gone through numerous changes over time, the formulary now circulates as two distinct books: the "Treatise on Cold Damage Disorders" and the "Essential Prescriptions of the Golden Casket", which were edited separately in the eleventh century, under the Song dynasty.
In the centuries that followed the completion of the "Yellow Emperor's Inner Canon", several shorter books tried to summarize or systematize its contents. The "Canon of Problems" (probably second century CE) tried to reconcile divergent doctrines from the "Inner Canon" and developed a complete medical system centered on needling therapy. The "AB Canon of Acupuncture and Moxibustion" ("Zhenjiu jiayi jing" 針灸甲乙經, compiled by Huangfu Mi sometime between 256 and 282 CE) assembled a consistent body of doctrines concerning acupuncture; whereas the "Canon of the Pulse" ("Maijing" 脈經; ca. 280) presented itself as a "comprehensive handbook of diagnostics and therapy."
In 1950, Chairman Mao Zedong made a speech in support of traditional Chinese medicine which was influenced by political necessity. Zedong believed he and the Chinese Communist Party should promote traditional Chinese medicine (TCM) but he did not personally believe in TCM and he didn't use it. In 1952, the president of the Chinese Medical Association said that, "This One Medicine, will possess a basis in modern natural sciences, will have absorbed the ancient and the new, the Chinese and the foreign, all medical achievements—and will be China’s New Medicine!"
Historical physicians.
These include Zhang Zhongjing, Hua Tuo, Sun Simiao, Tao Hongjing, Zhang Jiegu, and Li Shizhen.
Philosophical background.
Traditional Chinese medicine (TCM) is a broad range of medicine practices sharing common concepts which have been developed in China and are based on a tradition of more than 2,000 years, including various forms of herbal medicine, acupuncture, massage (Tui na), exercise (qigong), and dietary therapy. It is primarily used as a complementary alternative medicine approach. TCM is widely used in China and it is also used in the West. Its philosophy is based on Yinyangism (i.e., the combination of Five Phases theory with Yin-yang theory), which was later absorbed by Daoism.
Yin and yang.
Yin and yang are ancient Chinese concepts which can be traced back to the Shang dynasty (1600–1100 BC). They represent two abstract and complementary aspects that every phenomenon in the universe can be divided into. Primordial analogies for these aspects are the sun-facing (yang) and the shady (yin) side of a hill. Two other commonly used representational allegories of yin and yang are water and fire. In the yin-yang theory, detailed attributions are made regarding the yin or yang character of things:
The concept of yin and yang is also applicable to the human body; for example, the upper part of the body and the back are assigned to yang, while the lower part of the body are believed to have the yin character. Yin and yang characterization also extends to the various body functions, and – more importantly – to disease symptoms (e.g., cold and heat sensations are assumed to be yin and yang symptoms, respectively). Thus, yin and yang of the body are seen as phenomena whose lack (or overabundance) comes with characteristic symptom combinations:
TCM also identifies drugs believed to treat these specific symptom combinations, i.e., to reinforce yin and yang.
Five Phases theory.
Five Phases (五行, ), sometimes also translated as the "Five Elements" theory, presumes that all phenomena of the universe and nature can be broken down into five elemental qualities – represented by wood (木, ), fire (火), earth (土, ), metal (金, ), and water (水, ). In this way, lines of correspondence can be drawn:
Strict rules are identified to apply to the relationships between the Five Phases in terms of sequence, of acting on each other, of counteraction, etc. All these aspects of Five Phases theory constitute the basis of the zàng-fǔ concept, and thus have great influence regarding the TCM model of the body. Five Phase theory is also applied in diagnosis and therapy.
Correspondences between the body and the universe have historically not only been seen in terms of the Five Elements, but also of the "Great Numbers" (大數, ) For example, the number of acu-points has at times been seen to be 365, corresponding with the number of days in a year; and the number of main meridians–12–has been seen as corresponding with the number of rivers flowing through the ancient Chinese empire.
Model of the body.
TCM "holds that the body's vital energy ("chi" or "qi") circulates through channels, called "meridians", that have branches connected to bodily organs and functions." Its view of the human body is only marginally concerned with anatomical structures, but focuses primarily on the body's "functions" (such as digestion, breathing, temperature maintenance, etc.):
These functions are aggregated and then associated with a primary functional entity – for instance, nourishment of the tissues and maintenance of their moisture are seen as connected functions, and the entity postulated to be responsible for these functions is xuě (blood). These functional entities thus constitute "concepts" rather than something with biochemical or anatomical properties.
The primary functional entities used by traditional Chinese medicine are qì, xuě, the five zàng organs, the six fǔ organs, and the meridians which extend through the organ systems. These are all theoretically interconnected: each zàng organ is paired with a fǔ organ, which are nourished by the blood and concentrate qi for a particular function, with meridians being extensions of those functional systems throughout the body.
Concepts of the body and of disease used in TCM have notions of a pre-scientific culture, similar to European humoral theory. – TCM is characterized as full of pseudoscience. Some practitioners no longer consider yin and yang and the idea of an energy flow to apply. Scientific investigation has not found any histological or physiological evidence for traditional Chinese concepts such as "qi", meridians, and acupuncture points. It is a generally held belief within the acupuncture community that acupuncture points and meridians structures are special conduits for electrical signals but no research has established any consistent anatomical structure or function for either acupuncture points or meridians. The scientific evidence for the anatomical existence of either meridians or acupuncture points is not compelling. Stephen Barrett of Quackwatch writes that, "TCM theory and practice are not based upon the body of knowledge related to health, disease, and health care that has been widely accepted by the scientific community. TCM practitioners disagree among themselves about how to diagnose patients and which treatments should go with which diagnoses. Even if they could agree, the TCM theories are so nebulous that no amount of scientific study will enable TCM to offer rational care."
TCM has been the subject of controversy within China. In 2006, the Chinese scholar Zhang Gongyao triggered a national debate when he published an article entitled "Farewell to Traditional Chinese Medicine," arguing that TCM was a pseudoscience that should be abolished in public healthcare and academia. The Chinese government however, interested in the opportunity of export revenues, took the stance that TCM is a science and continued to encourage its development.
Qi.
TCM distinguishes many kinds of qi (). In a general sense, qi is something that is defined by five "cardinal functions":
Vacuity of qi will be characterized especially by pale complexion, lassitude of spirit, lack of strength, spontaneous sweating, laziness to speak, non-digestion of food, shortness of breath (especially on exertion), and a pale and enlarged tongue.
Qi is believed to be partially generated from food and drink, and partially from air (by breathing). Another considerable part of it is inherited from the parents and will be consumed in the course of life.
TCM uses special terms for qi running inside of the blood vessels and for qi that is distributed in the skin, muscles, and tissues between those. The former is called yíng-qì (); its function is to complement xuè and its nature has a strong yin aspect (although qi in general is considered to be yang). The latter is called weì-qì (); its main function is defence and it has pronounced yang nature.
Qi is said to circulate in the meridians. Just as the qi held by each of the zang-fu organs, this is considered to be part of the 'principal' qi () of the body (also called 真氣 , ‘’true‘’ qi, or 原氣 , ‘’original‘’ qi).
Xie.
In contrast to the majority of other functional entities, xuè (血, "blood") is correlated with a physical form – the red liquid running in the blood vessels. Its concept is, nevertheless, defined by its functions: nourishing all parts and tissues of the body, safeguarding an adequate degree of moisture, and sustaining and soothing both consciousness and sleep.
Typical symptoms of a lack of xuě (usually termed "blood vacuity" [血虚, ]) are described as: Pale-white or withered-yellow complexion, dizziness, flowery vision, palpitations, insomnia, numbness of the extremities; pale tongue; "fine" pulse.
Jinye.
Closely related to xuě are the jīnyė (津液, usually translated as "body fluids"), and just like xuě they are considered to be yin in nature, and defined first and foremost by the functions of nurturing and moisturizing the different structures of the body. Their other functions are to harmonize yin and yang, and to help with the secretion of waste products.
Jīnyė are ultimately extracted from food and drink, and constitute the raw material for the production of xuě; conversely, xuě can also be transformed into jīnyė. Their palpable manifestations are all bodily fluids: tears, sputum, saliva, gastric acid, joint fluid, sweat, urine, etc.
Zang-fu.
The zàng-fǔ () constitute the centre piece of TCM's systematization of bodily functions. Bearing the names of organs, they are, however, only secondarily tied to (rudimentary) anatomical assumptions (the fǔ a little more, the zàng much less). As they are primarily defined by their functions, they are not equivalent to the anatomical organs–to highlight this fact, their names are usually capitalized.
The term zàng (臟) refers to the five entities considered to be yin in nature–Heart, Liver, Spleen, Lung, Kidney–, while fǔ (腑) refers to the six yang organs–Small Intestine, Large Intestine, Gallbladder, Urinary Bladder, Stomach and Sānjiaō.
The zàng's essential functions consist in production and storage of qì and xuě; they are said to regulate digestion, breathing, water metabolism, the musculoskeletal system, the skin, the sense organs, aging, emotional processes, and mental activity, among other structures and processes. The fǔ organs' main purpose is merely to transmit and digest (傳化, ) substances such as waste and food.
Since their concept was developed on the basis of Wǔ Xíng philosophy, each zàng is paired with a fǔ, and each zàng-fǔ pair is assigned to one of five elemental qualities (i.e., the Five Elements or Five Phases). These correspondences are stipulated as:
The zàng-fǔ are also connected to the twelve standard meridians–each yang meridian is attached to a fǔ organ, and five of the yin meridians are attached to a zàng. As there are only five zàng but six yin meridians, the sixth is assigned to the Pericardium, a peculiar entity almost similar to the Heart zàng.
Jing-luo.
The meridians (经络, ) are believed to be channels running from the
zàng-fǔ in the interior (里, ) of the body to the limbs and joints ("the surface" [表, ]), transporting qi and xuĕ. TCM identifies 12 "regular" and 8 "extraordinary" meridians; the Chinese terms being 十二经脉 (, lit. "the Twelve Vessels") and 奇经八脉 () respectively. There's also a number of less customary channels branching from the "regular" meridians.
Concept of disease.
In general, disease is perceived as a disharmony (or imbalance) in the functions or interactions of yin, yang, qi, xuĕ, zàng-fǔ, meridians etc. and/or of the interaction between the human body and the environment. Therapy is based on which "pattern of disharmony" can be identified. Thus, "pattern discrimination" is the most important step in TCM diagnosis. It is also known to be the most difficult aspect of practicing TCM.
In order to determine which pattern is at hand, practitioners will examine things like the color and shape of the tongue, the relative strength of pulse-points, the smell of the breath, the quality of breathing or the sound of the voice. For example, depending on tongue and pulse conditions, a TCM practitioner might diagnose bleeding from the mouth and nose as: "Liver fire rushes upwards and scorches the Lung, injuring the blood vessels and giving rise to reckless pouring of blood from the mouth and nose.". He might then go on to prescribe treatments designed to clear heat or supplement the Lung.
Disease entities.
In TCM, a disease has two aspects: "bìng" and "zhèng". The former is often translated as "disease entity", "disease category", "illness", or simply "diagnosis". The latter, and more important one, is usually translated as "pattern" (or sometimes also as "syndrome"). For example, the disease entity of a common cold might present with a pattern of wind-cold in one person, and with the pattern of wind-heat in another.
From a scientific point of view, most of the disease entitites (病, ) listed by TCM constitute mere symptoms. Examples include headache, cough, abdominal pain, constipation etc.
Since therapy will not be chosen according to the disease entity but according to the pattern, two people with the same disease entity but different patterns will receive different therapy. Vice versa, people with similar patterns might receive similar therapy even if their disease entities are different. This is called 异病同治，同病异治 (,"different diseases, same treatment; same disease, different treatments").
Patterns.
In TCM, "pattern" (证, ) refers to a "pattern of disharmony" or "functional disturbance" within the functional entities the TCM model of the body is composed of. There are disharmony patterns of qi, xuě, the body fluids, the zàng-fǔ, and the meridians. They are ultimately defined by their symptoms and "signs" (i.e., for example, pulse and tongue findings).
In clinical practice, the identified pattern usually involves a combination of affected entities (compare with typical examples of patterns). The concrete pattern identified should account for "all" the symptoms a person has.
Six Excesses.
The Six Excesses (六淫, , sometimes also translated as "Pathogenic Factors", or "Six Pernicious Influences"; with the alternative term of 六邪, , – "Six Evils" or "Six Devils") are allegorical terms used to describe disharmony patterns displaying certain typical symptoms. These symptoms resemble the effects of six climatic factors. In the allegory, these symptoms can occur because one or more of those climatic factors (called 六气, , "the six qi") were able to invade the body surface and to proceed to the interior. This is sometimes used to draw causal relationships (i.e., prior exposure to wind/cold/etc. is identified as the cause of a disease), while other authors explicitly deny a direct cause-effect relationship between weather conditions and disease, pointing out that the Six Excesses are primarily descriptions of a certain combination of symptoms translated into a pattern of disharmony. It is undisputed, though, that the Six Excesses can manifest inside the body without an external cause. In this case, they might be denoted "internal", e.g., "internal wind" or "internal fire (or heat)".
The Six Excesses and their characteristic clinical signs are:
Six-Excesses-patterns can consist of only one or a combination of Excesses (e.g., wind-cold, wind-damp-heat). They can also transform from one into another.
Typical examples of patterns.
For each of the functional entities (qi, xuĕ, zàng-fǔ, meridians etc.), typical disharmony patterns are recognized; for example: qi vacuity and qi stagnation in the case of qi; blood vacuity, blood stasis, and blood heat in the case of xuĕ; Spleen qi vacuity, Spleen yang vacuity, Spleen qi vacuity with down-bearing qi, Spleen qi vacuity with lack of blood containment, cold-damp invasion of the Spleen, damp-heat invasion of Spleen and Stomach in case of the Spleen zàng; wind/cold/damp invasion in the case of the meridians.
TCM gives detailed prescriptions of these patterns regarding their typical symptoms, mostly including characteristic tongue and/or pulse findings. For example:
Basic principles of pattern discrimination.
The process of determining which actual pattern is on hand is called 辩证 (, usually translated as "pattern diagnosis", "pattern identification" or "pattern discrimination"). Generally, the first and most important step in pattern diagnosis is an evaluation of the present signs and symptoms on the basis of the "Eight Principles" (八纲, ).
These eight principles refer to four pairs of fundamental qualities of a disease: exterior/interior, heat/cold, vacuity/repletion, and yin/yang. Out of these, heat/cold and vacuity/repletion have the biggest clinical importance. The yin/yang quality, on the other side, has the smallest importance and is somewhat seen aside from the other three pairs, since it merely presents a general and vague conclusion regarding what other qualities are found. In detail, the Eight Principles refer to the following:
After the fundamental nature of a disease in terms of the Eight Principles is determined, the investigation focuses on more specific aspects. By evaluating the present signs and symptoms against the background of typical disharmony patterns of the various entities, evidence is collected whether or how specific entities are affected. This evaluation can be done
There are also three special pattern diagnosis systems used in case of febrile and infectious diseases only ("Six Channel system" or "six division pattern" [六经辩证, ]; "Wei Qi Ying Xue system" or "four division pattern" [卫气营血辩证, ]; "San Jiao system" or "three burners pattern" [三角辩证, ]).
Considerations of disease causes.
Although TCM and its concept of disease do not strongly differentiate between cause and effect, pattern discrimination can include considerations regarding the disease cause; this is called 病因辩证 (, "disease-cause pattern discrimination").
There are three fundamental categories of disease causes (三因, ) recognized:
Diagnostics.
In TCM, there are five diagnostic methods: inspection, auscultation, olfaction, inquiry, and palpation.
Tongue and pulse.
Examination of the tongue and the pulse are among the principal diagnostic methods in TCM. Certain sectors of the tongue's surface are believed to correspond to the zàng-fŭ. For example, teeth marks on one part of the tongue might indicate a problem with the Heart, while teeth marks on another part of the tongue might indicate a problem with the Liver.
Pulse palpation involves measuring the pulse both at a superficial and at a deep level at three different locations on the radial artery ("Cun, Guan, Chi", located two fingerbreadths from the wrist crease, one fingerbreadth from the wrist crease, and right at the wrist crease, respectively, usually palpated with the index, middle and ring finger) of each arm, for a total of twelve pulses, all of which are thought to correspond with certain zàng-fŭ. The pulse is examined for several characteristics including rhythm, strength and volume, and described with qualities like "floating, slippery, bolstering-like, feeble, thready and quick"; each of these qualities indicate certain disease patterns. Learning TCM pulse diagnosis can take several years.
Herbal medicine.
The term "herbal medicine" is somewhat misleading in that, while plant elements are by far the most commonly used substances in TCM, other, non-botanic substances are used as well: animal, human, and mineral products are also utilized. Thus, the term "medicinal" (instead of herb) is usually preferred.
Prescriptions.
Typically, one batch of medicinals is prepared as a decoction of about 9 to 18 substances. Some of these are considered as main herbs, some as ancillary herbs; within the ancillary herbs, up to three categories can be distinguished.
Raw materials.
There are roughly 13,000 medicinals used in China and over 100,000 medicinal recipes recorded in the ancient literature. In the classic "Handbook of Traditional Drugs" from 1941, 517 drugs were listed – out of these, 45 were animal parts, and 30 were minerals.
Animal substances.
Some animal parts used as medicinals can be considered rather strange such as cows' gallstones, hornet's nest, leech, and scorpion. Other examples of animal parts include horn of the antelope or buffalo, deer antlers, testicles and penis bone of the dog, and snake bile. Some TCM textbooks still recommend preparations containing animal tissues, but there has been little research to justify the claimed clinical efficacy of many TCM animal products.
Some medicinals can include the parts of endangered species, including tiger bones and rhinoceros horn
which is used for many ailments (though not as an aphrodisiac as is commonly misunderstood by the West). 
The black market in rhinoceros horn (driven not just by TMC but also unrelated status-seeking) has reduced the world's rhino population by more than 90 percent over the past 40 years.
Concerns have also arisen over the use of turtle plastron, seahorses, and the gill plates of mobula and manta rays. Poachers hunt restricted or endangered species animals to supply the black market with TCM products. There is no scientific evidence of efficacy for tiger medicines. Concern over China considering to legalize the trade in tiger parts prompted the 171-nation Convention on International Trade in Endangered Species (CITES) to endorse a decision opposing the resurgence of trade in tigers. Fewer than 30,000 saiga antelopes remain, which are exported to China for use in traditional fever therapies. Organized gangs illegally export the horn of the antelopes to China. The pressures on seahorses (Hippocampus spp.) used in traditional medicine is large; tens of millions of animals are unsustainably caught annually. Many species of syngnathid are currently part of the IUCN Red List of Threatened Species or national equivalents.
Since TCM recognizes bear bile as a medicinal, more than 12,000 asiatic black bears are held in bear farms. The bile is extracted through a permanent hole in the abdomen leading to the gall bladder, which can cause severe pain. This can lead to bears trying to kill themselves. As of 2012, approximately 10,000 bears are farmed in China for their bile. This practice has spurred public outcry across the country. The bile is collected from live bears via a surgical procedure. The deer penis is believed to have therapeutic benefits according to traditional Chinese medicine. It is typically very big and, proponents believe, in order to preserve its properties, it should be extracted from a living deer. Medicinal tiger parts from poached animals include tiger penis, believed to improve virility, and tiger eyes. The illegal trade for tiger parts in China has driven the species to near-extinction because of its popularity in traditional medicine. Laws protecting even critically endangered species such as the Sumatran tiger fail to stop the display and sale of these items in open markets. Shark fin soup is traditionally regarded in Chinese medicine as beneficial for health in East Asia, and its status as an elite dish has led to huge demand with the increase of affluence in China, devastating shark populations. The shark fins have been a part of traditional Chinese medicine for centuries. Shark finning is banned in many countries, but the trade is thriving in Hong Kong and China, where the fins are part of shark fin soup, a dish considered a delicacy, and used in some types of traditional Chinese medicine.
The tortoise (guiban) and the turtle (biejia) species used in traditional Chinese medicine are raised on farms, while restrictions are made on the accumulation and export of other endangered species. However, issues concerning the overexploitation of Asian turtles in China have not been completely solved. Australian scientists have developed methods to identify medicines containing DNA traces of endangered species.
Human body parts.
Traditional Chinese Medicine also includes some human parts: the classic Materia medica (Bencao Gangmu) describes the use of 35 human body parts and excreta in medicines, including bones, fingernail, hairs, dandruff, earwax, impurities on the teeth, feces, urine, sweat, organs, but most are no longer in use.
Human placenta has been used an ingredient in certain traditional Chinese medicines, including using dried human placenta, known as "Ziheche", to treat infertility, impotence and other conditions. The consumption of the human placenta is a potential source of infection.
Traditional categorization.
The traditional categorizations and classifications that can still be found today are:
The classification according to the Four Natures (四气, ): hot, warm, cool, or cold (or, neutral in terms of temperature) and hot and warm herbs are used to treat cold diseases, while cool and cold herbs are used to treat heat diseases.
The classification according to the Five Flavors, (五味, , sometimes also translated as Five Tastes): acrid, sweet, bitter, sour, and salty. Substances may also have more than one flavor, or none (i.e., a "bland" flavor). Each of the Five Flavors corresponds to one of zàng organs, which in turn corresponds to one of the Five Phases. A flavor implies certain properties and therapeutic actions of a substance; e.g., saltiness drains downward and softens hard masses, while sweetness is supplementing, harmonizing, and moistening.
The classification according to the meridian – more precise, the zàng-organ including its associated meridian – which can be expected to be primarily affected by a given medicinal.
The categorization according to the specific function mainly include: exterior-releasing or exterior-resolving, heat-clearing, downward-draining, or precipitating wind-damp-dispelling, dampness-transforming, promoting the movement of water and percolating dampness or dampness-percolating, interior-warming, qi-regulating or qi-rectifying, dispersing food accumulation or food-dispersing, worm-expelling, stopping bleeding or blood-stanching, quickening the Blood and dispelling stasis or blood-quickening, transforming phlegm, stopping coughing and calming wheezing or phlegm-transforming and cough- and panting-suppressing, Spirit-quieting, calming the liver and expelling wind or liver-calming and wind-extinguishingl orifice-openingl supplementing which includes qi-supplementing, blood-nourishing, yin-enriching, and yang-fortifying, astriction-promoting or securing and astringing, vomiting-inducing, and substances for external application.
Efficacy.
A 2013 review found the data too weak to support use of Chinese herbal medicine (CHM) for benign prostatic hyperplasia. A 2013 review found the research on the benefit and safety of CHM for idiopathic sudden sensorineural hearing loss is of poor quality and cannot be relied upon to support their use. A 2013 Cochrane review found inconclusive evidence that CHM reduces the severity of eczema. The traditional medicine ginger, which has shown anti-inflammatory properties in laboratory experiments, has been used to treat rheumatism, headache and digestive and respiratory issues, though there is no firm evidence supporting these uses. A 2012 Cochrane review found no difference in decreased mortality when Chinese herbs were used alongside Western medicine versus Western medicine exclusively. A 2012 Cochrane review found insufficient evidence to support the use of TCM for people with adhesive small bowel obstruction. A 2011 review found low quality evidence that suggests CHM improves the symptoms of Sjogren's syndrome. A 2010 review found TCM seems to be effective for the treatment of fibromyalgia but the findings were of insufficient methodological rigor. A 2009 Cochrane review found insufficient evidence to recommend the use of TCM for the treatment of epilepsy. A 2008 Cochrane review found promising evidence for the use of Chinese herbal medicine in relieving painful menstruation, but the trials assessed were of such low methodological quality that no conclusion could be drawn about the remedies' suitability as a recommendable treatment option. Turmeric has been used in traditional Chinese medicine for centuries to treat various conditions. This includes jaundice and hepatic disorders, rheumatism, anorexia, diabetic wounds, and menstrual complications. Most of its effects have been attributed to curcumin. Research that curcumin shows strong anti-inflammatory and antioxidant activities have instigated mechanism of action studies on the possibility for cancer and inflammatory diseases prevention and treatment. It also exhibits immunomodulatory effects. A 2005 Cochrane review found insufficient evidence for the use of CHM in HIV-infected people and people with AIDS.
Drug research.
With an eye to the enormous Chinese market, pharmaceutical companies have explored the potential for creating new drugs from traditional remedies. A "Nature" editorial described TCM as "fraught with pseudoscience", and stated that having "no rational mechanism of action for most of its therapies" is the "most obvious answer" to why its study didn't provide a "flood of cures", while advocates responded that "researchers are missing aspects of the art, notably the interactions between different ingredients in traditional therapies."
One of the few successes was the development in the 1970s of the antimalarial drug artemisinin, which is a processed extract of "Artemisia annua", a herb traditionally used as a fever treatment. Researcher Tu Youyou discovered that a low-temperature extraction process could isolate an effective antimalarial substance from the plant. She says she was influenced by a traditional source saying that this herb should be steeped in cold water, after initially finding high-temperature extraction unsatisfactory. The extracted substance, once subject to detoxification and purification processes, is a usable antimalarial drug – a 2012 review found that artemisinin-based remedies were the most effective drugs for the treatment of malaria. For her work on malaria, Tu received the 2015 Nobel Prize in Physiology or Medicine. Despite global efforts in combating malaria, it remains a large burden for the population. Although WHO recommends artemisinin-based remedies for treating uncomplicated malaria, artemisinin resistance can no longer be ignored.
Also in the 1970s Chinese researcher Zhang TingDong and colleagues investigated the potential use of the traditionally used substance arsenic trioxide to treat acute promyelocytic leukemia (APL). Building on his work, research both in China and the West eventually led to the development of the drug Trisenox, which was approved for leukemia treatment by the FDA in 2000.
Huperzine A, which is extracted from traditional herb "Huperzia serrata", has attracted the interest of medical science because of alleged neuroprotective properties. Despite earlier promising results, a 2013 systematic review and meta-analysis found "Huperzine A appears to have beneficial effects on improvement of cognitive function, daily living activity, and global clinical assessment in participants with Alzheimer’s disease. However, the findings should be interpreted with caution due to the poor methodological quality of the included trials."
Ephedrine in its natural form, known as "má huáng" (麻黄) in traditional Chinese medicine, has been documented in China since the Han dynasty (206 BC – 220 AD) as an antiasthmatic and stimulant. In 1885, the chemical synthesis of ephedrine was first accomplished by Japanese organic chemist Nagai Nagayoshi based on his research on Japanese and Chinese traditional herbal medicines
Cost-effectiveness.
A 2012 systematic review found there is a lack of available cost-effectiveness evidence in TCM.
Safety.
From the earliest records regarding the use of medicinals to today, the toxicity of certain substances has been described in all Chinese materiae medicae. Since TCM has become more popular in the Western world, there are increasing concerns about the potential toxicity of many traditional Chinese medicinals including plants, animal parts and minerals. Traditional Chinese herbal remedies are conveniently available from grocery stores in most Chinese neighborhoods; some of these items may contain toxic ingredients, are imported into the U.S. illegally, and are associated with claims of therapeutic benefit without evidence. For most medicinals, efficacy and toxicity testing are based on traditional knowledge rather than laboratory analysis. The toxicity in some cases could be confirmed by modern research (i.e., in scorpion); in some cases it couldn't (i.e., in "Curculigo"). Traditional herbal medicines can contain extremely toxic chemicals and heavy metals, and naturally occurring toxins, which can cause illness, exacerbate pre-existing poor health or result in death. Botanical misidentification of plants can cause toxic reactions in humans. The description on some plants used in traditional Chinese medicine have changed, leading to unintended intoxication of the wrong plants. A concern is also contaminated herbal medicines with microorganisms and fungal toxins, including aflatoxin. Traditional herbal medicines are sometimes contaminated with toxic heavy metals, including lead, arsenic, mercury and cadmium, which inflict serious health risks to consumers.
Substances known to be potentially dangerous include "Aconitum", secretions from the Asiatic toad, powdered centipede, the Chinese beetle ("Mylabris phalerata"), certain fungi, "Aristolochia", "Aconitum", Arsenic sulfide (Realgar), mercury sulfide, and cinnabar. Asbestos ore (Actinolite, Yang Qi Shi, 阳起石) is used to treat impotence in TCM. Lead, mercury, arsenic, copper, cadmium, and thallium have been detected in TCM products sold in the U.S. and China.
To avoid its toxic adverse effects "Xanthium sibiricum" must be processed. Hepatotoxicity has been reported with products containing "Polygonum multiflorum", glycyrrhizin, "Senecio" and "Symphytum". The herbs indicated as being hepatotoxic included "Dictamnus dasycarpus", "Astragalus membranaceous", and "Paeonia lactiflora". Contrary to popular belief, "Ganoderma lucidum" mushroom extract, as an adjuvant for cancer immunotherapy, appears to have the potential for toxicity. A 2013 review suggested that although the antimalarial herb "Artemisia annua" may not cause hepatotoxicity, haematotoxicity, or hyperlipidemia, it should be used cautiously during pregnancy due to a potential risk of embryotoxicity at a high dose.
However, many adverse reactions are due misuse or abuse of Chinese medicine. For example, the misuse of the dietary supplement "Ephedra" (containing ephedrine) can lead to adverse events including gastrointestinal problems as well as sudden death from cardiomyopathy. Products adulterated with pharmaceuticals for weight loss or erectile dysfunction are one of the main concerns. Chinese herbal medicine has been a major cause of acute liver failure in China.
Acupuncture and moxibustion.
Acupuncture means insertion of needles into superficial structures of the body (skin, subcutaneous tissue, muscles) – usually at acupuncture points (acupoints) – and their subsequent manipulation; this aims at influencing the flow of qi. According to TCM it relieves pain and treats (and prevents) various diseases.
Acupuncture is often accompanied by moxibustion – the Chinese characters for acupuncture () literally meaning "acupuncture-moxibustion" – which involves burning mugwort on or near the skin at an acupuncture point. According to the American Cancer Society, "available scientific evidence does not support claims that moxibustion is effective in preventing or treating cancer or any other disease".
In electroacupuncture, an electric current is applied to the needles once they are inserted, in order to further stimulate the respective acupuncture points.
Efficacy.
A 2013 editorial by Steven P. Novella and David Colquhoun found that the inconsistency of results of acupuncture studies (i.e. acupuncture relieved pain in some conditions but had no effect in other very similar conditions) suggests false positive results, which may be caused by factors like biased study designs, poor blinding, and the classification of electrified needles (a type of TENS) as a form of acupuncture. The same editorial suggested that given the inability to find consistent results despite more than 3,000 studies of acupuncture, the treatment seems to be a placebo effect and the existing equivocal positive results are noise one expects to see after a large number of studies are performed on an inert therapy. The editorial concluded that the best controlled studies showed a clear pattern, in which the outcome does not rely upon needle location or even needle insertion, and since "these variables are those that define acupuncture, the only sensible conclusion is that acupuncture does not work."
A 2012 meta-analysis concluded that the mechanisms of acupuncture "are clinically relevant, but that an important part of these total effects is not due to issues considered to be crucial by most acupuncturists, such as the correct location of points and depth of needling ... are ... associated with more potent placebo or context effects". Commenting on this meta-analysis, both Edzard Ernst and David Colquhoun said the results were of negligible clinical significance.
A 2011 overview of Cochrane reviews found high quality evidence that suggests acupuncture is effective for some but not all kinds of pain. A 2010 systematic review found that there is evidence "that acupuncture provides a short-term clinically relevant effect when compared with a waiting list control or when acupuncture is added to another intervention" in the treatment of chronic low back pain. Two review articles discussing the effectiveness of acupuncture, from 2008 and 2009, have concluded that there is not enough evidence to conclude that it is effective beyond the placebo effect.
Acupuncture is generally safe when administered using Clean Needle Technique (CNT). Although serious adverse effects are rare, acupuncture is not without risk. Severe adverse effects, including death have continued to be reported.
Tui na.
Tui na (推拿) is a form of massage akin to acupressure (from which shiatsu evolved). Asian massage is typically administered with the person fully clothed, without the application of grease or oils. Techniques employed may include thumb presses, rubbing, percussion, and assisted stretching.
Qigong.
Qìgōng (气功 or 氣功) is a TCM system of exercise and meditation that combines regulated breathing, slow movement, and focused awareness, purportedly to cultivate and balance qi. One branch of qigong is qigong massage, in which the practitioner combines massage techniques with awareness of the acupuncture channels and points.
Other therapies.
Cupping.
Cupping (Chinese: 拔罐; pinyin: báguàn) is a type of Chinese massage, consisting of placing several glass "cups" (open spheres) on the body. A match is lit and placed inside the cup and then removed before placing the cup against the skin. As the air in the cup is heated, it expands, and after placing in the skin, cools, creating lower pressure inside the cup that allows the cup to stick to the skin via suction. When combined with massage oil, the cups can be slid around the back, offering "reverse-pressure massage".
It has not been found to be effective for the treatment of any disease. The 2008 "Trick or Treatment" book said that no evidence exists of any beneficial effects of cupping for any medical condition.
Gua Sha.
Gua Sha (Chinese: 刮痧； pinyin: guāshā) is abrading the skin with pieces of smooth jade, bone, animal tusks or horns or smooth stones; until red spots then bruising cover the area to which it is done. It is believed that this treatment is for almost any ailment including cholera. The red spots and bruising take 3 to 10 days to heal, there is often some soreness in the area that has been treated.
Die-da.
Diē-dá (跌打) or bone-setting is usually practiced by martial artists who know aspects of Chinese medicine that apply to the treatment of trauma and injuries such as bone fractures, sprains, and bruises. Some of these specialists may also use or recommend other disciplines of Chinese medical therapies (or Western medicine in modern times) if serious injury is involved. Such practice of bone-setting (整骨 or 正骨) is not common in the West.
Chinese food therapy.
Traditional Chinese characters and for the words "yin" and "yang" denote different classes of foods, and it is important to consume them in a balanced fashion. The meal sequence should also observe these classes:
Regulations.
Many governments have enacted laws to regulate TCM practice.
Australia.
From 1 July 2012 Chinese medicine practitioners must be registered under the national registration and accreditation scheme with the Chinese Medicine Board of Australia and meet the Board's Registration Standards, in order to practice in Australia.
Canada.
TCM is regulated in five provinces in Canada: Alberta, British Columbia, Ontario, Quebec, and Newfoundland.
Hong Kong.
The Chinese Medicine Council of Hong Kong was established in 1999. It regulates the medicinals and professional standards for TCM practitioners. All TCM practitioners in Hong Kong are required to register with the Council. The eligibility for registration includes a recognised 5-year university degree of TCM, a 30-week minimum supervised clinical internship, and passing the licensing exam.
Malaysia.
The Traditional and Complementary Medicine Bill was passed by Parliament in 2012 establishing the Traditional and Complementary Medicine Council to register and regulate traditional and complementary medicine practitioners, including traditional Chinese medicine practitioners as well as other traditional and complementary medicine practitioners such as those in traditional Malay medicine and traditional Indian medicine.
Singapore.
The TCM Practitioners Act was passed by Parliament in 2000 and the TCM Practitioners Board was established in 2001 as a statutory board under the Ministry of Health, to register and regulate TCM practitioners. The requirements for registration include possession of a diploma or degree from a TCM educational institution/university on a gazetted list, either structured TCM clinical training at an approved local TCM educational institution or foreign TCM registration together with supervised TCM clinical attachment/practice at an approved local TCM clinic, and upon meeting these requirements, passing the Singapore TCM Physicians Registration Examination (STRE) conducted by the TCM Practitioners Board.
United States.
As of July 2012, only six states do not have existing legislation to regulate the professional practice of TCM. These six states are Alabama, Kansas, North Dakota, South Dakota, Oklahoma, and Wyoming. In 1976, California established an Acupuncture Board and became the first state licensing professional acupuncturists.
Indonesia.
All traditional medicines, including TCM, are regulated on Indonesian Minister of Health Regulation in 2013 about Traditional Medicine. Traditional Medicine License ("Surat Izin Pengobatan Tradisional" -SIPT) will be granted to the practitioners whose methods are scientifically recognized as safe and bring the benefit for health. The TCM clinics are registered but there is no explicit regulation for it. The only TCM method which is accepted by medical logic and is empirically proofed is acupuncture. The acupuncturists can get SIPT and participate on health care facilities.

</doc>
<doc id="5993" url="https://en.wikipedia.org/wiki?curid=5993" title="Chemical bond">
Chemical bond

A chemical bond is a lasting attraction between atoms that enables the formation of chemical compounds. The bond may result from the electrostatic force of attraction between atoms with opposite charges, or through the sharing of electrons as in the covalent bonds. The strength of chemical bonds varies considerably; there are "strong bonds" such as covalent or ionic bonds and "weak bonds" such as Dipole-dipole interaction, the London dispersion force and hydrogen bonding.
Since opposite charges attract via a simple electromagnetic force, the negatively charged electrons that are orbiting the nucleus and the positively charged protons in the nucleus attract each other. An electron positioned between two nuclei will be attracted to both of them, and the nuclei will be attracted toward electrons in this position. This attraction constitutes the chemical bond. Due to the matter wave nature of electrons and their smaller mass, they must occupy a much larger amount of volume compared with the nuclei, and this volume occupied by the electrons keeps the atomic nuclei relatively far apart, as compared with the size of the nuclei themselves. This phenomenon limits the distance between nuclei and atoms in a bond.
In general, strong chemical bonding is associated with the sharing or transfer of electrons between the participating atoms. The atoms in molecules, crystals, metals and diatomic gases—indeed most of the physical environment around us—are held together by chemical bonds, which dictate the structure and the bulk properties of matter.
All bonds can be explained by quantum theory, but, in practice, simplification rules allow chemists to predict the strength, directionality, and polarity of bonds. The octet rule and VSEPR theory are two examples. More sophisticated theories are valence bond theory which includes orbital hybridization and resonance, and the linear combination of atomic orbitals molecular orbital method which includes ligand field theory. Electrostatics are used to describe bond polarities and the effects they have on chemical substances.
Overview of main types of chemical bonds.
A chemical bond is an attraction between atoms. This attraction may be seen as the result of different behaviors of the outermost or valence electrons of atoms. Although all of these behaviors merge into each other seamlessly in various bonding situations so that there is no clear line to be drawn between them, the behaviors of atoms become so qualitatively different as the character of the bond changes quantitatively, that it remains useful and customary to differentiate between the bonds that cause these different properties of condensed matter.
In the simplest view of a covalent bond one or more electrons (often a pair of electrons) are drawn into the space between the two atomic nuclei. The reduction in energy from bond formation comes not from the reduction in potential energy, because the attraction of the two electrons to the two protons is offset by the electron-electron and proton-proton repulsions. Instead, the reduction in overall energy (and hence stability of the bond) arises from the reduction in kinetic energy due to the electrons being in a more spatially distributed (i.e., longer deBroglie wavelength) orbital compared with each electron being confined closer to its respective nucleus. These bonds exist between two particular identifiable atoms and have a direction in space, allowing them to be shown as single connecting lines between atoms in drawings, or modeled as sticks between spheres in models.
In a polar covalent bond, one or more electrons are unequally shared between two nuclei. Covalent bonds often result in the formation of small collections of better-connected atoms called molecules, which in solids and liquids are bound to other molecules by forces that are often much weaker than the covalent bonds that hold the molecules internally together. Such weak intermolecular bonds give organic molecular substances, such as waxes and oils, their soft bulk character, and their low melting points (in liquids, molecules must cease most structured or oriented contact with each other). When covalent bonds link long chains of atoms in large molecules, however (as in polymers such as nylon), or when covalent bonds extend in networks through solids that are not composed of discrete molecules (such as diamond or quartz or the silicate minerals in many types of rock) then the structures that result may be both strong and tough, at least in the direction oriented correctly with networks of covalent bonds. Also, the melting points of such covalent polymers and networks increase greatly.
In a simplified view of an "ionic" bond, the bonding electron is not shared at all, but transferred. In this type of bond, the outer atomic orbital of one atom has a vacancy which allows the addition of one or more electrons. These newly added electrons potentially occupy a lower energy-state (effectively closer to more nuclear charge) than they experience in a different atom. Thus, one nucleus offers a more tightly bound position to an electron than does another nucleus, with the result that one atom may transfer an electron to the other. This transfer causes one atom to assume a net positive charge, and the other to assume a net negative charge. The "bond" then results from electrostatic attraction between atoms and the atoms become positive or negatively charged ions. Ionic bonds may be seen as extreme examples of polarization in covalent bonds. Often, such bonds have no particular orientation in space, since they result from equal electrostatic attraction of each ion to all ions around them. Ionic bonds are strong (and thus ionic substances require high temperatures to melt) but also brittle, since the forces between ions are short-range and do not easily bridge cracks and fractures. This type of bond gives rise to the physical characteristics of crystals of classic mineral salts, such as table salt.
A less often mentioned type of bonding is "metallic" bonding. In this type of bonding, each atom in a metal donates one or more electrons to a "sea" of electrons that reside between many metal atoms. In this sea, each electron is free (by virtue of its wave nature) to be associated with great many atoms at once. The bond results because the metal atoms become somewhat positively charged due to loss of their electrons while the electrons remain attracted to many atoms, without being part of any given atom. Metallic bonding may be seen as an extreme example of delocalization of electrons over a large system of covalent bonds, in which every atom participates. This type of bonding is often very strong (resulting in the tensile strength of metals). However, metallic bonding is more collective in nature than other types, and so they allow metal crystals to more easily deform, because they are composed of atoms attracted to each other, but not in any particularly-oriented ways. This results in the malleability of metals. The sea of electrons in metallic bonding causes the characteristically good electrical and thermal conductivity of metals, and also their "shiny" reflection of most frequencies of white light.
History.
Early speculations into the nature of the chemical bond, from as early as the 12th century, supposed that certain types of chemical species were joined by a type of chemical affinity. In 1704, Sir Isaac Newton famously outlined his atomic bonding theory, in "Query 31" of his Opticks, whereby atoms attach to each other by some "force". Specifically, after acknowledging the various popular theories in vogue at the time, of how atoms were reasoned to attach to each other, i.e. "hooked atoms", "glued together by rest", or "stuck together by conspiring motions", Newton states that he would rather infer from their cohesion, that "particles attract one another by some force, which in immediate contact is exceedingly strong, at small distances performs the chemical operations, and reaches not far from the particles with any sensible effect."
In 1819, on the heels of the invention of the voltaic pile, Jöns Jakob Berzelius developed a theory of chemical combination stressing the electronegative and electropositive character of the combining atoms. By the mid 19th century, Edward Frankland, F.A. Kekulé, A.S. Couper, Alexander Butlerov, and Hermann Kolbe, building on the theory of radicals, developed the theory of valency, originally called "combining power", in which compounds were joined owing to an attraction of positive and negative poles. In 1916, chemist Gilbert N. Lewis developed the concept of the electron-pair bond, in which two atoms may share one to six electrons, thus forming the single electron bond, a single bond, a double bond, or a triple bond; in Lewis's own words, "An electron may form a part of the shell of two different atoms and cannot be said to belong to either one exclusively."
That same year, Walther Kossel put forward a theory similar to Lewis' only his model assumed complete transfers of electrons between atoms, and was thus a model of ionic bonding. Both Lewis and Kossel structured their bonding models on that of Abegg's rule (1904).
In 1927, the first mathematically complete quantum description of a simple chemical bond, i.e. that produced by one electron in the hydrogen molecular ion, H2+, was derived by the Danish physicist Oyvind Burrau. This work showed that the quantum approach to chemical bonds could be fundamentally and quantitatively correct, but the mathematical methods used could not be extended to molecules containing more than one electron. A more practical, albeit less quantitative, approach was put forward in the same year by Walter Heitler and Fritz London. The Heitler-London method forms the basis of what is now called valence bond theory. In 1929, the linear combination of atomic orbitals molecular orbital method (LCAO) approximation was introduced by Sir John Lennard-Jones, who also suggested methods to derive electronic structures of molecules of F2 (fluorine) and O2 (oxygen) molecules, from basic quantum principles. This molecular orbital theory represented a covalent bond as an orbital formed by combining the quantum mechanical Schrödinger atomic orbitals which had been hypothesized for electrons in single atoms. The equations for bonding electrons in multi-electron atoms could not be solved to mathematical perfection (i.e., "analytically"), but approximations for them still gave many good qualitative predictions and results. Most quantitative calculations in modern quantum chemistry use either valence bond or molecular orbital theory as a starting point, although a third approach, density functional theory, has become increasingly popular in recent years.
In 1933, H. H. James and A. S. Coolidge carried out a calculation on the dihydrogen molecule that, unlike all previous calculation which used functions only of the distance of the electron from the atomic nucleus, used functions which also explicitly added the distance between the two electrons. With up to 13 adjustable parameters they obtained a result very close to the experimental result for the dissociation energy. Later extensions have used up to 54 parameters and gave excellent agreement with experiments. This calculation convinced the scientific community that quantum theory could give agreement with experiment. However this approach has none of the physical pictures of the valence bond and molecular orbital theories and is difficult to extend to larger molecules.
Bonds in chemical formulas.
The fact that atoms and molecules are three-dimensional makes it difficult to use a single technique for indicating orbitals and bonds. In molecular formulas the chemical bonds (binding orbitals) between atoms are indicated by various methods according to the type of discussion. Sometimes, they are completely neglected. For example, in organic chemistry chemists are sometimes concerned only with the functional group of the molecule. Thus, the molecular formula of ethanol may be written in a paper in conformational form, three-dimensional form, full two-dimensional form (indicating every bond with no three-dimensional directions), compressed two-dimensional form (CH3–CH2–OH), by separating the functional group from another part of the molecule (C2H5OH), or by its atomic constituents (C2H6O), according to what is discussed. Sometimes, even the non-bonding valence shell electrons (with the two-dimensional approximate directions) are marked, i.e. for elemental carbon .'C'. Some chemists may also mark the respective orbitals, i.e. the hypothetical ethene−4 anion (\/C=C/\ −4) indicating the possibility of bond formation.
Strong chemical bonds.
Strong chemical bonds are the "intramolecular" forces which hold atoms together in molecules. A strong chemical bond is formed from the transfer or sharing of electrons between atomic centers and relies on the electrostatic attraction between the protons in nuclei and the electrons in the orbitals.
The types of strong bond differ due to the difference in electronegativity of the constituent elements. A large difference in electronegativity leads to more polar (ionic) character in the bond.
Ionic bonding.
Ionic bonding is a type of electrostatic interaction between atoms which have a large electronegativity difference. There is no precise value that distinguishes ionic from covalent bonding, but a difference of electronegativity of over 1.7 is likely to be ionic, and a difference of less than 1.7 is likely to be covalent. Ionic bonding leads to separate positive and negative ions. Ionic charges are commonly between −3e to +3e.
Ionic bonding commonly occurs in metal salts such as sodium chloride (table salt). A typical feature of ionic bonds is that the species form into ionic crystals, in which no ion is specifically paired with any single other ion, in a specific directional bond. Rather, each species of ion is surrounded by ions of the opposite charge, and the spacing between it and each of the oppositely charged ions near it, is the same for all surrounding atoms of the same type. It is thus no longer possible to associate an ion with any specific other single ionized atom near it. This is a situation unlike that in covalent crystals, where covalent bonds between specific atoms are still discernible from the shorter distances between them, as measured via such techniques as X-ray diffraction.
Ionic crystals may contain a mixture of covalent and ionic species, as for example salts of complex acids, such as sodium cyanide, NaCN. X-ray diffraction shows that in NaCN, for example, the bonds between sodium cations (Na+) and the cyanide anions (CN−) are "ionic", with no sodium ion associated with any particular cyanide. However, the bonds between C and N atoms in cyanide are of the "covalent" type, making each of the carbon and nitrogen associated with "just one" of its opposite type, to which it is physically much closer than it is to other carbons or nitrogens in a sodium cyanide crystal.
When such crystals are melted into liquids, the ionic bonds are broken first because they are non-directional and allow the charged species to move freely. Similarly, when such salts dissolve into water, the ionic bonds are typically broken by the interaction with water, but the covalent bonds continue to hold. For example, in solution, the cyanide ions, still bound together as single CN− ions, move independently through the solution, as do sodium ions, as Na+. In water, charged ions move apart because each of them are more strongly attracted to a number of water molecules, than to each other. The attraction between ions and water molecules in such solutions is due to a type of weak dipole-dipole type chemical bond. In melted ionic compounds, the ions continue to be attracted to each other, but not in any ordered or crystalline way.
Covalent bond.
Covalent bonding is a common type of bonding, in which the electronegativity difference between the bonded atoms is small or nonexistent. Bonds within most organic compounds are described as covalent. See sigma bonds and pi bonds for LCAO-description of such bonding.
A polar covalent bond is a covalent bond with a significant ionic character. This means that the electrons are closer to one of the atoms than the other, creating an imbalance of charge. They occur as a bond between two atoms with moderately different electronegativities and give rise to dipole-dipole interactions. The electronegativity of these bonds is 0.3 to 1.7.
A coordinate covalent bond is one where both bonding electrons are from one of the atoms involved in the bond. These bonds give rise to Lewis acids and bases. The electrons are shared roughly equally between the atoms in contrast to ionic bonding. Such bonding occurs in molecules such as the ammonium ion (NH4+) and are shown by an arrow pointing to the Lewis acid. Also known as non-polar covalent bond, the electronegativity of these bonds range from 0 to 0.3.
Molecules which are formed primarily from non-polar covalent bonds are often immiscible in water or other polar solvents, but much more soluble in non-polar solvents such as hexane.
Single and multiple bonds.
A single bond between two atoms corresponds to the sharing of one pair of electrons. The electron density of these two bonding electrons is concentrated in the region between the two atoms, which is the defining quality of a sigma bond.
A double bond between two atoms is formed by the sharing of two pairs of electrons, one in a sigma bond and one in a pi bond, with electron density concentrated on two opposite sides of the internuclear axis. A triple bond consists of three shared electron pairs, forming one sigma and two pi bonds.
Quadruple and higher bonds are very rare and occur only between certain transition metal atoms.
Metallic bonding.
In metallic bonding, bonding electrons are delocalized over a lattice of atoms. By contrast, in ionic compounds, the locations of the binding electrons and their charges are static. The freely-moving or delocalization of bonding electrons leads to classical metallic properties such as luster (surface light reflectivity), electrical and thermal conductivity, ductility, and high tensile strength.
Intermolecular bonding.
There are four basic types of bonds that can be formed between two or more (otherwise non-associated) molecules, ions or atoms. Intermolecular forces cause molecules to be attracted or repulsed by each other. Often, these define some of the physical characteristics (such as the melting point) of a substance.
Theories of chemical bonding.
In the (unrealistic) limit of "pure" ionic bonding, electrons are perfectly localized on one of the two atoms in the bond. Such bonds can be understood by classical physics. The forces between the atoms are characterized by isotropic continuum electrostatic potentials. Their magnitude is in simple proportion to the charge difference.
Covalent bonds are better understood by valence bond theory or molecular orbital theory. The properties of the atoms involved can be understood using concepts such as oxidation number. The electron density within a bond is not assigned to individual atoms, but is instead delocalized between atoms. In valence bond theory, the two electrons on the two atoms are coupled together with the bond strength depending on the overlap between them. In molecular orbital theory, the linear combination of atomic orbitals (LCAO) helps describe the delocalized molecular orbital structures and energies based on the atomic orbitals of the atoms they came from. Unlike pure ionic bonds, covalent bonds may have directed anisotropic properties. These may have their own names, such as sigma bond and pi bond.
In the general case, atoms form bonds that are intermediate between ionic and covalent, depending on the relative electronegativity of the atoms involved. This type of bond is sometimes called polar covalent.

</doc>
<doc id="5995" url="https://en.wikipedia.org/wiki?curid=5995" title="Cell">
Cell

Cell may refer to:

</doc>
<doc id="5999" url="https://en.wikipedia.org/wiki?curid=5999" title="Climate">
Climate

Climate is the statistics (usually, mean or variability) of weather, usually over a 30-year interval. It is measured by assessing the patterns of variation in temperature, humidity, atmospheric pressure, wind, precipitation, atmospheric particle count and other meteorological variables in a given region over long periods of time. Climate differs from weather, in that weather only describes the short-term conditions of these variables in a given region.
A region's climate is generated by the climate system, which has five components: atmosphere, hydrosphere, cryosphere, lithosphere, and biosphere.
The climate of a location is affected by its latitude, terrain, and altitude, as well as nearby water bodies and their currents. Climates can be classified according to the average and the typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme was Köppen climate classification originally developed by Wladimir Köppen. The Thornthwaite system, in use since 1948, incorporates evapotranspiration along with temperature and precipitation information and is used in studying biological diversity and the potential effects on it of climate changes. The Bergeron and Spatial Synoptic Classification systems focus on the origin of air masses that define the climate of a region.
Paleoclimatology is the study of ancient climates. Since direct observations of climate are not available before the 19th century, paleoclimates are inferred from "proxy variables" that include non-biotic evidence such as sediments found in lake beds and ice cores, and biotic evidence such as tree rings and coral. Climate models are mathematical models of past, present and future climates. Climate change may occur over long and short timescales from a variety of factors; recent warming is discussed in global warming.
Definition.
Climate (from Ancient Greek "klima", meaning "inclination") is commonly defined as the weather averaged over a long period. The standard averaging period is 30 years, but other periods may be used depending on the purpose. Climate also includes statistics other than the average, such as the magnitudes of day-to-day or year-to-year variations. The Intergovernmental Panel on Climate Change (IPCC) 2001 glossary definition is as follows:
The World Meteorological Organization (WMO) describes climate "normals" as "reference points used by climatologists to compare current climatological trends to that of the past or what is considered 'normal'. A Normal is defined as the arithmetic average of a climate element (e.g. temperature) over a 30-year period. A 30 year period is used, as it is long enough to filter out any interannual variation or anomalies, but also short enough to be able to show longer climatic trends." The WMO originated from the International Meteorological Organization which set up a technical commission for climatology in 1929. At its 1934 Wiesbaden meeting the technical commission designated the thirty-year period from 1901 to 1930 as the reference time frame for climatological standard normals. In 1982 the WMO agreed to update climate normals, and in these were subsequently completed on the basis of climate data from 1 January 1961 to 31 December 1990.
The difference between climate and weather is usefully summarized by the popular phrase "Climate is what you expect, weather is what you get." Over historical time spans there are a number of nearly constant variables that determine climate, including latitude, altitude, proportion of land to water, and proximity to oceans and mountains. These change only over periods of millions of years due to processes such as plate tectonics. Other climate determinants are more dynamic: the thermohaline circulation of the ocean leads to a 5 °C (9 °F) warming of the northern Atlantic Ocean compared to other ocean basins. Other ocean currents redistribute heat between land and water on a more regional scale. The density and type of vegetation coverage affects solar heat absorption, water retention, and rainfall on a regional level. Alterations in the quantity of atmospheric greenhouse gases determines the amount of solar energy retained by the planet, leading to global warming or global cooling. The variables which determine climate are numerous and the interactions complex, but there is general agreement that the broad outlines are understood, at least insofar as the determinants of historical climate change are concerned.
Climate classification.
There are several ways to classify climates into similar regimes. Originally, climes were defined in Ancient Greece to describe the weather depending upon a location's latitude. Modern climate classification methods can be broadly divided into "genetic" methods, which focus on the causes of climate, and "empiric" methods, which focus on the effects of climate. Examples of genetic classification include methods based on the relative frequency of different air mass types or locations within synoptic weather disturbances. Examples of empiric classifications include climate zones defined by plant hardiness, evapotranspiration, or more generally the Köppen climate classification which was originally designed to identify the climates associated with certain biomes. A common shortcoming of these classification schemes is that they produce distinct boundaries between the zones they define, rather than the gradual transition of climate properties more common in nature.
Bergeron and Spatial Synoptic.
The simplest classification is that involving air masses. The Bergeron classification is the most widely accepted form of air mass classification. Air mass classification involves three letters. The first letter describes its moisture properties, with c used for continental air masses (dry) and m for maritime air masses (moist). The second letter describes the thermal characteristic of its source region: T for tropical, P for polar, A for Arctic or Antarctic, M for monsoon, E for equatorial, and S for superior air (dry air formed by significant downward motion in the atmosphere). The third letter is used to designate the stability of the atmosphere. If the air mass is colder than the ground below it, it is labeled k. If the air mass is warmer than the ground below it, it is labeled w. While air mass identification was originally used in weather forecasting during the 1950s, climatologists began to establish synoptic climatologies based on this idea in 1973.
Based upon the Bergeron classification scheme is the Spatial Synoptic Classification system (SSC). There are six categories within the SSC scheme: Dry Polar (similar to continental polar), Dry Moderate (similar to maritime superior), Dry Tropical (similar to continental tropical), Moist Polar (similar to maritime polar), Moist Moderate (a hybrid between maritime polar and maritime tropical), and Moist Tropical (similar to maritime tropical, maritime monsoon, or maritime equatorial).
Köppen.
The Köppen classification depends on average monthly values of temperature and precipitation. The most commonly used form of the Köppen classification has five primary types labeled A through E. These primary types are A, tropical; B, dry; C, mild mid-latitude; D, cold mid-latitude; and E, polar. The five primary classifications can be further divided into secondary classifications such as rain forest, monsoon, tropical savanna, humid subtropical, humid continental, oceanic climate, Mediterranean climate, steppe, subarctic climate, tundra, polar ice cap, and desert.
Rain forests are characterized by high rainfall, with definitions setting minimum normal annual rainfall between and . Mean monthly temperatures exceed during all months of the year.
A monsoon is a seasonal prevailing wind which lasts for several months, ushering in a region's rainy season. Regions within North America, South America, Sub-Saharan Africa, Australia and East Asia are monsoon regimes.
A tropical savanna is a grassland biome located in semiarid to semi-humid climate regions of subtropical and tropical latitudes, with average temperatures remain at or above year round and rainfall between and a year. They are widespread on Africa, and are found in India, the northern parts of South America, Malaysia, and Australia.
The humid subtropical climate zone where winter rainfall (and sometimes snowfall) is associated with large storms that the westerlies steer from west to east. Most summer rainfall occurs during thunderstorms and from occasional tropical cyclones. Humid subtropical climates lie on the east side continents, roughly between latitudes 20° and 40° degrees away from the equator.
A humid continental climate is marked by variable weather patterns and a large seasonal temperature variance. Places with more than three months of average daily temperatures above and a coldest month temperature below and which do not meet the criteria for an arid or semiarid climate, are classified as continental.
An oceanic climate is typically found along the west coasts at the middle latitudes of all the world's continents, and in southeastern Australia, and is accompanied by plentiful precipitation year round.
The Mediterranean climate regime resembles the climate of the lands in the Mediterranean Basin, parts of western North America, parts of Western and South Australia, in southwestern South Africa and in parts of central Chile. The climate is characterized by hot, dry summers and cool, wet winters.
A steppe is a dry grassland with an annual temperature range in the summer of up to and during the winter down to .
A subarctic climate has little precipitation, and monthly temperatures which are above for one to three months of the year, with permafrost in large parts of the area due to the cold winters. Winters within subarctic climates usually include up to six months of temperatures averaging below .
Tundra occurs in the far Northern Hemisphere, north of the taiga belt, including vast areas of northern Russia and Canada.
A polar ice cap, or polar ice sheet, is a high-latitude region of a planet or moon that is covered in ice. Ice caps form because high-latitude regions receive less energy as solar radiation from the sun than equatorial regions, resulting in lower surface temperatures.
A desert is a landscape form or region that receives very little precipitation. Deserts usually have a large diurnal and seasonal temperature range, with high or low, depending on location daytime temperatures (in summer up to ), and low nighttime temperatures (in winter down to ) due to extremely low humidity. Many deserts are formed by rain shadows, as mountains block the path of moisture and precipitation to the desert.
Thornthwaite.
Devised by the American climatologist and geographer C. W. Thornthwaite, this climate classification method monitors the soil water budget using evapotranspiration. It monitors the portion of total precipitation used to nourish vegetation over a certain area. It uses indices such as a humidity index and an aridity index to determine an area's moisture regime based upon its average temperature, average rainfall, and average vegetation type. The lower the value of the index in any given area, the drier the area is.
The moisture classification includes climatic classes with descriptors such as hyperhumid, humid, subhumid, subarid, semi-arid (values of −20 to −40), and arid (values below −40). Humid regions experience more precipitation than evaporation each year, while arid regions experience greater evaporation than precipitation on an annual basis. A total of 33 percent of the Earth's landmass is considered either arid of semi-arid, including southwest North America, southwest South America, most of northern and a small part of southern Africa, southwest and portions of eastern Asia, as well as much of Australia. Studies suggest that precipitation effectiveness (PE) within the Thornthwaite moisture index is overestimated in the summer and underestimated in the winter. This index can be effectively used to determine the number of herbivore and mammal species numbers within a given area. The index is also used in studies of climate change.
Thermal classifications within the Thornthwaite scheme include microthermal, mesothermal, and megathermal regimes. A microthermal climate is one of low annual mean temperatures, generally between and which experiences short summers and has a potential evaporation between and . A mesothermal climate lacks persistent heat or persistent cold, with potential evaporation between and . A megathermal climate is one with persistent high temperatures and abundant rainfall, with potential annual evaporation in excess of .
Record.
Modern.
Details of the modern climate record are known through the taking of measurements from such weather instruments as thermometers, barometers, and anemometers during the past few centuries. The instruments used to study weather over the modern time scale, their known error, their immediate environment, and their exposure have changed over the years, which must be considered when studying the climate of centuries past.
Paleoclimatology.
Paleoclimatology is the study of past climate over a great period of the Earth's history. It uses evidence from ice sheets, tree rings, sediments, coral, and rocks to determine the past state of the climate. It demonstrates periods of stability and periods of change and can indicate whether changes follow patterns such as regular cycles.
Climate change.
Climate change is the variation in global or regional climates over time. It reflects changes in the variability or average state of the atmosphere over time scales ranging from decades to millions of years. These changes can be caused by processes internal to the Earth, external forces (e.g. variations in sunlight intensity) or, more recently, human activities.
In recent usage, especially in the context of environmental policy, the term "climate change" often refers only to changes in modern climate, including the rise in average surface temperature known as global warming. In some cases, the term is also used with a presumption of human causation, as in the United Nations Framework Convention on Climate Change (UNFCCC). The UNFCCC uses "climate variability" for non-human caused variations.
Earth has undergone periodic climate shifts in the past, including four major ice ages. These consisting of glacial periods where conditions are colder than normal, separated by interglacial periods. The accumulation of snow and ice during a glacial period increases the surface albedo, reflecting more of the Sun's energy into space and maintaining a lower atmospheric temperature. Increases in greenhouse gases, such as by volcanic activity, can increase the global temperature and produce an interglacial period. Suggested causes of ice age periods include the positions of the continents, variations in the Earth's orbit, changes in the solar output, and volcanism.
Climate models.
Climate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface and ice. They are used for a variety of purposes; from the study of the dynamics of the weather and climate system, to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the earth with outgoing energy as long wave (infrared) electromagnetic radiation from the earth. Any imbalance results in a change in the average temperature of the earth.
The most talked-about applications of these models in recent years have been their use to infer the consequences of increasing greenhouse gases in the atmosphere, primarily carbon dioxide (see greenhouse gas). These models predict an upward trend in the global mean surface temperature, with the most rapid increase in temperature being projected for the higher latitudes of the Northern Hemisphere.
Models can range from relatively simple to quite complex:
Climate forecasting is a way by some scientists are using to predict climate change. In 1997 the prediction division of the International Research Institute for Climate and Society at Columbia University began generating seasonal climate forecasts on a real-time basis. To produce these forecasts an extensive suite of forecasting tools was developed, including a multimodel ensemble approach that required thorough validation of each model's accuracy level in simulating interannual climate variability.

</doc>
<doc id="6000" url="https://en.wikipedia.org/wiki?curid=6000" title="History of the Comoros">
History of the Comoros

The history of the Comoros goes back some 1,500 years. It has been inhabited by various groups throughout this time. France colonised the islands in the 19th century. The Comoros finally became independent in 1975.
Early inhabitants.
According to myth, the Comoros islands were first visited by Phoenician sailors. The earliest inhabitants of the islands were probably Bantu-speaking Africans; the earliest evidence of settlement of the islands dates from the sixth century. Traces of this original culture have blended with successive waves of African, Arab and Malagasy. Shirazi immigrants appear to have arrived some time after the tenth century A.D.
In the 16th century, social changes on the East African coast probably linked to the arrival of the Portuguese saw the arrival of a number of Arabs of Hadrami who established alliances with the Shirazis and founded several royal clans.
Over the centuries, the Comoro Islands have been settled by a succession of diverse groups from the coast of Africa, the Persian Gulf, Southeast Asia and Madagascar. Portuguese explorers first visited the archipelago in 1505.
Apart from a visit by the French Parmentier brothers in 1529, for much of the 16th century the only Europeans to visit the islands were Portuguese; British and Dutch ships began arriving around the start of the 17th century and the island of Ndzwani soon became a major supply point on the route to the East. Ndzwani was generally ruled by a single sultan, who occasionally attempted to extend his authority to Mayotte and Mwali; Ngazidja was more fragmented, on occasion being divided into as many as 12 small kingdoms.
Both the British and the French turned their attention to the Comoros islands in the middle of the 19th century. The French finally acquired the islands through a cunning mixture of strategies, including the policy of 'divide and conquer', chequebook politics and a serendipitous affair between a sultana and a French trader that was put to good use by the French, who kept control of the islands, quelling unrest and the occasional uprising.
William Sunley, a planter and British Consul from 1848–1866, was an influence on Anjouan.
Colonial rule.
On 25 March 1841, France purchased the island of Maore (the name of the island was corrupted in French to "Mayotte") (ratified 13 June 1843), which became a colony.
In 1850 Sultan Selim of Johanna island seized the American whaler "Maria" and imprisoned her commander, named Moores. In response the United States Navy launched the Johanna Expedition in February 1852 to gain the release of Moores and extract compensation. Initially the sultan did not meet the demands, and the sloop-of-war USS "Dale" bombarded the island's fortifications; ultimately Selim paid US$1,000 and released Captain Moores.
In 1886 Said Ali bin Said Omar, Sultan of Bambao, signed an agreement with the French government that allowed France to establish a protectorate over the entire island of Ngazidja (Grande Comore); protectorates were also established over Ndzwani (Anjouan), and Mwali (Mohéli island in French) the same year. Résidents were posted on the three islands.
On 9 April 1908, France declared the protectorates and Mayotte a single colony, Mayotte and dependencies.
On 25 July 1912, it was annexed to Madagascar as a province of that colony.
From 16 June 1940 - 1942 the colonial administration remained loyal to Vichy France (from 1942, under Free French), but 25 September 1942 - 13 October 1946 they were, like Madagascar, under British occupation.
Until the opening of the Suez Canal, the islands used to be an important refuelling and provisioning station for ships from Europe to the Indian Ocean.
Independence came gradually for the Comoros. During the middle of the 20th century the French reluctantly began to accede to requests for constitutional changes and in 1946 the Comoros had become a separately administered colony from Madagascar.
After World War II, the islands became a French overseas territory and were represented in France's National Assembly. Internal political autonomy was granted in 1961. Agreement was reached with France in 1973 for the Comoros to become independent in 1978. On July 6, 1975, however, the Comorian parliament passed a resolution declaring unilateral independence. The deputies of Mayotte abstained.
In two referendums, in December 1974 and February 1976, the population of Mayotte voted against independence from France (by 63.8% and 99.4% respectively). Mayotte thus remains under French administration, and the Comorian Government has effective control over only Grande Comore, Anjouan, and Mohéli.
In 1961 it was granted autonomous rule and, seven years after the general unrest and left-wing riots of 1968, the Comoros broke all ties with France and established itself as an independent republic. From the very beginning Mayotte refused to join the new republic and aligned itself even more firmly to the French Republic, but the other islands remained committed to independence. The first president of the Comoros, Ahmed Abdallah Abderemane, did not last long before being ousted in a coup by Ali Soilih, an atheist with an Islamic background.
Soilih began with a set of solid socialist ideals designed to modernize the country. However, the regime faced problems. A French mercenary by the name of Bob Denard, arrived in the Comoros at dawn on 13 May 1978, and removed Soilih from power. Solih was shot and killed during the coup. Abdallah returned to govern the country and the mercenaries were given key positions in government.
Later, French settlers, French-owned companies, and Arab merchants established a plantation-based economy that now uses about one-third of the land for export crops.
Abdallah regime.
In 1978, president Ali Soilih, who had a firm anti-French line, was killed and Ahmed Abdallah came to power. Under the reign of Abdallah, Denard was commander of the Presidential Guard (PG) and "de facto" ruler of the country. He was trained, supported and funded by the white regimes in South Africa (SA) and Rhodesia (now Zimbabwe) in return for permission to set up a secret listening post on the islands. South-African agents kept an ear on the important ANC bases in Lusaka and Dar es Salaam and watched the war in Mozambique, in which SA played an active role. The Comoros were also used for the evasion of arms sanctions.
When in 1981 François Mitterrand was elected president Denard lost the support of the French intelligence service, but he managed to strengthen the link between SA and the Comoros. Besides the military, Denard established his own company SOGECOM, for both the security and construction, and seemed to profit by the arrangement. Between 1985 an 1987 the relationship of the PG with the local Comorians became worse.
At the end of the 1980s the South Africans did not wish to continue to support the mercenary regime and France was in agreement. Also President Abdallah wanted the mercenaries to leave. Their response was a (third) coup resulting in the death of President Abdallah, in which Denard and his men were probably involved. South Africa and the French government subsequently forced Denard and his mercenaries to leave the islands in 1989.
1989-1996.
Said Mohamed Djohar became president. His time in office was turbulent, including an impeachment attempt in 1991 and a coup attempt in 1992.
On September 28, 1995 Bob Denard and a group of mercenaries took over the Comoros islands in a coup (named operation Kaskari by the mercenaries) against President Djohar. France immediately severely denounced the coup, and backed by the 1978 defense agreement with the Comoros, President Jacques Chirac ordered his special forces to retake the island. Bob Denard began to take measures to stop the coming invasion. A new presidential guard was created. Strong points armed with heavy machine guns were set up around the island, particularly around the island's two airports.
On October 3, 1995, 11 p.m., the French deployed 600 men against a force of 33 mercenaries and a 300-man dissident force. Denard however ordered his mercenaries not to fight. Within 7 hours the airports at Iconi and Hahaya and the French Embassy in Moroni were secured. By 3:00 p.m. the next day Bob Denard and his mercenaries had surrendered. This (response) operation, codenamed "Azalée", was remarkable, because there were no casualties, and just in seven days, plans were drawn up and soldiers were deployed. Denard was taken to France and jailed. Prime minister Caambi El-Yachourtu became acting president until Djohar returned from exile in January, 1996. In March 1996, following presidential elections, Mohamed Taki Abdoulkarim, a member of the civilian government that Denard had tried to set up in October 1995, became president.
Secession of Anjouan and Mohéli.
In 1997, the islands of Anjouan and Mohéli declared their independence from the Comoros. A subsequent attempt by the government to re-establish control over the rebellious islands by force failed, and presently the African Union is brokering negotiations to effect a reconciliation. This process is largely complete, at least in theory. According to some sources, Mohéli did return to government control in 1998. In 1999, Anjouan had internal conflicts and on August 1 of that year, the 80-year-old first president Foundi Abdallah Ibrahim resigned, transferring power to a national coordinator, Said Abeid. The government was overthrown in a coup by army and navy officers on August 9, 2001. Mohamed Bacar soon rose to leadership of the junta that took over and by the end of the month he was the leader of the country. Despite two coup attempts in the following three months, including one by Abeid, Bacar's government remained in power, and was apparently more willing to negotiate with the Comoros. Presidential elections were held for all of the Comoros in 2002, and presidents have been chosen for all three islands as well, which have become a confederation. Most notably, Mohammed Bacar was elected for a 5-year term as president of Anjouan. Grande Comore had experienced troubles of its own in the late 1990s, when President Taki died on November 6, 1998. Colonel Azali Assoumani became president following a military coup in 1999. There have been several coup attempts since, but he gained firm control of the country after stepping down temporarily and winning a presidential election in 2002.
In May 2006, Ahmed Abdallah Sambi was elected from the island of Anjouan to be the president of the Union of the Comoros. He is a well-respected Sunni cleric who studied in the Sudan, Iran and Saudi Arabia. He is respectfully called "Ayatollah" by his supporters but is considered, and is, a moderate Islamist. He has been quoted as stating that the Comoros is not ready to become an Islamic state, nor shall the veil be forced upon any women in the Comoros.

</doc>
<doc id="6001" url="https://en.wikipedia.org/wiki?curid=6001" title="Geography of the Comoros">
Geography of the Comoros

The Comoros archipelago consists of four main islands aligned along a northwest-southeast axis at the north end of the Mozambique Channel, between Mozambique and the island of Madagascar. Still widely known by their French names, the islands officially have been called by their Swahili names by the Comorian government. They are Grande Comore (Njazidja), Mohéli (Mwali), Anjouan (Nzwani), and Mayotte (Mahoré). The islands' distance from each other—Grande Comore is some 200 kilometers from Mayotte, forty kilometers from Mohéli, and eighty kilometers from Grande Comore—along with a lack of good harbor facilities, make transportation and communication difficult. The islands have a total land area of 2,236 square kilometers (including Mayotte), and claim territorial waters of 320 square kilometers. Mount Karthala (2316 m) on Grande Comore is an active volcano. From April 17 to 19, 2005, the volcano began spewing ash and gas, forcing as many as 10,000 people to flee.
Geographic coordinates:
Grande Comore.
Grande Comore is the largest island, sixty-seven kilometers long and twenty-seven kilometers wide, with a total area of 1,146 square kilometers. The most recently formed of the four islands in the archipelago, it is also of volcanic origin. Two volcanoes form the island's most prominent topographic features: La Grille in the north, with an elevation of 1,000 meters, is extinct and largely eroded; Kartala in the south, rising to a height of 2,361 meters, last erupted in 1977. A plateau averaging 600 to 700 meters high connects the two mountains. Because Grande Comore is geologically a relatively new island, its soil is thin and rocky and cannot hold water. As a result, water from the island's heavy rainfall must be stored in catchment tanks. There are no coral reefs along the coast, and the island lacks a good harbor for ships. One of the largest remnants of the Comoros' once-extensive rain forests is on the slopes of Kartala. The national capital has been at Moroni since 1962.
Anjouan.
Anjouan, triangular shaped and forty kilometers from apex to base, has an area of 424 square kilometers. Three mountain chains—Sima, Nioumakele, and Jimilime—emanate from a central peak, Mtingui (1,575 m), giving the island its distinctive shape. Older than Grande Comore, Anjouan has deeper soil cover, but overcultivation has caused serious erosion. A coral reef lies close to shore; the island's capital of Mutsamudu is also its main port.
Mohéli.
Mohéli is thirty kilometers long and twelve kilometers wide, with an area of 290 square kilometers. It is the smallest of the four islands and has a central mountain chain reaching 860 meters at its highest. Like Grande Comore, it retains stands of rain forest. Mohéli's capital is Fomboni.
Mayotte.
Mayotte, geologically the oldest of the four islands, is thirty-nine kilometers long and twenty-two kilometers wide, totaling 375 square kilometers, and its highest points are between 500 and 600 meters above sea level. Because of greater weathering of the volcanic rock, the soil is relatively rich in some areas. A well-developed coral reef that encircles much of the island ensures protection for ships and a habitat for fish. Dzaoudzi, capital of the Comoros until 1962 and now Mayotte's administrative center, is situated on a rocky outcropping off the east shore of the main island. Dzaoudzi is linked by a causeway to le Pamanzi, which at ten kilometers in area is the largest of several islets adjacent to Mayotte. Islets are also scattered in the coastal waters of Grande Comore, Anjouan, and Mohéli.
Flora and fauna.
Comorian waters are the habitat of the coelacanth, a rare fish with limblike fins and a cartilaginous skeleton, the fossil remains of which date as far back as 400 million years and which was once thought to have become extinct about 70 million years ago. A live specimen was caught in 1938 off southern Africa; other coelacanths have since been found in the vicinity of the Comoro Islands.
Several mammals are unique to the islands themselves. Livingstone's fruit bat, although plentiful when discovered by explorer David Livingstone in 1863, has been reduced to a population of about 120, entirely on Anjouan. The world's largest bat, the jet-black Livingstone fruit bat has a wingspan of nearly two meters. A British preservation group sent an expedition to the Comoros in 1992 to bring some of the bats to Britain to establish a breeding population.
A hybrid of the common brown lemur ("Eulemur fulvus") originally from Madagascar, was introduced prior by humans prior to European colonization and is found on Mayotte. The mongoose lemur ("Eulemur mongoz"), also introduced from Madagascar by humans, can be found on the islands of Mohéli and Anjouan.
22 species of bird are unique to the archipelago and 17 of these are restricted to the Union of the Comoros. These include the Karthala scops-owl, Anjouan scops-owl and Humblot's flycatcher.
Partly in response to international pressures, Comorians in the 1990s have become more concerned about the environment. Steps are being taken not only to preserve the rare fauna, but also to counteract degradation of the environment, especially on densely populated Anjouan. Specifically, to minimize the cutting down of trees for fuel, kerosene is being subsidized, and efforts are being made to replace the loss of the forest cover caused by ylang-ylang distillation for perfume. The Community Development Support Fund, sponsored by the International Development Association (IDA, a World Bank affiliate) and the Comorian government, is working to improve water supply on the islands as well.
Climate.
The climate is marine tropical, with two seasons: hot and humid from November to April, the result of the northeastern monsoon, and a cooler, drier season the rest of the year. Average monthly temperatures range from along the coasts. Although the average annual precipitation is , water is a scarce commodity in many parts of the Comoros. Mohéli and Mayotte possess streams and other natural sources of water, but Grande Comore and Anjouan, whose mountainous landscapes retain water poorly, are almost devoid of naturally occurring running water. Cyclones, occurring during the hot and wet season, can cause extensive damage, especially in coastal areas. On the average, at least twice each decade houses, farms, and harbor facilities are devastated by these great storms.
Extreme points.
This is a list of the extreme points of the Comoros, the points that are farther north, south, east or west than any other location. This list excludes the French-administered island of Mayotte which is claimed by the Comorian government.
Statistics.
Area:
2,235 km2
Coastline:
340 km
Climate:
tropical marine; rainy season (November to May)
Terrain:
volcanic islands, interiors vary from steep mountains to low hills
Elevation extremes:
"lowest point:"
Indian Ocean 0 m
"highest point:"
Karthala 2,360 m
Natural resources:
fish
Land use:
"arable land:"
47.29%
"permanent crops:"
29.55%
"other:"
23.16% (2012 est.)
Irrigated land:
1.3 km2 (2003)
Total renewable water resources:
1.2 km3 (2011)
Freshwater withdrawal (domestic/industrial/agricultural):
"total:"
0.01 km3/yr (48%/5%/47%)
"per capital:"
16.86 m3/yr (1999)
Natural hazards:
cyclones possible during rainy season (December to April); volcanic activity on Grand Comore
Environmental - current issues:
soil degradation and erosion results from crop cultivation on slopes without proper terracing; deforestation

</doc>
<doc id="6002" url="https://en.wikipedia.org/wiki?curid=6002" title="Demographics of the Comoros">
Demographics of the Comoros

The Comorians inhabiting Grande Comore, Anjouan, and Mohéli (86% of the population) share African-Arab origins. Islam is the dominant religion, and Quranic schools for children reinforce its influence. Although Islamic culture is firmly established throughout, a small minority are Christian.
The most common language is Comorian, related to Swahili. French and Arabic also are spoken. About 89% of the population is literate.
The Comoros have had seven censuses since World War II:
Population density figures conceal a great disparity between the republic's most crowded island, Nzwani, which had a density of 470 persons per square kilometer in 1991; Ngazidja, which had a density of 250 persons per square kilometer in 1991; and Mwali, where the 1991 population density figure was 120 persons per square kilometer. Overall population density increased to about 285 persons per square kilometer by 1994.
By comparison, estimates of the population density per square kilometer of the Indian Ocean's other island microstates ranged from 241 (Seychelles) to 690 (Maldives) in 1993. Given the rugged terrain of Ngazidja and Nzwani, and the dedication of extensive tracts to agriculture on all three islands, population pressures on the Comoros are becoming increasingly critical.
The age structure of the population of the Comoros is similar to that of many developing countries, in that the republic has a very large proportion of young people. In 1989, 46.4 percent of the population was under fifteen years of age, an above-average proportion even for sub-Saharan Africa. The population's rate of growth was a relatively high 3.5 percent per annum in the mid 1980s, up substantially from 2.0 percent in the mid-1970s and 2.1 percent in the mid-1960s.
In 1983 the Abdallah regime borrowed US$2.85 million from the International Development Association to devise a national family planning program. However, Islamic reservations about contraception made forthright advocacy and implementation of birth control programs politically hazardous, and consequently little was done in the way of public policy.
The Comorian population has become increasingly urbanized in recent years. In 1991 the percentage of Comorians residing in cities and towns of more than 5,000 persons was about 30 percent, up from 25 percent in 1985 and 23 percent in 1980. The Comoros' largest cities were the capital, Moroni, with about 30,000 people, and the port city of Mutsamudu, on the island of Nzwani, with about 20,000 people.
Migration among the various islands is important. Natives of Nzwani have settled in significant numbers on less crowded Mwali, causing some social tensions, and many Nzwani also migrate to Maore. In 1977 Maore expelled peasants from Ngazidja and Nzwani who had recently settled in large numbers on the island. Some were allowed to reenter starting in 1981 but solely as migrant labor.
The number of Comorians living abroad has been estimated at between 80,000 and 100,000; during the colonial period, most of them lived in Tanzania, Madagascar, and other parts of Southeast Africa. The number of Comorians residing in Madagascar was drastically reduced after anti-Comorian rioting in December 1976 in Mahajanga, in which at least 1,400 Comorians were killed. As many as 17,000 Comorians left Madagascar to seek refuge in their native land in 1977 alone. About 100,000 Comorians live in France; many of them had gone there for a university education and never returned. Small numbers of Indians, Malagasy, South Africans, and Europeans (mostly French) live on the islands and play an important role in the economy. Most French left after independence in 1975.
Population.
UN population projections.
Numbers are in thousands. UN medium variant projections.
Vital statistics.
Fertility and Births.
Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):
Structure of the population (DHS 2012) (Males 11 088, Females 12 284 = 23 373) :
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.

</doc>
<doc id="6003" url="https://en.wikipedia.org/wiki?curid=6003" title="Politics of the Comoros">
Politics of the Comoros

Politics of the Union of the Comoros takes place in a framework of a federal presidential republic, whereby the President of the Comoros is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Federal legislative power is vested in both the government and parliament.
As of 2008, Comoros and Mauritania are considered by US-based organization Freedom House as the only real “electoral democracies” of the Arab World.
Political background.
The Union of the Comoros, known as the Islamic Federal Republic of the Comoros until 2003, is ruled by Ahmed Abdallah Sambi. The political situation in Comoros has been extremely fluid since the country's independence in 1975, subject to the volatility of coups and political insurrection. Colonel Azali Assoumani seized power in a bloodless coup in April 1999, overthrowing Interim President Tadjidine Ben Said Massounde, who himself had held the office since the death of democratically elected President Mohamed Taki Abdoulkarim in November, 1998. 
In May 1999, Azali decreed a constitution that gave him both executive and legislative powers. Bowing somewhat to international criticism, Azali appointed a civilian Prime Minister, Bainrifi Tarmidi, in December 1999; however, Azali retained the mantle of Head of State and army Commander. In December 2000, Azali named a new civilian Prime Minister, Hamada Madi, and formed a new civilian Cabinet. When Azali took power he also pledged to step down in April 2000 and relinquish control to a democratically elected president—a pledge with mixed results.
In a separate nod to pressure to restore civilian rule, the government organized several committees to compose a new constitution, including the August 2000 National Congress and November 2000 Tripartite Commission. The opposition parties initially refused to participate in the Tripartite Commission, but on 17 February, representatives of the government, the Anjouan separatists, the political opposition, and civil society organizations signed a "Framework Accord for Reconciliation in Comoros," brokered by the Organization for African Unity
The accord called for the creation of a new Tripartite Commission for National Reconciliation to develop a "New Comorian Entity" with a new constitution. The new federal Constitution came into effect in 2002; it included elements of consociationalism, including a presidency that rotates every four years among the islands and extensive autonomy for each island. Presidential elections were held in 2002, at which Azali Assoumani was elected President. In April 2004 legislative elections were held, completing the implementation of the new constitution. 
The new Union of the Comoros consists of three islands, Grande Comore, Anjouan and Mohéli. Each island has a president, who shares the presidency of the Union on a rotating basis. The president and his vice-presidents are elected for a term of four years. The constitution states that, "the islands enjoy financial autonomy, freely draw up and manage their budgets".
President Assoumani Azali of Grande Comore is the first Union president. President Mohamed Bacar of Anjouan formed his 13-member government at the end of April, 2003.
On 15 May 2006, Ahmed Abdallah Sambi, a cleric and successful businessman educated in Iran, Saudi Arabia and Sudan, was declared the winner of elections for President of the Republic. He is considered a moderate Islamist and is called Ayatollah by his supporters. He beat out retired French air force officer Mohamed Djaanfari and long-time politician Ibrahim Halidi, whose candidacy was backed by Azali Assoumani, the outgoing president.
A referendum took place on May 16, 2009 to decide whether to cut down the government's unwieldy political bureaucracy. 52.7% of those eligible voted, and 93.8% of votes were cast in approval of the referendum. The referendum would cause each island's president to become a governor and the ministers to become councilors.
Autonomous islands.
The constitution gives Grande Comore, Anjouan and Mohéli the right to govern most of their own affairs with their own presidents, except the activities assigned to the Union of the Comoros like Foreign Policy, Defense, Nationality, Banking and others. Comoros considers Mayotte, an overseas collectivity of France, to be part of its territory, with an autonomous status 
Executive branch.
The federal presidency is rotated between the islands' presidents.
The Union of Comoros abolished the position of Prime Minister.
Legislative branch.
The Assembly of the Union has 33 seats, 18 elected in single seat constituencies and 15 representatives of the regional assemblies.
Judicial branch.
The Supreme Court or Cour Supreme, has two members appointed by the president, two members elected by the Federal Assembly, one by the Council of each island, and former presidents of the republic.
International organization participation.
The Comoros are member of the ACCT, ACP, AfDB, AMF, African Union, FAO, G-77, IBRD, ICAO, ICCt (signatory), ICRM, IDA, IDB, IFAD, IFC, IFRCS, ILO, IMF, InOC, Interpol, IOC, ITU, LAS, NAM, OIC, OPCW (signatory), United Nations, UNCTAD, UNESCO, UNIDO, UPU, WCO, WHO, WMO.

</doc>
<doc id="6005" url="https://en.wikipedia.org/wiki?curid=6005" title="Telecommunications in the Comoros">
Telecommunications in the Comoros

In large part thanks to international aid programs, Moroni has international telecommunications service. Telephone service, however, is largely limited to the islands' few towns.
Communications in the Comoros
Telephones - main lines in use:
5,000 (1995)
Telephones - mobile cellular:
0 (1995)
Telephone system:
sparse system of microwave radio relay and HF radiotelephone communication stations
<br>"domestic:"
HF radiotelephone communications and microwave radio relay<br>
CMDA mobile network (Huri, operated by Comores Telecom)
<br>"international:"
HF radiotelephone communications to Madagascar and Réunion
Radio broadcast stations:
AM 1, FM 2, shortwave 1 (1998)
Radios:
90,000 (1997)
Television broadcast stations:
0 (1998)
Televisions:
1,000 (1997)
Internet Service Providers (ISPs): 1 (1999)
Country code (Top-level domain): KM
Special projects.
In October 2011 the State of Qatar launched a special program for the construction of a wireless network to interconnect the three islands of the archipelago, by means of low cost, repeatable technology. The project has been developed by Qatar University and Politecnico di Torino, under the supervision of prof. Mazen Hasna and prof. Daniele Trinchero, with a major participation of local actors (Comorian Government, NRTIC, University of the Comoros). The project has been referred as an example of technology transfer and Sustainable Inclusion in developing countries

</doc>
<doc id="6006" url="https://en.wikipedia.org/wiki?curid=6006" title="Transport in the Comoros">
Transport in the Comoros

There are a number of systems of transport in the Comoros. The Comoros possesses of road, of which are paved. It has three seaports: Fomboni, Moroni and Moutsamoudou, but does not have a merchant marine, and no longer has any railway network. It has four airports, all with paved runways, one with runways over long, with the others having runways shorter than .
The isolation of the Comoros had made air traffic a major means of transportation. One of President Abdallah's accomplishments was to make the Comoros more accessible by air. During his administration, he negotiated agreements to initiate or enhance commercial air links with Tanzania and Madagascar. The Djohar regime reached an agreement in 1990 to link Moroni and Brussels by air. By the early 1990s, commercial flights connected the Comoros with France, Mauritius, Kenya, South Africa, Tanzania, and Madagascar. The national airline was Air Comores. Daily flights linked the three main islands, and air service was also available to Mahoré; each island had airstrips. In 1986 the republic received a grant from the French government's CCCE to renovate and expand Hahaya airport, near Moroni. Because of the absence of scheduled sea transport between the islands, nearly all interisland passenger traffic is by air.
More than 99% of freight is transported by sea. Both Moroni on Njazidja and Mutsamudu on Nzwani have artificial harbors. There is also a harbor at Fomboni, on Mwali. Despite extensive internationally financed programs to upgrade the harbors at Moroni and Mutsamudu, by the early 1990s only Mutsamudu was operational as a deepwater facility. Its harbor could accommodate vessels of up to eleven meters' draught. At Moroni, ocean-going vessels typically lie offshore and are loaded or unloaded by smaller craft, a costly and sometimes dangerous procedure. Most freight continues to be sent to Kenya, Reunion, or Madagascar for transshipment to the Comoros. Use of Comoran ports is further restricted by the threat of cyclones from December through March. The privately operated Comoran Navigation Company ("Société Comorienne de Navigation") is based in Moroni, and provides services to Madagascar.
Roads serve the coastal areas, rather than the interior, and the mountainous terrain makes surface travel difficult.

</doc>
<doc id="6007" url="https://en.wikipedia.org/wiki?curid=6007" title="Foreign relations of the Comoros">
Foreign relations of the Comoros

In November 1975, Comoros became the 143rd member of the United Nations. The new nation was defined as consisting of the entire archipelago, despite the fact that France maintains control over Mayotte.
Overview.
Comoros also is a member of the African Union, the Arab League, the European Development Fund, the World Bank, the International Monetary Fund, the Indian Ocean Commission, and the African Development Bank.
The government fostered close relationships with the more conservative (and oil-rich) Arab states, such as Saudi Arabia and Kuwait. It frequently received aid from those countries and the regional financial institutions they influenced, such as the Arab Bank for Economic Development in Africa and the Arab Fund for Economic and Social Development. In October 1993, Comoros joined the League of Arab States, after having been rejected when it applied for membership initially in 1977.
Regional relations generally were good. In 1985 Madagascar, Mauritius, and Seychelles agreed to admit Comoros as the fourth member of the Indian Ocean Commission (IOC), an organization established in 1982 to encourage regional cooperation. In 1993 Mauritius and Seychelles had two of the five embassies in Moroni, and Mauritius and Madagascar were connected to the republic by regularly scheduled commercial flights.
In November 1975, Comoros became the 143d member of the UN. In the 1990s, the republic continued to represent Mahoré in the UN. Comoros was also a member of the OAU, the EDF, the World Bank, the IMF, the IOC, and the African Development Bank.
Comoros thus cultivated relations with various nations, both East and West, seeking to increase trade and obtain financial assistance. In 1994, however, it was increasingly facing the need to control its expenditures and reorganize its economy so that it would be viewed as a sounder recipient of investment. Comoros also confronted domestically the problem of the degree of democracy the government was prepared to grant to its citizens, a consideration that related to its standing in the world community.
Bilateral relations.
China.
Comoros also hosted an embassy of China, which established relations during the Soilih regime. The Chinese had long been a source of aid and apparently wished to maintain contact with Comoros to counterbalance Indian and Soviet (later Russian) influence in the Indian Ocean.
In August 2008, a Comorian delegation visited China on a good-will visit. Together with the Chinese defense minister Liang Guanglie, and Chief of Staff of the Comoros armed forces Salimou Mohamed Amiri, pledged to increase cooperation between the military of the two nations. Amiri stated that Comoros will continue to adhere to the One-China policy.
A comprehensive Chinese-assisted treatment campaign has apparently eliminated malaria from the Comorian island of Moheli (population 36,000). Administered by Li Guoqiao at the Tropical Medicine Institute, the program relies on hybrid Artemisia annua of hybrid ancestry, which was used for a drug regimen by which all residents of the island, whether or not visibly ill, took two doses at a 40-day interval. This eliminated the human reservoir of the disease and reduced hospital admissions to 1% or less of January 2008 levels. Visitors to Moheli are now required to take antimalarial drugs, a mix of artemisinin, primaquine and pyrimethamine that China provides for free. When asked about Artemisia exports, Li was quoted, "We want to grow them in China and whatever we export depends on bilateral relationships." Comoros has requested a similar program for Grande Comore and Anjouan, total population 760,000, and Li said that Beijing has agreed in principle.
France.
Comoros' most significant international relationship is that with France. The three years of estrangement following the unilateral declaration of independence and the nationalistic Soilih regime were followed during the conservative Abdallah and Djohar regimes by a period of growing trade, aid, cultural, and defense links between the former colony and France, punctuated by frequent visits to Paris by the head of state and occasional visits by the French president to Moroni. The leading military power in the region, France has detachments on Mahoré and Réunion, and its Indian Ocean fleet sails the waters around the islands. France and Comoros signed a mutual security treaty in 1978; following the mercenary coup against Abdallah in 1989, French troops restored order and took responsibility for reorganizing and training the Comorian army. With Mahoré continuing to gravitate politically and economically toward France, and Comoros increasingly dependent on the French for help with its own considerable social, political, and economic problems, the issue of Mahoré diminished somewhat in urgency.
Comoros claims French-administered Mayotte & the Glorioso Islands.
Georgia.
The Comoros and Georgia established full diplomatic relations on 26 March 2010.
Japan.
Comorian relations with Japan were also significant because Japan was the second largest provider of aid, consisting of funding for fisheries, food, and highway development.
South Africa.
The close relationship Comoros developed with South Africa in the 1980s was much less significant to both countries in the 1990s. With the reform of its apartheid government, South Africa no longer needed Comoros as evidence of its ostensible ability to enjoy good relations with a black African state; the end of the Cold War had also diminished Comoros' strategic value to Pretoria. Although South Africa continued to provide developmental aid, it closed its consulate in Moroni in 1992. Since the 1989 coup and subsequent expulsion of South African-financed mercenaries, Comoros likewise turned away from South Africa and toward France for assistance with its security needs.
South Korea.
Diplomatic relations between the Republic of Korea (South Korea) and the Union of Comoros were established on 19 February 1979. Four South Koreans were living in Comoros in 2011–12.
Turkey.
Embassy of Comoros in Egypt is accredited to Turkey. Embassy of Turkey in Kenya is accredited to Comoros.
United States.
The United States established diplomatic relations in 1977 but in September 1993 closed its embassy in Moroni and reopened it in March 2015. The two countries enjoy friendly relations. The historic under-commitment by the US within France's sphere of interest in the Indian Ocean looks set to continue after a November 2009 meeting between heads of state. Future friendly relations continue to look promising between the Comoros and America.
Yemen.
In April 2008, the Ministry of Agriculture and Irrigation of Yemen and Comoros Ministry of Fishery and Environment signed a "Memo of Understanding" (MOU) concerning agricultural cooperation.

</doc>
<doc id="6008" url="https://en.wikipedia.org/wiki?curid=6008" title="Military of the Comoros">
Military of the Comoros

The Comorian Security Force (French "Armée nationale de développement") consist of a small standing army and a 500-member police force, as well as a 500-member defense force. A defense treaty with France provides naval resources for protection of territorial waters, training of Comorian military personnel, and air surveillance. France maintains a small troop presence in the Comoros at government request. France maintains a small maritime base and a Foreign Legion Detachment (DLEM) on Mayotte.
Aircraft inventory.
The Comorian Security Force operates only 4 aircraft.
In addition to the CSF, the Police Force operates a further 6 aircraft fleet available for paramilitary duties.

</doc>
<doc id="6010" url="https://en.wikipedia.org/wiki?curid=6010" title="Computer worm">
Computer worm

A computer worm is a standalone malware computer program that replicates itself in order to spread to other computers. Often, it uses a computer network to spread itself, relying on security failures on the target computer to access it. Unlike a computer virus, it does not need to attach itself to an existing program. Worms almost always cause at least some harm to the network, even if only by consuming bandwidth, whereas viruses almost always corrupt or modify files on a targeted computer.
Many worms that have been created are designed only to spread, and do not attempt to change the systems they pass through. However, as the Morris worm and Mydoom showed, even these "payload free" worms can cause major disruption by increasing network traffic and other unintended effects. A "payload" is code in the worm designed to do more than spread the worm—it might delete files on a host system (e.g., the ExploreZip worm), encrypt files in a ransomware attack, or send documents via e-mail. A very common payload for worms is to install a backdoor in the infected computer to allow the creation of a "zombie" computer under control of the worm author. Networks of such machines are often referred to as botnets and are very commonly used by spam senders for sending junk email or to cloak their website's address. Spammers are therefore thought to be a source of funding for the creation of such worms, and the worm writers have been caught selling lists of IP addresses of infected machines. Others try to blackmail companies with threatening DoS attacks.
Users can minimize the threat posed by worms by keeping their computers' operating system and other software up to date, avoiding opening unrecognized or unexpected emails and running firewall and antivirus software.
Backdoors can be exploited by other malware, including worms. Examples include Doomjuice, which can spread using the backdoor opened by Mydoom, and at least one instance of malware taking advantage of the rootkit and backdoor installed by the Sony/BMG DRM software utilized by millions of music CDs prior to late 2005. 
History.
The actual term "worm" was first used in John Brunner's 1975 novel, "The Shockwave Rider". In that novel, Nichlas Haflinger designs and sets off a data-gathering worm in an act of revenge against the powerful men who run a national electronic information web that induces mass conformity. "You have the biggest-ever worm loose in the net, and it automatically sabotages any attempt to monitor it... There's never been a worm with that tough a head or that long a tail!"
On November 2, 1988, Robert Tappan Morris, a Cornell University computer science graduate student, unleashed what became known as the Morris worm, disrupting a large number of computers then on the Internet, guessed at the time to be one tenth of all those connected During the Morris appeal process, the U.S. Court of Appeals estimated the cost of removing the virus from each installation was in the range of $200–53,000, and prompting the formation of the CERT Coordination Center and Phage mailing list. Morris himself became the first person tried and convicted under the 1986 Computer Fraud and Abuse Act.
Protecting against dangerous computer worms.
Worms spread by exploiting vulnerabilities in operating systems.
Vendors with security problems supply regular security updates (see "Patch Tuesday"), and if these are installed to a machine then the majority of worms are unable to spread to it. If a vulnerability is disclosed before the security patch released by the vendor, a zero-day attack is possible.
Users need to be wary of opening unexpected email, and should not run attached files or programs, or visit web sites that are linked to such emails. However, as with the ILOVEYOU worm, and with the increased growth and efficiency of phishing attacks, it remains possible to trick the end-user into running malicious code.
Anti-virus and anti-spyware software are helpful, but must be kept up-to-date with new pattern files at least every few days. The use of a firewall is also recommended.
In the April–June, 2008, issue of IEEE Transactions on Dependable and Secure Computing, computer scientists describe a potential new way to combat internet worms. The researchers discovered how to contain the kind of worm that scans the Internet randomly, looking for vulnerable hosts to infect. They found that the key is for software to monitor the number of scans that machines on a network sends out. When a machine starts sending out too many scans, it is a sign that it has been infected, allowing administrators to take it off line and check it for malware. In addition, machine learning techniques can be used to detect new worms, by analyzing the behavior of the suspected computer.
Worms with good intent.
Beginning with the very first research into worms at Xerox PARC, there have been attempts to create useful worms. Those worms allowed testing by John Shoch and Jon Hupp of the Ethernet principles on their network of Xerox Alto computers. The Nachi family of worms tried to download and install patches from Microsoft's website to fix vulnerabilities in the host system—by exploiting those same vulnerabilities. In practice, although this may have made these systems more secure, it generated considerable network traffic, rebooted the machine in the course of patching it, and did its work without the consent of the computer's owner or user. Regardless of their payload or their writers' intentions, most security experts regard all worms as malware.
Several worms, like XSS worms, have been written to research how worms spread. For example, the effects of changes in social activity or user behavior. One study proposed what seems to be the first computer worm that operates on the second layer of the OSI model (Data link Layer), it utilizes topology information such as Content-addressable memory (CAM) tables and Spanning Tree information stored in switches to propagate and probe for vulnerable nodes until the enterprise network is covered.

</doc>
<doc id="6011" url="https://en.wikipedia.org/wiki?curid=6011" title="Chomsky hierarchy">
Chomsky hierarchy

Within the fields of computer science and linguistics, specifically in the area of formal languages, the Chomsky hierarchy (occasionally referred to as Chomsky-Schützenberger hierarchy) is a containment hierarchy of classes of formal grammars.
This hierarchy of grammars was described by Noam Chomsky in 1956. It is also named after Marcel-Paul Schützenberger, who played a crucial role in the development of the theory of formal languages. 
Formal grammars.
A formal grammar of this type consists of a finite set of "production rules" ("left-hand side" → "right-hand side"), where each side consists of a sequence of the following symbols:
A formal grammar defines (or "generates") a "formal language", which is a (usually infinite) set of finite-length sequences of symbols that may be constructed by applying production rules to another sequence of symbols (which initially contains just the start symbol). A rule may be applied by replacing an occurrence of the symbols on its left-hand side with those that appear on its right-hand side. A sequence of rule applications is called a "derivation". Such a grammar defines the formal language: all words consisting solely of terminal symbols which can be reached by a derivation from the start symbol.
Nonterminals are often represented by uppercase letters, terminals by lowercase letters, and the start symbol by . For example, the grammar with terminals , nonterminals , production rules
and start symbol , defines the language of all words of the form formula_1 (i.e. copies of followed by copies of ).
The following is a simpler grammar that defines the same language: 
Terminals , Nonterminals , Start symbol , Production rules
As another example, a grammar for a toy subset of English language is given by:
and start symbol . An example derivation is
Other sequences that can be derived from this grammar are: ""ideas hate great linguists"", and ""ideas generate"". While these sentences are nonsensical, they are syntactically correct. A syntactically incorrect sentence ( e.g. ""ideas ideas great hate"") cannot be derived from this grammar. See "Colorless green ideas sleep furiously" for a similar example given by Chomsky in 1957; see Phrase structure grammar and Phrase structure rules for more natural language examples and the problems of formal grammar in that area.
The hierarchy.
The Chomsky hierarchy consists of the following levels:
Note that the set of grammars corresponding to recursive languages is not a member of this hierarchy; these would be properly between Type-0 and Type-1.
Every regular language is context-free, every context-free language (not containing the empty string) is context-sensitive, every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages which are not context-sensitive, context-sensitive languages which are not context-free and context-free languages which are not regular.
Summary.
The following table summarizes each of Chomsky's four types of grammars, the class of language it generates, the type of automaton that recognizes it, and the form its rules must have.
There are further categories of formal languages, some of which are given in the expandable navigation box at the bottom of this page.

</doc>
<doc id="6013" url="https://en.wikipedia.org/wiki?curid=6013" title="CRT">
CRT

CRT may refer to:

</doc>
<doc id="6014" url="https://en.wikipedia.org/wiki?curid=6014" title="Cathode ray tube">
Cathode ray tube

The cathode ray tube (CRT) is a vacuum tube containing one or more electron guns, and a phosphorescent screen used to view images. It has a means to accelerate and deflect the electron beam(s) onto the screen to create the images. The images may represent electrical waveforms (oscilloscope), pictures (television, computer monitor), radar targets or others. CRTs have also been used as memory devices, in which case the visible light emitted from the fluorescent material (if any) is not intended to have significant meaning to a visual observer (though the visible pattern on the tube face may cryptically represent the stored data).
The CRT uses an evacuated glass envelope which is large, deep (i.e. long from front screen face to rear end), fairly heavy, and relatively fragile. As a matter of safety, the face is typically made of thick lead glass so as to be highly shatter-resistant and to block most X-ray emissions, particularly if the CRT is used in a consumer product.
Since the late 2000s, CRTs have largely been superseded by newer display technologies such as LCD, plasma display, and OLED, which have lower manufacturing costs, power consumption, weight and bulk.
The vacuum level inside the tube is high vacuum on the order of to 
In television sets and computer monitors, the entire front area of the tube is scanned repetitively and systematically in a fixed pattern called a raster. An image is produced by controlling the intensity of each of the three electron beams, one for each additive primary color (red, green, and blue) with a video signal as a reference. In all modern CRT monitors and televisions, the beams are bent by "magnetic deflection", a varying magnetic field generated by coils and driven by electronic circuits around the neck of the tube, although electrostatic deflection is commonly used in oscilloscopes, a type of diagnostic instrument.
History.
Cathode rays were discovered by Johann Hittorf in 1869 in primitive Crookes tubes. He observed that some unknown rays were emitted from the cathode (negative electrode) which could cast shadows on the glowing wall of the tube, indicating the rays were traveling in straight lines. In 1890, Arthur Schuster demonstrated cathode rays could be deflected by electric fields, and William Crookes showed they could be deflected by magnetic fields. In 1897, J. J. Thomson succeeded in measuring the mass of cathode rays, showing that they consisted of negatively charged particles smaller than atoms, the first "subatomic particles", which were later named "electrons". The earliest version of the CRT was known as the "Braun tube", invented by the German physicist Ferdinand Braun in 1897. It was a cold-cathode diode, a modification of the Crookes tube with a phosphor-coated screen.
In 1907, Russian scientist Boris Rosing used a CRT in the receiving end of an experimental video signal to form a picture. He managed to display simple geometric shapes onto the screen, which marked the first time that CRT technology was used for what is now known as television.
The first cathode ray tube to use a hot cathode was developed by John B. Johnson (who gave his name to the term Johnson noise) and Harry Weiner Weinhart of Western Electric, and became a commercial product in 1922.
It was named by inventor Vladimir K. Zworykin in 1929. RCA was granted a trademark for the term (for its cathode ray tube) in 1932; it voluntarily released the term to the public domain in 1950.
The first commercially made electronic television sets with cathode ray tubes were manufactured by Telefunken in Germany in 1934.
Oscilloscope CRTs.
In oscilloscope CRTs, electrostatic deflection is used, rather than the magnetic deflection commonly used with television and other large CRTs. The beam is deflected horizontally by applying an electric field between a pair of plates to its left and right, and vertically by applying an electric field to plates above and below. Televisions use magnetic rather than electrostatic deflection because the deflection plates obstruct the beam when the deflection angle is as large as is required for tubes that are relatively short for their size.
Phosphor persistence.
Various phosphors are available depending upon the needs of the measurement or display application. The brightness, color, and persistence of the illumination depends upon the type of phosphor used on the CRT screen. Phosphors are available with persistences ranging from less than one microsecond to several seconds. For visual observation of brief transient events, a long persistence phosphor may be desirable. For events which are fast and repetitive, or high frequency, a short-persistence phosphor is generally preferable.
Microchannel plate.
When displaying fast one-shot events, the electron beam must deflect very quickly, with few electrons impinging on the screen, leading to a faint or invisible image on the display. Oscilloscope CRTs designed for very fast signals can give a brighter display by passing the electron beam through a micro-channel plate just before it reaches the screen. Through the phenomenon of secondary emission, this plate multiplies the number of electrons reaching the phosphor screen, giving a significant improvement in writing rate (brightness) and improved sensitivity and spot size as well.
Graticules.
Most oscilloscopes have a graticule as part of the visual display, to facilitate measurements. The graticule may be permanently marked inside the face of the CRT, or it may be a transparent external plate made of glass or acrylic plastic. An internal graticule eliminates parallax error, but cannot be changed to accommodate different types of measurements. Oscilloscopes commonly provide a means for the graticule to be illuminated from the side, which improves its visibility.
Image storage tubes.
These are found in "analog phosphor storage oscilloscopes". These are distinct from "digital storage oscilloscopes" which rely on solid state digital memory to store the image.
Where a single brief event is monitored by an oscilloscope, such an event will be displayed by a conventional tube only while it actually occurs. The use of a long persistence phosphor may allow the image to be observed after the event, but only for a few seconds at best. This limitation can be overcome by the use of a direct view storage cathode ray tube (storage tube). A storage tube will continue to display the event after it has occurred until such time as it is erased. A storage tube is similar to a conventional tube except that it is equipped with a metal grid coated with a dielectric layer located immediately behind the phosphor screen. An externally applied voltage to the mesh initially ensures that the whole mesh is at a constant potential. This mesh is constantly exposed to a low velocity electron beam from a 'flood gun' which operates independently of the main gun. This flood gun is not deflected like the main gun but constantly 'illuminates' the whole of the storage mesh. The initial charge on the storage mesh is such as to repel the electrons from the flood gun which are prevented from striking the phosphor screen.
When the main electron gun writes an image to the screen, the energy in the main beam is sufficient to create a 'potential relief' on the storage mesh. The areas where this relief is created no longer repel the electrons from the flood gun which now pass through the mesh and illuminate the phosphor screen. Consequently, the image that was briefly traced out by the main gun continues to be displayed after it has occurred. The image can be 'erased' by resupplying the external voltage to the mesh restoring its constant potential. The time for which the image can be displayed was limited because, in practice, the flood gun slowly neutralises the charge on the storage mesh. One way of allowing the image to be retained for longer is temporarily to turn off the flood gun. It is then possible for the image to be retained for several days. The majority of storage tubes allow for a lower voltage to be applied to the storage mesh which slowly restores the initial charge state. By varying this voltage a variable persistence is obtained. Turning off the flood gun and the voltage supply to the storage mesh allows such a tube to operate as a conventional oscilloscope tube.
Color CRTs.
Color tubes use three different phosphors which emit red, green, and blue light respectively. They are packed together in stripes (as in aperture grille designs) or clusters called "triads" (as in shadow mask CRTs). Color CRTs have three electron guns, one for each primary color, arranged either in a straight line or in an equilateral triangular configuration (the guns are usually constructed as a single unit). (The triangular configuration is often called "delta-gun", based on its relation to the shape of the Greek letter delta.) A grille or mask absorbs the electrons that would otherwise hit the wrong phosphor. A shadow mask tube uses a metal plate with tiny holes, placed so that the electron beam only illuminates the correct phosphors on the face of the tube; the holes are tapered so that the electrons that strike the inside of any hole will be reflected back, if they are not absorbed (e.g. due to local charge accumulation), instead of bouncing through the hole to strike a random (wrong) spot on the screen. Another type of color CRT uses an aperture grille of tensioned vertical wires to achieve the same result.
Convergence and purity in color CRTs.
Due to limitations in the dimensional precision with which CRTs can be manufactured economically, it has not been practically possible to build color CRTs in which three electron beams could be aligned to hit phosphors of respective color in acceptable coordination, solely on the basis of the geometric configuration of the electron gun axes and gun aperture positions, shadow mask apertures, etc. The shadow mask ensures that one beam will only hit spots of certain colors of phosphors, but minute variations in physical alignment of the internal parts among individual CRTs will cause variations in the exact alignment of the beams through the shadow mask, allowing some electrons from, for example, the red beam to hit, say, blue phosphors, unless some individual compensation is made for the variance among individual tubes.
Color convergence and color purity are two aspects of this single problem. Firstly, for correct color rendering it is necessary that regardless of where the beams are deflected on the screen, all three hit the same spot (and nominally pass through the same hole or slot) on the shadow mask. This is called convergence. More specifically, the convergence at the center of the screen (with no deflection field applied by the yoke) is called static convergence, and the convergence over the rest of the screen area is called dynamic convergence. The beams may converge at the center of the screen and yet stray from each other as they are deflected toward the edges; such a CRT would be said to have good static convergence but poor dynamic convergence. Secondly, each beam must only strike the phosphors of the
color it is intended to strike and no others. This is called purity. Like convergence, there is static purity and dynamic purity, with the same meanings of "static" and "dynamic" as for convergence. Convergence and purity are distinct parameters; a CRT could have good purity but poor convergence, or vice versa. Poor convergence causes color "shadows" or "ghosts" along displayed edges and contours, as if the image on the screen were intaglio printed with poor registration. Poor purity causes objects on the screen to appear off-color while their edges remain sharp. Purity and convergence problems can occur at the same time, in the same or different areas of the screen or both over the whole screen, and either uniformly or to greater or lesser degrees over different parts of the screen.
The solution to the static convergence and purity problems is a set of color alignment magnets installed around the neck of the CRT. These movable weak permanent magnets are usually mounted on the back end of the deflection yoke assembly and are set at the factory to compensate for any static purity and convergence errors that are intrinsic to the unadjusted tube. Typically there are two or three pairs of two magnets in the form of rings made of plastic impregnated with a magnetic material, with their magnetic fields parallel to the planes of the magnets, which are perpendicular to the electron gun axes. Each pair of magnetic rings forms a single effective magnet whose field vector can be fully and freely adjusted (in both direction and magnitude). By rotating a pair of magnets relative to each other, their relative field alignment can be varied, adjusting the effective field strength of the pair. (As they rotate relative to each other, each magnet's field can be considered to have two opposing components at right angles, and these four components each for two magnets form two pairs, one pair reinforcing each other and the other pair opposing and canceling each other. Rotating away from alignment, the magnets' mutually reinforcing field components decrease as they are traded for increasing opposed, mutually cancelling components.) By rotating a pair of magnets together, preserving the relative angle between them, the direction of their collective magnetic field can be varied. Overall, adjusting all of the convergence/purity magnets allows a finely tuned slight electron beam deflection and/or lateral offset to be applied, which compensates for minor static convergence and purity errors intrinsic to the uncalibrated tube. Once set, these magnets are usually glued in place, but normally they can be freed and readjusted in the field (e.g. by a TV repair shop) if necessary.
On some CRTs, additional fixed adjustable magnets are added for dynamic convergence and/or dynamic purity at specific points on the screen, typically near the corners or edges. Further adjustment of dynamic convergence and purity typically cannot be done passively, but requires active compensation circuits.
Dynamic color convergence and purity are one of the main reasons why until late in their history, CRTs were long-necked (deep) and had biaxially curved faces; these geometric design characteristics are necessary for intrinsic passive dynamic color convergence and purity. Only starting around the 1990s did sophisticated active dynamic convergence compensation circuits become available that made short-necked and flat-faced CRTs workable. These active compensation circuits use the deflection yoke to finely adjust beam deflection according to the beam target location. The same techniques (and major circuit components) also make possible the adjustment of display image rotation, skew, and other complex raster geometry parameters through electronics under user control.
Degaussing.
If the shadow mask becomes magnetized, its magnetic field deflects the electron beams passing through it, causing color purity distortion as the beams bend through the mask holes and hit some phosphors of a color other than that which they are intended to strike; e.g. some electrons from the red beam may hit blue phosphors, giving pure red parts of the image a magenta tint. (Magenta is the additive combination of red and blue.) This effect is localized to a specific area of the screen if the magnetization of the shadow mask is localized. Therefore, it is important that the shadow mask is unmagnetized. (A magnetized aperture grille has a similar effect, and everything stated in this subsection about shadow masks applies as well to aperture grilles.)
Most color CRT displays, i.e. television sets and computer monitors, each have a built-in degaussing (demagnetizing) circuit, the primary component of which is a degaussing coil which is mounted around the perimeter of the CRT face inside the bezel. Upon power-up of the CRT display, the degaussing circuit produces a brief, alternating current through the degaussing coil which smoothly decays in strength (fades out) to zero over a period of a few seconds, producing a decaying alternating magnetic field from the coil. This degaussing field is strong enough to remove shadow mask magnetization in most cases. In unusual cases of strong magnetization where the internal degaussing field is not sufficient, the shadow mask may be degaussed externally with a stronger portable degausser or demagnetizer. However, an excessively strong magnetic field, whether alternating or constant, may mechanically deform (bend) the shadow mask, causing a permanent color distortion on the display which looks very similar to a magnetization effect.
The degaussing circuit is often built of a thermo-electric (not electronic) device containing a small ceramic heating element and a positive thermal coefficient (PTC) resistor, connected directly to the switched AC power line with the resistor in series with the degaussing coil. When the power is switched on, the heating element heats the PTC resistor, increasing its resistance to a point where degaussing current is minimal, but not actually zero. In older CRT displays, this low-level current (which produces no significant degaussing field) is sustained along with the action of the heating element as long as the display remains switched on. To repeat a degaussing cycle, the CRT display must be switched off and left off for at least several seconds to reset the degaussing circuit by allowing the PTC resistor to cool to the ambient temperature; switching the display off and immediately back on will result in a weak degaussing cycle or effectively no degaussing cycle.
This simple design is effective and cheap to build, but it wastes some power continuously. Later models, especially Energy Star rated ones, use a relay to switch the entire degaussing circuit on and off, so that the degaussing circuit uses energy only when it is functionally active and needed. The relay design also enables degaussing on user demand through the unit's front panel controls, without switching the unit off and on again. This relay can often be heard clicking off at the end of the degaussing cycle a few seconds after the monitor is turned on, and on and off during a manually initiated degaussing cycle.
Vector monitors.
Vector monitors were used in early computer aided design systems and are in some late-1970s to mid-1980s arcade games such as "Asteroids".
They draw graphics point-to-point, rather than scanning a raster. Either monochrome or color CRTs can be used in vector displays, and the essential principles of CRT design and operation are the same for either type of display; the main difference is in the beam deflection patterns and circuits.
CRT resolution.
Dot pitch defines the maximum resolution of the display, assuming delta-gun CRTs. In these, as the scanned resolution approaches the dot pitch resolution, moiré appears, as the detail being displayed is finer than what the shadow mask can render. Aperture grille monitors do not suffer from vertical moiré; however, because their phosphor stripes have no vertical detail. In smaller CRTs, these strips maintain position by themselves, but larger aperture grille CRTs require one or two crosswise (horizontal) support strips.
Gamma.
CRTs have a pronounced triode characteristic, which results in significant gamma (a nonlinear relationship in an electron gun between applied video voltage and beam intensity).
Other types of CRTs.
Cat's eye.
In better quality tube radio sets a tuning guide consisting of a phosphor tube was used to aid the tuning adjustment. This was also known as a "Magic Eye" or "Tuning Eye". Tuning would be adjusted until the width of a radial shadow was minimized. This was used instead of a more expensive electromechanical meter, which later came to be used on higher-end tuners when transistor sets lacked the high voltage required to drive the device. The same type of device was used with tape recorders as a recording level meter, and for various other applications including electrical test equipment.
Charactrons.
Some displays for early computers (those that needed to display more text than was practical using vectors, or that required high speed for photographic output) used Charactron CRTs. These incorporate a perforated metal character mask (stencil), which shapes a wide electron beam to form a character on the screen. The system selects a character on the mask using one set of deflection circuits, but that causes the extruded beam to be aimed off-axis, so a second set of deflection plates has to re-aim the beam so it is headed toward the center of the screen. A third set of plates places the character wherever required. The beam is unblanked (turned on) briefly to draw the character at that position. Graphics could be drawn by selecting the position on the mask corresponding to the code for a space (in practice, they were simply not drawn), which had a small round hole in the center; this effectively disabled the character mask, and the system reverted to regular vector behavior. Charactrons had exceptionally long necks, because of the need for three deflection systems.
Nimo.
Nimo was the trademark of a family of small specialised CRTs manufactured by Industrial Electronics Engineers. These had 10 electron guns which produced electron beams in the form of digits in a manner similar to that of the charactron. The tubes were either simple single-digit displays or more complex 4- or 6- digit displays produced by means of a suitable magnetic deflection system. Having little of the complexities of a standard CRT, the tube required a relatively simple driving circuit, and as the image was projected on the glass face, it provided a much wider viewing angle than competitive types (e.g., nixie tubes).
Williams tube.
The Williams tube or Williams-Kilburn tube was a cathode ray tube used to electronically store binary data. It was used in computers of the 1940s as a random-access digital storage device. In contrast to other CRTs in this article, the Williams tube was not a display device, and in fact could not be viewed since a metal plate covered its screen.
Flood beam CRT.
Flood beam CRT's are small tubes that are arranged as pixels for large screens like Jumbotrons. The first screen using this technology was introduced by Mitsubishi Electric for the 1980 Major League Baseball All-Star Game. It differs from a normal CRT in that the electron gun within does not produce a focused controllable beam. Instead, electrons are sprayed in a wide cone across the entire front of the phosphor screen, basically making each unit act as a single light bulb. Each one is coated with a red, green or blue phosphor, to make up the color sub-pixels. This technology has largely been replaced with light emitting diode displays. A similar device has been proposed by one manufacturer as a lamp.
Zeus thin CRT display.
In the late 1990s and early 2000s Philips Research Laboratories experimented with a type of thin CRT known as the "Zeus" display which contained CRT-like functionality in a flat panel display. The devices were demonstrated but never marketed.
The future of CRT technology.
Demise.
Although a mainstay of display technology for decades, CRT-based computer monitors and televisions constitute a dead technology. The demand for CRT screens has dropped precipitously since 2007, and this falloff had accelerated in the last two years of that decade. The rapid advances and falling prices of LCD flat panel technology, first for computer monitors and then for televisions, has been the key factor in the demise of competing display technologies such as CRT, rear-projection, and plasma display.
The end of most high-end CRT production by around 2010 (including high-end Sony and Mitsubishi product lines) means an erosion of the CRT's capability. In Canada and the United States, the sale and production of high-end CRT TVs (30-inch screens) in these markets had all but ended by 2007. Just a couple of years later, inexpensive combo CRT TVs (20-inch screens with an integrated VHS player) disappeared from discount stores. It has been common to replace CRT-based televisions and monitors in as little as 5–6 years, although they generally are capable of satisfactory performance for a much longer time.
Companies are responding to this trend. Electronics retailers such as Best Buy have been steadily reducing store spaces for CRTs. In 2005, Sony announced that they would stop the production of CRT computer displays. Samsung did not introduce any CRT models for the 2008 model year at the 2008 Consumer Electronics Show, and on 4 February 2008 Samsung removed their 30" wide screen CRTs from their North American website and has not replaced them with new models.
However, the demise of CRTs has been happening more slowly in the developing world. According to iSupply, production in units of CRTs was not surpassed by LCDs production until 4Q 2007, owing largely to CRT production at factories in China.
In the United Kingdom, DSG (Dixons), the largest retailer of domestic electronic equipment, reported that CRT models made up 80–90% of the volume of televisions sold at Christmas 2004 and 15–20% a year later, and that they were expected to be less than 5% at the end of 2006. Dixons ceased selling CRT televisions in 2006.
Causes.
CRTs, despite recent advances, have remained relatively heavy and bulky and take up a lot of space in comparison to other display technologies. CRT screens have much deeper cabinets compared to flat panels and rear-projection displays for a given screen size, and so it becomes impractical to have CRTs larger than . The CRT disadvantages became especially significant in light of rapid technological advancements in LCD and plasma flat-panels which allow them to easily surpass as well as being thin and wall-mountable, two key features that were increasingly being demanded by consumers.
Slimmer CRT.
Some CRT manufacturers, both LG Display and Samsung Display, have innovated CRT technology by creating a slimmer tube. Slimmer CRT has a trade name Superslim and Ultraslim. A 21-inch flat CRT has 447.2 millimeter depth. The depth of Superslim is 352 millimeters and Ultraslim is 295.7 millimeters.
Resurgence in specialized markets.
In the first quarter of 2008, CRTs retook the #2 technology position in North America from plasma, due to the decline and consolidation of plasma display manufacturers. DisplaySearch has reported that although in the 4Q of 2007 LCDs surpassed CRTs in worldwide sales, CRTs then outsold LCDs in the 1Q of 2008.
CRTs are useful for displaying photos with high pixels per unit area and correct color balance. LCDs, as currently the most common flatscreen technology, have generally inferior color rendition (despite having greater overall brightness) due to the fluorescent lights commonly used as a backlight.
CRTs are still popular in the printing and broadcasting industries as well as in the professional video, photography, and graphics fields due to their greater color fidelity, contrast, and better viewing from off-axis (wider viewing angle). CRTs also still find adherents in vintage video gaming because of their higher resolution per initial cost, lowest possible input lag, fast response time, and multiple native resolutions such as 576p. Many retro gamers also prefer to use older CRTs with lower resolutions and scan rates due to the distinct scan line effect created when displaying the 240p analog signal generated by most retro consoles. CRT TVs are also the only way for almost the entire library of light gun games to be played, as newer TVs have a different signal latency which prevents light guns from working.
CRT monitors are still widely used in the study of the brain's visual processing (e.g. in psychophysics). The speed and fidelity of their response, combined with the simplicity of their design, makes them well-suited for experiments where scientists need to have very fine control over stimuli which are presented to an observer.
Health concerns.
Ionizing radiation.
CRTs can emit a small amount of X-ray radiation as a result of the electron beam's bombardment of the shadow mask/aperture grille and phosphors. The amount of radiation escaping the front of the monitor is widely considered not to be harmful. The Food and Drug Administration regulations in are used to strictly limit, for instance, television receivers to 0.5 milliroentgens per hour (mR/h) (0.13 µC/(kg·h) or 36 pA/kg) at a distance of from any external surface; since 2007, most CRTs have emissions that fall well below this limit.
Toxicity.
Older color and monochrome CRTs may contain toxic substances, such as cadmium, in the phosphors. The rear glass tube of modern CRTs may be made from leaded glass, which represent an environmental hazard if disposed of improperly. By the time personal computers were produced, glass in the front panel (the viewable portion of the CRT) used barium rather than lead, though the rear of the CRT was still produced from leaded glass. Monochrome CRTs typically do not contain enough leaded glass to fail EPA TCLP tests. While the TCLP process grinds the glass into fine particles in order to expose them to weak acids to test for leachate, intact CRT glass does not leache (The lead is vitrified, contained inside the glass itself, similar to leaded glass crystalware).
In October 2001, the United States Environmental Protection Agency created rules stating that CRTs must be brought to special recycling facilities. In November 2002, the EPA began fining companies that disposed of CRTs through landfills or incineration. Regulatory agencies, local and statewide, monitor the disposal of CRTs and other computer equipment.
In Europe, disposal of CRT televisions and monitors is covered by the WEEE Directive.
Flicker.
At low refresh rates (60 Hz and below), the periodic scanning of the display may produce a flicker that some people perceive more easily than others, especially when viewed with peripheral vision. Flicker is commonly associated with CRT as most televisions run at 50 Hz (PAL) or 60 Hz (NTSC), although there are some 100 Hz PAL televisions that are flicker-free. Typically only low-end monitors run at such low frequencies, with most computer monitors supporting at least 75 Hz and high-end monitors capable of 100 Hz or more to eliminate any perception of flicker. Non-computer CRTs or CRT for sonar or radar may have long persistence phosphor and are thus flicker free. If the persistence is too long on a video display, moving images will be blurred.
High-frequency audible noise.
50 Hz/60 Hz CRTs used for television operate with horizontal scanning frequencies of 15,734 Hz (for NTSC systems) or 15,625 Hz (for PAL systems). These frequencies are at the upper range of human hearing and are inaudible to many people; however, some people (especially children) will perceive a high-pitched tone near an operating television CRT. The sound is due to magnetostriction in the magnetic core and periodic movement of windings of the flyback transformer.
This problem does not occur on 100/120 Hz TVs and on non-CGA computer displays, because they are working on much higher frequencies (22 kHz to >100 kHz) compared to the low-frequency noise (50 Hz or 60 Hz) of mains hum.
Implosion.
High vacuum inside glass-walled cathode ray tubes permits electron beams to fly freely—without colliding into molecules of air or other gas. If the glass is damaged, atmospheric pressure can collapse the vacuum tube into dangerous fragments which accelerate inward and then spray at high speed in all directions. The implosion energy is proportional to the evacuated volume of the CRT. Although modern cathode ray tubes used in televisions and computer displays have epoxy-bonded face-plates or other measures to prevent shattering of the envelope, CRTs must be handled carefully to avoid personal injury.
Security concerns.
Under some circumstances, the signal radiated from the electron guns, scanning circuitry, and associated wiring of a CRT can be captured remotely and used to reconstruct what is shown on the CRT using a process called Van Eck phreaking. Special TEMPEST shielding can mitigate this effect. Such radiation of a potentially exploitable signal, however, occurs also with other display technologies and with electronics in general.
Recycling.
As electronic waste, CRTs are considered one of the hardest types to recycle. CRTs have relatively high concentration of lead and phosphors (not phosphorus), both of which are necessary for the display. There are several companies in the United States that charge a small fee to collect CRTs, then subsidize their labour by selling the harvested copper, wire, and printed circuit boards. The United States Environmental Protection Agency (EPA) includes discarded CRT monitors in its category of "hazardous household waste" but considers CRTs that have been set aside for testing to be commodities if they are not discarded, speculatively accumulated, or left unprotected from weather and other damage.
Leaded CRT glass is sold to be remelted into other CRTs, or even broken down and used in road construction.

</doc>
<doc id="6015" url="https://en.wikipedia.org/wiki?curid=6015" title="Crystal">
Crystal

A crystal or crystalline solid is a solid material whose constituents, such as atoms, molecules or ions, are arranged in a highly ordered microscopic structure, forming a crystal lattice that extends in all directions. In addition, macroscopic single crystals are usually identifiable by their geometrical shape, consisting of flat faces with specific, characteristic orientations.
The scientific study of crystals and crystal formation is known as crystallography. The process of crystal formation via mechanisms of crystal growth is called crystallization or solidification.
The word "crystal" is derived from the Ancient Greek word (), meaning both “ice” and “rock crystal”, from (), "icy cold, frost".
Examples of large crystals include snowflakes, diamonds, and table salt. Most inorganic solids are not crystals but polycrystals, i.e. many microscopic crystals fused together into a single solid. Examples of polycrystals include most metals, rocks, ceramics, and ice. A third category of solids is amorphous solids, where the atoms have no periodic structure whatsoever. Examples of amorphous solids include glass, wax, and many plastics.
Crystal structure (microscopic).
The scientific definition of a "crystal" is based on the microscopic arrangement of atoms inside it, called the crystal structure. A crystal is a solid where the atoms form a periodic arrangement. (Quasicrystals are an exception, see below.)
Not all solids are crystals. For example, when liquid water starts freezing, the phase change begins with small ice crystals that grow until they fuse, forming a "polycrystalline" structure. In the final block of ice, each of the small crystals (called "crystallites" or "grains") is a true crystal with a periodic arrangement of atoms, but the whole polycrystal does "not" have a periodic arrangement of atoms, because the periodic pattern is broken at the grain boundaries. Most macroscopic inorganic solids are polycrystalline, including almost all metals, ceramics, ice, rocks, etc. Solids that are neither crystalline nor polycrystalline, such as glass, are called "amorphous solids", also called glassy, vitreous, or noncrystalline. These have no periodic order, even microscopically. There are distinct differences between crystalline solids and amorphous solids: most notably, the process of forming a glass does not release the latent heat of fusion, but forming a crystal does.
A crystal structure (an arrangement of atoms in a crystal) is characterized by its "unit cell", a small imaginary box containing one or more atoms in a specific spatial arrangement. The unit cells are stacked in three-dimensional space to form the crystal.
The symmetry of a crystal is constrained by the requirement that the unit cells stack perfectly with no gaps. There are 219 possible crystal symmetries, called crystallographic space groups. These are grouped into 7 crystal systems, such as cubic crystal system (where the crystals may form cubes or rectangular boxes, such as halite shown at right) or hexagonal crystal system (where the crystals may form hexagons, such as ordinary water ice).
Crystal faces and shapes.
Crystals are commonly recognized by their shape, consisting of flat faces with sharp angles. These shape characteristics are not "necessary" for a crystal—a crystal is scientifically defined by its microscopic atomic arrangement, not its macroscopic shape—but the characteristic macroscopic shape is often present and easy to see.
Euhedral crystals are those with obvious, well-formed flat faces. Anhedral crystals do not, usually because the crystal is one grain in a polycrystalline solid.
The flat faces (also called facets) of a euhedral crystal are oriented in a specific way relative to the underlying atomic arrangement of the crystal: They are planes of relatively low Miller index. This occurs because some surface orientations are more stable than others (lower surface energy). As a crystal grows, new atoms attach easily to the rougher and less stable parts of the surface, but less easily to the flat, stable surfaces. Therefore, the flat surfaces tend to grow larger and smoother, until the whole crystal surface consists of these plane surfaces. (See diagram on right.)
One of the oldest techniques in the science of crystallography consists of measuring the three-dimensional orientations of the faces of a crystal, and using them to infer the underlying crystal symmetry.
A crystal's habit is its visible external shape. This is determined by the crystal structure (which restricts the possible facet orientations), the specific crystal chemistry and bonding (which may favor some facet types over others), and the conditions under which the crystal formed.
Occurrence in nature.
Rocks.
By volume and weight, the largest concentrations of crystals in the Earth are part of its solid bedrock. Crystals found in rocks typically range in size from a fraction of a millimetre to several centimetres across, although exceptionally large crystals are occasionally found. , the world's largest known naturally occurring crystal is a crystal of beryl from Malakialina, Madagascar, long and in diameter, and weighing .
Some crystals have formed by magmatic and metamorphic processes, giving origin to large masses of crystalline rock. The vast majority of igneous rocks are formed from molten magma and the degree of crystallization depends primarily on the conditions under which they solidified. Such rocks as granite, which have cooled very slowly and under great pressures, have completely crystallized; but many kinds of lava were poured out at the surface and cooled very rapidly, and in this latter group a small amount of amorphous or glassy matter is common. Other crystalline rocks, the metamorphic rocks such as marbles, mica-schists and quartzites, are recrystallized. This means that they were at first fragmental rocks like limestone, shale and sandstone and have never been in a molten condition nor entirely in solution, but the high temperature and pressure conditions of metamorphism have acted on them by erasing their original structures and inducing recrystallization in the solid state.
Other rock crystals have formed out of precipitation from fluids, commonly water, to form druses or quartz veins.
The evaporites such as halite, gypsum and some limestones have been deposited from aqueous solution, mostly owing to evaporation in arid climates.
Ice.
Water-based ice in the form of snow, sea ice and glaciers is a very common manifestation of crystalline or polycrystalline matter on Earth. A single snowflake is typically a single crystal, while an ice cube is a polycrystal.
Organigenic crystals.
Many living organisms are able to produce crystals, for example calcite and aragonite in the case of most molluscs or hydroxylapatite in the case of vertebrates.
Polymorphism and allotropy.
The same group of atoms can often solidify in many different ways. Polymorphism is the ability of a solid to exist in more than one crystal form. For example, water ice is ordinarily found in the hexagonal form Ice Ih, but can also exist as the cubic Ice Ic, the rhombohedral ice II, and many other forms. The different polymorphs are usually called different "phases".
In addition, the same atoms may be able to form noncrystalline phases. For example, water can also form amorphous ice, while SiO2 can form both fused silica (an amorphous glass) and quartz (a crystal). Likewise, if a substance can form crystals, it can also form polycrystals.
For pure chemical elements, polymorphism is known as allotropy. For example, diamond and graphite are two crystalline forms of carbon, while amorphous carbon is a noncrystalline form. Polymorphs, despite having the same atoms, may have wildly different properties. For example, diamond is among the hardest substances known, while graphite is so soft that it is used as a lubricant.
Polyamorphism is a similar phenomenon where the same atoms can exist in more than one amorphous solid form.
Crystallization.
Crystallization is the process of forming a crystalline structure from a fluid or from materials dissolved in a fluid. (More rarely, crystals may be deposited directly from gas; see thin-film deposition and epitaxy.)
Crystallization is a complex and extensively-studied field, because depending on the conditions, a single fluid can solidify into many different possible forms. It can form a single crystal, perhaps with various possible phases, stoichiometries, impurities, defects, and habits. Or, it can form a polycrystal, with various possibilities for the size, arrangement, orientation, and phase of its grains. The final form of the solid is determined by the conditions under which the fluid is being solidified, such as the chemistry of the fluid, the ambient pressure, the temperature, and the speed with which all these parameters are changing.
Specific industrial techniques to produce large single crystals (called "boules") include the Czochralski process and the Bridgman technique. Other less exotic methods of crystallization may be used, depending on the physical properties of the substance, including hydrothermal synthesis, sublimation, or simply solvent-based crystallization.
Large single crystals can be created by geological processes. For example, selenite crystals in excess of 10 meters are found in the Cave of the Crystals in Naica, Mexico. For more details on geological crystal formation, see above.
Crystals can also be formed by biological processes, see above. Conversely, some organisms have special techniques to "prevent" crystallization from occurring, such as antifreeze proteins.
Defects, impurities, and twinning.
An "ideal" crystal has every atom in a perfect, exactly repeating pattern. However, in reality, most crystalline materials have a variety of crystallographic defects, places where the crystal's pattern is interrupted. The types and structures of these defects may have a profound effect on the properties of the materials.
A few examples of crystallographic defects include vacancy defects (an empty space where an atom should fit), interstitial defects (an extra atom squeezed in where it does not fit), and dislocations (see figure at right). Dislocations are especially important in materials science, because they help determine the mechanical strength of materials.
Another common type of crystallographic defect is an impurity, meaning that the "wrong" type of atom is present in a crystal. For example, a perfect crystal of diamond would only contain carbon atoms, but a real crystal might perhaps contain a few boron atoms as well. These boron impurities change the diamond's color to slightly blue. Likewise, the only difference between ruby and sapphire is the type of impurities present in a corundum crystal.
In semiconductors, a special type of impurity, called a dopant, drastically changes the crystal's electrical properties. Semiconductor devices, such as transistors, are made possible largely by putting different semiconductor dopants into different places, in specific patterns.
Twinning is a phenomenon somewhere between a crystallographic defect and a grain boundary. Like a grain boundary, a twin boundary has different crystal orientations on its two sides. But unlike a grain boundary, the orientations are not random, but related in a specific, mirror-image way.
Mosaicity is a spread of crystal plane orientations. A mosaic crystal is supposed to consist of smaller crystalline units that are somewhat misaligned with respect to each other.
Chemical bonds.
In general, solids can be held together by various types of chemical bonds, such as metallic bonds, ionic bonds, covalent bonds, van der Waals bonds, and others. None of these are necessarily crystalline or non-crystalline. However, there are some general trends as follows.
Metals are almost always polycrystalline, though there are exceptions like amorphous metal and single-crystal metals. The latter are grown synthetically. (A microscopically-small piece of metal may naturally form into a single crystal, but larger pieces generally do not.) Ionically bonded solids are usually crystalline or polycrystalline. In practice, large salt crystals can be created by solidification of a molten fluid, or by crystallization out of a solution. Covalently bonded crystals are also very common, notable examples being diamond, quartz, and graphite. Polymer materials generally will form crystalline regions, but the lengths of the molecules usually prevent complete crystallization—and sometimes polymers are completely amorphous. Weak van der Waals forces also help hold together certain crystals, including graphite.
Quasicrystals.
A quasicrystal consists of arrays of atoms that are ordered but not strictly periodic. They have many attributes in common with ordinary crystals, such as displaying a discrete pattern in x-ray diffraction, and the ability to form shapes with smooth, flat faces.
Quasicrystals are most famous for their ability to show five-fold symmetry, which is impossible for an ordinary periodic crystal (see crystallographic restriction theorem).
The International Union of Crystallography has redefined the term "crystal" to include both ordinary periodic crystals and quasicrystals ("any solid having an essentially discrete diffraction diagram").
Quasicrystals, first discovered in 1982, are quite rare in practice. Only about 100 solids are known to form quasicrystals, compared to about 400,000 periodic crystals measured to date. The 2011 Nobel Prize in Chemistry was awarded to Dan Shechtman for the discovery of quasicrystals.
Special properties from anisotropy.
Crystals can have certain special electrical, optical, and mechanical properties that glass and polycrystals normally cannot. These properties are related to the anisotropy of the crystal, i.e. the lack of rotational symmetry in its atomic arrangement. One such property is the piezoelectric effect, where a voltage across the crystal can shrink or stretch it. Another is birefringence, where a double image appears when looking through a crystal. Moreover, various properties of a crystal, including electrical conductivity, electrical permittivity, and Young's modulus, may be different in different directions in a crystal. For example, graphite crystals consist of a stack of sheets, and although each individual sheet is mechanically very strong, the sheets are rather loosely bound to each other. Therefore, the mechanical strength of the material is quite different depending on the direction of stress.
Not all crystals have all of these properties. Conversely, these properties are not quite exclusive to crystals. They can appear in glasses or polycrystals that have been made anisotropic by working or stress—for example, stress-induced birefringence.
Crystallography.
"Crystallography" is the science of measuring the crystal structure (in other words, the atomic arrangement) of a crystal. One widely used crystallography technique is X-ray diffraction. Large numbers of known crystal structures are stored in crystallographic databases.

</doc>
<doc id="6016" url="https://en.wikipedia.org/wiki?curid=6016" title="Cytosine">
Cytosine

Cytosine (; C) is one of the four main bases found in DNA and RNA, along with adenine, guanine, and thymine (uracil in RNA). It is a pyrimidine derivative, with a heterocyclic aromatic ring and two substituents attached (an amine group at position 4 and a keto group at position 2). The nucleoside of cytosine is cytidine. In Watson-Crick base pairing, it forms three hydrogen bonds with guanine.
History.
Cytosine was discovered and named by Albrecht Kossel and Albert Neumann in 1894 when it was hydrolyzed from calf thymus tissues. A structure was proposed in 1903, and was synthesized (and thus confirmed) in the laboratory in the same year.
Cytosine recently found use in quantum computation. The first time any quantum mechanical properties were harnessed to process information took place on August 1 in 1998 when researchers at Oxford implemented David Deutsch's algorithm on a two qubit nuclear magnetic resonance quantum computer (NMRQC) based on cytosine.
In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.
Chemical reactions.
Cytosine can be found as part of DNA, as part of RNA, or as a part of a nucleotide. As cytidine triphosphate (CTP), it can act as a co-factor to enzymes, and can transfer a phosphate to convert adenosine diphosphate (ADP) to adenosine triphosphate (ATP).
In DNA and RNA, cytosine is paired with guanine. However, it is inherently unstable, and can change into uracil (spontaneous deamination). This can lead to a point mutation if not repaired by the DNA repair enzymes such as uracil glycosylase, which cleaves a uracil in DNA.
When found third in a codon of RNA, cytosine is synonymous with uracil, as they are interchangeable as the third base.
When found as the second base in a codon, the third is always interchangeable. For example, UCU, UCC, UCA and UCG are all serine, regardless of the third base.
Cytosine can also be methylated into 5-methylcytosine by an enzyme called DNA methyltransferase or be methylated and hydroxylated to make 5-hydroxymethylcytosine.
Active enzymatic deamination of cytosine or 5-methylcytosine by the APOBEC family of cytosine deaminases could have both beneficial and detrimental implications on various cellular processes as well as on organismal evolution. The implications of deamination on 5-hydroxymethylcytosine, on the other hand, remains less understood.

</doc>
<doc id="6019" url="https://en.wikipedia.org/wiki?curid=6019" title="Computational chemistry">
Computational chemistry

Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids. Its necessity arises from the fact that — apart from relatively recent results concerning the hydrogen molecular ion (see references therein for more details) — the quantum many-body problem cannot be solved analytically, much less in closed form. While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.
Examples of such properties are structure (i.e. the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity or other spectroscopic quantities, and cross sections for collision with other particles.
The methods employed cover both static and dynamic situations. In all cases the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied. That system can be a single molecule, a group of molecules, or a solid. Computational chemistry methods range from highly accurate to very approximate; highly accurate methods are typically feasible only for small systems. "Ab initio" methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they employ additional empirical parameters.
Both "ab initio" and semi-empirical approaches involve approximations. These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all. For example, most "ab initio" calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation. In principle, "ab initio" methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced. In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains. The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.
In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are employed, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target.
History.
Building on the founding discoveries and theories in the history of quantum mechanics, the first theoretical calculations in chemistry were those of Walter Heitler and Fritz London in 1927. The books that were influential in the early development of computational quantum chemistry include Linus Pauling and E. Bright Wilson's 1935 "Introduction to Quantum Mechanics – with Applications to Chemistry", Eyring, Walter and Kimball's 1944 "Quantum Chemistry", Heitler's 1945 "Elementary Wave Mechanics – with Applications to Quantum Chemistry", and later Coulson's 1952 textbook "Valence", each of which served as primary references for chemists in the decades to follow.
With the development of efficient computer technology in the 1940s, the solutions of elaborate wave equations for complex atomic systems began to be a realizable objective. In the early 1950s, the first semi-empirical atomic orbital calculations were carried out. Theoretical chemists became extensive users of the early digital computers. A very detailed account of such use in the United Kingdom is given by Smith and Sutcliffe. The first "ab initio" Hartree–Fock calculations on diatomic molecules were carried out in 1956 at MIT, using a basis set of Slater orbitals. For diatomic molecules, a systematic study using a minimum basis set and the first calculation with a larger basis set were published by Ransil and Nesbet respectively in 1960. The first polyatomic calculations using Gaussian orbitals were carried out in the late 1950s. The first configuration interaction calculations were carried out in Cambridge on the EDSAC computer in the 1950s using Gaussian orbitals by Boys and coworkers. By 1971, when a bibliography of "ab initio" calculations was published, the largest molecules included were naphthalene and azulene. Abstracts of many earlier developments in "ab initio" theory have been published by Schaefer.
In 1964, Hückel method calculations (using a simple linear combination of atomic orbitals (LCAO) method for the determination of electron energies of molecular orbitals of π electrons in conjugated hydrocarbon systems) of molecules ranging in complexity from butadiene and benzene to ovalene, were generated on computers at Berkeley and Oxford. These empirical methods were replaced in the 1960s by semi-empirical methods such as CNDO.
In the early 1970s, efficient "ab initio" computer programs such as ATMOL, Gaussian, IBMOL, and POLYAYTOM, began to be used to speed up "ab initio" calculations of molecular orbitals. Of these four programs, only GAUSSIAN, now massively expanded, is still in use, but many other programs are now in use. At the same time, the methods of molecular mechanics, such as MM2, were developed, primarily by Norman Allinger.
One of the first mentions of the term "computational chemistry" can be found in the 1970 book "Computers and Their Role in the Physical Sciences" by Sidney Fernbach and Abraham Haskell Taub, where they state "It seems, therefore, that 'computational chemistry' can finally be more and more of a reality." During the 1970s, widely different methods began to be seen as part of a new emerging discipline of "computational chemistry". The "Journal of Computational Chemistry" was first published in 1980.
Computational chemistry has featured in a number of Nobel Prize awards, most notably in 1998 and 2013. Walter Kohn, "for his development of the density-functional theory", and John Pople, "for his development of computational methods in quantum chemistry", received the 1998 Nobel Prize in Chemistry. Martin Karplus, Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for "the development of multiscale models for complex chemical systems".
Fields of application.
The term "theoretical chemistry" may be defined as a mathematical description of chemistry, whereas "computational chemistry" is usually used when a mathematical method is sufficiently well developed that it can be automated for implementation on a computer. In theoretical chemistry, chemists, physicists and mathematicians develop algorithms and computer programs to predict atomic and molecular properties and reaction paths for chemical reactions. Computational chemists, in contrast, may simply apply existing computer programs and methodologies to specific chemical questions.
There are two different aspects to computational chemistry:
Thus, computational chemistry can assist the experimental chemist or it can challenge the experimental chemist to find entirely new chemical objects.
Several major areas may be distinguished within computational chemistry:
Accuracy.
The words "exact" and "perfect" do not appear here, as very few aspects of chemistry can be computed exactly. However, almost every aspect of chemistry can be described in a qualitative or approximate quantitative computational scheme.
Molecules consist of nuclei and electrons, so the methods of quantum mechanics apply. Computational chemists often attempt to solve the non-relativistic Schrödinger equation, with relativistic corrections added, although some progress has been made in solving the fully relativistic Dirac equation. In principle, it is possible to solve the Schrödinger equation in either its time-dependent or time-independent form, as appropriate for the problem in hand; in practice, this is not possible except for very small systems. Therefore, a great number of approximate methods strive to achieve the best trade-off between accuracy and computational cost.
Accuracy can always be improved with greater computational cost. Significant errors can present themselves in ab initio models comprising many electrons, due to the computational expense of full relativistic-inclusive methods. This complicates the study of molecules interacting with high atomic mass unit atoms, such as transitional metals and their catalytic properties. Present algorithms in computational chemistry can routinely calculate the properties of molecules that contain up to about 40 electrons with sufficient accuracy. Errors for energies can be less than a few kJ/mol. For geometries, bond lengths can be predicted within a few picometres and bond angles within 0.5 degrees. The treatment of larger molecules that contain a few dozen electrons is computationally tractable by approximate methods such as density functional theory (DFT).
There is some dispute within the field whether or not the latter methods are sufficient to describe complex chemical reactions, such as those in biochemistry. Large molecules can be studied by semi-empirical approximate methods. Even larger molecules are treated by classical mechanics methods that employ what are called molecular mechanics. In QM/MM methods, small portions of large complexes are treated quantum mechanically (QM), and the remainder is treated approximately (MM).
Methods.
A single molecular formula can represent a number of molecular isomers. Each isomer is a local minimum on the energy surface (called the potential energy surface) created from the total energy (i.e., the electronic energy, plus the repulsion energy between the nuclei) as a function of the coordinates of all the nuclei. A stationary point is a geometry such that the derivative of the energy with respect to all displacements of the nuclei is zero. A local (energy) minimum is a stationary point where all such displacements lead to an increase in energy. The local minimum that is lowest is called the global minimum and corresponds to the most stable isomer. If there is one particular coordinate change that leads to a decrease in the total energy in both directions, the stationary point is a transition structure and the coordinate is the reaction coordinate. This process of determining stationary points is called geometry optimization.
The determination of molecular structure by geometry optimization became routine only after efficient methods for calculating the first derivatives of the energy with respect to all atomic coordinates became available. Evaluation of the related second derivatives allows the prediction of vibrational frequencies if harmonic motion is estimated. More importantly, it allows for the characterization of stationary points. The frequencies are related to the eigenvalues of the Hessian matrix, which contains second derivatives. If the eigenvalues are all positive, then the frequencies are all real and the stationary point is a local minimum. If one eigenvalue is negative (i.e., an imaginary frequency), then the stationary point is a transition structure. If more than one eigenvalue is negative, then the stationary point is a more complex one, and is usually of little interest. When one of these is found, it is necessary to move the search away from it if the experimenter is looking solely for local minima and transition structures.
The total energy is determined by approximate solutions of the time-dependent Schrödinger equation, usually with no relativistic terms included, and by making use of the Born–Oppenheimer approximation, which allows for the separation of electronic and nuclear motions, thereby simplifying the Schrödinger equation. This leads to the evaluation of the total energy as a sum of the electronic energy at fixed nuclei positions and the repulsion energy of the nuclei. A notable exception are certain approaches called direct quantum chemistry, which treat electrons and nuclei on a common footing. Density functional methods and semi-empirical methods are variants on the major theme. For very large systems, the relative total energies can be compared using molecular mechanics. The ways of determining the total energy to predict molecular structures are:
"Ab initio" methods.
The programs used in computational chemistry are based on many different quantum-chemical methods that solve the molecular Schrödinger equation associated with the molecular Hamiltonian. Methods that do not include any empirical or semi-empirical parameters in their equations – being derived directly from theoretical principles, with no inclusion of experimental data – are called "ab initio methods". This does not imply that the solution is an exact one; they are all approximate quantum mechanical calculations. It means that a particular approximation is rigorously defined on first principles (quantum theory) and then solved within an error margin that is qualitatively known beforehand. If numerical iterative methods have to be employed, the aim is to iterate until full machine accuracy is obtained (the best that is possible with a finite word length on the computer, and within the mathematical and/or physical approximations made).
The simplest type of "ab initio" electronic structure calculation is the Hartree–Fock (HF) scheme, an extension of molecular orbital theory, in which the correlated electron–electron repulsion is not specifically taken into account; only its average effect is included in the calculation. As the basis set size is increased, the energy and wave function tend towards a limit called the Hartree–Fock limit. Many types of calculations (known as post-Hartree–Fock methods) begin with a Hartree–Fock calculation and subsequently correct for electron–electron repulsion, referred to also as electronic correlation. As these methods are pushed to the limit, they approach the exact solution of the non-relativistic Schrödinger equation. In order to obtain exact agreement with experiment, it is necessary to include relativistic and spin orbit terms, both of which are only really important for heavy atoms. In all of these approaches, in addition to the choice of method, it is necessary to choose a basis set. This is a set of functions, usually centered on the different atoms in the molecule, which are used to expand the molecular orbitals with the LCAO ansatz. Ab initio methods need to define a level of theory (the method) and a basis set.
The Hartree–Fock wave function is a single configuration or determinant. In some cases, particularly for bond breaking processes, this is quite inadequate, and several configurations need to be used. Here, the coefficients of the configurations and the coefficients of the basis functions are optimized together.
The total molecular energy can be evaluated as a function of the molecular geometry; in other words, the potential energy surface. Such a surface can be used for reaction dynamics. The stationary points of the surface lead to predictions of different isomers and the transition structures for conversion between isomers, but these can be determined without a full knowledge of the complete surface.
A particularly important objective, called computational thermochemistry, is to calculate thermochemical quantities such as the enthalpy of formation to chemical accuracy. Chemical accuracy is the accuracy required to make realistic chemical predictions and is generally considered to be 1 kcal/mol or 4 kJ/mol. To reach that accuracy in an economic way it is necessary to use a series of post-Hartree–Fock methods and combine the results. These methods are called quantum chemistry composite methods.
Density functional methods.
Density functional theory (DFT) methods are often considered to be "ab initio methods" for determining the molecular electronic structure, even though many of the most common functionals use parameters derived from empirical data, or from more complex calculations. In DFT, the total energy is expressed in terms of the total one-electron density rather than the wave function. In this type of calculation, there is an approximate Hamiltonian and an approximate expression for the total electron density. DFT methods can be very accurate for little computational cost. Some methods combine the density functional exchange functional with the Hartree–Fock exchange term and are known as
hybrid functional methods.
Semi-empirical and empirical methods.
Semi-empirical quantum chemistry methods are based on the Hartree–Fock formalism, but make many approximations and obtain some parameters from empirical data. They are very important in computational chemistry for treating large molecules where the full Hartree–Fock method without the approximations is too expensive. The use of empirical parameters appears to allow some inclusion of correlation effects into the methods.
Semi-empirical methods follow what are often called empirical methods, where the two-electron part of the Hamiltonian is not explicitly included. For π-electron systems, this was the Hückel method proposed by Erich Hückel, and for all valence electron systems, the extended Hückel method proposed by Roald Hoffmann.
Molecular mechanics.
In many cases, large molecular systems can be modeled successfully while avoiding quantum mechanical calculations entirely. Molecular mechanics simulations, for example, use a single classical expression for the energy of a compound, for instance the harmonic oscillator. All constants appearing in the equations must be obtained beforehand from experimental data or "ab initio" calculations.
The database of compounds used for parameterization, i.e., the resulting set of parameters and functions is called the force field, is crucial to the success of molecular mechanics calculations. A force field parameterized against a specific class of molecules, for instance proteins, would be expected to only have any relevance when describing other molecules of the same class.
These methods can be applied to proteins and other large biological molecules, and allow studies of the approach and interaction (docking) of potential drug molecules.
Methods for solids.
Computational chemical methods can be applied to solid state physics problems. The electronic structure of a crystal is in general described by a band structure, which defines the energies of electron orbitals for each point in the Brillouin zone. Ab initio and semi-empirical calculations yield orbital energies; therefore, they can be applied to band structure calculations. Since it is time-consuming to calculate the energy for a molecule, it is even more time-consuming to calculate them for the entire list of points in the Brillouin zone.
Chemical dynamics.
Once the electronic and nuclear variables are separated (within the Born–Oppenheimer representation), in the time-dependent approach, the wave packet corresponding to the nuclear degrees of freedom is propagated via the time evolution operator (physics) associated to the time-dependent Schrödinger equation (for the full molecular Hamiltonian). In the complementary energy-dependent approach, the time-independent Schrödinger equation is solved using the scattering theory formalism. The potential representing the interatomic interaction is given by the potential energy surfaces. In general, the potential energy surfaces are coupled via the vibronic coupling terms.
The most popular methods for propagating the wave packet associated to the molecular geometry are:
Molecular dynamics.
Molecular dynamics (MD) use either quantum mechanics, Newton's laws of motion or a mixed model to examine the time-dependent behavior of systems, including vibrations or Brownian motion and reactions. MD combined with density functional theory leads to hybrid models.
Interpreting molecular wave functions.
The Atoms in molecules or QTAIM model of Richard Bader was developed in order to effectively link the quantum mechanical picture of a molecule, as an electronic wavefunction, to chemically useful concepts such as atoms in molecules, functional groups, bonding, the theory of Lewis pairs and the valence bond model. Bader has demonstrated that these empirically useful chemistry concepts can be related to the topology of the observable charge density distribution, whether measured or calculated from a quantum mechanical wavefunction. QTAIM analysis of molecular wavefunctions is implemented, for example, in the AIMAll software package.
Software packages.
There are many self-sufficient software packages used by computational chemists. Some include many methods covering a wide range, while others concentrating on a very specific range or even a single method. Details of most of them can be found in:

</doc>
<doc id="6020" url="https://en.wikipedia.org/wiki?curid=6020" title="Crash (J. G. Ballard novel)">
Crash (J. G. Ballard novel)

Crash is a novel by English author J. G. Ballard, first published in 1973. It is a story about symphorophilia or car-crash sexual fetishism: its protagonists become sexually aroused by staging and participating in real car-crashes.
It was a highly controversial novel: one publisher's reader returned the verdict "This author is beyond psychiatric help. Do Not Publish!" In 1996, the novel was made into a film of the same name by David Cronenberg. An earlier, apparently unauthorized adaptation called "Nightmare Angel" was filmed in 1986 by Susan Emerling and Zoe Beloff. This short film bears the credit "Inspired by J.G. Ballard."
Plot summary.
The story is told through the eyes of narrator James Ballard, named after the author himself, but it centers on the sinister figure of Dr. Robert Vaughan, a "former TV-scientist, turned nightmare angel of the expressways". Ballard meets Vaughan after being involved in a car accident himself near London Airport. Gathering around Vaughan is a group of alienated people, all of them former crash victims, who follow him in his pursuit to re-enact the crashes of celebrities and experience what the narrator calls "a new sexuality, born from a perverse technology". Vaughan's ultimate fantasy is to die in a head-on collision with movie star Elizabeth Taylor.
References in popular art.
Music.
The Normal's 1978 song "Warm Leatherette" was inspired by the novel, as was "Miss the Girl," a 1983 single by The Creatures.
The Manic Street Preachers' song "Mausoleum" from 1994's "The Holy Bible" contains the famous Ballard quote about his reasons for writing the book.
The singer Glenn Danzig sings about this in his band Samhain's song "Kiss Of Steel" on the album "November-Coming-Fire"
Other.
The RanXerox comics story "RanXerox in New York" features a character called Timothy who declares he is sexually aroused by "the meeting of flesh and metal in car crashes", then shows a photomontage he made of a female model with photos of wounds from a medical dictionary, claims to be in love with Brooke Shields and finally declares that his "dream is to die in an accident at the same time she does and to have an orgasm at the moment of impact". He almost fulfills his dream: forgotten in the trunk of the taxi RanXerox drives while living in New York, he is a collateral casualty of a titanic clash between RanXerox and the Boyband he was racing against. Still Standing, RanX remembers about him and rushes to rescue him. His ecstatic, dying remarks are: "Wow, Ranx... Incredible... who did we crash with?" at which the synthtic thug replies "Brooke Shields", and Timothy expires happily. the original title of the story was "Buon Compleanno Lubna" ("Happy Birthday Lubna").

</doc>
<doc id="6021" url="https://en.wikipedia.org/wiki?curid=6021" title="C (programming language)">
C (programming language)

C (, as in the letter "c") is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.
C was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs, and used to re-implement the Unix operating system. It has since become one of the most widely used programming languages of all time, with C compilers from various vendors available for the majority of existing computer architectures and operating systems. C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).
Design.
C is an imperative (procedural) language. It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal run-time support. C was therefore useful for many applications that had formerly been coded in assembly language, such as in system programming.
Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant and portably written C program can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code. The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.
Overview.
Like most imperative languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion, while a static type system prevents many unintended operations. In C, all executable code is contained within subroutines, which are called "functions" (although not in the strict sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.
The C language also exhibits the following characteristics:
C does not include some features found in newer, more modern high-level languages, including object orientation and garbage collection.
Relations to other languages.
Many later languages have borrowed directly or indirectly from C, including C++, D, Go, Rust, Java, JavaScript, Limbo, LPC, C#, Objective-C, Perl, PHP, Python, Verilog (hardware description language), and Unix's C shell. These languages have drawn many of their control structures and other basic features from C. Most of them (with Python being the most dramatic exception) are also very syntactically similar to C in general, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.
History.
Early developments.
The origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Ritchie and Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. The developers were considering rewriting the system using the B language, Thompson's simplified version of BCPL. However B's inability to take advantage of some of the PDP-11's features, notably byte addressability, led to C.
The development of C started in 1972 on the PDP-11 Unix system and first appeared in Version 2 Unix. The language was not initially designed with portability in mind, but soon ran on different platforms as well: a compiler for the Honeywell 6000 was written within the first year of C's history, while an IBM System/370 port followed soon. The name of C simply continued the alphabetic order started by B.
Also in 1972, a large part of Unix was rewritten in C. By 1973, with the addition of codice_12 types, the C language had become powerful enough that most of the Unix's kernel was now in C.
Unix was one of the first operating system kernels implemented in a language other than assembly. (Earlier instances include the Multics system (written in PL/I), and MCP (Master Control Program) for the Burroughs B5000 written in ALGOL in 1961.) Circa 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.
K&R C.
In 1978, Brian Kernighan and Dennis Ritchie published the first edition of "The C Programming Language". This book, known to C programmers as "K&R", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as "K&R C". The second edition of the book covers the later ANSI C standard, described below.
K&R introduced several language features:
Even after the publication of the 1989 ANSI standard, for many years K&R C was still considered the "lowest common denominator" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.
In early versions of C, only functions that returned a non-codice_28 value needed to be declared if used before the function definition; a function used without any previous declaration was assumed to return type codice_28, if its value was used.
For example:
The codice_28 type specifiers which are commented out could be omitted in K&R C, but are required in later standards.
Since K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.
In the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC) and some other vendors. These included:
The large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.
ANSI C and ISO C.
During the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.
In 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 "Programming Language C". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.
In 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms "C89" and "C90" refer to the same programming language.
ANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.
One of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), codice_15 pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.
C89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.
In cases where code must be compilable by either standard-conforming or K&R C-based compilers, the codice_36 macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.
After the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995 Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.
C99.
The C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as "C99". It has since been amended three times by Technical Corrigenda.
C99 introduced several new features, including inline functions, several new data types (including codice_37 and a codice_38 type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with codice_39, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.
C99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has codice_28 implicitly assumed. A standard macro codice_41 is defined with value codice_42 to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.
C11.
In 2007, work began on another revision of the C standard, informally called "C1X" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.
The C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro codice_41 is defined as codice_44 to indicate that C11 support is available.
Embedded C.
Historically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.
In 2008, the C Standards Committee published a technical report extending the C language to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.
Syntax.
C has a formal grammar specified by the C standard. Unlike languages such as FORTRAN 77, C source code is free-form which allows arbitrary use of whitespace to format code, rather than column-based or text-line-based restrictions; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters codice_45 and codice_46, or (since C99) following codice_39 until the end of the line. Comments delimited by codice_45 and codice_46 do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.
C source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as codice_12, codice_33, and codice_14, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as codice_53 and codice_28 specify built-in types. Sections of code are enclosed in braces (codice_55 and codice_56, sometimes called "curly brackets") to limit the scope of declarations and to act as a single statement for control structures.
As an imperative language, C uses "statements" to specify actions. The most common statement is an "expression statement", consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by codice_57(-codice_58) conditional execution and by codice_59-codice_3, codice_3, and codice_1 iterative execution (looping). The codice_1 statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. codice_64 and codice_65 can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured codice_66 statement which branches directly to the designated label within the function. codice_4 selects a codice_68 to be executed based on the value of an integer expression.
Expressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next "sequence point"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (codice_69, codice_70, codice_71 and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.
Kernighan and Ritchie say in the Introduction of "The C Programming Language": "C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better." The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.
Character set.
The basic C source character set includes the following characters:
Newline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.
Additional multibyte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multinational Unicode characters to be embedded portably within C source text by using codice_79 or codice_80 encoding (where the codice_81 denotes a hexadecimal character), although this feature is not yet widely implemented.
The basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return.
Run-time support for extended character sets has increased with each revision of the C standard.
Keywords.
C89 has 32 keywords (reserved words with special meaning):
C99 adds five more keywords:
C11 adds seven more keywords:
Most of the recently added keywords begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved keyword called codice_126, but this was seldom implemented, and has now been removed as a reserved word.
Operators.
C supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:
C uses the codice_132 operator, reserved in mathematics to express equality, to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. The similarity between C's operator for assignment and that for equality (codice_152) has been criticized as it makes it easy to accidentally substitute one for the other. In many cases, each may be used in the context of the other without a compilation error (although some compilers produce warnings). For example, the conditional expression in codice_172 is true if codice_72 is not zero after the assignment. Additionally, C's operator precedence is non-intuitive, such as codice_152 binding more tightly than codice_9 and codice_145 in expressions like codice_177, which would need to be written codice_178 to be properly evaluated.
"Hello, world" example.
The "hello, world" example, which appeared in the first edition of K&R, has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints "hello, world" to the standard output, which is usually a terminal or screen display.
The original version was:
<syntaxhighlight lang="c">
main()
</syntaxhighlight>
A standard-conforming "hello, world" program is:
<syntaxhighlight lang="c">
int main(void)
</syntaxhighlight>
The first line of the program contains a preprocessing directive, indicated by codice_179. This causes the compiler to replace that line with the entire text of the codice_180 standard header, which contains declarations for standard input and output functions such as codice_181. The angle brackets surrounding codice_180 indicate that codice_180 is located using a search strategy that prefers headers in the compiler's include path to other headers having the same name; double quotes are used to include local or project-specific header files.
The next line indicates that a function named codice_184 is being defined. The codice_184 function serves a special purpose in C programs; the run-time environment calls the codice_184 function to begin program execution. The type specifier codice_28 indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the codice_184 function, is an integer. The keyword codice_15 as a parameter list indicates that this function takes no arguments.
The opening curly brace indicates the beginning of the definition of the codice_184 function.
The next line "calls" (diverts execution to) a function named codice_181, which is supplied from a system library. In this call, the codice_181 function is "passed" (provided with) a single argument, the address of the first character in the string literal codice_193. The string literal is an unnamed array with elements of type codice_53, set up automatically by the compiler with a final 0-valued character to mark the end of the array (codice_181 needs to know this). The codice_196 is an "escape sequence" that C translates to a "newline" character, which on output signifies the end of the current line. The return value of the codice_181 function is of type codice_28, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the codice_181 function succeeded.) The semicolon codice_200 terminates the statement.
The closing curly brace indicates the end of the code for the codice_184 function. According to the C99 specification and newer, the codice_184 function, unlike any other function, will implicitly return a status of codice_76 upon reaching the codice_56 that terminates the function. This is interpreted by the run-time system as an exit code indicating successful execution.
Data types.
The type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal. There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, characters, and enumerated types (codice_14). C99 added a boolean datatype. There are also derived types including arrays, pointers, records (codice_12), and untagged unions (codice_33).
C is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a "type cast" to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.
Some find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: "declaration reflects use".)
C's "usual arithmetic conversions" allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.
Pointers.
C supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be "dereferenced" to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers. Many data types, such as trees, are commonly implemented as dynamically allocated codice_12 objects linked together using pointers. Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.
A "null pointer value" explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no "next" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a "null pointer constant" can be written as codice_76, with or without explicit casting to a pointer type, or as the codice_210 macro defined by several standard headers. In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.
Void pointers (codice_211) point to objects of unspecified type, and can therefore be used as "generic" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.
Careless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may be deallocated and reused (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.
Arrays.
Array types in C are traditionally of a fixed, static size specified at compile time. (The more recent C99 standard also allows a form of variable-length arrays.) However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's codice_212 function, and treat it as an array. C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.
Since arrays are always accessed (in effect) via pointers, array accesses are typically "not" checked against the underlying array size, although some compilers may provide bounds checking as an option. Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions. If bounds checking is desired, it must be done manually.
C does not have a special provision for declaring multidimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting "multidimensional array" can be thought of as increasing in row-major order.
Multidimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional "row vector" of pointers to the columns.)
C99 introduced "variable-length arrays" which address some, but not all, of the issues with ordinary C arrays.
Array–pointer interchangeability.
The subscript notation codice_213 designates a pointer) is a syntactic sugar for codice_214. Taking advantage of the compiler's knowledge of the pointer type, the address that codice_215 points to is not the base address (pointed to by codice_216) incremented by codice_217 bytes, but rather is defined to be the base address incremented by codice_217 multiplied by the size of an element that codice_216 points to. Thus, codice_220 designates the codice_221th element of the array.
Furthermore, in most expression contexts (a notable exception is as operand of codice_104), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.
The size of an element can be determined by applying the operator codice_104 to any dereferenced element of codice_216, as in codice_225 or <code>n = sizeof xand the number of elements in a declared array codice_74 can be determined as codice_227. The latter only applies to array names: variables declared with subscripts (<code>int ADue to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (codice_212); code such as codice_229 (where codice_230 designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested. Since array name arguments to codice_104 are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same codice_104 issues as array pointers.
Thus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array "points to" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the codice_233 function, or by accessing the individual elements.
Memory management.
One of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:
These three approaches are appropriate in different situations and have various tradeoffs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.
Where possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary. Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on codice_212 for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)
Unless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.
Another issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before codice_238 is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a "memory leak." Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)
Libraries.
The C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single "archive" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., codice_239, shorthand for "math library").
The most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation. (Implementations which target limited environments such as embedded systems may provide only a subset of the standard library.) This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, codice_180) specify the interfaces for these and other standard library facilities.
Another common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.
Since many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.
Language tools.
Tools have been created to help C programmers avoid some of the problems inherent in the language, such as statements with undefined behavior or statements that are not a good practice because they are likely to result in unintended behavior or run-time errors.
Automated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.
There are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as array bounds checking, buffer overflow detection, serialization, and automatic garbage collection.
Tools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.
Uses.
C is widely used for "system programming", including implementing operating systems and embedded system applications, due to a combination of desirable characteristics such as code portability and efficiency, ability to access specific hardware addresses, ability to pun types to match externally imposed data access requirements, and low run-time demand on system resources. C can also be used for website programming using CGI as a "gateway" for information between the Web application, the server, and the browser. Some reasons for choosing C over interpreted languages are its speed, stability, and near-universal availability.
One consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The primary implementations of Python, Perl 5 and PHP, for example, are all written in C.
Due to its thin layer of abstraction and low overhead, C allows efficient implementations of algorithms and data structures, which is useful for programs that perform a lot of computations. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica and MATLAB are completely or partially written in C.
C is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, it is not necessary to develop machine-specific code generators. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, which support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.
C has also been widely used to implement end-user applications, but much of that development has shifted to newer, higher-level languages.
Related languages.
C has directly or indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell. The most pervasive influence has been syntactical: all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.
Several C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.
When object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.
The C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax. C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.
Objective-C was originally a very "thin" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.
In addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C.

</doc>
<doc id="6022" url="https://en.wikipedia.org/wiki?curid=6022" title="Cytology">
Cytology

Cytology (from Greek , "kytos", "a hollow"; and , "-logia") is the study of cells. Cytology is that branch of life science that deals with the study of cells in terms of structure, function and chemistry. Robert Hooke is sometimes seen as the father of cytology. 
Based on usage it can refer to: 
The International Academy of Cytology has as its official journal "Acta Cytologica".

</doc>
<doc id="6023" url="https://en.wikipedia.org/wiki?curid=6023" title="Castle of the Winds">
Castle of the Winds

Castle of the Winds is a tile-based roguelike video game for Microsoft Windows. It was developed by SaadaSoft in 1989 and distributed by Epic MegaGames in 1993.
Release history.
The game is composed of two parts: A Question of Vengeance, released as shareware, and "Lifthransir's Bane", sold commercially. A combined license for both parts was also sold.
In 1998, the game's author, Rick Saada, decided to distribute the entirety of "Castle of the Winds" free of charge.
As a 16-bit application, the program will not run natively under 64-bit versions of the Windows operating system.
In 2012, game developer Leptoon began an MMO version of Castle of the Winds using the original source code given to them by Rick Saada in 2007. This MMO remake is based in the BYOND game development engine. Development is currently halted due to lack of funding.
Gameplay.
The game differs from most roguelikes in a number of ways. Its interface is mouse-dependent, but supports keyboard shortcuts (such as 'g' to get an item). "Castle of the Winds" also allows the player to restore saved games after dying.
Magic.
The game favors the use of magic in combat, as spells are the only weapons that work from a distance. The player character automatically gains a spell with each experience level, and can permanently gain others using corresponding books, until all thirty spells available are learned. There are two opposing pairs of elements: cold vs. fire and lightning vs. acid/poison. Spells are divided into six categories: attack, defense, healing, movement, divination, and miscellaneous.
Items.
"Castle of the Winds" possesses an inventory system that limits a player's load based on weight and bulk, rather than by number of items. It allows the character to use different containers, including packs, belts, chests, and bags. Other items include weapons, armor, protective clothing, purses, and ornamental jewellery. Almost every item in the game can be normal, cursed, or enchanted, with curses and enchantments working in a manner similar to "NetHack". Although items do not break with use, they may already be broken or rusted when found. Most objects that the character currently carries can be renamed.
Town Services.
Wherever the player goes before entering the dungeon, there is always a town which offers the basic services of a temple for healing and curing curses, a junk store where anything can be sold for a few copper coins, a sage who can identify items and (from the second town onwards) a bank for storing the total capacity of coins to lighten the player's load. Other services that differ and vary in what they sell are outfitters, weaponsmiths, armoursmiths, magic shops and general stores.
Time.
The game tracks how much time has been spent playing the game. Although story events are not triggered by the passage of time, it does determine when merchants rotate their stock. Victorious players are listed as "Valhalla's Champions" in the order of time taken, from fastest to slowest. If the player dies, they are still put on the list, but are categorized as "Dead", with their experience point total listed as at the final killing blow. The amount of time spent also determines the difficulty of the last boss.
Plot.
Though it is secondary to its hack and slash gameplay, "Castle of the Winds" has a plot loosely based on Norse mythology, told with setting changes, unique items, and occasional passages of text.
The player begins in a tiny hamlet, near which he/she used to live. His/her farm has been destroyed and godparents killed. After clearing out an abandoned mine, the player finds a scrap of parchment that reveals the death of the player's godparents was ordered by an unknown enemy. The player then returns to the hamlet to find it pillaged, and decides to travel to Bjarnarhaven.
Once in Bjarnarhaven, the player explores the levels beneath a nearby fortress, eventually facing Hrungnir, the Hill Giant Lord, responsible for ordering the player's godparents' death. Hrungnir carries the Enchanted Amulet of Kings. Upon activating the amulet, the player is informed of his/her past by his/her dead father, after which the player is transported to the town of Crossroads, and "Part I" ends. The game can be imported or started over in "Part II".
The town of Crossroads is run by a Jarl who at first does not admit the player, but later (on up to three occasions) provides advice and rewards. The player then enters the nearby ruined titular Castle of the Winds. There the player meets his/her deceased grandfather, who instructs him/her to venture into the dungeons below, defeat Surtur, and reclaim his birthright. Venturing deeper, the player encounters monsters run rampant, a desecrated crypt, a necromancer, and the installation of various special rooms for elementals. The player eventually meets and defeats the Wolf-Man leader, Bear-Man leader, the four Jotun kings, a Demon Lord, and finally Surtur. Upon defeating Surtur and escaping the dungeons, the player sits upon the throne, completing the game.
Graphics.
All terrain tiles, some landscape features, all monsters and objects, and some spell/effect graphics take the form of Windows 3.1 icons. Multi-tile graphics, such as ball spells and town buildings, are bitmaps included in the executable file. No graphics use colors other than the Windows-standard 16-color palette, plus transparency. They exist in monochrome versions as well, meaning that the game will display well on monochrome monitors.
The map view is identical to the playing-field view, except for scaling to fit on one screen. A simplified map view is available to improve performance on slower computers. The latter functionality also presents a cleaner display, as the aforementioned scaling routine does not always work correctly.

</doc>
<doc id="6024" url="https://en.wikipedia.org/wiki?curid=6024" title="Calvinism">
Calvinism

Calvinism (also called the Reformed tradition, Reformed Christianity, Reformed Protestantism or the Reformed faith) is a major branch of Protestantism that follows the theological tradition and forms of Christian practice of John Calvin and other Reformation-era theologians.
Calvinists broke with the Roman Catholic Church but differed with Lutherans on the real presence of Christ in the Eucharist, theories of worship, and the use of God's law for believers, among other things. Calvinism can be a misleading term because the religious tradition it denotes is and has always been diverse, with a wide range of influences rather than a single founder. The movement was first called "Calvinism" by Lutherans who opposed it, and many within the tradition would prefer to use the word "Reformed". Since the Arminian controversy, the Reformed tradition — as a branch of Protestantism distinguished from Lutheranism — divided into separate groups, Arminians and Calvinists.
Reformed churches may exercise several forms of ecclesiastical polity, but most are presbyterian or congregationalist with some being episcopalian.
While the Reformed theological tradition addresses all of the traditional topics of Christian theology, the word "Calvinism" is sometimes used to refer to particular Calvinist views on soteriology and predestination, which are summarized in part by the Five Points of Calvinism. Some have also argued that Calvinism as a whole stresses the sovereignty or rule of God in all things including salvation.
Early influential Reformed theologians include John Calvin, Ulrich Zwingli, Martin Bucer, Heinrich Bullinger, Peter Martyr Vermigli, Theodore Beza, and John Knox. In the twentieth century Abraham Kuyper, Herman Bavinck, B. B. Warfield, Karl Barth, Martyn Lloyd-Jones, Cornelius Van Til and Gordon Clark were influential, while contemporary Reformed theologians include J. I. Packer, R. C. Sproul, Timothy J. Keller, John Piper, and Michael Horton.
The biggest Reformed association is the World Communion of Reformed Churches with more than 80 million members in 211 member denominations around the world. There are more conservative Reformed federations like the World Reformed Fellowship and the International Conference of Reformed Churches, as well as independent churches.
History.
First-generation Reformed theologians include Huldrych Zwingli (1484–1531), Martin Bucer (1491–1551), Wolfgang Capito (1478–1541), John Oecolampadius (1482–1531), and Guillaume Farel (1489–1565). These reformers came from diverse academic backgrounds, but later distinctions within Reformed theology can already be detected in their thought, especially the priority of scripture as a source of authority. Scripture was also viewed as a unified whole, which led to a covenantal theology of the sacraments of baptism and the Lord's Supper as visible signs of the covenant of grace. Another Reformed distinctive present in these theologians was their denial of the bodily presence of Christ in the Lord's supper. Each of these theologians also understood salvation to be by grace alone, and affirmed a doctrine of particular election (the teaching that some people are chosen by God for salvation). Martin Luther and his successor Philipp Melanchthon were undoubtedly significant influences on these theologians, and to a larger extent later Reformed theologians. The doctrine of justification by faith alone was a direct inheritance from Luther.
John Calvin (1509–64), Heinrich Bullinger (1504–75), Wolfgang Musculus (1497–1563), Peter Martyr Vermigli (1500–62), and Andreas Hyperius (1511–64) belong to the second generation of Reformed theologians. Calvin's "Institutes of the Christian Religion" (1536–59) was one of the most influential theologies of the era. Toward the middle of the 16th century, the Reformed began to commit their beliefs to confessions of faith, which would shape the future definition of the Reformed faith. The 1549 "Consensus Tigurinus" brought together those who followed Zwingli and Bullinger's memorialist theology of the Lord's supper, which taught that the supper simply serves as a reminder of Christ's death, and Calvin's view that the supper serves as a means of grace with Christ actually present, though spiritually rather than bodily. The document demonstrates the diversity as well as unity in early Reformed theology. The remainder of the 16th century saw an explosion of confessional activity. The stability and breadth of Reformed theology during this period stand in marked contrast to the bitter controversy experienced by Lutherans prior to the 1579 Formula of Concord.
Due to Calvin's missionary work in France, his programme of reform eventually reached the French-speaking provinces of the Netherlands. Calvinism was adopted in the Electorate of the Palatinate under Frederick III, which led to the formulation of the Heidelberg Catechism in 1563. This and the Belgic Confession were adopted as confessional standards in the first synod of the Dutch Reformed Church in 1571. Leading divines, either Calvinist or those sympathetic to Calvinism, settled in England (Martin Bucer, Peter Martyr, and Jan Łaski) and Scotland (John Knox). During the English Civil War, the Calvinistic Puritans produced the Westminster Confession, which became the confessional standard for Presbyterians in the English-speaking world. Having established itself in Europe, the movement continued to spread to other parts of the world including North America, South Africa, and Korea.
Calvin did not live to see the foundation of his work grow into an international movement; but his death allowed his ideas to break out of their city of origin, to succeed far beyond their borders, and to establish their own distinct character.
Spread.
Although much of Calvin's work was in Geneva, his publications spread his ideas of a "correctly" Reformed church to many parts of Europe. In Switzerland, some cantons are still Reformed and some are Catholic. Calvinism became the theological system of the majority in Scotland (see John Knox), the Netherlands (see William Ames, T. J. Frelinghuysen and Wilhelmus à Brakel) and parts of Germany (especially these adjacent to the Netherlands) in the Palatinate, Kassel and Lippe with the likes of Olevianus and his colleague Zacharias Ursinus. In Hungary and the then-independent Transylvania, Calvinism was a significant religion. In the 16th century, the Reformation gained many supporters in Eastern Hungary and Hungarian-populated regions in Transylvania. In these parts, the Reformed nobles protected the faith. Almost all Transylvanian dukes were Reformed. Today there are about 3.5 million Hungarian Reformed people worldwide. It was influential in France, Lithuania and Poland before being mostly erased due to the counter-reformational activities taken up by the monarch in each country. Calvinism gained some popularity in Scandinavia, especially Sweden, but was rejected in favor of Lutheranism after the Synod of Uppsala in 1593.
Most settlers in the American Mid-Atlantic and New England were Calvinists, including the English Puritans, the French Huguenots and Dutch settlers of New Amsterdam (New York), and the Scotch-Irish Presbyterians of the Appalachian back country. Nonconforming Protestants, Puritans, Separatists, Independents, English religious groups coming out of the English Civil War, and other English dissenters not satisfied with the degree to which the Church of England had been reformed, held overwhelmingly Reformed views. They are often cited among the primary founders of the United States of America. Dutch Calvinist settlers were also the first successful European colonizers of South Africa, beginning in the 17th century, who became known as Boers or Afrikaners.
Sierra Leone was largely colonized by Calvinist settlers from Nova Scotia, who were largely Black Loyalists, blacks who had fought for the British during the American War of Independence. John Marrant had organized a congregation there under the auspices of the Huntingdon Connection. Some of the largest Calvinist communions were started by 19th and 20th century missionaries. Especially large are those in Indonesia, Korea and Nigeria. In South Korea there are 20,000 Presbyterian congregations with about 9–10 million church members, scattered in more than 100 Presbyterian denominations. In South Korea, Presbyterianism is the largest Christian denomination.
A 2011 report of the Pew Forum on Religious and Public Life estimated that members of Presbyterian or Reformed churches make up 7% of the estimated 801 million Protestants globally, or approximately 56 million people. Though the broadly defined Reformed faith is much larger, as it constitutes Congregationalist (0.5%), most of the United and uniting churches (unions of different denominations) (7.2%) and most likely some of the other Protestant denominations (38.2%). All three are distinct categories from Presbyterian or Reformed (7%) in this report.
The Reformed family of churches is one of the largest Christian denominations. According to adherents.com the Reformed/Presbyterian/Congregational/United churches represent 75 million believers worldvide.
The World Communion of Reformed Churches, which includes some United Churches (most of these are primarily Reformed), has 80 million believers. WCRC is the third largest Christian communion in the world, after the Roman Catholic Church and the Eastern Orthodox Churches.
Many conservative Reformed churches which are strongly Calvinistic formed the World Reformed Fellowship which has about 70 member denominations. Most are not part of the World Communion of Reformed Churches because of its ecumenial attire. The International Conference of Reformed Churches is another conservative association.
Reformed identity.
Reformed Protestants disagree among themselves about the definition and boundaries of the Reformed tradition, and often find it difficult to define what makes them Reformed. Reformed churches do not have a single global leader or set of confessional documents to summarize their beliefs. Instead, different Reformed churches have written different confessional documents. Reformed theologians have proposed several interpretations of Reformed identity. Some define the tradition by a certain church polity, some by a set of essential beliefs, some by doctrinal themes and emphases in the tradition, some by a certain "habitus" or character, and some by a certain theological grammar. Liberal Reformed theologians such as Shirley Guthrie have argued that churches which have revised and rejected long-held theological beliefs are more faithful to the Reformed tradition than more conservative theologians because Reformed confessions are provisional and always open to revision. Historical theologian Richard Muller, however, has argued that the Reformed tradition has historically maintained certain distinctive beliefs and practices despite confessional and theological diversity.
Theology.
Revelation and Scripture.
Reformed theologians believe that God communicates knowledge of himself to people through the Word of God. People are not able to know anything about God except through this self-revelation. Speculation about anything which God has not revealed through his Word is not warranted. The knowledge people have of God is different from that which they have of anything else because God is infinite, and finite people are incapable of comprehending an infinite being. While the knowledge revealed by God to people is never incorrect, it is also never comprehensive.
According to Reformed theologians, God's self-revelation is always through his son Jesus Christ, because Christ is the only mediator between God and people. Revelation of God through Christ comes through two basic channels. The first is creation and providence, which is God's creating and continuing to work in the world. This action of God gives everyone knowledge about God, but this knowledge is only sufficient to make people culpable for their sin; it does not include knowledge of the gospel. The second channel through which God reveals himself is redemption, which is the gospel of salvation from condemnation which is punishment for sin.
In Reformed theology, the Word of God takes several forms. Jesus Christ himself is the Word Incarnate. The prophesies about him said to be found in the Old Testament and the ministry of the apostles who saw him and communicated his message are also the Word of God. Further, the preaching of ministers about God is the very Word of God because God is considered to be speaking through them. God also speaks through human writers in the Bible, which is composed of texts set apart by God for self-revelation. Reformed theologians emphasize the Bible as a uniquely important means by which God communicates with people. People gain knowledge of God from the Bible which cannot be gained in any other way.
Reformed theologians affirm that the Bible is true, but differences emerge among them over the meaning and extent of its truthfulness. Conservative followers of the Princeton theologians take the view that the Bible is true and inerrant, or incapable of error or falsehood, in every place. This view is very similar to that of Catholic orthodoxy as well as modern Evangelicalism. Another view, influenced by the teaching of Karl Barth and Neo-Orthodoxy, is found in the Presbyterian Church (U.S.A.)'s Confession of 1967. Those who take this view believe the Bible to be the primary source of our knowledge of God, but also that some parts of the Bible may be false, not witnesses to Christ, and not normative for today's church. In this view, Christ is the revelation of God, and the scriptures witness to this revelation rather than be the revelation itself. Dawn DeVries, a professor at Union Presbyterian Seminary, has written that Barth's doctrine of Scripture is not capable of resolving conflicts in contemporary churches, and proposed that Scripture not be thought of as the Word of God at all, but only human reports of the revealed Jesus Christ.
Covenant.
Reformed theologians use the concept of covenant to describe the way God enters fellowship with people in history. The concept of covenant is so prominent in Reformed theology that Reformed theology as a whole is sometimes called "covenant theology". However, sixteenth and seventeenth-century theologians developed a particular theological system called "covenant theology" or "federal theology" which many conservative Reformed churches continue to affirm today. This framework orders God's life with people primarily in two covenants: the covenant of works and the covenant of grace. The covenant of works is made with Adam and Eve in the Garden of Eden. The terms of the covenant are that God provides a blessed life in the garden on condition that Adam and Eve obey God's law perfectly. Because Adam and Eve broke the covenant by eating the forbidden fruit, they became subject to death and were banished from the garden. This sin was passed down to all mankind because all people are said to be in Adam as a covenantal or "federal" head. Federal theologians usually infer that Adam and Eve would have gained immortality had they obeyed perfectly.
A second covenant, called the covenant of grace, is said to have been made immediately following Adam and Eve's sin. In it, God graciously offers salvation from death on condition of faith in God. This covenant is administered in different ways throughout the Old and New Testaments, but retains the substance of being free of a requirement of perfect obedience.
Through the influence of Karl Barth, many contemporary Reformed theologians have discarded the covenant of works, along with other concepts of federal theology. Barth saw the covenant of works as disconnected from Christ and the gospel, and rejected the idea that God works with people in this way. Instead, Barth argued that God always interacts with people under the covenant of grace, and that the covenant of grace is free of all conditions whatsoever. Barth's theology and that which follows him has been called "monocovenantal" as opposed to the "bi-covenantal" scheme of classical federal theology. Conservative contemporary Reformed theologians, such as John Murray, have also rejected the idea of covenants based on law rather than grace. Michael Horton, however, has defended the covenant of works as combining principles of law and love.
God.
For the most part, the Reformed tradition did not modify the medieval consensus on the doctrine of God. God's character is described primarily using three adjectives: eternal, infinite, and unchangeable. Reformed theologians such as Shirley Guthrie have proposed that rather than conceiving of God in terms of his attributes and freedom to do as he pleases, the doctrine of God is to be based on God's work in history and his freedom to live with and empower people.
Traditionally, Reformed theologians have also followed the medieval tradition going back to the early church councils of Nicaea and Chalcedon on the doctrine of the Trinity. God is affirmed to be one God in three persons: Father, Son, and Holy Spirit. The Son (Christ) is held to be eternally begotten by the Father and the Holy Spirit eternally proceeding from the Father and Son. However, contemporary theologians have been critical of aspects of Western views here as well. Drawing on the Eastern tradition, these Reformed theologians have proposed a "Social Trinity" where the persons of the Trinity only exist in their life together as persons-in-relationship. Contemporary Reformed confessions such as the Barmen Confession and Brief Statement of Faith of the Presbyterian Church (USA) have avoided language about the attributes of God and have emphasized his work of reconciliation and empowerment of people. Feminist theologian Letty Russell used the image of partnership for the persons of the Trinity. According to Russell, thinking this way encourages Christians to interact in terms of fellowship rather than reciprocity. Conservative Reformed theologian Michael Horton, however, has argued that social trinitarianism is untenable because it abandons the essential unity of God in favor of a community of separate beings in community.
Christ and atonement.
Reformed theologians affirm the historic Christian belief that Christ is eternally one person with a divine and a human nature. Reformed Christians have especially emphasized that Christ truly became human so that people could be saved. Christ's human nature has been a point of contention between Reformed and Lutheran Christology. In accord with the belief that finite humans cannot comprehend infinite divinity, Reformed theologians hold that Christ's human body cannot be in multiple locations at the same time. Because Lutherans believe that Christ is bodily present in the Eucharist, they hold that Christ is bodily present in many locations simultaneously. For Reformed Christians, such a belief denies that Christ actually became human. Some contemporary Reformed theologians have moved away from the traditional language of one person in two natures, viewing it as unintelligible to contemporary people. Instead, theologians tend to emphasize Jesus' context and particularity as a first-century Jew.
John Calvin and many Reformed theologians who followed him describe Christ's work of redemption in terms of three offices: prophet, priest, and king. Christ is said to be a prophet in that he teaches perfect doctrine, a priest in that he intercedes to the Father on believers' behalf and offered himself as a sacrifice for sin, and a king in that he rules the church and fights on believers' behalf. The threefold office links the work of Christ to God's work in ancient Israel. Many, but not all, Reformed theologians continue to make use of the threefold office as a framework because of its emphasis on the connection of Christ's work to Israel. They have, however, often reinterpreted the meaning of each of the offices. For example, Karl Barth interpreted Christ's prophetic office in terms of political engagement on behalf of the poor.
Christians believe Jesus' death and resurrection makes it possible for believers to attain forgiveness for sin and reconciliation with God through the atonement. Reformed Protestants generally subscribe to a particular view of the atonement called substitutionary atonement, which explains Christs death as a sacrificial payment for sin. Christ is believed to have died in place of the believer, who is accounted righteous as a result of this sacrificial payment. Contemporary Reformed theologians such as William Placher and Nancy Duff have criticized this view, claiming it makes God appear abusive or vindictive and sanctions violence by the strong against the weak.
Sin.
In Christian theology, people are created good and in the image of God but have become corrupted by sin, which causes them to be imperfect and self-interested. Reformed Christians, following the tradition of Augustine of Hippo, believe that this corruption of human nature was brought on by Adam and Eve's first sin, a doctrine called original sin. Reformed theologians emphasize that this sinfulness affects all of a person's nature, including their will. This view, that sin so dominates people that they are unable to avoid sin, has been called total depravity. In colloquial English, the term "total depravity" can be easily misunderstood to mean that people are absent of any goodness or unable to do any good. However the Reformed teaching is actually that while people continue to bear God's image and may do things that are outwardly good, their sinful intentions affect all of their nature and actions so that they are not wholly pleasing to God.
Some contemporary theologians in the Reformed tradition, such as those associated with the PC(USA)'s Confession of 1967, have emphasized the social character of human sinfulness. These theologians have sought to bring attention to issues of environmental, economic, and political justice as areas of human life that have been affected by sin.
Salvation.
Reformed theologians, along with other Protestants, believe salvation from punishment for sin to be given to all those who have faith in Christ. Faith is not purely intellectual, but involves trust in God's promise to save. Protestants do not hold there to be any other requirement for salvation, but that faith alone is sufficient.
Justification is the part of salvation where God pardons the sin of those who believe in Christ. It is historically held by Protestants to be the most important article of Christian faith, though more recently it is sometimes given less importance out of ecumenical concerns. People are not on their own able even to fully repent of their sin or prepare themselves to repent because of their sinfulness. Therefore, justification is held to arise solely from God's free and gracious act.
Sanctification is the part of salvation in which God makes the believer holy, by enabling them to exercise greater love for God and for other people. The good works accomplished by believers as they are sanctified are considered to be the necessary outworking of the believer's salvation, though they do not cause the believer to be saved. Sanctification, like justification, is by faith, because doing good works is simply living as the son of God one has become.
Predestination.
Reformed theologians teach that sin so affects human nature that they are unable even to exercise faith in Christ by their own will. While people are said to retain will, in that they willfully sin, they are unable to not sin because of the corruption of their nature due to original sin. To remedy this, Reformed Christians believe that God predestined some people to be saved. This choice by God to save some is held to be unconditional and not based on any characteristic or action on the part of the person chosen. This view is opposed to the Arminian view that God's choice of whom to save is conditional or based on his foreknowledge of who would respond positively to God.
Karl Barth reinterpreted the Reformed doctrine of predestination to apply only to Christ. Individual people are only said to be elected through their being in Christ. Reformed theologians who followed Barth, including Jürgen Moltmann, David Migliore, and Shirley Guthrie, have argued that the traditional Reformed concept of predestination is speculative and have proposed alternative models. These theologians claim that a properly trinitarian doctrine emphasizes God's freedom to love all people, rather than choosing some for salvation and others for damnation. God's justice towards and condemnation of sinful people is spoken of by these theologians as out of his love for them and a desire to reconcile them to himself.
Five points of Calvinism.
Most objections to and attacks on Calvinism focus on the "five points of Calvinism," also called the doctrines of grace, and remembered by the mnemonic "TULIP." The five points are popularly said to summarize the Canons of Dort; however, there is no historical relationship between them, and some scholars argue that their language distorts the meaning of the Canons, Calvin's theology, and the theology of 17th-century Calvinistic orthodoxy, particularly in the language of total depravity and limited atonement. The five points were popularized in the 1963 booklet "The Five Points of Calvinism Defined, Defended, Documented" by David N. Steele and Curtis C. Thomas. The origins of the five points and the acronym are uncertain, but the acronym was used by Cleland Boyd McAfee as early as circa 1905. An early printed appearance of the T-U-L-I-P acronym is in Loraine Boettner's 1932 book, "The Reformed Doctrine of Predestination". The acronym was very cautiously if ever used by Calvinist apologists and theologians before the booklet by Steele and Thomas. More recently, theologians have sought to reformulate the TULIP acronym to more accurately reflect the Canons of Dort.
The central assertion of these points is that God saves every person upon whom he has mercy, and that his efforts are not frustrated by the unrighteousness or inability of humans.
Church.
Reformed Christians see the Christian Church as the community with which God has made the covenant of grace, a promise of eternal life and relationship with God. This covenant extends to those under the "old covenant" whom God chose, beginning with Abraham and Sarah. The church is conceived of as both invisible and visible. The invisible church is the body of all believers, known only to God. The visible church is the institutional body which contains both members of the invisible church as well as those who appear to have faith in Christ, but are not truly part of God's elect.
In order to identify the visible church, Reformed theologians have spoken of certain marks of the Church. For some, the only mark is the pure preaching of the gospel of Christ. Others, including John Calvin, also including the right administration of the sacraments. Others, such as those following the Scots Confession, include a third mark of rightly administered church discipline, or exercise of censure against unrepentant sinners. These marks allowed the Reformed to identify the church based on its conformity to the Bible rather than the Magisterium or church tradition.
Worship.
Regulative principle of worship.
The regulative principle of worship is a teaching shared by some Calvinists and Anabaptists on how the Bible orders public worship. The substance of the doctrine regarding worship is that God institutes in the Scriptures everything he requires for worship in the Church and that everything else is prohibited. As the regulative principle is reflected in Calvin's own thought, it is driven by his evident antipathy toward the Roman Catholic Church and its worship practices, and it associates musical instruments with icons, which he considered violations of the Ten Commandments' prohibition of graven images.
On this basis, many early Calvinists also eschewed musical instruments and advocated a capella exclusive psalmody in worship, though Calvin himself allowed other scriptural songs as well as psalms, and this practice typified presbyterian worship and the worship of other Reformed churches for some time. The original Lord's Day service designed by John Calvin was a highly liturgical service with the Creed, Alms, Confession and Absolution, the Lord's supper, Doxologies, prayers, Psalms being sung, the Lords prayer being sung, Benedictions.
Since the 19th century, however, some of the Reformed churches have modified their understanding of the regulative principle and make use of musical instruments, believing that Calvin and his early followers went beyond the biblical requirements and that such things are circumstances of worship requiring biblically-rooted wisdom, rather than an explicit command. Despite the protestations of those who hold to a strict view of the regulative principle, today hymns and musical instruments are in common use, as are contemporary worship music styles with elements such as worship bands.
Sacraments.
The Westminster Confession of Faith limits the sacraments to baptism and the Lord's Supper. Sacraments are denoted "signs and seals of the covenant of grace." Westminster speaks of "a sacramental relation, or a sacramental union, between the sign and the thing signified; whence it comes to pass that the names and effects of the one are attributed to the other." Baptism is for infant children of believers as well as believers, as it is for all the Reformed except Baptists and some Congregationalists. Baptism admits the baptized into the visible church, and in it all the benefits of Christ are offered to the baptized. On the Lord's supper, Westminster takes a position between Lutheran sacramental union and Zwinglian memorialism: "the Lord's supper really and indeed, yet not carnally and corporally, but spiritually, receive and feed upon Christ crucified, and all benefits of his death: the body and blood of Christ being then not corporally or carnally in, with, or under the bread and wine; yet, as really, but spiritually, present to the faith of believers in that ordinance as the elements themselves are to their outward senses."
The 1689 London Baptist Confession of Faith does not use the term sacrament, but describes baptism and the Lord's supper as ordinances, as do most Baptists Calvinist or otherwise. Baptism is only for those who "actually profess repentance towards God," and not for the children of believers. Baptists also insist on immersion or dipping, in contradistinction to other Reformed Christians. The Baptist Confession, describes the Lord's supper as "the body and blood of Christ being then not corporally or carnally, but spiritually present to the faith of believers in that ordinance," similarly to the Westminster Confession. There is significant latitude in Baptist congregations regarding the Lord's supper, and many hold the Zwinglian view.
Logical order of God's decree.
There are two schools of thought regarding the logical order of God's decree to ordain the fall of man: supralapsarianism (from the Latin: "supra", "above", here meaning "before" + "lapsus", "fall") and infralapsarianism (from the Latin: "infra", "beneath", here meaning "after" + "lapsus", "fall"). The former view, sometimes called "high Calvinism", argues that the Fall occurred partly to facilitate God's purpose to choose some individuals for salvation and some for damnation. Infralapsarianism, sometimes called "low Calvinism", is the position that, while the Fall was indeed planned, it was not planned with reference to who would be saved.
Supralapsarians believe that God chose which individuals to save logically prior to the decision to allow the race to fall and that the Fall serves as the means of realization of that prior decision to send some individuals to hell and others to heaven (that is, it provides the grounds of condemnation in the reprobate and the need for salvation in the elect). In contrast, infralapsarians hold that God planned the race to fall logically prior to the decision to save or damn any individuals because, it is argued, in order to be "saved", one must first need to be saved from something and therefore the decree of the Fall must precede predestination to salvation or damnation.
These two views vied with each other at the Synod of Dort, an international body representing Calvinist Christian churches from around Europe, and the judgments that came out of that council sided with infralapsarianism (Canons of Dort, First Point of Doctrine, Article 7). The Westminster Confession of Faith also teaches (in Hodge's words "clearly impl") the infralapsarian view, but is sensitive to those holding to supralapsarianism. The Lapsarian controversy has a few vocal proponents on each side today, but overall it does not receive much attention among modern Calvinists.
Variants.
Amyraldism.
Amyraldism (or sometimes Amyraldianism, also known as the School of Saumur, hypothetical universalism, post redemptionism, moderate Calvinism, or four-point Calvinism) is the belief that God, prior to his decree of election, decreed Christ's atonement for all alike if they believe, but seeing that none would believe on their own, he then elected those whom he will bring to faith in Christ, thereby preserving the Calvinist doctrine of unconditional election. The efficacy of the atonement remains limited to those who believe.
Named after its formulator Moses Amyraut, this doctrine is still viewed as a variety of Calvinism in that it maintains the particularity of sovereign grace in the application of the atonement. However, detractors like B. B. Warfield have termed it "an inconsistent and therefore unstable form of Calvinism."
Hyper-Calvinism.
Hyper-Calvinism first referred to a view that appeared among the early English Particular Baptists in the 18th century. Their system denied that the call of the gospel to "repent and believe" is directed to every single person and that it is the duty of every person to trust in Christ for salvation. The term also occasionally appears in both theological and secular controversial contexts, where it usually connotes a negative opinion about some variety of theological determinism, predestination, or a version of Evangelical Christianity or Calvinism that is deemed by the critic to be unenlightened, harsh, or extreme.
The Westminster Confession of Faith says that the gospel is to be freely offered to sinners, and the Larger Catechism makes clear that the gospel is offered to the non-elect.
Neo-Calvinism.
Neo-Calvinism, a form of Dutch Calvinism, is the movement initiated by the theologian and former Dutch prime minister Abraham Kuyper. James Bratt has identified a number of different types of Dutch Calvinism: The Seceders—split into the Reformed Church "West" and the Confessionalists; and the Neo-Calvinists—the Positives and the Antithetical Calvinists. The Seceders were largely infralapsarian and the Neo-Calvinists usually supralapsarian.
Kuyper wanted to awaken the church from what he viewed as its pietistic slumber. He declared:
No single piece of our mental world is to be sealed off from the rest and there is not a square inch in the whole domain of human existence over which Christ, who is sovereign over all, does not cry: 'Mine!' 
This refrain has become something of a rallying call for Neo-Calvinists.
Christian Reconstructionism.
Christian Reconstructionism is a fundamentalist Calvinist theonomic movement that has remained rather obscure. Founded by R. J. Rushdoony, the movement has had an important influence on the Christian Right in the United States. The movement declined in the 1990s and was declared dead in a 2008 "Church History" journal article. Christian Reconstructionists are usually postmillennialists and followers of the presuppositional apologetics of Cornelius Van Til. They tend to support a decentralized political order resulting in laissez-faire capitalism.
New Calvinism.
The New Calvinism is a growing perspective within conservative Evangelicalism that embraces the fundamentals of 16th century Calvinism while also trying to be relevant in the present day world. In March 2009, "Time" magazine described the New Calvinism as one of the "10 ideas changing the world". Some of the major figures in this area are John Piper, Mark Driscoll, Al Mohler, Mark Dever, C. J. Mahaney, Joshua Harris, and Tim Keller. New Calvinists have been criticized for blending Calvinist soteriology with popular Evangelical positions on the sacraments and continuationism.
Social and economic influences.
Usury and capitalism.
One school of thought attributes Calvinism with setting the stage for the later development of capitalism in northern Europe. In this view, elements of Calvinism represented a revolt against the medieval condemnation of usury and, implicitly, of profit in general. Such a connection was advanced in influential works by R. H. Tawney (1880–1962) and by Max Weber (1864–1920).
Calvin expressed himself on usury in a 1545 letter to a friend, Claude de Sachin, in which he criticized the use of certain passages of scripture invoked by people opposed to the charging of interest. He reinterpreted some of these passages, and suggested that others of them had been rendered irrelevant by changed conditions. He also dismissed the argument (based upon the writings of Aristotle) that it is wrong to charge interest for money because money itself is barren. He said that the walls and the roof of a house are barren, too, but it is permissible to charge someone for allowing him to use them. In the same way, money can be made fruitful.
He qualified his view, however, by saying that money should be lent to people in dire need without hope of interest, while a modest interest rate of 5% should be permitted in relation to other borrowers.
Politics and society.
Calvin's concept of God and man contained strong elements of freedom that were gradually put into practice after his death, in particular in the fields of politics and society. After the successful fight for independence from Spain (1579), the Netherlands, under Calvinist leadership, became the freest country in Europe. It granted asylum to persecuted religious minorities, e.g. French Huguenots, English Independents (Congregationalists), and Jews from Spain and Portugal. The ancestors of philosopher Baruch Spinoza were Portuguese Jews. Aware of the trial against Galileo, René Descartes lived in the Netherlands, out of reach of the Inquisition. Pierre Bayle, a Reformed Frenchman, also felt safer in the Netherlands than in his home country. He was the first prominent philosopher who demanded tolerance for atheists. Hugo Grotius was able to publish a rather liberal interpretation of the Bible and his ideas about natural law. Moreover, the Calvinist Dutch authorities allowed the printing of books that could not be published elsewhere, e.g. Galileo's "Discorsi".
Even more important than the liberal development of the Netherlands was the rise of modern democracy in England and North America. In the Middle Ages state and church had been closely connected. Martin Luther's doctrine of the two kingdoms separated state and church in principle. His doctrine of the priesthood of all believers raised the laity to the same level as the clergy. Going one step further, Calvin included elected laymen (church elders, presbyters) in his concept of church government. The Huguenots added synods whose members were also elected by the congregations. The other Reformed churches took over this system of church self-government which was essentially a representative democracy. Baptists, Quakers, and Methodists are organized in a similar way. These denominations and the Anglican Church were influenced by Calvin's theology in varying degrees.
Another precondition for the rise of democracy in the Anglo-American world was the fact that Calvin favored a mixture of democracy and aristocracy as the best form of government (mixed government). He appreciated the advantages of democracy. The aim of his political thought was to safeguard the rights and freedoms of ordinary men and women. In order to minimize the misuse of political power he suggested dividing it among several institutions in a system of checks and balances (separation of powers). Finally, Calvin taught that if worldly rulers rise up against God they should be put down. In this way, he and his followers stood in the vanguard of resistance to political absolutism and furthered the cause of democracy. The Congregationalists who founded Plymouth Colony (1620) and Massachusetts Bay Colony (1628) were convinced that the democratic form of government was the will of God. Enjoying self-rule they practiced separation of powers. Rhode Island, Connecticut, and Pennsylvania, founded by Roger Williams, Thomas Hooker, and William Penn, respectively, combined democratic government with freedom of religion. These colonies became safe havens for persecuted religious minorities, including Jews.
In England, Baptists Thomas Helwys and John Smyth influenced the liberal political thought of Presbyterian poet and politician John Milton and philosopher John Locke, who in turn had both a strong impact on the political development in their home country (English Civil War, Glorious Revolution) as well as in North America. The ideological basis of the American Revolution was largely provided by the radical Whigs, who had been inspired by Milton, Locke, James Harrington, Algernon Sidney, and other thinkers. The Whigs' "perceptions of politics attracted widespread support in America because they revived the traditional concerns of a Protestantism that had always verged on Puritanism." The United States Declaration of Independence, the United States Constitution and (American) Bill of Rights initiated a tradition of human and civil rights that was continued in the French Declaration of the Rights of Man and the Citizen and the constitutions of numerous countries around the world, e. g. Latin America, Japan, Germany, and other European countries. It is also echoed in the United Nations Charter and the Universal Declaration of Human Rights.
In the nineteenth century, the churches that were based on Calvin's theology or influenced by it were deeply involved in social reforms, e.g. the abolition of slavery (William Wilberforce, Harriet Beecher Stowe, Abraham Lincoln, and others), women suffrage, and prison reforms. Members of these churches formed co-operatives to help the impoverished masses. Henry Dunant, a Reformed pietist, founded the Red Cross and initiated the Geneva Conventions.
Some sources would view Calvinist influence as not always being solely positive. The Boers and so-called Afrikaner Calvinists allegedly used a twisted form of Calvinism and Kuyperian theology to justify apartheid in South Africa (see Afrikaner Calvinism). As late as 1974, the majority of the Dutch Reformed Church in South Africa was convinced that their theological stances (including the story of the Tower of Babel) could justify apartheid. In 1990, the Dutch Reformed Church document "Church and Society" maintained that although they were changing their stance on apartheid, they believed that within apartheid and under God's sovereign guidance, "...everything was not without significance, but was of service to the Kingdom of God." It should be noted that these views were not universal and were condemned by many Calvinists outside South Africa. It was pressure from both outside and inside the Dutch Reformed Calvinist church which helped reverse apartheid in South Africa.
Throughout the world, the Reformed churches operate hospitals, homes for handicapped or elderly people, and educational institutions on all levels. For example, American Congregationalists founded Harvard (1636), Yale (1701), and about a dozen other colleges. Princeton was a Presbyterian foundation.

</doc>
<doc id="6026" url="https://en.wikipedia.org/wiki?curid=6026" title="Countable set">
Countable set

In mathematics, a countable set is a set with the same cardinality (number of elements) as some subset of the set of natural numbers. A countable set is either a finite set or a "countably infinite" set. Whether finite or infinite, the elements of a countable set can always be counted one at a time and, although the counting may never finish, every element of the set is associated with a natural number.
Some authors use countable set to mean "countably infinite" alone. To avoid this ambiguity, the term "at most countable" may be used when finite sets are included and "countably infinite", "enumerable", or "denumerable" otherwise.
Georg Cantor introduced the term "countable set", contrasting sets that are countable with those that are "uncountable" (i.e., "nonenumerable" or "nondenumerable"). Today, countable sets form the foundation of a branch of mathematics called "discrete mathematics".
Definition.
A set is "countable" if there exists an injective function from to the natural numbers }.
If such an can be found that is also surjective (and therefore bijective), then is called "countably infinite."
In other words, a set is "countably infinite" if it has one-to-one correspondence with the natural number set, .
As noted above, this terminology is not universal.: Some authors use countable to mean what is here called "countably infinite," and do not include finite sets.
Alternative (equivalent) formulations of the definition in terms of a bijective function or a surjective function can also be given. See below.
History.
In 1874, in his first set theory article, Cantor proved that the set of real numbers is uncountable, thus showing that not all infinite sets are countable. In 1878, he used one-to-one correspondences to define and compare cardinalities. In 1883, he extended the natural numbers with his infinite ordinals, and used sets of ordinals to produce an infinity of sets having different infinite cardinalities.
Introduction.
A "set" is a collection of "elements", and may be described in many ways. One way is simply to list all of its elements; for example, the set consisting of the integers 3, 4, and 5 may be denoted {3, 4, 5}. This is only effective for small sets, however; for larger sets, this would be time-consuming and error-prone. Instead of listing every single element, sometimes an ellipsis ("...") is used, if the writer believes that the reader can easily guess what is missing; for example, {1, 2, 3, ..., 100} presumably denotes the set of integers from 1 to 100. Even in this case, however, it is still "possible" to list all the elements, because the set is "finite".
Some sets are "infinite"; these sets have more than "n" elements for any integer "n". For example, the set of natural numbers, denotable by {0, 1, 2, 3, 4, 5, ...}, has infinitely many elements, and we cannot use any normal number to give its size. Nonetheless, it turns out that infinite sets do have a well-defined notion of size (or more properly, of "cardinality", which is the technical term for the number of elements in a set), and not all infinite sets have the same cardinality.
To understand what this means, we first examine what it "does not" mean. For example, there are infinitely many odd integers, infinitely many even integers, and (hence) infinitely many integers overall. However, it turns out that the number of even integers, which is the same as the number of odd integers, is also the same as the number of integers overall. This is because we arrange things such that for every integer, there is a distinct even integer: ... −2→−4, −1→−2, 0→0, 1→2, 2→4, ...; or, more generally, "n"→2"n", see picture. What we have done here is arranged the integers and the even integers into a "one-to-one correspondence" (or "bijection"), which is a function that maps between two sets such that each element of each set corresponds to a single element in the other set.
However, not all infinite sets have the same cardinality. For example, Georg Cantor (who introduced this concept) demonstrated that the real numbers cannot be put into one-to-one correspondence with the natural numbers (non-negative integers), and therefore that the set of real numbers has a greater cardinality than the set of natural numbers.
A set is "countable" if: (1) it is finite, or (2) it has the same cardinality (size) as the set of natural numbers. Equivalently, a set is "countable" if it has the same cardinality as some subset of the set of natural numbers. Otherwise, it is "uncountable".
Formal overview without details.
By definition a set "S" is "countable" if there exists an injective function "f" : "S" → N from "S" to the natural numbers N = {0, 1, 2, 3, ...}.
It might seem natural to divide the sets into different classes: put all the sets containing one element together; all the sets containing two elements together; ...; finally, put together all infinite sets and consider them as having the same size.
This view is not tenable, however, under the natural definition of size.
To elaborate this we need the concept of a bijection. Although a "bijection" seems a more advanced concept than a number, the usual development of mathematics in terms of set theory defines functions before numbers, as they are based on much simpler sets. This is where the concept of a bijection comes in: define the correspondence
Since every element of {"a", "b", "c"} is paired with "precisely one" element of {1, 2, 3}, "and" vice versa, this defines a bijection.
We now generalize this situation and "define" two sets as of the same size if (and only if) there is a bijection between them. For all finite sets this gives us the usual definition of "the same size". What does it tell us about the size of infinite sets?
Consider the sets "A" = {1, 2, 3, ... }, the set of positive integers and "B" = {2, 4, 6, ... }, the set of even positive integers. We claim that, under our definition, these sets have the same size, and that therefore "B" is countably infinite. Recall that to prove this we need to exhibit a bijection between them. But this is easy, using "n" ↔ 2"n", so that
As in the earlier example, every element of A has been paired off with precisely one element of B, and vice versa. Hence they have the same size. This is an example of a set of the same size as one of its proper subsets, which is impossible for finite sets.
Likewise, the set of all ordered pairs of natural numbers is countably infinite, as can be seen by following a path like the one in the picture: The resulting mapping is like this:
This mapping covers all such ordered pairs.
Interestingly: if you treat each pair as being the numerator and denominator of a vulgar fraction, then for every positive fraction, we can come up with a distinct number corresponding to it. This representation includes also the natural numbers, since every natural number is also a fraction "N"/1. So we can conclude that there are exactly as many positive rational numbers as there are positive integers. This is true also for all rational numbers, as can be seen below.
Theorem: The Cartesian product of finitely many countable sets is countable.
This form of triangular mapping recursively generalizes to vectors of finitely many natural numbers by repeatedly mapping the first two elements to a natural number. For example, (0,2,3) maps to (5,3), which maps to 39.
Sometimes more than one mapping is useful. This is where you map the set you want to show is countably infinite onto another set—and then map this other set to the natural numbers. For example, the positive rational numbers can easily be mapped to (a subset of) the pairs of natural numbers because "p"/"q "maps to ("p", "q").
What about infinite subsets of countably infinite sets? Do these have fewer elements than N?
Theorem: Every subset of a countable set is countable. In particular, every infinite subset of a countably infinite set is countably infinite.
For example, the set of prime numbers is countable, by mapping the "n"-th prime number to "n":
What about sets being naturally "larger than" N? For instance, Z the set of all integers or Q, the set of all rational numbers, which intuitively may seem much bigger than N. But looks can be deceiving, for we assert:
Theorem: Z (the set of all integers) and Q (the set of all rational numbers) are countable.
In a similar manner, the set of algebraic numbers is countable.
These facts follow easily from a result that many individuals find non-intuitive.
Theorem: Any finite union of countable sets is countable. 
With the foresight of knowing that there are uncountable sets, we can wonder whether or not this last result can be pushed any further. The answer is "yes" and "no", we can extend it, but we need to assume a new axiom to do so.
Theorem: (Assuming the axiom of countable choice) The union of countably many countable sets is countable.
For example, given countable sets a, b, c, ...
Using a variant of the triangular enumeration we saw above:
Note that this only works if the sets a, b, c, ... are disjoint. If not, then the union is even smaller and is therefore also countable by a previous theorem.
Also note that we need the axiom of countable choice to index "all" the sets a, b, c, ... simultaneously.
Theorem: The set of all finite-length sequences of natural numbers is countable.
This set is the union of the length-1 sequences, the length-2 sequences, the length-3 sequences, each of which is a countable set (finite Cartesian product). So we are talking about a countable union of countable sets, which is countable by the previous theorem.
Theorem: The set of all finite subsets of the natural numbers is countable.
If you have a finite subset, you can order the elements into a finite sequence. There are only countably many finite sequences, so also there are only countably many finite subsets.
The following theorem gives equivalent formulations in terms of a bijective function or a surjective function. A proof of this result can be found in Lang's text.
(Basic) Theorem: Let "S" be a set. The following statements are equivalent:
Corollary: Let "S" and "T" be sets.
Cantor's Theorem asserts that if "A" is a set and "P"("A") is its power set, i.e. the set of all subsets of "A", then there is no surjective function from "A" to "P"("A"). A proof is given in the article Cantor's Theorem. As an immediate consequence of this and the Basic Theorem above we have:
Proposition: The set "P"(N) is not countable; i.e. it is uncountable.
For an elaboration of this result see Cantor's diagonal argument.
The set of real numbers is uncountable (see Cantor's first uncountability proof), and so is the set of all infinite sequences of natural numbers.
For (2) observe that if "S" is countable there is a surjective function "h" : N → "S". Then if "g" : "S" → "T" is surjective the composition "g" o "h" : N → "T" is surjective, so "T" is countable.
Some technical details.
The proofs of the statements in the above section rely upon the existence of functions with certain properties. This section presents functions commonly used in this role, but not the verifications that these functions have the required properties. The Basic Theorem and its Corollary are often used to simplify proofs. Observe that in that theorem can be replaced with any countably infinite set.
Proposition: Any finite set is countable.
Proof: By definition, there is a bijection between a non-empty finite set and the set {1, 2, ..., } for some positive natural number . This function is an injection from into .
Proposition: Any subset of a countable set is countable.
Proof: The restriction of an injective function to a subset of its domain is still injective.
Proposition: If is a countable set and , then } is countable.
Proof: Let be an injection. Define by and 
Proposition: If and are countable sets then is countable.
Proof: Let and be injections. Define a new injection by if is in and if is in but not in .
Proposition: The Cartesian product of two countable sets and is countable.
Proof: Observe that is countable as a consequence of the definition because the function given by is injective. It then follows from the Basic Theorem and the Corollary that the Cartesian product of any two countable sets is countable. This follows because if and are countable there are surjections and . So
is a surjection from the countable set to the set and the Corollary implies is countable. This result generalizes to the Cartesian product of any finite collection of countable sets and the proof follows by induction on the number of sets in the collection.
Proposition: The integers are countable and the rational numbers are countable.
Proof: The integers are countable because the function given by if is non-negative and if is negative, is an injective function. The rational numbers are countable because the function given by is a surjection from the countable set to the rationals .
Proposition: If is a countable set for each in then the union of all is also countable.
Proof: This is a consequence of the fact that for each there is a surjective function and hence the function
given by is a surjection. Since is countable, the Corollary implies that the union is countable. We use the axiom of countable choice in this proof to pick for each in a surjection from the non-empty collection of surjections from to .
A topological proof for the uncountability of the real numbers is described at finite intersection property.
Minimal model of set theory is countable.
If there is a set that is a standard model (see inner model) of ZFC set theory, then there is a minimal standard model ("see" Constructible universe). The Löwenheim-Skolem theorem can be used to show that this minimal model is countable. The fact that the notion of "uncountability" makes sense even in this model, and in particular that this model "M" contains elements that are:
was seen as paradoxical in the early days of set theory, see Skolem's paradox.
The minimal standard model includes all the algebraic numbers and all effectively computable transcendental numbers, as well as many other kinds of numbers.
Total orders.
Countable sets can be totally ordered in various ways, e.g.:
Note that in both examples of well orders here, any subset has a "least element"; and in both examples of non-well orders, "some" subsets do not have a "least element".
This is the key definition that determines whether a total order is also a well order.

</doc>
<doc id="6034" url="https://en.wikipedia.org/wiki?curid=6034" title="Cahn–Ingold–Prelog priority rules">
Cahn–Ingold–Prelog priority rules

The Cahn–Ingold–Prelog (CIP) sequence rules, named for organic chemists R.S. Cahn, C.K. Ingold, and V. Prelog—alternatively termed the CIP priority rules, "system", or "conventions"—are a standard process used in organic chemistry to completely and unequivocally name a stereoisomer of a molecule. The purpose of the CIP system is to assign an to each stereocenter and an E or Z descriptor to each double bond so that the configuration of the entire molecule can be specified uniquely by including the descriptors in its systematic name. A molecule may contain any number of stereocenters and any number of double bonds, and each usually gives rise to two possible isomers. (A molecule with an integer formula_1 describing the number of its stereogenic centers will usually have formula_2 stereoisomers, formula_3 diastereomers each having an associated pair of enantiomers. The CIP sequence rules contribute to the precise naming of every stereoisomer of every organic and organometallic molecule with all atoms of ligancy of fewer than 4 (but including ligancy of 6 as well, this term referring to the "number of neighboring atoms" bonded to a center).
The key article setting out the CIP sequence rules was published in 1966, and was followed by further refinements, before it was incorporated into the rules of the International Union of Pure and Applied Chemistry, the official body that defines organic nomenclature. The IUPAC presentation of the rules constitute the official, formal standard for their use, and it notes that "the method has been developed to cover all compounds with ligancy up to 4... and… to the case of ligancy 6… well as for all configurations and conformations of such compounds." Nevertheless, though the IUPAC documentation presents a thorough introduction, it includes the caution that "it is essential to study the original papers, especially the 1966 paper, before using the sequence rule for other than fairly simple cases."
Steps for naming.
The steps for naming molecules using the CIP system are often presented as:
Assignment of priorities.
Isotopes.
If two groups differ only in isotopes, atomic masses are used at each step to break ties in atomic number.
Double and triple bonds.
If an atom "A" is double-bonded to an atom "B", "A" is treated as being singly bonded to two atoms: "B" and a "ghost atom" that is a duplicate of "B" (has the same atomic number) but is not attached to anything except "A". When "B" is replaced with a list of attached atoms, "A" itself is excluded in accordance with the general principle of not doubling back along a bond that has just been followed. A triple bond is handled the same way except that "A" and "B" both have duplicated 'ghost' atoms..
Geometric Isomers.
If two substituents on an atom are geometric isomers, the Z-isomer has higher priority than the E-isomer.
Cycles.
To handle a molecule containing one or more cycles, one must first expand it into a tree (called a hierarchical digraph by the authors) by traversing bonds in all possible paths starting at the stereocenter. When the traversal encounters an atom through which the current path has already passed, a ghost atom is generated in order to keep the tree finite. A single atom of the original molecule may appear in many places (some as ghosts, some not) in the tree.
Assigning descriptors.
Stereocenters: R/S.
After the substituents of a stereocenter have been assigned their priorities, the molecule is oriented in space so that the group with the lowest priority is pointed away from the observer. If the substituents are numbered from 1 (highest priority) to 4 (lowest priority), then the sense of rotation of a curve passing through 1, 2 and 3 distinguishes the stereoisomers. A center with a clockwise sense of rotation is an "R" or "rectus" center and a center with a counterclockwise sense of rotation is an "S" or "sinister" center. The names are derived from the Latin for right and left, respectively.
A practical method of determining whether an enantiomer is R or S is by using the right-hand rule: one wraps the molecule with the fingers in the direction 1→2→3. If the thumb points in the direction of the 4th substituent, the enantiomer is R. Otherwise, it's S.
It is possible in rare cases that two substituents on an atom differ only in their absolute configuration ("R" or "S"). If the relative priorities of these substituents need to be established, "R" takes priority over "S". When this happens, the descriptor of the stereocenter is a lowercase letter ("r" or "s") instead of the uppercase letter normally used.
Double bonds: E/Z.
For alkenes and similar double bonded molecules, the same prioritizing process is followed for the substituents. In this case, it is the placing of the two highest priority substituents with respect to the double bond which matters. If both high priority substituents are on the same side of the double bond, i.e. in the cis configuration, then the stereoisomer is assigned a "Z" or "Zusammen configuration". If, by contrast they are in a trans configuration, then the stereoisomer is assigned an "E" or "Entgegen configuration". In this case the identifying letters are derived from German for 'together' and 'in opposition to', respectively.
Examples.
The following are examples of application of the nomenclature.
Describing multiple centers.
If a compound has more than one stereocenter each center is denoted by either R or S. For example, ephedrine exists with both (1R,2S) and (1S,2R) configuration, known as enantiomers. This compound also exists with a (1R,2R) and (1S,2S) configuration. The last two stereoisomers are not ephedrine, but pseudoephedrine. All isomers are 2-methylamino-1-phenyl-1-propanol in systematic nomenclature. Pseudoephedrine is chemically distinct from ephedrine with only the three-dimensional configuration in space, as notated by the Cahn–Ingold–Prelog rules. The two compounds, ephedrine and pseudoephedrine, are diastereomers, or stereoisomers that are not enantiomers. They have different names because, as diastereomers, they have different chemical properties.
In pairs of enantiomers, all descriptors are opposite: R,R and S,S or R,S and S,R. Diastereomers have one descriptor in common: R,S and R,R or S,R and S,S. This holds true for compounds with more than two stereocenters; if at least one descriptor is the same in both pairs, the compounds are diastereomers. If "all" the stereocenters are opposite, they are enantiomers.
Relative configuration.
The relative configuration of two stereoisomers may be denoted by the descriptors R and S with an asterisk (*). "R*,R*" means two centers having identical configurations (R,R or S,S); "R*,S*" means two centers having opposite configurations (R,S or S,R). To begin, the lowest numbered (according to IUPAC systematic numbering) stereogenic center is given the R* descriptor.
To designate two anomers the relative stereodescriptors alpha (α) and beta (β) are used. In the α anomer the "anomeric carbon" and the "reference atom" do have opposite configurations (R,S or S,R), whereas in the β anomer they are the same (both R or both S).
Faces.
Stereochemistry also plays a role assigning "faces" to trigonal molecules such as ketones. A nucleophile in a nucleophilic addition can approach the carbonyl group from two opposite sides or faces. When an achiral nucleophile attacks acetone, both faces are identical and there is only one reaction product. When the nucleophile attacks butanone, the faces are not identical ("enantiotopic") and a racemic product results. When the nucleophile is a chiral molecule diastereoisomers are formed. When one face of a molecule is shielded by substituents or geometric constraints compared to the other face the faces are called diastereotopic. The same rules that determine the stereochemistry of a stereocenter (R or S) also apply when assigning the face of a molecular group. The faces are then called the re-faces and si-faces. In the example displayed on the right, the compound acetophenone is viewed from the re face. Hydride addition as in a reduction process from this side will form the S-enantiomer and attack from the opposite Si face will give the R-enantiomer. However, one should note that adding a chemical group to the prochiral center from the re-face will not always lead to an S stereocenter, as the priority of the chemical group has to be taken into account. That is, the absolute stereochemistry of the product is determined on its own and not by considering which face it was attacked from. In the above-mentioned example, if chloride (Cl-) was added to the prochiral center from the re-face, this would result in an R-enantiomer.

</doc>
