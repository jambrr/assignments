<doc id="62760" url="https://en.wikipedia.org/wiki?curid=62760" title="Éowyn">
Éowyn

Éowyn is a fictional character in Tolkien's legendarium who appears in his most famous work, "The Lord of the Rings". She is a noblewoman of Rohan who is described as a shieldmaiden.
Literature.
In "The Two Towers", Éowyn, a daughter of the House of Eorl and the niece of King Théoden of Rohan, is introduced in Meduseld, the king's hall at Edoras. She was the daughter of Théodwyn (Théoden's sister) and Éomund and the sister of Éomer. When she was only seven years old, her father was killed fighting orcs and her mother died of grief. Éowyn and Éomer were raised in her uncle's household as if they were his own children.
Tolkien writes that she longed to win renown in battle—especially since she was royal—but being female, her duties were reckoned to be at Edoras. When Théoden's mind was poisoned by his adviser Gríma Wormtongue, Éowyn was obliged to care for her uncle, and his deterioration pained her deeply. To make matters worse, she was constantly harassed by Gríma, who lusted after her. However, when Gandalf arrived, he freed Théoden from Wormtongue's influence. 
Éowyn fell in love with Aragorn, but it soon became clear that he could not return her love although he did not mention his betrothal to Arwen, except by indirect allusion, and would not allow her to join him in going to war. As Aragorn pointed out, her duty was with her people; she had to shoulder the responsibility of ruling Rohan in Théoden's stead when the war-host of Rohan went to war, a duty he deemed no less valiant. Likening her situation to a "cage", Éowyn said she feared "o stay behind bars, until use and old age accept them, and all chance of great deeds is gone beyond recall or desire."
In "The Return of the King", she disguised herself as a man and under the alias of "Dernhelm" (from Old English "dern" meaning "secret, concealed"), traveled with the Riders of Rohan to the Battle of the Pelennor Fields outside the White City of Minas Tirith in Gondor, carrying with her Meriadoc Brandybuck, who had also been ordered to remain behind, on her horse Windfola.
During the battle of the Pelennor Fields, she confronted the Witch-king of Angmar, Lord of the Nazgûl, after Théoden was injured. The Witch-king threatened to "bear away to the houses of lamentation, beyond all darkness, where [her flesh shall be devoured, and shriveled mind be left naked to the Lidless Eye." The Witch-king further boasted that "[no living man may hinder me," referring to the 1,000-year-old prophecy by the Elf-lord Glorfindel, foretelling that the Witch-king would not fall "by the hand of man". Éowyn then removed her helmet and declared:
The Witch-king attacked Éowyn with his steed, but she slew it with her sword. He then shattered her shield and broke her shield-arm with his mace, but was distracted by Merry, who stabbed him behind the knee with a barrow-blade. Éowyn seized the opportunity to strike the Witch-king with a killing thrust "between crown and mantle". Then, as her sword shattered, his withering form collapsed and he vanished with a final cry of anguish.
Éowyn soon passed out from the pain in her arm, and was believed dead until Prince Imrahil of Dol Amroth realized she still lived. Éowyn was brought to the Houses of Healing, hovering near death from the effects of having struck the Witch-king. There Éowyn met Faramir, with whom she soon fell in love. Her outlook on life also changed: "Then the heart of Éowyn changed, or else at last she understood it. [...] I will be a shieldmaiden no longer, nor vie with the great Riders, nor take joy only in the songs of slaying. I will be a healer, and love all things that grow and are not barren."
After the demise of Sauron, Éowyn and Faramir married and settled in Ithilien, of which Faramir was made the ruling Prince by King Elessar, the name with which Aragorn ascended the throne of the Reunited Kingdom. Faramir and Éowyn had at least one son, Elboron, and their grandson was Barahir, who wrote "The Tale of Aragorn and Arwen" in the Fourth Age.
Characteristics.
Éowyn is described to be very beautiful; she was tall, slim, pale, and graceful, with long golden hair and grey eyes. In temperament she was idealistic, spirited, brave and high-minded, but very lonely, having sacrificed her own happiness for years to care for her sick uncle and meet the responsibilities of a shield-maiden.
Names and titles.
In Old English, the language Tolkien used to represent his invented language of Rohirric, the word "eoh" (or "eh") means "war-horse, charger" while "wyn" means "delight, pleasure" (in addition, some sample text within "Bosworth and Toller" translates "wyn" as "joy, joyous"). Therefore, even though no such word appears in the lexicon of Old English, the name "Éowyn" can be taken to mean "delightful charger".
The first syllable of "Éowyn" sounds like "eh-oh," with the "oh" just barely pronounced. As in the North Germanic languages or Finnish, the y in the second syllable is the same sound as the German letter "ü" or the French "u".
Tolkien maintained "Éowyn" was not the character's actual name. Her real name in Rohirric is not given, but it, as well as Éomer and Éomund, would have started with the element "Lô-" or "Loh-", meaning "horse", which he represented with Old English "Eoh-".
Although she never carried the title of princess, she was a niece to one King of Rohan and sister to another as well as the wife of a Gondorian prince. Éowyn's titles included the (White) Lady of Rohan, Lady of Ithilien and Lady of Emyn Arnen. She was also known as the Lady of the Shield-arm in recognition of her triumph over the Witch-king.
Concept and creation.
Originally, Tolkien intended for Éowyn to marry Aragorn. Later, however, he decided against it because Aragorn was "too old and lordly and grim." He considered making Éowyn the twin sister of Éomund, and having her die "to avenge or save Théoden". He also considered having Aragorn truly love Éowyn and regret never marrying after her death.
At one point Tolkien described Éowyn as "a stern Amazon woman". Later he wrote: "Though not a 'dry nurse' in temper, she was also not really a soldier or 'Amazon', but like many brave women was capable of great military gallantry at a crisis." (Here he alludes to Éowyn's statement to Aragorn: "But am I not of the House of Eorl, a shieldmaiden and not a dry-nurse?")
Portrayal in adaptations.
The voice of Éowyn was provided by Nellie Bellflower in the 1980 Rankin/Bass animated version of "The Return of the King", and by Elin Jenkins in BBC Radio's 1981 serialisation.
Éowyn also appears briefly in Ralph Bakshi's 1978 adaptation of "The Lord of the Rings", but does not have any dialogue.
In Peter Jackson's films ' (2002) and ' (2003), Éowyn is played by Miranda Otto. (The role was first offered to Iben Hjejle, who turned it down because she did not like the idea of being away from Denmark; Uma Thurman was slated for the role at one point.)
In the original novel and Jackson's adaptation, it is implied that Saruman promised her to Gríma as payment for his services as a spy. In one scene, while mourning for her dead cousin, she is subjected to Gríma's obnoxious affections, which she spurns. She sings the dirge at Théodred's funeral. In the extended edition of "The Two Towers," Éowyn is shown discovering, to her astonishment, that Aragorn is a long-lived Dúnadan.
In the original theatrical release of "The Lord of the Rings: The Return of the King", Éowyn plays a much larger role in the Battle of Pelennor Fields than in the book, where the only fighting mentioned is her conflict with the Witch-king and also Gothmog. Her speech revealing her identity is cut, replaced with the simple declaration "I am no man!" She also replaces Merry as the person to sit with Théoden as he dies. In the Extended Edition of the film, Éowyn is portrayed as being near death following her fight with the Witch-King; her brother finds her and screams in anguish because he fears that she is dead. She is later seen being healed by Aragorn, and meeting Faramir in the Houses of Healing.
While she disguises herself in the film to ride into battle, she never takes on the name "Dernhelm". In the Extended Edition, Théoden notices her dispatching several Orcs, but it is not clear if he realizes that she is his niece. The production team stated that while in a book it was easy to disguise Éowyn's identity, in the medium of cinema the audience could visually tell that it was she, and it would have strained the credibility of the scenes to try to make it a secret.
Her final appearance occurs at Aragorn's coronation, where she is shown standing next to Faramir. The Extended Edition restores a scene in which she falls in love with Faramir at the Houses of Healing, though even this version never states that they eventually marry. According to the DVD commentaries, an entire set-piece Faramir/Éowyn wedding scene was actually filmed, which Oscar-winning costume designer Ngila Dickson states features what she feels are the best costumes she produced for the entire film trilogy. While this scene has been described in the DVD commentaries and other interviews, it was ultimately cut and not even included in the Extended Edition, nor have any photos of the scene ever been made public.

</doc>
<doc id="62761" url="https://en.wikipedia.org/wiki?curid=62761" title="Éomer">
Éomer

Éomer is a fictional character in J. R. R. Tolkien's legendarium. He appears in "The Two Towers" and "The Return of the King", the second and third volumes of Tolkien's fantasy novel "The Lord of the Rings".
The name Éomer, literally translating to "Horse-famous", can be found in "Beowulf", an Anglo-Saxon poem Tolkien had studied extensively and drew from while creating his characters.
Appearances.
Literature.
The son of Théodwyn and Éomund, belonging to the House of Eorl, Éomer is the third Marshal of the Riddermark at the start of the "Lord of the Rings". Both he and his sister, Éowyn, were adopted by their uncle Théoden, king of the Rohirrim, after their parents' death. His first appearance in the story is in "The Two Towers", as the leader of the "éored" who attacked and killed the Uruk-hai who had kidnapped the Hobbits Meriadoc Brandybuck and Peregrin Took as they camped near Fangorn forest. He helps Aragorn, Gimli and Legolas by providing them two horses, Hasufel and Arod, and guiding them to the spot where the attack had taken place.
On his return to Edoras, Éomer reports to Théoden on his meeting the Ranger and his friends, and is promptly imprisoned on the orders of Gríma Wormtongue, Théoden's sinister advisor, who was keeping the king in a sickly stupor on the orders of the wizard Saruman. Soon thereafter, Aragorn, Gimli and Legolas arrive in Edoras themselves with Gandalf the White, another powerful wizard, who releases the king from Gríma's spell. Éomer is released and restored in honour, in which role he contributes to success at the battle of the Hornburg, where he and his "éored", led by Théoden and Aragorn, drive Saruman's army of Orcs and Dunlendings from the walls of the Hornburg, buying valuable time for Gandalf's reinforcements to arrive.
Éomer has a major role in the Battle of the Pelennor Fields, the pivotal battle of "The Return of the King". After fighting bravely for Rohan and Gondor, he is dismayed to find Théoden mortally wounded in the battle and Éowyn lying unconscious, seemingly dead nearby. Théoden appoints him King of Rohan with his dying breath, and Éomer decides to throw himself and the remaining Rohirrim at the enemy, hoping to weaken them as much as he can even at the sacrifice of his men and himself. Aragorn saves them when he arrives unexpectedly from Pelargir, fulfilling his prediction that they would fight together again. Aragorn's arrival and reinforcements provoke a rout among the Orcs, and he and Éomer win the battle. Aragorn's healing hands later restore Éowyn to perfect health.
At the climax of "The Return of the King", Éomer accompanies Aragorn to the Gates of Mordor for the final stand against Sauron, distracting him long enough for the One Ring to be destroyed in Mount Doom, leading to Sauron's downfall. After Théoden's funeral, he stays on in Minas Tirith to help Aragorn, now crowned King Elessar of the Reunited Kingdom, rebuild his kingdom.
Éomer had met Princess Lothíriel of Dol Amroth, during his stay in Gondor, and they were soon wed. She bore him a son named Elfwine, known as "Elfwine the Fair".
Éomer's sword was called Gúthwinë (, "battle friend"), and was a gift from his father, Éomund.
Adaptations.
In the 1978 animated adaptation of "The Lord of the Rings" by Ralph Bakshi, Éomer is portrayed as a renegade. He does not have any lines and is not fully animated (seen as a live action individual painted over), but is still important to the plot. He also appears in the 1980 Rankin/Bass animated version of "The Return of the King", albeit without lines.
In Peter Jackson's "The Lord of the Rings" film trilogy, Éomer was played by New Zealand actor Karl Urban. His role is somewhat diminished in comparison to the books. In "", he is exiled by Gríma before meeting Aragorn. As an outcast, he leads a troop of riders loyal to Théoden northward out of Rohan rather than being imprisoned in Edoras.
In both the Bakshi and Jackson versions, he arrives at the climax of the "Battle of Helm's Deep", accompanied by Gandalf (although the animated film does not single him out at Helm's Deep). In this sense, his character has been combined with the character of Erkenbrand, who, in the book, is the one with whom Gandalf returns to Helm's Deep.
Éomer's actions in Jackson's adaptation of "The Return of the King" did not significantly depart from those in the book, save for a few scenes (such as Éomer letting out a defiant cry at the approaching Corsair ships during the Battle of the Pelennor Fields, only to realize the ships have been captured by Aragorn) that were omitted for time. In Jackson's adaptation Éomer is also responsible for the death of the leader of the Mûmak-riding Haradrim, while in the book it is Théoden who slays the Haradrim chieftain, who is on horseback. Also, Éomer's speech after Théoden's death in the book is spoken instead by Théoden himself before the first charge in the movie. Neither is Éomer present at the death of Théoden in the film. The close friendship he shares with Aragorn in the books is not developed in the films, but during Aragorn's coronation in "The Return of King" he can be seen bowing honorably towards him.

</doc>
<doc id="62766" url="https://en.wikipedia.org/wiki?curid=62766" title="Tom Bombadil">
Tom Bombadil

Tom Bombadil is a supporting character in J. R. R. Tolkien's legendarium. He appears in Tolkien's high fantasy epic "The Lord of the Rings", published in 1954 and 1955. In the first volume, "The Fellowship of the Ring", Frodo Baggins and company meet Bombadil in the Old Forest. He also appears in "The Adventures of Tom Bombadil", a book of verse first published in 1962, purporting to be a selection of Hobbit poems, two of which concern Bombadil.
Appearances.
"The Adventures of Tom Bombadil".
Tolkien invented Tom Bombadil in memory of his children's Dutch doll, and wrote light-hearted children's poems about him, imagining him as a nature-spirit evocative of the English countryside.
Tolkien's 1934 poem "The Adventures of Tom Bombadil" depicts Bombadil as a "merry fellow" living in a small valley close to the Withywindle river, where he wanders and explores nature at his leisure. Several of the valley's mysterious residents, including the River-spirit Goldberry (also known as the "River-woman's daughter"), the malevolent tree-spirit Old Man Willow, the Badger-folk and a Barrow-wight, attempt to capture Bombadil for their own ends, but quail at the power of Tom's voice, which defeats their enchantments and commands them to return to their natural existence. At the end of the poem, Bombadil captures and marries Goldberry. Throughout the poem, Bombadil is unconcerned by the attempts to capture him and brushes them off with the power in his words.
The later poem "Bombadil Goes Boating" anchors Bombadil in Middle-earth, featuring a journey down the Withywindle to the Brandywine river, where hobbits ("Little Folk I know there") live at Hays-End. Bombadil is challenged by various river-residents on his journey, including birds, otters and hobbits, but charms them all with his voice, ending his journey at the farm of Farmer Maggot, where he drinks ale and dances with the family. At the end of the poem, the charmed birds and otters work together to bring Bombadil's boat home. The poem includes a reference to the Norse lay of Ótr, when Bombadil threatens to give the hide of a disrespectful otter to the Barrow-wights, who he says will cover it with gold apart from a single whisker. The poem mentions a number of Middle-earth locations, including Hays-End, Bree and the Tower Hills, and hints at the events of the end of the Third Age, speaking of "Tall Watchers by the Ford, Shadows on the Marches".
The poems were published in the collections "The Adventures of Tom Bombadil" and later in "Tales from the Perilous Realm".
"The Lord of the Rings".
In "The Lord of the Rings", Tom Bombadil is a mysterious character who aids Frodo and his companions on their journey. He and his wife Goldberry, the "Daughter of the River", still live in their house on the Withywindle, and some of the characters and situations from the original poem appear in "The Lord of the Rings". In the book, he is described as "Master of wood, water and hill", and nearly always speaks or sings in stress-timed metre: 7-beat lines broken into groups of 4 and 3 (old English metre as first noted in Caedmons Hymn in the story of Bede, discovered in the 19th century). He appears in three chapters, "The Old Forest", "In the House of Tom Bombadil" and "Fog on the Barrow-downs". He is mentioned in the chapter "The Council of Elrond" as a possible keeper and protector of the One Ring, as well as at the end of the story in "Homeward Bound" and "The Grey Havens". Behind Bombadil's simple façade are hints of great knowledge and power, though limited to his own domain. However, there is a certain amount of evidence that he may be the oldest being in Middle-earth -- he tells Frodo, Sam, Merry and Pippin that he was there when the Elves first came West, Elrond states that Bombadil was considered old during the Elder Days (even by high Elf lords), and Bombadil names himself "Eldest".
Tom first appears when Merry and Pippin are trapped by Old Man Willow, and Frodo and Sam cry for help. Tom commands Old Man Willow to release them, singing him to sleep, and shelters the hobbits in his house for two nights. Here it is seen that the One Ring has no power over Bombadil; he can see Frodo when the Ring makes him invisible to others, and can wear it himself with no effect. He even tosses the Ring in the air and makes it disappear, but then produces it from his other hand and returns it to Frodo. While this seems to demonstrate that he has unique and mysterious power over the Ring, the idea of giving him the Ring for safekeeping is rejected in Book Two's second chapter, "The Council of Elrond". Gandalf says, rather, that "the Ring has no power over him..." and believes that Tom would not find the Ring to be very important and so might simply misplace it.
Frodo spends two nights in Tom Bombadil's house, each night dreaming a different dream, which appear to be either clairvoyant or prophetic. The first night he dreams of fearful things, including Gandalf's imprisonment atop Orthanc in Isengard. The second night he dreams of a song that "seemed to come like a pale light behind a grey rain-curtain, and growing stronger to turn the veil all to glass and silver, until at last it was rolled back, and a far green country opened before him under a swift sunrise."
Before sending the hobbits on their way, Tom teaches them a rhyme to summon him if they fall into danger again within his borders. This proves fortunate, as the four encounter Barrow-wights in the following chapter, "Fog on the Barrow-downs". After saving them from the Barrow-wights, Tom gives each hobbit a long dagger taken from the treasure in the barrow. As the hobbits leave the Old Forest, he refuses to pass the borders of his own land, but before he goes he directs them to The Prancing Pony Inn at Bree.
Towards the end of "The Return of the King", when Gandalf leaves the hobbits, he mentions that he wants to have a long talk with Bombadil, calling him a "moss-gatherer". Gandalf says, in response to Frodo's query of how well Bombadil is getting along, that Bombadil is "as well as ever", "quite untroubled" and "not much interested in anything that we have done and seen", save their visits to the Ents. At the very end of "The Lord of the Rings", as Frodo sails into the West and leaves Middle-earth, he has what seems to him the very experience that appeared to him in the house of Bombadil in his dream of the second night.
Characteristics.
Tom Bombadil is spry, with a quick, playful wit. He speaks in a rhyming whimsical way: "Hey dol! merry dol! ring a dong dillo! Ring a dong! hop along! fal lal the willow! Tom Bom, jolly Tom, Tom Bombadillo!" He has a jolly, carefree attitude, and little seems to concern him. He sometimes refers to himself in the third person, as if simultaneously weaving his own epic narrative, even as he lives it.
In "The Lord of the Rings" he twice describes himself in his songs as: "Bright blue his jacket is, and his boots are yellow."
Bombadil does not seem concerned about the One Ring, though he seems to know at least as much as the hobbits about its provenance and power. Although the deliberations at the Council of Elrond at Rivendell suggest that Bombadil would be vulnerable to Sauron if the latter recovered the Ring, Bombadil seems unaffected by the Ring's power and more concerned with keeping his own "country" around the Withywindle in order.
Tolkien says little about Tom Bombadil's origins in the cosmology of Middle-earth. Bombadil calls himself the "Eldest" and the "Master". He claims to remember "the first raindrop and the first acorn", and that he "knew the dark under the stars when it was fearless — before the Dark Lord came from Outside". He does not fit neatly into the categories of beings Tolkien created. Readers have speculated about his true nature, suggesting that he is one of the Ainur, angelic beings who shaped the earth, or even God (Eru Ilúvatar in Tolkien's legendarium), pointing to the passage where Frodo asks Goldberry just who Tom Bombadil is; she responds simply by saying "He is". But Tolkien rejected the notion that Bombadil is God, and carefully differentiated Goldberry's response from the Biblical "I Am that I Am". Robert Foster in "The Complete Guide to Middle Earth" describes Tom Bombadil as "a Maia 'gone native'".
At the Council of Elrond, Galdor suggests that Bombadil would be unable to withstand a siege by Sauron "unless such power is in the earth itself", implying that the character may be a manifestation of Middle-earth's inherent properties. This connection would explain Bombadil's seeming obliviousness to the transient concerns of mortals, as evidenced in Gandalf's concern that Tom would not understand the importance of the Ring and would lose it if entrusted with it.
In reference to Bombadil, Tolkien himself said that some things should remain mysterious in any narrative, "especially if an explanation actually exists". Tom Bombadil is not the only being whose nature is unexplained. While passing Caradhras in Book Two of "The Fellowship of the Ring", Aragorn mentions beings more ancient even than Sauron. In Book Three of "The Two Towers", when describing his fall in the pits of Moria, Gandalf mentions dark creatures who gnaw the world.
Names and titles.
Gandalf calls Tom Bombadil the eldest being in existence; this is evidenced by his Sindarin name "Iarwain Ben-adar" (Eldest and Fatherless). Dwarves called him "Forn" (Scandinavian, meaning "Ancient" or "Belonging to the distant past"; in Icelandic it can also mean that he has magical abilities), Men "Orald" (compare to German: "uralt", original old, eldest). All these names apparently mean "Eldest". Treebeard calls himself the eldest living being of Middle-earth and says that he was there before anyone else. However, Tolkien remarked in another context: "Treebeard is a character in my story, not me; and though he has a great memory and some earthy wisdom, he is not one of the Wise, and there is quite a lot he does not know or understand."
Concept and creation.
As with "Roverandom", Tolkien's initial inspiration came from an incident with his children playing with toys. Tolkien invented Tom Bombadil in memory of a Dutch doll which had been flushed down a lavatory. These original poems far pre-date the writing of "The Lord of the Rings", into which Tolkien introduced Tom Bombadil from the earliest drafts.
In response to a letter from one of his readers, Tolkien described Tom's role in "The Lord of the Rings":
Tom Bombadil is not an important person — to the narrative. I suppose he has some importance as a 'comment.' I mean, I do not really write like that: he is just an invention (who first appeared in "The Oxford Magazine" about 1933), and he represents something that I feel important, though I would not be prepared to analyse the feeling precisely. I would not, however, have left him in, if he did not have some kind of function.
Tolkien did go on to analyse the character's role further:
I might put it this way. The story is cast in terms of a good side, and a bad side, beauty against ruthless ugliness, tyranny against kingship, moderated freedom with consent against compulsion that has long lost any object save mere power, and so on; but both sides in some degree, conservative or destructive, want a measure of control. But if you have, as it were, taken 'a vow of poverty', renounced control, and take your delight in things for themselves without reference to yourself, watching, observing, and to some extent knowing, then the questions of the rights and wrongs of power and control might become utterly meaningless to you, and the means of power quite valueless...
It is a natural pacifist view, which always arises in the mind when there is a war ... the view of Rivendell seems to be that it is an excellent thing to have represented, but that there are in fact things with which it cannot cope; and upon which its existence nonetheless depends. Ultimately only the victory of the West will allow Bombadil to continue, or even to survive. Nothing would be left for him in the world of Sauron.
Tolkien even seems to justify Tom Bombadil's presence:
And even in a mythical Age there must be some enigmas, as there always are. Tom Bombadil is one (intentionally).
In a letter to Stanley Unwin, Tolkien called Tom Bombadil the spirit of the vanishing landscapes of Oxfordshire and Berkshire.
Adaptations.
In most film and radio adaptations of the story, Bombadil is notable by his absence (an exception is the Mind's Eye recordings). Both Ralph Bakshi and Peter Jackson stated that the reason the character was omitted from their films was because, in their view, he does little to advance the story, and would make their films unnecessarily long. Christopher Lee concurred, stating the scenes were left out to make time for showing Saruman's capture of Gandalf. Some of Bombadil's dialogue, as well as the scene in which the hobbits meet Old Man Willow, are transferred into scenes which Merry and Pippin share with Treebeard in Jackson's adaptation, included in the extended edition DVD.
Although Tom Bombadil was not portrayed in Peter Jackson's film trilogy of "The Lord of the Rings", a Tom Bombadil card exists in "The Lord of the Rings Trading Card Game" by Decipher, Inc. (part of the trilogy's merchandise). The model portraying Bombadil on this card is Harry Wellerchew.
Bombadil has appeared in a number of other adaptations, including the Mind's Eye radio adaptation. He was played by Norman Shelley in the 1955–1956 BBC radio adaptation of "The Lord of the Rings". He was portrayed by Esko Hukkanen in the 1993 Finnish miniseries "Hobitit".
Although the character does not actually appear in the musical adaptation, towards the end of the show Gandalf explains to Frodo that on his journey back to the Shire he will spend some time in Bombadil's company.
Tom Bombadil is an NPC in the MMORPG game ', serving as a main character in Book 1 of the epic quests. He also makes an appearance in EA Games' ' as a summonable hero for the forces of light (except the dwarves) where his only real use is skipping through the battlefield, kicking enemy troops out of the way as he goes. Tom is a usable model in "The Lord of the Rings Strategy Battle Game" produced by Games Workshop. In this, he is invincible, in that he can never be harmed in any way, but neither can he cause harm to any of his opponents. He can only be played in the Old Forest, as in keeping with his story. He also appears in "" video game, and has an appearance as a purchasable character in "Lego The Lord of the Rings" and "Lego The Hobbit".
The "Harvard Lampoon" novel "Bored of the Rings" parodies Bombadil as "Tim Benzedrine", a stereotypical hippie married to "Hashberry".
In the "Portlandia" episode "Motorcycle", segment "Small Hatchback", several hippies are shown singing Bombadil's name.

</doc>
<doc id="62767" url="https://en.wikipedia.org/wiki?curid=62767" title="Radagast">
Radagast

Radagast the Brown is a fictional character in J. R. R. Tolkien's Middle-earth legendarium. He is one of the Istari, also known as "Wizards", who were sent by the angelic Valar to aid the Elves and Men of Middle-earth in their struggle against the Dark Lord Sauron. Radagast appears in "The Lord of the Rings" and "Unfinished Tales", and is mentioned in "The Hobbit" and "The Silmarillion".
Literature.
"Unfinished Tales" explains that Radagast, like the other Wizards, came from Valinor around the year 1000 of the Third Age of Middle-earth and was one of the Maiar. His original name was Aiwendil, meaning "bird-friend" in Tolkien's invented language of Quenya. The Vala Yavanna forced the wizard Saruman to accept Radagast as a companion, which, Tolkien says, may have been one of the reasons Saruman was contemptuous of him, to the point of scornfully calling him "simple" and "a fool". However, he was an ally and confidant of Gandalf, who describes him in "The Hobbit" as his "cousin". He was also friends with the skin-changer Beorn, who deemed him to be "not a bad fellow as wizards go" and also said to Gandalf that he "used to see him now and again".
Radagast lived for much of his time in Middle-earth at Rhosgobel in the Vales of Anduin, on the western eaves of Mirkwood, between Carrock and the Old Forest Road, near the Gladden Fields, its name deriving from Sindarin "rhosc gobel" meaning "brown village". Radagast had a strong affinity for – and relationship with – wild animals, and it seemed his greatest concern was with the "olvar" and "kelvar" (flora and fauna) of Middle-earth. He was wiser than any Man in all things concerning herbs and beasts. It is said he spoke the many tongues of birds, and was a "master of shapes and changes of hue". Radagast is also described by Gandalf as "never a traveller, unless driven by great need", "a worthy Wizard", and "honest".
In "The Fellowship of the Ring", Radagast was unwittingly used by Saruman to lure Gandalf to his tower of Orthanc, where Gandalf was captured. Fortuitously, Radagast also helped rescue him by sending Gwaihir the Eagle to Orthanc with news of the movements of Sauron's forces. When Gwaihir saw that Gandalf was imprisoned on the top of the tower he carried him off to safety before Saruman realized he was gone.
The only other reference to Radagast in "The Lord of the Rings" is after the Council of Elrond when it is decided to summon all the allies against Sauron together. Scouts are sent to look for help, and it is reported that Radagast is not at his home at Rhosgobel and cannot be found. Tolkien makes no mention of what has happened to Radagast, and he plays no further role in events.
Tolkien wrote that Radagast gave up his mission as one of the Wizards by becoming too obsessed with animals and plants. He also wrote that he did not believe that Radagast's failure was as great as Saruman's. However, Christopher Tolkien notes in "Unfinished Tales" that the assumption Radagast failed in his task may not be entirely accurate considering that he was specifically chosen by Yavanna, and he may have been assigned to protect the flora and fauna of Middle-earth, a task that would not end with the defeat of Sauron and the end of the War of the Ring.
Names and titles.
According to the essay "The Istari" from the "Unfinished Tales", the name "Radagast" means "tender of beasts" in Adûnaic, another of Tolkien's fictional languages. However, Christopher Tolkien indicates that his father intended to change this derivation and bring Radagast in line with the other wizard-names, Gandalf and Saruman, by associating it with the old language of the Men of the Vales of Anduin. No alternative meaning is provided with this new association – indeed, Tolkien stated that the name was "not now clearly interpretable". His title "The Brown" is simply a reference to his earth-brown robes; each of the wizards had a cloak of a different colour.
In the real world, "Radagast" or "Rodogast" is extant as a variant of "Radagaisus", the name of a Gothic warlord who led an invasion of Italy in 405. Radegast is also the name of a deity in Slavic mythology.
Adaptations.
In film.
Radagast is not included in Peter Jackson's "The Lord of the Rings" film trilogy. Gandalf's escape from Orthanc is instead instigated by a moth that Gandalf uses to convey a message to the Eagles.
In Jackson's "", the character (played by Sylvester McCoy) is greatly fleshed out, compared to the original book in which he is mentioned only once. He is portrayed as an eccentric who prefers the company of animals to men, at one point using his powers to heal a dying hedgehog. Radagast is shown to be able to communicate with birds, some of which nest in his hair. In the film, Radagast is the first wizard to visit Dol Guldur after he realizes that an evil power had infected the wood he lives in. He discovers that a Necromancer (Sauron) has taken residence in the ruined fortress. In Dol Guldur he encounters the spirit of the Witch-king of Angmar, as well as the shadow of the Necromancer himself, and escapes with the Morgul blade taken from the Witch-king.
Radagast's means of transportation is a sled pulled by enormous rabbits (called "Rhosgobel rabbits"), a concept entirely original to the movie. Radagast meets Gandalf, Bilbo, and the Dwarves en route to Erebor (but in Rhudaur), tells them of his discovery in Dol Guldur, and gives Gandalf the dagger to present before the White Council. When Thorin's Company are attacked by Orcs riding wargs, Radagast mounts his sled and provides a distraction, leading the enemy on a chase while Gandalf and the others escape. 
Later, Saruman makes contemptuous remarks about Radagast during a meeting with Gandalf, Elrond, and Galadriel. He accuses the Brown Wizard of indulging in mushrooms, and dismisses Radagast's claim about the Necromancer being a true threat.
In "", Radagast appears with Gandalf in a few scenes. The two wizards investigate an empty tomb, determining that the Nazgûl are once again awake and have been summoned. Gandalf bids Radagast to go and tell Galadriel of all they find, and that the White Council must make a pre-emptive move on Dol Guldur. A worried Radagast watches as Gandalf enters Dol Guldur. Inside the ruins, Gandalf confronts the Necromancer and finds that he is indeed Sauron, just as Radagast had thought.
In "" Radagast arrives in Dol Guldur as the White Council battle Sauron and the Nazgûl, and takes the wounded Gandalf to his house. Gandalf decides to go to Erebor and tells Radagast to gather the birds and beasts; in recompense Radagast gives Gandalf his staff as Sauron shattered his prior one. Later when the eagles arrive to help defeat the Orcs at the Battle of the Five Armies, Radagast is riding one of the eagles.
Others.
Radagast appears as a non-player character in "" in the city of Ost Guruth. In the epic quest line (Volume I Book II) the player aids Radagast in dealing with corruption in the Lone-Lands, ending in a confrontation with the Gaunt-Lord Ivar, who is bested by Radagast and driven away. After the instance, Radagast relocates to the tower of Barad Dhorn in Agamaur.
In the "Lord of the Rings Strategy Battle Game", Radagast is portrayed as a user of subtle magics, contrasted with the more overt kinds used by Gandalf and Saruman. Nevertheless, he has certain unique powers.
In the video game "", Radagast is portrayed as a bumbling and eccentric character, albeit very wise and powerful. After the player character rescues Radagast from the clutches of the spider-queen Saenathra, Radagast (with the help of his animal-friends) provides Eradan, Andriel and Farin with information on the location of the dragon Urgost.
In "", Radagast is a playable character who can be found in the outskirts of Bree. In the portable versions of the title, he gives the player an optional quest to find three of his birds which have been captured by enemy forces. In battle, his abilities are similar to Gandalf the Grey's, though he lacks a sword. Radagast also appears as a character in "Lego The Hobbit", but with an appearance closer to his film incarnation.

</doc>
<doc id="62768" url="https://en.wikipedia.org/wiki?curid=62768" title="Déagol">
Déagol

Déagol is a fictional character in J. R. R. Tolkien's legendarium. His story is related in "The Fellowship of the Ring", the first of three volumes comprising Tolkien's most famous novel, "The Lord of the Rings", in the chapter "The Shadow of the Past".
Biography.
Déagol was a Stoor Hobbit who lived in a small community bound by kinship ties—akin to a clan. He had a relative named Sméagol, whose grandmother was the matriarch of the community.
In 2463 of the Third Age, Déagol became the third bearer of the One Ring, after Sauron and Isildur. He found the Ring—which had been lost for thousands of years—while fishing with Sméagol in the Gladden river.
Instantly ensnared by its beauty and seductive power, Sméagol demanded the Ring as his "birthday-present". When Déagol refused to give it up, Sméagol strangled him and hid his body, which was never found; nevertheless, the murderer (nicknamed "Gollum" after the swallowing noises he made) was eventually driven from his home and into the Misty Mountains.
Name.
The name "Déagol" is from Old English "dēagol" (also "dēogol"), meaning "secretive, hidden". 
In Tolkien's "Red Book of Westmarch", the name "Déagol" is used as a translation of the "original" name in the author-invented language of Westron, "Nahald", which has the same meaning.
Adaptations.
Déagol appears in the prologue to Ralph Bakshi's 1978 animated adaptation of ""The Lord of the Rings". 
In Peter Jackson's 2001–2003 live-action adaptations of "The Lord of the Rings", Déagol is played by New Zealand actor Thomas Robins. His scenes with Andy Serkis (Sméagol/Gollum) were originally planned to be in ', but were moved to '.

</doc>
<doc id="62772" url="https://en.wikipedia.org/wiki?curid=62772" title="Anacamptis pyramidalis">
Anacamptis pyramidalis

Anacamptis pyramidalis, the pyramidal orchid, is a perennial herbaceous plant belonging to the genus "Anacamptis" of the family Orchidaceae. The scientific name "Anacamptis" derives from Greek ανακάμτειν 'anakamptein' meaning 'bend forward', while the Latin name "pyramidalis" refers to the pyramidal form of the inflorescence.
Description.
This hardy plant reaches on average of height, with a maximum of . The stem is erect and unbranched. The basal leaves are linear-lanceolate with parallel venation, up to long, the cauline ones are shorter and barely visible on the stem. The arrangement of hermaphroditic flowers in a compact pyramidal shape is very distinctive and gives the orchid its common name. The colour of the flower varies from pink to purple, or rarely white, and the scent is described as "foxy". The flowers have six tepals, being three small sepals and three petals. Two small petals are on the sides, while the third and lower (labellum) is large and trilobate. At the back of the flower there is a tubular spur of about long, while the labellum bears two lateral small flaps. The flowering period extends from April through July.
Reproduction.
The flowers are pollinated by butterflies and moths. To ensure the fertilization, their morphology is well adapted to the proboscis of Lepidoptera, especially "Euphydryas", "Melanargia", "Melitaea", "Pieris" and "Zygaena" species. The mechanism by which its pairs of pollinia attach themselves to an insect's proboscis was discovered by Charles Darwin and described in his book on the "Fertilisation of Orchids".
Distribution.
This orchid is native to southwestern Eurasia, from western Europe through the Mediterranean region eastwards to Iran. In Germany, it is rare and was declared Orchid of the Year in 1990 to heighten awareness of this plant. This orchid is especially common on the Isle of Wight in the South of England, and was designated the county plant in 2008. On the Isle of Wight, it favours growth in chalky or sandstone-rich soil, and thus can easily be found on the Downland and cliffs to the west and south of the island.
Habitat.
"Anacamptis pyramidalis" requires a sunny spot on diverse soils: loamy or clay. It can even grow on very alkaline soil. It can be found on meadows or dry and well exposed slopes, at an altitude of above sea level.
Varieties.
There are some notable varieties, which are sometimes treated as subspecies – and as they seem to be limited to certain regions, this may be correct:
The variety "alba" can be found anywhere in the Pyramidal Orchid's range; its flowers are white.
Medicinal uses.
The dried and ground tuber gives a fine white powder, called salep. This is a very nutritious sweet starchlike substance. It is used in drinks, cereals and in making bread. It is also used medicinally in diets for children and convalescents.
Culture.
The pyramidal orchid was voted the County flower of the Isle of Wight in 2002 following a poll by the wild flora conservation charity Plantlife.

</doc>
<doc id="62773" url="https://en.wikipedia.org/wiki?curid=62773" title="Gymnadenia conopsea">
Gymnadenia conopsea

Gymnadenia conopsea, the fragrant orchid, is a herbaceous plant belonging to the family Orchidaceae.
Etymology.
The name of the genus "Gymnadenia" is formed from Greek words (', "nude") and (', "gland") and refers to the characteristics of the organs for secreting nectar. The specific Latin name "conopsea" derives from the Greek ' ('), literally meaning "mosquito-like", probably because of the similarity of the long spur of the flower with the mouthparts of a mosquito.
The scientific binomial name of this plant was initially "Orchis conopsea", proposed by the Swedish naturalist and botanist Carl von Linné (1707–1778) in his ' of 1753. The name has been subsequently amended to the one currently accepted ("Gymnadenia conopsea"), by the British botanist Robert Brown (1773–1858) in 1813. In German, this plant is called ' or '; in French, is called ' or '; in Italy, it is called ' ("pink hand"); in Croatian, it is known under "".
Description.
"Gymnadenia conopsea" reaches on average of height, with a maximum of . These plants are bulbous geophytes, as they bring their buds in underground tubers or bulbs, organs that annually produce new stems, leaves and flowers. Furthermore, these orchids are "terrestrial", because unlike "epiphyte" species do not live at the expense of other plants of major sizes.
The stem is leafy and robust, with a striated surface. The leaves are long, narrow and lanceolate and vary from 3 to 7. The leaf color is gray-green. Size of leaf: width 1 to 2 cm, length 10 – 25 cm.
These orchids have two ovoidal bulbs, deeply webbed and with many small and short lobes. Size of tubers: 1 to 3.5 cm.
The inflorescence is long and it is composed of flowers gathered in dense cylindrical spikes (up to 50 flowers per spike). These inflorescences are scented and genes underlying eugenol (a volatile scent compound) production have been identified in Gymnadenia conopsea, Gymnadenia odoratissima and Gymnadenia densiflora The flowers are petiolated, placed in the axils of long bracts and reach on average . They have a distinctive three lobed lip and long spurs. Their light scent is similar to cloves. Their colors vary from white and pink to pink-purple, more rarely white. These flowers bloom in the Summer, from June to July. They are hermaphrodite and pollinated by insects (entomophily), including moths. The species is almost exclusively pollinated by moths (Lepidoptera). The most common pollinators are the small elephant hawk-moth ("Deilephila porcellus"), hummingbird hawk-moth ("Macroglossum stellatarum"), silver Y ("Autographa gamma"), burnished brass ("Diachrysia chrysitis") and large yellow underwing ("Noctua pronuba"). Fruit set is high with an average of 73%. The seeds germination is conditioned by the presence of specific fungi.
Distribution.
This plant is quite common throughout northern Europe with the exception of the Dinaric Alps. In Asia it is common in areas to the north of the Himalayas.
Habitat.
This species habitat includes mountain meadows and pastures, grassland and fens. They grow on siliceous and calcareous substrate, mildly damp and with low nutritional value, at an altitude of above sea level.

</doc>
<doc id="62775" url="https://en.wikipedia.org/wiki?curid=62775" title="Pre-abelian category">
Pre-abelian category

In mathematics, specifically in category theory, a pre-abelian category is an additive category that has all kernels and cokernels.
Spelled out in more detail, this means that a category C is pre-abelian if:
Note that the zero morphism in item 3 can be identified as the identity element of the hom-set Hom("A","B"), which is an abelian group by item 1; or as the unique morphism "A" → "O" → "B", where "O" is a zero object, guaranteed to exist by item 2.
Examples.
The original example of an additive category is the category Ab of abelian groups.
Ab is preadditive because it is a closed monoidal category, the biproduct in Ab is the finite direct sum, the kernel is inclusion of the ordinary kernel from group theory and the cokernel is the quotient map onto the ordinary cokernel from group theory.
Other common examples:
These will give you an idea of what to think of; for more examples, see abelian category (every abelian category is pre-abelian).
Elementary properties.
Every pre-abelian category is of course an additive category, and many basic properties of these categories are described under that subject.
This article concerns itself with the properties that exist specifically because of the existence of kernels and cokernels.
Although kernels and cokernels are special kinds of equalisers and coequalisers, a pre-abelian category actually has "all" equalisers and coequalisers.
We simply construct the equaliser of two morphisms "f" and "g" as the kernel of their difference "g" − "f"; similarly, their coequaliser is the cokernel of their difference.
Since pre-abelian categories have all finite products and coproducts (the biproducts) and all binary equalisers and coequalisers (as just described), then by a general theorem of category theory, they have all finite limits and colimits.
That is, pre-abelian categories are finitely complete.
The existence of both kernels and cokernels gives a notion of image and coimage.
We can define these as
That is, the image is the kernel of the cokernel, and the coimage is the cokernel of the kernel.
Note that this notion of image may not correspond to the usual notion of image, or range, of a function, even assuming that the morphisms in the category "are" functions.
For example, in the category of topological abelian groups, the image of a morphism actually corresponds to the inclusion of the "closure" of the range of the function.
For this reason, people will often distinguish the meanings of the two terms in this context, using "image" for the abstract categorical concept and "range" for the elementary function-theoretic concept.
In many common situations, such as the category of sets, where images and coimages exist, their objects are isomorphic.
Put more precisely, we have a factorisation of "f": "A" → "B" as
where the morphism on the left is the coimage, the morphism on the right is the image, and the morphism in the middle (called the "parallel" of "f") is an isomorphism.
In a pre-abelian category, "this is not necessarily true".
The factorisation shown above does always exist, but the parallel might not be an isomorphism.
In fact, the parallel of "f" is an isomorphism for every morphism "f" if and only if the pre-abelian category is an abelian category.
An example of a non-abelian, pre-abelian category is, once again, the category of topological abelian groups.
As remarked, the image is the inclusion of the "closure" of the range; however, the coimage is a quotient map onto the range itself.
Thus, the parallel is the inclusion of the range into its closure, which is not an isomorphism unless the range was already closed.
Exact functors.
Recall that all finite limits and colimits exist in a pre-abelian category.
In general category theory, a functor is called "left exact" if it preserves all finite limits and "right exact" if it preserves all finite colimits. (A functor is simply "exact" if it's both left exact and right exact.)
In a pre-abelian category, exact functors can be described in particularly simple terms.
First, recall that an additive functor is a functor "F": C → D between preadditive categories that acts as a group homomorphism on each hom-set.
Then it turns out that a functor between pre-abelian categories is left exact if and only if it is additive and preserves all kernels, and it's right exact if and only if it's additive and preserves all cokernels.
Note that an exact functor, because it preserves both kernels and cokernels, preserves all images and coimages.
Exact functors are most useful in the study of abelian categories, where they can be applied to exact sequences.
Special cases.
The pre-abelian categories most commonly studied are in fact abelian categories; for example, Ab is an abelian category.

</doc>
<doc id="62777" url="https://en.wikipedia.org/wiki?curid=62777" title="Henry Benedict Stuart">
Henry Benedict Stuart

Henry Benedict Thomas Edward Maria Clement Francis Xavier Stuart, Cardinal Duke of York (6 March 1725 – 13 July 1807) was a Roman Catholic Cardinal, as well as the fourth and final Jacobite heir to claim the thrones of England, Scotland, France, and Ireland publicly. Unlike his father, James Francis Edward Stuart, and brother, Charles Edward Stuart, Henry made no effort to seize the throne. After Charles's death in January 1788 the Papacy did not recognise Henry as the lawful ruler of England, Scotland, and Ireland, but referred to him as the Cardinal Duke of York.
He spent his life in the Papal States and had a long career in the clergy of the Roman Catholic Church, rising to become the Dean of the College of Cardinals and Cardinal-Bishop of Ostia and Velletri. At the time of his death he was (and still is) one of the longest serving Cardinals in the Church's history.
In his youth, Henry's father made him Duke of York (in the Jacobite Peerage), and it was by this title that he was best known. Upon the death of his brother in 1788 Henry became known by Jacobites, and within his personal entourage, as Henry IX of England and Ireland, and I of Scotland, although publicly he referred to himself as Cardinal-Duke of York "nuncupatus".
Early life.
Henry was born in exile at the Palazzo Muti in Rome on 6 March 1725 and baptized on the same day by Pope Benedict XIII, 37 years after his grandfather James II and VII lost the throne, and ten years after his father's failed attempt to regain it. His father was James Francis Edward Stuart, known to his opponents as "the Old Pretender". His mother was the Princess Maria Klementyna Sobieska, granddaughter of the Polish King and Lithuanian Grand-Duke, John III Sobieski.
Henry went to France in 1745 to help his brother, Prince Charles Edward Stuart ("Bonnie Prince Charlie", or "the Young Pretender") prepare the Jacobite rising of 1745. After its defeat, Henry Stuart returned to Italy. On 30 June 1747 Pope Benedict XIV conferred him with tonsure and created him Cardinal-Deacon of "S. Maria in Portico" in a special consistory held on the 3 July 1747. On 27 August 1747 he was promoted to the four minor orders by the Pope. He received the subdiaconate on 18 August 1748 and diaconate on 25 August 1748. He was ordained priest on 1 September 1748 and consecrated titular Archbishop of Corinth on 2 October 1758.
He was advanced to the order of Cardinal Priest in 1748, maintaining title to "S. Maria in Portico". In 1752 he transferred to the "titulus" of "Ss. XII Apostoli". He was made Cardinal-Bishop of Frascati on 13 July 1761, and eventually succeeded to the See of Ostia and Velletri on his appointment as Dean of the Sacred College of Cardinals on 26 September 1803. He lived and worked in Frascati for many years, descending each afternoon in his carriage to Rome, where his position as vice-chancellor entitled him to the "Palazzo della Cancelleria".
Henry was the last claimant to the British throne to touch for the King's Evil.
French Revolution and later life.
At the time of the French Revolution, he lost his French Royal benefices and sacrificed many other resources to assist Pope Pius VI. This, in addition to the seizure of his Frascati property by the French, caused him to descend into poverty. The British Minister in Venice arranged for Henry to receive an annuity of £4,000 from King George III of Great Britain. Although the British government represented this as an act of charity, Henry and the Jacobites considered it to be a first installment on the money which was legally owed to him. (For many years the British government had promised to return the English dowry of his grandmother, Mary of Modena, but never did so.)
Henry returned to Frascati in 1803. In September of that year he became the Dean of the College of Cardinals and hence Cardinal Bishop of Ostia and Velletri, though he still lived in the episcopal palace at Frascati. He died there on 13 July 1807, aged 82.
Personal relationships.
Historians have drawn upon contemporary perceptions to explore the suggestion that Henry was homosexual. These accounts include the writings of Hester Lynch Thrale (1741–1821), and the diplomat and writer Giuseppe Gorani (1740–1819). Gorani admitted to having gathered evidence insufficient to confirm his suspicions either way, but drew attention to the number of handsome clerics that were to be found in Henry's palace. The historian Andrew Lang alluded to James's comment that his younger son would never marry although many marriages had been planned for him.
The writer Gaetano Moroni provides the lengthiest account of Henry's close attachment with his majordomo Monsignor Giovanni Lercari (1722–1802), whom Henry was said to have "loved beyond measure". This closeness led to serious tensions between the cardinal and his father who in 1752 eventually tried to have Lercari dismissed from service and sent from Rome. Henry reacted by attempting to separate his household finances from those of his father, and refused himself to return to Rome from Bologna without Lercari by his side. A public scandal was only narrowly avoided after the personal intervention of Pope Benedict XIV who acted as peace-maker. It was agreed that Lercari would leave the household and due to Henry's influence was later made Archbishop of Genoa.
Things became easier after the death of James in 1766. From 1769 onwards Henry remained close to Monsignor Angelo Cesarini, a nobleman from Perugia, who thanks to Henry's protection, won various honours, was made canon of the cathedral in Frascati, and finally in 1801 became Bishop of Milevi. When Henry died, Cesarini was still at his side, as he had been for 32 years. Cesarini was later buried in the church of Santa Maria in Vallicella.
Caution should nevertheless be given against assuming any active sexual relationships, even if they may have had a romantic element, because equally clear in contemporary sources is York's horror of all impropriety.
Post mortem.
Under his will, which he signed as "Henry R", he was succeeded in all his claimed British rights by his nearest blood-relative and friend, Charles Emmanuel IV of Sardinia. Charles neither asserted nor renounced his Jacobite claims, nor have any of his successors to this day. Charles Emmanuel and the following Kings Sardinia - eventually, Kings of Italy - had other vital interests in their Italian environment, to which championing a hopeless cause in Britain would have been detrimental. 
Contrary to popular belief, he did not leave the Crown Jewels to the Prince of Wales, afterwards George IV of the United Kingdom. All his property was entrusted to Monsignor Angelo Cesarini, for distribution. Cesarini sent the Prince of Wales several jewels from Henry's private collection. These included a "Lesser George" (thought to have been worn by Charles I at his execution, and now at Windsor Castle) and a St Andrew's Cross (now at Edinburgh Castle), which are insignia of the orders of the Garter and the Thistle, and also a ruby ring. Even so, the act of sending them to the Hanoverian Prince of Wales constituted en effective, even if not formal, renounciation of the Jacobite claim. 
Henry Benedict, his brother, his father and his mother are buried in the crypt of St. Peter's Basilica in the Vatican. There is a monument to the Royal Stuarts on one of the columns in the basilica proper, designed by Antonio Canova. The monument was originally commissioned by Monsignor Angelo Cesarini, executor of Henry Benedict's estate. Among the subscribers, curiously, was King George IV, who (once the Jacobite threat to his throne had ended with Henry Benedict's death) was an admirer of the Stuart legend.
The monument was restored within living memory, at the expense of the late Queen Elizabeth the Queen Mother.
Titles, styles, honours and arms.
Titles as cardinal.
During his life, Cardinal Stuart was assigned the following "Diaconia" and "Tituli":
In March 1774 he became Sub-dean, and on 15 September 1803 – Dean of the Sacred College of Cardinals.
He was a cardinal elector in the papal conclaves of 1758, 1769, 1774-75 and 1799–1800.
Arms.
During the pretence of his father and brother, Henry claimed a coat of arms consisting of those of the kingdom, differenced by a "crescent argent" or white crescent.

</doc>
<doc id="62778" url="https://en.wikipedia.org/wiki?curid=62778" title="Austrasia">
Austrasia

Austrasia is a territory which formed the northeastern section of the Merovingian Kingdom of the Franks during the 6th to 8th centuries.
Austrasia, centred on the Middle Rhine and the Moselle rivers, was the original territory of the Frankish tribes prior to their unification under Clovis I. In AD 567, it became a separate kingdom within the Frankish Empire and was ruled by Sigebert I. It eventually lost its territorial character in the Carolingian Empire in the 8th century.
Name.
The name "Austrasia" is not well attested in the Merovingian period. It is a latinisation of an Old Frankish name recorded first by Gregory of Tours in c. AD 580 and then by Aimoin of Fleury in c. AD 1000. As with the name "Austria", it contains the word for "east", i.e. meaning "eastern land" to designate the original territory of the Franks in contrast to Neustria, the "new western land" in northern Gaul conquered in the wake of the Battle of Soissons of 486.
Geography.
The area of Austrasia was centred on the Middle Rhine and included the basins the Moselle, Main and Meuse rivers. It bordered on Frisia and Saxony to the north, Thuringia to the east, Swabia and Burgundy to the south and to Neustria and Flanders to the west.
Metz served as the Austrasian capital, although some Austrasian kings ruled from Reims, Trier, and Cologne. Other important cities included Verdun, Worms and Speyer. Fulda monastery was founded in eastern Austrasia in the final decade of the Merovingian period. 
In the High Middle Ages, its territory became divided among the duchies of Lotharingia and Franconia in Germany, with its some western portions including Reims and Rethel passing to France.
Its exact boundaries were somewhat fluid over the history of the Frankish sub-kingdoms, but Austrasia can be taken to correspond roughly to the territory of present-day Luxembourg, parts of eastern Belgium, north-eastern France (Lorraine and Champagne-Ardenne), west-central Germany (the Rhineland, Hesse and Franconia) and the southern Netherlands (Limburg, North Brabant, with a salient north of the Rhine including Utrecht and parts of Gelderland).
History.
After the death of the Frankish king Clovis I in 511, his four sons partitioned his kingdom amongst themselves, with Theuderic I receiving the lands that were to become Austrasia. Descended from Theuderic, a line of kings ruled Austrasia until 555, when it was united with the other Frankish kingdoms of Chlothar I, who inherited all the Frankish realms by 558. He redivided the Frankish territory amongst his four sons, but the four kingdoms coalesced into three on the death of Charibert I in 567: Austrasia under Sigebert I, Neustria under Chilperic I, and Burgundy under Guntram. These three kingdoms defined the political division of Francia until the rise of the Carolingians and even thereafter.
From 567 to the death of Sigbert II in 613, Neustria and Austrasia fought each other almost constantly, with Burgundy playing the peacemaker between them. These struggles reached their climax in the wars between Brunhilda and Fredegund, queens respectively of Austrasia and Neustria. Finally, in 613, a rebellion by the nobility against Brunhilda saw her betrayed and handed over to her nephew and foe in Neustria, Chlothar II. Chlothar then took control of the other two kingdoms and set up a united Frankish kingdom with its capital in Paris. During this period the first "majores domus" or mayors of the palace appeared. These officials acted as mediators between king and people in each realm. The first Austrasian mayors came from the Pippinid family, which experienced a slow but steady ascent until it eventually displaced the Merovingians on the throne.
In 623, the Austrasians asked Chlothar II for a king of their own and he appointed his son Dagobert I to rule over them with Pepin of Landen as regent. Dagobert's government in Austrasia was widely admired. In 629, he inherited Neustria and Burgundy. Austrasia was again neglected until, in 633, the people demanded the king's son as their own king again. Dagobert complied and sent his elder son Sigebert III to Austrasia. Historians often categorise Sigebert as the first "roi fainéant" or do-nothing king of the Merovingian dynasty. His court was dominated by the mayors. In 657, the mayor Grimoald the Elder succeeded in putting his son Childebert the Adopted on the throne, where he remained until 662. Thereafter, Austrasia was predominantly the kingdom of the Arnulfing mayors of the palace and their base of power. With the Battle of Tertry in 687, Pepin of Heristal defeated the Neustrian king Theuderic III and established his mayoralty over all the Frankish kingdoms. This was even regarded by contemporaries as the beginning of his "reign". It also signalled the dominance of Austrasia over Neustria, which would last until the end of the Merovingian era.
In 718, Karl Martel, with Austrasian support in his war against Neustria – each territory struggling to unite Francia under their hegemony – appointed Chlothar IV to rule in Austrasia. This was the last Frankish ruler who did not rule over all the Franks. In 719, Francia was united permanently under Austrasian hegemony.
Under the Carolingians and subsequently, Austrasia is sometimes used as a denominator for the east of their realm, the Carolingian Empire. It has been used as a synonym for East Francia, though this is somewhat inaccurate.

</doc>
<doc id="62781" url="https://en.wikipedia.org/wiki?curid=62781" title="Complete category">
Complete category

In mathematics, a complete category is a category in which all small limits exist. That is, a category "C" is complete if every diagram "F" : "J" → "C" where "J" is small has a limit in "C". Dually, a cocomplete category is one in which all small colimits exist. A bicomplete category is a category which is both complete and cocomplete.
The existence of "all" limits (even when "J" is a proper class) is too strong to be practically relevant. Any category with this property is necessarily a thin category: for any two objects there can be at most one morphism from one object to the other.
A weaker form of completeness is that of finite completeness. A category is finitely complete if all finite limits exists (i.e. limits of diagrams indexed by a finite category "J"). Dually, a category is finitely cocomplete if all finite colimits exist.
Theorems.
It follows from the existence theorem for limits that a category is complete if and only if it has equalizers (of all pairs of morphisms) and all (small) products. Since equalizers may be constructed from pullbacks and binary products (consider the pullback of ("f", "g") along the diagonal Δ), a category is complete if and only if it has pullbacks and products.
Dually, a category is cocomplete if and only if it has coequalizers and all (small) coproducts, or, equivalently, pushouts and coproducts.
Finite completeness can be characterized in several ways. For a category "C", the following are all equivalent:
The dual statements are also equivalent.
A small category "C" is complete if and only if it is cocomplete. A small complete category is necessarily thin.
A posetal category vacuously has all equalizers and coequalizers, whence it is (finitely) complete if and only if it has all (finite) products, and dually for cocompleteness. Without the finiteness restriction a posetal category with all products is automatically cocomplete, and dually, by a theorem about complete lattices.

</doc>
<doc id="62784" url="https://en.wikipedia.org/wiki?curid=62784" title="Soybean">
Soybean

The soybean in North America, also called the soya bean ("Glycine max"), is a species of legume native to East Asia, widely grown for its edible bean which has numerous uses. The plant is classed as an oilseed rather than a pulse by the UN Food and Agriculture Organization (FAO).
Fat-free (defatted) soybean meal is a significant and cheap source of protein for animal feeds and many packaged meals; soy vegetable oil is another product of processing the soybean crop. For example, soybean products such as textured vegetable protein (TVP) are ingredients in many meat and dairy analogues. Soybeans produce significantly more protein per acre than most other uses of land.
Traditional nonfermented food uses of soybeans include soy milk, from which tofu and tofu skin are made. Fermented foods include soy sauce, fermented bean paste, natto, and tempeh, among others. The oil is used in many industrial applications. The main producers of soy are the United States (36%), Brazil (36%), Argentina (18%), China (5%) and India (4%). The beans contain significant amounts of phytic acid, alpha-linolenic acid, and isoflavones.
Name.
The plant is known as the "large bean" in Chinese, Japanese, and Korean (; Japanese romaji: "daizu"; Korean romaja: "daedu") or "yellow bean" (). Both the immature soybean and its dish are called "edamame" in Japan, but in English, "edamame" refers only to a specific dish. The genus name, "Glycine", is the same as a simple amino acid.
Since the early twentieth century soybeans have been called the 'golden bean' or 'miracle bean' in America. The English words "soy" and "soya" are ultimately derived from the Japanese pronunciation of , the Sino-Japanese word for soy sauce, through the German adaptation of the same word, "soja".
Classification.
The genus "Glycine" Willd. is divided into two subgenera, "Glycine" and "Soja". The subgenus "Soja" (Moench) F.J. Herm. includes the cultivated soybean, "Glycine max" (L.) Merr., and the wild soybean, "Glycine soja" Sieb. & Zucc. Both species are annuals. "Glycine soja" is the wild ancestor of "Glycine max", and grows wild in China, Japan, Korea, Taiwan and Russia. The subgenus "Glycine" consists of at least 25 wild perennial species: for example, "Glycine canescens" F.J. Herm. and "G. tomentella" Hayata, both found in Australia and Papua New Guinea. Perennial soybean ("Neonotonia wightii") originated in Africa and is now a widespread pasture crop in the tropics.
Like some other crops of long domestication, the relationship of the modern soybean to wild-growing species can no longer be traced with any degree of certainty. It is a cultural variety with a very large number of cultivars.
Description.
Like most plants, soybeans grow in distinct morphological stages as they develop from seeds into fully mature plants.
Germination.
The first of stage of growth is germination, a process that first becomes apparent as a seed's radicle emerges. This is the first stage of root growth and occurs within the first 48 hours under ideal growing conditions. The first photosynthetic structures, the cotyledons, develop from the hypocotyl, the first plant structure to emerge from the soil. These cotyledons both act as leaves and as a source of nutrients for the immature plant, providing the seedling nutrition for its first 7 to 10 days.
Maturation.
The first true leaves develop as a pair of single blades. Subsequent to this first pair, mature nodes form compound leaves with three blades. Mature trifoliolate leaves, having three to four leaflets per leaf, are often between long and broad. Under ideal conditions, stem growth continues, producing new nodes every four days. Before flowering, roots can grow 1.9 cm (0.75 inch) per day. If rhizobia are present, root nodulation begins by the time the third node appears. Nodulation typically continues for 8 weeks before the symbiotic infection process stabilizes. The final characteristics of a soybean plant are variable, with factors such as genetics and climate affecting its form; however, the plant rarely exceeds in height.
Flowering.
Flowering is triggered by day length, often beginning once days become shorter than 12.8 hours. This trait is highly variable however, with different varieties reacting differently to changing day length. Soybeans form inconspicuous, self-fertile flowers which are borne in the axil of the leaf and are white, pink or purple. Depending of the soybean variety, node growth may cease once flowering begins. Strains that continue nodal development after flowering are termed "indeterminates" and are best suited to climates with longer growing seasons. Often soybeans drop their leaves before the seeds are fully mature.
The fruit is a hairy pod that grows in clusters of three to five, each pod is 3–8 cm long (1–3 in) and usually contains two to four (rarely more) seeds 5–11 mm in diameter. Soybeans occur in various sizes, and in many hull or seed coat colors, including black, brown, blue, yellow, green and mottled.
Seed resilience.
The hull of the mature bean is hard, water-resistant, and protects the cotyledon and hypocotyl (or "germ") from damage. If the seed coat is cracked, the seed will not germinate. The scar, visible on the seed coat, is called the hilum (colors include black, brown, buff, gray and yellow) and at one end of the hilum is the micropyle, or small opening in the seed coat which can allow the absorption of water for sprouting.
Remarkably, seeds such as soybeans containing very high levels of protein can undergo desiccation, yet survive and revive after water absorption. A. Carl Leopold, son of Aldo Leopold, began studying this capability at the Boyce Thompson Institute for Plant Research at Cornell University in the mid-1980s. He found soybeans and corn to have a range of soluble carbohydrates protecting the seed's cell viability. Patents were awarded to him in the early 1990s on techniques for protecting "biological membranes" and proteins in the dry state.
Nitrogen-fixing ability.
Many legumes (alfalfa, clover, lupins, peas, beans, lentils, soybeans, peanuts and others) contain symbiotic bacteria called "Rhizobia" within nodules of their root systems. These bacteria have the special ability of fixing nitrogen from atmospheric, molecular nitrogen (N2) into ammonia (NH3). The chemical reaction is:
Ammonia is then converted to another form, ammonium (NH4+), usable by (some) plants by the following reaction:
This arrangement means that the root nodules are sources of nitrogen for legumes, making them relatively rich in plant proteins.
Chemical composition.
Together, protein and soybean oil content account for 56% of dry soybeans by weight (36% protein and 20% fat, table). The remainder consists of 30% carbohydrates, 9% water and 5% ash (table). Soybeans comprise approximately 8% seed coat or hull, 90% cotyledons and 2% hypocotyl axis or germ.
Nutrition.
Soybeans are an exceptional source of essential nutrients, providing in a 100 gram serving (raw, for reference) high contents of the Daily Value (DV) especially for protein (36% DV), dietary fiber (37%), iron (121%), manganese (120%), phosphorus (101%) and several B vitamins, including folate (94%) (table). High contents also exist for vitamin K, magnesium, zinc and potassium (table).
A 100 gram serving of soybeans supplies 446 calories and 11 grams of polyunsaturated fat (table).
For human consumption, soybeans must be cooked with "wet" heat to destroy the trypsin inhibitors (serine protease inhibitors). Raw soybeans, including the immature green form, are toxic to all monogastric animals.
Protein.
Most soy protein is a relatively heat-stable storage protein. This heat stability enables soy food products requiring high temperature cooking, such as tofu, soy milk and textured vegetable protein (soy flour) to be made.
Soybeans are considered by many agencies to be a source of complete protein. A complete protein is one that contains significant amounts of all the essential amino acids that must be provided to the human body because of the body's inability to synthesize them. For this reason, soy is a good source of protein, amongst many others, for vegetarians and vegans or for people who want to reduce the amount of meat they eat. According to the US Food and Drug Administration:
The gold standard for measuring protein quality, since 1990, is the Protein Digestibility Corrected Amino Acid Score (PDCAAS) and by this criterion soy protein is the nutritional equivalent of meat, eggs, and casein for human growth and health. Soybean protein isolate has a biological value of 74, whole soybeans 96, soybean milk 91, and eggs 97.
Soy protein is essentially identical to the protein of other legume seeds and pulses. Moreover, soybeans can produce at least twice as much protein per acre than any other major vegetable or grain crop besides hemp, five to 10 times more protein per acre than land set aside for grazing animals to make milk, and up to 15 times more protein per acre than land set aside for meat production.
Carbohydrates.
The principal soluble carbohydrates of mature soybeans are the disaccharide sucrose (range 2.5–8.2%), the trisaccharide raffinose (0.1–1.0%) composed of one sucrose molecule connected to one molecule of galactose, and the tetrasaccharide stachyose (1.4 to 4.1%) composed of one sucrose connected to two molecules of galactose. While the oligosaccharides raffinose and stachyose protect the viability of the soybean seed from desiccation (see above section on physical characteristics) they are not digestible sugars, so contribute to flatulence and abdominal discomfort in humans and other monogastric animals, comparable to the disaccharide trehalose. Undigested oligosaccharides are broken down in the intestine by native microbes, producing gases such as carbon dioxide, hydrogen, and methane.
Since soluble soy carbohydrates are found in the whey and are broken down during fermentation, soy concentrate, soy protein isolates, tofu, soy sauce, and sprouted soybeans are without flatus activity. On the other hand, there may be some beneficial effects to ingesting oligosaccharides such as raffinose and stachyose, namely, encouraging indigenous bifidobacteria in the colon against putrefactive bacteria.
The insoluble carbohydrates in soybeans consist of the complex polysaccharides cellulose, hemicellulose, and pectin. The majority of soybean carbohydrates can be classed as belonging to dietary fiber.
Fats.
Raw soybeans are 20% fat, including saturated fat (3%), monounsaturated fat (4%) and polyunsaturated fat, mainly as linoleic acid (table).
Within soybean oil or the lipid portion of the seed is contained four phytosterols: stigmasterol, sitosterol, campesterol, and brassicasterol accounting for about 2.5% of the lipid fraction; and which can be converted into steroid hormones.
Comparison to other major staple foods.
The following table shows the nutrient content of green soybean and other major staple foods, each in respective raw form. Raw soybeans, however, aren't edible and cannot be digested. These must be sprouted, or prepared and cooked for human consumption. In sprouted and cooked form, the relative nutritional and anti-nutritional contents of each of these grains is remarkably different from that of raw form of these grains reported in this table. The nutritional value of soybean and each cooked staple depends on the processing and the method of cooking: boiling, frying, roasting, baking, etc.
Soy protein.
All spermatophytes except for the grass/cereal family contain soybean-like 7S (vicilin) and/or 11S (legumin), {S denotes Svedberg, sedimentation coefficients} desiccation-tolerant seed storage globulin proteins. Oats and rice are anomalous in that they also contain a majority of soybean-like protein. Cocoa, for example, contains the 7S globulin, which contributes to cocoa/chocolate taste and aroma;, whereas coffee beans (coffee grounds) contain the 11S globulin responsible for coffee's aroma and flavor.
Vicilin and legumin proteins belong to the cupin superfamily, a large and functionally diverse 'superfamily' of proteins that have a common origin and whose evolution can be followed from bacteria to eukaryotes including animals and higher plants.
2S albumins form a major group of homologous storage proteins in many dicot species and in some monocots but not in grasses (cereals). Soybeans contain a small but significant 2S storage protein. 2S albumin are grouped in the prolamin superfamily. Other allergenic proteins included in this 'superfamily' are the non-specific plant lipid transfer proteins, alpha amylase inhibitor, trypsin inhibitors, and prolamin storage proteins of cereals and grasses.
Peanuts, for instance, contain 20% 2S albumin but only 6% 7S globulin and 74% 11S. It is the high 2S albumin and low 7S globulin that is responsible for the relatively low lysine content of peanut protein compared to soy protein.
Cultivation.
Soybeans are a globally important crop, providing oil and protein. In the United States, the bulk of the harvest is solvent-extracted with hexane, and the "toasted" defatted soymeal (50% protein) then makes possible the raising of farm animals (e.g. chicken, hog, turkey) on an industrial scale never before seen in human history. A very small proportion of the crop is consumed directly by humans. Soybean products do, however, appear in a large variety of processed foods.
During World War II, soybeans became important in both North America and Europe chiefly as substitutes for other protein foods and as a source of edible oil. During the war, the soybean was discovered as fertilizer by the United States Department of Agriculture. In the 1960–1 Dillon round of the General Agreement on Tariffs and Trade (GATT), the United States secured tariff-free access for its soybeans to the European market. In the 1960s, the United States exported over 90% of the world's soybeans. By 2005, the top soybeans exporters were Argentina (39% of world soybean exports), United States (37%) and Brazil (16%), while top importers were China (41% of world soybean imports), European Union (22%), Japan (6%) and Mexico (6%).
Cultivation is successful in climates with hot summers, with optimum growing conditions in mean temperatures of ; temperatures of below 20 °C and over 40 °C (68 °F, 104 °F) stunt growth significantly. They can grow in a wide range of soils, with optimum growth in moist alluvial soils with a good organic content. Soybeans, like most legumes, perform nitrogen fixation by establishing a symbiotic relationship with the bacterium "Bradyrhizobium japonicum" (syn. "Rhizobium japonicum"; Jordan 1982). For best results, though, an inoculum of the correct strain of bacteria should be mixed with the soybean (or any legume) seed before planting. Modern crop cultivars generally reach a height of around , and take 80–120 days from sowing to harvesting.
The U.S., Argentina, Brazil, China and India are the world's largest soybean producers and represent more than 90% of global soybean production. The U.S. produced 75 million tons of soybeans in 2000, of which more than one-third was exported. In the 2010–2011 production year, this figure is expected to be over 90 million tons.
The average worldwide yield for soybean crops, in 2010, was 2.5 tonnes per hectare. The three largest producers had an average nationwide soybean crop yields of about 3 tonnes per hectare. The most productive soybean farms in the world in 2010 were in Turkey, with a nationwide average farm yield of 3.7 tonnes per hectare. The world record for soybean yield is 10.8 tonnes per hectare, demonstrated in 2010 by Kip Cullers, a farmer in Purdy, Missouri. Kip Cullers claims the secret to his record breaking soybean crop yields year after year is attention to detail, proactive management style, irrigation, herbicides, keeping plants healthy and stress free for the entire growing season.
Environmental groups, such as Greenpeace and the WWF, have reported soybean cultivation and the probability of increased soybean cultivation in Brazil has destroyed huge areas of Amazon rainforest, and is encouraging further deforestation.
American soil scientist Andrew McClung, who first showed that the ecologically biodiverse savannah of the Cerrado region of Brazil could grow profitable soybeans, was awarded the 2006 World Food Prize on October 19, 2006. However, even correcting for poor soils soybeans were an unlikely cash crop for the Cerrado. Soy did not fare well in the low latitudes. More than the heat and humidity, it was a lack of seasons that hampered production. In the higher more northerly latitudes, flowering coincides with the summer solstice, when the plants reach their maximum height. The first soybeans planted in the Cerrado, however, flowered early and, deprived of long summer days, remained stunted. For soy agriculture to take root in Mato Grosso it was first necessary to develop a "tropical soybean"—one that would flower later, giving the plants more time to fully mature. The feat was accomplished after years of laborious crossbreeding by scientists within Embrapa, the respected research arm of the Brazilian Ministry of Agriculture.
Human sewage sludge can be used as fertilizer to grow soybeans. Soybeans grown in sewage sludge likely contain elevated concentrations of metals. Soybean plants are vulnerable to a wide range of bacterial diseases, fungal diseases, viral diseases and parasites. One important pest is the corn earworm moth, which is the most common and destructive pest of soybean growth in Virginia.
History.
Soybeans were a crucial crop in East Asia long before written records began. There is evidence for soybean domestication between 9000 and 8600 BP (before present) in China, between 7000 BP and 5000 BP in Japan and 3000 BP in Korea. They are now a major crop in the United States, Brazil, Argentina, India, and China. Prior to fermented products such as fermented black soybeans ("douchi"), "jiang" (Chinese miso), soy sauce, tempeh, natto, and miso, soy was considered sacred for its beneficial effects in crop rotation. Soy was introduced to Africa from China in the late 19th century, and is now widespread across the continent.
Asia.
The closest living relative of the soybean is "Glycine soja" (previously called "G. ussuriensis"), a legume native to central China.
According to the ancient Chinese myth, in 2853 BCE, the legendary Emperor Shennong of China proclaimed that five plants were sacred: soybeans, rice, wheat, barley, and millet.
Cultivation of soybeans took place over long periods of time in the prehistory of modern-day Japan, Korea and Northern China, based on archaeological evidence.
The origin of soy bean cultivation remains scientifically debated. Recent research indicates that seeding of wild forms started early (before 5000 BCE) in multiple locations through China, Korea and Japan Great Soviet Encyclopedia records soybean cultivation originated in China about 5000 years ago. Some scholars suggest that soybean originated in China and was domesticated about 3500 BCE. The oldest preserved soybeans resembling modern varieties in size and shape were found in archaeological sites in Korea dated about 1000 BCE Radiocarbon dating of soybean samples recovered through flotation during excavations at the Early Mumun period Okbang site in Korea indicated soybean was cultivated as a food crop in around 1000–900 BCE. Soy bean from the Jomon period in Japan from 3000 BCE are also significantly larger than wild varieties. The cultivation of soybeans began in the eastern half of northern China by 2000 BCE, but is almost certainly much older.
From about the first century CE to the Age of Discovery (15–16th century), soybeans were introduced into several countries, such as India, Japan, Indonesia, the Philippines, Vietnam, Thailand, Cambodia, Malaysia, Burma and Nepal. This spread was due to the establishment of sea and land trade routes. The earliest Japanese textual reference to the soybean is in the classic "Kojiki" (Records of Ancient Matters), which was completed in 712 CE.
Many people have claimed soybeans in Asia were historically only used after a fermentation process, which lowers the high phytoestrogens content found in the raw plant. However, terms similar to "soy milk" have been in use since 82 CE, and there is evidence of tofu consumption that dates to 220.
North America.
Soybeans were first introduced to North America from China in 1765, by Samuel Bowen, a former East India Company sailor who had visited China in conjunction with James Flint, the first Englishman legally permitted by the Chinese authorities to learn Chinese. The first 'New World' soybean crop was grown on Skidaway Island, Georgia in 1765 by Henry Yonge from seeds given him by Samuel Bowen. Bowen grew soy near Savannah, Georgia, possibly using funds from Flint, and made soy sauce for sale to England. Although, soybean was introduced into North America in 1765,for the next 155 years, the crop was grown primarily for forage.
It wasn't until Lafayette Mendel and Thomas Burr Osborne (chemist) showed that the nutritional value of soybean seeds could be greatly increased by cooking, wet heat, that soy went from merely a forage to a farm animal feed and became much more appreciated as a human food.
William Morse is considered the 'father' of modern soybean agriculture in America. He and Charles Piper, Dr. C.V. Piper, took what was an unknown Oriental peasant crop in 1910 and transformed it into a 'golden bean' for America becoming one of America's largest farm crops and its most nutritious.
Prior to the 1920 in the USA, the soybean was mainly a forage crop, a source of oil, meal (for feed) and industrial products, with very little used as food. However, it took on an important role after World War I. During the Great Depression, the drought-stricken (Dust Bowl) regions of the United States were able to use soy to regenerate their soil because of its nitrogen-fixing properties. Farms were increasing production to meet with government demands, and Henry Ford became a great leader in the soybean industry.
In 1931, Ford hired chemists Robert Boyer and Frank Calvert to produce artificial silk. They succeeded in making a textile fiber of spun soy protein fibers, hardened or tanned in a formaldehyde bath, which was given the name Azlon. It was usable in the making of suits, felt hats, and overcoats. Though pilot production of Azlon reached 5000 pounds per day in 1940, it never reached the commercial market; Dupont's nylon was the winner in the quest to produce artificial silk. In 1932–33, the Ford Motor Company spent approximately $1,250,000 on soybean research. By 1935, every Ford car had soy involved in its manufacture. For example, soybean oil was used to paint the automobiles, as well as fluid for shock absorbers. Ford's involvement with the soybean opened many doors for agriculture and industry to be linked more strongly than ever before.
Henry Ford promoted the soybean, helping to develop uses for it both in food and in industrial products, even demonstrating auto body panels made of soy-based plastics. Ford's interest led to two bushels (120 pounds) of soybeans being used in each Ford car, as well as products like the first commercial soy milk, ice cream and all-vegetable nondairy whipped topping. The Ford development of so-called soy-based plastics was based on the addition of soybean flour and wood flour to phenol formaldehyde plastics. A prototype vehicle, colloquially titled the "Soybean Car", was built in 1941 out of such plastics.
South America.
The soybean first arrived in South America in Argentina in 1882.
Andrew McClung showed in the early 1950s that with soil amendments the Cerrado region of Brazil would grow soybeans. The march of soybeans into deforested areas of the Amazon rain forest would come later.
Africa.
The soybean first arrived in Africa via Egypt in 1857.
Australia.
Wild soybeans were discovered in northeastern Australia in 1770 by explorers Banks and Solander. In 1804, the first soyfood product ("Fine India Soy" ) was sold in Sydney. In 1879, the first domesticated soybeans arrived in Australia, a gift of the Minister of the Interior Department, Japan.
Canada.
In 1831, the first soy product "a few dozen India Soy" arrived in Canada. Soybeans were probably first cultivated in Canada by 1855, and definitely in 1895 at Ontario Agricultural College.
Caribbean and West Indies.
The soybean arrived in the Caribbean in the form of soy sauce made by Samuel Bowen in Savannah, Georgia, in 1767. It remains only a minor crop there, but its uses for human food are growing steadily.
Central Asia.
The soybean is first in cultivated Transcaucasia in Central Asia in 1876, by the Dungans. This region has never been important for soybean production.
Mexico and Central America.
The first reliable reference to the soybean in this region dates from Mexico in 1877.
Southeast Asia.
By the 13th century, the soybean had arrived in Indonesia; it probably arrived much earlier, carried by traders or merchants from southern China.
South Asia and Indian Subcontinent.
By the 1600s, soy sauce spread from southern Japan across the region through the Dutch East India Company (VOC). The soybean probably arrived from southern China, moving southward into northern India.
Europe.
In 1873, Professor Friedrich J. Haberlandt first became interested in soybeans when he obtained the seeds of 19 soybean varieties at the Vienna World Exposition (Wiener Weltausstellung). He cultivated these seeds in Vienna, and soon began to distribute them throughout Central and Western Europe. Most of the farmers who received seeds from him cultivated them, then reported their results back to him. Starting in Feb. 1876, he published these results first in various journal articles, and finally in his magnus opum, Die Sojabohne ( The Soybean) in 1878. In northern Europe lupin/lupine is known as the "soybean of the north"
A Hitler Youth manual from the 1930s promoted soy beans, which it called "Nazi beans" as an alternative to meat.
Austria and Switzerland.
In 1861, soybeans were first cultivated in Switzerland.
In Austria, at the Vienna World Exposition of 1873, Prof. Friedrich Haberlandt, of the Royal College of Agriculture in Vienna (Wiener Hochschule für Bodenkultur), gathered a number of soybean varieties from the Chinese, Japanese, Mongolian, Transcaucasian and East Indian expositions. In 1875 he first grew the soybeans in Vienna, then in early 1876 he sent samples of seeds to seven cooperators in central Europe, who planted and tested the seeds in the spring of 1876, with good or fairly good results in each case.
France.
The soybean was first cultivated in France by 1779 (and perhaps as early as 1740). The two key early people and organizations introducing the soybean to France were the Society of Acclimatization (starting in 1855) and Li Yu-ying (from 1910). Li started a large tofu factory, where the first commercial soyfoods in France were made.
Greece.
1935 - Soybeans are first introduced to Greece by Anton Brillmayer, an Austrian soybean breeder (Brillmayer. 1947. "Die Kultur der Soja in Oesterreich," p. 14-18).
1939 - By now, soybeans have been cultivated in Greece (Matagrin. 1939. "Le Soja et les Industries du Soja," p. 47-48).
An entire book has been published on the history of soybeans and soyfoods in Greece.
Italy.
The soybean was first cultivated in Italy by 1760 in the Botanical Garden of Turin. During the 1780s it was grown at at least three other botanical gardens in Italy.
Spain and Portugal.
In 1603, "Vocabvlario da Lingoa de Iapam", a famous Japanese-Portuguese dictionary, was compiled and published by Jesuit priests in Nagasaki. It contains short but clear definitions for about 20 words related to soyfoods - the first in any European language.
In 1880, the soybean was first cultivated in Portugal in the Botanical Gardens at Coimbra (Crespi 1935).
In about 1910 in Spain the first attempts at Soybean cultivation were made by the Count of San Bernardo, who cultivated soybeans on his estates at Almillo (in southwest Spain) about 48 miles east-northeast of Seville.
Genetic modification.
Soybeans are one of the "biotech food" crops that have been genetically modified, and genetically modified soybeans are being used in an increasing number of products. In 1995, Monsanto company introduced glyphosate-tolerant soybeans that have been genetically modified to be resistant to Monsanto's glyphosate herbicides through substitution of the "Agrobacterium sp." (strain CP4) gene EPSP (5-enolpyruvyl shikimic acid-3-phosphate) synthase. The substituted version is not sensitive to glyphosate.
In 1997, about 8% of all soybeans cultivated for the commercial market in the United States were genetically modified. In 2010, the figure was 93%. As with other glyphosate-tolerant crops, concern is expressed over damage to biodiversity. A 2003 study concluded the RR gene had been bred into so many different soybean cultivars, there had been little decline in genetic diversity, but "diversity was limited among elite lines from some companies".
The widespread use of such types of GM soybeans in the Americas has caused problems with exports to some regions. GM crops require extensive certification before they can be legally imported into the European Union, where there is considerable supplier and consumer reluctance to use GM products for consumer or animal use. Difficulties with coexistence and subsequent traces of cross-contamination of non-GM stocks have caused shipments to be rejected and have put a premium on non-GM soy.
A 2006 United States Department of Agriculture report found the adoption of genetically engineered (GE) soy, corn and cotton reduced the amount of pesticides used overall, but did result in a slightly greater amount of herbicides used for soy specifically. The use of GE soy was also associated with greater conservation tillage, indirectly leading to better soil conservation, as well as increased income from off-farming sources due to the greater ease with which the crops can be managed. Though the overall estimated benefits of the adoption of GE soybeans in the United States was $310 million, the majority of this benefit was experienced by the companies selling the seeds (40%), followed by biotechnology firms (28%) and farmers (20%). The patent on glyphosate-tolerant soybeans expired in 2014, so benefits can be expected to shift.
In 2010, a team of American scientists announced they had sequenced the soybean genome – the first legume to be sequenced.
Uses.
Among the legumes, the soybean is valued for its high (38–45%) protein content as well as its high (approximately 20%) oil content. Soybeans are the second-most valuable agricultural export in the United States, behind corn. Approximately 85% of the world's soybean crop is processed into soybean meal and soybean oil, the remainder processed in other ways or eaten whole.
Soybeans can be broadly classified as "vegetable" (garden) or field (oil) types. Vegetable types cook more easily, have a mild, nutty flavor, better texture, are larger in size, higher in protein, and lower in oil than field types. Tofu and soy milk producers prefer the higher protein cultivars bred from vegetable soybeans originally brought to the United States in the late 1930s. The "garden" cultivars are generally not suitable for mechanical combine harvesting because there is a tendency for the pods to shatter upon reaching maturity.
Soybean oil.
Soybean seed contains 18-19% oil. To extract soybean oil from seed, the soybeans are cracked, adjusted for moisture content, rolled into flakes and solvent-extracted with commercial hexane. The oil is then refined, blended for different applications, and sometimes hydrogenated. Soybean oils, both liquid and partially hydrogenated, are exported abroad, sold as "vegetable oil", or end up in a wide variety of processed foods.
Soybean meal.
Soybean meal, or soymeal, is the material remaining after solvent extraction of oil from soybean flakes, with a 50% soy protein content. The meal is 'toasted' (a misnomer because the heat treatment is with moist steam) and ground in a hammer mill. Ninety-seven percent of soybean meal production globally is used as livestock feed. Soybean meal is also used in some dog foods.
Livestock feed.
One of the major uses of soybeans globally is as livestock feed, predominantly in the form of soybean meal. Spring grasses are rich in omega-3 fatty acids, whereas soy is predominantly omega-6.
Food for human consumption.
In addition to their use in livestock feed, soybean products are widely used for human consumption. Common soybean products include soy sauce, soy milk, tofu, soy meal, soy flour, textured vegetable protein (TVP), tempeh, soy lecithin and soybean oil. Soybeans may also be eaten with minimal processing, for example in the Japanese food , in which immature soybeans are boiled whole in their pods and served with salt.
In China, Japan, and Korea, soybean and soybean products are a common part of the diet. Tofu (豆腐 "dòufu") is thought to have originated in China, along with soy sauce and several varieties of soybean paste used as seasonings. Japanese foods made from soya include "miso" (味噌), "nattō" (納豆), "kinako" (黄粉) and "edamame" (枝豆), as well as products made with tofu such as atsuage and aburaage. In Korean cuisine, soybean sprouts (콩나물 "kongnamul") are used in a variety of dishes, and are the base ingredient in "doenjang", "cheonggukjang" and "ganjang". In Vietnam, soybeans are used to make soybean paste ("tương") in the North with the most popular products are "tương Bần", "tương Nam Đàn", "tương Cự Đà" as a garnish for "phở" and "gỏi cuốn" dishes, as well as tofu ("đậu hũ" or "đậu phụ" or "tàu hũ"), soy sauce ("nước tương"), soy milk ("nước đậu" in the North or "sữa đậu nành" in the South), and "đậu hũ nước đường" (tofu sweet soup).
Flour.
Soy flour refers to soybeans ground finely enough to pass through a 100-mesh or smaller screen where special care was taken during desolventizing (not toasted) to minimize denaturation of the protein to retain a high protein dispersibility index, for uses such as food extrusion of textured vegetable protein. It is the starting material for production of soy concentrate and soy protein isolate.
Soy flour is made by roasting the soybean, removing the coat, and grinding into a flour. Soy flour is manufactured with different fat levels. Alternatively, raw soy flour omits the roasting step.
Soy lecithin can be added (up to 15%) to soy flour to make lecithinated soy flour. It increases dispersibility and gives it emulsifying properties.
Soy flour has 50% protein and 5% fiber. It has higher levels of protein, thiamine, riboflavin, phosphorus, calcium, and iron than wheat flour. It does not contain gluten. As a result, yeast-raised breads made with soy flour are dense in texture. Among many uses, soy flour thickens sauces, prevents staling in baked food, and reduces oil absorption during frying. Baking food with soy flour gives it tenderness, moistness, a rich color, and a fine texture.
Soy grits are similar to soy flour except the soybeans have been toasted and cracked into coarse pieces.
"Kinako" is a soy flour used in Japanese cuisine.
Soy-based infant formula.
Soy-based infant formula (SBIF) is sometimes given to infants who are not being strictly breastfed; it can be useful for infants who are either allergic to pasteurized cow milk proteins or who are being fed a vegan diet. It is sold in powdered, ready-to-feed, and concentrated liquid forms.
Some reviews have expressed the opinion that more research is needed to determine what effect the phytoestrogens in soybeans may have on infants. Diverse studies have concluded there are no adverse effects in human growth, development, or reproduction as a result of the consumption of soy-based infant formula. One of these studies, published in the "Journal of Nutrition", concludes that there are:
... no clinical concerns with respect to nutritional adequacy, sexual development, neurobehavioral development, immune development, or thyroid disease. SBIFs provide complete nutrition that adequately supports normal infant growth and development. FDA has accepted SBIFs as safe for use as the sole source of nutrition.
Meat and dairy alternatives and extenders.
Soybeans can be processed to produce a texture and appearance similar to many other foods. For example, soybeans are the primary ingredient in many dairy product substitutes (e.g., soy milk, margarine, soy ice cream, soy yogurt, soy cheese, and soy cream cheese) and meat alternatives (e.g. veggie burgers). These substitutes are readily available in most supermarkets. Soy milk does not naturally contain significant amounts of digestible calcium. Many manufacturers of soy milk sell calcium-enriched products, as well. Soy is also used in tempeh: the beans (sometimes mixed with grain) are fermented into a solid cake.
Soy products also are used as a low-cost substitute in meat and poultry products. Food service, retail and institutional (primarily school lunch and correctional) facilities regularly use such "extended" products. Extension may result in diminished flavor, but fat and cholesterol are reduced. Vitamin and mineral fortification can be used to make soy products nutritionally equivalent to animal protein; the protein quality is already roughly equivalent. The soy-based meat substitute textured vegetable protein has been used for more than 50 years as a way of inexpensively extending ground beef without reducing its nutritional value.
Soynut butter.
The soybean is used to make a product similar to peanut butter, called SoyNut Butter and with the slogan "I.M. Healthy" printed on the label.
Other products.
Soybeans with black hulls are used in Chinese fermented black beans, "douchi", not to be confused with black turtle beans.
Soybeans are also used in industrial products, including oils, soap, cosmetics, resins, plastics, inks, crayons, solvents, and clothing. Soybean oil is the primary source of biodiesel in the United States, accounting for 80% of domestic biodiesel production. Soybeans have also been used since 2001 as fermenting stock in the manufacture of a brand of vodka. In 1936, Ford Motor Company developed a method where soybeans and fibers were rolled together producing a soup which was then pressed into various parts for their cars, from the distributor cap to knobs on the dash board. Ford also informed in public relation releases that in 1935 over five million acres (20,000 km) was dedicated to growing soybeans in the United States.
Health.
Cancer.
According to the American Cancer Society, "Studies in humans have not shown harm from eating soy foods. Moderate consumption of soy foods appears safe for both breast cancer survivors and the general population, and may even lower breast cancer risk." They caution however that soy supplements should be avoided.
Alpha-linolenic acid.
Soybean oil is one of the vegetable oils that contain a significant amount of the omega-3 fatty acid alpha-linolenic acid (18:3n−3, ALA). Other plant oils containing ALA include canola, walnut, hemp, and flax. Soybean oil has an omega-3:omega-6 ratio of 1:7. This is a significantly higher omega-3 content than in other vegetable cooking oils except for Hemp and flaxseed. While hemp, and flaxseed are both higher with a ratio of 3:1, they are not very practical for cooking. For more information on the health benefits of omega-3 and omega-6 fatty acids, see Essential fatty acids.
Phytochemicals.
Saponins, a class of natural surfactants (soaps), are sterols that are present naturally in a wide variety of food-plants, including vegetables, legumes, and cereals–ranging from beans and spinach to tomatoes, potatoes and oats. Whole soybeans contain from 0.17 to 6.16% saponins, 0.35 to 2.3% in defatted soy flour and 0.06 to 1.9% in tofu. Legumes such as soybean and chickpeas are the major source of saponins in the human diet. Sources of non-dietary saponins include alfalfa, sunflower, herbs and barbasco.
Soy contains isoflavones like genistein and daidzein. It also contains glycitein, an O-methylated isoflavone which accounts for 5–10% of the total isoflavones in soy food products. Glycitein is a phytoestrogen with weak estrogenic activity, comparable to that of the other soy isoflavones.
Isoflavones.
Soybeans also contain the isoflavones genistein and daidzein, types of phytoestrogen.
Soy's content of isoflavones are as much as 3 mg/g dry weight. Isoflavones are polyphenol compounds, produced primarily by beans and other legumes, including peanuts and chickpeas. Isoflavones are closely related to the antioxidant flavonoids found in other plants, vegetables and flowers. Isoflavones such as genistein and daidzein are found in only some plant families, because most plants do not have an enzyme, chalcone isomerase which converts a flavone precursor into an isoflavone.
Glyceollins are molecules belonging to the pterocarpans family. They are also found in the soybean and have been found to have an antifungal activity against "Aspergillus sojae", the fungal ferment used to produce soy sauce. They are phytoalexins with an antiestrogenic activity.
Cholesterol and heart diseases.
The dramatic increase in soyfood sales is largely credited to the Food and Drug Administration's (FDA) approval of soy as a cholesterol-lowering food, along with other heart and health benefits.
One review concluded that soy protein is correlated with significant decreases in serum cholesterol, LDL (bad cholesterol) and triglycerides. However, HDL (good cholesterol) did not increase by a significant amount. Soy phytoestrogens (isoflavones: genistein and daidzein) adsorbed onto the soy protein were suggested as the agent reducing serum cholesterol levels.
The FDA granted the following health claim for soy: "25 grams of soy protein a day, as part of a diet low in saturated fat and cholesterol, may reduce the risk of heart disease." One serving, (1 cup or 240 mL) of soy milk, for instance, contains 6 or 7 grams of soy protein. Solae resubmitted their original petition, asking for a more vague health claim, after their original was challenged and highly criticized. Solae also submitted a petition for a health claim that soy can help prevent cancer. They quickly withdrew the petition for lack of evidence and after more than 1,000 letters of protest were received. On February 18, 2008 Weston A. Price Foundation submitted a petition for removal of this health claim. 25 g/day soy protein was established as the threshold intake because most trials used at least this much protein and not because less than this amount is inefficacious. In fact, there is evidence suggesting that lower amounts are indeed efficacious.
An American Heart Association (AHA) review of a decade long study of soy protein benefits casts doubt on the FDA allowed "Heart Healthy" claim for soy protein and does not recommend isoflavone supplementation. The review panel also found that soy isoflavones have not been shown to reduce post-menopausal "hot flashes" and the efficacy and safety of isoflavones to help prevent cancers of the breast, uterus or prostate is in question. However, AHA concludes that "many soy products should be beneficial to cardiovascular and overall health because of their high content of polyunsaturated fats, fiber, vitamins, and minerals and low content of saturated fat".
The AHA did not conduct a formal statistical analysis of the 22 studies upon which they based their estimate of the potency of soy protein. When such an analysis was conducted, Jenkins et al. found that the AHA had considerably underestimated the hypocholesterolemic effects of soy protein. Further, when the analysis was limited to the 11 studies that provided evidence that the control and soy diets were matched, soy protein was found to lower LDL by 5.2 percent. This estimate is in line with the results of other recently published meta-analyses. Furthermore, recent research suggests that soy protein decreases postprandial triglyceride levels, which is increasingly viewed as important for reducing coronary heart disease risk.
Phytic acid.
Soybeans contain a high level of phytic acid, which has many effects including acting as an antioxidant and a chelating agent. The beneficial claims for phytic acid include reducing cancer, minimizing diabetes, and reducing inflammation. However, phytic acid is also criticized for reducing vital minerals due to its chelating effect, especially for diets already low in minerals.
Health risks.
Allergy.
Allergy to soy is common, and the food is listed with other foods that commonly cause allergy, such as milk, eggs, peanuts, tree nuts, shellfish. The problem has been reported among younger children, and the diagnosis of soy allergy is often based on symptoms reported by parents and results of skin tests or blood tests for allergy. Only a few reported studies have attempted to confirm allergy to soy by direct challenge with the food under controlled conditions. It is very difficult to give a reliable estimate of the true prevalence of soy allergy in the general population. To the extent that it does exist, soy allergy may cause cases of urticaria and angioedema, usually within minutes to hours of ingestion. In rare cases, true anaphylaxis may also occur. The reason for the discrepancy is likely that soy proteins, the causative factor in allergy, are far less potent at triggering allergy symptoms than the proteins of peanut and shellfish. An allergy test that is positive demonstrates that the immune system has formed IgE antibodies to soy proteins. However, this is only a factor when soy proteins reach the blood without being digested, in sufficient quantities to reach a threshold to provoke actual symptoms.
Soy can also trigger symptoms via food intolerance, a situation where no allergic mechanism can be proven. One scenario is seen in very young infants who have vomiting and diarrhoea when fed soy-based formula, which resolves when the formula is withdrawn. Older infants can suffer a more severe disorder with vomiting, diarrhoea that may be bloody, anemia, weight loss and failure to thrive. The most common cause of this unusual disorder is a sensitivity to cow's milk, but soy formulas can also be the trigger. The precise mechanism is unclear and it could be immunologic, although not through the IgE-type antibodies that have the leading role in urticaria and anaphylaxis. However it is also self-limiting and will often disappear in the toddler years.
Phytoestrogen.
Soybeans contain isoflavones called genistein and daidzein, which are one source of phytoestrogens in the human diet. Because most naturally occurring phytoestrogens act as selective estrogen receptor modulators, or SERMs, which do not necessarily act as direct agonists of estrogen receptors, normal consumption of foods that contain these phytoestrogens should not provide sufficient amounts to elicit a physiological response in humans.
Plant lignans associated with high fiber foods such as cereal brans and beans are the principal precursor to mammalian lignans which have an ability to bind to human estrogen sites. Soybeans are a significant source of mammalian lignan precursor secoisolariciresinol containing 13–273 µg/100 g dry weight. Another phytoestrogen in the human diet with estrogen activity is coumestans, which are found in beans, split-peas, with the best sources being alfalfa, clover, and soybean sprouts. Coumestrol, an isoflavone coumarin derivative is the only coumestan in foods.
Soybeans and processed soy foods are among the richest foods in total phytoestrogens (wet basis per 100 g), which are present primarily in the form of the isoflavones daidzein and genistein. When compared to human breast fed or cow milk formula fed diets, which contain isoflavone levels of 0.005-0.01 mg/day, soy-based infant formulas contain isoflavone levels of 6–47 mg/day that constitute several orders of magnitude greater than they receive from other sources of nutrition.
Breast Cancer.
Women.
A 2001 literature review suggested that women with current or past breast cancer should be aware of the risks of potential tumor growth when taking soy products, based on the effect of phytoestrogens to promote breast cancer cell growth in animals. A 2006 commentary reviewed the relationship with soy and breast cancer. They stated that soy may decrease the risk of breast cancer, but cautioned that the impact of isoflavones on breast tissue needs to be evaluated at the cellular level in women at high risk for breast cancer. 
A high consumption of omega-6 polyunsaturated fatty acids, which are found in most types of vegetable oil including soybean oil, may increase the likelihood that postmenopausal women will develop breast cancer. Another analysis suggests an inverse association between total polyunsaturated fatty acids and breast cancer risk. A 2011 analysis of the literature said: "Our study suggests soy isoflavones intake is associated with a significant reduced risk of breast cancer incidence in Asian populations, but not in Western populations."
Men.
Because of the phytoestrogen content, some studies have suggested that soybean ingestion may influence testosterone levels in men. However, a 2010 meta-analysis of 15 placebo controlled studies showed that neither soy foods nor isoflavone supplements alter measures of bioavailable testosterone or estrogen concentrations in men. It has been hypothesized that soy foods and enterolactone may increase the development of prostate cancer although no significant associations were observed for the soy isoflavones. Furthermore, soy consumption has been shown to have no effect on the levels and quality of sperm. A 2009 meta-analysis of the research on the association between soy consumption and prostate cancer risk in men concluded that "consumption of soy foods is associated with a reduction in prostate cancer risk in men."
Brain.
Because of mixed results from animal studies and epidemiological studies, a (relatively definitive but expensive) controlled study of the impacts of soy on cognitive skills was performed; it found no impact.
Though there is some evidence that estrogen can help protect and repair the brain after injury in rats, there is also evidence that phytoestrogens may be harmful for the recovery of rats in other situations that have sustained brain injury.
Similarly, epidemiological evidence of humans eating soya products is currently divided: a study of Japanese men between 1965 and 1999 demonstrated a positive correlation between brain atrophy and consumption of tofu meals.
A 2001 literature review noted that disturbing data on soy's effect on the cognitive function of the elderly existed. In 2008, an epidemiological study of 719 Indonesian elderly individuals found that tofu intake was associated with worse memory, but tempeh (a fermented soy product) intake was associated with better memory.
The cover article in the Center for Science in the Public Interest's September 2014 newsletter reported that a controlled study at USC prompted by suggestive epidemiological evidence found no impact on cognitive skills of years of soy vs milk protein diet enrichment.
Carcinogenicity.
Though raw soy flour is known to be correlated with pancreatic cancer in rats the cooked flour has not been found carcinogenic. Whether soy might promote pancreatic cancer in humans is unknown because studies have not yet attempted to single out soy intake and the incidence of pancreatic cancer in humans, and the amount of soy fed to the rats is proportionately far larger than what humans would normally consume. However, the soy isoflavone genistein has been suggested as a chemopreventive agent against pancreatic cancer, by interfering with the chemical pathways that promote the creation and growth of tumors.
The Cancer Council of New South Wales, Australia has released a statement saying scientific research suggests that overall the moderate consumption of soy products does not appear to present a risk to women with breast cancer, and there is equivocal evidence that consuming large amounts of soy products may have a protective effect against developing breast and prostate cancer. However, the Council does not recommend taking soy dietary supplements as there is no evidence they are either effective or safe at preventing or treating cancers.
Gout.
Soybeans and soy products contain significant amounts of purines, a class of organic compounds. For people who suffer from gout, eating foods containing moderate or high levels of purines may make the condition worse. The U.S. National Institutes of Health (NIH) recommends that gout sufferers limit consumption of soy products (although also suggesting that soy may have health benefits by reducing the risk for heart disease). However, other researchers have found little or no association between consumption of purine-rich vegetables (including beans) and gout.
Futures.
Soybean futures are traded on the Chicago Board of Trade and have delivery dates in January (F), March (H), May (K), July (N), August (Q), September (U), November (X).
It is also traded on other commodity futures exchanges under different contract specifications:

</doc>
<doc id="62785" url="https://en.wikipedia.org/wiki?curid=62785" title="Edamame">
Edamame

, or edamame bean is a preparation of immature soybeans in the pod, found in the cuisine of China, Japan, Korea and Hawaii. The pods are boiled or steamed and served with salt. In Japan, it is usually blanched in 4% salt water and not served with salt. 
Outside East Asia, the dish is most often found in Japanese restaurants, some Chinese restaurants, and health food restaurants. In the United States it is sold packaged in frozen sections of grocery stores, in cans, or fresh in the produce sections of health food stores.
Name.
The Japanese name, , is used commonly to refer to the dish. It literally means, "stem bean" ("eda" = "branch" or "stem" + "mame" = "bean"), because the beans were often sold while still attached to the stem.
History.
The earliest documented reference to the term "edamame" dates from the year 1275, when the Japanese monk Nichiren wrote a note thanking a parishioner for the gift of "edamame" he had left at the temple.
In 1406 during the Ming Dynasty in China, the leaves of the soybeans were eaten and during outbreaks of famine, it was recommended for citizens to eat the beans whole or use them ground up and added to flour. Years later in China in 1620 they are referred to again, but as Maodou, which translates to the term “hairy bean”. They are found in the records of the Runan vegetable gardens and stated as having a medicinal purpose as well as being a snack type food.
Edamame appeared in haikai verse in Japanese in the Edo period (1603 – 1868), with one example as early as 1638.
They were first recognized in the United States in 1855 when a farmer commented on the difficulties he had shelling them after harvest. 
In March 1923, the immature soy bean is first referred to in text in the United States. In this book they are first pictured and shown as being eaten out of open shell pods. The first nutritional facts about them are published and some recipes are included as they were a new type of vegetable to the public. The earliest recorded usage in English of the word "edamame" is in 1951 in the journal "Folklore Studies". "Edamame" appeared as a new term in the Oxford English Dictionary in 2003, and in the Merriam-Webster dictionary in 2008.
In 2008, the first soybeans were grown in Europe to be sold in grocery stores as edamame and eaten as an alternative source of protein.
Preparation.
Harvesting.
Edamame is typically harvested by hand to avoid damaging the crop's stems and leaves. Green soybean pods are picked before they fully ripen, typically 35 to 40 days after the crop first flowers. Soybeans harvested at this stage are sweeter because they contain more sucrose than soybeans picked later in the growing season. Other factors contributing to edamame's desirable flavor include free amino acids such as glutamic acid, aspartic acid, and alanine. Often these unbound amino acids decrease as the pods fully expand and ripen.
Cooking.
Pods may be boiled in water, steamed, or microwaved. The ends of the pod are sometimes cut before boiling or steaming. The most common preparations use salt for taste, either dissolved in the boiling water before introducing the soybean pods or added after cooking. 
Edamame is a popular side dish at Japanese izakaya restaurants with local varieties being in demand, depending on the season. Salt and garlic are typical condiments for edamame. In Japan, a coarse salt wet with brine is preferred on beans eaten directly from the pod.
Storage.
Edamame purchased fresh is preferably eaten the same day, with flavor degradation being noticeable in as few as 10 hours after harvest. However, edamame will stay edible for three days when stored in the refrigerator (i.e. if not already brown). Damaged pods brown more rapidly however, mainly due to the enzyme polyphenol oxidase. If stored fresh, the pods should be kept humid to prevent discoloration and wilting. This can be accomplished by wrapping the pods in plastic or another material which traps moisture.
Freezing fresh edamame is another option for maintaining good quality over a few months. Fresh edamame should be blanched first before being frozen.
Nutrient content.
The United States Department of Agriculture states that edamame beans are, "a soybean that can be eaten fresh and are best known as a snack with a nutritional punch".
Edamame and other preparations of soybeans are rich in protein, dietary fiber, and micronutrients, particularly folate, manganese, phosphorus and vitamin K (table).
The balance of fatty acids in 100 grams of edamame is 361 mg of omega-3 fatty acids to 1794 mg of omega-6 fatty acids.
As a significant source of plant protein, edamame beans are under research to establish whether a relationship exists for soy consumption with reduction of disease risk.

</doc>
<doc id="62787" url="https://en.wikipedia.org/wiki?curid=62787" title="Soya">
Soya

Soya can mean:

</doc>
<doc id="62798" url="https://en.wikipedia.org/wiki?curid=62798" title="Fabaceae">
Fabaceae

The Fabaceae, Leguminosae or Papilionaceae, commonly known as the legume, pea, or bean family, is a large and economically important family of flowering plants. It includes trees, shrubs, and perennial or annual herbaceous plants, which are easily recognized by their fruit (legume) and their compound, stipulated leaves. The group is widely distributed and is the third-largest land plant family in terms of number of species, behind only the Orchidaceae and Asteraceae, with 630 genera and over 18,860 species.
The five largest of the 630 legume genera are "Astragalus" (over 2,000 species), "Acacia" (over 1000 species), "Indigofera" (around 700 species), "Crotalaria" (around 700 species), and "Mimosa" (around 500 species), which constitute about a quarter of all legume species. About 18,000 legume species are known, amounting to about 7% of flowering plant species. Fabaceae is the most common family found in tropical rainforests and in dry forests in the Americas and Africa.
Recent molecular and morphological evidence supports the fact that the Fabaceae is a single monophyletic family. This point of view has been supported not only by the degree of interrelation shown by different groups within the family compared with that found among the Leguminosae and their closest relations, but also by all the recent phylogenetic studies based on DNA sequences. These studies confirm that the Leguminosae are a monophyletic group that is closely related with the Polygalaceae, Surianaceae and Quillajaceae families and that they belong to the order Fabales.
Along with the cereals, some fruits and tropical roots a number of Leguminosae have been a staple human food for millennia and their use is closely related to human evolution.
A number are important agricultural and food plants, including "Glycine max" (soybean), "Phaseolus" (beans), "Pisum sativum" (pea), "Cicer arietinum" (chickpeas), "Medicago sativa" (alfalfa), "Arachis hypogaea" (peanut), "Lathyrus odoratus" (sweet pea), "Ceratonia siliqua" (carob), and "Glycyrrhiza glabra" (liquorice). A number of species are also weedy pests in different parts of the world, including: "Cytisus scoparius" (broom), "Robinia pseudoacacia" (black locust)", Ulex europaeus" (gorse), "Pueraria lobata" (kudzu), and a number of "Lupinus" species.
Etymology.
The name 'Fabaceae' comes from the defunct genus "Faba", now included in "Vicia". The term "faba" comes from Latin, and appears to simply mean "bean". Leguminosae is an older name still considered valid, and refers to the fruit of these plants, which are called legumes.
Description.
Fabaceae range in habit from giant trees (like "Koompassia excelsa") to small annual herbs, with the majority being herbaceous perennials. Plants have indeterminate inflorescences, which are sometimes reduced to a single flower. The flowers have a short hypanthium and a single carpel with a short gynophore, and after fertilization produce fruits that are legumes.
Growth habit.
The Leguminosae have a wide variety of growth forms including trees, shrubs or herbaceous plants or even vines or lianas. The herbaceous plants can be annuals, biennials or perennials, without basal or terminal leaf aggregations. They are upright plants, epiphytes or vines. The latter support themselves by means of shoots that twist around a support or through cauline or foliar tendrils. Plants can be heliophytes, mesophytes or xerophytes.
Leaves.
The leaves are usually alternate and compound. Most often they are even- or odd-pinnately compound (e.g. "Caragana" and "Robinia" respectively), often trifoliate (e.g. "Trifolium", "Medicago") and rarely palmately compound (e.g. "Lupinus"), in the Mimosoideae and the Caesalpinioideae commonly bipinnate (e.g. "Acacia", "Mimosa"). They always have stipules, which can be leaf-like (e.g. "Pisum"), thorn-like (e.g. "Robinia") or be rather inconspicuous. Leaf margins are entire or, occasionally, serrate. Both the leaves and the leaflets often have wrinkled pulvini to permit nastic movements. In some species, leaflets have evolved into tendrils (e.g. "Vicia").
Many species have leaves with structures that attract ants that protect the plant from herbivore insects (a form of mutualism). Extrafloral nectaries are common among the Mimosoideae and the Caesalpinioideae, and are also found in some Faboideae (e.g. "Vicia sativa"). In some "Acacia", the modified hollow stipules are inhabited by ants and are known as domatia.
Roots.
Many Fabaceae host bacteria in their roots within structures called root nodules. These bacteria, known as rhizobia, have the ability to take nitrogen gas (N2) out of the air and convert it to a form of nitrogen that is usable to the host plant ( NO3− or NH3 ). This process is called nitrogen fixation. The legume, acting as a host, and rhizobia, acting as a provider of usable nitrate, form a symbiotic relationship.
Flowers.
The flowers often have five generally fused sepals and five free petals. They are generally hermaphrodite, and have a short hypanthium, usually cup shaped. There are normally ten stamens and one elongated superior ovary, with a curved style. They are usually arranged in indeterminate inflorescences. Fabaceae are typically entomophilous plants (i.e. they are pollinated by insects), and the flowers are usually showy to attract pollinators.
In the Caesalpinioideae, the flowers are often zygomorphic, as in "Cercis", or nearly symmetrical with five equal petals in "Bauhinia". The upper petal is the innermost one, unlike in the Faboideae. Some species, like some in the genus "Senna", have asymmetric flowers, with one of the lower petals larger than the opposing one, and the style bent to one side. The calyx, corolla, or stamens can be showy in this group.
In the Mimosoideae, the flowers are actinomorphic and arranged in globose inflorescences. The petals are small and the stamens, which can be more than just 10, have long, coloured filaments, which are the showiest part of the flower. All of the flowers in an inflorescence open at once.
In the Faboideae, the flowers are zygomorphic, and have a specialized structure. The upper petal, called the banner, is large and envelops the rest of the petals in bud, often reflexing when the flower blooms. The two adjacent petals, the wings, surround the two bottom petals. The two bottom petals are fused together at the apex (remaining free at the base), forming a boat-like structure called the keel. The stamens are always ten in number, and their filaments can be fused in various configurations, often in a group of nine stamens plus one separate stamen. Various genes in the "CYCLOIDEA (CYC)/DICHOTOMA (DICH)" family are expressed in the upper (also called dorsal or adaxial) petal; in some species, such as "Cadia", these genes are expressed throughout the flower, producing a radially symmetrical flower.
Fruit.
The ovary most typically develops into a legume. A legume is a simple dry fruit that usually dehisces (opens along a seam) on two sides. A common name for this type of fruit is a "pod", although that can also be applied to a few other fruit types. A few species have evolved samarae, loments, follicles, indehiscent legumes, achenes, drupes, and berries from the basic legume fruit.
Physiology and biochemistry.
The Leguminosae are rarely cyanogenic, however, where they are, the cyanogenic compounds are derived from tyrosine, phenylalanine or leucine. They frequently contain alkaloids. Proanthocyanidins can be present either as cyanidin or delphinidine or both at the same time. Flavonoids such as kaempferol, quercitin and myricetin are often present. Ellagic acid has never been found in any of the genera or species analysed. Sugars are transported within the plants in the form of sucrose. C3 photosynthesis has been found in a wide variety of genera. The family has also evolved a unique chemistry. Pterocarpans are a class of molecules (derivatives of isoflavonoids) found only in the Fabaceae.
Ecology.
Distribution and habitat.
The Fabaceae have an essentially worldwide distribution, being found everywhere except Antarctica and the high arctic. The trees are often found in tropical regions, while the herbaceous plants and shrubs are predominant in extratropical regions.
Biological nitrogen fixation.
Biological nitrogen fixation (BNF, performed by the organisms called diazotrophs) is a very old process that probably originated in the Archean eon when the primitive atmosphere lacked oxygen. It is only carried out by Euryarchaeota and just 6 of the more than 50 phyla of bacteria. Some of these lineages co-evolved together with the flowering plants establishing the molecular basis of a mutually beneficial symbiotic relationship. BNF is carried out in nodules that are mainly located in the root cortex, although they are occasionally located in the stem as in "Sesbania rostrata". The spermatophytes that co-evolved with actinorhizal diazotrophs ("Frankia") or with rhizobia to establish their symbiotic relationship belong to 11 families contained within the Rosidae clade (as established by the gene molecular phylogeny of "rbcL", a gene coding for part of the RuBisCO enzyme in the chloroplast). This grouping indicates that the predisposition for forming nodules probably only arose once in flowering plants and that it can be considered as an ancestral characteristic that has been conserved or lost in certain lineages. However, such a wide distribution of families and genera within this lineage indicates that nodulation had multiple origins. Of the 10 families within the Rosidae, 8 have nodules formed by actinomyces (Betulaceae, Casuarinaceae, Coriariaceae, Datiscaceae, Elaeagnaceae, Myricaceae, Rhamnaceae and Rosaceae), and the two remaining families, Ulmaceae and Fabaceae have nodules formed by rhizobia.
The rhizobia and their hosts must be able to recognize each other for nodule formation to commence. Rhizobia are specific to particular host species although a rhizobia species may often infect more than one host species. This means that one plant species may be infected by more than one species of bacteria. For example, nodules in "Acacia senegal" can contain seven species of rhizobia belonging to three different genera. The most distinctive characteristics that allow rhizobia to be distinguished apart are the rapidity of their growth and the type of root nodule that they form with their host. Root nodules can be classified as being either indeterminate, cylindrical and often branched, and determinate, spherical with prominent lenticels. Indeterminate nodules are characteristic of legumes from temperate climates, while determinate nodules are commonly found in species from tropical or subtropical climates.
Nodule formation is common throughout the leguminosae, it is found in the majority of its members that only form an association with rhizobia, which in turn form an exclusive symbiosis with the leguminosae (with the exception of "Parasponia", the only genus of the 18 Ulmaceae genera that is capable of forming nodules). Nodule formation is present in all the leguminosae sub-families, although it is less common in the Caesalpinioideae. All types of nodule formation are present in the sub-family Papilionoideae: indeterminate (with the meristem retained), determinate (without meristem) and the type included in "Aeschynomene". The latter two are thought to be the most modern and specialised type of nodule as they are only present in some lines of the Papilionoideae sub-family. Even though nodule formation is common in the two monophyletic subfamilies Papilionoideae and Mimosoideae they also contain species that do not form nodules. The presence or absence of nodule-forming species within the three sub-families indicates that nodule formation has arisen several times during the evolution of the leguminosae and that this ability has been lost in some lineages. For example, within the genus "Acacia", a member of the Mimosoideae, "A. pentagona" does not form nodules, while other species of the same genus readily form nodules, as is the case for "Acacia senegal", which forms both rapidly and slow growing rhizobial nodules.
Evolution, phylogeny and taxonomy.
Evolution.
The order Fabales contains around 7.3% of eudicot species and the greatest part of this diversity is contained in just one of the four families that order contains: Fabaceae. This clade also includes the Polygalaceae, Surianaceae and Quillajaceae families and its origins date back 94 to 89 million years, although it started its diversification some 79 to 74 million years ago. In fact, the leguminosae have diversified during the early tertiary to become a ubiquitous part of the modern earth’s biota, along with many other families belonging to the flowering plants.
The leguminosae have an abundant and diverse fossil record, especially for the Tertiary period. Fossils of flowers, fruit, leaves, wood and pollen from this period have been found in numerous locations.
The earliest fossils that can be definitively assigned to the leguminosae appeared in the late Palaeocene (approximately 56 million years ago). Representatives of the 3 sub-families traditionally recognised as being members of the Leguminosae – Cesalpinioideae, Papilionoideae and Mimosoideae — as well as members of the large clades within these sub-families – such as the genistoides – have been found in periods a little later, starting between 55 to 50 million years ago. In fact, a wide variety of taxa representing the main Leguminosae lineages have been found in the fossil record dating from the middle to the late Eocene, suggesting that the majority of the modern fabaceae groups were already present and that a broad diversification occurred during this period.
Therefore, the Fabaceae started their diversification approximately 60 million years ago and the most important clades separated some 50 million years ago.
The age of the main Cesalpinioideae clades have been estimated as between 56 and 34 million years and the basal group of the Mimosoideae as 44 ± 2.6 million years.
The division between Mimosoideae and Faboideae is dated as occurring between 59 and 34 million years ago and the basal group of the Faboideae as 58.6 ± 0.2 million years ago. It has been possible to date the divergence of some of the groups within the Faboideae, even though diversification within each genus was relatively recent. For instance, "Astragalus" separated from the "Oxytropis" some 16 to 12 million years ago. In addition, the separation of the aneuploid species of "Neoastragalus" started 4 million years ago. "Inga," another genus of the Papilionoideae with approximately 350 species, seems to have diverged in the last 2 million years.
It has been suggested, based on fossil and phylogenetic evidence, that legumes originally evolved in arid and/or semi-arid regions along the Tethys seaway during the Palaeogene Period. However, others contend that Africa (or even the Americas) cannot yet be ruled out as the origin of the family.
The current hypothesis about the evolution of the genes needed for nodulation is that they were recruited from other pathways after a polyploidy event. Several different pathways have been implicated as donating duplicated genes to the pathways need for nodulation. The main donors to the pathway were the genes associated with the arbuscular mycorrhiza symbiosis genes, the pollen tube formation genes and the haemoglobin genes. One of the main genes shown to be shared between the arbuscular mycorrhiza pathway and the nodulation pathway is SYMRK and it is involved in the plant-bacterial recognition. The pollen tube growth is similar to the infection thread development in that infection threads grow in a polar manner that is similar to a pollen tubes polar growth towards the ovules. Both pathways include the same type of enzymes, pectin-degrading cell wall enzymes. The enzymes needed to reduce nitrogen, nitrogenases, require a substantial input of ATP but at the same time are sensitive to free oxygen. To meet the requirements of this paradoxical situation, the plants express a type of haemoglobin called leghaemoglobin that is believed to be recruited after a duplication event. These three genetic pathways are believed to be part of a gene duplication event then recruited to work in nodulation.
Phylogeny and taxonomy.
Phylogeny.
The phylogeny of the legumes has been the object of many studies by research groups from around the world. These studies have used morphology, DNA data (the chloroplast intron "trnL", the chloroplast genes "rbcL" and "matK", or the ribosomal spacers "ITS") and cladistic analysis in order to investigate the relationships between the family’s different lineages. The studies have confirmed that the traditional sub-families Mimosoideae and Papilionoideae are each monophyletic but both are nested within the paraphyletic sub-family Caesalpinioideae. All the different approaches have yielded similar results regarding the relationships between the family's main clades, as shown in the cladogram below.
Asterisks (*) indicate clades traditionally assigned to Caesalpinioideae; the other two subfamilies (which are nested within Caesalpinioideae) are underlined.
Taxonomy.
The Fabaceae are placed in the order Fabales according to most taxonomic systems, including the APG III system. The family includes three subfamilies:
These three subfamilies have been alternatively treated at the family level, as in the Cronquist and Dahlgren systems. However, this choice has not been supported by late 20th and early 21st century evidence, which has shown the Caesalpinioideae to be paraphyletic and the Fabaceae "sensu lato" to be monophyletic. While the Mimosoideae and the Faboideae are largely monophyletic, the Caesalpinioideae appear to be paraphyletic and the tribe Cercideae is probably sister to the rest of the family. Moreover, there are a number of genera whose placement into the Caesalpinioideae is not always agreed on (e.g. "Dimorphandra").
Genera.
The 730 genera included in this family can be viewed on the following three pages:
Economic and cultural importance.
Legumes are economically and culturally important plants due to their extraordinary diversity and abundance, the wide variety of edible vegetables they represent and due to the variety of uses they can be put to: in horticulture and agriculture, as a food, for the compounds they contain that have medicinal uses and for the oil and fats they contain that have a variety of uses.
Food and forage.
The history of legumes is tied in closely with that of human civilization, appearing early in Asia, the Americas (the common bean, several varieties) and Europe (broad beans) by 6,000 BCE, where they became a staple, essential as a source of protein.
Their ability to fix atmospheric nitrogen reduces fertilizer costs for farmers and gardeners who grow legumes, and means that legumes can be used in a crop rotation to replenish soil that has been depleted of nitrogen. Legume seeds and foliage have a comparatively higher protein content than non-legume materials, due to the additional nitrogen that legumes receive through the process. Some legume species perform hydraulic lift, which makes them ideal for intercropping.
Farmed legumes can belong to numerous classes, including forage, grain, blooms, pharmaceutical/industrial, fallow/green manure and timber species, with most commercially farmed species filling two or more roles simultaneously.
There are of two broad types of forage legumes. Some, like alfalfa, clover, vetch, and "Arachis", are sown in pasture and grazed by livestock. Other forage legumes such as "Leucaena" or "Albizia" are woody shrub or tree species that are either broken down by livestock or regularly cut by humans to provide stock feed.
Grain legumes are cultivated for their seeds, and are also called pulses. The seeds are used for human and animal consumption or for the production of oils for industrial uses. Grain legumes include both herbaceous plants like beans, lentils, lupins, peas and peanuts. and trees such as carob, mesquite and tamarind.
Bloom legume species include species such as lupin, which are farmed commercially for their blooms as well as being popular in gardens worldwide. "Laburnum", "Robinia", "Gleditsia", "Acacia", "Mimosa", and "Delonix" are ornamental trees and shrubs.
Industrial farmed legumes include "Indigofera", cultivated for the production of indigo, "Acacia", for gum arabic, and "Derris", for the insecticide action of rotenone, a compound it produces.
Fallow or green manure legume species are cultivated to be tilled back into the soil to exploit the high nitrogen levels found in most legumes. Numerous legumes are farmed for this purpose, including "Leucaena", "Cyamopsis" and "Sesbania".
Various legume species are farmed for timber production worldwide, including numerous "Acacia" species, "Dalbergia" species, and "Castanospermum australe".
Melliferous plants offer nectar to bees and other insects to encourage them to carry pollen from the flowers of one plant to others thereby ensuring pollination.A number of legume species are good nectar providers such as alfalfa, white clover, sweet clover and various Prosopis species. Many plants in the "Fabaceae" family are an important source of pollen for the bumblebee species "Bombus hortorum". This bee species is especially fond of one species in particular; "Trifolium pratense", also known as red clover, is a popular food source in the diet of "Bombus hortorum".
Industrial uses.
Natural gums.
Natural gums are vegetable exudates that are released as the result of damage to the plant such as that resulting from the attack of an insect or a natural or artificial cut. These exudates contain heterogeneous polysaccharides formed of different sugars and usually containing uronic acids. They form viscous colloidal solutions. There are different species that produce gums. The most important of these species belong to the leguminosae. They are widely used in the pharmaceutical, cosmetic, food and textile sectors. They also have interesting therapeutic properties; for example gum arabic is antitussive and anti-inflammatory. The most well known gums are tragacanth ("Astragalus gummifer"), gum arabic ("Acacia senegal") and guar gum ("Cyamopsis tetragonoloba").
Dyes.
The species used to produce dyes include the following: Logwood "Haematoxylon campechianum"; a large spiny tree that can grow up to 15 m tall. Its cork is thin and soft and its wood is hard. The heartwood is used to produce dyes that are red and purple. The histological stain called haematoxylin is produced from this species. Brazilwood tree ("Caesalpinia echinata") is similar to the previous tree but smaller and with red or purple flowers. The wood is also used to produce a red or purple dye. The Madras thorn ("Pithecallobium dulce") is another spiny tree native to Latin America, it grows up to 4 m high and has yellow or green flowers that grow in florets. Its fruit is reddish and is used to produce a yellow dye. Indigo dye is extracted from the True indigo plant "Indigofera tinctoria" that is native to Asia. In Central and South America dyes are produced from two species related to this species, indigo from "Indigofera suffruticosa" and Natal indigo from "Indigofera arrecta".yellow dye is extracted from "Butea monosperma" commonly called as flame of the forest.
Ornamentals.
Legumes have been used as ornamental plants throughout the world for many centuries. Their vast diversity of heights, shapes, foliage and flower colour means that this family is commonly used in the design and planting of everything from small gardens to large parks. The following is a list of the main ornamental legume species, listed by sub-family.

</doc>
<doc id="62808" url="https://en.wikipedia.org/wiki?curid=62808" title="Soul music">
Soul music

Soul music is a popular music genre that originated in the United States in the late 1950s and early 1960s. It combines elements of African-American gospel music, rhythm and blues and jazz. Soul music became popular for dancing and listening in the United States; where record labels such as Motown, Atlantic and Stax were influential in the civil rights era. Soul also became popular around the world, directly influencing rock music and the music of Africa.
According to the Rock and Roll Hall of Fame, soul is "music that arose out of the black experience in America through the transmutation of gospel and rhythm & blues into a form of funky, secular testifying". Catchy rhythms, stressed by handclaps and extemporaneous body moves, are an important feature of soul music. Other characteristics are a call and response between the lead vocalist and the chorus and an especially tense vocal sound. The style also occasionally uses improvisational additions, twirls and auxiliary sounds. Soul music reflected the African-American identity and it stressed the importance of an African-American culture. The new-found African-American consciousness led to new styles of music, which boasted pride in being black.
Soul music dominated the U.S. R&B chart in the 1960s, and many recordings crossed over into the pop charts in the U.S., Britain and elsewhere. By 1968, the soul music genre had begun to splinter. Some soul artists developed funk music, while other singers and groups developed slicker, more sophisticated, and in some cases more politically conscious varieties. By the early 1970s, soul music had been influenced by psychedelic rock and other genres, leading to psychedelic soul. The United States saw the development of neo soul around 1994. There are also several other subgenres and offshoots of soul music.
The key subgenres of soul include the Detroit (Motown) style, a rhythmic music influenced by gospel; "deep soul" and "southern soul", driving, energetic soul styles combining R&B with southern gospel music sounds; Memphis soul, a shimmering, sultry style; New Orleans soul, which came out of the rhythm and blues style; Chicago soul, a lighter gospel-influenced sound; Philadelphia soul, a lush orchestral sound with doo-wop-inspired vocals; Psychedelic soul, a blend of psychedelic rock and soul music; as well as categories such as Blue-eyed soul, which is soul music performed by white artists; British soul; and Northern soul, rare soul music played by DJs at nightclubs in Northern England.
Origins.
Soul music has its roots in traditional African-American gospel music and rhythm and blues, and the hybridization of their respective religious and secular styles, in both lyrical content and instrumentation, that began to occur in the 1950s. The term "soul" had been used among African-American musicians to emphasize the feeling of being an African-American in the U.S. According to musicologist Barry Hansen,Though this hybrid produced a clutch of hits in the R&B market in the early 1950s, only the most adventurous white fans felt its impact at the time; the rest had to wait for the coming of soul music in the 1960s to feel the rush of rock and roll sung gospel-style.
According to another source, "Soul music was the result of the urbanization and commercialization of rhythm and blues in the '60s." The phrase "soul music" itself, referring to gospel-style music with secular lyrics, is first attested in 1961. The term 'soul' in African-American parlance has connotations of African-American pride and culture. Gospel groups in the 1940s and 1950s occasionally used the term as part of their name. The jazz style that derived from gospel came to be called soul jazz. As singers and arrangers began using techniques from gospel and soul jazz in African-American popular music during the 1960s, soul music gradually functioned as an umbrella term for the African-American popular music at the time.
Important innovators whose recordings in the 1950s contributed to the emergence of soul music included Clyde McPhatter, Hank Ballard, and Etta James. Ray Charles is often cited as popularizing the soul genre with his string of hits starting with 1954's "I Got a Woman". Singer Bobby Womack said: "Ray was the genius. He turned the world onto soul music." Charles was open in acknowledging the influence of Pilgrim Travelers vocalist Jesse Whitaker on his singing style.
Little Richard (who inspired Otis Redding) and James Brown were equally influential. Brown was known as the "Godfather of Soul" and Richard proclaimed himself the "king of rockin' and rollin', rhythm and blues soulin'", because his music embodied elements of all three, and because he inspired artists in all three genres.
Sam Cooke and Jackie Wilson are also often acknowledged as soul forefathers. Cooke became popular as the lead singer of gospel group The Soul Stirrers, before controversially moving into secular music. His recording of "You Send Me" in 1957 launched a successful pop career, and his 1962 recording of "Bring It On Home To Me" has been described as "perhaps the first record to define the soul experience". Jackie Wilson, a contemporary of both Cooke and James Brown, also achieved crossover success in 1957 with "Reet Petite", and was particularly influential for his dramatic delivery and performances.
1960s.
Writer Peter Guralnick is among those to identify Solomon Burke as a key figure in the emergence of soul music, and Atlantic Records as the key record label. Burke's early 1960s songs, including "Cry to Me", "Just Out of Reach" and "Down in the Valley" are considered classics of the genre. Guralnick wrote:"Soul started, in a sense, with the 1961 success of Solomon Burke's "Just Out Of Reach". Ray Charles, of course, had already enjoyed enormous success (also on Atlantic), as had James Brown and Sam Cooke — primarily in a pop vein. Each of these singers, though, could be looked upon as an isolated phenomenon; it was only with the coming together of Burke and Atlantic Records that you could begin to see anything even resembling a movement."
Ben E. King also achieved success in 1961 with "Stand By Me", a song directly based on a gospel hymn. By the mid-1960s, the initial successes of Burke, King and others had been surpassed by new soul singers, including Stax artists such as Otis Redding and Wilson Pickett, who mainly recorded in Memphis, Tennessee, and Muscle Shoals, Alabama. According to Jon Landau:"Between 1962 and 1964 Redding recorded a series of soul ballads characterized by unabashedly sentimental lyrics usually begging forgiveness or asking a girlfriend to come home... He soon became known as "Mr. Pitiful" and earned a reputation as the leading performer of soul ballads."
The most important female soul singer to emerge was Aretha Franklin, originally a gospel singer who began to make secular recordings in 1960 but whose career was later revitalised by her recordings for Atlantic. Her 1967 recordings, such as "I Never Loved a Man (The Way I Love You)", "Respect" (originally sung by Otis Redding), and "Do Right Woman, Do Right Man" (written by Chips Moman and Dan Penn), were significant and commercially successful productions.
Soul music dominated the U.S. African-American music charts in the 1960s, and many recordings crossed over into the pop charts in the U.S. Otis Redding was a huge success at the Monterey Pop Festival in 1967. The genre also became highly popular in the UK, where many leading acts toured in the late 1960s. "Soul" became an umbrella term, used to describe an increasingly wide variety of R&B-based music styles — from the dance and pop-oriented acts at Motown Records in Detroit, such as The Temptations, Marvin Gaye and Stevie Wonder, to "deep soul" performers such as Percy Sledge and James Carr. Different regions and cities within the U.S., including New York City, Detroit, Chicago, Memphis, New Orleans, Philadelphia, and Muscle Shoals, Alabama (the home of FAME Studios and Muscle Shoals Sound Studios) became noted for different subgenres of the music and recording styles.
By 1968, the soul music movement had begun to splinter. Artists such as James Brown and Sly & the Family Stone developed funk music, while other singers such as Marvin Gaye, Stevie Wonder, Curtis Mayfield and Al Green developed slicker, more sophisticated and in some cases more politically conscious varieties of the genre. However, "although soul music evolved, it never went away — not only did the music inform all of the R&B of the '70s, '80s, and '90s, there were always pockets of musicians around the world that kept performing traditional soul."
1970s and later.
Later examples of soul music include recordings by The Staple Singers (such as "I'll Take You There"), and Al Green's 1970s recordings, done at Willie Mitchell's' Royal Recording in Memphis. Mitchell's Hi Records continued the Stax tradition in that decade, releasing many hits by Green, Ann Peebles, Otis Clay, O.V. Wright and Syl Johnson. Bobby Womack, who recorded with Chips Moman in the late 1960s, continued to produce soul recordings in the 1970s and 1980s.
In Detroit, producer Don Davis worked with Stax artists such as Johnnie Taylor and The Dramatics. Early 1970s recordings by The Detroit Emeralds, such as "Do Me Right", are a link between soul and the later disco style. Motown Records artists such as Marvin Gaye, Michael Jackson, Stevie Wonder and Smokey Robinson contributed to the evolution of soul music, although their recordings were considered more in a pop music vein than those of Redding, Franklin and Carr. Although stylistically different from classic soul music, recordings by Chicago-based artists are often considered part of the genre.
By the early 1970s, soul music had been influenced by psychedelic rock and other genres. The social and political ferment of the times inspired artists like Gaye and Curtis Mayfield to release album-length statements with hard-hitting social commentary. Artists like James Brown led soul towards funk music, which became typified by 1970s bands like Parliament-Funkadelic and The Meters. More versatile groups like War, the Commodores and Earth, Wind and Fire became popular around this time. During the 1970s, some slick and commercial blue-eyed soul acts like Philadelphia's Hall & Oates and Oakland's Tower of Power achieved mainstream success, as did a new generation of street-corner harmony or "city-soul" groups such as The Delfonics and the historically black Howard University's Unifics.
The syndicated music/dance variety television series "Soul Train", hosted by Chicago native Don Cornelius, debuted in 1971. The show provided an outlet for soul music for several decades, also spawning a franchise that saw the creation of a record label (Soul Train Records) that distributed music by The Whispers, Carrie Lucas, and an up-and-coming group known as Shalamar. Numerous disputes led to Cornelius spinning off the record label to his talent booker, Dick Griffey, who transformed the label into Solar Records, itself a prominent soul music label throughout the 1980s. The TV series continued to air until 2006, although other predominantly African-American music genres such as hip-hop began overshadowing soul on the show beginning in the 1980s.
As disco and funk were dominating the charts in the late 1970s and early 1980s, soul went in the direction of quiet storm. With its relaxed tempos and soft melodies, quiet storm soul took influences from soft rock and adult contemporary. Many funk bands, such as Con Funk Shun, Cameo, and Lakeside would have a few quiet storm tracks on their albums. Among the most successful acts in this era include Smokey Robinson, Teddy Pendergrass, Peabo Bryson, Atlantic Starr, and Larry Graham.
After the decline of disco and funk in the early 1980s, soul music became influenced by electro music. It became less raw and more slickly produced, resulting in a style known as contemporary R&B, which sounded very different from the original rhythm and blues style. The United States saw the development of neo-soul around 1994. Mainstream record label marketing support for soul genres cooled in the 2000s due to the industry's re-focus on hip-hop.
Notable record labels and producers.
Motown Records.
Berry Gordy's successful Tamla/Motown group of labels was notable for being African-American owned, unlike most of the earlier independent R&B labels. Notable artists under this label were The Supremes, The Temptations, The Miracles, the Four Tops, The Marvelettes, Mary Wells, Jr. Walker & The All-Stars, Stevie Wonder, Marvin Gaye, Tammi Terrell, Martha and the Vandellas, and The Jackson Five.
Hits were made using a quasi-industrial "production-line" approach. Some considered the sound to be mechanistic, but the producers and songwriters brought artistic sensitivity to the three-minute tunes. Brian Holland, Lamont Dozier and Eddie Holland were rarely out of the charts for their work as songwriters and record producers for The Supremes, the Four Tops and Martha and the Vandellas. They allowed important elements to shine through the dense musical texture. Rhythm was emphasized by handclaps or tambourine. Smokey Robinson was another writer and record producer who added lyrics to "The Tracks Of My Tears" by his group The Miracles, which was one of the most important songs of the decade.
Stax Records and Atlantic Records.
Stax Records and Atlantic Records were independent labels that produced high-quality dance records featuring many well known singers of the day. They tended to have smaller ensembles marked by expressive gospel-tinged vocals. Brass and saxophones were also used extensively. Stax Records, founded by siblings Estelle and James Stewart, was the second most successful record label behind Motown Records. They were responsible for releasing hits by Otis Redding, Wilson Pickett, The Staple Singers and many more. Ahmet Ertegun, who had anticipated being a diplomat until 1944 when his father died, founded Atlantic Records in 1947 with his friend Herb Abramson. Ertegun wrote many songs for Ray Charles and The Clovers. He even sang backup vocals for his artist Big Joe Turner on the song, "Shake Rattle and Roll."
Subgenres.
Detroit (Motown) soul.
Dominated by Berry Gordy's Motown Records empire, Detroit soul is strongly rhythmic and influenced by gospel music. The Motown sound often includes hand clapping, a powerful bassline, violins and bells. Motown Records' house band was The Funk Brothers. AllMusic cites Motown as the pioneering label of pop-soul, a style of soul music with raw vocals, but polished production and toned-down subject matter intended for pop radio and crossover success. Artists of this style included Diana Ross, the Jackson 5, Stevie Wonder, and Billy Preston. Popular during the 1960s, the style became glossier during the 1970s and led to disco.
Deep soul and southern soul.
The terms "deep soul" and "southern soul" generally refer to a driving, energetic soul style combining R&B's energy with pulsating southern United States gospel music sounds. Memphis, Tennessee label Stax Records nurtured a distinctive sound, which included putting vocals further back in the mix than most contemporary R&B records, using vibrant horn parts in place of background vocals, and a focus on the low end of the frequency spectrum. The vast majority of Stax releases were backed by house bands Booker T and the MGs (with Booker T. Jones, Steve Cropper, Duck Dunn, and Al Jackson) and the Memphis Horns (the splinter horn section of the Mar-Keys, trumpeter Wayne Jackson and saxophonist Andrew Love).
Memphis soul.
"Memphis soul" is a shimmering, sultry style of soul music produced in the 1960s and 1970s at Stax Records and Hi Records in Memphis, Tennessee. It featured melancholic and melodic horns, Hammond organ, bass, and drums, as heard in recordings by Hi's Al Green and Stax's Booker T. & the M.G.'s. The latter group also sometimes played in the harder-edged Southern soul style. The Hi Records house band (Hi Rhythm Section) and producer Willie Mitchell developed a surging soul style heard in the label's 1970s hit recordings. Some Stax recordings fit into this style, but had their own unique sound.
New Orleans soul.
The New Orleans soul scene directly came out of the rhythm and blues era, when such artists as Little Richard, Fats Domino, and Huey Piano Smith made a huge impact on the pop and R&B charts and a huge direct influence on the birth of Funk music. The principal architect of Crescent City’s soul was songwriter, arranger, and producer Allen Toussaint. He worked with such artists as Irma Thomas (“the Soul Queen of New Orleans”), Jessie Hill, Kris Kenner, Benny Spellman, and Ernie K. Doe on the Minit/Instant label complex to produce a distinctive New Orleans soul sound that generated a passel of national hits. Other notable New Orleans hits came from Robert Parker, Betty Harris, and Aaron Neville. While record labels in New Orleans largely disappeared by the mid-1960s, producers in the city continued to record New Orleans soul artists for other mainly New York City- and Los Angeles-based record labels—notably Lee Dorsey for New York–based Amy Records and the Meters for New York–based Josie and then LA-based Reprise.
Chicago soul.
Chicago soul generally had a light gospel-influenced sound, but the large number of record labels based in the city tended to produce a more diverse sound than other cities. Vee Jay Records, which lasted until 1966, produced recordings by Jerry Butler, Betty Everett, Dee Clark, and Gene Chandler. Chess Records, mainly a blues and rock and roll label, produced a number of major soul artists, including The Dells and Billy Stewart. Curtis Mayfield not only scored many hits with his group, The Impressions, but wrote many hit songs for Chicago artists and produced hits on his own labels for The Fascinations, Major Lance, and the Five Stairsteps.
Philadelphia soul.
Based primarily in the Philadelphia International record label, Philadelphia soul (or Philly Soul) had a lush orchestral sound and doo-wop-inspired vocals. Thom Bell, and Kenneth Gamble & Leon Huff are considered the founders of Philadelphia soul, which produced hits for The O'Jays, The Intruders, The Delfonics, The Stylistics, Harold Melvin & The Blue Notes, and The Spinners.
Psychedelic soul.
Psychedelic soul, sometimes known as "black rock", was a blend of psychedelic rock and soul music in the late 1960s, which paved the way for the mainstream emergence of funk music a few years later. Early pioneers of this subgenre of soul music include Jimi Hendrix, James Brown, and Stevie Wonder. While psychedelic rock began its decline, the influence of psychedelic soul continued on and remained prevalent through the 1970s.
Blue-eyed soul.
Blue-eyed soul is R&B or soul music performed by white artists. The meaning of "blue-eyed soul" has evolved over decades. Originally the term was associated with mid-1960s white artists who performed soul and R&B that was similar to the music released by Motown Records and Stax Records. The term continued to be used in the 1970s and 1980s, particularly by the British media to refer to a new generation of singers who adopted elements of the Stax and Motown sounds. To a lesser extent, the term has been applied to singers in other music genres that are influenced by soul music. Artists like Hall and Oates, David Bowie, Christina Aguilera, Amy Winehouse and Adele are known as Blue-eyed soul singers.
British soul.
Soul has been a major influence on British popular music since the 1960s including bands of the British Invasion, most significantly The Beatles. There were a handful of significant British Blue-eyed soul acts, including Dusty Springfield and Tom Jones. American soul was extremely popular among some youth sub-cultures like the Northern soul and Modern soul movements, but a clear genre of British soul did not emerge until the 1980s when a number of artists including George Michael, Sade, Simply Red, Lisa Stansfield and Soul II Soul enjoyed commercial success. The popularity of British soul artists in the U.S., most notably Amy Winehouse, Adele, Estelle, Duffy, Joss Stone, and Leona Lewis led to talk of a "third British Invasion" or soul invasion in the 2000s and 2010s.
Neo soul.
The term "neo soul" is a marketing phrase coined in the early 1990s by producer and record label executive Kedar Massenburg to describe a blend of 1970s soul-style vocals and instrumentation with contemporary R&B sounds, hip-hop beats and poetic interludes. The style was developed in the early to mid-1990s. A key element in neo soul is a heavy dose of Fender Rhodes or Wurlitzer electric piano "pads" over a mellow, grooving interplay between the drums (usually with a rim shot snare sound) and a muted, deep funky bass. The Fender Rhodes piano sound gives the music a warm, organic character.
Northern soul and modern soul.
The phrase "northern soul" was coined by journalist Dave Godin and popularised in 1970 through his column in "Blues and Soul" magazine. The term refers to rare soul music that was played by DJs at nightclubs in northern England. The playlists originally consisted of obscure 1960s and early 1970s American soul recordings with an uptempo beat, such as those on Motown Records and more obscure labels such as Okeh Records. Modern soul developed when northern soul DJs began looking in record shops in the United States and United Kingdom for music that was more complex and contemporary. What emerged was a richer sound that was more advanced in terms of Hi-Fi and FM radio technology.
Nu-jazz and soul-influenced electronica.
Many artists in various genres of electronic music (such as house, drum n bass, UK garage, and downtempo) are heavily influenced by soul, and have produced many soul-inspired compositions.

</doc>
<doc id="62809" url="https://en.wikipedia.org/wiki?curid=62809" title="Warren Beatty">
Warren Beatty

Henry Warren Beatty ( ; born March 30, 1937) is an American actor and filmmaker. He has been nominated for fourteen Academy Awards – four for Best Actor, four for Best Picture, two for Best Director, three for Original Screenplay, and one for Adapted Screenplay – winning Best Director for "Reds" (1981). Beatty is the first and only person to have been twice nominated for acting in, directing, writing and producing the same film – first with "Heaven Can Wait" (1978), and again with "Reds".
In 1999, he was awarded the Academy's highest honor, the Irving G. Thalberg Award. Beatty has been nominated for eighteen Golden Globe Awards, winning six, including the Golden Globe Cecil B. DeMille Award, which he was honored with in 2007. Among his Golden Globe-nominated films are "Splendor in the Grass" (1961), "Bonnie and Clyde" (1967), "Shampoo" (1975), "Dick Tracy" (1990), "Bugsy" (1991), and "Bulworth" (1998).
Early life.
Henry Warren Beaty was born in Richmond, Virginia. His mother, Kathlyn Corinne (née MacLean), who was Canadian, was a teacher from Nova Scotia, and his father, Ira Owens Beaty, had a PhD in educational psychology, was a public school administrator, and dealt in real estate. Beatty's grandparents were also educators. The family was Baptist. In 1945, the family moved from Richmond to Arlington, Virginia. During the 1950s, the family resided in the Dominion Hills section of Arlington. Beatty's elder sister is the actress, dancer and writer Shirley MacLaine. His uncle, by marriage, was Canadian politician A. A. MacLeod. Beatty is not related to actor Ned Beatty (who was also born in 1937).
Education.
Beatty was a star football player at Washington-Lee High School in Arlington. Encouraged to act by the success of his sister, who had recently established herself as a Hollywood star, he decided to work as a stagehand at the National Theatre in Washington, D.C. during the summer before his senior year. He was reportedly offered ten football scholarships to college, but rejected them to study liberal arts at Northwestern University (1954–55), where he joined the Sigma Chi fraternity. After his first year, he left college to move to New York City, where he studied acting under Stella Adler at the Stella Adler Studio of Acting.
Military service.
Beatty enlisted in the California Air National Guard on February 11, 1960 under his original name, Henry W. Beaty. On January 1, 1961, Beatty was discharged from the Air National Guard due to physical disability. He was simultaneously discharged from the United States Air Force Reserve, and served on inactive duty only.
Career.
1950s and 1960s.
Beatty started his career making appearances on television shows such as "Studio One" (1957), "Kraft Television Theatre" (1957), and "Playhouse 90" (1959). He was a semi-regular on "The Many Loves of Dobie Gillis" during its first season (1959–60). His performance in William Inge's "A Loss of Roses" on Broadway garnered him a 1960 Tony Award nomination for Best Featured Actor in a Play and a 1960 Theatre World Award. It was his sole appearance on Broadway. He made his film debut in Elia Kazan's "Splendor in the Grass" (1961), opposite Natalie Wood. The film was a critical and box office success and Beatty was nominated for a Golden Globe Award for Best Actor, and received the award for New Star of the Year – Actor.
He followed his initial film with Tennessee Williams' "The Roman Spring of Mrs. Stone" (1961), with Vivien Leigh and Lotte Lenya, directed by Jose Quintero; "All Fall Down" (1962), with Angela Lansbury, Karl Malden and Eva Marie Saint, directed by John Frankenheimer; "Lilith" (1963), with Jean Seberg and Peter Fonda, directed by Robert Rossen; "Promise Her Anything" (1964), with Leslie Caron, Bob Cummings and Keenan Wynn, directed by Arthur Hiller; "Mickey One" (1965), with Alexandra Stewart and Hurd Hatfield, directed by Arthur Penn; and "Kaleidoscope" (1966), with Susannah York and Clive Revill, directed by Jack Smight.
In 1967, when he was 29 years old, he produced and acted in "Bonnie and Clyde". He assembled a team that included the writers Robert Benton and David Newman and the director Arthur Penn, chose Faye Dunaway, Gene Hackman and Estelle Parsons for lead roles, oversaw the script and spearheaded the delivery of the film. It was a critical and commercial success, and was nominated for ten Academy Awards, including Best Picture and Best Actor, and seven Golden Globe Awards, including Best Picture and Best Actor.
1970s and 1980s.
After "Bonnie and Clyde", Beatty acted with Elizabeth Taylor in "The Only Game in Town" (1970), directed by George Stevens; "McCabe & Mrs. Miller" (1971), directed by Robert Altman; "Dollars" (1971), directed by Richard Brooks; "The Parallax View" (1974), directed by Alan Pakula; and "The Fortune" (1975), directed by Mike Nichols. Beatty produced, co-wrote and acted in "Shampoo" (1975), directed by Hal Ashby, which was nominated for four Academy Awards, including Best Original Screenplay, as well as five Golden Globe Awards, including Best Motion Picture and Best Actor. In 1978, Beatty directed, produced, wrote and acted in "Heaven Can Wait" (1978) (sharing co-directing credit with Buck Henry). The film was nominated for nine Academy Awards, including Best Picture, Director, Actor, and Adapted Screenplay. It also won three Golden Globe Awards, including Best Motion Picture and Best Actor.
Beatty's next film was "Reds" (1981), an historical epic about American Communist journalist John Reed who observed the Russian October Revolution – a project Beatty had begun researching and filming for as far back as 1970. It was a critical and commercial success, despite being an American film about an American Communist made and released at the height of the Cold War. It received twelve Academy Award nominations – including four for Beatty (for Best Picture, Director, Actor, and Original Screenplay), winning three; Beatty won for Best Director, Maureen Stapleton won for Best Supporting Actress (playing anarchist Emma Goldman), and Vittorio Storaro won for Best Cinematography. The film received seven Golden Globe nominations, including Best Motion Picture, Director, Actor and Screenplay. Beatty won the Golden Globe Award for Best Director. Following "Reds", Beatty did not appear in a film for five years until 1987's "Ishtar", written and directed by Elaine May. Following severe criticism in press reviews by the new British studio chief David Puttnam just prior to its release, the film received mixed reviews and was commercially unsuccessful. Puttnam attacked several other over-budget U.S. films greenlit by his predecessor, and was fired shortly thereafter.
1990s and 2000s.
Beatty next produced, directed and played the title role as comic strip based detective Dick Tracy in the 1990 film of the same name. The film received critical acclaim and was one of the highest grossing of the year. It received seven Academy Award nominations, winning three for Best Art Direction, Best Makeup, and Best Original Song. It also received four Golden Globe Award nominations, including Best Motion Picture.
In 1991, he produced and starred as the real-life gangster Bugsy Siegel in the critically and commercially acclaimed "Bugsy", directed by Barry Levinson, which was nominated for ten Academy Awards, including Best Picture and Best Actor; it later won two of the awards for Best Art Direction and Best Costume Design. The film also received eight Golden Globe Award nominations, including Best Motion Picture and Best Actor, winning for Best Motion Picture. Beatty's next film, "Love Affair" (1994), directed by Glenn Gordon Caron, received mixed reviews and was unimpressive commercially.
In 1998, he wrote, produced, directed and starred in the political satire "Bulworth", which was critically acclaimed and was nominated for the Academy Award for Best Original Screenplay. The film also received three Golden Globe Award nominations, for Best Motion Picture, Best Actor, and Best Screenplay. Beatty has appeared briefly in numerous documentaries, including ' (1991) and ' (2005).
Following the disastrous box office performance of "Town & Country" (2001), in which Beatty starred, he did not appear in or direct another film until a Howard Hughes biopic (Untitled Warren Beatty project). The latter movie is as yet untitled and is slated for release in 2016.
2010s.
In 2010, Beatty directed and reprised his role as Dick Tracy in a 30-minute comedy film titled "Dick Tracy Special", which premiered on TCM. The short metafiction film stars Dick Tracy and film critic and historian Leonard Maltin, the latter of whom discusses the history and creation of Tracy. Tracy talks about how he admired Ralph Byrd and Morgan Conway who portrayed him in several films, but says he didn't care much for Beatty's portrayal of him or his film.
In April 2016, at an event honoring producer Arnon Milchan, Beatty said he is "very serious" about making a "Dick Tracy" sequel with Milchan producing. 
In the mid-1970s, Beatty signed a contract with Warner Bros. to star, produce, write, and possibly direct a film about Howard Hughes. It was also during this period that Beatty approached Paul Schrader to write a script on Hughes' life, which he declined. However, the project was put on hold when Beatty began "Heaven Can Wait". Initially, Beatty planned to film the life story of John Reed and Hughes back-to-back, but as he was getting deeper into the project, he eventually focused primarily on the John Reed film "Reds". After years of being away from the camera, in June 2011, it was reported that Beatty would produce, write, direct and star in a film about Hughes, focusing on an affair he had with a younger woman in the final years of his life. During this period, Beatty approached actors to star in his ensemble cast. He met with Andrew Garfield, Alec Baldwin, Owen Wilson, Justin Timberlake, Shia La Beouf, Jack Nicholson, Evan Rachel Wood, Rooney Mara, his wife Annette Bening, and his personal choice for the female lead, Felicity Jones. After Paramount Pictures exited the film, Regency Enterprises picked up the film in September 2011. The project began principal photography in February 2014 and wrapped in June of the same year. Some have said that Beatty's film is 40 years in the making.
Honors.
Beatty has received the Eleanor Roosevelt Award from the Americans for Democratic Action, the Brennan Legacy Award from the Brennan Center for Justice at the New York University School of Law, the Phillip Burton Public Service Award from the Foundation for Taxpayer and Consumer Rights, and the Spirit of Hollywood Award from the Associates for Breast and Prostate Cancer Studies. Beatty was a founding board member of the Center for National Policy, a founding member of the Progressive Majority, a member of the Council on Foreign Relations, has served as the Campaign Chair for the Permanent Charities Committee, and has participated in the World Economic Forum at Davos, Switzerland. He served on the Board of Trustees at the Scripps Research Institute, and the Board of Directors of the Motion Picture and Television Fund Foundation. He was named Honorary Chairman of the Stella Adler Studio of Acting in 2004.
The National Association of Theatre Owners awarded him with the Star of the Year Award in 1975, and in 1978 the Director of the Year Award and the Producer of the Year Award. He received the Alan J. Pakula Memorial Award from the National Board of Review in 1998. He received the Akira Kurosawa Lifetime Achievement Award in 2002 from the San Francisco International Film Festival. He has received the Board of Governors Award from the American Society of Cinematographers, the Distinguished Director Award from the Costume Designers Guild, the Life Achievement Award from the Publicists Guild, and the Outstanding Contribution to Cinematic Imagery Award from the Art Directors Guild. In 2004, he received the Kennedy Center Honors in Washington, D.C., and the Milestone Award from the Producers Guild of America. He was honored with the American Film Institute's Life Achievement Award in 2008. In March 2013, he was inducted into the California Hall of Fame.
Beatty has received a number of international awards: in 1992, he was made a Commander of the Order of Arts and Letters (France); in 1998, he was nominated for a Golden Lion for Best Film ("Bulworth"), and received a Career Golden Lion from the Venice Film Festival; in 2001, he received the Donostia Lifetime Achievement Award from the San Sebastián International Film Festival; in 2002, he received the British Academy Fellowship from BAFTA; and in 2011, he was awarded the Stanley Kubrick Britannia Award.
Personal life.
Beatty has been married to actress Annette Bening since 1992. They have four children: Stephen (born January 8, 1992), Benjamin (born August 23, 1994), Isabel (born January 11, 1997) and Ella (born April 8, 2000).
Prior to marrying Bening, Beatty was well known for his high profile romantic relationships that received generous media coverage. He had relationships with Madonna, Cher,
Serena, Twiggy, Iman, Natalie and Lana Wood, Michelle Phillips, Diane Keaton, Julie Christie, Leslie Caron, Isabelle Adjani, Mary Tyler Moore, Goldie Hawn, Kate Jackson, Joan Collins, Diane Sawyer, Connie Chung, Britt Ekland, Melanie Griffith, Barbara Hershey, Jacqueline Onassis, Maya Plisetskaya, Vanessa Redgrave, Dewi Sukarno, Princess Margaret, Jessica Savitch, Susan Strasberg, Brigitte Bardot, Janice Dickinson, 
Christine Kaufmann, Jane Fonda, Daryl Hannah, Barbara Minty, Margaux Hemingway, Mamie Van Doren, Barbara Harris, Elizabeth Hubbard, Princess Elizabeth of Yugoslavia, Joni Mitchell, Linda McCartney, Inger Stevens, Dayle Haddon, Carol Alt, Maria Callas, Brooke Hayward, Juliet Prowse, Joyce Hyser, Carole Mallory, Liv Ullmann, Diane von Furstenberg, Elle Macpherson, and Stephanie Seymour. Singer-songwriter Carly Simon also dated Beatty, and confirmed in November 2015 that she wrote a verse in her hit song "You're So Vain" about him.
Beatty is a longtime supporter of the Democratic Party. In 1972, Beatty was part of the "inner circle" of Senator George McGovern's presidential campaign. He traveled extensively and was instrumental in organizing fundraising.
In May 2005, Beatty sued Tribune Media, claiming he still maintained the rights to "Dick Tracy". On March 25, 2011, U.S. District Judge Dean Pregerson ruled in Beatty's favor.

</doc>
<doc id="62810" url="https://en.wikipedia.org/wiki?curid=62810" title="Reelin">
Reelin

Reelin (RELN) is a large secreted extracellular matrix glycoprotein that helps regulate processes of neuronal migration and positioning in the developing brain by controlling cell–cell interactions. Besides this important role in early development, reelin continues to work in the adult brain. It modulates synaptic plasticity by enhancing the induction and maintenance of long-term potentiation. It also stimulates dendrite and dendritic spine development and regulates the continuing migration of neuroblasts generated in adult neurogenesis sites like subventricular and subgranular zones. It is found not only in the brain, but also in the spinal cord, blood, and other body organs and tissues.
Reelin has been suggested to be implicated in pathogenesis of several brain diseases. The expression of the protein has been found to be significantly lower in schizophrenia and psychotic bipolar disorder, but the cause of this observation remains uncertain as studies show that psychotropic medication itself affects reelin expression. Moreover, epigenetic hypotheses aimed at explaining the changed levels of reelin expression are controversial. Total lack of reelin causes a form of lissencephaly. Reelin may also play a role in Alzheimer's disease, temporal lobe epilepsy and autism.
Reelin's name comes from the abnormal reeling gait of "reeler" mice, which were later found to have a deficiency of this brain protein and were homozygous for mutation of the RELN gene.
The primary phenotype associated with loss of reelin function is a failure of neuronal positioning throughout the developing central nervous system (CNS). The mice heterozygous for the reelin gene, while having little neuroanatomical defects, display the endophenotypic traits linked to psychotic disorders.
Discovery.
Mutant mice have provided insight into the underlying molecular mechanisms of the development of the central nervous system. Useful spontaneous mutations were first identified by scientists who were interested in motor behavior, and it proved relatively easy to screen littermates for mice that showed difficulties moving around the cage. A number of such mice were found and given descriptive names such as reeler, weaver, lurcher, nervous, and staggerer.
The "reeler" mouse was described for the first time in 1951 by D.S.Falconer in Edinburgh University as a spontaneous variant arising in a colony of mice maintained by geneticist Charlotte Auerbach. Histopathological studies in the 1960s revealed that the cerebellum of reeler mice is dramatically decreased in size while the normal laminar organization found in several brain regions is disrupted. The 1970s brought the discovery of cellular layers inversion in the mice neocortex, which attracted more attention to the reeler mutation.
In 1994, a new allele of reeler was obtained by means of insertional mutagenesis. This provided the first molecular marker of the locus, permitting the RELN gene to be mapped to chromosome 7q22 and subsequently cloned and identified. Japanese scientists at Kochi Medical School successfully raised antibodies against normal brain extracts in reeler mice, later these antibodies were found to be specific monoclonal antibodies for reelin, and were termed CR-50 (Cajal-Retzius marker 50). They noted that CR-50 reacted specifically with Cajal-Retzius neurons, whose functional role was unknown until then.
The Reelin receptors, apolipoprotein E receptor 2 (ApoER2) and very-low-density lipoprotein receptor (VLDLR), were discovered by Trommsdorff, Herz and colleagues, who initially found that the cytosolic adaptor protein Dab1 interacts with the cytoplasmic domain of LDL receptor family members. They then went on to show that the double knockout mice for ApoER2 and VLDLR, which both interact with Dab1, had cortical layering defects similar to those in reeler.
The downstream pathway of reelin was further clarified with the help of other mutant mice, including yotari and scrambler. These mutants have phenotypes similar to that of reeler mice, but without mutation in reelin. It was then demonstrated that the mouse "disabled homologue 1" (Dab1) gene is responsible for the phenotypes of these mutant mice, as Dab1 protein was absent (yotari) or only barely detectable (scrambler) in these mutants. Targeted disruption of Dab1 also caused a phenotype similar to that of reeler. Pinpointing the DAB1 as a pivotal regulator of the reelin signaling cascade started the tedious process of deciphering its complex interactions.
There followed a series of speculative reports linking reelin's genetic variation and interactions to schizophrenia, Alzheimer's disease, autism and other highly complex dysfunctions. These and other discoveries, coupled with the perspective of unraveling the evolutionary changes that allowed for the creation of human brain, highly intensified the research. As of 2008, some 13 years after the gene coding the protein was discovered, hundreds of scientific articles address the multiple aspects of its structure and functioning.
Tissue distribution and secretion.
Studies show that reelin is absent from synaptic vesicles and is secreted via constitutive secretory pathway, being stored in Golgi secretory vesicles. Reelin's release rate is not regulated by depolarization, but strictly depends on its synthesis rate. This relationship is similar to that reported for the secretion of other extracellular matrix proteins.
During the brain development, reelin is secreted in the cortex and hippocampus by the so-called Cajal-Retzius cells, Cajal cells, and Retzius cells. Reelin-expressing cells in the prenatal and early postnatal brain are predominantly found in the marginal zone (MZ) of the cortex and in the temporary subpial granular layer (SGL), which is manifested to the highest extent in human, and in the hippocampal stratum lacunosum-moleculare and the upper marginal layer of the dentate gyrus.
In the developing cerebellum, reelin is expressed first in the external granule cell layer (EGL), before the granule cell migration to the internal granule cell layer (IGL) takes place.
Having peaked just after the birth, the synthesis of reelin subsequently goes down sharply, becoming more diffuse compared with the distinctly laminar expression in the developing brain. In the adult brain, reelin is expressed by GABA-ergic interneurons of the cortex and glutamatergic cerebellar neurons, and by the few extant Cajal-Retzius cells. Among GABAergic interneurons, reelin seems to be detected predominantly in those expressing calretinin and calbindin, like bitufted, horizontal, and Martinotti cells, but not parvalbumin-expressing cells, like chandelier or basket neurons. In the white matter, a minute proportion of interstitial neurons has also been found to stain positive for reelin expression.
Outside the brain, reelin is found in adult mammalian blood, liver, pituitary pars intermedia, and adrenal chromaffin cells. In the liver, reelin is localized in hepatic stellate cells. The expression of reelin increases when the liver is damaged, and returns to normal following its repair.
In the eyes, reelin is secreted by retinal ganglion cells and is also found in the endothelial layer of the cornea. Just as in the liver, its expression increases after an injury has taken place.
The protein is also produced by the odontoblasts, which are cells at the margins of the dental pulp. Reelin is found here both during odontogenesis and in the mature tooth. Some authors suggest that odontoblasts play an additional role as sensory cells able to transduce pain signals to the nerve endings. According to the hypothesis, reelin participates in the process by enhancing the contact between odontoblasts and the nerve terminals.
Structure.
Reelin is composed of 3461 amino acids with a relative molecular mass of 388 kDa. It also has serine protease activity. Murine RELN gene consists of 65 exons spanning approximately 450 kb. One exon, coding for only two amino acids near the protein's C-terminus, undergoes alternative splicing, but the exact functional impact of this is unknown. Two transcription initiation sites and two polyadenylation sites are identified in the gene structure.
The reelin protein starts with a signaling peptide 27 amino acids in length, followed by a region bearing similarity to F-spondin (the reeler domain), marked as "SP" on the scheme, and by a region unique to reelin, marked as "H". Next comes 8 repeats of 300–350 amino acids. These are called "reelin repeats" and have an epidermal growth factor motif at their center, dividing each repeat into two subrepeats, "A" (the BNR/Asp-box repeat) and "B" (the EGF-like domain). Despite this interruption, the two subdomains make direct contact, resulting in a compact overall structure.
The final reelin domain contains a highly basic and short C-terminal region (CTR, marked "+") with a length of 32 amino acids. This region is highly conserved, being 100% identical in all investigated mammals. It was thought that CTR is necessary for reelin secretion, because the Orleans reeler mutation, which lacks a part of 8th repeat and the whole CTR, is unable to secrete the misshaped protein, leading to its concentration in cytoplasm. However, other studies have shown that the CTR is not essential for secretion itself, but mutants lacking the CTR were much less efficient in activating downstream signaling events.
Reelin is cleaved "in vivo" at two sites located after domains 2 and 6 – approximately between repeats 2 and 3 and between repeats 6 and 7, resulting in the production of three fragments. This splitting does not decrease the protein's activity, as constructs made of the predicted central fragments (repeats 3–6) bind to lipoprotein receptors, trigger Dab1 phosphorylation and mimic functions of reelin during cortical plate development. Moreover, the processing of reelin by embryonic neurons may be necessary for proper corticogenesis.
Function.
The primary functions of Reelin are the regulation of corticogenesis and neuronal cell positioning in the prenatal period, but the protein also continues to play a role in adults. Reelin is found in numerous tissues and organs, and one could roughly subdivide its functional roles by the time of expression and by localisation of its action.
During development.
A number of non-nervous tissues and organs express reelin during development, with the expression sharply going down after organs have been formed. The role of the protein here is largely unexplored, because the knockout mice show no major pathology in these organs. Reelin's role in the growing central nervous system has been extensively characterized. It promotes the differentiation of progenitor cells into radial glia and affects the orientation of its fibers, which serve as the guides for the migrating neuroblasts. The position of reelin-secreting cell layer is important, because the fibers orient themselves in the direction of its higher concentration. For example, reelin regulates the development of layer-specific connections in hippocampus and entorhinal cortex.
Mammalian corticogenesis is another process where reelin plays a major role. In this process the temporary layer called preplate is split into the marginal zone on the top and subplate below, and the space between them is populated by neuronal layers in the inside-out pattern. Such an arrangement, where the newly created neurons pass through the settled layers and position themselves one step above, is a distinguishing feature of mammalian brain, in contrast to the evolutionary older reptile cortex, in which layers are positioned in an "outside-in" fashion. When reelin is absent, like in the mutant reeler mouse, the order of cortical layering becomes roughly inverted, with younger neurons finding themselves to be unable to pass the settled layers. Subplate neurons fail to stop and invade the upper most layer, creating the so-called superplate in which they mix with Cajal-Retzius cells and some cells normally destined for the second layer.
There is no agreement concerning the role of reelin in the proper positioning of cortical layers. The original hypothesis, that the protein is a stop signal for the migrating cells, is supported by its ability to induce the dissociation, its role in asserting the compact granule cell layer in the hippocampus, and by the fact that migrating neuroblasts evade the reelin-rich areas. But an experiment in which murine corticogenesis went normally despite the malpositioned reelin secreting layer, and lack of evidence that reelin affects the growth cones and leading edges of neurons, caused some additional hypotheses to be proposed. According to one of them, reelin makes the cells more susceptible to some yet undescribed positional signaling cascade.
Reelin may also ensure correct neuronal positioning in the spinal cord: according to one study, location and level of its expression affects the movement of sympathetic preganglionic neurons.
The protein is thought to act on migrating neuronal precursors and thus controls correct cell positioning in the cortex and other brain structures. The proposed role is one of a dissociation signal for neuronal groups, allowing them to separate and go from tangential chain-migration to radial individual migration. Dissociation detaches migrating neurons from the glial cells that are acting as their guides, converting them into individual cells that can strike out alone to find their final position.
Reelin takes part in the developmental change of NMDA receptor configuration, increasing mobility of NR2B-containing receptors and thus decreasing the time they spend at the synapse. It has been hypothesized that this may be a part of the mechanism behind the "NR2B-NR2A switch" that is observed in the brain during its postnatal development. Ongoing reelin secretion by GABAergic hippocampal neurons is necessary to keep NR2B-containing NMDA receptors at a low level.
In adults.
In the adult nervous system, reelin plays an eminent role at the two most active neurogenesis sites, the subventricular zone and the dentate gyrus. In some species, the neuroblasts from the subventricular zone migrate in chains in the rostral migratory stream (RMS) to reach the olfactory bulb, where reelin dissociates them into individual cells that are able to migrate further individually. They change their mode of migration from tangential to radial, and begin using the radial glia fibers as their guides. There are studies showing that along the RMS itself the two receptors, ApoER2 and VLDLR, and their intracellular adapter DAB1 function independently of Reelin, most likely by the influence of a newly proposed ligand, thrombospondin-1. In the adult dentate gyrus, reelin provides guidance cues for new neurons that are constantly arriving to the granule cell layer from subgranular zone, keeping the layer compact.
Reelin also plays an important role in the adult brain by modulating cortical pyramidal neuron dendritic spine expression density, the branching of dendrites, and the expression of long-term potentiation as its secretion is continued diffusely by the GABAergic cortical interneurons those origin is traced to the medial ganglionic eminence.
In the adult organism the non-neural expression is much less widespread, but goes up sharply when some organs are injured. The exact function of reelin upregulation following an injury is still being researched.
Evolutionary significance.
Reelin-DAB1 interactions could have played a key role in the structural evolution of the cortex that evolved from a single layer in the common predecessor of the amniotes into multiple-layered cortex of contemporary mammals. Research shows that reelin expression goes up as the cortex becomes more complex, reaching the maximum in the human brain in which the reelin-secreting Cajal-Retzius cells have significantly more complex axonal arbour. Reelin is present in the telencephalon of all the vertebrates studied so far, but the pattern of expression differs widely. For example, zebrafish have no Cajal-Retzius cells at all; instead, the protein is being secreted by other neurons. These cells do not form a dedicated layer in amphibians, and radial migration in their brains is very weak.
As the cortex becomes more complex and convoluted, migration along the radial glia fibers becomes more important for the proper lamination. The emergence of a distinct reelin-secreting layer is thought to play an important role in this evolution. There are conflicting data concerning the importance of this layer, and these are explained in the literature either by the existence of an additional signaling positional mechanism that interacts with the reelin cascade, or by the assumption that mice that are used in such experiments have redundant secretion of reelin compared with more localized synthesis in the human brain.
Cajal-Retzius cells, most of which disappear around the time of birth, coexpress reelin with the HAR1 gene that is thought to have undergone the most significant evolutionary change in humans compared with chimpanzee, being the most "evolutionary accelerated" of the genes from the human accelerated regions. There is also evidence of that variants in the DAB1 gene have been included in a recent selective sweep in Chinese populations.
Mechanism of action.
Receptors.
Reelin's control of cell–cell interactions is thought to be mediated by binding of reelin to the two members of low density lipoprotein receptor gene family: VLDLR and the ApoER2. The two main reelin receptors seem to have slightly different roles: VLDLR conducts the stop signal, while ApoER2 is essential for the migration of late-born neocortical neurons. It also has been shown that the N-terminal region of reelin, a site distinct from the region of reelin shown to associate with VLDLR/ApoER2 binds to the alpha-3-beta-1 integrin receptor. The proposal that the protocadherin CNR1 behaves as a Reelin receptor has been disproven.
As members of lipoprotein receptor superfamily, both VLDLR and ApoER2 have in their structure an internalization domain called NPxY motif. After binding to the receptors reelin is internalized by endocytosis, and the N-terminal fragment of the protein is re-secreted. This fragment may serve postnatally to prevent apical dendrites of cortical layer II/III pyramidal neurons from overgrowth, acting via a pathway independent of canonical reelin receptors.
Reelin receptors are present on both neurons and glial cells. Furthermore, radial glia express the same amount of ApoER2 but being ten times less rich in VLDLR. beta-1 integrin receptors on glial cells play more important role in neuronal layering than the same receptors on the migrating neuroblasts.
Reelin-dependent strengthening of long-term potentiation is caused by ApoER2 interaction with NMDA receptor. This interaction happens when ApoER2 has a region coded by exon 19. ApoER2 gene is alternatively spliced, with the exon 19-containing variant more actively produced during periods of activity. According to one study, the hippocampal reelin expression rapidly goes up when there is need to store a memory, as demethylases open up the RELN gene. The activation of dendrite growth by reelin is apparently conducted through Src family kinases and is dependent upon the expression of Crk family proteins, consistent with the interaction of Crk and CrkL with tyrosine-phosphorylated Dab1. Moreover, a Cre-loxP recombination mouse model that lacks Crk and CrkL in most neurons was reported to have the reeler phenotype, indicating that Crk/CrkL lie between DAB1 and Akt in the reelin signaling chain.
Signaling cascades.
Reelin activates the signaling cascade of Notch-1, inducing the expression of FABP7 and prompting progenitor cells to assume radial glial phenotype. In addition, corticogenesis "in vivo" is highly dependent upon reelin being processed by embrionic neurons, which are thought to secrete some as yet unidentified metalloproteinases that free the central signal-competent part of the protein. Some other unknown proteolytic mechanisms may also play a role. It is supposed that full-sized reelin sticks to the extracellular matrix fibers on the higher levels, and the central fragments, as they are being freed up by the breaking up of reelin, are able to permeate into the lower levels. It is possible that as neuroblasts reach the higher levels they stop their migration either because of the heightened combined expression of all forms of reelin, or due to the peculiar mode of action of the full-sized reelin molecules and its homodimers.
The intracellular adaptor DAB1 binds to the VLDLR and ApoER2 through an NPxY motif and is involved in transmission of Reelin signals through these lipoprotein receptors. It becomes phosphorylated by Src and Fyn kinases and apparently stimulates the actin cytoskeleton to change its shape, affecting the proportion of integrin receptors on the cell surface, which leads to the change in adhesion. Phosphorylation of DAB1 leads to its ubiquitination and subsequent degradation, and this explains the heightened levels of DAB1 in the absence of reelin. Such negative feedback is thought to be important for proper cortical lamination. Activated by two antibodies, VLDLR and ApoER2 cause DAB1 phosphorylation but seemingly without the subsequent degradation and without rescuing the reeler phenotype, and this may indicate that a part of the signal is conducted independently of DAB1.
A protein having an important role in lissencephaly and accordingly called LIS1 (PAFAH1B1), was shown to interact with the intracellular segment of VLDLR, thus reacting to the activation of reelin pathway.
Complexes.
Reelin molecules have been shown to form a large protein complex, a disulfide-linked homodimer. If the homodimer fails to form, efficient tyrosine phosphorylation of DAB1 "in vitro" fails. Moreover, the two main receptors of reelin are able to form clusters that most probably play a major role in the signaling, causing the intracellular adaptor DAB1 to dimerize or oligomerize in its turn. Such clustering has been shown in the study to activate the signaling chain even in the absence of Reelin itself. In addition, reelin itself can cut the peptide bonds holding other proteins together, being a serine protease, and this may affect the cellular adhesion and migration processes. Reelin signaling leads to phosphorylation of actin-interacting protein cofilin 1 at ser3; this may stabilize the actin cytoskeleton and anchor the leading processes of migrating neuroblasts, preventing their further growth.
Interaction with Cdk5.
Cyclin-dependent kinase 5 (Cdk5), a major regulator of neuronal migration and positioning, is known to phosphorylate DAB1 and other cytosolic targets of reelin signaling, such as Tau, which could be activated also via reelin-induced deactivation of GSK3B, and NUDEL, associated with Lis1, one of the DAB1 targets. LTP induction by reelin in hippocampal slices fails in p35 knockouts. P35 is a key Cdk5 activator, and double p35/Dab1, p35/RELN, p35/ApoER2, p35/VLDLR knockouts display increased neuronal migration deficits, indicating a synergistic action of reelin → ApoER2/VLDLR → DAB1 and p35/p39 → Cdk5 pathways in the normal corticogenesis.
Possible pathological role.
Lissencephaly.
Disruptions of the RELN gene are considered to be the cause of the rare form of lissencephaly with cerebellar hypoplasia called Norman-Roberts syndrome. The mutations disrupt splicing of the RELN mRNA transcript, resulting in low or undetectable amounts of reelin protein. The phenotype in these patients was characterized by hypotonia, ataxia, and developmental delay, with lack of unsupported sitting and profound mental retardation with little or no language development. Seizures and congenital lymphedema are also present. A novel chromosomal translocation causing the syndrome was described in 2007. The mutations affecting reelin in human are usually associated with consanguineous marriage.
Schizophrenia.
Reduced expression of reelin and its mRNA levels in the brains of schizophrenia sufferers had been reported in 1998 and 2000 and independently confirmed in the postmortem studies of hippocampus, cerebellum, basal ganglia, and in the cortex studies. The reduction may reach up to 50% in some brain regions and is coupled with reduced expression of GAD-67 enzyme, which catalyses the transition of glutamate to GABA. Blood levels of reelin and its isoforms are also altered in schizophrenia, along with mood disorders, according to one study. Reduced reelin mRNA prefrontal expression in schizophrenia was found to be the most statistically relevant disturbance found in the multicenter study conducted in 14 separate laboratories in 2001 by Stanley Foundation Neuropathology Consortium.
Epigenetic hypermethylation of DNA in schizophrenia patients is proposed as a cause of the reduction, in agreement with the observations dating from the 1960s that administration of methionine to schizophrenic patients results in a profound exacerbation of schizophrenia symptoms in sixty to seventy percent of patients. The proposed mechanism is a part of the "epigenetic hypothesis for schizophrenia pathophysiology" formulated by a group of scientists in 2008 (D. Grayson; A. Guidotti; E. Costa). A postmortem study comparing a DNA methyltransferase (DNMT1) and Reelin mRNA expression in cortical layers I and V of schizophrenic patients and normal controls demonstrated that in the layer V both DNMT1 and Reelin levels were normal, while in the layer I DNMT1 was threefold higher, probably leading to the twofold decrease in the Reelin expression. There is evidence that the change is selective, and DNMT1 is overexpressed in reelin-secreting GABAergic neurons but not in their glutamatergic neighbours. Methylation inhibitors and histone deacetylase inhibitors, such as valproic acid, increase reelin mRNA levels, while L-methionine treatment downregulates the phenotypic expression of reelin. One study indicated the upregulation of histone deacetylase HDAC1 in the hippocampi of patients. Histone deacetylases suppress gene promoters; hyperacetylation of hystones was shown in murine models to demethylate the promoters of both reelin and GAD67. DNMT1 inhibitors in animals have been shown to increase the expression of both reelin and GAD67, and both DNMT inhibitors and HDAC inhibitors shown in one study to activate both genes with comparable dose- and time-dependence. As one study shows, S-adenosyl methionine (SAM) concentration in patients' prefrontal cortex is twice as high as in the cortices of non-affected people. SAM, being a methyl group donor necessary for DNMT activity, could further shift epigenetic control of gene expression.
The factors mentioned above serve to corroborate the epigenetic hypothesis. But it is worth mentioning that in contrast with initial data, two recent studies have failed to confirm the RELN hypermethylation, and psychotropic medication could in itself affect the reelin expression in the brain, as animal studies show (see below).
Other interesting findings probably linking reelin pathway to developmental hypotheses of schizophrenia are noted in the studies on mice that are either prenatally infected with influenza virus or have their immune system activated artificially during pregnancy. The Cajal-Retzius cells in the newborns secrete significantly less reelin despite keeping their expression of calretinin and nNos within normal range. These data run in parallel with the findings of increased risk of schizophrenia in humans after a prenatal infection during the second trimester.
Chromosome region 7q22 that harbours the "RELN" gene is associated with schizophrenia, and the gene itself was associated with the disease in a large study that found the polymorphism rs7341475 to increase the risk of the disease in women, but not in men. The women that have the single-nucleotide polymorphism (SNP) are about 1.4 times more likely to get ill, according to the study. Allelic variations of RELN have also been correlated with working memory, memory and executive functioning in nuclear families where one of the members suffers from schizophrenia. The association with working memory was later replicated. In one small study, nonsynonymous polymorphism Val997Leu of the gene was associated with left and right ventricular enlargement in patients.
One study showed that patients have decreased levels of one of reelin receptors, VLDLR, in the peripheral lymphocytes. After six months of antipsychotic therapy the expression went up; according to authors, peripheral VLRLR levels may serve as a reliable peripheral biomarker of schizophrenia.
Considering the role of reelin in promoting dendritogenesis, suggestions were made that the localized dendritic spine deficit observed in schizophrenia could be in part connected with the downregulation of reelin.
Reelin pathway could also be linked to schizophrenia and other psychotic disorders through its interaction with risk genes. One example is the neuronal transcription factor NPAS3, disruption of which is linked to schizophrenia and learning disability. Knockout mice lacking NPAS3 or the similar protein NPAS1 have significantly lower levels of reelin; the precise mechanism behind this is unknown. Another example is the schizophrenia-linked gene MTHFR, with murine knockouts showing decreased levels of reelin in the cerebellum. Along the same line, it is worth noting that the gene coding for the subunit NR2B that is presumably affected by reelin in the process of NR2B->NR2A developmental change of NMDA receptor composition, stands as one of the strongest risk gene candidates. Another shared aspect between NR2B and RELN is that they both can be regulated by the TBR1 transcription factor.
The heterozygous reeler mouse, which is haploinsufficient for the RELN gene, shares several neurochemical and behavioral abnormalities with schizophrenia and bipolar disorder, but is considered not suitable for use as a genetic mouse model of schizophrenia.
Bipolar disorder.
Decrease in RELN expression with concurrent upregulation of DNMT1 is typical of bipolar disorder with psychosis, but is not characteristic of patients with major depression without psychosis, which could speak of specific association of the change with psychoses. One study suggests that unlike in schizophrenia, such changes are found only in the cortex and do not affect the deeper structures in psychotic bipolar patients, as their basal ganglia were found to have the normal levels of DNMT1 and subsequently both the reelin and GAD67 levels were within the normal range.
In a genetic study conducted in 2009, preliminary evidence requiring further DNA replication suggested that variation of the RELN gene (SNP rs362719) may be associated with susceptibility to bipolar disorder in women.
Autism.
Autism is a neurodevelopmental disorder that is generally believed to be caused by mutations in several locations, likely triggered by environmental factors. The role of reelin in autism is not decided yet.
Reelin was originally in 2001 implicated in a study finding associations between autism and a polymorphic GGC/CGG repeat preceding the 5' ATG initiator codon of the RELN gene in an Italian population. Longer triplet repeats in the 5’ region were associated with an increase in autism susceptibility. However, another study of 125 multiple-incidence families and 68 single-incidence families from the subsequent year found no significant difference between the length of the polymorphic repeats in affected and controls. Although, using a family based association test larger "reelin" alleles were found to be transmitted more frequently than expected to affected children. An additional study examining 158 subjects with German lineage likewise found no evidence of triplet repeat polymorphisms associated with autism. And a larger study from 2004 consisting of 395 families found no association between autistic subjects and the CGG triplet repeat as well as the allele size when compared to age of first word.
In 2010 a large study using data from 4 European cohorts would find some evidence for an association between autism and the rs362780 RELN polymorphism.
Studies of transgenic mice have been suggestive of an association, but not definitive.
Temporal lobe epilepsy: granule cell dispersion.
Decreased reelin expression in the hippocampal tissue samples from patients with temporal lobe epilepsy was found to be directly correlated with the extent of granule cell dispersion (GCD), a major feature of the disease that is noted in 45%–73% of patients. The dispersion, according to a small study, is associated with the RELN promoter hypermethylation. According to one study, prolonged seizures in a rat model of mesial temporal lobe epilepsy have led to the loss of reelin-expressing interneurons and subsequent ectopic chain migration and aberrant integration of newborn dentate granule cells. Without reelin, the chain-migrating neuroblasts failed to detach properly. Moreover, in a kainate-induced mouse epilepsy model, exogenous reelin had prevented GCD, according to one study.
Alzheimer's disease.
The Reelin receptors ApoER2 and VLDLR belong to the LDL receptor gene family. All members of this family are receptors for Apolipoprotein E (ApoE). Therefore, they are often synonymously referred to as 'ApoE receptors'. ApoE occurs in 3 common isoforms (E2, E3, E4) in the human population. ApoE4 is the primary genetic risk factor for late-onset Alzheimer's disease. This strong genetic association has led to the proposal that ApoE receptors play a central role in the pathogenesis of Alzheimer's Disease. According to one study, reelin expression and glycosylation patterns are altered in Alzheimer's disease. In the cortex of the patients, reelin levels were 40% higher compared with controls, but the cerebellar levels of the protein remain normal in the same patients. This finding is in agreement with an earlier study showing the presence of Reelin associated with amyloid plaques in a transgenic AD mouse model. A large genetic study of 2008 showed that RELN gene variation is associated with an increased risk of Alzheimer's disease in women. The number of reelin-producing Cajal-Retzius cells is significantly decreased in the first cortical layer of patients. Reelin has been shown to interact with amyloid precursor protein, and, according to one in-vitro study, is able to counteract the Aβ-induced dampening of NMDA-receptor activity. This is modulated by ApoE isoforms, which selectively alter the recycling of ApoER2 as well as AMPA and NMDA receptors.
Cancer.
DNA methylation patterns are often changed in tumours, and the RELN gene could be affected: according to one study, in the pancreatic cancer the expression is suppressed, along with other reelin pathway components In the same study, cutting the reelin pathway in cancer cells that still expressed reelin resulted in increased motility and invasiveness. On the contrary, in prostate cancer the RELN expression is excessive and correlates with Gleason score. Retinoblastoma presents another example of RELN overexpression. This gene has also been seen recurrently mutated in cases of acute lymphoblastic leukaemia.
Other conditions.
One genome-wide association study indicates a possible role for RELN gene variation in otosclerosis, an abnormal growth of bone of the middle ear. In a statistical search for the genes that are differentially expressed in the brains of cerebral malaria-resistant versus cerebral malaria-susceptible mice, Delahaye et al. detected a significant upregulation of both RELN and DAB1 and speculated on possible protective effects of such over-expression.
Factors affecting reelin expression.
The expression of reelin is controlled by a number of factors besides the sheer number of Cajal-Retzius cells. For example, TBR1 transcription factor regulates RELN along with other T-element-containing genes. On a higher level, increased maternal care was found to correlate with reelin expression in rat pups; such correlation was reported in hippocampus and in the cortex. According to one report, prolonged exposure to corticosterone significantly decreased reelin expression in murine hippocampi, a finding possibly pertinent to the hypothetical role of corticosteroids in depression. One small postmortem study has found increased methylation of RELN gene in the neocortex of persons past their puberty compared with those that had yet to enter the period of maturation.
Psychotropic medication.
As reelin is being implicated in a number of brain disorders and its expression is usually measured posthumously, assessing the possible medication effects is important.
According to the epigenetic hypothesis, drugs that shift the balance in favour of demethylation have a potential to alleviate the proposed methylation-caused downregulation of RELN and GAD67. In one study, clozapine and sulpiride but not haloperidol and olanzapine were shown to increase the demethylation of both genes in mice pretreated with l-methionine. Valproic acid, a histone deacetylase inhibitor, when taken in combination with antipsychotics, is proposed to have some benefits. But there are studies conflicting the main premise of the epigenetic hypothesis, and a study by Fatemi et al. shows no increase in RELN expression by valproic acid; that indicates the need for further investigation.
Fatemi et al. conducted the study in which RELN mRNA and reelin protein levels were measured in rat prefrontal cortex following a 21-day of intraperitoneal injections of the following drugs:
In 2009, Fatemi et al. published the more detailed work on rats using the same medication. Here, cortical expression of several participants (VLDLR, DAB1, GSK3B) of the signaling chain was measured besides reelin itself, and also the expression of GAD65 and GAD67.

</doc>
<doc id="62811" url="https://en.wikipedia.org/wiki?curid=62811" title="Walter Matthau">
Walter Matthau

Walter Matthau (; October 1, 1920 – July 1, 2000) was an American actor best known for his role as Oscar Madison in "The Odd Couple" and his frequent collaborations with "Odd Couple" co-star Jack Lemmon. He won the Academy Award for Best Supporting Actor for his performance in the 1966 Billy Wilder film "The Fortune Cookie." Besides the Oscar, he was the winner of BAFTA, Golden Globe and Tony awards.
Early life.
Matthau was born Walter John Matthow on October 1, 1920, in New York City's Lower East Side.
His mother, Rose (née Berolsky), was a Lithuanian Jewish immigrant who worked in a garment sweatshop, and his father, Milton Matthow, was a Russian Jewish peddler and electrician, from Kyiv, Ukraine. As part of a lifelong love of practical jokes, Matthau himself created the rumors that his middle name was Foghorn and his last name was originally Matuschanskayasky (under which he is credited for a cameo role in the film "Earthquake").
As a young boy, Matthow attended a Jewish non-profit sleepaway camp, Tranquillity Camp, where he first began acting in the shows the camp would stage on Saturday nights. He also attended Surprise Lake Camp. His high school was Seward Park High School. He worked for a short time as a concession stand cashier in the Yiddish Theater District.
Career.
During World War II, Matthau served in the U.S. Army Air Forces with the Eighth Air Force in Britain as a B-24 Liberator radioman-gunner, in the same 453rd Bombardment Group as James Stewart. He was based at RAF Old Buckenham, Norfolk during this time. He reached the rank of staff sergeant and became interested in acting. 
He took classes in acting at the Dramatic Workshop of the New School with German director Erwin Piscator. He often joked that his best early review came in a play where he posed as a derelict. One reviewer said, "The others just looked like actors in make-up, Walter Matthau really looks like a skid row bum!" Matthau was a respected stage actor for years in such fare as "Will Success Spoil Rock Hunter?" and "A Shot in the Dark". He won the 1962 Tony Award for Best Featured Actor in a play.
Matthau appeared in the pilot of "Mister Peepers" (1952) with Wally Cox. For reasons unknown he used the name Leonard Elliot. His role was of the gym teacher Mr. Wall. He made his motion picture debut as a whip-wielding bad guy in "The Kentuckian" (1955) opposite Burt Lancaster. He played a villain in "King Creole" (1958), in which he gets beaten up by Elvis Presley). Around the same time, he made "Ride a Crooked Trail" with Audie Murphy, and "Onionhead" (both 1958) starring Andy Griffith; the latter was a flop. Matthau had a featured role opposite Griffith in the well received drama "A Face in the Crowd" (1957), directed by Elia Kazan. Matthau also directed a low-budget movie called "The Gangster Story" (1960) and was a sympathetic sheriff in "Lonely are the Brave" (1962), which starred Kirk Douglas. He appeared opposite Audrey Hepburn in "Charade" (1963).
Appearances on television were common too, including two on "Naked City", as well as an episode of "The Eleventh Hour" ("A Tumble from a Tall White House", 1963) . He appeared eight times between 1962 and 1964 on "The DuPont Show of the Week" and as Franklin Gaer in an episode of "Dr. Kildare" ("Man Is a Rock", 1964). Additionally he featured in the syndicated crime drama "Tallahassee 7000", as a Florida-based state police investigator (1961–62).
Comedies were rare in Matthau's work at that time. He was cast in a number of stark dramas, such as "Fail Safe" (1964), in which he portrayed Pentagon adviser Dr. Groeteschele, who urges an all-out nuclear attack on the Soviet Union in response to an accidental transmission of an attack signal to U.S. Air Force bombers. Neil Simon cast him in the play "The Odd Couple" in 1965, with Matthau playing slovenly sportswriter Oscar Madison, opposite Art Carney as Felix Unger. Matthau later reprised the role in the film version, with Jack Lemmon as Felix Ungar. He played detective Ted Casselle in the Hitchcockian thriller "Mirage" (1965), directed by Edward Dmytryk.
He achieved great success in the comedy film, "The Fortune Cookie" (1966), as a shyster lawyer, William H. "Whiplash Willie" Gingrich, starring opposite Lemmon, and the first of many collaborations with Billy Wilder, and a role that would earn him an Oscar for Best Supporting Actor. Filming had to be placed on a five-month hiatus after Matthau suffered a serious heart attack. He gave up his three pack a day smoking habit as a result. Matthau appeared during the Oscar telecast shortly after having been injured in a bicycle accident; nonetheless, he scolded actors who had not attended the ceremony, especially the other major award winners that night: Paul Scofield, Elizabeth Taylor and Sandy Dennis.
Oscar nominations would come Matthau's way again for "Kotch" (1971), directed by Lemmon, and "The Sunshine Boys" (1975), another adaptation of a Neil Simon stage play, this time about a pair of former vaudeville stars. For the latter role he won a Golden Globe award for Best Actor in a Musical or Comedy.
Broadway hits turned into films continued to cast Matthau in lead roles in "Hello, Dolly!" and "Cactus Flower" (both 1969); for the latter film, Goldie Hawn received an Oscar for Best Supporting Actress. Matthau played three roles in the film version of Simon's "Plaza Suite" (1971) and was in the cast of its followup "California Suite" (1978).
Matthau starred in three crime dramas in the mid-1970s, as a detective investigating a mass murder on a bus in "The Laughing Policeman" (1973), as a bank robber on the run from the Mafia and the law in "Charley Varrick" (also 1973) and as a New York transit cop in the action-adventure "The Taking of Pelham One Two Three" (1974). A change of pace about misfits on a Little League baseball team turned-out to be a solid hit when Matthau starred as coach Morris Buttermaker in the comedy "The Bad News Bears" (1976). Matthau portrayed Herbert Tucker in "I Ought to Be in Pictures" (1982), with Ann-Margret and Dinah Manoff.
Matthau played Albert Einstein in the film "I.Q." (1994), starring Tim Robbins and Meg Ryan. His partnership with Lemmon became one of the most successful pairings in Hollywood. They became lifelong friends after making "The Fortune Cookie" and would make a total of 10 movies together—11 counting "Kotch", in which Lemmon has a cameo as a sleeping bus passenger. Apart from their many comedies, the two appeared (although they did not share any scenes) in the Oliver Stone drama, "JFK" (1991). Matthau narrated the "Doctor Seuss Video Classics: How the Grinch Stole Christmas!" (1992) and played the role of Mr. Wilson in the film "Dennis the Menace" (1993).
Matthau and Lemmon reunited for the comedy "Grumpy Old Men" (1993), co-starring Ann-Margret, and its sequel, "Grumpier Old Men" (1995), also co-starring Sophia Loren. This led to further pairings late in their careers, "Out to Sea" (1997) and a Simon-scripted sequel to their much earlier success, "The Odd Couple II" (1998). "Hanging Up" (2000), directed by Diane Keaton, was Matthau's final appearance onscreen.
Personal life.
Marriages.
Matthau was married twice; first to Grace Geraldine Johnson from 1948 to 1958, and then to Carol Marcus from 1959 until he died in 2000. He had two children, Jenny and David, by his first wife, and a son, Charlie Matthau, with his second wife. David is a radio news reporter, currently at WKXW "New Jersey 101.5" in Trenton, New Jersey. Jenny is president of the Natural Gourmet Institute in New York City. Matthau also helped raise his stepchildren, Aram Saroyan and Lucy Saroyan. His grandchildren include William Matthau, an engineer, and Emily Rose Roman, a student at Binghamton University. Charlie Matthau directed his father in "The Grass Harp" (1995).
Health problems.
A heavy smoker and drinker, Matthau suffered a heart attack in 1966, the first of at least three in his lifetime. In 1976, ten years after his first heart attack, he underwent heart bypass surgery. After working in freezing Minnesota weather for "Grumpy Old Men" (1993), he was hospitalized for double pneumonia. In December 1995 he had a colon tumor removed; it tested benign. He was also hospitalized in May 1999 for more than two months owing to pneumonia once more.
Death.
Matthau suffered from atherosclerotic heart disease. He died of a heart attack in Santa Monica on July 1, 2000. He was 79 years old. His remains are interred in the Westwood Village Memorial Park Cemetery in Los Angeles.
Less than a year later, the remains of Jack Lemmon (who died of colon and bladder cancer) were buried at the same cemetery. After Matthau's death, Lemmon as well as other friends and relatives had appeared on "Larry King Live" in an hour of tribute and remembrance; many of those same people appeared on the show one year later, reminiscing about Lemmon. Carol Marcus, also a native of New York, died of a brain aneurysm in 2003. Her remains are buried on top of those of her husband, Matthau. The remains of actor George C. Scott are buried to the left of those of Walter Matthau, in an unmarked grave, and Farrah Fawcett's remains are buried to the right.

</doc>
<doc id="62812" url="https://en.wikipedia.org/wiki?curid=62812" title="Neuropil">
Neuropil

Neuropil, sometimes referred to as "neuropile," is a broad term defined as any area in the nervous system composed of mostly unmyelinated axons, dendrites and glial cell processes that forms a synaptically dense region containing a relatively low number of cell bodies. The most prevalent anatomical region of neuropil is the brain which, although not completely composed of neuropil, does have the largest and highest synaptically-concentrated areas of neuropil in the body. For example, the neocortex and olfactory bulb both contain neuropil.
White matter, which is mostly composed of axons and glial cells, is generally not considered to be a part of the neuropil.
Neuropil (pl. neuropils) comes from the Greek: "neuro", meaning "tendon, sinew; nerve" and "pilos", meaning "felt." The term's origin can be traced back to the late 19th century.
Examples.
Neuropil has been found in the following regions: outer neocortex layer, barrel cortex, inner plexiform layer and outer plexiform layer, posterior pituitary, and glomeruli of the cerebellum. These are all found in humans, with the exception of the barrel cortex, but many species have counterparts similar to our own regions of neuropil. However, the degree of similarity depends upon the composition of neuropil being compared. The concentrations of neuropil within certain regions are important to determine because simply using the proportions of the different postsynaptic elements does not verify the necessary, conclusive evidence. Comparing the concentrations can determine whether or not proportions of different postsynaptic elements contacted a particular axonal pathway. Relative concentrations could signify a reflection of different postsynaptic elements in the neuropil or show that axons sought out and formed synapses only with specific postsynaptic elements.
Function.
Since neuropils have a diverse role in the nervous system, it is difficult to define a certain overarching function for all neuropils. For instance, the olfactory glomeruli function as sorts of way-stations for the information flowing from the olfactory receptor neurons to the olfactory cortex. The inner plexiform layer of the retina is a little more complex. The bipolar cells post-synaptic to either rods or cones are either depolarized or hyperpolarized depending on whether the bipolar cells have sign-inverting synapses or a sign-conserving synapses.
Neuropil in humans.
Efficiency in the brain.
Neurons are necessary for all connections made in the brain, and thus can be thought of as the "wires" of the brain. As in computing, an entity is most efficient when its wires are optimized; therefore, a brain which has undergone millions of years of natural selection would be expected to have optimized neural circuitry. To have an optimized neural system it must balance four variables — it must "minimize conduction delays in axons, passive cable attenuation in dendrites, and the length of 'wire' used to construct circuits" as well as "maximize the density of synapses", essentially optimizing the neuropil. Researchers at Cold Spring Harbor Laboratory formulated the optimal balance of the four variables and calculated the optimal ratio of axon plus dendrite volume (i.e. the "wire" volume or neuropil volume) to total volume of grey matter. The formula predicted an optimal brain with 3/5 (60%) of its volume occupied by neuropil. Experimental evidence taken from three mouse brains agrees with this result. The "fraction of wire is 0.59 ± 0.036 for layer IV of visual cortex, 0.62 ± 0.055 for layer Ib of piriform cortex, and 0.54 ± 0.035 for the stratum radiatum of hippocampal field CA1. The overall average is 0.585 ± 0.043; these values are not statistically different from the optimal 3/5."
Neuropil and disease.
Schizophrenia.
It has been shown that a certain protein is lost in schizophrenics that causes dendrites and spines to deteriorate in the dorsolateral prefrontal cortex, a part of the neocortex, which plays a key role in information processing, attention, memory, orderly thinking and planning which are all functions that deteriorate in schizophrenics. The deterioration of the neuropil in this cortex has been proposed as the cause of schizophrenia.
Alzheimer's disease.
Alzheimer's is a neuropathological disease that is hypothesized to result from the loss of dendritic spines and/or deformation of these spines in the patient's frontal and temporal cortices. Researchers have tied the disease to a decrease in the expression of drebrin, a protein thought to play a role in long-term potentiation, meaning the neurons would lose plasticity and have trouble forming new connections. This malfunction presents itself in the form of helical filaments that tangle together in the neuropil. Interestingly, this same phenomenon seems to occur in the elderly as well.
Neuropil in non-humans.
A significant non-human area of neuropil is the barrel cortex found in mammals with whiskers (e.g. cats, dogs and rodents); each "barrel" in the cortex is a region of neuropil where the input from a single whisker terminates.
Significance of neuropil difference in chimpanzees and humans.
Neuropil have been hypothesized to be a key factor in differentiating human cognitive capacity from that of other animals. In one study comparing chimpanzee and human frontopolar cortex and the frontoinsular cortex neuropil, it was found that humans exhibit a significantly higher neuropil fraction than the other areas of their brain. This suggests that as we evolved our prefrontal cortex developed denser neuropil which translates to more neural connections. In chimpanzees these prefrontal regions did not display significantly more neuropil.
Research.
Research has focused on where neuropil is found in many different species in order to unveil the range of significance it has and possible functions.
Recent studies.
In chimpanzees and humans the neuropil provides a proxy measure of total connectivity within a local region because it is composed mostly of dendrites, axons, and synapses.
In insects the central complex plays an important role in higher-order brain function. The neuropil in "Drosophila" Ellipsoid is composed of four substructures. Each section has been observed in several insects as well as the influence it has on behavior, however the exact function of this neuropil has proven elusive. Abnormal walking behavior and flight behavior are controlled primarily by the central complex and genetic mutations that disrupt the structure support the hypothesis that the central complex neuropil is a site of behavioral control. However, it is interesting that only specific components of the behavior were affected with the genetic mutations. For example, basic leg coordination of walking was normal, whereas speed, activity, and turning were affected. These observations suggest that the central complex not only plays a role in locomotor behavior, but fine tuning as well. There is also additional evidence that the neuropil may function in olfactory associative learning and memory.
In humans, schizophrenia may be caused by deterioration of neuropil, with much evidence specifically pointing to dysfunction in the dorsolateral prefrontal cortex (DLPFC). Research has shown reduced neuropil in area 9 of schizophrenics, as well as consistent findings of reduced spine density in layer III pyramidal neurons of the temporal and frontal cortices. Since neuropil is the location of most cortical synapses it is likely that the deterioration greatly affects processing and produces the symptoms schizophrenics exhibit.

</doc>
<doc id="62815" url="https://en.wikipedia.org/wiki?curid=62815" title="Department of Energy">
Department of Energy

Department of Energy may refer to:

</doc>
<doc id="62820" url="https://en.wikipedia.org/wiki?curid=62820" title="478 BC">
478 BC

__NOTOC__
Year 478 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Mamercus and Structus (or, less frequently, year 276 "Ab urbe condita"). The denomination 478 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="62822" url="https://en.wikipedia.org/wiki?curid=62822" title="Bennelong">
Bennelong

Woollarawarre Bennelong (c. 1764 – 3 January 1813) "(also: "Baneelon")" was a senior man of the Eora, an Aboriginal (Koori) people of the Port Jackson area, at the time of the , in 1788. Bennelong served as an interlocutor between the Eora and the British, both in Sydney and in the United Kingdom.
Personal details.
Bennelong was a member of the Wangal Clan, connected with the south side of Parramatta River, having close ties with the Wallumedegal clan, on the west side of the river, and the Burramattagal clan near today's Parramatta.He had several sisters, including Warreeweer and Carangarang, who married important men from nearby clans, thereby creating political links for their brother. Bennelong had a daughter named Dilboong who died in infancy, and a son who was adopted by the Rev. William Walker, who christened him Thomas Walker Coke. Thomas died after a short illness aged about 20.
Capture and life in the British settlement.
Bennelong was brought to the settlement at Sydney Cove in November 1789 by order of the governor, Arthur Phillip, who was under instructions from King George III to establish relationships with the indigenous populations. At that time the Eora conscientiously avoided contact with the newcomers, and in desperation Phillip resorted to kidnap. A man named Arabanoo was captured, but he, like many other Aboriginal people near the settlement, died in a smallpox epidemic a few months later in May 1789. Bennelong (married at the time to Barangaroo) was captured with Colbee (married to Daringa) in December 1789 as part of Phillip's plan to learn the language and customs of the local people. His age, at the time of his capture, was estimated at 25, and he was described as being 'of good stature, stoutly made', with a 'bold, intrepid countenance'. His appetite was such that 'the ration of a week was insufficient to have kept him for a day', and 'love and war seemed his favourite pursuits'. Colbee soon escaped, but Bennelong stayed in the settlement for about six months. He then escaped also, but renewed contact with Phillip as a free man.
About three months after his escape, he organised for Phillip to visit Manly where he was speared in the shoulder. He maintained ongoing good relations with the colony and in a gesture of kinship, gave Phillip the Aboriginal name Wolawaree. He learned to speak English.
In 1790, Bennelong asked the governor to build him a hut on what became known as Bennelong Point, now the site of the Sydney Opera House.
Visit to England.
Bennelong and also another Aborigine named Yemmerrawanne (or Imeerawanyee) travelled with Phillip on "Atlantic" to England in 1792. Many historians have claimed that they were presented to King George III, but there is no direct evidence that this occurred. Soon after their arrival in England they were hurriedly made clothes that would have been suitable for their presentation to the King.
Jack Brook reconstructs some of their activities from the expense claims lodged with the government. They visited St Paul's Cathedral and the Tower of London. A boat was hired, and they went bathing. They went to the theatre. While in London they resided with Henry Waterhouse, and when Yemmerrawanne became sick, they moved to Eltham and resided at the house of Edward Kent where they were tended by Mr and Mrs Phillips, and met Lord Sydney.
Yemmerrawanne died while in Britain after a serious chest infection, and Bennelong's health deteriorated. He returned to Sydney in February 1795 on HMS "Reliance", the ship that took surgeon George Bass to the colony for the first time. He taught Bass some of his language on the voyage.
Return to New South Wales.
Bennelong arrived back in Sydney on 7 September 1795. He returned to a respected position in the colony, advising Governor Hunter as he had advised and educated Phillip, and also returned to a prominent position in Eora political and cultural life. He frequently participated in payback battles, and officiated at ceremonies, including the last recorded initiation ceremony in Port Jackson in 1797. By the early 19th century, he was the leader of a 100-strong clan living on the north side of the river to the west of Kissing Point in Wallumedagal country.
A letter he had drafted in 1796 to Mr and Mrs Phillips is the first known text written in English by an indigenous Australian, thanking Mrs Phillips for caring for him in England, and asking for stockings and a handkerchief.
Death.
Bennelong's health was perhaps damaged by the consumption of alcohol, one of the most popular pastimes in the colony. He died at Kissing Point (now known as Putney, in Sydney's North West) on 2 January 1813, and was buried in the orchard of the brewer James Squire, a great friend to Bennelong and his clan.
On 20 March 2011 Dr Peter Mitchell of Macquarie University announced that he had located the actual grave site in the garden of a private house in present-day Putney. He stated that local aboriginal authorities will be consulted about possible further exploration of the site.
His obituary in the "Sydney Gazette" was unflattering, insisting that "...he was a thorough savage, not to be warped from the form and character that nature gave him...", which reflected the feelings of some in Sydney's white society that Bennelong had abandoned his role as ambassador in his last years, and also reflects the deteriorating relations between the two groups as more and more land was cleared and fenced for farming, and the hardening attitudes of many colonists towards 'savages' who were not willing to give up their country and become labourers and servants useful to the colonists.
Bennelong's people mourned his death with a traditional payback battle for which about two hundred people gathered. It was witnessed by a passenger on the schooner "Henrietta" who reported it in a letter to the "Caledonian Mercury". They wrote that spears flew very thick, and about thirty men were wounded.
As a mark of respect, Colebee's nephew Nanberry, who died in 1821, was buried with Bennelong at his request. Bidgee Bidgee, who led the Kissing Point clan for twenty years after Bennelong's death, also asked to be buried with Bennelong, but there is no record of his death or where he is buried.

</doc>
<doc id="62844" url="https://en.wikipedia.org/wiki?curid=62844" title="Density matrix">
Density matrix

A density matrix is a matrix that describes a quantum system in a "mixed state", a statistical ensemble of several quantum states. This should be contrasted with a single state vector that describes a quantum system in a "pure state". The density matrix is the quantum-mechanical analogue to a phase-space probability measure (probability distribution of position and momentum) in classical statistical mechanics.
Mixed states arise in situations where the experimenter does not know which particular states are being manipulated. Examples include a system in thermal equilibrium (or additionally chemical equilibrium) or a system with an uncertain or randomly varying preparation history (so one does not know which pure state the system is in). Also, if a quantum system has two or more subsystems that are entangled, then each subsystem must be treated as a mixed state even if the complete system is in a pure state. The density matrix is also a crucial tool in quantum decoherence theory.
The density matrix is a representation of a linear operator called the "density operator". The close relationship between matrices and operators is a basic concept in linear algebra. In practice, the terms "density matrix" and "density operator" are often used interchangeably. Both matrix and operator are self-adjoint (or Hermitian), positive semi-definite, of trace one, and may
be infinite-dimensional.
History.
The formalism of density operators and matrices was introduced by John von Neumann in 1927 and independently, but less systematically by Lev Landau and Felix Bloch in 1927 and 1946 respectively.
Definition.
Suppose a quantum system may be found in state
with probability , or it may be found in state
with probability , and so on. The density operator for this system is
where
is a spanning set of "normalized" Hilbert space vectors, but not necessarily orthogonal. The numbers satisfy, since they are probabilities,
By choosing an orthonormal basis
one may resolve the density operator into the density matrix, whose elements are
The density operator can also be defined in terms of the density matrix,
Ensemble average of an operator.
For an operator
(which may describe an observable of the system), the "ensemble average"
is given by
Here the quantity
is just the ordinary expectation value of the operator in the pure state . In words, the expectation value of for the mixed state is the sum of the expectation values of for each of the pure states
weighted by the probabilities and can be computed as the trace of the product of the density matrix with the matrix representation of in the same basis. This is basis independent since traces are invariant under unitary transformations.
Pure and mixed states.
In quantum mechanics, a quantum system is represented by a state vector (or ket) formula_14. A quantum system with a state vector formula_14 is called a "pure state". However, it is also possible for a system to be in a statistical ensemble of different state vectors: For example, there may be a 50% probability that the state vector is formula_16 and a 50% chance that the state vector is formula_2. This system would be in a "mixed state". The density matrix is especially useful for mixed states, because any state, pure or mixed, can be characterized by a single density matrix.
A mixed state is different from a quantum superposition. In fact, a quantum superposition of pure states is another pure state, for example formula_18.
A state is pure if and only if its density matrix formula_19 satisfies formula_20.
Example: Light polarization.
An example of pure and mixed states is light polarization. Photons can have two helicities, corresponding to two orthogonal quantum states, formula_21 (right circular polarization) and formula_22 (left circular polarization). A photon can also be in a superposition state, such as formula_23 (vertical polarization) or formula_24 (horizontal polarization). More generally, it can be in any state formula_25, corresponding to linear, circular, or elliptical polarization. If we pass formula_23 polarized light through a circular polarizer which allows either only formula_21 polarized light, or only formula_22 polarized light, intensity would be reduced by half in both cases. This may make it "seem" like half of the photons are in state formula_21 and the other half in state formula_22. But this is not correct: Both formula_21 and formula_22 photons are partly absorbed by a vertical linear polarizer, but the formula_23 light will pass through that polarizer with no absorption whatsoever.
However, unpolarized light (such as the light from an incandescent light bulb) is different from any state like formula_25 (linear, circular, or elliptical polarization). Unlike linearly or elliptically polarized light, it passes through a polarizer with 50% intensity loss whatever the orientation of the polarizer; and unlike circularly polarized light, it cannot be made linearly polarized with any wave plate because randomly oriented polarization will emerge from a wave plate with random orientation. Indeed, unpolarized light cannot be described as "any" state of the form formula_25 in a definite sense. However, unpolarized light "can" be described with ensemble averages, e.g. that each photon is either formula_36 with 50% probability or formula_37 with 50% probability. The same behavior would occur if each photon was either vertically polarized with 50% probability or horizontally polarized with 50% probability.
Therefore, unpolarized light cannot be described by any pure state, but can be described as a statistical ensemble of pure states in at least two ways (the ensemble of half left and half right circularly polarized, or the ensemble of half vertically and half horizontally linearly polarized). These two ensembles are completely indistinguishable experimentally, and therefore they are considered the same mixed state. One of the advantages of the density matrix is that there is just one density matrix for each mixed state, whereas there are many statistical ensembles of pure states for each mixed state. Nevertheless, the density matrix contains all the information necessary to calculate any measurable property of the mixed state.
Where do mixed states come from? To answer that, consider how to generate unpolarized light. One way is to use a system in thermal equilibrium, a statistical mixture of enormous numbers of microstates, each with a certain probability (the Boltzmann factor), switching rapidly from one to the next due to thermal fluctuations. Thermal randomness explains why an incandescent light bulb, for example, emits unpolarized light. A second way to generate unpolarized light is to introduce uncertainty in the preparation of the system, for example, passing it through a birefringent crystal with a rough surface, so that slightly different parts of the beam acquire different polarizations. A third way to generate unpolarized light uses an EPR setup: A radioactive decay can emit two photons traveling in opposite directions, in the quantum state formula_38. The two photons "together" are in a pure state, but if you only look at one of the photons and ignore the other, the photon behaves just like unpolarized light.
More generally, mixed states commonly arise from a statistical mixture of the starting state (such as in thermal equilibrium), from uncertainty in the preparation procedure (such as slightly different paths that a photon can travel), or from looking at a subsystem entangled with something else.
Mathematical description.
The state vector formula_14 of a pure state completely determines the statistical behavior of a measurement. For concreteness, take an observable quantity, and let "A" be the associated observable operator that has a representation on the Hilbert space formula_40 of the quantum system. For any real-valued, analytical function "F" defined on the real numbers, suppose that "F"("A") is the result of applying "F" to the outcome of a measurement. The expectation value of "F"("A") is
Now consider a mixed state prepared by statistically combining two different pure states formula_42 and formula_43, with the associated probabilities "p" and , respectively. The associated probabilities mean that the preparation process for the quantum system ends in the state formula_44 with probability "p" and in the state formula_43 with probability .
It is not hard to show that the statistical properties of the observable for the system prepared in such a mixed state are completely determined. However, there is no state vector formula_46 which determines this statistical behavior in the sense that the expectation value of "F"("A") is
Nevertheless, there "is" a unique operator "ρ" such that the expectation value of "F(A)" can be written as
where the operator "ρ" is the density operator of the mixed system. A simple calculation shows that the operator "ρ" for the above discussion is given by
For the above example of unpolarized light, the density operator is
Formulation.
For a finite-dimensional function space, the most general density operator is of the form
where the coefficients "p""j" are non-negative and add up to one. This represents a statistical mixture of pure states. If the given system is closed, then one can think of a mixed state as representing a single system with an uncertain preparation history, as explicitly detailed above; "or" we can regard the mixed state as representing an ensemble of systems, i.e. a large number of copies of the system in question, where "p""j" is the proportion of the ensemble being in the state formula_52. An ensemble is described by a pure state if every copy of the system in that ensemble is in the same state, i.e. it is a "pure ensemble". If the system is not closed, however, then it is simply not correct to claim that it has some definite but unknown state vector, as the density operator may record physical entanglements to other systems.
Consider a quantum ensemble of size "N" with occupancy numbers "n"1, "n"2...,"nk" corresponding to the orthonormal states formula_53, respectively, where "n"1+...+"nk" = "N", and, thus, the coefficients "pj" = "nj" /"N". For a pure ensemble, where all "N" particles are in state formula_54, we have "nj" = 0, for all "j" ≠ "i", from which we recover the corresponding density operator formula_55. However, the density operator of a mixed state does not capture all the information about the ingredients that went into the mixture; in particular, the coefficients "p""j" and the kets ψ"j" are not recoverable from the operator ρ without additional information. This non-uniqueness implies that different ensembles or mixtures may correspond to the same density operator. Such equivalent ensembles or mixtures cannot be distinguished by measurement of observables alone. This equivalence can be characterized precisely. Two ensembles ψ, ψ' define the same density operator if and only if there is a matrix U with
i.e., U is unitary and such that
This is simply a restatement of the following fact from linear algebra: for two square matrices "M" and "N", "M M"* = "N N"* if and only if "M" = "NU" for some unitary "U". (See square root of a matrix for more details.) Thus there is a unitary freedom in the ket mixture or ensemble that gives the same density operator. However, if the kets making up the mixture are restricted to be orthonormal, then the original probabilities "p""j" are recoverable as the eigenvalues of the density matrix.
In operator language, a density operator is a positive semidefinite, hermitian operator of trace 1 acting on the state space. A density operator describes a pure state if it is a rank one projection. Equivalently, a density operator ρ describes a pure state if and only if
i.e. the state is idempotent. This is true regardless of whether "H" is finite-dimensional or not.
Geometrically, when the state is not expressible as a convex combination of other states, it is a pure state. The family of mixed states is a convex set and a state is pure if it is an extremal point of that set.
It follows from the spectral theorem for compact self-adjoint operators that every mixed state is a countable convex combination of pure states. This representation is not unique. Furthermore, a theorem of Andrew Gleason states that certain functions defined on the family of projections and taking values in [0,1] (which can be regarded as quantum analogues of probability measures) are determined by unique mixed states. See quantum logic for more details.
Measurement.
Let "A" be an observable of the system, and suppose the ensemble is in a mixed state such that each of the pure states formula_59 occurs with probability "pj". Then the corresponding density operator is:
The expectation value of the measurement can be calculated by extending from the case of pure states (see Measurement in quantum mechanics):
where formula_62 denotes trace. Moreover, if "A" has spectral resolution
where formula_64, the corresponding density operator after the measurement is given by:
Note that the above density operator describes the full ensemble after measurement. The sub-ensemble for which the measurement result was the particular value "ai" is described by the different density operator
This is true assuming that formula_67 is the only eigenket (up to phase) with eigenvalue "ai"; more generally, "Pi" in this expression would be replaced by the projection operator into the eigen"space" corresponding to eigenvalue "ai".
Entropy.
The von Neumann entropy formula_68 of a mixture can be expressed in terms of the eigenvalues of formula_69 or in terms of the trace and logarithm of the density operator formula_69. Since formula_71 is a positive semi-definite operator, it has a spectral decomposition such that formula_72 where formula_73 are orthonormal vectors, formula_74 and formula_75. Then the entropy of a quantum system with density matrix formula_71 is
Also it can be shown that
when formula_79 have orthogonal support, where formula_80 is the Shannon entropy.
This entropy can increase but never decrease with a projective measurement, however generalised measurements can decrease entropy. The entropy of a pure state is zero, while that of a proper mixture always greater than zero. Therefore, a pure state may be converted into a mixture by a measurement, but a proper mixture can "never" be converted into a pure state. Thus the act of measurement induces a fundamental irreversible change on the density matrix; this is analogous to the "collapse" of the state vector, or wavefunction collapse. Perhaps counterintuitively, the measurement actually "decreases information" by erasing quantum interference in the composite system—cf. quantum entanglement, einselection, and quantum decoherence.
The von Neumann equation for time evolution.
Just as the Schrödinger equation describes how pure states evolve in time, the von Neumann equation (also known as the Liouville–von Neumann equation) describes how a density operator evolves in time (in fact, the two equations are equivalent, in the sense that either can be derived from the other.) The von Neumann equation dictates that
where the brackets denote a commutator.
Note that this equation only holds when the density operator is taken to be in the Schrödinger picture, even though this equation seems at first look to emulate the Heisenberg equation of motion in the Heisenberg picture, with a crucial sign difference:
where formula_83 is some "Heisenberg picture" operator; but in this picture the density matrix is "not time-dependent", and the relative sign ensures that the time derivative of the expected value formula_10 comes out "the same as in the Schrödinger picture".
Taking the density operator to be in the Schrödinger picture makes sense, since it is composed of 'Schrödinger' kets and bras evolved in time, as per the Schrödinger picture.
If the Hamiltonian is time-independent, this differential equation can be easily solved to yield
"Quantum Liouville", Moyal's equation.
The density matrix operator may also be realized in phase space. Under the Wigner map, the density matrix transforms into the equivalent Wigner function,
The equation for the time-evolution of the Wigner function is then the Wigner-transform of the above von Neumann equation,
where "H(q,p)" is the Hamiltonian, and { { •,• } } is the Moyal bracket, the transform of the quantum commutator.
The evolution equation for the Wigner function is then analogous to that of its classical limit, the Liouville equation of classical physics. In the limit of vanishing Planck's constant ħ, "W(q,p,t)" reduces to the classical Liouville probability density function in phase space.
The classical Liouville equation can be solved using the method of characteristics for partial differential equations, the characteristic equations being Hamilton's equations. The Moyal equation in quantum mechanics similarly admits formal solutions in terms of quantum characteristics, predicated on the ∗−product of phase space, although, in actual practice, solution-seeking follows different methods.
Composite systems.
The joint density matrix of a composite system of two systems A and B is described by formula_88. Then the subsystems are described by their reduced density operator.
formula_90 is called "partial trace" over system B.
If A and B are two distinct and independent systems then formula_91 which is a "product state".
C*-algebraic formulation of states.
It is now generally accepted that the description of quantum mechanics in which all self-adjoint operators represent observables is untenable. For this reason, observables are identified with elements of an abstract C*-algebra "A" (that is one without a distinguished representation as an algebra of operators) and states are positive linear functionals on "A". However, by using the GNS construction, we can recover Hilbert spaces which realize "A" as a subalgebra of operators.
Geometrically, a pure state on a C*-algebra "A" is a state which is an extreme point of the set of all states on "A". By properties of the GNS construction these states correspond to irreducible representations of "A".
The states of the C*-algebra of compact operators "K"("H") correspond exactly to the density operators, and therefore the pure states of "K"("H") are exactly the pure states in the sense of quantum mechanics.
The C*-algebraic formulation can be seen to include both classical and quantum systems. When the system is classical, the algebra of observables become an abelian C*-algebra. In that case the states become probability measures, as noted in the introduction.

</doc>
<doc id="62866" url="https://en.wikipedia.org/wiki?curid=62866" title="United States Department of Energy">
United States Department of Energy

The United States Department of Energy (DOE) is a Cabinet-level department of the United States Government concerned with the United States' policies regarding energy and safety in handling nuclear material. Its responsibilities include the nation's nuclear weapons program, nuclear reactor production for the United States Navy, energy conservation, energy-related research, radioactive waste disposal, and domestic energy production. It also directs research in genomics; the Human Genome Project originated in a DOE initiative. DOE sponsors more research in the physical sciences than any other U.S. federal agency, the majority of which is conducted through its system of National Laboratories.
The agency is administered by the United States Secretary of Energy, and its headquarters are located in Southwest Washington, D.C., on Independence Avenue in the James V. Forrestal Building, named for James Forrestal, as well as in Germantown, Maryland.
History.
In 1942, during World War II, the United States started the Manhattan Project, a project to develop the atomic bomb, under the eye of the U.S. Army Corps of Engineers. After the war in 1946, the Atomic Energy Commission (AEC) was created to control the future of the project.
In 1974 the AEC gave way to the Nuclear Regulatory Commission, which was tasked with regulating the nuclear power industry, and the Energy Research and Development Administration, which was tasked to manage the nuclear weapon, naval reactor, and energy development programs.
The 1973 oil crisis called attention to the need to consolidate energy policy. On August 4, 1977, President Jimmy Carter signed into law "The Department of Energy Organization Act of 1977" (), which created the Department of Energy. The new agency, which began operations on October 1, 1977, consolidated the Federal Energy Administration, the Energy Research and Development Administration, the Federal Power Commission, and programs of various other agencies. Former Secretary of Defense James Schlesinger, who served under Presidents Nixon and Ford during the Vietnam War, was appointed as the first secretary.
Weapon plans stolen.
In December 1999, The FBI was investigating how China obtained plans for a specific nuclear device. Wen Ho Lee was accused of stealing nuclear secrets from Los Alamos National Laboratory for the People's Republic of China. Federal officials, including then-Energy Secretary Bill Richardson, publicly named Lee as a suspect before he was charged with a crime. The U. S. Congress held hearings to investigate the Department of Energy's mishandling of his case. Republican senators thought that an independent agency should be in charge of nuclear weapons and security issues, not the Department of Energy. All but one of the 59 charges against Lee were eventually dropped because the investigation finally proved that the plans the Chinese obtained could not have come from Lee. Lee filed suit and won a $1.6 million settlement against the federal government and news agencies.
Organization.
The Department is under the control and supervision of a United States Secretary of Energy, a political appointee of the President of the United States. The Energy Secretary is assisted in managing the Department by a United States Deputy Secretary of Energy, also appointed by the President, who assumes the duties of the Secretary in his absence. The Department also has three Under Secretaries, each appointed by the President, who oversee the major areas of the Department's work. The President also appoints seven officials with the rank of Assistant Secretary of Energy who have line management responsibility for major organizational elements of the Department. The Energy Secretary assigns their functions and duties.
Facilities.
As a leading federal research and development agency in the United States, the Department of Energy operates a system of national laboratories and technical facilities.
The DOE National Laboratories are as follows:
Other major DOE facilities include:
Responsibility for nuclear weapons.
The DOE/NNSA has federal responsibility for the design, testing and production of all nuclear weapons. NNSA in turn uses contractors to carry out its responsibilities at the following government owned sites:
Budget.
President Barack Obama unveiled on May 7, 2009, a $26.4 billion budget request for DOE for fiscal year (FY) 2010, including $2.3 billion for the DOE Office of Energy Efficiency and Renewable Energy (EERE). The budget aims to substantially expand the use of renewable energy sources while improving energy transmission infrastructure. It also makes significant investments in hybrids and plug-in hybrids, in smart grid technologies, and in scientific research and innovation.
As part of the $789 billion economic stimulus package in the American Recovery and Reinvestment Act of 2009, Congress provided Energy with an additional $38.3 billion for fiscal years 2009 and 2010, adding about 75 percent to Energy's annual budgets. Most of the stimulus spending was in the form of grants and contracts.
For fiscal year 2013, each of the operating units of the Department of Energy operate with the following budgets:
Energy Savings Performance Contract.
Energy Savings Performance Contracts (ESPCs) are contracts under which a contractor designs, constructs, and obtains the necessary financing for an energy savings project, and the federal agency makes payments over time to the contractor from the savings in the agency's utility bills. The contractor guarantees the energy improvements will generate savings, and after the contract ends, all continuing cost savings accrue to the federal agency.
Loan Guarantee Program.
Title XVII of Energy Policy Act of 2005 authorizes the DOE to issue loan guarantees to eligible projects that "avoid, reduce, or sequester air pollutants or anthropogenic emissions of greenhouse gases" and "employ new or significantly improved technologies as compared to technologies in service in the United States at the time the guarantee is issued".
In loan guarantees, a conditional commitment requires to meet an equity commitment, as well as other conditions, before the loan guarantee is completed.
Energy Innovation Hubs.
Energy Innovation Hubs are multi-disciplinary meant to advance highly promising areas of energy science and technology from their early stages of research to the point that the risk level will be low enough for industry to commercialize the technologies. The Consortium for Advanced Simulation of Light Water Reactors (CASL) was the first DOE Energy Innovation Hub established in July 2010, for the purpose of providing advanced modeling and simulation (M&S) solutions for commercial nuclear reactors.
The DOE budget includes $280 million to fund eight Energy Innovation Hubs, each of which is focused on a particular energy challenge. Two of the eight hubs are included in the EERE budget and will focus on integrating smart materials, designs, and systems into buildings to better conserve energy and on designing and discovering new concepts and materials needed to convert solar energy into electricity. Another two hubs, included in the DOE Office of Science budget, will tackle the challenges of devising advanced methods of energy storage and creating fuels directly from sunlight without the use of plants or microbes. Yet another hub will develop "smart" materials that will allow the electrical grid to adapt and respond to changing conditions.
In 2012, The DOE awarded $120 million to the Ames Laboratory to start a new EIH, the Critical Materials Institute, which will focus on improving the supply of rare earth elements, which is controlled by China.
Symbolism in the seal.
Excerpt from the code of federal regulations, Title 10: Energy
The official seal of the Department of energy "includes a green shield bisected by a gold-colored lightning bolt, on which is emblazoned a gold-colored symbolic sun, atom, oil derrick, windmill, and dynamo. It is crested by the white head of an eagle, atop a white rope. Both appear on a blue field surrounded by concentric circles in which the name of the agency, in gold, appears on a green background."
"The eagle represents the care in planning and the purposefulness of efforts required to respond to the Nation's increasing demands for energy. The sun, atom, oil derrick, windmill, and dynamo serve as representative technologies whose enhanced development can help meet these demands. The rope represents the cohesiveness in the development of the technologies and their link to our future capabilities. The lightning bolt represents the power of the natural forces from which energy is derived and the Nation's challenge in harnessing the forces."
"The color scheme is derived from nature, symbolizing both the source of energy and the support of man's existence. The blue field represents air and water, green represents mineral resources and the earth itself, and gold represents the creation of energy in the release of natural forces. By invoking this symbolism, the color scheme represents the Nation's commitment to meet its energy needs in a manner consistent with the preservation of the natural environment."

</doc>
<doc id="62884" url="https://en.wikipedia.org/wiki?curid=62884" title="Faboideae">
Faboideae

The Faboideae are a subfamily of the flowering plant family Fabaceae or Leguminosae. An acceptable alternative name for the subfamily is Papilionoideae, or Papilionaceae when this group of plants is treated as a family.
This subfamily is widely distributed, and members are adapted to a wide variety of environments. Faboideae may be trees, shrubs, or herbaceous plants. Members include the pea, the sweet pea, the laburnum, and other legumes. The flowers are classically pea-shaped, and root nodulation is very common.
Genera.
The type genus, "Faba", is a synonym of "Vicia", and is listed here as "Vicia".
Systematics.
Modern molecular phylogenetics recommend a clade-based classification of Faboideae as a superior alternative to the traditional tribal classification of Polhill:
Note: Minor branches have been omitted.

</doc>
<doc id="62888" url="https://en.wikipedia.org/wiki?curid=62888" title="Gordon Setter">
Gordon Setter

The Gordon Setter is a large breed of dog, a member of the setter family that also includes both the better-known Irish Setter and the English Setter. Setter breeds are classified as members of either the Sporting or Gundog Group depending on the national kennel club or council. The original purpose of the breed was to hunt gamebirds. Their quarry in the United Kingdom, may be partridge or grouse, pheasant, ptarmigan, blackgame, snipe or woodcock: whilst overseas bird dogs are worked on quail, willow grouse, sand grouse, guinea fowl, sagehen, francolin and any other bird that will sit to a dog—that is to say, will attempt to avoid a potential predator by concealment rather than by taking to the wing at the first sign of danger. It is this combination of a bird that will sit fast in front of a dog that will remain on point that makes bird dog work possible.
Description.
Appearance.
Gordon setters, also known as "black and tans", have a coal-black coat with distinctive markings of a rich chestnut or mahogany colour on their paws and lower legs, vents, throat, and muzzles; one spot above each eye; and two spots on their chest. A small amount of white is allowed on the chest. Although uncommon, red Gordons are occasionally born to normal-coloured parents, the result of expression of a recessive red gene. Predominantly tan, red, or buff dogs are ineligible for showing. A Gordon's coat is straight or slightly waved (but not curly), long and silky, with chest, stomach, ear, leg, and tail feathering. According to the AKC breed standard, "the bearing is intelligent, noble, and dignified". They are the heaviest of the setter breeds, with males reaching at the withers and up to in weight.
Temperament.
The AKC describes the Gordon Setter temperament as "alert, interested, and confident. He is fearless and willing, intelligent, and capable. He is loyal and affectionate, and strong-minded enough to stand the rigors of training". Gordons are intensely loyal to their owners; thrive in an attentive, loving environment; and are good family dogs. Puppies and adult dogs can be quite boisterous, and although they are patient by nature, may not be suitable for households with very young children. Gordons are sensitive and empathic, eager to learn, and need firm but gentle handling. Early socialisation and obedience training is important. They are known as great talkers. The breed is one of the slowest to mature, not hitting prime until three years of age or more, and will show puppy-like characteristics well into their older years.
Gordons were bred to run, and require 60 to 80 minutes of vigorous exercise daily. Young dogs should not be over-exercised or begin agility training until they are at least 18 months old, to avoid joint problems later in life. Because of their hunting instincts, Gordons should not be allowed to roam freely if unsupervised, as they are apt to wander into a potentially dangerous traffic situation while following a scent.
Health.
Although not as prone to hip dysplasia as many of the larger breeds, Gordons can suffer from the condition. Other health issues can include hypothyroidism, gastric torsion (bloat) and eye diseases such as progressive retinal atrophy (PRA), and cataracts. Life expectancy for the breed is generally about 10 to 12 years.
At the beginning of 2009, a report was issued to all of the Breed Clubs in the UK concerning cases of late onset PRA in Gordon Setters.
On March 14, 2011, Animal Health Trust (AHT) made a DNA test available. The mutation is termed rcd4 (for rod-cone degeneration 4) to distinguish it from other, previously described forms of rod-cone degeneration. It is recessive, and 19 out of the 21 Gordons in their study who had clinical signs of PRA were homozygous for this mutation, indicating it is the major cause of PRA in the breed.
As many as 50% of Gordon Setters may be carriers.
History.
Origins.
"Domesticated Dogs Finding Their Game by Scent, But Not Killing It, Being Chiefly Used in Aid of the Gun"."The Dog In Health & Disease"—Longmans, Green & Co. 1859
This is the title of the chapter covering pointers and setters in Stonehenge's work on dogs published around a hundred and fifty years ago. The term "gun dogs" would pretty well cover all the dogs described in the chapter. Many of the gun dogs described by Stonehenge are no longer to be found in the United Kingdom or have been absorbed into one of the other breeds. The Russian Setter, the Welsh Setter, Northern Irish Water Spaniel, Southern Irish Water Spaniel and English Water Spaniel, the Spanish Pointer and the Portuguese Pointer have all disappeared in the past hundred and fifty years, and the pictures of some of the breeds that are still with us show considerable differences to the breed as we see them today. Edward Laverick wrote in "The Setter", published in 1872: "the setter is but an improved spaniel"; while the Rev Pearce in "The Dog", published in the same year, said, 'he is a direct descendant of the Spaniel: "a Setting Spaniel" was the first Setter'. Since then this is the generally agreed with conclusion that the Setter was primarily derived from the old Land Spaniel, so called so as to distinguish it from the Water Spaniel. It is however likely that outside crosses with Hounds or Pointers did influence its development. William Taplin in "The Sportsman's Cabinet" (1803–04) maintained that it was "originally produced by a commixture between the Spanish pointer and the larger breed of the English spaniel".
We now really need not to go back to the Spaniel and its specialised development into the setting-dog, as it was called, and can be found in the work by the famous French sportsman, Gaston de Foix, Vicomte de Béarn (1331–91), who it is said owned about 1500 dogs 'brought from all countries of Europe' and was known as 'Gaston Phèbus' owing to his love for the chase. This work is called "Livre de Chasse" or "Miroir de Phèbus", and was started in 1387. This work was the bases of "The Master of Game" written between 1406 and 1413 by Edward III's grandson, Edward, second Duke of York, who acknowledged his debt to de Foix. Below is the main passage referring to the Spaniel and the Setting-dog, as republished in 1904:
<poem>Another kind of dog is that is called falcon-dog or spaniel ["espaignols" in the French original] because it comes from Spain, notwithstanding that there are many in other countries...
A good spaniel should not be too rough, though his tail should be rough. The good qualities that such a dogs are these: They love well their masters and follow them without losing, although they be in a great crowd of men, and commonly they go before their master, running and wagging their tail, and raise or start fowl and wild beasts. But their right craft is of the partridge and of the quail. It is good for a man that has a noble goshawk, or a tierecel, or a sparrowhawk for the partridges to have such dogs; and also, when they are taught to be couchers ("chiens couchants" in the original French—ed.), they are good for taking partridge and quail with the net...</poem>
The modern Gordon Setter is a predominantly black dog with rich tan marking on the muzzle, legs and chest. A little bigger and heavier than either the Irish or English, he is nevertheless descended from the same genetic mixing pot, which undoubtedly has its origins among those setting spaniels we met earlier. The Kennel Club applied the name 'Gordon Setter' to the breed in 1924. Before that they were known as black and tan setters, and were found in many kennels beside those of the Alexander Gordon, 4th Duke of Gordon (1743–1827). Indeed, as we shall see, there is plenty of evidence that the majority of the setters at Gordon Castle during the Duke's time were tri-coloured rather than pure black and tan.
The breed was brought to the United States by George Blunt and Daniel Webster in 1842, with the purchase of two dogs named Rake and Rachel from the Duke's kennels. The American Kennel Club officially recognised the breed in 1892.
Breed development.
The Gordon Setter as a gundog in the United Kingdom.
Among the many changes which took place in sport and country affairs during the last century where those concerned with the method of shooting and consequent role of the gundog. These changes were accelerated after the Second World War, prior to which there were many "dogging moors" in the north of the UK, especially in Scotland. Walking up game became largely superseded by driving and field-craft by marksmanship. The function of the gundog was as a result limited to the recovery of dead or wounded birds and – in the age of specialisation – this meant that the Labrador Retriever came to the forefront while the number of working Pointers and Setters declined year after year.
Besides the modernisation of the style of shooting and the work required of gundogs, the situation was altered by the new developments that also took place in farming, which helped to bring about a marked reduction in the partridge population. This came about with the introduction of modernisation such as early cutting of silage, the use of fast-moving mechanical equipment, the burning or ploughing of stubble-fields soon after harvest, the destruction of hedgerows and the use of chemical sprays for weed-killing. The hedgerows had provided shelter and nesting sites; the weeds and other herbage supplied food and cover; whilst the stubble-fields had been a primary source of winter food; so the partridges were deprived of some important assets, whilst the wide use of chemicals on the land exercised a direct harmful effect.
It will be obvious to the reader that these changes significantly affected the status of Setters and Pointers, not least that of the Gordon. Though often used as a general purpose gundog, the Gordon Setter is essentially a wide-ranging dog employed in the UK to locate red grouse and ptarmigan on the Scottish or North of England moors and partridges on the stubble-fields of the south of England. Up to the late 1930s most Gordons were kept for this type of work, so that the majority were to be found in Scotland and the north of England; but now they are more evenly distributed and there are no large working kennels.
The function of the Setter is well summarised by Captain Blaine as follows: ‘The work required of the setter and pointer differs from that of all other breeds of dog. It is their business to range and hunt independently for game, at a distance from the sportsman, using their own initiative and intelligence to find it, and having done so, to remain staunchly “on point” awaiting his approach. They must search for the body, and not for the foot scent, and be able to maintain a fast steady gallop for long periods without fatigue. For the purpose a dog should have independence of character, speed, endurance, and a sensitive nose, combined with natural ability for hunting the terrain, in the best method of finding game’ (Croxton Smith, 1932, p70).
Only one Gordon Setter has achieved the title of Dual Champion since the second World War. This was a bitch whose registered name was Amscot Irresista Belle; her pet name was Trisca. The Kennel Club regulations state this title can only be claimed by dogs who have achieved the title of Show Champion and Field Trial Champion.
Perhaps one of the best descriptions of the Setter at work in the field is a poem by the poet William Somerville (1675–1742) in the following lines:
<poem>When autumn smiles, all beauteous in decay,
And paints each chequered grove with various hues,
My setter ranges in the new shorn fields,
His nose in air erect; from ridge to ridge,
Panting, he bounds, his quartered ground divides
In equal intervals, nor careless leaves
One inch untried. At length the tainted gale
His nostrils wide inhale, quick joy elates
His beating heart, which, awed by discipline
Severe, he dares not own, but cautious creeps
Low-cowering, step by step; at last attains
His proper distance, there he stops at once,
And points with his instructive nose upon
The trembling prey. On wings of wind and upborne
The floating net unfolded flies; then drops,
And the poor fluttering captives rise in vain.</poem>
The Gordon Castle and other historically important kennels.
Alexander, the 4th Duke of Gordon (1743–1827), established his kennel of Black and Tan Setters at Gordon Castle, which was situated near Fochabers, not far from the River Spey and a few miles from the coast of Moray. The exact date when this occurred is not known. A Colonel Thornton visited the place during his tour of the Highlands in 1786. He makes no mention of any kennel of Setters at that time, although he does note that ‘The Duke of Gordon still keeps up a diversion of falconry….I saw, also, here a true Highland greyhound, which is now become very scarce….’. The Duke was indeed devoted to country pursuits and was among the last of his day in Scotland to keep hawks and practise falconry; he was celebrated for his Scottish Deerhounds as well as his Setters. However all that can be inferred from the Colonel’s remarks is that there are unlikely to have been any Setters of note at the Castle in 1786.
There is much on record that seems reliable about the origin or derivation of the Duke of Gordon’s Setters, though verification at this late date is of course impossible. Most of this evidence comes from Samuel Brown, the Veterinary Surgeon of Melton Mowbray, who was a great authority on the breed. In a letter to ‘’The Field’’ of 12 November 1864 Samuel Brown stated: ‘An old gentleman sportsman, and one too who has shot over the same breed for fifty years and knew them during his boyhood, assures me that the late Duke of Gordon, Marquis of Anglesey, and several other noblemen, had their original stock of setters from the late Mr Coke of Longford, and that the colour was usually black-white- and -tan. Mine are descended from the original breed of Mr Coke, the Gordon ‘’Regent’’ and ‘’Fan’’, and within the last five years from a black-white-and –tan bitch which I got direct from the Beaudesart kennel’ (i.e. the Marquees of Anglesey’s – Ed.). Five years late, in another letter to the same journal, the Rev F. W. Adye wrote: ‘Mr Brown was told by Mr Coke himself that he often sent dogs to the Duke of Gordon and received others in exchange, in order now and then to obtain fresh blood’ (‘’The Field,’’ 8 January 1870). These facts were well known to J. H. Walsh (‘Stonehenge’), Editor of ‘’The Field’’ and a leading authority on sporting dogs, for it is he who mentions in the first chapter of his book ‘’The Dogs of the British Islands’’ (1867) a Setter ‘from Mr Coke of Norfolk and doubtless related to the late Duke of Gordon’s kennel, as Mr Coke and the duke bred together and interchanged setters frequently’. Therefore it does appear to be reasonably established that Mr Coke provided most of the original Setters for the Duke’s kennel. The Rev Hutchinson, who wrote under the pseudonym ‘Sixty-one’, insisted that ‘the original setter taken or sent to Gordon Castle by the first Marquis of Anglesea’ (‘’The Field’’, 29 January 1870), however what has been seen is that, according to Samuel Brown’s ‘old gentleman sportsman’, the Marquees of Anglesey likewise had his original stock of Setters at Beaudesart from Mr Coke – probably, although this cannot be confirmed, some years before the Gordon Castle kennel was founded; for in 1869 the Beaudesart Setters were said to have been maintained ‘for sixty years pure and unmixed with any blood’ (‘’The Field’’, 11 December 1869). It is most unlikely that the Duke obtained his setters from only one source, we know that he interbreed with other kennels besides Mr Coke’s, notably with Lord Lovat’s.

</doc>
<doc id="62889" url="https://en.wikipedia.org/wiki?curid=62889" title="Night monkey">
Night monkey

The night monkeys, also known as the owl monkeys or douroucoulis, are the members of the genus Aotus of New World monkeys (monotypic in family Aotidae). The only nocturnal monkeys, they are native to Panama and much of tropical South America. Night monkeys constitute one of the few monkey species that are affected by the often deadly human malaria protozoan "Plasmodium falciparum", making them useful as non-human primate experimental models in malaria research. 
Taxonomy.
Until 1983, all night monkeys were placed into only one ("A. lemurimus") or two species ("A. lemurinus" and "A. azarae"). Chromosome variability showed that there was more than one species in the genus and Hershkovitz (1983) used morphological and karyological evidence to propose nine species, one of which is now recognised as a junior synonym. He split "Aotus" into two groups: a northern, gray-necked group ("A. lemurinus", "A. hershkovitzi", "A. trivirgatus" and "A. vociferans") and a southern, red-necked group ("A. miconax", "A. nancymaae", "A. nigriceps" and "A. azarae"). Arguably, the taxa otherwise considered subspecies of "A. lemurinus" – "brumbacki", "griseimembra" and "zonalis" – should be considered separate species, whereas "A. hershkovitzi" arguably is a junior synonym of "A. lemurinus". A new species from the gray-necked group was recently described as "A. jorgehernandezi". As is the case with some other splits in this genus, an essential part of the argument for recognizing this new species was differences in the chromosomes. Chromosome evidence has also been used as an argument for merging "species", as was the case for considering "infulatus" a subspecies of "A. azarae" rather than a separate species. Fossil species have (correctly or incorrectly) been assigned to this genus, but only extant species are listed below.
Classification.
Family Aotidae
Physical characteristics.
Night monkeys have large brown eyes; the size improves their nocturnal vision, thus increasing their ability to be active at night. Their ears are rather difficult to see; this is why their genus name, "Aotus" (meaning "earless") was chosen. There is little data on the weights of wild night monkeys. From the figures that have been collected, it appears that males and females are similar in weight; the heaviest species is Azara's night monkey at around , and the lightest is Brumback's night monkey, which weighs between . The male is slightly taller than the female, measuring , respectively.
Ecology.
Night monkeys can be found in Panama, Colombia, Ecuador, Peru, Brazil, Paraguay, Argentina, Bolivia, and Venezuela. The species that live at higher elevations tend to have thicker fur than the monkeys at sea level. The night monkey can live in forests undisturbed by humans (primary forest) as well as forests that are recovering from human logging efforts (secondary forest).
Behavior.
The name "night monkey" comes from the fact that all species are active at night and are, in fact, the only truly nocturnal monkeys (an exception is the subspecies "Aotus azarae azarae", which is cathemeral). Night monkeys make a notably wide variety of vocal sounds, with up to eight categories of distinct calls (gruff grunts, resonant grunts, sneeze grunts, screams, low trills, moans, gulps, and hoots), and a frequency range of 190-1,950 Hz. Unusual among the New World monkeys, they are monochromats, that is, they have no colour vision, presumably because it is of no advantage given their nocturnal habits. They have a better spatial resolution at low light levels than other primates, which contributes to their ability to capture insects and move at night. Night monkeys live in family groups consisting of a mated pair and their immature offspring. Family groups defend territories by vocal calls and scent marking.
The night monkey is socially monogamous, and all night monkeys form pair bonds. Only one infant is born each year. The male is the primary caregiver, and the mother only carries the infant for the first week or so of its life. This is believed to have developed because it increases the survival of the infant and reduces the metabolic costs on the female. Adults will occasionally be evicted from the group by same-sex individuals, either kin or outsiders.

</doc>
<doc id="62891" url="https://en.wikipedia.org/wiki?curid=62891" title="English Setter">
English Setter

The English Setter is a medium size breed of dog. It is part of the Setter family, which includes the red Irish Setters, Irish Red and White Setters, and black-and-tan Gordon Setters. The mainly white body coat is of medium length with long silky fringes on the back of the legs, under the belly and on the tail. The coat features flecks of colour, and the different colour varieties are referred to as belton.
A gentle but at times strong-willed, mischievous gun dog, bred for a mix of endurance and athleticism, it is used to hunt for game such as quail, pheasant, and grouse. When working, the dog will hunt methodically seeking the airborne scent of its prey. It is sometimes referred to as the Laverack or Llewellin Setter as these were famous strains of the breed during the major development period in the 19th-century. Those from hunting stock are generally of a finer build and with less coat than those bred for show exhibition.
Generally reasonably healthy, they have an average life span of 11 to 12 years. The Kennel Club advises UK breeders to screen for hip dysplasia.
Description.
Appearance.
The English Setter is a medium-sized dog which should have an elegant overall appearance. Its size can range from for females up to for males. The field or hunting type can be finer in build and construction than those from bench or show lines. The breed was designed to hunt game such as quail, pheasant, and grouse so should be able to cover a lot of ground when seeking the airborne scent of the birds, carrying its head high. The head should be slightly domed with a muzzle of good depth and show chiselling under the eyes, which should be dark in colour with a kind, gentle expression. The top of the ears (sometimes the ears are referred to as "leathers") are positioned in line with the eyes and lie in an elegant fold. It has a long muscular neck, well angled shoulders and a brisket of good depth. The body is of a moderate length proportionate to its height and it has strong powerful hindquarters. It carries its tail in line with its back and the tail should be long enough to reach the hock.
The main body coat is short to medium length, lies flat and has a silky texture. Long silky coat – usually called "feathering", forms fringes on the outside of the ears, neck, chest, down the back of the front legs, under the belly and on the back legs. The tail is also feathered with long coat.
The body coat and feathering should be straight and flat but not profuse and never curly although a slight wave can be seen.
The bench or show type has a long, flowing coat that requires regular grooming. The field or hunting type has a shorter coat that requires less grooming.
The base colour of the coat is white with differing coloured ticking also called flecks or speckling. The various speckled coat colours when occurring in English Setters are referred to as "belton"; valid combinations are white with black ("blue belton"), white with orange flecks ("orange belton"), white with orange flecks and lighter nose ("lemon belton"), white with liver flecks ("liver belton"), or "tricolour" which is blue or liver belton with tan markings on the face, chest, and legs. The flecking should not form large patches on the body and the flecks should be distributed all over the body. The use of the word "belton" was first coined by Laverack, who developed the breed in the 19th-century, to describe his ideal for flecking and is also the name of a village in the extreme north of England. Puppies' coats may not have all the markings that they have as adults.
Temperament.
This breed's standard temperament is best described as a "Gentleman by Nature". However, it can also be strong-willed and mischievous, especially if coming from working/field breeding lines. English Setters are energetic, people-oriented dogs, that are well suited to families who can give them attention and activity, or to working with a hunter, where they have a job to do. They are active dogs that need plenty of exercise and up to two hours a day of exercise is recommended. Inside they tend to be lower energy and love to be couch potatoes and lap dogs; the breed is described as "intensely friendly," "good natured," and "adores visitors and is particularly happy with children."
They rank 37th in Stanley Coren's The Intelligence of Dogs, being of above average working/obedience intelligence. English Setters are very intelligent and can be trained to perform about any task another breed can do, with the exception of herding. However, they are not always easy to train, as their natural bird instinct tends to distract them in outdoor environments. Their temperament is considered to be gentle and as English Setters can be very sensitive to criticism, positive reinforcement training methods using treats and praise work best when undertaking basic training.
Health.
Dogs, both pedigree and cross breeds, can be affected with genetic problems. Those known to sometimes occur in English Setters can include congenital deafness, which was reported as affecting 12.4 percent of the 701 English Setters tested by the Louisiana State University in 2010. As at 2013, there has not been any detailed research on this condition undertaken in the UK; autoimmune thyroiditis, which was shown to affect 26.2 percent of 747 English Setters examined between January 1974 until December 2012 in an Orthopedic Foundation for Animals listing; canine hypothyroidism; elbow dysplasia; and allergies, which can include some sensitivity to certain food ingredients and also skin conditions, are known to occur.
In 2004, the UK Kennel Club established the Accredited Breeders Scheme, which was later called the Assured Breeders Scheme (ABS). The scheme received UKAS accreditation in April 2013. ABS members are required to adhere to additional criteria than those necessary for basic KC registration. Among the extra requirements is "Ensuring that the parents of each litter are readily identifiable by either Microchip, Tattoo or DNA profile." As at March 2013, breeders of English Setters who are members of the ABS must screen for hip dysplasia.
Some members of the breed may be affected by cancer and this was identified as the most common cause of death of English Setters in a survey undertaken by the Kennel Club; the age of death from this disease was mainly after reaching ten years of age. However, the survey had only received a small response rate. Life expectancy is between 11 to 12 years, though 13 to 15 years is not uncommon.
Function.
Setters hunt by ranging over large distances in a systematic, methodical manner, silently seeking game by scent. When prey is found by scenting the air, the dog will freeze rather than give chase. The dog will stop in a sort of crouch or "set" by freezing in a standing position upon finding their quarry and this distinctive stance is how the term “setter” evolved. Once the dog has indicated where the birds are by freezing on point, on command it would then slowly creep forward to disturb the birds into flight. Once the birds were in flight the hunter who had been following the dog would release hawks to capture the birds in the air. When netting superseded the use of hawks, setting dogs would still be used to indicate the whereabouts of the birds but the hunter would come up behind the dog and throw a net over the birds. In the mid-1600s, guns became more readily available and shooting game birds became a popular pastime of the landed gentry. The basic work of setters was still to find and point to the location of game birds but it also had to be steady to shot.
The scent of game birds is airborne so to pick up this scent the setter carries its head well up and should never follow foot scent. Most setters are born with a natural proclivity to hunting. Dogs that show excitement and interest in birds are described as being "birdy", and trainers look for puppies that show this particular trait. Training is usually done with quail as a first choice or domesticated pigeons.
Writing in 1876, Arnold Burges described the "pure-blooded English Setter" as "the best animal for American upland shooting" in his book "The American Kennel and Sporting Field".
Early history.
"Setting dogges" is an ancient term used for setters and the original purpose of the English Setter was to set or point upland game birds. From the best available information, it appears that the English Setter was a trained bird dog in England more than 400 years ago and there are works of art created in the early 15th century showing dogs that are discernible as being of a “setter type”. There is evidence that the English Setter originated in crosses of the Spanish Pointer, large Water Spaniel, and English Springer Spaniel, which combined to produce an excellent bird dog with a high degree of proficiency in finding and pointing game in open country.
Writing in 1576, Dr Johannes Caius states: "There is also at this date among us a new kind of dogge brought out of Fraunce, and they bee speckled all over with white and black, which mingled colours incline to a marble blewe". Argue speculates this may be a description of the blue belton colour found in English Setters.
Caius went on to describe the dog called a setter using the Latin name Index:
By the 17th century setters, or "setting dogges", had become established and were widespread on British estates, although the evolution into the more specific individual breeds of setters occurred at a later date. The interbreeding of the different colours was still taking place during this period but it gradually changed and sportsman/breeders began to segregate matings to dogs adapted to the terrain it was required to work on.
Breed development.
The modern English Setter owes its appearance to Edward Laverack (1800–1877), who developed his own strain of the breed by careful breeding during the 19th century in England and to another Englishman, R. Purcell Llewellin (1840–1925), who founded his strain using Laverack's best dogs and outcrossed them with the Duke, Rhoebe and later Duke's littermate Kate bloodlines with the best results.
Historically, many dogs descending from the same bloodline were referred to by the name of their breeder or owner and the nomenclatures "Laverack Setter" and "LLewellin Setter" describe English Setters bred by Laverack and Llewellin. Horace Lytle, one time gundog editor of the "Field & Stream", author and a well-known gundog trainer, clarified this in the book "How to train your bird dog", which he wrote in 1928:
Around 1826, Reverend A Harrison of Carlisle in Cumbria sold a male dog called "Ponto" and a female named "Old Moll" to Laverack and this pair formed the foundation of his English Setters. Laverack did not know the exact pedigree of these dogs but maintained the strain had been pure-bred for the previous thirty-five years. Laverack closely inbred to these two dogs for generations and his bloodline was successful in dog shows and as a working dog in field trials.
In 1874, C. H. Raymond from Morris Plains, New Jersey imported the first English Setter from the Laverack bloodline to America. The working setter Count Noble descended from these early imports and is commemorated in the Carnegie Museums of Pittsburgh.
Llewellin's strain was based on Laverack's best dogs, which were then outcrossed with the bloodlines of his dogs Duke, Rhoebe and later Duke's littermate, Kate. It was Kate bred with Laverack's best hunting males that produced Llewellin's ideals Fd.Ch.Ch. Armstrong's Dash II and later Fd.Ch.Ch Dashing Bondhu. They were the foundation of Llewellin's personal strain known as "Dashing Bondhu". William Humphrey (1882–1963) inherited them from Llewellin in 1925 and continued them pure until his death in 1963.
Jim the Wonder Dog, described as a Llewellin setter, was said to have "possessed an occult power" and there is a bronze statue of him in a memorial garden on the square in Marshall, Missouri, built to commemorate him.
In competitions.
The field type and show type English Setter look very different, even though they are the same breed. Field type setters are often smaller and are seen with less feathering and usually more distinctive spotting than show type setters. Both traits are beneficial in the field: less feathering makes getting burs out of their coat easier and the spotting makes them easier to see in the field.
English Setters are classified within the gundog group in the UK and the Sporting group in America and Canada. The FCI place them in section 2, British and Irish Pointers and Setters, of Group 7.
In the English Setter breed, compared to other breeds, there are very few Dual Champions. The Kennel Club have four champion titles available to be achieved by setters competing in the UK. These are Show Champion (Sh Ch) which is awarded to dogs who have won three Challenge Certificates (CCs) under three different judges with at least one CC won after 12 months of age; Champion (Ch) is the title gained by dogs who have won a Sh Ch title plus a field trial award, Diploma of Merit or a Show Gundog Working Certificate; Field Trial Champion (Ft Ch) means the dog has won a pointer or setter open stake or two first prizes at two different stakes under two different A Panel judges. There must be no less than 16 runners entered; and a Dual Champion – the highest award available to setters – is a dog who has achieved the titles of Show Champion and Field Trial Champion.
An English Setter called "Countess" was the first gun dog to ever attain a Dual Champion title. She was sired by Dash 2nd and her dam was Moll 3rd. Her breeder was Laverack, who sold her to Sam Lang; he in turn passed her on to Llewellin in whose name she was entered in field trials.
In the UK, the breed has been successful at Crufts and secured the award of best in show in 1964, 1977 and 1988. At the Westminster show in America an English Setter won the best in show title in 1938. He was only 11 months old and at his very first show. This was before entry to the show was restricted to Champions in 1992. As of 2013, he is the only setter to achieve best in show at Westminster since the award of best in show started to be made in 1907.
Registrations.
When the American Kennel Club was established in 1878, English Setters, together with eight other Sporting breeds, were accepted as the first pure-bred registrations by the Club. The very first dog registered with the AKC and the holder of registration number one was an English Setter named "Adonis". He was born in 1875 and is recorded as sired by "Leicester" out of a bitch named "Dart". His colours were given as black, white and tan. He was owned by his breeder George E. Delano of New Bedford, MA.
English Setters were especially popular in the UK during the 1960s, 70s and 80s and registrations of puppies reached 1344 during 1974. In 2012, the Kennel Club listed the English Setter amongst the Vulnerable Native Breeds as only 234 puppies were registered. A decade earlier, in 2002, there were 568 English Setter puppies registered. However, during 2012 the number of English Setter puppies registered increased to 314, so the breed was moved to the Kennel Club's "At Watch" list, which is for breeds with registrations from 300–450. In 2015 registrations fell to 289 resulting in a return to the Vulnerable Native Breeds list for 2016. The breed is still fairly well represented in Italy, where it is popular as a working gun-dog. Even in Italy, however the breed is in sharp decline, going from 20,999 registrations in 2002, to 14510 registrations in 2011. In contrast, the American Kennel Club stated that 2011 was the "year of the setters, with all four making big jumps over the past year". English Setters had previously ranked at 101 in 2010 but moved up to 87 in 2011, a position the breed maintained in 2012.
References.
Citations
Bibliography

</doc>
<doc id="62893" url="https://en.wikipedia.org/wiki?curid=62893" title="Dingo">
Dingo

The dingo ("Canis lupus dingo") is a wild dog found in Australia. Its exact ancestry is debated, but dingoes are generally believed to be descended from semi-domesticated dogs from East or South Asia, which returned to a wild lifestyle when introduced to Australia. Both dingo and domestic dog are classified as a subspecies of "Canis lupus" in Mammal Species of the World.
The dingo's habitat ranges from deserts to grasslands and the edges of forests. Dingoes will normally make their dens in deserted rabbit holes and hollow logs close to an essential supply of water.
The dingo is the largest terrestrial predator in Australia, and plays an important role as an apex predator. However, the dingo is seen as a pest by livestock farmers due to attacks on animals. Conversely, their predation on rabbits, kangaroos and rats may be of benefit to graziers.
For many Australians, the dingo is a cultural icon. The dingo is seen by many as being responsible for thylacine extinction on the Australian mainland about two thousand years ago, although a recent study challenges this view. Dingoes have a prominent role in the culture of Aboriginal Australians as a feature of stories and ceremonies, and they are depicted on rock carvings and cave paintings.
Despite being an efficient hunter, it is listed as vulnerable to extinction. It is proposed that this is due to susceptibility to genetic pollution: a controversial concept according to which interbreeding with domestic dogs may dilute the dingo's unique adaptations to the Australian environment.
Etymology.
The most commonly used name is dingo, which has its origins in the early European colonisation in New South Wales and is most likely derived from the word "tingo", used by the Aboriginal people of Port Jackson for their camp dogs. Depending on where they live, local dingoes can be called "alpine dingoes," "desert dingoes," "northern dingoes," "Cape York dingoes," or "tropical dingoes". More recently, people have begun to call dingoes "Australian native dogs" or, by reasoning that they are a subspecies of "Canis lupus", "Australian wolves".
In Australia, the term "wild dog" is also widely used, but generally includes dingoes as well as dingo-hybrids and other feral dogs.
The dingo has been given different names in the Indigenous Australian languages, including "joogong", "mirigung", "noggum", "boolomo", "papa-inura", "wantibirri", "maliki", "kal", "dwer-da", "kurpany", "aringka", "palangamwari", "repeti" and "warrigal". Some languages have different names for the dingoes depending on where they live; the Yarralin, for instance, call the dingoes that live with them "walaku" and those in the wilderness "ngurakin".
Taxonomy.
Since its first official nomenclature in 1792 ("Canis antarcticus"), the scientific name of the dingo has changed several times.
Current taxonomy classifies the Australian dingo, together with its closest relatives outside of Australia, as a subspecies of "Canis lupus" as "Canis lupus dingo", separate from the dog, "Canis lupus familiaris". An older taxonomy, used throughout most of the 20th century, applied the epithet "Canis familiaris dingo". This taxonomy assumed that domestic dogs are a distinct species from "Canis lupus", with the dingo classified as a subspecies of domestic dog. However, the term "Canis dingo", which classifies the dingo as a separate species from both dogs and wolves, has gained support in 2014 in a study that established a reference description of the dingo based on pre-20th century specimens that are unlikely to have been influenced by hybridisation. The dingo differs from the domestic dog by relatively larger palatal width, relatively longer rostrum, relatively shorter skull height and relatively wider top ridge of skull. A sample of 19th century dingo skins the study examined suggests that there was considerable variability in the colour of dingoes and included various combinations of yellow, white, ginger and darker variations from tan to black. Although it remained difficult to provide consistent and clear diagnostic features, the study placed morphological limits on what can be considered a dingo.
Description.
Domestic and pariah dogs in southern Asia share so many characteristics with Australian dingoes that they are now considered to be members of the same taxon "Canis lupus dingo", a particular subspecies of "Canis lupus". While the relationship with humans varies widely among these animals, they are all quite similar in terms of physical features.
A dingo has a relatively broad head, a pointed muzzle and erect ears. Eye colour varies from yellow over orange to brown. Compared to other similarly sized "familiaris" dogs, dingoes have longer muzzles, larger carnassials (large teeth found in many carnivorous mammals), longer canine teeth, and flatter skulls with larger nuchal lines.
Size.
The average Australian dingo is tall at the shoulders and measures from nose to tail tip. The average weight is ; however, there are a few records of outsized dingoes weighing up to . Males are typically larger and heavier than females of the same age. Dingoes from northern and northwestern Australia are larger than central and southern populations. Australian dingoes are invariably heavier than Asian ones. The legs are about half the length of the body and the head put together. The hind feet make up a third of the hind legs and have no dewclaws. Dingoes can have sabre-form tails (typically carried erect with a curve towards the back) or tails carried directly on the back.
Fur.
Fur of an adult dingo is short and soft, bushy on the tail, and varies in thickness and length depending on the climate. The fur colour is mostly sandy to reddish brown, but can include tan patterns and sometimes be black, light brown, or white. Completely black dingoes might have been more prevalent in Australia in the past, but have only been rarely sighted in recent times. They are now more common in Asia.
Most dingoes are at least bi-coloured, with small, white markings on the chest, muzzle, tag, legs and paws being the most common feature. "Pure" dingoes are also found in white or cream (not albinism). They are also found in black and tan colourations. In the case of reddish individuals, there can be small, distinctive, dark stripes on the shoulders.
Origin and genetic status.
Since dingoes were the largest wild placental mammals in Australia at the time of colonisation and looked similar to domestic dogs, their origin has always been questioned and much debated. Archaeological and morphological studies indicated a relatively late introduction and a close relationship to other domestic dogs. Their exact descent, place of origin and date of arrival in Australia were not identified, nor whether they had once been domesticated or half-domesticated and had gone feral, or whether they had already existed as truly wild animals.
It is widely held that dingoes have evolved or were bred from the Indian wolf or Arabian wolf around 6,000 to 10,000 years ago, as was assumed for all domestic dogs. This theory was based on the morphological similarities of dingo skulls and the skulls of these subspecies of wolves. However, genetic analyses indicated a much earlier domestication. New studies suggest dingoes may have originated in southern China, travelling to Australia anywhere between 4600 and 18,300 years ago.
DNA analysis.
Analyses of amino acid sequences of the haemoglobin from a "pure" dingo in the 1970s supported the theory that dingoes are more closely related to domestic dogs than they are to grey wolves or coyotes. As a result, it was assumed that dingoes and other similar Asian dogs belong to a group of domestic dogs that went feral at a very early time. DNA studies on Australian dingoes and domestic dogs were also undertaken in an effort to reliably differentiate between both populations and to try to determine the extent of interbreeding.
The first two examinations looked firstly at 14 loci (the specific locations of the DNA-sequence of a chromosome), with five of these being more closely examined. No genetic difference could be found. The analyses were then extended to cover 16 loci, comparing dingoes from Central Australia, dingoes from the Eastern Highlands, dingo-hybrids and domestic dogs of other origin. Again, no differences could be found, regardless of the type of examination used. It was reasoned that dingoes and domestic dogs must have a very similar gene pool. However, since only a few differences in the enzymes of different species of the genus "Canis" could be found, it was assumed that a lack of differences might not indicate a close taxonomic relationship. It was also reasoned that the degree of interbreeding in the wild would be hard to determine.
During further analyses in the late 1990s, researchers examined 14 loci and detected a significantly lower genetic variability among Australian dingoes than among domestic dogs, leading to consideration of the possibility of a small founding population. There was one locus found that might have been suitable for differentiation, but not in the case of interbreeding of a dingo-hybrid with a "pure" dingo. Additionally, it was suspected that findings of other suitable loci might be used to determine whether there are clearly separate sub-populations of the "pure" dingoes.
More recent studies have concluded that the previous assumptions about the origins of the dingo are incorrect, with the dingo appearing to have no ancestral relationship with the wolf. Dr Mathew Crowther of Sydney University says they based their research using specimens collected in the 19th century, and the dingo should be described as "Canis dingo" rather than "Canis lupus dingo".
Mitochondrial DNA sequences.
To determine the origin and time of arrival of Australian dingoes, mitochondrial DNA (mtDNA) sequences of 211 dingoes and 19 archaeological samples from pre-European Polynesia were compared with mtDNA samples of 676 domestic dogs and 38 grey wolves in 2004. The domestic dog samples came from China, Africa, Southwest Asia, India, Siberia, the arctic America, Europe, Mongolia, Korea, Japan, Vietnam, Cambodia, Thailand, Indonesia, the Philippines, Malaysia, New Zealand, Hawaii and the highlands of New Guinea. The dingo samples came from zoos, wildlife parks, dingo conservation groups, dingo lovers and 192 wild living specimens from 27 areas in Australia, mainly located in the Pilbara region, New South Wales and northeastern Victoria. The wild specimens had been selected based on similarities of external appearance to exclude the influence of dingo-hybrids and domestic dogs as far as possible.
Compared to wolves and domestic dogs, the variation of mtDNA sequences was very limited. Among dingoes, only 20 mtDNA sequences differing in two point mutations at most could be found. In comparison, 114 mtDNA-sequences with a maximal difference of 16 point mutations between the DNA-types could be found among domestic dogs. Two of the dingo mtDNA-types were similar to that of domestic dogs (A9, A29), while the other 18 types were unique to dingoes.
In a phylogenetic tree of wolves and domestic dogs, dingoes were included in the main clade (A), which contained 70% of all domestic dog types. Within this clade, the dingo-types formed a group around the type A29, which was surrounded by 12 less frequent dingo-types, as well as a set of other domestic dog types. This mtDNA-type was found in 53% of the dingoes and was also found among some domestic dogs from East Asia, New Guinea and the American Arctic. Based on these findings, it was reasoned that all dingo-mtDNA-types originated in A29. A9 was only found in one individual, and it was regarded as possible that this type is the result of a parallel mutation.
Based on a mutation rate of mtDNA with A29 being the only founder type, it was considered that dingoes probably arrived in Australia between 4,600 and 5,400 years ago, which was consistent with archaeological findings. However, it was also considered that dingoes might have arrived from an even earlier date of up to 10,800 years ago in the event of the mtDNA-mutation rate being slower than assumed. It was further reasoned that these findings strongly indicate a descent of dingoes from East Asian domestic dogs and not from Indian domestic dogs or from wolves. In addition these findings indicated two possibilities of descent: all Australian dingoes are descended from a few domestic dogs, theoretically one pregnant female; and all Australian dingoes are descended from a group of domestic dogs, who radically lost their genetic diversity through one or several severe genetic bottlenecks on their way from the Asian continent over South East Asia.
Other types.
Nonetheless, the existence of other mtDNA-types on the islands surrounding Australia indicate there have been other types apart from A29 and only one single founding event. These results also indicated that there hasn't been any significant introduction of other domestic dog on the Australian continent prior to the arrival of the Europeans. Also, a shared origin and some sort of genetic exchange between Australian dingoes and the New Guinea singing dogs was regarded as possible. The current state of the Australian dingoes was ascribed to the long wild existence of these dogs and assumed that they are an isolated example of early domestic dogs.
Despite accordant claims, these findings did not show that only dingo females mate with non-dingo males and not vice versa. The findings would not allow such a conclusion, since the mating of a dingo female with a non-dingo male could not be detected via analyses of mtDNA. Furthermore, the researchers made sure from the start that dingo-hybrids were excluded as far as possible.
Communication.
Like all domestic dogs, dingoes tend towards phonetic communication. However, in contrast to domestic dogs, dingoes howl and whimper more, and bark less. Eight sound classes with 19 sound types have been identified.
Barking.
Compared to most domestic dogs, the bark of a dingo is short and monosyllabic, and is rarely used. Barking was observed to make up only 5% of vocalisations. Dog barking has always been distinct from wolf barking. Australian dingoes bark mainly in swooshing noises or in a mixture of atonal and tonal sounds. In addition, barking is almost exclusively used for giving warnings. Warn-barking in a homotypical sequence and a kind of "warn-howling" in a heterotypical sequence have also been observed. The bark-howling starts with several barks and then fades into a rising and ebbing howl and is probably (similar to coughing) used to warn the puppies and members of the pack. Additionally, dingoes emit a sort of "wailing" sound, which they mostly use when approaching a wateringhole, probably to warn already present dingoes.
According to the present state of knowledge, it is not possible to get Australian dingoes to bark more frequently by putting them in contact with other domestic dogs. However, German zoologist Alfred Brehm reported a dingo that learned the more "typical" form of barking and how to use it, while its brother did not. Whether dingoes bark or bark-howl less frequently in general is not certain.
Howling.
Dingoes have three basic forms of howling (moans, bark-howls and snuffs) with at least 10 variations. Usually, three kinds of howls are distinguished: long and persistent, rising and ebbing, and short and abrupt.
Observations have shown that each kind of howling has several variations, though their purpose is unknown. The frequency of howling varies with the season and time of day, and is also influenced by breeding, migration, lactation, social stability and dispersal behaviour. Howling can be more frequent in times of food shortage, because the dogs become more widely distributed within their home range.
Additionally, howling seems to have a group function, and is sometimes an expression of joy (for example, greeting-howls). Overall howling was observed less frequently in dingoes than among grey wolves. It may happen that one dog will begin to howl, and several or all other dogs will howl back and bark from time to time. In the wilderness, dingoes howl over long distances to attract other members of the pack, to find other dogs, or to keep intruders at bay. Dingoes howl in chorus with significant pitches, and with increasing number of pack-members, the variability of pitches also increases. Therefore, it is suspected that dingoes can measure the size of a pack without visual contact. Moreover, it has been proposed that their highly variable chorus howls may generate a confounding effect in the receivers by making pack size appear larger.
Other forms of communication.
Growling, making up approximately 65% of the vocalisations, is used in an agonistic context for dominance, and as a defensive sound. Similar to many domestic dogs, a reactive usage of defensive growling is only rarely observed. Growling very often occurs in combination with other sounds, and has been observed almost exclusively in swooshing noises (similar to barking).
During observations in Germany, dingoes were heard to produce a sound that observers have called "Schrappen". It was only observed in an agonistic context, mostly as a defence against obtrusive pups or for defending resources. It was described as a bite intention, during which the receiver is never touched or hurt. Only a clashing of the teeth could be heard.
Aside from vocal communication, dingoes communicate, like all domestic dogs, via scent marking specific objects (for example, "Spinifex") or places (such as waters, trails and hunting grounds) using chemical signals from their urine, feces and scent glands. Males scent-mark more frequently than females, especially during the mating season. They also scent-rub, whereby a dog rolls its neck, shoulders, or back on something that is usually associated with food or the scent markings of other dogs.
Unlike wolves, dingoes can react to social cues and gestures from humans.
Behaviour.
Dingoes tend to be nocturnal in warmer regions, but less so in cooler areas. Their main period of activity is around dusk and dawn. The periods of activity are short (often less than one hour) with short times of resting. Dingoes have two kinds of movement: a searching movement (apparently associated with hunting) and an exploratory movement (probably for contact and communication with other dogs).
In general, dingoes are shy towards humans. However, there are reports of dingoes that were agitated by the presence of humans, such as around camps in national parks, near streets or suburbs. According to studies in Queensland, the wild dogs (dingo hybrids) there, move freely at night through urban areas and cross streets and seem to get along quite well.
Dietary habits.
About 170 species (from insects to buffalo) have been identified as part of the dingo's diet. In general, livestock seems to make up only a small proportion of their diet. In continent-wide examinations, 80% of the diet of wild dogs consisted of 10 species: red kangaroo, swamp wallaby, cattle, dusky rat, magpie goose, common brushtail possum, long-haired rat, agile wallaby, European rabbit and the common wombat. This narrow range of major prey indicates these wild dogs are rather specialised, but in the tropical rainforests of northeastern Australia, dingoes are supposed to be opportunistic hunters of a wide range of mammals. In certain areas, they tend to specialise on the most common prey, with a preference for medium- to large-sized mammals. Their consumption of domestic cats has also been proven. Non-mammalian prey is irregularly eaten and makes up only 10% of the dingo's diet. Big reptiles are only rarely captured, at least in eastern Australia, although they are widespread. It is possible that especially large monitor lizards are too defensive and well-armed, or they are simply able to flee fast enough into dens or climb trees.
Dietary composition varies from region to region. In the gulf region of Queensland, feral pigs and agile wallabies are the dingo's main prey. In the rainforests of the north, the main prey consists of magpie geese, rodents and agile wallabies. In the southern regions of the Northern Territory, the dogs mainly eat European rabbits, rodents, lizards and red kangaroo; in arid Central Australia, rabbits, rodents, lizards, red kangaroo and cattle carcasses; and in the dry northwest, eastern wallaroos and red kangaroo. In the deserts of the southwest, they primarily eat rabbits, and in the eastern and southeastern highlands, they eat wallabies, possums and wombats.
To what extent the availability of rabbits influences the composition of the diet cannot be clarified. However, because rabbit haemorrhagic disease killed a large part of the Australian rabbit population at the end of the 20th century, it is suspected that the primary prey of the dogs has changed in the affected areas. Also, on Fraser Island, fish have been proven to be a part of the dingo diet. The main prey species, though, are bandicoots and several rodents. Dingoes also eat a lot of echidnas, crabs, small skinks, fruits and other plants, as well as insects (mostly beetles). During these observations, only 10% of the examined faeces-samples contained human garbage (in earlier studies 50% were reported).
When scavenging for food, wild dogs (presumably, all dogs free to roam, not just dingoes) primarily eat cattle and kangaroo carcasses. Dingoes in coastal regions regularly patrol the coast for dead fish, seals, penguins and other washed-up birds.
Dingoes in general drink one litre of water a day in the summer and about half a litre a day in winter. During the winter in arid regions, dingoes could potentially live from the liquid in the bodies of their prey, as long as the number of prey is sufficient. Similarly, weaned pups in Central Australia are able to draw their necessary requirements of liquid from their food. There, regurgitation of water by the females for the pups was observed. During lactation, females have no higher need of water than usual, since they consume the urine and feces of the pups and therefore recycle the water and keep the den clean.
17 tracked dingoes have survived 22 days without water in the winter.
Hunting behaviour.
Dingoes often kill by biting the throat, and they adjust their hunting strategies to suit circumstances. For larger prey, due to strength and potential danger, two or more individuals are needed to bring down the prey. Such group formations are unnecessary when hunting rabbits or other small prey.
Kangaroo hunts are probably more successful in open areas than in places with high densities of vegetation, and juvenile kangaroos are killed more often than adults. Dingoes typically hunt large kangaroos by having lead dingoes chase the quarry toward their waiting packmates, which are skilled at cutting corners in chases. In one area of Central Australia, dingoes hunt kangaroos by chasing them toward a wire fence that hindered their escape.
Birds can be captured when they do not fly or fail to take off fast enough. Dingoes also steal the prey of eagles and the coordinated attack of three dingoes for killing a large monitor lizard has been observed.
Reports state that some dingoes live almost entirely on human food through stealing, scavenging, or begging. In fact, dingoes are well known for such behaviour in some parts of Australia. It is suspected that this might cause the loss of hunting strategies or a change in the social structures.
During studies at the Fortescue River in the mid-1970s, observation showed that most of the studied dingoes learned to hunt and kill sheep very quickly, even without prior contact with sheep. Although the dingoes killed many sheep at that time, they still killed and ate kangaroos.
During the early 1990s, wild dogs were observed to have an extraordinarily high success rate when killing sheep, and did not have to hunt in a coordinated manner to achieve success. Often, a dog may chase and outrun a single sheep, only to turn away suddenly and chase another. Therefore, only a small proportion of the injured or killed sheep and goats are eaten, which seems to be the rule and not the exception. The dog probably falls into some kind of "killing spree," due to the rather panicked and uncontrolled flight behaviour of the sheep, which run in front of the dingoes time and again and, therefore, cause one attack after another. Dingoes often attack sheep from behind during the sheep's flight, which causes injuries to the sheep's hind legs. Rams are normally attacked from the side – probably in order to avoid the horns – or sometimes on the testicles. Inexperienced dingoes, or those that kill "for fun," sometimes cause significant damage to the sheep's hind legs, which often causes death.
Nearly all dingo attacks on cattle and water buffalo are directed against calves. Hunting success depends on the health and condition of the adult bovines and on their ability to defend their calves. The defence behaviour of the mother can be sufficient to fend off an attack. Therefore, the basic dingo tactics of attack are distracting the mother, rousing the herd/group and waiting (sometimes for hours), and testing of the herd to find the weakest members.
While locating a cattle herd, dingoes have been observed to make several feint attacks, during which they concentrate on the calves at first then, later on, attack the mothers to distract them. Thereupon, the dingoes retreat and wait at a distance from the herd until the rest of the cows have gathered their calves and move on.
During another observed attack, "subgroups" of a dingo pack took turns in attacking and resting, until the mother was too tired to effectively defend her calf. Dingoes have been observed hunting a water buffalo with an estimated weight of 200 kg, and taking turns biting the buffalo's legs during the chase.
Social behaviour.
The dingo's social behaviour is about as flexible as that of a coyote or gray wolf, which is perhaps one of the reasons it was initially believed that the dingo was descended from the Indian wolf. While young males are often solitary and nomadic in nature, breeding adults will often form a settled pack. However, in areas of the dingo's habitat with a widely spaced population, breeding pairs remain together, apart from others.
Where conditions are favourable among dingo packs, the pack is stable with a distinct territory and little overlap between neighbors. The size of packs often appears to correspond to the size of prey that appears in the pack's territory. Desert areas have smaller groups of dingoes with a more loose territorial behaviour and sharing of the water sites. It has been noted that the average monthly pack size was between three and twelve members.
Similar to other canids, a dingo pack largely consists of a mated pair, their current year's offspring, and occasionally a previous year's offspring. There are dominance hierarchies both between and within males and females, with males usually being more dominant than females. However, a few exceptions have been noted in captive packs. During travel, while eating prey, or when approaching a water source for the first time, the breeding male will be seen as the leader, or alpha. Subordinate dingoes will approach a more dominant dog in a slightly crouched posture, ears flat and tail down, to ensure peace in the pack. Establishment of artificial packs in captive dingoes have failed.
Reproduction.
Dingoes breed once annually, depending on the estrus cycle of the females which, according to most sources, only come in heat once per year. Dingo females can come in heat twice per year, but can only be pregnant once a year, with the second time only seeming to be pregnant.
Males are virile throughout the year in most regions, but have a lower sperm production during the summer in most cases. During studies on dingoes from the Eastern Highlands and Central Australia in captivity, no specific breeding cycle could be observed. All were potent throughout the year. The breeding was only regulated by the heat of the females. A rise in testosterone was observed in the males during the breeding season, but this was attributed to the heat of the females and copulation. In contrast to the captive dingoes, captured dingo males from Central Australia did show evidence of a male breeding cycle. Those dingoes showed no interest in females in heat (this time other domestic dogs) outside of the mating season (January to July) and did not breed with them.
The mating season usually occurs in Australia between March and May (according to other sources between April and June). In South East Asia, mating occurs between August and September. During this time, dingoes may actively defend their territories using vocalisations, dominance behaviour, growling and barking.
Most females in the wild start breeding at the age of two years. Within packs, the alpha female tends to go into heat before subordinates and actively suppresses mating attempts by other females. Males become sexually mature between the ages of one and three years. The precise start of breeding varies depending on age, social status, geographic range and seasonal conditions. Among dingoes in captivity, the pre-estrus was observed to last 10–12 days. However, it is suspected that the pre-estrus may last as long as 60 days in the wild.
In general, the only dingoes in a pack that successfully breed are the alpha pair, and the other pack members help with raising the pups. Subordinates are actively prevented from breeding by the alpha pair and some subordinate females have a false pregnancy. Low-ranking or solitary dingoes can successfully breed if the pack structure breaks up.
The gestation period lasts for 61–69 days and the size of the litter can range from one to 10 (usually five) pups, with the number of males born tending to be higher than that of females. Pups of subordinate females usually get killed by the alpha female, which causes the population increase to be low even in good times. This behaviour possibly developed as an adaptation to the fluctuating environmental conditions in Australia. Pups are usually born between May and August (the winter period), but in tropical regions, breeding can occur at any time of the year.
At the age of three weeks, the pups leave the den for the first time, and leave it completely at eight weeks. In Australia, dens are mostly underground. There are reports of dens in abandoned rabbit burrows, rock formations, under boulders in dry creeks, under large spinifex, in hollow logs, in augmented burrows of monitor lizards and wombat burrows. The pups usually stray around the den within a radius of 3 km, and are accompanied by older dogs during longer travels. The transition to consuming solid food is normally accompanied by all members of the pack during the age of 9 to 12 weeks. Apart from their own experiences, pups also learn through observation. Young dingoes usually become independent at the age of three to six months or they disperse at the age of 10 months when the next mating season starts.
Migration.
Dingoes usually remain in one area and do not undergo seasonal migrations. However, during times of famine, even in normally "safe" areas, dingoes travel into pastoral areas, where intensive, human-induced control measures are undertaken. It was noted in Western Australia in the 1970s that young dogs can travel for long distances when necessary. About 10% of the dogs captured—all younger than 12 months—were later recaptured far away from their first location. Among these, 10% of the travelled distance for males was 21.7 km and for females 11 km. Therefore, travelling dingoes had lower chances of survival in foreign territories, and it was apparently unlikely that they would survive long migrations through occupied territories. The rarity of long migration routes seemed to confirm this. During investigations in the Nullarbor Plain, even longer migration routes were recorded. The longest recorded migration route of a radio-collared dingo was about 250 km.
Mortality and health.
Documented evidence shows that dingoes in captivity have survived for up to 24 years.
The main cause of death for dingoes is being killed by humans, crocodiles and dogs, including other dingoes. Other causes of death include starvation and dehydration during times of drought or after strong bush fires, infanticide, snake bites, killing of pups by wedge-tailed eagles, and injuries caused by cattle and buffalo.
Dingoes are susceptible to the same diseases as domestic dogs. At present, 38 species of parasites and pathogens have been detected in Australian dingoes. The bulk of these diseases have a minimal influence on their survival. The exceptions include canine distemper, hookworms and heart worms in North Australia and southeastern Queensland. Dingo pups can also be killed by lungworms, whipworms, hepatitis, coccidiosis, lice and ticks. Sarcoptic mange is a widespread parasitic disease among the dingoes of Australia, but is seldom debilitating. Free-roaming dogs are the primary host of Echinococcosis (tapeworms) and have an infection rate of 70 to 90%.
Distribution.
It is only possible to give a crude description of the dingo's distribution area and the accordant population density. Giving an exact assessment of the distribution of dingoes and other domestic dogs is difficult since the exact extent of interbreeding between the two is not known. The following information on the distribution of the dingo applies to dogs classified as dingoes based on fur colour, body form and breeding cycle. Therefore, the maps illustrating their distribution might be conflicting.
Distribution in the past.
Based on fossil, molecular and anthropogenic evidence, dingoes might have once had a widespread distribution. These early dingoes would have associated with nomadic hunter-gatherer societies and later with the rising agricultural centres. It is further assumed that they would have been tamed there and then transported to various places in the world. Findings of dingo habitation in Thailand and Vietnam, regarded as the oldest findings, have been estimated at 5,000 to 5,500 years old. The age of similar findings from the highlands of Indonesia vary from 2,500 to 5,000 years.
Originally, the dingo was suspected to have been introduced to Australia in the Pleistocene by Aborigines, which led to confusion concerning the dingo's nomenclature. Today, the most common theory is that the dingo arrived in Australia about 4,000 years ago. In 1979, an eroding dingo skeleton was excavated by Brown and Gollan (ANU) on the mid-coast of southern New South Wales, dated to 6,000 years of age. More recent mitochondrial DNA research estimates the arrival of dingoes to be between 4,600 and 18,300 years. Evidence of dingoes appears to be absent from Tasmania, which was separated from the main Australian landmass around 12,000 years ago due to a rise in sea level, which led to the theory that dingoes have not been in Australia longer than this time. To reach Australia from Asia, there would have been at least 50 km of open sea to be crossed, even at the lowest sea level. Since there are few, if any, cases of a large land animal making such a journey by itself (the Falkland Islands wolf being a possible exception), the ancestors of modern dingoes most likely were brought to Australia on boats by Asian seafarers. A dance of the Aborigines in the coastal regions of the Kimberley, during which they depict dogs running excitedly up and down a boat and finally jumping into the water, is seen as further evidence for the introduction of dingoes by seafarers. These dogs possibly were used as food or eventually guard dogs. Potentially, the dingo came to Australia and the islands of South East Asia and the Pacific during the course of expansion of the Austronesian culture.
The two main theories concerning the geographical origin and travel routes of the modern dingo's ancestors and their arrival in Australia are:
Whether there were several introductions of dingoes to Australia or just one is not yet known.
The first official report of a "wild dog" in Australia comes from Captain William Dampier in 1699. At the time, dingoes were probably widespread over the main part of the continent and lived in the wild, as well as alongside the Aboriginals. They were mostly tolerated by the European settlers and sometimes kept as pets. The number of dingoes was probably low in those times and increased since then in some parts of Australia. Their numbers probably increased strongly around the 1880s due to the establishment of the pastoral economy and artesian watering places, and probably peaked in the 1930s and 1950s. Afterwards, the numbers remained high, but the percentage of dingo-hybrids has significantly increased since then.
Present distribution.
Today, dingoes live in many diverse habitats, including the snow-covered mountain forests of eastern Australia, the deserts of Central Australia, and Northern Australia's tropical forest wetlands. The absence of dingoes in many parts of the Australian grasslands is probably due to human persecution. Based on skull characteristics, size, fur colour and breeding cycles, distinct regional populations could be seen to exist between Australia and Asia, but not within Australia.
The wild dog population of Australia now includes dingoes and a wide panoply of feral domestic dogs (mostly mixed-breeds and dingo-hybrids) having an enormous variety of colours. Due to the increased availability of water, native and introduced prey, livestock and human-provided food, this population is on the increase. Reports from some parts of Australia indicate that wild dogs now hunt in packs there, where they had previously been solitary hunters. Dingo densities have been measured at up to 0.3 per square kilometre in both the Guy Fawkes River region of New South Wales and in South Australia at the height of a rabbit plague.
"Pure" dingoes are regarded as widespread in Northern, North West and Central Australia; rare in Southern and Northeast Australia; and possibly extinct in the Southeastern and Southwestern areas.
The establishment of agriculture caused a significant decrease in dingo numbers, and dingoes were practically expelled from the territories occupied by the sheep industry, primarily affecting large parts of southern Queensland, New South Wales, Victoria and South Australia. This situation was maintained by the construction of the Dingo Fence. Although dingoes were eradicated from most areas south of the Dingo Fence, they still exist in an area of about 58,000 km2 in the dry northern areas north of the Dingo Fence and, therefore, on about 60% of the entire area.
In Victoria, wild dog populations are currently concentrated on the densely forested areas of the Eastern Highlands, from the border to New South Wales, south to Healesville and Gembrook. They also exist in the large desert in the northwest of the state. Wild dog populations in New South Wales primarily exist along the Great Dividing Range and the hinterlands on the coast, as well as in the Sturt National Park in the northwest of the state.
In the rest of the continent, dingoes are regarded as widespread, with the exception of the arid eastern half of Western Australia. In the bordering areas of South Australia and the Northern Territory, they are regarded as naturally scarce. Wild dogs are widespread in the Northern Territory, with the exception of the Tanami and Simpson Deserts, where they are rare due to the lack of watering holes. However, local concentrations exist there near artificial water sources. According to DNA examinations from 2004, the dingoes of Fraser Island are "pure". However, skull measurements from the mid-1990s had a different result. A 2013 study showed that dingoes living in the Tanami Desert are among the "purest" in Australia.
Outside Australia, dingoes were proven to exist in Thailand, based on comparisons between the skulls of Thai dogs and those of fossil and present-day dingoes. The population there probably has the largest proportion of "pure" dingoes. They are widespread in northern and central Thailand and rare in the southern regions. They may also exist in Burma (Myanmar), China, India, Indonesia, Laos, Malaysia, Papua New Guinea, the Philippines and Vietnam, but if they exist there, their distribution is unknown. Dingoes are regarded as widespread in Sulawesi, but their distribution in the rest of Indonesia is unknown. They are regarded as rare in the Philippines and are probably extinct on many islands. In Korea, Japan and Oceania, a few local dog breeds with dingo-like features exist, but dingoes are considered extinct there.
Ecological impact of the dingo after its arrival in mainland Australia.
The dingo is suspected to have caused the extinction of the thylacine, the Tasmanian devil and the Tasmanian nativehen from mainland Australia, since a correlation in space and time is found between the arrival of the dingo and the extinctions of these species. Recent studies have questioned this theory, suggesting that climate change and increasing human populations may have been the cause. Dingoes do not seem to have had the same ecological impact that the red fox had in later times. This might be connected to the dingo's way of hunting and the size of their favoured prey, as well as to the low number of dingoes in the time before European colonisation.
The assumption that dingoes and thylacines were competitors for the same prey stems from their external similarities; the thylacine had a stronger and more efficient bite, but was probably dependent on relatively small prey, while the dingo's stronger skull and neck would have allowed it to bring down bigger prey. The dingo was probably a superior hunter, as it hunted cooperatively in packs and could better defend resources, while the thylacine was probably more solitary. Also, wild dingo populations might have had demographic support from conspecific living with humans and may have introduced new diseases that affected the thylacine more severely.
The extinction of the thylacine on the continent around 2,000 years ago has also been linked to changes in climate and land use by the Aborigines. It is plausible to name the dingo as the cause of the extinction, but significant morphological differences between the two suggest that the ecological overlapping of both species might be exaggerated. The dingo has the dentition of a generalist, while the thylacine had the dentition of a specialist carnivore without any signs of consumption of carrion or bones. It is also argued that the thylacine was a flexible predator that should have withstood the competition by the dingo, but was instead wiped out due to human persecution.
This theory does not explain how the Tasmanian devil and the dingo coexisted on the same continent until about 430 years ago, when the dingo supposedly caused the Tasmanian devil's demise. The group dynamics of dingoes should have successfully kept devils away from carrion, and since dingoes are able to break bones, little would have been left for the devils to scavenge. Additionally, devils are successful hunters of small- to medium-sized prey, so there should have been an overlapping of the species in this area, too. Furthermore, the arguments that the dingo caused the extinction of the thylacine, the devil and the hen are in direct conflict with each other. If the dingo were really so similar to the thylacine and the Tasmanian devil in its ecological role and suppressed both, then coexisting with both for such an extended time is strange. Although this is a possible result of the dingo's introduction, critics regard the evidence for this as insubstantial.
Impact.
Reliable information about the exact ecological, cultural and economic impact of wild dogs does not yet exist. Furthermore, the impact of wild dogs depends on several factors, and a distinction between dingoes and other domestic dogs is not necessarily made.
The appearance of a wild dog is sometimes very important when it comes to the cultural and economical impact. Here, it is often desired that the wild dog's appearance complies to what is demanded, that it is a "pure" dingo or at least looks like one. In the case of their economic impact, their appearance only seems to be important when "pure" dingoes are used as a tourist attraction. Where wild dogs are regarded as pests, their appearance is only of minor importance, if at all.
The impact wild dogs have in urban areas and whether they are a danger to humans (such as direct attacks or diseases) is currently unknown.
Ecological impact.
The dingo is regarded as part of the native Australian fauna by many environmentalists and biologists, as these dogs existed on the continent before the arrival of the Europeans and a mutual adaptation of the dingoes and their surrounding ecosystems had occurred. However, the contrary view has dingoes as just another introduced predator that are only native to Thailand.
Much of the present place of wild dogs in the Australian ecosystem, especially in the urban areas, remains unknown. Although the ecological role of dingoes in Northern and Central Australia is well understood, the same does not apply to the role of wild dogs in the east of the continent. In contrast to some claims, dingoes are assumed to have a positive impact on biodiversity in areas where feral foxes are present.
Dingoes are regarded as apex predators and possibly perform an ecological key function. It is likely (with increasing evidence from scientific research) that they control the diversity of the ecosystem by limiting the number of prey and keeping the competition in check. Wild dogs hunt feral livestock such as goats and pigs, as well as native prey and introduced animals. The low number of feral goats in Northern Australia is possibly caused by the presence of the dingoes, but whether they control the goats' numbers or not is still disputable. Studies from 1995 in the northern wet forests of Australia found the dingoes there did not reduce the number of feral pigs, but their predation only has an impact on the pig population together with the presence of water buffalos (which hinder the pigs' access to food).
Observations concerning the mutual impact of dingoes and red fox and cat populations suggest dingoes limit the access of foxes and cats to certain resources. As a result, it is assumed that a disappearance of the dingoes may cause an increase of red fox and feral cat numbers and, therefore, a higher pressure on native animals. These studies found the presence of dingoes is one of the factors that keep fox numbers in an area low, and therefore reduces pressure on native animals, which then do not disappear from the area. The countrywide numbers of red foxes are especially high where dingo numbers are low, but other factors might responsible for this, depending on the area. Evidence was found for a competition between wild dogs and red foxes in the Blue Mountains of New South Wales, since there were many overlaps in the spectrum of preferred prey, but there was only evidence for local competition, not on a grand scale.
It is also possible that dingoes can live with red foxes and feral cats without reducing their numbers in areas with sufficient food resources (for example, high rabbit numbers) and hiding places. Nearly nothing is known about the relationship of wild dogs and feral cats, except both mostly live in the same areas. Although wild dogs also eat cats, it is not known whether this has an impact on the cat populations. At the moment, the Invasive Animals Cooperative Research Centre is investigating the exact effects of dingoes on the fox and cat populations to determine the benefits of keeping the dog in certain areas of Australia. In many areas, wild dogs live together with the most species of quolls , except for the eastern quoll, which is probably extinct on the mainland, so wild dogs are not regarded as a threat to them.
Additionally, the disappearance of dingoes might increase the prevalence of kangaroo, rabbit and turkey numbers. In the areas outside the Dingo Fence, the number of dingoes and emus is lower than in the areas inside. However, the numbers changed depending on the habitat. Since the environment is the same on both sides of the fence, the dingo was assumed to be a strong factor for the regulation of these species. Therefore, some people demand that dingo numbers should be allowed to increase or dingoes should be reintroduced in areas with low dingo populations to lower the pressure on endangered populations of native species and to reintroduce them in certain areas. In addition, the presence of the Australian brushturkey in Queensland increased significantly after dingo baiting was conducted.
Cultural impact.
Cultural opinions about the dingo are often based on its perceived "cunning", and the idea that it is an intermediate between civilisation and wildness.
Some of the early European settlers looked on dingoes as domestic dogs, while others thought they were more like wolves. Over the years, dingoes began to attack sheep, and their relationship to the Europeans changed very quickly: they were regarded as devious and cowardly, since they did not fight bravely in the eyes of the Europeans, and vanished into the bush. Dingoes were seen as predators that killed wantonly, rather than out of hunger (similar claims are made today concerning dingo-hybrids). Additionally, they were seen as promiscuous or as devils with a venomous bite or saliva, and so they could be killed unreservedly. Over the years, dingo trappers gained some prestige for their work, especially when they managed to kill hard to catch dingoes. Dingoes were associated with thieves, vagabonds, bushrangers and parliamentary opponents. From the 1960s, politicians began calling their opponents "dingo," meaning they were cowardly and treacherous, and it has become a popular form of attack since then. Today, the word "dingo" still stands for "coward" and "cheat," with verb and adjective forms used, as well.
The image of the dingo now ranges from the romantic to the demonic. While some Australians see the dingo as a wild dog, others see them as slightly tame wolves, and cultural biases about each of these animals affect general perceptions about dingoes. Some Australians believe the dingo which should be preserved (at least in its "pure" form), and its possible "extinction" is compared to that of the thylacine.
Traditionally, dogs have a privileged position in the Aboriginal cultures of Australia (which the dingo may have adopted from the thylacine), and the dingo is a well-known part of rock carvings and cave paintings. Ceremonies (like a keen at the Cape York Peninsula in the form of howling) and dreamtime stories are connected to the dingo, which were passed down through the generations.
Similar to how Europeans acquired dingoes, the Aboriginal people of Australia acquired dogs from the immigrants very quickly. This process was so fast that Francis Barrallier (the first European to explore the Outback) discovered in 1802 that five dogs of European origin were there before him. One theory holds that other domestic dogs will adopt the role of the "pure" dingo. In fact, the majority of the myths about dingoes simply call them "dogs" (whether that role was adopted, or whether there was no difference for the storyteller, is unknown), and other introduced animals, such as the water buffalo and the domestic cat, have been adopted into the indigenous Aboriginal culture in the forms of rituals, traditional paintings and dreamtime stories.
The dingo is connected to holy places, totems, rituals and dreamtime characters. There are stories that dogs can see the supernatural, serve as guard dogs, and warn against evil powers. There is evidence that dogs have been buried with their owners to protect them against evil even after death. Most of the published myths originate from the Western Desert and show a remarkable complexity. In some stories, dingoes are the central characters, while in others, they are only minor ones. One time, it is an ancestor from the dreamtime who created humans and dingoes or gave them their current shape. There are stories about creation, socially acceptable behaviour, and explanations why some things are the way they are. There are myths about shapeshifters (human to dingo or vice versa), "dingo-people," and the creation of certain landscapes or elements of those landscapes, like waterholes or mountains.
In other stories, the dingo is responsible for death. In some myths, advice and warnings are given to those who do not want to follow the social rules. Stories can show the borders of one's territory or the dingo in it might stand for certain members of the community; for example, rebellious dingoes stand for "wild" members of the tribe. The dingo has a wild and uncontrollable face in other stories, and there are many tales about dingoes that kill and eat humans (for example, the Mamu, which catches and devours the spirit of every child who roams too far from the campfire). Other stories tell of a giant devil dingo, from which ordinary dingoes originate.
The dog is thereby depicted as a homicidal, malicious creature that—apart from the lack of a subtle mind—is similar to a trickster, since it plays the role of a mischievous adversary for other mythological beings. Many of them fall victim to blood-thirsty dogs or escape them. Here, individual beings have a significant meaning or sometimes become part of the landscape. The actions of these dogs result, for instance, in the creation of stones and trees from flying bones and meat or ochre from the spilled blood.
Economic impact.
Wild dogs are responsible for a wide range of negative and undesired impacts on the livestock industry of Australia, and they have been regarded as pests since the start of the European livestock industry. Sheep are the most frequent prey, followed by cattle and goats. Research on the real extent of the damage, though, and the reason for this problem, only started recently. Livestock can die from many causes and, when the carcass is found, it is often difficult to determine with certainty the cause of death. Since the outcome of an attack on livestock depends to a high degree on the behaviour and experience of the predator and the prey, only direct observation is certain to determine whether an attack was by dingoes or another domestic dog. Even the existence of remnants of the prey in the scat of wild dogs do not prove they are pests, since wild dogs also eat carrion. Exact numbers or reliable estimates of the damage caused by wild dogs are, therefore, hard to obtain and are seldom reliable. Even if livestock is not a big part of the dingo's diet, the extent of damage dingoes could potentially cause to the livestock industry could be much larger because of wanton killing.
The significance of dingoes as a pest is based primarily on the predation of sheep and, to a lesser extent, on cattle, and is not connected only to the direct loss of livestock. Harassment of sheep can cause a less optimal use of grassland and miscarriages.
The cattle industry can tolerate low to moderate, and sometimes high, grades of wild dogs (therefore dingoes are not so easily regarded as pests in these areas). In the case of sheep and goats, a zero-tolerance attitude is common. The biggest threats are dogs that live inside or near the paddock areas. The extent of sheep loss is hard to determine, due to the wide pasture lands in some parts of Australia. The numbers of cattle losses is much more variable and less well-documented. Although the loss of cattle can rise up to 30%, the normal loss rate is about 0–10%.
Therefore, factors such as availability of native prey, as well as the defending behaviour and health of the cattle, play an important role in the number of losses. A study in Central Australia in 2003 confirmed that dingoes only have a low impact on cattle numbers when a sufficient supply of other prey (such as kangaroos and rabbits) is available. In some parts of Australia, it is assumed that the loss of calves can be minimised if horned cattle are used instead of polled. The precise economic impact is not known in this case, and it is unlikely that the rescue of some calves compensates for the necessary costs of control measures. Calves usually suffer less lethal wounds than sheep due to their size and the protection by the adult cattle, and therefore have a higher chance of surviving an attack. As a result, the evidence of a dog attack may only be discovered after the cattle have been herded back into the enclosure, and signs such as bitten ears, tails and other wounds are discovered.
The opinions of cattle owners regarding dingoes are more variable than the those of sheep owners. Some cattle owners believe that it is better that the weakened mother loses her calf in times of drought so that she does not have to care for her calf, too. Therefore, these owners are more hesitant to kill dingoes. The cattle industry may benefit from the predation of dingoes on rabbits, kangaroos and rats. Furthermore, the mortality rate of calves has many possible causes, and it is difficult to discriminate between them. The only reliable method to document the damage would be to document all pregnant cows, then observe their development and that of their calves. The loss of calves in observed areas where dingoes were controlled was higher than in other areas. Loss of livestock is, therefore, not necessarily caused by the occurrence of dingoes and is independent from wild dogs. One researcher has stated that for cattle stations where dingoes were controlled, kangaroos were abundant, and this affects the availability of grass.
Domestic dogs are the only terrestrial predators in Australia that are big enough to kill fully-grown sheep, and only a few sheep manage to recover from the severe injuries. In the case of lambs, death can have many causes apart from attacks by predators, which are blamed for the deaths because they eat from the carcasses. Although attacks by red foxes are possible, such attacks are more rare than previously thought. The fact that the sheep and goat industry is much more susceptible to damage caused by wild dogs than the cattle industry is mostly due to two factors: the flight behaviour of the sheep and their tendency to flock together in the face of danger, and the hunting methods of wild dogs, along with their efficient way of handling goat and sheep.
Therefore, the damage to the livestock industry does not correlate to the numbers of wild dogs in an area (except that there is no damage where no wild dogs occur). Even if there are only a few wild dogs in an area, the damage to the sheep industry can be very high, since surplus killing can occur. Sometimes, extreme losses of livestock are reported (once reportedly 2,000 sheep in one night) and are supposed to be increasing.
According to a report from the government of Queensland, wild dogs cost the state about $30 million annually due to livestock losses, the spread of diseases and control measures. Losses for the livestock industry alone were estimated to be as high as $18 million. In Barcaldine, Queensland, up to one-fifth of all sheep are killed by dingoes annually, a situation which has been described as an "epidemic". According to a survey among cattle owners in 1995, performed by the Park and Wildlife Service, owners estimated their annual losses due to wild dogs (depending on the district) to be from 1.6% to 7.1%.
Despite the variety of estimates, there is little doubt that predation by dingoes can cause enormous economic damage, especially in times of drought when natural prey is sparse and the number of dingoes is still relatively high. Furthermore, wild dogs are involved in the spread of echinococcosis among cattle and sheep. An infection with echinococcosis can lead to confiscation of 90% of the intestines, which further leads to a value decrease of the meat and high economical damage.
Among the indigenous Australians, dingoes were also used as hunting aids, living hot water bottles and camp dogs. Their scalps were used as a kind of currency, their teeth were traditionally used for decorative purposes, and their fur for traditional costumes. In some parts of Australia, premiums are paid for dingo fur and scalps. The fur of dingoes generally has only a low value, and export of this fur is forbidden in states where they are protected. There is no widespread commercial catching and killing of dingoes for the purposes of obtaining their fur.
Sometimes "pure" dingoes are important for tourism, when they are used to attract visitors. However, this seems to be common only on Fraser Island, where the dingoes are extensively used as a symbol to enhance the attraction of the island. Tourists are drawn to the experience of personally interacting with dingoes. Pictures of dingoes appear on brochures, many websites and postcards advertising the island. The use of dingo-urine as a repellent against dingoes and wallabies has been considered, but has not yet been economically implemented.
Legal status.
Until 2004, the dingo was categorised as of "least concern" on the Red List of Threatened Species. However, it has since been recategorised as "vulnerable," following the decline in numbers to around 30% of "pure" dingoes, due to crossbreeding with domestic dogs. The dingo is regarded as a regulated, but not threatened, native species under the "Environment Protection and Biodiversity Conservation Act 1999" in the Commonwealth of Nations and is, therefore, protected in the national parks of the Commonwealth, as well as in World Heritage Sites and other conservation areas. However, this law also allows that dingoes can be controlled in areas where they have a proven impact on the environment. The law forbids the export of dingoes or their body parts from Australia, except for cases where it is regulated by the law. The legal status of the dingo and other wild dogs varies across the Australian federal states and territories.
Control measures.
Dingo attacks on livestock led to widescale efforts to repel them from areas with intensive agricultural usage, and all states and territories have enacted laws for the control of dingoes. In the early 20th century, fences were erected to keep dingoes away from areas frequented by sheep, and a tendency to routinely eradicate dingoes developed among some livestock owners. Established methods for the control of dingoes in sheep areas entailed the employment of specific workers on every property. The job of these people (who were nicknamed "doggers") was to reduce the number of dingoes by using steel traps, baits, firearms and other methods. The responsibility for the control of wild dogs lay solely in the hands of the landowners. At the same time, the government was forced to control the number of dingoes. As a result, a number of measures for the control of dingoes developed over time. It was also considered that dingoes travel over long distances to reach areas with richer prey populations, and the control methods were often concentrated along "paths" or "trails" and in areas that were far away from sheep areas. All dingoes were regarded as a potential danger and were hunted.
Apart from the introduction of 1080 (extensively used for 40 years and nicknamed "doggone"), the methods and strategies for controlling wild dogs have changed little over time. Information concerning cultural importance to indigenous people and the importance of dingoes and the impact of control measures on other species is also lacking in some areas. Historically, the attitudes and needs of indigenous people were not taken into account when dingoes were controlled. Other factors that might be taken into account are the genetic status (degree of interbreeding) of dingoes in these areas, ownership and land usage, as well as a reduction of killing measures to areas outside of the zones. However, most control measures and the appropriate studies are there to minimise the loss of livestock and not to protect dingoes.
Increasing pressure from environmentalists against the random killing of dingoes, as well as the impact on other animals, demanded that more information needed to be gathered to prove the necessity of control measures and to disprove the claim of unnecessary killings.Today, permanent population control is regarded as necessary to reduce the impact of all wild dogs and to ensure the survival of the "pure" dingo in the wild.
Deterrence.
One method that does not have any proven effect is to hang dead dogs along the borders of the property in the belief that this would repel wild dogs.
Guardian animals.
To protect livestock, livestock guardian dogs (for example, Maremmas), donkeys, alpacas and llamas are used.
To keep wild dogs away from certain areas, efforts are taken to make these areas unattractive for them (for example, by getting rid of food waste) and therefore forcing them to move elsewhere. Control through deliberately spreading disease is normally not considered. Such attempts probably would not be successful, because typical dog diseases are already present in the population. Additionally, dogs under human care would also be susceptible. Other biological control methods are not regarded as achievable, since there would be a high risk of decimating dogs under human care.
Dingo Fence.
In the 1920s, the Dingo Fence was erected on the basis of the "Wild Dog Act (1921)" and, until 1931, thousands of miles of Dingo Fences had been erected in several areas of South Australia. In the year 1946, these efforts were directed to a single goal, and the Dingo Fence was finally completed. The fence connected with other fences in New South Wales and Queensland. The main responsibilities in maintaining the Dingo Fence still lies with the landowners, whose properties border on the fence and receive financial support from the government.
Reward system.
A reward system (local, as well from the government) was active from 1846 to the end of the 20th century, but there is no evidence that – despite the billions of dollars spent – it was ever an efficient control method. Therefore, its importance declined over time.
Poisoning.
Strychnine is still used in all parts of Australia.
Baits with the poison 1080 are regarded as the fastest and safest method for dog control, since they are extremely susceptible. Even small amounts of poison per dog are sufficient (0.3 mg per kg). The application of aerial baiting is regulated in the Commonwealth by the "Civil Aviation Regulations (1988)". The assumption that the tiger quoll might be damaged by the poison led to the dwindling of areas where aerial baiting could be performed. In areas where aerial baiting is no longer possible, it is necessary to put down baits.
Over the last years, cyanide-ejectors and protection collars (filled with 1080 on certain spots) have been tested.
The eradication of dingoes due to livestock damage decreased along with the importance of the sheep industry and the usage of strychnine (which beforehand had been used for 100 years) in the 1970s. The number of doggers also decreased and the frequency of government-approved aerial baiting increased. During this period, many farmers in Western Australia switched to the cattle industry, and findings in the area of biology led to a significant change in control measures and techniques in association with reduced costs and increased efficiency. At the same time, the importance of 1080 increased.
Neutering.
Owners of dingoes and other domestic dogs are sometimes asked to neuter their pets and keep them under observation to reduce the number of stray/feral dogs and prevent interbreeding with dingoes (for instance under the "Territory Parks and Wildlife Conservation Act (2000)").
Efficiency of measures.
The efficiency of control measures was questioned in the past and is often questioned today, as well as whether they stand in a good cost-benefit ratio. The premium system proved to be susceptible to deception and to be useless on a large scale, and can therefore only be used for getting rid of "problem-dogs". Animal traps are considered inhumane and inefficient on a large scale, due to the limited efficacy of baits. Based on studies, it is assumed that only young dogs that would have died anyway can be captured. Furthermore, wild dogs are capable of learning and sometimes are able to detect and avoid traps quite efficiently. In one case, a dingo bitch followed a dogger and triggered his traps one after another by carefully pushing her paw through the sand that covered the trap.
Poisonous baits can be very effective when they are of good meat quality; however, they do not last long and are occasionally taken by red foxes, quolls, ants and birds. Aerial baiting can nearly eliminate whole dingo populations. Livestock guardian dogs can effectively minimise livestock losses, but are less effective on wide open areas with widely distributed livestock. Furthermore, they can be a danger to the livestock or be killed by control measures themselves when they are not sufficiently supervised by their owners. Fences are reliable in keeping wild dogs from entering certain areas, but they are expensive to build, need permanent maintenance, and only cause the problem to be relocated.
According to studies, control measures can eliminate 66% to 84% of a wild dog population, but the population can reach its old numbers very quickly over the course of a year, depending on the season, such as by immigration of young dogs from other areas. Only a cohesive coordinated control in all areas could be efficient in the long run, if at all. Control measures mostly result in smaller packs and a disruption of pack structure. The measures seem to be rather detrimental to the livestock industry because the empty territories are taken over by young dogs and the predation then increases. Nonetheless, it is regarded as unlikely that the control measures could completely eradicate the dingo in Central Australia, and the elimination of all wild dogs is not considered a realistic option.
It has been shown that culling a small percentage of immature dingoes on Fraser Island have little significant negative impact on the overall island population, though this is being disputed.
Conservation.
Dingoes are reasonably abundant in large parts of Australia, but there is some argument that they are endangered due to interbreeding with other dogs in many parts of their range. Dingoes are not a protected species, but they are regulated under federal law and, thus, their status varies in different states and territories. Dingoes receive varying levels of protection in conservation areas such as national parks and natural reserves in New South Wales, the Northern Territory and Victoria, Arnhem Land and other Aboriginal lands, UNESCO World Heritage Sites, and the whole of the Australian Capital Territory. In some states, dingoes are regarded as declared pests and landowners are allowed to control the local populations. Throughout Australia, all other wild dogs are considered pests.
The dingoes of Fraser Island are considered to be of significant conservational value. Due to their geographic and genetic isolation, they are considered to be the most similar to the original dingoes, and they are seen as the most pure dingo population. The dingoes there are not "threatened" by interbreeding with other domestic dogs. Because of their conservational value, outrage was sparked in January 2013 when two six-month-old dingo pups were found dead, believed to have been run over near Lake McKenzie. The couple who found the dingoes were outraged at the reaction of the rangers, and Fraser Coast area manager Ross Belcher said that there will be serious penalties for those who kill or injure Fraser Island dingoes.
In February 2013, a report on Fraser Island dingo management strategies was released, with options including ending the intimidation of dingoes, tagging practice changes and regular veterinarian checkups, as well as a permanent dingo sanctuary on the island.
Groups that have devoted themselves to the conservation of the "pure" dingo by using breeding programs include the "Australian Native Dog Conservation Society" and the "Australian Dingo Conservation Association". Presently, the efforts of the dingo conservation groups are considered to be ineffective because most of their dogs are untested or are known to be hybrids.
Dingo conservation efforts focus primarily on preventing interbreeding between dingoes and other domestic dogs in order to conserve the population of pure dingoes. This is extremely difficult and costly. Conservation efforts are hampered by the fact that it is not known how many pure dingoes still exist in Australia. Steps to conserve the pure dingo can only be effective when the identification of dingoes and other domestic dogs is absolutely reliable, especially in the case of living specimens. Additionally, conservation efforts are in conflict with control measures.
Conservation of pure and survivable dingo populations is promising in remote areas, where contact with humans and other domestic dogs is rare. Under New South Wales state policy in parks, reserves and other areas not used by agriculture, these populations are only to be controlled when they pose a threat to the survival of other native species. The introduction of "dog-free" buffer zones around areas with pure dingoes is regarded as a realistic method to stop interbreeding. This is enforced in the way that all wild dogs can be killed outside of the conservation areas. However, studies from the year 2007 indicate that even an intensive control of core areas is probably not able to stop the process of interbreeding.
There is presently no information regarding what opinions the public has regarding the conservation of dingoes. There is no unity on the definition of "pure" dingoes and to what extent they should be controlled.
As a pet and working dog.
Opinion is divided about the keeping of dingoes as pets or as working dogs. Some consider the dingo unsuitable for domestication, while others see a domesticated dingo as no different from any other domesticated dog. In this vein, dingoes would have the right to be recognised as a dog breed, and domestication would be the only reliable way of ensuring the survival of the "pure" dingo. Some disagree that the dingo should be labeled a dog breed, as they believe "true" dingoes can be tamed but not truly domesticated.
Dingoes can be very tame when they come in frequent contact with humans. Furthermore, some dingoes live with humans (due to practical, as well as emotional reasons). Many indigenous Australians and early European settlers lived alongside dingoes. Indigenous Australians would take dingo pups from the den and tame them until sexual maturity and the dogs would leave. Alfred Brehm reported cases where dingoes that were completely tame and, in some cases, behaved exactly like other domestic dogs (one was used for shepherding heavy livestock), as well as specimens that remained wild and shy. He also reported about dingoes that were aggressive and completely uncontrollable, but he was of the opinion that these reports "should not get more attention than they deserve," since the behaviour depends on how the dingo was raised since early puppyhood. He believed that these dogs could become very decent pets.
According to Eberhard Trumler, dingoes are very smart and affectionate. To would-be owners, he recommended the provision of a large escape-proof enclosure and a partner of the opposite sex. During heat, dingoes are harder to manage than other domestic dogs which, combined with their attachment to their owners, can lead to problems, since they want to follow their owners and never miss the opportunity to feed. Dingoes are supposed to find every weak spot of an enclosure or residence, escape for a while and stray through towns and villages. Their intellectual ability is supposedly connected to an enormous ability to learn and a lightning perception. Dingoes have a reputation for not handling pressure, but this conflicts with their record as working dogs. They are suitable as shepherd dogs, appearing to see a purpose in it (keeping together a familiar group is in their nature) and, even today, some dingoes are employed as shepherd dogs. In addition, dingoes have strong toileting instincts and can easily be housebroken.
In 1976, the Australian Native Dog Training Society of NSW Ltd. was founded. Until this time, the ownership of dingoes was illegal. In mid-1994, the Australian National Kennel Council (ANKC) officially recognised the dingo as Australia's national dog breed, and a breed standard was published some years later. The dingo is listed in Group 4 (hound) of the ANKC. However, it is still illegal in some states to own, breed or sell dingoes, as it is in some countries.
In South Australia, dingoes can only be kept in specially licensed zoos, circuses and authorised research institutions. South Australia is a particularly sensitive region, due to extensive sheep farming conflicting with large populations of wild dingoes in the north of the state. Dingoes are bred by certain clubs and private individuals in Australia and the United States. The dingo is not regarded as a dog breed by the Fédération Cynologique Internationale. However, the American Rare Breed Association (ARBA) regards the dingo as a breed belonging to the Spitz and Primitive Group.
Goals.
In 1998 in New South Wales, the dingo was reclassified as a pet in order to save the species from extinction. Breeding programs were introduced, which were considered to be the best option available for safeguarding the continuation of the species, with the goal of returning them to the wild at a later date.
Dingoes have also been bred for sale or use as working dogs. The use of dingoes at customs was first attempted in 1976 in Victoria. However, some people speculated that these dogs were cross-breeds of dingoes and shepherd dogs.
Criticism.
The ownership of dingoes as pets and their breeding is widely criticised. The main criticism is that the activities and the resulting consequences of the dingo conservation groups, "dingo farms" and legislation for legal ownership of dingoes for people in public, is seen to be an additional threat to the survival of the pure dingoes. This fear exists because the majority of these breeding activities effectively expedite the interbreeding of dingoes and other domestic dogs, when the identification of a pure dingo is not absolutely correct respectively when hybrids are sold as "pure" dingoes.
Supporters of breeding programmes are only mildly optimistic about a successful outcome. Success in the form of a population viable for future re-wilding cannot be easily accomplished. According to David Jenkins, the breeding and reintroduction of pure dingoes is no easy option and, at the time, there were no studies that seriously dealt with this topic, especially in areas where dingo populations are already present.
An additional threat is that breeders may unconsciously select tamer dingoes by breeding individuals who are easier to manage. Therefore, it may happen that, over the years, the tame populations may become less suitable for living in the wild than their ancestors. In addition, a loss of genetic diversity (thus resulting in a higher susceptibility to diseases) might occur due to a small founding population, and negative changes could occur simply because the dogs were captive-bred. Furthermore, some features that are necessary for survival in the wild might "fade" under the conditions of domestication (for example, hunting techniques) because they are no longer needed.
Another criticism is that adult dingoes are viewed by some to be unsuitable as pets in the same ways as other domestic dogs. Dingoes are regarded as more independent-minded than other domestic dogs, making domestication reportedly difficult. As dingoes age, they succumb to their natural instincts and become more likely to escape into the wild. Furthermore, most people are unable to provide a dingo with what it needs, and dingoes may not react positively to domestication and training. Supposedly, only few dingoes and dingo-hybrids would reach an old age, since the owners would not know how to handle them. When a dingo is not socialised, it would be hard to control and develop behavioural problems from aspects of domestic life more easily tolerated by other dog breeds. To make dingoes more suitable as lapdogs, breeders would need to cross them with other domestic dogs.
Interbreeding with domestic dogs.
European domestic dogs first arrived in Australia during the European colonisation. These dogs reverted to the wild (both unintentionally and intentionally), produced feral populations and interbred with the existing dingoes. Hybrids of dingoes and domestic dogs exist today in all wild dog populations of Australia, with their numbers having increased to such a degree that any completely "pure" populations may no longer exist. The degree of interbreeding is locally so high, for instance in urban and rural areas, that there are big populations consisting purely of hybrids. Estimates from the 1990s assumed a proportion of dingo-hybrids of about 78% in the wild. It is not clear how large the current population of hybrids is today.
Dingo-like domestic dogs and dingo-hybrids can be generally distinguished from "pure" dingoes by their fur colour, since there is a wider range of colours and patterns among them than among dingoes. In addition, the more dog-typical kind of barking exists among the hybrids, and differences in the breeding cycle, certain skull characteristics, and genetic analyses can be used for differentiation. Despite all the characteristics that can be used for distinguishing between dingoes and other domestic dogs, there are two problems that should not be underestimated. First, there is no real clarity regarding at what point a dog is regarded as a "pure" dingo, and, secondly, no distinguishing feature is completely reliable—it is not known which characteristics permanently remain under the conditions of natural selection.
In science, there are two main opinions regarding this process of interbreeding. The first, and likely most common, position states that the "pure" dingo should be preserved via strong controls of the wild dog populations, and only "pure" or nearly "pure" dingoes should be protected. The second position is relatively new and is of the opinion that people must accept that the dingo has changed and that it is impossible to bring the "pure" dingo back. Conservation of these dogs should therefore be based on where and how they live, as well as their cultural and ecological role, instead of concentrating on precise definitions or concerns about "genetic purity". Both positions are controversially discussed.
There is a wider range of fur colours, skull shapes and body size in the modern-day wild dog population than in the time before the arrival of the Europeans. Over the course of the last 40 years, there has been an increase of about 20% in the average wild dog body size. It is currently unknown whether, in the case of the disappearance of "pure" dingoes, remaining hybrids would alter the predation pressure on other animals. It is also unclear what kind of role these hybrids would play in the Australian ecosystems. However, it likely that the dynamics of the various ecosystems will not be disturbed by this process.
Attacks on humans.
Although dingoes are large enough to be dangerous, they generally avoid conflict with humans. Apart from the well-known case in which an infant was taken from a campsite (see below), there have been numerous confirmed dingo attacks, often involving people feeding wild dingoes, particularly on Fraser Island, a special center of dingo-related tourism (see main article). Most dingo attacks are minor in nature, but some can be major, and a few can be fatal. Many Australian national parks have signs advising visitors not to feed wildlife, partly because this practice is not healthy for the animals, and partly because it may encourage undesirable behaviour, such as snatching or biting by dingoes, kangaroos, goannas and some birds.
Azaria Chamberlain dingo attack.
On 17 August 1980, a nine-week-old girl named Azaria Chamberlain was taken by a dingo near Uluru (then known as Ayers Rock) and killed. Her mother, Lindy Chamberlain, was suspected and wrongly convicted of murder, and her father, Michael Chamberlain, with being an accessory after the fact, as the court did not believe that an animal generally shy of humans would be capable of such an act. After serving more than three years of her sentence, Lindy was released from prison when the jacket of the baby was found in a dingo den. The parents were thereafter found innocent, but the cause of death was not officially listed as a dingo attack until 12 June 2012.
Fraser Island attacks.
Dingo conservation groups on Fraser Island have become frustrated with the killing of dingoes that attack humans. It has been proposed that problem dingoes be relocated to a wildlife sanctuary. Queensland Environment Minister Andrew Powell said that the Fraser Island government should work to better educate people about dingoes to help stop attacks.
Conclusions.
Articles published about dingo attacks blame them on habituation, especially through the intentional and unintentional feeding of dingoes. The more frequently these animals are fed or allowed to scavenge on waste food, the more likely they are to react aggressively towards humans when they no longer receive or find food. It is further thought that dingoes might have started to regard the food sources found (garbage cans, leftovers and handouts) as part of their territory. Attacks then occur with humans seen as competition, and dingoes simply reacting to protect their food supply.
Even when habituation to humans seems to be the general cause for attacks, it is not absolutely clear, and therefore the overall threat towards people is not known for sure. Some attacks might result from the "play" of young pups, especially with children. Attacks can also be caused by mistaken reactions of humans to aggressive and dominant behaviour of dingoes. That some dingoes might regard humans as prey is a possibility, as children or incapacitated adults could be theoretically overpowered. Dr. Bradley Smith said that Fraser Island has a problem with humans and not with the dingoes, that dogs who were labelled "aggressive" were simply behaving naturally.
The behaviour of humans might undermine efforts to guard against dingo attacks. Therefore, the change in human behaviour is at the centre of attention. Warning signs like "Beware of Dingoes" seem to have lost their effect on Fraser Island, despite the high number of such signs. Furthermore, some humans do not realise how adaptive and quick dingoes are. Therefore, humans do not remain attentive enough. They do not consider, for instance, that dingoes steal food like fruits and vegetables. In addition, some tourists seemed to be confused by the high number of rules in some parks, and they have been prompted in some cases to actively feed the wild animals.
Problems in classification.
There is no general agreement (scientific or otherwise) regarding what the dingo is, in a biological sense, since it has been called "wolf," "dingo," "dog," and "wild dog". Even within the scientific community, the dingo is given several names. There is no consensus regarding whether the dingo is a feral or native animal, or what kinds of dogs should be classed as "dingoes". Thus, one writer considered the New Guinea singing dog, the Basenji, the Carolina Dog and certain other dog populations to be dingoes. Evidence indicates a discord concerning the status of these dogs, as well.
Dingoes have been variously considered to be wild dogs, the progenitor of domestic dogs, the ancestor of modern dog breeds, a separate species, a link between wolf and domestic dog, a primitive canine species or primitive domestic dog, a "dog-like" relative of wolves or a subspecies of the domestic dog. Others consider them to be native dogs of Asia, a relatively unchanged form of early domestic dog, part wolf and part dog, or to have been selectively bred from wolves. According to present scientific consensus and knowledge, dingoes are domestic dogs that arrived at their present distribution with humans, adapted to the respective conditions and are no more "primitive" or "primordial" than other domestic dogs.
The Australian dingo has never been subject to the artificial selection that produced modern dog breeds, and it may be an undomesticated descendant of an extinct Asian wolf. However, compared to the European grey wolf, dingoes have an approximately 30% lower relative brain size, reduced facial expressions, reduced impressive behaviour, curled tails that can be carried over the back, and generally a permanent fertility in males—features that all known domestic dogs share and are considered to be caused by domestication. It might happen that one and the same source names the dingo as a subspecies of the grey wolf, but lists all other domestic dogs as separate species. Likewise, the scientific name of the dingo might be "Canis lupus dingo", but the dingo is regarded as a separate species, nonetheless.
Alfred Brehm originally considered the dingo to be a separate species but, after examining several different specimens, he came to the conclusion that they could only be domestic dogs. In contrast, William Jardine considered the dingo to be an entirely separate species, while contemporary French naturalists regarded them as feral dogs. Even among modern-day scientists, dingoes and other domestic dogs are sometimes considered two separate species, despite small genetic, morphological and behavioural differences.
The phenomenon of interbreeding between both is then attributed to the statement that all wolf-like species can interbreed and produce fertile offspring. However, breeding experiments in Germany could only prove an unrestricted fertility in the offspring of domestic dogs and grey wolves. Hybrids between domestic dogs and coyotes, and domestic dogs and golden jackals, had communication problems among each other, as well to the parent species. From the third hybrid generation on, a decrease in fertility and an increase in genetic damage was observed among the coyote-hybrids and jackal-hybrids. Observations of this kind have never been made for hybrids of dingoes and other domestic dogs, only that dingoes and other domestic dogs can freely interbreed with each other.
The choice of classification can have a direct impact on the dingo. Dingoes officially cease to exist outside of national parks and become unprotected wild dogs. The term "wild dog," itself, sometimes only includes dingoes and their hybrids or respectively excludes dingoes. Another view is that dingoes are "only" feral outside of national parks, with this term having a more negative meaning than the term "wild".
On the other hand, dingoes have been "rehabilitated" in some way, by changing their status from pests to "Australia's native dog" or, more subtly, from a subspecies of the domestic dog to that of the grey wolf. The undertone in the Australian press seemed to be that being a grey wolf or an Asian wolf means that the dingo is more "wild" and, therefore, more desirable than a companion animal (domestic dog). It is possible that the habit of calling the dingo only "dog" (not "wild dog") in colloquial language indicates a form of familiarity or debasing. In the last case, it might be morally easier to kill a dog when it causes problems because it would not have the "high status" of a wolf or dingo. Sometimes, it is considered bad that dingoes are domestic dogs, that they are descended from them and not "directly" from the grey wolf. In short, if the dingo is regarded as native, then it is worthy of protection. But if it is considered to be "just" a variant of the domestic dog, it is regarded as a pest and should be eradicated.

</doc>
<doc id="62895" url="https://en.wikipedia.org/wiki?curid=62895" title="Australian Kelpie">
Australian Kelpie

The Australian Kelpie, or simply Kelpie, is an Australian sheep dog successful at mustering and droving with little or no guidance. It is a medium-sized dog and comes in a variety of colours. The Kelpie has been exported throughout the world and is used to muster livestock, primarily sheep, cattle and goats.
The breed has been separated into two distinct varieties: the Show (or Bench) Kelpie and the Working Kelpie. The Show Kelpie is seen at conformation dog shows in some countries and is selected for appearance rather than working instinct, while the Working Kelpie is bred for working ability rather than appearance.
Appearance.
The Kelpie is a soft-coated, medium-sized dog, generally with prick ears and an athletic appearance. Coat colours include black, black and tan, red, red and tan, blue, blue and tan, fawn, fawn and tan, cream, black and blue, and white and gold. The Kelpie generally weighs and measures at the withers.
Breed standards.
Robert Kaleski published the first standard for the Kelpie in 1904. The standard was accepted by leading breeders of the time and adopted by the Kennel Club of New South Wales. Contemporary breed standards vary depending on whether the registry is for working or show Kelpies. It is possible for a dog to both work and show, but options for competition in conformation shows might be limited depending on ancestry and the opinions of the kennel clubs or breed clubs involved.
In Australia, there are two separate registries for Kelpies."Working Kelpies" are registered with the Working Kelpie Council (WKC) and/or the Australian Sheepdog Workers Association. The WKC encourages breeding for working ability, and allows a wide variety of coat colours. "Show Kelpies" are registered with the Australian National Kennel Council, which encourages breeding for a certain appearance and limits acceptable colours. The wide standards allowed by the WKC mean that Working Kelpies do not meet the standard for showing.
Outside Australia.
In the US, the Kelpie is not recognised as a breed by the American Kennel Club (AKC). However, the United Kennel Club and the Canadian Kennel Club recognise the Kelpie and allow them to compete in official events. As of 2015, Australian Kelpies have been accepted by the AKC as Herding Dogs allowed to compete in AKC sanctioned Sheep Herding Trials. 
Working Kelpie.
The Working Kelpie comes in three coat types: smooth, short, and rough. The coat can be almost every colour from black through light tan or cream. Some Kelpies have a white blaze on the chest, and a few have white points. Kelpies sometimes have a double coat, which sheds out in spring in temperate climates. Agouti is not unusual, and can look like a double coat.
Working Kelpies vary in size, ranging from about 19 inches (48cm) to as much as 25 inches (63.5cm) and from 28-60 lbs (12.7-27 kg). The dog's working ability is unrelated to appearance, so stockmen looking for capable working dogs disregard the dog's appearance.
A Working Kelpie can be a cheap and efficient worker that can save farmers and graziers the cost of several hands when mustering livestock. The good working Kelpies are herding dogs that will prevent stock from moving away from the stockman. This natural instinct is crucial when mustering stock in isolated gorge country, where a good dog will silently move ahead of the stockman and block up the stock (usually cattle) until the rider appears. The preferred dogs for cattle work are Kelpies, often of a special line, or a Kelpie cross. They will drive a mob of livestock long distances in extremes of climates and conditions. Kelpies have natural instincts for managing livestock. They will work sheep, cattle, goats, pigs, poultry, and other domestic livestock. The Kelpie's signature move is to jump on the backs of sheep and walk across the tops of the sheep to reach the other side and break up the jam. A good working Kelpie is a versatile dog—they can work all day on the farm, ranch, or station, and trial on the weekends. Kelpies compete and are exhibited in livestock working trials, ranging from yards or arenas to large open fields working sheep, goats, cattle, or ducks.
Show Kelpies.
"Show Kelpies" are restricted to solid colours (black, chocolate, red, smoky blue, fawn, black and tan, red and tan) in a short double coat with pricked ears. It was during the early 20th century that Kelpies were first exhibited, at the Sydney Royal Easter Show. Different kennel clubs' breed standards have preferences for certain colours. Show Kelpies are generally heavier and shorter than working Kelpies.
Temperament.
Show Kelpies generally excel in agility trials and may be shown in conformation in Australia.
Health.
Kelpies are a hardy breed with few health problems, but they are susceptible to disorders common to all breeds, like cryptorchidism, hip dysplasia, cerebellar abiotrophy and luxating patella. Current research is underway to find the genetic marker for cerebellar abiotrophy in the breed.
History.
The ancestors of the Kelpie were simply (black) dogs, called Colleys or Collies. The word "collie" has the same root as "coal" and "collier (ship)". Some of these collies were imported to Australia for stock work in the early 19th century, and were bred to other types of dogs (possibly including the occasional Dingo), but always with an eye to working sheep without direct supervision. Today's Collie breeds were not formed until about ten or 15 years after the Kelpie was established as a breed, with the first official Border Collie not brought to Australia until after Federation in 1901.
Kelpies are partly descended from Dingos, with 3-4% of their genes coming from the native Australian Dog. At the time of the origin of the breed, it was illegal to keep dingoes as pets, some dingo owners registered their animals as Kelpies or Kelpie crosses. Kelpies and dingoes are similar in conformation and colouring. There is no doubt that some people have deliberately mated dingoes to their Kelpies, and some opinion holds that the best dilution is 1/16–1/32, but that 1/2 and 1/4 will work. As the Dingo has been regarded as a savage sheep-killer since the first European settlement of Australia, few will admit to the practice.
The first "Kelpie" was a black and tan female pup with floppy ears bought by Jack Gleeson about 1872 from a litter born on Warrock Station near Casterton, owned by George Robertson, a Scot. This dog was named kelpie after the mythological shape shifting water spirit of Celtic folklore. Legend has it that "Kelpie" was sired by a dingo, but there is little evidence for or against this. In later years she was referred to as "(Gleeson's) Kelpie", to differentiate her from "(King's) Kelpie", her daughter.
The second "Kelpie" was "(King's) Kelpie", another black and tan bitch out of "Kelpie" by "Caesar", a pup from two sheep-dogs imported from Scotland. Again, there are legends that these two sheep-dogs may never have seen Scotland, and may have had dingo blood. "(King's) Kelpie" tied the prestigious Forbes Trial in 1879, and the strain was soon popularly referred to as "Kelpie's pups", or just Kelpies. The King brothers joined another breeder, McLeod, to form a dog breeding partnership whose dogs dominated trials during 1900 to 1920.
An early Kelpie, Sally was mated to Moss a smooth haired Collie and she produced a black pup that was named Barb after the black horse, The Barb who won the Melbourne Cup in 1866. This then was how black Kelpies became known as Barb Kelpies.
There were a number of Kelpies called 'Red Cloud'. The first and most famous was John Quinn's Red Cloud in the early 20th century, and then in the 1960s another "Red Cloud" that became very well known in Western Australia. This started the tradition in Western Australia of calling all red or red and tan Kelpies, especially those with white chests, Red Cloud Kelpies.
Kelpies have now been exported to many countries including Argentina, Canada, Italy, Korea, New Caledonia, New Zealand, Sweden, the United Kingdom and the United States for various pursuits.
Recently Kelpies have been trained as scent dogs with good success rates. In Sweden they are widely used for tracking and rescue work.
The Australian legend Red Dog died November 21, 1979. A movie based on this story was made in 2011.

</doc>
<doc id="62896" url="https://en.wikipedia.org/wiki?curid=62896" title="George McGovern">
George McGovern

George Stanley McGovern (July 19, 1922 – October 21, 2012) was an American historian, author, U.S. Representative, U.S. Senator, and the Democratic Party presidential nominee in the 1972 presidential election.
McGovern grew up in Mitchell, South Dakota, where he was a renowned debater. He volunteered for the U.S. Army Air Forces upon the country's entry into World War II and as a B-24 Liberator pilot flew 35 missions over German-occupied Europe. Among the medals bestowed upon him was a Distinguished Flying Cross for making a hazardous emergency landing of his damaged plane and saving his crew. After the war he gained degrees from Dakota Wesleyan University and Northwestern University, culminating in a PhD, and was a history professor. He was elected to the U.S. House of Representatives in 1956 and re-elected in 1958. After a failed bid for the U.S. Senate in 1960, he was a successful candidate in 1962.
As a senator, McGovern was an exemplar of modern American liberalism. He became most known for his outspoken opposition to the growing U.S. involvement in the Vietnam War. He staged a brief nomination run in the 1968 presidential election as a stand-in for the assassinated Robert F. Kennedy. The subsequent McGovern–Fraser Commission fundamentally altered the presidential nominating process, by greatly increasing the number of caucuses and primaries and reducing the influence of party insiders. The McGovern–Hatfield Amendment sought to end the Vietnam War by legislative means but was defeated in 1970 and 1971. McGovern's long-shot, grassroots-based 1972 presidential campaign found triumph in gaining the Democratic nomination but left the party badly split ideologically, and the failed vice-presidential pick of Thomas Eagleton undermined McGovern's credibility. In the general election McGovern lost to incumbent Richard Nixon in one of the biggest landslides in American electoral history. Re-elected Senator in 1968 and 1974, McGovern was defeated in a bid for a fourth term in 1980.
Throughout his career, McGovern was involved in issues related to agriculture, food, nutrition, and hunger. As the first director of the Food for Peace program in 1961, McGovern oversaw the distribution of U.S. surpluses to the needy abroad and was instrumental in the creation of the United Nations-run World Food Programme. As sole chair of the Senate Select Committee on Nutrition and Human Needs from 1968 to 1977, McGovern publicized the problem of hunger within the United States and issued the "McGovern Report", which led to a new set of nutritional guidelines for Americans. McGovern later served as U.S. Ambassador to the United Nations Agencies for Food and Agriculture from 1998 to 2001 and was appointed the first UN Global Ambassador on World Hunger by the World Food Programme in 2001. The McGovern-Dole International Food for Education and Child Nutrition Program has provided school meals for millions of children in dozens of countries since 2000 and resulted in McGovern's being named World Food Prize co‑laureate in 2008.
Early years and early education.
McGovern was born in the 600‑person farming community of Avon, South Dakota. His father, Rev. Joseph C. McGovern, born in 1868, was pastor of the local Wesleyan Methodist Church there. Joseph – the son of an alcoholic who had immigrated from Ireland – had grown up in several states, working in coal mines from the age of nine and parentless from the age of thirteen. He had been a professional baseball player in the minor leagues, but had given it up due to his teammates' heavy drinking, gambling and womanizing, and entered the seminary instead. George's mother was the former Frances McLean, born c. 1890 and initially raised in Ontario; her family had later moved to Calgary, Alberta, and then she came to South Dakota looking for work as a secretary. George was the second oldest of four children. Joseph McGovern's salary never reached $100 per month, and he often received compensation in the form of potatoes, cabbages, or other food items. Joseph and Frances McGovern were both firm Republicans, but were not politically active or doctrinaire.
When George was about three years old, the family moved to Calgary for a while to be near Frances' ailing mother, and he formed memories of events such as the Calgary Stampede. When George was six, the family returned to the United States and moved to Mitchell, South Dakota, a community of 12,000. McGovern attended public schools there and was an average student. He was painfully shy as a child and was afraid to speak in class during first grade. His only reproachable behavior was going to see movies, which were among the worldly amusements forbidden to good Wesleyan Methodists. Otherwise he had a normal childhood marked by visits to the renowned Mitchell Corn Palace and what he later termed "a sense of belonging to a particular place and knowing your part in it". He would, however, long remember the Dust Bowl storms and grasshopper plagues that swept the prairie states during the Great Depression. The McGovern family lived on the edge of the poverty line for much of the 1920s and 1930s. Growing up amid that lack of affluence gave young George a lifelong sympathy for underpaid workers and struggling farmers. He was influenced by the currents of populism and agrarian unrest and by the "practical divinity" teachings of cleric John Wesley that sought to fight poverty, injustice, and ignorance.
McGovern attended Mitchell High School, where he was a solid but unspectacular member of the track team. A turning point came when his tenth-grade English teacher recommended him to the debate team, where he became quite active. His high-school debate coach, a history teacher who capitalized on McGovern's interest in that subject, proved to be a great influence in his life, and McGovern spent many hours honing his meticulous, if colorless, forensic style. McGovern and his debating partner won events in his area and gained renown in a state where debating was passionately followed by the general public. Debate changed McGovern's life, giving him a chance to explore ideas to their logical end, broadening his perspective, and instilling a sense of personal and social confidence. He graduated in 1940 in the top ten percent of his class.
McGovern enrolled at small Dakota Wesleyan University in Mitchell and became a star student there. He supplemented a forensic scholarship by working a variety of odd jobs. With World War II underway overseas and feeling insecure about his own courage, McGovern took flying lessons in an Aeronca aircraft and received a pilot's license through the government's Civilian Pilot Training Program. McGovern recalled: "Frankly, I was scared to death on that first solo flight. But when I walked away from it, I had an enormous feeling of satisfaction that I had taken the thing off the ground and landed it without tearing the wings off." In late 1940 or early 1941, McGovern had a brief affair with an acquaintance that resulted in her giving birth to a daughter during 1941, although this did not become public knowledge during his lifetime. In April 1941, McGovern began dating fellow student Eleanor Stegeberg, who had grown up in Woonsocket, South Dakota. They had first encountered each other during a high school debate in which Eleanor and her twin sister Ila defeated McGovern and his partner.
McGovern was listening to a radio broadcast of the New York Philharmonic Orchestra for a sophomore-year music appreciation class when he heard the news of the December 7, 1941, attack on Pearl Harbor. In January 1942 he drove with nine other students to Omaha, Nebraska, and volunteered to join the United States Army Air Forces. The military accepted him, but they did not yet have enough airfields, aircraft, or instructors to start training all the volunteers, so McGovern stayed at Dakota Wesleyan. George and Eleanor became engaged, but initially decided not to marry until the war was over. During his sophomore year, McGovern won the statewide intercollegiate South Dakota Peace Oratory Contest with a speech called "My Brother's Keeper", which was later selected by the National Council of Churches as one of the nation's twelve best orations of 1942. Smart, handsome, and well-liked, McGovern was elected president of his sophomore class and voted "Glamour Boy" during his junior year. In February 1943, during his junior year, he and a partner won a regional debate tournament at North Dakota State University that featured competitors from thirty-two schools across a dozen states; upon his return to campus, he discovered that the Army had finally called him up.
Military service.
Soon thereafter McGovern was sworn in as a private at Fort Snelling in Minnesota. He spent a month at Jefferson Barracks Military Post in Missouri and then five months at Southern Illinois Normal University in Carbondale, Illinois, for ground school training; McGovern later maintained that both the academic work and physical training were the toughest he ever experienced. He spent two months at a base in San Antonio, Texas, and then went to Hatbox Field in Muskogee, Oklahoma, for basic flying school, training in a single-engined PT‑19. McGovern married Eleanor Stegeberg on October 31, 1943 during a three-day leave (lonely and in love, the couple had decided to not wait any longer); his father presided over the ceremony at the small Methodist church in Woonsocket. After three months in Muskogee, McGovern went to Coffeyville Army Airfield in Kansas for a further three months of training on the BT‑13. Around April 1944, McGovern went on to advanced flying school at Pampa Army Airfield in Texas for twin-engine training on the AT‑17 and AT‑9. Throughout, Air Cadet McGovern showed skill as a pilot, with his exceptionally good depth perception aiding him. Eleanor McGovern followed him to these different duty stations, and was present when he received his wings and was commissioned a Second Lieutenant.
McGovern was assigned to Liberal Army Airfield in Kansas and its transition school to learn to fly the B‑24 Liberator, an assignment he was pleased with. McGovern recalled later: "Learning how to fly the B‑24 was the toughest part of the training. It was a difficult airplane to fly, physically, because in the early part of the war, they didn't have hydraulic controls. If you can imagine driving a Mack truck without any power steering or power brakes, that's about what it was like at the controls. It was the biggest bomber we had at the time." Eleanor was constantly afraid of her husband's suffering an accident while training, which claimed a huge toll of airmen during the entire war. This schooling was followed by a stint at Lincoln Army Airfield in Nebraska, where McGovern met his B-24 crew. Traveling around the country and mixing with people from different backgrounds proved to be a broadening experience for McGovern and others of his generation. The USAAF sped up training times for McGovern and others due to the heavy losses that bombing missions were suffering over Europe. Despite, and partly because of, the risk that McGovern might not come back from combat, the McGoverns decided to have a child, and Eleanor became pregnant. In June 1944, McGovern's crew received final training at Mountain Home Army Air Field in Idaho. They then shipped out via Camp Patrick Henry in Virginia, where McGovern found history books with which to fill downtime, especially during the trip overseas on a slow troopship.
In September 1944, McGovern joined the 741st Squadron of the 455th Bombardment Group of the Fifteenth Air Force, stationed at San Giovanni Airfield near Cerignola in the Apulia region of Italy. There he and his crew found a starving, disease-ridden local population wracked by the ill fortunes of war and far worse off than anything they had seen back home during the Depression. Those sights would form part of his later motivation to fight hunger. Starting on November 11, 1944, McGovern flew 35 missions over enemy territory from San Giovanni, the first five as co-pilot for an experienced crew and the rest as pilot for his own plane, known as the "Dakota Queen" after his wife Eleanor. His targets were in Austria; Czechoslovakia; Germany; Hungary; Poland; and northern, German-controlled Italy, and were often either oil refinery complexes or rail marshalling yards, all as part of the U.S. strategic bombing campaign in Europe. The eight- or nine-hour missions were grueling tests of endurance for pilots and crew, and while German fighter aircraft were a diminished threat by this time as compared to earlier in the war, his missions often faced heavy anti-aircraft artillery fire that filled the sky with flak bursts.
On McGovern's December 15 mission over Linz, his second as pilot, a piece of shrapnel from flak came through the windshield and missed fatally wounding him by only a few inches. The following day on a mission to Brüx, he nearly collided with another bomber during close-formation flying in complete cloud cover. The following day, he was recommended for a medal after surviving a blown wheel on the always-dangerous B-24 take-off, completing a mission over Germany, and then landing without further damage to the plane. On a December 20 mission against the Škoda Works at Pilsen, Czechoslovakia, McGovern's plane had one engine out and another in flames after being hit by flak. Unable to return to Italy, McGovern flew to a British airfield on Vis, a small island in the Adriatic Sea off the Yugoslav coast that was controlled by Josip Broz Tito's Partisans. The short field, normally used by small fighter planes, was so unforgiving to four-engined aircraft that many of the bomber crews who tried to make emergency landings there perished. But McGovern successfully landed, saving his crew, a feat for which he was awarded the Distinguished Flying Cross.
In January 1945, McGovern used R&R time to see every sight that he could in Rome, and to participate in an audience with the Pope. Bad weather prevented many missions from being carried out during the winter, and during such downtime McGovern spent much time reading and discussing how the war had come about. He resolved that if he survived it, he would become a history professor. In February, McGovern was promoted to First Lieutenant. On March 14, McGovern had an incident over Austria in which he accidentally bombed a family farmhouse when a jammed bomb improvidentally released above the structure and destroyed it, an event which haunted McGovern. (Four decades later, after a McGovern public appearance in that country, the owner of the farm approached the media to let the Senator know that he was the victim of that incident but that no one had been hurt and the farmer felt that it had been worth the price if that event helped achieve the defeat of Nazi Germany in some small way. McGovern was greatly relieved.) On returning to base from the flight, McGovern was told his first child Ann had been born four days earlier. April 25 saw McGovern's 35th mission, which marked fulfillment of the Fifteenth Air Force's requirement for a combat tour, against heavily defended Linz. The sky turned black and red with flak – McGovern later said "Hell can't be any worse than that" – and the "Dakota Queen" was hit multiple times, resulting in 110 holes in its fuselage and wings and an inoperative hydraulic system. McGovern's waist gunner was injured, and his flight engineer was so unnerved by his experience that he would subsequently be hospitalized with battle fatigue, but McGovern managed to bring back the plane safely with the assistance of an improvised landing technique.
In May and June 1945, following the end of the European war, McGovern flew food relief flights to northern Italy, then flew back to the United States with his crew. McGovern was discharged from the Army Air Forces in July 1945, with the rank of First Lieutenant. He was also awarded the Air Medal with three oak leaf clusters, one instance of which was for the safe landing on his final mission.
Later education and early career.
Upon coming home, McGovern returned to Dakota Wesleyan University, aided by the G.I. Bill, and graduated from there in June 1946 with a B.A. degree "magna cum laude". For a while he suffered from nightmares about flying through flak barrages or his plane being on fire. He continued with debate, again winning the state Peace Oratory Contest with a speech entitled "From Cave to Cave" that presented a Christian-influenced Wilsonian outlook. The couple's second daughter, Susan, was born in March 1946.
McGovern switched from Wesleyan Methodism to less fundamentalist regular Methodism. Influenced by Walter Rauschenbusch and the Social Gospel movement, McGovern began divinity studies at Garrett Theological Seminary in Evanston, Illinois, near Chicago. He preached as a Methodist student supply minister at Diamond Lake Church in Mundelein, Illinois, during 1946 and 1947, but became dissatisfied by the minutiae of his pastoral duties. In late 1947, McGovern left the ministry and enrolled in graduate studies at Northwestern University in Evanston, where he also worked as a teaching assistant. The relatively small history program there was among the best in the country and McGovern took courses given by noted academics Ray Allen Billington, Richard W. Leopold, and L. S. Stavrianos. He received an M.A. in history in 1949.
McGovern then returned to his alma mater, Dakota Wesleyan, and became a professor of history and political science. With the assistance of a Hearst fellowship for 1949–1950, he continued pursuing graduate studies during summers and other free time. The couple's third daughter, Teresa, was born in June 1949. Eleanor McGovern began to suffer from bouts of depression, but continued to assume the large share of household and child-rearing duties. McGovern earned a PhD in history from Northwestern University in 1953. His 450-page dissertation, "The Colorado Coal Strike, 1913–1914", was a sympathetic account of the miners' revolt against Rockefeller interests in the Colorado Coalfield War. His thesis advisor, noted historian Arthur S. Link, later said he had not seen a better student than McGovern in 26 years of teaching. McGovern was influenced not only by Link and the "Consensus School" of American historians but also by the previous generation of "progressive" historians. Most of his future analyses of world events would be informed by his training as a historian, as well as his personal experiences during the Great Depression and World War II. Meanwhile, McGovern had become a popular if politically outspoken teacher at Dakota Wesleyan, with students dedicating the college yearbook to him in 1952.
Nominally a Republican growing up, McGovern began to admire Democratic President Franklin Delano Roosevelt during World War II, even though he supported Roosevelt's opponent Thomas Dewey in the 1944 presidential election. At Northwestern, his exposure to the work of China scholars John King Fairbank and Owen Lattimore had convinced him that unrest in Southeast Asia was homegrown and that U.S. foreign policy towards Asia was counterproductive. Discouraged by the onset of the Cold War, and never thinking well of incumbent President Harry S. Truman, in the 1948 presidential election McGovern was attracted to the campaign of former Vice President and Secretary of Agriculture Henry A. Wallace. He wrote columns supporting Wallace in the "Mitchell Daily Republic" and attended the Wallace Progressive Party's first national convention as a delegate. There he became disturbed by aspects of the convention atmosphere, decades later referring to "a certain rigidity and fanaticism on the part of a few of the strategists." But he remained a public supporter of Wallace afterward, although, because Wallace was kept off the ballot in Illinois where McGovern was now registered, McGovern did not vote in the general election.
By 1952, McGovern was coming to think of himself as a Democrat. He was captivated by a radio broadcast of Governor Adlai Stevenson's speech accepting the presidential nomination at the 1952 Democratic National Convention . He immediately dedicated himself to Stevenson's campaign, publishing seven articles in the "Mitchell Daily Republic" newspaper outlining the historical issues that separated the Democratic Party from the Republicans. The McGoverns named their only son Steven, born immediately after the convention, after his new hero. Although Stevenson lost the election, McGovern remained active in politics, believing that "the engine of progress in our time in America is the Democratic Party". In early 1953, McGovern left a tenure-track position at the university to become executive secretary of the South Dakota Democratic Party, the state chair having recruited him after reading his articles. Democrats in the state were at a low, holding no statewide offices and only 2 of the 110 seats in the state legislature. Friends and political figures had counseled McGovern against making the move, but despite his mild, unassuming manner, McGovern had an ambitious nature and was intent upon starting a political career of his own.
McGovern spent the following years rebuilding and revitalizing the party, building up a large list of voter contacts via frequent travel around the state. Democrats showed improvement in the 1954 elections, winning 25 seats in the state legislature. From 1954 to 1956 he also was on a political organization advisory group for the Democratic National Committee. The McGoverns' fifth and final child, Mary, was born in 1955.
U.S. House of Representatives.
In 1956, McGovern sought elective office himself, and ran for the House of Representatives from South Dakota's 1st congressional district, which consisted of the counties east of the Missouri River. He faced four-term incumbent Republican Party Representative Harold O. Lovre. Aided by the voter lists he had earlier accumulated, McGovern ran a low-budget campaign, spending $12,000 while borrowing $5,000. His quiet personality appealed to voters he met, while Lovre suffered from a general unhappiness over Eisenhower administration farm policy. When polls showed McGovern gaining, Lovre's campaign implied that McGovern's support for admitting the People's Republic of China to the United Nations and his past support for Henry Wallace meant that McGovern was a Communist appeaser or sympathizer. In his closing speech, McGovern responded: "I have always despised communism and every other ruthless tyranny over the mind and spirit of man." McGovern staged an upset victory, gaining 116,516 votes to his opponent's 105,835, and became the first Democrat elected to Congress from South Dakota in 22 years. The McGoverns established a home in Chevy Chase, Maryland.
Entering the 85th United States Congress, McGovern became a member of the House Committee on Education and Labor. As a representative, McGovern was attentive to his district. He became a staunch supporter of higher commodity prices, farm price supports, grain storage programs, and beef import controls, believing that such stored commodities programs guarded against drought and similar emergencies. He favored rural development, federal aid to small business and to education, and medical coverage for the aged under Social Security. In 1957, he traveled and studied conditions in the Middle East under a fellowship from the American Christian Palestine Committee. McGovern first allied with the Kennedy family by supporting a House version of Senator John F. Kennedy's eventually unsuccessful labor reform bill.
In his 1958 reelection campaign, McGovern faced a strong challenge from South Dakota's two-term Republican Governor and World War II Medal of Honor recipient Joe Foss, who was initially considered the favorite to win. But McGovern ran an effective campaign that showcased his political strengths of having firm beliefs and the ability to articulate them in debates and on the stump. He prevailed with a slightly larger margin than two years before.
In the 86th United States Congress, McGovern was assigned to the House Committee on Agriculture. The longtime chair of the committee, Harold D. Cooley, would subsequently say, "I cannot recall a single member of Congress who has fought more vigorously or intelligently for American farmers than Congressman McGovern." He helped pass a new food-stamp law. He was one of nine representatives from Congress to the NATO Parliamentary Assembly conferences of 1958 and 1959. Along with Senator Hubert H. Humphrey, McGovern strongly advocated a reconstruction of Public Law 480 (an agricultural surplus act which had come into being under Eisenhower) with a greater emphasis on feeding the hungry around the world, the establishment of an executive office to run operations, and the goal of promoting peace and stability around the world. During his time in the House, McGovern was regarded as a liberal overall, and voted in accordance with the rated positions of Americans for Democratic Action (ADA) 34 times and against 3 times. Two of the themes of his House career, improvements for rural America and the war on hunger, would be defining ones of his legislative career and public life.
In 1960, McGovern decided to run for the U.S. Senate and challenge the Republican incumbent Karl Mundt, a formidable figure in South Dakota politics whom McGovern loathed as an old-style McCarthyite. The race centered mostly around rural issues, but John F. Kennedy's Catholicism was a drawback at the top of the ticket in the mostly Protestant state. McGovern made careless charges during the campaign, and the press turned against him; he would say eleven years later, "It was my worst campaign. I hated so much I lost my sense of balance." McGovern was defeated in the November 1960 election, gaining 145,217 votes to Mundt's 160,579, but the margin was one-third of Kennedy's loss to Vice President Richard M. Nixon in the state's presidential contest.
Food for Peace director.
Having relinquished his House seat to run for the Senate, McGovern was available for a position in the new Kennedy administration. McGovern was picked to become a Special Assistant to the President and first director of Kennedy's high-priority Food for Peace program, which realized what McGovern had been advocating in the House. McGovern assumed the post on January 21, 1961.
As director, McGovern urged the greater use of food to enable foreign economic development, saying, "We should thank God that we have a food abundance and use the over-supply among the under-privileged at home and abroad." He found space for the program in the Executive Office Building rather than be subservient to either the State Department or Department of Agriculture. McGovern worked with deputy director James W. Symington and Kennedy advisor Arthur M. Schlesinger, Jr. in visiting South America to discuss surplus grain distribution, and attended meetings of the United Nations' Food and Agriculture Organization. In June 1961, McGovern became seriously ill with hepatitis, contracted from an infected White House dispensary needle used to give him inoculations for his South American trip; he was hospitalized and unable to come to his office for two months.
By the close of 1961, the Food for Peace program was operating in a dozen countries, and 10 million more people had been fed with American surplus than the year before. In February 1962, McGovern visited India and oversaw a greatly expanded school lunch program thanks to Food for Peace; subsequently one in five Indian schoolchildren would be fed from it, and by mid-1962, 35 million children around the world. During an audience in Rome, Pope John XXIII warmly praised McGovern's work, and the distribution program was also popular among South Dakota's wheat farmers. In addition, McGovern was instrumental in the creation of the United Nations-run World Food Programme in December 1961; it started distributing food to stricken regions of the world the following year and would go on to become the largest humanitarian agency fighting hunger worldwide.
Administration was never McGovern's strength, however, and he was restless for another try at the Senate. With the approval of President Kennedy, McGovern resigned his post on July 18, 1962. Kennedy said that under McGovern, the program had "become a vital force in the world", improving living conditions and economies of allies and creating "a powerful barrier to the spread of Communism". Columnist Drew Pearson wrote that it was one of the "most spectacular achievements of the young Kennedy administration," while Schlesinger would later write that Food for Peace had been "the greatest unseen weapon of Kennedy's third-world policy".
U.S. Senator.
1962 election and early years as a senator.
In April 1962, McGovern announced he would run for election to South Dakota's other Senate seat, intending to face incumbent Republican Francis H. Case. Case died in June, however, and McGovern instead faced an appointed senator, former Lieutenant Governor Joseph H. Bottum. Much of the campaign revolved around policies of the Kennedy administration and its New Frontier; Bottum accused the Kennedy family of trying to buy the Senate seat. McGovern appealed to those worried about the outflux of young people from the state, and had the strong support of the Farmers Union. Polls showed Bottum slightly ahead throughout the race and McGovern was hampered by a recurrence of his hepatitis problem in the final weeks of the campaign (during this hospitalization, McGovern read Theodore H. White's classic "The Making of the President, 1960" and for the first time began thinking about running for the office someday). Eleanor McGovern campaigned for her ailing husband and may have preserved his chance of winning. The November 1962 election result was very close and required a recount, but McGovern's 127,458 votes prevailed by a margin of 597, making him the first Democratic senator from the state in 26 years and only the third since statehood in 1889.
When he joined the Senate in January 1963 for the 88th Congress, McGovern was seated on the Senate Agriculture and Forestry Committee and Senate Interior and Insular Affairs Committee. On the Agriculture Committee, McGovern supported high farm prices, full parity, and controls on beef importation, as well as the administration's Feed Grains Acreage Diversion Program. McGovern had a fractious relationship with Secretary of Agriculture Orville Freeman, who was less sympathetic to farmers; McGovern's 1966 resolution to informally scold Freeman made the senator popular back in his home state. Fellow new senator Edward M. Kennedy saw McGovern as a serious voice on farm policy and often sought McGovern's guidance on agriculture-related votes. McGovern was largely inactive on the Interior Committee until 1967, when he was given the chair of the Subcommittee on Indian Affairs. However, Interior Committee chair Henry M. Jackson, who did not get along with McGovern personally or politically, refused to allow McGovern his own staff, greatly limiting his effectiveness. McGovern regretted not accomplishing more for South Dakota's 30,000 Sioux Indians, although after a McGovern-introduced resolution on Indian self-determination passed in 1969, the Oglala Sioux named McGovern "Great White Eagle".
In his first speech on the Senate floor in March 1963, McGovern praised Kennedy's Alliance for Progress initiative, but spoke out against U.S. policy towards Cuba, saying that it suffered from "our Castro fixation". In August 1963, McGovern advocated reducing the $53 billion defense budget by $5 billion; influenced by advisor Seymour Melman, he held a special antipathy towards the doctrine of nuclear "overkill". McGovern would try to reduce defense appropriations or limit military expenditures in almost every year during the 1960s. He also voted against many weapons programs, especially missile and anti-missile systems, and also opposed military assistance to foreign nations. In 1964 McGovern published his first book, "War Against Want: America's Food for Peace Program". In it he argued for expanding his old program, and a Senate measure he introduced was eventually passed, adding $700 million to the effort's funding.
Preferring to focus on broad policy matters and speeches, McGovern was not a master of Senate legislative tactics, and developed a reputation among some other senators for "not doing his homework". Described as "a very private, unchummy guy", he was not a member of the Senate "club" nor did he want to be, turning down in 1969 a chance to join the powerful Senate Rules Committee. Relatively few pieces of legislation bore his name, and his legislative accomplishments were generally viewed as modest, although he would try to influence the contents of others' bills. In terms of ideology, McGovern fit squarely within modern American liberalism; through 1967 he had voted in accordance with the rated positions of the ADA 92 percent of the time, and when lacking specific knowledge on a particular matter, he would ask his staff, "What are the liberals doing?"
Opposition to Vietnam War.
In a speech on the Senate floor in September 1963, McGovern became the first member to challenge the growing U.S. military involvement in Vietnam. Bothered by the Buddhist crisis and other recent developments, and with concerns influenced by Vietnam historian Bernard Fall, McGovern said:
However, the speech was little noticed, and McGovern backed away from saying anything publicly for over a year afterward, partly because of the November 1963 assassination of President Kennedy and partly to not appear strident. Though more skeptical about it than most senators, McGovern voted in favor of the August 1964 Gulf of Tonkin Resolution, which turned out to be an essentially unbounded authorization for President Lyndon B. Johnson to escalate U.S. involvement in the war. McGovern thought the commander-in-chief should be given limited authority to retaliate against an attack; subsequently he said his instinct had been to vote no, but that he had voted yes based on Senator J. William Fulbright's urging to stand behind Johnson politically. Indeed, the day after the resolution vote, McGovern spoke concerning his fears that the vote would lead to greater involvement in the war; Wayne Morse, one of only two senators to oppose the resolution, sardonically noted that this fell into the category of "very interesting, but very belated". This would become the vote that McGovern most bitterly regretted.
In January 1965, McGovern made his first major address on Vietnam, saying that "We are not winning in South Vietnam ... I am very much opposed to the policy, now gaining support in Washington, of extending the war to the north." McGovern instead proposed a five-point plan advocating a negotiated settlement involving a federated Vietnam with local autonomy and a UN presence to guarantee security and fair treatment. The speech gave McGovern national visibility as one of the "doves" in the debate over Vietnam. However, McGovern made moderate-to-hawkish statements at times too, flatly rejecting unconditional withdrawal of U.S. forces and criticizing anti-war draft-card burnings as "immature, impractical, and illegal". He eschewed personal criticism of Johnson. In November 1965, McGovern traveled to South Vietnam for three weeks. The human carnage he saw in hospital wards deeply upset him, and he became increasingly outspoken about the war upon his return, more convinced than ever that Vietnam was a political, not military, problem. Now he was ready, as he later said, "not merely to dissent, but to crusade" against the war.
McGovern voted in favor of Vietnam military appropriations in 1966 through 1968, not wanting to deprive U.S. forces of necessary equipment. Nevertheless, his anti-war rhetoric increased throughout 1967. Over the years, Johnson had invited McGovern and other Senate doves to the White House for attempts to explain the rationale for his actions in Vietnam; McGovern came away from the final such visit, in August 1967, shaken by the sight of a president "tortured and confused ... by the mess he has gotten into in Vietnam."
1968 presidential and Senate campaigns.
In August 1967, activist Allard K. Lowenstein founded the Dump Johnson movement, and soon it was seeking a Democratic Party figure to make a primaries campaign challenge against Johnson in the 1968 presidential election. The group's first choice was Senator Robert Kennedy, who declined, as did another, and by late September 1967 they approached McGovern. After much deliberation McGovern declined, largely because he feared such a run would significantly damage his own chances for reelection to his Senate seat in 1968. A month later the anti-Johnson forces were able to convince Senator Eugene McCarthy to run, who was one of the few "dove" senators not up for reelection that year.
In the 1968 Democratic primary campaign, McCarthy staged a strong showing. Robert Kennedy entered the race, President Johnson withdrew and Vice President Hubert Humphrey ran instead. While McGovern privately favored Kennedy, McCarthy and Humphrey were both from the neighboring state of Minnesota and publicly McGovern remained neutral. McGovern hosted all three as they campaigned for the June 4 South Dakota Democratic primary, which resulted in a strong win by Kennedy to go along with his win in the crucial California primary that night. McGovern spoke with Kennedy by phone minutes before Kennedy was assassinated in Los Angeles. The death of Bobby Kennedy left McGovern the most emotionally distraught he had ever been to that point in his life.
Within days, some of Kennedy's aides were urging McGovern to run in his place; their antipathy towards McCarthy and ideological opposition to Humphrey made them unwilling to support either candidate. McGovern delayed making a decision, making sure that Bobby's brother Ted Kennedy did not want to enter, and with his staff still concerned about the senator's own reelection prospects. Indeed, McGovern's voting had changed during 1968, with his ADA rating falling to 43 as he sought more middle-of-the-road stances. In late July, McGovern's decision became more complicated when his daughter Teresa was arrested in Rapid City on marijuana possession charges. She had led a troubled life since her teenage years, developing problems with alcohol and depression and suffering the consequences of a relationship with an unstable neighborhood boy. Based on a recently enacted strict state drugs law, Terry now faced a minimum five-year prison sentence if found guilty. McGovern was also convinced that the socially conservative voters of South Dakota would reject him due to his daughter's arrest. Charges against her were subsequently dropped due to a technically invalid search warrant.
McGovern formally announced his candidacy on August 10, 1968, in Washington, two weeks in advance of the 1968 Democratic National Convention, committing himself to "the goals for which Robert Kennedy gave his life". Asked why he was a better choice than McCarthy, he said, "Well – Gene really doesn't want to be President, and I do." At the convention in Chicago, Humphrey was the near-certain choice while McGovern became the initial rallying point for around 300 leaderless Kennedy delegates. The chaotic circumstances of the convention found McGovern denouncing the Chicago police tactics against demonstrators as "police brutality". Given the internal politics of the party, it was difficult for McGovern to gain in delegate strength, and black protest candidate Channing E. Phillips drew off some of his support. In the actual roll call, McGovern came in third with 146½ delegates, far behind Humphrey's 1760¼ and McCarthy's 601.
McGovern endorsed Humphrey at the convention, to the dismay of some anti-war figures who considered it a betrayal. Humphrey went on to lose the general election to Richard Nixon. McGovern returned to his Senate reelection race, facing Republican former Governor Archie M. Gubbrud. While South Dakota voters sympathized with McGovern over his daughter's arrest, he initially suffered a substantial drop in popularity over the events in Chicago. However, McGovern conducted an energetic campaign that focused on his service to the state, while Gubbrud ran a lackluster effort. In November, McGovern won 57 percent of the vote in what he would consider the easiest and most decisive victory of his career.
Middle Senate years and continued opposition to the Vietnam War.
During the 1968 Democratic Convention, a motion had been passed to establish a commission to reform the Democratic Party nomination process. In 1969, McGovern was named chair of the Commission on Party Structure and Delegate Selection, also known as the McGovern–Fraser Commission; due to the influence of former McCarthy and Kennedy supporters on the staff, the commission significantly reduced the role of party officials and insiders in the nomination process, increased the role of caucuses and primaries, and mandated quotas for proportional black, female, and youth delegate representation. A somewhat unintended consequence of the commission's reforms was a massive increase in the number of presidential primaries; this became true for the Republican Party as well. The U.S. presidential nominating process has been different ever since, with scholars and politicians debating whether all the changes are for the better.
In the wake of several high-profile reports about hunger and malnutrition in the United States, the Senate Select Committee on Nutrition and Human Needs had been created in July 1968, with McGovern as its chair. Seeking to dramatize the problem, in March 1969 McGovern took the committee to Immokalee, Florida, the base for 20,000 migrant farm workers. They saw graphic examples of hunger and malnutrition firsthand, but also encountered resistance and complaints about bad publicity from local and state officials. McGovern battled the Nixon administration and Southerners in Congress during much of the next year over an expanded Food Stamp Program; he had to compromise on a number of points, but legislation signed in 1970 established the principles of free food stamps and a nationwide standard for eligibility.
McGovern generally lacked both interest and expertise in economics, but was outspoken in reaction to Nixon's imposition of wage and price controls in 1971. McGovern declared: "This administration, which pledged to slow inflation and reduce unemployment, has instead given us the highest rate of inflation and the highest rate of unemployment in a decade." "60 Minutes" included him in a 1971 report about liberal politicians and journalists who advocated integrated schooling while avoiding it for their children.
But most of all, McGovern was known for his continued opposition to the Vietnam War. In March 1969, he became the first senator to explicitly criticize the new president's policy there, an action that was seen as a breach of customary protocol by other Senate doves. The diversion during these years of much of Food for Peace's aid to South Vietnam, instead of other badly stricken countries around the world, greatly upset him. By the end of 1969, McGovern was calling for an immediate cease-fire and a total withdrawal of all American troops within a year. In October 1969, McGovern was a featured speaker before 100,000 demonstrators in Boston at the Moratorium to End the War in Vietnam, and in November he spoke before 350,000 at Moratorium/Mobilization's anti-war march to the Washington Monument. Afterward, he decided that radicalized peace demonstrations were counterproductive and criticized anti-war figures such as Rennie Davis, Tom Hayden, Huey Newton, Abbie Hoffman, and Jerry Rubin as "reckless" and "irresponsible".
Instead, McGovern focused on legislative means to bring the war to an end. The McGovern–Hatfield Amendment to the annual military procurement bill, co-sponsored by Republican Mark Hatfield of Oregon, required via funding cutoff a complete withdrawal of all American forces from Indochina by the end of 1970. It underwent months of public discussion and alterations to make it acceptable to more senators, including pushing the deadline out to the end of 1971. In May 1970, McGovern obtained a second mortgage on his Washington home in order to fund a half-hour televised panel discussion on the amendment on NBC. The broadcast brought in over $500,000 in donations that furthered work on passage, and eventually the amendment gained the support of the majority of the public in polls. The effort was denounced by opposition groups organized by White House aide Charles Colson, which called McGovern and Hatfield "apostles of retreat and defeat" and "salesmen of surrender" and maintained that only the president could conduct foreign policy. The amendment was defeated in September 1970 by a 55–39 vote, just short of what McGovern had hoped would constitute at least a moral victory. During the floor debate McGovern criticized his colleagues opposing the measure:
The Senate reacted in startled, stunned silence, and some faces showed anger and fury; when one member told McGovern he had been personally offended by the speech, McGovern said, "That's what I meant to do." McGovern believed Vietnam an immoral war that was destroying much of what was pure, hopeful, and different about America's character as a nation.
The defeat of the amendment left McGovern embittered and somewhat more radicalized. He accused Vice President of South Vietnam Nguyen Cao Ky of running a heroin trafficking operation that was addicting American soldiers. In a retort to the powerful Senate Armed Services Committee chair John Stennis' suggestion that U.S. troops might have to return to Cambodia, McGovern declared, "I'm tired of old men dreaming up wars for young men to fight. If he wants to use American ground troops in Cambodia, let him lead the charge himself." He denounced Nixon's policy of Vietnamization as "subsidiz the continued killing of the people of Indochina by technology and mercenaries". In a "Playboy" interview, he said that Ho Chi Minh was the North Vietnamese George Washington.
McGovern–Hatfield was put up for a vote again in 1971, with somewhat weaker provisions designed to gain more support. In polls, a large majority of the public now favored its intent, and McGovern took his name off a final form of it, as some senators were just objecting to him. Nevertheless, in June 1971, it failed to pass again, gaining only a few more votes than the year before. McGovern was now certain that the only way the war would come to a quick end was if there was a new president.
1972 presidential campaign.
McGovern announced his candidacy on January 18, 1971, during a televised speech from the studios of KELO-TV in Sioux Falls, South Dakota. At the time of his announcement, McGovern ranked fifth among Democrats in a presidential preference Gallup Poll. The earliest such entry since Andrew Jackson was designed to give him time to overcome the large lead of the front-runner, Maine Senator Edmund Muskie. Nevertheless, by January 1972, McGovern had only 3 percent national support among Democrats in the Gallup Poll and had not attracted significant press coverage.
McGovern's campaign manager, Gary Hart, decided on a guerrilla-like insurgency strategy of battling Muskie in only selected primaries, not everywhere, so as to focus the campaign's organizational strength and resources.
Muskie fell victim to inferior organizing, an over-reliance on party endorsements, and Nixon's "dirty tricks" operatives, and in the March 7, 1972, New Hampshire primary, did worse than expected with McGovern coming in a close second. As Muskie's campaign funding and support dried up, Hubert Humphrey, who had rejoined the Senate, became McGovern's primary rival for the nomination, with Alabama governor George Wallace also in the mix after dominating the March 14 primary in Florida. McGovern won a key breakthrough victory over Humphrey and Wallace on April 4 in Wisconsin, where he added blue-collar economic populism to his appeal. He followed that by dominating the April 25 primary in Massachusetts. At that point, McGovern had become the front runner. A late decision to enter the May 2 Ohio primary, considered a Humphrey stronghold, paid dividends when McGovern managed a very close second there amid charges of election fraud by pro-Humphrey forces. The other two leading candidates for the nomination also won primaries, but Wallace's campaign effectively ended when he was seriously wounded in a May assassination attempt, and McGovern's operation was effective in garnering delegates in caucus states. The climactic contest took place in California, with Humphrey attacking McGovern in several televised debates; in the June 6 vote, McGovern defeated him by five percentage points and claimed all the delegates due to the state's winner-take-all rules. He then appeared to clinch the nomination with delegates won in the New York primary on June 20. However, Humphrey's attacks on McGovern as being too radical began a downward slide in the latter's poll standing against Nixon.
McGovern became tagged with the label "amnesty, abortion and acid", supposedly reflecting his positions.
During his primary victories, McGovern used an approach that stressed grassroots-level organization while bypassing conventional campaign techniques and traditional party power centers. He capitalized on support from anti-war activists and reform liberals; thousands of students engaged in door-to-door campaigning for him. He benefited by the eight primaries he won being those the press focused on the most; he showed electoral weakness in the South and industrial Midwest, and actually received fewer primary votes overall than Humphrey and had only a modest edge over Wallace.
McGovern ran on a platform that advocated withdrawal from the Vietnam War in exchange for the return of American prisoners of war and amnesty for draft evaders who had left the country.
McGovern's platform also included an across-the-board, 37-percent reduction in defense spending over three years. He proposed a "demogrant" program that would give a $1,000 payment to every citizen in America. Based around existing ideas such as the negative income tax and intended to replace the welfare bureaucracy and complicated maze of existing public-assistance programs, it nonetheless garnered considerable derision as a poorly thought-out "liberal giveaway" and was dropped from the platform in August.
An "Anybody But McGovern" coalition, led by southern Democrats and organized labor, formed in the weeks following the final primaries. McGovern's nomination did not become assured until the first night of the 1972 Democratic National Convention in Miami Beach, Florida, where, following intricate parliamentary maneuverings led by campaign staffer Rick Stearns, a Humphrey credentials challenge regarding the California winner-take-all rules was defeated. Divisive arguments over the party platform then followed; what resulted was arguably the most liberal one of any major U.S. party. On July 12, 1972, McGovern officially won the Democratic nomination. In doing so and in taking over the party's processes and platform, McGovern produced what "The New York Times" termed "a stunning sweep". The convention distractions led to a hurried process to pick a vice presidential running mate. Turned down by his first choice, Ted Kennedy, as well as by several others, McGovern selected – with virtually no vetting – Missouri Senator Thomas Eagleton. On the final night of the convention, procedural arguments over matters such as a new party charter, and a prolonged vice presidential nomination process that descended into farce, delayed the nominee's acceptance speech. As a result, McGovern delivered his speech, "Come home America!", at three o'clock in the morning, reducing his television audience from about 70 million people to about 15 million.
Just over two weeks after the convention, it was revealed that Eagleton had been hospitalized and received electroshock therapy for "nervous exhaustion" and "depression" several times during the early to mid-1960s (years later, Eagleton's diagnosis was refined to bipolar II disorder). McGovern initially supported Eagleton, in part because he saw parallels with his daughter Terry's battles with mental illness, and on the following day, July 26, stated publicly, "I am 1,000 percent for Tom Eagleton and have no intention of dropping him from the ticket." Though many people still supported Eagleton's candidacy, an increasing number of influential politicians and newspapers questioned his ability to handle the office of vice president and, potentially, president or questioned the McGovern campaign's ability to survive the distraction. The resulting negative attention – combined with McGovern's consultation with preeminent psychiatrists, including Karl Menninger, as well as doctors who had treated Eagleton – prompted McGovern to accept, and announce on August 1, Eagleton's offer to withdraw from the ticket. It remains the only time major party vice-presidential nominee has been forced off the ticket. Five prominent Democrats then publicly turned down McGovern's offer of the vice presidential slot: in sequence, Kennedy again, Abraham Ribicoff, Humphrey, Reubin Askew, and Muskie (Larry O'Brien was also approached but no offer made). Finally, he named United States Ambassador to France Sargent Shriver, a brother-in-law of John F. Kennedy. McGovern's 1,000 percent statement and subsequent reneging made him look both indecisive and an opportunist, and has since been considered one of the worst gaffes in presidential campaign history. McGovern himself would long view the Eagleton affair as having been "catastrophic" for his campaign.
The general election campaign did not go well for McGovern. Nixon did little campaigning; he was buoyed by the success of his visit to China and arms-control-signing summit meeting in the Soviet Union earlier that year and, shortly before the election, Henry Kissinger's somewhat premature statement that "peace is at hand" in Vietnam. Top Republican figures attacked McGovern for being weak on defense issues and "encouraging the enemy"; Nixon asserted that McGovern was for "peace at any price" in Vietnam, rather than the "peace with honor" that Nixon said he would bring about. McGovern chose to not emphasize his own war record during the campaign. The McGovern Commission changes to the convention rules marginalized the influence of establishment Democratic Party figures, and McGovern struggled to get endorsements from figures such as former President Johnson and Chicago mayor Richard J. Daley. The AFL–CIO remained neutral, after having always endorsed the Democratic presidential candidate in the past. Some southern Democrats, led by former Texas governor John Connally, switched their support to the incumbent President Nixon through a campaign effort called "Democrats for Nixon". Nixon outspent McGovern by more than two-to-one.
Nixon directly requested that his aides use government records to try to dig up dirt on McGovern and his top contributors. McGovern was publicly attacked by Nixon surrogates and was the target of various operations of the Nixon "dirty tricks" campaign. The infamous Watergate break-in of the Democratic National Committee headquarters in June 1972 was an alternate target after bugging McGovern's headquarters was explored. The full dimensions of the subsequent Watergate scandal did not emerge during the election, however; the vast majority of the press focused on McGovern's difficulties and other news, rather than the break-in or who was behind it, and a majority of voters were unaware of Watergate. In the end, Nixon's covert operations had little effect in either direction on the election outcome.
By the final week of the campaign, McGovern knew he was going to lose. While he was appearing in Battle Creek, Michigan, on November 2, a Nixon admirer heckled him. McGovern told the heckler, "I've got a secret for you," then said softly into his ear, "Kiss my ass." The incident was overheard and reported in the press, and became part of the tale of the campaign.
In the general election on November 7, 1972, the McGovern–Shriver ticket suffered a 61-percent to 37-percent defeat to Nixon – at the time, the second biggest landslide in American history, with an Electoral College total of 520 to 17. McGovern's two electoral vote victories came in Massachusetts and Washington, D.C.; McGovern failed to win his home state of South Dakota (which had gone Democratic in only three of the previous eighteen presidential elections).
Remaining Senate years.
After this loss, McGovern remained in the Senate. He was scarred by the enormous defeat, and his wife Eleanor took it even worse; during the winter of 1972–1973, the couple seriously considered moving to England. His allies were replaced in positions of power within the Democratic Party leadership, and the McGoverns did not get publicly introduced at party affairs they attended. On January 20, 1973, a few hours after Richard Nixon was re-inaugurated, McGovern gave a speech at the Oxford Union that talked about the abuses of Nixon's presidency; it brought criticism, including from some Democrats, for being ill-mannered. In order to get past the "bitterness and self-pity" he felt, McGovern forced himself to deal with the defeat humorously before audiences; starting at the March 1973 Gridiron Dinner, he frequently related his campaign misadventures in a self-deprecating fashion, such as saying, "For many years, I wanted to run for the Presidency in the worst possible way – and last year I sure did." Nevertheless, emotions surrounding the loss would remain with McGovern for decades, as it did with some other defeated presidential nominees. Nixon resigned in August 1974 due to the Watergate scandal. McGovern said President Gerald R. Ford's subsequent September 1974 pardon of Nixon was difficult to understand given that Nixon's subordinates were going to prison.
McGovern displayed the political resiliency he had shown in the past. In the 1974 U.S. Senate elections, McGovern faced possible political peril due to having neglected the state during his long presidential campaign, and by May 1973, he had already begun campaigning for reelection. An Air Force pilot and Medal of Honor recipient, Leo K. Thorsness, had just been repatriated after six years as a prisoner of war in North Vietnam; he publicly accused McGovern of having given aid and comfort to the enemy and of having prolonged his time as a POW. McGovern replied that if there had been no war, there would have been no POWs, and that everything he had done had been towards the goal of ending the war sooner. Thorsness became the Republican nominee against McGovern, but despite the two men's different roles in it, the war did not become a significant issue. Instead, the campaign was dominated by farm policy differences and economic concerns over the 1973–75 recession. Thorsness charged McGovern with being a "part-time senator" more concerned with national office and with spending over $2 million on his re‑election bid, while McGovern labelled Thorsness a carpetbagger due to his having grown up in Minnesota. In a year in which Democrats were advantaged by the aftereffects of the Watergate scandal, McGovern won re-election in November 1974 with 53 percent of the vote.
Following the victory, McGovern harbored thoughts of running in the 1976 presidential election, but given the magnitude of his presidential defeat, the Democratic Party wanted nothing to do with him then or later. Unfamiliar and uncomfortable with Democratic nominee Jimmy Carter, McGovern secretly voted for Ford instead. McGovern's view on intervention in Southeast Asia took a turn in 1978 in reaction to the ongoing Cambodian genocide. Noting that it affected a percentage of the population which made "Hitler's operation look tame", he advocated an international military intervention in Cambodia to put the Khmer Rouge regime out of power.
McGovern's Select Committee on Nutrition and Human Needs expanded its scope to include national nutrition policy. In 1977 it issued a new set of nutritional guidelines for Americans that sought to combat leading killer health conditions. Titled "Dietary Goals for the United States", but also known as the "McGovern Report", it suggested that Americans eat less fat, less cholesterol, less refined and processed sugars, and more complex carbohydrates and fiber. While many public health officials had said all of this for some time, the committee's issuance of the guidelines gave it higher public profile. The recommendations proved controversial with the cattle, dairy, egg, and sugar industries, including from McGovern's home state. The McGovern committee guidelines led to reorganization of some federal executive functions and became the predecessor to the more detailed Dietary Guidelines for Americans later issued twice a decade by the Center for Nutrition Policy and Promotion.
In the 1980 Senate election in South Dakota, McGovern was one of several liberal Democratic senators targeted for defeat by the National Conservative Political Action Committee (NCPAC), which put out a year's worth of negative portrayals of McGovern. It and other pro-life groups especially focused on McGovern's support for pro-choice abortion laws. McGovern faced a Democratic primary challenge for the first time, from a pro-life candidate. McGovern's Republican opponent was James Abdnor, a four-term incumbent congressman who held identical positions to McGovern's on farm issues, was solidly conservative on national issues, and was well liked within the state. Abdnor's campaign focused on both McGovern's liberal voting record and what it said was McGovern's lack of involvement in South Dakota affairs. McGovern made an issue of NCPAC's outside involvement, and that group eventually withdrew from the campaign after Abdnor denounced a letter it had sent out. Far behind in the polls earlier, McGovern outspent Abdnor 2-to-1 and repeatedly criticized Abdnor's refusal to debate him, thereby drawing attention to a slight speech defect Abdnor had. Showing the comeback pattern of some of his past races in the state, McGovern closed the gap for a while. However, in November 1980 McGovern was solidly defeated for re-election, getting only 39 percent of the vote to Abdnor's 58 percent. McGovern became one of many Democratic casualties of that year's Republican sweep, which became known as the "Reagan Revolution".
Post-Senate life and 1984 presidential campaign.
McGovern did not mourn leaving the Senate. Although being rejected by his own state stung, intellectually he could accept that South Dakotans wanted a more conservative representative; he and Eleanor felt out of touch with the country and in some ways liberated by the loss. Nevertheless, he refused to believe that American liberalism was dead in the time of Reagan; remaining active in politics, in January 1981 he founded the political organization Americans for Common Sense. The group sought to rally liberals, encourage liberal thinking, and combat the Moral Majority and other new Christian right forces. In 1982, he turned the group into a political action committee, which raised $1.2 million for liberal candidates in the 1982 U.S. Congressional elections. McGovern shut the committee down when he decided to run for president again.
McGovern also began teaching and lecturing at a number of universities in the U.S. and Europe, accepting one-year contracts or less. From 1981 to 1982, McGovern replaced historian Stephen Ambrose as a professor at the University of New Orleans. McGovern also began making frequent speeches, earning several hundred thousand dollars a year.
McGovern attempted another presidential run in the 1984 Democratic primaries. Friends and political admirers of McGovern initially feared the effort would prove an embarrassment, and McGovern knew himself that his chances of winning were remote, but he felt compelled to try to influence the intraparty debate in a liberal direction. Freed from the practical concerns of trying to win, McGovern outlined a ten-point program of sweeping domestic and foreign policy changes; because he was not seen as a threat, fellow competitors did not attack his positions, and media commentators praised him as the "conscience" of the Democratic Party.
While having name recognition, McGovern had little funding or staff, although he did garner critical funding from some celebrities and statesmen. He won a surprise third-place showing in the Iowa caucuses amidst a crowded field of candidates, but finished fifth in the New Hampshire primary. He announced he would drop out unless he finished first or second in the Massachusetts primary, and when he came in third behind his former campaign manager Gary Hart and former Vice President Walter Mondale, he made good on his promise. He later endorsed Mondale, the eventual Democratic nominee. McGovern hosted "Saturday Night Live" on April 14, 1984.
McGovern addressed the party's platform committee, and his name was placed in nomination at the 1984 Democratic National Convention, where he delivered a speech that strongly criticized President Reagan and praised Democratic unity. He received the votes of four delegates. He went on to actively support the Mondale-Geraldine Ferraro ticket, whose eventual landslide defeat bore some similarities to his own in 1972.
During the 1980s, McGovern was a fellow at the Institute for Policy Studies, a think tank in Washington, D.C.
McGovern had made several real estate investments in the D.C. area and became interested in hotel operations. In 1988, using the money he had earned from his speeches, the McGoverns bought, renovated, and began running a 150-room inn in Stratford, Connecticut, with the goal of providing a hotel, restaurant and public conference facility. It went into bankruptcy in 1990 and closed the following year. In 1992, McGovern's published reflections on the experience appeared in "Wall Street Journal" and the "Nation's Restaurant News". He attributed part of the failure to the early 1990s recession, but also part to the cost of dealing with federal, state and local regulations that were passed with good intentions but made life difficult for small businesses, and to the cost of dealing with frivolous lawsuits. McGovern wrote, "I ... wish that during the years I was in public office I had had this firsthand experience about the difficulties business people face every day. That knowledge would have made me a better U.S. senator and a more understanding presidential contender."
After briefly exploring another presidential run in the 1992 contest,
McGovern instead became president of the Middle East Policy Council (a non-profit organization that seeks to educate American citizens and policy makers about the political, economic and security issues impacting U.S. national interests in the Middle East) in July 1991; he had previously served on its board since 1986. He held this position until 1997, when he was replaced by Charles W. Freeman, Jr.
On the night of December 12–13, 1994, McGovern's daughter Teresa fell into a snowbank in Madison, Wisconsin, while heavily intoxicated and died of hypothermia. Heavy press attention followed, and McGovern revealed his daughter had battled her alcoholism for years and had been in and out of many treatment programs while having had one extended period of sobriety. He authored an account of her life, "Terry: My Daughter's Life-and-Death Struggle with Alcoholism"; published in 1996, it presented a harrowing, unsparing view of the depths to which she had descended, the torment that he and the rest of his family had experienced in trying unsuccessfully to help her, and his ongoing thoughts and guilt about whether the demands of his political career and the time he had spent away from the family had made things worse for her. The book was a modest best-seller, and with the proceeds, he founded the Teresa McGovern Center in Madison to help others suffering from the combination of alcoholism and mental health problems. He would later say that Terry's death was by far the most painful event in his life: "You never get over it, I'm sure of that. You get so you can live with it, that's all."
Ambassador to food agencies and other later activities.
In April 1998, McGovern returned to public service when he began a three-year stint as United States Ambassador to the United Nations Agencies for Food and Agriculture, serving in Rome, Italy, after having been named to the post by President Bill Clinton. In an effort to meet the UN's goal of reducing the number of hungry people in the world by half by 2015, he formulated detailed plans, urging delivery of more surplus food to foreign school-lunch programs and the establishment of specific targets such as had been done in old American programs. He began working again with fellow former Senator Bob Dole to convince the Senate to support this effort, as well as expanded school lunch, food stamps, and nutritional help for pregnant women and poor children in the U.S.
The George McGovern–Robert Dole International Food for Education and Nutrition Program that was created in 2000, and funded largely through the Congress, would go on to provide more than 22 million meals to children in 41 countries over the next eight years. It was also credited with improving school attendance, especially among girls, who were more likely to be allowed to go to school if a meal was being provided. In August 2000, President Clinton presented McGovern with the Presidential Medal of Freedom, the nation's highest civilian honor, in recognition of McGovern's service in the effort to eradicate world hunger. McGovern's book "The Third Freedom: Ending Hunger In Our Time" was published in January 2001; with its title making reference to Roosevelt's Four Freedoms speech, it proposed a plan whereby chronic world hunger could be eliminated within thirty years. In January 2001, McGovern was asked to stay on at the UN post for a while by the incoming George W. Bush administration and then concluded his stint in September 2001.
In October 2001, McGovern was appointed as the first UN Global Ambassador on World Hunger by the World Food Programme, the agency he had helped found forty years earlier. He was still active in this Goodwill Ambassador position as of 2011 and remained in it until his death. McGovern was an honorary life member of the board of Friends of the World Food Program. McGovern also served as a Senior Policy Advisor at Olsson Frank Weeda, a food and drug regulatory counseling law and lobbying firm in Washington, D.C., where he specialized on issues of food, nutrition, and agriculture.
McGovern's wartime story was at the center of Ambrose's 2001 best-selling profile of the men who flew B‑24s over Germany in World War II, "The Wild Blue". It was the first time much of the public became familiar with that part of his life; throughout his political career, McGovern had rarely mentioned his war service or the medals he had won.
McGovern continued to lecture and make public appearances, sometimes appearing with Dole on college campuses. McGovern and Dole contributed essays to the 2005 volume "Ending Hunger Now: A Challenge to Persons of Faith". From around 2003 to 2005, McGovern owned a bookstore in his summer home of Stevensville in Montana's Bitterroot Valley, until deciding to sell it due to lack of sufficient market. In 2003, the McGoverns became part-time residents of Marco Island, Florida; by then, Eleanor was struggling with heart disease.
In October 2006, the $8.5 million George and Eleanor McGovern Library and Center for Leadership and Public Service was dedicated at Dakota Wesleyan University. The couple had helped raise the funds for it. It seeks to prepare the college's best students for future careers in public service through classes, seminars, research, and internships, and also to raise the visibility of the university. The dignitaries in attendance were led by former President Clinton.
McGovern's wife Eleanor was too ill to attend the ceremony, and she died of heart disease on January 25, 2007, at their home in Mitchell.
Later in 2007, several events were held at Dakota Wesleyan and in Washington, D.C., to celebrate McGovern's 85th birthday and the 35th anniversary of his nomination for president. Hundreds of former staff, volunteers, supporters and friends attended, along with public officials.
McGovern still sought to have his voice heard in the American political scene.
He became an outspoken opponent of the Iraq War, likening U.S. involvement in that country to that of the failed Vietnam effort, and in 2006 co-wrote the book "Out of Iraq: A Practical Plan for Withdrawal Now".
In January 2008, McGovern wrote an op-ed in the "Washington Post" calling for the impeachment of President George W. Bush and Vice-President Dick Cheney, saying they had violated the U.S. Constitution, transgressed national and international law, and repeatedly lied to the American people. The subtitle of the article read "Nixon Was Bad. These Guys Are Worse."
In the tumultuous 2008 Democratic Party presidential nomination campaign, he first endorsed U.S. Senator Hillary Rodham Clinton and then later switched to Senator Barack Obama after concluding Clinton could no longer win.
On October 16, 2008, McGovern and Dole were made World Food Prize laureates for their efforts to curb hunger in the world and in particular for their joint program for school feeding and enhanced school attendance.
Final years and death.
By 2009, McGovern had moved to St. Augustine Beach, Florida. McGovern's seventh book (as author, co-author, or contributing editor) issued in the first decade of the 2000s, "Abraham Lincoln", was published by Times Books and released at the close of 2008. Throughout 2009, McGovern embarked on a book tour, including a prominent visit to the Nixon Presidential Library and Museum.
He was treated for exhaustion during 2011 and then was hospitalized after a serious fall in December 2011 on his way to participate in a live C-SPAN program about his 1972 presidential campaign. By January 2012, he was promoting his latest book, "What It Means to Be a Democrat". He was hospitalized again in April 2012 due to fainting spells.
McGovern's 90th birthday was celebrated on July 19, 2012, with a Washington event hosted by World Food Program USA and attended by many liberal Democratic politicians, along with (as the "Washington Post" termed it) "one respectful conservative", South Dakota's Republican Senator John Thune.
On July 27, 2012, McGovern's son Steven died at age 60. McGovern's daughter Ann said, "Steve had a long struggle with alcoholism. We will all miss him deeply, but are grateful that he is now at peace."
In August 2012, McGovern moved back to Sioux Falls, South Dakota, to be nearer to his family. His final public appearance was on October 6, 2012, when he introduced his recorded narration for Aaron Copland's "Lincoln Portrait" with the South Dakota Symphony Orchestra.
On October 15, 2012, McGovern's family announced he had entered Dougherty Hospice House, a Sioux Falls hospice; his daughter Ann said, "He's coming to the end of his life". 
On the morning of October 21, 2012, McGovern died at the age of 90 at the Sioux Falls hospice, surrounded by family and lifelong friends.
The family released this statement, "We are blessed to know that our father lived a long, successful and productive life advocating for the hungry, being a progressive voice for millions and fighting for peace. He continued giving speeches, writing and advising all the way up to and past his 90th birthday, which he celebrated this summer." In addition to his three remaining children, he was survived by ten grandchildren and eight great-grandchildren. President Obama paid tribute to him as "a champion for peace" and a "statesman of great conscience and conviction". At a memorial service in Sioux Falls, Vice President Joe Biden eulogized McGovern, addressing McGovern's World War II service and his opposition to the Vietnam War in saying to his family, "Your father was a genuine hero. ... Had your father not been in the Senate, so much more blood, so much more treasure would have been wasted." His funeral was held in the Washington Pavilion of Arts and Science in Sioux Falls with his ashes to be buried alongside his wife and daughter Terry at Rock Creek Cemetery in Washington.
On July 26, 2015, the "Argus Leader", the daily newspaper in Sioux Falls, South Dakota, published an article detailing the extensive files on McGovern compiled through the years by the Federal Bureau of Investigation, including letters and notations from FBI Director J. Edgar Hoover, revealing that Hoover had a direct interest in the FBI monitoring of McGovern. The newspaper also published the complete FBI file on McGovern, obtained through a Freedom of Information Act request filed shortly after McGovern's death.
Legacy.
Due to his resounding loss to Nixon in the 1972 election and the causes behind it, "McGovernism" became a label that a generation of Democratic politicians tried to avoid. In 1992, nationally syndicated "Chicago Tribune" columnist Bob Greene wrote, "Once again politicians – mostly Republicans, but some Democrats, too – are using his name as a synonym for presidential campaigns that are laughable and out of touch with the American people." Conservatives used McGovern's name as a ready synonym for what they saw as liberal failures. Indeed, according to Daniel McCarthy of "The American Conservative", the Republican Party began to act after 1972 as if "every Democratic leader, no matter how Southern, how pro-war, how middle-of-the-road, is really a McGovernite. Indeed, for nearly 40 years the conservative movement has defined itself in opposition to the Democratic standard-bearer of 1972. Anti-McGovernism has come to play for the Right the unifying role that anticommunism once played, much to the detriment of older principles such as limited government, fiscal continence, and prudence in foreign policy." The association with dovishness and weakness on defense has been especially prevalent, although McGovern publicly stated in 1972 that he was not a pacifist and that use of force was sometimes necessary, such as in World War II. McGovern later said in 2001 that his political image had been exaggerated: "I am a liberal and always have been – just not the wild-eyed character the Republicans made me out to be." He continued to feel that he was marginalized with his views miscast. He saw himself as a son of the prairie, in 2005 reciting his traditional upbringing and family values, culminating with "I'm what a normal, healthy, ideal American should be like," and in 2006 said, "How the hell do you get elected in South Dakota for twenty years if you're a wild-eyed radical?"
In later decades the former senator remained a symbol, or standard-bearer, of the political left, particularly in relation to the turbulent 1960s and early 1970s when the country was torn by U.S. involvement in the Vietnam War and the corruption and abuse of power of the Nixon administration. Throughout his career, McGovern's positions reflected his own experiences as well as a personal synthesis of the traditions of American liberalism and progressivism. Southern Methodist University historian Thomas J. Knock wrote in 2003 that "[McGovern's] career was extraordinary and historic ... primarily because of his impress as searching and prophetic critic" and that "few political careers offer an alternative understanding of the American Century as compelling and instructive as McGovern's."
As chair of the McGovern–Fraser Commission in 1969–1970, McGovern instituted major changes in Democratic party rules that continue to this day and, to a large degree, were ultimately adopted by the Republican Party as well, with large institutional changes taking place in both. Among those was the centralization of decisions about the nominating process at the national party level, rather than with the states. His 1972 campaign fundamentally altered how presidential primary campaigns were waged. Within the Democratic Party, power shifted from the New Deal coalition to younger, more affluent, issue-oriented activists; the women's movement and gay rights movement found a place; skepticism about military buildups and foreign interventions took hold; and the 1960s "New Politics" found its culmination in McGovern's nomination. In turn, the overwhelming defeat of McGovern in the general election led to the liberal wing of the party's being stigmatized for decades to come and a turn in the party towards centrist directions. McGovern himself recognized the mixed results of his 1972 candidacy, saying, "We made a serious effort to open the doors of the Democratic Party – and as soon as we did, half the Democrats walked out." SUNY Albany political scientist Bruce Miroff wrote in 2007 that the McGovern campaign was the last time in presidential politics that liberals had "their chance to speak of their goals with enthusiasm and their dreams with fire ... Yet almost at the instant that the insurgents successfully stormed the heights of American politics, they found themselves on the brink of one of the worst free falls on record."
Staffers who worked on McGovern's 1972 campaign later became influential within the Democratic Party. Campaign manager Gary Hart staged his own presidential runs in 1984 and 1988. Future president Bill Clinton, with assistance from his future wife and politician Hillary Rodham, had managed the McGovern campaign's operations in Texas. Hart both embraced and moved away from aspects of his past affiliation with McGovern, while Clinton, and the Democratic Leadership Council movement of which he was a part, explicitly rejected McGovern's ideology. But there was still a legacy in terms of staffing, as the Clinton White House would be full of former "McGovernites".
McGovern's post-political career generally enhanced his reputation; Tom Brokaw, who referred to McGovern as part of the "Greatest Generation", wrote in 1998 that "He remains one of the country's most decent and thoughtful public servants."
McGovern's legacy also includes his commitment to combating hunger both in the United States and around the globe. He said, "After I'm gone, I want people to say about me: He did the best he could to end hunger in this country and the world." In the view of Knock, McGovern in all his activities arguably accomplished more for people in need than most presidents or secretaries of state in U.S. history. Responding to the Serenity Prayer's desire to "grant me the serenity to accept the things I cannot change", McGovern said simply that he rejected that notion: "I keep trying to change them."

</doc>
<doc id="62897" url="https://en.wikipedia.org/wiki?curid=62897" title="Sherden">
Sherden

The Sherden (Egyptian "šrdn", "š3rd3n3" or "š3rdyn3", Ugaritic "šrdnn(m)" and "trtn(m)", possibly Akkadian "še–er–ta–an–nu"; also glossed “Shardana” or “Sherdanu”) are one of several groups of "Sea Peoples" who appear in fragmentary historical and iconographic records (Egyptian and Ugaritic) from the Eastern Mediterranean in the late second millennium BCE.
On reliefs they are shown carrying a round shield and spear, dirk, or sword, perhaps of Naue II type. In some cases they are shown wearing corselets and kilts, but their key distinguishing feature is a horned helmet which, in all cases but three, features a circular accouterment at the crest. At Medinet Habu the corselet appears similar to that worn by the Philistines. The Sherden sword, it has been suggested by archaeologists since James Henry Breasted, may have developed from an enlargement of European daggers, and been associated with the exploitation of Bohemian tin. Robert Drews has recently suggested that use of this weapon amongst groups of Sherden and Philistine mercenaries made them capable of withstanding attacks by chariotry, making them valuable allies in warfare, though Drews' theory has been widely criticized by contemporary scholars.
Early historical references to the Sherden.
The earliest known mention of the people called "Srdn-w", more usually called Sherden or Shardana, is generally thought to be the Akkadian reference to "še–er–ta–an–nu" people in the Amarna Letters correspondence from Rib-Hadda, mayor ("hazannu") of Byblos, to the Pharaoh Amenhotep III or Akhenaten in the 14th century BCE. Though they have been referred to as sea raiders and mercenaries, prepared to offer their services to local employers, these texts do not provide any evidence of that association, nor do they shed light on what the function of these "širdannu-people" was at this time.
The first certain mention of the Sherden is found in the records of Ramesses II (ruled 1279-1213 BCE), who defeated them in his second year (1278 BCE) when they attempted to raid Egypt's coast. The pharaoh subsequently incorporated many of these warriors into his personal guard. An inscription by Ramesses II on a stele from Tanis which recorded the Sherden pirates' raid and subsequent defeat, speaks of the constant threat which they posed to Egypt's Mediterranean coasts: 
After Ramesses II succeeded in defeating the invaders and capturing some of them, Sherden captives are depicted in this Pharaoh's bodyguard, where they are conspicuous by their helmets with horns with a ball projecting from the middle, their round shields and the great Naue II swords, with which they are depicted in inscriptions of the Battle with the Hittites at Kadesh. Ramesses tells us, in his Kadesh inscriptions, that he incorporated some of the Sherden into his own personal guard at the Battle of Kadesh.
Years later other waves of Sea People, Sherden included, were defeated by Merneptah, son of Ramesses II, and Ramesses III. An Egyptian work written around 1100 BC named Onomasticon of Amenope, documents the presence of the Sherden in Palestine. After being defeated by Pharaoh Ramsses III, in fact, they, along with other "Sea Peoples", would be allowed to settle in this territory, subject to Egyptian rule.
The Italian orientalist Giovanni Garbini identified the territory settled by the Sherden in Northern Palestine as that occupied, according to the Bible, by the Israelite tribe of Zebulun where also appears a village named "Sared". Archaeologist Adam Zertal suggests that some Sherden settled in what is now northern Israel. He hypothesizes that Biblical Sisera was a Sherden general and that the archaeological site at el-Ahwat (whose architecture resembles Nuraghe sites in Sardinia) was Sisera's capital, Harosheth Haggoyim, though this theory has not received wide acceptance in the scholarly community.
Connection with Sea Peoples.
The Sherden seem to have been one of the more prominent groups of pirates that engaged in coastal raiding and the disruption of trade in the years surrounding the 13th century BC. They are first mentioned by name in the Tanis II rhetorical stele of Ramesses II, which says in part “As for the Sherden of rebellious mind, whom none could ever fight against, who came bold-hearted, they sailed in, in warships from the midst of the Sea, those whom none could withstand; but he plundered them by the victories of his valiant arm, they being carried off to Egypt.” It is possible that some of the Sherden captured in the battle recounted in Tanis II were pressed into Egyptian service, perhaps even as shipwrights or advisers on maritime technology, a role in which they may have assisted in the construction of the hybrid Egyptian warships seen on the monumental relief at Medinet Habu showing the naval battle between Egyptians and Sea Peoples.
Michael Wood has suggested that their raids contributed greatly to the collapse of the Mycenaean civilization. However, while some Aegean attributes can be seen in the material culture of the Philistines, one of the Sea Peoples who established cities on the southern coastal plain of Canaan at the beginning of the Iron Age, the association of the Sherden with this geographic area is based entirely on their association with this group and the Sea Peoples phenomenon writ large, rather than on physical or literary evidence (of which almost all testifies to their presence in Egypt rather than their port of origin).
Origins.
No mention of the Sherden has ever been found in Hittite or Greek legends or documents, suggesting that they did not originate from either sphere of influence. Some, who draw attention to the etymological connections between Sherden and Sardinia, Shekelesh with Sicily, and "Trs-w" (Teresh or Tursci) with Etruscans, suggested that these people came from the Western Mediterranean. Others think that this theory is archaeologically not satisfactory, arguing that there is evidence that these people arrived in the areas in which they lived after the period of Ramesses III, rather than before.
Archaeologist Margaret Guido concludes the evidence for the Sherden, Shekelesh or Teresh coming from the western Mediterranean is flimsy. Guido in 1963 suggests that the Sherden may ultimately derive from Ionia, in the central west coast of Anatolia, in the region of Hermos, east of the island of Chios. It is suggested that Sardis, and the Sardinian plain nearby, may preserve a cultural memory of their name. Until recently it was assumed that Sardis was only settled in the period after the Anatolian and Aegean Dark Age, but American excavations have shown the place was settled in the Bronze Age and was a site of a significant population. If this is so, the Sherden, pushed by Hittite expansionism of the Late Bronze Age and prompted by the famine that affected this region at the same time, may have been pushed to the Aegean islands, where shortage of space led them to seek adventure and expansion overseas. It is suggested that from here they may have later migrated to Sardinia. Guido suggests that if a "few dominating leaders arrived as heroes only a few centuries before Phoenician trading posts were established, several features of Sardinian prehistory might be explained as innovations introduced by them: oriental types of armour, and fighting perpetuated in the bronze representation of warriors several centuries later; the arrival of the Cypriot copper ingots of the Serra Ilixi type; the sudden advance in and inventiveness of design of the Sardinian nuraghes themselves at about the turn of the first Millennium; the introduction of certain religious practices such as the worship of water in sacred wells - if this fact was not introduced by the Phoenician settlers".
Weapons and armour similar to those of the Sherden were found in Sardinia dating only to several centuries after the period of the Sea Peoples (although recent discoveries such as the arsenical bronze swords of "Sant'Iroxi" dating back to the 1600 BC and the nuragic bronze sculptures depicting warriors dating back to 1200 BC, suggest that the peoples of Sardinia actually used those types of armors and weapons since the mid-to-late 2nd millennium BC). If the theory that the Sherden moved to Sardinia only after their defeat by Ramesses III is true, then it could be inferred from this that the finds in Sardinia are survivals of earlier types of weapons and armour. On the other hand, if the Sherden only moved into the Western Mediterranean in the ninth century, associated perhaps with the movement of early Etruscans and even Phoenician seafaring peoples into the Western Mediterranean at that time, it would remain unknown where they were located between the period of the Sea Peoples and their eventual appearance in Sardinia.
These theoretical coincidences (enforced, as said, by linguistic considerations) could suggest that a group of skilled sailors left the Eastern Mediterranean and established themselves in Sardinia. They very probably would have encountered some resistance on their way there. It is also possible that they were explorers. If so, it is likely that only a warrior people like the Sherden could have organized such an expedition.
The theory that postulate a migration of peoples from the Eastern Mediterranean into Sardinia during the Late Bronze Age had been firmly rejected by Italian archaeologists like Massimo Pallottino and, more recently, Giovanni Ugas, that identifies the Sherden with the indigenous Nuragic peoples. Giovanni Lilliu noted that the period in which the Sherden are mentioned in the Egyptian sources coincides with the apogee of the Nuragic civilization. In 2010 nuragic pottery had been found at Kokkinokremnos, Cyprus, a site attributed to the Sea Peoples.

</doc>
