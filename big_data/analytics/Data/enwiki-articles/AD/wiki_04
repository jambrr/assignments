<doc id="18878" url="https://en.wikipedia.org/wiki?curid=18878" title="Monopoly">
Monopoly

A monopoly (from Greek μόνος "mónos" ("alone" or "single") and πωλεῖν "pōleîn" ("to sell")) exists when a specific person or enterprise is the only supplier of a particular commodity (this contrasts with a monopsony which relates to a single entity's control of a market to purchase a good or service, and with oligopoly which consists of a few entities dominating an industry). Monopolies are thus characterized by a lack of economic competition to produce the good or service, a lack of viable substitute goods, and the possibility of a high monopoly price well above the firm's marginal cost that leads to a high monopoly profit. The verb "monopolise" or "monopolize" refers to the "process" by which a company gains the ability to raise prices or exclude competitors. In economics, a monopoly is a single seller. In law, a monopoly is a business entity that has significant market power, that is, the power to charge overly high prices. Although monopolies may be big businesses, size is not a characteristic of a monopoly. A small business may still have the power to raise prices in a small industry (or market).
A monopoly is distinguished from a monopsony, in which there is only one "buyer" of a product or service; a monopoly may also have monopsony control of a sector of a market. Likewise, a monopoly should be distinguished from a cartel (a form of oligopoly), in which several providers act together to coordinate services, prices or sale of goods. Monopolies, monopsonies and oligopolies are all situations such that one or a few of the entities have market power and therefore interact with their customers (monopoly), suppliers (monopsony) and the other companies (oligopoly) in ways that leave market interactions distorted.
Monopolies can be established by a government, form naturally, or form by integration.
In many jurisdictions, competition laws restrict monopolies. Holding a dominant position or a monopoly of a market is often not illegal in itself, however certain categories of behavior can be considered abusive and therefore incur legal sanctions when business is dominant. A government-granted monopoly or "legal monopoly", by contrast, is sanctioned by the state, often to provide an incentive to invest in a risky venture or enrich a domestic interest group. Patents, copyrights, and trademarks are sometimes used as examples of government-granted monopolies. The government may also reserve the venture for itself, thus forming a government monopoly.
Market structures.
In economics, the idea of monopoly will be important for the study of management structures, which directly concerns normative aspects of economic competition, and provides the basis for topics such as industrial organization and economics of regulation. There are four basic types of market structures by traditional economic analysis: perfect competition, monopolistic competition, oligopoly and monopoly. A monopoly is a structure in which a single supplier produces and sells a given product. If there is a single seller in a certain industry and there are not any close substitutes for the product, then the market structure is that of a "pure monopoly". Sometimes, there are many sellers in an industry and/or there exist many close substitutes for the goods being produced, but nevertheless companies retain some market power. This is termed monopolistic competition, whereas in oligopoly the companies interact strategically.
In general, the main results from this theory compare price-fixing methods across market structures, analyze the effect of a certain structure on welfare, and vary technological/demand assumptions in order to assess the consequences for an abstract model of society. Most economic textbooks follow the practice of carefully explaining the "perfect competition" model, mainly because of its usefulness to understand "departures" from it (the so-called "imperfect competition" models).
The boundaries of what constitutes a market and what doesn't are relevant distinctions to make in economic analysis. In a general equilibrium context, a good is a specific concept entangling geographical and time-related characteristics ("grapes sold during October 2009 in Moscow" is a different good from "grapes sold during October 2009 in New York"). Most studies of market structure relax a little their definition of a good, allowing for more flexibility at the identification of substitute-goods. Therefore, one can find an economic analysis of the market of "grapes in Russia", for example, which is not a market in the strict sense of general equilibrium theory monopoly.
Sources of monopoly power.
Monopolies derive their market power from barriers to entry – circumstances that prevent or greatly impede a potential competitor's ability to compete in a market. There are three major types of barriers to entry: economic, legal and deliberate.
In addition to barriers to entry and competition, barriers to exit may be a source of market power. Barriers to exit are market conditions that make it difficult or expensive for a company to end its involvement with a market. Great liquidation costs are a primary barrier for exiting. Market exit and shutdown are separate events. The decision whether to shut down or operate is not affected by exit barriers. A company will shut down if price falls below minimum average variable costs.
Monopoly versus competitive markets.
While monopoly and perfect competition mark the extremes of market structures there is some similarity. The cost functions are the same. Both monopolies and perfectly competitive (PC) companies minimize cost and maximize profit. The shutdown decisions are the same. Both are assumed to have perfectly competitive factors markets. There are distinctions, some of the more important of which are as follows:
The most significant distinction between a PC company and a monopoly is that the monopoly has a downward-sloping demand curve rather than the "perceived" perfectly elastic curve of the PC company. Practically all the variations mentioned above relate to this fact. If there is a downward-sloping demand curve then by necessity there is a distinct marginal revenue curve. The implications of this fact are best made manifest with a linear demand curve. Assume that the inverse demand curve is of the form x = a − by. Then the total revenue curve is TR = ay − by2 and the marginal revenue curve is thus MR = a − 2by. From this several things are evident. First the marginal revenue curve has the same y intercept as the inverse demand curve. Second the slope of the marginal revenue curve is twice that of the inverse demand curve. Third the x intercept of the marginal revenue curve is half that of the inverse demand curve. What is not quite so evident is that the marginal revenue curve is below the inverse demand curve at all points. Since all companies maximise profits by equating MR and MC it must be the case that at the profit-maximizing quantity MR and MC are less than price, which further implies that a monopoly produces less quantity at a higher price than if the market were perfectly competitive.
The fact that a monopoly has a downward-sloping demand curve means that the relationship between total revenue and output for a monopoly is much different than that of competitive companies. Total revenue equals price times quantity. A competitive company has a perfectly elastic demand curve meaning that total revenue is proportional to output. Thus the total revenue curve for a competitive company is a ray with a slope equal to the market price. A competitive company can sell all the output it desires at the market price. For a monopoly to increase sales it must reduce price. Thus the total revenue curve for a monopoly is a parabola that begins at the origin and reaches a maximum value then continuously decreases until total revenue is again zero. Total revenue has its maximum value when the slope of the total revenue function is zero. The slope of the total revenue function is marginal revenue. So the revenue maximizing quantity and price occur when MR = 0. For example, assume that the monopoly’s demand function is P = 50 − 2Q. The total revenue function would be TR = 50Q − 2Q2 and marginal revenue would be 50 − 4Q. Setting marginal revenue equal to zero we have
So the revenue maximizing quantity for the monopoly is 12.5 units and the revenue maximizing price is 25.
A company with a monopoly does not experience price pressure from competitors, although it may experience pricing pressure from potential competition. If a company increases prices too much, then others may enter the market if they are able to provide the same good, or a substitute, at a lesser price. The idea that monopolies in markets with easy entry need not be regulated against is known as the "revolution in monopoly theory".
A monopolist can extract only one premium, and getting into complementary markets does not pay. That is, the total profits a monopolist could earn if it sought to leverage its monopoly in one market by monopolizing a complementary market are equal to the extra profits it could earn anyway by charging more for the monopoly product itself. However, the one monopoly profit theorem is not true if customers in the monopoly good are stranded or poorly informed, or if the tied good has high fixed costs.
A pure monopoly has the same economic rationality of perfectly competitive companies, i.e. to optimise a profit function given some constraints. By the assumptions of increasing marginal costs, exogenous inputs' prices, and control concentrated on a single agent or entrepreneur, the optimal decision is to equate the marginal cost and marginal revenue of production. Nonetheless, a pure monopoly can – unlike a competitive company – alter the market price for its own convenience: a decrease of production results in a higher price. In the economics' jargon, it is said that pure monopolies have "a downward-sloping demand". An important consequence of such behaviour is worth noticing: typically a monopoly selects a higher price and lesser quantity of output than a price-taking company; again, less is available at a higher price.
The inverse elasticity rule.
A monopoly chooses that price that maximizes the difference between total revenue and total cost. The basic markup rule can be expressed as (P − MC)/P = 1/PED. The markup rules indicate that the ratio between profit margin and the price is inversely proportional to the price elasticity of demand. The implication of the rule is that the more elastic the demand for the product the less pricing power the monopoly has.
Market power.
Market power is the ability to increase the product's price above marginal cost without losing all customers. Perfectly competitive (PC) companies have zero market power when it comes to setting prices. All companies of a PC market are price takers. The price is set by the interaction of demand and supply at the market or aggregate level. Individual companies simply take the price determined by the market and produce that quantity of output that maximizes the company's profits. If a PC company attempted to increase prices above the market level all its customers would abandon the company and purchase at the market price from other companies. A monopoly has considerable although not unlimited market power. A monopoly has the power to set prices or quantities although not both. A monopoly is a price maker. The monopoly is the market and prices are set by the monopolist based on his circumstances and not the interaction of demand and supply. The two primary factors determining monopoly market power are the company's demand curve and its cost structure.
Market power is the ability to affect the terms and conditions of exchange so that the price of a product is set by a single company (price is not imposed by the market as in perfect competition). Although a monopoly's market power is great it is still limited by the demand side of the market. A monopoly has a negatively sloped demand curve, not a perfectly inelastic curve. Consequently, any price increase will result in the loss of some customers.
Price discrimination.
Price discrimination allows a monopolist to increase its profit by charging higher prices for identical goods to those who are willing or able to pay more. For example, most economic textbooks cost more in the United States than in developing countries like Ethiopia. In this case, the publisher is using its government-granted copyright monopoly to price discriminate between the generally wealthier American economics students and the generally poorer Ethiopian economics students. Similarly, most patented medications cost more in the U.S. than in other countries with a (presumed) poorer customer base. Typically, a high general price is listed, and various market segments get varying discounts. This is an example of framing to make the process of charging some people higher prices more socially acceptable. Perfect price discrimination would allow the monopolist to charge each customer the exact maximum amount he would be willing to pay. This would allow the monopolist to extract all the consumer surplus of the market. While such perfect price discrimination is a theoretical construct, advances in information technology and micromarketing may bring it closer to the realm of possibility
It is important to realize that partial price discrimination can cause some customers who are inappropriately pooled with high price customers to be excluded from the market. For example, a poor student in the U.S. might be excluded from purchasing an economics textbook at the U.S. price, which the student may have been able to purchase at the Ethiopian price'. Similarly, a wealthy student in Ethiopia may be able to or willing to buy at the U.S. price, though naturally would hide such a fact from the monopolist so as to pay the reduced third world price. These are deadweight losses and decrease a monopolist's profits. As such, monopolists have substantial economic interest in improving their market information and "market segmenting".
There is important information for one to remember when considering the monopoly model diagram (and its associated conclusions) displayed here. The result that monopoly prices are higher, and production output lesser, than a competitive company follow from a requirement that the monopoly not charge different prices for different customers. That is, the monopoly is restricted from engaging in price discrimination (this is termed first degree price discrimination, such that all customers are charged the same amount). If the monopoly were permitted to charge individualised prices (this is termed third degree price discrimination), the quantity produced, and the price charged to the "marginal" customer, would be identical to that of a competitive company, thus eliminating the deadweight loss; however, all gains from trade (social welfare) would accrue to the monopolist and none to the consumer. In essence, every consumer would be indifferent between (1) going completely without the product or service and (2) being able to purchase it from the monopolist.
As long as the price elasticity of demand for most customers is less than one in absolute value, it is advantageous for a company to increase its prices: it receives more money for fewer goods. With a price increase, price elasticity tends to increase, and in the optimum case above it will be greater than one for most customers.
A company maximizes profit by selling where marginal revenue equals marginal cost. A company that does not engage in price discrimination will charge the profit maximizing price, P*, to all its customers. In such circumstances there are customers who would be willing to pay a higher price than P* and those who will not pay P* but would buy at a lower price. A price discrimination strategy is to charge less price sensitive buyers a higher price and the more price sensitive buyers a lower price. Thus additional revenue is generated from two sources. The basic problem is to identify customers by their willingness to pay.
The purpose of price discrimination is to transfer consumer surplus to the producer. Consumer surplus is the difference between the value of a good to a consumer and the price the consumer must pay in the market to purchase it. Price discrimination is not limited to monopolies.
Market power is a company’s ability to increase prices without losing all its customers. Any company that has market power can engage in price discrimination. Perfect competition is the only market form in which price discrimination would be impossible (a perfectly competitive company has a perfectly elastic demand curve and has zero market power).
There are three forms of price discrimination. First degree price discrimination charges each consumer the maximum price the consumer is willing to pay. Second degree price discrimination involves quantity discounts. Third degree price discrimination involves grouping consumers according to willingness to pay as measured by their price elasticities of demand and charging each group a different price. Third degree price discrimination is the most prevalent type.
There are three conditions that must be present for a company to engage in successful price discrimination. First, the company must have market power. Second, the company must be able to sort customers according to their willingness to pay for the good. Third, the firm must be able to prevent resell.
A company must have some degree of market power to practice price discrimination. Without market power a company cannot charge more than the market price. Any market structure characterized by a downward sloping demand curve has market power – monopoly, monopolistic competition and oligopoly. The only market structure that has no market power is perfect competition.
A company wishing to practice price discrimination must be able to prevent middlemen or brokers from acquiring the consumer surplus for themselves. The company accomplishes this by preventing or limiting resale. Many methods are used to prevent resale. For example, persons are required to show photographic identification and a boarding pass before boarding an airplane. Most travelers assume that this practice is strictly a matter of security. However, a primary purpose in requesting photographic identification is to confirm that the ticket purchaser is the person about to board the airplane and not someone who has repurchased the ticket from a discount buyer.
The inability to prevent resale is the largest obstacle to successful price discrimination. Companies have however developed numerous methods to prevent resale. For example, universities require that students show identification before entering sporting events. Governments may make it illegal to resale tickets or products. In Boston, Red Sox baseball tickets can only be resold legally to the team.
The three basic forms of price discrimination are first, second and third degree price discrimination. In "first degree price discrimination" the company charges the maximum price each customer is willing to pay. The maximum price a consumer is willing to pay for a unit of the good is the reservation price. Thus for each unit the seller tries to set the price equal to the consumer’s reservation price. Direct information about a consumer’s willingness to pay is rarely available. Sellers tend to rely on secondary information such as where a person lives (postal codes); for example, catalog retailers can use mail high-priced catalogs to high-income postal codes. First degree price discrimination most frequently occurs in regard to professional services or in transactions involving direct buyer/seller negotiations. For example, an accountant who has prepared a consumer's tax return has information that can be used to charge customers based on an estimate of their ability to pay.
In "second degree price discrimination" or quantity discrimination customers are charged different prices based on how much they buy. There is a single price schedule for all consumers but the prices vary depending on the quantity of the good bought. The theory of second degree price discrimination is a consumer is willing to buy only a certain quantity of a good at a given price. Companies know that consumer’s willingness to buy decreases as more units are purchased. The task for the seller is to identify these price points and to reduce the price once one is reached in the hope that a reduced price will trigger additional purchases from the consumer. For example, sell in unit blocks rather than individual units.
In "third degree price discrimination" or multi-market price discrimination the seller divides the consumers into different groups according to their willingness to pay as measured by their price elasticity of demand. Each group of consumers effectively becomes a separate market with its own demand curve and marginal revenue curve. The firm then attempts to maximize profits in each segment by equating MR and MC, Generally the company charges a higher price to the group with a more price inelastic demand and a relatively lesser price to the group with a more elastic demand. Examples of third degree price discrimination abound. Airlines charge higher prices to business travelers than to vacation travelers. The reasoning is that the demand curve for a vacation traveler is relatively elastic while the demand curve for a business traveler is relatively inelastic. Any determinant of price elasticity of demand can be used to segment markets. For example, seniors have a more elastic demand for movies than do young adults because they generally have more free time. Thus theaters will offer discount tickets to seniors.
Example.
Assume that by a uniform pricing system the monopolist would sell five units at a price of $10 per unit. Assume that his marginal cost is $5 per unit. Total revenue would be $50, total costs would be $25 and profits would be $25. If the monopolist practiced price discrimination he would sell the first unit for $50 the second unit for $40 and so on. Total revenue would be $150, his total cost would be $25 and his profit would be $125.00. Several things are worth noting. The monopolist acquires all the consumer surplus and eliminates practically all the deadweight loss because he is willing to sell to anyone who is willing to pay at least the marginal cost. Thus the price discrimination promotes efficiency. Secondly, by the pricing scheme price = average revenue and equals marginal revenue. That is the monopolist behaving like a perfectly competitive company. Thirdly, the discriminating monopolist produces a larger quantity than the monopolist operating by a uniform pricing scheme.
Classifying customers.
Successful price discrimination requires that companies separate consumers according to their willingness to buy. Determining a customer's willingness to buy a good is difficult. Asking consumers directly is fruitless: consumers don't know, and to the extent they do they are reluctant to share that information with marketers. The two main methods for determining willingness to buy are observation of personal characteristics and consumer actions. As noted information about where a person lives (postal codes), how the person dresses, what kind of car he or she drives, occupation, and income and spending patterns can be helpful in classifying.
Monopoly and efficiency.
According to the standard model, in which a monopolist sets a single price for all consumers, the monopolist will sell a lesser quantity of goods at a higher price than would companies by perfect competition. Because the monopolist ultimately forgoes transactions with consumers who value the product or service more than its price, monopoly pricing creates a deadweight loss referring to potential gains that went neither to the monopolist nor to consumers. Given the presence of this deadweight loss, the combined surplus (or wealth) for the monopolist and consumers is necessarily less than the total surplus obtained by consumers by perfect competition. Where efficiency is defined by the total gains from trade, the monopoly setting is less efficient than perfect competition.
It is often argued that monopolies tend to become less efficient and less innovative over time, becoming "complacent", because they do not have to be efficient or innovative to compete in the marketplace. Sometimes this very loss of psychological efficiency can increase a potential competitor's value enough to overcome market entry barriers, or provide incentive for research and investment into new alternatives. The theory of contestable markets argues that in some circumstances (private) monopolies are forced to behave "as if" there were competition because of the risk of losing their monopoly to new entrants. This is likely to happen when a market's barriers to entry are low. It might also be because of the availability in the longer term of substitutes in other markets. For example, a canal monopoly, while worth a great deal during the late 18th century United Kingdom, was worth much less during the late 19th century because of the introduction of railways as a substitute.
Natural monopoly.
A natural monopoly is an organization that experiences increasing returns to scale over the relevant range of output and relatively high fixed costs. A natural monopoly occurs where the average cost of production "declines throughout the relevant range of product demand". The relevant range of product demand is where the average cost curve is below the demand curve. When this situation occurs, it is always cheaper for one large company to supply the market than multiple smaller companies; in fact, absent government intervention in such markets, will naturally evolve into a monopoly. An early market entrant that takes advantage of the cost structure and can expand rapidly can exclude smaller companies from entering and can drive or buy out other companies. A natural monopoly suffers from the same inefficiencies as any other monopoly. Left to its own devices, a profit-seeking natural monopoly will produce where marginal revenue equals marginal costs. Regulation of natural monopolies is problematic. Fragmenting such monopolies is by definition inefficient. The most frequently used methods dealing with natural monopolies are government regulations and public ownership. Government regulation generally consists of regulatory commissions charged with the principal duty of setting prices.
To reduce prices and increase output, regulators often use average cost pricing. By average cost pricing, the price and quantity are determined by the intersection of the average cost curve and the demand curve. This pricing scheme eliminates any positive economic profits since price equals average cost. Average-cost pricing is not perfect. Regulators must estimate average costs. Companies have a reduced incentive to lower costs. Regulation of this type has not been limited to natural monopolies. Average-cost pricing does also have some disadvantages. By setting price equal to the intersection of the demand curve and the average total cost curve, the firm's output is allocatively inefficient as the price exceeds the marginal cost (which is the output quantity for a perfectly competitive and allocatively efficient market).
Government-granted monopoly.
A government-granted monopoly (also called a "de jure monopoly") is a form of coercive monopoly by which a government grants exclusive privilege to a private individual or company to be the sole provider of a commodity; potential competitors are excluded from the market by law, regulation, or other mechanisms of government enforcement.
Monopolist shutdown rule.
A monopolist should shut down when price is less than average variable cost for every output level – in other words where the demand curve is entirely below the average variable cost curve. Under these circumstances at the profit maximum level of output (MR = MC) average revenue would be less than average variable costs and the monopolists would be better off shutting down in the short term.
Breaking up monopolies.
In a free market, monopolies can be ended at any time by new competition, breakaway businesses, or consumers seeking alternatives. In a highly regulated market environment a government will often either regulate the monopoly, convert it into a publicly owned monopoly environment, or forcibly fragment it (see Antitrust law and trust busting). Public utilities, often being naturally efficient with only one operator and therefore less susceptible to efficient breakup, are often strongly regulated or publicly owned. American Telephone & Telegraph (AT&T) and Standard Oil are debatable examples of the breakup of a private monopoly by government: When AT&T, a monopoly previously protected by force of law, was broken up into various components in 1984, MCI, Sprint, and other companies were able to compete effectively in the long distance phone market.
Law.
The existence of a very high market share does not always mean consumers are paying excessive prices since the threat of new entrants to the market can restrain a high-market-share company's price increases. Competition law does not make merely having a monopoly illegal, but rather abusing the power a monopoly may confer, for instance through exclusionary practices (i.e. pricing high just because you are the only one around.) It may also be noted that it is illegal to try to obtain a monopoly, by practices of buying out the competition, or equal practices. If one occurs naturally, such as a competitor going out of business, or lack of competition, it is not illegal until such time as the monopoly holder abuses the power.
First it is necessary to determine whether a company is dominant, or whether it behaves "to an appreciable extent independently of its competitors, customers and ultimately of its consumer". As with collusive conduct, market shares are determined with reference to the particular market in which the company and product in question is sold. The Herfindahl-Hirschman Index (HHI) is sometimes used to assess how competitive an industry is. In the US, the merger guidelines state that a post-merger HHI below 1000 is viewed as unconcentrated while HHIs above that will provoke further review.
By European Union law, very large market shares raise a presumption that a company is dominant, which may be rebuttable. If a company has a dominant position, then there is "a special responsibility not to allow its conduct to impair competition on the common market". The lowest yet market share of a company considered "dominant" in the EU was 39.7%.
Certain categories of abusive conduct are usually prohibited by a country's legislation. The main recognised categories are:
Despite wide agreement that the above constitute abusive practices, there is some debate about whether there needs to be a causal connection between the dominant position of a company and its actual abusive conduct. Furthermore, there has been some consideration of what happens when a company merely attempts to abuse its dominant position.
Historical monopolies.
Origin.
The term "monopoly" first appears in Aristotle's "Politics". Aristotle describes Thales of Miletus's cornering of the market in olive presses as a monopoly ("μονοπωλίαν").
The meaning and understanding of the English word 'monopoly' has changed over the years.
Monopolies of resources.
Salt.
Vending of common salt (sodium chloride) was historically a natural monopoly. Until recently, a combination of strong sunshine and low humidity or an extension of peat marshes was necessary for producing salt from the sea, the most plentiful source. Changing sea levels periodically caused salt "famines" and communities were forced to depend upon those who controlled the scarce inland mines and salt springs, which were often in hostile areas (e.g. the Sahara desert) requiring well-organised security for transport, storage, and distribution.
The Salt Commission was a legal monopoly in China. Formed in 758, the Commission controlled salt production and sales in order to raise tax revenue for the Tang Dynasty.
The "Gabelle" was a notoriously high tax levied upon salt in the Kingdom of France. The much-hated levy had a role in the beginning of the French Revolution, when strict legal controls specified who was allowed to sell and distribute salt. First instituted in 1286, the Gabelle was not permanently abolished until 1945.
Coal.
Robin Gollan argues in "The Coalminers of New South Wales" that anti-competitive practices developed in the coal industry of Australia's Newcastle as a result of the business cycle. The monopoly was generated by formal meetings of the local management of coal companies agreeing to fix a minimum price for sale at dock. This collusion was known as "The Vend". The Vend ended and was reformed repeatedly during the late 19th century, ending by recession in the business cycle. "The Vend" was able to maintain its monopoly due to trade union assistance, and material advantages (primarily coal geography). During the early 20th century, as a result of comparable monopolistic practices in the Australian coastal shipping business, the Vend developed as an informal and illegal collusion between the steamship owners and the coal industry, eventually resulting in the High Court case Adelaide Steamship Co. Ltd v. R. & AG.
Petroleum.
Standard Oil was an American oil producing, transporting, refining, and marketing company. Established in 1870, it became the largest oil refiner in the world. John D. Rockefeller was a founder, chairman and major shareholder. The company was an innovator in the development of the business trust. The Standard Oil trust streamlined production and logistics, lowered costs, and undercut competitors. "Trust-busting" critics accused Standard Oil of using aggressive pricing to destroy competitors and form a monopoly that threatened consumers. Its controversial history as one of the world's first and largest multinational corporations ended in 1911, when the United States Supreme Court ruled that Standard was an illegal monopoly. The Standard Oil trust was dissolved into 33 smaller companies; two of its surviving "child" companies are ExxonMobil and the Chevron Corporation.
Steel.
U.S. Steel has been accused of being a monopoly. J. P. Morgan and Elbert H. Gary founded U.S. Steel in 1901 by combining Andrew Carnegie's Carnegie Steel Company with Gary's Federal Steel Company and William Henry "Judge" Moore's National Steel Company. At one time, U.S. Steel was the largest steel producer and largest corporation in the world. In its first full year of operation, U.S. Steel made 67 percent of all the steel produced in the United States. However, U.S. Steel's share of the expanding market slipped to 50 percent by 1911, and anti-trust prosecution that year failed.
Diamonds.
De Beers settled charges of price fixing in the diamond trade in the 2000s. De Beers is well known for its monopoloid practices throughout the 20th century, whereby it used its dominant position to manipulate the international diamond market. The company used several methods to exercise this control over the market. Firstly, it convinced independent producers to join its single channel monopoly, it flooded the market with diamonds similar to those of producers who refused to join the cartel, and lastly, it purchased and stockpiled diamonds produced by other manufacturers in order to control prices through limiting supply.
In 2000, the De Beers business model changed due to factors such as the decision by producers in Russia, Canada and Australia to distribute diamonds outside the De Beers channel, as well as rising awareness of blood diamonds that forced De Beers to "avoid the risk of bad publicity" by limiting sales to its own mined products. De Beers' market share by value fell from as high as 90% in the 1980s to less than 40% in 2012, having resulted in a more fragmented diamond market with more transparency and greater liquidity.
In November 2011 the Oppenheimer family announced its intention to sell the entirety of its 40% stake in De Beers to Anglo American plc thereby increasing Anglo American's ownership of the company to 85%. The transaction was worth £3.2 billion ($5.1 billion) in cash and ended the Oppenheimer dynasty's 80-year ownership of De Beers.
Utilities.
A public utility (or simply "utility") is an organization or company that maintains the infrastructure for a public service or provides a set of services for public consumption. Common examples of utilities are electricity, natural gas, water, sewage, cable television, and telephone. In the United States, public utilities are often natural monopolies because the infrastructure required to produce and deliver a product such as electricity or water is very expensive to build and maintain.
Western Union was criticized as a "price gouging" monopoly in the late 19th century.
American Telephone & Telegraph was a telecommunications giant. AT&T was broken up in 1984.
In the case of Telecom New Zealand, local loop unbundling was enforced by central government.
Telkom is a semi-privatised, part state-owned South African telecommunications company.
Deutsche Telekom is a former state monopoly, still partially state owned. Deutsche Telekom currently monopolizes high-speed VDSL broadband network.
The Long Island Power Authority (LIPA) provided electric service to over 1.1 million customers in Nassau and Suffolk counties of New York, and the Rockaway Peninsula in Queens.
The Comcast Corporation is the largest mass media and communications company in the world by revenue. It is the largest cable company and home Internet service provider in the United States, and the nation's third largest home telephone service provider. Comcast has a monopoly in Boston, Philadelphia, Chicago, and many other small towns across the US.
Transportation.
The United Aircraft and Transport Corporation was an aircraft manufacturer holding company that was forced to divest itself of airlines in 1934.
Iarnród Éireann, the Irish Railway authority, is a current monopoly as Ireland does not have the size for more companies.
The Long Island Rail Road (LIRR) was founded in 1834, and since the mid-1800s has provided train service between Long Island and New York City. In the 1870s, LIRR became the sole railroad in that area through a series of acquisitions and consolidations. In 2013, the LIRR's commuter rail system is the busiest commuter railroad in North America, serving nearly 335,000 passengers daily.
Foreign trade.
Dutch East India Company was created as a legal trading monopoly in 1602. The "Vereenigde Oost-Indische Compagnie" enjoyed huge profits from its spice monopoly through most of the 17th century.
The British East India Company was created as a legal trading monopoly in 1600. The East India Company was formed for pursuing trade with the East Indies but ended up trading mainly with the Indian subcontinent, North-West Frontier Province, and Balochistan. The Company traded in basic commodities, which included cotton, silk, indigo dye, salt, saltpetre, tea and opium.
Professional sports.
Major League Baseball survived U.S. anti-trust litigation in 1922, though its special status is still in dispute as of 2009.
The National Football League survived anti-trust lawsuit in the 1960s but was convicted of being an illegal monopoly in the 1980s.
Countering monopolies.
According to professor Milton Friedman, laws against monopolies cause more harm than good, but unnecessary monopolies should be countered by removing tariffs and other regulation that upholds monopolies.
However, professor Steve H. Hanke believes that although private monopolies are more efficient than public ones, often by a factor of two, sometimes private natural monopolies, such as local water distribution, should be regulated (not prohibited) by, e.g., price auctions.
Thomas DiLorenzo asserts, however, that during the early days of utility companies where there was little regulation, there were no natural monopolies and there was competition. Only when companies realized that they could gain power through government did monopolies begin to form.

</doc>
<doc id="18879" url="https://en.wikipedia.org/wiki?curid=18879" title="Massachusetts Institute of Technology">
Massachusetts Institute of Technology

The Massachusetts Institute of Technology (MIT) is a private research university in Cambridge, Massachusetts. Founded in 1861 in response to the increasing industrialization of the United States, MIT adopted a European polytechnic university model and stressed laboratory instruction in applied science and engineering. Researchers worked on computers, radar, and inertial guidance during World War II and the Cold War. Post-war defense research contributed to the rapid expansion of the faculty and campus under James Killian. The current campus opened in 1916 and extends over along the northern bank of the Charles River basin.
MIT, with five schools and one college which contain a total of 32 departments, is often cited as among the world's top universities. The Institute is traditionally known for its research and education in the physical sciences and engineering, and more recently in biology, economics, linguistics, and management as well. The "Engineers" sponsor 31 sports, most teams of which compete in the NCAA Division III's New England Women's and Men's Athletic Conference; the Division I rowing programs compete as part of the EARC and EAWRC.
, 85 Nobel laureates, 52 National Medal of Science recipients, 65 Marshall Scholars, 45 Rhodes Scholars, 38 MacArthur Fellows, 34 astronauts, 19 Turing award winners, and 6 Fields Medalists have been affiliated with MIT. The school has a strong entrepreneurial culture, and the aggregated revenues of companies founded by MIT alumni would rank as the eleventh-largest economy in the world.
History.
Foundation and vision.
In 1859, a proposal was submitted to the Massachusetts General Court to use newly filled lands in Back Bay, Boston for a "Conservatory of Art and Science", but the proposal failed. A charter for the incorporation of the Massachusetts Institute of Technology, proposed by William Barton Rogers, was signed by the governor of Massachusetts on April 10, 1861.
Rogers, a professor from the University of Virginia, wanted to establish an institution to address rapid scientific and technological advances. He did not wish to found a professional school, but a combination with elements of both professional and liberal education, proposing that:
The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws.
The Rogers Plan reflected the German research university model, emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories.
Early developments.
Two days after the charter was issued, the first battle of the Civil War broke out. After a long delay through the war years, MIT's first classes were held in the Mercantile Building in Boston in 1865. The new institute was founded as part of the Morrill Land-Grant Colleges Act to fund institutions "to promote the liberal and practical education of the industrial classes", and was a land-grant school. In 1863 under the same act, the Commonwealth of Massachusetts founded the Massachusetts Agricultural College, which developed as the University of Massachusetts Amherst. In 1866, the proceeds from land sales went toward new buildings in the Back Bay.
MIT was informally called "Boston Tech". The institute adopted the European polytechnic university model and emphasized laboratory instruction from an early date. Despite chronic financial problems, the institute saw growth in the last two decades of the 19th century under President Francis Amasa Walker. Programs in electrical, chemical, marine, and sanitary engineering were introduced, new buildings were built, and the size of the student body increased to more than one thousand.
The curriculum drifted to a vocational emphasis, with less focus on theoretical science. The fledgling school still suffered from chronic financial shortages which diverted the attention of the MIT leadership. During these "Boston Tech" years, MIT faculty and alumni rebuffed Harvard University president (and former MIT faculty) Charles W. Eliot's repeated attempts to merge MIT with Harvard College's Lawrence Scientific School. There would be at least six attempts to absorb MIT into Harvard. In its cramped Back Bay location, MIT could not afford to expand its overcrowded facilities, driving a desperate search for a new campus and funding. Eventually the MIT Corporation approved a formal agreement to merge with Harvard, over the vehement objections of MIT faculty, students, and alumni. However, a 1917 decision by the Massachusetts Supreme Judicial Court effectively put an end to the merger scheme.
In 1916, the MIT administration and the MIT charter crossed the Charles River on the ceremonial barge "Bucentaur" built for the occasion, to signify MIT's move to a spacious new campus largely consisting of filled land on a mile-long tract along the Cambridge side of the Charles River. The neoclassical "New Technology" campus was designed by William W. Bosworth and had been funded largely by anonymous donations from a mysterious "Mr. Smith", starting in 1912. In January 1920, the donor was revealed to be the industrialist George Eastman of Rochester, New York, who had invented methods of film production and processing, and founded Eastman Kodak. Between 1912 and 1920, Eastman donated $20 million ($ million in 2015 dollars) in cash and Kodak stock to MIT.
Curricular reforms.
In the 1930s, President Karl Taylor Compton and Vice-President (effectively Provost) Vannevar Bush emphasized the importance of pure sciences like physics and chemistry and reduced the vocational practice required in shops and drafting studios. The Compton reforms "renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering." Unlike Ivy League schools, MIT catered more to middle-class families, and depended more on tuition than on endowments or grants for its funding. The school was elected to the Association of American Universities in 1934.
Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that "the Institute is widely conceived as basically a vocational school", a "partly unjustified" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities. The School of Humanities, Arts, and Social Sciences and the MIT Sloan School of Management were formed in 1950 to compete with the powerful Schools of Science and Engineering. Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs. The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more humanistically oriented presidents Howard W. Johnson and Jerome Wiesner between 1966 and 1980.
Defense research.
MIT's involvement in military research surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT. Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly affected both the war and subsequent research in the area. Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory; the development of a digital computer for flight simulations under Project Whirlwind; and high-speed and high-altitude photography under Harold Edgerton. By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush), employing nearly 4000 in the Radiation Laboratory alone and receiving in excess of $100 million ($ billion in 2015 dollars) before 1946. Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.
These activities affected MIT profoundly. A 1949 report noted the lack of "any great slackening in the pace of life at the Institute" to match the return to peacetime, remembering the "academic tranquility of the prewar years", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities. The faculty doubled and the graduate student body quintupled during the terms of Karl Taylor Compton, president of MIT between 1930 and 1948; James Rhyne Killian, president from 1948 to 1957; and Julius Adams Stratton, chancellor from 1952 to 1957, whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer simply benefited the industries with which it had worked for three decades, and it had developed closer working relationships with new patrons, philanthropic foundations and the federal government.
In late 1960s and early 1970s, student and faculty activists protested against the Vietnam War and MIT's defense research. The Union of Concerned Scientists was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research toward environmental and social problems. MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the Lincoln Laboratory facility in 1973 in response to the protests. The student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities. Johnson was seen to be highly successful in leading his institution to "greater strength and unity" after these times of turmoil.
Recent history.
MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies, students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like "Spacewar!" and created much of modern hacker slang and culture. Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology; the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee; the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002; and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.
MIT was named a sea-grant college in 1976 to support its programs in oceanography and marine sciences and was named a space-grant college in 1989 to support its aeronautics and astronautics programs. Despite diminishing government financial support over the past quarter century, MIT launched several successful development campaigns to significantly expand the campus: new dormitories and athletics buildings on west campus; the Tang Center for Management Education; several buildings in the northeast corner of campus supporting research into biology, brain and cognitive sciences, genomics, biotechnology, and cancer research; and a number of new "backlot" buildings on Vassar Street including the Stata Center. Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest. In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing global energy consumption.
In 2001, inspired by the open source and open access movements, MIT launched OpenCourseWare to make the lecture notes, problem sets, syllabuses, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed. While the cost of supporting and hosting the project is high, OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in at least six languages. In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its "MITx" program, for a modest fee. The "edX" online platform supporting MITx was initially developed in partnership with Harvard and its analogous "Harvardx" initiative. The courseware platform is open source, and other universities have already joined and added their own course content.
Three days after the Boston Marathon bombings of April 2013, MIT Police patrol officer Sean Collier was fatally shot by the suspects, setting off a violent manhunt that shut down the campus and much of the Boston metropolitan area for a day. One week later, Collier's memorial service was attended by more than 10,000 people, in a ceremony hosted by the MIT community with thousands of police officers from the New England region and Canada. On November 25, 2013, MIT announced the creation of the Collier Medal, to be awarded annually to "an individual or group that embodies the character and qualities that Officer Collier exhibited as a member of the MIT community and in all aspects of his life". The announcement further stated that "Future recipients of the award will include those whose contributions exceed the boundaries of their profession, those who have contributed to building bridges across the community, and those who consistently and selflessly perform acts of kindness".
Campus.
MIT's campus in the city of Cambridge spans approximately a mile along the north side of the Charles River basin. The campus is divided roughly in half by Massachusetts Avenue, with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the Harvard Bridge, which is known for being marked off in a non-standard unit of length – the smoot. 
The Kendall MBTA Red Line station is located on the northeastern edge of the campus, in Kendall Square. The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings, as well as socio-economically diverse residential neighborhoods. In early 2016, MIT presented its updated Kendall Square Initiative to the City of Cambridge, with plans for mixed-use educational, retail, residential, startup incubator, and office space in a dense high-rise transit-oriented development plan. The MIT Museum will eventually be moved immediately adjacent to a Kendall Square subway entrance, joining the List Visual Arts Center on the eastern end of the campus.
Each building at MIT has a number (possibly preceded by a "W", "N", "E", or "NW") designation and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings. Many of the buildings are connected above ground as well as through an extensive network of underground tunnels, providing protection from the Cambridge weather as well as a venue for roof and tunnel hacking.
MIT's on-campus nuclear reactor is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial, but MIT maintains that it is well-secured. In 1999 Bill Gates donated US$20 million to MIT for the construction of a computer laboratory named the "William H. Gates Building", and designed by architect Frank O. Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.
Other notable campus facilities include a pressurized wind tunnel and a towing tank for testing ship and ocean structure designs. MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering of campus.
In 2001, the Environmental Protection Agency sued MIT for violating Clean Water Act and Clean Air Act with regard to its hazardous waste storage and disposal procedures. MIT settled the suit by paying a $155,000 fine and launching three environmental projects. In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running alternative fuel campus shuttles, subsidizing public transportation passes, and building a low-emission cogeneration plant that serves most of the campus electricity, heating, and cooling requirements.
The MIT Police with state and local authorities, in the 2009-2011 period, have investigated reports of 12 forcible sex offenses, 6 robberies, 3 aggravated assaults, 164 burglaries, 1 case of arson, and 4 cases of motor vehicle theft on campus; affecting a community of around 22,000 students and employees.
Architecture.
MIT's School of Architecture, now the School of Architecture and Planning, was the first in the United States, and it has a history of commissioning progressive buildings. The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the "Maclaurin buildings" after Institute president Richard Maclaurin who oversaw their construction. Designed by William Welles Bosworth, these imposing buildings were built of reinforced concrete, a first for a non-industrial – much less university – building in the US. Bosworth's design was influenced by the City Beautiful Movement of the early 1900s, and features the Pantheon-esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where graduation ceremonies are held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers. The spacious Building 7 atrium at 77 Massachusetts Avenue is regarded as the entrance to the Infinite Corridor and the rest of the campus.
Alvar Aalto's Baker House (1947), Eero Saarinen's MIT Chapel and Kresge Auditorium (1955), and I.M. Pei's Green, Dreyfus, Landau, and Wiesner buildings represent high forms of post-war modernist architecture. More recent buildings like Frank Gehry's Stata Center (2004), Steven Holl's Simmons Hall (2002), Charles Correa's Building 46 (2005), and Fumihiko Maki's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus "starchitecture". These buildings have not always been well received; in 2010, "The Princeton Review" included MIT in a list of twenty schools whose campuses are "tiny, unsightly, or both".
Housing.
Undergraduates are guaranteed four-year housing in one of MIT's 12 undergraduate dormitories. Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters. Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the "Yale Daily News" staff's "The Insider's Guide to the Colleges, 2010", "The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture." MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families.
MIT has an active Greek and co-op housing system, including thirty-six fraternities, sororities, and independent living groups (FSILGs). , 98% of all undergraduates lived in MIT-affiliated housing; 54% of the men participated in fraternities and 20% of the women were involved in sororities. Most FSILGs are located across the river in Back Bay near where MIT was founded, and there is also a cluster of fraternities on MIT's West Campus that face the Charles River Basin. After the 1997 alcohol-related death of Scott Krueger, a new pledge at the Phi Gamma Delta fraternity, MIT required all freshmen to live in the dormitory system starting in 2002. Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy could not be implemented until Simmons Hall opened in that year.
Organization and administration.
MIT is chartered as a non-profit organization and is owned and governed by a privately appointed board of trustees known as the MIT Corporation. The current board consists of 43 members elected to five-year terms, 25 life members who vote until their 75th birthday, 3 elected officers (President, Treasurer, and Secretary), and 4 "ex officio" members (the president of the alumni association, the Governor of Massachusetts, the Massachusetts Secretary of Education, and the Chief Justice of the Massachusetts Supreme Judicial Court). The board is chaired by Robert Millard, a co-founder of 
L-3 Communications Holdings. The Corporation approves the budget, new programs, degrees and faculty appointments, and elects the President to serve as the chief executive officer of the university and preside over the Institute's faculty. MIT's endowment and other financial assets are managed through a subsidiary called MIT Investment Management Company (MITIMCo). Valued at $9.7 billion in 2011, MIT's endowment is the sixth-largest among American colleges and universities.
MIT has five schools (Science, Engineering, Architecture and Planning, Management, and Humanities, Arts, and Social Sciences) and one college (Whitaker College of Health Sciences and Technology), but no schools of law or medicine. While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs, the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President. The current president is L. Rafael Reif, who formerly served as provost under President Susan Hockfield, the first woman to hold the post.
Academics.
MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs. The university has been accredited by the New England Association of Schools and Colleges since 1929. MIT operates on a 4–1–4 academic calendar with the fall semester beginning after Labor Day and ending in mid-December, a 4-week "Independent Activities Period" in the month of January, and the spring semester beginning in early February and ending in late May.
MIT students refer to both their majors and classes using numbers or acronyms alone. Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is , while Linguistics and Philosophy is . Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; the introductory calculus-based classical mechanics course is simply "8.01" at MIT.
Undergraduate program.
The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and has been dubbed "most selective" by "U.S. News", admitting few transfer students and 8.0% of its applicants in the 2015 admissions cycle. MIT offers 44 undergraduate degrees across its five schools. In the 2010–2011 academic year, 1,161 bachelor of science degrees (abbreviated "SB") were granted, the only type of undergraduate degree MIT now awards. In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%). The largest undergraduate degree programs were in Electrical Engineering and Computer Science (), Computer Science and Engineering (), Mechanical Engineering (), Physics (), and Mathematics ().
All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs). The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be "communication-intensive", including "substantial instruction and practice in oral presentation". Finally, all students are required to complete a swimming test; non-varsity athletes must also take four quarters of physical education classes.
Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets ("p-sets"), and periodic quizzes or tests. While the pace and difficulty of MIT coursework has been compared to "drinking from a fire hose", the freshmen retention rate at MIT is similar other research universities. The "pass/no-record" grading system relieves some pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded. (Grading had previously been "pass/no record" all freshman year, but was amended for the Class of 2006 to prevent students from gaming the system by completing required major classes in their freshman year.) Also, freshmen may choose to join alternative learning communities, such as Experimental Study Group, Concourse, or Terrascope.
In 1969, Margaret MacVicar founded the Undergraduate Research Opportunities Program (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects ("UROPs") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly. A substantial majority of undergraduates participate. Students often become published, file patent applications, and/or launch start-up companies based upon their experience in UROPs.
In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published "The Hidden Curriculum," arguing that education at MIT was often slighted in favor of following a set of unwritten expectations, and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled "course bibles"—collections of problem-set and examination questions and answers for later students to use as references. This sort of gamesmanship, Snyder argued, hindered development of a creative intellect and contributed to student discontent and unrest.
Graduate program.
MIT's graduate program has high coexistence with the undergraduate program, and many courses are taken by qualified students at both levels. MIT offers a comprehensive doctoral program with degrees in the humanities, social sciences, and STEM fields as well as professional degrees. The Institute offers graduate programs leading to academic degrees such as the Master of Science (MS), various Engineer's Degrees, Doctor of Philosophy (PhD), and Doctor of Science (ScD); professional degrees such as Master of Architecture (MArch), Master of Business Administration (MBA), Master of City Planning (MCP), Master of Engineering (MEng), Master of Finance (MFin) and Master of Science in Real Estate Development (MSRED), and interdisciplinary graduate programs such as the MD-PhD (with Harvard Medical School).
Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships (RAs), or teaching assistantships (TAs).
MIT awarded 1,547 master's degrees and 609 doctoral degrees in the academic year 2010–11. In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%), and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.
University rankings.
MIT places among the top ten in many overall rankings of universities (see right) and rankings based on students' revealed preferences. For several years, "U.S. News & World Report", the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report. In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.
In 2014, "Money" magazine ranked MIT as third in the US "Best Colleges for Your Money", based on its assessment of "the most bang for your tuition buck", factoring in quality of education, affordability, and career outcomes. , "Forbes" magazine rated MIT as the second "Most Entrepreneurial University", based on the percentage of alumni and students self-identifying as founders or business owners on LinkedIn. In 2015, Brookings Fellow Jonathan Rothwell issued a report "Beyond College Rankings", placing MIT as third in the US, with an estimated 45% value-added to mid-career salary.
Collaborations.
The university historically pioneered research and training collaborations between academia, industry and government.  In 1946, President Compton, Harvard Business School professor Georges Doriot, and Massachusetts Investor Trust chairman Merrill Grisswold founded American Research and Development Corporation, the first American venture-capital firm.  In 1948, Compton established the MIT Industrial Liaison Program.  Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a declining economy by transferring taxpayer-funded research and technology to international – especially Japanese — firms that were competing with struggling American businesses. On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as presidential scientific advisers since 1940. MIT established a Washington Office in 1991 to continue effective lobbying for research funding and national science policy.
The Justice Department began an investigation in 1989, and in 1991 filed an antitrust suit against MIT, the eight Ivy League colleges, and eleven other institutions for allegedly engaging in price-fixing during their annual "Overlap Meetings", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships. While the Ivy League institutions settled, MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students. MIT ultimately prevailed when the Justice Department dropped the case in 1994.
MIT's proximity to Harvard University ("the other school up the river") has led to a substantial number of research collaborations such as the Harvard-MIT Division of Health Sciences and Technology and the Broad Institute. In addition, students at the two schools can cross-register for credits toward their own school's degrees without any additional fees. A cross-registration program between MIT and Wellesley College has also existed since 1969, and in 2002 the Cambridge–MIT Institute launched an undergraduate exchange program between MIT and the University of Cambridge. MIT has more modest cross-registration programs with Boston University, Brandeis University, Tufts University, Massachusetts College of Art, and the School of the Museum of Fine Arts, Boston.
MIT maintains substantial research and faculty ties with independent research organizations in the Boston area, such as the Charles Stark Draper Laboratory, the Whitehead Institute for Biomedical Research, and the Woods Hole Oceanographic Institution. Ongoing international research and educational collaborations include the Singapore-MIT Alliance, MIT-Politecnico di Milano, MIT-Zaragoza International Logistics Program, and projects in other countries through the MIT International Science and Technology Initiatives (MISTI) program.
The mass-market magazine "Technology Review" is published by MIT through a subsidiary company, as is a special edition that also serves as an alumni magazine. The MIT Press is a major university press, publishing over 200 books and 30 journals annually, emphasizing science and technology as well as arts, architecture, new media, current events, and social issues.
Libraries, collections and museums.
The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries. Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music, the List Visual Arts Center's rotating exhibitions of contemporary art, and the Compton Gallery's cross-disciplinary exhibitions. MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection.
The MIT Museum was founded in 1971 and collects, preserves, and exhibits artifacts significant to the culture and history of MIT. The Museum now engages in significant educational outreach programs for the general public, including the annual Cambridge Science Festival, the first celebration of this kind in the United States. Since 2005, its official mission has been, "to engage the wider community with MIT's science, technology and other areas of scholarship in ways that will best serve the nation and the world in the 21st century".
Research.
MIT was elected to the Association of American Universities in 1934 and remains a research university with a very high level of research activity; research expenditures totaled $718.2 million in 2009. The federal government was the largest source of sponsored research, with the Department of Health and Human Services granting $255.9 million, Department of Defense $97.5 million, Department of Energy $65.8 million, National Science Foundation $61.4 million, and NASA $27.4 million. MIT employs approximately 1300 researchers in addition to faculty. In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties. Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures.
In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers. Harold Eugene Edgerton was a pioneer in high speed photography and sonar. Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory. In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography. At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.
Current and previous physics faculty have won eight Nobel Prizes, four Dirac Medals, and three Wolf Prizes predominantly for their contributions to subatomic and quantum theory. Members of the chemistry department have been awarded three Nobel Prizes and one Wolf Prize for the discovery of novel syntheses and methods. MIT biologists have been awarded six Nobel Prizes for their contributions to genetics, immunology, oncology, and molecular biology. Professor Eric Lander was one of the principal leaders of the Human Genome Project. Positronium atoms, synthetic penicillin, synthetic self-replicating molecules, and the genetic bases for Amyotrophic lateral sclerosis (also known as ALS or Lou Gehrig's disease) and Huntington's disease were first discovered at MIT. Jerome Lettvin transformed the study of cognitive science with his paper "What the frog's eye tells the frog's brain".
In the domain of humanities, arts, and social sciences, MIT economists have been awarded five Nobel Prizes and nine John Bates Clark Medals. Linguists Noam Chomsky and Morris Halle authored seminal texts on generative grammar and phonology. The MIT Media Lab, founded in 1985 within the School of Architecture and Planning and known for its unconventional research, has been home to influential researchers such as constructivist educator and Logo creator Seymour Papert.
Spanning many of the above fields, MacArthur Fellowships (the so-called "Genius Grants") have been awarded to 38 people associated with MIT. Four Pulitzer Prize–winning writers currently work at or have retired from MIT. Four current or former faculty are members of the American Academy of Arts and Letters.
Allegations of research misconduct or improprieties have received substantial press coverage. Professor David Baltimore, a Nobel Laureate, became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991. Professor Ted Postol has accused the MIT administration since 2000 of attempting to whitewash potential research misconduct at the Lincoln Lab facility involving a ballistic missile defense test, though a final investigation into the matter has not been completed. Associate Professor Luk Van Parijs was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the United States Office of Research Integrity in 2009.
Researchers developed a system to convert MRI scans into 3D printed physical models.
Traditions and student activities.
The faculty and student body place a high value on meritocracy and on technical proficiency. MIT has never awarded an honorary degree, nor does it award athletic scholarships, ad eundem degrees, or Latin honors upon graduation. However, MIT has twice awarded honorary professorships: to Winston Churchill in 1949 and Salman Rushdie in 1993.
Many upperclass students and alumni wear a large, heavy, distinctive class ring known as the "Brass Rat". Originally created in 1929, the ring's official name is the "Standard Technology Ring." The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a beaver. The initialism IHTFP, representing the informal school motto "I Hate This Fucking Place" and jocularly euphemized as "I Have Truly Found Paradise," "Institute Has The Finest Professors," "It's Hard to Fondle Penguins," and other variations, has occasionally been featured on the ring given its historical prominence in student culture.
Activities.
MIT has over 500 recognized student activity groups, including a campus radio station, "The Tech" student newspaper, an annual entrepreneurship competition, and weekly screenings of popular films by the Lecture Series Committee. Less traditional activities include the "world's largest open-shelf collection of science fiction" in English, a model railroad club, and a vibrant folk dance scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the MIT Museum, Edgerton Center, and MIT Public Service Center.
The Independent Activities Period is a four-week-long "term" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are the 6.270, 6.370, and MasLab competitions, the annual "mystery hunt", and Charm School. More than 250 students pursue externships annually at companies in the US and abroad.
Many MIT students also engage in "hacking", which encompasses both the physical exploration of areas that are generally off-limits (such as rooftops and steam tunnels), as well as elaborate practical jokes. Recent high-profile hacks have included the abduction of Caltech's cannon, reconstructing a Wright Flyer atop the Great Dome, and adorning the John Harvard statue with the Master Chief's Mjölnir Helmet.
Athletics.
MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs.  MIT participates in the NCAA's Division III, the New England Women's and Men's Athletic Conference, the New England Football Conference, the Pilgrim League for men's lacrosse, NCAA's Division I Eastern Association of Women's Rowing Colleges (EAWRC) for women's crew, and the Collegiate Water Polo Association (CWPA) for Men's Water Polo. Men's crew competes outside the NCAA in the Eastern Association of Rowing Colleges (EARC). In April 2009, budget cuts lead to MIT eliminating eight of its 41 sports, including the mixed men's and women's teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and men's programs in golf and wrestling.
People.
Students.
MIT enrolled 4,384 undergraduates and 6,510 graduate students in 2011–2012. Women constituted 45 percent of undergraduate students. Undergraduate and graduate students were drawn from all 50 states as well as 115 foreign countries.
MIT received 17,909 applications for admission to the undergraduate Class of 2015; 1,742 were admitted (9.7 percent) and 1128 enrolled (64.8 percent). 19,446 applications were received for graduate and advanced degree program across all departments; 2,991 were admitted (15.4 percent) and 1,880 enrolled (62.8 percent).
The interquartile range on the SAT was 2090–2340 and 97 percent of students ranked in the top tenth of their high school graduating class. 97 percent of the Class of 2012 returned as sophomores; 82 percent of the Class of 2007 graduated within 4 years, and 93 percent (91 percent of the men and 95 percent of the women) graduated within 6 years.
Undergraduate tuition and fees total $40,732 and annual expenses are estimated at $52,507 as of 2012. 62 percent of students received need-based financial aid in the form of scholarships and grants from federal, state, institutional, and external sources averaging $38,964 per student. Students were awarded a total of $102 million in scholarships and grants, primarily from institutional support ($84 million). The annual increase in expenses has led to a student tradition (dating back to the 1960s) of tongue-in-cheek "tuition riots".
MIT has been nominally co-educational since admitting Ellen Swallow Richards in 1870. Richards also became the first female member of MIT's faculty, specializing in sanitary chemistry. Female students remained a minority prior to the completion of the first wing of a women's dormitory, McCormick Hall, in 1963. Between 1993 and 2009, the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students. Women currently outnumber men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning, and Biological Engineering.
A number of student deaths in the late 1990s and early 2000s resulted in considerable media attention to MIT's culture and student life. After the alcohol-related death of Scott Krueger in September 1997 as a new member at the Phi Gamma Delta fraternity, MIT began requiring all freshmen to live in the dormitory system. The 2000 suicide of MIT undergraduate Elizabeth Shin drew attention to suicides at MIT and created a controversy over whether MIT had an unusually high suicide rate. In late 2001 a task force's recommended improvements in student mental health services were implemented, including expanding staff and operating hours at the mental health center. These and later cases were significant as well because they sought to prove the negligence and liability of university administrators "in loco parentis".
Faculty and staff.
, MIT had 1,030 faculty members, of whom 225 were women. Faculty are responsible for lecturing classes, advising both graduate and undergraduate students, and sitting on academic committees, as well as conducting original research. Between 1964 and 2009, a total of seventeen faculty and staff members affiliated with MIT were awarded Nobel Prizes (thirteen in the last 25 years). MIT faculty members past or present have won a total of twenty-seven Nobel Prizes, the majority in Economics or Physics. , among current faculty and teaching staff there are 67 Guggenheim Fellows, 6 Fulbright Scholars, and 22 MacArthur Fellows. Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as Institute Professors for the remainder of their tenures.
A 1998 MIT study concluded that a systemic bias against female faculty existed in its School of Science, although the study's methods were controversial. Since the study, though, women have headed departments within the Schools of Science and of Engineering, and MIT has appointed several female vice presidents, although allegations of sexism continue to be made. Susan Hockfield, a molecular neurobiologist, was MIT's president from 2004 to 2012 and was the first woman to hold the post.
Tenure outcomes have vaulted MIT into the national spotlight on several occasions. The 1984 dismissal of David F. Noble, a historian of technology, became a "cause célèbre" about the extent to which academics are granted freedom of speech after he published several books and papers critical of MIT's and other research universities' reliance upon financial support from corporations and the military. Former materials science professor Gretchen Kalonji sued MIT in 1994 alleging that she was denied tenure because of sexual discrimination. Several years later, the lawsuit was settled with undisclosed payments, and establishment of a project to encourage women and minorities to seek faculty positions. In 1997, the Massachusetts Commission Against Discrimination issued a probable cause finding supporting UMass Boston Professor James Jennings' allegations of racial discrimination after a senior faculty search committee in the Department of Urban Studies and Planning did not offer him reciprocal tenure. In 2006–2007, MIT's denial of tenure to African-American stem cell scientist professor James Sherley reignited accusations of racism in the tenure process, eventually leading to a protracted public dispute with the administration, a brief hunger strike, and the resignation of Professor Frank L. Douglas in protest. April Simpson of The Boston Globe reported on February 6, 2007: "Less than half of MIT's junior faculty members are granted tenure. After Sherley was initially denied tenure, his case was examined three times before the university established that neither racial discrimination nor conflict of interest affected the decision. Twenty-one of Sherley's colleagues issued a statement yesterday saying that the professor was treated fairly in tenure review."
MIT faculty members have often been recruited to lead other colleges and universities. Founding faculty member Charles W. Eliot was recruited in 1869 to become president of Harvard University, a post he would hold for 40 years, during which he wielded considerable influence on both American higher education and secondary education. MIT alumnus and faculty member George Ellery Hale played a central role in the development of the California Institute of Technology (Caltech), and other faculty members have been key founders of Franklin W. Olin College of Engineering in nearby Needham, Massachusetts.
, former provost Robert A. Brown is president of Boston University; former provost Mark Wrighton is chancellor of Washington University in St. Louis; former associate provost Alice Gast is president of Lehigh University; and former professor Suh Nam-pyo is president of KAIST. Former dean of the School of Science Robert J. Birgeneau was the chancellor of the University of California, Berkeley (2004–2013); former professor John Maeda was president of Rhode Island School of Design (RISD, 2008–2013); former professor David Baltimore was president of Caltech (1997–2006); and MIT alumnus and former assistant professor Hans Mark served as chancellor of the University of Texas system (1984–1992).
In addition, faculty members have been recruited to lead governmental agencies; for example, former professor Marcia McNutt is president of the National Academy of Sciences, urban studies professor Xavier de Souza Briggs is currently the associate director of the White House Office of Management and Budget, and biology professor Eric Lander is a co-chair of the President's Council of Advisors on Science and Technology. In 2013, faculty member Ernest Moniz was nominated by President Obama and later confirmed as United States Secretary of Energy. Former professor Hans Mark served as Secretary of the Air Force from 1979 to 1981. Alumna and Institute Professor Sheila Widnall served as Secretary of the Air Force between 1993 and 1997, making her the first female Secretary of the Air Force and first woman to lead an entire branch of the US military in the Department of Defense.
Based on feedback from employees, MIT was ranked #7 as a place to work, among US colleges and universities . Surveys cited a "smart", "creative", "friendly" environment, noting that the work-life balance tilts towards a "strong work ethic", but complaining about "low pay".
Alumni.
Many of MIT's over 120,000 alumni have had considerable success in scientific research, public service, education, and business. , 27 MIT alumni have won the Nobel Prize, 47 have been selected as Rhodes Scholars, and 61 have been selected as Marshall Scholars.
Alumni in American politics and public service include former Chairman of the Federal Reserve Ben Bernanke, former MA-1 Representative John Olver, former CA-13 Representative Pete Stark, former National Economic Council chairman Lawrence H. Summers, and former Council of Economic Advisors chairwoman Christina Romer. MIT alumni in international politics include Foreign Affairs Minister of Iran Ali Akbar Salehi, Israeli Prime Minister Benjamin Netanyahu, President of Colombia Virgilio Barco Vargas, President of the European Central Bank Mario Draghi, Governor of the Reserve Bank of India Raghuram Rajan, former British Foreign Minister David Miliband, former Greek Prime Minister Lucas Papademos, former UN Secretary General Kofi Annan, and former Iraqi Deputy Prime Minister Ahmed Chalabi.
MIT alumni founded or co-founded many notable companies, such as Intel, McDonnell Douglas, Texas Instruments, 3Com, Qualcomm, Bose, Raytheon, Koch Industries, Rockwell International, Genentech, Dropbox, and Campbell Soup. According to the British newspaper, "The Guardian", "a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms collectively generate global revenues of about $1.9 trillion (£1.2 trillion) a year. If MIT were a country, it would have the 11th highest GDP of any nation in the world."
Prominent institutions of higher education have been led by MIT alumni, including the University of California system, Harvard University, New York Institute of Technology, Johns Hopkins University, Carnegie Mellon University, Tufts University, Rochester Institute of Technology, Rhode Island School of Design (RISD), Northeastern University, Lahore University of Management Sciences, Rensselaer Polytechnic Institute, Tecnológico de Monterrey, Purdue University, Virginia Polytechnic Institute, KAIST, and Quaid-e-Azam University. Berklee College of Music, the largest independent college of contemporary music in the world, was founded and led by MIT alumnus Lawrence Berk for more than three decades.
More than one third of the United States' manned spaceflights have included MIT-educated astronauts (among them Apollo 11 Lunar Module Pilot Buzz Aldrin), more than any university excluding the United States service academies. Alumnus and former faculty member Qian Xuesen was instrumental in the PRC rocket program.
Noted alumni in non-scientific fields include author Hugh Lofting, sculptor Daniel Chester French, guitarist Tom Scholz of the band Boston, the British "BBC" and "ITN" correspondent and political advisor David Walter, "The New York Times" columnist and Nobel Prize Winning economist Paul Krugman, "The Bell Curve" author Charles Murray, United States Supreme Court building architect Cass Gilbert, Pritzker Prize-winning architects I.M. Pei and Gordon Bunshaft.

</doc>
<doc id="18880" url="https://en.wikipedia.org/wiki?curid=18880" title="Monopolistic competition">
Monopolistic competition

Monopolistic competition is a type of imperfect competition such that many producers sell products that are differentiated from one another (e.g. by branding or quality) and hence are not perfect substitutes. In monopolistic competition, a firm takes the prices charged by its rivals as given and ignores the impact of its own prices on the prices of other firms. In the presence of coercive government, monopolistic competition will fall into government-granted monopoly. Unlike perfect competition, the firm maintains spare capacity. Models of monopolistic competition are often used to model industries. Textbook examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities. The "founding father" of the theory of monopolistic competition is Edward Hastings Chamberlin, who wrote a pioneering book on the subject, "Theory of Monopolistic Competition" (1933). Joan Robinson published a book "The Economics of Imperfect Competition" with a comparable theme of distinguishing perfect from imperfect competition.
Monopolistically competitive markets have the following characteristics:
The long-run characteristics of a monopolistically competitive market are almost the same as a perfectly competitive market. Two differences between the two are that monopolistic competition produces heterogeneous products and that monopolistic competition involves a great deal of non-price competition, which is based on subtle product differentiation. A firm making profits in the short run will nonetheless only break even in the long run because demand will decrease and average total cost will increase. This means in the long run, a monopolistically competitive firm will make zero economic profit. This illustrates the amount of influence the firm has over the market; because of brand loyalty, it can raise its prices without losing all of its customers. This means that an individual firm's demand curve is downward sloping, in contrast to perfect competition, which has a perfectly elastic demand schedule.
Major characteristics.
There are six characteristics of monopolistic competition (MC):
Product differentiation.
MC firms sell products that have real or perceived non-price differences. However, the differences are not so great as to eliminate other goods as substitutes. Technically, the cross price elasticity of demand between goods in such a market is positive. In fact, the XED would be high. MC goods are best described as close but imperfect substitutes. The goods perform the same basic functions but have differences in qualities such as type, style, quality, reputation, appearance, and location that tend to distinguish them from each other. For example, the basic function of motor vehicles is the same—to move people and objects from point to point in reasonable comfort and safety. Yet there are many different types of motor vehicles such as motor scooters, motor cycles, trucks and cars, and many variations even within these categories.
Many firms.
There are many firms in each MC product group and many firms on the side lines prepared to enter the market. A product group is a "collection of similar products". The fact that there are "many firms" gives each MC firm the freedom to set prices without engaging in strategic decision making regarding the prices of other firms and each firm's actions have a negligible impact on the market. For example, a firm could cut prices and increase sales without fear that its actions will prompt retaliatory responses from competitors.
How many firms will an MC market structure support at market equilibrium? The answer depends on factors such as fixed costs, economies of scale and the degree of product differentiation. For example, the higher the fixed costs, the fewer firms the market will support. Also the greater the degree of product differentiation—the more the firm can separate itself from the pack—the fewer firms there will be at market equilibrium.
No entry and exit costs.
In the long run there are no entry and exit costs. There are numerous firms waiting to enter the market, each with their own "unique" product or in pursuit of positive profits. Any firm unable to cover its costs can leave the market without incurring liquidation costs. This assumption implies that there are low start up costs, no sunk costs and no exit costs.
Independent decision making.
Each MC firm independently sets the terms of exchange for its product. The firm gives no consideration to what effect its decision may have on competitors. The theory is that any action will have such a negligible effect on the overall market demand that an MC firm can act without fear of prompting heightened competition. In other words, each firm feels free to set prices as if it were a monopoly rather than an oligopoly.
Market power.
MC firms have some degree of market power. Market power means that the firm has control over the terms and conditions of exchange. An MC firm can raise its prices without losing all its customers. The firm can also lower prices without triggering a potentially ruinous price war with competitors. The source of an MC firm's market power is not barriers to entry since they are low. Rather, an MC firm has market power because it has relatively few competitors, those competitors do not engage in strategic decision making and the firms sells differentiated product. Market power also means that an MC firm faces a downward sloping demand curve. The demand curve is highly elastic although not "flat".
Imperfect information.
No sellers or buyers have complete market information, like market demand or market supply.
Inefficiency.
There are two sources of inefficiency in the MC market structure. First, at its optimum output the firm charges a price that exceeds marginal costs, The MC firm maximizes profits where marginal revenue = marginal cost. Since the MC firm's demand curve is downward sloping this means that the firm will be charging a price that exceeds marginal costs. The monopoly power possessed by a MC firm means that at its profit maximizing level of production there will be a net loss of consumer (and producer) surplus. The second source of inefficiency is the fact that MC firms operate with excess capacity. That is, the MC firm's profit maximizing output is less than the output associated with minimum average cost. Both a PC and MC firm will operate at a point where demand or price equals average cost. For a PC firm this equilibrium condition occurs where the perfectly elastic demand curve equals minimum average cost. A MC firm’s demand curve is not flat but is downward sloping. Thus in the long run the demand curve will be tangential to the long run average cost curve at a point to the left of its minimum. The result is excess capacity.
Problems.
Monopolistically competitive firms are inefficient, it is usually the case that the costs of regulating prices for products sold in monopolistic competition exceed the benefits of such regulation. . A monopolistically competitive firm might be said to be marginally inefficient because the firm produces at an output where average total cost is not a minimum. A monopolistically competitive market is productively inefficient market structure because marginal cost is less than price in the long run. Monopolistically competitive markets are also allocatively inefficient, as the price given is higher than Marginal cost. Product differentiation increases total utility by better meeting people's wants than homogenous products in a perfectly competitive market. 
Another concern is that monopolistic competition fosters advertising and the creation of brand names. Advertising induces customers into spending more on products because of the name associated with them rather than because of rational factors. Defenders of advertising dispute this, arguing that brand names can represent a guarantee of quality and that advertising helps reduce the cost to consumers of weighing the tradeoffs of numerous competing brands. There are unique information and information processing costs associated with selecting a brand in a monopolistically competitive environment. In a monopoly market, the consumer is faced with a single brand, making information gathering relatively inexpensive. In a perfectly competitive industry, the consumer is faced with many brands, but because the brands are virtually identical information gathering is also relatively inexpensive. In a monopolistically competitive market, the consumer must collect and process information on a large number of different brands to be able to select the best of them. In many cases, the cost of gathering information necessary to selecting the best brand can exceed the benefit of consuming the best brand instead of a randomly selected brand. The result is that the consumer is confused. Some brands gain prestige value and can extract an additional price for that.
Evidence suggests that consumers use information obtained from advertising not only to assess the single brand advertised, but also to infer the possible existence of brands that the consumer has, heretofore, not observed, as well as to infer consumer satisfaction with brands similar to the advertised brand.
Examples.
In many markets, such as toothpastes, Smart Phones and toilet paper, producers practice product differentiation by altering the physical composition of products, using special packaging, or simply claiming to have superior products based on brand images or advertising.

</doc>
<doc id="18881" url="https://en.wikipedia.org/wiki?curid=18881" title="Mathematical induction">
Mathematical induction

Mathematical induction is a mathematical proof technique, most commonly used to establish a given statement for all natural numbers, although it can be used to prove statements about any well-ordered set. It is a form of direct proof, and it is done in two steps. The first step, known as the base case, is to prove the given statement for the first natural number. The second step, known as the inductive step, is to prove that the given statement for any one natural number implies the given statement for the next natural number. From these two steps, mathematical induction is the rule from which we infer that the given statement is established for all natural numbers.
The method can be extended to prove statements about more general well-founded structures, such as trees; this generalization, known as structural induction, is used in mathematical logic and computer science. Mathematical induction in this extended sense is closely related to recursion. Mathematical induction, in some form, is the foundation of all correctness proofs for computer programs.
Although its name may suggest otherwise, mathematical induction should not be misconstrued as a form of inductive reasoning (also see Problem of induction). Mathematical induction is an inference rule used in proofs. In mathematics, proofs including those using mathematical induction are examples of deductive reasoning, and inductive reasoning is excluded from proofs.
History.
In 370 BC, Plato's Parmenides may have contained an early example of an implicit inductive proof. The earliest implicit traces of mathematical induction can be found in Euclid's proof that the number of primes is infinite and in Bhaskara's "cyclic method". An opposite iterated technique, counting "down" rather than up, is found in the Sorites paradox, where one argued that if 1,000,000 grains of sand formed a heap, and removing one grain from a heap left it a heap, then a single grain of sand (or even no grains) forms a heap.
An implicit proof by mathematical induction for arithmetic sequences was introduced in the "al-Fakhri" written by al-Karaji around 1000 AD, who used it to prove the binomial theorem and properties of Pascal's triangle.
None of these ancient mathematicians, however, explicitly stated the inductive hypothesis. Another similar case (contrary to what Vacca has written, as Freudenthal carefully showed) was that of Francesco Maurolico in his "Arithmeticorum libri duo" (1575), who used the technique to prove that the sum of the first "n" odd integers is "n"2. The first explicit formulation of the principle of induction was given by Pascal in his "Traité du triangle arithmétique" (1665). Another Frenchman, Fermat, made ample use of a related principle, indirect proof by infinite descent. The inductive hypothesis was also employed by the Swiss Jakob Bernoulli, and from then on it became more or less well known. The modern rigorous and systematic treatment of the principle came only in the 19th century, with George Boole, Augustus de Morgan, Charles Sanders Peirce, Giuseppe Peano, and Richard Dedekind.
Description.
The simplest and most common form of mathematical induction infers that a statement involving a natural number "n" holds for all values of "n". The proof consists of two steps:
The hypothesis in the inductive step that the statement holds for some "n" is called the induction hypothesis (or inductive hypothesis). To perform the inductive step, one assumes the induction hypothesis and then uses this assumption to prove the statement for "n" + 1.
Whether "n" = 0 or "n" = 1 depends on the definition of the natural numbers. If 0 is considered a natural number, as is common in the fields of combinatorics and mathematical logic, the base case is given by "n" = 0. If, on the other hand, 1 is taken as the first natural number, then the base case is given by "n" = 1.
Example.
Mathematical induction can be used to prove that the following statement, which we will call "P"("n"), holds for all natural numbers "n".
"P"("n") gives a formula for the sum of the natural numbers less than or equal to number "n". The proof that "P"("n") is true for each natural number "n" proceeds as follows.
Basis: Show that the statement holds for "n" = 0. <br>
"P"(0) amounts to the statement:
In the left-hand side of the equation, the only term is 0, and so the left-hand side is simply equal to 0. <br>
In the right-hand side of the equation, 0·(0 + 1)/2 = 0. <br>
The two sides are equal, so the statement is true for "n" = 0. Thus it has been shown that "P"(0) holds.
Inductive step: Show that "if" "P"("k") holds, then also holds. This can be done as follows.
Assume "P"("k") holds (for some unspecified value of "k"). It must then be shown that holds, that is:
Using the induction hypothesis that "P"("k") holds, the left-hand side can be rewritten to:
Algebraically:
thereby showing that indeed holds.
Since both the basis and the inductive step have been performed, by mathematical induction, the statement "P"("n") holds for all natural numbers "n". Q.E.D.
Axiom of induction.
Mathematical induction as an inference rule can be formalized as a second-order axiom. The "axiom of induction" is, in logical symbols,
where "P" is any predicate and "k" and "n" are both natural numbers.
In words, the basis "P"(0) and the inductive step (namely, that the inductive hypothesis "P"("k") implies "P"("k" + 1)) together imply that "P"("n") for any natural number "n". The axiom of induction asserts that the validity of inferring that "P"("n") holds for any natural number "n" from the basis and the inductive step.
Note that the first quantifier in the axiom ranges over "predicates" rather than over individual numbers. This is a second-order quantifier, which means that this axiom is stated in second-order logic. Axiomatizing arithmetic induction in first-order logic requires an axiom schema containing a separate axiom for each possible predicate. The article Peano axioms contains further discussion of this issue.
Characterizing the structure of N by the induction axiom.
Having proven the base case and the inductive step, then the structure of formula_7 is such that any value can be obtained by performing the inductive step repeatedly. It may be helpful to think of the domino effect. Consider a half line of dominoes each standing on end, and extending infinitely to the right (see picture). Suppose that:
With these assumptions one can conclude (using mathematical induction) that all of the dominoes will fall right.
If the dominoes are arranged in another way, this conclusion needn't hold (see Peano axioms#Formulation for a counter example). Similarly, the induction axiom describes an essential property of formula_7, viz. that each of its members can be reached from 0 by sufficiently often adding 1. While there is only one structure that satisfies all Peano axioms (including induction), there is no set of only first-order axioms that fulfils the same task.
Variants.
In practice, proofs by induction are often structured differently, depending on the exact nature of the property to be proved.
Induction basis other than 0 or 1.
If we want to prove a statement not for all natural numbers but only for all numbers greater than or equal to a certain number "b" then the proof by induction consists of:
This can be used, for example, to show that "n"2 ≥ 3"n" for "n" ≥ 3. A more substantial example is a proof that
In this way we can prove that "P"("n") holds for all "n" ≥1, or even "n" ≥−5. This form of mathematical induction is actually a special case of the previous form because if the statement that we intend to prove is "P"("n") then proving it with these two rules is equivalent with proving "P"("n" + "b") for all natural numbers "n" with the first two steps.
Induction basis equal to 2.
In mathematics, many standard functions, including operations such as "+" and relations such as "=", are binary, meaning that they take two arguments. Often these functions possess properties that implicitly extend them to more than two arguments. For example, once addition "a" + "b" is defined and is known to satisfy the associativity property ("a" + "b") + "c" = "a" + ("b" + "c"), then the ternary addition "a" + "b" + "c" makes sense, either as ("a" + "b") + "c" or as "a" + ("b" + "c"). Similarly, many axioms and theorems in mathematics are stated only for the binary versions of mathematical operations and relations, and implicitly extend to higher-arity versions.
Suppose that we wish to prove a statement about an "n"-ary operation implicitly defined from a binary operation, using mathematical induction on "n". In this case it is natural to take 2 for the induction basis.
Example: product rule for the derivative.
In this example, the binary operation in question is multiplication (of functions). The usual product rule for the derivative taught in calculus states:
or in logarithmic derivative form
This can be generalized to a product of "n" functions. One has
or in logarithmic derivative form
In each of the "n" terms of the usual form, just one of the factors is a derivative; the others are not.
When this general fact is proved by mathematical induction, the "n" = 0 case is trivial,formula_16 (since the empty product is 1, and the empty sum is 0). The "n" = 1 case is also trivial, formula_17 And for each "n" ≥ 3, the case is easy to prove from the preceding "n" − 1 case. The real difficulty lies in the "n" = 2 case, which is why that is the one stated in the standard product rule.
Induction on more than one counter.
It is sometimes desirable to prove a statement involving two natural numbers, "n" and "m", by iterating the induction process. That is, one performs a basis step and an inductive step for "n", and in each of those performs a basis step and an inductive step for "m". See, for example, the proof of commutativity accompanying "addition of natural numbers". More complicated arguments involving three or more counters are also possible.
Infinite descent.
The method of infinite descent was one of Pierre de Fermat's favorites. This method of proof can assume several slightly different forms. For example, it might begin by showing that if a statement is true for a natural number "n" it must also be true for some smaller natural number "m" ("m" < "n"). Using mathematical induction (implicitly) with the inductive hypothesis being that the statement is false for all natural numbers less than or equal to "m", we can conclude that the statement cannot be true for any natural number "n".
Although this particular form of infinite-descent proof is clearly a mathematical induction, whether one holds all proofs "by infinite descent" to be mathematical inductions depends on how one defines the term "proof by infinite descent." One might, for example, use the term to apply to proofs in which the well-ordering of the natural numbers is assumed, but not the principle of induction. Such, for example, is the usual proof that 2 has no rational square root (see Infinite descent).
Prefix induction.
The most common form of induction requires proving that
or equivalently
whereupon the induction principle "automates" n applications of this inference in getting from "P"(0) to "P"("n"). This could be called "predecessor induction" because each step proves something about a number from something about that number's predecessor.
A variant of interest in computational complexity is "prefix induction", in which one needs to prove
or equivalently
The induction principle then "automates" log "n" applications of this inference in getting from "P"(0) to "P"("n"). (It is called "prefix induction" because each step proves something about a number from something about the "prefix" of that number formed by truncating the low bit of its binary representation.)
If traditional predecessor induction is interpreted computationally as an "n"-step loop, prefix induction corresponds to a log "n"-step loop, and thus proofs using prefix induction are "more feasibly constructive" than proofs using predecessor induction.
Predecessor induction can trivially simulate prefix induction on the same statement. Prefix induction can simulate predecessor induction, but only at the cost of making the statement more syntactically complex (adding a bounded universal quantifier), so the interesting results relating prefix induction to polynomial-time computation depend on excluding unbounded quantifiers entirely, and limiting the alternation of bounded universal and existential quantifiers allowed in the statement. See 
One could take it a step farther to "prefix of prefix induction": one must prove
whereupon the induction principle "automates" log log "n" applications of this inference in getting from "P"(0) to "P"("n"). This form of induction has been used, analogously, to study log-time parallel computation.
Complete induction.
Another variant, called complete induction, course of values induction or strong induction (in contrast to which the basic form of induction is sometimes known as weak induction)
makes the inductive step easier to prove by using a stronger hypothesis:
one proves the statement P("m+1") under the assumption that P("n") holds for all natural "n" less than "m+1";
by contrast, the basic form only assumes P("m").
The name "strong induction" does not mean that this method can prove more than "weak induction",
but merely refers to the stronger hypothesis used in the inductive step;
in fact the two methods are equivalent, as explained below.
In this form of complete induction one still has to prove the base case, P(0), and it may even be necessary to prove extra base cases such as P(1)
before the general argument applies, as in the example below of the Fibonacci number "Fn".
Although the form just described requires one to prove the base case,
this is unnecessary if one can prove P("m") (assuming P("n") for all lower "n") for all "m" ≥ 0.
This is a special case of transfinite induction as described below.
In this form the base case is subsumed by the case "m" = 0, where P(0) is proved with no other P("n") assumed;
this case may need to be handled separately, but sometimes the same argument applies for "m" = 0 and "m" > 0,
making the proof simpler and more elegant.
In this method it is, however, vital to ensure that the proof of P("m") does not implicitly assume that "m" > 0,
e.g. by saying "choose an arbitrary "n" < "m"" or assuming that a set of "m" elements has an element.
Complete induction is equivalent to ordinary mathematical induction as described above, in the sense that a proof by one method can be transformed into a proof by the other. Suppose we have a proof of P("n") by complete induction. Let Q("n") mean "P("m") holds for all "m" such that 0 ≤ "m" ≤ "n"". Then Q("n") holds for all "n" if and only if P("n") holds for all "n", and our proof of P("n") is easily transformed into a proof of Q("n") by (ordinary) induction. If, on the other hand, we have proved P("n") by ordinary induction, we already effectively have a proof by complete induction: P(0) is proved in the base case, using no assumptions, and P("n"+1) is proved in the inductive step, where we may assume all earlier cases but need only use the case P("n").
Examples of complete induction.
Complete induction is most useful when several instances of the inductive hypothesis are required for each inductive step. For example, complete induction can be used to show that
where "Fn" is the "n"th Fibonacci number, φ = (1 + √5)/2 (the golden ratio) and ψ = (1 − √5)/2 are the roots of the polynomial "x"2 − "x" − 1. By using the fact that "F""n" + 2 = "F""n" + 1 + "F""n" for each "n" ∈ N, the identity above can be verified by direct calculation for "F""n" + 2 if we assume that it already holds for both "F""n" + 1 and "F""n". To complete the proof, the identity must be verified in the two base cases "n" = 0 and "n" = 1.
Another proof by complete induction uses the hypothesis that the statement holds for "all" smaller "n" more thoroughly. Consider the statement that "every natural number greater than 1 is a product of (one or more) prime numbers", and assume that for a given "m" > 1 it holds for all smaller "n" > 1. If "m" is prime then it is certainly a product of primes, and if not, then by definition it is a product: "m" = "n"1 "n"2, where neither of the factors is equal to 1; hence neither is equal to "m", and so both are smaller than "m". The induction hypothesis now applies to "n"1 and "n"2, so each one is a product of primes. Thus "m" is a product of products of primes; i.e. a product of primes.
Transfinite induction.
The last two steps can be reformulated as one step:
This form of mathematical induction is not only valid for statements about natural numbers, but for statements about elements of any well-founded set, that is, a set with an irreflexive relation < that contains no infinite descending chains.
This form of induction, when applied to ordinals (which form a well-ordered and hence well-founded class), is called "transfinite induction". It is an important proof technique in set theory, topology and other fields.
Proofs by transfinite induction typically distinguish three cases:
Strictly speaking, it is not necessary in transfinite induction to prove the basis, because it is a vacuous special case of the proposition that if "P" is true of all "n" < "m", then "P" is true of "m". It is vacuously true precisely because there are no values of "n" < "m" that could serve as counterexamples.
Equivalence with the well-ordering principle.
The principle of mathematical induction is usually stated as an axiom of the natural numbers; see Peano axioms. However, it can be proved from the well-ordering principle. Indeed, suppose the following:
To derive simple induction from these axioms, we must show that if P("n") is some proposition predicated of "n", and if:
then P("n") holds for all "n".
"Proof." Let "S" be the set of all natural numbers for which P("n") is false. Let us see what happens if we assert that "S" is nonempty. Well-ordering tells us that S has a least element, say "t". Moreover, since P(0) is true, "t" is not 0. Since every natural number is either zero or some "n" + 1, there is some natural number "n" such that "n" + 1 = "t". Now "n" is less than "t", and "t" is the least element of "S". It follows that "n" is not in "S", and so P("n") is true. This means that P("n" + 1) is true, and so P("t") is true. This is a contradiction, since "t" was in "S". Therefore, S is empty.
It can also be proved that induction, given the other axioms, implies the well-ordering principle.
"Proof." Suppose there exists a non-empty set, "S", of naturals with no least element. Let "P"("k") be the assertion that "k" is not in "S". Then "P"(0) is true for if it were false then 0 is the least element of "S". Furthermore, suppose "P"(1), "P"(2)..., "P"("k") is true. Then if "P"("k"+1) is false "k"+1 is in "S", thus it is the minimal element is "S", a contradiction. Thus "P"("k"+1) is true. Therefore by the induction axiom "S" is empty, a contradiction.
Example of error in the inductive step.
This example demonstrated a subtle error in the proof of the inductive step.
Joel E. Cohen proposed the following argument, which purports to prove by mathematical induction that all horses are of the same color:
The basis case "n" = 1 is trivial (as any horse is the same color as itself), and the inductive step is correct in all cases "n" > 1. However, the logic of the inductive step is incorrect for "n" = 1, because the statement that "the two sets overlap" is false (there are only "n" + 1 = 2 horses prior to either removal, and after removal the sets of one horse each do not overlap).

</doc>
<doc id="18884" url="https://en.wikipedia.org/wiki?curid=18884" title="Matrix">
Matrix

Matrix may refer to:

</doc>
<doc id="18885" url="https://en.wikipedia.org/wiki?curid=18885" title="Morton Downey, Jr.">
Morton Downey, Jr.

Sean Morton Downey (December 9, 1932 – March 12, 2001), known more commonly by his stage name Morton Downey, Jr., was an American singer, songwriter and later a television talk show host of the late 1980s who pioneered the "trash TV" format on his program "The Morton Downey, Jr. Show".
The film company Ironbound Films produced a documentary film about Downey titled "", which premiered April 19, 2012, at the 2012 Tribeca Film Festival.
Early life.
Morton Downey, Jr.'s parents were also in show business; his father, Morton Downey, was a popular singer, and his mother, Barbara Bennett, was a singer and dancer. Downey did not use his legal first name (Sean) in his stage name. His aunts included Hollywood film stars Constance and Joan Bennett, from whom he was estranged, and his maternal grandfather was the celebrated matinée idol Richard Bennett. Born into a life of luxury, he was raised during the summers next door to the Kennedy compound in Hyannis Port, Massachusetts. Downey attended New York University.
Career.
He was a program director and announcer at radio station WPOP in Hartford, Connecticut in the 1950s. He went on to work as a disc jockey, sometimes using the moniker "Doc" Downey, in various markets around the U.S., including Phoenix (KRIZ), Miami (WFUN), Kansas City (KUDL), San Diego (KDEO) and Seattle (KJR). Like his father, Downey pursued a career in music, recording in both pop and country styles. He sang on a few records and then began to write songs, several of which were popular in the 1950s and 1960s. He joined ASCAP as a result. In 1958, he recorded "Boulevard Of Broken Dreams," which he sang on national television on a set that resembled a dark street with one street light. In 1981, "Green Eyed Girl" charted on the "Billboard Magazine" country chart, peaking at #95.
In the 1980s, Downey was a talk show host at KFBK-AM in Sacramento, California, where he employed his abrasive style. He was fired in 1984. He was replaced on KFBK by Rush Limbaugh, who has held the time slot ever since, later via his national syndication. Downey also had a stint on WMAQ-AM in Chicago where he unsuccessfully tried to get other on air radio personalities to submit to drug testing. Downey's largest effect on American culture came from his popular, yet short-lived, syndicated late 1980s television talk show, "The Morton Downey Jr. Show".
Pro-life activism.
On January 22, 1980, Downey, a devoted pro-life activist, hosted the California State Rally for Life at the invitation of the California ProLife Council and United Students for Life. At that time, he was also running for President of the United States, as a Democrat. The United Students for Life, at California State University, Sacramento helped organize his California presidential rallies. Downey worked to help promote pro-life candidates in California and around the country.
Television.
Downey headed to Secaucus, New Jersey, where his highly controversial television program "The Morton Downey Jr. Show" was taped. Starting as a local program on New York-New Jersey superstation WWOR-TV in October 1987, it expanded into national syndication in early 1988. The program featured screaming matches among Downey, his guests, and audience members. Using a large silver bowl for an ashtray, he would chainsmoke during the show and blow smoke in his guests' faces. Downey's fans became known as "Loudmouths," patterned after the studio lecterns decorated with gaping cartoon mouths, from which Downey's guests would go head-to-head against each other on their respective issues.
Downey's signature phrases "pablum puking liberal" (in reference to left-liberals) and "zip it!" briefly enjoyed some popularity in the contemporary vernacular. He particularly enjoyed making his guests angry with each other, which on a few occasions resulted in physical confrontations. One that occurred on a 1988 show taped at the Apollo Theater, involving Al Sharpton and CORE National Chairman Roy Innis. The exchange between the two men culminated in Innis shoving Sharpton into his chair, knocking him to the floor and Downey intervening to separate the pair.
Because of the controversial format and content of the show, distributor MCA Television had problems selling the show to a number of stations and advertisers. Even Downey's affiliates, many of which were low-rated independent television stations in small to medium markets, were so fearful of advertiser and viewer backlash that they would air one or even two local disclaimers during the broadcast. 
During one controversial episode Downey introduced his gay brother, Tony Downey, to his studio audience and informed them Tony was HIV positive. During the episode Downey stated he was afraid his audience would abandon him if they knew he had a gay brother, but then said he did not care.
"The Washington Post" wrote about him, "Suppose a maniac got hold of a talk show. Or need we suppose?" David Letterman said, "I'm always amazed at what people will fall for. We see this every ten or twelve years, an attempt at this, and I guess from that standpoint I don't quite understand why everybody's falling over backwards over the guy."
Celebrity, cancellation, and bankruptcy.
The success of the show made Downey a pop culture celebrity, leading to an appearance on "Saturday Night Live" in 1988, WrestleMania V in 1989 in which he traded insults with Roddy Piper and Brother Love on "Piper's Pit", and later roles in movies such as "Predator 2" and "". He was also cast in several television roles, often playing tabloid TV hosts or other obnoxious media types. Downey notably starred in the "Tales from the Crypt" episode "Television Terror" which utilized several scenes shot by characters within the story, a format which became popular in horror films nearly a decade later with the found footage genre.
In 1989, Downey released an album of songs based on his show entitled "Morton Downey, Jr. Sings". The album's single, "Zip It!" (a catch-phrase from the TV show, used to quiet an irate guest), became a surprise hit on some college radio stations. Over the course of the 1988–89 television season, his TV show suffered a decline in viewership, resulting from many markets downgrading its time slot; even flagship station WWOR moved Downey's program from its original 9:00 PM slot to 11:30 PM in the fall of 1988. Beginning in January 1989, the time slot immediately following Downey's program was given to the then-new "Arsenio Hall Show". However, following Hall's strong early ratings, the two series swapped time slots several weeks later, thus relegating Downey to 12:30 AM in the number-one television market. 
In late April 1989, he was involved in an incident in a San Francisco International Airport restroom in which he claimed to have been attacked by neo-Nazis who painted a swastika on his face and attempted to shave his head. Some inconsistencies in Downey's account (e.g., the swastika was painted in reverse, suggesting that Downey had drawn it himself in a mirror), and the failure of the police to find supportive evidence, led many to suspect the incident was a hoax and a plea for attention. In July 1989, his show was cancelled, with the owners of the show announcing that the last show had been taped on June 30, and that no new shows would air after September 15, 1989.
At the time of its cancellation, the show was airing on a total of 70 stations across the country, and its advertisers had been reduced primarily to "direct-response" ads (such as 900 chat line and phone sex numbers). In February 1990, Downey filed for bankruptcy in the US Bankruptcy Court for the District of New Jersey.
Later career.
In 1990, Downey resurfaced on CNBC with an interview program called "Showdown", which was followed by three attempted talk radio comebacks: first in 1992 on Washington, D.C. radio station WWRC; then in 1993 on Dallas radio station KGBS, where he would scream insults at his callers. He was also hired as the station's VP of Operations. The following year he returned to CNBC with a short-lived television show, "Downey", in one episode, Downey claimed to have had a psychic communication with O.J. Simpson's murdered ex-wife, Nicole Brown Simpson.
His third – and final – attempt at a talk radio comeback occurred in 1997 on Cleveland radio station WTAM in a late evening time slot. It marked his return to the Cleveland market, where Downey had been a host for crosstown radio station WERE in the early 1980s prior to joining KFBK. This stint came shortly after the surgery for lung cancer that removed one of his lungs. At WTAM, Downey abandoned the confrontational schtick of his TV and previous radio shows, and conducted this program in a much more conversational and jovial manner.
On August 30, 1997, Downey quit his WTAM radio talk show to focus on pursuing legal action against Howard Stern. Downey had accused Stern of spreading rumors that he resumed his smoking habits, to which publicist Les Schecter retorted, "He hasn't picked up a cigarette." His replacement was former WERE host Rick Gilmour.
Following his death, news reports and obituaries incorrectly (according to the "Orange County Register") credited him as the composer of "Wipe Out." As of 2008, Downey's official website (and others) continue to make this claim. Prior to Downey's death, "Spin" in April 1989 had identified the "Wipe Out" authorship as a myth.
Controversies.
In 1984, at KFBK radio, Downey used the word "Chinaman" while telling a joke. His use of the word upset portions of the sizable Asian community in Sacramento. One Asian-American city councilman called for an apology and pressured the station for Downey's resignation. Downey refused to apologize and was forced to resign with Rush Limbaugh taking his place.
Downey was sued for allegedly appropriating the words and music to his theme song from two songwriters. He was sued for $40 million after bringing then-stripper Kellie Everts onto the show and calling her a "slut," a "pig," a "hooker," and a "tramp," claiming that she had venereal diseases, and banging his pelvis against hers.
In April 1988, he was arraigned on criminal charges for allegedly attacking a gay guest on his show, in a never-aired segment. In another lawsuit, he was accused of slandering a newscaster (a former colleague), and of indecently exposing himself to her and slapping her. Downey punched Stuttering John during an interview done for "The Howard Stern Show", while also shouting verbal insults at John, referring to him as an "uneducated slob". The situation then began to evolve into a brawl between the two until Downey had to be pulled off of John by security; the entire incident was caught on camera. When an "Inside Edition" camera crew approached Downey in 1989 to question him about his involvement in an alleged business scam, Downey grabbed the boom mike and struck the soundman's head with it.
In his later years, Downey expressed remorse for some of the extreme theatrics of his TV show, as well as various incidents outside the studio, including the "Inside Edition" confrontation. However, he also claimed that his show was of a higher quality and not as "sleazy" as Jerry Springer's show.
"Évocateur: The Morton Downey Jr. Movie".
Released in 2012, the documentary film "" touches upon Downey's upbringing and formative years in radio and politics before launching into the history of "The Morton Downey Jr. Show" and Downey's influence on trash TV. The film also looks at Downey's relationship with Al Sharpton and other important 80s figures, as well as Downey's role as a predecessor for commentators like Glenn Beck and Rush Limbaugh.
Personal life.
Downey was married four times and had four children from three of those marriages. With wife Helen, he had Melissa; with Joan, he had daughters Tracey and Kelli; and, with fourth wife Lori, he had daughter Seanna Micaela. He and Lori met when she appeared as a dancer in a show he attended in Atlantic City. According to Terry Pluto's book, "Loose Balls", Downey was one of the owners of the New Orleans Buccaneers basketball team in the American Basketball Association in the late 1960s. Downey was also president and co-founder of the proposed World Baseball Association in 1974.
Legacy.
In 1998, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.
In the "Super Mario" video game series, the Koopaling Morton Koopa, Jr. was named after him and has a large mouth in reference to Downey's penchant for shouting during his talk show.
Death.
In June 1996, Downey was diagnosed with lung cancer while being treated for pneumonia, and had one of his lungs removed. He did a complete about-face on the issue of tobacco use, going from a one-time member of the National Smokers Alliance to a staunch anti-smoking activist. He continued to speak against smoking until his death from lung cancer and pneumonia in 2001.
After being diagnosed with lung cancer, he commented, "I had spawned a generation of kids to think it was cool to smoke a cigarette. Kids walked up to me until a matter of weeks ago, they'd have a cigarette in their hand and they'd say, 'Hey, Mort,' or, 'Hey, Mouth, autograph my cigarette.' And I'd do it." He also blamed tobacco companies for lying to consumers about cigarettes.

</doc>
<doc id="18886" url="https://en.wikipedia.org/wiki?curid=18886" title="List of male singles tennis players">
List of male singles tennis players

This is a list of top international male singles tennis players, both past and present.
It includes players who have been officially ranked among the top 25 singles players in the world during the "Open Era"; been ranked in the top 10 prior to the Open Era; have been a singles quarterfinalist or better at a Grand Slam tournament; have reached the finals of the season-ending event; or have been singles medalists at the Olympics.
If a player has reached the quarterfinals or better at a specific Grand Slam event more than once; or has reached the finals or better at the season-ending event more than once, only his best result(s) will be listed.
Players who have won more than one Grand Slam singles title or have been ranked world no. 1 in singles have been put in bold font. Players who are still active on the tour have been put in "italics".

</doc>
<doc id="18887" url="https://en.wikipedia.org/wiki?curid=18887" title="Metaphilosophy">
Metaphilosophy

Metaphilosophy (sometimes called philosophy of philosophy) is "the investigation of the nature of philosophy". Its subject matter includes the aims of philosophy, the boundaries of philosophy, and its methods. It is considered by some to be a subject apart from philosophy, while others see it as automatically a part of philosophy, and still others see it as a combination of these subjects. The interest in metaphilosophy led to the establishment of the journal "Metaphilosophy" in January 1970.
Relationship to philosophy.
Some philosophers consider metaphilosophy to be a subject apart from philosophy, above or beyond it, while others object to that idea. Timothy Williamson argues that the philosophy of philosophy is "automatically part of philosophy", as is the philosophy of anything else. Nicholas Bunnin and Jiyuan Yu write that the separation of first- from second-order study has lost popularity as philosophers find it hard to observe the distinction. As evidenced by these contrasting opinions, debate persists as to whether the evaluation of the nature of philosophy is 'second order philosophy' or simply 'plain philosophy'.
Many philosophers have expressed doubts over the value of metaphilosophy. Among them is Gilbert Ryle : "preoccupation with questions about methods tends to distract us from prosecuting the methods themselves. We run as a rule, worse, not better, if we think a lot about our feet. So let us... not speak of it all but just do it."
Terminology.
The designations "metaphilosophy" and "philosophy of philosophy" have a variety of meanings, sometimes taken to be synonyms, and sometimes seen as distinct.
Morris Lazerowitz claims to have created the term 'metaphilosophy' around 1940 and used it in print in 1942. Lazerowitz proposed that metaphilosophy is 'the investigation of the nature of philosophy.' Earlier uses have been found in translations from the French. The term is derived from Greek word "meta" μετά ("after", "beyond", "with") and "philosophía" φιλοσοφία ("love of wisdom").
The term 'metaphilosophy' is used by Paul Moser in the sense of a 'second-order' or more fundamental undertaking than philosophy itself, in the manner suggested by Charles Griswold:
This usage was considered nonsense by Ludwig Wittgenstein, who rejected the analogy between metalanguage and a metaphilosophy. As expressed by Martin Heidegger:
Some other philosophers treat the prefix "meta" as simply meaning '"about..."', rather than as referring to a metatheoretical 'second-order' form of philosophy, among them Rescher and Double. Others, such as Williamson, prefer the term "'philosophy of philosophy instead of 'metaphilosophy' as it avoids the connotation of a 'second-order' discipline that looks down on philosophy, and instead denotes something that is a part of it. Joll suggests that to take metaphilosophy as 'the application of the methods of philosophy to philosophy itself' is too vague, while the view that sees metaphilosophy as a 'second-order' or more abstract discipline, outside philosophy, "is narrow and tendentious".
In the analytical tradition, the term "metaphilosophy" is mostly used to tag commenting and research on previous works as opposed to original contributions towards solving philosophical problems.
Writings.
Ludwig Wittgenstein wrote about the nature of philosophical puzzles and philosophical understanding. He suggested philosophical errors arose from confusions about the nature of philosophical inquiry. In the "Philosophical Investigations", Wittgenstein wrote that there is not a metaphilosophy in the sense of a metatheory of philosophy.
C. D. Broad distinguished Critical from Speculative philosophy in his "The Subject-matter of Philosophy, and its Relations to the special Sciences," in "Introduction to Scientific Thought", 1923. Curt Ducasse, in "Philosophy as a Science", examines several views of the nature of philosophy, and concludes that philosophy has a distinct subject matter: appraisals. Ducasse's view has been among the first to be described as 'metaphilosophy'.
Henri Lefebvre in "Metaphilosophie" (1965) argued, from a marxian standpoint, in favor of an "ontological break", as a necessary methodological approach for critical social theory (whilst criticizing Louis Althusser's "epistemological break" with subjective marxism, which represented a fundamental theoretical tool for the school of marxist structuralism).
Paul Moser writes that typical metaphilosophical discussion includes determining the conditions under which a claim can be said to be a philosophical one. He regards meta-ethics, the study of ethics, to be a form of metaphilosophy, as well as meta-epistemology, the study of epistemology.
Topics.
Many sub-disciplines of philosophy have their own branch of 'metaphilosophy', examples being Meta-aesthetics, Meta-epistemology, Meta-ethics, Meta-ontology, and so forth. However, some topics within 'metaphilosophy' cut across the various subdivisions of philosophy to consider fundamentals important to all its sub-disciplines. Some of these are mentioned below.
Aims.
Some philosophers (e.g. existentialists, pragmatists) think philosophy is ultimately a practical discipline that should help us lead meaningful lives by showing us who we are, how we relate to the world around us and what we should do. Others (e.g. analytic philosophers) see philosophy as a technical, formal, and entirely theoretical discipline, with goals such as "the disinterested pursuit of knowledge for its own sake". Other proposed goals of philosophy include "discover the absolutely fundamental reason of everything it investigates", "making explicit the nature and significance of ordinary and scientific beliefs", and unifying and transcending the insights given by science and religion. Others proposed that philosophy is a complex discipline because it has 4 or 6 different dimensions.
Boundaries.
Defining philosophy and its boundaries is itself problematic; Nigel Warburton has called it "notoriously difficult". There is no straightforward definition, and most interesting definitions are controversial. As Bertrand Russell wrote:
While there is some agreement that philosophy involves general or fundamental topics, there is no clear agreement about a series of demarcation issues, including:
Methods.
Philosophical method (or philosophical methodology) is the study of how to do philosophy. A common view among philosophers is that philosophy is distinguished by the ways that philosophers follow in addressing philosophical questions. There is not just one method that philosophers use to answer philosophical questions.
Recently, some philosophers have cast doubt about intuition as a basic tool in philosophical inquiry, from Socrates up to contemporary philosophy of language. In "Rethinking Intuition" various thinkers discard intuition as a valid source of knowledge and thereby call into question 'a priori' philosophy. Experimental philosophy is a form of philosophical inquiry that makes at least partial use of empirical research—especially "opinion polling"—in order to address persistent philosophical questions. This is in contrast with the methods found in analytic philosophy, whereby some say a philosopher will sometimes begin by appealing to his or her intuitions on an issue and then form an argument with those intuitions as premises. However, disagreement about what experimental philosophy can accomplish is widespread and several philosophers have offered criticisms. One claim is that the empirical data gathered by experimental philosophers can have an indirect effect on philosophical questions by allowing for a better understanding of the underlying psychological processes which lead to philosophical intuitions.
Progress.
A prominent question in metaphilosophy is that of whether or not philosophical progress occurs and more so, whether such progress in philosophy is even possible. It has even been disputed, most notably by Ludwig Wittgenstein, whether genuine philosophical problems actually exist. The opposite has also been claimed, for example by Karl Popper, who held that such problems do exist, that they are solvable, and that he had actually found definite solutions to some of them.

</doc>
<doc id="18888" url="https://en.wikipedia.org/wiki?curid=18888" title="Mandolin">
Mandolin

A mandolin ( ; literally "small mandola") is a musical instrument in the lute family and is usually plucked with a plectrum or "pick". It commonly has four courses of doubled metal strings tuned in unison (8 strings), although five (10 strings) and six (12 strings) course versions also exist. The courses are normally tuned in a succession of perfect fifths. It is the soprano member of a family that includes the mandola, octave mandolin, mandocello and mandobass.
There are many styles of mandolin, but three are common, the "Neapolitan" or "round-backed" mandolin, the "carved-top" mandolin and the "flat-backed" mandolin. The round-back has a deep bottom, constructed of strips of wood, glued together into a bowl. The carved-top or "arch-top" mandolin has a much shallower, arched back, and an arched top—both carved out of wood. The flat-backed mandolin uses thin sheets of wood for the body, braced on the inside for strength in a similar manner to a guitar. Each style of instrument has its own sound quality and is associated with particular forms of music. Neapolitan mandolins feature prominently in European classical music and traditional music. Carved-top instruments are common in American folk music and bluegrass music. Flat-backed instruments are commonly used in Irish, British and Brazilian folk music. Some modern Brazilian instruments feature an extra fifth course tuned a fifth lower than the standard fourth course.
Other mandolin varieties differ primarily in the number of strings and include four-string models (tuned in fifths) such as the Brescian and Cremonese, six-string types (tuned in fourths) such as the Milanese, Lombard and the Sicilian and 6 course instruments of 12 strings (two strings per course) such as the Genoese. There has also been a twelve-string (three strings per course) type and an instrument with sixteen-strings (four strings per course).
Much of mandolin development revolved around the soundboard (the top). Pre-mandolin instruments were quiet instruments, strung with as many as six courses of gut strings, and were plucked with the fingers or with a quill. However, modern instruments are louder—using four courses of metal strings, which exert more pressure than the gut strings. The modern soundboard is designed to withstand the pressure of metal strings that would break earlier instruments. The soundboard comes in many shapes—but generally round or teardrop-shaped, sometimes with scrolls or other projections. There is usually one or more "sound holes" in the soundboard, either round, oval, or shaped like a calligraphic "F" (f-hole). A round or oval sound hole may be covered or bordered with decorative rosettes or purfling.
History.
Mandolins evolved from the lute family in Italy during the 17th and 18th centuries, and the deep bowled mandolin, produced particularly in Naples, became common in the 19th century.
Early precursors.
Dating to around c. 13,000 BC, a cave painting in the Trois Frères cave in France depicts what some believe to be a musical bow, a hunting bow used as a single-stringed musical instrument.
From the musical bow, families of stringed instruments developed; since each string played a single note, adding strings added new notes, creating bow harps, harps and lyres. In turn, this led to being able to play dyads and chord. Another innovation occurred when the bow harp was straightened out and a bridge used to lift the strings off the stick-neck, creating the lute.
First lutes.
The plucked family of stringed instruments included lute-like instruments in Mesopotamia prior to 3000 BC . A cylinder seal from c. 3100 BC or earlier (now in the possession of the British Museum) shows what is thought to be a woman playing a stick lute.
The Mesopotamian lutes developed into a long variety and a short. The line of short lutes was further developed to the east of Mesopotamia, in Bactria and Gandhara, into a short, almond-shaped lute.
Persian barbat, Arab oud.
Andalusia.
Bactria and Gandhara became part of the Sasanian Empire (224–651 AD). Under the Sasanians, a short almond shaped lute from Bactria came to be called the barbat or barbud, which was developed into the later Islamic world's "oud" or "ud". When the Moors conquered Andalusia in 711 AD, they brought their ud along, into a country that had already known a lute tradition under the Romans, the pandura.
During the 8th and 9th centuries, many musicians and artists from across the Islamic world flocked to Iberia. Among them was Abu l-Hasan ‘Ali Ibn Nafi‘ (789-857), a prominent musician who had trained under Ishaq al-Mawsili (d. 850) in Baghdad and was exiled to Andalusia before 833 AD. He taught and has been credited with adding a fifth string to his oud and with establishing one of the first schools of music in Córdoba.
By the 11th century, Muslim Iberia had become a center for the manufacture of instruments. These goods spread gradually to Provence, influencing French troubadours and trouvères and eventually reaching the rest of Europe.
From Sicily to Germany.
Beside the introduction of the lute to Spain (Andalusia) by the Moors, another important point of transfer of the lute from Arabian to European culture was Sicily, where it was brought either by Byzantine or later by Muslim musicians. There were singer-lutenists at the court in Palermo following the Norman conquest of the island from the Muslims, and the lute is depicted extensively in the ceiling paintings in the Palermo’s royal Cappella Palatina, dedicated by the Norman King Roger II of Sicily in 1140. His Hohenstaufen grandson Frederick II, Holy Roman Emperor (1194 - 1250) continued integrating Muslims into his court, including Moorish musicians. By the 14th century, lutes had disseminated throughout Italy and, probably because of the cultural influence of the Hohenstaufen kings and emperor, based in Palermo, the lute had also made significant inroads into the German-speaking lands.
European lute beginnings.
A distinct European tradition of lute development is noticeable in pictures and sculpture from the 13th century onward. As early as the beginning of the 14th century, strings were doubled into courses on the miniature lute or gittern, used throughout Europe. The small zigzag-shaped soundhole became a round soundhole covered with a decoration. The "mandore", appeared in the late 16th century, and the mandolino or Baroque mandolin in the 17th century.
Development in Italy, birth of Neapolitan mandolin.
The mandore was not a final form, and the design was tinkered with wherever it was built. The Italians redesigned it and produced the "mandolino" (a small catgut-strung mandola with 4, 5 or 6 courses tuned e' a' d" g"; b e a d g; G b e a d g, that we may call "Baroque mandolin" and always played with finger-style, never with a plectrum until the second half of the XVIII century. At this point, all such instruments were strung with gut strings.
Vinaccia.
First metal-string mandolins.
The first evidence of modern metal-string mandolins is from literature regarding popular Italian players who travelled through Europe teaching and giving concerts. Notable are Signor Leone and Giovanni Battista Gervasio, who travelled widely between 1750 and 1810. This, with the records gleaned from the Italian Vinaccia family of luthiers in Naples, Italy, led some musicologists to believe that the modern steel-string mandolins were developed in Naples by the Vinaccia family.
There is confusion currently as to the name of the eldest Vinaccia luthier who first ran the shop. His name has been put forth as Gennaro Vinaccia (active c. 1710 to c. 1788) and Nic. Vinaccia. His son Antonio Vinaccia was active c. 1734 to c. 1796. An early extant example of a mandolin is one built by Antonio Vinaccia in 1759, which resides at the University of Edinburgh. Another is by Giuseppe Vinaccia, built in 1893, is also at the University of Edinburgh. The earliest extant mandolin was built in 1744 by Antonio's son, Gaetano Vinaccia. It resides in the Conservatoire Royal de Musique in Brussels, Belgium.
Family mandolin modified to create Neapolitan mandolin.
Gaetano's son, Pasquale Vinaccia (b.1806-d.1881), modernized the mandolin, adding features, creating the "Neapolitan" mandolin c. 1835. Pasquale remodeled, raised and extended the fingerboard to 17 frets, introduced stronger wire strings made of high-tension steel and substituted a machine head for the friction tuning pegs, then standard. The new wire strings required that he strengthen the mandolin's body, and he deepened the mandolin's bowl, giving the tonal quality more resonance.
Calace, Embergher and others.
Other luthiers who built mandolins included Raffaele Calace (1863 onwards) in Naples, Luigi Embergher (1856–1943) in Rome and Arpino, the Ferrari family (1716 onwards, also originally mandolino makers) in Rome, and De Santi (1834–1916) in Rome. The Neapolitan style of mandolin construction was adopted and developed by others, notably in Rome, giving two distinct but similar types of mandolin – Neapolitan and Roman.
Rising and falling fortunes.
First wave.
The transition from the mandolino to the mandolin began around 1744 with the designing of the metal-string mandolin by the Vinaccia family, 3 brass strings and one of gut, using friction tuning pegs on a fingerboard that sat "flush" with the sound table. The mandolin grew in popularity over the next 60 years, in the streets where it was used by young men courting and by street musicians, and in the concert hall. After the Napoleonic Wars of 1815, however, its popularity began to fall. The 19th century produced some prominent players, including Bartolomeo Bortolazzi of Venice and Pietro Vimercati. However, professional virtuosity was in decline, and the mandolin music changed as the mandolin became a folk instrument; "the large repertoire of notated instrumental music for the mandolino and the mandoline was completely forgotten". The export market for mandolins from Italy dried up around 1815, and when Carmine de Laurentiis wrote a mandolin method in 1874, the "Music World" magazine wrote that the mandolin was "out of date." Salvador Léonardi mentioned this decline in his 1921 book, "Méthode pour Banjoline ou Mandoline-Banjo", saying that the mandolin had been declining in popularity from previous times.
It was during this slump in popularity (specifically in 1835) that Pasquale Vinaccia made his modifications to the instrument that his family made for generations, creating the Neapolitan mandolin. The mandolin was largely forgotten outside of Italy by that point, but the stage was set for it to become known again, starting with the Paris Exposition in 1878.
Second wave, the "Golden Age" of mandolins.
[[File:Estudiantina Espagnola au jardin des Tuileries 1878.JPG|thumb|right|
The Parisian crowd with the Estudiantina Espanola during Mardi Gras March 5, 1878 at the Tuileries Gardens. Seven days later they attracted a crowd of 50,000 in the streets of Paris.]]
Beginning with the Paris Exposition of 1878, the instrument's popularity rebounded. The Exposition was one of many stops for a popular new performing group the "Estudiantes Españoles" ("Spanish Students"). They danced and played guitars, violins and the bandurria, which became confused with the mandolin. Along with the energy and awareness created by the day's hit sensation, a wave of Italian mandolinists travelled Europe in the 1880s and 1890s and in the United States by the mid-1880s, playing and teaching their instrument. The instrument's popularity continued to increase during the 1890s and mandolin popularity was at its height in "early years of the 20th century." Thousands were taking up the instrument as a pastime, and it became an instrument of "society", taken up by young men and women. Mandolin orchestras were formed worldwide, incorporating not only the mandolin family of instruments, but also guitars, double basses and zithers.
That era (from the late 19th century into the early 20th century) has come to be known as the "Golden Age" of the mandolin. The term is used online by mandolin enthusiasts to name the time period when the mandolin had become popular, when mandolin orchestras were being organized worldwide, and new and high-quality instruments were increasingly common.
After the First World War, the instrument's popularity again fell, although gradually. Reasons cited included the rise of Jazz, for which the instrument was too quiet. Also the changing pace of life was cited, as people became busier and as modern conveniences (phonograph records, bicycle and automobiles, outdoor sports) competed with learning to play an instrument for fun.
Aftermath.
The second decline was not as complete as the first. Thousands of people had learned to play the instrument. Even as the second wave of mandolin popularity declined in the early 20th century, new versions of the mandolin began to be used in new forms of music. Luthiers created the resonator mandolin, the flatback mandolin, the carved-top or arched-top mandolin, the mandolin-banjo and the electric mandolin. Musicians began playing it in Celtic, Bluegrass, Jazz and Rock-n-Roll styles — and "Classical" too.
Construction.
Mandolins have a body which acts as a resonator, attached to a neck. The resonating body may be shaped as a bowl () or a box (). Traditional Italian mandolins, such as the Neapolitan mandolin, meet the necked bowl description. The necked box instruments include the carved top mandolins and the flatback mandolins.
Strings run between mechanical tuning machines at the top of the neck to a tailpiece that anchors the other end of the strings. The strings are suspended over the neck and soundboard and pass over a floating bridge. The bridge is kept in contact with the soundboard by the downward pressure from the strings. The neck is either flat or has a slight radius, and is covered with a fingerboard with frets. The action of the strings on the bridge causes the soundboard to vibrate, producing sound.
Like any plucked instrument, mandolin notes decay to silence rather than sound out continuously as with a bowed note on a violin, and mandolin notes decay faster than larger stringed instruments like the guitar. This encourages the use of tremolo (rapid picking of one or more pairs of strings) to create sustained notes or chords. The mandolin's paired strings facilitate this technique: the plectrum (pick) strikes each of a pair of strings alternately, providing a more full and continuous sound than a single string would.
Various design variations and amplification techniques have been used to make mandolins comparable in volume with louder instruments and orchestras, including the creation of mandolin-banjo hybrid with the louder banjo, adding metal resonators (most notably by Dobro and the National String Instrument Corporation) to make a resonator mandolin, and amplifying electric mandolins through amplifiers.
Tuning.
A variety of different tunings are used. Usually, courses of 2 adjacent strings are tuned in unison. The most common tuning by far, G3-D4-A4-E5, is the same as violin tuning:
Note that the numbers of Hz shown above assume a 440 Hz A, standard in most parts of the western world. Some players use As up to 10 Hz above or below a 440, mainly outside of the United States.
Other tunings exist, including "cross-tunings," in which the usually doubled string runs are tuned to different pitches. Additionally, guitarists may sometimes tune a mandolin to mimic a portion of the intervals on a standard guitar tuning to achieve familiar fretting patterns.
Mandolin family.
Soprano.
The mandolin is the soprano member of the mandolin family, as the violin is the soprano member of the violin family. Like the violin, its scale length is typically about 13 inches (330 mm). Modern American mandolins modelled after Gibsons have a longer scale, about 13-7/8" (352 mm). The strings in each of its double-strung courses are tuned in unison, and the courses use the same tuning as the violin: G3-D4-A4-E5.
Contrabass.
The relatively rare eight-string mandobass, or tremolo-bass also exists, with double courses like the rest of the mandolin family, and is tuned either G1-D2-A2-E3, two octaves lower than the mandolin, or C1-G1-D2-A2, two octaves below the mandola.
Variations.
Bowlback.
Bowlback mandolins (also known as roundbacks), are used worldwide. They are most commonly manufactured in Europe, where the long history of mandolin development has created local styles. However, Japanese luthiers also make them.
Neapolitan and Roman styles.
The Neapolitan style has an almond-shaped body resembling a bowl, constructed from curved strips of wood. It usually has a bent sound table, canted in two planes with the design to take the tension of the 8 metal strings arranged in four courses. A hardwood fingerboard sits on top of or is flush with the sound table. Very old instruments may use wooden tuning pegs, while newer instruments tend to use geared metal tuners. The bridge is a movable length of hardwood. A pickguard is glued below the sound hole under the strings. European roundbacks commonly use a 13-inch scale instead of the 13.876 common on archtop Mandolins.
Intertwined with the Neapolitan style is the Roman style mandolin, which has influenced it. The Roman mandolin had a fingerboard that was more curved and narrow. The fingerboard was lengthened over the sound hole for the e strings, the high pitched strings. The shape of the back of the neck was different, less rounded with an edge, the bridge was curved making the g strings higher. The Roman mandolin had mechanical tuning gears before the Neapolitan.
Prominent Italian manufacturers include Vinaccia (Naples), Embergher (Rome) and Calace (Naples). Other modern manufacturers include Lorenzo Lippi (Milan), Hendrik van den Broek (Netherlands), Brian Dean (Canada), Salvatore Masiello and Michele Caiazza (La Bottega del Mandolino) and Ferrara, Gabriele Pandini.
Lombardic styles, Milanese and Brescian.
Another family of bowlback mandolins came from Milan and Lombardy. These mandolins are closer to the mandolino or mandore than other modern mandolins. They are shorter and wider than the standard Neapolitan mandolin, with a shallow back. The instruments have 6 strings, 3 wire treble-strings and 3 gut or wire-wrapped-silk bass-strings. The strings ran between the tuning pegs and a bridge that was glued to the soundboard, as a guitar's. The Lombardic mandolins were tuned g b e' a' d" g". A developer of the Milanese stye was Antonio Monzino (Milan) and his family who made them for 6 generations.
Samuel Adelstein described the Lombardi mandolin in 1893 as wider and shorter than the Neapolitan mandolin, with a shallower back and a shorter and wider neck, with six single strings to the regular mandolin's set of 4. The Lombardi was tuned C, D, A, E, B, G. The strings were fastened to the bridge like a guitar's. There were 20 frets, covering three octaves, with an additional 5 notes. When Adelstein wrote, there were no nylon strings, and the gut and single strings "do not vibrate so clearly and sweetly as the double steel string of the Neapolitan."
Brescian Mandolin.
Brescian mandolins that have survived in museums have four gut strings instead of six. The mandolin was tuned in fifths, like the Neapolitan mandolin.
Cremonese mandolin.
In his 1805 mandolin method, "Anweisung die Mandoline von selbst zu erlernen nebst einigen Uebungsstucken von Bortolazzi", Bartolomeo Bortolazzi popularised the Cremonese mandolin, which had four single-strings and a fixed bridge, to which the strings were attached. Bortolazzi said in this book that the new wire strung mandolins were uncomfortable to play, when compared with the gut-string instruments. Also, he felt they had a "less pleasing...hard, zither-like tone" as compared to the gut string's "softer, full-singing tone."
He favored the four single strings of the Cremonese instrument, which were tuned the same as the Neapolitan.
Manufacturers outside of Italy.
In the United States, when the bowlback was being made in numbers, Lyon and Healy was a major manufacturer, especially under the "Washburn" brand. Other American manufacturers include Martin, Vega, and Larson Brothers.
In Canada, Brian Dean has manufactured instruments in Neapolitan, Roman, German and American styles but is also known for his original 'Grand Concert' design created for American virtuoso Joseph Brent.
German manufacturers include Albert & Mueller, Dietrich, Klaus Knorr, Reinhold Seiffert and Alfred Woll. The German bowlbacks use a style developed by Seiffert, with a larger and rounder body.
Japanese brands include Kunishima and Suzuki. Other Japanese manufacturers include Oona, Kawada, Noguchi, Toichiro Ishikawa, Rokutaro Nakade, Otiai Tadao, Yoshihiko Takusari, Nokuti Makoto, Watanabe, Kanou Kadama and Ochiai.
Archtop.
At the very end of the 19th century, a new style, with a carved top and back construction inspired by violin family instruments began to supplant the European-style bowl-back instruments in the United States. This new style is credited to mandolins designed and built by Orville Gibson, a Kalamazoo, Michigan luthier who founded the "Gibson Mandolin-Guitar Manufacturing Co., Limited" in 1902. Gibson mandolins evolved into two basic styles: the Florentine or F-style, which has a decorative scroll near the neck, two points on the lower body and usually a scroll carved into the headstock; and the A-style, which is pear shaped, has no points and usually has a simpler headstock.
These styles generally have either two f-shaped soundholes like a violin (F-5 and A-5), or an oval sound hole (F-4 and A-4 and lower models) directly under the strings. Much variation exists between makers working from these archetypes, and other variants have become increasingly common. Generally, in the United States, Gibson F-hole F-5 mandolins and mandolins influenced by that design are strongly associated with bluegrass, while the A-style is associated other types of music, although it too is most often used for and associated with bluegrass. The F-5's more complicated woodwork also translates into a more expensive instrument.
Internal bracing to support the top in the F-style mandolins is usually achieved with parallel tone bars, similar to the bass bar on a violin. Some makers instead employ "x-bracing," which is two tone bars mortised together to form an X. Some luthiers now using a "modified x-bracing" that incorporates both a tone bar and x-bracing.
Numerous modern mandolin makers build instruments that largely replicate the Gibson F-5 Artist models built in the early 1920s under the supervision of Gibson acoustician Lloyd Loar. Original Loar-signed instruments are sought after and extremely valuable. Other makers from the Loar period and earlier include Lyon and Healy, Vega and Larson Brothers. Some notable modern American carved mandolin manufacturers include, in addition to Kay, Gibson, Weber, Monteleone and Collings. Mandolins from other countries include The Loar (China), Santa, Rosa (China), Michael Kelly (Korea), Eastman (China), Kentucky (China), Heiden (Canada), Gilchrist (Australia) and Morgan Monroe (China).
Flatback.
Flatback mandolins use a thin sheet of wood with bracing for the back, as a guitar uses, rather than the bowl of the bowlback or the arched back of the carved mandolins.
Like the bowlback, the flatback has a round sound hole. This has been sometimes modified to an elongated hole, called a "D" hole. The body has a rounded almond shape with flat or sometimes canted soundboard.
The type was developed in Europe in the 1850s. The French and Germans called it a Portuguese mandolin, although they also developed it locally. The Germans used it in Wandervogel.
The bandolim is commonly used wherever the Spanish and Portuguese took it: in South America, in Brazil (Choro) and in the Philippines.
In the early 1970s English luthier Stefan Sobell developed a large-bodied, flat-backed mandolin with a carved soundboard, based on his own cittern design; this is often called a 'Celtic' mandolin.
American forms include the Army-Navy mandolin, the flatiron and the pancake mandolins.
Double top.
A variation of the flatback has a double top which encloses a resonating chamber, sound holes on the side and a convex back. It is made by one manufacturer in Israel, luthier Arik Kerman. Players include Avi Avital, Alon Sariel, Jacob Reuven, and Tom Cohen.
Tone.
The tone of the flatback is described as warm or mellow, suitable for folk music and smaller audiences. The instrument sound does not punch through the other players' sound like a carved top does.
Others.
Mandolinetto.
Other American-made variants include the mandolinetto or Howe-Orme guitar-shaped mandolin (manufactured by the Elias Howe Company between 1897 and roughly 1920), which featured a cylindrical bulge along the top from fingerboard end to tailpiece and the Vega mando-lute (more commonly called a cylinder-back mandolin manufactured by the Vega Company between 1913 and roughly 1927), which had a similar longitudinal bulge but on the back rather than the front of the instrument.
Banjolin or mandolin-banjo.
The mandolin was given a banjo body in an 1882 patent by Benjamin Bradbury of Brooklyn and given the name "banjolin" by John Farris in 1885. Today "banjolin" describes an instrument with four strings, while the version with the four courses of double strings is called a "mandolin-banjo".
Resonator mandolin.
A resonator mandolin or "resophonic mandolin" is a mandolin whose sound is produced by one or more metal cones (resonators) instead of the customary wooden soundboard (mandolin top/face). Historic brands include Dobro and National.
Electric mandolin.
As with almost every other contemporary string instrument, another modern variant is the electric mandolin. These mandolins can have four or five individual or double courses of strings.
They have been around since the late 1920s or early 1930s depending on the brand. They come in solid body and acoustic electric forms.
Instruments have been designed that overcome the mandolin's lack of sustain with its plucked notes. Fender released a model in 1992 with an additional string (a high a, above the e string), a tremolo bridge and extra humbucker pickup (total of two). The result was an instrument capable of playing heavy metal style guitar riffs or violin-like passages with sustained notes that can be adjusted as with an electric guitar.
Playing traditions in Italy and worldwide.
The international repertoire of music for mandolin is almost unlimited, and musicians use it to play various types of music. This is especially true of violin music, since the mandolin has the same tuning as the violin. Following its invention and early development in Italy the mandolin spread throughout the European continent. The instrument was primarily used in a classical tradition with Mandolin orchestras, so called "Estudiantinas" or in Germany "Zupforchestern" appearing in many cities. Following this continental popularity of the mandolin family local traditions appeared outside of Europe in the Americas and in Japan. Travelling mandolin virtuosi like Giuseppe Pettine, Raffaele Calace and Silvio Ranieri contributed to the mandolin becoming a "fad" instrument in the early 20th century. This "mandolin craze" was fading by the 1930s, but just as this practice was falling into disuse, the mandolin found a new niche in American country, old-time music, bluegrass and folk music. More recently, the Baroque and Classical mandolin repertory and styles have benefited from the raised awareness of and interest in Early music, with media attention to classical players such as Israeli Avi Avital, Italian Carlo Aonzo and American Joseph Brent.
Australia.
The earliest references to the mandolin in Australia come from Phil Skinner (1903–1991). In his article "Recollections" he mentions a Walter Stent, who was “active in the early part of the century and organised possibly the first Mandolin Orchestra in Sydney.”
Phil Skinner played a key role in 20th century development of the mandolin movement in Australia, and was awarded an MBE in 1979 for services to music and the community. He was born Harry Skinner in Sydney in 1903 and started learning music at age 10 when his uncle tutored him on the banjo. Skinner began teaching part-time at age 18, until the Great Depression forced him to begin teaching full-time and learn a broader range of instruments. Skinner founded the Sydney Mandolin Orchestra, the oldest surviving mandolin orchestra in Australia.
The Sydney Mandolins (Artistic Director: Adrian Hooper) have contributed greatly to the repertoire through commissioning over 200 works by Australian and International composers. Most of these works have been released on Compact Disks and can regularly be heard on radio stations on the ABC and MBS networks. One of their members, mandolin virtuoso Paul Hooper, has had a number of Concertos written for him by composers such as Eric Gross. He has performed and recorded these works with the Sydney Symphony Orchestra and the Tasmanian Symphony Orchestra. As well, Paul Hooper has had many solo works dedicated to him by Australian composers e.g., Caroline Szeto, Ian Shanahan, Larry Sitsky and Michael Smetanin.
In January 1979, the Federation of Australian Mandolin Ensembles (FAME) Inc. formed. Bruce Morey from Melbourne is the first FAME President. An Australian Mandolin Orchestra toured Germany in May 1980.
Australian popular groups such as My Friend The Chocolate Cake use the mandolin extensively. The McClymonts also use the mandolin, as do Mic Conway's National Junk Band and the Blue Tongue Lizards. Nevertheless, in folk and traditional styles, the mandolin remains more popular in Irish Music and other traditional repertoires.
Belgium.
In the early 20th century several mandolin orchestras (Estudiantinas) were active in Belgium. Today only a few groups remain: Royal Estudiantina la Napolitaine (founded in 1904) in Antwerp, Brasschaats mandoline orkest in Brasschaat and an orchestra in Mons (Bergen). Gerda Abts is a well known mandolin virtuoso in Belgium. She is also mandolin teacher and gives lessons in the music academies of Lier, Wijnegem and Brasschaat. She is now also professor mandolin at the music high school “Koninklijk Conservatorium Artesis Hogeschool Antwerpen”. She also gives various concerts each year in different ensembles. She is in close contact to the Brasschaat mandolin Orchestra. Her site is www.gevoeligesnaar.be
Brazil.
The mandolin has a long and rich tradition in Brazilian folk music ("bandolim" in Portuguese), especially in the style called choro. The composer and mandolin virtuoso Jacob do Bandolim did much to popularize the instrument through many recordings, and his influence continues to the present day. Some contemporary mandolin players in Brazil include Jacob's disciple Déo Rian, and Hamilton de Holanda (the former, a traditional choro-style player, the latter an eclectic innovator).
The mandolin came into Brazil by way of Portugal. Portuguese music has a long tradition of mandolins and mandolin-like instruments (see, for example, the Portuguese guitar).
In Brazilian music, the mandolin is almost exclusively a melody instrument. The cavaquinho, a steel stringed instrument similar to a ukulele provides chordal accompaniment. The mandolin's popularity has risen and fallen with instrumental folk music styles, especially choro. The later part of the 20th century saw a renaissance of choro in Brazil, and with it, a revival of the country's mandolin tradition.
Croatia.
Mandolin is staple of folk and traditional music on Croatian coast.
Czech and Slovak republics.
From Italy mandolin music extended in popularity throughout Europe in the early 20th century, with mandolin orchestras appearing throughout the continent.
In the 21st century an increased interest in bluegrass music, especially in Central European countries such as the Czech Republic and Slovak Republic, has inspired many new mandolin players and builders. These players often mix traditional folk elements with bluegrass.
Finland.
Finland has mandolin players rooted in the folk music scene. Prominent names include Petri Hakala, Seppo Sillanpää and Heikki Lahti, who have taught and recorded albums.
France.
Prior to the "Golden Age of Mandolins", France had a history with the mandolin, with mandolinists playing in Paris until the Napoleonic Wars. The players, teachers and composers included Giovanni Fouchetti, Eduardo Mezzacapo, Gabriele Leon, and Gervasio. During the Golden age itself (1880s-1920s), the mandolin had a strong presence in France. Prominent mandolin players or composers included Jules Cottin and his sister Madeleine Cottin, Jean Pietrapertosa, and Edgar Bara. Paris had dozens of "estudiantina" mandolin orchestras in the early 1900s. Mandolin magazines included "L'Estudiantina", "Le Plectre", "École de la mandolie".
Today, French mandolinists include Patrick Vaillant, a prominent modern player, composer and recording artist for the mandolin, who also organises courses for aspiring players.
Greece.
The mandolin has a long tradition in the Ionian islands (the "Heptanese") and Crete. It has long been played in the Aegean islands outside of the control of the Ottoman Empire. It is common to see choirs accompanied by mandolin players (the "mandolinátes") in the Ionian islands and especially in the cities of Corfu, Zakynthos and Kefalonia. The evolution of the repertoire for choir and mandolins ("kantádes") occurred during Venetian rule over the islands.
On the island of Crete, along with the lyra and the laouto (lute), the mandolin is one of the main instruments used in Cretan Music. It appeared on Crete around the time of the Venetian rule of the island. Different variants of the mandolin, such as the "mantola," were used to accompany the lyra, the violin, and the laouto. Stelios Foustalierakis reported that the mandolin and the "mpoulgari" were used to accompany the lyra in the beginning of the 20th century in the city of Rethimno. There are also reports that the mandolin was mostly a woman's musical instrument. Nowadays it is played mainly as a solo instrument in personal and family events on the Ionian islands and Crete.
India.
Mandolin music was used in Indian Movies as far back as the 1940s by the Raj Kapoor Studios in movies such as Barsaat. The movie Dilwale Dulhania Le Jayenge (1995) used mandolin in several places.
Adoption of the mandolin in Carnatic music is recent and involves an electric instrument. U. Srinivas has, over the last couple of decades, made his version of the mandolin very popular in India and abroad.
Many adaptations of the instrument have been done to cater to the special needs of Indian Carnatic music.
In Indian classical music and Indian light music, the mandolin, which bears little resemblance to the European mandolin, is usually tuned E-B-E-B. As there is no concept of absolute pitch in Indian classical music, any convenient tuning maintaining these relative pitch intervals between the strings can be used. Another prevalent tuning with these intervals is C-G-C-G, which corresponds to Sa-Pa-Sa-Pa in the Indian carnatic classical music style. This tuning corresponds to the way violins are tuned for carnatic classical music. This type of mandolin is also used in Bhangra, dance music popular in Punjabi culture.
Use of the mandolin also spread into Afghanistan and the mandolin is often used in Afghan popular music.
Ireland.
The mandolin has become a more common instrument amongst Irish traditional musicians. Fiddle tunes are readily accessible to the mandolin player because of the equivalent tuning and range of the two instruments, and the practically identical (allowing for the lack of frets on the fiddle) left hand fingerings.
Though almost any variety of acoustic mandolin might be adequate for Irish traditional music, virtually all Irish players prefer flat-backed instruments with oval sound holes to the Italian-style bowl-back mandolins or the carved-top mandolins with f-holes favoured by bluegrass mandolinists. The former are often too soft-toned to hold their own in a session (as well as having a tendency to not stay in place on the player's lap), whilst the latter tend to sound harsh and overbearing to the traditional ear. The f-hole mandolin, however, does come into its own in a traditional session, where its brighter tone cuts through the sonic clutter of a pub. Greatly preferred for formal performance and recording are flat-topped "Irish-style" mandolins (reminiscent of the WWI-era Martin Army-Navy mandolin) and carved (arch) top mandolins with oval soundholes, such as the Gibson A-style of the 1920s.
Noteworthy Irish mandolinists include Andy Irvine (who, like Johnny Moynihan, almost always tunes the top E down to D, to achieve an open tuning of GDAD), Paul Brady, Mick Moloney, Paul Kelly and Claudine Langille. John Sheahan and the late Barney McKenna, respectively fiddle player and tenor banjo player with The Dubliners, are also accomplished Irish mandolin players. The instruments used are either flat-backed, oval hole examples as described above (made by UK luthier Roger Bucknall of Fylde Guitars), or carved-top, oval hole instruments with arched back (made by Stefan Sobell in Northumberland). The Irish guitarist Rory Gallagher often played the mandolin on stage, and he most famously used it in the song "Going To My Hometown."
Israel.
Israel has four especially prominent mandolinists: Avi Avital, Alon Sariel, Jacob Reuven, and Tom Cohen.
Italy.
Important performers in the Italitan tradition include Raffaele Calace (luthier, virtuoso and composer of 180 works for many instruments including mandolin), Pietro Denis (whole also composed "Sonata for mandolin & continuo No. 1 in D major" and "Sonata No. 3"), Giovanni Fouchetti, Gabriele Leone, Carlo Munier (1859-1911), Giuseppe Branzoli (1835-1909), Giovanni Gioviale (1885-1949) and Silvio Ranieri (1882-1956).
Antonio Vivaldi composed a mandolin concerto ("Concerto in C major Op.3 6") and two concertos for two mandolins and orchestra. Wolfgang Amadeus Mozart placed it in his 1787 work "Don Giovanni" and Beethoven created four variations of it. Antonio Maria Bononcini composed "La conquista delle Spagne di Scipione Africano il giovane" in 1707 and George Frideric Handel composed "Alexander Balus" in 1748. Others include Giovani Battista Gervasio ("Sonata in D major for Mandolin and Basso Continuo"), Giuseppe Giuliano ("Sonata in D major for Mandolin and Basso Continuo"), Emanuele Barbella ("Sonata in D major for Mandolin and Basso Continuo"), Domenico Scarlatti ("Sonata n.54 (K.89) in D minor for Mandolin and Basso Continuo"), and Addiego Guerra ("Sonata in G major for Mandolin and Basso Continuo").
More contemporary composers for the mandolin include Giuseppe Anedda (a virtuoso performer and teacher of the first chair of the Conservatory of Italian mandolin), Carlo Aonzo and Dorina Frati.
Japan.
Instruments of the mandolin family are popular in Japan, particularly Neapolitan (round-back) style instruments, and Roman-Embergher style mandolins are still being made there. Japan became seriously interested in mandolins at the beginning of the 20th Century during a process of becoming westernized.
Where interest in the mandolin declined in the United States and parts of Europe after World War I, in Japan there was a boom, with orchestras being formed all over the country.
Connections to the West, including cultural connections with World War II ally Italy, were forming. One musical connection which encouraged mandolin music growth was a visit by mandolin virtuoso Raffaele Calace, who toured extensively at the end of 1924, into 1925, and who gave a performance for the Japanese emperor. Another visiting mandolin virtuoso, Samuel Adelstein, toured from his home in the United States.
The expansion of mandolin use continued after World War II through the late 1960s, and Japan still maintains a strong classical music tradition using mandolins, with active orchestras and university music programs. New orchestras were founded and new orchestral compositions composed. Japanese mandolin orchestras today may consist of up to 40 or 50 members, and can include woodwind, percussion, and brass sections. Japan also maintains an extensive collection of 20th Century mandolin music from Europe and one of the most complete collections of mandolin magazines from mandolin's golden age, purchased by Morishige Takei.
Morishige Takei (1890–1949), who studied Italian at Tokyo College of Language and was a member of the court of Emperor Hirohito, established the mandolin orchestra in the Italian style before World War II. He was also a major composer, with 114 compositions for mandolin.
The military government could not persecute Japanese mandolinists by the authority of Takei So the Japanese mandolin orchestras continued to perform old Italian works after World War II, and they are prosperous today.
Another composer, Jiro Nakano (1902–2000), arranged many of the Italian works for regular orchestras or winds composed before World War II as new repertoires for Japanese mandolin orchestras.
Original compositions for mandolin orchestras were composed increasingly after World War II. Seiichi Suzuki (1901–1980) composed music for early Kurosawa films. Others include Tadashi Hattori (1908–2008), and Hiroshi Ohguri (1918–1982). Ohguri was influenced by Béla Bartók and composed many symphonic works for Japanese mandolin orchestras. Yasuo Kuwahara (1946–2003) used German techniques. Many of his works were published in Germany.
New Zealand.
The Auckland Mandolinata mandolin orchestra was formed in 1969 by Doris Flameling (1932–2004). Soon after arriving from the Netherlands with her family, Doris started teaching guitar and mandolin in West Auckland. In 1969, she formed a small ensemble for her pupils. This ensemble eventually developed into a full size mandolin orchestra, which survives today. Doris was the musical director and conductor of this orchestra for many years. The orchestra is currently led by Bryan Holden (conductor).
The early history of the mandolin in New Zealand is currently being researched by members of the Auckland Mandolinata.
Portugal.
The "bandolim" (Portuguese for "mandolin") was a favourite instrument within the Portuguese bourgeoisie of the 19th century, but its rapid spread took it to other places, joining other instruments. Today you can see mandolins as part of the traditional and folk culture of Portuguese singing groups and the majority of the mandolin scene in Portugal is in Madeira Island. Madeira has over 17 active mandolin Orchestras and Tunas. The mandolin virtuoso Fabio Machado is one of Portugal's most accomplished mandolin players. The Portuguese influence brought the mandolin to Brazil.
South Africa.
Mandolin has been a prominent instrument in the recordings of Johnny Clegg and his bands Juluka and Savuka. Since 1992, Andy Innes has been the mandolinist for Johnny Clegg and Savuka.
Sri Lanka.
The mandolin was brought to Sri Lanka by the Portuguese, who colonized Sri Lanka from 1505-1658. The instrument has been heavily used in baila, a genre of Sri Lankan music which formed from a mixture of Portuguese, African and Sinhala music. For example, the mandolin features prominently in M.S. Fernando's baila song, "Bola Bola Meti".
Turkey.
Turkey has been the home of a mandolin-banjo manufacturer, Cümbüs, since the early 20th century. The country had what was claimed to be its first Mandolin festival in June 2015, and has at least one Mandolin Orchestra.
One professional musician to use the mandolin is Sumru Ağıryürüyen who is known for singing and playing many styles of music including world music, Klezmer, Turkish folk, Balkan folk, blues, jazz, krautrock, protest rock and maqam.
United Kingdom.
The mandolin has been used extensively in the traditional music of England and Scotland for generations. Simon Mayor is a prominent British player who has produced six solo albums, instructional books and DVDs, as well as recordings with his mandolin quartet the Mandolinquents. The instrument has also found its way into British rock music. The mandolin was played by Mike Oldfield (and introduced by Vivian Stanshall) on Oldfield's album "Tubular Bells", as well as on a number of his subsequent albums (particularly prominently on "Hergest Ridge" (1974) and "Ommadawn" (1975)). It was used extensively by the British folk-rock band Lindisfarne, who featured two members on the instrument, Ray Jackson and Simon Cowe, and whose "Fog on the Tyne" was the biggest selling UK album of 1971-1972. The instrument was also used extensively in the UK folk revival of the 1960s and 1970s with bands such as Fairport Convention and Steeleye Span taking it on as the lead instrument in many of their songs. "Maggie May" by Rod Stewart, which hit No. 1 on both the British charts and the Billboard Hot 100, also featured Jackson's playing. It has also been used by other British rock musicians. Led Zeppelin's bassist John Paul Jones is an accomplished mandolin player and has recorded numerous songs on mandolin including "Going to California" and "That's the Way"; the mandolin part on "The Battle of Evermore" is played by Jimmy Page, who composed the song. Other Led Zeppelin songs featuring mandolin are "Hey Hey What Can I Do", and "Black Country Woman." Pete Townshend of The Who played mandolin on the track "Mike Post Theme", along with many other tracks on Endless Wire. McGuinness Flint, for whom Graham Lyle played the mandolin on their most successful single, "When I'm Dead And Gone", is another example. Lyle was also briefly a member of Ronnie Lane's Slim Chance, and played mandolin on their hit "How Come." One of the more prominent early mandolin players in popular music was Robin Williamson in The Incredible String Band. Ian Anderson of Jethro Tull is a highly accomplished mandolin player (beautiful track "Pussy Willow"), as is his guitarist Martin Barre. The popular song "Please Please Please Let Me Get What I Want" by The Smiths featured a mandolin solo played by Johnny Marr. More recently, the Glasgow-based band Sons and Daughters featured the mandolin, played by Ailidh Lennon, on tracks such as "Fight," "Start to End," and "Medicine." British folk-punk icons the Levellers also regularly use the mandolin in their songs. Current bands are also beginning to use the Mandolin and its unique sound - such as South London's Indigo Moss who use it throughout their recordings and live gigs. The mandolin has also featured in the playing of Matthew Bellamy in the rock band Muse. It also forms the basis of Paul McCartney's 2007 hit "Dance Tonight." That was not the first time a Beatle played a mandolin, however; that distinction goes to George Harrison on Gone Troppo, the title cut from the 1982 album of the same name. The mandolin is taught in Lanarkshire by the Lanarkshire Guitar and Mandolin Association to over 100 people. Also more recently hard rock supergroup Them Crooked Vultures have been playing a song based primarily using a mandolin. This song was left off their debut album, and features former Led Zeppelin bassist John Paul Jones.
In the Classical style, performers such as Hugo D'Alton, Alison Stephens and Michael Hooper have continued to play music by British composers such as Michael Finnissy, James Humberstone and Elspeth Brooke.
United States.
Mandolin orchestras and classical-music virtuosos.
The mandolin's popularity in the United States was spurred by the success of a group of touring young European musicians known as the Estudiantina Figaro, or in the United States, simply the "Spanish Students." The group landed in the U.S. on January 2, 1880 in New York City, and played in Boston and New York to wildly enthusiastic crowds. Ironically, this ensemble did not play mandolins but rather bandurrias, which are also small, double-strung instruments that resemble the mandolin. The success of the Figaro Spanish Students spawned other groups who imitated their musical style and costumes. An Italian musician, Carlo Curti, hastily started a musical ensemble after seeing the Figaro Spanish Students perform; his group of Italian born Americans called themselves the "Original Spanish Students," counting on the American public to not know the difference between the Spanish bandurrias and Italian mandolins. The imitators' use of mandolins helped to generate enormous public interest in an instrument previously relatively unknown in the United States.
Mandolin awareness in the United States blossomed in the 1880s, as the instrument became part of a fad that continued into the mid-1920s. According to Clarence L. Partee, the first mandolin made in the United States was made in 1883 or 1884 by Joseph Bohmann, who was an established maker of violins in Chicago. Partee characterized the early instrument as being larger than the European instruments he was used to, with a "peculiar shape" and "crude construction," and said that the quality improved, until American instruments were "superior" to imported instruments. At the time, Partee was using an imported French-made mandolin.
Instruments were marketed by teacher-dealers, much as the title character in the popular musical "The Music Man". Often, these teacher-dealers conducted mandolin orchestras: groups of 4-50 musicians who played various mandolin family instruments. However, alongside the teacher-dealers were serious musicians, working to create a spot for the instrument in classical music, ragtime and jazz. Like the teacher-dealers, they traveled the U.S., recording records, giving performances and teaching individuals and mandolin orchestras. Samuel Siegel played mandolin in Vaudeville and became one of America's preeminent mandolinists. Seth Weeks was an African American who not only taught and performed in the United States, but also in Europe, where he recorded records. Another pioneering African American musician and director who made his start with a mandolin orchestra was composer James Reese Europe. W. Eugene Page toured the country with a group, and was well known for his mandolin and mandola performances. Other names include Valentine Abt, Samuel Adelstein, William Place, Jr., and Aubrey Stauffer.
The instrument was primarily used in an ensemble setting well into the 1930s, and although the fad died out at the beginning of the 1930s, the instruments that were developed for the orchestra found a new home in bluegrass. The famous Lloyd Loar Master Model from Gibson (1923) was designed to boost the flagging interest in mandolin ensembles, with little success. However, The "Loar" became the defining instrument of bluegrass music when Bill Monroe purchased F-5 S/N 73987 in a Florida barbershop in 1943 and popularized it as his main instrument.
The mandolin orchestras never completely went away, however. In fact, along with all the other musical forms the mandolin is involved with, the mandolin ensemble (groups usually arranged like the string section of a modern symphony orchestra, with first mandolins, second mandolins, mandolas, mandocellos, mando-basses, and guitars, and sometimes supplemented by other instruments) continues to grow in popularity. Since the mid-nineties, several public-school mandolin-based guitar programs have blossomed around the country, including Fretworks Mandolin and Guitar Orchestra, the first of its kind. The national organization, Classical Mandolin Society of America, founded by Norman Levine, represents these groups. Prominent modern mandolinists and composers for mandolin in the classical music tradition include Samuel Firstman, Howard Fry, Rudy Cipolla, Dave Apollon, Neil Gladd, Evan Marshall, Marilynn Mair and Mark Davis (the Mair-Davis Duo), Brian Israel, David Evans, Emanuil Shynkman, Radim Zenkl, David Del Tredici and Ernst Krenek.
Bluegrass, Blues, and the jug band.
When Cowan Powers and his family recorded their old-time music from 1924-1926, his daughter Orpha Powers was one of the earliest known southern-music artists to record with the mandolin. By the 1930s, single mandolins were becoming more commonly used in southern string band music, most notably by brother duets such as the sedate Blue Sky Boys (Bill Bolick and Earl Bolick) and the more hard-driving Monroe Brothers (Bill Monroe and Charlie Monroe). However, the mandolin's modern popularity in country music can be directly traced to one man: Bill Monroe, the father of bluegrass music. After the Monroe Brothers broke up in 1939, Bill Monroe formed his own group, after a brief time called the Blue Grass Boys, and completed the transition of mandolin styles from a "parlor" sound typical of brother duets to the modern "bluegrass" style. He joined the Grand Ole Opry in 1939 and its powerful clear-channel broadcast signal on WSM-AM spread his style throughout the South, directly inspiring many musicians to take up the mandolin. Monroe famously played Gibson F-5 mandolin, signed and dated July 9, 1923, by Lloyd Loar, chief acoustic engineer at Gibson. The F-5 has since become the most imitated tonally and aesthetically by modern builders.
Monroe's style involved playing lead melodies in the style of a fiddler, and also a percussive chording sound referred to as "the chop" for the sound made by the quickly struck and muted strings. He also perfected a sparse, percussive blues style, especially up the neck in keys that had not been used much in country music, notably B and E. He emphasized a powerful, syncopated right hand at the expense of left-hand virtuosity. Monroe's most influential follower of the second generation is Frank Wakefield and nowadays Mike Compton of the Nashville Bluegrass Band and David Long, who often tour as a duet. Tiny Moore of the Texas Playboys developed an electric five-string mandolin and helped popularize the instrument in Western Swing music.
Other major bluegrass mandolinists who emerged in the early 1950s and are still active include Jesse McReynolds (of Jim and Jesse) who invented a syncopated banjo-roll-like style called crosspicking—and Bobby Osborne of the Osborne Brothers, who is a master of clarity and sparkling single-note runs. Highly respected and influential modern bluegrass players include Herschel Sizemore, Doyle Lawson, and the multi-genre Sam Bush, who is equally at home with old-time fiddle tunes, rock, reggae, and jazz. Ronnie McCoury of the Del McCoury Band has won numerous awards for his Monroe-influenced playing. The late John Duffey of the original Country Gentlemen and later the Seldom Scene did much to popularize the bluegrass mandolin among folk and urban audiences, especially on the east coast and in the Washington, D.C. area.
Jethro Burns, best known as half of the comedy duo Homer and Jethro, was also the first important jazz mandolinist. Tiny Moore popularized the mandolin in Western swing music. He initially played an 8-string Gibson but switched after 1952 to a 5-string solidbody electric instrument built by Paul Bigsby. Modern players David Grisman, Sam Bush, and Mike Marshall, among others, have worked since the early 1970s to demonstrate the mandolin's versatility for all styles of music. Chris Thile of California is a well-known player, and has accomplished many feats of traditional bluegrass, classical, contemporary pop and rock; the band Nickel Creek featured his playing in its blend of traditional and pop styles, and he now plays in his band Punch Brothers. Most commonly associated with bluegrass, mandolin has been used a lot in country music over the years. Some well-known players include Marty Stuart, Vince Gill, and Ricky Skaggs.
Mandolin has also been used in blues music, most notably by Ry Cooder, who performed outstanding covers on his very first recordings, Yank Rachell, Johnny "Man" Young, Carl Martin, and Gerry Hundt. Howard Armstrong, who is famous for blues violin, got his start with his father's mandolin and played in string bands similar to the other Tennessee string bands he came into contact with, with band makeup including "mandolins and fiddles and guitars and banjos. And once in a while they would ease a little ukulele in there and a bass fiddle." Other blues players from the era's string bands include Willie Black (Whistler And His Jug Band), Dink Brister, Jim Hill, Charles Johnson, Coley Jones (Dallas String Band), Bobby Leecan (Need More Band), Alfred Martin, Charlie McCoy (1909-1950), Al Miller, Matthew Prater, and Herb Quinn.
It saw some use in jug band music, since that craze began as the mandolin fad was waning, and there were plenty of instruments available at relatively low cost.
Rock and Celtic.
The mandolin has been used occasionally in rock music, first appearing in the psychedelic era of the late 1960s. Levon Helm of The Band occasionally moved from his drum kit to play mandolin, most notably on "Rag Mama Rag," "Rockin' Chair," and "Evangeline." Ian Anderson of Jethro Tull played mandolin on "Fat Man," from their second album, "Stand Up", and also occasionally on later releases. Rod Stewarts 1971 No. 1 hit "Maggie May" features a significant mandolin riff. David Grisman played mandolin on two Grateful Dead songs on the "American Beauty" album, "Friend of the Devil" and "Ripple", which became instant favorites among amateur pickers at jam sessions and campground gatherings. John Paul Jones and Jimmy Page both played mandolin on Led Zeppelin songs. The popular alt rock group Imagine Dragons feature the mandolin on a few of their songs, most prominently being It's Time. Dash Crofts of the soft rock duo Seals and Crofts extensively used mandolin in their repertoire during the 1970s. Styx released the song Boat on the River in 1980, which featured Tommy Shaw on vocals and mandolin. The song didn't chart in the United States but was popular in much of Europe and the Philippines.
Some rock musicians today use mandolins, often single-stringed electric models rather than double-stringed acoustic mandolins. One example is Tim Brennan of the Irish-American punk rock band Dropkick Murphys. In addition to electric guitar, bass, and drums, the band uses several instruments associated with traditional Celtic music, including mandolin, tin whistle, and Great Highland bagpipes. The band explains that these instruments accentuate the growling sound they favor. The 1991 R.E.M. hit "Losing My Religion" was driven by a few simple mandolin licks played by guitarist Peter Buck, who also played the mandolin in nearly a dozen other songs. The single peaked at No. 4 on the Billboard Hot 100 chart (#1 on the rock and alternative charts), Luther Dickinson of North Mississippi Allstars and The Black Crowes has made frequent use of the mandolin, most notably on the Black Crowes song "Locust Street." Armenian American rock group System of A Down makes extensive use of the mandolin on their 2005 double album Mezmerize/Hypnotize. Pop punk band Green Day has used a mandolin in several occasions, especially on their 2000 album, "Warning". Boyd Tinsley, violin player of the Dave Matthews Band has been using an electric mandolin since 2005. Frontman Colin Meloy and guitarist Chris Funk of The Decemberists regularly employ the mandolin in the band's music. Nancy Wilson, rhythm guitarist of Heart, uses a mandolin in Heart's song "Dream of the Archer" from the album "Little Queen", as well as in Heart's cover of Led Zeppelin's song "The Battle of Evermore." "Show Me Heaven" by Maria McKee, the theme song to the film "Days of Thunder", prominently features a mandolin.
Many folk punk bands will also feature the mandolin. One such band is Days N' Daze, who make use of the mandolin, banjo, ukulele, as well as several other acoustic plucked string instruments. Other folk punk acts include Blackbird Raum, and Johnny Hobo and the Freight Trains.
Venezuela.
As in Brazil, the mandolin has played an important role in the Music of Venezuela. It has enjoyed a privileged position as the main melodic instrument in several different regions of the country. Specifically, the eastern states of Sucre, Nueva Esparta, Anzoategui and Monagas have made the mandolin the main instrument in their versions of Joropo as well as Puntos, Jotas, Polos, Fulias, Merengues and Malagueñas. Also, in the west of the country the sound of the mandolin is intrinsically associated with the regional genres of the Venezuelan Andes: Bambucos, Pasillos, Pasodobles, and Waltzes. In the western city of Maracaibo the Mandolin has been played in Decimas, Danzas and Contradanzas Zulianas; in the capital, Caracas, the Merengue Rucaneao, Pasodobles and Waltzes have also been played with mandolin for almost a century. Today, Venezuelan mandolists include an important group of virtuoso players and ensembles such as Alberto Valderrama, Jesus Rengel, Ricardo Sandoval, Saul Vera, and Cristobal Soto.
Notable literature.
Art or "classical" music.
The tradition of so-called "classical music" for the mandolin has been somewhat spotty, due to its being widely perceived as a "folk" instrument. Significant composers did write music specifically for the mandolin, but few "large" works were composed for it by the most widely regarded composers. The total number of works these works is rather small in comparison to—say—those composed for violin. One result of this dearth being that there were few positions for mandolinists in regular orchestras.
Beethoven composed mandolin music and enjoyed playing the mandolin. His 4 small pieces date from 1796: Sonatine WoO 43a; Adagio ma non troppo WoO 43b; Sonatine WoO 44a and Andante con Variazioni WoO 44b.
The opera "Don Giovanni" by Mozart includes mandolin parts, including the accompaniment to the famous aria "Deh vieni alla finestra", and Verdi's opera Otello calls for guzla accompaniment in the aria "Dove guardi splendono raggi", but the part is commonly performed on mandolin.
Vivaldi created a concerto for mandolin (Concerto in C major Op.3 # 6) and two concertos for mandolins and orchestra: one for mandolin, string bass & continuous in C major, (RV 425), and one for two mandolins, bass strings & continuous in G major, (RV 532).
Gustav Mahler used the mandolin in his Symphony No. 7, Symphony No. 8 and Das Lied von der Erde.
Parts for mandolin are included in works by Schoenberg (Variations op. 31), Stravinsky (Agon), Prokofiev (Romeo and Juliet) and Webern (opus Parts 10)
Some 20th century composers also used the mandolin as their instrument of choice (amongst these are: Schoenberg, Webern, Stravinsky and Prokofiev).
Among the most important European mandolin composers of the 20th century are Raffaele Calace (composer, performer and luthier) and Giuseppe Anedda (virtuoso concert pianist and professor of the first chair of the Conservatory of Italian Mandolin, Padua, 1975). Today representatives of Italian classical music and Italian classical-contemporary music include Ugo Orlandi, Carlo Aonzo, Dorina Frati, Mauro Squillante and Duilio Galfetti.
Japanese composers also produced orchestral music for mandolin in the 20th century, but these are not well known outside Japan.
To fill this gap in the literature, mandolin orchestras have traditionally played many arrangements of music written for regular orchestras or other ensembles. Some players have sought out contemporary composers to solicit new works. Traditional mandolin orchestras remain especially popular in Japan and Germany, but also exist throughout the United States, Europe and the rest of the world. They perform works composed for mandolin family instruments, or re-orchestrations of traditional pieces. The structure of a contemporary traditional mandolin orchestra consists of: first and second mandolins, mandolas (either octave mandolas, tuned an octave below the mandolin, or tenor mandolas, tuned like the viola), mandocellos (tuned like the cello), and bass instruments (conventional string bass or, rarely, mandobasses). Smaller ensembles, such as quartets composed of two mandolins, mandola, and mandocello, may also be found.
Further reading.
Chord dictionaries
Method and instructional guides

</doc>
<doc id="18889" url="https://en.wikipedia.org/wiki?curid=18889" title="Microphotonics">
Microphotonics

Microphotonics is a branch of technology that deals with directing light on a microscopic scale. It is used in optical networking.
Microphotonics employs at least two different materials with a large differential index of refraction to squeeze the light down to a small size. Generally speaking virtually all of microphotonics relies on Fresnel reflection to guide the light. If the photons reside mainly in the higher index material, the confinement is due to total internal reflection. If the confinement is due many distributed Fresnel reflections, the device is termed a photonic crystal. There are many different types of geometries used in microphotonics including optical waveguides, optical microcavities, and Arrayed waveguide gratings.
Photonic crystals.
Photonic crystals are non-conducting materials that reflect various wavelengths of light almost perfectly. Such a crystal can be referred to as a perfect mirror. Other devices employed in microphotonics include micromirrors and photonic wire waveguides. These tools are used to "mold the flow of light", a famous phrase for describing the goal of microphotonics.
Currently microphotonics technology is being developed to replace electronics devices. For instance, the long-standing goal of an all-optical router would eliminate electronic bottlenecks, speeding up the network. Perfect mirrors are being developed for use in fiber optic cables.
Microdisks, microtoroids, and microspheres.
An optical microdisk, optical microtoroid, or optical microsphere uses internal reflection in a circular geometry to hold on to the photons. This type of circularly symmetric optical resonance is called a Whispering gallery mode, after Lord Rayleigh coined the term.
See also.
<br>

</doc>
<doc id="18890" url="https://en.wikipedia.org/wiki?curid=18890" title="Microsoft Windows">
Microsoft Windows

Microsoft Windows (or simply Windows) is a metafamily of graphical operating systems developed, marketed, and sold by Microsoft. It consists of several families of operating systems, each of which cater to a certain sector of the computing industry. Active Windows families include Windows NT, Windows Embedded and Windows Phone; these may encompass subfamilies, e.g. Windows Embedded Compact (Windows CE) or Windows Server. Defunct Windows families include Windows 9x and Windows Mobile.
Microsoft introduced an operating environment named "Windows" on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs). Microsoft Windows came to dominate the world's personal computer market with over 90% market share, overtaking Mac OS, which had been introduced in 1984. However, since 2012, because of the massive growth of smartphones, Windows sells less than Android, which became the most popular operating system in 2014, when counting all of the computing platforms each operating system runs on; in 2014, the number of Windows devices sold were less than 25% of Android devices sold. However, comparisons across different markets are not fully relevant; and for personal computers, Windows is still the most popular operating system.
, the most recent version of Windows for personal computers, tablets, smartphones and embedded devices is Windows 10. The most recent versions for server computers is Windows Server 2012 R2. A specialized version of Windows runs on the Xbox One game console.
Genealogy.
By marketing role.
Microsoft, the developer of Windows, has registered several trademarks each of which denote a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families are being actively developed:
The following Windows families are no longer being developed:
Version history.
The term "Windows" collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:
Early versions.
The history of Windows dates back to September 1981, when Chase Bishop, a computer scientist, designed the first model of an electronic device and project Interface Manager was started. It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985. Windows 1.0 was to compete with Apple's operating system, but achieved little popularity. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows.
Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management. Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights. Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.
Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of the Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area. 
In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.
The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for file system services. However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control.
Windows 3.x.
Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications. Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.
Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.
Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1. The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language. Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities.
Windows 9x.
The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that "by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world." Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer. Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.
Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.
On September 14, 2000, Microsoft released Windows ME (Millennium Edition), the last DOS-based version of Windows. Windows ME incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs), expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools. However, Windows ME was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. "PC World" considered Windows ME to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time.
Windows NT.
Early versions.
In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as "NT OS/2". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.
The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993, with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.
Home versions of Windows NT.
The next major version of Windows NT, Windows XP, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a "task-oriented" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, modes to help provide compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.
At retail, Windows XP was now marketed in two main editions: the "Home" edition was targeted towards consumers, while the "Professional" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the "Media Center" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the "Tablet PC" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications). Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.
After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003. It was followed in December 2005, by Windows Server 2003 R2.
Windows Vista.
After a lengthy development process, Windows Vista was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, Windows Server 2008 was released in early 2008.
Windows 7.
On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible. Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar, a home networking system called HomeGroup, and performance improvements.
Windows 8 and 8.1.
Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own SkyDrive and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture. An update to Windows 8, called Windows 8.1, was released on October 17, 2013, and includes features such as new live tile sizes, deeper SkyDrive integration, and many other revisions. Windows 8 and Windows 8.1 has been subject to some criticism, such as removal of Start Menu.
Windows 10.
On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. Windows 10 is said to be available to update from qualified Windows 7 with SP1 and Windows 8.1 computers from the Get Windows 10 Application (for Windows 7, Windows 8.1) or Windows Update (Windows 7).
On November 12, 2015, an update to Windows 10, version 1511, was released.
This update can be activated with a Windows 7, 8 or 8.1 product key as well as Windows 10 product keys. Features include new icons and right-click context menus, default printer management, four times as many tiles allowed on the Start menu, Find My Device, and Edge updates.
Multilingual support.
Multilingual support is built into Windows. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs.
Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) - they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the Windows Update service (except Windows 8).
The interface language of installed applications are not affected by changes in the Windows interface language. Availability of languages depends on the application developers themselves.
Windows 8 and Windows Server 2012 introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets.
Platform support.
Windows NT included support for several different platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000. (Although some these platforms implement 64-bit computing, the operating system treated them as 32-bit.) However, Windows 2000, the successor of Windows NT 4.0, dropped support for all platforms except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Window NT family still runs on IA-32, although the Windows Server line has ceased supporting this platform with the release of Windows Server 2008 R2.
With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continued to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.
On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support the x86-64 (or simply x64), the eighth generation of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported.
An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated.
Windows CE.
Windows CE (officially known as "Windows Embedded Compact"), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.
Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.
Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel.
Xbox OS.
Xbox OS is an unofficial name given to the version of Windows that runs on the Xbox One. It is a more specific implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications.
Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC. The Windows 10-based Core had replaced the Windows 8-based one in this update, and the new system is sometimes referred to as "Windows 10 on Xbox One" or "OneCore". 
Xbox One's system also allows backward compatibility with Xbox 360, and the Xbox 360's system is backwards compatible with the original Xbox.
Security.
Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset. However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.
These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's "Counterpane Internet Security" reported that it had seen over 1,000 new viruses and worms in the previous six months. In 2005, Kaspersky Lab found around 11,000 malicious programs—viruses, Trojans, back-doors, and exploits written for Windows.
Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary. In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.
While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security.
The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time.
Windows Vista changes this by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows Shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or "Run as administrator" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.
File permissions.
All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGLP (Accounts, Global, Local, Permissions) AGDLP which in essence where file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directory to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.
Windows Defender.
On January 6, 2005, Microsoft released a Beta version of Microsoft AntiSpyware, based upon the previously released Giant AntiSpyware. On February 14, 2006, Microsoft AntiSpyware became Windows Defender with the release of Beta 2. Windows Defender is a freeware program designed to protect against spyware and other unwanted software. Windows XP and Windows Server 2003 users who have genuine copies of Microsoft Windows can freely download the program from Microsoft's web site, and Windows Defender ships as part of Windows Vista and 7. In Windows 8, Windows Defender and Microsoft Security Essentials have been combined into a single program, named Windows Defender. It is based on Microsoft Security Essentials, borrowing its features and user interface. Although it is enabled by default, it can be turned off to use another anti-virus solution. Windows Malicious Software Removal Tool and the optional Microsoft Safety Scanner are two other free security products offered by Microsoft.
Third-party analysis.
In an article based on a report by Symantec, internetnews.com has described Microsoft Windows as having the "fewest number of patches and the shortest average patch development time of the five operating systems it monitored in the last six months of 2006."
A study conducted by Kevin Mitnick and marketing communications firm Avantgarde in 2004, found that an unprotected and unpatched Windows XP system with Service Pack 1 lasted only 4 minutes on the Internet before it was compromised, and an unprotected and also unpatched Windows Server 2003 system was compromised after being connected to the internet for 8 hours. The computer that was running Windows XP Service Pack 2 was not compromised. The AOL National Cyber Security Alliance Online Safety Study of October 2004, determined that 80% of Windows users were infected by at least one spyware/adware product. Much documentation is available describing how to increase the security of Microsoft Windows products. Typical suggestions include deploying Microsoft Windows behind a hardware or software firewall, running anti-virus and anti-spyware software, and installing patches as they become available through Windows Update.
Alternative implementations.
Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:

</doc>
<doc id="18892" url="https://en.wikipedia.org/wiki?curid=18892" title="Mojo (African-American culture)">
Mojo (African-American culture)

Mojo , in the African-American folk belief called hoodoo, is an amulet consisting of a flannel bag containing one or more magical items. It is a "prayer in a bag", or a spell that can be carried with or on the host's body.
Alternative American names for the mojo bag include hand, mojo hand, conjure hand, lucky hand, conjure bag, trick bag, root bag, toby, jomo, and gris-gris bag.
Ideology.
The most common synonym for the word mojo is "gris-gris", which literally means "fetish" or "charm", thus a gris-gris bag is a charm bag. In the Caribbean, an almost identical African-derived bag is called a "wanga" or "oanga" bag, but that term is uncommon in the United States. The word "conjure" is an ancient alternative to "hoodoo", which is a direct variation of African-American folklore. Because of this, a conjure hand is also considered a hoodoo bag; usually made by a respected community conjure doctor.
The word "hand" in this context is defined as a combination of ingredients. The term may derive from the use of finger and hand bones from the dead in mojo bags, or from ingredients such as the lucky hand root (favored by gamblers). The latter suggests an analogy between the varied bag ingredients and the several cards that make up a "hand" in card games. Mojo reaches as far back as West African culture, where it is said to drive away evil spirits, keep good luck in the household, manipulate a fortune, and lure and persuade lovers. The ideology of the ancestors and the descendants of the mojo hand used this "prayer in a bag" based on their belief of spiritual inheritance, where the omniscient forefathers of their families would provide protection and favor; especially when they used the mojo. Through this, a strong belief was placed in the idealism of whomever used mojo, creating a spiritual trust in the magic itself.
Making.
Although most Southern-style conjure bags are made of red flannel material, most seasoned conjurers use color-symbolism. This practice embodies itself in the practice of hoodoo, where green flannel is used for a money mojo, white flannel is used for a baby-blessing mojo, red flannel is used for love mojo, and so on. West Indians also use mojo bags, but often use leather instead of flannel.
The contents of each bag vary directly with the aim of the conjurer. For example, a mojo carried for love-drawing will contain different ingredients than one for gambling luck or magical protection. Ingredients can include roots, herbs, animal parts, minerals, coins, crystals, good luck tokens, and carved amulets. The more personalized objects are used to add extra power because of the symbolic value.
Maintenance.
Fixing and feeding a mojo hand.
There is a process to fixing a proper mojo. A ritual must be put in place in order to successfully prepare a mojo by being filled and awakened to life. This can be done by smoking incense and candles, or breathed upon to bring it to life. Prayers may be said, and other methods may be used to accomplish this essential step. Once prepared, the mojo is "dressed" or "fed" with a liquid such as alcohol, perfume, water, or bodily fluids. The reason why it is said to feed the mojo to keep it working is because it is alive with spirit. One story from the work entitled "From My People" describes a slave who went out and sought a mojo conjurer that gave him a mojo to run away from home. The story describes the slave's mojo as fixing him into many formations, and he ultimately dies because he misuses its power. Had he fixed and believed in the specific mojo himself, he might have escaped the plantation alive.
Hiding the mojo.
Mojos are traditionally made for an individual, and so must be concealed on the person at all times. Men usually keep the trinkets hidden in the pants pocket, while women are more prone to clip it to the bra. They are also commonly pinned to clothes below the waist. Depending on the type of mojo, the hiding place will be crucial to its success, as those who make conjure bags to carry love spells sometimes specify that the mojo must be worn next to the skin. A story from the book "From My People" described the story of Moses, and the task he went through to bring his people out of slavery. It described how "Hoodoo Lost his Hand", as Moses' mojo was hidden through his staff. When he turned it into a snake, the pharaoh made his soothsayers and magicians create the same effect. As a result, the Pharaoh's snake was killed by Moses' snake; and that is how Hoodoo lost his hand.

</doc>
<doc id="18894" url="https://en.wikipedia.org/wiki?curid=18894" title="Matt Groening">
Matt Groening

Matthew Abram "Matt" Groening ( ; born February 15, 1954) is an American cartoonist, writer, producer, animator, and voice actor. He is the creator of the comic strip "Life in Hell" (1977–2012) and the television series "The Simpsons" (1989–present) and "Futurama" (1999–2003, 2008–2013). "The Simpsons" has gone on to become the longest running U.S. primetime television series in history, as well as the longest running animated series and sitcom.
Groening made his first professional cartoon sale of "Life in Hell" to the avant-garde "Wet" magazine in 1978. At its peak, the cartoon was carried in 250 weekly newspapers. "Life in Hell" caught the attention of James L. Brooks. In 1985, Brooks contacted Groening with the proposition of working in animation for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Fearing the loss of ownership rights, Groening decided to create something new and came up with a cartoon family, the Simpson family, and named the members after his own parents and sisters—while Bart was an anagram of the word brat. The shorts would be spun off into their own series "The Simpsons", which has since aired episodes. In 1997, Groening and former "Simpsons" writer David X. Cohen developed "Futurama", an animated series about life in the year 3000, which premiered in 1999, running for four years on Fox, then picked up by Comedy Central for additional seasons.
Groening has won 12 Primetime Emmy Awards, ten for "The Simpsons" and two for "Futurama" as well as a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2002, he won the National Cartoonist Society Reuben Award for his work on "Life in Hell". He received a star on the Hollywood Walk of Fame on February 14, 2012.
Early life.
Groening was born on February 15, 1954 in Portland, Oregon, the middle of five children (older brother Mark and sister Patty were born in 1950 and 1952, while the younger sisters Lisa and Maggie in 1956 and 1958, respectively). His Norwegian-American mother, Margaret Ruth (née Wiggum; March 23, 1919 – April 22, 2013), was once a teacher, and his German American father, Homer Philip Groening (December 30, 1919 – March 15, 1996), was a filmmaker, advertiser, writer and cartoonist. Homer, born in Main Centre, Saskatchewan, Canada, grew up in a Mennonite, Plautdietsch-speaking family.
Matt's grandfather, Abram Groening, was a professor at Tabor College, a Mennonite Brethren liberal arts college in Hillsboro, Kansas before moving to Albany College (now known as Lewis and Clark College) in Oregon in 1930.
Groening grew up in Portland, and attended Ainsworth Elementary School and Lincoln High School. From 1972 to 1977, Groening attended The Evergreen State College in Olympia, Washington, a liberal arts school that he described as "a hippie college, with no grades or required classes, that drew every weirdo in the Northwest." He served as the editor of the campus newspaper, "The Cooper Point Journal", for which he also wrote articles and drew cartoons. He befriended fellow cartoonist Lynda Barry after discovering that she had written a fan letter to Joseph Heller, one of Groening's favorite authors, and had received a reply. Groening has credited Barry with being "probably biggest inspiration." He first became interested in cartoons after watching the Disney animated film "One Hundred and One Dalmatians", and he has also cited "Peanuts" and its creator Charles M. Schulz as inspirations.
Career.
Early career.
In 1977, at the age of 23, Groening moved to Los Angeles to become a writer. He went through what he described as "a series of lousy jobs," including being an extra in the television movie "When Every Day Was the Fourth of July", busing tables, washing dishes at a nursing home, clerking at the Hollywood Licorice Pizza record store, landscaping in a sewage treatment plant, and chauffeuring and ghostwriting for a retired Western director.
"Life in Hell".
Groening described life in Los Angeles to his friends in the form of the self-published comic book "Life in Hell", which was loosely inspired by the chapter "How to Go to Hell" in Walter Kaufmann's book "Critique of Religion and Philosophy". Groening distributed the comic book in the book corner of Licorice Pizza, a record store in which he worked. He made his first professional cartoon sale to the avant-garde "Wet" magazine in 1978. The strip, titled "Forbidden Words," appeared in the September/October issue of that year.
Groening had gained employment at the "Los Angeles Reader", a newly formed alternative newspaper, delivering papers, typesetting, editing and answering phones. He showed his cartoons to the editor, James Vowell, who was impressed and eventually gave him a spot in the paper. "Life in Hell" made its official debut as a comic strip in the "Reader" on April 25, 1980. Vowell also gave Groening his own weekly music column, "Sound Mix," in 1982. However, the column would rarely actually be about music, as he would often write about his "various enthusiasms, obsessions, pet peeves and problems" instead. In an effort to add more music to the column, he "just made stuff up," concocting and reviewing fictional bands and non-existent records. In the following week's column, he would confess to fabricating everything in the previous column and swear that everything in the new column was true. Eventually, he was finally asked to give up the "music" column. Among the fans of the column was Harry Shearer, who would later become a voice on "The Simpsons".
"Life in Hell" became popular almost immediately. In November 1984, Deborah Caplan, Groening's then-girlfriend and co-worker at the "Reader", offered to publish "Love is Hell", a series of relationship-themed "Life in Hell" strips, in book form. Released a month later, the book was an underground success, selling 22,000 copies in its first two printings. "Work is Hell" soon followed, also published by Caplan. Soon afterward, Caplan and Groening left and put together the Life in Hell Co., which handled merchandising for "Life in Hell". Groening also started Acme Features Syndicate, which syndicated "Life in Hell", Lynda Barry and John Callahan, but now only syndicates "Life in Hell". At the end of its run, "Life in Hell" was carried in 250 weekly newspapers and has been anthologized in a series of books, including "School is Hell", "Childhood is Hell", "The Big Book of Hell", and "The Huge Book of Hell". Although Groening has stated, "I'll never give up the comic strip. It's my foundation," he announced that the June 16, 2012 strip would mark "Life in Hell"s conclusion. After Groening ended the strip, the Center for Cartoon Studies commissioned a poster that was presented to Groening in honor of his work. The poster contained tribute cartoons by 22 of Groening's cartoonist friends who were influenced by "Life in Hell".
"The Simpsons".
Creation.
"Life in Hell" caught the eye of Hollywood writer-producer and Gracie Films founder James L. Brooks, who had been shown the strip by fellow producer Polly Platt. In 1985, Brooks contacted Groening with the proposition of working in animation on an undefined future project, which would turn out to be developing a series of short animated skits, called "bumpers," for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Groening feared that he would have to give up his ownership rights, and that the show would fail and would take down his comic strip with it. Groening conceived of the idea for The Simpsons in the lobby of James L. Brooks's office and hurriedly sketched out his version of a dysfunctional family: Homer, the overweight father; Marge, the slim mother; Bart, the bratty oldest child; Lisa, the intelligent middle child; and Maggie, the baby. Groening famously named the main Simpson characters after members of his own family: his parents, Homer and Margaret (Marge or Marjorie in full), and his younger sisters, Lisa and Margaret (Maggie). Claiming that it was a bit too obvious to name a character after himself, he chose the name "Bart," an anagram of brat. However, he stresses that aside from some of the sibling rivalry, his family is nothing like the Simpsons. Groening also has an older brother and sister, Mark and Patty, and in a 1995 interview Groening divulged that Mark "is the actual inspiration for Bart."
Maggie Groening has co-written a few "Simpsons" books featuring her cartoon namesake.
"The Tracey Ullman Show".
The family was crudely drawn, because Groening had submitted basic sketches to the animators, assuming they would clean them up; instead, they just traced over his drawings. The entire Simpson family was designed so that they would be recognizable in silhouette. When Groening originally designed Homer, he put his own initials into the character's hairline and ear: the hairline resembled an 'M', and the right ear resembled a 'G'. Groening decided that this would be too distracting though, and redesigned the ear to look normal. He still draws the ear as a 'G' when he draws pictures of Homer for fans. Marge's distinct beehive hairstyle was inspired by "Bride of Frankenstein" and the style that Margaret Groening wore during the 1960s, although her hair was never blue. Bart's original design, which appeared in the first shorts, had spikier hair, and the spikes were of different lengths. The number was later limited to nine spikes, all of the same size. At the time Groening was primarily drawing in black and "not thinking that would eventually be drawn in color" gave him spikes that appear to be an extension of his head. Lisa's physical features are generally not used in other characters; for example, in the later seasons, no character other than Maggie shares her hairline. While designing Lisa, Groening "couldn't be bothered to even think about girls' hair styles". When designing Lisa and Maggie, he "just gave them this kind of spiky starfish hair style, not thinking that they would eventually be drawn in color". Groening storyboarded and scripted every short (now known as "The Simpsons shorts"), which were then animated by a team including David Silverman and Wes Archer, both of whom would later become directors on the series.
The Simpsons shorts first appeared in "The Tracey Ullman Show" on April 19, 1987. Another family member, Grampa Simpson, was introduced in the later shorts. Years later, during the early seasons of "The Simpsons", when it came time to give Grampa a first name, Groening says he refused to name him after his own grandfather, Abraham Groening, leaving it to other writers to choose a name. By coincidence, they chose Abraham, unaware that it was the name of Groening's grandfather.
Half-hour.
Although "The Tracey Ullman Show" was not a big hit, the popularity of the shorts led to a half-hour spin-off in 1989. A team of production companies adapted "The Simpsons" into a half-hour series for the Fox Broadcasting Company. The team included what is now the Klasky Csupo animation house. James L. Brooks negotiated a provision in the contract with the Fox network that prevented Fox from interfering with the show's content. Groening said his goal in creating the show was to offer the audience an alternative to what he called "the mainstream trash" that they were watching. The half-hour series premiered on December 17, 1989 with "Simpsons Roasting on an Open Fire", a Christmas special. "Some Enchanted Evening" was the first full-length episode produced, but it did not broadcast until May 1990, as the last episode of the first season, because of animation problems.
The series quickly became a worldwide phenomenon, to the surprise of many. Groening said: "Nobody thought "The Simpsons" was going to be a big hit. It sneaked up on everybody." "The Simpsons" was co-developed by Groening, Brooks, and Sam Simon, a writer-producer with whom Brooks had worked on previous projects. Groening and Simon, however, did not get along and were often in conflict over the show; Groening once described their relationship as "very contentious." Simon eventually left the show in 1993 over creative differences.
Like the main family members, several characters from the show have names that were inspired by people, locations or films. The name "Wiggum" for police chief Chief Wiggum is Groening's mother's maiden name. The names of a few other characters were taken from major street names in Groening's hometown of Portland, Oregon, including Flanders, Lovejoy, Powell, Quimby and Kearney. Despite common fan belief that Sideshow Bob Terwilliger was named after SW Terwilliger Boulevard in Portland, he was actually named after the character Dr. Terwilliker from the film "The 5,000 Fingers of Dr. T".
Although Groening has pitched a number of spin-offs from "The Simpsons", those attempts have been unsuccessful. In 1994, Groening and other "Simpsons" producers pitched a live-action spin-off about Krusty the Clown (with Dan Castellaneta playing the lead role), but were unsuccessful in getting it off the ground. Groening has also pitched "Young Homer" and a spin-off about the non-Simpsons citizens of Springfield.
In 1995, Groening got into a major disagreement with Brooks and other "Simpsons" producers over "A Star Is Burns", a crossover episode with "The Critic", an animated show also produced by Brooks and staffed with many former "Simpsons" crew members. Groening claimed that he feared viewers would "see it as nothing but a pathetic attempt to advertise "The Critic" at the expense of "The Simpsons"," and was concerned about the possible implication that he had created or produced "The Critic". He requested his name be taken off the episode.
Groening is credited with writing or co-writing the episodes "Some Enchanted Evening", "The Telltale Head", "Colonel Homer" and "22 Short Films About Springfield", as well as "The Simpsons Movie", released in 2007. He has had several cameo appearances in the show, with a speaking role in the episode "My Big Fat Geek Wedding". He currently serves at "The Simpsons" as an executive producer and creative consultant.
"Futurama".
After spending a few years researching science fiction, Groening got together with "Simpsons" writer/producer David X. Cohen (still known as David S. Cohen at the time) in 1997 and developed "Futurama", an animated series about life in the year 3000. By the time they pitched the series to Fox in April 1998, Groening and Cohen had composed many characters and storylines; Groening claimed they had gone "overboard" in their discussions. Groening described trying to get the show on the air as "by far the worst experience of grown-up life." The show premiered on March 28, 1999. Groening's writing credits for the show are for the premiere episode, "Space Pilot 3000" (co-written with Cohen), "Rebirth" (story) and "In-A-Gadda-Da-Leela" (story).
After four years on the air, the show was canceled by Fox. In a situation similar to "Family Guy", however, strong DVD sales and very stable ratings on Adult Swim brought Futurama back to life. When Comedy Central began negotiating for the rights to air "Futurama" reruns, Fox suggested that there was a possibility of also creating new episodes. When Comedy Central committed to sixteen new episodes, it was decided that four straight-to-DVD films—' (2007), ' (2008), ' (2008) and ' (2009)—would be produced.
Since no new "Futurama" projects were in production, the movie "Into the Wild Green Yonder" was designed to stand as the "Futurama" series finale. However, Groening had expressed a desire to continue the "Futurama" franchise in some form, including as a theatrical film. In an interview with CNN, Groening said that "we have a great relationship with Comedy Central and we would love to do more episodes for them, but I don't know...We're having discussions and there is some enthusiasm but I can't tell if it's just me." Comedy Central commissioned an additional 26 new episodes, and began airing them in 2010. The show continued in to 2013, before Comedy Central announced in April 2013 that they would not be renewing it beyond its seventh season. The final episode aired on September 4, 2013.
Other pursuits.
In 1994, Groening formed Bongo Comics (named after the character Bongo from "Life in Hell") with Steve Vance, Cindy Vance and Bill Morrison, which publishes comic books based on "The Simpsons" and "Futurama" (including "Futurama Simpsons Infinitely Secret Crossover Crisis", a crossover between the two), as well as a few original titles. According to Groening, the goal with Bongo is to "to bring humor into the fairly grim comic book market." He also formed Zongo Comics in 1995, an imprint of Bongo that published comics for more mature readers, which included three issues of Mary Fleener's "Fleener" and seven issues of his close friend Gary Panter's "Jimbo" comics.
Groening is known for his eclectic taste in music. His favorite band is Frank Zappa and The Mothers of Invention and his favorite album is "Trout Mask Replica" by Captain Beefheart (which was produced by Zappa). He guest-edited Da Capo Press's "Best Music Writing 2003" and curated a US All Tomorrow's Parties music festival in 2003. He illustrated the cover of Frank Zappa's posthumous album "" (1996). In May 2010, he curated another edition of All Tomorrow's Parties in Minehead, England. He also plays the drums in the all-author rock and roll band The Rock Bottom Remainders (although he is listed as the cowbell player), whose other members include Dave Barry, Ridley Pearson, Scott Turow, Amy Tan, James McBride, Mitch Albom, Roy Blount Jr., Stephen King, Kathi Kamen Goldmark, Sam Barry and Greg Iles. In July 2013, Groening co-authored "Hard Listening" (2013) with the rest of the Rock Bottom Remainders (published by Coliloquy, LLC).
On January 15, 2016, it was announced that Groening is in talks with Netflix to develop a new animated series. Netflix is considering giving the series a two-season order, totalling 20 episodes.
Personal life.
Groening and Deborah Caplan married in 1986 and had two sons together, Homer (who goes by Will) and Abe, both of whom Groening occasionally portrays as rabbits in "Life in Hell". The couple divorced in 1999 after thirteen years of marriage. In 2011, Groening married Argentinian artist Agustina Picasso after a four-year relationship, and became stepfather to her daughter Camille. In May 2013, Picasso gave birth to Nathaniel Philip Picasso Groening, named after writer Nathanael West. She joked that "his godfather is SpongeBob's creator Stephen Hillenburg". Matt is the brother-in-law of "Hey Arnold!" and "Dinosaur Train" creator, Craig Bartlett, who is married to Groening's sister, Lisa. Arnold used to appear in "Simpsons Illustrated".
Groening identifies himself as agnostic and a liberal and has often made campaign contributions to Democratic Party candidates. His first cousin, Laurie Monnes Anderson, is a member of the Oregon State Senate representing eastern Multnomah County.
Awards.
Groening has been nominated for 25 Emmy Awards and has won twelve: ten for "The Simpsons" and two for "Futurama" in the "Outstanding Animated Program (for programming one hour or less)" category. Groening received the 2002 National Cartoonist Society Reuben Award, and had been nominated for the same award in 2000. He received a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2007, he was ranked fourth (and highest American by birth) in a list of the "top 100 living geniuses", published by British newspaper "The Daily Telegraph".
He received the 2,459th star on the Hollywood Walk of Fame on February 14, 2012.

</doc>
<doc id="18895" url="https://en.wikipedia.org/wiki?curid=18895" title="Metaphysics">
Metaphysics

Metaphysics is a traditional branch of philosophy concerned with explaining the fundamental nature of being and the world that encompasses it, although the term is not easily defined. Traditionally, metaphysics attempts to answer two basic questions in the broadest possible terms:
A person who studies metaphysics is called a "metaphysician". Among other things, the metaphysician attempts to clarify the fundamental notions by which people understand the world, e.g., existence, objects and their properties, space and time, cause and effect, and possibility. A central branch of metaphysics is ontology, the investigation into the basic categories of being and how they relate to each other. Some include epistemology as another central focus of metaphysics, but others question this. Another central branch of metaphysics is metaphysical cosmology, an area of philosophy that seeks to understand the origin of the universe and determine whether there is an ultimate meaning behind its existence. Metaphysical cosmology differs from physical cosmology, the study of the physical origins and evolution of the Universe.
Prior to the modern history of science, scientific questions were addressed as a part of metaphysics known as natural philosophy. Originally, the term "science" (Latin "scientia") simply meant "knowledge". The scientific method, however, transformed natural philosophy into an empirical activity deriving from experiment unlike the rest of philosophy. By the end of the 18th century, it had begun to be called "science" to distinguish it from philosophy. Thereafter, metaphysics denoted philosophical enquiry of a non-empirical character into the nature of existence. Some philosophers of science, such as the neo-positivists, say that natural science rejects the study of metaphysics, while other philosophers of science strongly disagree.
Etymology.
The word "metaphysics" derives from the Greek words μετά ("metá", "beyond", "upon" or "after") and φυσικά ("physiká", "physics"). It was first used as the title for several of Aristotle's works, because they were usually anthologized after the works on physics in complete editions. The prefix "meta-" ("after") indicates that these works come "after" the chapters on physics. However, Aristotle himself did not call the subject of these books "Metaphysics": he referred to it as "first philosophy." The editor of Aristotle's works, Andronicus of Rhodes, is thought to have placed the books on first philosophy right after another work, "Physics", and called them ("ta meta ta physika biblia") or "the books that come after the on physics". This was misread by Latin scholiasts, who thought it meant "the science of what is beyond the physical".
However, once the name was given, the commentators sought to find intrinsic reasons for its appropriateness. For instance, it was understood to mean "the science of the world beyond nature" ("physis" in Greek), that is, the science of the immaterial. Again, it was understood to refer to the chronological or pedagogical order among our philosophical studies, so that the "metaphysical sciences" would mean "those that we study after having mastered the sciences that deal with the physical world" (St. Thomas Aquinas, "Expositio in librum Boethii De hebdomadibus", V, 1).
There is a widespread use of the term in current popular literature which replicates this understanding, i.e. that the metaphysical equates to the non-physical: thus, "metaphysical healing" means healing by means of remedies that are not physical."
Central questions.
Cosmology and cosmogony.
Metaphysical Cosmology is the branch of metaphysics that deals with the world as the totality of all phenomena in space and time. Historically, it has had quite a broad scope, and in many cases was founded in religion. The ancient Greeks drew no distinction between this use and their model for the cosmos. However, in modern times it addresses questions about the Universe which are beyond the scope of the physical sciences. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods (e.g. dialectics).
Cosmogony deals specifically with the origin of the universe.
Modern metaphysical cosmology and cosmogony try to address questions such as:
Being and ontology.
Ontology deals with the determination whether "categories of being" are fundamental and discusses in what sense the items in those categories may be said to "be". It is the inquiry into being "in so much as" it is being ("being "qua" being"), or into beings insofar as they exist—and not insofar as (for instance) particular facts may be obtained about them or particular properties belong to them.
Some philosophers, notably of the Platonic school, contend that all nouns (including abstract nouns) refer to existent entities. Other philosophers contend that nouns do not always name entities, but that some provide a kind of shorthand for reference to a collection of either objects or events. In this latter view, "mind", instead of referring to an entity, refers to a collection of "mental events" experienced by a "person"; "society" refers to a collection of "persons" with some shared characteristics, and "geometry" refers to a collection of a specific kind of intellectual activity. Between these poles of realism and nominalism, stand a variety of other positions. An ontology may give an account of which words refer to entities, which do not, why, and what categories result.
Determinism and free will.
Determinism is the philosophical proposition that every event, including human cognition, decision and action, is causally determined by an unbroken chain of prior occurrences. It holds that nothing happens that has not already been determined. The principal consequence of the deterministic claim is that it poses a challenge to the existence of free will.
The problem of free will is the problem of whether rational agents exercise control over their own actions and decisions. Addressing this problem requires understanding the relation between freedom and causation, and determining whether the laws of nature are causally deterministic. Some philosophers, known as Incompatibilists, view determinism and free will as mutually exclusive. If they believe in determinism, they will therefore believe free will to be an illusion, a position known as "Hard Determinism". Proponents range from Baruch Spinoza to Ted Honderich.
Others, labeled Compatibilists (or "Soft Determinists"), believe that the two ideas can be reconciled coherently. Adherents of this view include Thomas Hobbes and many modern philosophers such as John Martin Fischer.
Incompatibilists who accept free will but reject determinism are called Libertarians, a term not to be confused with the political sense. Robert Kane and Alvin Plantinga are modern defenders of this theory.
Identity and change.
The Greeks took some extreme positions on the nature of change: Parmenides denied that change occurs at all, while Heraclitus thought change was ubiquitous: "ou cannot step into the same river twice."
Identity, sometimes called Numerical Identity, is the relation that a "thing" bears to itself, and which no "thing" bears to anything other than itself (cf. sameness). According to Leibniz, if some object x is identical to some object y, then any property that x has, y will have as well. However, it seems, too, that objects can change over time. If one were to look at a tree one day, and the tree later lost a leaf, it would seem that one could still be looking at that same tree. Two rival theories to account for the relationship between change and identity are Perdurantism, which treats the tree as a series of tree-stages, and Endurantism, which maintains that the tree—the same tree—is present at every stage in its history.
Mind and matter.
The nature of matter was a problem in its own right in early philosophy. Aristotle himself introduced the idea of matter in general to the Western world, adapting the term "hyle", which originally meant "lumber." Early debates centered on identifying a single underlying principle. Water was claimed by Thales, air by Anaximenes, "Apeiron" (the Boundless) by Anaximander, fire by Heraclitus. Democritus, in conjunction with his mentor, Leucippus, conceived of an atomic theory many centuries before it was accepted by modern science. It is worth noting, however, that the grounds necessary to ensure validity to the proposed theory's veridical nature were not scientific, but just as philosophical as those traditions espoused by Thales and Anaximander.
The nature of the mind and its relation to the body has been seen as more of a problem as science has progressed in its mechanistic understanding of the brain and body. Proposed solutions often have ramifications about the nature of mind as a whole. René Descartes proposed substance dualism, a theory in which mind and body are essentially different, with the mind having some of the attributes traditionally assigned to the soul, in the seventeenth century. This creates a conceptual puzzle about how the two interact (which has received some strange answers, such as occasionalism). Evidence of a close relationship between brain and mind, such as the Phineas Gage case, have made this form of dualism increasingly unpopular.
Another proposal discussing the mind–body problem is idealism, in which the material is sweepingly eliminated in favor of the mental. Idealists, such as George Berkeley, claim that material objects do not exist unless perceived and only as perceptions. The "German idealists" such as Fichte, Hegel and Schopenhauer took Kant as their starting-point, although it is debatable how much of an idealist Kant himself was. Idealism is also a common theme in Eastern philosophy. Related ideas are panpsychism and panexperientialism, which say everything "has" a mind rather than everything exists "in" a mind. Alfred North Whitehead was a twentieth-century exponent of this approach.
Idealism is a monistic theory which holds that there is a single universal substance or principle. Neutral monism, associated in different forms with Baruch Spinoza and Bertrand Russell, seeks to be less extreme than idealism, and to avoid the problems of substance dualism. It claims that existence consists of a single substance that in itself is neither mental nor physical, but is capable of mental and physical aspects or attributesthus it implies a dual-aspect theory.
For the last one hundred years, the dominant metaphysics has without a doubt been materialistic monism. Type identity theory, token identity theory, functionalism, reductive physicalism, nonreductive physicalism, eliminative materialism, anomalous monism, property dualism, epiphenomenalism and emergence are just some of the candidates for a scientifically informed account of the mind. (It should be noted that while many of these positions are dualisms, none of them are "substance" dualism.)
Prominent recent philosophers of mind include David Armstrong, Ned Block, David Chalmers, Patricia and Paul Churchland, Donald Davidson, Daniel Dennett, Fred Dretske, Douglas Hofstadter, Jerry Fodor, David Lewis, Thomas Nagel, Hilary Putnam, John Searle, John Smart, Ludwig Wittgenstein, and Fred Alan Wolf.
Necessity and possibility.
Metaphysicians investigate questions about the ways the world could have been. David Lewis, in "On the Plurality of Worlds," endorsed a view called Concrete Modal realism, according to which facts about how things could have been are made true by other concrete worlds, just as in ours, in which things are different. Other philosophers, such as Gottfried Leibniz, have dealt with the idea of possible worlds as well. The idea of necessity is that any necessary fact is true across all possible worlds. A possible fact is true in some possible world, even if not in the actual world. For example, it is possible that cats could have had two tails, or that any particular apple could have not existed. By contrast, certain propositions seem necessarily true, such as analytic propositions, e.g., "All bachelors are unmarried." The particular example of analytic truth being necessary is not universally held among philosophers. A less controversial view might be that self-identity is necessary, as it seems fundamentally incoherent to claim that for any x, it is not identical to itself; this is known as the "law of identity", a putative "first principle". Aristotle describes the "principle of non-contradiction", "It is impossible that the same quality should both belong and not belong to the same thing ... This is the most certain of all principles ... Wherefore they who demonstrate refer to this as an ultimate opinion. For it is by nature the source of all the other axioms."
Religion and spirituality.
Theology is the study of a god or gods and the nature of the divine. Whether there is a god (monotheism), many gods (polytheism) or no gods (atheism), or whether it is unknown or unknowable whether any gods exist (agnosticism; apophatic theology), and whether a divine entity directly intervenes in the world (theism), or its sole function is to be the first cause of the universe (deism); these and whether a god or gods and the world are different (as in panentheism and dualism), or are identical (as in pantheism), are some of the primary metaphysical questions concerning philosophy of religion.
Within the standard Western philosophical tradition, theology reached its peak under the medieval school of thought known as scholasticism, which focused primarily on the metaphysical aspects of Christianity. The work of the scholastics is still an integral part of modern philosophy, with key figures such as Thomas Aquinas still playing an important role in the philosophy of religion.
The nature of metaphysics.
Some philosophers, such as Amie Thomasson, have argued that many metaphysical questions can be dissolved just by looking at the way we use words; others, such as Ted Sider, have argued that metaphysical questions are substantive, and that we can make progress toward answering them by comparing theories according to a range of theoretical virtues inspired by the sciences, such as simplicity and explanatory power.
History and schools of metaphysics.
Pre-Socratic metaphysics in Greece.
The first known philosopher, according to Aristotle, is Thales of Miletus. Rejecting mythological and divine explanations, he sought a single first cause or "Arche" (origin or beginning) under which all phenomena could be explained, and concluded that this first cause was in fact moisture or water. Thales also taught that the world is harmonious, has a harmonious structure, and thus is intelligible to rational understanding. Other Miletians, such as Anaximander and Anaximenes, also had a monistic conception of the first cause.
Another school was the Eleatics, Italy. The group was founded in the early fifth century BCE by Parmenides, and included Zeno of Elea and Melissus of Samos. Methodologically, the Eleatics were broadly rationalist, and took logical standards of clarity and necessity to be the criteria of truth. Parmenides' chief doctrine was that reality is a single unchanging and universal Being. Zeno used "reductio ad absurdum", to demonstrate the illusory nature of change and time in his paradoxes.
Heraclitus of Ephesus, in contrast, made change central, teaching that "all things flow". His philosophy, expressed in brief aphorisms, is quite cryptic. For instance, he also taught the unity of opposites.
Democritus and his teacher Leucippus, are known for formulating an atomic theory for the cosmos. They are considered forerunners of the scientific method.
Socrates and Plato.
Socrates is known for his dialectic or questioning approach to philosophy rather than a positive metaphysical doctrine.
His pupil, Plato is famous for his theory of forms (which he places in the mouth of Socrates in the dialogues he wrote to expound it). Platonic realism (also considered a form of idealism) is considered to be a solution to the problem of universals; i.e., what particular objects have in common is that they share a specific Form which is universal to all others of their respective kind.
The theory has a number of other aspects:
Platonism developed into Neoplatonism, a philosophy with a monotheistic and mystical flavour that survived well into the early Christian era.
Aristotle.
Plato's pupil Aristotle wrote widely on almost every subject, including metaphysics. His solution to the problem of universals contrasts with Plato's. Whereas Platonic Forms are existentially apparent in the visible world, Aristotelian essences "indwell" in particulars.
Potentiality and Actuality are principles of a dichotomy which Aristotle used throughout his philosophical works to analyze motion, causality and other issues.
The Aristotelian theory of change and causality stretches to four causes: the material, formal, efficient and final. The efficient cause corresponds to what is now known as a cause "simpliciter". Final causes are explicitly teleological, a concept now regarded as controversial in science. The Matter/Form dichotomy was to become highly influential in later philosophy as the substance/essence distinction.
The opening arguments in Aristotle's "Metaphysics", Book I, revolve around the senses, knowledge, experience, theory, and wisdom. The first main focus in the Metaphysics is attempting to determine how intellect "advances from sensation through memory, experience, and art, to theoretical knowledge". Aristotle claims that eyesight provides us with the capability to recognize and remember experiences, while sound allows us to learn.
Metaphysics in India.
Sāṃkhya.
"Sāṃkhya" is an ancient system of Indian philosophy based on a dualism involving the ultimate principles of consciousness and matter."Samkhya", Webster's College Dictionary (2010), Random House, ISBN 978-0-375-40741-3, Quote: "Samkhya is a system of Hindu philosophy stressing the reality and duality of spirit and matter."</ref> It is described as the rationalist school of Indian philosophy. It is most related to the Yoga school of Hinduism, and its method was most influential on the development of Early Buddhism.
The Sāmkhya is an enumerationist philosophy whose epistemology accepts three of six pramanas (proofs) as the only reliable means of gaining knowledge. These include "pratyakṣa" (perception), "anumāṇa" (inference) and "śabda" ("āptavacana", word/testimony of reliable sources).
Samkhya is strongly dualist. Sāmkhya philosophy regards the universe as consisting of two realities; puruṣa (consciousness) and prakṛti (matter). Jiva (a living being) is that state in which puruṣa is bonded to prakṛti in some form. This fusion, state the Samkhya scholars, led to the emergence of "buddhi" ("spiritual awareness") and "ahaṅkāra" (ego consciousness). The universe is described by this school as one created by purusa-prakṛti entities infused with various permutations and combinations of variously enumerated elements, senses, feelings, activity and mind. During the state of imbalance, one of more constituents overwhelm the others, creating a form of bondage, particularly of the mind. The end of this imbalance, bondage is called liberation, or moksha, by the Samkhya school.
The existence of God or supreme being is not directly asserted, nor considered relevant by the Samkhya philosophers. Sāṃkhya denies the final cause of Ishvara (God). While the Samkhya school considers the Vedas as a reliable source of knowledge, it is an atheistic philosophy according to Paul Deussen and other scholars. A key difference between Samkhya and Yoga schools, state scholars, is that Yoga school accepts a "personal, yet essentially inactive, deity" or "personal god".
Samkhya is known for its theory of guṇas (qualities, innate tendencies). Guṇa, it states, are of three types: "sattva" being good, compassionate, illuminating, positive, and constructive; "rajas" is one of activity, chaotic, passion, impulsive, potentially good or bad; and "tamas" being the quality of darkness, ignorance, destructive, lethargic, negative. Everything, all life forms and human beings, state Samkhya scholars, have these three guṇas, but in different proportions. The interplay of these guṇas defines the character of someone or something, of nature and determines the progress of life. The Samkhya theory of guṇas was widely discussed, developed and refined by various schools of Indian philosophies, including Buddhism. Samkhya's philosophical treatises also influenced the development of various theories of Hindu ethics.
Vedānta.
Realization of the nature of Self-identity is the principal object of the Vedanta system of Indian metaphysics. In the Upanishads, self-consciousness is not the first-person indexical self-awareness or the self-awareness which is self-reference without identification, and also not the self-consciousness which as a kind of desire is satisfied by another self-consciousness. It is Self-realisation; the realisation of the Self consisting of consciousness that leads all else.
The word "Self-consciousness" in the Upanishads means the knowledge about the existence and nature of Brahman. It means the consciousness of our own real being, the primary reality. Self-consciousness means Self-knowledge, the knowledge of Prajna i.e. of Prana which is Brahman. According to the Upanishads the Atman or Paramatman is phenomenally unknowable; it is the object of realisation. The Atman is unknowable in its essential nature; it is unknowable in its essential nature because it is the eternal subject who knows about everything including itself. The Atman is the knower and also the known.
Metaphysicians regard the Self either to be distinct from the Absolute or entirely identical with the Absolute. They have given form to three schools of thought – a) the "Dualistic school", b) the "Quasi-dualistic school" and c) the "Monistic school", as the result of their varying mystical experiences. Prakrti and Atman, when treated as two separate and distinct aspects form the basis of the Dualism of the Shvetashvatara Upanishad. Quasi-dualism is reflected in the Vaishnavite-monotheism of Ramanuja and the absolute Monism, in the teachings of Adi Shankara.
Self-consciousness is the Fourth state of consciousness or "Turiya", the first three being "Vaisvanara", "Taijasa" and "Prajna". These are the four states of individual consciousness.
There are three distinct stages leading to Self-realisation. The First stage is in mystically apprehending the glory of the Self within us as though we were distinct from it. The Second stage is in identifying the "I-within" with the Self, that we are in essential nature entirely identical with the pure Self. The Third stage is in realising that the Atman is Brahman, that there is no difference between the Self and the Absolute. The Fourth stage is in realising "I am the Absolute" - "Aham Brahman Asmi". The Fifth stage is in realising that Brahman is the "All" that exists, as also that which does not exist.
Scholasticism and the Middle Ages.
Between about 1100 and 1500, philosophy as a discipline took place as part of the Catholic church's teaching system, known as scholasticism. Scholastic philosophy took place within an established
framework blending Christian theology with Aristotelian teachings. Although fundamental orthodoxies could not be challenged, there were nonetheless deep metaphysical disagreements, particularly over the problem of universals, which engaged Duns Scotus and Pierre Abelard. William of Ockham is remembered for his principle of ontological parsimony.
Rationalism and Continental Rationalism.
In the early modern period (17th and 18th centuries), the system-building "scope" of philosophy is often linked to the rationalist "method" of philosophy, that is the technique of deducing the nature of the world by pure reason. The scholastic concepts of substance and accident were employed.
British empiricism.
British empiricism marked something of a reaction to rationalist and system-building philosophy, or "speculative" metaphysics as it was pejoratively termed. The sceptic David Hume famously declared that most metaphysics should be consigned to the flames (see below). Hume was notorious among his contemporaries as one of the first philosophers to openly doubt religion, but is better known now for his critique of causality. John Stuart Mill, Thomas Reid and John Locke were less sceptical, embracing a more cautious style of metaphysics based on realism, common sense and science. Other philosophers, notably George Berkeley were led from empiricism to idealistic metaphysics.
Kant.
Immanuel Kant attempted a grand synthesis and revision of the trends already mentioned: scholastic philosophy, systematic metaphysics, and skeptical empiricism, not to forget the burgeoning science of his day. As did the systems builders, he had an overarching framework in which all questions were to be addressed. Like Hume, who famously woke him from his 'dogmatic slumbers', he was suspicious of metaphysical speculation, and also places much emphasis on the limitations of the human mind.
Kant saw rationalist philosophers as aiming for a kind of metaphysical knowledge he defined as the "synthetic apriori"—that is knowledge that does not come from the senses (it is a priori) but is nonetheless about reality (synthetic). Inasmuch as it is about reality, it differs from abstract mathematical propositions (which he terms analytical apriori), and being apriori it is distinct from empirical, scientific knowledge (which he terms synthetic aposteriori). The only synthetic apriori knowledge we can have is of how our minds organise the data of the senses; that organising framework is space and time, which for Kant have no mind-independent existence, but nonetheless operate uniformly in all humans. Apriori knowledge of space and time is all that remains of metaphysics as traditionally conceived. There "is" a reality beyond sensory data or phenomena, which he calls the realm of noumena; however, we cannot know it as it is in itself, but only as it appears to us. He allows himself to speculate that the origins of God, morality, and free will "might" exist in the noumenal realm, but these possibilities have to be set against its basic unknowability for humans. Although he saw himself as having disposed of metaphysics, in a sense, he has generally been regarded in retrospect as having a metaphysics of his own.
Nineteenth century philosophy was overwhelmingly influenced by Kant and his successors. Schopenhauer, Schelling, Fichte and Hegel all purveyed their own panoramic versions of German Idealism, Kant's own caution about metaphysical speculation, and refutation of idealism, having fallen by the wayside. The idealistic impulse continued into the early twentieth century with British idealists such as F. H. Bradley and J. M. E. McTaggart.
Followers of Karl Marx took Hegel's dialectic view of history and re-fashioned it as materialism.
Early analytical philosophy and positivism.
During the period when idealism was dominant in philosophy, science had been making great advances. The arrival of a new generation of scientifically minded philosophers led to a sharp decline in the popularity of idealism during the 1920s.
Analytical philosophy was spearheaded by Bertrand Russell and G. E. Moore. Russell and William James tried to compromise between idealism and materialism with the theory of neutral monism.
The early to mid twentieth century philosophy also saw a trend to reject metaphysical questions as meaningless. The driving force behind this tendency was the philosophy of logical positivism as espoused by the Vienna Circle.
At around the same time, the American pragmatists were steering a middle course between materialism and idealism.
System-building metaphysics, with a fresh inspiration from science, was revived by A. N. Whitehead and Charles Hartshorne.
Continental philosophy.
The forces that shaped analytical philosophy—the break with idealism, and the influence of science—were much less significant outside the English speaking world, although there was a shared turn toward language. Continental philosophy continued in a trajectory from post Kantianism.
The phenomenology of Husserl and others was intended as a collaborative project for the investigation of the features and structure of consciousness common to all humans, in line with Kant's basing his synthetic apriori on the uniform operation of consciousness. It was officially neutral with regards to ontology, but was nonetheless to spawn a number of metaphysical systems. Brentano's concept of intentionality would become widely influential, including on analytical philosophy.
Heidegger, author of "Being and Time", saw himself as re-focusing on Being-qua-being, introducing the novel concept of "Dasein" in the process. Classing himself an existentialist, Sartre wrote an extensive study of "Being and Nothingness".
The speculative realism movement marks a return to full blooded realism.
Process metaphysics.
There are two fundamental aspects of everyday experience: change and persistence. Until recently, the Western philosophical tradition has arguably championed substance and persistence, with some notable exceptions however. According to process thinkers, novelty, flux and accident do matter, and sometimes they constitute the ultimate reality.
In a broad sense, process metaphysics is as old as Western philosophy, with figures such as Heraclitus, Plotinus, Duns Scotus, Leibniz, David Hume, Georg Wilhelm Friedrich Hegel, Friedrich Wilhelm Joseph von Schelling, Gustav Theodor Fechner, Friedrich Adolf Trendelenburg, Charles Renouvier, Karl Marx, Ernst Mach, Friedrich Wilhelm Nietzsche, Émile Boutroux, Henri Bergson, Samuel Alexander and Nicolas Berdyaev. It seemingly remains an open question whether major "Continental" figures such as the late Martin Heidegger, Maurice Merleau-Ponty, Gilles Deleuze, Michel Foucault, or Jacques Derrida should be included.
In a strict sense, process metaphysics may be limited to the works of a few founding fathers: G. W. F. Hegel, Charles Sanders Peirce, William James, Henri Bergson, A. N. Whitehead, and John Dewey. From a European perspective, there was a very significant and early Whiteheadian influence on the works of outstanding scholars such as Émile Meyerson (1859–1933), Louis Couturat (1868–1914), Jean Wahl (1888–1974), Robin George Collingwood (1889–1943), Philippe Devaux (1902–1979), Hans Jonas (1903–1993), Dorothy M. Emmett (1904–2000), Maurice Merleau Ponty (1908–1961), Enzo Paci (1911–1976), Charlie Dunbar Broad (1887–1971), Wolfe Mays (1912–), Ilya Prigogine (1917–2003), Jules Vuillemin (1920–2001), Jean Ladrière (1921–), Gilles Deleuze (1925–1995), Wolfhart Pannenberg (1928–), and Reiner Wiehl (1929–2010).
Later analytical philosophy.
While early analytic philosophy tended to reject metaphysical theorizing, under the influence of logical positivism, it was revived in the second half of the twentieth century. Philosophers such as David K. Lewis and David Armstrong developed elaborate theories on a range of topics such as universals, causation, possibility and necessity and abstract objects. However, the focus of analytical philosophy generally is away from the construction of all-encompassing systems and toward close analysis of individual ideas.
Among the developments that led to the revival of metaphysical theorizing were Quine's attack on the analytic–synthetic distinction, which was generally taken to undermine Carnap's distinction between existence questions internal to a framework and those external to it.
The philosophy of fiction, the problem of empty names, and the debate over existence's status as a property have all risen out of relative obscurity to become central concerns, while perennial issues such as free will, possible worlds, and the philosophy of time have had new life breathed into them.
Rejections of metaphysics.
A number of individuals have suggested that much of metaphysics should be rejected. In the eighteenth century, David Hume took an extreme position, arguing that all genuine knowledge involves either mathematics or matters of fact and that metaphysics, which goes beyond these, is worthless. He concludes his "Enquiry Concerning Human Understanding" with the statement:
If we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, "Does it contain any abstract reasoning concerning quantity or number?" No. "Does it contain any experimental reasoning concerning matter of fact and existence?" No. Commit it then to the flames: for it can contain nothing but sophistry and illusion.
In the 1930s, A. J. Ayer and Rudolf Carnap endorsed Hume's position; Carnap quoted the passage above. They argued that metaphysical statements are neither true nor false but meaningless since, according to their verifiability theory of meaning, a statement is meaningful only if there can be empirical evidence for or against it. Thus, while Ayer rejected the monism of Spinoza, noted above, he avoided a commitment to pluralism, the contrary position, by holding both views to be without meaning. Carnap took a similar line with the controversy over the reality of the external world.
Thirty-three years after Hume's "Enquiry" appeared, Immanuel Kant published his "Critique of Pure Reason". Although he followed Hume in rejecting much of previous metaphysics, he argued that there was still room for some "synthetic a priori" knowledge, concerned with matters of fact yet obtainable independent of experience. These included fundamental structures of space, time, and causality. He also argued for the freedom of the will and the existence of "things in themselves", the ultimate (but unknowable) objects of experience.
The logical atomist Ludwig Wittgenstein introduced the concept that metaphysics could be influenced by theories of Aesthetics, via Logic, vis. a world composed of "atomical facts".
Arguing against such rejections, the Scholastic philosopher Edward Feser has observed that Hume's critique of metaphysics, and specifically Hume's fork, is "notoriously self-refuting". Feser argues that Hume's fork itself is not a conceptual truth and is not empirically testable.
Metaphysics in science.
Much recent work has been devoted to analyzing the role of metaphysics in scientific theorizing. Alexandre Koyré led this movement, declaring in his book "Metaphysics and Measurement", "It is not by following experiment, but by outstripping experiment, that the scientific mind makes progress." Imre Lakatos maintained that all scientific theories have a metaphysical "hard core" essential for the generation of hypotheses and theoretical assumptions. Thus, according to Lakatos, "scientific changes are connected with vast cataclysmic metaphysical revolutions."
An example from biology of Lakatos' thesis: David Hull has argued that changes in the ontological status of the species concept have been central in the development of biological thought from Aristotle through Cuvier, Lamarck, and Darwin. Darwin's ignorance of metaphysics made it more difficult for him to respond to his critics because he could not readily grasp the ways in which their underlying metaphysical views differed from his own.
In physics, new metaphysical ideas have arisen in connection with quantum mechanics, where subatomic particles arguably do not have the same sort of individuality as the particulars with which philosophy has traditionally been concerned. Also, adherence to a deterministic metaphysics in the face of the challenge posed by the quantum-mechanical uncertainty principle led physicists such as Albert Einstein to propose alternative theories that retained determinism. A. N. Whitehead is famous for creating a metaphysics inspired by electromagnetism and special relativity.
In chemistry, Gilbert Newton Lewis addressed the nature of motion, arguing that an electron should not be said to move when it has none of the properties of motion.
Katherine Hawley notes that the metaphysics even of a widely accepted scientific theory may be challenged if it can be argued that the metaphysical presuppositions of the theory make no contribution to its predictive success.

</doc>
<doc id="18896" url="https://en.wikipedia.org/wiki?curid=18896" title="Human spaceflight">
Human spaceflight

Human spaceflight (also referred to as manned spaceflight) is space travel with a crew or passengers aboard the spacecraft. Spacecraft carrying people may be operated directly, by human crew, or it may be either remotely operated from ground stations on Earth or be autonomous, able to carry out a specific mission with no human involvement.
The first human spaceflight was launched by the Soviet Union on 12 April 1961 as a part of the Vostok program, with cosmonaut Yuri Gagarin aboard. Humans have been continually present in space for on the International Space Station. All early human spaceflight was crewed, where at least some of the passengers acted to carry out tasks of piloting or operating the spacecraft. After 2015, several human-capable spacecraft are being explicitly designed with the ability to operate autonomously.
Since the retirement of the US Space Shuttle in 2011, only Russia and China have maintained human spaceflight capability with the Soyuz program and Shenzhou program. Currently, all expeditions to the International Space Station use Soyuz vehicles, which remain attached to the station to allow quick return if needed. The United States is developing commercial crew transportation to facilitate domestic access to ISS and low Earth orbit, as well as the Orion vehicle for beyond-low Earth orbit applications.
While spaceflight has typically been a government-directed activity, commercial spaceflight has gradually been taking on a greater role. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight, and a number of non-governmental companies have been working to develop a space tourism industry. NASA has also played a role to stimulate private spaceflight through programs such as Commercial Orbital Transportation Services (COTS) and Commercial Crew Development (CCDev). With its 2011 budget proposals released in 2010, the Obama administration moved towards a model where commercial companies would supply NASA with transportation services of both people and cargo transport to low Earth orbit. The vehicles used for these services could then serve both NASA and potential commercial customers. Commercial resupply of ISS began two years after the retirement of the Shuttle, and commercial crew launches could begin by 2017.
History.
First human space flights.
Human spaceflight capability was first developed during the Cold War between the United States and the Soviet Union (USSR), which developed the first intercontinental ballistic missile rockets to deliver nuclear weapons. These rockets were large enough to be adapted to carry the first artificial satellites into low Earth orbit. After the first satellites were launched in 1957 and 1958, the US worked on Project Mercury to launch men singly into orbit, while the USSR secretly pursued the Vostok program to accomplish the same thing. The USSR launched the first human in space, Yuri Gagarin into a single orbit in Vostok 1 on a Vostok 3KA rocket, on April 12, 1961. The US launched its first astronaut, Alan Shepard on a suborbital flight aboard "Freedom 7" on a Mercury-Redstone rocket, on May 5, 1961. The first American in orbit was John Glenn aboard "Friendship 7", launched February 20, 1962 on a Mercury-Atlas rocket. The USSR launched five more cosmonauts in Vostok capsules, including the first woman in space, Valentina Tereshkova aboard Vostok 6 on June 16, 1963. The US launched a total of two astronauts in suborbital flight and four in orbit through 1963.
US President John F. Kennedy raised the stakes of the Space Race by setting the goal of landing a man on the Moon and returning him safely by the end of the 1960s. The US started the three-man Apollo program in 1961 to accomplish this, launched by the Saturn family of launch vehicles, and the interim two-man Project Gemini in 1962, which flew 10 missions launched by Titan II rockets in 1965 and 1966. Gemini's objective was to support Apollo by developing American orbital spaceflight experience and techniques to be used in the Moon mission.
Meanwhile, the USSR remained silent about their intentions to send humans to the Moon, and proceeded to stretch the limits of their single-pilot Vostok capsule into a two- or three-person Voskhod capsule to compete with Gemini. They were able to launch two orbital flights in 1964 and 1965 and achieved the first spacewalk, made by Alexei Leonov on Voskhod 2 on March 8, 1965. But Voskhod did not have Gemini's capability to maneuver in orbit, and the program was terminated. The US Gemini flights did not accomplish the first spacewalk, but overcame the early Soviet lead by performing several spacewalks and solving the problem of astronaut fatigue caused by overcoming the lack of gravity, demonstrating up to two weeks endurance in a human spaceflight, and the first space rendezvous and dockings of spacecraft.
The US succeeded in developing the Saturn V rocket necessary to send the Apollo spacecraft to the Moon, and sent Frank Borman, James Lovell, and William Anders into 10 orbits around the Moon in Apollo 8 in December 1968. In July 1969, Apollo 11 accomplished Kennedy's goal by landing Neil Armstrong and Buzz Aldrin on the Moon July 21 and returning them safely on July 24 along with Command Module pilot Michael Collins. A total of six Apollo missions landed 12 men to walk on the Moon through 1972, half of which drove electric powered vehicles on the surface. The crew of Apollo 13, Lovell, Jack Swigert, and Fred Haise, survived a catastrophic in-flight spacecraft failure and returned to Earth safely without landing on the Moon.
Meanwhile, the USSR secretly pursued human lunar lunar orbiting and landing programs. They successfully developed the three-person Soyuz spacecraft for use in the lunar programs, but failed to develop the N1 rocket necessary for a human landing, and discontinued the lunar programs in 1974. On losing the Moon race, they concentrated on the development of space stations, using the Soyuz as a ferry to take cosmonauts to and from the stations. They started with a series of Salyut sortie stations from 1971 to 1986.
After the Apollo program, the US launched the Skylab sortie space station in 1973, manning it for 171 days with three crews aboard Apollo spacecraft. President Richard Nixon and Soviet Premier Leonid Brezhnev negotiated an easing of relations known as détente, an easing of Cold War tensions. As part of this, they negotiated the Apollo-Soyuz Test Project, in which an Apollo spacecraft carrying a special docking adapter module rendezvoused and docked with Soyuz 19 in 1975. The American and Russian crews shook hands in space, but the purpose of the flight was purely diplomatic and symbolic.
Nixon appointed his Vice President Spiro Agnew to head a Space Task Group in 1969 to recommend follow-on human spaceflight programs after Apollo. The group proposed an ambitious Space Transportation System based on a reusable Space Shuttle which consisted of a winged, internally fueled orbiter stage burning liquid hydrogen, launched by a similar, but larger kerosene-fueled booster stage, each equipped with airbreathing jet engines for powered return to a runway at the Kennedy Space Center launch site. Other components of the system included a permanent modular space station, reusable space tug and nuclear interplanetary ferry, leading to a human expedition to Mars as early as 1986, or as late as 2000, depending on the level of funding allocated. However, Nixon knew the American political climate would not support Congressional funding for such an ambition, and killed proposals for all but the Shuttle, possibly to be followed by the space station. Plans for the Shuttle were scaled back to reduce development risk, cost, and time, replacing the piloted flyback booster with two reusable solid rocket boosters, and the smaller orbiter would use an expendable external propellant tank to feed its hydrogen-fueled main engines. The orbiter would have to make unpowered landings.
The two nations continued to compete rather than cooperate in space, as the US turned to developing the Space Shuttle and planning the space station, dubbed "Freedom". 
The USSR launched three Almaz military sortie stations from 1973 to 1977, disguised as Salyuts. They followed Salyut with the development of "Mir", the first modular, semi-permanent space station, the construction of which took place from 1986 to 1996. "Mir" orbited at an altitude of , at a 51.6° inclination. It was occupied for 4,592 days, and made a controlled reentry in 2001.
The Space Shuttle started flying in 1981, but the US Congress failed to approve sufficient funds to make "Freedom" a reality. A fleet of four shuttles was built: "Columbia", "Challenger", "Discovery", and "Atlantis". A fifth shuttle, "Endeavour", was built to replace "Challenger" which was destroyed in an accident during launch which killed 7 astronauts on January 28, 1986. Twenty-two Shuttle flights carried a European Space Agency sortie space station called Spacelab in the payload bay from 1983 to 1998.
The USSR copied the reusable Space Shuttle orbiter, which it called "Buran". It was designed to be launched into orbit by the expendable Energia rocket, and capable of robotic orbital flight and landing. Unlike the US Shuttle, "Buran" had no main rocket engines, but used its orbital maneuvering engines to insert itself into orbit; but it had airbreathing jet engines for powered landings. A single unmanned orbital test flight was successfully made in November 1988. A second test flight was planned by 1993, but the program was cancelled due to lack of funding and the dissolution of the Soviet Union in 1991. Two more orbiters were never completed, and the first one was destroyed in a hangar roof collapse in May 2002.
US / Russian cooperation.
The dissolution of the Soviet Union in 1991 brought an end to the Cold War and opened the door to true cooperation between the US and Russia. The Soviet Soyuz and Mir programs were taken over by the Russian Federal Space Agency, now known as the Roscosmos State Corporation. The Shuttle-Mir Program included American Space Shuttles visiting the "Mir" space station, Russian cosmonauts flying on the Shuttle, and an American astronaut flying aboard a Soyuz spacecraft for long-duration expeditions aboard "Mir".
In 1993, President Bill Clinton secured Russia's cooperation in converting the planned Space Station "Freedom" into the International Space Station (ISS). Construction of the station began in 1998. The station orbits at an altitude of and an inclination of 51.65°.
The Space Shuttle was retired in 2011 after 135 orbital flights, several of which helped assemble, supply, and crew the ISS. "Columbia" was destroyed in another accident during reentry, which killed 7 astronauts on February 1, 2003.
China.
After Russia's launch of Sputnik 1 in 1957, Chairman Mao Zedong intended to place a Chinese satellite in orbit by 1959 to celebrate the 10th anniversary of the founding of the People's Republic of China (PRC), However, China did not successfully launch its first satellite until April 24, 1970. Mao and Premier Zhou Enlai decided on July 14, 1967, that the PRC should not be left behind, and started China's own human spaceflight program. The first attempt, the Shuguang spacecraft copied from the US Gemini, was cancelled on May 13, 1972.
China later designed the Shenzhou spacecraft resembling the Russian Soyuz, and became the third nation to achieve independent human spaceflight capability by launching Yang Liwei on a 21-hour flight aboard Shenzhou 5 on October 15, 2003. China launched the Tiangong-1 space station on September 29, 2011, and two sortie missions to it: Shenzhou 9 June 16–29, 2012, with China's first female astronaut Liu Yang; and Shenzhou 10, June 13–26, 2013.
Abandoned programs of other nations.
The European Space Agency began development in 1987 of the Hermes spaceplane, to be launched on the Ariane 5 expendable launch vehicle. The project was cancelled in 1992, when it became clear that neither cost nor performance goals could be achieved. No Hermes shuttles were ever built.
Japan began development in the 1980s of the HOPE-X experimental spaceplane, to be launched on its H-IIA expendable launch vehicle. A string of failures in 1998 led to funding reduction, and the project's cancellation in 2003.
United States post-Space Shuttle gap.
Under the Bush administration, the Constellation Program included plans for retiring the Shuttle program and replacing it with the capability for spaceflight beyond low Earth orbit. In the 2011 United States federal budget, the Obama administration cancelled Constellation for being over budget and behind schedule while not innovating and investing in critical new technologies. For beyond low earth orbit human spaceflight NASA is developing the Orion spacecraft to be launched by the Space Launch System. Under the Commercial Crew Development plan, NASA will rely on transportation services provided by the private sector to reach low earth orbit, such as Space X's Falcon 9/Dragon V2, Sierra Nevada Corporation's Dream Chaser, or Boeing's CST-100. The period between the retirement of the shuttle in 2011 and the initial operational capability of new systems in 2017, similar to the gap between the end of Apollo in 1975 and the first space shuttle flight in 1981, is referred to by a presidential Blue Ribbon Committee as the U.S. human spaceflight gap.
Commercial private spaceflight.
After the early 2000s, a variety of private spaceflight ventures were undertaken. Several of the companies formed by 2005, including Blue Origin, SpaceX, Virgin Galactic, and XCOR Aerospace have explicit plans to advance human spaceflight. , all four of those companies have development programs underway to fly commercial passengers before 2018.
Commercial suborbital spacecraft aimed at the space tourism market include Virgin Galactic SpaceshipTwo, and XCOR's Lynx spaceplane which are both under development and could reach space before 2017.
More recently, Blue Origin has begun a multi-year test program of their New Shepardvehicle with plans to test in 2015–2016 while carrying no passengers, then adding "test passengers" in 2017, and initiate commercial flights in 2018.
SpaceX and Boeing are both developing passenger-capable orbital space capsules as of 2015, planning to fly NASA astronauts to the International Space Station as soon as 2018. SpaceX will be carrying passengers on Dragon 2 launched on a Falcon 9 launch vehicle. Boeing will be doing it with their CST-100 launched on a United Launch Alliance Atlas V launch vehicle.
Development funding for these orbital-capable technologies has been provided by a mix of government and private funds, with SpaceX providing a greater portion of total development funding for this human-carrying capability from private investment.
There have been no public announcements of commercial offerings for orbital flights from either company, although both companies are planning some flights with their own private, not NASA, astronauts on board.
Milestones.
Svetlana Savitskaya became the first woman to walk in space on 25 July 1984.
Sally Ride became the first American woman in space in 1983. Eileen Collins was the first female shuttle pilot, and with shuttle mission STS-93 in 1999 she became the first woman to command a U.S. spacecraft.
The longest single human spaceflight is that of Valeri Polyakov, who left Earth on 8 January 1994, and did not return until 22 March 1995 (a total of 437 days 17 h 58 min 16 s). Sergei Krikalyov has spent the most time of anyone in space, 803 days, 9 hours, and 39 minutes altogether. The longest period of continuous human presence in space is on the International Space Station, exceeding the previous record of almost 10 years (or 3,634 days) held by Mir, spanning the launch of Soyuz TM-8 on 5 September 1989 to the landing of Soyuz TM-29 on 28 August 1999.
For many years, only the USSR (later Russia) and the United States had their own astronauts. Citizens of other nations flew in space, beginning with the flight of Vladimir Remek, a Czech, on a Soviet spacecraft on 2 March 1978, in the Interkosmos programme. , citizens from 38 nations (including space tourists) have flown in space aboard Soviet, American, Russian, and Chinese spacecraft.
Space programs.
Human spaceflight programs have been conducted by the former Soviet Union and current Russian Federation, the United States, the People's Republic of China and by private spaceflight company Scaled Composites.
Current programs.
Space vehicles are spacecraft used for transportation between the Earth's surface and outer space, or between locations in outer space. The following space vehicles and spaceports are currently used for launching human spaceflights:
The following space stations are currently maintained in Earth orbit for human occupation:
Numerous private companies attempted human spaceflight programs in an effort to win the $10 million Ansari X Prize. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight. SpaceShipOne captured the prize on 4 October 2004, when it accomplished two consecutive flights within one week. SpaceShipTwo, launching from the carrier aircraft White Knight Two, is planned to conduct regular suborbital space tourism.
Most of the time, the only humans in space are those aboard the ISS, whose crew of six spends up to six months at a time in low Earth orbit.
NASA and ESA use the term "human spaceflight" to refer to their programs of launching people into space. These endeavors have also been referred to as "manned space missions," though because of gender specificity this is no longer official parlance according to NASA style guides.
Planned future programs.
The Indian Space Research Organisation (ISRO) has begun work on pre-project activities of a human space flight mission program. The objective is to carry a crew of two to Low Earth Orbit (LEO) and return them safely to a predefined destination on Earth. The program is proposed to be implemented in defined phases. Currently, the pre-project activities are progressing with a focus on the development of critical technologies for subsystems such as the Crew Module (CM), Environmental Control and Life Support System (ECLSS), Crew Escape System, etc. The department has initiated pre-project activities to study technical and managerial issues related to crewed missions. The program envisages the development of a fully autonomous orbital vehicle carrying 2 or 3 crew members to about 300 km low earth orbit and their safe return.
The United States’ National Aeronautics and Space Administration (NASA) is developing a plan to land humans on Mars by the 2030s. The first step in this mission begins sometime during 2020, when NASA plans to send an unmanned craft into deep space to retrieve an asteroid. The asteroid will be pushed into the moon’s orbit, and studied by astronauts aboard Orion, NASA’s first human spacecraft in a generation. Orion’s crew will return to Earth with samples of the asteroid and their collected data. In addition to broadening America’s space capabilities, this mission will test newly developed technology, such as solar electric propulsion, which uses solar arrays for energy and requires ten times less propellant than the conventional chemical counterpart used for powering space shuttles to orbit.
Several other countries and space agencies have announced and begun human spaceflight programs by their own technology, Japan (JAXA), Iran (ISA) and Malaysia (MNSA).
Safety concerns.
There are two main sources of hazard in space flight: those due to the "environment" of space which make it hostile to the human body, and the potential for "mechanical" malfunctions of the equipment required to accomplish space flight.
Environmental hazards.
Planners of human spaceflight missions face a number of safety concerns.
Life support.
The immediate needs for breathable air and drinkable water are addressed by the life support system of the spacecraft.
Medical issues.
Medical consequences such as possible blindness and bone loss have been associated with human space flight.
On 31 December 2012, a NASA-supported study reported that spaceflight may harm the brain of astronauts and accelerate the onset of Alzheimer's disease.
In October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.
Microgravity.
Medical data from astronauts in low earth orbits for long periods, dating back to the 1970s, show several adverse effects of a microgravity environment: loss of bone density, decreased muscle strength and endurance, postural instability, and reductions in aerobic capacity. Over time these deconditioning effects can impair astronauts’ performance or increase their risk of injury.
In a weightless environment, astronauts put almost no weight on the back muscles or leg muscles used for standing up, which causes them to weaken and get smaller. Astronauts can lose up to twenty per cent of their muscle mass on spaceflights lasting five to eleven days. The consequent loss of strength could be a serious problem in case of a landing emergency. Upon return to Earth from long-duration flights, astronauts are considerably weakened, and are not allowed to drive a car for twenty-one days.
Astronauts experiencing weightlessness will often lose their orientation, get motion sickness, and lose their sense of direction as their bodies try to get used to a weightless environment. When they get back to Earth, or any other mass with gravity, they have to readjust to the gravity and may have problems standing up, focusing their gaze, walking and turning. Importantly, those body motor disturbances after changing from different gravities only get worse the longer the exposure to little gravity. These changes will affect operational activities including approach and landing, docking, remote manipulation, and emergencies that may happen while landing. This can be a major roadblock to mission success.
In addition, after long space flight missions, male astronauts may experience severe eyesight problems. Such eyesight problems may be a major concern for future deep space flight missions, including a crewed mission to the planet Mars.
Radiation.
Without proper shielding, the crews of missions beyond low Earth orbit (LEO) might be at risk from high-energy protons emitted by solar flares. Lawrence Townsend of the University of Tennessee and others have studied the most powerful solar flare ever recorded. That flare was seen by the British astronomer Richard Carrington in September 1859. Radiation doses astronauts would receive from a Carrington-type flare could cause acute radiation sickness and possibly even death.
Another type of radiation, galactic cosmic rays, presents further challenges to human spaceflight beyond low Earth orbit.
There is also some scientific concern that extended spaceflight might slow down the body’s ability to protect itself against diseases. Some of the problems are a weakened immune system and the activation of dormant viruses in the body. Radiation can cause both short and long term consequences to the bone marrow stem cells which create the blood and immune systems. Because the interior of a spacecraft is so small, a weakened immune system and more active viruses in the body can lead to a fast spread of infection.
Isolation.
During long missions, astronauts are isolated and confined into small spaces. Depression, cabin fever and other psychological problems may impact the crew's safety and mission success.
Astronauts may not be able to quickly return to Earth or receive medical supplies, equipment or personnel if a medical emergency occurs. The astronauts may have to rely for long periods on their limited existing resources and medical advice from the ground.
Mechanical hazards.
Space flight requires much higher velocities than ground or air transportation, which in turn requires the use of high energy density propellants for launch, and the dissipation of large amounts of energy, usually as heat, for safe reentry through the Earth's atmosphere.
Launch.
Since rockets carry the potential for fire or explosive destruction, space capsules generally employ some sort of launch escape system, consisting either of a tower-mounted solid fuel rocket to quickly carry the capsule away from the launch vehicle (employed on Mercury, Apollo, and Soyuz), or else ejection seats (employed on Vostok and Gemini) to carry astronauts out of the capsule and away for individual parachute landing. The escape tower is discarded at some point before the launch is complete, at a point where an abort can be performed using the spacecraft's engines.
Such a system is not always practical for multiple crew member vehicles (particularly spaceplanes), depending on location of egress hatch(es). When the single-hatch Vostok capsule was modified to become the 2 or 3-person Voskhod, the single-cosmonaut ejection seat could not be used, and no escape tower system was added. The two Voskhod flights in 1964 and 1965 avoided launch mishaps. The Space Shuttle carried ejection seats and escape hatches for its pilot and copilot in early flights, but these could not be used for passengers who sat below the flight deck on later flights, and so were discontinued.
The only in-flight launch abort of a crewed flight occurred on Soyuz 18a on April 5, 1975. The abort occurred after the launch escape system had been jettisoned, when the launch vehicle's spent second stage failed to separate before the third stage ignited. The vehicle strayed off course, and the crew separated the spacecraft and fired its engines to pull it away from the errant rocket. Both cosmonauts landed safely.
In the only use of a launch escape system on a crewed flight, the planned Soyuz T-10a launch on September 26, 1983 was aborted by a launch vehicle fire 90 seconds before liftoff. Both cosmonauts aboard landed safely.
The only crew fatality during launch occurred on January 28, 1986, when the Space Shuttle "Challenger" broke apart 73 seconds after liftoff, due to failure of a solid rocket booster seal which caused separation of the booster and failure of the external fuel tank, resulting in explosion of the fuel. All seven crew members were killed.
Reentry and landing.
The single pilot of Soyuz 1, Vladimir Komarov was killed when his capsule's parachutes failed during an emergency landing on April 24, 1967, causing the capsule to crash.
The crew of seven aboard the Space Shuttle "Columbia" were killed on reentry after completing a successful mission in space on February 1, 2003. A wing leading edge reinforced carbon-carbon heat shield had been damaged by a piece of frozen external tank foam insulation which broke off and struck the wing during launch. Hot reentry gasses entered and destroyed the wing structure, leading to breakup of the orbiter vehicle.
Artificial atmosphere.
There are two basic choices for an artificial atmosphere: either an Earth-like mixture of oxygen in an inert gas such as nitrogen or helium, or pure oxygen, which can be used at lower than standard atmospheric pressure. A nitrogen-oxygen mixture is used in the International Space Station and Soyuz spacecraft, while low-pressure pure oxygen is commonly used in space suits for extravehicular activity.
Use of a gas mixture carries risk of decompression sickness (commonly known as "the bends") when transitioning to or from the pure oxygen space suit environment. There have also been instances of injury and fatalities caused by suffocation in the presence of too much nitrogen and not enough oxygen.
A pure oxygen atmosphere carries risk of fire. The original design of the Apollo spacecraft used pure oxygen at greater than atmospheric pressure prior to launch. An electrical fire started in the cabin of Apollo 1 during a ground test at Cape Kennedy Air Force Station Launch Complex 34 on January 27, 1967, and spread rapidly. The high pressure (increased even higher by the fire) prevented removal of the plug door hatch cover in time to rescue the crew. All three, Gus Grissom, Edward H. White, and Roger Chaffee, were killed. This led NASA to use a nitrogen/oxygen atmosphere before launch, and low pressure pure oxygen only in space.
Reliability.
The March 1966 Gemini 8 mission was aborted in orbit when an attitude control system thruster stuck in the on position, sending the craft into a dangerous spin which threatened the lives of Neil Armstrong and David Scott. Armstrong had to shut the control system off and use the reentry control system to stop the spin. The craft made an emergency reentry and the astronauts landed safely. The most probable cause was determined to be an electrical short due to a static electricity discharge, which caused the thruster to remain powered even when switched off. The control system was modified to put each thruster on its own isolated circuit.
The third lunar landing expedition Apollo 13 in April 1970, was aborted and the lives of the crew, James Lovell, Jack Swigert and Fred Haise, were threatened by failure of a cryogenic liquid oxygen tank en route to the Moon. The tank burst when electrical power was applied to internal stirring fans in the tank, causing the immediate loss of all of its contents, and also damaging the second tank, causing the loss of its remaining oxygen in a span of 130 minutes. This in turn caused loss of electrical power provided by fuel cells to the command spacecraft. The crew managed to return to Earth safely by using the lunar landing craft as a "life boat". The tank failure was determined to be caused by two mistakes. The tank's drain fitting had been damaged when it was dropped during factory testing. This necessitated use of its internal heaters to boil out the oxygen after a pre-launch test, which in turn damaged the fan wiring's electrical insulation, because the thermostats on the heaters did not meet the required voltage rating due to a vendor miscommunication.
Fatality risk.
, 22 crew members have died in accidents aboard spacecraft. Over 100 others have died in accidents during activity directly related to spaceflight or testing.

</doc>
<doc id="18899" url="https://en.wikipedia.org/wiki?curid=18899" title="Mendelevium">
Mendelevium

Mendelevium is a synthetic element with chemical symbol Md (formerly Mv) and atomic number 101. A metallic radioactive transuranic element in the actinide series, it is the first element that currently cannot be produced in macroscopic quantities through neutron bombardment of lighter elements. It is the antepenultimate actinide and the ninth transuranic element. It can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of sixteen mendelevium isotopes are known, the most stable being 258Md with a half-life of 51 days; nevertheless, the shorter-lived 256Md (half-life 1.27 hours) is most commonly used in chemistry because it can be produced on a larger scale.
Mendelevium was discovered by bombarding einsteinium with alpha particles in 1955, the same method still used to produce it today. It was named after Dmitri Mendeleev, father of the periodic table of the chemical elements. Using available microgram quantities of the isotope einsteinium-253, over a million mendelevium atoms may be produced each hour. The chemistry of mendelevium is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. Owing to the small amounts of produced mendelevium and all of its isotopes having relatively short half-lives, there are currently no uses for it outside of basic scientific research.
Discovery.
Mendelevium was the ninth transuranic element to be synthesized. It was first synthesized by Albert Ghiorso, Glenn T. Seaborg, Gregory R. Choppin, Bernard G. Harvey, and team leader Stanley G. Thompson in early 1955 at the University of California, Berkeley. The team produced 256Md (half-life of 87 minutes) when they bombarded an 253Es target consisting of only a billion (109) einsteinium atoms with alpha particles (helium nuclei) in the Berkeley Radiation Laboratory's 60-inch cyclotron, thus increasing the target's atomic number by two. 256Md thus became the first isotope of any element to be synthesized one atom at a time. In total, seventeen mendelevium atoms were produced. This discovery was part of a program, begun in 1952, that irradiated plutonium with neutrons to transmute it into heavier actinides. This method was necessary as the previous method used to synthesize transuranic elements, neutron capture, could not work because of a lack of beta decaying isotopes of fermium that would produce isotopes of the next element, mendelevium, and also due to the very short half-life to spontaneous fission of fermium-258 that thus constituted a hard limit to the success of the neutron capture process.
To predict if the production of mendelevium would be possible, the team made use of a rough calculation. The number of atoms that would be produced would be approximately equal to the product of the number of atoms of target material, the target's cross section, the ion beam intensity, and the time of bombardment; this last factor was related to the half-life of the product when bombarding for a time on the order of its half-life. This gave one atom per experiment. Thus under optimum conditions, the preparation of only one atom of element 101 per experiment could be expected. This calculation demonstrated that it was feasible to go ahead with the experiment. The target material, einsteinium-253, could be produced readily from irradiating plutonium: one year of irradiation would give a billion atoms, and its three-week half-life meant that the element 101 experiments could be conducted in one week after the produced einsteinium was separated and purified to make the target. However, it was necessary to upgrade the cyclotron to obtain the needed intensity of 1014 alpha particles per second; Seaborg applied for the necessary funds.
While Seaborg applied for funding, Harvey worked on the einsteinium target, while Thomson and Choppin focused on methods for chemical isolation. Choppin suggested using α-hydroxyisobutyric acid to separate the mendelevium atoms from those of the lighter actinides. The actual synthesis was done by a recoil technique, introduced by Albert Ghiorso. In this technique, the einsteinium was placed on the opposite side of the target from the beam, so that the recoiling mendelevium atoms would get enough momentum to leave the target and be caught on a catcher foil made of gold. This recoil target was made by an electroplating technique, developed by Alfred Chetham-Strode. This technique gave a very high yield, which was absolutely necessary when working with such a rare and valuable product as the einsteinium target material. The recoil target consisted of 109 atoms of 253Es which were deposited electrolytically on a thin gold foil. It was bombarded by 41 MeV alpha particles in the Berkeley cyclotron with a very high beam density of 6×1013 particles per second over an area of 0.05 cm2. The target was cooled by water or liquid helium, and the foil could be replaced.
Initial experiments were carried out in September 1954. No alpha decay was seen from mendelevium atoms; thus, Ghiorso suggested that the mendelevium had all decayed by electron capture to fermium and that the experiment should be repeated to search instead for spontaneous fission events. The repetition of the experiment happened in February 1955.
On the day of discovery, 19 February, alpha irradiation of the einsteinium target occurred in three three-hour sessions. The cyclotron was in the University of California campus, while the Radiation Laboratory was on the next hill. To deal with this situation, a complex procedure was used: Ghiorso took the catcher foils (there were three targets and three foils) from the cyclotron to Harvey, who would use aqua regia to dissolve it and pass it through an anion-exchange resin column to separate out the transuranium elements from the gold and other products. The resultant drops entered a test tube, which Choppin and Ghiorso took in a car to get to the Radiation Laboratory as soon as possible. There Thompson and Choppin used a cation-exchange resin column and the α-hydroxyisobutyric acid. The solution drops were collected on platinum disks and dried under heat lamps. The three disks were expected to contain respectively the fermium, no new elements, and the mendelevium. Finally, they were placed in their own counters, which were connected to recorders such that spontaneous fission events would be recorded as huge deflections in a graph showing the number and time of the decays. There thus was no direct detection, but by observation of spontaneous fission events arising from its electron-capture daughter 256Fm. The first one was identified with a "hooray" followed by a "double hooray" and a "triple hooray". The fourth one eventually officially proved the chemical identification of the 101st element, mendelevium. In total, five decays were reported up till 4 a.m. Seaborg was notified and the team left to sleep. Additional analysis and further experimentation showed the produced mendelevium isotope to have mass 256 and to decay by electron capture to fermium-256 with a half-life of 1.5 h.
Being the first of the second hundred of the chemical elements, it was decided that the element would be named "mendelevium" after the Russian chemist Dmitri Mendeleev, father of the periodic table. Due to the fact that this discovery came during the Cold War, Seaborg had to request permission of the government of the United States to propose that the element be named for a Russian, but it was granted. The name "mendelevium" was accepted by the International Union of Pure and Applied Chemistry (IUPAC) in 1955 with symbol "Mv", which was changed to "Md" in the next IUPAC General Assembly (Paris, 1957).
Characteristics.
Physical.
In the periodic table, mendelevium is located to the right of the actinide fermium, to the left of the actinide nobelium, and below the lanthanide thulium. Mendelevium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.
The lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have f"n"d1s2 configurations, whereas the latter have f"n"+1s2 configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the configuration over the [Rn5f137s2 configuration for mendelevium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number. Thermochromatographic studies with trace quantities of mendelevium by Zvara and Hübener from 1976 to 1982 confirmed this prediction. In 1990, Haire and Gibson estimated mendelevium metal to have an enthalpy of sublimation between 134 and 142 kJ·mol−1. Divalent mendelevium metal should have a metallic radius of around (194 ± 10) pm. Mendelevium's melting point has been estimated at 827 °C, the same value as that predicted for the neighboring element nobelium.
Chemical.
The chemistry of mendelevium is mostly known only in solution, in which it can take on the +3 or +2 oxidation states. The +1 state has also been reported, but has not yet been confirmed.
Before mendelevium's discovery, Seaborg and Katz predicted that it should be predominantly trivalent in aqueous solution and hence should behave similarly to other tripositive lanthanides and actinides. After the synthesis of mendelevium in 1955, these predictions were confirmed, first in the observation at its discovery that it eluted just after fermium in the trivalent actinide elution sequence from a cation-exchange column of resin, and later the 1967 observation that mendelevium could form insoluble hydroxides and fluorides that coprecipitated with trivalent lanthanide salts. Cation-exchange and solvent extraction studies led to the conclusion that mendelevium was a trivalent actinide with an ionic radius somewhat smaller than that of the previous actinide, fermium. Mendelevium can form coordination complexes with 1,2-cyclohexanedinitrilotetraacetic acid (DCTA).
In reducing conditions, mendelevium(III) can be easily reduced to mendelevium(II), which is stable in aqueous solution. The standard reduction potential of the "E"°(Md3+→Md2+) couple has been variously estimated as −0.10 V or −0.20 V. In comparison, "E"°(Md3+→Md0) should be around −1.74 V, and "E"°(Md2+→Md0) should be around −2.5 V. Mendelevium(II)'s elution behavior has been compared with that of strontium(II) and europium(II).
In 1973, mendelevium(I) was reported to have been produced by Russian scientists, who obtained it by reducing higher oxidation states of mendelevium with samarium(II). It was found to be stable in neutral water–ethanol solution and be homologous to caesium(I). However, later experiments found no evidence for mendelevium(I) and found that mendelevium behaved like divalent elements when reduced, not like the monovalent alkali metals. Nevertheless, the Russian team conducted further studies on the thermodynamics of cocrystallizing mendelevium with alkali metal chlorides, and concluded that mendelevium(I) had formed and could form mixed crystals with divalent elements, thus cocrystallizing with them. The status of the +1 oxidation state is still tentative.
Although "E"°(Md4+→Md3+) was predicted in 1975 to be +5.4 V, suggesting that mendelevium(III) could be easily oxidized to mendelevium(IV), 1967 experiments with the strong oxidizing agent sodium bismuthate were unable to oxidize mendelevium(III) to mendelevium(IV).
Atomic.
A mendelevium atom has 101 electrons, of which at least three (and perhaps four) can act as valence electrons. They are expected to be arranged in the configuration (ground state term symbol 2F7/2), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, three valence electrons may be lost, leaving behind a [Rn5f12 core: this conforms to the trend set by the other actinides with their  5f"n" electron configurations in the tripositive state. The first ionization potential of mendelevium was measured to be at most (6.58 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to mendelevium's scarcity and high radioactivity. The ionic radius of hexacoordinate Md3+ had been preliminarily estimated in 1978 to be around 91.2 pm; 1988 calculations based on the logarithmic trend between distribution coefficients and ionic radius produced a value of 89.6 pm, as well as an enthalpy of hydration of −(3654 ± 12) kJ·mol−1. Md2+ should have an ionic radius of 115 pm and hydration enthalpy −1413 kJ·mol−1; Md+ should have ionic radius 117 pm.
Isotopes.
Sixteen isotopes of mendelevium are known, with mass numbers from 245 to 260; all are radioactive. Additionally, five nuclear isomers are known: 245mMd, 247mMd, 249mMd, 254mMd, and 258mMd. Of these, the longest-lived isotope is 258Md with a half-life of 51.5 days, and the longest-lived isomer is 258mMd with a half-life of 58.0 minutes. Nevertheless, the slightly shorter-lived 256Md (half-life 1.27 hours) is more often used in chemical experimentation because it can be produced in larger quantities from alpha particle irradiation of einsteinium. After 258Md, the next most stable mendelevium isotopes are 260Md with a half-life of 31.8 days, 257Md with a half-life of 5.52 hours, 259Md with a half-life of 1.60 hours, and 256Md with a half-life of 1.27 hours. All of the remaining mendelevium isotopes have half-lives that are less than an hour, and the majority of these have half-lives that are less than 5 minutes.
The half-lives of mendelevium isotopes mostly increase smoothly from 245Md onwards, reaching a maximum at 258Md. Experiments and predictions suggest that the half-lives will then decrease, apart from 260Md with a half-life of 31.8 days, as spontaneous fission becomes the dominant decay mode due to the mutual repulsion of the protons posing a limit to the island of relative stability of long-lived nuclei in the actinide series.
Mendelevium-256, the chemically most important isotope of mendelevium, decays through electron capture 90.7% of the time and alpha decay 9.9% of the time. It is most easily detected through the spontaneous fission of its electron-capture daughter fermium-256, but in the presence of other nuclides that undergo spontaneous fission, alpha decays at the characteristic energies for mendelevium-256 (7.205 and 7.139 MeV) can provide more useful identification.
Production and isolation.
The lightest mendelevium isotopes (245Md to 247Md) are mostly produced through bombardment of bismuth targets with heavy argon ions, while slightly heavier ones (248Md to 253Md) are produced by bombarding plutonium and americium targets with lighter ions of carbon and nitrogen. The most important and most stable isotopes are in the range from 254Md to 258Md and are produced through bombardment of einsteinium isotopes with alpha particles: einsteinium-253, -254, and -255 can all be used. 259Md is produced as a daughter of 259No, and 260Md can be produced in a transfer reaction between einsteinium-254 and oxygen-18. Typically, the most commonly used isotope 256Md is produced by bombarding either einsteinium-253 or -254 with alpha particles: einsteinium-254 is preferred when available because it has a longer half-life and therefore can be used as a target for longer. Using available microgram quantities of einsteinium, femtogram quantities of mendelevium-256 may be produced.
The recoil momentum of the produced mendelevium-256 atoms is used to bring them physically far away from the einsteinium target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum. This eliminates the need for immediate chemical separation, which is both costly and prevents reusing of the expensive einsteinium target. The mendelevium atoms are then trapped in a gas atmosphere (frequently helium), and a gas jet from a small opening in the reaction chamber carries the mendelevium along. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the mendelevium atoms can be transported over tens of meters to be chemically analyzed and have their quantity determined. The mendelevium can then be separated from the foil material and other fission products by applying acid to the foil and then coprecipitating the mendelevium with lanthanum fluoride, then using a cation-exchange resin column with a 10% ethanol solution saturated with hydrochloric acid, acting as an eluant. However, if the foil is made of gold and thin enough, it is enough to simply dissolve the gold in aqua regia before separating the trivalent actinides from the gold using anion-exchange chromatography, the eluant being 6 M hydrochloric acid.
Mendelevium can finally be separated from the other trivalent actinides using selective elution from a cation-exchange resin column, the eluant being ammonia α-HIB. Using the gas-jet method often renders the first two steps unnecessary. The above procedure is the most commonly used one for the separation of transeinsteinium elements.
Another possible way to separate the trivalent actinides is via solvent extraction chromatography using bis-(2-ethylhexyl) phosphoric acid (abbreviated as HDEHP) as the stationary organic phase and nitric acid as the mobile aqueous phase. The actinide elution sequence is reversed from that of the cation-exchange resin column, so that the heavier actinides elute later. The mendelevium separated by this method has the advantage of being free of organic complexing agent compared to the resin column; the disadvantage is that mendelevium then elutes very late in the elution sequence, after fermium.
Another method to isolate mendelevium exploits the distinct elution properties of Md2+ from those of Es3+ and Fm3+. The initial steps are the same as above, and employs HDEHP for extraction chromatography, but coprecipitates the mendelevium with terbium fluoride instead of lanthanum fluoride. Then, 50 mg of chromium is added to the mendelevium to reduce it to the +2 state in 0.1 M hydrochloric acid with zinc or mercury. The solvent extraction then proceeds, and while the trivalent and tetravalent lanthanides and actinides remain on the column, mendelevium(II) does not and stays in the hydrochloric acid. It is then reoxidized to the +3 state using hydrogen peroxide and then isolated by selective elution with 2 M hydrochloric acid (to remove impurities, including chromium) and finally 6 M hydrochloric acid (to remove the mendelevium). It is also possible to use a column of cationite and zinc amalgam, using 1 M hydrochloric acid as an eluant, reducing Md(III) to Md(II) where it behaves like the alkaline earth metals. Thermochromatographic chemical isolation could be achieved using the volatile mendelevium hexafluoroacetylacetonate: the analogous fermium compound is also known and is also volatile.

</doc>
<doc id="18900" url="https://en.wikipedia.org/wiki?curid=18900" title="Modus ponens">
Modus ponens

In propositional logic, modus ponendo ponens (Latin for "the way that affirms by affirming"; generally abbreviated to MP or modus ponens) or implication elimination is a valid, simple argument form and rule of inference. It can be summarized as ""P" implies "Q"; "P" is asserted to be true, so therefore "Q" must be true." The history of "modus ponens" goes back to antiquity.
While "modus ponens" is one of the most commonly used concepts in logic it must not be mistaken for a logical law; rather, it is one of the accepted mechanisms for the construction of deductive proofs that includes the "rule of definition" and the "rule of substitution". "Modus ponens" allows one to eliminate a conditional statement from a logical proof or argument (the antecedents) and thereby not carry these antecedents forward in an ever-lengthening string of symbols; for this reason modus ponens is sometimes called the rule of detachment. Enderton, for example, observes that "modus ponens can produce shorter formulas from longer ones", and Russell observes that "the process of the inference cannot be reduced to symbols. Its sole record is the occurrence of ⊦q consequent . . . an inference is the dropping of a true premise; it is the dissolution of an implication".
A justification for the "trust in inference is the belief that if the two former assertions antecedents are not in error, the final assertion consequent is not in error". In other words: if one statement or proposition implies a second one, and the first statement or proposition is true, then the second one is also true. If "P" implies "Q" and "P" is true, then "Q" is true. An example is:
"Modus ponens" can be stated formally as:
where the rule is that whenever an instance of ""P" → "Q"" and ""P"" appear by themselves on lines of a logical proof, "Q" can validly be placed on a subsequent line; furthermore, the premise "P" and the implication "dissolves", their only trace being the symbol "Q" that is retained for use later e.g. in a more complex deduction.
It is closely related to another valid form of argument, "modus tollens". Both have apparently similar but invalid forms such as affirming the consequent, denying the antecedent, and evidence of absence. Constructive dilemma is the disjunctive version of modus ponens. Hypothetical syllogism is closely related to modus ponens and sometimes thought of as "double modus ponens."
Formal notation.
The "modus ponens" rule may be written in sequent notation:
where ⊢ is a metalogical symbol meaning that "Q" is a syntactic consequence of "P" → "Q" and "P" in some logical system;
or as the statement of a truth-functional tautology or theorem of propositional logic:
where "P", and "Q" are propositions expressed in some formal system.
Explanation.
The argument form has two premises (hypothesis). The first premise is the "if–then" or conditional claim, namely that "P" implies "Q". The second premise is that "P", the antecedent of the conditional claim, is true. From these two premises it can be logically concluded that "Q", the consequent of the conditional claim, must be true as well. In artificial intelligence, "modus ponens" is often called forward chaining.
An example of an argument that fits the form "modus ponens":
This argument is valid, but this has no bearing on whether any of the statements in the argument are true; for "modus ponens" to be a sound argument, the premises must be true for any true instances of the conclusion. An argument can be valid but nonetheless unsound if one or more premises are false; if an argument is valid "and" all the premises are true, then the argument is sound. For example, John might be going to work on Wednesday. In this case, the reasoning for John's going to work (because it is Wednesday) is unsound. The argument is not only sound on Tuesdays (when John goes to work), but valid on every day of the week. A propositional argument using "modus ponens" is said to be deductive.
In single-conclusion sequent calculi, "modus ponens" is the Cut rule. The cut-elimination theorem for a calculus says that every proof involving Cut can be transformed (generally, by a constructive method) into a proof without Cut, and hence that Cut is admissible.
The Curry–Howard correspondence between proofs and programs relates "modus ponens" to function application: if "f" is a function of type "P" → "Q" and "x" is of type "P", then "f x" is of type "Q".
Justification via truth table.
The validity of "modus ponens" in classical two-valued logic can be clearly demonstrated by use of a truth table.
<br>
In instances of "modus ponens" we assume as premises that "p" → "q" is true and "p" is true. Only one line of the truth table—the first—satisfies these two conditions ("p" and "p" → "q"). On this line, "q" is also true. Therefore, whenever "p" → "q" is true and "p" is true, "q" must also be true.

</doc>
<doc id="18901" url="https://en.wikipedia.org/wiki?curid=18901" title="Modus tollens">
Modus tollens

In propositional logic, modus tollens (or modus tollendo tollens and also denying the consequent) (Latin for "the way that denies by denying") is a valid argument form and a rule of inference. It is an application of the general truth that if a statement is true, then so is its contra-positive.
The first to explicitly describe the argument form "modus tollens" were the Stoics.
The inference rule "modus tollens" validates the inference from formula_1 implies formula_2 and the contradictory of formula_2 to the contradictory of formula_1.
The "modus tollens" rule can be stated formally as:
where formula_6 stands for the statement "P implies Q". formula_7 stands for "it is not the case that Q" (or in brief "not Q"). Then, whenever "formula_6" and "formula_9" each appear by themselves as a line of a proof, then "formula_10" can validly be placed on a subsequent line. The history of the inference rule "modus tollens" goes back to antiquity.
"Modus tollens" is closely related to "modus ponens". There are two similar, but invalid, forms of argument: affirming the consequent and denying the antecedent. See also contraposition and proof by contrapositive.
Formal notation.
The "modus tollens" rule may be written in sequent notation:
where formula_12 is a metalogical symbol meaning that formula_10 is a syntactic consequence of formula_6 and formula_9 in some logical system;
or as the statement of a functional tautology or theorem of propositional logic:
where formula_1 and formula_2 are propositions expressed in some formal system;
or including assumptions:
though since the rule does not change the set of assumptions, this is not strictly necessary.
More complex rewritings involving "modus tollens" are often seen, for instance in set theory:
Also in first-order predicate logic:
Strictly speaking these are not instances of "modus tollens", but they may be derived from "modus tollens" using a few extra steps.
Explanation.
The argument has two premises. The first premise is a conditional or "if-then" statement, for example that if P then Q. The second premise is that it is not the case that Q . From these two premises, it can be logically concluded that it is not the case that P.
Consider an example:
Supposing that the premises are both true (the dog will bark if it detects an intruder, and does indeed not bark), it follows that no intruder has been detected. This is a valid argument since it is not possible for the conclusion to be false if the premises are true. (It is conceivable that there may have been an intruder that the dog did not detect, but that does not invalidate the argument; the first premise is "if the watch-dog detects an intruder." The thing of importance is that the dog detects or doesn't detect an intruder, not if there is one.)
Another example:
Relation to "modus ponens".
Every use of "modus tollens" can be converted to a use of "modus ponens" and one use of transposition to the premise which is a material implication. For example:
Likewise, every use of "modus ponens" can be converted to a use of "modus tollens" and transposition.
Justification via truth table.
The validity of "modus tollens" can be clearly demonstrated through a truth table.
In instances of "modus tollens" we assume as premises that p → q is true and q is false. There is only one line of the truth table—the fourth line—which satisfies these two conditions. In this line, p is false. Therefore, in every instance in which p → q is true and q is false, p must also be false.

</doc>
<doc id="18902" url="https://en.wikipedia.org/wiki?curid=18902" title="Mathematician">
Mathematician

A mathematician is someone who uses an extensive knowledge of mathematics in their work, typically to solve mathematical problems. Mathematics is concerned with numbers, data, quantity, structure, space, models and change.
History.
One of the earliest known mathematicians was Thales of Miletus (c. 624–c.546 BC); he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.
The number of known mathematicians grew when Pythagoras of Samos (c. 582–c. 507 BC) established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number". It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins.
The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350 - 415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria punished her, presuming she was involved, by stripping her naked and scraping off her skin with clamshells (some say roofing tiles).
Science and mathematics in the Islamic world during the Middle Ages followed various models and modes of funding varied based primarily on scholars. It was extensive patronage and strong intellectual policies implemented by specific rulers that allowed scientific knowledge to develop in many areas. Funding for translation of scientific texts in other languages was ongoing throughout the reign of certain caliphs, and it turned out that certain scholars became experts in the works they translated and in turn received further support for continuing to develop certain sciences. As these sciences received wider attention from the elite, more scholars were invited and funded to study particular sciences. An example of a translator and mathematician who benefited from this type of support was al-Khawarizmi. A notable feature of many scholars working under Muslim rule in medieval times is that they were often polymaths. Examples include the work on optics, maths and astronomy of Ibn al-Haytham.
The Renaissance brought an increased emphasis on mathematics and science to Europe. During this period of transition from a mainly feudal and ecclesiastical culture to a predominantly secular one, many notable mathematicians had other occupations: Luca Pacioli (founder of accounting); Niccolò Fontana Tartaglia (notable engineer and bookkeeper); Gerolamo Cardano (earliest founder of probability and binomial expansion); Robert Recorde (physician) and François Viète (lawyer).
As time passed, many mathematicians gravitated towards universities. An emphasis on free thinking and experimentation had begun in Britain's oldest universities beginning in the seventeenth century at Oxford with the scientists Robert Hooke and Robert Boyle, and at Cambridge where Isaac Newton was Lucasian Professor of Mathematics & Physics. Moving into the 19th century, the objective of universities all across Europe evolved from teaching the “regurgitation of knowledge” to “encourag productive thinking.” In 1810, Humboldt convinced the King of Prussia to build a university in Berlin based on Friedrich Schleiermacher’s liberal ideas; the goal was to demonstrate the process of the discovery of knowledge and to teach students to “take account of fundamental laws of science in all their thinking.” Thus, seminars and laboratories started to evolve. 
British universities of this period adopted some approaches familiar to the Italian and German universities, but as they already enjoyed substantial freedoms and autonomy the changes there had begun with the Age of Enlightenment, the same influences that inspired Humboldt. The Universities of Oxford and Cambridge emphasized the importance of research, arguably more authentically implementing Humboldt’s idea of a university than even German universities, which were subject to state authority. Overall, science (including mathematics) became the focus of universities in the 19th and 20th centuries. Students could conduct research in seminars or laboratories and began to produce doctoral theses with more scientific content. According to Humboldt, the mission of the University of Berlin was to pursue scientific knowledge. The German university system fostered professional, bureaucratically regulated scientific research performed in well-equipped laboratories, instead of the kind of research done by private and individual scholars in Great Britain and France. In fact, Rüegg asserts that the German system is responsible for the development of the modern research university because it focused on the idea of “freedom of scientific research, teaching and study.”
Notable mathematicians.
A chronological list of some notable mathematicians.
Some other notable mathematicians include Brahmagupta, Johann Bernoulli, Jacob Bernoulli, Aryabhata, Bhāskara II, Hasan Ibn Haytham, Bonaventura Cavalieri, Alexander Grothendieck, Paul Erdős, Blaise Pascal, John von Neumann, Alan Turing, Tullio Levi-Civita Kurt Gödel, Augustin-Louis Cauchy, Georg Cantor, William Rowan Hamilton, Carl Jacobi, Nikolai Lobachevsky, Andrey Kolmogorov, Joseph Fourier, Giuseppe Peano and Pierre-Simon Laplace.
Required education.
Mathematicians usually cover a breadth of topics within mathematics in their undergraduate education, and then proceed to specialize in topics of their own choice at the graduate-level. In some universities, a qualifying exam serves to test both the breadth and depth of a student's understanding of mathematics; the students, who pass, are permitted to work on a doctoral dissertation.
Activities.
Applied mathematics.
Mathematicians involved with solving problems with applications in real life are called applied mathematicians. Applied mathematicians are mathematical scientists who, with their specialized knowledge and professional methodology, approach many of the imposing problems presented in related scientific fields. With professional focus on a wide variety of problems, theoretical systems, and localized constructs, applied mathematicians work regularly in the study and formulation of mathematical models. Mathematicians and applied mathematicians are considered to be two of the STEM (science, technology, engineering, and mathematics) careers.
The discipline of applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry; thus, "applied mathematics" is a mathematical science with specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on problems, often concrete but sometimes abstract. As professionals focused on problem solving, "applied mathematicians" look into the "formulation, study, and use of mathematical models" in science, engineering, business, and other areas of mathematical practice.
Abstract mathematics.
Pure mathematics is mathematics that studies entirely abstract concepts. From the eighteenth century onwards, this was a recognized category of mathematical activity, sometimes characterized as "speculative mathematics", and at variance with the trend towards meeting the needs of navigation, astronomy, physics, economics, engineering, and other applications.
Another insightful view put forth is that "pure mathematics is not necessarily applied mathematics": it is possible to study abstract entities with respect to their intrinsic nature, and not be concerned with how they manifest in the real world. Even though the pure and applied viewpoints are distinct philosophical positions, in practice there is much overlap in the activity of pure and applied mathematicians.
To develop accurate models for describing the real world, many applied mathematicians draw on tools and techniques that are often considered to be "pure" mathematics. On the other hand, many pure mathematicians draw on natural and social phenomena as inspiration for their abstract research.
Mathematics teaching.
Many professional mathematicians also engage in the teaching of mathematics. Duties may include:
Consulting.
Many careers in mathematics outside of universities involve consulting. For instance, actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner which will help ensure that the plans are maintained on a sound financial basis.
As another example, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock ("see: Valuation of options; Financial modeling").
Occupations.
According to the Dictionary of Occupational Titles occupations in mathematics include the following.
Quotations about mathematicians.
The following are quotations about mathematicians, or by mathematicians.
Women in mathematics.
While the majority of mathematicians are male, there have been some demographic changes since World War II. For example, in Europe, from 1992 onwards, several women have been laureates of the prestigious EMS Prize. Some prominent female mathematicians throughout History are Hypatia of Alexandria (ca. 400 AD), Ada Lovelace (1815–1852), Maria Gaetana Agnesi (1718–1799), Emmy Noether (1882–1935), Sophie Germain (1776–1831), Sofia Kovalevskaya (1850–1891), Alicia Boole Stott (1860–1940), Rózsa Péter (1905–1977), Julia Robinson (1919–1985), Olga Taussky-Todd (1906–1995), Émilie du Châtelet (1706–1749), Mary Cartwright (1900–1998), Olga Ladyzhenskaya (1922–2004), Olga Oleinik (1925–2001) and Maryam Mirzakhani.
The Association for Women in Mathematics is a professional society whose purpose is "to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity and the equal treatment of women and girls in the mathematical sciences."
The American Mathematical Society and other mathematical societies offer several prizes aimed at increasing the representation of women and minorities in the future of mathematics.
Prizes in mathematics.
There is no Nobel Prize in mathematics, though sometimes mathematicians have won the Nobel Prize in a different field, such as economics. Prominent prizes in mathematics include the Abel Prize, the Chern Medal, the Fields Medal, the Gauss Prize, the Nemmers Prize, the Balzan Prize, the Crafoord Prize, the Shaw Prize, the Steele Prize, the Wolf Prize, the Schock Prize, and the Nevanlinna Prize.
Mathematical autobiographies.
Several well known mathematicians have written autobiographies in part to explain to a general audience what it is about mathematics that has made them want to devote their lives to its study. These provide some of the best glimpses into what it means to be a mathematician. The following list contains some works that are not autobiographies, but rather essays on mathematics and mathematicians with strong autobiographical elements.

</doc>
<doc id="18905" url="https://en.wikipedia.org/wiki?curid=18905" title="Armed forces">
Armed forces

The Armed forces of a country are its government-sponsored defense, fighting forces, and organizations. They exist to further the foreign and domestic policies of their governing body and to defend that body and the nation it represents from external and internal aggressors. In broad usage, the terms "armed forces" and "military" are often treated synonymously, although in technical usage a distinction is sometimes made in which a country's armed forces may include both its military and other paramilitary forces. Armed force is the use of armed forces to achieve political objectives.
The study of the use of armed forces is called military science. Broadly speaking, this involves considering offense and defense at three "levels": strategy, operational art, and tactics. All three levels study the application of the use of force in order to achieve a desired objective.
Organization.
In most countries the basis of the armed forces is the military, divided into basic military branches. However, armed forces can include other paramilitary structures.
Benefits and costs.
The obvious benefit to a country in maintaining armed forces is in providing protection from foreign threats and from internal conflict. In recent decades armed forces personnel have also been used as emergency civil support roles in post-disaster situations. On the other hand, they may also harm a society by engaging in counter-productive (or merely unsuccessful) warfare. Expenditure on science and technology to develop weapons and systems sometimes produces side benefits, although some claim that greater benefits could come from targeting the money directly.

</doc>
<doc id="18906" url="https://en.wikipedia.org/wiki?curid=18906" title="Microfluidics">
Microfluidics

Microfluidics is a multidisciplinary field intersecting engineering, physics, chemistry, biochemistry, nanotechnology, and biotechnology, with practical applications to the design of systems in which low volumes of fluids are processed to achieve multiplexing, automation, and high-throughput screening. Microfluidics emerged in the beginning of the 1980s and is used in the development of inkjet printheads, DNA chips, lab-on-a-chip technology, micro-propulsion, and micro-thermal technologies. 
It deals with the behavior, precise control and manipulation of fluids that are geometrically constrained to a small, typically sub-millimeter, scale.
Typically, micro means one of the following features:
Typically fluids are moved, mixed, separated or otherwise processed. Numerous applications employ passive fluid control techniques like capillary forces. In some applications external actuation means are additionally used for a directed transport of the media. Examples are rotary drives applying centrifugal forces for the fluid transport on the passive chips. Active microfluidics refers to the defined manipulation of the working fluid by active (micro) components such as micropumps or micro valves. Micro pumps supply fluids in a continuous manner or are used for dosing. Micro valves determine the flow direction or the mode of movement of pumped liquids. Often processes which are normally carried out in a lab are miniaturized on a single chip in order to enhance efficiency and mobility as well as reducing sample and reagent volumes.
Microscale behavior of fluids.
The behavior of fluids at the microscale can differ from 'macrofluidic' behavior in that factors such as surface tension, energy dissipation, and fluidic resistance start to dominate the system. Microfluidics studies how these behaviors change, and how they can be worked around, or exploited for new uses.
At small scales (channel diameters of around 100 nanometers to several hundred micrometers) some interesting and sometimes unintuitive properties appear. In particular, the Reynolds number (which compares the effect of momentum of a fluid to the effect of viscosity) can become very low. A key consequence of this is that fluids, when side-by-side, do not necessarily mix in the traditional sense, as flow becomes laminar rather than turbulent; molecular transport between them must often be through diffusion.
High specificity of chemical and physical properties (concentration, pH, temperature, shear force, etc.) can also be ensured resulting in more uniform reaction conditions and higher grade products in single and multi-step reactions.
Key application areas.
Microfluidic structures include micropneumatic systems, i.e. microsystems for the handling of off-chip fluids (liquid pumps, gas valves, etc.), and microfluidic structures for the on-chip handling of nano- and picolitre volumes. To date, the most successful commercial application of microfluidics is the inkjet printhead. Significant research has also been applied to microfluidic synthesis and production of various biofunctionalized nanoparticles including quantum dots (QDs) and metallic nanoparticles, and other industrially relevant materials (e.g., polymer particles). Additionally, advances in microfluidic manufacturing allow the devices to be produced in low-cost plastics and part quality may be verified automatically.
Advances in microfluidics technology are revolutionizing molecular biology procedures
for enzymatic analysis (e.g., glucose and lactate assays), DNA analysis
(e.g., polymerase chain reaction and high-throughput sequencing), and proteomics.
The basic idea of microfluidic biochips is to integrate assay operations such as detection,
as well as sample pre-treatment and sample preparation on one chip.
An emerging application area for biochips is clinical pathology,
especially the immediate point-of-care diagnosis of diseases.
In addition, microfluidics-based devices, capable of continuous sampling and real-time
testing of air/water samples for biochemical toxins and other dangerous
pathogens, can serve as an always-on "bio-smoke alarm" for early warning.
Continuous-flow microfluidics.
These technologies are based on the manipulation of continuous
liquid flow through microfabricated channels.
Actuation of liquid flow is implemented either by external pressure sources, external mechanical pumps,
integrated mechanical micropumps, or by combinations of capillary forces and electrokinetic mechanisms. Continuous-flow microfluidic operation is the mainstream approach because it is easy to implement and less sensitive to protein fouling problems. Continuous-flow devices
are adequate for many well-defined and simple biochemical applications, and for certain tasks such
as chemical separation, but they are less suitable for tasks requiring a high
degree of flexibility or ineffect fluid manipulations. These closed-channel
systems are inherently difficult to integrate and scale because the parameters
that govern flow field vary along the flow path making the fluid flow at any
one location dependent on the properties of the entire system. Permanently etched microstructures also lead to limited reconfigurability and poor fault tolerance capability.
Process monitoring capabilities in continuous-flow systems can be achieved with highly sensitive microfluidic flow sensors based on MEMS technology which offer resolutions down to the nanoliter range.
Droplet-based microfluidics.
Droplet-based microfluidics as a subcategory of microfluidics in contrast with continuous microfluidics has the distinction of manipulating discrete volumes of fluids in immiscible phases with low Reynolds number and laminar flow regimes. Interest in droplet-based microfluidics systems has been growing substantially in past decades. Microdroplets offer the feasibility of handling miniature volumes of fluids conveniently, provide better mixing and are suitable for high throughput experiments. Exploiting the benefits of droplet based microfluidics efficiently requires a deep understanding of droplet generation, droplet motion, droplet merging, and droplet breakup
One of the key advantages of droplet-based microfluidics is the ability to use droplets as incubators for single cells.
Devices capable of generating thousands of droplets per second opens new ways characterize cell population, not only based on a specific marker measured at a specific time point, but also based on cells kinetic behavior such as protein secretion, enzyme activity or proliferation. 
Recently, a method was found to generate a stationary array of microscopic droplets for single-cell incubation that does not require the use of a surfactant 
Digital microfluidics.
Alternatives to the above closed-channel continuous-flow systems include novel open structures, where discrete, independently controllable droplets
are manipulated on a substrate using electrowetting. Following the analogy of digital microelectronics, this approach is referred to as digital microfluidics. Le Pesant et al. pioneered the use of electrocapillary forces to move droplets on a digital track. The "fluid transistor" pioneered by Cytonix also played a role. The technology was subsequently commercialized by Duke University. By using discrete unit-volume droplets, a microfluidic function can be reduced to a set of repeated basic operations, i.e., moving one unit of fluid over one unit of
distance. This "digitization" method facilitates the use of a hierarchical
and cell-based approach for microfluidic biochip design. Therefore, digital
microfluidics offers a flexible and scalable system architecture as well as
high fault-tolerance capability. Moreover, because each droplet can be
controlled independently, these systems also have dynamic reconfigurability,
whereby groups of unit cells in a microfluidic array can be reconfigured to
change their functionality during the concurrent execution of a set of
bioassays. Although droplets are manipulated in confined microfluidic channels, since the control on droplets is not independent, it should not be confused as "digital microfluidics". One common actuation method for digital microfluidics is electrowetting-on-dielectric (EWOD). Many lab-on-a-chip applications have been demonstrated within the digital microfluidics paradigm using electrowetting. However, recently other techniques for droplet manipulation have also been demonstrated using surface acoustic waves, optoelectrowetting, mechanical actuation, etc.
DNA chips (microarrays).
Early biochips were based on the idea of a DNA microarray,
e.g., the GeneChip DNAarray from Affymetrix, which is a piece of glass,
plastic or silicon substrate on which pieces of DNA (probes) are affixed in a microscopic
array. Similar to a DNA microarray, a protein array is a miniature array
where a multitude of different capture agents, most frequently monoclonal
antibodies, are deposited on a chip surface; they are used to determine the
presence and/or amount of proteins in biological samples, e.g., blood. A
drawback of DNA and protein arrays is that they are neither
reconfigurable nor scalable after manufacture. Digital microfluidics has been described as a means for carrying out Digital PCR.
Molecular biology.
In addition to microarrays, biochips have been designed for two-dimensional electrophoresis, transcriptome analysis, and PCR amplification. Other applications include various electrophoresis and liquid chromatography applications for proteins and DNA, cell separation, in particular blood cell separation, protein analysis, cell manipulation and analysis including cell viability analysis and microorganism capturing.
Evolutionary biology.
By combining microfluidics with landscape ecology and nanofluidics, a nano/micro fabricated fluidic landscape can be constructed by building local patches of bacterial habitat and connecting them by dispersal corridors. The resulting landscapes can be used as physical implementations of an adaptive landscape, by generating a spatial mosaic of patches of opportunity distributed in space and time. The patchy nature of these fluidic landscapes allows for the study of adapting bacterial cells in a metapopulation system. The evolutionary ecology of these bacterial systems in these synthetic ecosystems allows for using biophysics to address questions in evolutionary biology.
Cell behavior.
The ability to create precise and carefully controlled chemoattractant gradients makes microfluidics the ideal tool to study motility, chemotaxis and the ability to evolve / develop resistance to antibiotics in small populations of microorganisms and in a short period of time. These microorganisms including bacteria and the broad range of organisms that form the marine microbial loop, responsible for regulating much of the oceans' biogeochemistry.
Microfluidics has also greatly aided the study of durotaxis by facilitating the creation of durotactic (stiffness) gradients.
Cellular biophysics.
By rectifying the motion of individual swimming bacteria, microfluidic structures can be used to extract mechanical motion from a population of motile bacterial cells. This way, bacteria-powered rotors can be built.
Optics.
The merger of microfluidics and optics is typical known as optofluidics. Examples of optofluidic devices are tuneable microlens arrays and optofluidic microscopes.
Microfluidic flow enables fast sample throughput, automated imaging of large sample populations, as well as 3D capabilities. or superresolution.
Acoustic droplet ejection (ADE).
Acoustic droplet ejection uses a pulse of ultrasound to move low volumes of fluids (typically nanoliters or picoliters) without any physical contact. This technology focuses acoustic energy into a fluid sample in order to eject droplets as small as a millionth of a millionth of a liter (picoliter = 10−12 liter). ADE technology is a very gentle process, and it can be used to transfer proteins, high molecular weight DNA and live cells without damage or loss of viability. This feature makes the technology suitable for a wide variety of applications including proteomics and cell-based assays.
Fuel cells.
Microfluidic fuel cells can use laminar flow to separate the fuel and its oxidant to control the interaction of the two fluids without a physical barrier as would be required in conventional fuel cells.
A tool for cell biological research.
Microfluidic technology has led to the creation of powerful tools for cell biologists to control the complete cellular environment, leading to new questions and new discoveries. Many diverse advantages of this technology for microbiology are listed below:

</doc>
<doc id="18908" url="https://en.wikipedia.org/wiki?curid=18908" title="Mersenne prime">
Mersenne prime

In mathematics, a Mersenne prime is a prime number that is one less than a power of two. That is, it is a prime number that can be written in the form for some integer . They are named after Marin Mersenne, a French Minim friar, who studied them in the early 17th century. The first four Mersenne primes are 3, 7, 31, and 127.
If is a composite number then so is . ( is divisible by both and .) The definition is therefore unchanged when written where is assumed prime.
More generally, numbers of the form without the primality requirement are called Mersenne numbers. Mersenne numbers are sometimes defined to have the additional requirement that be prime, equivalently that they be pernicious Mersenne numbers, namely those pernicious numbers whose binary representation contains no zeros. The smallest composite pernicious Mersenne number is .
Mersenne primes are also noteworthy due to their connection to perfect numbers.
, 49 Mersenne primes are known. The largest known prime number is a Mersenne prime.
Since 1997, all newly found Mersenne primes have been discovered by the “Great Internet Mersenne Prime Search” (GIMPS), a distributed computing project on the Internet.
About Mersenne primes.
Many fundamental questions about Mersenne primes remain unresolved. It is not even known whether the set of Mersenne primes is finite or infinite. The Lenstra–Pomerance–Wagstaff conjecture asserts that there are infinitely many Mersenne primes and predicts their order of growth. It is also not known whether infinitely many Mersenne numbers with prime exponents are composite, although this would follow from widely believed conjectures about prime numbers, for example, the infinitude of Sophie Germain primes congruent to 3 (mod 4), for these primes , (which is also prime) will divide , e.g., , , , , , , , and . . Since for these primes , is congruent to 7 mod 8, so 2 is a quadratic residue mod , and the multiplicative order of 2 mod must divide formula_1 = . Since is a prime, it must be or 1. However, it cannot be 1 since formula_2 and 1 has no prime factors, so it must be . Hence, divides formula_3 and = cannot be prime.
The first four Mersenne primes are
A basic theorem about Mersenne numbers states that if is prime, then the exponent must also be prime. This follows from the identity
This rules out primality for Mersenne numbers with composite exponent, such as .
Though the above examples might suggest that is prime for all primes , this is not the case, and the smallest counterexample is the Mersenne number
The evidence at hand does suggest that a randomly selected Mersenne number is much more likely to be prime than an arbitrary randomly selected integer of similar size. Nonetheless, prime appear to grow increasingly sparse as increases. In fact, of the 2,007,537 prime numbers up to 32,582,657, is prime for only 44 of them.
The lack of any simple test to determine whether a given Mersenne number is prime makes the search for Mersenne primes a difficult task, since Mersenne numbers grow very rapidly. The Lucas–Lehmer primality test (LLT) is an efficient primality test that greatly aids this task. The search for the largest known prime has somewhat of a cult following. Consequently, a lot of computer power has been expended searching for new Mersenne primes, much of which is now done using distributed computing.
Mersenne primes are used in pseudorandom number generators such as the Mersenne twister, Park–Miller random number generator, Generalized Shift Register and Fibonacci RNG.
Perfect numbers.
Mersenne primes are also noteworthy due to their connection to perfect numbers. In the 4th century BC, Euclid proved that if is prime, then ) is a perfect number. This number, also expressible as , is the th triangular number and the th hexagonal number. In the 18th century, Leonhard Euler proved that, conversely, all even perfect numbers have this form. This is known as the Euclid–Euler theorem. It is unknown whether there are any odd perfect numbers.
History.
Mersenne primes take their name from the 17th-century French scholar Marin Mersenne, who compiled what was supposed to be a list of Mersenne primes with exponents up to 257, as follows:
His list was completely accurate until 31, but then becomes largely incorrect, as Mersenne mistakenly included and (which are composite), and omitted , , and (which are prime). Mersenne gave little indication how he came up with his list. 
Édouard Lucas proved in 1876 that is indeed prime, as Mersenne claimed. This was the largest known prime number for 75 years, and the largest ever found by hand. was determined to be prime in 1883 by Ivan Mikheevich Pervushin, though Mersenne claimed it was composite, and for this reason it is sometimes called Pervushin's number. This was the second-largest known prime number, and it remained so until 1911. Lucas had shown another error in Mersenne's list in 1876. Without finding a factor, Lucas demonstrated that is actually composite. No factor was found until a famous talk by Frank Nelson Cole in 1903. Without speaking a word, he went to a blackboard and raised 2 to the 67th power, then subtracted one. On the other side of the board, he multiplied and got the same number, then returned to his seat (to applause) without speaking. He later said that the result had taken him "three years of Sundays" to find. A correct list of all Mersenne primes in this number range was completed and rigorously verified only about three centuries after Mersenne published his list.
Searching for Mersenne primes.
Fast algorithms for finding Mersenne primes are available, and the eleven largest known prime numbers are Mersenne primes.
The first four Mersenne primes , , and were known in antiquity. The fifth, , was discovered anonymously before 1461; the next two ( and ) were found by Pietro Cataldi in 1588. After nearly two centuries, was verified to be prime by Leonhard Euler in 1772. The next (in historical, not numerical order) was , found by Édouard Lucas in 1876, then by Ivan Mikheevich Pervushin in 1883. Two more ( and ) were found early in the 20th century, by R. E. Powers in 1911 and 1914, respectively.
The best method presently known for testing the primality of Mersenne numbers is the Lucas–Lehmer primality test. Specifically, it can be shown that for prime , is prime if and only if divides , where and for .
During the era of manual calculation, all the exponents up to and including 257 were tested with the Lucas–Lehmer test and found to be composite. A notable contribution was made by retired Yale physics professor Horace Scudder Uhler, who did the calculations for exponents 157, 167, 193, 199, 227, and 229. Unfortunately for those investigators, the interval they were testing contains the largest known gap between Mersenne primes, in relative terms: the next prime exponent would turn out to be more than four times larger than the previous record of 127.
The search for Mersenne primes was revolutionized by the introduction of the electronic digital computer. Alan Turing searched for them on the Manchester Mark 1 in 1949, but the first successful identification of a Mersenne prime, , by this means was achieved at 10:00 pm on January 30, 1952 using the U.S. National Bureau of Standards Western Automatic Computer (SWAC) at the Institute for Numerical Analysis at the University of California, Los Angeles, under the direction of Lehmer, with a computer search program written and run by Prof. R. M. Robinson. It was the first Mersenne prime to be identified in thirty-eight years; the next one, , was found by the computer a little less than two hours later. Three more — , ,  — were found by the same program in the next several months. is the first Mersenne prime that is titanic, is the first gigantic, and was the first megaprime to be discovered, being a prime with at least 1,000,000 digits. All three were the first known prime of any kind of that size.
In September 2008, mathematicians at UCLA participating in GIMPS won part of a $100,000 prize from the Electronic Frontier Foundation for their discovery of a very nearly 13-million-digit Mersenne prime. The prize, finally confirmed in October 2009, is for the first known prime with at least 10 million digits. The prime was found on a Dell OptiPlex 745 on August 23, 2008. This is the eighth Mersenne prime discovered at UCLA.
On April 12, 2009, a GIMPS server log reported that a 47th Mersenne prime had possibly been found. This report was apparently overlooked until June 4, 2009. The find was verified on June 12, 2009. The prime is . Although it is chronologically the 47th Mersenne prime to be discovered, it is smaller than the largest known at the time, which was the 45th to be discovered.
On January 25, 2013, Curtis Cooper, a mathematician at the University of Central Missouri, discovered a 48th Mersenne prime, (a number with 17,425,170 digits), as a result of a search executed by a GIMPS server network. This was the third Mersenne prime discovered by Dr. Cooper and his team in the past seven years.
On January 19, 2016, Cooper also published about his discovery of a 49th Mersenne prime, (a number with 22,338,618 digits), as a result of a search executed by a GIMPS server network. This was the fourth Mersenne prime discovered by Dr. Cooper and his team in the past ten years.
List of known Mersenne primes.
The table below lists all known Mersenne primes (sequence () and () in OEIS):
All Mersenne numbers below the 48th Mersenne prime () have been tested at least once but some have not been double-checked. Primes are not always discovered in increasing order. For example, the 29th Mersenne prime was discovered "after" the 30th and the 31st. Similarly, was followed by two smaller Mersenne primes, first 2 weeks later and then 8 months later.
The largest known Mersenne prime is also the largest known prime number. To help visualize its size, displaying the number in base 10 would require 5,957 pages with 75 digits per line and 50 lines per page. was the first discovered prime number with more than 10 million decimal digits.
In modern times, the largest known prime has almost always been a Mersenne prime.
Factorization of composite Mersenne numbers.
The factors of a prime number are by definition one, and the number itself - this section is about composite numbers. Mersenne numbers are very good test cases for the special number field sieve algorithm, so often the largest number factorized with this algorithm has been a Mersenne number. , 2 − 1 is the record-holder, using a variant on the special number field sieve allowing the factorisation of several numbers at once. See integer factorization records for links to more information. The special number field sieve can factorize numbers with more than one large factor. If a number has only one very large factor then other algorithms can factorize larger numbers by first finding small factors and then making a primality test on the cofactor. , the largest factorization with probable prime factors allowed is , where is a 1,042,896-digit probable prime.
Mersenne primitive part.
The primitive part of Mersenne number is , the th cyclotomic polynomial at 2, they are
Besides, if we notice those prime factors, and delete "old prime factors", for example, 3 divides the 2nd, 6th, 18th, 54th, 162nd, ... terms of this sequence, we only allow the 2nd term divided by 3, if we do, they are
The numbers which is prime are
The numbers which has an only primitive prime factor are
Mersenne numbers in nature and elsewhere.
In computer science, unsigned -bit integers can be used to express numbers up to . Signed -bit integers can express values between and , using the two's complement representation.
In the mathematical problem Tower of Hanoi, solving a puzzle with an -disc tower requires steps, assuming no mistakes are made.
The asteroid with minor planet number 8191 is named 8191 Mersenne after Marin Mersenne, because 8191 is a Mersenne prime (3 Juno, 7 Iris, 31 Euphrosyne and 127 Johanna having been discovered and named during the 19th century).
Mersenne–Fermat primes.
A Mersenne–Fermat number is defined as , with prime, natural number, and can be written as , when , it is a Mersenne number, and when , it is a Fermat number, the only known Mersenne–Fermat prime with are
In fact, , where is the cyclotomic polynomial.
Generalizations.
The simplest generalized Mersenne primes are prime numbers of the form , where is a low-degree polynomial with small integer coefficients. An example is , in this case, , and ; another example is , in this case, , and .
It is also natural to try to generalize primes of the form to primes of the form (for and ). However (see also theorems above), is always divisible by , so unless the latter is a unit, the former is not a prime. There are two ways to deal with that:
Complex numbers.
In the ring of integers (on real numbers), if is a unit, then is either 2 or 0. But are the usual Mersenne primes, and the formula does not lead to anything interesting (since it is always −1 for all ). Thus, we can regard a ring of "integers" on complex numbers instead of real numbers, like Gaussian integers and Eisenstein integers.
Gaussian Mersenne primes.
If we regard the ring of Gaussian integers, we get the case and , and can ask (WLOG) for what the number is a "Gaussian prime" which will then be called a Gaussian Mersenne prime.
This sequence is in many ways similar to the list of exponents of ordinary Mersenne primes.
The norms (i.e. squares of absolute values) of these Gaussian primes are rational primes:
Eisenstein Mersenne primes.
We can also regard the ring of Eisenstein integers, we get the case and , and can ask for what the number is an "Eisenstein prime" which will then be called a Eisenstein Mersenne prime.
The norms (i.e. squares of absolute values) of these Eisenstein primes are rational primes:
Divide an integer.
Repunit primes.
The other way to deal with the fact that is always divisible by , it is to simply take out this factor and ask which values of make
be prime. (The integer can be either positive or negative.) If for example we take , we get values of:
These primes are called repunit primes. Another example is when we take , we get values of:
It is a conjecture that for every integer which is not a perfect power, there are infinitely many values of such that is prime. (When is a perfect power, it can be shown that there is at most one value such that is prime)
Least such that is prime are (starting with )
For negative bases , they are (starting with )
Least base such that is prime are
For negative bases , they are
Other generalized Mersenne primes.
Another generalized Mersenne number is
with , any coprime integers, and . (Since is always divisible by , the division is necessary for there to be any chance of finding prime numbers. In fact, this number is the same as the Lucas number , since and are the roots of the quadratic equation , and this number equals 1 when ) We can ask which make this number prime. It can be shown that such must be primes themselves or equal to 4, and can be 4 if and only if and is prime. (Since . Thus, in this case the pair must be and must be prime. That is, must be in .) It is a conjecture that for any pair such that for every natural number , and are not both perfect th powers, and is not a perfect fourth power. there are infinitely many values of such that is prime. (When and are both perfect th powers for an or when is a perfect fourth power, it can be shown that there are at most two values with this property, since if so, then can be factored algebraically) However, this has not been proved for any single value of .
A conjecture related to the generalized Mersenne primes: (the conjecture predicts where is the next generalized Mersenne prime, if the conjecture is true, then there are infinitely many primes for all such pairs)
For any integers and which satisfy the conditions:
has prime numbers of the form
for prime , the prime numbers will be distributed near the best fit line
where
and there are about
prime numbers of this form less than .
We also have the following three properties:
If this conjecture is true, then for all such pairs, let be the th prime of the form , the graph of versus is almost linear. (See )
When , it is , a difference of two perfect th powers, and if is prime, then must be , because it is divisible by .
Least such that is prime are
Least such that is prime are

</doc>
<doc id="18909" url="https://en.wikipedia.org/wiki?curid=18909" title="Magnesium">
Magnesium

Magnesium is a chemical element with symbol Mg and atomic number 12. It is a shiny gray solid which bears a close physical resemblance to the other five elements in the second column (Group 2, or alkaline earth metals) of the periodic table: all Group 2 elements have the same electron configuration in the outer electron shell and a similar crystal structure.
Magnesium is the ninth most abundant element in the universe. It is produced in large, aging stars from the sequential addition of three helium nuclei to a carbon nucleus. When such stars explode as supernova, much of the magnesium is expelled into the interstellar medium where it may recycle into new star systems. Magnesium is the eighth most abundant element in the Earth's crust and the fourth most common element in the Earth (after iron, oxygen and silicon), making up 13% of the planet's mass and a large fraction of the planet's mantle. It is the third most abundant element dissolved in seawater, after sodium and chlorine.
Magnesium occurs naturally only in combination with other elements, where it invariably has a +2 oxidation state. The free element (metal) can be produced artificially, and is highly reactive (though in the atmosphere, it is soon coated in a thin layer of oxide that partly inhibits reactivity — see passivation). The free metal burns with a characteristic brilliant-white light. The metal is now obtained mainly by electrolysis of magnesium salts obtained from brine, and is used primarily as a component in aluminium-magnesium alloys, sometimes called "magnalium" or "magnelium". Magnesium is less dense than aluminium, and the alloy is prized for its combination of lightness and strength.
Magnesium is the eleventh most abundant element by mass in the human body, and the ions are essential to all cells. Magnesium ions interact with polyphosphate compounds such as ATP, DNA, and RNA. Hundreds of enzymes require magnesium ions to function. Magnesium compounds are used medicinally as common laxatives, antacids (e.g., milk of magnesia), and to stabilize abnormal nerve excitation or blood vessel spasm in such conditions as eclampsia. Magnesium ions are sour to the taste, and in low concentrations they help impart a natural tartness to fresh mineral waters. Magnesium is the metallic ion at the center of chlorophyll, and is a common component in fertilizers.
Characteristics.
Physical properties.
Elemental magnesium is a gray-white lightweight metal, two-thirds the density of aluminium. It tarnishes slightly when exposed to air, although, unlike the other alkaline earth metals, an oxygen-free environment is unnecessary for storage because magnesium is protected by a thin layer of oxide that is fairly impermeable and difficult to remove. Magnesium has the lowest melting () and the lowest boiling point of any of the alkali earth metals.
Magnesium reacts with water at room temperature, though it reacts much more slowly than calcium, a similar Group 2 metal. When submerged in water, hydrogen bubbles form slowly on the surface of the metal—though, if powdered, it reacts much more rapidly. The reaction occurs faster with higher temperatures (see Precautions). Magnesium's reversible reaction with water can be harnessed to store energy and run a magnesium-based engine.
Magnesium also reacts exothermically with most acids such as hydrochloric acid (HCl), producing the metal chloride and hydrogen gas, similar to the HCl reaction with aluminium, zinc, and many other metals.
Chemical properties.
Flammability.
Magnesium is highly flammable, especially when powdered or shaved into thin strips, though it is difficult to ignite in mass or bulk. Flame temperatures of magnesium and magnesium alloys can reach , although flame height above the burning metal is usually less than . Once ignited, such fires are difficult to extinguish, with combustion continuing in nitrogen (forming magnesium nitride), carbon dioxide (forming magnesium oxide and carbon), and water (forming magnesium oxide and hydrogen). This property was used in incendiary weapons during the firebombing of cities in World War II, where the only practical civil defense was to smother a burning flare under dry sand to exclude atmosphere from the combustion. 
Magnesium may also be used as an igniter for thermite, a mixture of aluminium and iron oxide powder that ignites only at a very high temperature.
Source of light.
When burning in air, magnesium produces a brilliant-white light that includes strong ultraviolet wavelengths. Magnesium powder (flash powder) was used for subject illumination in the early days of photography. Later, magnesium filament was used in electrically ignited single-use photography flashbulbs. Magnesium powder is used in fireworks and marine flares where a brilliant white light is required. It was also used for various thatrical effects, such as lightening, pistol flashes, and supernatural appearances.
Occurrence.
Magnesium is the eighth-most-abundant element in the Earth's crust by mass and tied in seventh place with iron in molarity. It is found in large deposits of magnesite, dolomite, and other minerals, and in mineral waters, where magnesium ion is soluble.
Although magnesium is found in more than 60 minerals, only dolomite, magnesite, brucite, carnallite, talc, and olivine are of commercial importance.
The cation is the second-most-abundant cation in seawater (about ⅛ the mass of sodium ions in a given sample), which makes seawater and sea salt attractive commercial sources for Mg. To extract the magnesium, calcium hydroxide is added to seawater to form magnesium hydroxide precipitate.
Magnesium hydroxide (brucite) is insoluble in water and can be filtered out and reacted with hydrochloric acid to produced concentrated magnesium chloride.
From magnesium chloride, electrolysis produces magnesium.
Forms.
Alloy.
As of 2013, magnesium alloy consumption was less than one million tons per year, compared with 50 million tons of aluminum alloys. Its use has been historically limited by its tendency to corrode, creep at high temperatures, and combust.
Corrosion.
The presence of iron, nickel, copper, and cobalt strongly activates corrosion. Greater than a very small percentage, these metals precipitate as intermetallic compounds, and the precipitate locales function as active cathodic sites that reduce water, causing the loss of magnesium. Controlling the quantity of these metals improves corrosion resistance. Sufficient manganese overcomes the corrosive effects of iron. This requires precise control over composition, increasing costs. Adding a cathodic poison captures atomic hydrogen within the structure of a metal. This prevents the formation of free hydrogen gas, an essential factor of corrosive chemical processes. The addition of about one in three hundred parts arsenic reduces its corrosion rate in a salt solution by a factor of nearly ten.
High-temperature creep and flammability.
Research showed that magnesium's tendency to creep at high-temperatures is eliminated by the adding scandium and gadolinium. Flammability is greatly reduced by a small amount of calcium in the alloy.
Compounds.
Magnesium forms a variety of compounds important to industry and biology, including magnesium carbonate, magnesium chloride, magnesium citrate, magnesium hydroxide (milk of magnesia), magnesium oxide, magnesium sulfate, and magnesium sulfate heptahydrate (Epsom salts).
Isotopes.
Magnesium has three stable isotopes: , and . All are present in significant amounts (see table of isotopes above). About 79% of Mg is . The isotope is radioactive and in the 1950s to 1970s was produced by several nuclear power plants for use in scientific experiments. This isotope has a relatively short half-life (21 hours) and its use was limited by shipping times.
The isomer has found application in isotopic geology, similar to that of aluminium. is a radiogenic daughter product of , which has a half-life of 717,000 years. Excessive quantities of stable have been observed in the Ca-Al-rich inclusions of some carbonaceous chondrite meteorites. This anomalous abundance is attributed to the decay of its parent in the inclusions, and researchers conclude that such meteorites were formed in the solar nebula before the had decayed. These are among the oldest objects in the solar system and contain preserved information about its early history.
It is conventional to plot / against an Al/Mg ratio. In an isochron dating plot, the Al/Mg ratio plotted is/. The slope of the isochron has no age significance, but indicates the initial / ratio in the sample at the time when the systems were separated from a common reservoir.
Production.
China is the dominant supplier of magnesium, with approximately 80% of the world market share. China is almost completely reliant on the silicothermic Pidgeon process (the reduction of the oxide at high temperatures with silicon, often provided by a ferrosilicon alloy in which the iron is but a spectator in the reactions) to obtain the metal. The process can also be carried out with carbon at approx 2300 °C:
In the United States, magnesium is obtained principally with the Dow process, by electrolysis of fused magnesium chloride from brine and sea water. A saline solution containing ions is first treated with lime (calcium oxide) and the precipitated magnesium hydroxide is collected:
The hydroxide is then converted to a partial hydrate of magnesium chloride by treating the hydroxide with hydrochloric acid and heating of the product:
The salt is then electrolyzed in the molten state. At the cathode, the ion is reduced by two electrons to magnesium metal:
At the anode, each pair of ions is oxidized to chlorine gas, releasing two electrons to complete the circuit:
A new process, solid oxide membrane technology, involves the electrolytic reduction of MgO. At the cathode, ion is reduced by two electrons to magnesium metal. The electrolyte is Yttria-stabilized zirconia (YSZ). The anode is a liquid metal. At the YSZ/liquid metal anode is oxidized. A layer of graphite borders the liquid metal anode, and at this interface carbon and oxygen react to form carbon monoxide. When silver is used as the liquid metal anode, there is no reductant carbon or hydrogen needed, and only oxygen gas is evolved at the anode. It has been reported that this method provides a 40% reduction in cost per pound over the electrolytic reduction method. This method is more environmentally sound than others because there is much less carbon dioxide emitted.
The United States has traditionally been the major world supplier of this metal, supplying 45% of world production even as recently as 1995. Today, the US market share is at 7%, with a single domestic producer left, US Magnesium, a Renco Group company in Utah born from now-defunct Magcorp.
History.
The name magnesium originates from the Greek word for a district in Thessaly called Magnesia. It is related to magnetite and manganese, which also originated from this area, and required differentiation as separate substances. See manganese for this history.
In 1618, a farmer at Epsom in England attempted to give his cows water from a well there. The cows refused to drink because of the water's bitter taste, but the farmer noticed that the water seemed to heal scratches and rashes. The substance became known as Epsom salts and its fame spread. It was eventually recognized as hydrated magnesium sulfate, ·7.
The metal itself was first produced by Sir Humphry Davy in England in 1808. He used electrolysis on a mixture of magnesia and mercuric oxide. Antoine Bussy prepared it in coherent form in 1831. Davy's first suggestion for a name was magnium, but the name magnesium is now used.
Uses as a metal.
Magnesium is the third-most-commonly-used structural metal, following iron and aluminium. It is called the lightest useful metal by The Periodic Table of Videos.
The main applications of magnesium are, in order: aluminium alloys, die-casting (alloyed with zinc), removing sulfur in the production of iron and steel, and the production of titanium in the Kroll process.
Magnesium is used in super-strong, lightweight materials and alloys. For example, when infused with silicon carbide nanoparticles, it has extremely high specific strength.
Historically, magnesium was one of the main aerospace construction metals and was used for German military aircraft as early as World War I and extensively for German aircraft in World War II.
The Germans coined the name "Elektron" for magnesium alloy, a term is still used today. In the commercial aerospace industry, magnesium was generally restricted to engine-related components, due fire and corrosion hazards. Currently, magnesium alloy use in aerospace is increasing, driven by the importance of fuel economy. Development and testing of new magnesium alloys continues, notably Elektron 21, which (in test) has proved suitable for aerospace engine, internal, and airframe components. The European Community runs three R&D magnesium projects in the Aerospace priority of Six Framework Program.
In the form of thin ribbons, magnesium is used to purify solvents; for example, preparing super-dry ethanol.
Automotive.
Both AJ62A and AE44 are recent developments in high-temperature low-creep magnesium alloys. The general strategy for such alloys is to form intermetallic precipitates at the grain boundaries, for example by adding mischmetal or calcium. New alloy development and lower costs that make magnesium competitive with aluminium will increase the number of automotive applications.
Electronics.
Because of low weight and good mechanical and electrical properties, magnesium is widely used for manufacturing of mobile phones, laptop and tablet computers, cameras, and other electronic components.
Other.
Magnesium, being readily available and relatively nontoxic, has a variety of uses:
Safety precautions.
Magnesium metal and its alloys can be explosive hazards; they are highly flammable in their pure form when molten or in powder or ribbon form. Burning or molten magnesium reacts violently with water. When working with powdered magnesium, safety glasses with eye protection and UV filters (such as welders use) are employed because burning magnesium produces ultraviolet light that can permanently damage the retina of a human eye.
Magnesium is capable of reducing water and releasing highly flammable hydrogen gas:
Therefore, water cannot extinguish magnesium fires. The hydrogen gas produced intensifies the fire. Dry sand is an effective smothering agent, but only on relatively level and flat surfaces.
Magnesium reacts with carbon dioxide to form magnesium oxide and carbon:
Hence, carbon dioxide fire extinguishers are ineffective for extinguishing magnesium fires.
Burning magnesium can be quenched by using a Class D dry chemical fire extinguisher, or by covering the fire with sand or magnesium foundry flux to remove its air source.
Useful compounds.
Magnesium compounds, primarily magnesium oxide (MgO), are used as a refractory material in furnace linings for producing iron, steel, nonferrous metals, glass, and cement. Magnesium oxide and other magnesium compounds are also used in the agricultural, chemical, and construction industries. Magnesium oxide from calcination is used as an electrical insulator in fire-resistant cables.
Magnesium reacted with an alkyl halide gives a Grignard reagent, which is a very useful tool for preparing alcohols.
Magnesium salts are included in various foods, fertilizers (magnesium is a component of chlorophyll), and microbe culture media.
Magnesium sulfite is used in the manufacture of paper (sulfite process).
Magnesium phosphate is used to fireproof wood used in construction.
Magnesium hexafluorosilicate is used for moth-proofing textiles.
Biological roles.
Mechanism of action.
The important interaction between phosphate and magnesium ions makes magnesium essential to the basic nucleic acid chemistry of all cells of all known living organisms. More than 300 enzymes require magnesium ions for their catalytic action, including all enzymes using or synthesizing ATP and those that use other nucleotides to synthesize DNA and RNA. The ATP molecule is normally found in a chelate with a magnesium ion. 
Dietary sources, recommended intake, and supplementation.
In the UK, the recommended daily values for magnesium is 300 mg for men and 270 mg for women. Reduced magnesium in the diet of modern Western countries (compared to earlier generations) may be related to food refining and modern fertilizers that contain no magnesium.
Numerous pharmaceutical preparations of magnesium and dietary supplements are available. Magnesium oxide, one of the most common forms in magnesium dietary supplements because of its high magnesium content per weight, is the least bioavailable.
Metabolism.
An adult has 22–26 grams of magnesium, with 60% in the skeleton, 39% intracellular (20% in skeletal muscle), and 1% extracellular. Serum levels are typically 0.7–1.0 mmol/L or 1.8–2.4 mEq/L. Serum magnesium levels may be normal even when intracellular magnesium is deficient. The mechanisms for maintaining the magnesium level in the serum are varying gastrointestinal absorption and renal excretion. Intracellular magnesium is correlated with intracellular potassium. Increased magnesium lowers calcium and can either prevent hypercalcemia or cause hypocalcemia depending on the initial level. Both low and high protein intake conditions inhibit magnesium absorption, as does the amount of phosphate, phytate, and fat in the gut. Excess dietary magnesium is excreted in feces, urine, and sweat. 
Detection in serum and plasma.
Magnesium status may be assessed by measuring serum and erythrocyte magnesium concentrations coupled with urinary and fecal magnesium content, but intravenous magnesium loading tests are more accurate and practical. A retention of 20% or more of the injected amount indicates deficiency. No biomarker has been established for magnesium.
Magnesium concentrations in plasma or serum may be monitored for efficacy and safety in those receiving the drug therapeutically, to confirm the diagnosis in potential poisoning victims, or to assist in the forensic investigation in a case of fatal overdose. The newborn children of mothers who received parenteral magnesium sulfate during labor may exhibit toxicity with normal serum magnesium levels.
Deficiency.
Magnesium deficiency (hypomagnesemia) is common: it is found in 2.5–15% of the general population. The primary cause of deficiency is decreased dietary intake: only 32% of people in the United States meet the recommended daily allowance. Other causes are increased renal or gastrointestinal loss, an increased intracellular shift, and proton-pump inhibitor antacid therapy. Most are asymptomatic, but symptoms referable to neuromuscular, cardiovascular, and metabolic dysfunction may occur. Alcoholism is often associated with magnesium deficiency. Chronically low serum magnesium levels are associated with metabolic syndrome, diabetes mellitus type 2, fasciculation, and hypertension.
Therapy.
Sorted by type of magnesium salt, other therapeutic applications include:
Overdose.
Overdose from dietary sources alone is unlikely because excess magnesium in the blood is promptly filtered by the kidneys. Overdose with magnesium tablets is possible in the presence of impaired renal function. There is a single case report of hypermagnesemia in a woman with normal renal function using high doses of magnesium salts for catharsis. The most common symptoms of overdose are nausea, vomiting, and diarrhea; other symptoms include hypotension, confusion, slowed heart and respiratory rate, deficiencies of other minerals, coma, cardiac arrhythmia, and death from cardiac arrest.
Function in plants.
Plants require magnesium to synthesize chlorophyll, essential for photosynthesis. Magnesium in the center of the porphyrin ring in chlorophyll functions in a manner similar to the iron in the center of the porphyrin ring in heme. Magnesium deficiency in plants causes late-season yellowing between leaf veins, especially in older leaves, and can be corrected by applying to the soil either Epsom salts (which is rapidly leached), or crushed dolomitic limestone.

</doc>
<doc id="18910" url="https://en.wikipedia.org/wiki?curid=18910" title="Markup language">
Markup language

A markup language is a system for annotating a document in a way that is syntactically distinguishable from the text. The idea and terminology evolved from the ""marking up"" of paper manuscripts, i.e., the revision instructions by editors, traditionally written with a blue pencil on authors' manuscripts.
In digital media this "blue pencil instruction text" was replaced by tags, that is, instructions are expressed directly by tags or "instruction text encapsulated by tags." Examples include typesetting instructions such as those found in troff, TeX and LaTeX, or structural markers such as XML tags. Markup instructs the software that displays the text to carry out appropriate actions, but is omitted from the version of the text that users see.
Some markup languages, such as the widely used HTML, have pre-defined presentation semantics—meaning that their specification prescribes how to present the structured data. Others, such as XML, do not.
HyperText Markup Language (HTML), one of the document formats of the World Wide Web, is an instance of SGML (though, strictly, it does not comply with all the rules of SGML), and follows many of the markup conventions used in the publishing industry in the communication of printed work between authors, editors, and printers.
Types.
There are main three general categories of electronic markup:
There is considerable blurring of the lines between the types of markup. In modern word-processing systems, presentational markup is often saved in descriptive-markup-oriented systems such as XML, and then processed procedurally by implementations. The programming constructs in procedural-markup systems such as TeX may be used to create higher-level markup systems that are more descriptive, such as LaTeX.
In recent years, a number of small and largely unstandardized markup languages have been developed to allow authors to create formatted text via web browsers, for use in wikis and web forums. These are sometimes called lightweight markup languages. Markdown or the markup language used by Wikipedia are examples of such wiki markup.
History.
Etymology and origin.
The term "markup" is derived from the traditional publishing practice of ""marking up"" a manuscript, which involves adding handwritten annotations in the form of conventional symbolic printer's instructions in the margins and text of a paper manuscript or printed proof. For centuries, this task was done primarily by skilled typographers known as "markup men" or "copy markers" who marked up text to indicate what typeface, style, and size should be applied to each part, and then passed the manuscript to others for typesetting by hand. Markup was also commonly applied by editors, proofreaders, publishers, and graphic designers, and indeed by document authors.
GenCode.
The first well-known public presentation of markup languages in computer text processing was made by William W. Tunnicliffe at a conference in 1967, although he preferred to call it "generic coding." It can be seen as a response to the emergence of programs such as RUNOFF that each used their own control notations, often specific to the target typesetting device. In the 1970s, Tunnicliffe led the development of a standard called GenCode for the publishing industry and later was the first chair of the International Organization for Standardization committee that created SGML, the first standard descriptive markup language. Book designer Stanley Rice published speculation along similar lines in 1970. Brian Reid, in his 1980 dissertation at Carnegie Mellon University, developed the theory and a working implementation of descriptive markup in actual use.
However, IBM researcher Charles Goldfarb is more commonly seen today as the "father" of markup languages. Goldfarb hit upon the basic idea while working on a primitive document management system intended for law firms in 1969, and helped invent IBM GML later that same year. GML was first publicly disclosed in 1973.
In 1975, Goldfarb moved from Cambridge, Massachusetts to Silicon Valley and became a product planner at the IBM Almaden Research Center. There, he convinced IBM's executives to deploy GML commercially in 1978 as part of IBM's Document Composition Facility product, and it was widely used in business within a few years.
SGML, which was based on both GML and GenCode, was developed by Goldfarb in 1974. Goldfarb eventually became chair of the SGML committee. SGML was first released by ISO as the ISO 8879 standard in October 1986.
troff and nroff.
Some early examples of computer markup languages available outside the publishing industry can be found in typesetting tools on Unix systems such as troff and nroff. In these systems, formatting commands were inserted into the document text so that typesetting software could format the text according to the editor's specifications. It was a trial and error iterative process to get a document printed correctly. Availability of WYSIWYG ("what you see is what you get") publishing software supplanted much use of these languages among casual users, though serious publishing work still uses markup to specify the non-visual structure of texts, and WYSIWYG editors now usually save documents in a markup-language-based format.
TeX.
Another major publishing standard is TeX, created and refined by Donald Knuth in the 1970s and '80s. TeX concentrated on detailed layout of text and font descriptions to typeset mathematical books. This required Knuth to spend considerable time investigating the art of typesetting. TeX is mainly used in academia, where it is a "de facto" standard in many scientific disciplines. A TeX macro package known as LaTeX provides a descriptive markup system on top of TeX, and is widely used.
Scribe, GML and SGML.
The first language to make a clean distinction between structure and presentation was Scribe, developed by Brian Reid and described in his doctoral thesis in 1980. Scribe was revolutionary in a number of ways, not least that it introduced the idea of styles separated from the marked up document, and of a grammar controlling the usage of descriptive elements. Scribe influenced the development of Generalized Markup Language (later SGML) and is a direct ancestor to HTML and LaTeX.
In the early 1980s, the idea that markup should be focused on the structural aspects of a document and leave the visual presentation of that structure to the interpreter led to the creation of SGML. The language was developed by a committee chaired by Goldfarb. It incorporated ideas from many different sources, including Tunnicliffe's project, GenCode. Sharon Adler, Anders Berglund, and James A. Marke were also key members of the SGML committee.
SGML specified a syntax for including the markup in documents, as well as one for separately describing "what" tags were allowed, and "where" (the Document Type Definition (DTD) or schema). This allowed authors to create and use any markup they wished, selecting tags that made the most sense to them and were named in their own natural languages. Thus, SGML is properly a meta-language, and many particular markup languages are derived from it. From the late '80s on, most substantial new markup languages have been based on SGML system, including for example TEI and DocBook. SGML was promulgated as an International Standard by International Organization for Standardization, ISO 8879, in 1986.
SGML found wide acceptance and use in fields with very large-scale documentation requirements. However, many found it cumbersome and difficult to learn—a side effect of its design attempting to do too much and be too flexible. For example, SGML made end tags (or start-tags, or even both) optional in certain contexts, because its developers thought markup would be done manually by overworked support staff who would appreciate saving keystrokes.
HTML.
In 1989, physicist Sir Tim Berners-Lee wrote a memo proposing an Internet-based hypertext system, then specified HTML and wrote the browser and server software in the last part of 1990. The first publicly available description of HTML was a document called "HTML Tags", first mentioned on the Internet by Berners-Lee in late 1991. It describes 18 elements comprising the initial, relatively simple design of HTML. Except for the hyperlink tag, these were strongly influenced by SGMLguid, an in-house SGML-based documentation format at CERN. Eleven of these elements still exist in HTML 4.
Berners-Lee considered HTML an SGML application. The Internet Engineering Task Force (IETF) formally defined it as such with the mid-1993 publication of the first proposal for an HTML specification: "Hypertext Markup Language (HTML)" Internet-Draft by Berners-Lee and Dan Connolly, which included an SGML Document Type Definition to define the grammar. Many of the HTML text elements are found in the 1988 ISO technical report TR 9537 "Techniques for using SGML", which in turn covers the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system. These formatting commands were derived from those used by typesetters to manually format documents. Steven DeRose argues that HTML's use of descriptive markup (and influence of SGML in particular) was a major factor in the success of the Web, because of the flexibility and extensibility that it enabled. HTML became the main markup language for creating web pages and other information that can be displayed in a web browser, and is quite likely the most used markup language in the world today.
XML.
XML (Extensible Markup Language) is a meta markup language that is now widely used. XML was developed by the World Wide Web Consortium, in a committee created and chaired by Jon Bosak. The main purpose of XML was to simplify SGML by focusing on a particular problem—documents on the Internet. XML remains a meta-language like SGML, allowing users to create any tags needed (hence "extensible") and then describing those tags and their permitted uses.
XML adoption was helped because every XML document can be written in such a way that it is also an SGML document, and existing SGML users and software could switch to XML fairly easily. However, XML eliminated many of the more complex and human-oriented features of SGML to simplify implementation environments such as documents and publications. However, it appeared to strike a happy medium between simplicity and flexibility, and was rapidly adopted for many other uses. XML is now widely used for communicating data between applications.
XHTML.
Since January 2000, all W3C Recommendations for HTML have been based on XML rather than SGML, using the abbreviation XHTML (Extensible HyperText Markup Language). The language specification requires that XHTML Web documents must be "well-formed" XML documents. This allows for more rigorous and robust documents while using tags familiar from HTML.
One of the most noticeable differences between HTML and XHTML is the rule that "all tags must be closed": empty HTML tags such as codice_2 must either be "closed" with a regular end-tag, or replaced by a special form: codice_3 (the space before the 'codice_4' on the end tag is optional, but frequently used because it enables some pre-XML Web browsers, and SGML parsers, to accept the tag). Another is that all attribute values in tags must be quoted. Finally, all tag and attribute names within the XHTML namespace must be lowercase to be valid. HTML, on the other hand, was case-insensitive.
Other XML-based applications.
Many XML-based applications now exist, including the Resource Description Framework as RDF/XML, XForms, DocBook, SOAP, and the Web Ontology Language (OWL). For a partial list of these, see List of XML markup languages.
Features.
A common feature of many markup languages is that they intermix the text of a document with markup instructions in the same data stream or file. This is not necessary; it is possible to isolate markup from text content, using pointers, offsets, IDs, or other methods to co-ordinate the two. Such "standoff markup" is typical for the internal representations that programs use to work with marked-up documents. However, embedded or "inline" markup is much more common elsewhere. Here, for example, is a small section of text marked up in HTML:
The codes enclosed in angle-brackets codice_5 are markup instructions (known as tags), while the text between these instructions is the actual text of the document. The codes codice_6, codice_7, and codice_8 are examples of "semantic" markup, in that they describe the intended purpose or meaning of the text they include. Specifically, codice_6 means "this is a first-level heading", codice_7 means "this is a paragraph", and codice_8 means "this is an emphasized word or phrase". A program interpreting such structural markup may apply its own rules or styles for presenting the various pieces of text, using different typefaces, boldness, font size, indentation, colour, or other styles, as desired.
A tag such as "h1" (header level 1) might be presented in a large bold sans-serif typeface, for example, or in a monospaced (typewriter-style) document it might be underscored – or it might not change the presentation at all.
In contrast, the codice_12 tag in HTML is an example of "presentational" markup; it is generally used to specify a particular characteristic of the text (in this case, the use of an italic typeface) without specifying the reason for that appearance.
The Text Encoding Initiative (TEI) has published extensive guidelines for how to encode texts of interest in the humanities and social sciences, developed through years of international cooperative work. These guidelines are used by projects encoding historical documents, the works of particular scholars, periods, or genres, and so on.
Alternative usage.
While the idea of markup language originated with text documents, there is increasing use of markup languages in the presentation of other types of information, including playlists, vector graphics, web services, content syndication, and user interfaces. Most of these are XML applications, because XML is a well-defined and extensible language.
The use of XML has also led to the possibility of combining multiple markup languages into a single profile, like XHTML+SMIL and XHTML+MathML+SVG.
Because markup languages, and more generally data description languages (not necessarily textual markup), are not programming languages (they are data without instructions), they are more easily manipulated than programming languages—for example, web pages are presented as HTML documents, not C code, and thus can be embedded within other web pages, displayed when only partially received, and so forth. This leads to the web design principle of the rule of least power, which advocates using the "least" (computationally) powerful language that satisfies a task to facilitate such manipulation and reuse.

</doc>
<doc id="18916" url="https://en.wikipedia.org/wiki?curid=18916" title="Meaning">
Meaning

Meaning may refer to:

</doc>
<doc id="18917" url="https://en.wikipedia.org/wiki?curid=18917" title="Meta-ethics">
Meta-ethics

Meta-ethics is the branch of ethics that seeks to understand the nature of ethical properties, statements, attitudes, and judgments. Meta-ethics is one of the four branches of ethics generally recognized by philosophers, the others being descriptive ethics, normative ethics and applied ethics.
While normative ethics addresses such questions as "What should I do?", thus endorsing some ethical evaluations and rejecting others, meta-ethics addresses questions such as "What "is" goodness?" and "How can we tell what is good from what is bad?", seeking to understand the nature of ethical properties and evaluations.
Some theorists argue that a metaphysical account of morality is necessary for the proper evaluation of actual moral theories and for making practical moral decisions; others reason from opposite premises and suggest that we must impart ideas of moral intuition onto proper action before we can give a proper account of morality's metaphysics.
Meta-ethical questions.
According to Richard Garner and Bernard Rosen, there are three kinds of meta-ethical problems, or three general questions:
A question of the first type might be, "What do the words 'good', 'bad', 'right' and 'wrong' mean?" (see value theory). The second category includes questions of whether moral judgments are universal or relative, of one kind or many kinds, etc. Questions of the third kind ask, for example, how we can know if something is right or wrong, if at all. Garner and Rosen say that answers to the three basic questions "are not unrelated, and sometimes an answer to one will strongly suggest, or perhaps even entail, an answer to another."
A meta-ethical theory, unlike a normative ethical theory, does not attempt to evaluate specific choices as being better, worse, good, bad, or evil; although it may have profound implications as to the validity and meaning of normative ethical claims. An answer to any of the three example questions above would not itself be a normative ethical statement.
Semantic theories.
These theories mainly put forward a position on the first of the three questions above, "What is the meaning of moral terms or judgments?" They may however imply or even entail answers to the other two questions as well.
Centralism and non-centralism.
Yet another way of categorizing meta-ethical theories is to distinguish between centralist and non-centralist theories. The debate between centralism and non-centralism revolves around the relationship between the so-called "thin" and "thick" concepts of morality. Thin moral concepts are those such as good, bad, right, and wrong; thick moral concepts are those such as courageous, inequitable, just, or dishonest. While both sides agree that the thin concepts are more general and the thick more specific, centralists hold that the thin concepts are antecedent to the thick ones and that the latter are therefore dependent on the former. That is, centralists argue that one must understand words like "right" and "ought" before understanding words like "just" and "unkind." Non-centralism rejects this view, holding that thin and thick concepts are on par with one another and even that the thick concepts are a sufficient starting point for understanding the thin ones.
Non-centralism has been of particular importance to ethical naturalists in the late 20th and early 21st centuries as part of their argument that normativity is a non-excisable aspect of language and that there is no way of analyzing thick moral concepts into a purely descriptive element attached to a thin moral evaluation, thus undermining any fundamental division between facts and norms. Allan Gibbard, R. M. Hare, and Simon Blackburn have argued in favor of the fact/norm distinction, meanwhile, with Gibbard going so far as to argue that, even if conventional English has only mixed normative terms (that is, terms that are neither purely descriptive nor purely normative), we could develop a nominally English metalanguage that still allowed us to maintain the division between factual descriptions and normative evaluations.
Substantial theories.
These theories attempt to answer the second of the above questions: "What is the nature of moral judgments?"
Justification theories.
These are theories that attempt to answer questions like, "How may moral judgments be supported or defended?" or "Why should I be moral?"
If one presupposes a cognitivist interpretation of moral sentences, morality is justified by the moralist's knowledge of moral facts, and the theories to justify moral judgements are epistemological theories.

</doc>
<doc id="18921" url="https://en.wikipedia.org/wiki?curid=18921" title="Montesquieu (disambiguation)">
Montesquieu (disambiguation)

Montesquieu (1689–1755) was a French lawyer, man of letters, and political philosopher.
Montesquieu may also refer to:

</doc>
<doc id="18925" url="https://en.wikipedia.org/wiki?curid=18925" title="Mormons">
Mormons

Mormons () are a religious and cultural group related to Mormonism, the principal branch of the Latter Day Saint movement of Restorationist Christianity, which began with Joseph Smith in upstate New York during the 1820s. After Smith's death in 1844, the Mormons followed Brigham Young to what would become the Utah Territory. Today, most Mormons are understood to be members of The Church of Jesus Christ of Latter-day Saints (LDS Church). Some Mormons are also either independent or non-practicing. The center of Mormon cultural influence is in Utah, and North America has more Mormons than any other continent, though the majority of Mormons live outside the United States.
Mormons have developed a strong sense of communality that stems from their doctrine and history. During the 19th century, Mormon converts tended to gather to a central geographic location, and between 1852 and 1890 a minority of Mormons openly practiced plural marriage, a form of religious polygamy. Mormons dedicate large amounts of time and resources to serving in their church, and many young Mormons choose to serve a full-time proselytizing mission. Mormons have a health code which eschews alcoholic beverages, tobacco, coffee, tea, and other addictive substances. They tend to be very family-oriented, and have strong connections across generations and with extended family, reflective of their belief that families can be sealed together beyond death. Mormons also have a strict law of chastity, requiring abstention from sexual relations outside of opposite-sex marriage and strict fidelity within marriage.
Mormons self-identify as Christian, although some non-Mormons dispute this and some of their beliefs differ from mainstream Christianity. Mormons believe in the Bible, as well as other books of scripture, such as the Book of Mormon. They have a unique view of cosmology, and believe that all people are spirit-children of God. Mormons believe that returning to God requires following the example of Jesus Christ, and accepting his atonement through ordinances such as baptism. They believe that Christ's church was restored through Joseph Smith and is guided by living prophets and apostles. Central to Mormon faith is the belief that God speaks to his children and answers their prayers.
Due to their high birth and conversion rates, the Mormon population has grown significantly in recent decades rising from around three million in 1970 to over 15 million in 2015.
Terminology.
The word "Mormons" most often refers to members of The Church of Jesus Christ of Latter-day Saints (LDS Church) because of their belief in the Book of Mormon, though members often refer to themselves as "Latter-day Saints" or sometimes just "Saints". The term "Mormons" has been embraced by most adherents of Mormonism, most notably Mormon fundamentalists, while other Latter Day Saint denominations, such as the Community of Christ, have rejected it. Both LDS Church members (or "Latter-day Saints") and members of fundamentalist groups commonly use the word "Mormon" in reference to themselves. The LDS Church, however, disagrees with this self-characterization, and encourages the use of the word "Mormon" only in reference to LDS Church members. Church leaders also encourage members to use the church's full name to emphasize its focus on Jesus Christ.
The word "Mormon" is often associated with polygamy (or plural marriage), which was a distinguishing practice of many early Mormons; however it was renounced by the LDS Church in 1890
and discontinued over the next 15 years.
Today, polygamy is practiced within Mormonism only by people that have broken with the LDS Church.
History.
The history of the Mormons has shaped them into a people with a strong sense of unity and communality. From the start, Mormons have tried to establish what they call "Zion", a utopian society of the righteous.
Mormon history can be divided into three broad time periods: (1) the early history during the lifetime of Joseph Smith, (2) a "pioneer era" under the leadership of Brigham Young and his successors, and (3) a modern era beginning around the turn of the 20th century. In the first period, Smith had tried literally to build a city called Zion, in which converts could gather. During the pioneer era, Zion became a "landscape of villages" in Utah. In modern times, Zion is still an ideal, though Mormons gather together in their individual congregations rather than a central geographic location.
Beginnings.
Mormons trace their origins to the visions that Joseph Smith reported having in the early 1820s while living in upstate New York. In 1823, Smith said an angel directed him to a buried book written on golden plates containing the religious history of an ancient people. Smith published what he said was a translation of these plates in March 1830 as the Book of Mormon, named after Mormon, the ancient prophet–historian who compiled the book. On April 6, 1830, Smith founded the Church of Christ. The early church grew westward as Smith sent missionaries to preach the restored gospel. In 1831, the church moved to Kirtland, Ohio where missionaries had made a large number of converts and Smith began establishing an outpost in Jackson County, Missouri, where he planned to eventually build the city of Zion (or the New Jerusalem). In 1833, Missouri settlers, alarmed by the rapid influx of Mormons, expelled them from Jackson County into the nearby Clay County, where local residents took them in.
After Smith led a mission, known as Zion's Camp, to recover the land, he began building Kirtland Temple in Lake County, Ohio, where the church flourished. When the Missouri Mormons were later asked to leave Clay County in 1836, they secured land in what would become Caldwell County.
The Kirtland era ended in 1838, after the failure of a church-sponsored bank caused widespread defections, and Smith regrouped with the remaining church in Far West, Missouri. During the fall of 1838, tensions escalated into the Mormon War with the old Missouri settlers. On October 27, the governor of Missouri ordered that the Mormons "must be treated as enemies" and be exterminated or driven from the state. Between November and April, some eight thousand displaced Mormons migrated east into Illinois.
In 1839, the Mormons converted a swampland on the banks of the Mississippi River into Nauvoo, Illinois and began construction of the Nauvoo Temple. The city became the church's new headquarters and gathering place, and it grew rapidly, fueled in part by converts immigrating from Europe. Meanwhile, Smith introduced temple ceremonies meant to seal families together for eternity, as well as the doctrines of eternal progression or exaltation, and plural marriage.
Smith created a service organization for women called the Relief Society, as well as an organization called the Council of Fifty, representing a future theodemocratic "Kingdom of God" on the earth.
Smith also published the story of his First Vision, in which the Father and the Son appeared to him while he was about 14 years old.
This vision would come to be regarded by some Mormons as the most important event in human history after the birth, ministry, and resurrection of Jesus Christ.
In 1844, local prejudices and political tensions, fueled by Mormon peculiarity and internal dissent, escalated into conflicts between Mormons and "anti-Mormons". On June 27, 1844, Smith and his brother Hyrum were killed by a mob in Carthage, Illinois. Because Hyrum was Smith's logical successor, their deaths caused a succession crisis, and Brigham Young assumed leadership over the majority of Latter Day Saints. Young had been a close associate of Smith's and was senior apostle of the Quorum of the Twelve. Smaller groups of Latter Day Saints followed other leaders to form other denominations of the Latter Day Saint movement.
Pioneer era.
For two years after Smith's death, conflicts escalated between Mormons and other Illinois residents. To prevent war, Brigham Young led the Mormon pioneers (constituting most of the Latter Day Saints) to a temporary winter quarters in Nebraska and then, eventually (beginning in 1847), to what became the Utah Territory. Having failed to build Zion within the confines of American society, the Mormons began to construct a society in isolation, based on their beliefs and values. The cooperative ethic that Mormons had developed over the last decade and a half became important as settlers branched out and colonized a large desert region now known as the Mormon Corridor. Colonizing efforts were seen as religious duties, and the new villages were governed by the Mormon bishops (local lay religious leaders). The Mormons viewed land as commonwealth, devising and maintaining a co-operative system of irrigation that allowed them to build a farming community in the desert.
From 1849–52, the Mormons greatly expanded their missionary efforts, establishing several missions in Europe, Latin America, and the South Pacific. Converts were expected to "gather" to Zion, and during Young's presidency (1847–77) over seventy thousand Mormon converts immigrated to America. Many of the converts came from England and Scandinavia, and were quickly assimilated into the Mormon community. Many of these immigrants crossed the Great Plains in wagons drawn by oxen, while some later groups pulled their possessions in small handcarts. During the 1860s, newcomers began using the new railroad that was under construction.
In 1852, church leaders publicized the previously secret practice of plural marriage, a form of polygamy. Over the next 50 years, many Mormons (between 20 and 30 percent of Mormon families) entered into plural marriages as a religious duty, with the number of plural marriages reaching a peak around 1860, and then declining through the rest of the century. Besides the doctrinal reasons for plural marriage, the practice made some economic sense, as many of the plural wives were single women who arrived in Utah without brothers or fathers to offer them societal support.
By 1857, tensions had again escalated between Mormons and other Americans, largely as a result of accusations involving polygamy and the theocratic rule of the Utah Territory by Brigham Young. In 1857, U.S. President James Buchanan sent an army to Utah, which Mormons interpreted as open aggression against them. Fearing a repeat of Missouri and Illinois, the Mormons prepared to defend themselves, determined to torch their own homes in the case that they were invaded. The relatively peaceful Utah War ensued from 1857 to 1858, in which the most notable instance of violence was the Mountain Meadows massacre, when leaders of a local Mormon militia ordered the killing of a civilian emigrant party that was traveling through Utah during the escalating tensions. In 1858, Young agreed to step down from his position as governor and was replaced by a non-Mormon, Alfred Cumming. Nevertheless, the LDS Church still wielded significant political power in the Utah Territory.
At Young's death in 1877, he was followed by other LDS Church presidents, who resisted efforts by the United States Congress to outlaw Mormon polygamous marriages. In 1878, the U.S. Supreme Court ruled in "Reynolds v. United States" that religious duty was not a suitable defense for practicing polygamy, and many Mormon polygamists went into hiding; later, Congress began seizing church assets. In September 1890, church president Wilford Woodruff issued a Manifesto that officially suspended the practice of polygamy. Although this Manifesto did not dissolve existing plural marriages, relations with the United States markedly improved after 1890, such that Utah was admitted as a U.S. state in 1896. After the Manifesto, some Mormons continued to enter into polygamous marriages, but these eventually stopped in 1904 when church president Joseph F. Smith disavowed polygamy before Congress and issued a "Second Manifesto" calling for all plural marriages in the church to cease. Eventually, the church adopted a policy of excommunicating members found practicing polygamy, and today seeks actively to distance itself from "fundamentalist" groups that continue the practice.
Modern times.
During the early 20th century, Mormons began to reintegrate into the American mainstream. In 1929, the Mormon Tabernacle Choir began broadcasting a weekly performance on national radio, becoming an asset for public relations. Mormons emphasized patriotism and industry, rising in socioeconomic status from the bottom among American religious denominations to middle-class.
In the 1920s and 1930s, Mormons began migrating out of Utah, a trend hurried by the Great Depression, as Mormons looked for work wherever they could find it. As Mormons spread out, church leaders created programs that would help preserve the tight-knit community feel of Mormon culture. In addition to weekly worship services, Mormons began participating in numerous programs such as Boy Scouting, a Young Women organization, church-sponsored dances, ward basketball, camping trips, plays, and religious education programs for youth and college students. During the Great Depression, the church started a welfare program to meet the needs of poor members, which has since grown to include a humanitarian branch that provides relief to disaster victims.
During the later half of the 20th century, there was a retrenchment movement in Mormonism in which Mormons became more conservative, attempting to regain their status as a "peculiar people".
Though the 1960s and 1970s brought changes such as Women's Liberation and the Civil Rights Movement, Mormon leaders were alarmed by the erosion of traditional values, the sexual revolution, the widespread use of recreational drugs, moral relativism, and other forces they saw as damaging to the family.
Partly to counter this, Mormons put an even greater emphasis on family life, religious education, and missionary work, becoming more conservative in the process. As a result, Mormons today are probably less integrated with mainstream society than they were in the early 1960s.
Although black people have been members of Mormon congregations since Joseph Smith's time, before 1978, black membership was small. From 1852 to 1978, the LDS Church enforced a policy that restricted men of black African descent from being ordained to the church's lay priesthood. The church was sharply criticized for its policy during the civil rights movement, but the policy remained in force until a 1978 reversal that was prompted in part by questions about mixed-race converts in Brazil. In general, Mormons greeted the change with joy and relief. Since 1978, black membership has grown, and in 1997 there were approximately 500,000 black members of the church (about 5 percent of the total membership), mostly in Africa, Brazil and the Caribbean. Black membership has continued to grow substantially, especially in West Africa, where two temples have been built. Many black Mormons are members of the Genesis Group, an organization of black members that predates the priesthood ban, and is endorsed by the church.
The LDS Church grew rapidly after World War II and became a world-wide organization as missionaries were sent across the globe. The church doubled in size every 15 to 20 years, and by 1996, there were more Mormons outside the United States than inside. In 2012, there were an estimated 14.8 million Mormons, with roughly 57 percent living outside the United States. It is estimated that approximately 4.5 million Mormons - roughly 30% of the total membership - regularly attend services. A majority of U.S. Mormons are white and non-Hispanic (84 percent). Most Mormons are distributed in North and South America, the South Pacific, and Western Europe. The global distribution of Mormons resembles a contact diffusion model, radiating out from the organization's headquarters in Utah. The church enforces general doctrinal uniformity, and congregations on all continents teach the same doctrines, and international Mormons tend to absorb a good deal of Mormon culture, possibly because of the church's top-down hierarchy and a missionary presence. However, international Mormons often bring pieces of their own heritage into the church, adapting church practices to local cultures.
Chile, Uruguay, and several areas in the South Pacific have a higher percentage of Mormons than the United States (which is at about 2 percent). South Pacific countries and dependencies that are more than 10 percent Mormon include American Samoa, the Cook Islands, Kiribati, Niue, Samoa, and Tonga.
Culture and practices.
Isolation in Utah had allowed Mormons to create a culture of their own. As the faith spread around the world, many of its more distinctive practices followed. Mormon converts are urged to undergo lifestyle changes, repent of sins, and adopt sometimes foreign standards of conduct. Practices common to Mormons include studying scriptures, praying daily, fasting regularly, attending Sunday worship services, participating in church programs and activities on weekdays, and refraining from work on Sundays when possible. The most important part of the church services is considered to be the Lord's Supper (commonly called sacrament), in which church members renew covenants made at baptism. Mormons also emphasize standards they believe were taught by Jesus Christ, including personal honesty, integrity, obedience to law, chastity outside of marriage and fidelity within marriage.
In 2010, around 13–14 percent of Mormons lived in Utah, the center of cultural influence for Mormonism. Utah Mormons (as well as Mormons living in the Intermountain West) are on average more culturally and/or politically conservative than those living in some cosmopolitan centers elsewhere in the U.S. Utahns self-identifying as Mormon also attend church somewhat more on average than Mormons living in other states. (Nonetheless, whether they live in Utah or elsewhere in the U.S., Mormons tend to be more culturally and/or politically conservative than members of other U.S. religious groups.) Utah Mormons often place a greater emphasis on pioneer heritage than international Mormons who generally are not descendants of the Mormon pioneers.
Mormons have a strong sense of communality that stems from their doctrine and history. LDS Church members have a responsibility to dedicate their time and talents to helping the poor and building the church. The church is divided by locality into congregations called "wards", with several wards making up a "stake". The vast majority of church leadership positions are lay positions, and church leaders may work 10 to 15 hours a week in unpaid church service. Observant Mormons also contribute 10 percent of their income to the church as tithing, and are often involved in humanitarian efforts. Many LDS young men, women and elderly couples choose to serve a proselytizing mission, during which they dedicate all of their time to the church, without pay.
Mormons adhere to the Word of Wisdom, a health law or code that is interpreted as prohibiting the consumption of tobacco, alcohol, coffee and tea, while encouraging the use of wholesome herbs, grains, fruits, and a moderate consumption of meat. The Word of Wisdom is also understood to forbid other harmful and addictive substances and practices, such as the use of illegal drugs and abuse of prescription drugs. Mormons also oppose behaviors such as viewing pornography and gambling.
The concept of a united family that lives and progresses forever is at the core of Latter-day Saint doctrine, and Mormons place a high importance on family life. Many Mormons hold weekly family home evenings, in which an evening is set aside for family bonding, study, prayer and other wholesome activities. Latter-day Saint fathers who hold the priesthood typically name and bless their children shortly after birth to formally give the child a name. Mormon parents hope and pray that their children will gain testimonies of the "gospel" so they can grow up and marry in temples.
Mormons have a strict law of chastity, requiring abstention from sexual relations outside of opposite-sex marriage and strict fidelity within marriage. All sexual activity (heterosexual and homosexual) outside of marriage is considered a serious sin, with marriage recognized as only between a man and a woman. Same-sex marriages are not performed or supported by the LDS Church. Church members are encouraged to marry and have children, and Latter-day Saint families tend to be larger than average. Mormons are opposed to abortion, except in some exceptional circumstances, such as when pregnancy is the result of incest or rape, or when the life or health of the mother is in serious jeopardy. Many practicing adult Mormons wear religious undergarments that remind them of covenants and encourage them to dress modestly. Latter-day Saints are counseled not to partake of any form of media that is obscene or pornographic in any way, including media that depicts graphic representations of sex or violence. Tattoos and body piercings are also discouraged, with the exception of a single pair of earrings for LDS women.
LGBT Mormons, or Mormons who self-identify as gay, lesbian, or bisexual, remain in good standing in the church if they abstain from homosexual relations and obey the law of chastity. While there are no official numbers, LDS Family Services estimates that there are on average four or five members per LDS ward who experience same-sex attraction. Gary Watts, former president of Family Fellowship, estimates that only 10 percent of homosexuals stay in the church. Many of these individuals have come forward through different support groups or websites discussing their homosexual attractions and concurrent church membership.
Groups within Mormonism.
Latter-day Saints.
Members of the LDS Church, also known as Latter-day Saints, constitute over 99 percent of Mormons.Also note the use of the lower case "d" and hyphen in "Latter-day Saints", as opposed to the larger "Latter Day Saint movement."</ref> The beliefs and practices of LDS Mormons are generally guided by the teachings of LDS Church leaders. There are, however, several smaller groups that differ from "mainstream" Mormonism in various ways.
LDS Church members who do not actively participate in worship services or church callings are often called "less-active" (akin to the qualifying expressions "non-observant" or "non-practicing" used in relation to members of other religious groups). The LDS Church does not release statistics on church activity, but it is likely that about 40 percent of Mormons in the United States and 30 percent worldwide regularly attend worship services. Reasons for inactivity can include lifestyle issues and problems with social integration. Activity rates tend to vary with age, and disengagement occurs most frequently between age 16 and 25. A majority of less active members return to church activity later in life. Former Latter-day Saints who seek to disassociate themselves from the religion are often referred to as ex-Mormons.
Fundamentalist Mormons.
Members of sects that broke with the LDS Church over the issue of polygamy have become known as fundamentalist Mormons; these groups differ from mainstream Mormonism primarily in their belief in and practice of plural marriage. There are thought to be between 20,000 and 60,000 members of fundamentalist sects, (0.1–0.4 percent of Mormons), with roughly half of them practicing polygamy. There are a number of fundamentalist sects, the largest two being the Fundamentalist Church of Jesus Christ of Latter-Day Saints (FLDS Church) and the Apostolic United Brethren (AUB). In addition to plural marriage, some of these groups also practice a form of Christian communalism known as the law of consecration or the United Order. The LDS Church seeks to distance itself from all such polygamous groups, excommunicating their members if discovered practicing or teaching it, and today a majority of Mormon fundamentalists have never been members of the LDS Church.
Liberal Mormons.
Liberal Mormons, also known as Progressive Mormons, take an interpretive approach to LDS teachings and scripture. They look to the scriptures for spiritual guidance, but do not necessarily believe the teachings to be literally or uniquely true. For liberal Mormons, revelation is a process through which God gradually brings fallible human beings to greater understanding. Liberal Mormons place doing good and loving fellow human beings above the importance of believing correctly. In a separate context, members of small progressive breakaway groups have also adopted the label.
Cultural Mormons.
Cultural Mormons are individuals who do not believe some (or many) of the doctrines of LDS Church, but who self-identify as Mormon. Usually this is a result of having been raised in the LDS faith, or as having converted and spent a large portion of one's life as an active member of the LDS Church. Cultural Mormons may or may not be actively involved with the church, and in some cases may not even be officially members of the church.
Beliefs.
Mormons have a scriptural canon consisting of the Bible (both Old and New Testaments), the Book of Mormon, and a collection of revelations and writings by Joseph Smith known as the Doctrine and Covenants and Pearl of Great Price. Mormons however have a relatively open definition of scripture. As a general rule, anything spoken or written by a prophet, while under inspiration, is considered to be the word of God. Thus, the Bible, written by prophets and apostles, is the word of God, so far as it is translated correctly. The Book of Mormon is also believed to have been written by ancient prophets, and is viewed as a companion to the Bible. By this definition, the teachings of Smith's successors are also accepted as scripture, though they are always measured against, and draw heavily from the scriptural canon.
Mormons believe in "a friendly universe", governed by a God whose aim it is to bring his children to immortality and eternal life. Mormons have a unique perspective on the nature of God, the origin of man, and the purpose of life. For instance, Mormons believe in a pre-mortal existence where people were literal spirit children of God, and that God presented a plan of salvation that would allow his children to progress and become more like him. The plan involved the spirits receiving bodies on earth and going through trials in order to learn, progress, and receive a "fulness of joy". The most important part of the plan involved Jesus, the eldest of God's children, coming to earth as the literal Son of God, to conquer sin and death so that God's other children could return. According to Mormons, every person who lives on earth will be resurrected, and nearly all of them will be received into various kingdoms of glory. To be accepted into the highest kingdom, a person must fully accept Christ through faith, repentance, and through ordinances such as baptism and the laying on of hands.
According to Mormons, a deviation from the original principles of Christianity, known as the Great Apostasy, began not long after the ascension of Jesus Christ. It was marked with the corruption of Christian doctrine by Greek and other philosophies, with followers dividing into different ideological groups. Mormons claim the martyrdom of the Apostles led to a loss of Priesthood authority to administer the church and its ordinances.
Mormons believe that God restored the early Christian church through Joseph Smith. In particular, Mormons believe that angels such as Peter, James, John, John the Baptist, Moses, and Elijah appeared to Smith and others and bestowed various priesthood authorities on them. Mormons believe that their church is the "only true and living church" because of the divine authority restored through Smith. Mormons self-identify as being Christian, while many Christians, particularly evangelical Protestants, disagree with this view. Mormons view other religions as having portions of the truth, doing good works, and having genuine value.
Though the LDS Church has a top-down hierarchical structure with a president–prophet dictating revelations for the whole church, there is a bottom-up aspect as well. Ordinary Mormons have access to the same inspiration that is thought to guide their prophets, and are encouraged to seek their own personal revelations. Mormons see Joseph Smith's first vision as proof that the heavens are open, and that God answers prayers. They place considerable emphasis on "asking God" to find out if something is true. Most Mormons do not claim to have had heavenly visions like Smith's in response to prayers, but feel that God talks to them in their hearts and minds through the Holy Ghost. Though Mormons have some beliefs that are considered strange in a modernized world, they continue to hold onto their beliefs because they feel God has spoken to them.

</doc>
