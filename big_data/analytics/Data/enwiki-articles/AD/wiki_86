<doc id="23660" url="https://en.wikipedia.org/wiki?curid=23660" title="Pierre Teilhard de Chardin">
Pierre Teilhard de Chardin

Pierre Teilhard de Chardin SJ ( (); 1 May 1881 – 10 April 1955) was a French idealist philosopher and Jesuit priest who trained as a paleontologist and geologist and took part in the discovery of Peking Man. He conceived the idea of the Omega Point (a maximum level of complexity and consciousness towards which he believed the universe was evolving) and developed Vladimir Vernadsky's concept of noosphere.
During his lifetime, many of Teilhard's writings were censored by the Catholic Church because of his views on original sin. However, Teilhard was praised by Pope Benedict XVI, and he was also noted for his contributions to theology in Pope Francis' 2015 encyclical "Laudato si"'.
Life.
Early years.
Pierre Teilhard de Chardin was born in the Château of Sarcenat at Orcines, close to Clermont-Ferrand, France, on May 1, 1881. On the Teilhard side he is descended from an ancient family of magistrates from Auvergne originating in Murat, Cantal, and on the de Chardin side he is descended from a family that was ennobled under Louis XVIII. He was the fourth of eleven children. His father, Emmanuel Teilhard (1844–1932), an amateur naturalist, collected stones, insects and plants and promoted the observation of nature in the household. Pierre Teilhard's spirituality was awakened by his mother, Berthe de Dompiere. When he was 12, he went to the Jesuit college of Mongré, in Villefranche-sur-Saône, where he completed baccalaureates of philosophy and mathematics. Then, in 1899, he entered the Jesuit novitiate at Aix-en-Provence, where he began a philosophical, theological and spiritual career.
As of the summer 1901, the Waldeck-Rousseau laws, which submitted congregational associations' properties to state control, prompted some of the Jesuits to exile themselves in the United Kingdom. Young Jesuit students continued their studies in Jersey. In the meantime, Teilhard earned a licentiate in literature in Caen in 1902.
Academic career.
From 1905 to 1908, he taught physics and chemistry in Cairo, Egypt, at the Jesuit College of the Holy Family. He wrote "... it is the dazzling of the East foreseen and drunk greedily ... in its lights, its vegetation, its fauna and its deserts." ("Letters from Egypt" (1905–1908) — "Éditions Aubier")
Teilhard studied theology in Hastings, in Sussex (United Kingdom), from 1908 to 1912. There he synthesized his scientific, philosophical and theological knowledge in the light of evolution. His reading of "L'Évolution Créatrice" (The Creative Evolution) by Henri Bergson was, he said, the "catalyst of a fire which devoured already its heart and its spirit." His views on evolution and religion particularly inspired the evolutionary biologist Theodosius Dobzhansky. Teilhard was ordained a priest on August 24, 1911, at age 30.
Paleontology.
From 1912 to 1914, Teilhard worked in the paleontology laboratory of the "Museum National d'Histoire Naturelle", in Paris, studying the mammals of the middle Tertiary period. Later he studied elsewhere in Europe. In June 1912 he formed part of the original digging team, with Arthur Smith Woodward and Charles Dawson at the Piltdown site, after the discovery of the first fragments of the (fraudulent) "Piltdown Man", with some even suggesting he participated in the hoax. Professor Marcellin Boule (specialist in Neanderthal studies), who so early as 1915 astutely recognised the non-hominid origins of the Piltdown finds, gradually guided Teilhard towards human paleontology. At the museum's Institute of Human Paleontology, he became a friend of Henri Breuil and took part with him, in 1913, in excavations in the prehistoric painted caves in the northwest of Spain, at the Cave of Castillo.
Service in World War I.
Mobilised in December 1914, Teilhard served in World War I as a stretcher-bearer in the 8th Moroccan Rifles. For his valour, he received several citations including the Médaille militaire and the Legion of Honour.
Throughout these years of war he developed his reflections in his diaries and in letters to his cousin, Marguerite Teillard-Chambon, who later edited them into a book: "Genèse d'une pensée" ("Genesis of a thought"). He confessed later: "...the war was a meeting ... with the Absolute." In 1916, he wrote his first essay: "La Vie Cosmique" ("Cosmic life"), where his scientific and philosophical thought was revealed just as his mystical life. He pronounced his solemn vows as a Jesuit in Sainte-Foy-lès-Lyon, on May 26, 1918, during a leave. In August 1919, in Jersey, he would write "Puissance spirituelle de la Matière" ("the spiritual Power of Matter"). The complete essays written between 1916 and 1919 are published under the following titles:
Teilhard followed at the Sorbonne three unit degrees of natural science: geology, botany and zoology. His thesis treated of the mammals of the French lower Eocene and their stratigraphy. After 1920, he lectured in geology at the Catholic Institute of Paris, then became an assistant professor after being granted a science doctorate in 1922.
Research in China.
In 1923, he travelled to China with Father Emile Licent, who was in charge in Tianjin of a significant laboratory collaboration between the Natural History Museum in Paris and Marcellin Boule's laboratory. Licent carried out considerable basic work in connection with missionaries who accumulated observations of a scientific nature in their spare time. He was known as 德日進 (pinyin: Dérìjìn) in China.
Teilhard wrote several essays, including "La Messe sur le Monde" (the "Mass on the World"), in the Ordos Desert. In the following year he continued lecturing at the Catholic Institute and participated in a cycle of conferences for the students of the Engineers' Schools. Two theological essays on Original Sin sent to a theologian at his request on a purely personal basis were wrongly understood.
The Church required him to give up his lecturing at the Catholic Institute and to continue his geological research in China.
Teilhard traveled again to China in April 1926. He would remain there more or less twenty years, with many voyages throughout the world. He settled until 1932 in Tientsin with Emile Licent then in Beijing. From 1926 to 1935, Teilhard made five geological research expeditions in China. They enabled him to establish a general geological map of China.
In 1926, Teilhard’s superiors in the Jesuit Order forbade him to teach any longer. In 1926–1927 after a missed campaign in Gansu, he traveled in the Sang-Kan-Ho valley near Kalgan (Zhangjiakou) and made a tour in Eastern Mongolia. He wrote "Le Milieu Divin" ("The Divine Milieu"). Teilhard prepared the first pages of his main work "Le Phénomène Humain" ("The Human Phenomenon"). The Holy See refused the Imprimatur for "Le Milieu Divin" in 1927.
He joined the ongoing excavations of the Peking Man Site at Zhoukoudian as an advisor in 1926 and continued in the role for the Cenozoic Research Laboratory of the Geological Survey of China following its founding in 1928.
He resided in Manchuria with Emile Licent, then stayed in Western Shansi (Shanxi) and northern Shensi (Shaanxi) with the Chinese paleontologist C. C. Young and with Davidson Black, Chairman of the Geological Survey of China.
After a tour in Manchuria in the area of Great Khingan with Chinese geologists, Teilhard joined the team of American Expedition Center-Asia in the Gobi Desert organised in June and July, by the American Museum of Natural History with Roy Chapman Andrews.
Henri Breuil and Teilhard discovered that the Peking Man, the nearest relative of "Pithecanthropus" from Java, was a "faber" (worker of stones and controller of fire). Teilhard wrote "L'Esprit de la Terre" ("the Spirit of the Earth").
Teilhard took part as a scientist in the Croisiere Jaune (Yellow Cruise) financed by André Citroën in Central Asia. Northwest of Beijing in Kalgan, he joined the Chinese group who joined the second part of the team, the Pamir group, in Aksu. He remained with his colleagues for several months in Urumqi, capital of Sinkiang. The following year the Sino-Japanese War (1937–1945) began.
In 1933, Rome ordered him to give up his post in Paris.
Teilhard undertook several explorations in the south of China. He traveled in the valleys of Yangtze River and Sichuan in 1934, then, the following year, in Kwang-If and Guangdong. The relationship with Marcellin Boule was disrupted; the museum cut its financing on the grounds that Teilhard worked more for the Chinese Geological Service than for the museum.
During all these years, Teilhard strongly contributed to the constitution of an international network of research in human paleontology related to the whole Eastern and south Eastern zone of the Asian continent. He would be particularly associated in this task with two friends, the English/Canadian Davidson Black and the Scot George B. Barbour. Many times he would visit France or the United States only to leave these countries to go on further expeditions.
World travels.
From 1927 to 1928, Teilhard stayed in France, based in Paris. He journeyed to Leuven, Belgium, to Cantal, and to Ariège, France. Between several articles in reviews, he met new people such as Paul Valéry and Bruno de Solages, who were to help him in issues with the Catholic Church.
Answering an invitation from Henry de Monfreid, Teilhard undertook a journey of two months in Obock, in Harrar and in Somalia with his colleague Pierre Lamarre, a geologist, before embarking in Djibouti to return to Tianjin. While in China, Teilhard developed a deep and personal friendship with Lucile Swan.
From 1930–1931, Teilhard stayed in France and in the United States. During a conference in Paris, Teilhard stated: "For the observers of the Future, the greatest event will be the sudden appearance of a collective humane conscience and a human work to make."
From 1932–1933, he began to meet people to clarify issues with the Congregation for the Doctrine of the Faith, regarding "Le Milieu divin" and "L'Esprit de la Terre". He met Helmut de Terra, a German geologist in the International Geology Congress in Washington, DC.
Teilhard participated in the 1935 Yale–Cambridge expedition in northern and central India with the geologist Helmut de Terra and Patterson, who verified their assumptions on Indian Paleolithic civilisations in Kashmir and the Salt Range Valley. He then made a short stay in Java, on the invitation of Professor Ralph van Koenigswald to the site of Java man. A second cranium, more complete, was discovered. This Dutch paleontologist had found (in 1933) a tooth in a Chinese apothecary shop in 1934 that he believed belonged to a giant tall ape that lived around half a million years ago.
In 1937, Teilhard wrote "Le Phénomène spirituel" ("The Phenomenon of the Spirit") on board the boat "the Empress of Japan", where he met the Raja of Sarawak. The ship conveyed him to the United States. He received the Mendel Medal granted by Villanova University during the Congress of Philadelphia in recognition of his works on human paleontology. He made a speech about evolution, origins and the destiny of Man. The "New York Times" dated March 19, 1937 presented Teilhard as the Jesuit who held that man descended from monkeys. Some days later, he was to be granted the "Doctor Honoris Causa" distinction from Boston College. Upon arrival in that city, he was told that the award had been cancelled.
1939: Rome banned his work "L’Énergie Humaine".
He then stayed in France, where he was immobilized by malaria. During his return voyage to Beijing he wrote "L'Energie spirituelle de la Souffrance" ("Spiritual Energy of Suffering") (Complete Works, tome VII).
1941: Teilhard submitted to Rome his most important work, "Le Phénomène Humain".
1947: Rome forbade him to write or teach on philosophical subjects.
1948: Teilhard was called to Rome by the Superior General of the Jesuits who hoped to acquire permission from the Holy See for the publication of his most important work "Le Phénomène Humain". But the prohibition to publish it issued in 1944, was again renewed. Teilhard was also forbidden to take a teaching post in the College de France.
1949: Permission to publish "Le Groupe Zoologique" was refused.
1950: Teilhard was named to the French Academy of Sciences.
1955: Teilhard was forbidden by his Superiors to attend the International Congress of
Paleontology.
1957: The Supreme Authority of the Holy Office, in a decree dated 15 November 1957, forbade the works of de Chardin to be retained in libraries, including those of religious institutes. His books were not to be sold in Catholic bookshops and were not to be translated into other languages.
1958: In April of this year, all Jesuit publications in Spain ("Razón y Fe", "Sal Terrae","Estudios de Deusto") etc., carried a notice from the Spanish Provincial of the Jesuits that de Chardin’s works had been published in Spanish without previous ecclesiastical examination and in defiance of the decrees of the Holy See.
1962: A decree of the Holy Office dated 30 June, under the authority of Pope John XXIII warned that "... it is obvious that in philosophical and theological matters, the said works [Teilhard’s] are replete with ambiguities or rather with serious errors which offend Catholic doctrine. That is why... the Rev. Fathers of the Holy Office urge all Ordinaries, Superiors, and Rectors... to effectively protect, especially the minds of the young, against the dangers of the works of Fr. Teilhard de Chardin and his followers". (AAS, 6 August 1962).
1963: The Vicariate of Rome (a diocese ruled in the name of Pope Paul VI (who had just become Pope in 1963) by his Cardinal Vicar) in a decree dated 30 September, required that Catholic booksellers in Rome should withdraw from circulation the works of Teilhard, together with those books which favour his erroneous doctrines. The text of this document was published in daily "L’Aurore" of Paris, dated 2 October 1963, and was reproduced in "Nouvelles De Chrétienté", 10 October 1963, p. 35.
Death.
Pierre Teilhard de Chardin died in New York City, where he was in residence at the Jesuit Church of St. Ignatius Loyola, Park Avenue. On March 15, 1955, at the house of his diplomat cousin Jean de Lagarde, Teilhard told friends he hoped he would die on Easter Sunday. In the Easter Sunday evening of April 10, 1955, during an animated discussion at the apartment of Rhoda de Terra, his personal assistant since 1949, the 73-year-old priest suffered a heart attack; regaining consciousness for a moment, he died a few minutes later. He was buried in the cemetery for the New York Province of the Jesuits at the Jesuit novitiate, St. Andrew's-on-the-Hudson in Poughkeepsie, upstate New York.
Teachings.
Teilhard de Chardin has two comprehensive works, "The Phenomenon of Man", and "The Divine Milieu".
His posthumously published book, "The Phenomenon of Man", sets forth a sweeping account of the unfolding of the cosmos and the evolution of matter to humanity, to ultimately a reunion with Christ. In the book, Chardin abandoned literal interpretations of creation in the Book of Genesis in favor of allegorical and theological interpretations. The unfolding of the material cosmos, is described from primordial particles to the development of life, human beings and the noosphere, and finally to his vision of the Omega Point in the future, which is "pulling" all creation towards it. He was a leading proponent of orthogenesis, the idea that evolution occurs in a directional, goal-driven way, argued in terms that today go under the banner of convergent evolution. Teilhard argued in Darwinian terms with respect to biology, and supported the synthetic model of evolution, but argued in Lamarckian terms for the development of culture, primarily through the vehicle of education. Teilhard made a total commitment to the evolutionary process in the 1920s as the core of his spirituality, at a time when other religious thinkers felt evolutionary thinking challenged the structure of conventional Christian faith. He committed himself to what the evidence showed.
Teilhard makes sense of the universe by its evolutionary process. He interprets complexity as the axis of evolution of matter into a geosphere, a biosphere, into consciousness (in man), and then to supreme consciousness (the Omega Point.)
Teilhard’s unique relationship to both paleontology and Catholicism allowed him to develop a highly progressive, cosmic theology which takes into account his evolutionary studies. Teilhard recognized the importance of bringing the Church into the modern world, and approached evolution as a way of providing ontological meaning for Christianity, particularly creation theology. For Teilhard, evolution was "the natural landscape where the history of salvation is situated."
Teilhard’s cosmic theology is largely predicated on his interpretation of Pauline scripture, particularly Colossians 1:15-17 (especially verse 1:17b) and 1Corinthians 15:28. Teilhard draws on the Christocentrism of these two Pauline passages to construct a cosmic theology which recognizes the absolute primacy of Christ. He understands creation to be "a teleological process towards union with the Godhead, effected through the incarnation and redemption of Christ, ‘in whom all things hold together’ (Col. 1:17)." He further posits that creation will not be complete until "participated being is totally united with God through Christ in the Pleroma, when God will be ‘all in all’ (1Cor. 15:28)." 
Teilhard's life work was predicated on the conviction that human spiritual development is moved by the same universal laws as material development. He wrote, "...everything is the sum of the past" and "...nothing is comprehensible except through its history. 'Nature' is the equivalent of 'becoming', self-creation: this is the view to which experience irresistibly leads us. ... There is nothing, not even the human soul, the highest spiritual manifestation we know of, that does not come within this universal law." There is no doubt that "The Phenomenon of Man" represents Teilhard's attempt at reconciling his religious faith with his academic interests as a paleontologist. One particularly poignant observation in Teilhard's book entails the notion that evolution is becoming an increasingly optional process. Teilhard points to the societal problems of isolation and marginalization as huge inhibitors of evolution, especially since evolution requires a unification of consciousness. He states that "no evolutionary future awaits anyone except in association with everyone else." Teilhard argued that the human condition necessarily leads to the psychic unity of humankind, though he stressed that this unity can only be voluntary; this voluntary psychic unity he termed "unanimization." Teilhard also states that "evolution is an ascent toward consciousness", giving encephalization as an example of early stages, and therefore, signifies a continuous upsurge toward the Omega Point, which for all intents and purposes, is God.
Teilhard also used his perceived correlation between spiritual and material to describe Christ, arguing that Christ not only has a mystical dimension, but also takes on a physical dimension as he becomes the organizing principle of the universe—that is, the one who "holds together" the universe (Col. 1:17b). For Teilhard, Christ forms not only the eschatological end toward which his mystical/ecclesial body is oriented, but he also "operates physically in order to regulate all things" becoming "the one from whom all creation receives its stability." In other words, as the one who holds all things together, "Christ exercises a supremacy over the universe which is physical, not simply juridical. He is the unifying centre of the universe and its goal. The function of holding all things together indicates that Christ is not only man and God; he also possesses a third aspect—indeed, a third nature—which is cosmic." In this way, the Pauline description of the Body of Christ is not simply a mystical or ecclesial concept for Teilhard; it is cosmic. This cosmic Body of Christ "extendthroughout the universe and compris[es all things that attain their fulfillment in Christ that . . . the Body of Christ is the one single thing that is being made in creation." Teilhard describes this cosmic amassing of Christ as "Christogenesis." According to Teilhard, the universe is engaged in Christogenesis as it evolves toward its full realization at Omega, a point which coincides with the fully realized Christ. It is at this point that God will be ‘all in all’ (1Cor. 15:28c).
Relationship with the Catholic Church.
In 1925, Teilhard was ordered by the Jesuit Superior General Wlodimir Ledóchowski to leave his teaching position in France and to sign a statement withdrawing his controversial statements regarding the doctrine of original sin. Rather than leave the Jesuit order, Teilhard signed the statement and left for China.
This was the first of a series of condemnations by certain ecclesiastical officials that would continue until after Teilhard's death. The climax of these condemnations was a 1962 monitum (reprimand) of the Holy Office cautioning on Teilhard's works. It said in part:
The Holy Office did not place any of Teilhard's writings on the Index Librorum Prohibitorum (Index of Forbidden Books), which existed during Teilhard's lifetime and at the time of the 1962 decree.
Shortly thereafter, prominent clerics mounted a strong theological defense of Teilhard's works. Henri de Lubac (later a Cardinal) wrote three comprehensive books on the theology of Teilhard de Chardin in the 1960s. While de Lubac mentioned that Teilhard was less than precise in some of his concepts, he affirmed the orthodoxy of Teilhard de Chardin and responded to Teilhard's critics: "We need not concern ourselves with a number of detractors of Teilhard, in whom emotion has blunted intelligence". Later that decade Joseph Ratzinger, a German theologian who became Pope Benedict XVI, spoke glowingly of Teilhard's Christology in Ratzinger's "Introduction to Christianity":
Over the next several decades prominent theologians and Church leaders, including leading Cardinals, Pope John Paul II and Pope Benedict XVI all wrote approvingly of Teilhard's ideas. In 1981, Cardinal Agostino Casaroli, on behalf of Pope John Paul II, wrote on the front page of the Vatican newspaper, "l'Osservatore Romano":
Cardinal Avery Dulles, S.J. said in 2004:
Cardinal Christoph Schönborn wrote in 2007:
Pope Benedict XVI, in his book "Spirit of the Liturgy" incorporates Teilhard's vision as a touchstone of the Catholic Mass:
in July 2009, Vatican spokesman Fr. Federico Lombardi said, "By now, no one would dream of saying that is a heterodox author who shouldn’t be studied."
Pope Francis refers to Teilhard's eschatological contribution in his encyclical Laudato si'.
Evaluations by scientists.
Sir Julian Huxley, evolutionary biologist and contributor to the modern synthesis, praised the thought of Teilhard de Chardin for looking at the way in which human development needs to be examined within a larger integrated universal sense of evolution.
In 1961, the Nobel Prize-winner Peter Medawar, a British immunologist, wrote a scornful review of "The Phenomenon Of Man" for the journal "Mind", calling it "a bag of tricks" and saying that the author had shown "an active willingness to be deceived": "the greater part of it, I shall show, is nonsense, tricked out with a variety of metaphysical conceits, and its author can be excused of dishonesty only on the grounds that before deceiving others he has taken great pains to deceive himself".
The evolutionary biologist Richard Dawkins called Medawar's review "devastating" and "The Phenomenon of Man" "the quintessence of bad poetic science". Similarly, Steven Rose wrote that "Teilhard is revered as a mystic of genius by some, but amongst most biologists is seen as little more than a charlatan."
David Sloan Wilson, professor of Biology and Anthropology, said Teilhard was "ahead of his time scientifically".
"Nothing in Biology Makes Sense Except in the Light of Evolution" by Theodosius Dobzhansky draws upon Teilhard's insistence that evolutionary theory provides the core of how man understands his relationship to nature, calling him "one of the great thinkers of our age". Key researchers credit Teilhard with the development of the modern evolutionary synthesis that accounts for natural selection in the light of Mendelian genetics.
Legacy.
Brian Swimme wrote "Teilhard was one of the first scientists to realize that the human and the universe are inseparable. The only universe we know about is a universe that brought forth the human." 
Pierre Teilhard de Chardin is honored with a feast day on the liturgical calendar of the Episcopal Church (USA) on April 10. George Gaylord Simpson named the most primitive and ancient genus of true primate, the Eocene genus "Teilhardina".
Teilhard and his work continue to influence the arts and culture. Characters based on Teilhard appear in several novels, including Jean Telemond in Morris West's "The Shoes of the Fisherman" (mentioned by name and quoted by Oskar Werner playing Fr. Telemond in the movie version of the novel) and Father Lankester Merrin in William Peter Blatty's "The Exorcist". In Dan Simmons' 1989–97 "Hyperion Cantos", Teilhard de Chardin has been canonized a saint in the far future. His work inspires the anthropologist priest character, Paul Duré. When Duré becomes Pope, he takes "Teilhard I" as his regnal name. Teilhard appears as a minor character in the play "Fake" by Eric Simonson, staged by Chicago's Steppenwolf Theatre Company in 2009, involving a fictional solution to the infamous Piltdown Man hoax.
References range from occasional quotations—an auto mechanic quotes Teilhard in Philip K. Dick's "A Scanner Darkly" – to serving as the philosophical underpinning of the plot, as Teilhard's work does in Julian May's 1987–94 Galactic Milieu Series. Teilhard also plays a major role in Annie Dillard's 1999 "For the Time Being". Teilhard is mentioned by name and the Omega Point briefly explained in Arthur C. Clarke's and Stephen Baxter's "The Light of Other Days".
The title of the short-story collection "Everything That Rises Must Converge" by Flannery O'Connor is a reference to Teilhard's work. The American novelist Don DeLillo's 2010 novel "Point Omega" borrows its title and some of its ideas from Teilhard de Chardin. Robert Wright, in his book "", compares his own naturalistic thesis that biological and cultural evolution are directional and, possibly, purposeful, with Teilhard's ideas.
Teilhard's work also inspired philosophical ruminations by Italian laureate architect Paolo Soleri, artworks such as French painter Alfred Manessier's "L'Offrande de la terre ou Hommage à Teilhard de Chardin" and American sculptor Frederick Hart's acrylic sculpture "The Divine Milieu: Homage to Teilhard de Chardin". A sculpture of the Omega Point by Henry Setter, with a quote from Teilhard de Chardin, can be found at the entrance to the Roesch Library at the University of Dayton. Edmund Rubbra's 1968 Symphony No. 8 is titled "Hommage à Teilhard de Chardin".
Several college campuses honor Teilhard. A building at the University of Manchester is named after him, as are residence dormitories at Gonzaga University and Seattle University.
"The De Chardin Project", a play celebrating Teilhard's life, ran from November 20 to December 14, 2014 in Toronto, Canada. "The Evolution of Teilhard de Chardin", a documentary film on Teilhard's life, is expected to be released in 2015.
Influence on the New Age movement.
Teihard has had a profound influence on the New Age movement and has been described as "perhaps the man most responsible for the spritiualization of evolution in a global and cosmic context". New Age figure and self-described evolutionary biologist Jeremy Griffith described Teilhard as a "visionary" philosopher and a contemporary "truth-sayer" or "prophet".
Bibliography.
The dates in parentheses are the dates of first publication in French and English. Most of these works were written years earlier, but Teilhard's ecclesiastical order forbade him to publish them because of their controversial nature. The essay collections are organized by subject rather than date, thus each one typically spans many years.

</doc>
<doc id="23661" url="https://en.wikipedia.org/wiki?curid=23661" title="Phutball">
Phutball

Phutball (short for Philosopher's Football) is a two-player strategy board game described in Elwyn Berlekamp, John Horton Conway, and Richard K. Guy's "Winning Ways for your Mathematical Plays".
Rules.
Phutball is played on the intersections of a 19×15 grid using one white stone and as many black stones as needed. 
In this article the two players are named Ohs (O) and Eks (X).
The board is labeled A through P (omitting I) from left to right and 1 to 19 from bottom to top from Ohs' perspective. 
Rows 0 and 20 represent "off the board" beyond rows 1 and 19 respectively.
As specialized phutball boards are hard to come by, the game is usually played on a 19×19 Go board, with a white stone representing the football and black stones representing the men.
The objective is to score goals by using the men (the black stones) to move the football (the white stone) onto or over the opponent's goal line. Ohs tries to move the football to rows 19 or 20 and Eks to rows 1 or 0. 
At the start of the game the football is placed on the central point, unless one player gives the other a handicap, in which case the ball starts nearer one player's goal.
Players alternate making moves. 
A move is either to add a man to any vacant point on the board or to move the ball. 
There is no difference between men played by Ohs and those played by Eks.
The football is moved by a series of jumps over adjacent men. 
Each jump is to the first vacant point in a straight line horizontally, vertically, or diagonally over one or more men. 
The jumped men are then removed from the board (before any subsequent jump occurs). 
This process repeats for as long as there remain men available to be jumped and the player desires. Jumping is optional: there is no requirement to jump. 
In contrast to checkers, multiple men in a row are jumped and removed as a group.
The diagram on the right illustrates a jump. 
If the football ends the move on or over the opponent's goal line then a goal has been scored. 
If the football passes through a goal line, but ends up elsewhere due to further jumps, the game continues.
Strategy.
The game is sufficiently complex that checking whether there is a win in one (on an m×n board) is NP-complete. It is not known whether any player has a winning strategy or both players have a drawing strategy.

</doc>
<doc id="23664" url="https://en.wikipedia.org/wiki?curid=23664" title="Papyrus">
Papyrus

The word papyrus refers to a thick type of paper made from the pith of the papyrus plant, "Cyperus papyrus". "Papyrus" can also refer to a document written on sheets of papyrus joined together side by side and rolled up into a scroll, an early form of a book. The plural for such documents is papyri.
Papyrus is first known to have been used in ancient Egypt (at least as far back as the First Dynasty), as the "Cyperus papyrus" plant was a wetland sedge that was once abundant in the Sudd of Southern Sudan along with the Nile Delta of Egypt. 
Papyrus was also used throughout the Mediterranean region and in Kingdom of Kush. The Ancient Egyptians used papyrus as a writing material, as well as employing it commonly in the construction of other artifacts such as reed boats, mats, rope, sandals, and baskets.
History.
Papyrus was first manufactured in Egypt as far back as the fourth millennium BCE. The earliest archaeological evidence of papyrus was excavated in 2012 and 2013 at Wadi al-Jarf, an ancient Egyptian harbor located on the Red Sea coast. These documents date from c. 2560–2550 BCE (end of the reign of Khufu). The papyrus rolls describe the last years of building the Great Pyramid of Giza. In the first centuries BCE and CE, papyrus scrolls gained a rival as a writing surface in the form of parchment, which was prepared from animal skins. Sheets of parchment were folded to form quires from which book-form codices were fashioned. Early Christian writers soon adopted the codex form, and in the Græco-Roman world, it became common to cut sheets from papyrus rolls to form codices.
Codices were an improvement on the papyrus scroll, as the papyrus was not pliable enough to fold without cracking and a long roll, or scroll, was required to create large-volume texts. Papyrus had the advantage of being relatively cheap and easy to produce, but it was fragile and susceptible to both moisture and excessive dryness. Unless the papyrus was of perfect quality, the writing surface was irregular, and the range of media that could be used was also limited.
Papyrus was replaced in Europe by the cheaper, locally produced products parchment and vellum, of significantly higher durability in moist climates, though Henri Pirenne's connection of its disappearance with the Muslim conquest of Egypt is contested. Its last appearance in the Merovingian chancery is with a document of 692, though it was known in Gaul until the middle of the following century. The latest certain dates for the use of papyrus are 1057 for a papal decree (typically conservative, all papal bulls were on papyrus until 1022), under Pope Victor II, and 1087 for an Arabic document. Its use in Egypt continued until it was replaced by more inexpensive paper introduced by Arabs who originally learned of it from the Chinese. By the 12th century, parchment and paper were in use in the Byzantine Empire, but papyrus was still an option.
Papyrus was made in several qualities and prices; these are listed, with minor differences, both by Pliny and Isidore of Seville.
Until the middle of the 19th century, only some isolated documents written on papyrus were known. They did not contain literary works. The first discovery of papyri rolls in modern days was made at Herculaneum in 1752. Before that date, the only papyri known were a few surviving from medieval times.
Etymology.
The English word "papyrus" derives, via Latin, from Greek πάπυρος ("papuros"), a loanword of unknown (perhaps Pre-Greek) origin. Greek has a second word for it, βύβλος ("bublos", said to derive from the name of the Phoenician city of Byblos). The Greek writer Theophrastus, who flourished during the 4th century BC, uses "papuros" when referring to the plant used as a foodstuff and "bublos" for the same plant when used for nonfood products, such as cordage, basketry, or writing surfaces. The more specific term βίβλος "biblos", which finds its way into English in such words as 'bibliography', 'bibliophile', and 'bible', refers to the inner bark of the papyrus plant. "Papyrus" is also the etymon of 'paper', a similar substance.
In the Egyptian language, papyrus was called "wadj" ("w3ḏ"), "tjufy" ("ṯwfy"), or "djet" ("ḏt").
Documents written on papyrus.
The word for the material papyrus is also used to designate documents written on sheets of it, often rolled up into scrolls. The plural for such documents is papyri. Historical papyri are given identifying names—generally the name of the discoverer, first owner or institution where they are kept—and numbered, such as "Papyrus Harris I". Often an abbreviated form is used, such as "pHarris I". These documents provide important information on ancient writings; they give us the only extant copy of Menander, the Egyptian Book of the Dead, Egyptian treatises on medicine (the Ebers Papyrus) and on surgery (the Edwin Smith papyrus), Egyptian mathematical treatises (the Rhind papyrus), and Egyptian folk tales (the Westcar papyrus). When, in the 18th century, a library of ancient papyri was found in Herculaneum, ripples of expectation spread among the learned men of the time. However, since these papyri were badly charred, their unscrolling and deciphering is still going on today.
Manufacture and use.
Papyrus is made from the stem of the papyrus plant, "Cyperus papyrus". The outer rind is first removed, and the sticky fibrous inner pith is cut lengthwise into thin strips of about long. The strips are then placed side by side on a hard surface with their edges slightly overlapping, and then another layer of strips is laid on top at a right angle. The strips may have been soaked in water long enough for decomposition to begin, perhaps increasing adhesion, but this is not certain. The two layers possibly were glued together. While still moist, the two layers are hammered together, mashing the layers into a single sheet. The sheet is then dried under pressure. After drying, the sheet is polished with some rounded object, possibly a stone or seashell or round hardwood.
Sheets could be cut to fit the obligatory size or glued together to create a longer roll. A wooden stick would be attached to the last sheet in a roll, making it easier to handle. To form the long strip scrolls required, a number of such sheets were united, placed so all the horizontal fibres parallel with the roll's length were on one side and all the vertical fibres on the other. Normally, texts were first written on the "recto", the lines following the fibres, parallel to the long edges of the scroll. Secondarily, papyrus was often reused, writing across the fibres on the "verso". Pliny the Elder describes the methods of preparing papyrus in his "Naturalis Historia".
In a dry climate, like that of Egypt, papyrus is stable, formed as it is of highly rot-resistant cellulose; but storage in humid conditions can result in molds attacking and destroying the material. Library papyrus rolls were stored in wooden boxes and chests made in the form of statues. Papyrus scrolls were organized according to subject or author, and identified with clay labels that specified their contents without having to unroll the scroll. In European conditions, papyrus seems to have lasted only a matter of decades; a 200-year-old papyrus was considered extraordinary. Imported papyrus once commonplace in Greece and Italy has since deteriorated beyond repair, but papyrus is still being found in Egypt; extraordinary examples include the Elephantine papyri and the famous finds at Oxyrhynchus and Nag Hammadi. The Villa of the Papyri at Herculaneum, containing the library of Lucius Calpurnius Piso Caesoninus, Julius Caesar's father-in-law, was preserved by the eruption of Mount Vesuvius, but has only been partially excavated.
Sporadic attempts to revive the manufacture of papyrus have been made since the mid-18th century. Scottish explorer James Bruce experimented in the late 18th century with papyrus plants from the Sudan, for papyrus had become extinct in Egypt. Also in the 18th century, Sicilian Saverio Landolina manufactured papyrus at Syracuse, where papyrus plants had continued to grow in the wild. During the 1920s, when Egyptologist Battiscombe Gunn lived in Maadi, outside Cairo, he experimented with the manufacture of papyrus, growing the plant in his garden. He beat the sliced papyrus stalks between two layers of linen, and produced successful examples of papyrus, one of which was exhibited in the Egyptian Museum in Cairo. The modern technique of papyrus production used in Egypt for the tourist trade was developed in 1962 by the Egyptian engineer Hassan Ragab using plants that had been reintroduced into Egypt in 1872 from France. Both Sicily and Egypt have centres of limited papyrus production.
Papyrus is still used by communities living in the vicinity of swamps, to the extent that rural householders derive up to 75% of their income from swamp goods. Particularly in East and Central Africa, people harvest papyrus, which is used to manufacture items that are sold or used locally. Examples include baskets, hats, fish traps, trays or winnowing mats and floor mats. Papyrus is also used to make roofs, ceilings, rope and fences. Although alternatives, such as eucalyptus, are increasingly available, papyrus is still used as fuel.
See also.
Other ancient writing materials:

</doc>
<doc id="23665" url="https://en.wikipedia.org/wiki?curid=23665" title="Pixel">
Pixel

In digital imaging, a pixel, pel, dots or picture element is a physical point in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen. The address of a pixel corresponds to its physical coordinates. LCD pixels are manufactured in a two-dimensional grid, and are often represented using dots or squares, but CRT pixels correspond to their timing mechanisms and sweep rates.
Each pixel is a sample of an original image; more samples typically provide more accurate representations of the original. The intensity of each pixel is variable. In color image systems, a color is typically represented by three or four component intensities such as red, green, and blue, or cyan, magenta, yellow, and black.
In some contexts (such as descriptions of camera sensors), the term "pixel" is used to refer to a single scalar element of a multi-component representation (more precisely called a "photosite" in the camera sensor context, although the neologism "sensel" is sometimes used to describe the elements of a digital camera's sensor), while in others the term may refer to the entire set of such component intensities for a spatial position. In color systems that use chroma subsampling, the multi-component concept of a pixel can become difficult to apply, since the intensity measures for the different color components correspond to different spatial areas in such a representation.
The word "pixel" is based on a contraction of "pix" (from word "pictures", where it is shortened to "pics", and "cs" in "pics" sounds like "x") and "el" (for "element"); similar formations with "el"  for "element" include the words "voxel", "texel" and "maxel" (for "magnetic pixel").
Etymology.
The word "pixel" was first published in 1965 by Frederic C. Billingsley of JPL, to describe the picture elements of video images from space probes to the Moon and Mars. However, Billingsley did not coin the term himself. Instead, he got the word "pixel" from Keith E. McFarland, at the Link Division of General Precision in Palo Alto, who did not know where the word originated. McFarland said simply it was "in use at the time" (circa 1963).
The word is a combination of "pix", for picture, and "element". The word "pix" appeared in "Variety" magazine headlines in 1932, as an abbreviation for the word "pictures", in reference to movies. By 1938, "pix" was being used in reference to still pictures by photojournalists.
The concept of a "picture element" dates to the earliest days of television, for example as ""Bildpunkt"" (the German word for "pixel", literally 'picture point') in the 1888 German patent of Paul Nipkow. According to various etymologies, the earliest publication of the term "picture element" itself was in "Wireless World" magazine in 1927, though it had been used earlier in various U.S. patents filed as early as 1911.
Some authors explain "pixel" as "picture cell," as early as 1972. In graphics and in image and video processing, "pel" is often used instead of "pixel". For example, IBM used it in their Technical Reference for the original PC.
Pixilation, spelled with a second "i", is an unrelated filmmaking technique that dates to the beginnings of cinema, in which live actors are posed frame by frame and photographed to create stop-motion animation. An archaic British word meaning "possession by spirits (pixies)," the term has been used to describe the animation process since the early 1950s; various animators, including Norman McLaren and Grant Munro, are credited with popularizing it.
Technical.
The measures dots per inch (dpi) and pixels per inch (ppi) are sometimes used interchangeably, but have distinct meanings, especially for printer devices, where dpi is a measure of the printer's density of dot (e.g. ink droplet) placement. For example, a high-quality photographic image may be printed with 600 ppi on a 1200 dpi inkjet printer. Even higher dpi numbers, such as the 4800 dpi quoted by printer manufacturers since 2002, do not mean much in terms of achievable resolution.
The more pixels used to represent an image, the closer the result can resemble the original. The number of pixels in an image is sometimes called the resolution, though resolution has a more specific definition. Pixel counts can be expressed as a single number, as in a "three-megapixel" digital camera, which has a nominal three million pixels, or as a pair of numbers, as in a "640 by 480 display", which has 640 pixels from side to side and 480 from top to bottom (as in a VGA display), and therefore has a total number of 640×480 = 307,200 pixels or 0.3 megapixels.
The pixels, or color samples, that form a digitized image (such as a JPEG file used on a web page) may or may not be in one-to-one correspondence with screen pixels, depending on how a computer displays an image. In computing, an image composed of pixels is known as a "bitmapped image" or a "raster image". The word "raster" originates from television scanning patterns, and has been widely used to describe similar halftone printing and storage techniques.
Sampling patterns.
For convenience, pixels are normally arranged in a regular two-dimensional grid. By using this arrangement, many common operations can be implemented by uniformly applying the same operation to each pixel independently. Other arrangements of pixels are possible, with some sampling patterns even changing the shape (or kernel) of each pixel across the image. For this reason, care must be taken when acquiring an image on one device and displaying it on another, or when converting image data from one pixel format to another.
For example:
Resolution of computer monitors.
Computers can use pixels to display an image, often an abstract image that represents a GUI. The resolution of this image is called the display resolution and is determined by the video card of the computer. LCD monitors also use pixels to display an image, and have a native resolution. Each pixel is made up of triads, with the number of these triads determining the native resolution. On some CRT monitors, the beam sweep rate may be fixed, resulting in a fixed native resolution. Most CRT monitors do not have a fixed beam sweep rate, meaning they do not have a native resolution at all - instead they have a set of resolutions that are equally well supported.
To produce the sharpest images possible on an LCD, the user must ensure the display resolution of the computer matches the native resolution of the monitor.
Resolution of telescopes.
The pixel scale used in astronomy is the angular distance between two objects on the sky that fall one pixel apart on the detector (CCD or infrared chip). The scale "s" measured in radians is the ratio of the pixel spacing "p" and focal length "f" of the preceding optics, "s"="p/f". (The focal length is the product of the focal ratio by the diameter of the associated lens or mirror.)
Because "p" is usually expressed in units of arcseconds per pixel, because 1 radian equals "180/π*3600≈206,265" arcseconds, and because diameters are often given in millimeters and pixel sizes in micrometers which yields another factor of 1,000, the formula is often quoted as "s=206p/f".
Bits per pixel.
The number of distinct colors that can be represented by a pixel depends on the number of bits per pixel (bpp). A 1 bpp image uses 1-bit for each pixel, so each pixel can be either on or off. Each additional bit doubles the number of colors available, so a 2 bpp image can have 4 colors, and a 3 bpp image can have 8 colors:
For color depths of 15 or more bits per pixel, the depth is normally the sum of the bits allocated to each of the red, green, and blue components. Highcolor, usually meaning 16 bpp, normally has five bits for red and blue, and six bits for green, as the human eye is more sensitive to errors in green than in the other two primary colors. For applications involving transparency, the 16 bits may be divided into five bits each of red, green, and blue, with one bit left for transparency. A 24-bit depth allows 8 bits per component. On some systems, 32-bit depth is available: this means that each 24-bit pixel has an extra 8 bits to describe its opacity (for purposes of combining with another image).
Subpixels.
Many display and image-acquisition systems are, for various reasons, not capable of displaying or sensing the different color channels at the same site. Therefore, the pixel grid is divided into single-color regions that contribute to the displayed or sensed color when viewed at a distance. In some displays, such as LCD, LED, and plasma displays, these single-color regions are separately addressable elements, which have come to be known as subpixels. For example, LCDs typically divide each pixel horizontally into three subpixels. When the square pixel is divided into three subpixels, each subpixel is necessarily rectangular. In display industry terminology, subpixels are often referred to as "pixels", as they are the basic addressable elements in a viewpoint of hardware, and hence "pixel circuits" rather than "subpixel circuits" is used.
Most digital camera image sensors use single-color sensor regions, for example using the Bayer filter pattern, and in the camera industry these are known as "pixels" just like in the display industry, not "subpixels".
For systems with subpixels, two different approaches can be taken:
This latter approach, referred to as subpixel rendering, uses knowledge of pixel geometry to manipulate the three colored subpixels separately, producing an increase in the apparent resolution of color displays. While CRT displays use red-green-blue-masked phosphor areas, dictated by a mesh grid called the shadow mask, it would require a difficult calibration step to be aligned with the displayed pixel raster, and so CRTs do not currently use subpixel rendering.
The concept of subpixels is related to samples.
Megapixel.
A megapixel (MP) is a million pixels; the term is used not only for the number of pixels in an image, but also to express the number of image sensor elements of digital cameras or the number of display elements of digital displays. For example, a camera that makes a 2048×1536 pixel image (3,145,728 finished image pixels) typically uses a few extra rows and columns of sensor elements and is commonly said to have "3.2 megapixels" or "3.4 megapixels", depending on whether the number reported is the "effective" or the "total" pixel count.
Digital cameras use photosensitive electronics, either charge-coupled device (CCD) or complementary metal–oxide–semiconductor (CMOS) image sensors, consisting of a large number of single sensor elements, each of which records a measured intensity level. In most digital cameras, the sensor array is covered with a patterned color filter mosaic having red, green, and blue regions in the Bayer filter arrangement, so that each sensor element can record the intensity of a single primary color of light. The camera interpolates the color information of neighboring sensor elements, through a process called demosaicing, to create the final image. These sensor elements are often called "pixels", even though they only record 1 channel (only red, or green, or blue) of the final color image. Thus, two of the three color channels for each sensor must be interpolated and a so-called "N-megapixel" camera that produces an N-megapixel image provides only one-third of the information that an image of the same size could get from a scanner. Thus, certain color contrasts may look fuzzier than others, depending on the allocation of the primary colors (green has twice as many elements as red or blue in the Bayer arrangement).
DxO Labs invented the Perceptual MegaPixel (P-MPix) to measure the sharpness that a camera produces when paired to a particular lens – as opposed to the MP a manufacturer states for a camera product which is based only on the camera's sensor. The new P-MPix claims to be a more accurate and relevant value for photographers to consider when weighing-up camera sharpness. As of mid-2013, the Sigma 35mm F1.4 DG HSM mounted on a Nikon D800 has the highest measured P-MPix. However, with a value of 23 MP, it still wipes-off more than one-third of the D800's 36.3 MP sensor.
A camera with a full-frame image sensor, and a camera with an APS-C image sensor, may have the same pixel count (for example, 16 MP), but the full-frame camera may allow better dynamic range, less noise, and improved low-light shooting performance than an APS-C camera. This is because the full-frame camera has a larger image sensor than the APS-C camera, therefore more information can be captured per pixel. A full-frame camera that shoots photographs at 36 megapixels has roughly the same pixel size as an APS-C camera that shoots at 16 megapixels.
One new method to add Megapixels has been introduced in a Micro Four Thirds System camera which only uses 16MP sensor, but can produce 64MP RAW (40MP JPEG) by expose-shift-expose-shift the sensor a half pixel each time to both directions. Using a tripod to take level multi-shots within an instance, the multiple 16MP images are then generated into a unified 64MP image.

</doc>
<doc id="23666" url="https://en.wikipedia.org/wiki?curid=23666" title="Prime number">
Prime number

A prime number (or a prime) is a natural number greater than 1 that has no positive divisors other than 1 and itself. A natural number greater than 1 that is not a prime number is called a composite number. For example, 5 is prime because 1 and 5 are its only positive integer factors, whereas 6 is composite because it has the divisors 2 and 3 in addition to 1 and 6. The fundamental theorem of arithmetic establishes the central role of primes in number theory: any integer greater than 1 can be expressed as a product of primes that is unique up to ordering. The uniqueness in this theorem requires excluding 1 as a prime because one can include arbitrarily many instances of 1 in any factorization, e.g., 3, 1 · 3, 1 · 1 · 3, etc. are all valid factorizations of 3.
The property of being prime (or not) is called primality. A simple but slow method of verifying the primality of a given number "n" is known as trial division. It consists of testing whether "n" is a multiple of any integer between 2 and formula_1. Algorithms much more efficient than trial division have been devised to test the primality of large numbers. These include the Miller–Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. , the largest known prime number has 22,338,618 decimal digits.
There are infinitely many primes, as demonstrated by Euclid around 300 BC. There is no known simple formula that separates prime numbers from composite numbers. However, the distribution of primes, that is to say, the statistical behaviour of primes in the large, can be modelled. The first result in that direction is the prime number theorem, proven at the end of the 19th century, which says that the probability that a given, randomly chosen number is prime is inversely proportional to its number of digits, or to the logarithm of "n".
Many questions regarding prime numbers remain open, such as Goldbach's conjecture (that every even integer greater than 2 can be expressed as the sum of two primes), and the twin prime conjecture (that there are infinitely many pairs of primes whose difference is 2). Such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. Primes are used in several routines in information technology, such as public-key cryptography, which makes use of properties such as the difficulty of factoring large numbers into their prime factors. Prime numbers give rise to various generalizations in other mathematical domains, mainly algebra, such as prime elements and prime ideals.
Definition and examples.
A natural number (i.e. 1, 2, 3, 4, 5, 6, etc.) is called a prime number (or a prime) if it has exactly two positive divisors, 1 and the number itself. Natural numbers greater than 1 that are not prime are called "composite".
Among the numbers 1 to 6, the numbers 2, 3, and 5 are the prime numbers, while 1, 4, and 6 are not prime. 1 is excluded as a prime number, for reasons explained below. 2 is a prime number, since the only natural numbers dividing it are 1 and 2. Next, 3 is prime, too: 1 and 3 do divide 3 without remainder, but 3 divided by 2 gives remainder 1. Thus, 3 is prime. However, 4 is composite, since 2 is another number (in addition to 1 and 4) dividing 4 without remainder:
5 is again prime: none of the numbers 2, 3, or 4 divide 5. Next, 6 is divisible by 2 or 3, since
Hence, 6 is not prime. The image at the right illustrates that 12 is not prime: . No even number greater than 2 is prime because by definition, any such number has at least three distinct divisors, namely 1, 2, and . This implies that is not prime. Accordingly, the term "odd prime" refers to any prime number greater than 2. Similarly, when written in the usual decimal system, all prime numbers larger than 5 end in 1, 3, 7, or 9, since even numbers are multiples of 2 and numbers ending in 0 or 5 are multiples of 5.
If is a natural number, then 1 and divide without remainder. Therefore, the condition of being a prime can also be restated as: a number is prime if it is greater than one and if none of
divides (without remainder). Yet another way to say the same is: a number is prime if it cannot be written as a product of two integers and , both of which are larger than 1:
In other words, is prime if items cannot be divided up into smaller equal-size groups of more than one item.
The set of all primes is often denoted by .
The first 168 prime numbers (all the prime numbers less than 1000) are:
Fundamental theorem of arithmetic.
The crucial importance of prime numbers to number theory and mathematics in general stems from the "fundamental theorem of arithmetic", which states that every integer larger than 1 can be written as a product of one or more primes in a way that is unique except for the order of the prime factors. Primes can thus be considered the “basic building blocks” of the natural numbers. For example:
As in this example, the same prime factor may occur multiple times. A decomposition:
of a number into (finitely many) prime factors , , ... to is called "prime factorization" of . The fundamental theorem of arithmetic can be rephrased so as to say that any factorization into primes will be identical except for the order of the factors. So, albeit there are many prime factorization algorithms to do this in practice for larger numbers, they all have to yield the same result.
If is a prime number and divides a product of integers, then divides or divides . This proposition is known as Euclid's lemma. It is used in some proofs of the uniqueness of prime factorizations.
Primality of one.
Most early Greeks did not even consider 1 to be a number, so they could not consider it to be a prime. By the Middle Ages and Renaissance many mathematicians included 1 as the first prime number. In the mid-18th century Christian Goldbach listed 1 as the first prime in his famous correspondence with Leonhard Euler -- who did not agree. In the 19th century many mathematicians still considered the number 1 to be a prime. For example, Derrick Norman Lehmer's list of primes up to 10,006,721, reprinted as late as 1956, started with 1 as its first prime. Henri Lebesgue is said to be the last professional mathematician to call 1 prime. By the early 20th century, mathematicians began to accept that 1 is not a prime number, but rather forms its own special category as a "unit".
A large body of mathematical work would still be valid when calling 1 a prime, but Euclid's fundamental theorem of arithmetic (mentioned above) would not hold as stated. For example, the number 15 can be factored as and ; if 1 were admitted as a prime, these two presentations would be considered different factorizations of 15 into prime numbers, so the statement of that theorem would have to be modified. Similarly, the sieve of Eratosthenes would not work correctly if 1 were considered a prime: a modified version of the sieve that considers 1 as prime would eliminate all multiples of 1 (that is, all other numbers) and produce as output only the single number 1. Furthermore, the prime numbers have several properties that the number 1 lacks, such as the relationship of the number to its corresponding value of Euler's totient function or the sum of divisors function.
History.
There are hints in the surviving records of the ancient Egyptians that they had some knowledge of prime numbers: the Egyptian fraction expansions in the Rhind papyrus, for instance, have quite different forms for primes and for composites. However, the earliest surviving records of the explicit study of prime numbers come from the Ancient Greeks. Euclid's Elements (circa 300 BC) contain important theorems about primes, including the infinitude of primes and the fundamental theorem of arithmetic. Euclid also showed how to construct a perfect number from a Mersenne prime. The Sieve of Eratosthenes, attributed to Eratosthenes, is a simple method to compute primes, although the large primes found today with computers are not generated this way.
After the Greeks, little happened with the study of prime numbers until the 17th century. In 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler). Fermat also conjectured that all numbers of the form 22"n" + 1 are prime (they are called Fermat numbers) and he verified this up to "n" = 4 (or 216 + 1). However, the very next Fermat number 232 + 1 is composite (one of its prime factors is 641), as Euler discovered later, and in fact no further Fermat numbers are known to be prime. The French monk Marin Mersenne looked at primes of the form 2"p" − 1, with "p" a prime. They are called Mersenne primes in his honor.
Euler's work in number theory included many results about primes. He showed the infinite series Proof that the sum of the reciprocals of the primes diverges is divergent.
In 1747 he showed that the even perfect numbers are precisely the integers of the form 2"p"−1(2"p" − 1), where the second factor is a Mersenne prime.
At the start of the 19th century, Legendre and Gauss independently conjectured that as "x" tends to infinity, the number of primes up to "x" is asymptotic to "x"/ln("x"), where ln("x") is the natural logarithm of "x". Ideas of Riemann in his 1859 paper on the zeta-function sketched a program that would lead to a proof of the prime number theorem. This outline was completed by Hadamard and de la Vallée Poussin, who independently proved the prime number theorem in 1896.
Proving a number is prime is not done (for large numbers) by trial division. Many mathematicians have worked on primality tests for large numbers, often restricted to specific number forms. This includes Pépin's test for Fermat numbers (1877), Proth's theorem (around 1878), the Lucas–Lehmer primality test (originated 1856), and the generalized Lucas primality test. More recent algorithms like APRT-CL, ECPP, and AKS work on arbitrary numbers but remain much slower.
For a long time, prime numbers were thought to have extremely limited application outside of pure mathematics. This changed in the 1970s when the concepts of public-key cryptography were invented, in which prime numbers formed the basis of the first algorithms such as the RSA cryptosystem algorithm.
Since 1951 all the largest known primes have been found by computers. The search for ever larger primes has generated interest outside mathematical circles. The Great Internet Mersenne Prime Search and other distributed computing projects to find large primes have become popular, while mathematicians continue to struggle with the theory of primes.
Number of prime numbers.
There are infinitely many prime numbers. Another way of saying this is that the sequence
of prime numbers never ends. This statement is referred to as "Euclid's theorem" in honor of the ancient Greek mathematician Euclid, since the first known proof for this statement is attributed to him. Many more proofs of the infinitude of primes are known, including an analytical proof by Euler, Goldbach's proof based on Fermat numbers, Furstenberg's proof using general topology, and Kummer's elegant proof.
Euclid's proof.
Euclid's proof (Book IX, Proposition 20) considers any finite set "S" of primes. The key idea is to consider the product of all these numbers plus one:
Like any other natural number, "N" is divisible by at least one prime number (it is possible that "N" itself is prime).
None of the primes by which "N" is divisible can be members of the finite set "S" of primes with which we started, because dividing "N" by any one of these leaves a remainder of 1. Therefore, the primes by which "N" is divisible are additional primes beyond the ones we started with. Thus any finite set of primes can be extended to a larger finite set of primes.
It is often erroneously reported that Euclid begins with the assumption that the set initially considered contains all prime numbers, leading to a contradiction, or that it contains precisely the "n" smallest primes rather than any arbitrary finite set of primes. Today, the product of the smallest "n" primes plus 1 is conventionally called the "n"th Euclid number.
Euler's analytical proof.
Euler's proof uses the partial sums of the reciprocals of primes,
For any arbitrary real number "x", there exists a prime "p" for which this partial sum is bigger than "x". This shows that there are infinitely many primes, because if there were finitely many primes the sum would reach its maximum value at the biggest prime rather than being unbounded. More precisely, the growth rate of "S"("p") is doubly logarithmic, as quantified by Mertens' second theorem. For comparison, the sum
does not grow to infinity as "n" goes to infinity (see Basel problem). In this sense, prime numbers occur more often than squares of natural numbers. Brun's theorem states that the sum of the reciprocals of twin primes,
is finite. Because of Brun's theorem, it is not possible to use Euler's method to solve the twin prime conjecture, that there exist infinitely many twin primes.
Testing primality and integer factorization.
There are various methods to determine whether a given number "n" is prime. The most basic routine, trial division, is of little practical use because of its slowness. One group of modern primality tests is applicable to arbitrary numbers, while more efficient tests are available for particular numbers. Most such methods only tell whether "n" is prime or not. Routines also yielding one (or all) prime factors of "n" are called factorization algorithms.
Trial division.
The most basic method of checking the primality of a given integer "n" is called "trial division". This routine consists of dividing "n" by each integer "m" that is greater than 1 and less than or equal to the square root of "n". If the result of any of these divisions is an integer, then "n" is not a prime, otherwise it is a prime. Indeed, if formula_6 is composite (with "a" and "b" ≠ 1) then one of the factors "a" or "b" is necessarily at most formula_1. For example, for formula_8, the trial divisions are by None of these numbers divides 37, so 37 is prime. This routine can be implemented more efficiently if a complete list of primes up to formula_1 is known—then trial divisions need to be checked only for those "m" that are prime. For example, to check the primality of 37, only three divisions are necessary ("m" = 2, 3, and 5), given that 4 and 6 are composite.
While a simple method, trial division quickly becomes impractical for testing large integers because the number of possible factors grows too rapidly as "n" increases. According to the prime number theorem explained below, the number of prime numbers less than formula_1 is approximately given by formula_11, so the algorithm may need up to this number of trial divisions to check the primality of "n". For , this number is 450 million—too large for many practical applications.
Sieves.
An algorithm yielding all primes up to a given limit, such as required in the primes-only trial division method, is called a prime number sieve. The oldest example, the sieve of Eratosthenes (see above), is still the most commonly used. The sieve of Atkin is another option. Before the advent of computers, lists of primes up to bounds like 107 were also used.
Primality testing versus primality proving.
Modern primality tests for general numbers "n" can be divided into two main classes, probabilistic (or "Monte Carlo") and deterministic algorithms. Deterministic algorithms provide a way to tell for sure whether a given number is prime or not. For example, trial division is a deterministic algorithm because, if performed correctly, it will always identify a prime number as prime and a composite number as composite. Probabilistic algorithms are normally faster, but do not completely prove that a number is prime. These tests rely on testing a given number in a partly random way. For example, a given test might pass all the time if applied to a prime number, but pass only with probability "p" if applied to a composite number. If we repeat the test "n" times and pass every time, then the probability that our number is composite is "1/(1-p)n", which decreases exponentially with the number of tests, so we can be as sure as we like (though never perfectly sure) that the number is prime. On the other hand, if the test ever fails, then we know that the number is composite.
A particularly simple example of a probabilistic test is the Fermat primality test, which relies on the fact (Fermat's little theorem) that "np≡n (mod p)" for any "n" if "p" is a prime number. If we have a number "b" that we want to test for primality, then we work out "nb (mod b)" for a random value of "n" as our test. A flaw with this test is that there are some composite numbers (the Carmichael numbers) that satisfy the Fermat identity even though they are not prime, so the test has no way of distinguishing between prime numbers and Carmichael numbers. Carmichael numbers are substantially rarer than prime numbers, though, so this test can be useful for practical purposes. More powerful extensions of the Fermat primality test, such as the Baillie-PSW, Miller-Rabin, and Solovay-Strassen tests, are guaranteed to fail at least some of the time when applied to a composite number.
Deterministic algorithms do not erroneously report composite numbers as prime. In practice, the fastest such method is known as elliptic curve primality proving. Analyzing its run time is based on heuristic arguments, as opposed to the rigorously proven complexity of the more recent AKS primality test. Deterministic methods are typically slower than probabilistic ones, so the latter ones are typically applied first before a more time-consuming deterministic routine is employed.
The following table lists a number of prime tests. The running time is given in terms of "n", the number to be tested and, for probabilistic algorithms, the number "k" of tests performed. Moreover, ε is an arbitrarily small positive number, and log is the logarithm to an unspecified base. The big O notation means that, for example, elliptic curve primality proving requires a time that is bounded by a factor (not depending on "n", but on ε) times log5+ε("n").
Special-purpose algorithms and the largest known prime.
In addition to the aforementioned tests applying to any natural number "n", a number of much more efficient primality tests is available for special numbers. For example, to run Lucas' primality test requires the knowledge of the prime factors of , while the Lucas–Lehmer primality test needs the prime factors of as input. For example, these tests can be applied to check whether
are prime. Prime numbers of this form are known as factorial primes. Other primes where either "p" + 1 or "p" − 1 is of a particular shape include the Sophie Germain primes (primes of the form 2"p" + 1 with "p" prime), primorial primes, Fermat primes and Mersenne primes, that is, prime numbers that are of the form , where "p" is an arbitrary prime. The Lucas–Lehmer test is particularly fast for numbers of this form. This is why the largest "known" prime has almost always been a Mersenne prime since the dawn of electronic computers.
The following table gives the largest known primes of the mentioned types. Some of these primes have been found using distributed computing. In 2009, the Great Internet Mersenne Prime Search project was awarded a US$100,000 prize for first discovering a prime with at least 10 million digits. The Electronic Frontier Foundation also offers $150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively. Some of the largest primes not known to have any particular form (that is, no simple formula such as that of Mersenne primes) have been found by taking a piece of semi-random binary data, converting it to a number n, multiplying it by 256k for some positive integer k, and searching for possible primes within the interval + 1, 256"k"("n" + 1) − 1.
Integer factorization.
Given a composite integer "n", the task of providing one (or all) prime factors is referred to as "factorization" of "n". Elliptic curve factorization is an algorithm relying on arithmetic on an elliptic curve.
Distribution.
In 1975, number theorist Don Zagier commented that primes both
The distribution of primes in the large, such as the question how many primes are smaller than a given, large threshold, is described by the prime number theorem, but no efficient formula for the "n"-th prime is known.
There are arbitrarily long sequences of consecutive non-primes, as for every positive integer formula_12 the formula_12 consecutive integers from formula_14 to formula_15 (inclusive) are all composite (as formula_16 is divisible by formula_17 for formula_17 between formula_19 and formula_20).
Dirichlet's theorem on arithmetic progressions, in its basic form, asserts that linear polynomials
with coprime integers "a" and "b" take infinitely many prime values. Stronger forms of the theorem state that the sum of the reciprocals of these prime values diverges, and that different such polynomials with the same "b" have approximately the same proportions of primes.
The corresponding question for quadratic polynomials is less well-understood.
Formulas for primes.
There is no known efficient formula for primes. For example, Mills' theorem and a theorem of Wright assert that there are real constants "A>1" and μ such that
are prime for any natural number "n". Here formula_23 represents the floor function, i.e., largest integer not greater than the number in question. The latter formula can be shown using Bertrand's postulate (proven first by Chebyshev), which states that there always exists at least one prime number "p" with "n" < "p" < 2"n" − 2, for any natural number "n" > 3. However, computing "A" or μ requires the knowledge of infinitely many primes to begin with. Another formula is based on Wilson's theorem and generates the number 2 many times and all other primes exactly once.
There is no non-constant polynomial, even in several variables, that takes "only" prime values. However, there is a set of Diophantine equations in 9 variables and one parameter with the following property: the parameter is prime if and only if the resulting system of equations has a solution over the natural numbers. This can be used to obtain a single formula with the property that all its "positive" values are prime.
Number of prime numbers below a given number.
The prime counting function π("n") is defined as the number of primes not greater than "n". For example, π(11) = 5, since there are five primes less than or equal to 11. There are known algorithms to compute exact values of π("n") faster than it would be possible to compute each prime up to "n". The "prime number theorem" states that π("n") is approximately given by
in the sense that the ratio of π("n") and the right hand fraction approaches 1 when "n" grows to infinity. This implies that the likelihood that a number less than "n" is prime is (approximately) inversely proportional to the number of digits in "n". A more accurate estimate for π("n") is given by the offset logarithmic integral
The prime number theorem also implies estimates for the size of the "n"-th prime number "pn" (i.e., "p"1 = 2, "p"2 = 3, etc.): up to a bounded factor, "pn" grows like . In particular, the prime gaps, i.e. the differences of two consecutive primes, become arbitrarily large. This latter statement can also be seen in a more elementary way by noting that the sequence (for the notation "n"! read factorial) consists of composite numbers, for any natural number "n".
Arithmetic progressions.
An arithmetic progression is the set of natural numbers that give the same remainder when divided by some fixed number "q" called modulus. For example,
is an arithmetic progression modulo . Except for 3, none of these numbers is prime, since so that the remaining numbers in this progression are all composite. (In general terms, all prime numbers above "q" are of the form "q"#·"n" + "m", where 0 < "m" < "q"#, and "m" has no prime factor ≤ "q".) Thus, the progression
can have infinitely many primes only when "a" and "q" are coprime, i.e., their greatest common divisor is one. If this necessary condition is satisfied, "Dirichlet's theorem on arithmetic progressions" asserts that the progression contains infinitely many primes. The picture below illustrates this with : the numbers are "wrapped around" as soon as a multiple of 9 is passed. Primes are highlighted in red. The rows (=progressions) starting with , 6, or 9 contain at most one prime number. In all other rows (, 2, 4, 5, 7, and 8) there are infinitely many prime numbers. What is more, the primes are distributed equally among those rows in the long run—the density of all primes congruent "a" modulo 9 is 1/6.
The Green–Tao theorem shows that there are arbitrarily long arithmetic progressions consisting of primes.
An odd prime "p" is expressible as the sum of two squares, , exactly if "p" is congruent 1 modulo 4 (Fermat's theorem on sums of two squares).
Prime values of quadratic polynomials.
Euler noted that the function
yields prime numbers for , a fact leading into deep algebraic number theory: more specifically, Heegner numbers. For greater "n", the expression also produces composite values. The Hardy-Littlewood conjecture F makes an asymptotic prediction about the density of primes among the values of quadratic polynomials (with integer coefficients "a", "b", and "c"),
in terms of Li("n") and the coefficients "a", "b", and "c". However, progress has been difficult. No quadratic polynomial (with ) is known to take infinitely many prime values. The Ulam spiral depicts all natural numbers in a spiral-like way. Primes cluster on certain diagonals and not others, suggesting that some quadratic polynomials take prime values more often than others.
Open questions.
Zeta function and the Riemann hypothesis.
The Riemann zeta function ζ("s") is defined as an infinite sum
where "s" is a complex number with real part bigger than 1. It is a consequence of the fundamental theorem of arithmetic that this sum agrees with the infinite product
The zeta function is closely related to prime numbers. For example, the aforementioned fact that there are infinitely many primes can also be seen using the zeta function: if there were only finitely many primes then ζ(1) would have a finite value. However, the harmonic series diverges (i.e., exceeds any given number), so there must be infinitely many primes. Another example of the richness of the zeta function and a glimpse of modern algebraic number theory is the following identity (Basel problem), due to Euler,
The reciprocal of ζ(2), 6/π2, is the probability that two numbers selected at random are relatively prime.
The unproven "Riemann hypothesis", dating from 1859, states that except for all zeroes of the ζ-function have real part equal to 1/2. The connection to prime numbers is that it essentially says that the primes are as regularly distributed as possible. From a physical viewpoint, it roughly states that the irregularity in the distribution of primes only comes from random noise. From a mathematical viewpoint, it roughly states that the asymptotic distribution of primes (about x/log "x" of numbers less than "x" are primes, the prime number theorem) also holds for much shorter intervals of length about the square root of "x" (for intervals near "x"). This hypothesis is generally believed to be correct. In particular, the simplest assumption is that primes should have no significant irregularities without good reason.
Other conjectures.
In addition to the Riemann hypothesis, many more conjectures revolving about primes have been posed. Often having an elementary formulation, many of these conjectures have withstood a proof for decades: all four of Landau's problems from 1912 are still unsolved. One of them is Goldbach's conjecture, which asserts that every even integer "n" greater than 2 can be written as a sum of two primes. , this conjecture has been verified for all numbers up to . Weaker statements than this have been proven, for example Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes. Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime, the product of two primes. Also, any even integer can be written as the sum of six primes. The branch of number theory studying such questions is called additive number theory.
Other conjectures deal with the question whether an infinity of prime numbers subject to certain constraints exists. It is conjectured that there are infinitely many Fibonacci primes and infinitely many Mersenne primes, but not Fermat primes. It is not known whether or not there are an infinite number of Wieferich primes and of prime Euclid numbers.
A third type of conjectures concerns aspects of the distribution of primes. It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2 (twin prime conjecture). Polignac's conjecture is a strengthening of that conjecture, it states that for every positive integer "n", there are infinitely many pairs of consecutive primes that differ by 2"n". It is conjectured there are infinitely many primes of the form "n"2 + 1. These conjectures are special cases of the broad Schinzel's hypothesis H. Brocard's conjecture says that there are always at least four primes between the squares of consecutive primes greater than 2. Legendre's conjecture states that there is a prime number between "n"2 and ("n" + 1)2 for every positive integer "n". It is implied by the stronger Cramér's conjecture.
Applications.
For a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of the self-interest of studying the topic with the exception of use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance. However, this vision was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public key cryptography algorithms. Prime numbers are also used for hash tables and pseudorandom number generators.
Some rotor machines were designed with a different number of pins on each rotor, with the number of pins on any one rotor either prime, or coprime to the number of pins on any other rotor. This helped generate the full cycle of possible rotor positions before repeating any position.
The International Standard Book Numbers work with a check digit, which exploits the fact that 11 is a prime.
Arithmetic modulo a prime and finite fields.
"Modular arithmetic" modifies usual arithmetic by only using the numbers
where "n" is a fixed natural number called modulus.
Calculating sums, differences and products is done as usual, but whenever a negative number or a number greater than "n" − 1 occurs, it gets replaced by the remainder after division by "n". For instance, for "n" = 7, the sum 3 + 5 is 1 instead of 8, since 8 divided by 7 has remainder 1. This is referred to by saying "3 + 5 is congruent to 1 modulo 7" and is denoted
Similarly, 6 + 1 ≡ 0 (mod 7), 2 − 5 ≡ 4 (mod 7), since −3 + 7 = 4, and 3 · 4 ≡ 5 (mod 7) as 12 has remainder 5. Standard properties of addition and multiplication familiar from the integers remain valid in modular arithmetic. In the parlance of abstract algebra, the above set of integers, which is also denoted Z/"n"Z, is therefore a commutative ring for any "n".
Division, however, is not in general possible in this setting. For example, for "n" = 6, the equation
a solution "x" of which would be an analogue of 2/3, cannot be solved, as one can see by calculating 3 · 0, ..., 3 · 5 modulo 6. The distinctive feature of prime numbers is the following: division "is" possible in modular arithmetic if and only if "n" is a prime. Equivalently, "n" is prime if and only if all integers "m" satisfying are "coprime" to "n", i.e. their only common divisor is one. Indeed, for "n" = 7, the equation
has a unique solution, . Because of this, for any prime "p", Z/"p"Z (also denoted F"p") is called a field or, more specifically, a finite field since it contains finitely many, namely "p", elements.
A number of theorems can be derived from inspecting F"p" in this abstract way. For example, Fermat's little theorem, stating
for any integer "a" not divisible by "p", may be proved using these notions. This implies
Giuga's conjecture says that this equation is also a sufficient condition for "p" to be prime. Another consequence of Fermat's little theorem is the following: if "p" is a prime number other than 2 and 5, 1/"p" is always a recurring decimal, whose period is or a divisor of . The fraction 1/"p" expressed likewise in base "q" (rather than base 10) has similar effect, provided that "p" is not a prime factor of "q". Wilson's theorem says that an integer "p" > 1 is prime if and only if the factorial ("p" − 1)! + 1 is divisible by "p". Moreover, an integer "n" > 4 is composite if and only if ("n" − 1)! is divisible by "n".
Fermat primes and constructible polygons.
Fermat primes are primes of the form
with "k" a natural number. They are named after Pierre de Fermat, who conjectured that all such numbers are prime. This was based on the evidence of the first five numbers in this series—3, 5, 17, 257, and 65,537—being prime. However, "F"5 is composite and so are all other Fermat numbers that have been verified as of 2015. A regular "n"-gon is constructible using straightedge and compass if and only if the odd prime factors of "n" (if any) are distinct Fermat primes.
Other mathematical occurrences of primes.
Many mathematical domains make great use of prime numbers. An example from the theory of finite groups are the Sylow theorems: if "G" is a finite group and "pn" is the highest power of the prime "p" that divides the order of "G", then "G" has a subgroup of order "pn". Also, any group of prime order is cyclic (Lagrange's theorem).
Public-key cryptography.
Several public-key cryptography algorithms, such as RSA and the Diffie–Hellman key exchange, are based on large prime numbers (for example, 512-bit primes are frequently used for RSA and 1024-bit primes are typical for Diffie–Hellman.). RSA relies on the assumption that it is much easier (i.e., more efficient) to perform the multiplication of two (large) numbers "x" and "y" than to calculate "x" and "y" (assumed coprime) if only the product "xy" is known. The Diffie–Hellman key exchange relies on the fact that there are efficient algorithms for modular exponentiation, while the reverse operation the discrete logarithm is thought to be a hard problem.
Prime numbers in nature.
The evolutionary strategy used by cicadas of the genus "Magicicada" make use of prime numbers. These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. The logic for this is believed to be that the prime number intervals between emergences make it very difficult for predators to evolve that could specialize as predators on "Magicicadas". If "Magicicadas" appeared at a non-prime number intervals, say every 12 years, then predators appearing every 2, 3, 4, 6, or 12 years would be sure to meet them. Over a 200-year period, average predator populations during hypothetical outbreaks of 14- and 15-year cicadas would be up to 2% higher than during outbreaks of 13- and 17-year cicadas. Though small, this advantage appears to have been enough to drive natural selection in favour of a prime-numbered life-cycle for these insects.
There is speculation that the zeros of the zeta function are connected to the energy levels of complex quantum systems.
Generalizations.
The concept of prime number is so important that it has been generalized in different ways in various branches of mathematics. Generally, "prime" indicates minimality or indecomposability, in an appropriate sense. For example, the prime field is the smallest subfield of a field "F" containing both 0 and 1. It is either Q or the finite field with "p" elements, whence the name. Often a second, additional meaning is intended by using the word prime, namely that any object can be, essentially uniquely, decomposed into its prime components. For example, in knot theory, a prime knot is a knot that is indecomposable in the sense that it cannot be written as the knot sum of two nontrivial knots. Any knot can be uniquely expressed as a connected sum of prime knots. Prime models and prime 3-manifolds are other examples of this type.
Prime elements in rings.
Prime numbers give rise to two more general concepts that apply to elements of any commutative ring "R", an algebraic structure where addition, subtraction and multiplication are defined: "prime elements" and "irreducible elements". An element "p" of "R" is called prime element if it is neither zero nor a unit (i.e., does not have a multiplicative inverse) and satisfies the following requirement: given "x" and "y" in "R" such that "p" divides the product "xy", then "p" divides "x" or "y". An element is irreducible if it is not a unit and cannot be written as a product of two ring elements that are not units. In the ring Z of integers, the set of prime elements equals the set of irreducible elements, which is
In any ring "R", any prime element is irreducible. The converse does not hold in general, but does hold for unique factorization domains.
The fundamental theorem of arithmetic continues to hold in unique factorization domains. An example of such a domain is the Gaussian integers Z["i"], that is, the set of complex numbers of the form "a" + "bi" where "i" denotes the imaginary unit and "a" and "b" are arbitrary integers. Its prime elements are known as Gaussian primes. Not every prime (in Z) is a Gaussian prime: in the bigger ring Z["i"], 2 factors into the product of the two Gaussian primes (1 + "i") and (1 − "i"). Rational primes (i.e. prime elements in Z) of the form 4"k" + 3 are Gaussian primes, whereas rational primes of the form 4"k" + 1 are not.
Prime ideals.
In ring theory, the notion of number is generally replaced with that of ideal. "Prime ideals", which generalize prime elements in the sense that the principal ideal generated by a prime element is a prime ideal, are an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry. The prime ideals of the ring of integers are the ideals (0), (2), (3), (5), (7), (11), … The fundamental theorem of arithmetic generalizes to the Lasker–Noether theorem, which expresses every ideal in a Noetherian commutative ring as an intersection of primary ideals, which are the appropriate generalizations of prime powers.
Prime ideals are the points of algebro-geometric objects, via the notion of the spectrum of a ring. Arithmetic geometry also benefits from this notion, and many concepts exist in both geometry and number theory. For example, factorization or ramification of prime ideals when lifted to an extension field, a basic problem of algebraic number theory, bears some resemblance with ramification in geometry. Such ramification questions occur even in number-theoretic questions solely concerned with integers. For example, prime ideals in the ring of integers of quadratic number fields can be used in proving quadratic reciprocity, a statement that concerns the solvability of quadratic equations
where "x" is an integer and "p" and "q" are (usual) prime numbers. Early attempts to prove Fermat's Last Theorem climaxed when Kummer introduced regular primes, primes satisfying a certain requirement concerning the failure of unique factorization in the ring consisting of expressions
where "a"0, ..., "ap"−1 are integers and ζ is a complex number such that root of unity.
Valuations.
Valuation theory studies certain functions from a field "K" to the real numbers R called valuations. Every such valuation yields a topology on "K", and two valuations are called equivalent if they yield the same topology. A "prime of K" (sometimes called a "place of K") is an equivalence class of valuations. For example, the "p"-adic valuation of a rational number "q" is defined to be the integer "vp"("q"), such that
where both "r" and "s" are not divisible by "p". For example, The "p"-adic norm is defined as
In particular, this norm gets smaller when a number is multiplied by "p", in sharp contrast to the usual absolute value (also referred to as the infinite prime). While completing Q (roughly, filling the gaps) with respect to the absolute value yields the field of real numbers, completing with respect to the "p"-adic norm |−|"p" yields the field of "p"-adic numbers. These are essentially all possible ways to complete Q, by Ostrowski's theorem. Certain arithmetic questions related to Q or more general global fields may be transferred back and forth to the completed (or local) fields. This local-global principle again underlines the importance of primes to number theory.
In the arts and literature.
Prime numbers have influenced many artists and writers. The French composer Olivier Messiaen used prime numbers to create ametrical music through "natural phenomena". In works such as "La Nativité du Seigneur" (1935) and "Quatre études de rythme" (1949–50), he simultaneously employs motifs with lengths given by different prime numbers to create unpredictable rhythms: the primes 41, 43, 47 and 53 appear in the third étude, "Neumes rythmiques". According to Messiaen this way of composing was "inspired by the movements of nature, movements of free and unequal durations".
In his science fiction novel "Contact", NASA scientist Carl Sagan suggested that prime numbers could be used as a means of communicating with aliens, an idea that he had first developed informally with American astronomer Frank Drake in 1975. In the novel "The Curious Incident of the Dog in the Night-Time" by Mark Haddon, the narrator arranges the sections of the story by consecutive prime numbers.
Many films, such as "Cube", "Sneakers", "The Mirror Has Two Faces" and "A Beautiful Mind" reflect a popular fascination with the mysteries of prime numbers and cryptography. Prime numbers are used as a metaphor for loneliness and isolation in the Paolo Giordano novel "The Solitude of Prime Numbers", in which they are portrayed as "outsiders" among integers.

</doc>
<doc id="23669" url="https://en.wikipedia.org/wiki?curid=23669" title="Piers Anthony">
Piers Anthony

Piers Anthony Dillingham Jacob (born 6 August 1934 in Oxford, England) is an English American author in the science fiction and fantasy genres, publishing under the name Piers Anthony. He is most famous for his long-running novel series set in the fictional realm of Xanth.
Many of his books have appeared on the New York Times Best Seller list. He has stated that one of his greatest achievements has been to publish a book for every letter of the alphabet, from "Anthonology" to "Zombie Lover".
Early life.
Anthony's family emigrated to the United States from Britain when he was six. He graduated from Goddard College in Vermont in 1956. On "This American Life" on July 27, 2012, Anthony revealed that his parents had divorced, he was bullied, and he had poor grades in school. Anthony referred to his high school as "a very fancy private school", and refuses to donate money to the school, because as a student, he recalls being part of "the lower crust", and that no one paid attention to or cared about him. He said, "I didn't like being a member of the under class, of the peons like that". He became a naturalized U.S. citizen while serving in the United States Army in 1958. After completing a two-year stint in military service, he briefly taught school at Admiral Farragut Academy in St. Petersburg, Florida before becoming a full-time writer.
Marriage and early career.
Anthony met his future wife, Carol Marble, while both were attending college. They were married in 1956, the same year he graduated from Goddard College, Plainfield, Vermont. After a series of odd jobs, Anthony decided to join the U.S. Army in 1957 for a steady source of income and medical coverage for his pregnant wife. He would stay in the Army until 1959; he became a U.S. citizen during this time. While in the army, he became an editor and cartoonist for the battalion newspaper. After leaving the army, he spent a brief stint as a public school teacher before trying his hand at becoming a full-time writer.
Anthony and his wife made a deal: if he could sell a piece of writing within one year, she would continue to work to support him. But if he could not sell anything in that year, then he would forever give up his dream of being a writer. At the end of the year, he managed to get a short story published. He credits his wife as the person who made his writing career possible, and he advises aspiring writers that they need to have a source of income other than their writing in order to get through the early years of a writing career.
Writing.
On multiple occasions Anthony has moved from one publisher to another (taking a profitable hit series with him), when he says he felt the editors were unduly tampering with his work. He has sued publishers for accounting malfeasance and won judgments in his favor. Anthony maintains an Internet Publishers Survey in the interest of helping aspiring writers. For this service, he won the 2003 "Friend of EPIC" award for service to the electronic publishing community. His website won the "Special Recognition for Service to Writers" award from Preditors and Editors, an author's guide to publishers and writing services.
Anthony was at one time an angel investor in Xlibris.
Many of his popular novel series have been optioned for movies. His popular series Xanth inspired the DOS video game "Companions of Xanth", by Legend Entertainment. The series also spawned the board game "Xanth" by Mayfair Games.
Anthony's novels usually end with a chapter-long Author's Note, in which he talks about himself, his life, and his experiences as they related to the process of writing the novel. He often discusses correspondence with readers and any real-world issues that influenced the novel.
Since about 2000, Anthony has written his novels in a Linux environment.
Anthony's "Xanth" series was ranked No. 99 in a 2011 NPR readers' poll of best science fiction and fantasy books.
In other media.
Act One of episode 470 of the radio program "This American Life" is an account of boyhood obsessions with Piers Anthony. The act is written and narrated by writer Logan Hill who, as a 12-year-old, was consumed with reading Anthony’s novels. For a decade he felt he must have been Anthony's number one fan, until, when he was 22, he met "Andy" at a wedding and discovered their mutual interest in the writer. Andy is interviewed for the story and explains that, as a teenager, he had used escapist novels in order to cope with his alienating school and home life in Buffalo, New York. In 1987, at age 15, he decided to run away to Florida in order to try to live with Piers Anthony. The story includes Piers Anthony’s reflections on these events.
"But What of Earth?" controversy.
Early in Anthony's literary career, there was a dispute surrounding the original publication (1976) of "But What of Earth?". Editor Roger Elwood commissioned the novel for his nascent science-fiction line Laser Books. According to Anthony, he completed "But What of Earth?", and Elwood accepted and purchased it. Elwood then told Anthony that he wished to make several minor changes, and in order not to waste Anthony's time, he had hired copy editor (and author) Robert Coulson to retype the manuscript with the changes. Anthony described Coulson as a friend and was initially open to his contribution.
However, Elwood told Coulson he was to be a full collaborator, free to make revisions to Anthony's text in line with suggestions made by other copy editors. Elwood promised Coulson a 50-50 split with Anthony on all future royalties. According to Anthony, the published novel was very different from his version, with changes to characters and dialog, and with scenes added and removed. Anthony felt the changes worsened the novel.
Laser's ultimate publication of "But What of Earth?" listed Anthony and Coulson together as collaborators. Publication rights were reverted to Anthony under threat of legal action. In 1989, Anthony (re)published his original "But What of Earth?" in an annotated edition through Tor Books. This edition contains an introduction and conclusion setting out the story of the novel's permutations and roughly 60 pages of notes by Anthony giving examples of changes to plot and characters, and describing some of the comments made by copy editors on his manuscript.
Personal life.
Anthony currently lives with his wife on a tree farm which he owns in Florida. He and his wife had two daughters, Penny and Cheryl, and one grandchild, Logan. Regarding his religious beliefs, Anthony wrote in the October 2004 entry of his personal website, "I'm agnostic, which means I regard the case as unproven, but I'm much closer to the atheist position than to the theist one."
On 3 September 2009, their daughter Penelope "Penny" Carolyn Jacob died from apparent respiratory paralysis following surgery for melanoma which had metastasized to her brain. She is survived by her husband and her daughter, Logan.
Bibliography.
For autobiographies refer to autobiographical subsection.

</doc>
<doc id="23670" url="https://en.wikipedia.org/wiki?curid=23670" title="Perfect number">
Perfect number

In number theory, a perfect number is a positive integer that is equal to the sum of its proper positive divisors, that is, the sum of its positive divisors excluding the number itself (also known as its aliquot sum). Equivalently, a perfect number is a number that is half the sum of all of its positive divisors (including itself) i.e. "σ"1("n") = 2"n".
This definition is ancient, appearing as early as Euclid's Elements (VII.22) where it is called "τέλειος ἀριθμός" ("perfect", "ideal", or "complete number"). Euclid also proved a formation rule (IX.36) whereby formula_1 is an even perfect number whenever formula_2 is what is now called a Mersenne prime—a prime of the form formula_3 for prime formula_4 Much later, Euler proved that all even perfect numbers are of this form. This is known as the Euclid–Euler theorem.
It is not known whether there are any odd perfect numbers, nor whether infinitely many perfect numbers exist.
Examples.
The first perfect number is 6, because 1, 2, and 3 are its proper positive divisors, and 1 + 2 + 3 = 6. Equivalently, the number 6 is equal to half the sum of all its positive divisors: ( 1 + 2 + 3 + 6 ) / 2 = 6. The next perfect number is 28 = 1 + 2 + 4 + 7 + 14. This is followed by the perfect numbers 496 and 8128 .
History.
These first four perfect numbers were the only ones known to early Greek mathematics, and the mathematician Nicomachus had noted 8128 as early as 100 AD. Philo of Alexandria in his first-century book "On the creation" mentions perfect numbers, claiming that the world was created in 6 days and the moon orbits in 28 days because 6 and 28 are perfect. Philo is followed by Origen, and by Didymus the Blind, who adds the observation that there are only four perfect numbers that are less than 10,000. (Commentary on Genesis 1. 14-19). St Augustine defines perfect numbers in City of God (Part XI, Chapter 30) in the early 5th century AD, repeating the claim that God created the world in 6 days because 6 is the smallest perfect number. The Egyptian mathematician Ismail ibn Fallūs (1194–1252) mentioned the next three perfect numbers (33,550,336, 8,589,869,056 and 137,438,691,328) and listed a few more which are now known to be incorrect. In a manuscript written between 1456 and 1461, an unknown mathematician recorded the earliest European reference to a fifth perfect number, with 33,550,336 being correctly identified for the first time. In 1588, the Italian mathematician Pietro Cataldi also identified the sixth (8,589,869,056) and the seventh (137,438,691,328) perfect numbers, and also proved that every perfect number obtained from Euclid's rule ends with a 6 or an 8.
Even perfect numbers.
Euclid proved that 2"p"−1(2"p" − 1) is an even perfect number whenever 2"p" − 1 is prime (Euclid, Prop. IX.36).
For example, the first four perfect numbers are generated by the formula 2"p"−1(2"p" − 1), with "p" a prime number, as follows:
Prime numbers of the form 2"p" − 1 are known as Mersenne primes, after the seventeenth-century monk Marin Mersenne, who studied number theory and perfect numbers. For 2"p" − 1 to be prime, it is necessary that "p" itself be prime. However, not all numbers of the form 2"p" − 1 with a prime "p" are prime; for example, 211 − 1 = 2047 = 23 × 89 is not a prime number. In fact, Mersenne primes are very rare—of the 9,592 prime numbers "p" less than 100,000,
2"p" − 1 is prime for only 28 of them.
Over a millennium after Euclid, Ibn al-Haytham (Alhazen) "circa" 1000 AD conjectured that "every" even perfect number is of the form 2"p"−1(2"p" − 1) where 2"p" − 1 is prime, but he was not able to prove this result. It was not until the 18th century that Leonhard Euler proved that the formula 2"p"−1(2"p" − 1) will yield all the even perfect numbers. Thus, there is a one-to-one correspondence between even perfect numbers and Mersenne primes; each Mersenne prime generates one even perfect number, and vice versa. This result is often referred to as the Euclid–Euler theorem. , 49 Mersenne primes are known, and therefore 49 even perfect numbers (the largest of which is 274207280 × (274207281 − 1) with 44,677,235 digits).
An exhaustive search by the GIMPS distributed computing project has shown that the first 44 even perfect numbers are 2"p"−1(2"p" − 1) for
Five higher perfect numbers have also been discovered, namely those for which "p" = 37156667, 42643801, 43112609, 57885161, and 74207281, though there may be others within this range. It is not known whether there are infinitely many perfect numbers, nor whether there are infinitely many Mersenne primes.
As well as having the form 2"p"−1(2"p" − 1), each even perfect number is the triangular number (and hence equal to the sum of the integers from 1 to ) and the hexagonal number. Furthermore, each even perfect number except for 6 is the centered nonagonal number and is equal to the sum of the first odd cubes:
Even perfect numbers (except 6) are of the form
with each resulting triangular number (after subtracting 1 from the perfect number and dividing the result by 9) ending in 3 or 5, the sequence starting with 3, 55, 903, 3727815, ... This can be reformulated as follows: adding the digits of any even perfect number (except 6), then adding the digits of the resulting number, and repeating this process until a single digit (called the digital root) is obtained, always produces the number 1. For example, the digital root of 8128 is 1, because 8 + 1 + 2 + 8 = 19, 1 + 9 = 10, and 1 + 0 = 1. This works with all perfect numbers 2"p"−1(2"p" − 1) with odd prime "p" and, in fact, with all numbers of the form 2"m"−1(2"m" − 1) for odd integer (not necessarily prime) "m".
Owing to their form, 2"p"−1(2"p" − 1), every even perfect number is represented in binary as "p" ones followed by "p" − 1  zeros:
Thus every even perfect number is a pernicious number.
Note that every even perfect number is also a practical number (c.f. Related concepts).
Odd perfect numbers.
It is unknown whether there is any odd perfect number, though various results have been obtained. In 1496, Jacques Lefèvre stated that Euclid's rule gives all perfect numbers, thus implying that no odd perfect number exists. More recently, Carl Pomerance has presented a heuristic argument suggesting that indeed no odd perfect number should exist. All perfect numbers are also Ore's harmonic numbers, and it has been conjectured as well that there are no odd Ore's harmonic numbers other than 1.
Any odd perfect number "N" must satisfy the following conditions:
In 1888, Sylvester stated:
Euler stated: "Whether (...) there are any odd perfect numbers is a most difficult question".
Minor results.
All even perfect numbers have a very precise form; odd perfect numbers either do not exist or are rare. There are a number of results on perfect numbers that are actually quite easy to prove but nevertheless superficially impressive; some of them also come under Richard Guy's strong law of small numbers:
Related concepts.
The sum of proper divisors gives various other kinds of numbers. Numbers where the sum is less than the number itself are called deficient, and where it is greater than the number, abundant. These terms, together with "perfect" itself, come from Greek numerology. A pair of numbers which are the sum of each other's proper divisors are called amicable, and larger cycles of numbers are called sociable. A positive integer such that every smaller positive integer is a sum of distinct divisors of it is a practical number.
By definition, a perfect number is a fixed point of the restricted divisor function , and the aliquot sequence associated with a perfect number is a constant sequence. All perfect numbers are also formula_15-perfect numbers, or Granville numbers.
A semiperfect number is a natural number that is equal to the sum of all or some of its proper divisors. A semiperfect number that is equal to the sum of all its proper divisors is a perfect number. Most abundant numbers are also semiperfect; abundant numbers which are not semiperfect are called weird numbers.

</doc>
<doc id="23672" url="https://en.wikipedia.org/wiki?curid=23672" title="Parthenon">
Parthenon

The Parthenon (; ; ) is a former temple on the Athenian Acropolis, Greece, dedicated to the goddess Athena, whom the people of Athens considered their patron. Construction began in 447 BC when the Athenian Empire was at the peak of its power. It was completed in 438 BC although decoration of the building continued until 432 BC. It is the most important surviving building of Classical Greece, generally considered the zenith of the Doric order. Its decorative sculptures are considered some of the high points of Greek art. The Parthenon is regarded as an enduring symbol of Ancient Greece, Athenian democracy and western civilization, and one of the world's greatest cultural monuments. The Greek Ministry of Culture is currently carrying out a program of selective restoration and reconstruction to ensure the stability of the partially ruined structure.
The Parthenon itself replaced an older temple of Athena, which historians call the Pre-Parthenon or Older Parthenon, that was destroyed in the Persian invasion of 480 BC. The temple is archaeoastronomically aligned to the Hyades. While a sacred building dedicated to the city's patron goddess, the Parthenon was actually used primarily as a treasury. For a time, it served as the treasury of the Delian League, which later became the Athenian Empire. In the final decade of the sixth century AD, the Parthenon was converted into a Christian church dedicated to the Virgin Mary.
After the Ottoman conquest, it was turned into a mosque in the early 1460s. On 26 September 1687, an Ottoman ammunition dump inside the building was ignited by Venetian bombardment. The resulting explosion severely damaged the Parthenon and its sculptures. From 1800 to 1803, Thomas Bruce, 7th Earl of Elgin removed some of the surviving sculptures with the alleged permission of the Ottoman Empire. These sculptures, now known as the Elgin Marbles or the Parthenon Marbles, were sold in 1816 to the British Museum in London, where they are now displayed. Since 1983 (on the initiative of Culture Minister Melina Mercouri), the Greek government has been committed to the return of the sculptures to Greece.
Etymology.
The origin of the Parthenon's name is from the Greek word παρθενών ("parthenon"), which referred to the "unmarried women's apartments" in a house and in the Parthenon's case seems to have been used at first only for a particular room of the temple; it is debated which room this is and how the room acquired its name. The Liddell–Scott–Jones "Greek–English Lexicon" states that this room was the western cella of the Parthenon. Jamauri D. Green holds that the parthenon was the room in which the peplos presented to Athena at the Panathenaic Festival was woven by the arrephoroi, a group of four young girls chosen to serve Athena each year. Christopher Pelling asserts that Athena Parthenos may have constituted a discrete cult of Athena, intimately connected with, but not identical to, that of Athena Polias. According to this theory, the name of the Parthenon means the "temple of the virgin goddess" and refers to the cult of Athena Parthenos that was associated with the temple. The epithet "parthénos" (), whose origin is also unclear, meant "maiden, girl", but also "virgin, unmarried woman" and was especially used for Artemis, the goddess of wild animals, the hunt, and vegetation, and for Athena, the goddess of strategy and tactics, handicraft, and practical reason. It has also been suggested that the name of the temple alludes to the maidens ("parthenoi"), whose supreme sacrifice guaranteed the safety of the city.
The first instance in which "Parthenon" definitely refers to the entire building is found in the writings of the 4th century BC orator Demosthenes. In 5th-century building accounts, the structure is simply called "ho naos" ("the temple"). The architects Iktinos and Callicrates are said to have called the building "Hekatompodos" ("the hundred footer") in their lost treatise on Athenian architecture, and, in the 4th century and later, the building was referred to as the "Hekatompedos" or the "Hekatompedon" as well as the Parthenon; the 1st-century-AD writer Plutarch referred to the building as the "Hekatompedon Parthenon".
Because the Parthenon was dedicated to the Greek goddess Athena, it has sometimes been referred to as the Temple of Minerva, the Roman name for Athena, particularly during the 19th century.
Function.
Although the Parthenon is architecturally a temple and is usually called so, it is not really one in the conventional sense of the word. A small shrine has been excavated within the building, on the site of an older sanctuary probably dedicated to Athena as a way to get closer to the goddess, but the Parthenon never hosted the cult of Athena Polias, patron of Athens: the cult image, which was bathed in the sea and to which was presented the "peplos", was an olivewood "xoanon", located at an older altar on the northern side of the Acropolis.
The colossal statue of Athena by Phidias was not related to any cult and is not known to have inspired any religious fervour. It did not seem to have any priestess, altar or cult name. 
According to Thucydides, Pericles once referred to the statue as a gold reserve, stressing that it "contained forty talents of pure gold and it was all removable". 
The Athenian statesman thus implies that the metal, obtained from contemporary coinage, could be used again without any impiety. 
The Parthenon should then be viewed as a grand setting for Phidias' votive statue rather than a cult site. It is said in many writings of the Greeks that there were many treasures stored inside the temple, such as Persian swords and small statue figures made of precious metals.
Archaeologist Joan Breton Connelly has recently argued for the coherency of the Parthenon’s sculptural program in presenting a succession of genealogical narratives that track Athenian identity back through the ages: from the birth of Athena, through cosmic and epic battles, to the final great event of the Athenian Bronze Age, the war of Erechtheus and Eumolpos. She argues a pedagogical function for the Parthenon’s sculptured decoration, one that establishes and perpetuates Athenian foundation myth, memory, values and identity. While some classicists, including Mary Beard, Peter Green, and Garry Wills have doubted or rejected Connelly's thesis, an increasing number of historians, archaeologists, and classical scholars support her work. They include: J.J. Pollitt, Brunilde Ridgway, Nigel Spivey, Caroline Alexander, A.E. Stallings, Rebecca Goldstein, Freeman Dyson, William St Clair, Donald Kagan, Gregory Nagy, and Angelos Chaniotis.
Early history.
Older Parthenon.
The first endeavor to build a sanctuary for on the site of the present Parthenon was begun shortly after the Battle of Marathon (c. 490–488 BC) upon a solid limestone foundation that extended and leveled the southern part of the Acropolis summit. This building replaced a "hekatompedon" (meaning "hundred-footer") and would have stood beside the archaic temple dedicated to "Athena Polias" ("of the city"). The Older or Pre-Parthenon, as it is frequently referred to, was still under construction when the Persians sacked the city in 480 BC and razed the Acropolis.
The existence of both the proto-Parthenon and its destruction were known from Herodotus, and the drums of its columns were plainly visible built into the curtain wall north of the Erechtheum. Further physical evidence of this structure was revealed with the excavations of Panagiotis Kavvadias of 1885–90. The findings of this dig allowed Wilhelm Dörpfeld, then director of the German Archaeological Institute, to assert that there existed a distinct substructure to the original Parthenon, called Parthenon I by Dörpfeld, not immediately below the present edifice as had been previously assumed. Dörpfeld's observation was that the three steps of the first Parthenon consisted of two steps of Poros limestone, the same as the foundations, and a top step of Karrha limestone that was covered by the lowest step of the Periclean Parthenon. This platform was smaller and slightly to the north of the final Parthenon, indicating that it was built for a wholly different building, now completely covered over. This picture was somewhat complicated by the publication of the final report on the 1885–90 excavations, indicating that the substructure was contemporary with the Kimonian walls, and implying a later date for the first temple.
If the original Parthenon was indeed destroyed in 480, it invites the question of why the site was left a ruin for thirty-three years. One argument involves the oath sworn by the Greek allies before the Battle of Plataea in 479 BC declaring that the sanctuaries destroyed by the Persians would not be rebuilt, an oath from which the Athenians were only absolved with the Peace of Callias in 450. The mundane fact of the cost of reconstructing Athens after the Persian sack is at least as likely a cause. However, the excavations of Bert Hodge Hill led him to propose the existence of a second Parthenon, begun in the period of Kimon after 468 BC. Hill claimed that the Karrha limestone step Dörpfeld thought was the highest of Parthenon I was in fact the lowest of the three steps of Parthenon II, whose stylobate dimensions Hill calculated at .
One difficulty in dating the proto-Parthenon is that at the time of the 1885 excavation the archaeological method of seriation was not fully developed; the careless digging and refilling of the site led to a loss of much valuable information. An attempt to discuss and make sense of the potsherds found on the Acropolis came with the two-volume study by Graef and Langlotz published in 1925–33. This inspired American archaeologist William Bell Dinsmoor to attempt to supply limiting dates for the temple platform and the five walls hidden under the re-terracing of the Acropolis. Dinsmoor concluded that the latest possible date for Parthenon I was no earlier than 495 BC, contradicting the early date given by Dörpfeld. Further, Dinsmoor denied that there were two proto-Parthenons, and held that the only pre-Periclean temple was what Dörpfeld referred to as Parthenon II. Dinsmoor and Dörpfeld exchanged views in the "American Journal of Archaeology" in 1935.
Present building.
In the mid-5th century BC, when the Athenian Acropolis became the seat of the Delian League and Athens was the greatest cultural centre of its time, Pericles initiated an ambitious building project that lasted the entire second half of the century. The most important buildings visible on the Acropolis today — the Parthenon, the Propylaia, the Erechtheion and the temple of Athena Nike — were erected during this period. The Parthenon was built under the general supervision of the artist Phidias, who also had charge of the sculptural decoration. The architects Ictinos and Callicrates began their work in 447 BC, and the building was substantially completed by 432, but work on the decorations continued until at least 431. Some of the financial accounts for the Parthenon survive and show that the largest single expense was transporting the stone from Mount Pentelicus, about from Athens, to the Acropolis. The funds were in part stolen by Pericles from the treasury of the Delian League, which was moved from the Panhellenic sanctuary at Delos to the Acropolis in 454 BC.
Architecture.
The Parthenon is a peripteral octastyle Doric temple with Ionic architectural features. It stands on a platform or stylobate of three steps. In common with other Greek temples, it is of post and lintel construction and is surrounded by columns ("peripteral") carrying an entablature. There are eight columns at either end ("octastyle") and seventeen on the sides. There is a double row of columns at either end. The colonnade surrounds an inner masonry structure, the "cella", which is divided into two compartments. At either end of the building the gable is finished with a triangular pediment originally filled with sculpture. The columns are of the Doric order, with simple capitals, fluted shafts and no bases. Above the architrave of the entablature is a frieze of carved pictorial panels (metopes), separated by formal architectural triglyphs, typical of the Doric order. Around the cella and across the lintels of the inner columns runs a continuous sculptured frieze in low relief. This element of the architecture is Ionic in style rather than Doric.
Measured at the stylobate, the dimensions of the base of the Parthenon are . The cella was 29.8 metres long by 19.2 metres wide (97.8 × 63.0 ft), with internal colonnades in two tiers, structurally necessary to support the roof. On the exterior, the Doric columns measure in diameter and are high. The corner columns are slightly larger in diameter. The Parthenon had 46 outer columns and 23 inner columns in total, each column containing 20 flutes. (A flute is the concave shaft carved into the column form.) The stylobate has an upward curvature towards its centre of on the east and west ends, and of on the sides. The roof was covered with large overlapping marble tiles known as imbrices and tegulae.
The Parthenon is regarded as the finest example of Greek architecture. The temple, wrote John Julius Cooper, "enjoys the reputation of being the most perfect Doric temple ever built. Even in antiquity, its architectural refinements were legendary, especially the subtle correspondence between the curvature of the stylobate, the taper of the naos walls and the "entasis" of the columns." "Entasis" refers to the slight diminution in diameter of the columns as they rise, though the observable effect on the Parthenon is considerably more subtle than on earlier temples. The stylobate is the platform on which the columns stand. As in many other classical Greek temples, it has a slight parabolic upward curvature intended to shed rainwater and reinforce the building against earthquakes. The columns might therefore be supposed to lean outwards, but they actually lean slightly inwards so that if they carried on, they would meet almost exactly a mile above the centre of the Parthenon; since they are all the same height, the curvature of the outer stylobate edge is transmitted to the architrave and roof above: "All follow the rule of being built to delicate curves", Gorham Stevens observed when pointing out that, in addition, the west front was built at a slightly higher level than that of the east front.
It is not universally agreed what the intended effect of these "optical refinements" was; they may serve as a sort of "reverse optical illusion". As the Greeks may have been aware, two parallel lines appear to bow, or curve outward, when intersected by converging lines. In this case, the ceiling and floor of the temple may seem to bow in the presence of the surrounding angles of the building. Striving for perfection, the designers may have added these curves, compensating for the illusion by creating their own curves, thus negating this effect and allowing the temple to be seen as they intended. It is also suggested that it was to enliven what might have appeared an inert mass in the case of a building without curves, but the comparison ought to be according to Smithsonian historian Evan Hadingham with the Parthenon's more obviously curved predecessors than with a notional rectilinear temple.
Some studies of the Acropolis, including the Parthenon, conclude that many of its proportions approximate the golden ratio. The Parthenon's façade as well as elements of its façade and elsewhere can be circumscribed by golden rectangles. This view that the golden ratio was employed in the design has been disputed in more recent studies.
Sculpture.
The cella of the Parthenon housed the chryselephantine statue of Athena Parthenos sculpted by Phidias and dedicated in 439 or 438 BC. The appearance of this is known from other images. The decorative stonework was originally highly coloured. The temple was dedicated to Athena at that time, though construction continued until almost the beginning of the Peloponnesian War in 432. By the year 438, the sculptural decoration of the Doric metopes on the frieze above the exterior colonnade, and of the Ionic frieze around the upper portion of the walls of the cella, had been completed. The richness of the Parthenon's frieze and metope decoration is in agreement with the function of the temple as a treasury. In the "opisthodomus" (the back room of the cella) were stored the monetary contributions of the Delian League, of which Athens was the leading member.
Only a few of the sculptures remain "in situ"; most of the surviving sculptures are today (controversially) in the British Museum in London as the Elgin Marbles, and the Athens Acropolis Museum, but a few pieces are also in the Louvre, and museums in Rome, Vienna and Palermo.
Metopes.
The frieze of the Parthenon's entablature contained ninety-two metopes, fourteen each on the east and west sides, thirty-two each on the north and south sides. They were carved in high relief, a practice employed until then only in treasuries (buildings used to keep votive gifts to the gods). According to the building records, the metope sculptures date to the years 446–440 BC. The metopes of the east side of the Parthenon, above the main entrance, depict the Gigantomachy (mythical battles between the Olympian gods and the Giants). The metopes of the west end show Amazonomachy (mythical battle of the Athenians against the Amazons). The metopes of the south side show the Thessalian Centauromachy (battle of the Lapiths aided by Theseus against the half-man, half-horse Centaurs). Metopes 13–21 are missing, but drawings from 1674 attributed to Jaques Carrey indicate a series of humans; these have been variously interpreted as scenes from the Lapith wedding, scenes from the early history of Athens and various myths. On the north side of the Parthenon, the metopes are poorly preserved, but the subject seems to be the sack of Troy.
The metopes present examples of the Severe Style in the anatomy of the figures' heads, in the limitation of the corporal movements to the contours and not to the muscles, and in the presence of pronounced veins in the figures of the Centauromachy. Several of the metopes still remain on the building, but, with the exception of those on the northern side, they are severely damaged. Some of them are located at the Acropolis Museum, others are in the British Museum, and one is at the Louvre museum.
In March 2011, archaeologists announced that they had discovered five metopes of the Parthenon in the south wall of the Acropolis, which had been extended when the Acropolis was used as a fortress. According to "Eleftherotypia" daily, the archaeologists claimed the metopes had been placed there in the 18th century when the Acropolis wall was being repaired. The experts discovered the metopes while processing 2,250 photos with modern photographic methods, as the white Pentelic marble they are made of differed from the other stone of the wall. It was previously presumed that the missing metopes were destroyed during the Morosini explosion of the Parthenon in 1687.
Frieze.
The most characteristic feature in the architecture and decoration of the temple is the Ionic frieze running around the exterior walls of the cella, which is the inside structure of the Parthenon. The bas-relief frieze was carved in situ; it is dated to 442 BC-438 BC.
One interpretation is that it depicts an idealized version of the Panathenaic procession from the Dipylon Gate in the Kerameikos to the Acropolis. In this procession held every year, with a special procession taking place every four years, Athenians and foreigners were participating to honour the goddess Athena, offering sacrifices and a new peplos (dress woven by selected noble Athenian girls called "ergastines").
Joan Breton Connelly offers a mythological interpretation for the frieze, one that is in harmony with the rest of the temple’s sculptural program which shows Athenian genealogy through a series of succession myths set in the remote past. She identifies the central panel above the door of the Parthenon as the pre-battle sacrifice of the daughter of King Erechtheus, a sacrifice that ensured Athenian victory over Eumolpos and his Thracian army. The great procession marching toward the east end of the Parthenon shows the post-battle thanksgiving sacrifice of cattle and sheep, honey and water, followed by the triumphant army of Erechtheus returning from their victory. This represents the very first Panathenaia set in mythical times, the model on which historic Panathenaic processions was based.
Pediments.
The traveller Pausanias, when he visited the Acropolis at the end of the 2nd century AD, only mentioned briefly the sculptures of the pediments (gable ends) of the temple, reserving the majority of his description for the gold and ivory statue of the goddess inside.
East pediment.
The east pediment narrates the birth of Athena from the head of her father, Zeus. According to Greek mythology, Zeus gave birth to Athena after a terrible headache prompted him to summon Hephaestus (the god of fire and the forge) for assistance. To alleviate the pain, he ordered Hephaestus to strike him with his forging hammer, and when he did, Zeus's head split open and out popped the goddess Athena in full armour. The sculptural arrangement depicts the moment of Athena's birth.
Unfortunately, the centrepieces of the pediment were destroyed even before Jacques Carrey created otherwise useful documentary drawings in 1674, so all reconstructions are subject to conjecture and speculation. The main Olympian gods must have stood around Zeus and Athena watching the wondrous event, with Hephaestus and Hera probably near them. The Carrey drawings are instrumental in reconstructing the sculptural arrangement beyond the center figures to the north and south.
West pediment.
The west pediment faced the Propylaia and depicted the contest between Athena and Poseidon during their competition for the honor of becoming the city's patron. Athena and Poseidon appear at the center of the composition, diverging from one another in strong diagonal forms, with the goddess holding the olive tree and the god of the sea raising his trident to strike the earth. At their flanks, they are framed by two active groups of horses pulling chariots, while a crowd of legendary personalities from Athenian mythology fills the space out to the acute corners of the pediment.
The work on the pediments lasted from 438 to 432 BC, and the sculptures of the Parthenon pediments are some of the finest examples of classical Greek art. The figures are sculpted in natural movement with bodies full of vital energy that bursts through their flesh, as the flesh in turn bursts through their thin clothing. The thin chitons reveal the body underneath as the focus of the composition. The distinction between gods and humans is blurred in the conceptual interplay between the idealism and naturalism bestowed on the stone by the sculptors. The pediments no longer exist.
Athena Parthenos.
The only piece of sculpture from the Parthenon known to be from the hand of Phidias was the statue of Athena housed in the "naos". This massive chryselephantine sculpture is now lost and known only from copies, vase painting, gems, literary descriptions and coins.
Later history.
Late antiquity.
A major fire broke out in the Parthenon shortly after the middle of the third century AD which destroyed the Parthenon's roof and much of the sanctuary's interior. Heruli pirates are also credited with sacking Athens in 276, and destroying most of the public buildings there, including the Parthenon. Repairs were made in the fourth century AD, possibly during the reign of Julian the Apostate. A new wooden roof overlaid with clay tiles was installed to cover the sanctuary. It sloped at a greater incline than the original roof and left the building's wings exposed.
The Parthenon survived as a temple dedicated to Athena for nearly one thousand years until Theodosius II decreed in 435 AD that all pagan temples in the Byzantine Empire be closed. At some point in the fifth century, Athena's great cult image was looted by one of the emperors and taken to Constantinople, where it was later destroyed, possibly during the siege of Constantinople during the Fourth Crusade in 1204 AD.
Christian church.
The Parthenon was converted into a Christian church in the final decade of the sixth century AD to become the Church of the Parthenos Maria (Virgin Mary), or the Church of the Theotokos (Mother of God). The orientation of the building was changed to face towards the east; the main entrance was placed at the building's western end, and the Christian altar and iconostasis were situated towards the building's eastern side adjacent to an apse built where the temple's pronaos was formerly located. A large central portal with surrounding side-doors was made in the wall dividing the cella, which became the church's nave, from the rear chamber, the church's narthex. The spaces between the columns of the "opisthodomus" and the peristyle were walled up, though a number of doorways still permitted access. Icons were painted on the walls and many Christian inscriptions were carved into the Parthenon's columns. These renovations inevitably led to the removal and dispersal of some of the sculptures. Those depicting gods were either possibly re-interpreted according to a Christian theme, or removed and destroyed.
The Parthenon became the fourth most important Christian pilgrimage destination in the Eastern Roman Empire, followed by Constantinople, Ephesos and Thessalonica. In 1018, the emperor Basil II went on a pilgrimage to Athens directly after his final victory over the Bulgarians for the sole purpose of worshipping at the Parthenon. In medieval Greek accounts it is called the Temple of Theotokos Atheniotissa and often indirectly referred to as famous without explaining exactly which temple they were referring to, thus establishing that it was indeed well known.
At the time of the Latin occupation, it became for about 250 years a Roman Catholic church of Our Lady. During this period a tower, used either as a watchtower or bell tower and containing a spiral staircase, was constructed at the southwest corner of the cella, and vaulted tombs were built beneath the Parthenon's floor.
Islamic mosque.
The precise circumstances under which the Turks appropriated it for use as a mosque are unclear; one account states that Mehmed II ordered its conversion as punishment for an Athenian plot against Ottoman rule. The apse became a mihrab, the tower previously constructed during the Roman Catholic occupation of the Parthenon was extended upwards to become a minaret, a minbar was installed, the Christian altar and iconostasis were removed, and the walls were whitewashed to cover icons of Christian saints and other Christian imagery.
Despite the alterations accompanying the Parthenon's conversion into a church and subsequently a mosque, its structure had remained basically intact. In 1667 the Turkish traveler Evliya Çelebi expressed marvel at the Parthenon's sculptures and figuratively described the building as "like some impregnable fortress not made by human agency". He composed a poetic supplication that it, as "a work less of human hands than of Heaven itself, should remain standing for all time". The French artist Jacques Carrey in 1674 visited the Acropolis and sketched the Parthenon's sculptural decorations. Early in 1687, an engineer named Plantier sketched the Parthenon for the Frenchman Graviers d’Ortières. These depictions, particularly those made by Carrey, provide important, and sometimes the only, evidence of the condition of the Parthenon and its various sculptures prior to the devastation it suffered in late 1687 and the subsequent looting of its art objects.
Destruction.
In 1687, the Parthenon was extensively damaged in the greatest catastrophe to befall it in its long history. The Venetians sent an expedition led by Francesco Morosini to attack Athens and capture the Acropolis. The Ottoman Turks fortified the Acropolis and used the Parthenon as a gunpowder magazine – despite having been forewarned of the dangers of this use by the 1656 explosion that severely damaged the Propylaea – and as a shelter for members of the local Turkish community. On 26 September a Venetian mortar round, fired from the Hill of Philopappus, blew up the magazine, and the building was partly destroyed. The explosion blew out the building's central portion and caused the cella's walls to crumble into rubble. Greek architect and archaeologist Kornilia Chatziaslani writes that "...three of the sanctuary’s four walls nearly collapsed and three-fifths of the sculptures from the frieze fell. Nothing of the roof apparently remained in place. Six columns from the south side fell, eight from the north, as well as whatever remained from eastern porch, except for one column. The columns brought down with them the enormous marble architraves, triglyphs and metopes." About three hundred people were killed in the explosion, which showered marble fragments over nearby Turkish defenders and caused large fires that burned until the following day and consumed many homes.
The following year, the Venetians abandoned Athens to avoid a confrontation with a large force the Turks had assembled at Chalcis; at that time, the Venetians had considered blowing up what remained of the Parthenon along with the rest of the Acropolis to deny its further use as a fortification to the Turks, but that idea was not pursued.
After the Turks had recaptured the Acropolis they used some of the rubble produced by this explosion to erect a smaller mosque within the shell of the ruined Parthenon. For the next century and a half, portions of the remaining structure were looted for building material and any remaining objects of value.
The 18th century was a period of Ottoman stagnation; as a result, many more Europeans found access to Athens, and the picturesque ruins of the Parthenon were much drawn and painted, spurring a rise in philhellenism and helping to arouse sympathy in Britain and France for Greek independence. Amongst those early travellers and archaeologists were James Stuart and Nicholas Revett, who were commissioned by the Society of Dilettanti to survey the ruins of classical Athens. What they produced was the first measured drawings of the Parthenon published in 1787 in the second volume of "Antiquities of Athens Measured and Delineated". In 1801, the British Ambassador at Constantinople, the Earl of Elgin, obtained a questionable "firman" (edict) from the Sultan, whose existence or legitimacy has not been proved until today, to make casts and drawings of the antiquities on the Acropolis, to demolish recent buildings if this was necessary to view the antiquities, and to remove sculptures from them.
Independent Greece.
When independent Greece gained control of Athens in 1832, the visible section of the minaret was demolished; only its base and spiral staircase up to the level of the architrave remain intact. Soon all the medieval and Ottoman buildings on the Acropolis were destroyed. However, the image of the small mosque within the Parthenon's cella has been preserved in Joly de Lotbinière's photograph, published in Lerebours's "Excursions Daguerriennes" in 1842: the first photograph of the Acropolis. The area became a historical precinct controlled by the Greek government. Today it attracts millions of tourists every year, who travel up the path at the western end of the Acropolis, through the restored Propylaea, and up the Panathenaic Way to the Parthenon, which is surrounded by a low fence to prevent damage.
Dispute over the marbles.
The dispute centres around the Parthenon Marbles removed by Thomas Bruce, 7th Earl of Elgin, from 1801 to 1803, which are in the British Museum. A few sculptures from the Parthenon are also in the Louvre in Paris, in Copenhagen, and elsewhere, but more than half are in the Acropolis Museum in Athens. A few can still be seen on the building itself. The Greek government has campaigned since 1983 for the British Museum to return the sculptures to Greece. The British Museum has steadfastly refused to return the sculptures, and successive British governments have been unwilling to force the Museum to do so (which would require legislation). Nevertheless, talks between senior representatives from Greek and British cultural ministries and their legal advisors took place in London on 4 May 2007. These were the first serious negotiations for several years, and there were hopes that the two sides may move a step closer to a resolution.
Restoration.
In 1975, the Greek government began a concerted effort to restore the Parthenon and other Acropolis structures. After some delay, a Committee for the Conservation of the Acropolis Monuments was established in 1983. The project later attracted funding and technical assistance from the European Union. An archaeological committee thoroughly documented every artifact remaining on the site, and architects assisted with computer models to determine their original locations. Particularly important and fragile sculptures were transferred to the Acropolis Museum. A crane was installed for moving marble blocks; the crane was designed to fold away beneath the roofline when not in use. In some cases, prior re-construction was found to be incorrect. These were dismantled, and a careful process of restoration began. Originally, various blocks were held together by elongated iron H pins that were completely coated in lead, which protected the iron from corrosion. Stabilizing pins added in the 19th century were not so coated, and corroded. Since the corrosion product (rust) is expansive, the expansion caused further damage by cracking the marble.

</doc>
<doc id="23673" url="https://en.wikipedia.org/wiki?curid=23673" title="Pachomius the Great">
Pachomius the Great

Saint Pachomius (, ca. 292–348), also known as Pachome and Pakhomius (), is generally recognized as the founder of Christian cenobitic monasticism. Coptic churches celebrate his feast day on 9 May, and Eastern Orthodox and Roman Catholic churches mark his feast on 15 May or 28 May. In the Lutheran Church, the saint is remembered as a renewer of the church, along with his contemporary (and fellow desert saint), Anthony of Egypt on January 17.
Life.
Saint Pachomius was born in 292 in Thebes (Luxor, Egypt) to pagan parents. According to his hagiography, at age 21, Pachomius was swept up against his will in a Roman army recruitment drive, a common occurrence during this period of turmoil and civil war. With several other youths, he was put onto a ship that floated down the Nile and arrived at Thebes in the evening. Here he first encountered local Christians, who customarily brought food and comfort daily to the impressed troops. This made a lasting impression, and Pachomius vowed to investigate Christianity further when he got out. He was able to leave the army without ever having to fight, was converted and baptized (314). 
Pachomius then came into contact with several well known ascetics and decided to pursue that path under the guidance of the hermit named Palaemon (317). One of his devotions, popular at the time, was praying with his arms stretched out in the form of a cross. After studying seven years with Palaemon, Pachomius set out to lead the life of a hermit near St. Anthony of Egypt, whose practices he imitated until Pachomius heard a voice in Tabennisi that told him to build a dwelling for the hermits to come to. An earlier ascetic named Macarius had created a number of proto-monasteries called lavra, or cells where holy men would live in a community setting who were physically or mentally unable to achieve the rigors of Anthony's solitary life. 
Pachomius established his first monastery between 318 and 323 at Tabennisi, Egypt. His elder brother John joined him, and soon more than 100 monks lived nearby. Pachomius set about organizing these cells into a formal organization. Until then, Christian asceticism had been solitary or "eremitic" with male or female monastics living in individual huts or caves and meeting only for occasional worship services. Pachomius created the community or "cenobitic" organization, in which male or female monastics lived together and held their property in common under the leadership of an abbot or abbess. Pachomius realized that some men, acquainted only with the eremitical life, might speedily become disgusted if the distracting cares of the cenobitical life were thrust too abruptly upon them. He therefore allowed them to devote their whole time to spiritual exercises, undertaking all the community's administrative tasks himself. The community hailed Pachomius as "Abba" (father), from which "Abbot" derives. 
The monastery at Tabennisi, though enlarged several times, soon became too small and a second was founded at Pabau (Faou). After 336, Pachomius spent most of his time at Pabau. Though Pachomius sometimes acted as lector for nearby shepherds, neither he nor any of his monks became priests. St Athanasius visited and wished to ordain him in 333, but Pachomius fled from him. Athanasius' visit was probably a result of Pachomius' zealous defence of orthodoxy against Arianism. Basil of Caesarea visited, then took many of Pachomius' ideas, which he adapted and implemented in Caesarea. This ascetic rule, or Ascetica, is still used today by the Eastern Orthodox Church, comparable to that of the Rule of St. Benedict in the West.
Death and legacy.
Pachomius continued as abbot to the cenobites for some forty years. During an epidemic (probably plague), Pachomius called the monks, strengthened their faith, and appointed his successor. Pachomius then died on 14 Pashons, 64 A.M. (9 May 348 A.D.)
By the time Pachomius died (c. 345) eight monasteries and several hundred monks followed his guidance. Within a generation, cenobic practices spread from Egypt to Palestine and the Judean Desert, Syria, North Africa and eventually Western Europe. The number of monks, rather than the number of monasteries, may have reached 7000. 
His reputation as a holy man has endured. As mentioned above, several liturgical calendars commemorate Pachomius. Among many miracles attributed to Pachomius, that though he had never learned the Greek or Latin tongues, he sometimes miraculously spoke them. Pachomius is also credited with being the first Christian to use and recommend use of a prayer rope.
Coptic literature.
Examples of purely Coptic literature are the works of Abba Antonius and Abba Pachomius, who spoke only Coptic, and the sermons and preachings of Abba Shenouda, who chose to write only in Coptic.
The Pachomian system tended to treat religious literature as mere written instructions.
Name.
The name of the saint is of Coptic origin: "pakhōm" from "akhōm" "eagle or falcon" ( "p"- at the beginning is the Coptic definite article). Into Greek it was adopted as Παχούμιος and Παχώμιος. By Greek folk etymology it was sometimes interpreted as "broad-shouldered" from παχύς "thick, large" and ὦμος "shoulder".
See also.
Further reading
References.
Notes

</doc>
<doc id="23674" url="https://en.wikipedia.org/wiki?curid=23674" title="Philosophical Investigations">
Philosophical Investigations

Philosophical Investigations () is a highly influential work by the 20th-century philosopher Ludwig Wittgenstein first published in 1953. In it, Wittgenstein discusses numerous problems and puzzles in the fields of semantics, logic, philosophy of mathematics, philosophy of psychology, philosophy of action, and the philosophy of mind. He puts forth the view that conceptual confusions surrounding language use are at the root of most philosophical problems, contradicting or discarding much of what he argued in his earlier work, the "Tractatus Logico-Philosophicus".
He alleges that the problems are traceable to a set of related assumptions about the nature of language, which themselves presuppose a particular conception of the essence of language. This conception is considered and ultimately rejected for being too general; that is, as an essentialist account of the nature of language it is simply too narrow to be able to account for the variety of things we do with language. Wittgenstein begins the book with a quotation from St. Augustine, whom he cites as a proponent of the generalized and limited conception that he then summarizes:
The individual words in language name objects—sentences are combinations of such names. In this picture of language we find the roots of the following idea: Every word has a meaning. This meaning is correlated with the word. It is the object for which the word stands.
He then sets out throughout the rest of the book to demonstrate the limitations of this conception, including, he argues, with many traditional philosophical puzzles and confusions that arise as a result of this limited picture. Within the Analytic tradition, the book is considered by many as being one of the most important philosophical works of the 20th century, and it continues to influence contemporary philosophers, especially those studying mind and language.
The text.
Editions.
The book was not ready for publication when Wittgenstein died in 1951. G. E. M. Anscombe translated Wittgenstein's manuscript, and it was first published in 1953. There are two popular editions of "Philosophical Investigations", both translated by Anscombe:
The text is divided into two parts, consisting of what Wittgenstein calls, in the preface, "Bemerkungen", translated by Anscombe as "remarks". In the first part, these remarks are rarely more than a paragraph long and are numbered sequentially. In the second part, the remarks are longer and numbered using Roman numerals. In the index, remarks from the first part are referenced by their number rather than page; however, references from the second part are cited by page number. The comparatively unusual nature of the second part is due to the fact that it comprises notes that Wittgenstein may have intended to re-incorporate into the first part. Subsequent to his death it was published as a "Part II" in the first, second and third editions. However, in light of continuing uncertainty about Wittgenstein's intentions regarding this material, the fourth edition (2009) re-titles "Part I" as "Philosophical Investigations" proper, and "Part II" as "Philosophy of Psychology – A Fragment."
Method and presentation.
"Philosophical Investigations" is unique in its approach to philosophy. A typical philosophical text presents a philosophical problem, summarizes and critiques various alternative approaches to solving it, presents its own approach, and then argues in favour of that approach. In contrast, Wittgenstein's book treats philosophy as an activity, rather along the lines of Socrates's famous method of maieutics; he has the reader work through various problems, participating actively in the investigation. Rather than presenting a philosophical problem and its solution, Wittgenstein engages in a dialogue, where he provides a thought experiment (a hypothetical example or situation), describes how one might be inclined to think about it, and then shows why that inclination suffers from conceptual confusion. The following is an excerpt from the first entry in the book that exemplifies this method:
...think of the following use of language: I send someone shopping. I give him a slip marked 'five red apples'. He takes the slip to the shopkeeper, who opens the drawer marked 'apples', then he looks up the word 'red' in a table and finds a colour sample opposite it; then he says the series of cardinal numbers—I assume that he knows them by heart—up to the word 'five' and for each number he takes an apple of the same colour as the sample out of the drawer.—It is in this and similar ways that one operates with words—"But how does he know where and how he is to look up the word 'red' and what he is to do with the word 'five'?" Well, I assume that he "acts" as I have described. Explanations come to an end somewhere.—But what is the meaning of the word 'five'? No such thing was in question here, only how the word 'five' is used.
This example is typical of the book's style. We can see each of the steps in Wittgenstein's method:
Similarly, Wittgenstein often uses the device of framing many of the remarks as a dialogue between himself and a disputant. For example, Remark 258 proposes a thought experiment in which a certain sensation is associated with the sign "S" written in a calendar. He then sets up a dialogue in which the disputant offers a series of ways of defining "S", and he meets each with a suitable objection, so drawing the conclusion that in such a case there is no "right" definition of "S".
Through such thought experiments, Wittgenstein attempts to get the reader to come to certain difficult philosophical conclusions independently; he does not simply argue in favor of his own theories.
Language, meaning, and use.
The "Investigations" deals largely with the difficulties of language and meaning. Wittgenstein viewed the tools of language as being fundamentally simple, and he believed that philosophers had obscured this simplicity by misusing language and by asking meaningless questions. He attempted in the "Investigations" to make things clear: ""Der Fliege den Ausweg aus dem Fliegenglas zeigen""—to show the fly the way out of the fly bottle.
Meaning is use.
A common summary of his argument is that meaning is use—words are not defined by reference to the objects they designate, nor by the mental representations one might associate with them, but by how they are used. For example, this means there is no need to postulate that there is something called "good" that exists independently of any good deed. This anthropological perspective contrasts with Platonic realism and with Gottlob Frege's notions of sense and reference. This argument has been labeled by some authors as "anthropological holism."
Meaning and definition.
Wittgenstein rejects a variety of ways of thinking about what the meaning of a word is, or how meanings can be identified. He shows how, in each case, the "meaning" of the word presupposes our ability to use it. He first asks the reader to perform a thought experiment: to come up with a definition of the word "game". While this may at first seem a simple task, he then goes on to lead us through the problems with each of the possible definitions of the word "game". Any definition that focuses on amusement leaves us unsatisfied since the feelings experienced by a world class chess player are very different from those of a circle of children playing Duck Duck Goose. Any definition that focuses on competition will fail to explain the game of catch, or the game of solitaire. And a definition of the word "game" that focuses on rules will fall on similar difficulties.
The essential point of this exercise is often missed. Wittgenstein's point is not that it is impossible to define "game", but that "we don't have a definition, and we don't need one", because even without the definition, we "use" the word successfully. Everybody understands what we mean when we talk about playing a game, and we can even clearly identify and correct inaccurate uses of the word, all without reference to any definition that consists of necessary and sufficient conditions for the application of the concept of a game. The German word for "game", "Spiele/Spiel", has a different sense than in English; the meaning of "Spiele" also extends to the concept of "play" and "playing." This German sense of the word may help readers better understand Wittgenstein's context in the remarks regarding games.
Wittgenstein argues that definitions emerge from what he termed "forms of life", roughly the culture and society in which they are used. Wittgenstein stresses the social aspects of cognition; to see how language works for most cases, we have to see how it functions in a specific social situation. It is this emphasis on becoming attentive to the social backdrop against which language is rendered intelligible that explains Wittgenstein's elliptical comment that "If a lion could talk, we could not understand him." However, in proposing the thought experiment involving the fictional character, Robinson Crusoe, a captain shipwrecked on a desolate island with no other inhabitant, Wittgenstein shows that language is not in all cases a social phenomenon (although, they are for most case); instead the criterion for a language is grounded in a set of interrelated normative activities: teaching, explanations, techniques and criteria of correctness. In short, it is essential that a language is shareable, but this does not imply that for a language to function that it is in fact already shared.
Wittgenstein rejects the idea that ostensive definitions can provide us with the meaning of a word. For Wittgenstein, the thing that the word stands for does "not" give the meaning of the word. Wittgenstein argues for this making a series of moves to show that to understand an ostensive definition presupposes an understanding of the way the word being defined is used. So, for instance, there is no difference between pointing to a piece of paper, to its colour, or to its shape; but understanding the difference is crucial to using the paper in an ostensive definition of a shape or of a colour.
Family resemblances.
Why is it that we are sure a particular activity — e.g. Olympic target shooting — is a game while a similar activity — e.g. military sharp shooting — is not? Wittgenstein's explanation is tied up with an important analogy. How do we recognize that two people we know are related to one another? We may see similar height, weight, eye color, hair, nose, mouth, patterns of speech, social or political views, mannerisms, body structure, last names, etc. If we see enough matches we say we've noticed a family resemblance. It is perhaps important to note that this is not always a conscious process — generally we don't catalog various similarities until we reach a certain threshold, we just intuitively "see" the resemblances. Wittgenstein suggests that the same is true of language. We are all familiar (i.e. socially) with enough things which "are games" and enough things which "are not games" that we can categorize new activities as either games or not.
This brings us back to Wittgenstein's reliance on indirect communication, and his reliance on thought-experiments. Some philosophical confusions come about because we aren't able to "see" family resemblances. We've made a mistake in understanding the vague and intuitive rules that language uses, and have thereby tied ourselves up in philosophical knots. He suggests that an attempt to untangle these knots requires more than simple deductive arguments pointing out the problems with some particular position. Instead, Wittgenstein's larger goal is to try to divert us from our philosophical problems long enough to become aware of our intuitive ability to "see" the family resemblances.
Language-games.
Wittgenstein develops this discussion of games into the key notion of a "language-game". Wittgenstein introduces the term using simple examples, but intends it to be used for the many ways in which we use language. The central component of language games is that they are uses of language, and language is used in multifarious ways. For example, in one language-game, a word might be used to stand for (or refer to) an object, but in another the same word might be used for giving orders, or for asking questions, and so on. The famous example is the meaning of the word "game". We speak of various kinds of games: board games, betting games, sports, "war games". These are all different uses of the word "games". Wittgenstein also gives the example of "Water!", which can be used as an exclamation, an order, a request, or as an answer to a question. The meaning, the word has, depends on the language-game in which it is used. Another way Wittgenstein puts the point is that the word "water" has no meaning apart from its use within a language-game. One might use the word as an order to have someone else bring you a glass of water. But it can also be used to warn someone that the water has been poisoned. One might even use the word as code by members of a secret society.
Wittgenstein does not limit the application of his concept of language games to word-meaning. He also applies it to sentence-meaning. For example, the sentence "Moses did not exist" (§79) can mean various things. Wittgenstein argues that independently of use the sentence does not yet 'say' anything. It is 'meaningless' in the sense of being insignificant for a particular purpose. It only acquires significance if we fix it within some context of use. Thus, it fails to say anything because the sentence as such does not yet determine some particular use. The sentence is only meaningful when it is used to say something. For instance, it can be used so as to say that no person or historical figure fits the set of descriptions attributed to the person that goes by the name of "Moses". But it can also mean that the leader of the Israelites was not called Moses. Or that there cannot have been anyone who accomplished all that the Bible relates of Moses. Etc. What the sentence means thus depends on its context of use.
Rules.
One general characteristic of games that Wittgenstein considers in detail is the way in which they consist in following rules. Rules constitute a family, rather than a class that can be explicitly defined. As a consequence, it is not possible to provide a definitive account of what it is to follow a rule. Indeed, he argues that "any" course of action can be made out to accord with some particular rule, and that therefore a rule cannot be used to explain an action. Rather, that one is following a rule or not is to be decided by looking to see if the actions conform to the expectations in the particular "form of life" in which one is involved. Following a rule is a social activity.
Private language.
Wittgenstein also ponders the possibility of a language that talks about those things that are known only to the user, whose content is inherently private. The usual example is that of a language in which one names one's sensations and other subjective experiences, such that the meaning of the term is decided by the individual alone. For example, the individual names a particular sensation, on some occasion, 'S', and intends to use that word to refer to that sensation. Such a language Wittgenstein calls a "private language".
Wittgenstein presents several perspectives on the topic. One point he makes is that it is incoherent to talk of "knowing" that one is in some particular mental state. Whereas others can learn of my pain, for example, I simply "have" my own pain; it follows that one does not "know" of one's own pain, one simply "has" a pain. For Wittgenstein, this is a grammatical point, part of the way in which the language-game involving the word "pain" is played.
Although Wittgenstein certainly argues that the notion of private language is incoherent, because of the way in which the text is presented the exact nature of the argument is disputed. First, he argues that a private language is not really a language at all. This point is intimately connected with a variety of other themes in his later works, especially his investigations of "meaning". For Wittgenstein, there is no single, coherent "sample" or "object" that we can call "meaning". Rather, the supposition that there are such things is the source of many philosophical confusions. Meaning is a complicated phenomenon that is woven into the fabric of our lives. A good first approximation of Wittgenstein's point is that meaning is a "social" event; meaning happens "between" language users. As a consequence, it makes no sense to talk about a private language, with words that "mean" something in the absence of other users of the language.
Wittgenstein also argues that one couldn't possibly "use" the words of a private language. He invites the reader to consider a case in which someone decides that each time she has a particular sensation she will place a sign S in a diary. Wittgenstein points out that in such a case one could have no criteria for the correctness of one's use of S. Again, several examples are considered. One is that perhaps using S involves mentally consulting a table of sensations, to check that one has associated S correctly; but in this case, how could the mental table be checked for its correctness? It is "s if someone were to buy several copies of the morning paper to assure himself that what it said was true", as Wittgenstein puts it. One common interpretation of the argument is that while one may have direct or privileged access to one's "current" mental states, there is no such infallible access to identifying previous mental states that one had in the past. That is, the only way to check to see if one has applied the symbol S correctly to a certain mental state is to introspect and determine whether the current sensation is identical to the sensation previously associated with S. And while identifying one's current mental state of remembering may be infallible, whether one remembered correctly is not infallible. Thus, for a language to be used at all it must have some public criterion of identity.
Often, what is widely regarded as a deep philosophical problem will vanish, argues Wittgenstein, and eventually be seen as a confusion about the significance of the words that philosophers use to frame such problems and questions. It is only in this way that it is interesting to talk about something like a "private language" — i.e., it is helpful to see how the "problem" results from a misunderstanding.
To sum up: Wittgenstein asserts that, if something is a language, it "cannot" be (logically) private; and if something "is" private, it is not (and cannot be) a language.
Wittgenstein's beetle.
Another point that Wittgenstein makes against the possibility of a private language involves the beetle-in-a-box thought experiment. He asks the reader to imagine that each person has a box, inside of which is something that everyone intends to refer to with the word "beetle". Further, suppose that no one can look inside another's box, and each claims to know what a "beetle" is only by examining their own box. Wittgenstein suggests that, in such a situation, the word "beetle" could not be the name of a thing, because supposing that each person has something completely different in their boxes (or nothing at all) does not change the meaning of the word; the beetle as a private object "drops out of consideration as irrelevant". Thus, Wittgenstein argues, if we can talk about something, then it is not "private", in the sense considered. And, contrapositively, if we consider something to be indeed private, it follows that we "cannot talk about it".
Kripke's account.
The discussion of private languages was revitalized in 1982 with the publication of Saul Kripke's book "Wittgenstein on Rules and Private Language". In this work, Kripke uses Wittgenstein's text to develop a particular type of skepticism about rules that stresses the "communal" nature of language-use as grounding meaning. Kripke's version of Wittgenstein, although philosophically interesting, has been facetiously called Kripkenstein, with some scholars such as Gordon Baker, Peter Hacker, Colin McGinn, and John McDowell seeing it as a radical misinterpretation of Wittgenstein's text.
Mind.
Wittgenstein's investigations of language lead to several issues concerning the mind. His key target of criticism is any form of extreme mentalism that posits mental states that are entirely unconnected to the subject's environment. For Wittgenstein, thought is inevitably tied to language, which is inherently social; therefore, there is no 'inner' space in which thoughts can occur. Part of Wittgenstein's credo is captured in the following proclamation: "An 'inner process' stands in need of outward criteria." This follows primarily from his conclusions about private languages: similarly, a private mental state (a sensation of pain, for example) cannot be adequately discussed without public criteria for identifying it.
According to Wittgenstein, those who insist that consciousness (or any other apparently subjective mental state) is conceptually unconnected to the external world are mistaken. Wittgenstein explicitly criticizes so-called conceivability arguments: "Could one imagine a stone's having consciousness? And if anyone can do so—why should that not merely prove that such image-mongery is of no interest to us?" He considers and rejects the following reply as well:
"But if I suppose that someone is in pain, then I am simply supposing that he has just the same as I have so often had." — That gets us no further. It is as if I were to say: "You surely know what 'It is 5 o'clock here' means; so you also know what 'It's 5 o'clock on the sun' means. It means simply that it is just the same there as it is here when it is 5 o'clock." — The explanation by means of "identity" does not work here.
Thus, according to Wittgenstein, mental states are intimately connected to a subject's environment, especially their linguistic environment, and conceivability or imaginability arguments that claim otherwise are misguided. Wittgenstein has also said that "language is inherent and transcendental", which is also not difficult to understand, since we can only comprehend and explain transcendental affairs through language.
Wittgenstein and behaviorism.
From his remarks on the importance of public, observable behavior (as opposed to private experiences), it may seem that Wittgenstein is simply a behaviorist—one who thinks that mental states are nothing over and above certain behavior. However, Wittgenstein resists such a characterization; he writes (considering what an objector might say):
"Are you not really a behaviourist in disguise? Aren't you at bottom really saying that everything except human behaviour is a fiction?" — If I do speak of a fiction, then it is of a "grammatical" fiction.
Clearly, Wittgenstein did not want to be a behaviorist, nor did he want to be a cognitivist or a phenomenologist. He is, of course, primarily concerned with facts of linguistic usage. However, some argue that Wittgenstein is basically a behaviorist because he considers facts about language use as all there is. Such a claim is controversial, since it is not explicitly endorsed in the "Investigations".
"Seeing that" vs. "seeing as".
In addition to ambiguous sentences, Wittgenstein discussed figures that can be seen and understood in two different ways. Often one can see something in a straightforward way — seeing "that" it is a rabbit, perhaps. But, at other times, one notices a particular aspect — seeing it "as" something.
An example Wittgenstein uses is the "duckrabbit", an ambiguous image that can be "seen as" either a duck or a rabbit. When one looks at the duck-rabbit and sees a rabbit, one is not "interpreting" the picture as a rabbit, but rather "reporting" what one sees. One just sees the picture as a rabbit. But what occurs when one sees it first as a duck, then as a rabbit? As the gnomic remarks in the "Investigations" indicate, Wittgenstein isn't sure. However, he is sure that it could not be the case that the external world stays the same while an 'internal' cognitive change takes place.
Relation to the "Tractatus".
According to the standard reading, in the "Philosophical Investigations" Wittgenstein repudiates many of his own earlier views, expressed in the "Tractatus Logico-Philosophicus". The "Tractatus", as Bertrand Russell saw it (though it should be noted that Wittgenstein took strong exception to Russell's reading), had been an attempt to set out a logically perfect language, building on Russell's own work. In the years between the two works Wittgenstein came to reject the idea that underpinned logical atomism, that there were ultimate "simples" from which a language should, or even could, be constructed.
In remark #23 of "Philosophical Investigations" he points out that the practice of human language is more complex than the simplified views of language that have been held by those who seek to explain or simulate human language by means of a formal system. It would be a disastrous mistake, according to Wittgenstein, to see language as being in any way analogous to formal logic.
Besides stressing the "Investigations"' opposition to the "Tractatus", there are critical approaches which have argued that there is much more continuity and similarity between the two works than supposed. One of these is the New Wittgenstein approach.
Norman Malcolm credits Piero Sraffa with providing Wittgenstein with the conceptual break that founded the "Philosophical Investigations", by means of a rude gesture on Sraffa's part:
"Wittgenstein was insisting that a proposition and that which it describes must have the same 'logical form', the same 'logical multiplicity', Sraffa made a gesture, familiar to Neapolitans as meaning something like disgust or contempt, of brushing the underneath of his chin with an outward sweep of the finger-tips of one hand. And he asked: 'What is the logical form of that?'"
Notes.
"Remarks in Part I of "Investigations" are preceded by the symbol ""§"". Remarks in Part II are referenced by their Roman numeral or their page number in the third edition.

</doc>
<doc id="23677" url="https://en.wikipedia.org/wiki?curid=23677" title="Poul Anderson">
Poul Anderson

Poul William Anderson (November 25, 1926 – July 31, 2001) was an American science fiction author who began his career during the Golden Age of the genre and continued to write and remain popular into the 21st century. Anderson also authored several works of fantasy, historical novels, and a prodigious number of short stories. He received numerous awards for his writing, including seven Hugo Awards and three Nebula Awards.
Biography.
Poul Anderson was born on November 25, 1926, in Bristol, Pennsylvania, of Scandinavian parents.
Shortly after his birth, his father, Anton Anderson, an engineer, moved the family to Texas, where they lived for over ten years. Following Anton Anderson's death, his widow took her children to Denmark. The family returned to the United States after the outbreak of World War II, settling eventually on a Minnesota farm. (The frame story of "Three Hearts and Three Lions", before the fantasy part begins, is partly set in the pre-WWII Denmark which the young Anderson personally experienced.) 
While he was an undergraduate student at the University of Minnesota, Anderson's first stories were published by John W. Campbell in "Astounding Science Fiction": "Tomorrow's Children" by Anderson and F. N. Waldrop in March 1947 and a sequel, "Chain of Logic" by Anderson alone, in July. He earned his B.A. in physics with honors but made no serious attempt to work as a physicist; instead he became a free-lance writer after his graduation in 1948—and placed his third story in the December "Astounding".
Anderson married Karen Kruse in 1953 and moved with her to the San Francisco Bay area. Their daughter Astrid (now married to science fiction author Greg Bear) was born in 1954. They made their home in Orinda, California. Over the years Poul gave many readings at The Other Change of Hobbit bookstore in Berkeley, and his wife later donated his typewriter and desk to the store. He died of cancer on July 31, 2001, after a month in the hospital. A few of his novels were first published posthumously.
Anderson was a founding member of the Society for Creative Anachronism in 1966 and of the Swordsmen and Sorcerers' Guild of America, also in the mid-1960s. The latter was a loose-knit group of Heroic Fantasy authors led by Lin Carter, originally eight in number, with entry by credentials as a fantasy writer alone. He was the sixth President of Science Fiction and Fantasy Writers of America, taking office in 1972. 
Robert A. Heinlein dedicated his 1985 novel "The Cat Who Walks Through Walls" to Anderson and eight of the other members of the Citizens' Advisory Council on National Space Policy.
The Science Fiction Writers of America made him its 16th SFWA Grand Master in 1998 and the Science Fiction and Fantasy Hall of Fame inducted him in 2000, its fifth class of two deceased and two living writers.
Political, moral and literary themes.
Anderson is probably best known for adventure stories in which larger-than-life characters succeed gleefully or fail heroically. His characters were nonetheless thoughtful, often introspective, and well developed. His plot lines frequently involved the application of social and political issues in a speculative manner appropriate to the science fiction genre. He also wrote some quieter works, generally of shorter length, which appeared more often during the latter part of his career.
Much of his science fiction is thoroughly grounded in science (with the addition of unscientific but standard speculations such as faster-than-light travel). A specialty was imagining scientifically plausible non-Earthlike planets. Perhaps the best known was the planet of "The Man Who Counts"; Anderson adjusted its size and composition so that humans could live in the open air but flying intelligent aliens could evolve, and he explored consequences of these adjustments.
Space and liberty.
In many stories, Anderson commented on society and politics. Whatever other vicissitudes his views went through, he firmly retained his belief in the direct and inextricable connection between human liberty and expansion into space, for which reason he strongly cried out against any idea of space exploration being "a waste of money" or "unnecessary luxury".
The connection between space flight and freedom is clearly (as is stated explicitly in some of the stories) an extension of the nineteenth-century American concept of the Frontier, where malcontents can advance further and claim some new land, and pioneers either bring life to barren asteroids (as in "Tales of the Flying Mountains") or settle on Earth-like planets teeming with life, but not intelligent forms (such as New Europe in "Star Fox").
As he repeatedly expressed in his nonfiction essays, Anderson firmly held that going into space was not an unnecessary luxury but an existential need, and that abandoning space would doom humanity to "a society of brigands ruling over peasants".
This is graphically expressed in the chilling short story "Welcome". In it, humanity has abandoned space and is left with an overcrowded Earth where a small elite not only treats all the rest as chattel slaves, but also regularly practices cannibalism, its members getting their chefs to prepare "roast suckling coolie" for their banquets.
Conversely, in the bleak Orwellian world of "The High Ones" where the Soviets have won the Third World War and gained control of the whole world, the dissidents still have some hope, precisely because space flight has not been abandoned. By the end of the story, rebels have established themselves at another stellar system—where their descendants, the reader is told, would eventually build a liberating fleet and set out back to Earth.
World government.
While horrified by the prospect of the Soviets winning complete rule over the Earth, Anderson was not enthusiastic about having Americans in that role either. Several stories and books describing the aftermath of a total American victory in another world war, such as "Sam Hall" and its loose sequel "Three Worlds to Conquer" as well as "Shield", are scarcely less bleak than the above-mentioned depictions of a Soviet victory. Like Heinlein in "Solution Unsatisfactory", Anderson assumed that the imposition of an American military rule over the rest of the world would necessarily entail the destruction of American democracy and the imposition of a harsh tyrannical rule over the United States' own citizens.
Both Anderson's depiction of a Soviet-dominated world and that of an American-dominated one mention a rebellion breaking out in Brazil in the early 21st century, which is in both cases brutally put down by the dominant world power—the Brazilian rebels being characterized as "counter-revolutionaries" in the one case and as "communists" in the other.
In the early years of the Cold War—when he had been, as described by his later, more conservative self, a "flaming liberal"—Anderson pinned his hopes on the United Nations developing into a true world government. This is especially manifest in "Un-man", a future thriller where the Good Guys are agents of the UN Secretary General working to establish a world government while the Bad Guys are nationalists (especially American nationalists) who seek to preserve their respective nations' sovereignty at all costs. (The title has a double meaning: the hero is literally a UN man and has superhuman abilities which make his enemies fear him as an "un-man").
In later years Anderson completely repudiated this idea (a half-humorous remnant is the beginning of "Tau Zero": a future where the nations of the world entrusted Sweden with overseeing disarmament and found themselves living under the rule of the Swedish Empire). In "The Star Fox", his unfavorable depiction of a future peace group called "World Militants for Peace" indicates clearly where he stood with regard to the Vietnam War, raging when the book was published. A more explicit expression of the same appears in the later "The Shield of Time" where a time-traveling young American woman from the 1990s pays a brief visit to a university campus of the 1960s and is not enthusiastic about what she sees there.
Libertarianism.
Instead of a world government, the above-mentioned "Shield" resolves the problem of an American-dominated world dictatorship in a truly libertarian manner: The protagonist, who is hunted by various power groups for the secret of a personal impregnable force field which he brought from Mars, finally decides to simply reveal it to the entire world, so that every individual could thumb his or her nose at each and every Authority.
Anderson often returned to libertarianism and to the business leader as hero, most notably his character Nicholas van Rijn. Van Rijn is, however, far from the modern type of business executive, being a kind of throwback to the merchant venturer of the Dutch Golden Age of the 17th century. If he spends any time in boardrooms or plotting corporate takeovers, the reader remains ignorant of it, since virtually all his appearances are in the wilds of a space frontier.
Beginning in the 1970s, Anderson's historically grounded works were influenced by the theories of the historian John K. Hord, who argued that all empires follow the same broad cyclical pattern, in which the Terran Empire of the Dominic Flandry spy stories fit neatly.
The writer Sandra Miesel (1978) has argued that Anderson's overarching theme is the struggle against entropy and the heat death of the universe, a condition of perfect uniformity where nothing can happen.
Fairness to the adversaries.
In his numerous books and stories depicting conflict in science-fictional or fantasy settings, Anderson takes trouble to make both sides' points of view comprehensible. Even where there can be no doubt as to whose side the author is on, the antagonists are usually not depicted as villains but as honourable on their own terms. The reader is given access to their thoughts and feelings, and they often have a tragic dignity in defeat. Typical examples are "The Winter of the World" and "The People of the Wind".
A common theme in Anderson's works, and one with obvious origins in the Northern European legends, is that doing the "right" (wisest) thing often involves performing actions that, at face value, seem dishonorable, illegal, destructive, or downright evil. "The Man who Counts", Nicholas van Rijn is "The Man" because he is prepared to be tyrannical and callously manipulative so that he and his companions can survive. In "High Treason" the protagonist disobeys orders and betrays his subordinates to prevent a war crime that would bring severe retribution upon Humanity. In "A Knight of Ghosts and Shadows", Dominic Flandry first (effectively) lobotomizes his own son and then bombards the home planet of the Chereionite race in order to do his duty and prop up the Terran Empire. These actions affect their characters in different ways, and dealing with the repercussions of having done the "right" (but unpleasant) thing is often the major focus of his short stories. The general lesson seems to be that guilt is the penalty for action.
In "The Star Fox", a relationship of grudging respect is built up between the hero, space privateer Gunnar Heim, and his enemy Cynbe, an exceptionally gifted Alerione trained from a young age to understand his species' human enemies to the point of being alienated from his own kind. In the final scene, Cynbe challenges Heim to a space battle which only one of them would survive. Heim accepts, whereupon Cynbe says, "I thank you, my brother."
Underestimating "primitives" as a costly mistake.
Anderson set much of his work in the past, often with the addition of magic, or in alternate or future worlds that resemble past eras. A specialty was his ancestral Scandinavia, as in his novel versions of the legends of Hrólf Kraki ("Hrolf Kraki's Saga") and Haddingus ("The War of the Gods"). Frequently he presented such worlds as superior to the dull, over-civilized present. Notable depictions of this superiority are the prehistoric world of "The Long Remembering", the quasi-medieval society of "No Truce with Kings", and the untamed Jupiter of "Call Me Joe" and "Three Worlds to Conquer". He handled the lure and power of atavism satirically in "Pact", critically in "The Queen of Air and Darkness" and "The Night Face", and tragically in "Goat Song".
His 1965 novel, "The Corridors of Time", alternates between the European Stone Age and a repressive future. In this vision of tomorrow, almost everyone is either an agricultural serf or an industrial slave, but the rulers genuinely believe they are creating a better world. Set largely in Denmark, it treats the Neolithic society with knowledge and respect while not hiding its own faults. It is there that the protagonist, having access to literally all periods of the past and future, finally decides to settle down and finds a happy and satisfying life.
In many stories, a representative of a technologically advanced society underestimates "primitives" and pays a high price for it. In "The High Crusade", aliens who land in medieval England in the expectation of an easy conquest find that they are not immune to swords and arrows. In "The Only Game in Town", a Mongol warrior, while not knowing that the two "magicians" he meets are time travelers from the future, correctly guesses their intentions—and captures them with the help of the "magic" flashlight they had given him in an attempt to impress him. In another time-travel tale, "The Shield of Time", a "time policeman" from the Twentieth Century, equipped with information and technologies from much further in the future, is outwitted by a medieval knight and barely escapes with his life. Yet another story, "The Man Who Came Early", features a 20th-century United States Army soldier stationed in Iceland who is transported to the tenth century. Although he is full of ideas, his lack of practical knowledge of how to implement them and his total unfamiliarity with the technology and customs of the period lead to his downfall.
Anderson wrote "Uncleftish Beholding", an introduction to atomic theory, using only Germanic-rooted words. Fitting his love for olden years, this kind of learned writing has been named Ander-Saxon after him.
Tragic conflicts.
The story told in "The Shield of Time" is also an example of a tragic conflict, another common theme in Anderson's writing. The knight tries to do his best in terms of his own society and time, but his actions might bring about a horrible Twentieth Century (even more horrible than the one we know). Therefore, the Time Patrol protagonists, who like the young knight and wish him well (the female protagonist comes close to falling in love with him), have no choice but to fight and ultimately kill him.
In "The Sorrow of Odin the Goth" a time-travelling American anthropologist is assigned to study the culture of an ancient Gothic tribe by regular visits every few decades. Gradually he is drawn into close involvement, feeling protective towards the Goths (many of them his own descendants, following a brief and poignant liaison with a Gothic girl who died in childbirth), and they identify him as the god Odin/Wodan. Then he finds that he must cruelly betray his beloved Goths, since a ballad says that Odin did so; failure to fulfill his prescribed role might change history and bring the whole of the Twentieth Century as we know it crashing down. In the final scene he cries out in anguish: "Not even the gods can defy the Norns!"—giving a new twist to this central aspect of the Norse religion.
In "The Pirate", the hero is duty-bound to deny a band of people from societies blighted by poverty the chance for a new start on a new planet, because their settling the planet would eradicate the remnants of the artistic and articulate beings who lived there before. A similar theme but with much higher stakes appears in "Sister Planet": although terraforming Venus would provide new hope to starving people on the overcrowded Earth, it would exterminate Venus's just-discovered intelligent race, and the hero can avert that genocide only by murdering his best friends.
In "Delenda Est" the stakes are the highest imaginable. Time-travelling outlaws have created a new 20th Century—"not better or worse, just completely different". The hero can fight the outlaws and restore his (and our) familiar history, but only at the price of totally destroying the world which has taken its place. "Risking your neck in order to negate a world full of people like yourself" is how the hero describes what he eventually undertakes.
Fictional appearances.
Philip K. Dick's story "Waterspider" features Poul Anderson as one of the main characters.
In the opening of S.M. Stirling's novel "In the Courts of the Crimson Kings", a group of science fiction authors, including Poul Anderson, watch first contact with the book's Martians while attending an SF convention. Poul supplies the beer.

</doc>
<doc id="23678" url="https://en.wikipedia.org/wiki?curid=23678" title="Panspermia">
Panspermia

Panspermia () is the hypothesis that life exists throughout the Universe, distributed by meteoroids, asteroids, comets, planetoids, and, also, by spacecraft in the form of unintended contamination by microorganisms.
Panspermia is a hypothesis proposing that microscopic life forms that can survive the effects of space, such as extremophiles, become trapped in debris that is ejected into space after collisions between planets and small Solar System bodies that harbor life. Some organisms may travel dormant for an extended amount of time before colliding randomly with other planets or intermingling with protoplanetary disks. If met with ideal conditions on a new planet's surfaces, the organisms become active and the process of evolution begins. Panspermia is not meant to address how life began, just the method that may cause its distribution in the Universe.
Pseudo-panspermia (sometimes called ""soft panspermia"" or ""molecular panspermia"") argues that the pre-biotic organic building blocks of life originated in space and were incorporated in the solar nebula from which the planets condensed and were further—and continuously—distributed to planetary surfaces where life then emerged (abiogenesis). From the early 1970s it was becoming evident that interstellar dust consisted of a large component of organic molecules. Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.
The chemistry leading to Life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. Though life is confirmed only on the Earth, some think that extraterrestrial life is not only plausible, but probable or inevitable. Other planets and moons in the Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilizations.
History.
The first known mention of the term was in the writings of the 5th century BC Greek philosopher Anaxagoras. Panspermia began to assume a more scientific form through the proposals of Jöns Jacob Berzelius (1834), Hermann E. Richter (1865), Kelvin (1871), Hermann von Helmholtz (1879) and finally reaching the level of a detailed hypothesis through the efforts of the Swedish chemist Svante Arrhenius (1903).
Fred Hoyle (1915–2001) and Chandra Wickramasinghe (born 1939) were influential proponents of panspermia. In 1974 they proposed the hypothesis that some dust in interstellar space was largely organic (containing carbon), which Wickramasinghe later proved to be correct. Hoyle and Wickramasinghe further contended that life forms continue to enter the Earth's atmosphere, and may be responsible for epidemic outbreaks, new diseases, and the genetic novelty necessary for macroevolution.
In an Origins Symposium presentation on April 7, 2009, physicist Stephen Hawking stated his opinion about what humans may find when venturing into space, such as the possibility of alien life through the theory of panspermia: "Life could spread from planet to planet or from stellar system to stellar system, carried on meteors."
In 2011 the results were published of a series of experiments conducted outside the International Space Station in the vacuum of space. It was reported that after 548 days in low Earth orbit, a community of prokaryotic and eukaryotic phototrophs survived for a year and a half and that these conditions acted as a selective pressure on these communities. The researchers also claimed that some organisms could have survived the unattenuated flux in an inactive state for considerable lengths of time.
Several simulations in laboratories and in low Earth orbit suggest that ejection, entry and impact is survivable for some simple organisms. In 2015, "remains of biotic life" were found in 4.1 billion-year-old rocks in Western Australia, when the young Earth was about 400 million years old. According to one of the researchers, "If life arose relatively quickly on Earth … then it could be common in the universe."
Proposed mechanisms.
Panspermia can be said to be either interstellar (between star systems) or interplanetary (between planets in the same star system); its transport mechanisms may include comets, radiation pressure and lithopanspermia (microorganisms embedded in rocks). Interplanetary transfer of nonliving material is well documented, as evidenced by meteorites of Martian origin found on Earth. Space probes may also be a viable transport mechanism for interplanetary cross-pollination in the Solar System or even beyond. However, space agencies have implemented planetary protection procedures to reduce the risk of planetary contamination, although, as recently discovered, some microorganisms, such as Tersicoccus phoenicis, may be resistant to procedures used in spacecraft assembly clean room facilities. In 2012, mathematician Edward Belbruno and astronomers Amaya Moro-Martín and Renu Malhotra proposed that gravitational low energy transfer of rocks among the young planets of stars in their birth cluster is commonplace, and not rare in the general galactic stellar population. Deliberate directed panspermia from space to seed Earth or sent from Earth to seed other planetary systems have also been proposed. One twist to the hypothesis by engineer Thomas Dehel (2006), proposes that plasmoid magnetic fields ejected from the magnetosphere may move the few spores lifted from the Earth's atmosphere with sufficient speed to cross interstellar space to other systems before the spores can be destroyed.
Radiopanspermia.
In 1903, Svante Arrhenius published in his article "The Distribution of Life in Space", the hypothesis now called radiopanspermia, that microscopic forms of life can be propagated in space, driven by the radiation pressure from stars. Arrhenius argued that particles at a critical size below 1.5 μm would be propagated at high speed by radiation pressure of the Sun. However, because its effectiveness decreases with increasing size of the particle, this mechanism holds for very tiny particles only, such as single bacterial spores. The main criticism of radiopanspermia hypothesis came from Shklovskii and Sagan, who pointed out the proofs of the lethal action of space radiations (UV and X-rays) in the cosmos. Regardless of the evidence, Wallis and Wickramasinghe argued in 2004 that the transport of individual bacteria or clumps of bacteria, is overwhelmingly more important than lithopanspermia in terms of numbers of microbes transferred, even accounting for the death rate of unprotected bacteria in transit.
Then, data gathered by the orbital experiments ERA, BIOPAN, EXOSTACK and EXPOSE, determined that isolated spores, including those of "B. subtilis", were killed by several orders of magnitude if exposed to the full space environment for a mere few seconds, but if shielded against solar UV, the spores were capable of surviving in space for up to 6 years while embedded in clay or meteorite powder (artificial meteorites). Though minimal protection is required to shelter a spore against UV radiation, exposure to solar UV and cosmic ionizing radiation of unprotected DNA, break it up into its bases. Also, exposing DNA to the ultrahigh vacuum of space alone is sufficient to cause DNA damage, so the transport of unprotected DNA or RNA during interplanetary flights powered solely by light pressure is extremely unlikely. The feasibility of other means of transport for the more massive shielded spores into the outer Solar System – for example, through gravitational capture by comets – is at this time unknown.
Based on experimental data on radiation effects and DNA stability, it has been concluded that for such long travel times, boulder sized rocks which are greater than or equal to 1 meter in diameter are required to effectively shield resistant microorganisms, such as bacterial spores against galactic cosmic radiation. These results clearly negate the radiopanspermia hypothesis, which requires single spores accelerated by the radiation pressure of the Sun, requiring many years to travel between the planets, and support the likelihood of interplanetary transfer of microorganisms within asteroids or comets, the so-called lithopanspermia hypothesis.
Lithopanspermia.
Lithopanspermia, the transfer of organisms in rocks from one planet to another either through interplanetary or interstellar space, remains speculative. Although there is no evidence that lithopanspermia has occurred in the Solar System, the various stages have become amenable to experimental testing.
Accidental panspermia.
Thomas Gold, a professor of astronomy, suggested in 1960 the hypothesis of "Cosmic Garbage", that life on Earth might have originated accidentally from a pile of waste products dumped on Earth long ago by extraterrestrial beings.
Directed panspermia.
Directed panspermia concerns the deliberate transport of microorganisms in space, sent to Earth to start life here, or sent from Earth to seed new planetary systems with life by introduced species of microorganisms on lifeless planets. The Nobel prize winner Francis Crick, along with Leslie Orgel proposed that life may have been purposely spread by an advanced extraterrestrial civilization, but considering an early "RNA world" Crick noted later that life may have originated on Earth. It has been suggested that 'directed' panspermia was proposed in order to counteract various objections, including the argument that microbes would be inactivated by the space environment and cosmic radiation before they could make a chance encounter with Earth.
Conversely, active directed panspermia has been proposed to secure and expand life in space. This may be motivated by biotic ethics that values, and seeks to propagate, the basic patterns of our organic gene/protein life-form. The panbiotic program would seed new planetary systems nearby, and clusters of new stars in interstellar clouds. These young targets, where local life would not have formed yet, avoid any interference with local life.
For example, microbial payloads launched by solar sails at speeds up to 0.0001 "c" (30,000 m/s) would reach targets at 10 to 100 light-years in 0.1 million to 1 million years. Fleets of microbial capsules can be aimed at clusters of new stars in star-forming clouds, where they may land on planets or captured by asteroids and comets and later delivered to planets. Payloads may contain extremophiles for diverse environments and cyanobacteria similar to early microorganisms. Hardy multicellular organisms (rotifer cysts) may be included to induce higher evolution.
The probability of hitting the target zone can be calculated from formula_1 where "A"(target) is the cross-section of the target area, "dy" is the positional uncertainty at arrival; "a" – constant (depending on units), "r"(target) is the radius of the target area; "v" the velocity of the probe; (tp) the targeting precision (arcsec/yr); and "d" the distance to the target, guided by high-resolution astrometry of 1×10−5 arcsec/yr (all units in SIU). These calculations show that relatively near target stars(Alpha PsA, Beta Pictoris) can be seeded by milligrams of launched microbes; while seeding the Rho Ophiochus star-forming cloud requires hundreds of kilograms of dispersed capsules.
Directed panspermia to secure and expand life in space is becoming possible because of developments in solar sails, precise astrometry, extrasolar planets, extremophiles and microbial genetic engineering. After determining the composition of chosen meteorites, astroecologists performed laboratory experiments that suggest that many colonizing microorganisms and some plants could obtain many of their chemical nutrients from asteroid and cometary materials. However, the scientists noted that phosphate (PO4) and nitrate (NO3–N) critically limit nutrition to many terrestrial lifeforms. With such materials, and energy from long-lived stars, microscopic life planted by directed panspermia could find an immense future in the galaxy.
A number of publications since 1979 have proposed the idea that directed panspermia could be demonstrated to be the origin of all life on Earth if a distinctive 'signature' message were found, deliberately implanted into either the genome or the genetic code of the first microorganisms by our hypothetical progenitor. In 2013 a team of physicists claimed that they had found mathematical and semiotic patterns in the genetic code which, they believe, is evidence for such a signature. Further investigations are needed.
A microscopic ball made of titanium and vanadium was found in Earth's upper atmosphere in early 2015. Milton Wainwright, a UK researcher and astrobiologist at the University of Buckingham claimed in a tabloid that the metal ball "could contain DNA." He speculates that it could be an alien device sent to Earth by extraterrestrials in order to continue seeding the planet with life.
Pseudo-panspermia.
Pseudo-panspermia (sometimes called soft panspermia, molecular panspermia or quasi-panspermia) proposes that the organic molecules used for life originated in space and were incorporated in the solar nebula, from which the planets condensed and were further —and continuously— distributed to planetary surfaces where life then emerged (abiogenesis). From the early 1970s it was becoming evident that interstellar dust consisted of a large component of organic molecules. The first suggestion came from Chandra Wickramasinghe, who proposed a polymeric composition based on the molecule formaldehyde (CH2O). Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. Usually this occurs when a molecule becomes ionized, often as the result of an interaction with cosmic rays. This positively charged molecule then draws in a nearby reactant by electrostatic attraction of the neutral molecule's electrons. Molecules can also be generated by reactions between neutral atoms and molecules, although this process is generally slower. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.
A 2008 analysis of 12C/13C isotopic ratios of organic compounds found in the Murchison meteorite indicates a non-terrestrial origin for these molecules rather than terrestrial contamination. Biologically relevant molecules identified so far include uracil, an RNA nucleobase, and xanthine. These results demonstrate that many organic compounds which are components of life on Earth were already present in the early Solar System and may have played a key role in life's origin.
In August 2009, NASA scientists identified one of the fundamental chemical building-blocks of life (the amino acid glycine) in a comet for the first time.
In August 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of DNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. In October 2011, scientists reported that cosmic dust contains complex organic matter ("amorphous organic solids with a mixed aromatic-aliphatic structure") that could be created naturally, and rapidly, by stars. One of the scientists suggested that these complex organic compounds may have been related to the development of life on Earth and said that, "If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life."
In August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.
In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics - "a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons "for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks."
In 2013, the Atacama Large Millimeter Array (ALMA Project) confirmed that researchers have discovered an important pair of prebiotic molecules in the icy particles in interstellar space (ISM). The chemicals, found in a giant cloud of gas about 25,000 light-years from Earth in ISM, may be a precursor to a key component of DNA and the other may have a role in the formation of an important amino acid. Researchers found a molecule called cyanomethanimine, which produces adenine, one of the four nucleobases that form the "rungs" in the ladder-like structure of DNA. The other molecule, called ethanamine, is thought to play a role in forming alanine, one of the twenty amino acids in the genetic code. Previously, scientists thought such processes took place in the very tenuous gas between the stars. The new discoveries, however, suggest that the chemical formation sequences for these molecules occurred not in gas, but on the surfaces of ice grains in interstellar space. NASA ALMA scientist Anthony Remijan stated that finding these molecules in an interstellar gas cloud means that important building blocks for DNA and amino acids can 'seed' newly formed planets with the chemical precursors for life.
In March 2013, a simulation experiment indicate that dipeptides (pairs of amino acids) that can be building blocks of proteins, can be created in interstellar dust.
In February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.
Extraterrestrial life.
The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe. Nonetheless, Earth is the only place in the universe known to harbor life. The sheer number of planets in the Milky Way galaxy, however, may make it probable that life has arisen somewhere else in the galaxy and the universe. It is generally agreed that the conditions required for the evolution of intelligent life as we know it are probably exceedingly rare in the universe, while simultaneously noting that simple single-celled microorganisms may be more likely.
The extrasolar planet results from the Kepler mission estimate 100–400 billion exoplanets, with over 3,500 as candidates or confirmed exoplanets. On 4 November 2013, astronomers reported, based on Kepler space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars. The nearest such planet may be 12 light-years away, according to the scientists.
It is estimated that space travel over cosmic distances would take an incredibly long time to an outside observer, and with vast amounts of energy required. However, there are reasons to hypothesize that faster-than-light interstellar space travel might be feasible. This has been explored by NASA scientists since at least 1995.
Hypotheses on extraterrestrial sources of illnesses.
Hoyle and Wickramasinghe have speculated that several outbreaks of illnesses on Earth are of extraterrestrial origins, including the 1918 flu pandemic, and certain outbreaks of polio and mad cow disease. For the 1918 flu pandemic they hypothesized that cometary dust brought the virus to Earth simultaneously at multiple locations—a view almost universally dismissed by experts on this pandemic. Hoyle also speculated that HIV came from outer space. After Hoyle's death, "The Lancet" published a letter to the editor from Wickramasinghe and two of his colleagues, in which they hypothesized that the virus that causes severe acute respiratory syndrome (SARS) could be extraterrestrial in origin and not originated from chickens. "The Lancet" subsequently published three responses to this letter, showing that the hypothesis was not evidence-based, and casting doubts on the quality of the experiments referenced by Wickramasinghe in his letter. A 2008 encyclopedia notes that "Like other claims linking terrestrial disease to extraterrestrial pathogens, this proposal was rejected by the greater research community."
Hoaxes.
A separate fragment of the Orgueil meteorite (kept in a sealed glass jar since its discovery) was found in 1965 to have a seed capsule embedded in it, whilst the original glassy layer on the outside remained undisturbed. Despite great initial excitement, the seed was found to be that of a European Juncaceae or Rush plant that had been glued into the fragment and camouflaged using coal dust. The outer "fusion layer" was in fact glue. Whilst the perpetrator of this hoax is unknown, it is thought that they sought to influence the 19th century debate on spontaneous generation — rather than panspermia — by demonstrating the transformation of inorganic to biological matter.
Extremophiles.
Until the 1970s, life was believed to depend on its access to sunlight. Even life in the ocean depths, where sunlight cannot reach, was believed to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible "Alvin", scientists discovered colonies of assorted creatures clustered around undersea volcanic features known as black smokers. It was soon determined that the basis for this food chain is a form of bacterium that derives its energy from oxidation of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. This chemosynthesis revolutionized the study of biology by revealing that terrestrial life need not be Sun-dependent; it only requires water and an energy gradient in order to exist.
It is now known that extremophiles, microorganisms with extraordinary capability to thrive in the harshest environments on Earth, can specialize to thrive in the deep-sea, ice, boiling water, acid, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. Living bacteria found in ice core samples retrieved from deep at Lake Vostok in Antarctica, have provided data for extrapolations to the likelihood of microorganisms surviving frozen in extraterrestrial habitats or during interplanetary transport. Also, bacteria have been discovered living within warm rock deep in the Earth's crust.
In order to test some these organisms' potential resilience in outer space, plant seeds and spores of bacteria, fungi and ferns have been exposed to the harsh space environment. Spores are produced as part of the normal life cycle of many plants, algae, fungi and some protozoans, and some bacteria produce endospores or cysts during times of stress. These structures may be highly resilient to ultraviolet and gamma radiation, desiccation, lysozyme, temperature, starvation and chemical disinfectants, while metabolically inactive. Spores germinate when favourable conditions are restored after exposure to conditions fatal to the parent organism.
Although computer models suggest that a captured meteoroid would typically take some tens of millions of years before collision with a planet, there are documented viable Earthly bacterial spores that are 40 million years old that are very resistant to radiation, and others able to resume life after being dormant for 25 million years, suggesting that lithopanspermia life-transfers are possible via meteorites exceeding 1 m in size.
The discovery of deep-sea ecosystems, along with advancements in the fields of astrobiology, observational astronomy and discovery of large varieties of extremophiles, opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats and possible transport of hardy microbial life through vast distances.
Research in outer space.
The question of whether certain microorganisms can survive in the harsh environment of outer space has intrigued biologists since the beginning of spaceflight, and opportunities were provided to expose samples to space. The first American tests were made in 1966, during the Gemini IX and XII missions, when samples of bacteriophage T1 and spores of "Penicillium roqueforti" were exposed to outer space for 16.8 h and 6.5 h, respectively. Other basic life sciences research in low Earth orbit started in 1966 with the Soviet biosatellite program Bion and the U.S. Biosatellite program. Thus, the plausibility of panspermia can be evaluated by examining life forms on Earth for their capacity to survive in space. The following experiments carried on low Earth orbit specifically tested some aspects of panspermia or lithopanspermia:
ERA.
The Exobiology Radiation Assembly (ERA) was a 1992 experiment on board the European Retrievable Carrier (EURECA) on the biological effects of space radiation. EURECA was an unmanned 4.5 tonne satellite with a payload of 15 experiments. It was an astrobiology mission developed by the European Space Agency (ESA). Spores of different strains of "Bacillus subtilis" and the "Escherichia coli" plasmid pUC19 were exposed to selected conditions of space (space vacuum and/or defined wavebands and intensities of solar ultraviolet radiation). After the approximately 11-month mission, their responses were studied in terms of survival, mutagenesis in the "his" ("B. subtilis") or "lac" locus (pUC19), induction of DNA strand breaks, efficiency of DNA repair systems, and the role of external protective agents. The data were compared with those of a simultaneously running ground control experiment:
BIOPAN.
BIOPAN is a multi-user experimental facility installed on the external surface of the Russian Foton descent capsule. Experiments developed for BIOPAN are designed to investigate the effect of the space environment on biological material after exposure between 13 and 17 days. The experiments in BIOPAN are exposed to solar and cosmic radiation, the space vacuum and weightlessness, or a selection thereof. Of the 6 missions flown so far on BIOPAN between 1992 and 2007, dozens of experiments were conducted, and some analyzed the likelihood of panspermia. Some bacteria, lichens ("Xanthoria elegans", "Rhizocarpon geographicum" and their mycobiont cultures, the black Antarctic microfungi "Cryomyces minteri" and "Cryomyces antarcticus"), spores, and even one animal (tardigrades) were found to have survived the harsh outer space environment and cosmic radiation.
EXOSTACK.
The German EXOSTACK experiment was deployed on 7 April 1984 on board the Long Duration Exposure Facility statellite. 30% of "Bacillus subtilis" spores survived the nearly 6 years exposure when embedded in salt crystals, whereas 80% survived in the presence of glucose, which stabilize the structure of the cellular macromolecules, especially during vacuum-induced dehydration.
If shielded against solar UV, spores of "B. subtilis" were capable of surviving in space for up to 6 years, especially if embedded in clay or meteorite powder (artificial meteorites). The data support the likelihood of interplanetary transfer of microorganisms within meteorites, the so-called lithopanspermia hypothesis.
EXPOSE.
EXPOSE is a multi-user facility mounted outside the International Space Station dedicated to astrobiology experiments. Results from the orbital mission, especially the experiments "SEEDS" and "LiFE", concluded that after an 18-month exposure, some seeds and lichens ("Stichococcus sp." and "Acarospora sp"., a lichenized fungal genus) may be capable to survive interplanetary travel if sheltered inside comets or rocks from cosmic radiation and UV radiation. The survival of some lichen species in space has also been characterized in simulated laboratory experiments.
A separate experiment on EXPOSE called Beer was designed to find microbes that could be used in life-support recycling equipment and future "bio-mining" projects on Mars. It carried group of microbes called OU-20 resembling cyanobacteria genus "Gloeocapsa", and it survived 553 days exposure outside the ISS.
Rosetta.
In 2014, the "Rosetta" spacecraft arrived at COMET 67P/Churyumov–Gerasimenko. A few months after arriving at the comet, "Rosetta" released a small lander, named "Philae", onto its surface. The plan was to investigate Churyumov-Gerasimenko up close for two years. "Philae's" battery has since died; however scientists hope that as the comet travels toward the sun greater solar energy will recharge "Philae" (via its solar panels) and "Philae" will resume operation. Rosetta's Project Scientist, Gerhard Schwehm, stated that sterilization is generally not crucial since comets are usually regarded as objects where prebiotic molecules can be found, but not living microorganisms. Notwithstanding, other scientists think it will be an opportunity to gather evidence for one of panspermia's hypotheses: the possibility of both active and dormant microbes inside comets.
In July 2015, scientists reported that upon the first touchdown of the "Philae" lander on comet 67/P surface, measurements by the COSAC and Ptolemy instruments revealed sixteen organic compounds, four of which were seen for the first time on a comet, including acetamide, acetone, methyl isocyanate and propionaldehyde.
Phobos LIFE.
The "Phobos LIFE" or "Living Interplanetary Flight Experiment", was developed by the Planetary Society and intended to send selected microorganisms on a three-year interplanetary round-trip in a small capsule aboard the Russian Fobos-Grunt spacecraft in 2011. Unfortunately, the spacecraft suffered technical difficulties soon after launch and fell back to Earth, so the experiment was never carried out. The experiment would have tested one aspect of panspermia: lithopanspermia, the hypothesis that life could survive space travel, if protected inside rocks blasted by impact off one planet to land on another.
Criticism.
Panspermia is criticized because it does not answer the question of the origin of life but merely places it on another celestial body. It was also criticized because it could not be tested experimentally. Furthermore, it was suggested that single spores will not survive the physical forces and environment of outer space.
The concept of panspermia was revived when technology provided the opportunity to study the survival of bacterial spores in the harsh environment of space. Wallis and Wickramasinghe argued in 2004 that the transport of individual bacteria or clumps of bacteria, is overwhelmingly more important than lithopanspermia in terms of numbers of microbes transferred, even accounting for the death rate of unprotected bacteria in transit. Then it was found that isolated spores of "B. subtilis" were killed by several orders of magnitude if exposed to the full space environment for a mere few seconds. These results clearly negate the original panspermia hypothesis, which requires single spores as space travelers accelerated by the radiation pressure of the Sun, requiring many years to travel between the planets. However, if shielded against solar UV, spores of "Bacillus subtilis" were capable of surviving in space for up to 6 years, especially if embedded in clay or meteorite powder (artificial meteorites). The data support the likelihood of interplanetary transfer of microorganisms within meteorites, the so-called lithopanspermia hypothesis.

</doc>
<doc id="23680" url="https://en.wikipedia.org/wiki?curid=23680" title="There's Plenty of Room at the Bottom">
There's Plenty of Room at the Bottom

"There's Plenty of Room at the Bottom" was a lecture given by physicist Richard Feynman at an American Physical Society meeting at Caltech on December 29, 1959. Feynman considered the possibility of direct manipulation of individual atoms as a more powerful form of synthetic chemistry than those used at the time. The talk went unnoticed and it didn't inspire the conceptual beginnings of the field. In the 1990s it was rediscovered and publicised as a seminal event in the field, probably to boost the history of nanotechnology with Feynman's reputation.
Conception.
Feynman considered a number of interesting ramifications of a general ability to manipulate matter on an atomic scale. He was particularly interested in the possibilities of denser computer circuitry, and microscopes that could see things much smaller than is possible with scanning electron microscopes. These ideas were later realized by the use of the scanning tunneling microscope, the atomic force microscope and other examples of scanning probe microscopy and storage systems such as Millipede, created by researchers at IBM.
Feynman also suggested that it should be possible, in principle, to make nanoscale machines that "arrange the atoms the way we want", and do chemical synthesis by mechanical manipulation.
He also presented the possibility of "swallowing the doctor," an idea that he credited in the essay to his friend and graduate student Albert Hibbs. This concept involved building a tiny, swallowable surgical robot.
As a thought experiment he proposed developing a set of one-quarter-scale manipulator hands slaved to the operator's hands to build one-quarter scale machine tools analogous to those found in any machine shop. This set of small tools would then be used by the small hands to build and operate ten sets of one-sixteenth-scale hands and tools, and so forth, culminating in perhaps a billion tiny factories to achieve massively parallel operations. He uses the analogy of a pantograph as a way of scaling down items. This idea was anticipated in part, down to the microscale, by science fiction author Robert A. Heinlein in his 1942 story "Waldo".
As the sizes got smaller, one would have to redesign some tools, because the relative strength of various forces would change. Although gravity would become unimportant, surface tension would become more important, Van der Waals attraction would become important, etc. Feynman mentioned these scaling issues during his talk. Nobody has yet attempted to implement this thought experiment, although it has been noted that some types of biological enzymes and enzyme complexes (especially ribosomes) function chemically in a way close to Feynman's vision.
Challenges.
At the meeting, Feynman concluded his talk with two challenges, and he offered a prize of $1000 for the first individuals to solve each one. The first challenge involved the construction of a tiny motor, which, to Feynman's surprise, was achieved by November 1960 by William McLellan, a meticulous craftsman, using conventional tools. The motor met the conditions, but did not advance the art. The second challenge involved the possibility of scaling down letters small enough so as to be able to fit the entire "Encyclopædia Britannica" on the head of a pin, by writing the information from a book page on a surface 1/25,000 smaller in linear scale. In 1985, Tom Newman, a Stanford graduate student, successfully reduced the first paragraph of "A Tale of Two Cities" by 1/25,000, and collected the second Feynman prize.
Impact.
K. Eric Drexler later took the Feynman concept of a billion tiny factories and added the idea that they could make more copies of themselves, via computer control instead of control by a human operator, in his 1986 book "Engines of Creation: The Coming Era of Nanotechnology".
After Feynman's death, scholars studying the historical development of nanotechnology have concluded that his actual role in catalyzing nanotechnology research was limited, based on recollections from many of the people active in the nascent field in the 1980s and 1990s. Chris Toumey, a cultural anthropologist at the University of South Carolina, has reconstructed the history of the publication and republication of Feynman’s talk, along with the record of citations to “Plenty of Room” in the scientific literature. In Toumey's 2008 article, "Reading Feynman into Nanotechnology", he found 11 versions of the publication of “Plenty of Room", plus two instances of a closely related talk by Feynman, “Infinitesimal Machinery,” which Feynman called “Plenty of Room, Revisited.” Also in Toumey’s references are videotapes of that second talk.
Toumey found that the published versions of Feynman’s talk had a negligible influence in the twenty years after it was first published, as measured by citations in the scientific literature, and not much more influence in the decade after the Scanning Tunneling Microscope was invented in 1981. Subsequently, interest in “Plenty of Room” in the scientific literature greatly increased in the early 1990s. This is probably because the term “nanotechnology” gained serious attention just before that time, following its use by Drexler in his 1986 book, "Engines of Creation: The Coming Era of Nanotechnology", which cited Feynman, and in a cover article headlined "Nanotechnology", published later that year in a mass-circulation science-oriented magazine, "OMNI". The journal "Nanotechnology" was launched in 1989; the famous Eigler-Schweizer experiment, precisely manipulating 35 xenon atoms, was published in "Nature" in April 1990; and "Science" had a special issue on nanotechnology in November 1991. These and other developments hint that the retroactive rediscovery of Feynman’s “Plenty of Room” gave nanotechnology a packaged history that provided an early date of December 1959, plus a connection to the charisma and genius of Richard Feynman.
Toumey’s analysis also includes comments from distinguished scientists in nanotechnology who say that “Plenty of Room” did not influence their early work, and in fact most of them had not read it until a later date.
Feynman's stature as a Nobel laureate and as an iconic figure in 20th century science surely helped advocates of nanotechnology and provided a valuable intellectual link to the past. More concretely, his stature and concept of atomically precise fabrication played a role in securing funding for nanotechnology research, illustrated by President Clinton January 2000 speech calling for a Federal program:
While the version of the Nanotechnology Research and Development Act that was passed by the House in May 2003 called for a study of the technical feasibility of molecular manufacturing, this study was removed to safeguard funding of less controversial research before the Act was passed by the Senate and finally signed into law by President Bush on December 3, 2003.

</doc>
<doc id="23681" url="https://en.wikipedia.org/wiki?curid=23681" title="Philately">
Philately

Philately ( ) is the study of stamps and postal history and other related items. Philately involves more than just stamp collecting, which does not necessarily involve the study of stamps. It is possible to be a philatelist without owning any stamps. For instance, the stamps being studied may be very rare, or reside only in museums.
Etymology.
The word "philately" is the English version of the French word "philatélie", coined by Georges Herpin in 1864. Herpin stated that stamps had been collected and studied for the previous six or seven years and a better name was required for the new hobby than "timbromanie", which was disliked. He took the Greek root word φιλ(ο)- "phil(o)-", meaning "an attraction or affinity for something", and ἀτέλεια "ateleia", meaning "exempt from duties and taxes" to form "philatelie". The introduction of postage stamps meant that the receipt of letters was now free of charge, whereas before stamps it was normal for postal charges to be paid by the recipient of a letter.
The alternative terms "timbromania", "timbrophily" and "timbrology" gradually fell out of use as "philately" gained acceptance during the 1860s.
Origins.
The origins of philately lie in the observation that in a number of apparently similar stamps, closer examination may reveal differences in the printed design, paper, watermark, colour, perforations and other areas of the stamp. Comparison with the records of postal authorities may or may not show that the variations were intentional, which leads to further inquiry as to how the changes could have happened, and why. To make things more interesting, thousands of forgeries have been produced over the years, some of them very good, and only a thorough knowledge of philately gives any hope of detecting the fakes.
Types.
Traditional philately is the study of the technical aspects of stamp production and stamp identification, including:
Tools.
Philately uses a number of tools, including stamp tongs (a specialized form of tweezers) to safely handle the stamps, a strong magnifying glass and a perforation gauge (odontometer) to measure the perforation gauge of the stamp.
The identification of watermarks is important and may be done with the naked eye by turning the stamp over or holding it up to the light. If this fails then "watermark fluid" may be used, which "wets" the stamp to reveal the mark.
Other common tools include stamp catalogues, stamp stock books and stamp hinges.
Organisations.
Philatelic organisations sprang up soon after people started collecting and studying stamps. They include local, national and international clubs and societies where collectors come together to share their hobby.

</doc>
<doc id="23682" url="https://en.wikipedia.org/wiki?curid=23682" title="Puget Sound">
Puget Sound

Puget Sound is a sound along the northwestern coast of the U.S. state of Washington, an inlet of the Pacific Ocean, and part of the Salish Sea. It is a complex estuarine system of interconnected marine waterways and basins, with one major and two minor connections to the open Pacific Ocean via the Strait of Juan de Fuca—Admiralty Inlet being the major connection and Deception Pass and Swinomish Channel being the minor. Flow through Deception Pass is approximately equal to 2% of the total tidal exchange between Puget Sound and the Strait of Juan de Fuca. Puget Sound extends approximately from Deception Pass in the north to Olympia, Washington in the south. Its average depth is and its maximum depth, off Point Jefferson between Indianola and Kingston, is . The depth of the main basin, between the southern tip of Whidbey Island and Tacoma, Washington, is approximately .
Since 2009, the term Salish Sea has been established by the United States Board on Geographic Names as the collective waters of Puget Sound, the Strait of Juan de Fuca, and the Strait of Georgia. Sometimes the terms "Puget Sound" and "Puget Sound and adjacent waters" are used for not only Puget Sound proper but also for waters to the north, such as Bellingham Bay and the San Juan Islands region.
The term "Puget Sound" is used not just for the body of water but also the Puget Sound region centered on the sound. Major cities on the sound include Seattle, Tacoma, Olympia, and Everett, Washington. The Seattle metropolitan area also includes Bellevue, Washington, just east of the Sound.
Names.
In 1792 George Vancouver gave the name "Puget's Sound" to the waters south of the Tacoma Narrows, in honor of Peter Puget, a Huguenot lieutenant accompanying him on the Vancouver Expedition. This name later came to be used for the waters north of Tacoma Narrows as well.
An alternative term for Puget Sound, still used by some Native Americans and environmental groups, is "Whulge" (or "Whulj"), an Anglicization of the Lushootseed name '"WulcH", which means "Salt Water".
Definitions.
The USGS defines Puget Sound as all the waters south of three entrances from the Strait of Juan de Fuca The main entrance at Admiralty Inlet is defined as a line between Point Wilson on the Olympic Peninsula, and Point Partridge on Whidbey Island. The second entrance is at Deception Pass along a line from West Point on Whidbey Island, to Deception Island, then to Rosario Head on Fidalgo Island. The third entrance is at the south end of the Swinomish Channel, which connects Skagit Bay and Padilla Bay. Under this definition, Puget Sound includes the waters of Hood Canal, Admiralty Inlet, Possession Sound, Saratoga Passage, and others. It does not include Bellingham Bay, Padilla Bay, the waters of the San Juan Islands or anything farther north.
Another definition, given by NOAA, subdivides Puget Sound into five basins or regions. Four of these correspond to areas within the USGS definition, but the fifth one, called "Northern Puget Sound" includes a large additional region. It is defined as bounded to the north by the international boundary with Canada, and to the west by a line running north from the mouth of the Sekiu River on the Olympic Peninsula. Under this definition significant parts of the Strait of Juan de Fuca and the Strait of Georgia are included in Puget Sound, with the international boundary marking an abrupt and hydrologically arbitrary limit.
According to Arthur Kruckeberg, the term "Puget Sound" is sometimes used for waters north of Admiralty Inlet and Deception Pass, especially for areas along the north coast of Washington and the San Juan Islands, essentially equivalent to NOAA's "Northern Puget Sound" subdivision described above. Kruckeberg uses the term "Puget Sound and adjacent waters".
Geology.
Continental ice sheets have repeatedly advanced and retreated from the Puget Sound region. The most recent glacial period, called the Fraser Glaciation, had three phases, or stades. During the third, or Vashon Glaciation, a lobe of the Cordilleran Ice Sheet, called the Puget Lobe, spread south about 15,000 years ago, covering the Puget Sound region with an ice sheet about thick near Seattle, and nearly at the present Canada-U.S. border. Since each new advance and retreat of ice erodes away much of the evidence of previous ice ages, the most recent Vashon phase has left the clearest imprint on the land. At its maximum extent the Vashon ice sheet extended south of Olympia to near Tenino, and covered the lowlands between the Olympic and Cascade mountain ranges. About 14,000 years ago the ice began to retreat. By 11,000 years ago it survived only north of the Canadian border.
The melting retreat of the Vashon Glaciation eroded the land, creating a drumlin field of hundreds of aligned drumlin hills. Lake Washington and Lake Sammamish (which are ribbon lakes), Hood Canal, and the main Puget Sound basin were altered by glacial forces. These glacial forces are not specifically "carving", as in cutting into the landscape via the mechanics of ice/glaciers, but rather eroding the landscape from melt water of the Vashon Glacier creating the drumlin field. As the ice retreated, vast amounts of glacial till were deposited throughout the Puget Sound region. The soils of the region, less than ten thousand years old, are still characterized as immature.
As the Vashon glacier receded a series of proglacial lakes formed, filling the main trough of Puget Sound and inundating the southern lowlands. Glacial Lake Russell was the first such large recessional lake. From the vicinity of Seattle in the north the lake extended south to the Black Hills, where it drained south into the Chehalis River. Sediments from Lake Russell form the blue-gray clay identified as the Lawton Clay. The second major recessional lake was Glacial Lake Bretz. It also drained to the Chehalis River until the Chimacum Valley, in the northeast Olympic Peninsula, melted, allowing the lake's water to rapidly drain north into the marine waters of the Strait of Juan de Fuca, which was rising as the ice sheet retreated.
As icebergs calved off the toe of the glacier, their embedded gravels and boulders were deposited in the chaotic mix of unsorted till geologists call "glaciomarine drift." Many beaches about the Sound display glacial erratics, rendered more prominent than those in coastal woodland solely by their exposed position; submerged glacial erratics sometimes cause hazards to navigation. The sheer weight of glacial-age ice depressed the landforms, which experienced post-glacial rebound after the ice sheets had retreated. Because the rate of rebound was not synchronous with the post-ice age rise in sea levels, the bed of what is Puget Sound, filled alternately with fresh and with sea water. The upper level of the lake-sediment Lawton Clay now lies about above sea level.
The Puget Sound system consists of four deep basins connected by shallower sills. The four basins are Hood Canal, west of the Kitsap Peninsula, Whidbey Basin, east of Whidbey Island, South Sound, south of the Tacoma Narrows, and the Main Basin, which is further subdivided into Admiralty Inlet and the Central Basin. Puget Sound's sills, a kind of submarine terminal moraine, separate the basins from one another, and Puget Sound from the Strait of Juan de Fuca. Three sills are particularly significant—the one at Admiralty Inlet which checks the flow of water between the Strait of Juan de Fuca and Puget sound, the one at the entrance to Hood Canal (about below the surface), and the one at the Tacoma Narrows (about ). Other sills that present less of a barrier include the ones at Blake Island, Agate Pass, Rich Passage, and Hammersley Inlet.
The depth of the basins is a result of the Sound being part of the Cascadia subduction zone, where the terranes accreted at the edge of the Juan de Fuca Plate are being subducted under the North American Plate. There has not been a major subduction zone earthquake here since the magnitude nine Cascadia earthquake; according to Japanese records, it occurred 26 January 1700. Lesser Puget Sound earthquakes with shallow epicenters, caused by the fracturing of stressed oceanic rocks as they are subducted, still cause great damage. The Seattle Fault cuts across Puget Sound, crossing the southern tip of Bainbridge Island and under Elliott Bay. To the south, the existence of a second fault, the Tacoma Fault, has buckled the intervening strata in the Seattle Uplift.
Typical Puget Sound profiles of dense glacial till overlying permeable glacial outwash of gravels above an impermeable bed of silty clay may become unstable after periods of unusually wet weather and slump in landslides.
Hydrology.
The United States Geological Survey (USGS) defines Puget Sound as a bay with numerous channels and branches; more specifically, it is a fjord system of flooded glacial valleys. Puget Sound is part of a larger physiographic structure termed the Puget Trough, which is a physiographic section of the larger Pacific Border province, which in turn is part of the larger Pacific Mountain System.
Puget Sound is a large salt water estuary, or system of many estuaries, fed by highly seasonal freshwater from the Olympic and Cascade Mountain watersheds. The mean annual river discharge into Puget Sound is , with a monthly average maximum of about and minimum of about . Puget Sound's shoreline is long, encompassing a water area of and a total volume of at mean high water. The average volume of water flowing in and out of Puget Sound during each tide is . The maximum tidal currents, in the range of 9 to 10 knots, occurs at Deception Pass.
The size of Puget Sound's watershed is . "Northern Puget Sound" is frequently considered part of the Puget Sound watershed, which enlarges its size to . The USGS uses the name "Puget Sound" for its hydrologic unit subregion 1711, which includes areas draining to Puget Sound proper as well as the Strait of Juan de Fuca, the Strait of Georgia, and the Fraser River. Significant rivers that drain to "Northern Puget Sound" include the Nooksack, Dungeness, and Elwha Rivers. The Nooksack empties into Bellingham Bay, the Dungeness and Elwha into the Strait of Juan de Fuca. The Chilliwack River flows north to the Fraser River in Canada.
Tides in Puget Sound are of the mixed type with two high and two low tides each tidal day. These are called Higher High Water (HHW), Lower Low Water (LLW), Lower High Water (LHW), and Higher Low Water (HLW). The configuration of basins, sills, and interconnections cause the tidal range to increase within Puget Sound. The difference in height between the Higher High Water and the Lower Low Water averages about at Port Townsend on Admiralty Inlet, but increases to about at Olympia, the southern end of Puget Sound.
Puget Sound is generally accepted as the start of the Inside Passage.
Flora and fauna.
Important marine flora of Puget Sound include eelgrass ("Zostera marina") and kelp, especially bull kelp ("Nereocystis luetkeana").
Among the marine mammals species found in Puget Sound are harbor seals ("Phoca vitulina"). Orca ("Orcinus orca") are famous throughout the Sound, and are a large tourist attraction. Although orca are sometimes seen in Puget Sound proper they are far more prevalent around the San Juan Islands north of Puget Sound.
Many fish species occur in Puget Sound. The various salmonid species, including salmon, trout, and char are particularly well-known and studied. Salmonid species of Puget Sound include chinook salmon ("Oncorhynchus tshawytscha"), chum salmon ("O. keta"), coho salmon ("O. kisutch"), pink salmon ("O. gorbuscha"), sockeye salmon ("O. nerka"), sea-run coastal cutthroat trout ("O. clarki clarki"), steelhead ("O. mykiss irideus"), sea-run bull trout ("Salvelinus confluentus"), and Dolly Varden trout ("Salvelinus malma malma").
Common forage fishes found in Puget Sound include Pacific herring ("Clupea pallasii"), surf smelt ("Hypomesus pretiosus"), and Pacific sand lance ("Ammodytes hexapterus"). Important benthopelagic fish of Puget Sound include North Pacific hake ("Merluccius productus"), Pacific cod ("Gadus macrocelhalus"), walleye pollock ("Theragra chalcogramma"), and the spiny dogfish ("Squalus acanthias"). There are about 28 species of Sebastidae (rockfish), of many types, found in Puget Sound. Among those of special interest are copper rockfish ("Sebastes caurinus"), quillback rockfish ("S. maliger"), black rockfish ("S. melanops"), yelloweye rockfish ("S. ruberrimus"), bocaccio rockfish ("S. paucispinis"), canary rockfish ("S. pinniger"), and Puget Sound rockfish ("S. emphaeus").
Many other fish species occur in Puget Sound, such as sturgeons, lampreys, various sharks, rays, and skates.
Puget Sound is home to numerous species of marine invertebrates, including sponges, sea anemones, chitons, clams, sea snails, limpets crabs, barnacles starfish, sea urchins, and sand dollars. Dungeness crabs ("Metacarcinus magister") occur throughout Washington waters, including Puget Sound. Many bivalves occur in Puget Sound, such as Pacific oysters ("Crassostrea gigas") and geoduck clams ("Panopea generosa"). The Olympia oyster ("Ostreola conchaphila"), once common in Puget Sound, was depleted by human activities during the 20th century. There are ongoing efforts to restore Olympia oysters in Puget Sound.
There are many seabird species of Puget Sound. Among these are grebes such as the western grebe ("Aechmophorus occidentalis"); loons such as the common loon ("Gavia immer"); auks such as the pigeon guillemot ("Cepphus columba"), rhinoceros auklet ("Cerorhinca monocerata"), common murre ("Uria aalge"), and marbled murrelet ("Brachyramphus marmoratus"); the brant goose ("Branta bernicla"); seaducks such as the long-tailed duck ("Clangula hyemalis"), harlequin duck ("Histrionicus histrionicus"), and surf scoter ("Melanitta perspicillata"); and cormorants such as the double-crested cormorant ("Phalacrocorax auritus"). Puget Sound is home to a non-migratory and marine-oriented subspecies of great blue herons ("Ardea herodias fannini"). Bald eagles ("Haliaeetus leucocephalus") occur in relative high densities in the Puget Sound region.
It is estimated that more than 100 million geoducks (pronounced "gooey ducks") are packed into Puget Sound's sediments. Also known as "king clam", geoducks are considered to be a delicacy in Asian countries.
History.
George Vancouver explored Puget Sound in 1792. Vancouver claimed it for Great Britain on 4 June 1792, naming it for one of his officers, Lieutenant Peter Puget.
After 1818 Britain and the United States, which both claimed the Oregon Country, agreed to "joint occupancy", deferring resolution of the Oregon boundary dispute until the 1846 Oregon Treaty. Puget Sound was part of the disputed region until 1846, after which it became US territory.
American maritime fur traders visited Puget Sound in the early 19th century.
The first European settlement in the Puget Sound area was Fort Nisqually, a fur trade post of the Hudson's Bay Company (HBC) built in 1833. Fort Nisqually was part of the HBC's Columbia District, headquartered at Fort Vancouver. The Puget Sound Agricultural Company, a subsidiary of the HBC, established farms and ranches near Fort Nisqually. British ships such as the "Beaver", exported foodstuffs and provisions from Fort Nisqually.
The first American settlement on Puget Sound was Tumwater. It was founded in 1845 by Americans who had come via the Oregon Trail. The decision to settle north of the Columbia River was made in part because one of the settlers, George Washington Bush, was considered black and the Provisional Government of Oregon banned the residency of mulattoes but did not actively enforce the restriction north of the river.
In 1853 Washington Territory was formed from part of Oregon Territory. In 1888 the Northern Pacific railroad line reached Puget Sound, linking the region to eastern states.
Transportation.
A unique state-run ferry system, the Washington State Ferries, connects the larger islands to the Washington mainland, as well as both sides of the sound, allowing people and cars to move about the greater Puget Sound region.
Environmental issues.
In the past 30 years there has been a large recession in the populations of the species which inhabit Puget Sound. The decrease has been seen in the populations of: forage fish, salmonids, bottom fish, marine birds, harbor porpoise and orcas. This decline is attributed to the various environmental issues in Puget Sound. Because of this population decline, there have been changes to the fishery practices, and an increase in petitioning to add species to the Endangered Species Act. There has also been an increase in recovery and management plans for many different area species.
The causes of these environmental issues are toxic contamination, eutrophication (low oxygen due to excess nutrients), and near shore habitat changes.

</doc>
<doc id="23688" url="https://en.wikipedia.org/wiki?curid=23688" title="Perjury">
Perjury

Perjury, also known as forswearing, is the intentional act of swearing a false oath or of falsifying an affirmation to tell the truth, whether spoken or in writing, concerning matters material to an official proceeding. Contrary to popular misconception, no crime has occurred when a false statement is (intentionally or unintentionally) made while under oath or subject to penalty—instead, criminal culpability only attaches at the instant the declarant falsely asserts the truth of statements (made or to be made) which are material to the outcome of the proceeding. For example, it is not perjury to lie about one's age except where age is a fact material to influencing the legal result, such as eligibility for old age retirement benefits or whether a person was of an age to have legal capacity.
Perjury is considered a serious offense as it can be used to usurp the power of the courts, resulting in miscarriages of justice. In the United States, for example, the general perjury statute under Federal law classifies perjury as a felony and provides for a prison sentence of up to five years. The California Penal Code allows for perjury to be a capital offense in cases causing wrongful execution. However, prosecutions for perjury are rare. In some countries such as France and Italy, suspects cannot be heard under oath or affirmation and thus cannot commit perjury, regardless of what they say during their trial.
The rules for perjury also apply when a person has made a statement "under penalty of perjury", even if the person has not been sworn or affirmed as a witness before an appropriate official. An example of this is the United States' income tax return, which, by law, must be signed as true and correct under penalty of perjury (see ). Federal tax law provides criminal penalties of up to three years in prison for violation of the tax return perjury statute. See: 
Statements which entail an "interpretation" of fact are not perjury because people often draw inaccurate conclusions unwittingly, or make honest mistakes without the intent to deceive. Individuals may have honest but mistaken beliefs about certain facts, or their recollection may be inaccurate, or may have a different perception of what is the accurate way to state the truth. Like most other crimes in the common law system, to be convicted of perjury one must have had the "intention" ("mens rea") to commit the act, and to have "actually committed" the act ("actus reus"). Further, statements that "are facts" cannot be considered perjury, even if they might arguably constitute an omission, and it is not perjury to lie about matters immaterial to the legal proceeding.
Subornation of perjury, attempting to induce another person to commit perjury, is itself a crime.
Canada.
The offence of perjury is created by section 132 of the Criminal Code. It is defined by section 131, which provides:
As to corroboration, see section 133.
Mode of trial and sentence
Every one who commits perjury is guilty of an indictable offence and liable to imprisonment for a term not exceeding fourteen years.
European Union.
A person who before the Court of Justice of the European Communities swears anything which he knows to be false or does not believe to be true is, whatever his nationality, guilty of perjury. Proceedings for this offence may be taken in any place in the State and the offence may for all incidental purposes be treated as having been committed in that place.
United Kingdom.
England and Wales.
Perjury is a statutory offence in England and Wales. It is created by section 1(1) of the Perjury Act 1911. Section 1 of that Act reads:
The words omitted from section 1(1) were repealed by section 1(2) of the Criminal Justice Act 1948.
A person guilty of an offence under section 11(1) of the European Communities Act 1972 may be proceeded against and punished in England and Wales as for an offence under section 1(1).
Section 1(4) has effect in relation to proceedings in the Court of Justice of the European Communities as it has effect in relation to a judicial proceeding in a tribunal of a foreign state.
Section 1(4) applies in relation to proceedings before a relevant convention court under the European Patent Convention as it applies to a judicial proceeding in a tribunal of a foreign state.
A statement made on oath by a witness outside the United Kingdom and given in evidence through a live television link by virtue of section 32 of the Criminal Justice Act 1988 must be treated for the purposes of section 1 as having been made in the proceedings in which it is given in evidence.
Section 1 applies in relation to a person acting as an intermediary as it applies in relation to a person lawfully sworn as an interpreter in a judicial proceeding; and for this purpose, where a person acts as an intermediary in any proceeding which is not a judicial proceeding for the purposes of section 1, that proceeding must be taken to be part of the judicial proceeding in which the witness’s evidence is given.
Where any statement made by a person on oath in any proceeding which is not a judicial proceeding for the purposes of section 1 is received in evidence in pursuance of a special measures direction, that proceeding must be taken for the purposes of section 1 to be part of the judicial proceeding in which the statement is so received in evidence.
Judicial proceeding
The definition in section 1(2) is not "comprehensive".
The book "Archbold" said that it appears to be immaterial whether the court, before which the statement is made, has jurisdiction in the particular cause in which the statement is made, because there is no express requirement in the Act that the court be one of "competent jurisdiction" and because the definition in section 1(2) does not appear to require this by implication either.
Actus reus
The actus reus of perjury might be considered to be the making of a statement, whether true or false, on oath in a judicial proceeding, where the person knows the statement to be false or believes it to be false.
Perjury is a conduct crime.
Mode of trial
Perjury is triable only on indictment.
Sentence
A person convicted of perjury is liable to imprisonment for a term not exceeding seven years, or to a fine, or to both.
The following cases are relevant:
See also the Crown Prosecution Service sentencing manual.
History.
In Anglo-Saxon legal procedure, the offence of perjury could only be committed by both jurors and by compurgators. With time witnesses began to appear in court they were not so treated despite the fact that their functions were akin to that of modern witnesses. This was due to the fact that their role were not yet differentiated from those of the juror – hence false evidence or perjury by witnesses was not made a crime. Even in the fourteenth century when witnesses started appearing before the jury to testify, perjury by them was not made a punishable offence. The maxim then was that every witness’s evidence on oath was true. Perjury by witnesses began to be punished before the end of fifteenth century by the Star Chamber. The immunity enjoyed by witnesses began also to be whittled down or interfered with by the parliament in England in 1540 with subornation of perjury and, in 1562, with perjury proper. The punishment for the offence then was in the nature of monetary penalty, recoverable in a civil action and, not by penal sanction. In 1613, the Star Chamber declared perjury by a witness to be a punishable offence at common law.
See section 3 of the Maintenance and Embracery Act 1540, the 5 Eliz 1 c 9 (An Act for the Punyshement of suche persones as shall procure or comit any wyllful Perjurye) and the Perjury Act 1728.
Materiality
The requirement that the statement be material can be traced back to, and has been credited to, Coke. He said:
Northern Ireland.
Perjury is a statutory offence in Northern Ireland. It is created by article 3(1) of the Perjury (Northern Ireland) Order 1979 (S.I. 1979/1714 (N.I. 19)). This replaces the Perjury Act (Northern Ireland) 1946 (c. 13) (N.I.).
United States.
Perjury operates in American law as an inherited principle of the common law of England, which defined the act as the "willful and corrupt giving, upon a lawful oath, or in any form allowed by law to be substituted for an oath, in a judicial proceeding or course of justice, of a false testimony material to the issue or matter of inquiry." William Blackstone touched on the subject in his Commentaries on the Laws of England, establishing perjury as "a crime committed when a lawful oath is administered, in some judicial proceeding, to a person who swears willfully, absolutely, and falsely, in a matter material to the issue or point in question." The punishment for perjury under the common law has varied from death to banishment and has included such grotesque penalties as severing the tongue of the perjurer. The definitional structure of perjury provides an important framework for legal proceedings, as the component parts of this definition have permeated jurisdictional lines, finding a home in American legal constructs. As such, the main tenets of perjury, including mens rea, a lawful oath, occurring during a judicial proceeding, a false testimony have remained necessary pieces of perjury’s definition in the United States.
Statutory definitions.
Perjury’s current position in the American legal system takes the form of state and federal statutes. Most notably, the United States Code prohibits perjury, which is defined in two senses for federal purposes as someone who:
The above statute provides for a fine and/or up to five years in prison as punishment. Within federal jurisdiction, statements made in two broad categories of judicial proceedings may qualify as perjurious: 1) Federal official proceedings, and 2) Federal Court or Grand Jury proceedings. A third type of perjury entails the procurement of perjurious statements from another person. More generally, the statement must occur in the "course of justice," but this definition leaves room open for interpretation. One particularly precarious aspect of this phrasing is that it entails knowledge of the accused person’s perception of the truthful nature of events and not necessarily the actual truth of those events. It is important to note the distinction here, between giving a false statement under oath and merely misstating a fact accidentally, though this distinction can be especially difficult to discern in court of law.
Precedents.
The development of perjury law in the United States centers on United States v. Dunnigan, a seminal case that set out the parameters of perjury within United States law. The court uses the Dunnigan based legal standard to determine if an accused person, "estifying under oath or affirmation violates this section if she gives false testimony concerning a material matter with the willful intent to provide false testimony, rather than as a result of confusion, mistake, or faulty memory." However, a defendant shown to be willfully ignorant may in fact be eligible for perjury prosecution. The Dunnigan distinction manifests its importance with regard to the relation between two component parts of perjury’s definition: in willfully giving a false statement, a person must understand that she is giving a false statement to be considered a perjurer under the Dunnigan framework. Deliberation on the part of the defendant is required for a statement to constitute perjury. Jurisprudential developments in the American law of perjury have revolved around the facilitation of "perjury prosecutions and thereby enhance the reliability of testimony before federal courts and grand juries." With this goal in mind, Congress has sometimes expanded the grounds on which an individual may be prosecuted for perjury, with section 1623 of the United States Code recognizing the utterance of two mutually incompatible statements as grounds for perjury indictment even if neither can unequivocally be proven false. However, the two statements must be so mutually incompatible that at least one must necessarily be false; it is irrelevant whether the false statement can be specifically identified from among the two. It thus falls on the government to show that a defendant (a) knowingly made a (b) false (c) material statement (d) under oath (e) in a legal proceeding. These proceedings can be ancillary to normal court proceedings, and thus, even such menial interactions as bail hearings can qualify as protected proceedings under this statute.
Wilfulness is an element of the offense. The mere existence of two mutually exclusive factual statements is not sufficient to prove perjury; the prosecutor nonetheless has the duty to plead and prove statement was willfully made. Mere contradiction will not sustain the charge; there must be strong corroborative evidence of the contradiction.
One significant legal distinction lies in the specific realm of knowledge necessarily possessed by a defendant for her statements to be properly called perjury. Though the defendant must knowingly render a false statement in a legal proceeding or under federal jurisdiction, the defendant need not know that they are speaking under such conditions for the statement to constitute perjury. All tenets of perjury qualification persist- the “knowingly” aspect of telling the false statement simply does not apply to the defendant’s knowledge about the person she intends to deceive.
Materiality.
Perjury law’s evolution in the United States has experienced the most debate with regards to the materiality requirement. Fundamentally, statements that are literally true cannot provide the basis for a perjury charge (as they do not meet the falsehood requirement) just as answers to truly ambiguous statements cannot constitute perjury. However, such fundamental truths of perjury law become muddled when discerning the materiality of a given statement and the way in which it was material to the given case. In "United States v. Brown", the court defined material statements as those with "a natural tendency to influence, or is capable of influencing, the decision of the decision-making body to be addressed," such as a jury or grand jury. While courts have specifically made clear certain instances which have succeeded or failed to meet the nebulous threshold for materiality, the topic remains unresolved in large part, except in certain legal areas where intent manifests itself in an abundantly clear fashion, such as with the so-called perjury trap, a specific situation in which a prosecutor calls a person to testify before a grand jury with the intent of drawing a perjurious statement from the person being questioned.
Defense of recantation.
Despite a tendency of American perjury law toward broad prosecutory power under perjury statutes, American perjury law has afforded potential defendants a new form of defense not found in the British Common Law. This defense requires that an individual admit to making a perjurious statement during that same proceeding and recanting the statement. Though this defensive loophole slightly narrows the types of cases which may be prosecuted for perjury, the effect of this statutory defense is to promote a truthful retelling of facts by witnesses, thus helping to ensure the reliability of American court proceedings just as broadened perjury statutes aimed to do.
Subornation of perjury.
Subornation of perjury stands as a subset of American perjury laws and prohibits an individual from inducing another to commit perjury. Subornation of perjury entails equivalent possible punishments as perjury on the federal level. This crime requires an extra level of satisfactory proof, as prosecutors must show not only that perjury occurred, but also that the defendant positively induced said perjury. Furthermore, the inducing defendant must know that the suborned statement is a false, perjurious statement.
Allegations of perjury.
Notable people who have been accused of perjury include:

</doc>
<doc id="23689" url="https://en.wikipedia.org/wiki?curid=23689" title="Phoenix">
Phoenix

Phoenix most often refers to:
Phoenix or The Phoenix may also refer to:

</doc>
<doc id="23690" url="https://en.wikipedia.org/wiki?curid=23690" title="Phosphate">
Phosphate

A phosphate () is an inorganic chemical and a salt of phosphoric acid. In organic chemistry, a phosphate, or organophosphate, is an ester of phosphoric acid. Of the various phosphoric acids and phosphates, organic phosphates are important in biochemistry and biogeochemistry (ecology), and inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry. At elevated temperatures in the solid state, phosphates can condense to form pyrophosphates.
The addition and removal of phosphates from proteins in all cells is a pivotal strategy in the regulation of metabolic processes. Phosphorylation and dephosphorylation are important ways that energy is stored and released in living systems.
Chemical properties.
[[Image:Phosphate Group.svg|upright|thumb|This is the structural formula of the phosphoric acid functional group as found in weakly acidic aqueous solution. In more basic aqueous solutions, the group donates the two hydrogen atoms and ionizes as a phosphate group with a negative charge of 2.
The phosphate ion is a polyatomic ion with the empirical formula and a molar mass of 94.97 g/mol. It consists of one central phosphorus atom surrounded by four oxygen atoms in a tetrahedral arrangement. The phosphate ion carries a −3 formal charge and is the conjugate base of the hydrogen phosphate ion, , which is the conjugate base of , the dihydrogen phosphate ion, which in turn is the conjugate base of , phosphoric acid. A phosphate salt forms when a positively charged ion attaches to the negatively charged oxygen atoms of the ion, forming an ionic compound. Many phosphates are not soluble in water at standard temperature and pressure. The sodium, potassium, rubidium, caesium, and ammonium phosphates are all water-soluble. Most other phosphates are only slightly soluble or are insoluble in water. As a rule, the hydrogen and dihydrogen phosphates are slightly more soluble than the corresponding phosphates. The pyrophosphates are mostly water-soluble.
Aqueous phosphate exists in four forms. In strongly basic conditions, the phosphate ion () predominates, whereas in weakly basic conditions, the hydrogen phosphate ion () is prevalent. In weakly acid conditions, the dihydrogen phosphate ion () is most common. In strongly acidic conditions, trihydrogen phosphate () is the main form.
More precisely, considering these three equilibrium reactions:
the corresponding constants at 25 °C (in mol/L) are (see phosphoric acid):
The speciation diagram obtained using these p"K" values shows three distinct regions. In effect, , and behave as separate weak acids. This is because the successive p"K" values differ by more than 4. For each acid, the pH at half-neutralization is equal to the p"K" value of the acid. The region in which the acid is in equilibrium with its conjugate base is defined by . Thus, the three pH regions are approximately 0–4, 5–9 and 10–14. This is idealized, as it assumes constant ionic strength, which will not hold in reality at very low and very high pH values.
For a neutral pH as in the cytosol, pH = 7.0
so that only and ions are present in significant amounts (62% , 38% . Note that in the extracellular fluid (pH = 7.4), this proportion is inverted (61% , 39% ).
Phosphate can form many polymeric ions such as pyrophosphate), , and triphosphate, . The various metaphosphate ions (which are usually long linear polymers) have an empirical formula of and are found in many compounds.
Biochemistry of phosphates.
In biological systems, phosphorus is found as a free phosphate ion in solution and is called inorganic phosphate, to distinguish it from phosphates bound in various phosphate esters. Inorganic phosphate is generally denoted Pi and at physiological (homeostatic) pH primarily consists of a mixture of and ions.
Inorganic phosphate can be created by the hydrolysis of pyrophosphate, which is denoted PPi:
However, phosphates are most commonly found in the form of adenosine phosphates (AMP, ADP, and ATP) and in DNA and RNA, and can be released by the hydrolysis of ATP or ADP. Similar reactions exist for the other nucleoside diphosphates and triphosphates. Phosphoanhydride bonds in ADP and ATP, or other nucleoside diphosphates and triphosphates, contain high amounts of energy which give them their vital role in all living organisms. They are generally referred to as high-energy phosphate, as are the phosphagens in muscle tissue. Compounds such as substituted phosphines have uses in organic chemistry, but do not seem to have any natural counterparts.
The addition and removal of phosphate from proteins in all cells is a pivotal strategy in the regulation of metabolic processes. Phosphorylation and dephosphorylation are important ways that energy is stored and released in living systems. Cells use ATP in this manner.
Phosphate is useful in animal cells as a buffering agent. Phosphate salts that are commonly used for preparing buffer solutions at cell pHs include Na2HPO4, NaH2PO4, and the corresponding potassium salts.
An important occurrence of phosphates in biological systems is as the structural material of bone and teeth. These structures are made of crystalline calcium phosphate in the form of hydroxyapatite. The hard dense enamel of mammalian teeth consists of fluoroapatite, a hydroxy calcium phosphate where some of the hydroxyl groups have been replaced by fluoride ions.
Plants take up phosphorus through several pathways: the arbuscular mycorrhizal pathway and the direct uptake pathway.
Occurrence and mining.
Phosphates are the naturally occurring form of the element phosphorus, found in many phosphate minerals. In mineralogy and geology, phosphate refers to a rock or ore containing phosphate ions. Inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry.
The largest global producer and exporter of phosphates is Morocco. Within North America, the largest deposits lie in the Bone Valley region of central Florida, the Soda Springs region of Idaho, and the coast of North Carolina. Smaller deposits are located in Montana, Tennessee, Georgia, and South Carolina. The small island nation of Nauru and its neighbor Banaba Island, which used to have massive phosphate deposits of the best quality, have been mined excessively. Rock phosphate can also be found in Egypt, Israel, Western Sahara, Navassa Island, Tunisia, Togo, and Jordan, countries that have large phosphate-mining industries.
Phosphorite mines are primarily found in:
In 2007, at the current rate of consumption, the supply of phosphorus was estimated to run out in 345 years. However, some scientists thought that a "peak phosphorus" will occur in 30 years and Dana Cordell from Institute for Sustainable Futures said that at "current rates, reserves will be depleted in the next 50 to 100 years." Reserves refer to the amount assumed recoverable at current market prices, and, in 2012, the USGS estimated 71 billion tons of world reserves, while 0.19 billion tons were mined globally in 2011. Phosphorus comprises 0.1% by mass of the average rock (while, for perspective, its typical concentration in vegetation is 0.03% to 0.2%), and consequently there are quadrillions of tons of phosphorus in Earth's 3 * 1019 ton crust, albeit at predominantly lower concentration than the deposits counted as reserves from being inventoried and cheaper to extract; if it is assumed that the phosphate minerals in phosphate rock are hydroxyapatite and fluoroapatite, phosphate minerals contain roughly 18.5% phosphorus by weight and if phosphate rock contains around 20% of these minerals, the average phosphate rock has roughly 3.7% phosphorus by weight.
Some phosphate rock deposits are notable for their inclusion of significant quantities of radioactive uranium isotopes. This syndrome is noteworthy because radioactivity can be released into surface waters in the process of application of the resultant phosphate fertilizer (e.g. in many tobacco farming operations in the southeast US).
In December 2012, Cominco Resources announced an updated JORC compliant resource of their Hinda project in Congo-Brazzaville of 531 Mt, making it the largest measured and indicated phosphate deposit in the world.
Ecology.
In ecological terms, because of its important role in biological systems, phosphate is a highly sought after resource. Once used, it is often a limiting nutrient in environments, and its availability may govern the rate of growth of organisms. This is generally true of freshwater environments, whereas nitrogen is more often the limiting nutrient in marine (seawater) environments. Addition of high levels of phosphate to environments and to micro-environments in which it is typically rare can have significant ecological consequences. For example, blooms in the populations of some organisms at the expense of others, and the collapse of populations deprived of resources such as oxygen (see eutrophication) can occur. In the context of pollution, phosphates are one component of total dissolved solids, a major indicator of water quality, but not all phosphorus is in a molecular form which algae can break down and consume.
Calcium hydroxyapatite and calcite precipitates can be found around bacteria in alluvial topsoil. As clay minerals promote biomineralization, the presence of bacteria and clay minerals resulted in calcium hydroxyapatite and calcite precipitates.
Phosphate deposits can contain significant amounts of naturally occurring heavy metals. Mining operations processing phosphate rock can leave tailings piles containing elevated levels of cadmium, lead, nickel, copper, chromium, and uranium. Unless carefully managed, these waste products can leach heavy metals into groundwater or nearby estuaries. Uptake of these substances by plants and marine life can lead to concentration of toxic heavy metals in food products.

</doc>
<doc id="23692" url="https://en.wikipedia.org/wiki?curid=23692" title="Prime number theorem">
Prime number theorem

In number theory, the prime number theorem (PNT) describes the asymptotic distribution of the prime numbers among the positive integers. It formalizes the intuitive idea that primes become less common as they become larger by precisely quantifying the rate at which this occurs. The theorem was proved independently by Jacques Hadamard and Charles Jean de la Vallée-Poussin in 1896 using ideas introduced by Bernhard Riemann (in particular, the Riemann zeta function).
The first such distribution found is , where π("N") is the prime-counting function and log("N") is the natural logarithm of "N". This means that for large enough "N", the probability that a random integer not greater than "N" is prime is very close to . Consequently, a random integer with at most 2"n" digits (for large enough "n") is about half as likely to be prime as a random integer with at most "n" digits. For example, among the positive integers of at most 1000 digits, about one in 2300 is prime (), whereas among positive integers of at most 2000 digits, about one in 4600 is prime (). In other words, the average gap between consecutive prime numbers among the first "N" integers is roughly log("N").
Statement.
Let π("x") be the prime-counting function that gives the number of primes less than or equal to "x", for any real number "x". For example, π(10) = 4 because there are four prime numbers (2, 3, 5 and 7) less than or equal to 10. The prime number theorem then states that "x" / log("x") is a good approximation to π("x"), in the sense that the limit of the "quotient" of the two functions π("x") and "x" / log("x") as "x" increases without bound is 1:
known as the asymptotic law of distribution of prime numbers. Using asymptotic notation this result can be restated as
This notation (and the theorem) does "not" say anything about the limit of the "difference" of the two functions as "x" increases without bound. Instead, the theorem states that "x"/log("x") approximates π("x") in the sense that the relative error of this approximation approaches 0 as "x" increases without bound.
The prime number theorem is equivalent to the statement that the "n"th prime number "p""n" satisfies
the asymptotic notation meaning, again, that the relative error of this approximation approaches 0 as "n" increases without bound. For example, the 200 · 1015th prime number is 8512677386048191063, and (200 · 1015)log(200 · 1015) rounds to 7967418752291744388, a relative error of about 6.4%.
The prime number theorem is also equivalent to formula_4, and formula_5, where formula_6 and formula_7 are the first and the second Chebyshev functions respectively.
History of the asymptotic law of distribution of prime numbers and its proof.
Based on the tables by Anton Felkel and Jurij Vega, Adrien-Marie Legendre conjectured in 1797 or 1798 that π("a") is approximated by the function "a"/(A log("a") + "B"), where "A" and B are unspecified constants. In the second edition of his book on number theory (1808) he then made a more precise conjecture, with "A" = 1 and "B" = −1.08366. Carl Friedrich Gauss considered the same question at age 15 or 16 "ins Jahr 1792 oder 1793", according to his own recollection in 1849. In 1838 Peter Gustav Lejeune Dirichlet came up with his own approximating function, the logarithmic integral li("x") (under the slightly different form of a series, which he communicated to Gauss). Both Legendre's and Dirichlet's formulas imply the same conjectured asymptotic equivalence of π("x") and "x" / log("x") stated above, although it turned out that Dirichlet's approximation is considerably better if one considers the differences instead of quotients.
In two papers from 1848 and 1850, the Russian mathematician Pafnuty L'vovich Chebyshev attempted to prove the asymptotic law of distribution of prime numbers. His work is notable for the use of the zeta function ζ("s") (for real values of the argument "s", as are works of Leonhard Euler, as early as 1737) predating Riemann's celebrated memoir of 1859, and he succeeded in proving a slightly weaker form of the asymptotic law, namely, that if the limit of π("x")/("x"/log("x")) as "x" goes to infinity exists at all, then it is necessarily equal to one. He was able to prove unconditionally that this ratio is bounded above and below by two explicitly given constants near 1, for all sufficiently large "x". Although Chebyshev's paper did not prove the Prime Number Theorem, his estimates for π("x") were strong enough for him to prove Bertrand's postulate that there exists a prime number between "n" and 2"n" for any integer "n" ≥ 2.
An important paper concerning the distribution of prime numbers was Riemann's 1859 memoir "On the Number of Primes Less Than a Given Magnitude", the only paper he ever wrote on the subject. Riemann introduced new ideas into the subject, the chief of them being that the distribution of prime numbers is intimately connected with the zeros of the analytically extended Riemann zeta function of a complex variable. In particular, it is in this paper of Riemann that the idea to apply methods of complex analysis to the study of the real function π("x") originates. Extending the ideas of Riemann, two proofs of the asymptotic law of the distribution of prime numbers were obtained independently by Jacques Hadamard and Charles Jean de la Vallée-Poussin and appeared in the same year (1896). Both proofs used methods from complex analysis, establishing as a main step of the proof that the Riemann zeta function ζ("s") is non-zero for all complex values of the variable "s" that have the form "s" = 1 + "it" with "t" > 0.
During the 20th century, the theorem of Hadamard and de la Vallée-Poussin also became known as the Prime Number Theorem. Several different proofs of it were found, including the "elementary" proofs of Atle Selberg and Paul Erdős (1949). While the original proofs of Hadamard and de la Vallée-Poussin are long and elaborate, later proofs introduced various simplifications through the use of Tauberian theorems but remained difficult to digest. A short proof was discovered in 1980 by American mathematician Donald J. Newman. Newman's proof is arguably the simplest known proof of the theorem, although it is non-elementary in the sense that it uses Cauchy's integral theorem from complex analysis.
Proof methodology.
In a lecture on prime numbers for a general audience, Fields medalist Terence Tao described one approach to proving the prime number theorem in poetic terms: listening to the "music" of the primes. We start with a "sound wave" that is "noisy" at the prime numbers and silent at other numbers; this is the von Mangoldt function. Then we analyze its notes or frequencies by subjecting it to a process akin to Fourier transform; this is the Mellin transform. The next and most difficult step is to prove that certain "notes" cannot occur in this music. This exclusion of certain notes leads to the statement of the prime number theorem. According to Tao, this proof yields much deeper insights into the distribution of the primes than the "elementary" proofs.
Proof sketch.
Here is a sketch of the proof referred to in Tao's lecture mentioned above. Like most proofs of the PNT, it starts out by reformulating the problem in terms of a less intuitive, but better-behaved, prime-counting function. The idea is to count the primes (or a related set such as the set of prime powers) with "weights" to arrive at a function with smoother asymptotic behavior. The most common such generalized counting function is the Chebyshev function formula_8, defined by
This is sometimes written as formula_10, where formula_11 is the von Mangoldt function, namely
It is now relatively easy to check that the PNT is equivalent to the claim that formula_13. Indeed, this follows from the easy estimates
and (using big O notation) for any formula_15,
The next step is to find a useful representation for formula_8. Let formula_18 be the Riemann zeta function. It can be shown that formula_18 is related to the von Mangoldt function formula_11, and hence to formula_8, via the relation
A delicate analysis of this equation and related properties of the zeta function, using the Mellin transform and Perron's formula, shows that for non-integer "x" the equation
holds, where the sum is over all zeros (trivial and non-trivial) of the zeta function. This striking formula is one of the so-called explicit formulas of number theory, and is already suggestive of the result we wish to prove, since the term "x" (claimed to be the correct asymptotic order of formula_8) appears on the right-hand side, followed by (presumably) lower-order asymptotic terms.
The next step in the proof involves a study of the zeros of the zeta function. The trivial zeros −2, −4, −6, −8, ... can be handled separately:
which vanishes for a large "x". The nontrivial zeros, namely those on the critical strip formula_26, can potentially be of an asymptotic order comparable to the main term "x" if formula_27, so we need to show that all zeros have real part strictly less than 1.
To do this, we take for granted that formula_18 is meromorphic in the half-plane formula_29, and is analytic there except for a simple pole at formula_30, and that there is a product formula formula_31 for formula_32 This product formula follows from the existence of unique prime factorization of integers, and shows that formula_18 is never zero in this region, so that its logarithm is defined there and formula_34 Write formula_35; then
Now observe the identity formula_37 so that
for all formula_39. Suppose now that formula_40. Certainly formula_41 is not zero, since formula_18 has a simple pole at formula_30. Suppose that formula_44 and let formula_45 tend to formula_46 from above. Since formula_18 has a simple pole at formula_30 and formula_49 stays analytic, the left hand side in the previous inequality tends to formula_50, a contradiction.
Finally, we can conclude that the PNT is "morally" true. To rigorously complete the proof there are still serious technicalities to overcome, due to the fact that the summation over zeta zeros in the explicit formula for formula_8 does not converge absolutely but only conditionally and in a "principal value" sense. There are several ways around this problem but many of them require rather delicate complex-analytic estimates that are beyond the scope of this article. Edwards's book provides the details. Another method is to use Ikehara's Tauberian theorem, though this theorem is itself quite hard to prove. D. J. Newman observed that the full strength of Ikehara's theorem is not needed for the prime number theorem, and one can get away with a special case that is much easier to prove.
Prime-counting function in terms of the logarithmic integral.
In a handwritten note on a reprint of his 1838 paper "Sur l'usage des séries infinies dans la théorie des nombres", which he mailed to Carl Friedrich Gauss, Peter Gustav Lejeune Dirichlet conjectured (under a slightly different form appealing to a series rather than an integral) that an even better approximation to π("x") is given by the offset logarithmic integral function Li("x"), defined by
Indeed, this integral is strongly suggestive of the notion that the 'density' of primes around "t" should be 1/log"t". This function is related to the logarithm by the asymptotic expansion
So, the prime number theorem can also be written as π("x") ~ Li("x"). In fact, in another paper in 1899 La Vallée Poussin proved that
for some positive constant "a", where "O"(...) is the big O notation. This has been improved to
Because of the connection between the Riemann zeta function and π("x"), the Riemann hypothesis has considerable importance in number theory: if established, it would yield a far better estimate of the error involved in the prime number theorem than is available today. More specifically, Helge von Koch showed in 1901 that, if and only if the Riemann hypothesis is true, the error term in the above relation can be improved to
The constant involved in the big O notation was estimated in 1976 by Lowell Schoenfeld: assuming the Riemann hypothesis,
for all "x" ≥ 2657. He also derived a similar bound for the Chebyshev prime-counting function ψ:
for all "x" ≥ 73.2. This latter bound has been shown to express a variance to mean power law (when regarded as a random function over the integers), 1/"f" noise and to also correspond to the Tweedie compound Poisson distribution. Parenthetically, the Tweedie distributions represent a family of scale invariant distributions that serve as foci of convergence for a generalization of the central limit theorem.
The logarithmic integral Li("x") is larger than π("x") for "small" values of "x". This is because it is (in some sense) counting not primes, but prime powers, where a power "p""n" of a prime "p" is counted as 1/"n" of a prime. This suggests that Li("x") should usually be larger than π("x") by roughly Li("x"1/2)/2, and in particular should usually be larger than π("x"). However, in 1914, J. E. Littlewood proved that this is not always the case. The first value of "x" where π("x") exceeds Li("x") is probably around "x" = 10316; see the article on Skewes' number for more details.
Elementary proofs.
In the first half of the twentieth century, some mathematicians (notably G. H. Hardy) believed that there exists a hierarchy of proof methods in mathematics depending on what sorts of numbers (integers, reals, complex) a proof requires, and that the prime number theorem (PNT) is a "deep" theorem by virtue of requiring complex analysis. This belief was somewhat shaken by a proof of the PNT based on Wiener's tauberian theorem, though this could be set aside if Wiener's theorem were deemed to have a "depth" equivalent to that of complex variable methods. There is no rigorous and widely accepted definition of the notion of elementary proof in number theory. One definition is "a proof that can be carried out in first order Peano arithmetic." There are number-theoretic statements (for example, the Paris–Harrington theorem) provable using second order but not first order methods, but such theorems are rare to date.
In March 1948, Atle Selberg established, by elementary means, the asymptotic formula
where
for primes formula_61. By July of that year, Selberg and Paul Erdős had each obtained elementary proofs of the PNT, both using Selberg's asymptotic formula as a starting point. These proofs effectively laid to rest the notion that the PNT was "deep," and showed that technically "elementary" methods (in other words Peano arithmetic) were more powerful than had been believed to be the case. In 1994, Charalambos Cornaros and Costas Dimitracopoulos proved the PNT using only formula_62, a formal system far weaker than Peano arithmetic. On the history of the elementary proofs of the PNT, including the Erdős–Selberg priority dispute, see an article by Dorian Goldfeld.
Computer verifications.
In 2005, Avigad "et al." employed the Isabelle theorem prover to devise a computer-verified variant of the Erdős–Selberg proof of the PNT. This was the first machine-verified proof of the PNT. Avigad chose to formalize the Erdős–Selberg proof rather than an analytic one because while Isabelle's library at the time could implement the notions of limit, derivative, and transcendental function, it had almost no theory of integration to speak of (Avigad et al. p. 19).
In 2009, John Harrison employed HOL Light to formalize a proof employing complex analysis. By developing the necessary analytic machinery, including the Cauchy integral formula, Harrison was able to formalize "a direct, modern and elegant proof instead of the more involved 'elementary' Erdős–Selberg argument".
Prime number theorem for arithmetic progressions.
Let formula_63 denote the number of primes in the arithmetic progression "a", "a" + "n", "a" + 2"n", "a" + 3"n", ... less than "x". Lejeune Dirichlet and Legendre conjectured, and Vallée-Poussin proved, that, if "a" and "n" are coprime, then
where φ is the Euler's totient function. In other words, the primes are distributed evenly among the residue classes ["a"] modulo "n" with gcd("a", "n") = 1. This is stronger than Dirichlet's theorem on arithmetic progressions (which only states that there is an infinity of primes in each class) and can be proved using similar methods used by Newman for his proof of the prime number theorem.
The Siegel–Walfisz theorem gives a good estimate for the distribution of primes in residue classes.
Prime number race.
Although we have in particular
empirically the primes congruent to 3 are more numerous and are nearly always ahead in this "prime number race"; the first reversal occurs at "x" = 26,861. However Littlewood showed in 1914 that there are infinitely many sign changes for the function
so the lead in the race switches back and forth infinitely many times. The phenomenon that π4,3("x") is ahead most of the time is called Chebyshev's bias. The prime number race generalizes to other moduli and is the subject of much research; Pál Turán asked whether it is always the case that π("x";"a","c") and π("x";"b","c") change places when "a" and "b" are coprime to "c". Granville and Martin give a thorough exposition and survey.
Bounds on the prime-counting function.
The prime number theorem is an "asymptotic" result. It gives an ineffective bound on π("x") as a direct consequence of the definition of the limit: for all ε > 0, there is an "S" such that for all "x" > "S",
However, better bounds on π("x") are known, for instance Pierre Dusart's
The first inequality holds for all "x" ≥ 599 and the second one for "x" ≥ 355991.
A weaker but sometimes useful bound for "x" ≥ 55 is
In Pierre Dusart's thesis there are stronger versions of this type of inequality that are valid for larger "x". Later in 2010, Dusart proved:
The proof by de la Vallée-Poussin implies the following.
For every ε > 0, there is an "S" such that for all "x" > "S",
Approximations for the "n"th prime number.
As a consequence of the prime number theorem, one gets an asymptotic expression for the "n"th prime number, denoted by "p""n":
A better approximation is
Again considering the 200 · 1015 prime number 8512677386048191063, this gives an estimate of 8512681315554715386; the first 5 digits match and relative error is about 0.00005%.
Rosser's theorem states that "p""n" is larger than "n" log "n". This can be improved by the following pair of bounds:
Table of π("x"), "x" / log "x", and li("x").
The table compares exact values of π("x") to the two approximations "x" / log "x" and li("x"). The last column, "x" / π("x"), is the average prime gap below "x".
The value for π(1024) was originally computed assuming the Riemann hypothesis; it has since been verified unconditionally.
Analogue for irreducible polynomials over a finite field.
There is an analogue of the prime number theorem that describes the "distribution" of irreducible polynomials over a finite field; the form it takes is strikingly similar to the case of the classical prime number theorem.
To state it precisely, let "F" = GF("q") be the finite field with "q" elements, for some fixed "q", and let "N""n" be the number of monic "irreducible" polynomials over "F" whose degree is equal to "n". That is, we are looking at polynomials with coefficients chosen from "F", which cannot be written as products of polynomials of smaller degree. In this setting, these polynomials play the role of the prime numbers, since all other monic polynomials are built up of products of them. One can then prove that
If we make the substitution "x" = "q""n", then the right hand side is just
which makes the analogy clearer. Since there are precisely "q""n" monic polynomials of degree "n" (including the reducible ones), this can be rephrased as follows: if a monic polynomial of degree "n" is selected randomly, then the probability of it being irreducible is about 1/"n".
One can even prove an analogue of the Riemann hypothesis, namely that
The proofs of these statements are far simpler than in the classical case. It involves a short combinatorial argument, summarised as follows. Every element of the degree "n" extension of "F" is a root of some irreducible polynomial whose degree "d" divides "n"; by counting these roots in two different ways one establishes that
where the sum is over all divisors "d" of "n". Möbius inversion then yields
where μ("k") is the Möbius function. (This formula was known to Gauss.) The main term occurs for "d" = "n", and it is not difficult to bound the remaining terms. The "Riemann hypothesis" statement depends on the fact that the largest proper divisor of "n" can be no larger than "n"/2.

</doc>
<doc id="23693" url="https://en.wikipedia.org/wiki?curid=23693" title="Conflict of laws">
Conflict of laws

Conflict of laws or private international law (both terms are used interchangeably) concerns relations across different legal jurisdictions between persons, and sometimes also companies, corporations and other legal entities.
Choice of laws.
Courts faced with a choice of law issue have a two-stage process:
Private international law on marriages and legal dissolution of marriages (divorce).
In divorce cases, when a court is attempting to distribute marital property, if the divorcing couple is local and the property is local, then the court applies its domestic law "lex fori". The case becomes more complicated if foreign elements are thrown into the mix, such as when the place of marriage is different from the territory where divorce was filed; when the parties' nationalities and residences do not match; when there is property in a foreign jurisdiction; or when the parties have changed residence several times during the marriage.
Whereas commercial agreements or prenuptial agreements generally do not require legal formalities to be observed, when married couples enter a property agreement, stringent requirements are imposed, including notarization, witnesses, special acknowledgment forms. In some countries, these must be filed (or docketed) with a domestic court, and the terms must be "so ordered" by a judge. This is done in order to ensure that no undue influence or oppression has been exerted by one spouse against the other. Upon presenting a property agreement between spouses to a court of divorce, that court will generally assure itself of the following factors: signatures, legal formalities, intent, later intent, free will, lack of oppression, reasonableness and fairness, consideration, performance, reliance, later repudiation in writing or by conduct, and whichever other concepts of contractual bargaining apply in the context.
Private international law on unmarried persons.
Unlike marriage which has an international recognised legal status, there are no international treaties on recognition of unmarried couple's legal status. If an unmarried couple change residence to different countries, then the local law on where the couple is last domiciled is applied to them. This covers legal status of the relationship, rights, obligations, and all worldwide movable and immovable property. To otherwise interpret the law would mean if the unmarried couple had assets in several different countries, they would then need separate legal cases in each country to resolve all their movable and immovable property.
In the absence of a valid and enforceable agreement for an unmarried couple, here’s how the conflict of law rules work:
Contracts.
Many contracts and other forms of legally binding agreement include a jurisdiction or arbitration clause specifying the parties' choice of venue for any litigation (called a forum selection clause). Then, choice of law clauses may specify which laws the court or tribunal should apply to each aspect of the dispute. This matches the substantive policy of freedom of contract. Judges have accepted that the principle of party autonomy allows the parties to select the law most appropriate to their transaction. This judicial acceptance of subjective intent excludes the traditional reliance on objective connecting factors; it also harms consumers as vendors often impose one-sided contractual terms selecting a venue far from the buyer's home or workplace.
Harmonization of laws.
To apply one national legal system as against another may never be an entirely satisfactory approach. The parties' interests may always be better protected by applying a law conceived with international realities in mind. The Hague Conference on Private International Law is a treaty organization that oversees conventions designed to develop a uniform system. The deliberations of the conference have recently been the subject of controversy over the extent of cross-border jurisdiction on electronic commerce and defamation issues. There is a general recognition that there is a need for an international law of contracts: for example, many nations have ratified the "Vienna Convention on the International Sale of Goods", the "Rome Convention on the Law Applicable to Contractual Obligations" offers less specialized uniformity, and there is support for the "UNIDROIT Principles of International Commercial Contracts", a private restatement, all of which represent continuing efforts to produce international standards as the internet and other technologies encourage ever more interstate commerce. But other branches of the law are less well served and the dominant trend remains the role of the forum law rather than a supranational system for Conflict purposes. Even the EU, which has institutions capable of creating uniform rules with direct effect, has failed to produce a universal system for the common market. Nevertheless, the Treaty of Amsterdam does confer authority on the Community's institutions to legislate by Council Regulation in this area with supranational effect. Article 177 would give the Court of Justice jurisdiction to interpret and apply their principles so, if the political will arises, uniformity may gradually emerge in letter. Whether the domestic courts of the Member States would be consistent in applying those letters is speculative.

</doc>
<doc id="23696" url="https://en.wikipedia.org/wiki?curid=23696" title="Timeline of programming languages">
Timeline of programming languages

This is a record of historically important programming languages, by decade.
Legend

</doc>
<doc id="23698" url="https://en.wikipedia.org/wiki?curid=23698" title="International Fixed Calendar">
International Fixed Calendar

The International Fixed calendar (also known as the Cotsworth plan, the Eastman plan, the 13 Month calendar or the Equal Month calendar) is a solar calendar proposal for calendar reform designed by Moses B. Cotsworth, who presented it in 1902. It provides for a year of 13 months of 28 days each. It is therefore a perennial calendar, with every date fixed always on the same weekday. Though it was never officially adopted in any country, entrepreneur George Eastman adopted it for use in his Eastman Kodak Company, where it was used from 1928 to 1989. It is also the official calendar of UK supermarket Sainsbury's.
Rules.
The calendar year has 13 months with 28 days each, divided into exactly 4 weeks (13 × 28 = 364). An extra day added as a holiday at the end of the year (December 29), sometimes called "Year Day", does not belong to any week and brings the total to 365 days. Each year coincides with the corresponding Gregorian year, so January 1 in the Cotsworth calendar always falls on Gregorian January 1. Twelve months are named and ordered the same as those of the Gregorian calendar, except that the extra month is inserted between June and July, and called "Sol". Situated in mid-summer (from the point of view of its Northern Hemisphere authors), the name of the new month was chosen in homage to the sun.
Leap year in the International Fixed Calendar contains 366 days, and its occurrence follows the Gregorian rule. There is a leap year in every year whose number is divisible by 4, but not if the year number is divisible by 100, unless it is also divisible by 400. So although the year 2000 was a leap year, the years 1700, 1800, and 1900 were common years. The International Fixed Calendar inserts the extra day in leap year as June 29 - between Saturday June 28 and Sunday Sol 1.
Each month begins on a Sunday, and ends on a Saturday; consequently, every year begins on Sunday. Neither Year Day nor Leap Day are considered to be part of any week; they are preceded by a Saturday and are followed by a Sunday.
All the months look like this:
The following shows how the 13 months and extra days of the International Fixed Calendar occur in relation to the dates of the Gregorian calendar:
History.
The simple idea of a 13-month perennial calendar has been around since at least the middle of the 18th century. Versions of the idea differ mainly on how the months are named, and the treatment of the extra day in leap year.
The "Georgian calendar" was proposed in 1745 by an American Colonist from Maryland writing under the pen name, Hirossa Ap-Iccim (=Rev. Hugh Jones). The author named the plan, and the thirteenth month, after King George II of Great Britain. The 365th day each year was to be set aside as Christmas. The treatment of leap year varied from the Gregorian rule, however; and the year would begin closer to the winter solstice. In a later version of the plan, published in 1753, the 13 months were all renamed for Christian saints.
In 1849 the French philosopher Auguste Comte (1798–1857) proposed the 13-month "Positivist Calendar", naming the months: Moses, Homer, Aristotle, Archimedes, Caesar, St. Paul, Charlemagne, Dante, Gutenberg, Shakespeare, Descartes, Frederic and Bichat. The days of the year were likewise dedicated to "saints" in the Positivist Religion of Humanity. Positivist weeks, months, and years begin with Monday instead of Sunday. Comte also reset the year number, beginning the era of his calendar (year 1) with the Gregorian year 1789. For the extra days of the year not belonging to any week or month, Comte followed the pattern of Ap-Iccim (Jones), ending each year with a festival on the 365th day, followed by a subsequent feast day occurring only in leap years.
Whether Moses Cotsworth was familiar with the 13-month plans that preceded his International Fixed Calendar is not known. He did follow Ap-Iccim (Jones) in designating the 365th day of the year as Christmas. His suggestion was that this last day of the year should be designated a Sunday, and hence, because the following day would be New Year's Day and a Sunday also, he called it a Double Sunday. Since Cotsworth's goal was a simplified, more "rational" calendar for business and industry, he would carry over all the features of the Gregorian calendar consistent with this goal, including the traditional month names, the week beginning on Sunday, and the Gregorian leap-year rule.
To promote Cotsworth's calendar reform The International Fixed Calendar League was founded in 1923, just after the plan was selected by the League of Nations as the best of 130 calendar proposals put forward. Sir Sandford Fleming, the inventor and driving force behind worldwide adoption of standard time, became the first president of the IFCL. The League opened offices in London and later in Rochester, New York. George Eastman, of the Eastman Kodak Company, became a fervent supporter of the IFC, and instituted its use at Kodak. The International Fixed Calendar League ceased operations shortly after the calendar plan failed to win final approval of the League of Nations in 1937.
Advantages.
The several advantages of The International Fixed Calendar are mainly related to its organization. 

</doc>
<doc id="23703" url="https://en.wikipedia.org/wiki?curid=23703" title="Potential energy">
Potential energy

In physics, potential energy is energy possessed by a body by virtue of its position relative to others, stresses within itself, electric charge, and other factors. 
Common types include the gravitational potential energy of an object that depends on its mass and its distance from the center of mass of another object, the elastic potential energy of an extended spring, and the electric potential energy of an electric charge in an electric field. The unit for energy in the International System of Units (SI) is the joule, which has the symbol "J".
The term "potential energy" was introduced by the 19th century Scottish engineer and physicist William Rankine, although it has links to Greek philosopher Aristotle's concept of potentiality.
Potential energy is associated with forces that act on a body in a way that depends only on the body's position in space. These forces can be represented by a vector at every point in space forming a vector field of forces, or a force field.
If the work of a force field acting on a body that moves from a start to an end position is determined only by these two positions, and does not depend on the trajectory of the body, then there is a function known as "potential energy" that can be evaluated at the two positions to determine this work. Furthermore, the force field is determined by this potential energy and is described as derivable from a potential.
Overview.
Potential energy is the stored energy of an object. It is the energy by virtue of an object's position relative to other objects. Potential energy is often associated with restoring forces such as a spring or the force of gravity. The action of stretching the spring or lifting the mass is performed by an external force that works against the force field of the potential. This work is stored in the force field, which is said to be stored as potential energy. If the external force is removed the force field acts on the body to perform the work as it moves the body back to the initial position, reducing the stretch of the spring or causing a body to fall.
The more formal definition is that potential energy is the energy difference between the energy of an object in a given position and its energy at a reference position.
There are various types of potential energy, each associated with a particular type of force. For example, the work of an elastic force is called elastic potential energy; work of the gravitational force is called gravitational potential energy; work of the Coulomb force is called electric potential energy; work of the strong nuclear force or weak nuclear force acting on the baryon charge is called nuclear potential energy; work of intermolecular forces is called intermolecular potential energy. Chemical potential energy, such as the energy stored in fossil fuels, is the work of the Coulomb force during rearrangement of mutual positions of electrons and nuclei in atoms and molecules. Thermal energy usually has two components: the kinetic energy of random motions of particles and the potential energy of their mutual positions.
Forces derivable from a potential are also called conservative forces. The work done by a conservative force is
where formula_2 is the change in the potential energy associated with the force. The negative sign provides the convention that work done against a force field increases potential energy, while work done by the force field decreases potential energy. Common notations for potential energy are "U", "V", and "Ep".
Work and potential energy.
Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points, then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.
If the work for an applied force is independent of the path, then the work done by the force is evaluated at the start and end of the trajectory of the point of application. This means that there is a function "U" (x), called a "potential," that can be evaluated at the two points xA and xB to obtain the work over any trajectory between these two points. It is tradition to define this function with a negative sign so that positive work is a reduction in the potential, that is 
where "C" is the trajectory taken from A to B. Because the work done is independent of the path taken, then this expression is true for any trajectory, "C", from A to B.
The function "U"(x) is called the potential energy associated with the applied force. Examples of forces that have potential energies are gravity and spring forces.
Derivable from a potential.
In this section the relationship between work and potential energy is presented in more detail. The line integral that defines work along curve "C" takes a special form if the force F is related to a scalar field φ(x) so that
In this case, work along the curve is given by
which can be evaluated using the gradient theorem to obtain
This shows that when forces are derivable from a scalar field, the work of those forces along a curve "C" is computed by evaluating the scalar field at the start point "A" and the end point "B" of the curve. This means the work integral does not depend on the path between "A" and "B" and is said to be independent of the path.
Potential energy "U"=-φ(x) is traditionally defined as the negative of this scalar field so that work by the force field decreases potential energy, that is
In this case, the application of the del operator to the work function yields,
and the force F is said to be "derivable from a potential." This also necessarily implies that F must be a conservative vector field. The potential "U" defines a force F at every point x in space, so the set of forces is called a force field.
Computing potential energy.
Given a force field F(x), evaluation of the work integral using the gradient theorem can be used to find the scalar function associated with potential energy. This is done by introducing a parameterized curve γ(t)=r(t) from γ(a)=A to γ(b)=B, and computing,
For the force field F, let v= dr/dt, then the gradient theorem yields,
The power applied to a body by a force field is obtained from the gradient of the work, or potential, in the direction of the velocity v of the point of application, that is
Examples of work that can be computed from potential functions are gravity and spring forces.
Potential energy for near Earth gravity.
The following function is called the potential energy of a near earth gravity field:
where m is in kg, g is 9.81 for earth and h is in metres.
In classical physics, gravity exerts a constant downward force F=(0, 0, "Fz") on the center of mass of a body moving near the surface of the Earth. The work of gravity on a body moving along a trajectory r(t) = ("x"(t), "y"(t), "z"(t)), such as the track of a roller coaster is calculated using its velocity, v=("v"x, "v"y, "v"z), to obtain
where the integral of the vertical component of velocity is the vertical distance. Notice that the work of gravity depends only on the vertical movement of the curve r(t).
Potential energy for a linear spring.
A horizontal spring exerts a force F = (−"kx", 0, 0) that is proportional to its deflection in the "x" direction. The work of this spring on a body moving along the space curve s("t") = ("x"("t"), "y"("t"), "z"("t")), is calculated using its velocity, v = ("v"x, "v"y, "v"z), to obtain
For convenience, consider contact with the spring occurs at "t" = 0, then the integral of the product of the distance "x" and the "x"-velocity, "xvx", is "x"2/2.
The function 
is called the potential energy of a linear spring.
Elastic potential energy is the potential energy of an elastic object (for example a bow or a catapult) that is deformed under tension or compression (or stressed in formal terminology). It arises as a consequence of a force that tries to restore the object to its original shape, which is most often the electromagnetic force between the atoms and molecules that constitute the object. If the stretch is released, the energy is transformed into kinetic energy.
Potential energy for gravitational forces between two bodies.
The gravitational potential function, also known as gravitational potential energy, is:
The negative sign follows the convention that work is gained from a loss of potential energy.
Derivation.
Gravitational potential energy between two bodies in space is obtained from the force exerted by a mass "M" on another mass "m" is given by
where r is the position vector from the object with mass "M" to the object with mass "m", and "G" is the gravitational constant.
This can also be expressed as
where formula_19 is a vector of length 1 pointing from "M" to "m".
Let the mass "m" move at the velocity v then the work of gravity on this mass as it moves from position r(t1) to r(t2) is given by
Notice that the position and velocity of the mass "m" are given by
where e"r" and e"t" are the radial and tangential unit vectors directed relative to the vector from "M" to "m". Use this to simplify the formula for work of gravity to,
This calculation uses the fact that
Potential energy for electrostatic forces between two bodies.
The electrostatic force exerted by a charge "Q" on another charge "q" is given by
where r is the position vector from "Q" to "q" and "ε"0 is the vacuum permittivity. This may also be written using Coulomb's constant .
The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by the potential function
Reference level.
The potential energy is a function of the state a system is in, and is defined relative to that for a particular state. This reference state is not always a real state, it may also be a limit, such as with the distances between all bodies tending to infinity, provided that the energy involved in tending to that limit is finite, such as in the case of inverse-square law forces. Any arbitrary reference state could be used, therefore it can be chosen based on convenience.
Typically the potential energy of a system depends on the "relative" positions of its components only, so the reference state can also be expressed in terms of relative positions.
Gravitational potential energy.
Gravitational energy is the potential energy associated with gravitational force, as work is required to elevate objects against Earth's gravity. The potential energy due to elevated positions is called gravitational potential energy, and is evidenced by water in an elevated reservoir or kept behind a dam. If an object falls from one point to another point inside a gravitational field, the force of gravity will do positive work on the object, and the gravitational potential energy will decrease by the same amount.
Consider a book placed on top of a table. As the book is raised from the floor, to the table, some external force works against the gravitational force. If the book falls back to the floor, the "falling" energy the book receives is provided by the gravitational force. Thus, if the book falls off the table, this potential energy goes to accelerate the mass of the book and is converted into kinetic energy. When the book hits the floor this kinetic energy is converted into heat, deformation and sound by the impact.
The factors that affect an object's gravitational potential energy are its height relative to some reference point, its mass, and the strength of the gravitational field it is in. Thus, a book lying on a table has less gravitational potential energy than the same book on top of a taller cupboard, and less gravitational potential energy than a heavier book lying on the same table. An object at a certain height above the Moon's surface has less gravitational potential energy than at the same height above the Earth's surface because the Moon's gravity is weaker. Note that "height" in the common sense of the term cannot be used for gravitational potential energy calculations when gravity is not assumed to be a constant. The following sections provide more detail.
Local approximation.
The strength of a gravitational field varies with location. However, when the change of distance is small in relation to the distances from the center of the source of the gravitational field, this variation in field strength is negligible and we can assume that the force of gravity on a particular object is constant. Near the surface of the Earth, for example, we assume that the acceleration due to gravity is a constant ("standard gravity"). In this case, a simple expression for gravitational potential energy can be derived using the "W" = "Fd" equation for work, and the equation
The amount of gravitational potential energy possessed by an elevated object is equal to the work done against gravity in lifting it. The work done equals the force required to move it upward multiplied with the vertical distance it is moved (remember "W = Fd"). The upward force required while moving at a constant velocity is equal to the weight, "mg", of an object, so the work done in lifting it through a height "h" is the product "mgh". Thus, when accounting only for mass, gravity, and altitude, the equation is:
where "U" is the potential energy of the object relative to its being on the Earth's surface, "m" is the mass of the object, "g" is the acceleration due to gravity, and "h" is the altitude of the object. If "m" is expressed in kilograms, "g" in m/s2 and "h" in metres then "U" will be calculated in joules.
Hence, the potential difference is
General formula.
However, over large variations in distance, the approximation that "g" is constant is no longer valid, and we have to use calculus and the general mathematical definition of work to determine gravitational potential energy. For the computation of the potential energy we can integrate the gravitational force, whose magnitude is given by Newton's law of gravitation, with respect to the distance "r" between the two bodies. Using that definition, the gravitational potential energy of a system of masses "m"1 and "M"2 at a distance "r" using gravitational constant "G" is
where "K" is an arbitrary constant dependent on the choice of datum from which potential is measured. Choosing the convention that "K"=0 (i.e. in relation to a point at infinity) makes calculations simpler, albeit at the cost of making "U" negative; for why this is physically reasonable, see below.
Given this formula for "U", the total potential energy of a system of "n" bodies is found by summing, for all formula_30 pairs of two bodies, the potential energy of the system of those two bodies.
therefore,
Why choose a convention where gravitational energy is negative?
As with all potential energies, only differences in gravitational potential energy matter for most physical purposes, and the choice of zero point is arbitrary. Given that there is no reasonable criterion for preferring one particular finite "r" over another, there seem to be only two reasonable choices for the distance at which "U" becomes zero: formula_33 and formula_34. The choice of formula_35 at infinity may seem peculiar, and the consequence that gravitational energy is always negative may seem counterintuitive, but this choice allows gravitational potential energy values to be finite, albeit negative.
The singularity at formula_33 in the formula for gravitational potential energy means that the only other apparently reasonable alternative choice of convention, with formula_35 for formula_33, would result in potential energy being positive, but infinitely large for all nonzero values of "r", and would make calculations involving sums or differences of potential energies beyond what is possible with the real number system. Since physicists abhor infinities in their calculations, and "r" is always non-zero in practice, the choice of formula_35 at infinity is by far the more preferable choice, even if the idea of negative energy in a gravity well appears to be peculiar at first.
The negative value for gravitational energy also has deeper implications that make it seem more reasonable in cosmological calculations where the total energy of the universe can meaningfully be considered; see inflation theory for more on this.
Uses.
Gravitational potential energy has a number of practical uses, notably the generation of pumped-storage hydroelectricity. For example, in Dinorwig, Wales, there are two lakes, one at a higher elevation than the other. At times when surplus electricity is not required (and so is comparatively cheap), water is pumped up to the higher lake, thus converting the electrical energy (running the pump) to gravitational potential energy. At times of peak demand for electricity, the water flows back down through electrical generator turbines, converting the potential energy into kinetic energy and then back into electricity. The process is not completely efficient and some of the original energy from the surplus electricity is in fact lost to friction.
Gravitational potential energy is also used to power clocks in which falling weights operate the mechanism. It's also used by counterweights for lifting up an elevator, crane, or sash window.
Roller coasters are an entertaining way to utilize potential energy - chains are used to move a car up an incline (building up gravitational potential energy), to then have that energy converted into kinetic energy as it falls.
Another practical use is utilizing gravitational potential energy to descend (perhaps coast) downhill in transportation such as the descent of an automobile, truck, railroad train, bicycle, airplane, or fluid in a pipeline. In some cases the kinetic energy obtained from potential energy of descent may be used to start ascending the next grade such as what happens when a road is undulating and has frequent dips. The commercialization of stored energy (in the form of rail cars raised to higher elevations) that is then converted to electrical energy when needed by an electrical grid, is being undertaken in the United States in a system called Advanced Rail Energy Storage (ARES).
Chemical potential energy.
Chemical potential energy is a form of potential energy related to the structural arrangement of atoms or molecules. This arrangement may be the result of chemical bonds within a molecule or otherwise. Chemical energy of a chemical substance can be transformed to other forms of energy by a chemical reaction. As an example, when a fuel is burned the chemical energy is converted to heat, same is the case with digestion of food metabolized in a biological organism. Green plants transform solar energy to chemical energy through the process known as photosynthesis, and electrical energy can be converted to chemical energy through electrochemical reactions.
The similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc.
Electric potential energy.
An object can have potential energy by virtue of its electric charge and several forces related to their presence. There are two main types of this kind of potential energy: electrostatic potential energy, electrodynamic potential energy (also sometimes called magnetic potential energy).
Electrostatic potential energy.
Electrostatic potential energy between two bodies in space is obtained from the force exerted by a charge "Q" on another charge "q" which is given by
where r is the position vector from "Q" to "q" and "ε"0 is the vacuum permittivity. This may also be written using Coulomb's constant .
If the electric charge of an object can be assumed to be at rest, then it has potential energy due to its position relative to other charged objects. The electrostatic potential energy is the energy of an electrically charged particle (at rest) in an electric field. It is defined as the work that must be done to move it from an infinite distance away to its present location, adjusted for non-electrical forces on the object. This energy will generally be non-zero if there is another electrically charged object nearby.
The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by the potential function
A related quantity called "electric potential" (commonly denoted with a "V" for voltage) is equal to the electric potential energy per unit charge.
Magnetic potential energy.
The energy of a magnetic moment in an externally produced magnetic B-field has potential energy
The magnetization in a field is
where the integral can be over all space or, equivalently, where is nonzero.
Magnetic potential energy is the form of energy related not only to the distance between magnetic materials, but also to the orientation, or alignment, of those materials within the field. For example, the needle of a compass has the lowest magnetic potential energy when it is aligned with the north and south poles of the Earth's magnetic field. If the needle is moved by an outside force, torque is exerted on the magnetic dipole of the needle by the Earth's magnetic field, causing it to move back into alignment. The magnetic potential energy of the needle is highest when its field is in the same direction as the Earth's magnetic field. Two magnets will have potential energy in relation to each other and the distance between them, but this also depends on their orientation. If the opposite poles are held apart, the potential energy will be the highest when they are near the edge of their attraction, and the lowest when they pull together. Conversely, like poles will have the highest potential energy when forced together, and the lowest when they spring apart.
Nuclear potential energy.
Nuclear potential energy is the potential energy of the particles inside an atomic nucleus. The nuclear particles are bound together by the strong nuclear force. Weak nuclear forces provide the potential energy for certain kinds of radioactive decay, such as beta decay.
Nuclear particles like protons and neutrons are not destroyed in fission and fusion processes, but collections of them have less mass than if they were individually free, and this mass difference is liberated as heat and radiation in nuclear reactions (the heat and radiation have the missing mass, but it often escapes from the system, where it is not measured). The energy from the Sun is an example of this form of energy conversion. In the Sun, the process of hydrogen fusion converts about 4 million tonnes of solar matter per second into electromagnetic energy, which is radiated into space.
Forces, potential and potential energy.
Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points, then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.
For example, gravity is a conservative force. The associated potential is the gravitational potential, often denoted by formula_44 or formula_45, corresponding to the energy per unit mass as a function of position. The gravitational potential energy of two particles of mass "M" and "m" separated by a distance "r" is
The gravitational potential (specific energy) of the two bodies is
where formula_48 is the reduced mass.
The work done against gravity by moving an infinitesimal mass from point A with formula_49 to point B with formula_50 is formula_51 and the work done going back the other way is formula_52 so that the total work done in moving from A to B and returning to A is
If the potential is redefined at A to be formula_54 and the potential at B to be formula_55, where formula_56 is a constant (i.e. formula_56 can be any number, positive or negative, but it must be the same at A as it is at B) then the work done going from A to B is
as before.
In practical terms, this means that one can set the zero of formula_59 and formula_44 anywhere one likes. One may set it to be zero at the surface of the Earth, or may find it more convenient to set zero at infinity (as in the expressions given earlier in this section).
A conservative force can be expressed in the language of differential geometry as a closed form. As Euclidean space is contractible, its de Rham cohomology vanishes, so every closed form is also an exact form, and can be expressed as the gradient of a scalar field. This gives a mathematical justification of the fact that all conservative forces are gradients of a potential field.

</doc>
<doc id="23704" url="https://en.wikipedia.org/wiki?curid=23704" title="Pyramid">
Pyramid

A pyramid (from "") is a structure whose outer surfaces are triangular and converge to a single point at the top, making the shape roughly a pyramid in the geometric sense. The base of a pyramid can be trilateral, quadrilateral, or any polygon shape, meaning that a pyramid has at least three outer triangular surfaces (at least four faces including the base). The square pyramid, with square base and four triangular outer surfaces, is a common version.
A pyramid's design, with the majority of the weight closer to the ground, and with the pyramidion on top means that less material higher up on the pyramid will be pushing down from above. This distribution of weight allowed early civilizations to create stable monumental structures. It has been demonstrated that the common shape of the pyramids of antiquity, from Egypt to Central America, represents the dry-stone construction that requires minimum human work.
Pyramids have been built by civilizations in many parts of the world. For thousands of years, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both of Egypt, the latter is the only one of the Seven Wonders of the Ancient World still remaining. Khufu's Pyramid is built mainly of limestone (with large red granite blocks used in some interior chambers), and is considered an architectural masterpiece. It contains over 2,000,000 blocks ranging in weight from to and is built on a square base with sides measuring about 230 m (755 ft), covering 13 acres. Its four sides face the four cardinal points precisely and it has an angle of 52 degrees. The original height of the Pyramid was 146.5 m (488 ft), but today it is only 137 m (455 ft) high, the 9 m (33 ft) that is missing is due to the theft of the fine quality white Tura limestone covering, or casing stones, for construction in Cairo. It is still the tallest pyramid.
The largest pyramid by volume is the Great Pyramid of Cholula, in the Mexican state of Puebla.
Ancient monuments.
Mesopotamia.
The Mesopotamians built the earliest pyramidal structures, called "ziggurats". In ancient times, these were brightly painted in gold/bronze. Since they were constructed of sun-dried mud-brick, little remains of them. Ziggurats were built by the Sumerians, Babylonians, Elamites, Akkadians, and Assyrians for local religions. Each ziggurat was part of a temple complex which included other buildings. The precursors of the ziggurat were raised platforms that date from the Ubaid period during the fourth millennium BC. The earliest ziggurats began near the end of the Early Dynastic Period. The latest Mesopotamian ziggurats date from the 6th century BC. 
Built in receding tiers upon a rectangular, oval, or square platform, the ziggurat was a pyramidal structure with a flat top. Sun-baked bricks made up the core of the ziggurat with facings of fired bricks on the outside. The facings were often glazed in different colors and may have had astrological significance. Kings sometimes had their names engraved on these glazed bricks. The number of tiers ranged from two to seven. It is assumed that they had shrines at the top, but there is no archaeological evidence for this and the only textual evidence is from Herodotus. Access to the shrine would have been by a series of ramps on one side of the ziggurat or by a spiral ramp from base to summit. 
The Mesopotamian ziggurats were not places for public worship or ceremonies. They were believed to be dwelling places for the gods and each city had its own patron god. Only priests were permitted on the ziggurat or in the rooms at its base, and it was their responsibility to care for the gods and attend to their needs. The priests were very powerful members of Sumerian society.
Egypt.
The most famous pyramids are the Egyptian pyramids — huge structures built of brick or stone, some of which are among the world's largest constructions. They are shaped as a reference to the rays of the sun. Most pyramids had a polished, highly reflective white limestone surface, to give them a shining appearance when viewed from a distance. The capstone was usually made of hard stone - granite or basalt - and could be plated with gold, silver, or electrum and would also be highly reflective.
After 2700 BC, the Egyptians began building pyramids, until about 1700 BC. The first pyramid was built during the Third Dynasty by king Djoser and his architect Imhotep, as a step pyramid by stacking six mastabas. The largest Egyptian pyramids are the pyramids at Giza. ""The Egyptian sun god Ra, considered the father of all pharaohs, was said to have created himself from a pyramid-shaped mound of earth before creating all other gods. The pyramid’s shape is thought to have symbolized the sun’s rays"" (Donald B. Redford, Ph.D., Penn State).
The age of the pyramids reached its zenith at Giza in 2575–2150 BC. Ancient Egyptian pyramids were in most cases placed west of the river Nile because the divine pharaoh’s soul was meant to join with the sun during its descent before continuing with the sun in its eternal round.
As of 2008, some 135 pyramids have been discovered in Egypt. The Great Pyramid of Giza is the largest in Egypt and one of the largest in the world. It was the tallest building in the world until Lincoln Cathedral was finished in 1311 AD. The base is over in area. While pyramids are associated with Egypt, the nation of Sudan has 220 extant pyramids, the most numerous in the world.
The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World. It is the only one to survive into modern times. The Ancient Egyptians covered the faces of pyramids with polished white limestone, containing great quantities of fossilized seashells. Many of the facing stones have fallen or have been removed and used for construction in Cairo.
Most pyramids are located near Cairo, with only one royal pyramid being located south of Cairo, at the Abydos temple complex. The pyramid at Abydos, Egypt were commissioned by Ahmose I who founded the 18th Dynasty and the New Kingdom.
The building of pyramids began in the Third Dynasty with the reign of King Djoser. Early kings such as Snefru built several pyramids, with subsequent kings adding to the number of pyramids until the end of the Middle Kingdom. The last king to build royal pyramids was Ahmose, with later kings hiding their tombs in the hills, such as those in the Valley of the Kings in Luxor's West Bank.
In Medinat Habu, or Deir el-Medina, smaller pyramids were built by individuals. Smaller pyramids were also built by the Nubians who ruled Egypt in the Late Period, though their pyramids had steeper sides.
Sudan.
Nubian pyramids were constructed (roughly 240 of them) at three sites in Sudan to serve as tombs for the kings and queens of Napata and Meroë. The pyramids of Kush, also known as Nubian Pyramids, have different characteristics than the pyramids of Egypt. The Nubian pyramids were constructed at a steeper angle than Egyptian ones. Pyramids were still being built in Sudan as late as 300 AD.
Nigeria.
One of the unique structures of Igbo culture was the Nsude Pyramids, at the Nigerian town of Nsude, northern Igboland. Ten pyramidal structures were built of clay/mud. The first base section was 60 ft. in circumference and 3 ft. in height. The next stack was 45 ft. in circumference. Circular stacks continued, till it reached the top. The structures were temples for the god Ala/Uto, who was believed to reside at the top. A stick was placed at the top to represent the god's residence. The structures were laid in groups of five parallel to each other. Because it was built of clay/mud like the Deffufa of Nubia, time has taken its toll requiring periodic reconstruction.
Greece.
Pausanias (2nd century AD) mentions two buildings resembling pyramids, one, 19 kilometres (12 mi) southwest of the still standing structure at Hellenikon, a common tomb for soldiers who died in a legendary struggle for the throne of Argos and another which he was told was the tomb of Argives killed in a battle around 669/8 BC. Neither of these still survive and there is no evidence that they resembled Egyptian pyramids.
There are also at least two surviving pyramid-like structures still available to study, one at Hellenikon and the other at Ligourio/Ligurio, a village near the ancient theatre Epidaurus. These buildings were not constructed in the same manner as the pyramids in Egypt. They do have inwardly sloping walls but other than those there is no obvious resemblance to Egyptian pyramids. They had large central rooms (unlike Egyptian pyramids) and the Hellenikon structure is rectangular rather than square, which means that the sides could not have met at a point. The stone used to build these structures was limestone quarried locally and was cut to fit, not into freestanding blocks like the Great Pyramid of Giza.
There are no remains or graves in or near the structures. Instead, the rooms that the walls housed were made to be locked from the inside. This coupled with the platform roof, means that one of the functions these structures could have served was as watchtowers. Another possibility for the buildings is that they are shrines to heroes and soldiers of ancient times, but the lock on the inside makes no sense for such a purpose.
The dating of these structures has been made from the pot shards excavated from the floor and on the grounds. The latest dates available from scientific dating have been estimated around the 5th and 4th centuries. Normally this technique is used for dating pottery, but here researchers have used it to try to date stone flakes from the walls of the structures. This has created some debate about whether or not these structures are actually older than Egypt, which is part of the Black Athena controversy. The basis for their use of thermoluminescence in order to date these structures is a new method of collecting samples for testing. Scientists from laboratories hired out by the recent excavators of the site, The Academy of Athens, say that they can use the electrons trapped on the inner surface of the stones to positively identify the date that the stones were quarried and put together.
Mary Lefkowitz has criticised this research. She suggests that some of the research was done not to determine the reliability of the dating method, as was suggested, but to back up an assumption of age and to make certain points about pyramids and Greek civilization. She notes that not only are the results not very precise, but that other structures mentioned in the research are not in fact pyramids, e.g. a tomb alleged to be the tomb of Amphion and Zethus near Thebes, a structure at Stylidha (Thessaly) which is just a long wall, etc. She also notes the possibility that the stones that were dated might have been recycled from earlier constructions. She also notes that earlier research from the 1930s, confirmed in the 1980s by Fracchia was ignored. She argues that they undertook their research using a novel and previously untested methodology in order to confirm a predetermined theory about the age of these structures.
Liritzis responded in a journal article published in 2011, stating that Lefkowitz failed to understand and misinterpreted the methodology.
Spain.
The Pyramids of Güímar refer to six rectangular pyramid-shaped, terraced structures, built from lava stone without the use of mortar. They are located in the district of Chacona, part of the town of Güímar on the island of Tenerife in the Canary Islands. The structures have been dated to the 19th century and their original function explained as a byproduct of contemporary agricultural techniques.
Local traditions as well as surviving images indicate that similar structures (also known as, "Morras", "Majanos", "Molleros", or "Paredones") could once have been found in many locations on the island. However, over time they have been dismantled and used as a cheap building material. In Güímar itself there were nine pyramids, only six of which survive.
China.
There are many square flat-topped mound tombs in China. The First Emperor Qin Shi Huang (circa 221 BC, who unified the 7 pre-Imperial Kingdoms) was buried under a large mound outside modern day Xi'an. In the following centuries about a dozen more Han Dynasty royals were also buried under flat-topped pyramidal earthworks.
Mesoamerica.
A number of Mesoamerican cultures also built pyramid-shaped structures. Mesoamerican pyramids were usually stepped, with temples on top, more similar to the Mesopotamian ziggurat than the Egyptian pyramid.
The largest pyramid by volume is the Great Pyramid of Cholula, in the Mexican state of Puebla. Constructed from the 3rd century BC to the 9th century AD, this pyramid is considered the largest monument ever constructed anywhere in the world, and is still being excavated. The third largest pyramid in the world, the Pyramid of the Sun, at Teotihuacan is also located in Mexico. There is an unusual pyramid with a circular plan at the site of Cuicuilco, now inside Mexico City and mostly covered with lava from an eruption of the Xitle Volcano in the 1st century BC. There are several circular stepped pyramids called Guachimontones in Teuchitlán, Jalisco as well.
Pyramids in Mexico were often used as places of human sacrifice. For the re-consecration of Great Pyramid of Tenochtitlan in 1487, the Aztecs reported that they sacrificed about 80,400 people over the course of four days.
North America.
Many pre-Columbian Native American societies of ancient North America built large pyramidal earth structures known as platform mounds. Among the largest and best-known of these structures is Monks Mound at the site of Cahokia in what became Illinois, completed around 1100 AD, which has a base larger than that of the Great Pyramid at Giza. Many of the mounds underwent multiple episodes of mound construction at periodic intervals, some becoming quite large. They are believed to have played a central role in the mound-building peoples' religious life and documented uses include semi-public chief's house platforms, public temple platforms, mortuary platforms, charnel house platforms, earth lodge/town house platforms, residence platforms, square ground and rotunda platforms, and dance platforms. Cultures who built substructure mounds include the Troyville culture, Coles Creek culture, Plaquemine culture and Mississippian cultures.
Roman Empire.
The 27-meter-high Pyramid of Cestius was built by the end of the 1st century BC and still exists today, close to the Porta San Paolo. Another one, named "Meta Romuli", standing in the "Ager Vaticanus" (today's Borgo), was destroyed at the end of the 15th century.
Medieval Europe.
Pyramids have occasionally been used in Christian architecture of the feudal era, e.g. as the tower of Oviedo's Gothic Cathedral of San Salvador.
India.
Many giant granite temple pyramids were made in South India during the Chola Empire, many of which are still in religious use today. Examples of such pyramid temples include Brihadisvara Temple at Thanjavur, the Temple of Gangaikondacholapuram and the Airavatesvara Temple at Darasuram. However the largest temple pyramid in the area is Sri Rangam in Srirangam, Tamil Nadu. The Thanjavur temple was built by Raja raja Chola in the 11th century. The Brihadisvara Temple was declared by UNESCO as a World Heritage Site in 1987; the Temple of Gangaikondacholapuram and the Airavatesvara Temple at Darasuram were added as extensions to the site in 2004.
Indonesia.
Next to menhir, stone table, and stone statue; Austronesian megalithic culture in Indonesia also featured earth and stone step pyramid structures called "Punden Berundak" as discovered in Pangguyangan, Cisolok and Gunung Padang, West Java. The construction of stone pyramids is based on the native beliefs that mountains and high places are the abode for the spirit of the ancestors.
The step pyramid is the basic design of 8th century Borobudur Buddhist monument in Central Java. However the later temples built in Java were influenced by Indian Hindu architecture, as displayed by the towering spires of Prambanan temple. In the 15th century Java during late Majapahit period saw the revival of Austronesian indigenous elements as displayed by Sukuh temple that somewhat resemble Mesoamerican pyramid.
Peru.
Andean cultures had used pyramids in various architectural structures such as the ones in Caral, Túcume and Chavín de Huantar.

</doc>
<doc id="23705" url="https://en.wikipedia.org/wiki?curid=23705" title="Predestination">
Predestination

Predestination, in theology, is the doctrine that all events have been willed by God, usually with reference to the eventual fate of the individual soul. Explanations of predestination often seek to address the "paradox of free will", whereby God's omniscience seems incompatible with human free will. In this usage, predestination can be regarded as a form of religious determinism; and usually predeterminism.
Christianity.
History.
New Testament period.
There is some disagreement among scholars regarding the views on predestination of first-century AD Judaism, out of which Christianity came. Josephus wrote during the first century that the three main Jewish sects differed on this question. He argued that the Essenes and Pharisees argued that God's providence orders all human events, but the Pharisees still maintained that people are able to choose between right and wrong. He wrote that the Sadducees did not have a doctrine of providence. 
Biblical scholar N. T. Wright argues that Josephus's portrayal of these groups is incorrect, and that the Jewish debates referenced by Josephus should be seen as having to do with God's work to liberate Israel rather than philosophical questions about predestination. Wright asserts that Essenes were content to wait for God to liberate Israel while Pharisees believed Jews needed to act in cooperation with God. John Barclay has response was that Josephus's description was an over-simplification and there were likely to be complex differences between these groups which may have been similar to those described by Josephus. Francis Watson has also argued on the basis of 4 Ezra, a document dated to the first century AD, that Jewish beliefs in predestination are primarily concerned with God's choice to save some individual Jews.
In the New Testament, Romans 8–11 presents a particularly clear statement on predestination. In Romans 8:28–30, Paul writes, 
Biblical scholars have interpreted this passage in several ways. Catholic biblical commentator Brendan Byrne wrote that the predestination mentioned in this passage should be interpreted as applied to the Christian community corporately rather than individuals. Another Catholic commentator, Joseph Fitzmyer, wrote that this passage teaches that God has predestined the salvation of all humans. Douglas Moo, a Protestant biblical interpreter, reads the passage as teaching that God has predestined a certain set of people to salvation. Similarly, N. T. Wright has written that his interpretation of the scripture is that in this passage Paul teaches that God will save those whom he has chosen, but Wright also emphasizes that Paul does not intend to suggest that god has eliminated human free will or responsibility. Instead, Wright asserts that god's will works through that of humans to accomplish salvation.
Patristic period.
Origen, writing in the third century, taught that God's providence extends to every individual. He believed God's predestination was based on God's foreknowledge of every individual's merits, whether in their current life or a previous life.
Later in the third and fourth centuries, Augustine of Hippo also taught that God orders all things while preserving human freedom. Prior to 396, Augustine believed that predestination was based on God's foreknowledge of whether individuals would believe. In response to Pelagius, Augustine later argued that God's choice is not based on individual characteristics, but on God's will alone. Scholars are divided over whether Augustine's teaching implies double predestination, or the belief that God chooses some people for damnation as well as some for salvation. Catholic scholars tend to deny that he held such a view while some Protestants and secular scholars have held that Augustine did believe in double predestination.
Augustine's position raised objections. Julian, bishop of Eclanum, expressed the view that Augustine was bringing Manichean thoughts into the church. For Vincent of Lérins, this was a disturbing innovation. This new tension eventually became obvious with the confrontation between Augustine and Pelagius culminating in condemnation of Pelagianism (as interpreted by Augustine) at the Council of Ephesus in 431. Pelagius denied Augustine's view of predestination in order to affirm that salvation is achieved by an act of free will.
The Council of Arles in the late fifth century condemned the position "that some have been condemned to death, others have been predestined to life", though this may seem to follow from Augustine's teaching. The Second Council of Orange in 529 also condemned the position that "some have been truly predestined to evil by divine power".
In the eighth century, John of Damascus emphasized the freedom of the human will in his doctrine of predestination, and argued that acts arising from peoples' wills are not part of God's providence at all. Damascene teaches that people's good actions are done in cooperation with God, but are not caused by him.
Middle Ages.
Gottschalk of Orbais, a ninth-century Saxon monk, argued that God predestines some people to hell as well as predestining some to heaven, a view known as double predestination. He was condemned by several synods, but his views remained popular. Irish theologian John Scottus Eriugena wrote a refutation of Gottschalk. Eriugena abandoned Augustine's teaching on predestination. He wrote that God's predestination should be equated with his foreknowledge of people's choices.
In the twelfth century, Thomas Aquinas taught that God predestines certain people to the beatific vision based solely on his own goodness rather than that of creatures. Aquinas also believed people are free in their choices, fully cause their own sin, and are solely responsible for it. Aquinas distinguished between several ways in which God wills actions. He directly wills the good, indirectly wills evil consequences of good things, and only permits evil. Aquinas held that in permitting evil, God does not will it to be done or not to be done.
In the thirteenth century, William of Ockham taught that God's does not cause human choices and equated predestination with divine foreknowledge. Though Ockham taught that God predestines based on people's foreseen works, he maintained that God's will was not constrained to do this.
Reformation.
John Calvin rejected the idea that God permits rather than actively decrees the damnation of sinners, as well as other evil. Calvin did not believe God to be guilty of sin, but he considered it an unfathomable mystery that God seems to simultaneously will sin and to also not will sin. Though he maintained God's predestination applies to damnation as well as salvation, he taught that the damnation of the damned is caused by their sin, but that the salvation of the saved is solely caused by God. Other Protestant Reformers, including Martin Luther and Huldrych Zwingli, also held double predestinarian views.
Views of Christian branches.
Eastern Orthodoxy.
The Eastern Orthodox view was summarized by Bishop Theophan the Recluse in response to the question, "What is the relationship between the Divine provision and our free will?"
Roman Catholicism.
Roman Catholicism teaches the doctrine of predestination, while rejecting the classical Calvinist view known as "double predestination." This means that while it is held that those whom God has elected to eternal life will infallibly attain it, and are therefore said to be predestined to salvation by God, those who perish are not predestined to damnation. But Catholicism has been generally discouraging to human attempts to guess or predict the Divine Will. The Catholic Encyclopedia entry on Predestination says:
Pope John Paul II wrote:
The Catholic Catechism says:
Catholics do not believe that any hints or evidence of the predestined status of individuals is available to humans, and predestination generally plays little or no part in Catholic teaching to the faithful, being a topic addressed in a professional theological context only.
St. Augustine of Hippo laid the foundation for much of the later Catholic teaching on predestination. His teachings on grace and free will were largely adopted by the Second Council of Orange (529), whose decrees were directed against the Semipelagians. Augustine wrote, 
Augustine also teaches that people have free will. For example, in "On Grace and Free Will," (see especially chapters II-IV) St. Augustine states that "He has revealed to us, through His Holy Scriptures, that there is in man a free choice of will," and that "God's precepts themselves would be of no use to a man unless he had free choice of will, so that by performing them he might obtain the promised rewards." (chap. II)
Thomas Aquinas' views concerning predestination are largely in agreement with Augustine and can be summarized by many of his writings in his Summa Theologiae:
Protestantism.
Comparison between Protestants.
This table summarizes the classical views of three different Protestant beliefs.
Lutheranism.
Lutherans do not believe that there are certain people that are predestined to salvation, but salvation is predestined for those who seek God. Lutherans believe Christians should be assured that they are among the predestined. However, they disagree with those who make predestination the source of salvation rather than Christ's suffering, death, and resurrection. Unlike some Calvinists, Lutherans do not believe in a predestination to damnation. Instead, Lutherans teach eternal damnation is a result of the unbeliever's sins, rejection of the forgiveness of sins, and unbelief.
Martin Luther's attitude towards predestination is set out in his "On the Bondage of the Will", published in 1525. This publication by Luther was in response to the published treatise by Desiderius Erasmus in 1524 known as "On Free Will". Luther based his views on Ephesians 2:8-10, which says: "For by grace you have been saved through faith, and that not of yourselves; it is the gift of God, not of works, lest anyone should boast. For we are His workmanship, created in Christ Jesus for good works, which God prepared beforehand that we should walk in them."
Calvinism.
The Belgic Confession of 1561 affirmed that God "delivers and preserves" from perdition "all whom he, in his eternal and unchangeable council, of mere goodness hath elected in Christ Jesus our Lord, without respect to their works" (Article XVI).
Calvinists believe that God picked those who he will save and bring with him to Heaven before the world was created. They also believe that those people God does not save will go to Hell. John Calvin thought people who were saved could never lose their salvation and the "elect" (those God saved) would know they were saved because of their actions.
In this common, loose sense of the term, to affirm or to deny predestination has particular reference to the Calvinist doctrine of unconditional election. In the Calvinist interpretation of the Bible, this doctrine normally has only pastoral value related to the assurance of salvation and the absolution of salvation by grace alone. However, the philosophical implications of the doctrine of election and predestination are sometimes discussed beyond these systematic bounds. Under the topic of the doctrine of God (theology proper), the predestinating decision of God cannot be contingent upon anything outside of himself, because all other things are dependent upon him for existence and meaning. Under the topic of the doctrines of salvation (soteriology), the predestinating decision of God is made from God's knowledge of his own will (Romans 9:15), and is therefore not contingent upon human decisions (rather, free human decisions are outworkings of the decision of God, which sets the total reality within which those decisions are made in exhaustive detail: that is, nothing left to chance). Calvinists do not pretend to understand how this works; but they are insistent that the Scriptures teach both the sovereign control of God and the responsibility and freedom of human decisions.
Calvinist groups use the term Hyper-Calvinism to describe Calvinistic systems that assert without qualification that God's intention to destroy some is equal to his intention to save others. Some forms of Hyper-Calvinism have racial implications, against which other Calvinists vigorously object (see Afrikaner Calvinism). The Dutch settlers of South Africa claimed that the Blacks were members of the non-elect, because they were the sons of Ham, whom Noah had cursed to be slaves, according to Genesis 9:18-19. The Dutch Calvinist theologian Franciscus Gomarus also argued that Jews, because of their refusal to worship Jesus Christ, were members of the non-elect.
Expressed sympathetically, the Calvinist doctrine is that God has mercy or withholds it, with particular consciousness of who are to be the recipients of mercy in Christ. Therefore, the particular persons are chosen, out of the total number of human beings, who will be rescued from enslavement to sin and the fear of death, and from punishment due to sin, to dwell forever in his presence. Those who are being saved are assured through the gifts of faith, the sacraments, and communion with God through prayer and increase of good works, that their reconciliation with him through Christ is settled by the sovereign determination of God's will. God also has particular consciousness of those who are passed over by his selection, who are without excuse for their rebellion against him, and will be judged for their sins.
Calvinists typically divide on the issue of predestination into infralapsarians (sometimes called 'sublapsarians') and supralapsarians. Infralapsarians interpret the biblical election of God to highlight his love (1 John 4:8; Ephesians 1:4b-5a) and chose his elect considering the situation after the Fall, while supralapsarians interpret biblical election to highlight God's sovereignty (Romans 9:16) and that the Fall was ordained by God's decree of election. In infralapsarianism, election is God's response to the Fall, while in supralapsarianism the Fall is part of God's plan for election. In spite of the division, many Calvinist theologians would consider the debate surrounding the infra- and supralapsarian positions one in which scant Scriptural evidence can be mustered in either direction, and that, at any rate, has little effect on the overall doctrine.
Some Calvinists decline from describing the eternal decree of God in terms of a sequence of events or thoughts, and many caution against the simplifications involved in describing any action of God in speculative terms. Most make distinctions between the positive manner in which God chooses some to be recipients of grace, and the manner in which grace is consciously withheld so that some are destined for everlasting punishments.
Debate concerning predestination according to the common usage, concerns the destiny of the damned, whether God is just if that destiny is settled prior to the existence of any actual volition of the individual, and whether the individual is in any meaningful sense responsible for his destiny if it is settled by the eternal action of God.
Arminianism.
Arminians hold that God does not predetermine, but instead infallibly knows who will believe and perseveringly be saved. This view is known as conditional election, because it states that election is conditional on the one who wills to have faith in God for salvation. Although God knows from the beginning of the world who will go where, the choice is still with the individual. The Dutch Calvinist theologian Franciscus Gomarus strongly opposed the views of Jacobus Arminius with his doctrine of supralapsarian predestination.
The Church of Jesus Christ of Latter-day Saints.
The LDS church rejects the doctrine of predestination, but does believe in foreordination. Foreordination, an important doctrine of The Church of Jesus Christ of Latter-day Saints (LDS Church), teaches that during the pre-mortal existence, God selected ("foreordained") particular people to fulfill certain missions ("callings") during their mortal lives. For example, prophets were foreordained to be the Lord's servants, all who receive the priesthood were foreordained to that calling, and Jesus was foreordained to enact the atonement.
The LDS church teaches the doctrine of agency, the ability to choose and act for ourselves, and decide whether to accept Christ's atonement.
Types of predestination.
Conditional election.
Conditional election is the belief that God chooses for eternal salvation those whom he foresees will have faith in Christ. This belief emphasizes the importance of a person's free will. The counter-view is known as unconditional election, and is the belief that God chooses whomever he will, based solely on his purposes and apart from an individual's free will. It has long been an issue in Calvinist–Arminian debate.
Supralapsarianism and infralapsarianism.
Infralapsarianism (also called sublapsarianism) holds that predestination logically coincides with the preordination of Man's fall into sin. That is, God predestined sinful men for salvation. Therefore, according to this view, God is the ultimate cause, but not the proximate source or "author" of sin. Infralapsarians often emphasize a difference between God's decree (which is inviolable and inscrutable), and his revealed will (against which man is disobedient). Proponents also typically emphasize the grace and mercy of God toward all men, although teaching also that only some are predestined for salvation.
In common English parlance, the doctrine of predestination often has particular reference to the doctrines of Calvinism. The version of predestination espoused by John Calvin, after whom Calvinism is named, is sometimes referred to as "double predestination" because in it God predestines some people for salvation (i.e. unconditional election) and some for condemnation (i.e. Reprobation) which results by allowing the individual's own sins to condemn them. Calvin himself defines predestination as "the eternal decree of God, by which he determined with himself whatever he wished to happen with regard to every man. Not all are created on equal terms, but some are preordained to eternal life, others to eternal damnation; and, accordingly, as each has been created for one or other of these ends, we say that he has been predestined to life or to death."
On the spectrum of beliefs concerning predestination, Calvinism is the strongest form among Christians. It teaches that God's predestining decision is based on the knowledge of his own will rather than foreknowledge, concerning every particular person and event; and, God continually acts with entire freedom, in order to bring about his will in completeness, but in such a way that the freedom of the creature is not violated, "but rather, established".
Calvinists who hold the infralapsarian view of predestination usually prefer that term to "sublapsarianism," perhaps with the intent of blocking the inference that they believe predestination is on the basis of foreknowledge ("sublapsarian" meaning, assuming the fall into sin). The different terminology has the benefit of distinguishing the Calvinist double predestination version of infralapsarianism from Lutheranism's view that predestination is a mystery, which forbids the unprofitable intrusion of prying minds since God only reveals partial knowledge to the human race.
Supralapsarianism is the doctrine that God's decree of predestination for salvation and reprobation logically precedes his preordination of the human race's fall into sin. That is, God decided to save, and to damn; he then determined the means by which that would be made possible. It is a matter of controversy whether or not Calvin himself held this view, but most scholars link him with the infralapsarian position. It is known, however, that Calvin's successor in Geneva, Theodore Beza, held to the supralapsarian view.
Double predestination.
Double predestination (), or the double decree, is the doctrine that God actively reprobates, or decrees damnation of some, as well as salvation for those whom he has elected. Augustine made statements that on their own seem to teach such a doctrine, but in the context of his other writings it is not clear whether he held it. Augustine's doctrine of predestination does seem to imply a double predestinarian view. Gottschalk of Orbais taught it more explicitly in the ninth century, and Gregory of Rimini in the fourteenth. During the Protestant Reformation, John Calvin, Martin Luther, and Huldrych Zwingli also held double predestinarian views.
Open theism.
Advocates of open theism, like most who affirm conditional predestination, understand predestination to be as corporate. In corporate election, God does not choose which individuals he will save prior to creation, but rather God chooses the church as a whole. Or put differently, God chooses what type of individuals he will save. Another way the New Testament puts this is to say that God chose the church in Christ (Eph. 1:4). In other words, God chose from all eternity to save all those who would be found in Christ, by faith in God. This choosing is not primarily about salvation from eternal destruction either but is about God's chosen agency in the world. Thus individuals have full freedom in terms of whether they become members of the church or not. Corporate election is thus consistent with the open view's position on God's omniscience, which states that the outcomes of individual free will cannot be known specifically before they are performed since who becomes a Christian is a matter of free will and not knowable.
Islam.
"Qadar" (, transliterated "qadar", meaning "fate", "divine fore-ordainment", "predestination") is the concept of divine destiny in Islam. It is one of Islam's six pillars of faith, along with belief in the Oneness of Allah, the Revealed Books, the Prophets of Islam, the Day of Resurrection and Angels.
In Islam, "predestination" is the usual English language rendering of a belief that Muslims call "al-qada wa al-qadar" in Arabic. The phrase means "the divine decree and the predestination". In Islam, Allah has predetermined, known, ordained, and is constantly creating every event that takes place in the world. This is entailed by his being omnipotent and omniscient. Sunni scholars hold that there is no contradiction in people's deeds (and naturally their choices) being created and predetermined by the creator, since they define free will to be the antonym of compulsion and coercion. People – in the Sunni perspective – do acknowledge that they are free, since they do not see anybody or anything forcing them to do whatever they chose to do. This, however, does not contradict that everything they do, including the choices they make, are predestined and predetermined by Allah. Consequently, people are already predestined to either heaven or hell at birth, as Sunnis believe; however, they will have no argument on the day of judgment since they never knew in advance what their fate would be, and they do acknowledge that they have choice; which is what moral responsibility comes with.
The concept of human will being predetermined by Allah's will is stated clearly in the Quran:
"Verily this (The Holy Quran) is no less than a Message to (all) the Worlds; (With profit) to whoever among you wills to go straight, "but ye shall not will except as God wills;" the Cherisher of the Worlds."
Judaism.
In Rabbinic literature, there is much discussion as to the apparent contradiction between God's omniscience and free will. The representative view is that "Everything is foreseen; yet free will is given" (Rabbi Akiva, "Pirkei Avoth" 3:15). Based on this understanding, the problem is formally described as a paradox, perhaps beyond our understanding.
Hasdai Crescas resolved this dialectical tension by taking the position that free will doesn't exist. All of a person's actions are predetermined by the moment of their birth, and thus their judgment in the eyes of God (so to speak) is effectively preordained. In this scheme this is not a result of God's predetermining one's fate, but rather that the universe is deterministic. Crescas's views on this topic were rejected by Judaism at large. In later centuries this idea independently developed among some in the Chabad (Lubavitch) movement of Hasidic Judaism. Many individuals within Chabad take this view seriously, and hence effectively deny the existence of free will.
However, many Chabad (Lubavitch) Jews attempt to hold both views. They affirm as infallible their rebbe's teachings that God knows and controls the fate of all, yet at the same time affirm the classical Jewish belief in free will. The inherent contradiction between the two results in their belief that such contradictions are only "apparent", due to man's inherent lack of ability to understand greater truths and due to the fact that Creator and Created exist in different realities. The same idea is strongly repeated by Rambam (Mishneh Torah, Laws of Repentance, Chapter 5).
Many other Jews (Orthodox, Conservative, Reform and secular) affirm that since free will exists, then by definition one's fate is not preordained. It is held as a tenet of faith that whether God is omniscient or not, nothing interferes with mankind's free will. Some Jewish theologians, both during the medieval era and today, have attempted to formulate a philosophy in which free will is preserved, while also affirming that God has knowledge of what decisions people will make in the future. Whether or not these two ideas are mutually compatible, or whether there is a contradiction between the two, is still a matter of great study and interest in philosophy today.
Zoroastrianism.
Predestination is rejected in Zoroastrian teaching. Humans bear responsibility for all situations they are in, and in the way they act toward one another. Reward, punishment, happiness, and grief all depend on how individuals live their lives.

</doc>
<doc id="23706" url="https://en.wikipedia.org/wiki?curid=23706" title="Primitive notion">
Primitive notion

In mathematics, logic, and formal systems, a primitive notion is an undefined concept. In particular, a primitive notion is not defined in terms of previously defined concepts, but is only motivated informally, usually by an appeal to intuition and everyday experience. In an axiomatic theory or other formal system, the role of a primitive notion is analogous to that of axiom. In axiomatic theories, the primitive notions are sometimes said to be "defined" by one or more axioms, but this can be misleading. Formal theories cannot dispense with primitive notions, under pain of infinite regress.
Alfred Tarski explained the role of primitive notions as follows:
An inevitable regress to primitive notions in the theory of knowledge was explained by Gilbert de B. Robinson:
Examples.
The necessity for primitive notions is illustrated in several axiomatic foundations in mathematics:

</doc>
<doc id="23707" url="https://en.wikipedia.org/wiki?curid=23707" title="Priest">
Priest

A priest or priestess (feminine) (from Greek πρεσβύτερος "presbýteros" through Latin "presbyter", "elder", or from Old High German "priast", "prest", from Vulgar Latin "prevost" "one put over others", from Latin "praepositus" "person placed in charge"), is a person authorized to perform the sacred rituals of a religion, especially as a mediatory agent between humans and one or more deities. They also have the authority or power to administer religious rites; in particular, rites of sacrifice to, and propitiation of, a deity or deities. Their office or position is the priesthood, a term which also may apply to such persons collectively.
Priests and priestesses have existed since the earliest of times (see Proto-indo-European trifunctional hypothesis) and in the simplest societies, most likely as a result of agricultural surplus and consequent social stratification. The necessity to read sacred texts and keep temple or church records helped foster literacy in many early societies. Priests exist in many religions today, such as all or some branches of Judaism, Christianity, Shintoism, Hinduism. They are generally regarded as having positive contact with the deity or deities of the religion to which they subscribe, often interpreting the meaning of events and performing the rituals of the religion. There is no common definition of the duties of priesthood between faiths; but generally it includes mediating the relationship between one's congregation, worshippers, and other members of the religious body, and its deity or deities, and administering religious rituals and rites. These often include blessing worshipers with prayers of joy at marriages, after a birth, and at consecrations, teaching the wisdom and dogma of the faith at any regular worship service, and mediating and easing the experience of grief and death at funerals - maintaining a spiritual connection to the afterlife in faiths where such a concept exists. Administering religious building grounds and office affairs and papers, including any religious library or collection of sacred texts, is also commonly a responsibility - for example, the modern term for clerical duties in a secular office refers originally to the duties of a cleric. The question of which religions have a "priest" depends on how the titles of leaders are used or translated into English. In some cases, leaders are more like those that other believers will often turn to for advice on spiritual matters, and less of a "person authorized to perform the sacred rituals." For example, clergy in Roman Catholicism and Eastern Orthodoxy are "priests", but in Protestant Christianity they are typically "minister" and "pastor". The terms "priest" and "priestess" are sufficiently generic that they may be used in an anthropological sense to describe the religious mediators of an unknown or otherwise unspecified religion.
In many religions, being a priest or priestess is a full-time position, ruling out any other career. Many Christian priests and pastors choose or are mandated to dedicate themselves to their churches and receive their living directly from their churches. In other cases it is a part-time role. For example, in the early history of Iceland the chieftains were titled "goði", a word meaning "priest". As seen in the saga of Hrafnkell Freysgoði, however, being a priest consisted merely of offering periodic sacrifices to the Norse gods and goddesses; it was not a full-time role, nor did it involve ordination.
In some religions, being a priest or priestess is by human election or human choice. In Judaism the priesthood is inherited in familial lines. In a theocracy a society is governed by its priesthood.
Etymology.
The word "priest", is ultimately derived from Greek, via Latin "presbyter", the term for "elder", especially elders of Jewish or Christian communities in Late Antiquity. It is possible that the Latin word was loaned into Old English, and only from Old English reached other Germanic languages via the Anglo-Saxon mission to the continent, giving Old Icelandic "prestr", Old Swedish "präster", Old High German "priast". Old High German also has the disyllabic "priester, priestar", apparently derived from Latin independently via Old French "presbtre". The Latin "presbyter" ultimately represents Greek "presbyteros", the regular Latin word for "priest" being "sacerdos", corresponding to Greek "hiereus".
That English should have only the single term "priest" to translate "presbyter" and "sacerdos" came to be seen as a problem in English Bible translations. The "presbyter" is the minister who both presides and instructs a Christian congregation, while the "sacerdos", offerer of sacrifices, or in a Christian context the eucharist, performs "mediatorial offices between God and man".
The feminine English noun, "priestess", was coined in the 17th century, to refer to female priests of the pre-Christian religions of classical antiquity. In the 20th century, the word was used in controversies surrounding the ordination of women. In the case of the ordination of women in the Anglican communion, it is more common to speak of "priests", regardless of gender.
Historical religions.
In historical polytheism, a priest administers the sacrifice to a deity, often in highly elaborate ritual. In the Ancient Near East, the priesthood also acted on behalf of the deities in managing their property.
Priestesses in antiquity often performed sacred prostitution, and in Ancient Greece, some priestesses such as Pythia, priestess at Delphi, acted as oracles.
Ancient Egypt.
In Egyptian ideology, the right and obligation to interact with the gods belonged to the pharaoh. He delegated this duty to priests, who were effectively bureaucrats authorized to act on his behalf. Priests staffed temples throughout Egypt, giving offerings to the cult statues in which the gods were believed to take up residence and performing other rituals for their benefit. Little is known about what training may have been required of priests, and the selection of personnel for positions was affected by a tangled set of traditions, although the pharaoh had the final say. In the New Kingdom, when temples owned great estates, the high priests of the most important cult—that of Amun at Karnak—were important political figures.
High-ranking priestly roles were usually held by men. Women were generally relegated to lower positions in the temple hierarchy, although some held specialized and influential positions, especially that of the God's Wife of Amun, whose religious importance overshadowed the High Priests of Amun in the Late Period.
Ancient Rome.
In Ancient Rome and throughout Italy, the ancient sanctuaries of Ceres and Proserpina were invariably led by female "sacerdotes", drawn from women of local and Roman elites. It was the only public priesthood attainable by Roman matrons and was held in great honor.
Abrahamic religions.
Judaism.
In ancient Israel the priests were required by the Law of Moses to be of direct paternal descendency from Aaron, Moses' elder brother. In Exodus 30:22–25 God instructs Moses to make a holy anointing oil to consecrate the priests "for all of eternity." During the times of the two Jewish Temples in Jerusalem, the Aaronic priests were responsible for the daily and special Jewish holiday offerings and sacrifices within the temples, these offerings are known as the "korbanot".
In Hebrew the word "priest" is "kohen" (singular כהן "kohen", plural כּהנִים "kohanim"), hence the family names "Cohen", "Cahn", "Kahn", "Kohn", "Kogan", etc. These families are from the tribe of Levi (Levites) and in twenty-four instances are called by scripture as such (Jerusalem Talmud to Mishnaic tractate Maaser Sheini p. 31a). In Hebrew the word for "priesthood" is kehunnah.
Since the destruction of the Second Temple, and (therefore) the cessation of the daily and seasonal temple ceremonies and sacrifices, Kohanim in traditional Judaism (Orthodox Judaism and to some extent, Conservative Judaism) continue to perform a number of priestly ceremonies and roles such as the Pidyon HaBen (redemption of a first-born son) ceremony and the Priestly Blessing, and have remained subject, particularly in Orthodox Judaism, to a number of restrictions, such as restrictions on certain marriages and ritual purity (see Kohanic disqualifications).
Orthodox Judaism regard the "kohanim" as being held in reserve for a future restored Temple. In all branches of Judaism, Kohanim do not perform roles of propitiation, sacrifice, or sacrament. Rather, a "kohen"'s principal religious function is to perform the Priestly Blessing, and, provided he is rabbinically qualified, to serve as an authoritative judge ("posek") and expositor of Jewish halakha law.
Christianity.
With the spread of Christianity and the formation of parishes, the Greek word "ἱερεύς" (hiereus), and Latin "sacerdos", which Christians had since the 3rd century applied to bishops and only in a secondary sense to presbyters, began in the 6th century to be used of presbyters, and is today commonly used of presbyters, distinguishing them from bishops.
Today the term "priest" is used in Roman Catholicism, Eastern Orthodoxy, Anglicanism, Oriental Orthodoxy, the Church of the East, and some branches of Lutheranism to refer to those who have been ordained to a ministerial position through receiving the sacrament of Holy Orders, although "presbyter" is also used. Since the Protestant Reformation, non-sacramental denominations are more likely to use the term "elder" to refer to their pastors.
However, nowhere in the New Testament is a Christian pastor (besides Christ) titled "hiereus," the distinctive Greek word for "priest," and thus its rendering into English is seen as an etymological corruption of the Greek word "presbuteros," which means "elder," and which is the word for the lead category of Christian leaders in the New Testament church, under the Lord Jesus Christ, the great High Priest (archiereus). In the New Testament, it is taught that as Christ made the perfect sacrifice for the forgiveness of sins, then believers have direct access to the Father through Him, () with the only priesthood that is named under Christ in the church being that which consists of all believers.
The New Testament Epistle to the Hebrews in particular draws a distinction between the Jewish priesthood and the high priesthood of Christ; it teaches that the sacrificial atonement by Jesus Christ on Calvary has made the Jewish priesthood and its prescribed ritual sacrifices redundant, along with the rest of the ceremonial acts of the Mosaic law, see Christian views on the Old Covenant for details. Thus, for Christians, Christ himself is the only high priest, and Christians have no priesthood independent or distinct from participation in the priesthood of Christ, the head of the Church. The one sacrifice of Christ, which he offered "once for all" () on the Cross, provides eternal sanctification and redemption. Roman Catholics, Eastern Orthodox, High Church Anglicans, Lutherans, and some Methodists consider the sacrifice to be "re-presented" in the Eucharist. The Church of Jesus Christ of Latter-day Saints (LDS Church) claims to uphold all priesthood positions of the primitive gospel by the laying on of hands.
The most known form of distinctive clothing for the priest is the easily identifiable clerical collar (or Roman collar), which takes form in either the traditional cassock, or modern day clerical shirt. The typical modern version consists of a white plastic tab, inserted into a specially made collar of a black shirt, although traditional cloth collars are still worn.
Roman Catholicism and Eastern Orthodoxy.
The most significant liturgical acts reserved to priests in these traditions are the administration of the Sacraments, including the celebration of the Holy Mass or Divine Liturgy (the terms for the celebration of the Eucharist in the Latin and Byzantine traditions, respectively), and the Sacrament of Reconciliation, also called Confession. The sacraments of Anointing of the Sick (Extreme Unction) and Confirmation or Chrismation are also administered by priests, though in the Western tradition Confirmation is ordinarily celebrated by a bishop. In the East, Chrismation is performed by the priest (using oil specially consecrated by a bishop) immediately after Baptism, and Unction is normally performed by several priests (ideally seven), but may be performed by one if necessary. In the West, Holy Baptism may be celebrated by anyone. The Vatican catechism states that "According to Latin tradition, the spouses as ministers of Christ's grace mutually confer upon each other the sacrament of Matrimony". Thus marriage is a sacrament administered by the couple to themselves, but may be witnessed and blessed by a deacon, or priest (who usually administers the ceremony). In the East, Holy Baptism and Marriage (which is called "Crowning") may be performed only by a priest. If a person is baptized "in extremis" (i.e., when in fear of immediate death), only the actual threefold immersion together with the scriptural words () may be performed by a layperson or deacon. The remainder of the rite, and Chrismation, must still be performed by a priest, if the person survives. The only sacrament which may be celebrated only by a bishop is that of Ordination ("cheirotonia", "Laying-on of Hands"), or Holy Orders.
In these traditions, only men who meet certain requirements may become priests. In Roman Catholicism the canonical minimum age is twenty-five. Bishops may dispense with this rule and ordain men up to one year younger. Dispensations of more than a year are reserved to the Holy See (Can. 1031 §§1, 4.) A Catholic priest must be incardinated by his bishop or his major religious superior in order to engage in public ministry. In Orthodoxy, the normal minimum age is thirty (Can. 9 of Neocaesarea) but a bishop may dispense with this if needed. In neither tradition may priests marry after ordination. In the Roman Catholic Church, priests in the Latin Rite, which covers the vast majority of Roman Catholicism, must be celibate except under special rules for married clergy converting from certain other Christian confessions. Married men may become priests in Eastern Orthodoxy and the Eastern Catholic Churches, but in neither case may they marry after ordination, even if they become widowed. Candidates for bishop are chosen only from among the celibate. Orthodox priests will either wear a clerical collar similar to the above mentioned, or simply a very loose black robe that does not have a collar.
Anglican or Episcopalian.
The role of a priest in the Anglican Communion is largely the same as within the Roman Catholic Church and Eastern Christianity, except that canon law in almost every Anglican province restricts the administration of confirmation to the bishop, just as with ordination. Whilst Anglican priests who are members of religious orders must remain celibate (although there are exceptions, such as priests in the Anglican Order of Cistercians), the secular clergy—bishops, priests, and deacons who are not members of religious orders—are permitted to marry before or after ordination. The Anglican churches, unlike the Roman Catholic or Eastern Christian traditions, have allowed the ordination of women as priests in some provinces since 1971. This practice remains controversial, however; a minority of provinces (10 out of the 38 worldwide) retain an all-male priesthood. Most Continuing Anglican churches do not ordain women to the priesthood.
As Anglicanism represents a broad range of theological opinion, its presbyterate includes priests who consider themselves no different in any respect from those of the Roman Catholic Church, and a minority who prefer to use the title "presbyter" in order to distance themselves from the more sacrificial theological implications which they associate with the word "priest". While "priest" is the official title of a member of the presbyterate in every Anglican province worldwide, the ordination rite of certain provinces (including the Church of England) recognizes the breadth of opinion by adopting the title "The Ordination of Priests (also called Presbyters)". Historically, the term "priest" has been more associated with the “High Church” or Anglo-Catholic wing, whereas the term “minister” has been more commonly used in “Low Church” or Evangelical circles.
Protestantism.
The general priesthood or the priesthood of all believers, is a Christian doctrine derived from several passages of the New Testament. It is a foundational concept of Protestantism. It is this doctrine that Martin Luther adduces in his 1520 "To the Christian Nobility of the German Nation" in order to dismiss the medieval Christian belief that Christians were to be divided into two classes: "spiritual" and "temporal" or non-spiritual.
The conservative reforms of Lutherans are reflected in the theological and practical view of the ministry of the Church. Much of European Lutheranism follows the traditional catholic governance of deacon, priest and bishop. The Lutheran archbishops of Finland, Sweden, etc. and Baltic countries are the historic national primates (See the original Catholic Church) and some ancient cathedrals and parishes in the Lutheran church were constructed many centuries before the Reformation. Indeed, ecumenical work within the Anglican communion and among Scandinavian Lutherans mutually recognize the historic apostolic legitimacy and full communion. Likewise in America, Lutherans have embraced the apostolic succession of bishops in the full communion with Episcopalians and most Lutheran ordinations are performed by a bishop. The Roman Catholic Church, however, does not recognise Episcopalians or Lutherans as having legitimate apostolic succession.
Ordained Protestant clergy often have the title of pastor, minister, reverend, etc. In some Lutheran churches, ordained clergy are called priests, while in others the term pastor is preferred.
Latter Day Saints.
In the Latter Day Saint movement, priesthood is the power and authority of God given to man, including the authority to perform ordinances and to act as a leader in the church. A body of priesthood holders is referred to as a quorum. Priesthood denotes elements of both power and authority. The priesthood includes the power Jesus gave his apostles to perform miracles such as the casting out of devils and the healing of sick (Luke 9:1). Latter Day Saints believe that the Biblical miracles performed by prophets and apostles were performed by the power of priesthood, including the miracles of Jesus, who holds all of the keys of the priesthood. The priesthood is formally known as the "Priesthood after the Order of the Son of God", but to avoid the too frequent use of the name of deity, the priesthood is referred to as the Melchizedek priesthood (Melchizedek being the high priest to whom Abraham paid tithes). As an authority, priesthood is the authority by which a bearer may perform ecclesiastical acts of service in the name of God. Latter Day Saints believe that acts (and in particular, ordinances) performed by one with priesthood authority are recognized by God and are binding in heaven, on earth, and in the afterlife. In addition, Latter Day Saints believe that leadership positions within the church are legitimized by the priesthood authority.
Islam.
Islam has no sacerdotal priesthood, as every believer "(mu'min)" is individually empowered to engage with God directly, without any mediator. There are, however, a variety of academic and administrative offices which have evolved to assist Muslims with this task; a full discussion can be found at Clergy#Islam.
Eastern religions.
Hinduism.
Hindu priests historically were members of the Brahmin caste. Priests are ordained and trained as well. There are two types of Hindu priests, "pujaris" and "purohits". A "pujari" performs rituals in a temple. These rituals include bathing the "murtis" (the statues of the gods/goddesses), performing "puja", a ritualistic offering of various items to the Gods, the waving of a "ghee" or oil lamp also called an offering in light, known in Hinduism as "aarti", before the "murtis". "Pujaris" are often married.
A "purohit", on the other hand, performs rituals and "saṃskāras" (sacraments) outside of the temple. There are special "purohits" who perform only funeral rites.
In many cases, a "purohit" also functions as a "pujari". Both women and men are ordained as "purohits" and "pujaris".
There are many priests in India who perform their work both inside and outside temples. The ones who perform it inside are called "pujaris" who are more common and are more significant in society. A few tasks of these "pujaris" would be to clean or bathe the statue of the God in the temple. They do earn from this but do not demand too much money. The other more debatable priests are the "purohits" who perform their duties outside the temple. They act as God for poor people and by talking or by providing 'hope', they earn a living.
Zoroastrianism.
In Zoroastrianism, the priesthood is reserved for men and is a mostly hereditary position. The priests prepare a drink from a sacred plant, which is called the "haoma" ritual. They officiate the "Yasna", pouring libations into the sacred fire to the accompaniment of ritual chants.
Taoism.
The Taoist priest is called a Daoshi (道士 "master of the Dao" p. 488). Daoshi act as interpreters of the principles of Yin-Yang 5 elements (fire, water, soil, wood, and metal p. 53) school of ancient Chinese philosophy, as they relate to marriage, death, festival cycles, and so on. The Daoshi seeks to share the benefits of meditation with his or her community through public ritual and liturgy (p. 326). In the ancient priesthood before the Tang, the priest was called "Jijiu" ("libationer" p. 550), with both male and female practitioners selected by merit. The system gradually changed into a male only hereditary "Daoshi" priesthood until more recent times (p. 550,551).
Indigenous and ethnic religions.
Shintoism.
The shinto priest is called a , originally pronounced "kamunushi", sometimes referred to as a . A Kannushi is the person responsible for the maintenance of a Shinto shrine, or jinja, purificatory rites, and for leading worship and veneration of a certain kami. Additionally, priests are aided by for many rites as a kind of shaman or medium. The maidens may either be family members in training, apprentices, or local volunteers.
Saiin were female relatives of the Japanese emperor (termed saiō) who served as High Priestesses in Kamo Shrine. Saiō also served at Ise Shrine. Saiin priestesses usually were elected from royalty. In principle, Saiin remained unmarried, but there were exceptions. Some Saiin became consorts of the emperor, called Nyōgo in Japanese. The Saiin order of priestesses existed throughout the Heian and Kamakura periods.
Africa.
The Yoruba people of western Nigeria practice an indigenous religion with a chiefly hierarchy of priests and priestesses that dates to AD 800–1000. Ifá priests and priestesses bear the titles Babalawo for men and Iyanifa for women. Priests and priestesses of the varied Orisha are titled Babalorisa for men and Iyalorisa for women. Initiates are also given an Orisa or Ifá name that signifies under which deity they are initiated. For example, a Priestess of Oshun may be named Osunyemi, and a Priest of Ifá may be named Ifáyemi. This ancient culture continues to this day as initiates from all around the world return to Nigeria for initiation into the traditional priesthood, and varied derivative sects in the New World (such as Cuban Santeria and Brazilian Umbanda) use the same titles to refer to their officers as well.
Neo-Paganism.
Wicca.
According to traditional Wiccan beliefs, every member of the religion is considered a priestess or priest, as it is believed that no person can stand between another and the Divine. However, in response to the growing number of Wiccan temples and churches, several denominations of the religion have begun to develop a core group of ordained priestesses and priests serving a larger laity. This trend is far from widespread, but is gaining acceptance due to increased interest in the religion.
Dress.
The dress of religious workers in ancient times may be demonstrated in frescoes and artifacts from the cultures. The dress is presumed to be related to the customary clothing of the culture, with some symbol of the deity worn on the head or held by the person. Sometimes special colors, materials, or patterns distinguish celebrants, as the white wool veil draped on the head of the Vestal Virgins.
Occasionally the celebrants at religious ceremonies shed all clothes in a symbolic gesture of purity. This was often the case in ancient times. An example of this is shown to the left on a Kylix dating from c. 500 BC where a priestess is featured. Modern religious groups tend to avoid such symbolism and some may be quite uncomfortable with the concept.
The retention of long skirts and vestments among many ranks of contemporary priests when they officiate may be interpreted to express the ancient traditions of the cultures from which their religious practices arose.
In most Christian traditions, priests wear clerical clothing, a distinctive form of street dress. Even within individual traditions it varies considerably in form, depending on the specific occasion. In Western Christianity, the stiff white clerical collar has become the nearly universal feature of priestly clerical clothing, worn either with a cassock or a clergy shirt. The collar may be either a full collar or a vestigial tab displayed through a square cutout in the shirt collar.
Eastern Christian priests mostly retain the traditional dress of two layers of differently cut cassock: the "rasson" (Greek) or "podriasnik" (Russian) beneath the outer "exorasson" (Greek) or "riasa" (Russian). If a pectoral cross has been awarded it is usually worn with street clothes in the Russian tradition, but not so often in the Greek tradition.
Distinctive clerical clothing is less often worn in modern times than formerly, and in many cases it is rare for a priest to wear it when not acting in a pastoral capacity, especially in countries that view themselves as largely secular in nature. There are frequent exceptions to this however, and many priests rarely if ever go out in public without it, especially in countries where their religion makes up a clear majority of the population. Pope John Paul II often instructed Catholic priests and religious to always wear their distinctive (clerical) clothing, unless wearing it would result in persecution or grave verbal attacks.
Christian traditions that retain the title of priest also retain the tradition of special liturgical vestments worn only during services. Vestments vary widely among the different Christian traditions.
In modern Pagan religions, such as Wicca, there is no one specific form of dress designated for the clergy. If there is, it is a particular of the denomination in question, and not a universal practice. However, there is a traditional form of dress, (usually a floor-length tunic and a knotted cord cincture, known as the "cingulum"), which is often worn by worshipers during religious rites. Among those traditions of Wicca that do dictate a specific form of dress for its clergy, they usually wear the traditional tunic in addition to other articles of clothing (such as an open-fronted robe or a cloak) as a distinctive form of religious dress, similar to a habit.
Assistant priest.
In many religions there are one or more layers of assistant priests.
In the Ancient Near East, hierodules served in temples as assistants to the priestess.
In ancient Judaism, the Priests (Kohanim) had a whole class of Levites as their assistants in making the sacrifices, in singing psalms and in maintaining the Temple. The Priests and the Levites were in turn served by servants called Nethinim. These lowest level of servants were not priests.
An assistant priest is a priest in the Anglican and Episcopal churches who is not the senior member of clergy of the parish to which they are appointed, but is nonetheless in priests' orders; there is no difference in function or theology, merely in 'grade' or 'rank'. Some assistant priests have a "sector ministry", that is to say that they specialize in a certain area of ministry within the local church, for example youth work, hospital work, or ministry to local light industry. They may also hold some diocesan appointment part-time. In most (though not all) cases an assistant priest has the legal status of assistant curate, although it should also be noted that not all assistant curates are priests, as this legal status also applies to many deacons working as assistants in a parochial setting.
The corresponding term in the Catholic Church is "parochial vicar" – an ordained priest assigned to assist the pastor (Latin: "parochus") of a parish in the pastoral care of parishioners. Normally, all pastors are also ordained priests; occasionally an auxiliary bishop will be assigned that role.
In Wicca, the leader of a coven or temple (either a high priestess or high priest) often appoints an assistant. This assistant is often called a 'deputy', but the more traditional terms 'maiden' (when female and assisting a high priestess) and 'summoner' (when male and assisting a high priest) are still used in many denominations.

</doc>
<doc id="23708" url="https://en.wikipedia.org/wiki?curid=23708" title="PL/I">
PL/I

PL/I ("Programming Language One", pronounced ) is a procedural, imperative computer programming language designed for scientific, engineering, business and system programming uses. It has been used by various academic, commercial and industrial organizations since it was introduced in the 1960s, and continues to be actively used .
PL/I's main domains are data processing, numerical computation, scientific computing, and system programming; it supports recursion, structured programming, linked data structure handling, fixed-point, floating-point, complex, character string handling, and bit string handling. The language syntax is English-like and suited for describing complex data formats, with a wide set of functions available to verify and manipulate them.
Early history.
In the 1950s and early 1960s business and scientific users programmed for different computer hardware using different programming languages. Business users were moving from Autocoders via COMTRAN to COBOL, while scientific users programmed in General Interpretive Programme (GIP), Fortran, ALGOL, GEORGE, and others. The IBM System/360 (announced in 1964 but not delivered until 1966) was designed as a common machine architecture for both groups of users, superseding all existing IBM architectures. Similarly, IBM wanted a single programming language for all users. It hoped that Fortran could be extended to include the features needed by commercial programmers. In October 1963 a committee was formed composed originally of three IBMers from New York and three members of SHARE, the IBM
scientific users group, to propose these extensions to Fortran. Given the constraints
of Fortran, they were unable to do this and embarked on the design of a “new programming language” based loosely on ALGOL labeled “NPL". This acronym conflicted with that of the UK’s National Physical Laboratory and was
replaced briefly by MPPL (MultiPurpose Programming Language) and, in 1965, with PL/I (with a Roman numeral “I” ). The first definition appeared in April 1964.
IBM took NPL as a starting point and completed the design to a level that the first compiler could
be written: the NPL definition was incomplete in scope and in detail. Control of the
PL/I language was vested initially in the New York Programming Center and later at the IBM UK Laboratory at Hursley. The SHARE and GUIDE user groups were involved in extending the
language and had a role in IBM’s process for controlling the language through their PL/I Projects.
The experience of defining such a large language showed the need for a formal definition of PL/I. A project was set up in 1967 in IBM Vienna to make an unambiguous and complete specification. This led in turn to one of the first large scale Formal Methods for development, VDM.
The language was first specified in detail in the manual “PL/I Language Specifications. C28-6571” written in New York from 1965 and superseded by “PL/I Language Specifications. GY33-6003” written in Hursley from 1967. IBM continued to develop PL/I in the late sixties and early seventies, publishing it in the GY33-6003 manual. These manuals were used by the Multics group and other early implementers.
The first compiler was delivered in 1966. The Standard for PL/I was approved in 1976.
Goals and principles.
The SHARE 3by3 committee set these goals for NPL:
These goals evolved during the early development of the language. Competitiveness with COBOL’s record handling and report writing capabilities was needed. The “scope of usefulness” of the language grew to include system programming and event-driven programming. The additional goals for PL/I were:
To meet these goals PL/I borrowed ideas from contemporary languages while adding substantial new capabilities and casting it with a distinctive concise and readable syntax. A number of principles and capabilities combined to give the language its character and were key in meeting the goals:
These principles inevitably resulted in a large language which would need compilers substantially more complex than those for COBOL or FORTRAN. This was not seen as a drawback since though the few—the compiler writers—would have more work, the many—the programmers—would have less.
Language summary.
The language is designed to be all things to all programmers. The summary is extracted from the ANSI PL/I Standard
and the ANSI PL/I General-Purpose Subset Standard.
A PL/I program consists of a set of procedures, each of which is written as a sequence of statements. The codice_5 construct is used to include text from other sources during program translation. All of the statement types are summarized here in groupings which give an overview of the language (the Standard uses this organization).
Names may be declared to represent data of the following types, either as single values, or as aggregates in the form of arrays, with a lower-bound and upper-bound per dimension, or structures (comprising nested structure, array and scalar variables):
The codice_6 type comprises these attributes:
The base, scale, precision and scale factor of the codice_7 type is encoded within the codice_8. The mode is specified separately, with the codice_9 applied to both the real and the imaginary parts.
Values are computed by expressions written using a specific set of operations and builtin functions, most of which may be applied to aggregates as well as to single values, together with user-defined procedures which, likewise, may operate on and return aggregate as well as single values. The assignment statement assigns values to one or more variables.
There are no reserved words in PL/I. A statement is terminated by a semi-colon. The maximum length of a statement is implementation defined. A comment may appear anywhere in a program where a space is permitted and is preceded by the characters forward slash, asterisk and is terminated by the characters asterisk, forward slash (i.e. ). Statements may have a label-prefix introducing an entry name (codice_10 and codice_11 statements) or label name, and a condition prefix enabling or disabling a computational condition - e.g. codice_12). Entry and label names may be single identifiers or identifiers followed by a subscript list of constants (as in codice_13).
A sequence of statements becomes a "group" when preceded by a codice_4 statement and followed by an codice_15 statement. Groups may include nested groups and begin blocks. The codice_16 statement specifies a group or a single statement as the codice_17 part and the codice_18 part (see the sample program). The group is the unit of iteration. The begin "block" (codice_19) may contain declarations for names and internal procedures local to the block. A "procedure" starts with a codice_11 statement and is terminated syntactically by an codice_15 statement. The body of a procedure is a sequence of blocks, groups, and statements and contains declarations for names and procedures local to the procedure or codice_22 to the procedure.
An "on-unit" is a single statement or block of statements written to be executed when one or more of these "conditions" occur:
a "computational condition",
or an "Input/Output" condition,
or one of the conditions:
A declaration of an identifier may contain one or more of the following attributes (but they need to be mutually consistent):
Current compilers from Kednos, Micro Focus, and particularly that from IBM implement many extensions over the standardized version of the language. The IBM extensions are summarised in the Implementation sub-section for the compiler later. Although there are some extensions common to these compilers the lack of a current standard means that compatibility is not guaranteed.
Standardization.
Language standardization began in April 1966 in Europe with ECMA TC10. In 1969 ANSI established a "Composite Language Development Committee", nicknamed "Kludge", which fortunately was renamed X3J1 PL/I. Standardization became a joint effort of ECMA TC/10 and ANSI X3J1. A subset of the GY33-6003 document was offered to the joint effort by IBM and became the base document for standardization. The major features omitted from the base document were multitasking and the attributes for program optimization (e.g. codice_26 and codice_27).
Proposals to change the base document were voted upon by both committees. In the event that the committees disagreed, the chairs, initially Michael Marcotty of General Motors and C.A.R. Hoare representing ICL had to resolve the disagreement. In addition to IBM, Honeywell, CDC, Data General, Digital Equipment, Prime Computer, Burroughs, RCA, and Univac served on X3J1 along with major users Eastman Kodak, MITRE, Union Carbide, Bell Laboratories, and various government and university representatives. Further development of the language occurred in the standards bodies, with continuing improvements in structured programming and internal consistency, and with the omission of the more obscure or contentious features.
As language development neared an end, X3J1/TC10 realized that there were a number of problems with a document written in English text. Discussion of a single item might appear in multiple places which might or might not agree. It was difficult to determine if there were omissions as well as inconsistencies. Consequently, David Beech (IBM), Robert Freiburghouse (Honeywell), Milton Barber (CDC), M. Donald MacLaren (Argonne National Laboratory), Craig Franklin (Data General), Lois Frampton (Digital Equipment), and editor, D.J. Andrews of IBM undertook to rewrite the entire document, each producing one or more complete chapters. The standard is couched as a formal definition using a "PL/I Machine" to specify the semantics. It was the first, and possibly the only, programming language standard to be written as a semi-formal definition.
A "PL/I General-Purpose Subset" ("Subset-G") standard was issued by ANSI in 1981 and a revision published in 1987. The General Purpose subset was widely adopted as the kernel for PL/I implementations.
Implementations.
IBM PL/I F and D compilers.
PL/I was first implemented by IBM, at its Hursley Laboratories in the United Kingdom, as part of the development of System/360. The first production PL/I compiler was the PL/I F compiler for the OS/360 Operating System, built by John Nash's team at Hursley in the UK: the runtime library team was managed by I.M. (Nobby) Clarke. The PL/I F compiler was written entirely in System/360 assembly language. Release 1 shipped in 1966. OS/360 was a real-memory environment and the compiler was designed for systems with as little as 64 kilobytes of real storage – F being 64 kB in S/360 parlance. To fit a large compiler into the 44 kilobytes of memory available on a 64-kilobyte machine, the compiler consisted of a control phase and a large number of compiler phases (approaching 100). The phases were brought into memory from disk, and released, one at a time to handle particular language features and aspects of compilation.
Aspects of the language were still being designed as PL/I F was implemented, so some were omitted until later releases. PL/I RECORD I/O was shipped with PL/I F Release 2. The list processing functions - Based Variables, Pointers, Areas and Offsets and LOCATE-mode I/O - were first shipped in Release 4. In a major attempt to speed up PL/I code to compete with Fortran object code, PL/I F Release 5 did substantial program optimization of DO-loops facilitated by the REORDER option on procedures.
A version of PL/I F was released on the TSS/360 timesharing operating system for the System/360 Model 67, adapted at the IBM Mohansic Lab. The IBM La Gaude Lab in France developed “Language Conversion Programs” to convert Fortran, Cobol, and Algol programs to the PL/I F level of PL/I.
The PL/I D compiler, using 16 kilobytes of memory, was developed by IBM Germany for the DOS/360 low end operating system. It implemented a subset of the PL/I language requiring all strings and arrays to have fixed extents, thus simplifying the run-time environment. Reflecting the underlying operating system it lacked dynamic storage allocation and the "controlled" storage class. It was shipped within a year of PL/I F.
Multics PL/I and derivatives.
Compilers were implemented by several groups in the early 1960s. The Multics project at MIT, one of the first to develop an operating system in a high-level language, used Early PL/I (EPL), a subset dialect of PL/I, as their implementation language in 1964. EPL was developed at Bell Labs and MIT by Douglas McIlroy, Robert Morris, and others. The influential Multics PL/I compiler, described on the "Multicians" website, was the source of compiler technology used by a number of manufacturers and software groups.
The Honeywell PL/I compiler (for Series 60) was an implementation of the full ANSI X3J1 standard.
IBM PL/I optimizing and checkout compilers.
The PL/I Optimizer and Checkout compilers produced in Hursley supported a common level of PL/I language and aimed to replace the PL/I F compiler. The checkout compiler was a rewrite of PL/I F in BSL, IBM's PL/I-like proprietary implementation language (later PL/S). The performance objectives set for the compilers are shown in an IBM presentation to the BCS. The compilers had to produce identical results - the Checkout Compiler was used to debug programs that would then be submitted to the Optimizer. Given that the compilers had entirely different designs and were handling the full PL/I language this goal was challenging: it was achieved.
The PL/I optimizing compiler took over from the PL/I F compiler and was IBM’s workhorse compiler from the 1970s to the 1990s. Like PL/I F, it was a multiple pass compiler with a 44 kilobyte design point, but it was an entirely new design. Unlike the F compiler, it had to perform compile time evaluation of constant expressions using the run-time library, reducing the maximum memory for a compiler phase to 28 kilobytes. A second-time around design, it succeeded in eliminating the annoyances of PL/I F such as cascading diagnostics. It was written in S/360 Macro Assembler by a team, led by Tony Burbridge, most of whom had worked on PL/I F. Macros were defined to automate common compiler services and to shield the compiler writers from the task of managing real-mode storage, allowing the compiler to be moved easily to other memory models. The gamut of program optimization techniques developed for the contemporary IBM Fortran H compiler were deployed: the Optimizer equaled Fortran execution speeds in the hands of good programmers. Announced with the IBM S/370 in 1970, it shipped first for the DOS/360 operating system in August 1971, and shortly afterward for OS/360, and the first virtual memory IBM operating systems OS/VS1, MVS, and VM/CMS. (The developers were unaware that while they were shoehorning the code into 28 kb sections, IBM Poughkeepsie was finally ready to ship virtual memory support in OS/360). It supported the batch programming environments and, under TSO and CMS, it could be run interactively. This compiler went through many versions covering all mainframe operating systems including the operating systems of the Japanese PCMs.
The compiler has been superseded by "IBM PL/I for OS/2, AIX, Linux, z/OS" below.
The PL/I checkout compiler, (colloquially "The Checker") announced in August 1970 was designed to speed and improve the debugging of PL/I programs. The team was led by Brian Marks. The three-pass design cut the time to compile a program to 25% of that taken by the F Compiler. It was run from an interactive terminal, converting PL/I programs into an internal format, “H-text”. This format was interpreted by the Checkout compiler at run-time, detecting virtually all types of errors. Pointers were represented in 16 bytes, containing the target address and a description of the referenced item, thus permitting "bad" pointer use to be diagnosed. In a conversational environment when an error was detected, control was passed to the user who could inspect any variables, introduce debugging statements and edit the source program. Over time the debugging capability of mainframe programming environments developed most of the functions offered by this compiler and it was withdrawn (in the 1990s?)
DEC PL/I.
Perhaps the most commercially successful implementation aside from IBM's was Digital Equipment's 1988 release of the ANSI PL/I 1987 subset. The implementation is "a strict superset of the ANSI X3.4-1981 PL/I General Purpose Subset and provides most of the features of the new ANSI X3.74-1987 PL/I General Purpose Subset". The front end was designed by Robert Freiburghouse, and the code generator was implemented by Dave Cutler, who managed the design and implementation of VAX/VMS. It runs on VMS on VAX and ALPHA and on Tru64. UniPrise Systems, Inc., was responsible for the compiler; it is currently supported by Kednos Corporation.
Teaching subset compilers.
In the late 1960s and early 1970s, many US and Canadian Universities were establishing time-sharing services on campus and needed conversational compiler/interpreters for use in teaching science, mathematics, engineering, and computer science. Dartmouth were developing BASIC, but PL/I was a popular choice, as it was concise and easy to teach. As the IBM offerings were unsuitable, a number of schools built their own subsets of PL/I and their own interactive support. Examples are:
A compiler developed at Cornell University for teaching a dialect called PL/C, which had the unusual capability of never failing to compile any program through the use of extensive automatic correction of many syntax errors and by converting any remaining syntax errors to output statements. The language was almost all of PL/I as implemented by IBM. PL/C was a very fast compiler.
PLAGO, created at the Polytechnic Institute of Brooklyn, used a simplified subset of the PL/I language and focused on good diagnostic error messages and fast compilation times.
The Computer Systems Research Group of the University of Toronto produced the SP/k compilers which supported a sequence of subsets of PL/I called SP/1, SP/2, SP/3, ..., SP/8 for teaching programming. Programs that ran without errors under the SP/k compilers produced the same results under other contemporary PL/I compilers such as IBM's PL/I F compiler, IBM's checkout compiler or Cornell University's PL/C compiler.
Other examples are PL0 by P. Grouse at the University of New South Wales, PLUM by Marvin Zelkowitz at the University of Maryland., and PLUTO from the University of Toronto.
IBM PL/I for OS/2, AIX, Linux, z/OS.
In a major revamp of PL/I, IBM Santa Teresa in California launched an entirely new compiler in 1992. The initial shipment was for OS/2 and included most ANSI-G features and many new PL/I features. Subsequent releases covered additional platforms (MVS, VM, OS/390, AIX and Windows) and continued to add functions to make PL/I fully competitive with other languages offered on the PC (particularly C and C++) in areas where it had been overtaken. The corresponding “IBM Language Environment" supports inter-operation of PL/I programs with Database and Transaction systems, and with programs written in C, C++, and COBOL, the compiler supports all the data types needed for intercommunication with these languages.
The PL/I design principles were retained and withstood this major extension comprising several new data types, new statements and statement options, new exception conditions, and new organisations of program source. The resulting language is a compatible super-set of the PL/I Standard and of the earlier IBM compilers. Major topics added to PL/I were:
The latest series of PL/I compilers for z/OS, called Enterprise PL/I for z/OS, leverage code generation for the latest z/Architecture processors (z990, zEC12, Z13) via the use of ARCHLVL parm control passed during compilation, and was the second High level language supported by z/OS Language Environment to do so (XL C/C++ being the first, and Enterprise COBOL v5 the last.)
Object orientation.
codice_29 is a new computational data type. The ordinal facilities are like those in Pascal,
e.g. codice_38
but in addition the name and internal values are accessible via built-in functions. Built-in functions provide access to an ordinal value's predecessor and successor.
The codice_39 (see below) allows additional codice_32s to be declared composed from PL/I's built-in attributes.
The codice_41 locator data type is similar to the codice_42 data type, but strongly typed to bind only to a particular data structure. The codice_43 operator is used to select a data structure using a handle.
The codice_33 attribute (equivalent to codice_45 in early PL/I specifications) permits several scalar variables, arrays, or structures to share the same storage in a unit that occupies the amount of storage needed for the largest alternative.
Competitiveness on PC and with C.
These attributes were added:
New string-handling functions were added - to centre text, to edit using a picture format, and to trim blanks or selected characters from the head or tail of text, codice_62 to codice_63 from the right. and codice_64 and codice_65 functions.
Compound assignment operators a la C e.g. codice_66, codice_67, codice_68, codice_69 were added. codice_70 is equivalent to codice_71.
Additional parameter descriptors and attributes were added for omitted arguments and variable length argument lists.
Program readability – making intentions explicit.
The codice_72 attribute declares an identifier as a constant (derived from a specific literal value or restricted expression).
Parameters can have the codice_73 (pass by address) or codice_36 (pass by value) attributes.
The codice_75 and codice_76 attributes prevent unintended assignments.
codice_77 obviates the need for the contrived construct codice_78.
The codice_39 introduces user-specified names (e.g. codice_80) for combinations of built-in attributes (e.g. codice_81). Thus codice_82 creates the codice_32 name codice_80 as an alias for the set of built-in attributes FIXED BINARY(31.0). codice_30 applies to structures and their members; it provides a codice_32 name for a set of structure attributes and corresponding substructure member declarations for use in a structure declaration (a generalisation of the codice_87 attribute).
Structured programming additions.
A codice_88 statement to exit a loop, and an codice_89 to continue with the next iteration of a loop.
codice_55 and codice_56 options on iterative groups.
The package construct consisting of a set of procedures and declarations for use as a unit. Variables declared outside of the procedures are local to the package, and can use codice_92, codice_93 or codice_94 storage. Procedure names used in the package also are local, but can be made external by means of the codice_95 option of the codice_96.
Interrupt handling.
The codice_97 executed in an ON-unit terminates execution of the ON-unit, and raises the condition again in the procedure that called the current one (thus passing control to the corresponding ON-unit for that procedure).
The codice_98 condition handles invalid operation codes detected by the PC processor, as well as illegal arithmetic operations such as subtraction of two infinite values.
The codice_99 condition is provided to intercept conditions for which no specific ON-unit has been provided in the current procedure.
The codice_100 condition is raised when an codice_101 statement is unable to obtain sufficient storage.
Other mainframe and minicomputer compilers.
A number of vendors produced compilers to compete with IBM PL/I F or Optimizing compiler on mainframes and minicomputers in the 1970s. In the 1980s the target was usually the emerging ANSI-G subset.
Usage.
PL/I implementations were developed for mainframes from the late 1960s, mini computers in the 1970s, and personal computers in the 1980s and 1990s. Although its main use has been on mainframes, there are PL/I versions for DOS, Microsoft Windows, OS/2, AIX, OpenVMS, and Unix.
It has been widely used in business data processing and for system use for authoring operating systems on certain platforms. Very complex and powerful systems have been built with PL/I:
The SAS System was initially written in PL/I; the SAS data step is still modeled on PL/I syntax.
The pioneering online airline reservation system Sabre was originally written for the IBM 7090 in assembler. The S/360 version was largely written using SabreTalk, a purpose built subset PL/I compiler for a dedicated control program.
PL/I was used to write an executable formal definition to interpret IBM's System Network Architecture
PL/I did not fulfill its supporters' hopes that it would displace Fortran and COBOL and become the major player on mainframes. It remained a minority but significant player. There cannot be a definitive explanation for this, but some trends in the 1970s and 1980s militated against its success by progressively reducing the territory on which PL/I enjoyed a competitive advantage.
First, the nature of the mainframe software environment changed. Application subsystems for database and transaction processing (CICS and IMS and Oracle on System 370) and application generators became the focus of mainframe users' application development. Significant parts of the language became irrelevant because of the need to use the corresponding native features of the subsystems (such as tasking and much of input/output). Fortran was not used in these application areas, confining PL/I to COBOL’s territory; most users stayed with COBOL. But as the PC became the dominant environment for program development, Fortran, COBOL and PL/I all became minority languages overtaken by C++, Java and the like.
Second, PL/I was overtaken in the system programming field. The IBM system programming community was not ready to use PL/I; instead, IBM developed and adopted a proprietary dialect of PL/I for system programming. – PL/S. With the success of PL/S inside IBM, and of C outside IBM, the unique PL/I strengths for system programming became less valuable.
Third, the development environments grew capabilities for interactive software development that, again, made the unique PL/I interactive and debugging strengths less valuable.
Fourth, COBOL and Fortran added features such as structured programming, character string operations, and object orientation, that further reduced PL/I's relative advantages.
On mainframes there were substantial business issues at stake too. IBM’s hardware competitors had little to gain and much to lose from success of PL/I. Compiler development was expensive, and the IBM compiler groups had an in-built competitive advantage. Many IBM users wished to avoid being locked into proprietary solutions. With no early support for PL/I by other vendors it was best to avoid PL/I.
Evolution of the PL/I language.
This article uses the PL/I standard as the reference point for language features. But a number of features of significance in the early implementations were not in the Standard; and some were offered by non-IBM compilers. And the de facto language continued to grow after the standard, ultimately driven by developments on the Personal Computer.
Significant features omitted from the standard.
Multi tasking.
"Multi tasking" was implemented by PL/I F, the Optimizer and the newer AIX and Z/OS compilers. It comprised the data types codice_103 and codice_104, the codice_105 on the codice_106 (Fork), the codice_107 (Join), the codice_108, codice_109s on the record I/O statements and the codice_110 statement to unlock locked records on codice_111 files. Event data identify a particular event and indicate whether it is complete ('1'B) or incomplete ('0'B): task data items identify a particular task (or process) and indicate its priority relative to other tasks.
Preprocessor.
The first IBM "Compile time preprocessor" was built by the IBM Boston Advanced Programming Center located in Cambridge, Mass, and shipped with the PL/I F compiler. The codice_5 statement was in the Standard, but the rest of the features were not. The DEC and Kednos PL/I compilers implemented much the same set of features as IBM, with some additions of their own. IBM has continued to add preprocessor features to its compilers. The preprocessor treats the written source program as a sequence of tokens, copying them to an output source file or acting on them. When a % token is encountered the following compile time statement is executed: when an identifier token is encountered and the identifier has been codice_113d, codice_114d, and assigned a compile time value, the identifier is replaced by this value. Tokens are added to the output stream if they do not require action (e.g. codice_115), as are the values of ACTIVATEd compile time expressions. Thus a compile time variable codice_116 could be declared, activated, and assigned using codice_117. Subsequent occurrences of codice_116 would be replaced by codice_119.
The data type supported are codice_120 integers and codice_121 strings of varying length with no maximum length. The structure statements are
and the simple statements, which also may have a [label-list:]
The feature allowed programmers to use identifiers for constants - e.g. product part numbers or mathematical constants - and was superseded in the standard by named constants for computational data. Conditional compiling and iterative generation of source code, possible with compile-time facilities, was not supported by the standard. Several manufacturers implemented these facilities.
Structured programming additions.
Structured programming additions were made to PL/I during standardization but were not accepted into the standard. These features were the codice_131 to exit from an iterative codice_4, the codice_133 and codice_134 added to codice_4, and a case statement of the general form:
codice_136<br>
These features were all included in DEC PL/I.
Debug facilities.
PL/I F had offered some debug facilities that were not put forward for the standard but were implemented by others - notably the CHECK(variable-list) condition prefix, codice_137 on-condition and the codice_138 option. The IBM Optimizing and Checkout compilers added additional features appropriate to the conversational mainframe programming environment (e.g. an codice_139 condition).
Significant features developed since the standard.
Several attempts had been made to design a structure member type that could have one of several datatypes (codice_45 in early IBM). With the growth of classes in programming theory, approaches to this became possible on a PL/I base - codice_33, codice_32 etc. have been added by several compilers.
PL/I had been conceived in a single-byte character world. With support for Japanese and Chinese language becoming essential, and the developments on International Code Pages, the character string concept was expanded to accommodate wide non-ASCII/EBCDIC strings.
Time and date handling were overhauled to deal with the millennium problem,
with the introduction of the DATETIME function that returned the date and time in one of about 35 different formats. Several other date functions deal with conversions to to and from days and seconds.
Criticisms.
Implementation issues.
Though the language is easy to learn and use, implementing a PL/I compiler was difficult and time-consuming. A language as large as PL/I needed subsets that most vendors could produce and most users master. This was not resolved until "ANSI G" was published. The compile time facilities, unique to PL/I, took added implementation effort and additional compiler passes. A PL/I compiler was two to four times as large as comparable Fortran or COBOL compilers, and also that much slower—fortunately offset by gains in programmer productivity. This was anticipated in IBM before the first compilers were written.
Some argued that PL/I was unusually hard to parse. The PL/I "keywords" were not reserved so programmers could use them as variable or procedure names in programs. Because the original PL/I F compiler attempted "auto-correction" when it encountered a keyword used in an incorrect context, it often assumed it was a variable name. This led to "cascading diagnostics", a problem solved by later compilers.
The effort needed to produce good object code was perhaps underestimated during the initial design of the language. Program optimization (needed to compete with the excellent program optimization carried out by available Fortran compilers) was unusually complex owing to side effects and pervasive problems with aliasing of variables. Unpredictable modification can occur asynchronously for ABNORMAL data, or in exception handlers, which may be provided by "ON statements" in (unseen) callers. Together, these make it difficult to reliably predict when a program's variables might be modified at runtime. In typical use, however, user-written error handlers (the codice_143) do not make assignments to variables. In spite of the aforementioned difficulties, IBM produced its PL/I optimising compiler in 1971.
PL/I contained many rarely used features, such as multitasking support, which added cost and complexity to the compiler, and its co-processing facilities required a multi-programming environment with support for non-blocking multiple threads for processes by the operating system. Compiler writers were free to select whether to implement these features.
An undeclared variable was by default being declared by first occurrence—thus misspelling might lead to unpredictable results. This was no different from FORTRAN programs. For PL/I F, however, an attribute listing enabled the programmer to detect any mis-spelled or undeclared variable.
Programmer issues.
Many programmers were slow to move from COBOL or Fortran due to a perceived complexity of the language and immaturity of the PL/I F compiler. Programmers were sharply divided into scientific programmers (who used Fortran) and business programmers (who used COBOL), with significant tension and even dislike between the groups. PL/I syntax borrowed from both COBOL and Fortran syntax. So instead of noticing features that would make their job easier, Fortran programmers of the time noticed COBOL syntax and had the opinion that it was a business language, while COBOL programmers noticed Fortran syntax and looked upon it as a scientific language.
Both COBOL and Fortran programmers viewed it as a "bigger" version of their own language, and both were somewhat intimidated by the language and disinclined to adopt it. Another factor was "pseudo"-similarities to COBOL, Fortran, and ALGOL. These were PL/I elements that looked similar to one of those languages, but worked differently in PL/I. Such frustrations left many experienced programmers with a jaundiced view of PL/I, and often an active dislike for the language. An early UNIX fortune file contained the following tongue-in-cheek description of the language:
Speaking as someone who has delved into the intricacies of PL/I, I am sure that only Real Men could have written such a machine-hogging, cycle-grabbing, all-encompassing monster. Allocate an array and free the middle third? Sure! Why not? Multiply a character string times a bit string and assign the result to a float decimal? Go ahead! Free a controlled variable procedure parameter and reallocate it before passing it back? Overlay three different types of variable on the same memory location? Anything you say! Write a recursive macro? Well, no, but Real Men use rescan. How could a language so obviously designed and written by Real Men not be intended for Real Man use?
On the positive side, full support for pointers to all data types (including pointers to structures), recursion, multitasking, string handling, and extensive built-in functions PL/I was indeed quite a leap forward compared to the programming languages of its time. However, these were not enough to convince a majority of programmers or shops to switch to PL/I.
The PL/I F compiler's compile time preprocessor was unusual (outside the Lisp world) in using its target language's syntax and semantics ("e.g." as compared to the C preprocessor's "#" directives).
Special topics in PL/I.
Storage classes.
PL/I provides several 'storage classes' to indicate how the lifetime of variables' storage is to be managed - codice_144 and codice_93. The simplest to implement is codice_92, which indicates that memory is allocated and initialized at load-time, as is done in COBOL "working-storage" and early Fortran. This is the default for codice_22 variables.
PL/I's default storage class for codice_148 variables is codice_149, similar to that of other block-structured languages influenced by ALGOL, like the "auto" storage class in the C language, and default storage allocation in Pascal and "local-storage" in IBM COBOL. Storage for codice_149 variables is allocated upon entry into the codice_151, procedure, or on-unit in which they are declared. The compiler and runtime system allocate memory for a stack frame to contain them and other housekeeping information. If a variable is declared with an codice_152, code to set it to an initial value is executed at this time. Care is required to manage the use of initialization properly. Large amounts of code can be executed to initialize variables every time a scope is entered, especially if the variable is an array or structure. Storage for codice_149 variables is freed at block exit: codice_154 or codice_93 variables are used to retain variables' contents between invocations of a procedure or block. codice_94 storage is also managed using a stack, but the pushing and popping of allocations on the stack is managed by the programmer, using codice_101 and codice_158 statements. Storage for codice_93 variables is managed using codice_160, but instead of a stack these allocations have independent lifetimes and are addressed through codice_161 or codice_42 variables.
Storage type sharing.
There are several ways of accessing allocated storage through different data declarations. Some of these are well defined and safe, some can be used safely with careful programming, and some are inherently unsafe and/or machine dependent.
Passing a variable as an argument to a parameter by reference allows the argument's allocated storage to be referenced using the parameter. The codice_163 attribute (e.g. codice_164) allows part or all of a variable's storage to be used with a different, but consistent, declaration. These two usages are safe and machine independent.
Record I/O and list processing produce situations where the programmer needs to fit a declaration to the storage of the next record or item, before knowing what type of data structure it has. Based variables and pointers are key to such programs. The data structures must be designed appropriately, typically using fields in a data structure to encode information about its type and size. The fields can be held in the preceding structure or, with some constraints, in the current one. Where the encoding is in the preceding structure, the program needs to allocate a based variable with a declaration that matches the current item (using expressions for extents where needed). Where the type and size information are to be kept in the current structure ("self defining structures") the type-defining fields must be ahead of the type dependent items and in the same place in every version of the data structure. The codice_102-option is used for self-defining extents (e.g. string lengths as in codice_166 - where codice_167 is used to allocate instances of the data structure. For self-defining structures, any typing and codice_168 fields are placed ahead of the "real" data. If the records in a data set, or the items in a list of data structures, are organised this way they can be handled safely in a machine independent way.
PL/I implementations do not (except for the PL/I Checkout compiler) keep track of the data structure used when storage is first allocated. Any codice_93 declaration can be used with a pointer into the storage to access the storage - inherently unsafe and machine dependent. However this usage has become important for "pointer arithmetic" (typically adding a certain amount to a known address). This has been a contentious subject in computer science. In addition to the problem of wild references and buffer overruns, issues arise due to the alignment and length for data types used with particular machines and compilers. Many cases where pointer arithmetic might be needed involve finding a pointer to an element inside a larger data structure. The codice_170 function computes such pointers, safely and machine independently.
Pointer arithmetic may be accomplished by aliasing a binary variable with a pointer as in 
codice_171 
It relies on pointers being the same length as codice_172 integers and aligned on the same boundaries.
With the prevalence of C and its free and easy attitude to pointer arithmetic, recent IBM PL/I compilers allow pointers to be used with the addition and subtraction operators to giving the simplest syntax (but compiler options can disallow these practices where safety and machine independence are paramount).
On-Units and exception handling.
When PL/I was designed, programs only ran in batch mode, with no possible intervention from the programmer at a terminal. An exceptional condition such as division by zero would abort the program yielding only a hexadecimal core dump. PL/I exception handling, via on-units, allowed the program to stay in control in the face of hardware or operating system exceptions and to recover debugging information before closing down more gracefully. As a program became properly debugged, most of the exception handling could be removed or disabled: this level of control became less important when conversational execution became commonplace.
Computational exception handling is enabled and disabled by condition prefixes on statements, blocks(including on-units) and procedures. – e.g. codice_173. Operating system exceptions for Input/Output and storage management are always enabled.
The on-unit is a single statement or codice_174-block introduced by an codice_175. Executing the ON statement enables the condition specified, e.g., codice_176. When the exception for this condition occurs and the condition is enabled, the on-unit for the condition is executed. On-units are inherited down the call chain. When a block, procedure or on-unit is activated, the on-units established by the invoking activation are inherited by the new activation. They may be over-ridden by another codice_175 and can be reestablished by the codice_178. The exception can be simulated using the codice_179 – e.g. to help debug the exception handlers. The dynamic inheritance principle for on-units allows a routine to handle the exceptions occurring within the subroutines it uses.
If no on-unit is in effect when a condition is raised a standard system action is taken (often this is to raise the codice_180 condition). The system action can be reestablished using the codice_181 option of the codice_175. With some conditions it is possible to complete executing an on-unit and return to the point of interrupt (e.g., the codice_183 conditions) and resume normal execution. With other conditions such as codice_184, the codice_180 condition is raised when this is attempted. An on-unit may be terminated with a codice_186 preventing a return to the point of interrupt, but permitting the program to continue execution elsewhere as determined by the programmer.
An on-unit needs to be designed to deal with exceptions that occur in the on-unit itself. The codice_187 statement allows a nested error trap; if an error occurs within an on-unit, control might pass to the operating system where a system dump might be produced, or, for some computational conditions, continue execution (as mentioned above).
The PL/I codice_188 I/O statements have relatively simple syntax as they do not offer options for the many situations from end-of-file to record transmission errors that can occur when a record is read or written. Instead, these complexities are handled in the on-units for the various file conditions. The same approach was adopted for codice_23 sub-allocation and the codice_23 condition.
The existence of exception handling on-units can have an effect on optimization, because variables can be inspected or altered in ON-units. Values of variables that might otherwise be kept in registers between statements, may need to be returned to storage between statements. This is discussed in the section on Implementation Issues above.

</doc>
<doc id="23711" url="https://en.wikipedia.org/wiki?curid=23711" title="Punctuation">
Punctuation

Punctuation is "the use of spacing, conventional signs, and certain typographical devices as aids to the understanding and the correct reading, both silently and aloud, of handwritten and printed texts." Another description is: "The practice, action, or system of inserting points or other small marks into texts, in order to aid interpretation; division of text into sentences, clauses, etc., by means of such marks."
In written English, punctuation is vital to disambiguate the meaning of sentences. For example: "woman, without her man, is nothing" (emphasizing the importance of men), and "woman: without her, man is nothing" (emphasizing the importance of women) have very different meanings; as do "eats shoots and leaves" (which means the subject consumes plant growths) and "eats, shoots, and leaves" (which means the subject eats first, then fires a weapon, and then leaves the scene). The sharp differences in meaning are produced by the simple differences in punctuation within the example pairs, especially the latter.
The rules of punctuation vary with language, location, register and time and are constantly evolving. Certain aspects of punctuation are stylistic and are thus the author's (or editor's) choice. Tachygraphic language forms, such as those used in online chat and text messages, may have will
History.
The first writing systems were either logographic or syllabicfor example, Chinese and Maya scriptwhich do not necessarily require punctuation, especially spacing. This is because the entire morpheme or word is typically clustered within a single glyph, so spacing does not help as much to distinguish where one word ends and the other starts. Disambiguation and emphasis can easily be communicated without punctuation by employing a separate written form distinct from the spoken form of the language that uses slightly different phraseology. Even today, formal written modern English differs subtly from spoken English because not all emphasis and disambiguation is possible to convey in print, even with punctuation.
Ancient Chinese classical texts were transmitted without punctuation. However, many Warring-states-era bamboo texts contain the symbols and indicating the end of a chapter and full stop, respectively. By the Song dynasty, addition of punctuation to texts by scholars to aid comprehension became common.
The earliest alphabetic writing had no capitalization, no spaces, no vowels and few punctuation marks. This worked as long as the subject matter was restricted to a limited range of topics (e.g., writing used for recording business transactions). Punctuation is historically an aid to reading aloud.
The oldest known document using punctuation is the Mesha Stele (9th century BC). This employs points between the words and horizontal strokes between the sense section as punctuation.
Western Antiquity.
Most texts were still written in "scriptura continua", that is without any separation between words. However, the Greeks were sporadically using punctuation marks consisting of vertically arranged dots—usually two (dicolon) or three (tricolon)—in around the 5th century as an aid in the oral delivery of texts. Greek playwrights such as Euripides and Aristophanes used symbols to distinguish the ends of phrases in written drama: this essentially helped the play's cast to know when to pause. After 200 , the Greeks used a system (called "théseis") of a single dot ("punctus") placed at varying heights to mark up speeches at rhetorical divisions:
In addition, the Greeks used the paragraphos (or gamma) to mark the beginning of sentences, marginal diples to mark quotations, and a koronis to indicate the end of major sections.
The Romans ("ca". 1st century ) also occasionally used symbols to indicate pauses, but the Greek "théseis"—under the name "distinctiones"—prevailed by the 4th century as reported by Donatus and Isidore of Seville (7th century). Also, texts were sometimes laid out "per capitula", that is, every sentence had its own separate line. Diples were used, but by the late period these often degenerated into comma-shaped marks.
Medieval.
Punctuation developed dramatically when large numbers of copies of the Bible started to be produced. These were designed to be read aloud, so the copyists began to introduce a range of marks to aid the reader, including indentation, various punctuation marks (diple, paragraphos, "simplex ductus"), and an early version of initial capitals ("litterae notabiliores"). St. Jerome and his colleagues, who made the Latin Vulgate translation of the Bible ("ca". 400), employed a layout system based on established practices for teaching the speeches of Demosthenes and Cicero. Under his layout "per cola et commata" every sense-unit was indented and given its own line. This layout was solely used for biblical manuscripts during the 5th-9th centuries but was abandoned in favor of punctuation.
In the 7th-8th centuries Irish and Anglo-Saxon scribes, whose native languages were not derived from Latin, added more visual cues to render texts more intelligible. Irish scribes introduced the practice of word separation. Likewise, insular scribes adopted the "distinctiones" system while adapting it for minuscule script (so as to be more prominent) by using not differing height but rather a differing number of marks—aligned horizontally (or sometimes triangularly)—to signify a pause's value: one mark for a minor pause, two for a medium one, and three for a major. Most common were the "punctus", a comma-shaped mark, and a 7-shaped mark ("comma positura"), often used in combination. The same marks could be used in the margin to mark off quotations.
In the late 8th century a different system emerged in the Carolingian empire. Originally indicating how the voice should be modulated when chanting the liturgy, the "positurae" migrated into any text meant to be read aloud, and then to all manuscripts. "Positurae" first reached England in the late 10th century probably during the Benedictine reform movement, but was not adopted until after the Norman conquest. The original "positurae" were the "punctus", "punctus elevatus", "punctus versus", and "punctus interrogativus", but a fifth symbol, the "punctus flexus", was added in the 10th century to indicate a pause of a value between the "punctus" and "punctus elevatus". In the late 11th/early 12th century the "punctus versus" disappeared and was taken over by the simple "punctus" (now with two distinct values).
The late Middle Ages saw the addition of the "virgula suspensiva" (slash or slash with a midpoint dot) which was often used in conjunction with the "punctus" for different types of pauses. Direct quotations were marked with marginal diples, as in Antiquity, but from at least the 12th century scribes also began entering diples (sometimes double) within the column of text.
Later developments.
From the invention of moveable type in Europe in the 1450s the amount of printed material and a readership for it began to increase. "The rise of printing in the 14th and 15th centuries meant that a standard system of punctuation was urgently required." The introduction of a standard system of punctuation has also been attributed to the Venetian printers Aldus Manutius and his grandson. They have been credited with popularizing the practice of ending sentences with the colon or full stop, inventing the semicolon, making occasional use of parentheses and creating the modern comma by lowering the virgule. By 1566, Aldus Manutius the Younger was able to state that the main object of punctuation was the clarification of syntax.
By the 19th century, punctuation in the western world had evolved "to classify the marks hierarchically, in terms of weight". Cecil Hartley's poem identifies their relative values:
The use of punctuation was not standardised until after the invention of printing. According to the 1885 edition of "The American Printer", the importance of punctuation was noted in various sayings by children such as:
With a semi-colon and a comma added it reads:
In a 19th-century manual of typography, Thomas MacKellar writes:
Shortly after the invention of printing, the necessity of stops or pauses in sentences for the guidance of the reader produced the colon and full point. In process of time, the comma was added, which was then merely a perpendicular line, proportioned to the body of the letter. These three points were the only ones used until the close of the fifteenth century, when Aldo Manuccio gave a better shape to the comma, and added the semicolon; the comma denoting the shortest pause, the semicolon next, then the colon, and the full point terminating the sentence. The marks of interrogation and admiration were introduced many years after.
The standards and limitations of evolving technologies have exercised further pragmatic influences. For example, minimisation of punctuation in typewritten matter became economically desirable in the 1960s and 1970s for the many users of carbon-film ribbons, since a period or comma consumed the same length of expensive non-reusable ribbon as did a capital letter.
Punctuation of English.
There are two major styles of punctuation in English: American or traditional punctuation; and British or logical punctuation. These two styles differ mainly in the way in which they handle quotation marks. The Oxford comma is the use of a comma for the penultimate item in a list.
Other languages.
Other European languages use much the same punctuation as English. The similarity is so strong that the few variations may confuse a native English reader. Quotation marks are particularly variable across European languages. For example, in French and Russian, quotes would appear as: « Je suis fatigué. » (in French, each "double punctuation", as the guillemet, requires a non-breaking space; in Russian it does not).
In Greek, the question mark is written as the English semicolon, while the functions of the colon and semicolon are performed by a raised point (·), known as the "ano teleia" (άνω τελεία).
In Georgian, three dots, , were formerly used as a sentence or paragraph divider. It is still sometimes used in calligraphy. 
Spanish uses an inverted question mark at the beginning of a question and the normal question mark at the end, as well as an inverted exclamation mark at the beginning of an exclamation and the normal exclamation mark at the end.
Armenian uses several punctuation marks of its own. The full stop is represented by a colon, and vice versa; the exclamation mark is represented by a diagonal similar to a tilde (~), while the question mark resembles the "at" symbol.
Arabic, Urdu, and Persian languages—written from right to left—use a reversed question mark: ؟, and a reversed comma: ، . This is a modern innovation; pre-modern Arabic did not use punctuation. Hebrew, which is also written from right to left, uses the same characters as in English, "," and "?" .
Originally, Sanskrit had no punctuation. In the 17th century, Sanskrit and Marathi, both written in the Devanagari script, started using the vertical bar (।) to end a line of prose and double vertical bars (॥) in verse.
Punctuation was not used in Chinese, Japanese, and Korean writing until the adoption of punctuation from the West in the late 19th and early 20th century. In unpunctuated texts, the grammatical structure of sentences in classical writing is inferred from context. Most punctuation marks in modern Chinese, Japanese, and Korean have similar functions to their English counterparts; however, they often look different and have different customary rules.
In the Indian Subcontinent (India, Pakistan, ...), :- is sometimes used in place of colon or after a subheading. Its origin is unclear, but could be a remnant of the British Raj. Another punctuation common in the Indian Subcontinent for writing money amounts is the use of /- or /= after the number. For example, Rs. 20/- or Rs. 20/= implies 20 rupees whole. 
"Further information: Armenian punctuation, Chinese punctuation, Hebrew punctuation, Japanese punctuation and Korean punctuation."
Novel punctuation marks.
“Love point” and similar marks.
In 1966, the French author Hervé Bazin proposed a series of six innovative punctuation marks in his book "Plumons l’Oiseau" (“Let's pluck the bird”, 1966). These were:
“Question comma”, “exclamation comma”.
An international patent application was filed, and published in 1992 under WO number WO9219458, for two new punctuation marks: the “question comma” and the “exclamation comma”. The "question comma" has a comma instead of the dot at the bottom of a question mark, while the "exclamation comma" has a comma in place of the point at the bottom of an exclamation mark. These were intended for use as question and exclamation marks within a sentence, a function for which normal question and exclamation marks can also be used, but which may be considered obsolescent. The patent application entered into the national phase only in Canada. It was advertised as lapsing in Australia on 27 January 1994 and in Canada on 6 November 1995.

</doc>
<doc id="23712" url="https://en.wikipedia.org/wiki?curid=23712" title="Pentomino">
Pentomino

A pentomino is a plane geometric figure formed by joining five equal squares edge to edge. It is a polyomino with five cells. There are twelve pentominoes, not counting rotations and reflections as distinct. They are used chiefly in recreational mathematics for puzzles and problems. Pentominoes were formally defined by American professor Solomon W. Golomb starting in 1953 and later in his 1965 book "Polyominoes: Puzzles, Patterns, Problems, and Packings". Golomb coined the term "pentomino" from the Ancient Greek / "pénte", "five", and the -omino of domino, fancifully interpreting the "d-" of "domino" as if it were a form of the Greek prefix "di-" (two). Golomb named the 12 "free" pentominoes after letters of the Latin alphabet that they resemble.
Ordinarily, the pentomino obtained by reflecting or rotating a pentomino does not count as a different pentomino. The F, L, N, P, Y, and Z pentominoes are chiral; adding their reflections (F', J, N', Q, Y', S) brings the number of "one-sided" pentominoes to 18. Pentominoes I, T, U, V, W, and X, remain the same when reflected. This matters in some video games in which the pieces may not be reflected, such as Tetris imitations and Rampart.
Each of the twelve pentominoes satisfies the Conway criterion; hence every pentomino is capable of tiling the plane. Each chiral pentomino can tile the plane without reflecting it.
John Horton Conway proposed an alternate labeling scheme for pentominoes, using O instead of I, Q instead of L, R instead of F, and S instead of N. The resemblance to the letters is more strained, especially for the O pentomino, but this scheme has the advantage of using 12 consecutive letters of the alphabet. It is used by convention in discussing Conway's Game of Life, where, for example, one speaks of the R-pentomino instead of the F-pentomino.
Symmetry.
Pentominoes have the following categories of symmetry:
If reflections of a pentomino are considered distinct, as they are with one-sided pentominoes, then the first and fourth categories above double in size, resulting in an extra 6 pentominoes for a total of 18. If rotations are also considered distinct, then the pentominoes from the first category count eightfold, the ones from the next three categories (T, U, V, W, Z) count fourfold, I counts twice, and X counts only once. This results in 5×8 + 5×4 + 2 + 1 = 63 "fixed" pentominoes.
For example, the eight possible orientations of the L, F, N, P, and Y pentominoes are as follows:
For 2D figures in general there are two more categories:
Tiling rectangles.
A standard pentomino puzzle is to tile a rectangular box with the pentominoes, i.e. cover it without overlap and without gaps. Each of the 12 pentominoes has an area of 5 unit squares, so the box must have an area of 60 units. Possible sizes are 6×10, 5×12, 4×15 and 3×20. The avid puzzler can probably solve these problems by hand within a few hours. A more challenging task, typically requiring a computer search, is to count the total number of solutions in each case.
The 6×10 case was first solved in 1960 by Colin Brian and Jenifer Haselgrove. There are exactly 2339 solutions, excluding trivial variations obtained by rotation and reflection of the whole rectangle, but including rotation and reflection of a subset of pentominoes (which sometimes provides an additional solution in a simple way). The 5×12 box has 1010 solutions, the 4×15 box has 368 solutions, and the 3×20 box has just 2 solutions (one is shown in the figure, and the other one can be obtained from the solution shown by rotating, as a whole, the block consisting of the L, N, F, T, W, Y, and Z pentominoes).
A somewhat easier (more symmetrical) puzzle, the 8×8 rectangle with a 2×2 hole in the center, was solved by Dana Scott as far back as 1958. There are 65 solutions. Scott's algorithm was one of the first applications of a backtracking computer program. Variations of this puzzle allow the four holes to be placed in any position. One of the external links uses this rule. Most such patterns are solvable, with the exceptions of placing each pair of holes near two corners of the board in such a way that both corners could only be fitted by a P-pentomino, or forcing a T-pentomino or U-pentomino in a corner such that another hole is created.
Efficient algorithms have been described to solve such problems, for instance by Donald Knuth. Running on modern hardware, these pentomino puzzles can now be solved in mere seconds.
Filling boxes.
A pentacube is a polycube of five cubes. Of the 29 pentacubes, exactly twelve pentacubes are flat (1-layer) and correspond to the twelve pentominoes extruded to a depth of one square.
A pentacube puzzle or 3D pentomino puzzle, amounts to filling a 3-dimensional box with the 12 flat pentacubes, i.e. cover it without overlap and without gaps. Since each pentacube has a volume of 5 unit cubes, the box must have a volume of 60 units. Possible sizes are 2×3×10 (12 solutions), 2×5×6 (264 solutions) and 3×4×5 (3940 solutions). Following are one solution of each case.
Alternatively one could also consider combinations of five cubes that are themselves 3D, i.e., are not part of one layer of cubes. However, in addition to the 12 extruded pentominoes, 6 sets of chiral pairs and 5 pieces make total 29 pieces, resulting 145 cubes, which will not make a 3D box.
Board game.
There are board games of skill based entirely on pentominoes. Such games are often simply called "Pentominoes".
One of the games is played on an 8×8 grid by two or three players. Players take turns in placing pentominoes on the board so that they do not overlap with existing tiles and no tile is used more than once. The objective is to be the last player to place a tile on the board. This version of Pentominoes is called "Golomb's Game".
The two-player version has been weakly solved in 1996 by Hilarie Orman. It was proved to be a first-player win by examining around 22 billion board positions.
Pentominoes, and similar shapes, are also the basis of a number of other tiling games, patterns and puzzles. For example, the French board game "Blokus" is played with 4 opposing color sets of polyominoes. "In Blokus", each color begins with every pentomino (12), as well as every tetromino (5), every triomino (2), every domino (1), and the monomino (1). Like the game "Pentominoes", the goal is to use all of your tiles, and a bonus is given if the monomino is played on the very last move. The player with the fewest blocks remaining wins.
The game of "Cathedral" is also based on polyominoes.
Parker Brothers released a multi-player pentomino board game called "Universe" in 1966. Its theme is based on an outtake from the movie in which the astronaut (seen playing chess in the final version) is playing a two-player pentomino game against a computer. The front of the board game box features scenes from the movie as well as a caption describing it as the "game of the future". The game comes with 4 sets of pentominoes in red, yellow, blue, and white. The board has two playable areas: a base 10x10 area for two players with an additional 25 squares (two more rows of 10 and one offset row of 5) on each side for more than two players.
Game manufacturer Lonpos has a number of games that use the same pentominoes, but on different game planes. Their "101 Game" has a 5 x 11 plane. By changing the shape of the plane, thousands of puzzles can be played, although only a relatively small selection of these puzzles are available in print.
Literature.
The first pentomino problem, written by Henry Dudeney, was published in 1907 in the Canterbury Puzzles.
Pentominoes were featured in a prominent subplot of Arthur C. Clarke's novel "Imperial Earth", published in 1975. Clarke also wrote an essay in which he described the game and how he got hooked on it.
They were also featured in Blue Balliett's "Chasing Vermeer", which was published in 2003 and illustrated by Brett Helquist, as well as its sequels, "The Wright 3" and "The Calder Game".
In the New York Times crossword puzzle for June 27, 2012, the clue for an 11-letter word at 37 across was "Complete set of 12 shapes formed by this puzzle's black squares."
Architecture.
On several occasions pentominoes had been used as decoration elements for outer walls of Plattenbau buildings, mainly in Eastern Europe. The patterns used were based on solutions of the 6×10 case puzzle.

</doc>
<doc id="23716" url="https://en.wikipedia.org/wiki?curid=23716" title="Programmer">
Programmer

A programmer, computer programmer, developer, coder, or software engineer is a person who writes computer software. The term "computer programmer" can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (Assembly, COBOL, C, C++, C#, Java, Lisp, Python, etc.) is often prefixed to these titles, and those who work in a Web environment often prefix their titles with "Web". The term "programmer" can be used to refer to a software developer, Web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, or software analyst. However, members of these professions possess other software engineering skills, beyond programming; for this reason, the term "programmer", or "code monkey", is sometimes considered an insulting or derogatory oversimplification of these other professions. This has sparked much debate amongst developers, analysts, computer scientists, programmers, and outsiders who continue to be puzzled at the subtle differences in the definitions of these occupations.
History.
British countess and mathematician Ada Lovelace is often considered the first computer programmer, as she was the first to publish an algorithm intended for implementation on Charles Babbage's analytical engine, in October 1842, intended for the calculation of Bernoulli numbers. Because Babbage's machine was never completed to a functioning standard in her time, she never saw this algorithm run.
The first person to run a program on a functioning modern electronically based computer was computer scientist Konrad Zuse, in 1941.
The ENIAC programming team, consisting of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were the first regularly working programmers.
International Programmers' Day is celebrated annually on 7 January. In 2009, the government of Russia decreed a professional annual holiday known as Programmers' Day to be celebrated on 13 September (12 September in leap years). It had also been an unofficial international holiday before that.
Nature of the work.
Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.
Programmers work in many settings, including corporate information technology ("IT") departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is widely considered a profession (although some authorities disagree on the grounds that only careers with legal licensing requirements count as a profession).
Programmers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours, more complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer’s supervision.
Programmers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ is widely used for both scientific and business applications. Java, C#, VB and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as "Java programmers", or by the type of function they perform or environment in which they work: for example, "database programmers", "mainframe programmers", or Web developers.
When making changes to the source code that programs are made up of, programmers need to make other programmers aware of the task that the routine is to perform. They do this by inserting comments in the source code so that others can understand the program more easily and by documenting their code. To save work, programmers often use libraries of basic code that can be modified or customized for a specific application. This approach yields more reliable and consistent programs and increases programmers' productivity by eliminating some routine steps.
Testing and debugging.
Programmers test a program by running it and looking for bugs (errors). As they are identified, the programmer usually makes the appropriate corrections, then rechecks the program until an acceptably low level and severity of bugs remain. This process is called testing and debugging. These are important parts of every programmer's job. Programmers may continue to fix these problems throughout the life of a program. Updating, repairing, modifying, and expanding existing programs is sometimes called "maintenance programming". Programmers may contribute to user guides and online help, or they may work with technical writers to do such work.
Application versus system programming.
Computer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives.
Types of software.
Programmers in software development companies may work directly with experts from various fields to create software – either programs designed for specific clients or packaged software for general use – ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations – even small ones – have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is for example the case in research laboratories.
In some organizations, particularly small ones, people commonly known as "programmer analysts" are responsible for both the systems analysis and the actual programming work. The transition from a mainframe environment to one that is based primarily on personal computers (PCs) has blurred the once rigid distinction between the programmer and the user. Increasingly, adept end users are taking over many of the tasks previously performed by programmers. For example, the growing use of packaged software, such as spreadsheet and database management software packages, allows users to write simple programs to access data and perform calculations.
In addition, the rise of the Internet has made web development a huge part of the programming field. Currently more software applications are web applications that can be used by anyone with a web browser. Examples of such applications include the Google search service, the Outlook.com e-mail service, and the Flickr photo-sharing service.
Programming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers for writing the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.
Globalization.
Market changes in the UK.
According to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey. The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.
Market changes in the US.
Computer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.
Large companies claim there is a skills shortage with regard to programming talent. However, US programmers and unions counter that large companies are exaggerating their case in order to obtain cheaper programmers from developing countries and to avoid paying for training in very specific technologies.
Enrollment in computer-related degrees in US has dropped recently due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers. This situation has resulted in confusion about whether the US economy is entering a "post-information age" and the nature of US comparative advantages.

</doc>
<doc id="23719" url="https://en.wikipedia.org/wiki?curid=23719" title="Periodic table (large cells)">
Periodic table (large cells)

This page shows large-cell versions of the periodic table. For each element name, symbol, atomic number, and mean atomic mass value for the natural isotopic composition of each element are shown. The periodic table of the chemical elements is a tabular method of displaying the chemical elements.
The two layout forms originate from two graphic forms of presentation of the same periodic table. Historically, when the f-block was identified it was drawn below the existing table, with markings for its in-table location (this page uses dots or asterisks). Also, a common presentation is to put all 15 lanthanide and actinide columns below, while the f-block only has 14 columns. The fifteenth (rightmost) lanthanide and actinide are d-block elements, belonging to group 3, with scandium and yttrium.
Although precursors to this table exist, its invention is generally credited to Russian chemist Dmitri Mendeleev in 1869. Mendeleev intended the table to illustrate recurring ("periodic") trends in the properties of the elements. The layout of the table has been refined and extended over time, as new elements have been discovered, and new theoretical models have been developed to explain chemical behavior.
__NOTOC__

</doc>
