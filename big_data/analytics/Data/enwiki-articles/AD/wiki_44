<doc id="21190" url="https://en.wikipedia.org/wiki?curid=21190" title="Nomic">
Nomic

Nomic is a game created in 1982 by philosopher Peter Suber in which the rules of the game include mechanisms for the players to change those rules, usually beginning through a system of democratic voting. 
The initial ruleset was designed by Peter Suber, but first published in Douglas Hofstadter's column "Metamagical Themas" in "Scientific American" in June 1982. The column discussed Suber's then-upcoming book, "The Paradox of Self-Amendment", which was published some years later. Nomic now refers to a large number of games, all based on the initial ruleset.
The game is in some ways modeled on modern government systems. It demonstrates that in any system where rule changes are possible, a situation may arise in which the resulting laws are contradictory or insufficient to determine what is in fact legal. Because the game models (and exposes conceptual questions about) a legal system and the problems of legal interpretation, it is named after (""), Greek for "law".
While the victory condition in Suber's initial ruleset is the accumulation of 100 points by the roll of dice, he once said that "this rule is deliberately boring so that players will quickly amend it to please themselves." Players can change the rules to such a degree that points can become irrelevant in favor of a true currency, or make victory an unimportant concern. Any rule in the game, including the rules specifying the criteria for winning and even the rule that rules must be obeyed, can be changed. Any loophole in the ruleset, however, may allow the first player to discover it a chance to pull a "scam" and modify the rules to win the game. Complicating this process is the fact that Suber's initial ruleset allows for the appointment of judges to preside over issues of rule interpretation.
Gameplay.
The game can be played face-to-face with as many written notes as are required, or through any of a number of Internet media (usually an archived mailing list or Internet forum).
Initially, gameplay occurs in clockwise order, with each player taking a turn. In that turn, they propose a change in rules that all the other players vote on, and then roll a die to determine the number of points they add to their score. If this rule change is passed, it comes into effect at the end of their round. Any rule can be changed with varying degrees of difficulty, including the core rules of the game itself. As such, the gameplay may quickly change.
Under Suber's initial ruleset, rules are divided into two types: mutable and immutable. The main difference between these is that immutable rules must be changed into mutable rules (called "transmuting") before they can be modified or removed. Immutable rules also take precedence over mutable ones. A rule change may be:
Alternative starting rulesets exist for Internet and mail games, wherein gameplay occurs in alphabetical order by surname, and points added to the score are based on the success of a proposed rule change rather than random dice rolls.
Variants.
Not only can every aspect of the rules be altered in some way over the course of a game of Nomic, but myriad variants also exist: some that have themes, begin with a single rule, or begin with a dictator instead of a democratic process to validate rules. Others combine Nomic with an existing game (such as Monopoly, chess, or in one humorously paradoxical attempt, Mornington Crescent). There is even a version in which the players are games of Nomic themselves. Even more unusual variants include a ruleset in which the rules are hidden from players' view, and a game which, instead of allowing voting on rules, splits into two sub-games, one with the rule, and one without it.
Online versions often have initial rulesets where play is not turn-based; typically, players in such games may propose rule changes at any time, rather than having to wait for their turn.
One spin-off of a now-defunct Nomic (Nomic World) is called the Fantasy Rules Committee; it adds every legal rule submitted by a player to the ruleset until the players run out of ideas, after which all the "fantasy rules" are repealed and the game begins again.
Online play.
The game of Nomic is particularly suited to being played online, where all proposals and rules can be shared in web pages or email archives for ease of reference. Such games of Nomic sometimes last for a very long time – Agora has been running since 1993. The longevity of nomic games can pose a serious problem, in that the rulesets can grow so complex that current players do not fully understand them and prospective players are deterred from joining. One currently active game, BlogNomic, gets around this problem by dividing the game into "dynasties"; every time someone wins, a new dynasty begins, and all the rules except a privileged few are repealed. This keeps the game relatively simple and accessible. Nomicron (now defunct) was similar in that it had rounds — when a player won a round, a convention was started to plan for the next round. A young game of Nomic on reddit, nommit, uses a similar mechanism modeled on Nomicron's system.
Another facet of Nomic is the way in which the implementation of the rules affects the way the game of Nomic itself works. ThermodyNomic, for example, had a ruleset in which rule changes were carefully considered before implementation, and rules were rarely introduced which provide loopholes for the players to exploit. B Nomic, by contrast, was once described by one of its players as "the equivalent of throwing logical hand grenades."
This is essentially part of the differentiation between "procedural" games, where the aim (acknowledged or otherwise) is to tie the entire ruleset into a paradoxical condition during each turn (a player who has no legal move available wins), and "substantive" games, which try to avoid paradox and reward winning by achieving certain goals, such as attaining a given number of points.
While "Nomic" is traditionally capitalized as the proper name of the game it describes, it has also sometimes been used in a more informal way as a lowercased generic term, "nomic", referring to anything with Nomic-like characteristics, including games where the rules may be changed during play as well as non-gaming situations where it can be alleged that "rules lawyers" are tinkering with the process used to amend rules and policies (in an organization or community) in a manner akin to a game of Nomic.
Computerized version.
The idea of a computerized Nomic is that the rules should be interpreted by a computer, rather than by humans. This implies that the rules should be written in a language that a computer can understand, typically some sort of programming language, or Game Description Language. Nomyx is such an implementation.

</doc>
<doc id="21197" url="https://en.wikipedia.org/wiki?curid=21197" title="Nintendo">
Nintendo

Abandoning previous ventures in favor of toys in the 1960s, Nintendo then developed into a video game company in the 1970s, ultimately becoming one of the most influential in the industry and Japan's third most-valuable company with a market value of over $85 billion. Nintendo of America is also the majority owner of Major League Baseball's Seattle Mariners.
The word "Nintendo" can be roughly translated from Japanese to English as "leave luck to heaven". , Nintendo has cumulative sales of over 670.43 million hardware units and 4.23 billion software units. The company is known for creating some of the best known video game franchises, including "Mario", "Pokémon", and "The Legend of Zelda".
History.
1889–1956: As a card company.
Nintendo was founded as a card company in late 1889, later (1951) named "Nintendo Koppai" (Nintendo Playing Card Co. Ltd.), by Fusajiro Yamauchi. Based in Kyoto, Japan, the business produced and marketed a playing card game called ""Hanafuda"". The handmade cards soon became popular, and Yamauchi hired assistants to mass-produce cards to satisfy demand. Nintendo continues to manufacture playing cards in Japan and organizes its own contract bridge tournament called the "Nintendo Cup".
1956–1974: New ventures.
In 1956, Hiroshi Yamauchi, grandson of Fusajiro Yamauchi, visited the U.S. to talk with the United States Playing Card Company, the dominant playing card manufacturer there. He found that the biggest playing card company in the world was using only a small office. Yamauchi's realization that the playing card business had limited potential was a turning point. He then acquired the license to use Disney characters on playing cards to drive sales.
In 1963, Yamauchi renamed Nintendo Playing Card Co. Ltd. to Nintendo Co., Ltd. The company then began to experiment in other areas of business using newly injected capital during the period of time between 1963 and 1968. Nintendo set up a taxi company called "Daiya". This business was earlier successful however Nintendo was forced to sell it because problems with the labour unions were making it too expensive to run the service. It also set up a love hotel chain, a TV network, a food company (selling instant rice) and several other ventures. All of these ventures eventually failed, and after the 1964 Tokyo Olympics, playing card sales dropped, and Nintendo's stock price plummeted to its lowest recorded level of ¥60.
In 1966, Nintendo moved into the Japanese toy industry with the Ultra Hand, an extendable arm developed by its maintenance engineer Gunpei Yokoi in his free time. Yokoi was moved from maintenance to the new "Nintendo Games" department as a product developer. Nintendo continued to produce popular toys, including the Ultra Machine, Love Tester and the "Kousenjuu" series of light gun games. Despite some successful products, Nintendo struggled to meet the fast development and manufacturing turnaround required in the toy market, and fell behind the well-established companies such as Bandai and Tomy.
In 1973, its focus shifted to family entertainment venues with the Laser Clay Shooting System, using the same light gun technology used in Nintendo's "Kousenjuu" series of toys, and set up in abandoned bowling alleys. Following some success, Nintendo developed several more light gun machines (such as the light gun shooter game "Wild Gunman") for the emerging arcade scene. While the Laser Clay Shooting System ranges had to be shut down following excessive costs, Nintendo had found a new market.
1974–1978: Early electronic era.
Nintendo's first venture into the video gaming industry was securing rights to distribute the Magnavox Odyssey video game console in Japan in 1974. Nintendo began to produce its own hardware in 1977, with the Color TV-Game home video game consoles. Four versions of these consoles were produced, each including variations of a single game (for example, Color TV Game 6 featured six versions of "Light Tennis").
A student product developer named Shigeru Miyamoto was hired by Nintendo at this time. He worked for Yokoi, and one of his first tasks was to design the casing for several of the Color TV-Game consoles. Miyamoto went on to create, direct and produce some of Nintendo's most famous video games and become one of the most recognizable figures in the video game industry.
In 1975, Nintendo moved into the video arcade game industry with "EVR Race", designed by their first game designer, Genyo Takeda, and several more titles followed. Nintendo had some small success with this venture, but the release of "Donkey Kong" in 1981, designed by Miyamoto, changed Nintendo's fortunes dramatically. The success of the game and many licensing opportunities (such as ports on the Atari 2600, Intellivision and ColecoVision) gave Nintendo a huge boost in profit and in addition, the game also introduced an early iteration of Mario, then known in Japan as Jumpman, the eventual company mascot.
1979–2003: Success with video games.
In 1979, Gunpei Yokoi conceived the idea of a handheld video game, while observing a fellow bullet train commuter who passed the time by interacting idly with a portable LCD calculator, which gave birth to "Game & Watch". In 1980, Nintendo launched "Game & Watch"—a handheld video game series developed by Yokoi. These systems do not contain interchangeable cartridges and thus the hardware was tied to the game. The first Game & Watch game released, titled "Ball", was distributed worldwide. The modern "cross" D-pad design was developed in 1982, by Yokoi for a "Donkey Kong" version. Proven to be popular, the design was patented by Nintendo. It later earned a Technology & Engineering Emmy Award.
In 1983, Nintendo launched the Family Computer (colloquialized as "Famicom") home video game console in Japan, alongside ports of its most popular arcade titles. In 1985, a cosmetically reworked version of the system known outside Japan as the Nintendo Entertainment System or NES, launched in North America. The practice of bundling the system along with select games helped to make "Super Mario Bros." one of the best-selling video games in history.
In 1988, Gunpei Yokoi and his team at Nintendo R&D1 conceived the new Game Boy handheld system, with the purpose of merging the two very successful ideas of the Game & Watch's portability along with the NES's cartridge interchangeability. Nintendo released the Game Boy in Japan on April 21, 1989, and in North America on July 31, 1989. Nintendo of America president Minoru Arakawa managed a deal to bundle the popular third party game "Tetris" along with the Game Boy, and the pair launched as an instant success.
In 1989, Nintendo announced plans to release the successor to the Famicom, the Super Famicom. Based on a 16-bit processor, Nintendo boasted significantly superior hardware specifications of graphics, sound, and game speed over the original 8-bit Famicom. The system was also said to have backwards compatibility with Famicom games, though this feature was ultimately cut upon release. The Super Famicom was finally released relatively late to the market in Japan on November 21, 1990, and released as the Super Nintendo Entertainment System (officially abbreviated the Super NES or SNES and commonly shortened to Super Nintendo) in North America on August 23, 1991 and in Europe in 1992. Its main rival was the 16-bit Mega Drive, known in North America as Genesis, which had been advertised aggressively against the nascent 8-bit NES. A console war between Sega and Nintendo ensued during the early 1990s. From 1990 to 1992, Nintendo opened "World of Nintendo" shops in the United States where consumers could test and buy Nintendo products.
In August 1993, Nintendo announced the SNES's successor, code-named "Project Reality". Featuring 64-bit graphics, the new system was developed as a joint venture between Nintendo and North-American-based technology company Silicon Graphics. The system was announced to be released by the end of 1995, but was subsequently delayed. Meanwhile, Nintendo continued the Nintendo Entertainment System family with the release of the NES-101, a smaller redesign of the original NES. Nintendo also announced a CD drive peripheral called the Super NES CD-ROM Adapter, which was co-developed first by Sony with the name "Play Station" and then by Philips. Bearing prototypes and joint announcements at the Consumer Electronics Show, it was on track for a 1994 release, but was controversially cancelled.
During 1995, Nintendo announced that it had sold one billion game cartridges worldwide, one tenth of it being from the Mario franchise. Nintendo deemed 1994 the "Year of the Cartridge". To further their support for cartridges, Nintendo announced that Project Reality, which had now been renamed the Ultra 64, would not use a CD format as expected, but would rather use cartridges as its primary media format. Nintendo IRD general manager Genyo Takeda was impressed by video game development company Rare Ltd.'s progress with real-time 3D graphics technology, using state of the art Silicon Graphics workstations. As a result, Nintendo bought a 25% stake in the company, eventually expanding to 49%, and offered their catalogue of characters to create a CGI game around, making Rare Nintendo's first western-based second-party developer. Their first game as partners with Nintendo was "Donkey Kong Country". The game was a critical success and sold over eight million copies worldwide, making it the second best-selling game in the SNES library. In September 1994, Nintendo, along with six other video game giants including Sega, Electronic Arts, Atari, Acclaim, Philips, and 3DO approached the United States Senate and demanded a ratings system for video games to be enforced, which prompted the decision to create the Entertainment Software Rating Board.
Aiming to produce an affordable virtual reality console, Nintendo released the Virtual Boy in 1995, designed by Gunpei Yokoi. The console consists of a head-mounted semi-portable system with one red-colored screen for each of the user's eyes, featuring stereoscopic graphics. Games are viewed through a binocular eyepiece and controlled using an affixed gamepad. Critics were generally disappointed with the quality of the games and the red-colored graphics, and complained of gameplay-induced headaches. The system sold poorly and was quietly discontinued. Amid the system's failure, Yokoi retired from Nintendo. During the same year, Nintendo launched the Satellaview in Japan, a peripheral for the Super Famicom. The accessory allowed users to play video games via broadcast for a set period of time. Various games were made exclusively for the platform, as well as various remakes.
In 1996, Nintendo released the Ultra 64 as the Nintendo 64 in Japan and North America. The console was later released in Europe and Australia in 1997. Despite the limitations set by using cartridges, the technical specifications of the Nintendo 64 surpassed its competitors. With its market shares slipping to the Sega Saturn and partner-turned-rival Sony PlayStation, Nintendo revitalized its brand by launching a $185 million marketing campaign centered around the "Play it Loud" slogan. During the same year, Nintendo also released the Game Boy Pocket in Japan, a smaller version of the Game Boy that generated more sales for the platform. On October 4, 1997, famed Nintendo developer Gunpei Yokoi died in a car crash. In 1997, Nintendo released the SNS-101 (called Super Famicom Jr. in Japan), a smaller redesigned version of the Super Nintendo Entertainment System.
In 1998, the successor to the Game Boy, the Game Boy Color, was released. The system had improved technical specifications allowing it to run games made specifically for the system as well as games released for the Game Boy, albeit with added color. The Game Boy Camera and Printer were also released as accessories. In October 1998, Retro Studios was founded as an alliance between Nintendo and former Iguana Entertainment founder Jeff Spangenberg. Nintendo saw an opportunity for the new studio to create games for the upcoming GameCube targeting an older demographic, in the same vein as Iguana Entertainment's successful "" series for the Nintendo 64.
In 2001, just three years later, Nintendo introduced the redesigned Game Boy Advance. The same year, Nintendo also released the GameCube to lukewarm sales, and it ultimately failed to regain the market share lost by the Nintendo 64. When Yamauchi, the company's president since 1949, retired on May 24, 2002, Satoru Iwata succeeded as Nintendo's fourth president, becoming the first Nintendo president who was unrelated to the Yamauchi family through blood or marriage since its founding in 1889.
In 2003, Nintendo released the Game Boy Advance SP, its fourth handheld system.
2004–2011: Nintendo DS and Wii.
In 2004, Nintendo released the Nintendo DS, its fourth major handheld system. The DS is a dual screened handheld featuring touch screen capabilities, which respond to either a stylus or the touch of a finger. Former Nintendo president and now chairman Hiroshi Yamauchi was translated by GameScience as explaining, "If we can increase the scope of the industry, we can re-energise the global market and lift Japan out of depression - that is Nintendo's mission." Regarding lukewarm GameCube sales which had yielded the company's first reported operating loss in over 100 years, Yamauchi continued: "The DS represents a critical moment for Nintendo's success over the next two years. If it succeeds, we rise to the heavens, if it fails, we sink into hell." Thanks to titles such as Nintendogs and Mario Kart DS, the DS became a success. In 2005, Nintendo released the Game Boy Micro in North America, a redesign of the Game Boy Advance. The last system in the Game Boy line, it was also the smallest Game Boy, and the least successful. In the middle of 2005, Nintendo opened the Nintendo World Store in New York City, which would sell Nintendo games, present a museum of Nintendo history, and host public parties such as for product launches.
In the first half of 2006, Nintendo released the Nintendo DS Lite, a version of the original Nintendo DS with lighter weight, brighter screen, and better battery life. In addition to this streamlined design, its prolific subset of casual games appealed to the masses, such as the "Brain Age" series. Meanwhile, "New Super Mario Bros." provided a substantial addition to the "Mario" series when it was launched to the top of sales charts. The successful direction of the Nintendo DS had a big influence on Nintendo's next home console (including the common Nintendo Wi-Fi Connection), which had been codenamed "Revolution" and was now renamed to "Wii". In August 2006, Nintendo published ES, a now-dormant, open source research operating system project designed around web application integration but for no specific purpose.
In the latter half of 2006, Nintendo released the Wii as the backward-compatible successor to the GameCube. Based upon intricate Wii Remote motion controls and a balance board, the Wii inspired several new game franchises, some targeted at entirely new market segments of casual and fitness gaming. At more than 100 million units, the Wii is the best selling console of the seventh generation, regaining the market share lost during the tenures of the Nintendo 64 and the GameCube.
On May 1, 2007, Nintendo acquired an 80% stake on video game development company Monolith Soft, previously owned by Bandai Namco. Monolith Soft is best known for developing role-playing games such as the Xenosaga and Baten Kaitos series.
During the holiday season of 2008, Nintendo followed up the success of the DS with the release of the Nintendo DSi in Japan. The system features a more powerful CPU and more RAM, two cameras, one facing towards the player and one facing outwards, and had an online distribution store called DSiWare. The DSi was later released worldwide during 2009. In the latter half of 2009, Nintendo released the Nintendo DSi XL in Japan, a larger version of the DSi. This updated system was later released worldwide in 2010.
2011–present: Nintendo 3DS and Wii U.
In 2011, Nintendo released the Nintendo 3DS, based upon a glasses-free 3D display. In February 2012, Nintendo acquired Mobiclip, a France-based research and development company specialized in highly optimized software technologies such as video compression. The company's name was later changed to Nintendo European Research & Development. During the fourth quarter of 2012, Nintendo released the Wii U. It sold slower than expected, despite being the first eighth generation console. By September 2013, however, sales had rebounded. Intending to broaden the 3DS market, Nintendo released 2013's cost-reduced Nintendo 2DS. The 2DS is compatible with but lacks the 3DS's more expensive but cosmetic autostereoscopic 3D feature. Nintendo also released the Wii Mini, a cheaper and non-networked redesign of the Wii.
On September 25, 2013, Nintendo announced it had purchased a 28% stake in a Panasonic spin-off company called PUX Corporation. The company specializes in face and voice recognition technology, with which Nintendo intends to improve the usability of future game systems. Nintendo has also worked with this company in the past to create character recognition software for a Nintendo DS touchscreen. After announcing a 30% dive in profits for the April to December 2013 period, president Satoru Iwata announced he would take a 50% pay-cut, with other executives seeing reductions by 20%-30%.
In January 2015, Nintendo announced its exit from the Brazilian market after four years of distributing products in the country. Nintendo cited high import duties and lack of local manufacturing operation as reasons for leaving. Nintendo continues its partnership with Juegos de Video Latinoamérica to distribute products to the rest of Latin America.
On July 11, 2015, Iwata died from a bile duct tumor at the age of 55. Following his death, representative directors Genyo Takeda and Shigeru Miyamoto jointly led the company on an interim basis until the appointment of Tatsumi Kimishima as Iwata's successor on September 16, 2015. In addition to Kimishima's appointment, the company's management organization was also restructured—Miyamoto was named "Creative Fellow" and Takeda was named "Technology Fellow".
Future: Mobile and NX.
On March 17, 2015, Nintendo announced a partnership with Japanese mobile developer DeNA to produce games for smart devices.
On the same day, Nintendo announced a new "dedicated games platform with a brand new concept" with the codename "NX" that would be further revealed in 2016. On October 16, 2015, "The Wall Street Journal" relayed speculation from unnamed inside sources that, although the NX hardware specifications were unknown, it may be intended to feature "industry leading" hardware specifications and include both a console and a mobile unit that could either be used with the console or taken on the road for separate use. It was also reported that Nintendo had begun distributing software development kits (SDKs) for NX to third-party developers, with the unnamed source further speculating that these moves "that the company is on track to introduce as early as [2016." At an investor's meeting on April 27, 2016, Nintendo announced that the NX would be released worldwide in March 2017.
In May 2015, Universal Parks and Resorts announced that it was partnering with Nintendo to create attractions at Universal Parks based upon Nintendo properties.
In October 2015, Nintendo announced its first smartphone app, "Miitomo", would be released in March 2016.
Products.
Home consoles.
Color TV-Game.
Released in 1977, Japan's highest selling first generation console was Nintendo's Color TV Game, with 3 million units sold.
Nintendo Entertainment System.
The Nintendo Entertainment System (abbreviated as NES) is an 8-bit video game console, which released in North America in 1985, and in Europe throughout 1986 and 1987. The console was initially released in Japan as the Family Computer (abbreviated as Famicom) in 1983. The best-selling gaming console of its time, the NES helped revitalize the US video game industry following the video game crash of 1983. With the NES, Nintendo introduced a now-standard business model of licensing third-party developers, authorizing them to produce and distribute titles for Nintendo's platform. The NES was bundled with "Super Mario Bros.", one of the best-selling video games of all time, and received ports of Nintendo's most popular arcade titles. , Nintendo reports sales of 61.91 million NES hardware units and 500.01 million NES software units worldwide.
Super NES.
The Super Nintendo Entertainment System (abbreviated as the Super NES or SNES) is a 16-bit video game console, which was released in North America in 1991, and in Europe in 1992. The console was initially released in Japan in 1990 as the Super Famicom, officially adopting the colloquially abbreviated name of its predecessor. The console introduced advanced graphics and sound capabilities compared with other consoles at the time. Soon, the development of a variety of enhancement chips which were integrated onto each new game cartridge's circuit boards, progressed the SNES's competitive edge. While even crude three-dimensional graphics had previously rarely been seen on home consoles, the Super NES's enhancement chips suddenly enabled a new caliber of games containing increasingly sophisticated faux 3D effects as seen in 1991's "Pilotwings" and 1992's "Super Mario Kart". Argonaut Games developed the Super FX chip in order to replicate 3D graphics from their earlier Atari ST and Amiga "Starglider" series on the Super NES (more specifically, "Starglider 2"), starting with Star Fox in 1993. The SNES is the best-selling console of the 16-bit era although having experienced a relatively late start and fierce competition from Sega's Mega Drive/Genesis console. , Nintendo reports sales of 49.10 million SNES hardware units and 379.06 million SNES software units worldwide.
Nintendo 64.
The Nintendo 64 was released in 1996, featuring 3D polygon model rendering capabilities and built-in multiplayer for up to four players. The system's controller introduced the analog stick and later introduced the Rumble Pak, an accessory for the controller that produces force feedback with compatible games. Both are the first such features to have come to market for home console gaming and eventually became the "de facto" industry standard. Announced in 1995, prior to the console's 1996 launch, the 64DD ("DD" standing for "Disk Drive") was designed to enable the development of new genre of video games by way of 64 MB writable magnetic disks, video editing, and Internet connectivity. Eventually released only in Japan in 1999, the 64DD peripheral's commercial failure there resulted in only nine games being released and precluded further worldwide release.
GameCube.
The GameCube (officially called Nintendo GameCube, abbreviated NGC in Japan and GCN in North America) was released in 2001, in Japan and North America, and in 2002 worldwide. The sixth-generation console is the successor to the Nintendo 64 and competed with Sony's PlayStation 2, Microsoft's Xbox, and Sega's Dreamcast. The GameCube is the first Nintendo console to use optical discs as its primary storage medium. The discs are similar to the miniDVD format, but the system was not designed to play standard DVDs or audio CDs. Nintendo introduced a variety of connectivity options for the GameCube. The GameCube's game library has sparse support for Internet gaming, a feature that requires the use of the aftermarket Nintendo GameCube Broadband Adapter and Modem Adapter. The GameCube supports connectivity to the Game Boy Advance, allowing players to access exclusive in-game features using the handheld as a second screen and controller. , Nintendo reports sales of 21.74 million GameCube hardware units and 208.57 million GameCube software units worldwide.
Wii.
The Wii was released during the holiday season of 2006 worldwide. The system the Wii Remote controller, which can be used as a handheld pointing device and which detects movement in three dimensions. Another notable feature of the console is WiiConnect24, which enables it to receive messages and updates over the Internet while in standby mode. It also features a game download service, called "Virtual Console", which features emulated games from past systems. Since its release, the Wii has spawned many peripheral devices, including the Wii Balance Board and Motion Plus, and has had several hardware revisions. The "Wii Family Edition" variant is identical to the original model, but is designed to sit horizontally and removes the GameCube compatibility. The "Wii Mini" is a smaller, redesigned Wii which lacks GameCube compatibility, online connectivity, the SD card slot and Wi-Fi support, and has only one USB port unlike the previous models' two. , Nintendo reports sales of 101.06 million Wii hardware units and 895.22 million Wii software units worldwide, making it Nintendo's best-selling home video game console.
Wii U.
The Wii U, the successor to the Wii, was released during the holiday season of 2012 worldwide. The Wii U is the first Nintendo console to support high-definition graphics. The Wii U's primary controller is the Wii U GamePad, which features an embedded touchscreen. Each software title may be designed to utilize this touchscreen as being supplemental to the main TV, or as the only screen for Off-TV Play. The system supports most Wii controllers and accessories, and the more classically shaped Wii U Pro Controller. The system is backward compatible with Wii software and accessories; this mode also utilizes Wii-based controllers, and it optionally offers the GamePad as its primary Wii display and motion sensor bar. The console has various online services powered by Nintendo Network, including: the Nintendo eShop for online distribution of software and content; and Miiverse, a social network which can be variously integrated with games and applications. As of December 2014, worldwide Wii U sales had totaled 9.20 million hardware units and 52.87 million software units.
Handheld consoles.
Game & Watch.
Game & Watch is a line of handheld electronic games produced by Nintendo from 1980 to 1991. Created by game designer Gunpei Yokoi, each "Game & Watch" features a single game to be played on an LCD screen in addition to a clock, an alarm, or both. It was the earliest Nintendo product to garner major success.
Game Boy.
After the success of the "Game & Watch" series, Yokoi developed the Game Boy handheld console, which was released in 1989. Eventually becoming the best-selling handheld of all time, the Game Boy remained dominant for more than a decade, seeing critically and commercially popular games such as "Pokémon Yellow" released as late as 1998 in Japan and 2000 in Europe. Incremental updates of the Game Boy, including "Game Boy Pocket", "Game Boy Light" and "Game Boy Color", did little to change the original formula, though the latter introduced color graphics to the Game Boy line.
The first major update to its handheld line since 1989, Game Boy Advance features improved technical specifications similar to those of the SNES. The "Game Boy Advance SP" was the first revision to the GBA line and introduced screen lighting and a clam shell design, while later iteration, the "Game Boy Micro", brought a smaller form factor.
Nintendo DS.
Although originally advertised as an alternative to the Game Boy Advance, the Nintendo DS replaced the Game Boy line after its initial release in 2004. It was distinctive for its dual screens and a microphone, as well as a touch-sensitive lower screen. The "Nintendo DS Lite" brought a smaller form factor while the "Nintendo DSi" features larger screens and two cameras, and was followed by an even larger model, the "Nintendo DSi XL", with a 90% bigger screen.
Nintendo 3DS.
Further expanding the Nintendo DS line, the Nintendo 3DS uses the process of autostereoscopy to produce a stereoscopic three-dimensional effect without glasses. Released to major markets during 2011, the 3DS got off to a slow start, initially missing many key features that were promised before the system launched. Partially as a result of slow sales, Nintendo stock declined in value. Subsequent price cuts and game releases helped to boost 3DS and 3DS software sales and to renew investor confidence in the company. As of August 2013, the 3DS was the best selling console in the United States for four consecutive months. The "Nintendo 3DS XL" was introduced in August 2012 and includes a 90% larger screen, a 4GB SD card and extended battery life. In August 2013, Nintendo announced the cost-reduced "Nintendo 2DS", a version of the 3DS without the 3D display. It has a slate-like design as opposed to the hinged, clamshell design of its predecessors.
A hardware revision, "New Nintendo 3DS", was unveiled in August 2014. It is produced in a standard-sized model and a larger XL model; both models feature upgraded processors and additional RAM, an eye-tracking sensor to improve the stability of the autostereoscopic 3D image, colored face buttons, and near-field communication support for native use of Amiibo products. The standard-sized model also features slightly larger screens, and support for faceplate accessories.
Organization.
Marketing.
Nintendo of America has engaged in several high-profile marketing campaigns to define and position its brand. One of its earliest and most enduring slogans was "Now you're playing with power!", used first to promote its Nintendo Entertainment System. It modified the slogan to include "SUPER power" for the Super Nintendo Entertainment System, and "PORTABLE power" for the Game Boy. Its 1994 "Play It Loud!" campaign played upon teenage rebellion and fostered an edgy reputation. During the Nintendo 64 era, the slogan was "Get N or get out." During the GameCube era, the "Who Are You?" suggested a link between the games and the players' identities. The company promoted its Nintendo DS handheld with the tagline "Touching is Good." For the Wii, they used the "Wii would like to play" slogan to promote the console with the people who tried the games including "Super Mario Galaxy" and "Super Paper Mario". The Nintendo DS's successor, the Nintendo 3DS, used the slogan "Take a look inside". The Wii's successor, the Wii U, used the slogan "How U will play next."
International divisions.
Nintendo Co., Ltd. (NCL).
Headquartered in Kyoto, Japan since the beginning, Nintendo Co., Ltd. oversees the organization's global operations and manages Japanese operations specifically. The company's two major subsidiaries, Nintendo of America and Nintendo of Europe, manage operations in North America and Europe respectively. Nintendo Co., Ltd. moved from its original Kyoto location to a new office in Higashiyama-ku, Kyoto,; in 2000, this became the research and development building when the head office relocated to its location in Minami-ku, Kyoto.
Nintendo of America (NOA).
Nintendo's North American subsidiary is based in Redmond, Washington. Originally the NOA headquarters handled sales, marketing, and advertising. However, the office in Redwood City, California now directs those functions. The company maintains distribution centers in Atlanta (Nintendo Atlanta) and North Bend, Washington (Nintendo North Bend). The Nintendo North Bend facility processes more than 20,000 orders a day to Nintendo customers, which include retail stores that sell Nintendo products in addition to consumers who shop Nintendo's web site. Nintendo of America's Canadian branch, Nintendo of Canada, Ltd. (NOCL), is based in Vancouver, British Columbia with a distribution center in Toronto, Ontario.
Nintendo of Europe (NOE).
Nintendo's European subsidiary was established in June 1990, based in Großostheim, close to Frankfurt, Germany. The company handles operations in Europe and South Africa. Nintendo of Europe's United Kingdom branch handles operations in that country and in Ireland from its headquarters in Windsor, Berkshire. In June 2014, NOE initiated a reduction and consolidation process, yielding a combined 130 layoffs: the closing of its office and warehouse, and termination of all employment, in Großostheim; and the consolidation of all of those operations into, and terminating some employment at, its Frankfurt location.
Nintendo Australia (NAL).
Nintendo's Australian subsidiary is based in Melbourne, Victoria. It handles the publishing, distribution, sales and marketing of Nintendo products in Australia, New Zealand, and Oceania (Cook Islands, Fiji, New Caledonia, Papua New Guinea, Samoa, and Vanuatu). It also manufactures some Wii games locally. Nintendo Australia is also a third-party distributor of some titles from Rising Star Games, Namco Bandai Games Europe, Atlus, The Tetris Company, Sega, Tecmo Koei Games Europe and Capcom Europe.
iQue, Ltd..
A Chinese joint venture between its founder, Wei Yen, and Nintendo, manufactures and distributes official Nintendo consoles and games for the mainland Chinese market, under the iQue brand. The product lineup for the Chinese market is considerably different from that for other markets. For example, Nintendo's only console in China is the iQue Player, a modified version of the Nintendo 64. The company has not released its more modern GameCube or Wii to the market, although a version of the Nintendo 3DS XL was released in 2012. As of 2013, it is a 100% Nintendo-owned subsidiary.
Nintendo of Korea (NOK).
Nintendo's South Korean subsidiary was established on July 7, 2006.
Research and development.
Divisions.
Nintendo's internal research and development operations are divided into three main divisions, formed after corporate restructuring in September 2015: Nintendo Entertainment Planning & Development (or EPD), the main software development division of Nintendo, which focuses on internal-only video game development; Nintendo Platform Technology Development (or PTD), the main hardware development division of Nintendo, which focuses on home and handheld video game console development; and Nintendo Business Development (or NBD), which focuses on refining business strategy and is responsible for overseeing the smart device arm of the business.
Subsidiaries.
Although most of the Research & Development is being done in Japan, there are some R&D facilities in the United States and Europe that are focused on developing software and hardware technologies used in Nintendo products. Although they all are subsidiaries of Nintendo (and therefore first party), they are often referred to as external resources when being involved in joint development processes with Nintendo's internal developers by the Japanese personal involved. This can be seen in a variety of "Iwata asks..." interviews. Nintendo Software Technology (NST) and Nintendo Technology Development (NTD) are located in Redmond, Washington, USA, while Nintendo European Research & Development ("NERD") is located in Paris, France, and Nintendo Network Service Database (NSD) is located in Kyoto, Japan.
Most external first-party software development is done in Japan, since the only overseas subsidiary is Retro Studios in the United States. Although these studios are all subsidiaries of Nintendo, they are often referred to as external resources when being involved in joint development processes with Nintendo's internal developers by the Nintendo Entertainment Planning & Development (EPD) division. 1-UP Studio and Nd Cube are located in Tokyo, Japan, while Monolith Soft has one studio located in Tokyo and another in Kyoto. Retro Studios is located in Austin, Texas.
Partners.
Since the release of the Famicom/Nintendo Entertainment System, Nintendo has built up a large group of second-party development partners, through publishing agreements and development collaboration. Most of these external Nintendo projects are overseen by the Entertainment Planning & Development (EPD) division, formerly by the Nintendo Software Planning & Development (SPD) division.
Policy.
Content guidelines.
For many years, Nintendo had a policy of strict content guidelines for video games published on its consoles. Although Nintendo of Japan allowed graphic violence in its video games, nudity and sexuality were strictly prohibited. Former Nintendo president Hiroshi Yamauchi believed that if the company allowed the licensing of pornographic games, the company's image would be forever tarnished. Nintendo of America went further in that games released for Nintendo consoles could not feature nudity, sexuality, profanity (including racism, sexism or slurs), blood, graphic or domestic violence, drugs, political messages or religious symbols (with the exception of widely unpracticed religions, such as the Greek Pantheon). The Japanese parent company was concerned that it may be viewed as a "Japanese Invasion" by forcing Japanese community standards on North American and European children. Despite the strict guidelines, some exceptions have occurred: "Bionic Commando" (though swastikas were eliminated in the US version), "Smash TV" and ' contained human violence, the latter also containing implied sexuality and tobacco use; "River City Ransom" and ' contained nudity, and the latter also contained religious images, as did ' and "".
A known side effect of this policy was the Genesis version of "Mortal Kombat" selling over double the number of the Super NES version, mainly because Nintendo had forced publisher Acclaim to recolor the red blood to look like white sweat and replace some of the more gory graphics in its release of the game, making it less violent. By contrast, Sega allowed blood and gore to remain in the Genesis version (though a code was required to unlock the gore). Nintendo allowed the Super NES version of "Mortal Kombat II" to ship uncensored the following year with a content warning on the packaging.
In 1994 and 2003, when the ESRB and PEGI (respectively) video game ratings systems were introduced, Nintendo chose to abolish most of these policies in favor of consumers making their own choices about the content of the games they played. Today, changes to the content of games are done primarily by the game's developer or, occasionally, at the request of Nintendo. The only clear-set rule is that ESRB AO-rated games will not be licensed on Nintendo consoles in North America, a practice which is also enforced by Sony and Microsoft, its two greatest competitors in the present market. Nintendo has since allowed several mature-content games to be published on its consoles, including: "Perfect Dark", "Conker's Bad Fur Day", "Doom" and "Doom 64", "BMX XXX", the "Resident Evil" series, "Killer7", the "Mortal Kombat" series, ', "BloodRayne", "Geist", ', "Bayonetta 2", "Devil's Third" and '. Certain games have continued to be modified, however. For example, Konami was forced to remove all references to cigarettes in the 2000 Game Boy Color game "Metal Gear Solid" (although the previous NES version of "Metal Gear" and the subsequent GameCube game ' both included such references, as did Wii title "MadWorld"), and maiming and blood were removed from the Nintendo 64 port of "Cruis'n USA". Another example is in the Game Boy Advance game "Mega Man Zero 3", in which one of the bosses, called Hellbat Schilt in the Japanese and European releases, was renamed Devilbat Schilt in the North American localization. In North America releases of the "Mega Man Zero" games, enemies and bosses killed with a saber attack would not gush blood as they did in the Japanese versions. However, the release of the Wii has been accompanied by a number of even more controversial mature titles, such as "Manhunt 2", "No More Heroes", ' and "MadWorld", the latter three of which are published exclusively for the console. The Nintendo DS also has violent games, such as ', ' and its sequel, "Ultimate Mortal Kombat", and '.
License guidelines.
Nintendo of America also had guidelines before 1993 that had to be followed by its licensees to make games for the Nintendo Entertainment System, in addition to the above content guidelines. Guidelines were enforced through the 10NES lockout chip.
The last rule was circumvented in a number of ways; for example, Konami, wanting to produce more games for Nintendo's consoles, formed Ultra Games and later Palcom to produce more games as a technically different publisher. This disadvantaged smaller or emerging companies, as they could not afford to start additional companies. In another side effect, Square Co. (now Square Enix) executives have suggested that the price of publishing games on the Nintendo 64 along with the degree of censorship and control that Nintendo enforced over its games, most notably "Final Fantasy VI", were factors in switching its focus towards Sony's PlayStation console.
In 1993, a class action suit was taken against Nintendo under allegations that their lockout chip enabled unfair business practices. The case was settled, with the condition that California consumers were entitled to a $3 discount coupon for a game of Nintendo's choice.
Emulation.
Nintendo is opposed to any third-party emulation of its video games and consoles, stating that it is the single largest threat to the intellectual property rights of video game developers. However, emulators have been used by Nintendo and licensed third party companies as a means to re-release older games (through the Virtual Console). Nintendo remains the only modern console manufacturer that has not sued an emulator developer.
Seal of Quality.
The gold sunburst seal was first used by Nintendo of America, and later Nintendo of Europe. It is displayed on any game, system, or accessory licensed for use on one of its video game consoles, denoting the game has been properly approved by Nintendo. The seal is also displayed on any Nintendo-licensed merchandise, such as trading cards, game guides, or apparel, albeit with the words "Official Nintendo Licensed Product".
Sid Meier in 2008 cited the Seal of Quality as one of the three most important innovations in videogame history, as it helped set a standard for game quality that protected consumers from shovelware.
NTSC regions.
In NTSC regions, this seal is an elliptical starburst titled "Official Nintendo Seal." Originally, for NTSC countries, the seal was a large, black and gold circular starburst. The seal read as follows: "This seal is your assurance that NINTENDO has approved and guaranteed the quality of this product." This seal was later altered in 1988: "approved and guaranteed" was changed to "evaluated and approved." In 1989, the seal became gold and white, as it currently appears, with a shortened phrase, "Official Nintendo Seal of Quality." It was changed in 2003 to read "Official Nintendo Seal."
The seal currently reads:
PAL regions.
In PAL regions, the seal is a circular starburst titled, "Original Nintendo Seal of Quality." Text near the seal in the Australian Wii manual states:
Environmental record.
Nintendo has consistently been ranked last in Greenpeace's "Guide to Greener Electronics" due to Nintendo's failure to publish information. Similarly, they are ranked last in the Enough Project's "Conflict Minerals Company Rankings" due to Nintendo's refusal to respond to multiple requests for information.
Like many other electronics companies, Nintendo does offer a take-back recycling program which allows customers to mail in old products they no longer use; Nintendo of America claimed that it took in 548 tons of returned products in 2011, 98% of which was either reused or recycled.
Trademark.
During the peak of Nintendo's success in the video game industry in the 1990s, their name was ubiquitously used to refer to any video game console, regardless of the manufacturer. To prevent their trademark from becoming generic, Nintendo pushed usage of the term "games console", and succeeded in preserving their trademark.

</doc>
<doc id="21201" url="https://en.wikipedia.org/wiki?curid=21201" title="Nobel Prize">
Nobel Prize

The Nobel Prize (, ; Swedish definite form, singular: "Nobelpriset"; ) is a set of annual international awards bestowed in a number of categories by Swedish and Norwegian committees in recognition of academic, cultural and/or scientific advances.
The will of the Swedish inventor Alfred Nobel established the prizes in 1895. The prizes in Chemistry, Literature, Peace, Physics, and Physiology or Medicine were first awarded in 1901. The related Nobel Memorial Prize in Economic Sciences was established by Sweden's central bank in 1968. Medals made before 1980 were struck in 23 carat gold, and later from 18 carat green gold plated with a 24 carat gold coating. Between 1901 and 2015, the Nobel Prizes and the Prize in Economic Sciences were awarded 573 times to 900 people and organizations. With some receiving the Nobel Prize more than once, this makes a total of 870 individuals (822 men and 48 women) and 23 organizations.
The prize ceremonies take place annually in Stockholm, Sweden, except for the peace prize which is held in Oslo, Norway and each recipient, or laureate, receives a gold medal, a diploma and a sum of money that has been decided by the Nobel Foundation. (, each prize was worth SEK8 million or about , €0.93 million or £0.6 million.) The Nobel Prize is widely regarded as the most prestigious award available in the fields of literature, medicine, physics, chemistry, peace, and economics.
The Royal Swedish Academy of Sciences awards the Nobel Prize in Physics, the Nobel Prize in Chemistry, and the Nobel Memorial Prize in Economic Sciences; the Nobel Assembly at Karolinska Institutet awards the Nobel Prize in Physiology or Medicine; the Swedish Academy grants the Nobel Prize in Literature; and the Nobel Peace Prize is awarded not by a Swedish organisation but by the Norwegian Nobel Committee.
The prize is not awarded posthumously; however, if a person is awarded a prize and dies before receiving it, the prize may still be presented. Though the average number of laureates per prize increased substantially during the 20th century, a prize may not be shared among more than three people.
History.
Alfred Nobel () was born on 21 October 1833 in Stockholm, Sweden, into a family of engineers. He was a chemist, engineer, and inventor. In 1894, Nobel purchased the Bofors iron and steel mill, which he made into a major armaments manufacturer. Nobel also invented ballistite. This invention was a precursor to many smokeless military explosives, especially the British smokeless powder cordite. As a consequence of his patent claims, Nobel was eventually involved in a patent infringement lawsuit over cordite. Nobel amassed a fortune during his lifetime, with most of his wealth from his 355 inventions, of which dynamite is the most famous.
In 1888, Nobel was astonished to read his own obituary, titled "The merchant of death is dead", in a French newspaper. As it was Alfred's brother Ludvig who had died, the obituary was eight years premature. The article disconcerted Nobel and made him apprehensive about how he would be remembered. This inspired him to change his will. On 10 December 1896, Alfred Nobel died in his villa in San Remo, Italy, from a cerebral haemorrhage. He was 63 years old.
Nobel wrote several wills during his lifetime. He composed the last over a year before he died, signing it at the Swedish–Norwegian Club in Paris on 27 November 1895. To widespread astonishment, Nobel's last will specified that his fortune be used to create a series of prizes for those who confer the "greatest benefit on mankind" in physics, chemistry, physiology or medicine, literature, and peace. Nobel bequeathed 94% of his total assets, 31 million SEK (c. US$186 million, €150 million in 2008), to establish the five Nobel Prizes.
Because of scepticism surrounding the will, it was not until 26 April 1897 that it was approved by the Storting in Norway. The executors of Nobel's will, Ragnar Sohlman and Rudolf Lilljequist, formed the Nobel Foundation to take care of Nobel's fortune and organise the award of prizes.
Nobel's instructions named a Norwegian Nobel Committee to award the Peace Prize, the members of whom were appointed shortly after the will was approved in April 1897. Soon thereafter, the other prize-awarding organisations were designated or established. These were Karolinska Institutet on 7 June, the Swedish Academy on 9 June, and the Royal Swedish Academy of Sciences on 11 June. The Nobel Foundation reached an agreement on guidelines for how the prizes should be awarded; and, in 1900, the Nobel Foundation's newly created statutes were promulgated by King Oscar II. In 1905, the personal union between Sweden and Norway was dissolved.
Nobel Foundation.
The Nobel Foundation was founded as a private organisation on 29 June 1900. Its function is to manage the finances and administration of the Nobel Prizes. In accordance with Nobel's will, the primary task of the Foundation is to manage the fortune Nobel left. Robert and Ludwig Nobel were involved in the oil business in Azerbaijan and, according to Swedish historian E. Bargengren, who accessed the Nobel family archives, it was this "decision to allow withdrawal of Alfred's money from Baku that became the decisive factor that enabled the Nobel Prizes to be established". Another important task of the Nobel Foundation is to market the prizes internationally and to oversee informal administration related to the prizes. The Foundation is not involved in the process of selecting the Nobel laureates. In many ways, the Nobel Foundation is similar to an investment company, in that it invests Nobel's money to create a solid funding base for the prizes and the administrative activities. The Nobel Foundation is exempt from all taxes in Sweden (since 1946) and from investment taxes in the United States (since 1953). Since the 1980s, the Foundation's investments have become more profitable and as of 31 December 2007, the assets controlled by the Nobel Foundation amounted to 3.628 billion Swedish "kronor" (c. US$560 million).
According to the statutes, the Foundation consists of a board of five Swedish or Norwegian citizens, with its seat in Stockholm. The Chairman of the Board is appointed by the Swedish King in Council, with the other four members appointed by the trustees of the prize-awarding institutions. An Executive Director is chosen from among the board members, a Deputy Director is appointed by the King in Council, and two deputies are appointed by the trustees. However, since 1995, all the members of the board have been chosen by the trustees, and the Executive Director and the Deputy Director appointed by the board itself. As well as the board, the Nobel Foundation is made up of the prize-awarding institutions (the Royal Swedish Academy of Sciences, the Nobel Assembly at Karolinska Institute, the Swedish Academy, and the Norwegian Nobel Committee), the trustees of these institutions, and auditors.
First prizes.
Once the Nobel Foundation and its guidelines were in place, the Nobel Committees began collecting nominations for the inaugural prizes. Subsequently they sent a list of preliminary candidates to the prize-awarding institutions.
The Nobel Committee's Physics Prize shortlist cited Wilhelm Röntgen's discovery of X-rays and Philipp Lenard's work on cathode rays. The Academy of Sciences selected Röntgen for the prize. In the last decades of the 19th century, many chemists had made significant contributions. Thus, with the Chemistry Prize, the Academy "was chiefly faced with merely deciding the order in which these scientists should be awarded the prize." The Academy received 20 nominations, eleven of them for Jacobus van't Hoff. Van't Hoff was awarded the prize for his contributions in chemical thermodynamics.
The Swedish Academy chose the poet Sully Prudhomme for the first Nobel Prize in Literature. A group including 42 Swedish writers, artists and literary critics protested against this decision, having expected Leo Tolstoy to be awarded. Some, including Burton Feldman, have criticised this prize because they consider Prudhomme a mediocre poet. Feldman's explanation is that most of the Academy members preferred Victorian literature and thus selected a Victorian poet. The first Physiology or Medicine Prize went to the German physiologist and microbiologist Emil von Behring. During the 1890s, von Behring developed an antitoxin to treat diphtheria, which until then was causing thousands of deaths each year.
Second World War.
In 1938 and 1939, Adolf Hitler's Third Reich forbade three laureates from Germany (Richard Kuhn, Adolf Friedrich Johann Butenandt, and Gerhard Domagk) from accepting their prizes. Each man was later able to receive the diploma and medal. Even though Sweden was officially neutral during the Second World War, the prizes were awarded irregularly. In 1939, the Peace Prize was not awarded. No prize was awarded in any category from 1940–42, due to the occupation of Norway by Germany. In the subsequent year, all prizes were awarded except those for literature and peace.
During the occupation of Norway, three members of the Norwegian Nobel Committee fled into exile. The remaining members escaped persecution from the Germans when the Nobel Foundation stated that the Committee building in Oslo was Swedish property. Thus it was a safe haven from the German military, which was not at war with Sweden. These members kept the work of the Committee going, but did not award any prizes. In 1944, the Nobel Foundation, together with the three members in exile, made sure that nominations were submitted for the Peace Prize and that the prize could be awarded once again.
Prize in Economic Sciences.
In 1968, Sveriges Riksbank celebrated its 300th anniversary by donating a large sum of money to the Nobel Foundation to be used to set up a prize in honor of Nobel. The following year, the Nobel Memorial Prize in Economic Sciences was awarded for the first time. The Royal Swedish Academy of Sciences became responsible for selecting laureates. The first laureates for the Economics Prize were Jan Tinbergen and Ragnar Frisch "for having developed and applied dynamic models for the analysis of economic processes." Although not a Nobel Prize, it is intimately identified with the other awards; the laureates are announced with the Nobel Prize recipients, and the Prize in Economic Sciences is presented at the Swedish Nobel Prize Award Ceremony. The Board of the Nobel Foundation decided that after this addition, it would allow no further new prizes.
Award process.
The award process is similar for all of the Nobel Prizes; the main difference is in who can make nominations for each of them.
Nominations.
Nomination forms are sent by the Nobel Committee to about 3,000 individuals, usually in September the year before the prizes are awarded. These individuals are generally prominent academics working in a relevant area. Regarding the Peace Prize, inquiries are also sent to governments, former Peace Prize laureates and current or former members of the Norwegian Nobel Committee. The deadline for the return of the nomination forms is 31 January of the year of the award. The Nobel Committee nominates about 300 potential laureates from these forms and additional names. The nominees are not publicly named, nor are they told that they are being considered for the prize. All nomination records for a prize are sealed for 50 years from the awarding of the prize.
Selection.
The Nobel Committee then prepares a report reflecting the advice of experts in the relevant fields. This, along with the list of preliminary candidates, is submitted to the prize-awarding institutions. The institutions meet to choose the laureate or laureates in each field by a majority vote. Their decision, which cannot be appealed, is announced immediately after the vote. A maximum of three laureates and two different works may be selected per award. Except for the Peace Prize, which can be awarded to institutions, the awards can only be given to individuals. If the Peace Prize is not awarded, the money is split among the scientific prizes. This has happened 19 times so far.
Posthumous nominations.
Although posthumous nominations are not presently permitted, individuals who died in the months between their nomination and the decision of the prize committee were originally eligible to receive the prize. This has occurred twice: the 1931 Literature Prize awarded to Erik Axel Karlfeldt, and the 1961 Peace Prize awarded to UN Secretary General Dag Hammarskjöld. Since 1974, laureates must be thought alive at the time of the October announcement. There has been one laureate, William Vickrey, who in 1996 died after the prize (in Economics) was announced but before it could be presented. On 3 October 2011, the laureates for the Nobel Prize in Physiology or Medicine were announced; however, the committee was not aware that one of the laureates, Ralph M. Steinman, had died three days earlier. The committee was debating about Steinman's prize, since the rule is that the prize is not awarded posthumously. The committee later decided that as the decision to award Steinman the prize "was made in good faith", it would remain unchanged.
Recognition time lag.
Nobel's will provided for prizes to be awarded in recognition of discoveries made "during the preceding year". Early on, the awards usually recognised recent discoveries. However, some of these early discoveries were later discredited. For example, Johannes Fibiger was awarded the 1926 Prize for Physiology or Medicine for his purported discovery of a parasite that caused cancer. To avoid repeating this embarrassment, the awards increasingly recognised scientific discoveries that had withstood the test of time. According to Ralf Pettersson, former chairman of the Nobel Prize Committee for Physiology or Medicine, "the criterion 'the previous year' is interpreted by the Nobel Assembly as the year when the full impact of the discovery has become evident."
The interval between the award and the accomplishment it recognises varies from discipline to discipline. The Literature Prize is typically awarded to recognise a cumulative lifetime body of work rather than a single achievement. The Peace Prize can also be awarded for a lifetime body of work. For example, 2008 laureate Martti Ahtisaari was awarded for his work to resolve international conflicts. However, they can also be awarded for specific recent events. For instance, Kofi Annan was awarded the 2001 Peace Prize just four years after becoming the Secretary-General of the United Nations. Similarly Yasser Arafat, Yitzhak Rabin, and Shimon Peres received the 1994 award, about a year after they successfully concluded the Oslo Accords.
Although Nobel's will stated that prizes should be awarded for contributions made "during the preceding year", awards for physics, chemistry, and medicine are typically awarded once the achievement has been widely accepted. Sometimes, this takes decades – for example, Subrahmanyan Chandrasekhar shared the 1983 Physics Prize for his 1930s work on stellar structure and evolution. Not all scientists live long enough for their work to be recognised. Some discoveries can never be considered for a prize if their impact is realised after the discoverers have died.
Award ceremonies.
Except for the Peace Prize, the Nobel Prizes are presented in Stockholm, Sweden, at the annual Prize Award Ceremony on 10 December, the anniversary of Nobel's death. The recipients' lectures are normally held in the days prior to the award ceremony. The Peace Prize and its recipients' lectures are presented at the annual Prize Award Ceremony in Oslo, Norway, usually on 10 December. The award ceremonies and the associated banquets are typically major international events. The Prizes awarded in Sweden's ceremonies' are held at the Stockholm Concert Hall, with the Nobel banquet following immediately at Stockholm City Hall. The Nobel Peace Prize ceremony has been held at the Norwegian Nobel Institute (1905–1946), at the auditorium of the University of Oslo (1947–1989) and at Oslo City Hall (1990–present).
The highlight of the Nobel Prize Award Ceremony in Stockholm occurs when each Nobel laureate steps forward to receive the prize from the hands of the King of Sweden. In Oslo, the Chairman of the Norwegian Nobel Committee presents the Nobel Peace Prize in the presence of the King of Norway. At first King Oscar II did not approve of awarding grand prizes to foreigners. It is said that his mind changed once his attention had been drawn to the publicity value of the prizes for Sweden.
Nobel Banquet.
After the award ceremony in Sweden, a banquet is held in the Blue Hall at the Stockholm City Hall, which is attended by the Swedish Royal Family and around 1,300 guests.
The Nobel Peace Prize banquet is held in Norway at the Oslo Grand Hotel after the award ceremony. Apart from the laureate, guests include the President of the Storting, the Prime Minister, and, since 2006, the King and Queen of Norway. In total, about 250 guests attend.
Nobel lecture.
According to the statutes of the Nobel Foundation, each laureate is required to give a public lecture on a subject related to the topic of their prize. The Nobel lecture as a rhetorical genre took decades to reach its current format. These lectures normally occur during Nobel Week (the week leading up to the award ceremony and banquet, which begins with the laureates arriving in Stockholm and normally ends with the Nobel banquet), but this is not mandatory. The laureate is only obliged to give the lecture within six months of receiving the prize. Some have happened even later. For example, U.S. President Theodore Roosevelt received the Peace Prize in 1906 but gave his lecture in 1910, after his term in office. The lectures are organized by the same association which selected the laureates.
Prizes.
Medals.
It was announced on 30 May 2012 that the Nobel Foundation had awarded the contract for the production of the five (Swedish) Nobel Prize medals to Svenska Medalj AB. Formerly, the Nobel Prize medals were minted by Myntverket (the Swedish Mint) from 1902 to 2010. Myntverket, Sweden's oldest company, ceased operations in 2011 after 1,017 years. In 2011 the Mint of Norway, located in Kongsberg, made the medals. The Nobel Prize medals are registered trademarks of the Nobel Foundation. Each medal features an image of Alfred Nobel in left profile on the obverse. The medals for physics, chemistry, physiology or medicine, and literature have identical obverses, showing the image of Alfred Nobel and the years of his birth and death. Nobel's portrait also appears on the obverse of the Peace Prize medal and the medal for the Economics Prize, but with a slightly different design. For instance, the laureate's name is engraved on the rim of the Economics medal. The image on the reverse of a medal varies according to the institution awarding the prize. The reverse sides of the medals for chemistry and physics share the same design.
All medals made before 1980 were struck in 23 carat gold. Since then they have been struck in 18 carat green gold plated with 24 carat gold. The weight of each medal varies with the value of gold, but averages about for each medal. The diameter is and the thickness varies between and . Because of the high value of their gold content and tendency to be on public display, Nobel medals are subject to medal theft. During World War II, the medals of German scientists Max von Laue and James Franck were sent to Copenhagen for safekeeping. When Germany invaded Denmark, chemist George de Hevesy dissolved them in aqua regia (nitro-hydrochloric acid), to prevent confiscation by Nazi Germany and to prevent legal problems for the holders. After the war, the gold was recovered from solution, and the medals re-cast.
Diplomas.
Nobel laureates receive a diploma directly from the hands of the King of Sweden or, in the case of the peace prize, the Chairman of the Norwegian Nobel Committee. Each diploma is uniquely designed by the prize-awarding institutions for the laureates that receive them. The diploma contains a picture and text in Swedish which states the name of the laureate and normally a citation of why they received the prize. None of the Nobel Peace Prize laureates has ever had a citation on their diplomas.
Award money.
The laureates are given a sum of money when they receive their prizes, in the form of a document confirming the amount awarded. The amount of prize money depends upon how much money the Nobel Foundation can award each year. The purse has increased since the 1980s, when the prize money was 880 000 SEK (c. 2.6 million SEK, US$350 000 or €295,000 today) per prize. In 2009, the monetary award was 10 million SEK (US$1.4 million, €950,000). In June 2012, it was lowered to 8 million SEK. If there are two laureates in a particular category, the award grant is divided equally between the recipients. If there are three, the awarding committee has the option of dividing the grant equally, or awarding one-half to one recipient and one-quarter to each of the others. It is common for recipients to donate prize money to benefit scientific, cultural, or humanitarian causes.
Controversies and criticisms.
Controversial recipients.
Among other criticisms, the Nobel Committees have been accused of having a political agenda, and of omitting more deserving candidates. They have also been accused of Eurocentrism, especially for the Literature Prize.
Among the most criticised Nobel Peace Prizes was the one awarded to Henry Kissinger and Lê Đức Thọ. This led to the resignation of two Norwegian Nobel Committee members. Lê Đức Thọ declined the prize. Kissinger and Thọ were awarded the prize for negotiating a ceasefire between North Vietnam and the United States in January 1973. However, when the award was announced, both sides were still engaging in hostilities. Many critics were of the opinion that Kissinger was not a peace-maker but the opposite, responsible for widening the war.
Yasser Arafat, Shimon Peres, and Yitzhak Rabin received the Peace Prize in 1994 for their efforts in making peace between Israel and Palestine. Immediately after the award was announced, one of the five Norwegian Nobel Committee members denounced Arafat as a terrorist and resigned. Additional misgivings about Arafat were widely expressed in various newspapers.
Another controversial Peace Prize was that awarded to Barack Obama in 2009. Nominations had closed only eleven days after Obama took office as President, but the actual evaluation occurred over the next eight months. Obama himself stated that he did not feel deserving of the award, or worthy of the company it would place him in. Past Peace Prize laureates were divided, some saying that Obama deserved the award, and others saying he had not secured the achievements to yet merit such an accolade. Obama's award, along with the previous Peace Prizes for Jimmy Carter and Al Gore, also prompted accusations of a left-wing bias.
The award of the 2004 Literature Prize to Elfriede Jelinek drew a protest from a member of the Swedish Academy, Knut Ahnlund. Ahnlund resigned, alleging that the selection of Jelinek had caused "irreparable damage to all progressive forces, it has also confused the general view of literature as an art." He alleged that Jelinek's works were "a mass of text shovelled together without artistic structure." The 2009 Literature Prize to Herta Müller also generated criticism. According to "The Washington Post" many US literary critics and professors were ignorant of her work. This made those critics feel the prizes were too Eurocentric.
In 1949, the neurologist António Egas Moniz received the Physiology or Medicine Prize for his development of the prefrontal leucotomy. The previous year Dr. Walter Freeman had developed a version of the procedure which was faster and easier to carry out. Due in part to the publicity surrounding the original procedure, Freeman's procedure was prescribed without due consideration or regard for modern medical ethics. Endorsed by such influential publications as "The New England Journal of Medicine", leucotomy or "lobotomy" became so popular that about 5,000 lobotomies were performed in the United States in the three years immediately following Moniz's receipt of the Prize.
Overlooked achievements.
The Norwegian Nobel Committee confirmed that Mahatma Gandhi was nominated for the Peace Prize in 1937–39, 1947 and a few days before he was assassinated in January 1948. Later members of the Norwegian Nobel Committee expressed regret that he was not given the prize. Geir Lundestad, Secretary of Norwegian Nobel Committee in 2006 said, "The greatest omission in our 106 year history is undoubtedly that Mahatma Gandhi never received the Nobel Peace prize. Gandhi could do without the Nobel Peace prize. Whether Nobel committee can do without Gandhi is the question". In 1948, the year of Gandhi's death, the Nobel Committee declined to award a prize on the grounds that "there was no suitable living candidate" that year. Later, when the Dalai Lama was awarded the Peace Prize in 1989, the chairman of the committee said that this was "in part a tribute to the memory of Mahatma Gandhi." Other high profile individuals with widely recognised contributions to peace have been missed out. "Foreign Policy" lists Eleanor Roosevelt, Václav Havel, Ken Saro-Wiwa, Sari Nusseibeh and Corazon Aquino as people who "never won the prize, but should have.". The physicist Arnold Sommerfeld was nominated 81 times but an award was never made.
In 1965, UN Secretary General U Thant was informed by the Norwegian Permananent Representative to the UN that he would be awarded that year's prize and asked whether or not he would accept. He consulted staff and later replied that he would. At the same time, Chairman Gunnar Jahn of the Nobel Peace prize committee, lobbied heavily against giving U Thant the prize and the prize was at the last minute awarded to UNICEF. The rest of the committee all wanted the prize to go to U Thant, for his work in defusing the Cuban Missile Crisis, ending the war in the Congo, and his ongoing work to mediate an end to the Vietnam War. The disagreement lasted three years and in 1966 and 1967 no prize was given, with Gunnar Jahn effectively vetoing an award to U Thant.
The Literature Prize also has controversial omissions. Adam Kirsch has suggested that many notable writers have missed out on the award for political or extra-literary reasons. The heavy focus on European and Swedish authors has been a subject of criticism. The Eurocentric nature of the award was acknowledged by Peter Englund, the 2009 Permanent Secretary of the Swedish Academy, as a problem with the award and was attributed to the tendency for the academy to relate more to European authors. This tendency towards European authors still leaves a number of European writers on a list of notable writers that have been overlooked for the Literature Prize, including Europe's Leo Tolstoy, Anton Chekhov, J. R. R. Tolkien, Émile Zola, Marcel Proust, Vladimir Nabokov, James Joyce, August Strindberg, Simon Vestdijk, the New World's Jorge Luis Borges, Ezra Pound, John Updike, Arthur Miller, Mark Twain, and Africa's Chinua Achebe.
The strict rule against awarding a prize to more than three people is also controversial. When a prize is awarded to recognize an achievement by a team of more than three collaborators, one or more will miss out. For example, in 2002, the prize was awarded to Koichi Tanaka and John Fenn for the development of mass spectrometry in protein chemistry, an award that did not recognize the achievements of Franz Hillenkamp and Michael Karas of the Institute for Physical and Theoretical Chemistry at the University of Frankfurt. According to one of the nominees for the prize in physics, the three person limit deprived him and two other members of his team of the honor in 2013: the team of Carl Hagen, Gerald Guralnik, and Tom Kibble published a paper in 1964 that gave answers to how the Cosmos began, but did not share the 2013 Physics Prize awarded to Peter Higgs and François Englert, who had also published papers in 1964 concerning the subject. All five physicists arrived at the same conclusion, albeit from different angles. Hagen contends that an equitable solution is to either abandon the three limit restriction, or expand the time period of recognition for a given achievement to two years.
Similarly, the prohibition of posthumous awards fails to recognise achievements by an individual or collaborator who dies before the prize is awarded. In 1962, Francis Crick, James D. Watson, and Maurice Wilkins were awarded the Physiology or Medicine Prize for discovering the structure of DNA. Rosalind Franklin, a key contributor in that discovery, died of ovarian cancer four years earlier. The Economics Prize was not awarded to Fischer Black, who died in 1995, when his co-author Myron Scholes received the honor in 1997 for their landmark work on option pricing along with Robert C. Merton, another pioneer in the development of valuation of stock options. In the announcement of the award that year, the Nobel committee prominently mentioned Black's key role.
Political subterfuge may also deny proper recognition. Lise Meitner and Fritz Strassmann, who co-discovered nuclear fission along with Otto Hahn, may have been denied a share of Hahn's 1944 Nobel Chemistry Award due to having fled Germany when the Nazis came to power. The Meitner and Strassmann roles in the research was not fully recognized until years later, when they joined Hahn in receiving the 1966 Enrico Fermi Award.
Emphasis on discoveries over inventions.
Alfred Nobel left his fortune to finance annual prizes to be awarded "to those who, during the preceding year, shall have conferred the greatest benefit on mankind." He stated that the Nobel Prizes in Physics should be given "to the person who shall have made the most important 'discovery' or 'invention' within the field of physics." Nobel did not emphasise discoveries, but they have historically been held in higher respect by the Nobel Prize Committee than inventions: 77% of the Physics Prizes have been given to discoveries, compared with only 23% to inventions. Christoph Bartneck and Matthias Rauterberg, in papers published in "Nature" and "Technoetic Arts", have argued this emphasis on discoveries has moved the Nobel Prize away from its original intention of rewarding the greatest contribution to society.
Specially distinguished laureates.
Multiple laureates.
Four people have received two Nobel Prizes. Marie Curie received the Physics Prize in 1903 for her work on radioactivity and the Chemistry Prize in 1911 for the isolation of pure radium, making her the only person to win a Nobel Prize in two different sciences. Linus Pauling won the 1954 Chemistry Prize for his research into the chemical bond and its application to the structure of complex substances. Pauling also won the Peace Prize in 1962 for his activism against nuclear weapons, making him the only laureate of two unshared prizes. John Bardeen received the Physics Prize twice: in 1956 for the invention of the transistor and in 1972 for the theory of superconductivity. Frederick Sanger received the prize twice in Chemistry: in 1958 for determining the structure of the insulin molecule and in 1980 for inventing a method of determining base sequences in DNA.
Two organisations have received the Peace Prize multiple times. The International Committee of the Red Cross received it three times: in 1917 and 1944 for its work during the world wars; and in 1963 during the year of its centenary. The United Nations High Commissioner for Refugees has won the Peace Prize twice for assisting refugees: in 1954 and 1981.
Family laureates.
The Curie family has received the most prizes, with four prizes won by five individual laureates. Marie Curie received the prizes in Physics (in 1903) and Chemistry (in 1911). Her husband, Pierre Curie, shared the 1903 Physics prize with her. Their daughter, Irène Joliot-Curie, received the Chemistry Prize in 1935 together with her husband Frédéric Joliot-Curie. In addition, the husband of Marie Curie's second daughter, Henry Labouisse, was the director of UNICEF when it won the Nobel Peace Prize in 1965.
Although no family matches the Curie family's record, there have been several with two laureates. The husband-and-wife team of Gerty Cori and Carl Ferdinand Cori shared the 1947 Prize in Physiology or Medicine as did the husband-and-wife team of May-Britt Moser and Edvard Moser in 2014 (along with John O'Keefe). J. J. Thomson was awarded the Physics Prize in 1906 for showing that electrons are particles. His son, George Paget Thomson, received the same prize in 1937 for showing that they also have the properties of waves. William Henry Bragg and his son, William Lawrence Bragg, shared the Physics Prize in 1915 for inventing the X-ray spectrometer. Niels Bohr won the Physics prize in 1922, as did his son, Aage Bohr, in 1975. Manne Siegbahn, who received the Physics Prize in 1924, was the father of Kai Siegbahn, who received the Physics Prize in 1981. Hans von Euler-Chelpin, who received the Chemistry Prize in 1929, was the father of Ulf von Euler, who was awarded the Physiology or Medicine Prize in 1970. C.V. Raman won the Physics Prize in 1930 and was the uncle of Subrahmanyan Chandrasekhar, who won the same prize in 1983. Arthur Kornberg received the Physiology or Medicine Prize in 1959. Kornberg's son, Roger later received the Chemistry Prize in 2006. Jan Tinbergen, who won the first Economics Prize in 1969, was the brother of Nikolaas Tinbergen, who received the 1973 Physiology or Medicine Prize. Alva Myrdal, Peace Prize laureate in 1982, was the wife of Gunnar Myrdal who was awarded the Economics Prize in 1974. Economics laureates Paul Samuelson and Kenneth Arrow were brothers-in-law.
Cultural impact.
Being a symbol of scientific or literary achievement that's recognisable worldwide, the Nobel Prize is often depicted in fiction. This includes films like "The Prize" and "Nobel Son" about fictional Nobel laureates as well as fictionalized accounts of stories surrounding real prizes such as "Nobel Chor", a film based on the unsolved theft of Rabindranath Tagore's prize.
Refusals and constraints.
Two laureates have voluntarily declined the Nobel Prize. In 1964 Jean-Paul Sartre was awarded the Literature Prize but refused, stating, "A writer must refuse to allow himself to be transformed into an institution, even if it takes place in the most honourable form." The other is Lê Đức Thọ, chosen for the 1973 Peace Prize for his role in the Paris Peace Accords. He declined, stating that there was no actual peace in Vietnam.
During the Third Reich, Adolf Hitler hindered Richard Kuhn, Adolf Butenandt, and Gerhard Domagk from accepting their prizes. All of them were awarded their diplomas and gold medals after World War II. In 1958, Boris Pasternak declined his prize for literature due to fear of what the Soviet Union government might do if he travelled to Stockholm to accept his prize. In return, the Swedish Academy refused his refusal, saying "this refusal, of course, in no way alters the validity of the award." The Academy announced with regret that the presentation of the Literature Prize could not take place that year, holding it until 1989 when Pasternak's son accepted the prize on his behalf. Aung San Suu Kyi was awarded the Nobel Peace Prize in 1991, but her children accepted the prize because she had been placed under house arrest in Burma; Suu Kyi delivered her speech two decades later, in 2012. Liu Xiaobo was awarded the Nobel Peace Prize in 2010 while he and his wife were under house arrest in China as political prisoners.
Legacy.
The memorial symbol "Planet of Alfred Nobel" was opened in Dnipropetrovsk University of Economics and Law in 2008. On the globe there are 802 Nobel laureates' reliefs made of a composite alloy obtained when disposing of military strategic missiles.

</doc>
<doc id="21210" url="https://en.wikipedia.org/wiki?curid=21210" title="Niels Bohr">
Niels Bohr

Niels Henrik David Bohr (; 7 October 1885 – 18 November 1962) was a Danish physicist who made foundational contributions to understanding atomic structure and quantum theory, for which he received the Nobel Prize in Physics in 1922. Bohr was also a philosopher and a promoter of scientific research.
Bohr developed the Bohr model of the atom, in which he proposed that energy levels of electrons are discrete and that the electrons revolve in stable orbits around the atomic nucleus but can jump from one energy level (or orbit) to another. Although the Bohr model has been supplanted by other models, its underlying principles remain valid. He conceived the principle of complementarity: that items could be separately analysed in terms of contradictory properties, like behaving as a wave or a stream of particles. The notion of complementarity dominated Bohr's thinking in both science and philosophy.
Bohr founded the Institute of Theoretical Physics at the University of Copenhagen, now known as the Niels Bohr Institute, which opened in 1920. Bohr mentored and collaborated with physicists including Hans Kramers, Oskar Klein, George de Hevesy and Werner Heisenberg. He predicted the existence of a new zirconium-like element, which was named hafnium, after the Latin name for Copenhagen, where it was discovered. Later, the element bohrium was named after him.
During the 1930s, Bohr helped refugees from Nazism. After Denmark was occupied by the Germans, he had a famous meeting with Heisenberg, who had become the head of the German nuclear weapon project. In September 1943, word reached Bohr that he was about to be arrested by the Germans, and he fled to Sweden. From there, he was flown to Britain, where he joined the British Tube Alloys nuclear weapons project, and was part of the British mission to the Manhattan Project. After the war, Bohr called for international cooperation on nuclear energy. He was involved with the establishment of CERN and the Research Establishment Risø of the Danish Atomic Energy Commission, and became the first chairman of the Nordic Institute for Theoretical Physics in 1957.
Early years.
Bohr was born in Copenhagen, Denmark, on 7 October 1885, the second of three children of Christian Bohr, a professor of physiology at the University of Copenhagen, and Ellen Adler Bohr, who came from a wealthy Danish Jewish family prominent in banking and parliamentary circles. He had an elder sister, Jenny, and a younger brother Harald. Jenny became a teacher, while Harald became a mathematician and Olympic footballer who played for the Danish national team at the 1908 Summer Olympics in London. Bohr was a passionate footballer as well, and the two brothers played several matches for the Copenhagen-based Akademisk Boldklub (Academic Football Club), with Bohr as goalkeeper.
Bohr was educated at Gammelholm Latin School, starting when he was seven. In 1903, Bohr enrolled as an undergraduate at Copenhagen University. His major was physics, which he studied under Professor Christian Christiansen, the university's only professor of physics at that time. He also studied astronomy and mathematics under Professor Thorvald Thiele, and philosophy under Professor Harald Høffding, a friend of his father.
In 1905, a gold medal competition was sponsored by the Royal Danish Academy of Sciences and Letters to investigate a method for measuring the surface tension of liquids that had been proposed by Lord Rayleigh in 1879. This involved measuring the frequency of oscillation of the radius of a water jet. Bohr conducted a series of experiments using his father's laboratory in the university; the university itself had no physics laboratory. To complete his experiments, he had to make his own glassware, creating test tubes with the required elliptical cross-sections. He went beyond the original task, incorporating improvements into both Rayleigh's theory and his method, by taking into account the viscosity of the water, and by working with finite amplitudes instead of just infinitesimal ones. His essay, which he submitted at the last minute, won the prize. He later submitted an improved version of the paper to the Royal Society in London for publication in the "Philosophical Transactions of the Royal Society".
Harald became the first of the two Bohr brothers to earn a master's degree, which he earned for mathematics in April 1909. Niels took another nine months to earn his. Students had to submit a thesis on a subject assigned by their supervisor. Bohr's supervisor was Christiansen, and the topic he chose was the electron theory of metals. Bohr subsequently elaborated his master's thesis into his much-larger Doctor of Philosophy (dr. phil.) thesis. He surveyed the literature on the subject, settling on a model postulated by Paul Drude and elaborated by Hendrik Lorentz, in which the electrons in a metal are considered to behave like a gas. Bohr extended Lorentz's model, but was still unable to account for phenomena like the Hall effect, and concluded that electron theory could not fully explain the magnetic properties of metals. The thesis was accepted in April 1911, and Bohr conducted his formal defence on 13 May. Harald had received his doctorate the previous year. Bohr's thesis was groundbreaking, but attracted little interest outside Scandinavia because it was written in Danish, a Copenhagen University requirement at the time. In 1921, the Dutch physicist Hendrika Johanna van Leeuwen would independently derive a theorem from Bohr's thesis that is today known as the Bohr–van Leeuwen theorem.
In 1910, Bohr met Margrethe Nørlund, the sister of the mathematician Niels Erik Nørlund. Bohr resigned his membership in the Church of Denmark on 16 April 1912, and he and Margrethe were married in a civil ceremony at the town hall in Slagelse on 1 August. Years later, his brother Harald similarly left the church before getting married. Bohr and Margrethe had six sons. The oldest, Christian, died in a boating accident in 1934, and another, Harald, died from childhood meningitis. Aage Bohr became a successful physicist, and in 1975 was awarded the Nobel Prize in physics, like his father. became a physician; , a chemical engineer; and Ernest, a lawyer. Like his uncle Harald, Ernest Bohr became an Olympic athlete, playing field hockey for Denmark at the 1948 Summer Olympics in London.
Physics.
Bohr model.
In September 1911, Bohr, supported by a fellowship from the Carlsberg Foundation, travelled to England. At the time, it was where most of the theoretical work on the structure of atoms and molecules was being done. He met J. J. Thomson of the Cavendish Laboratory and Trinity College, Cambridge. He attended lectures on electromagnetism given by James Jeans and Joseph Larmor, and did some research on cathode rays, but failed to impress Thomson. He had more success with younger physicists like the Australian William Lawrence Bragg, and New Zealand's Ernest Rutherford, whose 1911 Rutherford model of the atom had challenged Thomson's 1904 plum pudding model. Bohr received an invitation from Rutherford to conduct post-doctoral work at Victoria University of Manchester, where Bohr met George de Hevesy and Charles Galton Darwin (whom Bohr referred to as "the grandson of the real Darwin").
Bohr returned to Denmark in July 1912 for his wedding, and travelled around England and Scotland on his honeymoon. On his return, he became a "privatdocent" at the University of Copenhagen, giving lectures on thermodynamics. Martin Knudsen put Bohr's name forward for a "docent", which was approved in July 1913, and Bohr then began teaching medical students. His three papers, which later became famous as "the trilogy", were published in "Philosophical Magazine" in July, September and November of that year. He adapted Rutherford's nuclear structure to Max Planck's quantum theory and so created his Bohr model of the atom.
Planetary models of atoms were not new, but Bohr's treatment was. Taking the 1912 paper by Darwin on the role of electrons in the interaction of alpha particles with a nucleus as his starting point, he advanced the theory of electrons travelling in orbits around the atom's nucleus, with the chemical properties of each element being largely determined by the number of electrons in the outer orbits of its atoms. He introduced the idea that an electron could drop from a higher-energy orbit to a lower one, in the process emitting a quantum of discrete energy. This became a basis for what is now known as the old quantum theory.
In 1885, Johann Balmer had come up with his Balmer series to describe the visible spectral lines of a hydrogen atoms:
where λ is the wavelength of the absorbed or emitted light and "R"H is the Rydberg constant. Balmer's formula was corroborated by the discovery of additional spectral lines, but for thirty years, no one could explain why it worked. In the first paper of his trilogy, Bohr was able to derive it from his model:
where "m"e is the electron's mass, "e" is its charge, "h" is Planck's constant and "Z" is the atom's atomic number (1 for hydrogen).
The model's first hurdle was the Pickering series, lines which did not fit Balmer's formula. When challenged on this by Alfred Fowler, Bohr replied that they were caused by ionised helium, helium atoms with only one electron. The Bohr model was found to work for such ions. Many older physicists, like Thomson, Rayleigh and Hendrik Lorentz, did not like the trilogy, but the younger generation, including Rutherford, David Hilbert, Albert Einstein, Enrico Fermi, Max Born and Arnold Sommerfeld saw it as a breakthrough. The trilogy's acceptance was entirely due to its ability to explain phenomena which stymied other models, and to predict results that were subsequently verified by experiments. Today, the Bohr model of the atom has been superseded, but is still the best known model of the atom, as it often appears in high school physics and chemistry texts.
Bohr did not enjoy teaching medical students. He decided to return to Manchester, where Rutherford had offered him a job as a reader in place of Darwin, whose tenure had expired. Bohr accepted. He took a leave of absence from the University of Copenhagen, which he started by taking a holiday in Tyrol with his brother Harald and aunt Hanna Adler. There, he visited the University of Göttingen and the Ludwig Maximilian University of Munich, where he met Sommerfeld and conducted seminars on the trilogy. The First World War broke out while they were in Tyrol, greatly complicating the trip back to Denmark and Bohr's subsequent voyage with Margrethe to England, where he arrived in October 1914. They stayed until July 1916, by which time he had been appointed to the Chair of Theoretical Physics at the University of Copenhagen, a position created especially for him. His docentship was abolished at the same time, so he still had to teach physics to medical students. New professors were formally introduced to King Christian X, who expressed his delight at meeting such a famous football player.
Institute of Physics.
In April 1917, Bohr began a campaign to establish an Institute of Theoretical Physics. He gained the support of the Danish government and the Carlsberg Foundation, and sizeable contributions were also made by industry and private donors, many of them Jewish. Legislation establishing the Institute was passed in November 1918. Now known as the Niels Bohr Institute, it opened its doors on 3 March 1921 with Bohr as its director. His family moved into an apartment on the first floor. Bohr's institute served as a focal point for researchers into quantum mechanics and related subjects in the 1920s and 1930s, when most of the world's best known theoretical physicists spent some time in his company. Early arrivals included Hans Kramers from the Netherlands, Oskar Klein from Sweden, George de Hevesy from Hungary, Wojciech Rubinowicz from Poland and Svein Rosseland from Norway. Bohr became widely appreciated as their congenial host and eminent colleague. Klein and Rosseland produced the Institute's first paper even before it opened.
The Bohr model worked well for hydrogen, but could not explain more complex elements. By 1919, Bohr was moving away from the idea that electrons orbited the nucleus, and he developed heuristics to describe them. The rare earth elements posed a particular classification problem for chemists, because they were so chemically similar. An important development came in 1924 with Wolfgang Pauli's discovery of the Pauli exclusion principle, which put Bohr's models on a firm theoretical footing. Bohr was then able to declare that the as-yet-undiscovered element 72 was not a rare earth element, but an element with chemical properties similar to those of zirconium. He was immediately challenged by the French chemist Georges Urbain, who claimed to have discovered a rare earth element 72, which he called "celtium". At the Institute in Copenhagen, Dirk Coster and George de Hevesy took up the challenge of proving Bohr right and Urbain wrong. Starting with a clear idea of the chemical properties of the unknown element greatly simplified the search process. They went through samples from Copenhagen's Museum of Mineralogy looking for a zirconium-like element, and soon found it. The element, which they named hafnium, "Hafnia" being the Latin name for Copenhagen, turned out to be more common than gold.
In 1922, Bohr was awarded the Nobel Prize in Physics "for his services in the investigation of the structure of atoms and of the radiation emanating from them". The award thus recognised both the Trilogy and his early leading work in the emerging field of quantum mechanics. For his Nobel lecture, Bohr gave his audience a comprehensive survey of what was then known about the structure of the atom, including the correspondence principle, which he had formulated. This states that the behaviour of systems described by quantum theory reproduces classical physics in the limit of large quantum numbers.
The discovery of Compton scattering by Arthur Holly Compton in 1923 convinced most physicists that light was composed of photons, and that energy and momentum were conserved in collisions between electrons and photons. In 1924, Bohr, Kramers and John C. Slater, an American physicist working at the Institute in Copenhagen, proposed the Bohr–Kramers–Slater theory (BKS). It was more a programme than a full physical theory, as the ideas it developed were not worked out quantitatively. BKS theory became the final attempt at understanding the interaction of matter and electromagnetic radiation on the basis of the old quantum theory, in which quantum phenomena were treated by imposing quantum restrictions on a classical wave description of the electromagnetic field.
Modelling atomic behaviour under incident electromagnetic radiation using "virtual oscillators" at the absorption and emission frequencies, rather than the (different) apparent frequencies of the Bohr orbits, led Max Born, Werner Heisenberg and Kramers to explore different mathematical models. They led to the development of matrix mechanics, the first form of modern quantum mechanics. The BKS theory also generated discussion of, and renewed attention to, difficulties in the foundations of the old quantum theory. The most provocative element of BKS – that momentum and energy would not necessarily be conserved in each interaction, but only statistically – was soon shown to be in conflict with experiments conducted by Walther Bothe and Hans Geiger. In light of these results, Bohr informed Darwin, "there is nothing else to do than to give our revolutionary efforts as honourable a funeral as possible."
Quantum mechanics.
The introduction of spin by George Uhlenbeck and Samuel Goudsmit in November 1925 was a milestone. The next month, Bohr travelled to Leiden to attend celebrations of the 50th anniversary of Hendrick Lorentz receiving his doctorate. When his train stopped in Hamburg, he was met by Wolfgang Pauli and Otto Stern, who asked for his opinion of the spin theory. Bohr pointed out that he had concerns about the interaction between electrons and magnetic fields. When he arrived in Leiden, Paul Ehrenfest and Albert Einstein informed Bohr that Einstein had resolved this problem using relativity. Bohr then had Uhlenbeck and Goudsmit incorporate this into their paper. Thus, when he met Werner Heisenberg and Pascual Jordan in Göttingen on the way back, he had become, in his own words, "a prophet of the electron magnet gospel".
Heisenberg first came to Copenhagen in 1924, then returned to Göttingen in June 1925, shortly thereafter developing the mathematical foundations of quantum mechanics. When he showed his results to Max Born in Göttingen, Born realised that they could best be expressed using matrices. This work attracted the attention of the British physicist Paul Dirac, who came to Copenhagen for six months in September 1926. Austrian physicist Erwin Schrödinger also visited in 1926. His attempt at explaining quantum physics in classical terms using wave mechanics impressed Bohr, who believed it contributed "so much to mathematical clarity and simplicity that it represents a gigantic advance over all previous forms of quantum mechanics".
When Kramers left the Institute in 1926 to take up a chair as professor of theoretical physics at the Utrecht University, Bohr arranged for Heisenberg to return and take Kramers's place as a "lektor" at the University of Copenhagen. Heisenberg worked in Copenhagen as a university lecturer and assistant to Bohr from 1926 to 1927.
Bohr became convinced that light behaved like both waves and particles, and in 1927, experiments confirmed the de Broglie hypothesis that matter (like electrons) also behaved like waves. He conceived the philosophical principle of complementarity: that items could have apparently mutually exclusive properties, such as being a wave or a stream of particles, depending on the experimental framework. He felt that it was not fully understood by professional philosophers.
In Copenhagen in 1927 Heisenberg developed his uncertainty principle, which Bohr embraced. In a paper he presented at the Volta Conference at Como in September 1927, he demonstrated that the uncertainty principle could be derived from classical arguments, without quantum terminology or matrices. Einstein preferred the determinism of classical physics over the probabilistic new quantum physics to which he himself had contributed. Philosophical issues that arose from the novel aspects of quantum mechanics became widely celebrated subjects of discussion. Einstein and Bohr had good-natured arguments over such issues throughout their lives.
In 1914, Carl Jacobsen, the heir to Carlsberg breweries, bequeathed his mansion to be used for life by the Dane who had made the most prominent contribution to science, literature or the arts, as an honorary residence (). Harald Høffding had been the first occupant, and upon his death in July 1931, the Royal Danish Academy of Sciences and Letters gave Bohr occupancy. He and his family moved there in 1932. He was elected president of the Academy on 17 March 1939.
By 1929, the phenomenon of beta decay prompted Bohr to again suggest that the law of conservation of energy be abandoned, but Enrico Fermi's hypothetical neutrino and the subsequent 1932 discovery of the neutron provided another explanation. This prompted Bohr to create a new theory of the compound nucleus in 1936, which explained how neutrons could be captured by the nucleus. In this model, the nucleus could be deformed like a drop of liquid. He worked on this with a new collaborator, the Danish physicist Fritz Kalckar, who died suddenly in 1938.
The discovery of nuclear fission by Otto Hahn in December 1938 (and its theoretical explanation by Lise Meitner) generated intense interest among physicists. Bohr brought the news to the United States where he opened the Fifth Washington Conference on Theoretical Physics with Fermi on 26 January 1939. When Bohr told George Placzek that this resolved all the mysteries of transuranic elements, Placzek told him that one remained: the neutron capture energies of uranium did not match those of its decay. Bohr thought about it for a few minutes and then announced to Placzek, Léon Rosenfeld and John Wheeler that "I have understood everything." Based on his liquid drop model of the nucleus, Bohr concluded that it was the uranium-235 isotope and not the more abundant uranium-238 that was primarily responsible for fission with thermal neutrons. In April 1940, John R. Dunning demonstrated that Bohr was correct. In the meantime, Bohr and Wheeler developed a theoretical treatment which they published in a September 1939 paper on "The Mechanism of Nuclear Fission".
Philosophy.
Bohr read the 19th-century Danish Christian existentialist philosopher, Søren Kierkegaard. Richard Rhodes argued in "The Making of the Atomic Bomb" that Bohr was influenced by Kierkegaard through Høffding. In 1909, Bohr sent his brother Kierkegaard's "Stages on Life's Way" as a birthday gift. In the enclosed letter, Bohr wrote, "It is the only thing I have to send home; but I do not believe that it would be very easy to find anything better ... I even think it is one of the most delightful things I have ever read." Bohr enjoyed Kierkegaard's language and literary style, but mentioned that he had some disagreement with Kierkegaard's philosophy. Some of Bohr's biographers suggested that this disagreement stemmed from Kierkegaard's advocacy of Christianity, while Bohr was an atheist.
There has been some dispute over the extent to which Kierkegaard influenced Bohr's philosophy and science. David Favrholdt argued that Kierkegaard had minimal influence over Bohr's work, taking Bohr's statement about disagreeing with Kierkegaard at face value, while Jan Faye argued that one can disagree with the content of a theory while accepting its general premises and structure.
Nazism and Second World War.
The rise of Nazism in Germany prompted many scholars to flee their countries, either because they were Jewish or because they were political opponents of the Nazi regime. In 1933, the Rockefeller Foundation created a fund to help support refugee academics, and Bohr discussed this programme with the President of the Rockefeller Foundation, Max Mason, in May 1933 during a visit to the United States. Bohr offered the refugees temporary jobs at the Institute, provided them with financial support, arranged for them to be awarded fellowships from the Rockefeller Foundation, and ultimately found them places at institutions around the world. Those that he helped included Guido Beck, Felix Bloch, James Franck, George de Hevesy, Otto Frisch, Hilde Levi, Lise Meitner, George Placzek, Eugene Rabinowitch, Stefan Rozental, Erich Ernst Schneider, Edward Teller, Arthur von Hippel and Victor Weisskopf.
In April 1940, early in the Second World War, Nazi Germany invaded and occupied Denmark. To prevent the Germans from discovering Max von Laue's and James Franck's gold Nobel medals, Bohr had de Hevesy dissolve them in aqua regia. In this form, they were stored on a shelf at the Institute until after the war, when the gold was precipitated and the medals re-struck by the Nobel Foundation. Bohr kept the Institute running, but all the foreign scholars departed.
Meeting with Heisenberg.
Bohr was aware of the possibility of using uranium-235 to construct an atomic bomb, referring to it in lectures in Britain and Denmark shortly before and after the war started, but he did not believe that it was technically feasible to extract a sufficient quantity of uranium-235. In September 1941, Heisenberg, who had become head of the German nuclear energy project, visited Bohr in Copenhagen. During this meeting the two men took a private moment outside, the content of which has caused much speculation, as both gave differing accounts.
According to Heisenberg, he began to address nuclear energy, morality and the war, to which Bohr seems to have reacted by terminating the conversation abruptly while not giving Heisenberg hints about his own opinions. Ivan Supek, one of Heisenberg's students and friends, claimed that the main subject of the meeting was Carl Friedrich von Weizsäcker, who had proposed trying to persuade Bohr to mediate peace between Britain and Germany.
In 1957, Heisenberg wrote to Robert Jungk, who was then working on the book "". Heisenberg explained that he had visited Copenhagen to communicate to Bohr the views of several German scientists, that production of a nuclear weapon was possible with great efforts, and this raised enormous responsibilities on the world's scientists on both sides. When Bohr saw Jungk's depiction in the Danish translation of the book, he drafted (but never sent) a letter to Heisenberg, stating that he never understood the purpose of Heisenberg's visit, was shocked by Heisenberg's opinion that Germany would win the war, and that atomic weapons could be decisive.
Michael Frayn's 1998 play "Copenhagen" explores what might have happened at the 1941 meeting between Heisenberg and Bohr. A BBC television film version of the play was first screened on 26 September 2002, with Stephen Rea as Bohr, and Daniel Craig as Heisenberg. The same meeting had previously been dramatised by the BBC's "Horizon" science documentary series in 1992, with Anthony Bate as Bohr, and Philip Anthony as Heisenberg.
Manhattan Project.
In September 1943, word reached Bohr and his brother Harald that the Nazis considered their family to be Jewish, since their mother, Ellen Adler Bohr, had been a Jew, and that they were therefore in danger of being arrested. The Danish resistance helped Bohr and his wife escape by sea to Sweden on 29 September. The next day, Bohr persuaded King Gustaf V of Sweden to make public Sweden's willingness to provide asylum to Jewish refugees. On 2 October 1943, Swedish radio broadcast that Sweden was ready to offer asylum, and the mass rescue of the Danish Jews by their countrymen followed swiftly thereafter. Some historians claim that Bohr's actions led directly to the mass rescue, while others say that, though Bohr did all that he could for his countrymen, his actions were not a decisive influence on the wider events. Eventually, over 7,000 Danish Jews escaped to Sweden.
When the news of Bohr's escape reached Britain, Lord Cherwell sent a telegram to Bohr asking him to come to Britain. Bohr arrived in Scotland on 6 October in a de Havilland Mosquito operated by the British Overseas Airways Corporation (BOAC). The Mosquitos were unarmed high-speed bomber aircraft that had been converted to carry small, valuable cargoes or important passengers. By flying at high speed and high altitude, they could cross German-occupied Norway, and yet avoid German fighters. Bohr, equipped with parachute, flying suit and oxygen mask, spent the three-hour flight lying on a mattress in the aircraft's bomb bay. During the flight, Bohr did not wear his flying helmet as it was too small, and consequently did not hear the pilot's intercom instruction to turn on his oxygen supply when the aircraft climbed to high altitude to overfly Norway. He passed out from oxygen starvation and only revived when the aircraft descended to lower altitude over the North Sea. Bohr's son Aage followed his father to Britain on another flight a week later, and became his personal assistant.
Bohr was warmly received by James Chadwick and Sir John Anderson, but for security reasons Bohr was kept out of sight. He was given an apartment at St James's Palace and an office with the British Tube Alloys nuclear weapons development team. Bohr was astonished at the amount of progress that had been made. Chadwick arranged for Bohr to visit the United States as a Tube Alloys consultant, with Aage as his assistant. On 8 December 1943, Bohr arrived in Washington, D.C., where he met with the director of the Manhattan Project, Brigadier General Leslie R. Groves, Jr. He visited Einstein and Pauli at the Institute for Advanced Study in Princeton, New Jersey, and went to Los Alamos in New Mexico, where the nuclear weapons were being designed. For security reasons, he went under the name of "Nicholas Baker" in the United States, while Aage became "James Baker". In May 1944 the Danish resistance newspaper De frie Danske reported that they had learned that 'the famous son of Denmark Professor Niels Bohr' in October the previous year had fled his country via Sweden to London and from there travelled to Moscow from where he could be assumed to support the war effort.
Bohr did not remain at Los Alamos, but paid a series of extended visits over the course of the next two years. Robert Oppenheimer credited Bohr with acting "as a scientific father figure to the younger men", most notably Richard Feynman. Bohr is quoted as saying, "They didn't need my help in making the atom bomb." Oppenheimer gave Bohr credit for an important contribution to the work on modulated neutron initiators. "This device remained a stubborn puzzle," Oppenheimer noted, "but in early February 1945 Niels Bohr clarified what had to be done."
Bohr recognised early that nuclear weapons would change international relations. In April 1944, he received a letter from Peter Kapitza, written some months before when Bohr was in Sweden, inviting him to come to the Soviet Union. The letter convinced Bohr that the Soviets were aware of the Anglo-American project, and would strive to catch up. He sent Kapitza a non-committal response, which he showed to the authorities in Britain before posting. Bohr met Churchill on 16 May 1944, but found that "we did not speak the same language". Churchill disagreed with the idea of openness towards the Russians to the point that he wrote in a letter: "It seems to me Bohr ought to be confined or at any rate made to see that he is very near the edge of mortal crimes."
Oppenheimer suggested that Bohr visit President Franklin D. Roosevelt to convince him that the Manhattan Project should be shared with the Soviets in the hope of speeding up its results. Bohr's friend, Supreme Court Justice Felix Frankfurter, informed President Roosevelt about Bohr's opinions, and a meeting between them took place on 26 August 1944. Roosevelt suggested that Bohr return to the United Kingdom to try to win British approval. When Churchill and Roosevelt met at Hyde Park on 19 September 1944, they rejected the idea of informing the world about the project, and the aide-mémoire of their conversation contained a rider that "enquiries should be made regarding the activities of Professor Bohr and steps taken to ensure that he is responsible for no leakage of information, particularly to the Russians".
In June 1950, Bohr addressed an "Open Letter" to the United Nations calling for international cooperation on nuclear energy.• </ref> In the 1950s, after the Soviet Union's first nuclear weapon test, the International Atomic Energy Agency was created along the lines of Bohr's suggestion. In 1957 he received the first ever Atoms for Peace Award.
Later years.
[of Arms of Niels Bohr.svg|thumb|right|240px|upright|Bohr's coat of arms. Argent, a tai-chi mandala [taiji or yin-yang symbol Gules and Sable. 
Motto: Contraria sunt complementa (opposites are complementary).]]
With the war now ended, Bohr returned to Copenhagen on 25 August 1945, and was re-elected President of the Royal Danish Academy of Arts and Sciences on 21 September. At a memorial meeting of the Academy on 17 October 1947 for King Christian X, who had died in April, the new king, Frederick IX, announced that he was conferring the Order of the Elephant on Bohr. This award was normally awarded only to royalty and heads of state, but the king said that it honoured not just Bohr personally, but Danish science. Bohr designed his own coat of arms which featured a taijitu (symbol of yin and yang) and a motto in , "opposites are complementary".
The Second World War demonstrated that science, and physics in particular, now required considerable financial and material resources. To avoid a brain drain to the United States, twelve European countries banded together to create CERN, a research organisation along the lines of the national laboratories in the United States, designed to undertake Big Science projects beyond the resources of any one of them alone. Questions soon arose regarding the best location for the facilities. Bohr and Kramers felt that the Institute in Copenhagen would be the ideal site. Pierre Auger, who organised the preliminary discussions, disagreed; he felt that both Bohr and his Institute were past their prime, and that Bohr's presence would overshadow others. After a long debate, Bohr pledged his support to CERN in February 1952, and Geneva was chosen as the site in October. The CERN Theory Group was based in Copenhagen until their new accommodation in Geneva was ready in 1957. Victor Weisskopf, who later became the Director General of CERN, summed up Bohr's role, saying that "there were other personalities who started and conceived the idea of CERN. The enthusiasm and ideas of the other people would not have been enough, however, if a man of his stature had not supported it."
Meanwhile, Scandinavian countries formed the Nordic Institute for Theoretical Physics in 1957, with Bohr as its chairman. He was also involved with the founding of the Research Establishment Risø of the Danish Atomic Energy Commission, and served as its first chairman from February 1956.
Bohr died of heart failure at his home in Carlsberg on 18 November 1962. He was cremated, and his ashes were buried in the family plot in the Assistens Cemetery in the Nørrebro section of Copenhagen, along with those of his parents, his brother Harald, and his son Christian. Years later, his wife's ashes were also interred there. On 7 October 1965, on what would have been his 80th birthday, the Institute for Theoretical Physics at the University of Copenhagen was officially renamed to what it had been called unofficially for many years: the Niels Bohr Institute.
Accolades.
Bohr received numerous honours and accolades. In addition to the Nobel Prize, he received the Hughes Medal in 1921, the Matteucci Medal in 1923, the Franklin Medal in 1926, the Copley Medal in 1938, the Order of the Elephant in 1947, the Atoms for Peace Award in 1957 and the Sonning Prize in 1961. In 1923 he became foreign member of the Royal Netherlands Academy of Arts and Sciences. The Bohr model's semicentennial was commemorated in Denmark on 21 November 1963 with a postage stamp depicting Bohr, the hydrogen atom and the formula for the difference of any two hydrogen energy levels: formula_3. Several other countries have also issued postage stamps depicting Bohr. In 1997, the Danish National Bank began circulating the 500-krone banknote with the portrait of Bohr smoking a pipe. An asteroid, 3948 Bohr, was named after him, as was the Bohr lunar crater, and bohrium, the chemical element with atomic number 107.

</doc>
<doc id="21211" url="https://en.wikipedia.org/wiki?curid=21211" title="National Football League">
National Football League

The National Football League (NFL) is a professional American football league consisting of 32 teams, divided equally between the National Football Conference (NFC) and the American Football Conference (AFC). The NFL is one of the four major professional sports leagues in North America, and the highest professional level of American football in the world. The NFL's 17-week regular season runs from the week after Labor Day to the week after Christmas, with each team playing 16 games and having one bye week. Following the conclusion of the regular season, six teams from each conference (four division winners and two wild card teams) advance to the playoffs, a single-elimination tournament culminating in the Super Bowl, played between the champions of the NFC and AFC.
The NFL was formed in 1920 as the American Professional Football Association (APFA) before renaming itself the National Football League for the 1922 season. The NFL agreed to merge with the American Football League (AFL) in 1966, and the first Super Bowl was held at the end of that season; the merger was completed in 1970. Today, the NFL has the highest average attendance (67,591) of any professional sports league in the world and is the most popular sports league in the United States. The Super Bowl is among the biggest club sporting events in the world and individual Super Bowl games account for many of the most watched television programs in American history, all occupying the Nielsen's Top 5 tally of the all-time most watched U.S. television broadcasts by 2015. The NFL's executive officer is the commissioner, who has broad authority in governing the league.
The team with the most NFL championships is the Green Bay Packers with thirteen; the team with the most Super Bowl championships is the Pittsburgh Steelers with six. The current NFL champions are the Denver Broncos, who defeated the Carolina Panthers 24–10 in Super Bowl 50.
History.
Founding and history.
On August 20, 1920, a meeting was held by representatives of the Akron Pros, Canton Bulldogs, Cleveland Indians, Rock Island Islanders and Dayton Triangles at the Jordan and Hupmobile auto showroom in Canton, Ohio. This meeting resulted in the formation of the American Professional Football Conference (APFC), a group who, according to the "Canton Evening Repository", intended to "raise the standard of professional football in every way possible, to eliminate bidding for players between rival clubs and to secure cooperation in the formation of schedules". Another meeting held on September 17, 1920 resulted in the renaming of the league to the American Professional Football Association (APFA). The league hired Jim Thorpe as its first president, and consisted of 14 teams. Only two of these teams, the Decatur Staleys (now the Chicago Bears) and the Chicago Cardinals (now the Arizona Cardinals), remain.
Although the league did not maintain official standings for its 1920 inaugural season and teams played schedules that included non-league opponents, the APFA awarded the Akron Pros the championship by virtue of their 8–0–3 (8 wins, 0 losses, and 3 ties) record. The first event occurred on September 26, 1920 when the Rock Island Independents defeated the non-league St. Paul Ideals 48-0 at Douglas Park. On October 3, 1920, the first full week of league play occurred. 
The following season resulted in the Chicago Staleys controversially winning the title over the Buffalo All-Americans. In 1922, the APFA changed its name to the National Football League (NFL).
In 1932, the season ended with the Chicago Bears (6-1-6) and the Portsmouth Spartans (6-1-4) tied for first in the league standings. At the time, teams were ranked on a single table and the team with the highest winning percentage (not including ties, which were not counted towards the standings) at the end of the season was declared the champion; the only tiebreaker was that in the event of a tie, if two teams played twice in a season, the result of the second game determined the title (the source of the 1921 controversy). This method had been used since the league's creation in 1920, but no situation had been encountered where two teams were tied for first. The league quickly determined that a playoff game between Chicago and Portsmouth was needed to decide the league's champion. The teams were originally scheduled to play the playoff game, officially a regular season game that would count towards the regular season standings, at Wrigley Field in Chicago, but a combination of heavy snow and extreme cold forced the game to be moved indoors to Chicago Stadium, which did not have a regulation-size football field. Playing with altered rules to accommodate the smaller playing field, the Bears won the game 9-0 and thus won the championship. Fan interest in the "de facto" championship game led the NFL, beginning in 1933, to split into two divisions with a championship game to be played between the division champions. The 1934 season also marked the first of 12 seasons in which African Americans were absent from the league. The "de facto" ban was rescinded in 1946, following public pressure and coinciding with the removal of a similar ban in Major League Baseball.
The NFL was always the foremost professional football league in the United States; it nevertheless faced a large number of rival professional leagues through the 1930s and 1940s. Rival leagues included at least three separate American Football Leagues and the All-America Football Conference (AAFC), on top of various regional leagues of varying caliber. Three NFL teams trace their histories to these rival leagues, including the Los Angeles Rams (who came from a 1936 iteration of the American Football League), the Cleveland Browns and San Francisco 49ers (the last two of which came from the AAFC). By the 1950s, the NFL had an effective monopoly on professional football in the United States; its only competition in North America was the professional Canadian football circuit, which formally became the Canadian Football League (CFL) in 1958. With Canadian football being a different football code than the American game, the CFL established a niche market in Canada and still survives as an independent league.
A new professional league, the fourth American Football League (AFL), began play in 1960. The upstart AFL began to challenge the established NFL in popularity, gaining lucrative television contracts and engaging in a bidding war with the NFL for free agents and draft picks. The two leagues announced a merger on June 8, 1966, to take full effect in 1970. In the meantime, the leagues would hold a common draft and championship game. The game, the Super Bowl, was held four times before the merger, with the NFL winning Super Bowl I and Super Bowl II, and the AFL winning Super Bowl III and Super Bowl IV. After the league merged, it was reorganized into two conferences: the National Football Conference (NFC), consisting of most of the pre-merger NFL teams, and the American Football Conference (AFC), consisting of all of the AFL teams as well as three pre-merger NFL teams.
Today, the NFL is considered the most popular sports league in North America; much of its growth is attributed to former Commissioner Pete Rozelle, who led the league from 1960 to 1989. Overall annual attendance increased from three million at the beginning of his tenure to seventeen million by the end of his tenure, and 400 million viewers watched 1989's Super Bowl XXIII. The NFL established NFL Properties in 1963. The league's licensing wing, NFL Properties earns the league billions of dollars annually; Rozelle's tenure also marked the creation of NFL Charities and a national partnership with United Way. Paul Tagliabue was elected as commissioner to succeed Rozelle; his seventeen-year tenure, which ended in 2006, was marked by large increases in television contracts and the addition of four expansion teams, as well as the introduction of league initiatives to increase the number of minorities in league and team management roles. The league's current Commissioner, Roger Goodell, has focused on reducing the number of illegal hits and making the sport safer, mainly through fining or suspending players who break rules. These actions are among many the NFL is taking to reduce concussions and improve player safety.
Season and playoff development.
From 1920 to 1934, the NFL did not have a set number of games for teams to play, instead setting a minimum. The league mandated a 12-game regular season for each team beginning in 1935, later shortening this to 11 games in 1937 and 10 games in 1943, mainly due to World War II. After the war ended, the number of games returned to 11 games in 1946 and to 12 in 1947. The NFL went to a 14-game schedule in 1961, which it retained until switching to the current 16-game schedule in 1978. Proposals to increase the regular season to 18 games have been made, but have been rejected in labor negotiations with the National Football League Players Association (NFLPA).
The NFL operated in a two-conference system from 1933 to 1966, where the champions of each conference would meet in the NFL Championship Game. If two teams tied for the conference lead, they would meet in a one-game playoff to determine the conference champion. In 1967, the NFL expanded from 15 teams to 16 teams. Instead of just evening out the conferences by adding the expansion New Orleans Saints to the seven-member Western Conference, the NFL realigned the conferences and split each into two four-team divisions. The four conference champions would meet in the NFL playoffs, a two-round playoff. The NFL also operated the Playoff Bowl (officially the Bert Bell Benefit Bowl) from 1960 to 1969. Effectively a third-place game, pitting the two conference runners-up against each other, the league considers Playoff Bowls to have been exhibitions rather than playoff games. The league discontinued the Playoff Bowl in 1970 due to its perception as a game for losers.
Following the addition of the former AFL teams into the NFL in 1970, the NFL split into two conferences with three divisions each. The expanded league, now with twenty-six teams, would also feature an expanded eight-team eight playoff, the participants being the three division champions from each conference as well as one 'wild card' team (the team with the best win percentage) from each conference. In 1978, the league added a second wild card team from each conference, bringing the total number of playoff teams to ten, and a further two wild card teams were added in 1990 to bring the total to twelve. When the NFL expanded to 32 teams in 2002, the league realigned, changing the division structure from three divisions in each conference to four divisions in each conference. As each division champion gets a playoff bid, the number of wild card teams from each conference dropped from three to two.
Corporate structure.
At the corporate level, the National Football League considers itself a trade association made up of and financed by its 32 member teams. Up until 2015, the league was an unincorporated nonprofit 501(c)(6) association. Section 501(c)(6) of the Internal Revenue Code provides an exemption from federal income taxation for "Business leagues, chambers of commerce, real-estate boards, boards of trade, or professional football leagues (whether or not administering a pension fund for football players), not organized for profit and no part of the net earnings of which inures to the benefit of any private shareholder or individual.". In contrast, each individual team (except the non-profit Green Bay Packers) is subject to tax because they make a profit. The NFL gave up the tax exempt status in 2015 following public criticism; in a letter to the club owners, Commissioner Roger Goodell labeled it a "distraction", saying "the effects of the tax exempt status of the league office have been mischaracterized repeatedly in recent years... Every dollar of income generated through television rights fees, licensing agreements, sponsorships, ticket sales, and other means is earned by the 32 clubs and is taxable there. This will remain the case even when the league office and Management Council file returns as taxable entities, and the change in filing status will make no material difference to our business". As a result, the league office might owe around US$10 million, but is no longer required to disclose the salaries of its executive officers.
The league has three defined officers: the commissioner, secretary, and treasurer. Each conference has one officer, the president. The commissioner is elected by affirmative vote of two-thirds or 18 (whichever is greater) of the members of the league, while the president of each conference is elected by an affirmative vote of three-fourths or ten of the conference members. The commissioner appoints the secretary and treasurer and has broad authority in disputes between clubs, players, coaches, and employees. He is the "principal executive officer" of the NFL and also has authority in hiring league employees, negotiating television contracts, disciplining individuals that own part or all of an NFL team, clubs, or employed individuals of an NFL club if they have violated league bylaws or committed "conduct detrimental to the welfare of the League or professional football". The commissioner can, in the event of misconduct by a party associated with the league, suspend individuals, hand down a fine of up to US$500,000, cancel contracts with the league, and award or strip teams of draft picks.
In extreme cases, the commissioner can offer recommendations to the NFL's Executive Committee up to and including the "cancellation or forfeiture" of a club's franchise or any other action he deems necessary. The commissioner can also issue sanctions up to and including a lifetime ban from the league if an individual connected to the NFL has bet on games or failed to notify the league of conspiracies or plans to bet on or fix games. The current Commissioner of the National Football League is Roger Goodell, who was elected in 2006 after Paul Tagliabue, the previous commissioner, retired.
Clubs.
The NFL consists of 32 clubs divided into two conferences of sixteen teams each. Each conference is divided into four divisions of four clubs each. During the regular season, each team is allowed a maximum of 53 players on its roster; only 46 of these may be active (eligible to play) on game days. Each team can also have an 10-player practice squad separate from its main roster, but the practice squad may only be composed of players who were not active for at least nine games in any of their seasons in the league. A player can only be on a practice squad for a maximum of three seasons.
Each NFL club is granted a franchise, the league's authorization for the team to operate in its home city. This franchise covers 'Home Territory' (the 75 miles surrounding the city limits, or, if the team is within 100 miles of another league city, half the distance between the two cities) and 'Home Marketing Area' (Home Territory plus the rest of the state the club operates in, as well as the area the team operates its training camp in for the duration of the camp). Each NFL member has the exclusive right to host professional football games inside its Home Territory and the exclusive right to advertise, promote, and host events in its Home Marketing Area. There are several exceptions to this rule, mostly relating to teams with close proximity to each other: the San Francisco 49ers and Oakland Raiders only have exclusive rights in their cities and share rights outside of it; and teams that operate in the same city (e.g. New York Giants and New York Jets) or the same state (e.g. California, Florida, and Texas) share the rights to the city's Home Territory and the state's Home Marketing Area, respectively.
Every NFL team is based in the contiguous United States. Although no team is based in a foreign country, the Buffalo Bills played one home game every season at Rogers Centre in Toronto, Ontario, as part of the Bills Toronto Series from until , and the Jacksonville Jaguars will play one home game a year from 2013 to 2016 at Wembley Stadium in London, England, as part of the NFL International Series. Mexico also has hosted an NFL regular-season game, a 2005 game between the San Francisco 49ers and Arizona Cardinals dubbed "Fútbol Americano", and 39 international preseason games were played from 1986 to 2005 as part of the American Bowl series. The Raiders and Houston Texans will play a game in Mexico City at Estadio Azteca on November 21, 2016.
The Dallas Cowboys, at approximately US $4 billion, are the most valuable NFL franchise, and are the most valuable sports team in the world, according to Forbes. Also, all 32 NFL teams rank among the top 50 most valuable sports teams in the world; and fourteen of the NFL's owners are listed on the Forbes 400, the most of any sports league or organization.
Season format.
The NFL season format consists of a four-week preseason, a seventeen-week regular season (each team plays 16 games), and a twelve-team single-elimination playoff culminating in the Super Bowl, the league's championship game.
Preseason.
The NFL preseason begins with the Pro Football Hall of Fame Game, played at Fawcett Stadium in Canton. Each NFL team is required to schedule four preseason games, two of which must be at its home stadium, but the teams involved in the Hall of Fame game, as well as any teams playing in an American Bowl game, play five preseason games. Preseason games are exhibition matches and do not count towards regular-season totals. Because the preseason does not count towards standings, teams do not focus on winning games; instead, they are used by coaches to evaluate their teams and by players to show their performance, both to their current team and to other teams if they get cut. The quality of preseason games has been criticized by some fans, who dislike having to pay full price for exhibition games, as well as by some players and coaches, who dislike the risk of injury the games have, while others have felt the preseason is a necessary part of the NFL season.
Regular season.
This chart of the 2015 season standings displays an application of the NFL scheduling formula. The Broncos in 2015 (highlighted in green) finished in first place in the AFC West. Thus, in 2016, the Broncos will play two games against each of its division rivals (highlighted in light blue), one game against each team in the AFC South and NFC South (highlighted in yellow), and one game each against the first-place finishers in the AFC East and AFC North (highlighted in orange).
Currently, the thirteen opponents each team faces over the 16-game regular season schedule are set using a pre-determined formula:
The National Football League runs a seventeen-week, 256-game regular season. Since 2001, the season has begun the week after Labor Day and concluded the week after Christmas. The opening game of the season is normally a primetime home game for the league's defending champion.
Most NFL games are played on Sundays, with a Monday night game typically held at least once a week and Thursday night games occurring on most weeks as well. NFL games are not normally played on Fridays or Saturdays until late in the regular season, as federal law prohibits professional football leagues from competing with college or high school football. Because high school and college teams typically play games on Friday and Saturday, respectively, the NFL cannot hold games on those days until the third Friday in December. NFL games are rarely scheduled for Tuesday or Wednesday, and those days have only been used twice since 1948: in 2010, when a Sunday game was rescheduled to Tuesday due to a blizzard, and in 2012, when the Kickoff game was moved from Thursday to Wednesday to avoid conflict with the Democratic National Convention.
NFL regular season matchups are determined according to a scheduling formula. Within a division, all four teams play fourteen out of their sixteen games against common opponents - two games (home and away) are played against the other three teams in the division, while one game is held against all the members of a division from the NFC and a division from the AFC division as determined by a rotating cycle (three years for the conference the team is in, and four years in the conference they are not in). The other two games are intraconference games, determined by the standings of the previous year - for example, if a team finishes first in its division, it will play two other first-place teams in its conference, while a team that finishes last would play two other last-place teams in the conference. In total, each team plays sixteen games and has one bye week, where they do not play any games.
Although the teams any given club will play are known by the end of the previous year's regular season, the exact dates, times, and home/away status for NFL games are not determined until much later because the league has to account for, among other things, the Major League Baseball postseason and local events that could pose a scheduling conflict with NFL games. During the 2010 season, over 500,000 potential schedules were created by computers, 5,000 of which were considered "playable schedules" and were reviewed by the NFL's scheduling team. After arriving at what they felt was the best schedule out of the group, nearly 50 more potential schedules were developed to try and ensure that the chosen schedule would be the best possible one.
Postseason.
Following the conclusion of the regular season, a twelve-team single elimination tournament, the NFL Playoffs, is held. Six teams are selected from each conference: the winners of each of the four divisions as well as two wild card teams (the two remaining teams with the best overall record). These teams are seeded according to overall record, with the division champions always ranking higher than either of the wild card teams. The top two teams (seeded one and two) from each conference are awarded a bye week, while the remaining four teams (seeded 3-6) from each conference compete in the first round of the playoffs, the Wild Card round, with the third seed competing against the sixth seed and the fourth seed competing against the fifth seed. The winners of the Wild Card round advance to the Divisional Round, which matches the lower seeded team against the first seed and the higher seeded team against the second seed. The winners of those games then compete in the Conference Championships, with the higher remaining seed hosting the lower remaining seed. The AFC and NFC champions then compete in the Super Bowl to determine the league champion.
The only other postseason event hosted by the NFL is the Pro Bowl, the league's all-star game. The Pro Bowl is held the week before the Super Bowl. The Pro Bowl is not considered as competitive as a regular-season game because the biggest concern of teams is to avoid injuries to the star players.
Trophies and awards.
Team trophies.
The National Football League has used three different trophies to honor its champion over its existence. The first trophy, the Brunswick-Balke Collender Cup, was donated to the NFL (then APFA) in 1920 by the Brunswick-Balke Collender Corporation. The trophy, the appearance of which is only known by its description as a "silver loving cup", was intended to be a traveling trophy and not to become permanent until a team had won at least three titles. The league awarded it to the Akron Pros, champions of the inaugural 1920 season; however, the trophy was discontinued and its current whereabouts are unknown.
A second trophy, the Ed Thorp Memorial Trophy, was issued by the NFL from 1934 to 1969. The trophy's namesake, Ed Thorp, was a referee in the league and a friend to many early league owners; upon his death in 1934, the league created the trophy to honor him. In addition to the main trophy, which would be in the possession of the current league champion, the league issued a smaller replica trophy to each champion, who would maintain permanent control over it. The current location of the Ed Thorp Memorial Trophy, like that of its predecessor, is unknown. The predominant theory is that the Minnesota Vikings, the last team to be awarded the trophy, somehow misplaced it after the 1969 season.
The current trophy of the NFL is the Vince Lombardi Trophy. The Super Bowl trophy was officially renamed in 1970 after Vince Lombardi, who as head coach led the Green Bay Packers to victories in the first two Super Bowls. Unlike the previous trophies, a new Vince Lombardi Trophy is issued to each year's champion, who maintains permanent control of it. Lombardi Trophies are made by Tiffany & Co. out of sterling silver and are worth anywhere from US$25,000 to US$300,000. Additionally, each player on the winning team as well as coaches and personnel are awarded Super Bowl rings to commemorate their victory. The winning team chooses the company that makes the rings; each ring design varies, with the NFL mandating certain ring specifications (which have a degree of room for deviation), in addition to requiring the Super Bowl logo be on at least one side of the ring. The losing team are also awarded rings, which must be no more than half as valuable as the winners' rings, but those are almost never worn.
The conference champions receive trophies for their achievement. The champions of the NFC receive the George Halas Trophy, named after Chicago Bears founder George Halas, who is also considered as one of the co-founders of the NFL. The AFC champions receive the Lamar Hunt Trophy, named after Lamar Hunt, the founder of the Kansas City Chiefs and the principal founder of the American Football League. Players on the winning team also receive a conference championship ring.
Player and coach awards.
The NFL recognizes a number of awards for its players and coaches at its annual NFL Honors presentation. The most prestigious award is the AP Most Valuable Player (MVP) award. Other major awards include the AP Offensive Player of the Year, AP Defensive Player of the Year, AP Comeback Player of the Year, and the AP Offensive and Defensive Rookie of the Year awards. Another prestigious award is the Walter Payton Man of the Year Award, which recognizes a player's off-field work in addition to his on-field performance. The NFL Coach of the Year award is the highest coaching award. The NFL also gives out weekly awards such as the FedEx Air & Ground NFL Players of the Week and the Pepsi MAX NFL Rookie of the Week awards.
Media coverage.
In the United States, the National Football League has television contracts with four networks: CBS, ESPN, Fox, and NBC. In general, CBS televises afternoon games in which the away team is an AFC team, and Fox carries afternoon games in which the away team belongs to the NFC. In fall 1984, the Skycam camera system was used for the first time in a live telecast, at a preseason National Football League game in San Diego between the Chargers and 49ers, and televised by CBS.
Since 2011, the league has reserved the right to give games that, under the contract, would normally air on one network to the other network. CBS also carries a package of eight games on Thursday nights during the 2014 season. NBC carries the primetime Sunday Night Football package, the NFL Kickoff game, and a primetime Thanksgiving Day game. ESPN carries all Monday Night Football games. The NFL's own network, NFL Network, carries all Thursday Night Football games, including those on CBS but not the ones on NBC.
The Super Bowl television rights are rotated on a three-year basis between CBS, Fox, and NBC. In 2011, all four stations signed new nine-year contracts with the NFL, each running until 2022; CBS, Fox, and NBC are estimated by "Forbes" to pay a combined total of US$3 billion a year, while ESPN will pay US$1.9 billion a year. The league also has deals with Spanish-language broadcasters NBC Universo, Fox Deportes and ESPN Deportes, which air Spanish language dubs of their respective English-language sister networks' games. The league's contracts do not cover preseason games, which individual teams are free to sell to local stations directly; a minority of preseason games are distributed among the league's national television partners.
Through the 2014 season, the NFL had a blackout policy in which games were 'blacked out' on local television in the home team's area if the home stadium was not sold out. Clubs could elect to set this requirement at only 85%, but they would have to give more ticket revenue to the visiting team; teams could also request a specific exemption from the NFL for the game. The vast majority of NFL games were not blacked out; only 6% of games were blacked out during the 2011 season, and only two games were blacked out in and none in . The NFL announced in March 2015 that it would suspend its blackout policy for at least the 2015 season. According to Nielsen, the NFL regular season since 2012 was watched by at least 200 million individuals, accounting for 80% of all television households in the United States and 69% of all potential viewers in the United States. NFL regular season games accounted for 31 out of the top 32 most-watched programs in the fall season and an NFL game ranked as the most-watched television show in all 17 weeks of the regular season. At the local level, NFL games were the highest-ranked shows in NFL markets 92% of the time. Super Bowls account for the 22 most-watched programs (based on total audience) in US history, including a record 167 million people that watched Super Bowl XLVIII, the conclusion to the 2013 season.
In addition to radio networks run by each NFL team, select NFL games are broadcast nationally by Westwood One (known as Dial Global for the 2012 season). These games are broadcast on over 500 networks, giving all NFL markets access to each primetime game. The NFL's deal with Westwood One was extended in 2012 and will run through 2017.
The NFL, as a one-time experiment, distributed the October 25, 2015 International Series game from Wembley Stadium in London between the Buffalo Bills and Jacksonville Jaguars. The game was live streamed on the Internet exclusively via Yahoo!, except for over-the-air broadcasts on the local CBS-TV affiliates in the Buffalo and Jacksonville markets.
In 2015, the NFL began sponsoring a series of public service announcements to bring attention to domestic abuse and sexual assault in response to what was seen as poor handling of incidents of violence by players.
Draft.
Each April (except in 2014 when it was moved to May), the NFL holds a draft of college players. The draft consists of seven rounds, with each of the 32 clubs getting one pick in each round. The draft order for non-playoff teams is determined by regular-season record; among playoff teams, teams are first ranked by the furthest round of the playoffs they reached, and then are ranked by regular-season record. For example, any team that reached the divisional round will be given a higher pick than any team that reached the conference championships, but will be given a lower pick than any team that did not make the divisional round. The Super Bowl champion always drafts last, and the runner-up always drafts next-to-last. All potential draftees must be at least three years removed from high school in order to be eligible for the draft. Underclassmen that have met that criterion to be eligible for the draft must write an application to the NFL by January 15 renouncing their remaining college eligibility. Clubs can trade away picks for future draft picks, but cannot trade the rights to players they have selected in previous drafts.
Aside from the 32 picks each club gets, compensatory draft picks are given to teams that have lost more compensatory free agents than they have gained. These are spread out from rounds 3 to 7, and a total of 32 are given. Clubs are required to make their selection within a certain period of time, the exact time depending on which round the pick is made in. If they fail to do so on time, the clubs behind them can begin to select their players in order, but they do not lose the pick outright. This happened in the 2003 draft, when the Minnesota Vikings failed to make their selection on time. The Jacksonville Jaguars and Carolina Panthers were able to make their picks before the Vikings were able to use theirs. Selected players are only allowed to negotiate contracts with the team that picked them, but if they choose not to sign they become eligible for the next year's draft. Under the current collective bargaining contract, all contracts to drafted players must be four-year deals with a club option for a fifth. Contracts themselves are limited to a certain amount of money, depending on the exact draft pick the player was selected with. Players who were draft eligible but not picked in the draft are free to sign with any club.
The NFL operates several other drafts in addition to the NFL draft. The league holds a supplemental draft annually. Clubs submit emails to the league stating the player they wish to select and the round they will do so, and the team with the highest bid wins the rights to that player. The exact order is determined by a lottery held before the draft, and a successful bid for a player will result in the team forfeiting the rights to its pick in the equivalent round of the next NFL draft. Players are only eligible for the supplemental draft after being granted a petition for special eligibility. The league holds expansion drafts, the most recent happening in 2002 when the Houston Texans began play as an expansion team. Other drafts held by the league include an allocation draft in 1950 to allocate players from several teams that played in the dissolved All-America Football Conference and a supplemental draft in 1984 to give NFL teams the rights to players who had been eligible for the main draft but had not been drafted because they had signed contracts with the United States Football League or Canadian Football League.
Like the other major sports leagues in the United States, the NFL maintains protocol for a disaster draft. In the event of a 'near disaster' (less than 15 players killed or disabled) that caused the club to lose a quarterback, they could draft one from a team with at least three quarterbacks. In the event of a 'disaster' (15 or more players killed or disabled) that results in a club's season being cancelled, a restocking draft would be held. Neither of these protocols has ever had to be implemented.
Free agency.
Free agents in the National Football League are divided into restricted free agents, who have three accrued seasons and whose current contract has expired, and unrestricted free agents, who have four or more accrued seasons and whose contract has expired. An accrued season is defined as "six or more regular-season games on a club's active/inactive, reserved/injured or reserve/physically unable to perform lists". Restricted free agents are allowed to negotiate with other clubs besides their former club, but the former club has the right to match any offer. If they choose not to, they are compensated with draft picks. Unrestricted free agents are free to sign with any club, and no compensation is owed if they sign with a different club.
Clubs are given one franchise tag to offer to any unrestricted free agent. The franchise tag is a one-year deal that pays the player 120% of his previous contract or no less than the average of the five highest-paid players at his position, whichever is greater. There are two types of franchise tags: exclusive tags, which do not allow the player to negotiate with other clubs, and non-exclusive tags, which allow the player to negotiate with other clubs but gives his former club the right to match any offer and two first-round draft picks if they decline to match it.
Clubs also have the option to use a transition tag, which is similar to the non-exclusive franchise tag but offers no compensation if the former club refuses to match the offer. Due to that stipulation, the transition tag is rarely used, even with the removal of the "poison pill" strategy (offering a contract with stipulations that the former club would be unable to match) that essentially ended the usage of the tag league-wide. Each club is subject to a salary cap, which is set at US$143.28 million for the 2015 season, US$10 million more than in 2014 and US$20 million more than in 2013.
References.
Explanatory notes
Citations
Bibliography

</doc>
<doc id="21212" url="https://en.wikipedia.org/wiki?curid=21212" title="Nazi Germany">
Nazi Germany

Nazi Germany and the Third Reich () are common English names for Germany from 1933 to 1945, when it was a dictatorship under the control of Adolf Hitler and the Nazi Party (NSDAP). Under Hitler's rule, Germany was transformed into a fascist totalitarian state which controlled nearly all aspects of life. The official name of the state was "Deutsches Reich" ("German Reich", "German Empire" or "German Realm") from 1933 to 1943 and "Großdeutsches Reich" (Greater German Reich) from 1943 to 1945. Nazi Germany ceased to exist after the Allied Forces defeated Germany in May 1945, ending World War II in Europe.
Hitler was appointed Chancellor of Germany by the President of the Weimar Republic Paul von Hindenburg on 30 January 1933. The Nazi Party then began to eliminate all political opposition and consolidate its power. Hindenburg died on 2 August 1934, and Hitler became dictator of Germany by merging the powers and offices of the Chancellery and Presidency. A national referendum held 19 August 1934 confirmed Hitler as sole Führer (leader) of Germany. All power was centralised in Hitler's person, and his word became above all laws. The government was not a coordinated, co-operating body, but a collection of factions struggling for power and Hitler's favour. In the midst of the Great Depression, the Nazis restored economic stability and ended mass unemployment using heavy military spending and a mixed economy. Extensive public works were undertaken, including the construction of "Autobahnen" (high speed highways). The return to economic stability boosted the regime's popularity.
Racism, especially antisemitism, was a central feature of the regime. The Germanic peoples (the Nordic race) were considered the purest of the Aryan race, and were therefore the master race. Millions of Jews and others deemed undesirable were murdered in the Holocaust. Opposition to Hitler's rule was ruthlessly suppressed. Members of the liberal, socialist, and communist opposition were killed, imprisoned, or exiled. The Christian churches were also oppressed, with many leaders imprisoned. Education focused on racial biology, population policy, and fitness for military service. Career and educational opportunities for women were curtailed. Recreation and tourism were organised via the Strength Through Joy program, and the 1936 Summer Olympics showcased the Third Reich on the international stage. Propaganda minister Joseph Goebbels made effective use of film, mass rallies, and Hitler's hypnotising oratory to control public opinion. The government controlled artistic expression, promoting specific art forms and banning or discouraging others.
Beginning in the late 1930s, Nazi Germany made increasingly aggressive territorial demands, threatening war if they were not met. It seized Austria and Czechoslovakia in 1938 and 1939. Hitler made a pact with Joseph Stalin and invaded Poland in September 1939, launching World War II in Europe. In alliance with Italy and smaller Axis powers, Germany conquered most of Europe by 1940 and threatened Great Britain. "Reichskommissariats" took control of conquered areas, and a German administration was established in what was left of Poland. Jews and others deemed undesirable were imprisoned and murdered in Nazi concentration camps and extermination camps. The regime's racial policies turned genocidal, culminating in the mass murder of Jews and other minorities in the Holocaust. The plan to exterminate the Jews of Europe was formalized in the 1942 Wannsee Conference, replacing the previous policy of forced emigration of Jews from the Reich.
Following the German invasion of the Soviet Union in 1941, the tide turned against the Nazis, who suffered major military defeats in 1943. Large-scale aerial bombing of Germany escalated in 1944, and the Nazis retreated from Eastern and Southern Europe. Following the Allied invasion of France, Germany was conquered by the Soviet Union from the east and the other Allied powers from the west and surrendered within a year. Hitler's refusal to admit defeat led to massive destruction of German infrastructure and additional war-related deaths in the closing months of the war. The victorious Allies initiated a policy of denazification and put many of the surviving Nazi leadership on trial for war crimes at the Nuremberg trials.
Name.
The official name of the state was "Deutsches Reich" (German Reich) from 1933 to 1943, and "Großdeutsches Reich" (Greater German Reich) from 1943 to 1945. The name "Deutsches Reich" is usually translated into English as "German Empire" or "German Reich". Modern Germans refer to the period as "Zeit des Nationalsozialismus" (National Socialist period), "Nationalsozialistische Gewaltherrschaft" (National Socialist tyranny), or simply as "das Dritte Reich" (the Third Reich).
Common English terms are "Nazi Germany" and "Third Reich". The latter, adopted by Nazi propaganda, was first used in a 1923 book by Arthur Moeller van den Bruck. The book counted the Holy Roman Empire (962–1806) as the first Reich and the German Empire (1871–1918) as the second. The Nazis used it to legitimize their regime as a successor state. After they seized power, Nazi propaganda retroactively referred to the Weimar Republic as the "Zwischenreich" ("Interim Reich").
History.
Background.
The German economy suffered severe setbacks after the end of World War I, partly because of reparations payments required under the 1919 Treaty of Versailles. The government printed money to make the payments and to repay the country's war debt; the resulting hyperinflation led to inflated prices for consumer goods, economic chaos, and food riots. When the government failed to make the reparations payments in January 1923, French troops occupied German industrial areas along the Ruhr. Widespread civil unrest followed.
The National Socialist German Workers' Party (NSDAP; Nazi Party) was the renamed successor of the German Workers' Party founded in 1919, one of several far-right political parties active in Germany at the time. The party platform included removal of the Weimar Republic, rejection of the terms of the Treaty of Versailles, radical antisemitism, and anti-Bolshevism. They promised a strong central government, increased "Lebensraum" (living space) for Germanic peoples, formation of a national community based on race, and racial cleansing via the active suppression of Jews, who would be stripped of their citizenship and civil rights. The Nazis proposed national and cultural renewal based upon the "Völkisch" movement.
When the stock market in the United States crashed on 24 October 1929, the impact in Germany was dire. Millions were thrown out of work, and several major banks collapsed. Hitler and the NSDAP prepared to take advantage of the emergency to gain support for their party. They promised to strengthen the economy and provide jobs. Many voters decided the NSDAP was capable of restoring order, quelling civil unrest, and improving Germany's international reputation. After the federal election of 1932, the Nazis were the largest party in the Reichstag, holding 230 seats with 37.4 percent of the popular vote.
Nazi seizure of power.
Although the Nazis won the greatest share of the popular vote in the two Reichstag general elections of 1932, they did not have a majority, so Hitler led a short-lived coalition government formed by the NSDAP and the German National People's Party. Under pressure from politicians, industrialists, and the business community, President Paul von Hindenburg appointed Hitler as Chancellor of Germany on 30 January 1933. This event is known as the "Machtergreifung" (seizure of power). In the following months, the NSDAP used a process termed "Gleichschaltung" (co-ordination) to rapidly bring all aspects of life under control of the party. All civilian organisations, including agricultural groups, volunteer organisations, and sports clubs, had their leadership replaced with Nazi sympathisers or party members. By June 1933, virtually the only organisations not in the control of the NSDAP were the army and the churches.
On the night of 27 February 1933, the Reichstag building was set afire; Marinus van der Lubbe, a Dutch communist, was found guilty of starting the blaze. Hitler proclaimed that the arson marked the start of a communist uprising. Violent suppression of communists by the "Sturmabteilung" (SA) was undertaken all over the country, and four thousand members of the Communist Party of Germany were arrested. The Reichstag Fire Decree, imposed on 28 February 1933, rescinded most German civil liberties, including rights of assembly and freedom of the press. The decree also allowed the police to detain people indefinitely without charges or a court order. The legislation was accompanied by a propaganda blitz that led to public support for the measure.
In March 1933, the Enabling Act, an amendment to the Weimar Constitution, passed in the Reichstag by a vote of 444 to 94. This amendment allowed Hitler and his cabinet to pass laws—even laws that violated the constitution—without the consent of the president or the Reichstag. As the bill required a two-thirds majority to pass, the Nazis used the provisions of the Reichstag Fire Decree to keep several Social Democratic deputies from attending; the Communists had already been banned. On 10 May the government seized the assets of the Social Democrats; they were banned in June. The remaining political parties were dissolved, and on 14 July 1933, Germany became a de facto one-party state when the founding of new parties was made illegal. Further elections in November 1933, 1936, and 1938 were entirely Nazi-controlled and saw only the Nazis and a small number of independents elected. The regional state parliaments and the "Reichsrat" (federal upper house) were abolished in January 1934.
The Nazi regime abolished the symbols of the Weimar Republic, including the black, red, and gold tricolour flag, and adopted reworked imperial symbolism. The previous imperial black, white, and red tricolour was restored as one of Germany's two official flags; the second was the swastika flag of the NSDAP, which became the sole national flag in 1935. The NSDAP anthem "Horst-Wessel-Lied" ("Horst Wessel Song") became a second national anthem.
In this period, Germany was still in a dire economic situation; millions were unemployed and the balance of trade deficit was daunting. Hitler knew that reviving the economy was vital. In 1934, using deficit spending, public works projects were undertaken. A total of 1.7 million Germans were put to work on the projects in 1934 alone. Average wages both per hour and per week began to rise.
The demands of the SA for more political and military power caused anxiety among military, industrial, and political leaders. In response, Hitler purged the entire SA leadership in the Night of the Long Knives, which took place from 30 June to 2 July 1934. Hitler targeted Ernst Röhm and other SA leaders who, along with a number of Hitler's political adversaries (such as Gregor Strasser and former chancellor Kurt von Schleicher), were rounded up, arrested, and shot.
On 2 August 1934, President von Hindenburg died. The previous day, the cabinet had enacted the "Law Concerning the Highest State Office of the Reich", which stated that upon Hindenburg's death, the office of president would be abolished and its powers merged with those of the chancellor. Hitler thus became head of state as well as head of government. He was formally named as "Führer und Reichskanzler" (leader and chancellor). Germany was now a totalitarian state with Hitler at its head. As head of state, Hitler became Supreme Commander of the armed forces. The new law altered the traditional loyalty oath of servicemen so that they affirmed loyalty to Hitler personally rather than the office of supreme commander or the state. On 19 August, the merger of the presidency with the chancellorship was approved by 90 percent of the electorate in a plebiscite.
Most Germans were relieved that the conflicts and street fighting of the Weimar era had ended. They were deluged with propaganda orchestrated by Joseph Goebbels, who promised peace and plenty for all in a united, Marxist-free country without the constraints of the Versailles Treaty. The first Nazi concentration camp, initially for political prisoners, was opened at Dachau in 1933. Hundreds of camps of varying size and function were created by the end of the war. Upon seizing power, the Nazis took repressive measures against their political opposition and rapidly began the comprehensive marginalisation of persons they considered socially undesirable. Under the guise of combating the Communist threat, the National Socialists secured immense power. Above all, their campaign against Jews living in Germany gained momentum.
Beginning in April 1933, scores of measures defining the status of Jews and their rights were instituted at the regional and national level. Initiatives and legal mandates against the Jews reached their culmination with the establishment of the Nuremberg Laws of 1935, stripping them of their basic rights. The Nazis would take from the Jews their wealth, their right to intermarry with non-Jews, and their right to occupy many fields of labour (such as practising law, medicine, or working as educators). They eventually declared them undesirable to remain among German citizens and society, which over time dehumanised the Jews; arguably, these actions desensitised Germans to the extent that it resulted in the Holocaust. Ethnic Germans who refused to ostracise Jews or who showed any signs of resistance to Nazi propaganda were placed under surveillance by the Gestapo, had their rights removed, or were sent to concentration camps. Everyone and everything was monitored in Nazi Germany. Inaugurating and legitimising power for the Nazis was thus accomplished by their initial revolutionary activities, then through the improvisation and manipulation of the legal mechanisms available, through the use of police powers by the Nazi Party (which allowed them to include and exclude from society whomever they chose), and finally by the expansion of authority for all state and federal institutions.
Militaristic foreign policy.
As early as February 1933, Hitler announced that rearmament must be undertaken, albeit clandestinely at first, as to do so was in violation of the Versailles Treaty. A year later he told his military leaders that 1942 was the target date for going to war in the east. He pulled Germany out of the League of Nations in 1933, claiming its disarmament clauses were unfair, as they applied only to Germany. The Saarland, which had been placed under League of Nations supervision for 15 years at the end of World War I, voted in January 1935 to become part of Germany. In March 1935 Hitler announced that the "Reichswehr" would be increased to 550,000 men and that he was creating an air force. Britain agreed that the Germans would be allowed to build a naval fleet with the signing of the Anglo-German Naval Agreement on 18 June 1935.
When the Italian invasion of Ethiopia led to only mild protests by the British and French governments, on 7 March 1936 Hitler ordered the "Wehrmacht Heer" ground forces to march 3,000 troops into the demilitarised zone in the Rhineland in violation of the Versailles Treaty; an additional 30,000 troops were on standby. As the territory was part of Germany, the British and French governments did not feel that attempting to enforce the treaty was worth the risk of war. In the one-party election held on 29 March, the NSDAP received 98.9 percent support. In 1936 Hitler signed an Anti-Comintern Pact with Japan and a non-aggression agreement with the Fascist Italy of Benito Mussolini, who was soon referring to a "Rome-Berlin Axis".
Hitler sent air and armoured units to assist General Francisco Franco and his Nationalist forces in the Spanish Civil War, which broke out in July 1936. The Soviet Union sent a smaller force to assist the Republican government. Franco's Nationalists were victorious in 1939 and became an informal ally of Nazi Germany.
Austria and Czechoslovakia.
In February 1938, Hitler emphasised to Austrian Chancellor Kurt Schuschnigg the need for Germany to secure its frontiers. Schuschnigg scheduled a plebiscite regarding Austrian independence for 13 March, but Hitler demanded that it be cancelled. On 11 March, Hitler sent an ultimatum to Schuschnigg demanding that he hand over all power to the Austrian NSDAP or face an invasion. The "Wehrmacht" entered Austria the next day, to be greeted with enthusiasm by the populace.
The Republic of Czechoslovakia was home to a substantial minority of Germans, who lived mostly in the Sudetenland. Under pressure from separatist groups within the Sudeten German Party, the Czechoslovak government offered economic concessions to the region. Hitler decided to incorporate not just the Sudetenland but the whole of Czechoslovakia into the Reich. The Nazis undertook a propaganda campaign to try to drum up support for an invasion. Top leaders of the armed forces were not in favour of the plan, as Germany was not yet ready for war. The crisis led to war preparations by the British, the Czechoslovaks, and France (Czechoslovakia's ally). Attempting to avoid war, British Prime Minister Neville Chamberlain arranged a series of meetings, the result of which was the Munich Agreement, signed on 29 September 1938. The Czechoslovak government was forced to accept the Sudetenland's annexation into Germany. Chamberlain was greeted with cheers when he landed in London bringing, he said, "peace for our time." The agreement lasted six months before Hitler seized the rest of Czech territory in March 1939. A puppet state was created in Slovakia.
Austrian and Czech foreign exchange reserves were soon seized by the Nazis, as were stockpiles of raw materials such as metals and completed goods such as weaponry and aircraft, which were shipped back to Germany. The "Reichswerke Hermann Göring" industrial conglomerate took control of steel and coal production facilities in both countries.
Poland.
In March 1939, Hitler demanded the return of the Free City of Danzig and the Polish Corridor, a strip of land that separated East Prussia from the rest of Germany. The British announced they would come to the aid of Poland if it was attacked. Hitler, believing the British would not actually take action, ordered an invasion plan should be readied for a target date of September 1939. On 23 May he described to his generals his overall plan of not only seizing the Polish Corridor but greatly expanding German territory eastward at the expense of Poland. He expected this time they would be met by force.
The Germans reaffirmed their alliance with Italy and signed non-aggression pacts with Denmark, Estonia, and Latvia. Trade links were formalised with Romania, Norway, and Sweden. Hitler's foreign minister, Joachim von Ribbentrop, arranged in negotiations with the Soviet Union a non-aggression pact, the Molotov–Ribbentrop Pact, which was signed in August 1939. The treaty also contained secret protocols dividing Poland and the Baltic states into German and Soviet spheres of influence.
World War II.
Foreign policy.
Germany's foreign policy during the war involved the creation of allied governments under direct or indirect control from Berlin. A main goal was obtaining soldiers from the senior allies, such as Italy and Hungary, and millions of workers and ample food supplies from subservient allies such as Vichy France. By the fall of 1942, there were 24 divisions from Romania on the Eastern Front, 10 from Italy, and 10 from Hungary. When a country was no longer dependable, Germany assumed full control, as it did with France in 1942, Italy in 1943, and Hungary in 1944. Although Japan was an official powerful ally, the relationship was distant and there was little co-ordination or co-operation. For example, Germany refused to share their formula for synthetic oil from coal until late in the war.
Outbreak of war.
Germany invaded Poland on 1 September 1939. Britain and France declared war on Germany two days later. World War II was under way. Poland fell quickly, as the Soviet Union attacked from the east on 17 September. Reinhard Heydrich, then head of the Gestapo, ordered on 21 September that Jews should be rounded up and concentrated into cities with good rail links. Initially the intention was to deport the Jews to points further east, or possibly to Madagascar. Using lists prepared ahead of time, some 65,000 Polish intelligentsia, noblemen, clergy, and teachers were killed by the end of 1939 in an attempt to destroy Poland's identity as a nation. The Soviet forces continued to attack, advancing into Finland in the Winter War, and German forces were involved in action at sea. But little other activity occurred until May, so the period became known as the "Phoney War".
From the start of the war, a British blockade on shipments to Germany had an impact on the Reich economy. The Germans were particularly dependent on foreign supplies of oil, coal, and grain. To safeguard Swedish iron ore shipments to Germany, Hitler ordered an attack on Norway, which took place on 9 April 1940. Much of the country was occupied by German troops by the end of April. Also on 9 April, the Germans invaded and occupied Denmark.
Conquest of Europe.
Against the judgement of many of his senior military officers, Hitler ordered an attack on France and the Low Countries, which began in May 1940. They quickly conquered Luxembourg, the Netherlands, and Belgium, and France surrendered on 22 June. The unexpectedly swift defeat of France resulted in an upswing in Hitler's popularity and a strong upsurge in war fever.
In spite of the provisions of the Hague Convention, industrial firms in the Netherlands, France, and Belgium were put to work producing war materiel for the occupying German military. Officials viewed this option as being preferable to their citizens being deported to the Reich as forced labour.
The Nazis seized from the French thousands of locomotives and rolling stock, stockpiles of weapons, and raw materials such as copper, tin, oil, and nickel. Financial demands were levied on the governments of the occupied countries as well; payments for occupation costs were received from France, Belgium, and Norway. Barriers to trade led to hoarding, black markets, and uncertainty about the future. Food supplies were precarious; production dropped in most areas of Europe, but not as much as during World War I. Greece experienced famine in the first year of occupation and the Netherlands in the last year of the war.
Hitler made peace overtures to the new British leader, Winston Churchill, and upon their rejection he ordered a series of aerial attacks on Royal Air Force airbases and radar stations. However, the German Luftwaffe failed to defeat the Royal Air Force in what became known as the Battle of Britain. By the end of October, Hitler realised the necessary air superiority for his planned invasion of Britain could not be achieved, and he ordered nightly air raids on British cities, including London, Plymouth, and Coventry.
In February 1941, the German "Afrika Korps" arrived in Libya to aid the Italians in the North African Campaign and attempt to contain Commonwealth forces stationed in Egypt. On 6 April, Germany launched the invasion of Yugoslavia and the battle of Greece. German efforts to secure oil included negotiating a supply from their new ally, Romania, who signed the Tripartite Pact in November 1940.
On 22 June 1941, contravening the Molotov–Ribbentrop Pact, 5.5 million Axis troops attacked the Soviet Union. In addition to Hitler's stated purpose of acquiring "Lebensraum", this large-scale offensive (codenamed Operation Barbarossa) was intended to destroy the Soviet Union and seize its natural resources for subsequent aggression against the Western powers. The reaction among Germans was one of surprise and trepidation. Many were concerned about how much longer the war would drag on or suspected that Germany could not win a war fought on two fronts.
The invasion conquered a huge area, including the Baltic republics, Belarus, and West Ukraine. After the successful Battle of Smolensk, Hitler ordered Army Group Centre to halt its advance to Moscow and temporarily divert its Panzer groups to aid in the encirclement of Leningrad and Kiev. This pause provided the Red Army with an opportunity to mobilise fresh reserves. The Moscow offensive, which resumed in October 1941, ended disastrously in December. On 7 December 1941, Japan attacked Pearl Harbor, Hawaii. Four days later, Germany declared war on the United States.
Food was in short supply in the conquered areas of the Soviet Union and Poland, with rations inadequate to meet nutritional needs. The retreating armies had burned the crops, and much of the remainder was sent back to the Reich. In Germany itself, food rations had to be cut in 1942. In his role as Plenipotentiary of the Four Year Plan, Hermann Göring demanded increased shipments of grain from France and fish from Norway. The 1942 harvest was a good one, and food supplies remained adequate in Western Europe.
Reichsleiter Rosenberg Taskforce was an organisation set up to loot artwork and cultural material from Jewish collections, libraries, and museums throughout Europe. Some 26,000 railroad cars full of art treasures, furniture, and other looted items were sent back to Germany from France alone. In addition, soldiers looted or purchased goods such as produce and clothing—items which were becoming harder to obtain in Germany—for shipment back home.
Turning point and collapse.
Germany, and Europe as a whole, was almost totally dependent on foreign oil imports. In an attempt to resolve the persistent shortage, Germany launched "Fall Blau" (Case Blue), an offensive against the Caucasian oilfields, in June 1942. The Red Army launched a counter-offensive on 19 November and encircled the Axis forces, who were trapped in Stalingrad on 23 November. Göring assured Hitler that the 6th Army could be supplied by air, but this turned out to be infeasible. Hitler's refusal to allow a retreat led to the deaths of 200,000 German and Romanian soldiers; of the 91,000 men who surrendered in the city on 31 January 1943, only 6,000 survivors returned to Germany after the war. Soviet forces continued to push the invaders westward after the failed German offensive at the Battle of Kursk, and by the end of 1943, the Germans had lost most of their territorial gains in the east.
In Egypt, Field Marshal Erwin Rommel's "Afrika Korps" were defeated by British forces under Field Marshal Bernard Montgomery in October 1942. Allied forces landed in Sicily in July 1943, and in Italy in September. Meanwhile, American and British bomber fleets, based in Britain, began operations against Germany. In an effort to destroy German morale, many sorties were intentionally given civilian targets. Soon German aircraft production could not keep pace with losses, and without air cover, the Allied bombing campaign became even more devastating. By targeting oil refineries and factories, they crippled the German war effort by late 1944.
On 6 June 1944, American, British, and Canadian forces established a western front with the D-Day landings in Normandy. On 20 July 1944, Hitler narrowly survived a bomb attack. He ordered savage reprisals, resulting in 7,000 arrests and the execution of more than 4,900 people. The failed Ardennes Offensive (16 December 1944 – 25 January 1945) was the last major German campaign of the war. Soviet forces entered Germany on 27 January. Hitler's refusal to admit defeat and his repeated insistence that the war be fought to the last man led to unnecessary death and destruction in the closing months of the war. Through his Justice Minister, Otto Georg Thierack, he ordered that anyone who was not prepared to fight should be summarily court-martialed. Thousands of people were put to death. In many areas, people looked for ways to surrender to the approaching Allies, in spite of exhortations of local leaders to continue the struggle. Hitler also ordered the intentional destruction of transport, bridges, industries, and other infrastructure—a scorched earth decree—but Armaments Minister Albert Speer was able to keep this order from being fully carried out.
During the Battle of Berlin (16 April 1945 – 2 May 1945), Hitler and his staff lived in the underground "Führerbunker", while the Red Army approached. On 30 April, when Soviet troops were one or two blocks away from the Reich Chancellery, Hitler and Eva Braun committed suicide in the "Führerbunker". On 2 May General Helmuth Weidling unconditionally surrendered Berlin to Soviet General Vasily Chuikov. Hitler was succeeded by Grand Admiral Karl Dönitz as Reich President and Goebbels as Reich Chancellor. Goebbels and his wife Magda committed suicide the next day, after murdering their six children. On 4–8 May 1945 most of the remaining German armed forces surrendered unconditionally. The German Instrument of Surrender was signed 7 May, marking the end of World War II in Europe.
Suicide rates in Germany increased as the war drew to a close, particularly in areas where the Red Army was advancing. More than a thousand people (out of a population of around 16,000) committed suicide in Demmin on and around 1 May 1945 as the 65th Army of 2nd Belorussian Front first broke into a distillery and then rampaged through the town, committing mass rapes, arbitrarily executing civilians, and setting fire to buildings. High numbers of suicides took place in many other locations, including Neubrandenburg (600 dead), Stolp in Pommern (1,000 dead), and Berlin, where at least 7,057 people committed suicide in 1945.
German casualties.
Estimates of the total German war dead range from 5.5 to 6.9 million persons. A study by German historian Rüdiger Overmans puts the number of German military dead and missing at 5.3 million, including 900,000 men conscripted from outside of Germany's 1937 borders, in Austria, and in east-central Europe. Overy estimated in 2014 that in all about 353,000 civilians were killed by British and American bombing of German cities. An additional 20,000 died in the land campaign. Some 22,000 citizens died during the Battle of Berlin. Other civilian deaths include 300,000 Germans (including Jews) who were victims of Nazi political, racial, and religious persecution, and 200,000 who were murdered in the Nazi euthanasia program. Political courts called "Sondergerichte" sentenced some 12,000 members of the German resistance to death, and civil courts sentenced an additional 40,000 Germans. Mass rapes of German women also took place.
At the end of the war, Europe had more than 40 million refugees, its economy had collapsed, and 70 percent of its industrial infrastructure was destroyed. Between twelve and fourteen million ethnic Germans fled or were expelled from east-central Europe to Germany. During the Cold War, the West German government estimated a death toll of 2.2 million civilians due to the flight and expulsion of Germans and through forced labour in the Soviet Union. This figure remained unchallenged until the 1990s, when some historians put the death toll at 500,000–600,000 confirmed deaths. In 2006 the German government reaffirmed its position that 2.0–2.5 million deaths occurred.
Geography.
Territorial changes.
As a result of their defeat in World War I and the resulting Treaty of Versailles, Germany lost Alsace-Lorraine, Northern Schleswig, and Memel. The Saarland temporarily became a protectorate of France, under the condition that its residents would later decide by referendum which country to join. Poland became a separate nation and was given access to the sea by the creation of the Polish Corridor, which separated Prussia from the rest of Germany. Danzig was made a free city.
Germany regained control of the Saarland via a referendum held in 1935 and annexed Austria in the Anschluss of 1938. The Munich Agreement of 1938 gave Germany control of the Sudetenland, and they seized the remainder of Czechoslovakia six months later. Under threat of invasion by sea, Lithuania surrendered the Memel district to the Nazis in March 1939.
Between 1939 and 1941, German forces invaded Poland, France, Luxembourg, the Netherlands, Belgium, and the Soviet Union. Trieste, South Tyrol, and Istria were ceded to Germany by Mussolini in 1943. Two puppet districts were set up in the area, the Operational Zone of the Adriatic Littoral and the Operational Zone of the Alpine Foothills.
Occupied territories.
Some of the conquered territories were immediately incorporated into Germany as part of Hitler's long-term goal of creating a Greater Germanic Reich. Several areas, such as Alsace-Lorraine, were placed under the authority of an adjacent "Gau" (regional district). Beyond the territories incorporated into Germany were the "Reichskommissariate" (Reich Commissariats), quasi-colonial regimes established in a number of occupied countries. Areas placed under German administration included the Protectorate of Bohemia and Moravia, "Reichskommissariat Ostland" (encompassing the Baltic states and Belarus), and "Reichskommissariat Ukraine". Conquered areas of Belgium and France were placed under control of the Military Administration in Belgium and Northern France. Belgian Eupen-Malmedy, which had been part of German until 1919, was annexed directly. Part of Poland was immediately incorporated into the Reich, and the General Government was established in occupied central Poland. Hitler intended to eventually incorporate many of these areas into the Reich.
The governments of Denmark, Norway ("Reichskommissariat Norwegen"), and the Netherlands ("Reichskommissariat Niederlande") were placed under civilian administrations staffed largely by natives.
Post-war changes.
With the issuance of the Berlin Declaration on 5 June 1945 and later creation of the Allied Control Council, the four Allied powers temporarily assumed governance of Germany. At the Potsdam Conference in August 1945, the Allies arranged for the Allied occupation and denazification of the country. Germany was split into four zones, each occupied by one of the Allied powers, who drew reparations from their zone. Since most of the industrial areas were in the western zones, the Soviet Union was transferred additional reparations. The Allied Control Council disestablished Prussia on 20 May 1947. Aid to Germany began arriving from the United States under the Marshall Plan in 1948. The occupation lasted until 1949, when the countries of East Germany and West Germany were created. Germany finalised her border with Poland by signing the Treaty of Warsaw (1970). Germany remained divided until 1990, when the Allies renounced all claims to German territory with the Treaty on the Final Settlement with Respect to Germany, under which Germany also renounced claims to territories lost during World War II.
Politics.
Ideology.
The NSDAP was a far-right political party which came into its own during the social and financial upheavals that occurred with the onset of the Great Depression in 1929. While in prison after the failed Beer Hall Putsch of 1923, Hitler wrote "Mein Kampf", which laid out his plan for transforming German society into one based on race. The ideology of Nazism brought together elements of antisemitism, racial hygiene, and eugenics, and combined them with pan-Germanism and territorial expansionism with the goal of obtaining more "Lebensraum" for the Germanic people. The regime attempted to obtain this new territory by attacking Poland and the Soviet Union, intending to deport or kill the Jews and Slavs living there, who were viewed as being inferior to the Aryan master race and part of a Jewish Bolshevik conspiracy. The Nazi regime believed that only Germany could defeat the forces of Bolshevism and save humanity from world domination by International Jewry. Others deemed life unworthy of life by the Nazis included the mentally and physically disabled, Romani people, homosexuals, Jehovah's Witnesses, and social misfits.
Influenced by the "Völkisch" movement, the regime was against cultural modernism and supported the development of an extensive military at the expense of intellectualism. Creativity and art were stifled, except where they could serve as propaganda media. The party used symbols such as the Blood Flag and rituals such as the Nazi Party rallies to foster unity and bolster the regime's popularity.
Government.
A law promulgated 30 January 1934 abolished the existing "Länder" (constituent states) of Germany and replaced them with new administrative divisions of Nazi Germany, the "Gaue", headed by NSDAP leaders ("Gauleiters"), who effectively became the governor of their region. The change was never fully implemented, as the Länder were still used as administrative divisions for some government departments such as education. This led to a bureaucratic tangle of overlapping jurisdictions and responsibilities typical of the administrative style of the Nazi regime.
Jewish civil servants lost their jobs in 1933, except for those who had seen military service in World War I. Members of the NSDAP or party supporters were appointed in their place. As part of the process of "Gleichschaltung", the Reich Local Government Law of 1935 abolished local elections. From that point forward, mayors were appointed by the Ministry of the Interior.
Hitler ruled Germany autocratically by asserting the "Führerprinzip" (leader principle), which called for absolute obedience of all subordinates. He viewed the government structure as a pyramid, with himself—the infallible leader—at the apex. Rank in the party was not determined by elections; positions were filled through appointment by those of higher rank. The party used propaganda to develop a cult of personality around Hitler. Historians such as Kershaw emphasise the psychological impact of Hitler's skill as an orator. Kressel writes, "Overwhelmingly ... Germans speak with mystification of Hitler's 'hypnotic' appeal". Roger Gill states, "His moving speeches captured the minds and hearts of a vast number of the German people: he virtually hypnotized his audiences."
Top officials reported to Hitler and followed his policies, but they had considerable autonomy. Officials were expected to "work towards the Führer" – to take the initiative in promoting policies and actions in line with his wishes and the goals of the NSDAP, without Hitler having to be involved in the day-to-day running of the country. The government was not a coordinated, co-operating body, but rather a disorganised collection of factions led by members of the party elite who struggled to amass power and gain the Führer's favour. Hitler's leadership style was to give contradictory orders to his subordinates and to place them in positions where their duties and responsibilities overlapped. In this way he fostered distrust, competition, and infighting among his subordinates to consolidate and maximise his own power.
Law.
On 20 August 1934, civil servants were required to swear an oath of unconditional obedience to Hitler; a similar oath had been required of members of the military several weeks prior. This law became the basis of the "Führerprinzip", the concept that Hitler's word overrode all existing laws. Any acts that were sanctioned by Hitler—even murder—thus became legal. All legislation proposed by cabinet ministers had to be approved by the office of Deputy Führer Rudolf Hess, who also had a veto over top civil service appointments.
Most of the judicial system and legal codes of the Weimar Republic remained in use during and after the Nazi era to deal with non-political crimes. The courts issued and carried out far more death sentences than before the Nazis took power. People who were convicted of three or more offences—even petty ones—could be deemed habitual offenders and jailed indefinitely. People such as prostitutes and pickpockets were judged to be inherently criminal and a threat to the racial community. Thousands were arrested and confined indefinitely without trial.
Although the regular courts handled political cases and even issued death sentences for these cases, a new type of court, the "Volksgerichtshof" (People's Court), was established in 1934 to deal with politically important matters. This court handed out over 5,000 death sentences until its dissolution in 1945. The death penalty could be issued for offences such as being a communist, printing seditious leaflets, or even making jokes about Hitler or other top party officials. Nazi Germany employed three types of capital punishment; hanging, decapitation, and death by shooting. The Gestapo was in charge of investigative policing to enforce National Socialist ideology. They located and confined political offenders, Jews, and others deemed undesirable. Political offenders who were released from prison were often immediately re-arrested by the Gestapo and confined in a concentration camp.
In September 1935 the Nuremberg Laws were enacted. These laws initially prohibited sexual relations and marriages between Aryans and Jews and were later extended to include "Gypsies, Negroes or their bastard offspring". The law also forbade the employment of German women under the age of 45 as domestic servants in Jewish households. The Reich Citizenship Law stated that only those of "German or related blood" were eligible for citizenship. At the same time the Nazis used propaganda to promulgate the concept of "Rassenschande" (race defilement) to justify the need for a restrictive law. Thus Jews and other non-Aryans were stripped of their German citizenship. The wording of the law also potentially allowed the Nazis to deny citizenship to anyone who was not supportive enough of the regime. A supplementary decree issued in November defined as Jewish anyone with three Jewish grandparents, or two grandparents if the Jewish faith was followed.
Military and paramilitary.
Wehrmacht.
The unified armed forces of Germany from 1935 to 1945 were called the Wehrmacht. This included the "Heer" (army), "Kriegsmarine" (navy), and the "Luftwaffe" (air force). From 2 August 1934, members of the armed forces were required to pledge an oath of unconditional obedience to Hitler personally. In contrast to the previous oath, which required allegiance to the constitution of the country and its lawful establishments, this new oath required members of the military to obey Hitler even if they were being ordered to do something illegal. Hitler decreed that the army would have to tolerate and even offer logistical support to the "Einsatzgruppen"—the mobile death squads responsible for millions of deaths in Eastern Europe—when it was tactically possible to do so. Members of the "Wehrmacht" also participated directly in the Holocaust by shooting civilians or undertaking genocide under the guise of anti-partisan operations. The party line was that the Jews were the instigators of the partisan struggle, and therefore needed to be eliminated. On 8 July 1941, Heydrich announced that all Jews were to be regarded as partisans, and gave the order for all male Jews between the ages of 15 and 45 to be shot.
In spite of efforts to prepare the country militarily, the economy could not sustain a lengthy war of attrition such as had occurred in World War I. A strategy was developed based on the tactic of "Blitzkrieg" (lightning war), which involved using quick coordinated assaults that avoided enemy strong points. Attacks began with artillery bombardment, followed by bombing and strafing runs. Next the tanks would attack and finally the infantry would move in to secure any ground that had been taken. Victories continued through mid-1940, but the failure to defeat Britain was the first major turning point in the war. The decision to attack the Soviet Union and the decisive defeat at Stalingrad led to the retreat of the German armies and the eventual loss of the war. The total number of soldiers who served in the "Wehrmacht" from 1935 to 1945 was around 18.2 million, of whom 5.3 million died.
The SA and SS.
The "Sturmabteilung" (SA; Storm Detachment; Brownshirts), founded in 1921, was the first paramilitary wing of the Nazi Party. Their initial assignment was to protect Nazi leaders at rallies and assemblies. They also took part in street battles against the forces of rival political parties and violent actions against Jews and others. By 1934, under Ernst Röhm's leadership, the SA had grown to over half a million members—4.5 million including reserves—at a time when the regular army was still limited to 100,000 men by the Versailles Treaty.
Röhm hoped to assume command of the army and absorb it into the ranks of the SA. Hindenburg and Defence Minister Werner von Blomberg threatened to impose martial law if the alarming activities of the SA were not curtailed. Hitler also suspected that Röhm was plotting to depose him, so he ordered the deaths of Röhm and other political enemies. Up to 200 people were killed from 30 June to 2 July 1934 in an event that became known as the Night of the Long Knives. After this purge the SA was no longer a major force.
Initially a force of a dozen men under the auspices of the SA, the "Schutzstaffel" (SS) grew to become one of the largest and most powerful groups in Nazi Germany. Led by "Reichsführer-SS" Heinrich Himmler from 1929, the SS had over a quarter million members by 1938 and continued to grow. Himmler envisioned the SS as being an elite group of guards, Hitler's last line of defence. The "Waffen-SS", the military branch of the SS, became a de facto fourth branch of the Wehrmacht.
In 1931 Himmler organised an SS intelligence service which became known as the "Sicherheitsdienst" (SD; Security Service) under his deputy, SS-"Obergruppenführer" Reinhard Heydrich. This organisation was tasked with locating and arresting communists and other political opponents. Himmler hoped it would eventually totally replace the existing police system. Himmler also established the beginnings of a parallel economy under the auspices of the SS Economy and Administration Head Office. This holding company owned housing corporations, factories, and publishing houses.
From 1935 forward the SS was heavily involved in the persecution of Jews, who were rounded up into ghettos and concentration camps. With the outbreak of World War II, SS units called "Einsatzgruppen" followed the army into Poland and the Soviet Union, where from 1941 to 1945 they killed more than two million people, including 1.3 million Jews. The "SS-Totenkopfverbände" (death's head units) were in charge of the concentration camps and extermination camps, where millions more were killed.
Economy.
Reich economics.
The most pressing economic matter the Nazis initially faced was the 30 percent national unemployment rate. Economist Dr. Hjalmar Schacht, President of the Reichsbank and Minister of Economics, created in May 1933 a scheme for deficit financing. Capital projects were paid for with the issuance of promissory notes called Mefo bills. When the notes were presented for payment, the Reichsbank printed money to do so. While the national debt soared, Hitler and his economic team expected that the upcoming territorial expansion would provide the means of repaying the debt. Schacht's administration achieved a rapid decline in the unemployment rate, the largest of any country during the Great Depression.
On 17 October 1933, aviation pioneer Hugo Junkers, owner of the Junkers Aircraft Works, was arrested. Within a few days his company was expropriated by the regime. In concert with other aircraft manufacturers and under the direction of Aviation Minister Göring, production was immediately ramped up industry-wide. From a workforce of 3,200 people producing 100 units per year in 1932, the industry grew to employ a quarter of a million workers manufacturing over 10,000 technically advanced aircraft per year less than ten years later.
An elaborate bureaucracy was created to regulate German imports of raw materials and finished goods with the intention of eliminating foreign competition in the German marketplace and improving the nation's balance of payments. The Nazis encouraged the development of synthetic replacements for materials such as oil and textiles. As the market was experiencing a glut and prices for petroleum were low, in 1933 the Nazi government made a profit-sharing agreement with IG Farben, guaranteeing them a 5 percent return on capital invested in their synthetic oil plant at Leuna. Any profits in excess of that amount would be turned over to the Reich. By 1936, Farben regretted making the deal, as the excess profits by then being generated had to be given to the government.
Major public works projects financed with deficit spending included the construction of a network of "Autobahns" and providing funding for programmes initiated by the previous government for housing and agricultural improvements. To stimulate the construction industry, credit was offered to private businesses and subsidies were made available for home purchases and repairs. On the condition that the wife would leave the workforce, a loan of up to 1,000 Reichsmarks could be accessed by young couples of Aryan descent who intended to marry. The amount that had to be repaid was reduced by 25 percent for each child born. The caveat that the woman had to remain unemployed was dropped by 1937 due to a shortage of skilled labourers.
Hitler envisioned widespread car ownership as part of the new Germany. He arranged for designer Ferdinand Porsche to draw up plans for the "KdF-wagen" (Strength Through Joy car), intended to be an automobile that every German citizen could afford. A prototype was displayed at the International Motor Show in Berlin on 17 February 1939. With the outbreak of World War II, the factory was converted to produce military vehicles. No production models were sold until after the war, when the vehicle was renamed the Volkswagen (people's car).
Six million people were unemployed when the Nazis took power in 1933, and by 1937 there were fewer than a million. This was in part due to the removal of women from the workforce. Real wages dropped by 25 percent between 1933 and 1938. Trade unions were abolished in May 1933 with the seizure of the funds and arrest of the leadership of the Social Democratic trade unions. A new organisation, the German Labour Front, was created and placed under NSDAP functionary Robert Ley. The average German worked 43 hours a week in 1933, and by 1939 this increased to 47 hours a week.
By early 1934 the focus shifted away from funding work creation schemes and toward rearmament. By 1935, military expenditures accounted for 73 percent of the government's purchases of goods and services. On 18 October 1936 Hitler named Göring as Plenipotentiary of the Four Year Plan, intended to speed up the rearmament programme. In addition to calling for the rapid construction of steel mills, synthetic rubber plants, and other factories, Göring instituted wage and price controls and restricted the issuance of stock dividends. Large expenditures were made on rearmament, in spite of growing deficits. With the introduction of compulsory military service in 1935, the "Reichswehr", which had been limited to 100,000 by the terms of the Versailles Treaty, expanded to 750,000 on active service at the start of World War II, with a million more in the reserve. By January 1939, unemployment was down to 301,800, and it dropped to only 77,500 by September.
Wartime economy and forced labour.
The Nazi war economy was a mixed economy that combined a free market with central planning; historian Richard Overy described it as being somewhere in between the command economy of the Soviet Union and the capitalist system of the United States.
In 1942, after the death of Armaments Minister Fritz Todt, Hitler appointed Albert Speer as his replacement. Speer improved production via streamlined organisation, the use of single-purpose machines operated by unskilled workers, rationalisation of production methods, and better co-ordination between the many different firms that made tens of thousands of components. Factories were relocated away from rail yards, which were bombing targets. By 1944, the war was consuming 75 percent of Germany's gross domestic product, compared to 60 percent in the Soviet Union and 55 percent in Britain.
The wartime economy relied heavily upon the large-scale employment of forced labourers. Germany imported and enslaved some 12 million people from 20 European countries to work in factories and on farms; approximately 75 percent were Eastern European. Many were casualties of Allied bombing, as they received poor air raid protection. Poor living conditions led to high rates of sickness, injury, and death, as well as sabotage and criminal activity.
Foreign workers brought into Germany were put into four different classifications; guest workers, military internees, civilian workers, and Eastern workers. Different regulations were placed upon the worker depending on their classification. To separate Germans and foreign workers, the Nazis issued a ban on sexual relations between Germans and foreign workers.
Women played an increasingly large role. By 1944 over a half million served as auxiliaries in the German armed forces, especially in anti-aircraft units of the Luftwaffe; a half million worked in civil aerial defence; and 400,000 were volunteer nurses. They also replaced men in the wartime economy, especially on farms and in small family-owned shops.
Very heavy strategic bombing by the Allies targeted refineries producing synthetic oil and gasoline as well as the German transportation system, especially rail yards and canals. The armaments industry began to break down by September 1944. By November fuel coal was no longer reaching its destinations, and the production of new armaments was no longer possible. Overy argues that the bombing strained the German war economy and forced it to divert up to one-fourth of its manpower and industry into anti-aircraft resources, which very likely shortened the war.
Racial policy.
Racism and antisemitism were basic tenets of the NSDAP and the Nazi regime. Nazi Germany's racial policy was based on their belief in the existence of a superior master race. The Nazis postulated the existence of a racial conflict between the Aryan master race and inferior races, particularly Jews, who were viewed as a mixed race that had infiltrated society and were responsible for the exploitation and repression of the Aryan race.
Persecution of Jews.
Discrimination against Jews began immediately after the seizure of power; following a month-long series of attacks by members of the SA on Jewish businesses, synagogues, and members of the legal profession, on 1 April 1933 Hitler declared a national boycott of Jewish businesses. The Law for the Restoration of the Professional Civil Service, passed on 7 April, forced all non-Aryan civil servants to retire from the legal profession and civil service. Similar legislation soon deprived Jewish members of other professions of their right to practise. On 11 April a decree was promulgated that stated anyone who had even one Jewish parent or grandparent was considered non-Aryan. As part of the drive to remove Jewish influence from cultural life, members of the National Socialist Student League removed from libraries any books considered un-German, and a nationwide book burning was held on 10 May.
Violence and economic pressure were used by the regime to encourage Jews to voluntarily leave the country. Jewish businesses were denied access to markets, forbidden to advertise in newspapers, and deprived of access to government contracts. Citizens were harassed and subjected to violent attacks. Many towns posted signs forbidding entry to Jews.
In November 1938, a young Jewish man requested an interview with the German ambassador in Paris. He met with a legation secretary, whom he shot and killed to protest his family's treatment in Germany. This incident provided the pretext for a pogrom the NSDAP incited against the Jews on 9 November 1938. Members of the SA damaged or destroyed synagogues and Jewish property throughout Germany. At least 91 German Jews were killed during this pogrom, later called "Kristallnacht", the Night of Broken Glass. Further restrictions were imposed on Jews in the coming months – they were forbidden to own businesses or work in retail shops, drive cars, go to the cinema, visit the library, or own weapons. Jewish pupils were removed from schools. The Jewish community was fined one billion marks to pay for the damage caused by "Kristallnacht" and told that any money received via insurance claims would be confiscated. By 1939 around 250,000 of Germany's 437,000 Jews emigrated to the United States, Argentina, Great Britain, Palestine, and other countries. Many chose to stay in continental Europe. Emigrants to Palestine were allowed to transfer property there under the terms of the Haavara Agreement, but those moving to other countries had to leave virtually all their property behind, and it was seized by the government.
Persecution of Roma and other groups.
Like the Jews, the Romani people were subjected to persecution from the early days of the regime. As a non-Aryan race, they were forbidden to marry people of German extraction. Romani were shipped to concentration camps starting in 1935 and were killed in large numbers. Action T4 was a programme of systematic murder of the physically and mentally handicapped and patients in psychiatric hospitals that mainly took place from 1939 to 1941 but continued until the end of the war. Initially the victims were shot by the "Einsatzgruppen" and others, but gas chambers were used by the end of 1941. Under the provisions of a law promulgated 14 July 1933, the Nazi regime carried out the compulsory sterilisation of over 400,000 individuals labelled as having hereditary defects. More than half the people sterilised were those considered mentally deficient, which included not only people who scored poorly on intelligence tests, but those who deviated from expected standards of behaviour regarding thrift, sexual behaviour, and cleanliness. Mentally and physically ill people were also targeted. The majority of the victims came from disadvantaged groups such as prostitutes, the poor, the homeless, and criminals. Other groups persecuted and killed included Jehovah's Witnesses, homosexuals, social misfits, and members of the political and religious opposition.
The Holocaust.
Germany's war in the East was based on Hitler's long-standing view that Jews were the great enemy of the German people and that "Lebensraum" was needed for Germany's expansion. Hitler focused his attention on Eastern Europe, aiming to defeat Poland, the Soviet Union and remove or kill the resident Jews and Slavs in the process. After the occupation of Poland, all Jews living in the General Government were confined to ghettos, and those who were physically fit were required to perform compulsory labour. In 1941 Hitler decided to destroy the Polish nation completely. He planned that within 10 to 20 years the section of Poland under German occupation would be cleared of ethnic Poles and resettled by German colonists. About 3.8 to 4 million Poles would remain as slaves, part of a slave labour force of 14 million the Nazis intended to create using citizens of conquered nations in the East.
The "Generalplan Ost" (General Plan for the East) called for deporting the population of occupied Eastern Europe and the Soviet Union to Siberia, for use as slave labour or to be murdered. To determine who should be killed, Himmler created the "Volksliste", a system of classification of people deemed to be of German blood. He ordered that those of Germanic descent who refused to be classified as ethnic Germans should be deported to concentration camps, have their children taken away, or be assigned to forced labour. The plan also included the kidnapping of children deemed to have Aryan-Nordic traits, who were presumed to be of German descent. The goal was to implement "Generalplan Ost" after the conquest of the Soviet Union, but when the invasion failed, Hitler had to consider other options. One suggestion was a mass forced deportation of Jews to Poland, Palestine, or Madagascar.
Somewhere around the time of the failed offensive against Moscow in December 1941, Hitler resolved that the Jews of Europe were to be exterminated immediately. Plans for the total eradication of the Jewish population of Europe—eleven million people—were formalised at the Wannsee Conference on 20 January 1942. Some would be worked to death and the rest would be killed in the implementation of "Die Endlösung der Judenfrage" (the Final Solution of the Jewish question). Initially the victims were killed with gas vans or by "Einsatzgruppen" firing squads, but these methods proved impracticable for an operation of this scale. By 1941, killing centres at Auschwitz concentration camp, Sobibor, Treblinka, and other Nazi extermination camps replaced "Einsatzgruppen" as the primary method of mass killing. The total number of Jews murdered during the war is estimated at 5.5 to six million people, including over a million children. Twelve million people were put into forced labour.
German citizens (despite much of the later denial) had access to information about what was happening, as soldiers returning from the occupied territories would report on what they had seen and done. Evans states that most German citizens disapproved of the genocide. Some Polish citizens tried to rescue or hide the remaining Jews, and members of the Polish underground got word to their government in exile in London as to what was happening.
In addition to eliminating Jews, the Nazis also planned to reduce the population of the conquered territories by 30 million people through starvation in an action called the Hunger Plan. Food supplies would be diverted to the German army and German civilians. Cities would be razed and the land allowed to return to forest or resettled by German colonists. Together, the Hunger Plan and "Generalplan Ost" would have led to the starvation of 80 million people in the Soviet Union. These partially fulfilled plans resulted in the democidal deaths of an estimated 19.3 million civilians and prisoners of war.
Oppression of ethnic Poles.
During the German occupation of Poland, 2.7 million ethnic Poles were killed by the Nazis. Polish civilians were subject to forced labour in German industry, internment, wholesale expulsions to make way for German colonists and mass executions. The German authorities engaged in a systematic effort to destroy Polish culture and national identity. During operation AB-Aktion, many university professors and members of the Polish intelligentsia were arrested and executed, or transported to concentration camps. During the war, Poland lost an estimated 39 to 45 percent of its physicians and dentists, 26 to 57 percent of its lawyers, 15 to 30 percent of its teachers, 30 to 40 percent of its scientists and university professors, and 18 to 28 percent of its clergy. Further, 43 percent of Poland's educational and research institutions and 14 percent of its museums had been destroyed.
Mistreatment of Soviet POWs.
During the war between June 1941 and January 1942, the Nazis killed an estimated 2.8 million Soviet prisoners of war. Many starved to death while being held in open-air pens at Auschwitz and elsewhere. The Soviet Union lost 27 million people during the war; less than nine million of these were combat deaths. One in four of the population were killed or wounded.
Society.
Education.
Antisemitic legislation passed in 1933 led to the removal all of Jewish teachers, professors, and officials from the education system. Most teachers were required to belong to the "Nationalsozialistischer Lehrerbund" (National Socialist Teachers League; NSLB), and university professors were required to join the National Socialist German Lecturers. Teachers had to take an oath of loyalty and obedience to Hitler, and those who failed to show sufficient conformity to party ideals were often reported by students or fellow teachers and dismissed. Lack of funding for salaries led to many teachers leaving the profession. The average class size increased from 37 in 1927 to 43 in 1938 due to the resulting teacher shortage.
Frequent and often contradictory directives were issued by Reich Minister of the Interior Wilhelm Frick, Bernhard Rust of the "Reichserziehungsministerium" (Ministry of Education), and various other agencies regarding content of lessons and acceptable textbooks for use in primary and secondary schools. Books deemed unacceptable to the regime were removed from school libraries. Indoctrination in National Socialist thought was made compulsory in January 1934. Students selected as future members of the party elite were indoctrinated from the age of 12 at Adolf Hitler Schools for primary education and National Political Institutes of Education for secondary education. Detailed National Socialist indoctrination of future holders of elite military rank was undertaken at Order Castles.
Primary and secondary education focused on racial biology, population policy, culture, geography, and especially physical fitness. The curriculum in most subjects, including biology, geography, and even arithmetic, was altered to change the focus to race. Military education became the central component of physical education, and education in physics was oriented toward subjects with military applications, such as ballistics and aerodynamics. Students were required to watch all films prepared by the school division of the Ministry of Public Enlightenment and Propaganda.
At universities, appointments to top posts were the subject of power struggles between the education ministry, the university boards, and the National Socialist German Students' League. In spite of pressure from the League and various government ministries, most university professors did not make changes to their lectures or syllabus during the Nazi period. This was especially true of universities located in predominately Catholic regions. Enrolment at German universities declined from 104,000 students in 1931 to 41,000 in 1939. But enrolment in medical schools rose sharply; Jewish doctors had been forced to leave the profession, so medical graduates had good job prospects. From 1934, university students were required to attend frequent and time-consuming military training sessions run by the SA. First-year students also had to serve six months in a labour camp for the "Reichsarbeitsdienst" (National Labour Service); an additional ten weeks service were required of second-year students.
Oppression of churches.
About 65 percent of the population of Germany was Protestant when the Nazis seized power in 1933. Under the "Gleichschaltung" process, Hitler attempted to create a unified Protestant Reich Church from Germany's 28 existing Protestant churches, with the ultimate goal of eradication of the churches in Germany. Ludwig Müller, a pro-Nazi, was installed as Reich Bishop, and the German Christians, a pro-Nazi pressure group, gained control of the new church. They objected to the Old Testament because of its Jewish origins, and demanded that converted Jews be barred from their church. Pastor Martin Niemöller responded with the formation of the Confessing Church, from which some clergymen opposed the Nazi regime. When in 1935 the Confessing Church synod protested the Nazi policy on religion, 700 of their pastors were arrested. Müller resigned and Hitler appointed Hanns Kerrl as Minister for Church Affairs, to continue efforts to control Protestantism. In 1936, a Confessing Church envoy protested to Hitler against the religious persecutions and human rights abuses. Hundreds more pastors were arrested. The church continued to resist, and by early 1937 Hitler abandoned his hope of uniting the Protestant churches. The Confessing Church was banned on 1 July 1937. Neimoller was arrested and confined, first in Sachsenhausen concentration camp and then at Dachau. Theological universities were closed and more pastors and theologians were arrested.
Persecution of the Catholic Church in Germany followed the Nazi takeover. Hitler moved quickly to eliminate political catholicism, rounding up functionaries of the Catholic-aligned Bavarian People's Party and Catholic Centre Party, which, along with all other non-Nazi political parties, ceased to exist by July. The "Reichskonkordat" (Reich Concordat) treaty with the Vatican was signed in 1933, amid continuing harassment of the church in Germany. The treaty required the regime to honour the independence of Catholic institutions and prohibited clergy from involvement in politics. Hitler routinely disregarded the Concordat, closing all Catholic institutions whose functions were not strictly religious. Clergy, nuns, and lay leaders were targeted, with thousands of arrests over the ensuing years, often on trumped-up charges of currency smuggling or immorality. Several high profile Catholic lay leaders were targeted in the 1934 Night of the Long Knives assassinations. Most Catholic youth groups refused to dissolve themselves and Hitler Youth leader Baldur von Schirach encouraged members to attack Catholic boys in the streets. Propaganda campaigns claimed the church was corrupt, restrictions were placed on public meetings, and Catholic publications faced censorship. Catholic schools were required to reduce religious instruction and crucifixes were removed from state buildings.
Pope Pius XI had the ""Mit brennender Sorge"" ("With Burning Concern") Encyclical smuggled into Germany for Passion Sunday 1937 and read from every pulpit. It denounced the systematic hostility of the regime toward the church. In response, Goebbels renewed the regime's crackdown and propaganda against Catholics. Enrolment in denominational schools dropped sharply, and by 1939 all such schools were disbanded or converted to public facilities. Later Catholic protests included the 22 March 1942 pastoral letter by the German bishops on "The Struggle against Christianity and the Church". About 30 percent of Catholic priests were disciplined by police during the Nazi era. A vast security network spied on the activities of clergy, and priests were frequently denounced, arrested, or sent to concentration camps – many to the dedicated clergy barracks at Dachau. In the areas of Poland annexed in 1940, the Nazis instigated a brutal suppression and systematic dismantling of the Catholic Church.
Health.
Nazi Germany had a strong anti-tobacco movement. Pioneering research by Franz H. Müller in 1939 demonstrated a causal link between tobacco smoking and lung cancer. The Reich Health Office took measures to try to limit smoking, including producing lectures and pamphlets. Smoking was banned in many workplaces, on trains, and among on-duty members of the military. Government agencies also worked to control other carcinogenic substances such as asbestos and pesticides. As part of a general public health campaign, water supplies were cleaned up, lead and mercury were removed from consumer products, and women were urged to undergo regular screenings for breast cancer.
Government-run health care insurance plans were available, but Jews were denied coverage starting in 1933. That same year, Jewish doctors were forbidden to treat government-insured patients. In 1937 Jewish doctors were forbidden to treat non-Jewish patients, and in 1938 their right to practice medicine was removed entirely.
Medical experiments, many of them pseudoscientific, were performed on concentration camp inmates beginning in 1941. The most notorious doctor to perform medical experiments was SS-"Hauptsturmführer" Dr Josef Mengele, camp doctor at Auschwitz. Many of his victims died or were intentionally killed. Concentration camp inmates were made available for purchase by pharmaceutical companies for drug testing and other experiments.
Role of women and family.
Women were a cornerstone of Nazi social policy. The Nazis opposed the feminist movement, claiming that it was the creation of Jewish intellectuals, and instead advocated a patriarchal society in which the German woman would recognise that her "world is her husband, her family, her children, and her home." Soon after the seizure of power, feminist groups were shut down or incorporated into the National Socialist Women's League. This organisation coordinated groups throughout the country to promote motherhood and household activities. Courses were offered on childrearing, sewing, and cooking. The League published the "NS-Frauen-Warte", the only NSDAP-approved women's magazine in Nazi Germany. Despite some propaganda aspects, it was predominantly an ordinary woman's magazine.
Women were encouraged to leave the workforce, and the creation of large families by racially suitable women was promoted through a propaganda campaign. Women received a bronze award—known as the "Ehrenkreuz der Deutschen Mutter" (Cross of Honour of the German Mother)—for giving birth to four children, silver for six, and gold for eight or more. Large families received subsidies to help with their utilities, school fees, and household expenses. Though the measures led to increases in the birth rate, the number of families having four or more children declined by five percent between 1935 and 1940. Removing women from the workforce did not have the intended effect of freeing up jobs for men. Women were for the most part employed as domestic servants, weavers, or in the food and drink industries—jobs that were not of interest to men. Nazi philosophy prevented large numbers of women from being hired to work in munitions factories in the build-up to the war, so foreign labourers were brought in. After the war started, slave labourers were extensively used. In January 1943 Hitler signed a decree requiring all women under the age of fifty to report for work assignments to help the war effort. Thereafter, women were funnelled into agricultural and industrial jobs. By September 1944, 14.9 million women were working in munitions production.
The Nazi regime discouraged women from seeking higher education. Nazi leaders held conservative views about women and endorsed the idea that rational and theoretical work was alien to a woman's nature since they were considered inherently emotional and instinctive – as such, engaging in academics and careerism would only "divert them from motherhood." The number of women allowed to enrol in universities dropped drastically, as a law passed in April 1933 limited the number of females admitted to university to ten percent of the number of male attendees. Female enrolment in secondary schools dropped from 437,000 in 1926 to 205,000 in 1937. The number of women enrolled in post-secondary schools dropped from 128,000 in 1933 to 51,000 in 1938. However, with the requirement that men be enlisted into the armed forces during the war, women comprised half of the enrolment in the post-secondary system by 1944.
Women were expected to be strong, healthy, and vital. The sturdy peasant woman who worked the land and bore strong children was considered ideal, and athletic women were praised for being tanned from working outdoors. Organisations were created for the indoctrination of Nazi values. From 25 March 1939, membership in the Hitler Youth became compulsory for all children over the age of ten. The "Jungmädelbund" (Young Girls League) section of the Hitler Youth was for girls age 10 to 14, and the "Bund Deutscher Mädel" (BDM; League of German Girls) was for young women age 14 to 18. The BDM's activities focused on physical education, with activities such as running, long jumping, somersaulting, tightrope walking, marching, and swimming.
The Nazi regime promoted a liberal code of conduct regarding sexual matters, and was sympathetic to women who bore children out of wedlock. Promiscuity increased as the war progressed, with unmarried soldiers often intimately involved with several women simultaneously. The same was the case for married women, who liaised with soldiers, civilians, or slave labourers. Sex was sometimes used as a commodity to obtain, for example, better work from a foreign labourer. Pamphlets enjoined German women to avoid sexual relations with foreign workers as a danger to their blood.
With Hitler's approval, Himmler intended that the new society of the Nazi regime should de-stigmatise illegitimate births, particularly of children fathered by members of the SS, who were vetted for racial purity. His hope was that each SS family would have between four and six children. The "Lebensborn" (Fountain of Life) association, founded by Himmler in 1935, created a series of maternity homes where single mothers could be accommodated during their pregnancies. Both parents were examined for racial suitability before acceptance. The resulting children were often adopted into SS families. The homes were also made available to the wives of SS and NSDAP members, who quickly filled over half the available spots.
Existing laws banning abortion except for medical reasons were strictly enforced by the Nazi regime. The number of abortions declined from 35,000 per year at the start of the 1930s to fewer than 2,000 per year at the end of the decade. In 1935 a law was passed allowing abortions for eugenics reasons.
Environmentalism.
Nazi society had elements supportive of animal rights, and many people were fond of zoos and wildlife. The government took several measures to ensure the protection of animals and the environment. In 1933, the Nazis enacted a stringent animal-protection law that had an impact on what was allowed for medical research. But the law was only loosely enforced. In spite of a ban on vivisection, the Ministry of the Interior readily handed out permits for experiments on animals.
The Reich Forestry Office, under Göring, enforced regulations that required foresters to plant a wide variety of trees to ensure suitable habitat for wildlife. A new Reich Animal Protection Act became law in 1933. The regime enacted the Reich Nature Protection Act in 1935 to protect the natural landscape from excessive economic development. The act allowed for the expropriation of privately owned land to create nature preserves and aided in long-range planning. Perfunctory efforts were made to curb air pollution, but little enforcement of existing legislation was undertaken once the war began.
Culture.
The regime promoted the concept of "Volksgemeinschaft", a national German ethnic community. The goal was to build a classless society based on racial purity and the perceived need to prepare for warfare, conquest, and a struggle against Marxism. The German Labour Front founded the "Kraft durch Freude" (KdF; Strength Through Joy) organisation in 1933. In addition to taking control of tens of thousands of previously privately run recreational clubs, it offered highly regimented holidays and entertainment experiences such as cruises, vacation destinations, and concerts.
The "Reichskulturkammer" (Reich Chamber of Culture) was organised under the control of the Propaganda Ministry in September 1933. Sub-chambers were set up to control various aspects of cultural life, such as films, radio, newspapers, fine arts, music, theatre, and literature. All members of these professions were required to join their respective organisation. Jews and people considered politically unreliable were prevented from working in the arts, and many emigrated. Books and scripts had to be approved by the Propaganda Ministry prior to publication. Standards deteriorated as the regime sought to use cultural outlets exclusively as propaganda media.
Radio became very popular in Germany during the 1930s, with over 70 percent of households owning a receiver by 1939, more than any other country. Radio station staffs were purged of leftists and others deemed undesirable by July 1933. Propaganda and speeches were typical radio fare immediately after the seizure of power, but as time went on Goebbels insisted that more music be played so that people would not turn to foreign broadcasters for entertainment.
As with other media, newspapers were controlled by the state, with the Reich Press Chamber shutting down or buying newspapers and publishing houses. By 1939 over two-thirds of the newspapers and magazines were directly owned by the Propaganda Ministry. The NSDAP daily newspaper, the "Völkischer Beobachter" (Ethnic Observer), was edited by Alfred Rosenberg, author of "The Myth of the Twentieth Century", a book of racial theories espousing Nordic superiority. Goebbels controlled the wire services and insisted that all newspapers in Germany should only publish content favourable to the regime. His propaganda ministry issued two dozen directives every week on exactly what news should be published and what angles to use; the typical newspaper followed the directives very closely. Newspaper readership plummeted, partly because of the decreased quality of the content, and partly because of the surge in popularity of radio.
Authors of books left the country in droves, and some wrote material highly critical of the regime while in exile. Goebbels recommended that the remaining authors should concentrate on books themed on Germanic myths and the concept of blood and soil. By the end of 1933 over a thousand books, most of them by Jewish authors or featuring Jewish characters, had been banned by the Nazi regime.
Hitler took a personal interest in architecture, and worked closely with state architects Paul Troost and Albert Speer to create public buildings in a neoclassical style based on Roman architecture. Speer constructed imposing structures such as the Nazi party rally grounds in Nuremberg and a new Reich Chancellery building in Berlin. Hitler's plans for rebuilding Berlin included a gigantic dome based on the Pantheon in Rome and a triumphal arch more than double the height of the Arc de Triomphe in Paris. Neither of these structures were ever built.
Hitler felt that abstract, Dadaist, expressionist, and modern art were decadent, an opinion that became the basis for policy. Many art museum directors lost their posts in 1933 and were replaced by party members. Some 6,500 modern works of art were removed from museums and replaced with works chosen by a Nazi jury. Exhibitions of the rejected pieces, under titles such as "Decadence in Art", were launched in sixteen different cities by 1935. The Degenerate Art Exhibition, organised by Goebbels, ran in Munich from July to November 1937. The exhibition proved wildly popular, attracting over two million visitors.
Composer Richard Strauss was appointed president of the "Reichsmusikkammer" (Reich Music Chamber) on its founding in November 1933. As was the case with other art forms, the Nazis ostracised musicians who were not deemed racially acceptable, and for the most part did not approve of music that was too modern or atonal. Jazz music was singled out as being especially inappropriate, and foreign musicians of this genre left the country or were expelled. Hitler favoured the music of Richard Wagner, especially pieces based on Germanic myths and heroic stories, and attended the Bayreuth Festival each year from 1933.
Movies were popular in Germany in the 1930s and 1940s, with admissions of over a billion people in 1942, 1943, and 1944. By 1934 German regulations restricting currency exports made it impossible for American film makers to take their profits back to America, so the major film studios closed their German branches. Exports of German films plummeted, as their heavily antisemitic content made them impossible to show in other countries. The two largest film companies, Universum Film AG and Tobis, were purchased by the Propaganda Ministry, which by 1939 was producing most German films. The productions were not always overtly propagandistic, but generally had a political subtext and followed party lines regarding themes and content. Scripts were pre-censored.
Leni Riefenstahl's "Triumph of the Will" (1935), documenting the 1934 Nuremberg Rally, and "Olympia" (1938), covering the 1936 Summer Olympics, pioneered techniques of camera movement and editing that influenced later films. New techniques such as telephoto lenses and cameras mounted on tracks were employed. Both films remain controversial, as their aesthetic merit is inseparable from their propagandising of national socialist ideals.
Legacy.
The Allied powers organised war crimes trials, beginning with the Nuremberg trials, held from November 1945 to October 1946, of 23 top Nazi officials. They were charged with four counts—conspiracy to commit crimes, crimes against peace, war crimes, and crimes against humanity—in violation of international laws governing warfare. All but three of the defendants were found guilty; twelve were sentenced to death. The victorious Allies outlawed the NSDAP and its subsidiary organisations. The display or use of Nazi symbolism such as flags, swastikas, or greetings, is illegal in Germany and Austria, and other restrictions, mainly on public display, apply in various countries. See Swastika § Post-WWII stigmatization for details.
Nazi ideology and the actions taken by the regime are almost universally regarded as gravely immoral. Hitler, Nazism, and the Holocaust have become symbols of evil in the modern world. Interest in Nazi Germany continues in the media and the academic world. Historian Sir Richard J. Evans remarks that the era "exerts an almost universal appeal because its murderous racism stands as a warning to the whole of humanity."
The Nazi era continues to inform how Germans view themselves and their country. Virtually every family suffered losses during the war or has a story to tell. For many years Germans kept quiet about their experiences and felt a sense of communal guilt, even if they were not directly involved in war crimes. Once study of Nazi Germany was introduced into the school curriculum starting in the 1970s, people began researching the experiences of their family members. Study of the era and a willingness to critically examine its mistakes has led to the development of a strong democracy in today's Germany, but with lingering undercurrents of antisemitism and neo-Nazi thought.

</doc>
<doc id="21214" url="https://en.wikipedia.org/wiki?curid=21214" title="Naraoiidae">
Naraoiidae

Naraoiidae is a family, of extinct, soft-shelled trilobite-like arthropods, that belongs to the order Nectaspida. Species included in the Naraoiidae are known from the second half of the Lower Cambrian to the end of the Upper Silurian. The total number of collection sites is limited and distributed over a vast period of time: Maotianshan Shale and Balang Formation (China), Burgess Shale and Bertie Formation (Canada), the Šárka Formation (Czech Republic), Emu Bay Shale (Australia), Idaho and Utah (USA). This is probably due to the rare occurrence of the right circumstances for soft tissue preservation, needed for these non-calcified exoskeletons.
Ecology.
Naraoiids probably were deposit feeders ("Naraoia" and "Pseudonaraoia"), predators or scavengers ("Misszhouia"), living on the sea floor.
Description.
The species of the family "Naraoiidae" are almost flat (dorso-ventrally). The upper (or dorsal) side of the body consists of a non-calcified transversely oval or semi-circular headshield (cephalon), and a circular to long oval tailshield (pygidium) equal to or longer than the cephalon, without any body segments in between. The body is narrowed at the articulation between cephalon and pygidium. The antennas are long and many-segmented. There are no eyes. The 17 to 25 pairs of legs have two branches on a common basis, like trilobites. The outer (dorsal) branches of the limbs (exopods) have flattened side branches (setae) on the shaft (probably acting as gills). The inner branches (or endopods are composed of 6 or 7 segments (or podomeres).
Differences with other Nectaspida.
Naraoiidae lack thoracic segments (or tergites), while the species of the sister family Liwiidae have between 3 and 6 tergites.
Taxonomic history.
The taxonomic placement of the Naraoiidae has long been debated until detailed appendages were uncovered, that showed that "N. compacta" shares biramous legs of very comparable anatomy with trilobites. Some debate is still going on if the parent taxon "Nektaspida" should be included in the Trilobita, or is better placed as a sister group.

</doc>
<doc id="21215" url="https://en.wikipedia.org/wiki?curid=21215" title="Northwest Passage">
Northwest Passage

The Northwest Passage is a sea route connecting the northern Atlantic and Pacific Oceans through the Arctic Ocean, along the northern coast of North America via waterways through the Canadian Arctic Archipelago. The various islands of the archipelago are separated from one another and from the Canadian mainland by a series of Arctic waterways collectively known as the Northwest Passages or Northwestern Passages. The Parliament of Canada renamed these waterways the "Canadian Northwest Passage" in a motion that was passed unanimously in December 2009.
Sought by explorers for centuries as a possible trade route, it was discovered in 1850 by Robert McClure and first navigated by Norwegian explorer Roald Amundsen with a small expedition in 1903–1906. Until 2009, the Arctic pack ice prevented regular marine shipping throughout most of the year. Change in the pack ice (Arctic shrinkage) has rendered the waterways more navigable.
The contested sovereignty claims over the waters may complicate future shipping through the region: the Canadian government considers the Northwestern Passages part of Canadian Internal Waters, but the United States and various European countries maintain they are an international strait and transit passage, allowing free and unencumbered passage. If, as has been claimed, parts of the eastern end of the Passage are barely deep, the route's viability as a Euro-Asian shipping route is reduced.
Overview.
Before the Little Ice Age, Norwegian Vikings sailed as far north and west as Ellesmere Island, Skraeling Island and Ruin Island for hunting expeditions and trading with the Inuit and people of the Dorset culture who already inhabited the region. Between the end of the 15th century and the 20th century, colonial powers from Europe dispatched explorers in an attempt to discover a commercial sea route north and west around North America. The Northwest Passage represented a new route to the established trading nations of Asia.
England called the hypothetical northern route the "Northwest Passage". The desire to establish such a route motivated much of the European exploration of both coasts of North America. When it became apparent that there was no route through the heart of the continent, attention turned to the possibility of a passage through northern waters. There was a lack of scientific knowledge about conditions; for instance, some people believed that seawater was incapable of freezing. (As late as the mid-18th century, Captain James Cook had reported that Antarctic icebergs had yielded fresh water, seemingly confirming the hypothesis.) Explorers thought that an open water route close to the North Pole must exist. The belief that a route lay to the far north persisted for several centuries and led to numerous expeditions into the Arctic. Many ended in disaster, including that by Sir John Franklin in 1845. In search for him, Robert McClure discovered the Northwest Passage in 1850. In 1906, the Norwegian explorer Roald Amundsen first successfully completed a passage from Greenland to Alaska in the sloop "Gjøa". Since that date, several fortified ships have made the journey.
From east to west, the direction of most early exploration attempts, expeditions entered the passage from the Atlantic Ocean via the Davis Strait and through Baffin Bay. Five to seven routes have been taken through the Canadian Arctic Archipelago, via the McClure Strait, Dease Strait, and the Prince of Wales Strait, but not all of them are suitable for larger ships. From there ships passed through waterways through the Beaufort Sea, Chukchi Sea, and Bering Strait (separating Russia and Alaska), into the Pacific Ocean.
In the 21st century, major changes to the ice pack due to climate change have stirred speculation that the passage may become clear enough of ice to permit safe commercial shipping for at least part of the year. On August 21, 2007, the Northwest Passage became open to ships without the need of an icebreaker. According to Nalan Koc of the Norwegian Polar Institute, this was the first time the Passage has been clear since they began keeping records in 1972. The Northwest Passage opened again on August 25, 2008.
Thawing ocean or melting ice simultaneously opened up the Northwest Passage and the Northeast Passage (and within it, the Northern Sea Route), making it possible to sail around the Arctic ice cap. Awaited by shipping companies, this 'historic event' will cut thousands of miles off their routes. Warning, however, that the NASA satellite images indicated the Arctic may have entered a "death spiral" caused by climate change, Professor Mark Serreze, a sea ice specialist at National Snow and Ice Data Center (NSIDC), USA, said: "The passages are open. It's a historic event. We are going to see this more and more as the years go by."
Due to Arctic shrinkage, the Beluga group of Bremen, Germany, sent the first Western commercial vessels through the Northern Sea Route (Northeast Passage) in 2009. However, Canada's Prime Minister Stephen Harper announced that "ships entering the North-West passage should first report to his government."
The first commercial cargo ship to have sailed through the Northwest Passage was the in August 1969.
Routes.
The Northwest Passage includes three sections:
Many attempts were made to find a salt water exit west from Hudson Bay, but the Fury and Hecla Strait in the far north is blocked by ice. The eastern entrance and main axis of the northwest passage, the Parry Channel, was found in 1819. The approach from the west through Bering Strait is impractical because of the need to sail around ice near Point Barrow. East of Point Barrow the coast is fairly clear in summer. This area was mapped in pieces from overland in 1821-1839. This leaves the large rectangle north of the coast, south of Parry Channel and east of Baffin Island. This area was mostly mapped in 1848-1854 by ships looking for Franklin's lost expedition. The first crossing was made by Amundsen in 1903-1905. He used a small ship and hugged the coast.
Extent.
The International Hydrographic Organization defines the limits of the Northwestern Passages as follows:
Historical expeditions.
As a result of their westward explorations and their settlement of Greenland, the Vikings sailed as far north and west as Ellesmere Island, Skraeling Island and Ruin Island for hunting expeditions and trading with Inuit groups. The subsequent arrival of the Little Ice Age is thought to have been one of the reasons that European seafaring into the Northwest Passage ceased until the late 15th century.
Strait of Anián.
In 1539, Hernán Cortés commissioned Francisco de Ulloa to sail along the Baja California Peninsula on the western coast of North America. Ulloa concluded that the Gulf of California was the southernmost section of a strait supposedly linking the Pacific with the Gulf of Saint Lawrence. His voyage perpetuated the notion of the Island of California and saw the beginning of a search for the Strait of Anián.
The strait probably took its name from Ania, a Chinese province mentioned in a 1559 edition of Marco Polo's book; it first appears on a map issued by Italian cartographer Giacomo Gastaldi about 1562. Five years later Bolognini Zaltieri issued a map showing a narrow and crooked Strait of Anian separating Asia from the Americas. The strait grew in European imagination as an easy sea lane linking Europe with the residence of Khagan (the Great Khan) in Cathay (northern China).
Cartographers and seamen tried to demonstrate its reality. Sir Francis Drake sought the western entrance in 1579. The Greek pilot Juan de Fuca, sailing from Acapulco (in Mexico) under the flag of the Spanish crown, claimed he had sailed the strait from the Pacific to the North Sea and back in 1592. The Spaniard Bartholomew de Fonte claimed to have sailed from Hudson Bay to the Pacific via the strait in 1640.
Northern Atlantic.
The first recorded attempt to discover the Northwest Passage was the east-west voyage of John Cabot in 1497, sent by Henry VII in search of a direct route to the Orient. In 1524, Charles V sent Estêvão Gomes to find a northern Atlantic passage to the Spice Islands. An English expedition was launched in 1576 by Martin Frobisher, who took three trips west to what is now the Canadian Arctic in order to find the passage. Frobisher Bay, which he first charted, is named after him.
As part of another expedition, in July 1583 Sir Humphrey Gilbert, who had written a treatise on the discovery of the passage and was a backer of Frobisher, claimed the territory of Newfoundland for the English crown. On August 8, 1585, the English explorer John Davis entered Cumberland Sound, Baffin Island.
The major rivers on the east coast were also explored in case they could lead to a transcontinental passage. Jacques Cartier's explorations of the Saint Lawrence River were initiated in hope of finding a way through the continent. Cartier became persuaded that the St. Lawrence was the Passage; when he found the way blocked by rapids at what is now Montreal, he was so certain that these rapids were all that was keeping him from China (in French, "la Chine"), that he named the rapids for China. To this day, they are known as the Lachine Rapids.
In 1609, Henry Hudson sailed up what is now called the Hudson River in search of the Passage; encouraged by the saltiness of the water in the estuary, he reached present-day Albany, New York, before giving up. He later explored the Arctic and Hudson Bay. In 1611, while in James Bay, Hudson's crew mutinied. They set Hudson and his teenage son John, along with seven sick, infirm, or loyal crewmen, adrift in a small open boat. He was never seen again. Cree oral legend reports that the survivors lived and traveled with the Cree for more than a year.
On May 9, 1619, under the auspices of King Christian IV of Denmark-Norway, Jens Munk set out with 65 men and the king's two ships, the "Einhörningen" (Unicorn), a small frigate, and "Lamprenen" (Lamprey), a sloop, which were outfitted under his own supervision. His mission was to discover the Northwest Passage to the Indies and China. Munk penetrated Davis Strait as far north as 69°, found Frobisher Bay, and then spent almost a month fighting his way through Hudson Strait. In September 1619, he found the entrance to Hudson Bay and spent the winter near the mouth of the Churchill River. Cold, famine, and scurvy destroyed so many of his men that only he and two other men survived. With these men, he sailed for home with the "Lamprey" on July 16, 1620, reaching Bergen, Norway, on September 20, 1620.
René-Robert Cavelier, Sieur de La Salle built the sailing ship, "Le Griffon", in his quest to find the Northwest Passage via the upper Great Lakes. "Le Griffon" disappeared in 1679 on the return trip of her maiden voyage. In the spring of 1682, La Salle made his famous voyage down the Mississippi River to the Gulf of Mexico. La Salle led an expedition from France in 1684 to establish a French colony on the Gulf of Mexico. He was murdered by his followers in 1687.
In 1772, Samuel Hearne travelled overland northwest from Hudson Bay to the Arctic Ocean, thereby proving that there was no strait connecting Hudson Bay to the Pacific Ocean.
Northern Pacific.
In 1728 Vitus Bering, a Danish Navy officer in Russian service, used the strait first discovered by Semyon Dezhnyov in 1648 but later accredited to and named after Bering (the Bering Strait). He concluded by this sailing that North America and Russia were separate land masses. In 1741 with Lieutenant Aleksei Chirikov, he explored seeking further lands beyond Siberia. While they were separated, Chirikov discovered several of the Aleutian Islands while Bering charted the Alaskan region. His ship was wrecked off the Kamchatka Peninsula, as many of his crew were disabled by scurvy.
In 1762, the English trading ship "Octavius" reportedly hazarded the passage from the west but became trapped in sea ice. In 1775, the whaler "Herald" found the "Octavius" adrift near Greenland with the bodies of her crew frozen below decks. Thus the "Octavius" may have earned the distinction of being the first Western sailing ship to make the passage, although the fact that it took 13 years and occurred after the crew was dead somewhat tarnishes this achievement. The ships captain, Hendrick van der Heul was the quartermaster on William Kidd's expedition to the Indian Ocean in 1696. (The veracity of the "Octavius" story is questionable.)
The Spanish made several voyages to the northwest coast of North America during the late 18th century. Determining whether a Northwest Passage existed was one of the motives for their efforts. Among the voyages that involved careful searches for a Passage included the 1775 and 1779 voyages of Juan Francisco de la Bodega y Quadra. The journal of Francisco Antonio Mourelle, who served as Quadra's second in command in 1775, fell into English hands. It was translated and published in London, stimulating exploration.
Captain James Cook made use of the journal during his explorations of the region. In 1791 Alessandro Malaspina sailed to Yakutat Bay, Alaska, which was rumoured to be a Passage. In 1790 and 1791 Francisco de Eliza led several exploring voyages into the Strait of Juan de Fuca, searching for a possible Northwest Passage and finding the Strait of Georgia. To fully explore this new inland sea, an expedition under Dionisio Alcalá Galiano was sent in 1792. He was explicitly ordered to explore all channels that might turn out to be a Northwest Passage.
Cook and Vancouver.
In 1776 Captain James Cook was dispatched by the Admiralty in Great Britain on an expedition to explore the Passage. A 1745 act, when extended in 1775, promised a £20,000 prize for whoever discovered the passage. Initially the Admiralty had wanted Charles Clerke to lead the expedition, with Cook (in retirement following his exploits in the Pacific) acting as a consultant. However Cook had researched Bering's expeditions, and the Admiralty ultimately placed their faith in the veteran explorer to lead, with Clerke accompanying him.
After journeying through the Pacific, to make an attempt from the west, Cook began at Nootka Sound in April 1778. He headed north along the coastline, charting the lands and searching for the regions sailed by the Russians 40 years previously. The Admiralty's orders had commanded the expedition to ignore all inlets and rivers until they reached a latitude of 65°N. Cook, however, failed to make any progress in sighting a Northwestern Passage.
Various officers on the expedition, including William Bligh, George Vancouver, and John Gore, thought the existence of a route was 'improbable'. Before reaching 65°N they found the coastline pushing them further south, but Gore convinced Cook to sail on into the Cook Inlet in the hope of finding the route. They continued to the limits of the Alaskan peninsula and the start of the chain of Aleutian Islands. Despite reaching 70°N, they encountered nothing but icebergs.
From 1792 to 1794, the Vancouver Expedition (led by George Vancouver who had previously accompanied Cook ) surveyed in detail all the passages from the Northwest Coast. He confirmed that there was no such passage south of the Bering Strait. This conclusion was supported by the evidence of Alexander MacKenzie, who explored the Arctic and Pacific oceans in 1793.
19th century.
In the first half of the 19th century, some parts of the Northwest Passage (north of the Bering Strait) were explored separately by many expeditions, including those by John Ross, Elisha Kent Kane, William Edward Parry, and James Clark Ross; overland expeditions were also led by John Franklin, George Back, Peter Warren Dease, Thomas Simpson, and John Rae. In 1826 Frederick William Beechey explored the north coast of Alaska, discovering Point Barrow.
Sir Robert McClure was credited with the discovery of the Northwest Passage in 1851 when he looked across McClure Strait from Banks Island and viewed Melville Island. However, this strait was not navigable to ships at that time. The only usable route linking the entrances of Lancaster Sound and Dolphin and Union Strait was discovered by John Rae in 1854.
Franklin expedition.
In 1845 a lavishly equipped two-ship expedition led by Sir John Franklin sailed to the Canadian Arctic to chart the last unknown swaths of the Northwest Passage. Confidence was high, as they estimated there was less than remaining of unexplored Arctic mainland coast. When the ships failed to return, relief expeditions and search parties explored the Canadian Arctic, which resulted in a thorough charting of the region, along with a possible passage. Many artifacts from the expedition were found over the next century and a half, including notes that the ships were ice-locked in 1846 near King William Island, about half way through the passage, and unable to break free. Records showed Franklin died in 1847 and Captain Francis Rawdon Moira Crozier took over command. In 1848 the expedition abandoned the two ships and its members tried to escape south across the tundra by sledge. Although some of the crew may have survived into the early 1850s, no evidence has ever been found of any survivors. In 1853 explorer John Rae was told by local Inuit about the disastrous fate of Franklin's expedition, but his reports were not welcomed in Britain.
Starvation, exposure and scurvy all contributed to the men's deaths. In 1981 Owen Beattie, an anthropologist from the University of Alberta, examined remains from sites associated with the expedition. This led to further investigations and the examination of tissue and bone from the frozen bodies of three seamen, John Torrington, William Braine and John Hartnell, exhumed from the permafrost of Beechey Island. Laboratory tests revealed high concentrations of lead in all three (the expedition carried 8,000 tins of food sealed with a lead-based solder). Another researcher has suggested botulism caused deaths among crew members. New evidence, confirming reports first made by John Rae in 1854 based on Inuit accounts, has shown that the last of the crew resorted to cannibalism of deceased members in an effort to survive.
McClure expedition.
During the search for Franklin, Commander Robert McClure and his crew in HMS "Investigator" traversed the Northwest Passage from west to east in the years 1850 to 1854, partly by ship and partly by sledge. McClure started out from England in December 1849, sailed the Atlantic Ocean south to Cape Horn and entered the Pacific Ocean. He sailed the Pacific north and passed through the Bering Strait, turning east at that point and reaching Banks Island.
McClure's ship was trapped in the ice for three winters near Banks Island, at the western end of Viscount Melville Sound. Finally McClure and his crew—who were by that time dying of starvation—were found by searchers who had travelled by sledge over the ice from a ship of Sir Edward Belcher's expedition. They rescued McClure and his crew, returning with them to Belcher's ships, which had entered the Sound from the east. McClure and his crew returned to England in 1854 on one of Belcher's ships. They were the first people known to circumnavigate the Americas and to discover and transit the Northwest Passage, albeit by ship and by sledge over the ice. (Both McClure and his ship were found by a party from HMS "Resolute", one of Belcher's ships, so his sledge journey was relatively short.)
This was an astonishing feat for that day and age, and McClure was knighted and promoted in rank. (He was made rear-admiral in 1867.) Both he and his crew also shared £10,000 awarded them by the British Parliament. In July 2010 Canadian archaeologists found his ship, HMS "Investigator," fairly intact but sunk about below the surface.
John Rae.
The expeditions by Franklin and McClure were in the tradition of British exploration: well-funded ship expeditions using modern technology, and usually including British Naval personnel. By contrast, John Rae was an employee of the Hudson's Bay Company, which operated a far-flung trade network and drove exploration of the Canadian North. They adopted a pragmatic approach and tended to be land-based. While Franklin and McClure tried to explore the passage by sea, Rae explored by land. He used dog sleds and techniques of surviving in the environment which he had learned from the native Inuit. The Franklin and McClure expeditions each employed hundreds of personnel and multiple ships. John Rae's expeditions included fewer than ten people and succeeded. Rae was also the explorer with the best safety record, having lost only one man in years of traversing Arctic lands. In 1854, Rae returned to the cities with information from the Inuit about the disastrous fate of the Franklin expedition.
Amundsen expedition.
The first explorer to conquer the Northwest Passage solely by ship was the Norwegian explorer Roald Amundsen. In a three-year journey between 1903 and 1906, Amundsen explored the passage with a crew of six. Amundsen, who had sailed to escape creditors seeking to stop the expedition, completed the voyage in the converted 45 net register tonnage () herring boat "Gjøa". "Gjøa" was much smaller than vessels used by other Arctic expeditions and had a shallow draft. Amundsen intended to hug the shore, live off the limited resources of the land and sea through which he was to travel, and had determined that he needed to have a tiny crew to make this work. (Trying to support much larger crews had contributed to the catastrophic failure of John Franklin's expedition fifty years previously). The ship's shallow draft was intended to help her traverse the shoals of the Arctic straits.
Amundsen set out from Kristiania (Oslo) in June 1903 and was west of the Boothia Peninsula by late September. The "Gjøa" was put into a natural harbour on the south shore of King William Island; by October 3 she was iced in. There the expedition remained for nearly two years, with the expedition members learning from the local Inuit people and undertaking measurements to determine the location of the North Magnetic Pole. The harbour, now known as Gjoa Haven, later developed as the only permanent settlement on the island.
After completing the Northwest Passage portion of this trip and having anchored near Herschel Island, Amundsen skied to the city of Eagle, Alaska. He sent a telegram announcing his success and skied the return 800 km to rejoin his companions. Although his chosen east–west route, via the Rae Strait, contained young ice and thus was navigable, some of the waterways were extremely shallow ( deep), making the route commercially impractical.
Later expeditions.
The first traversal of the Northwest Passage via dog sled was accomplished by Greenlander Knud Rasmussen while on the Fifth Thule Expedition (1921–1924). Rasmussen and two Greenland Inuit travelled from the Atlantic to the Pacific over the course of 16 months via dog sled.
Canadian RCMP officer Henry Larsen was the second to sail the passage, crossing west to east, leaving Vancouver 23 June 1940 and arriving at Halifax on 11 October 1942. More than once on this trip, he was uncertain whether the "St. Roch," a Royal Canadian Mounted Police "ice-fortified" schooner, would survive the pressures of the sea ice. At one point, Larsen wondered "if we had come this far only to be crushed like a nut on a shoal and then buried by the ice." The ship and all but one of her crew survived the winter on Boothia Peninsula. Each of the men on the trip was awarded a medal by Canada's sovereign, King George VI, in recognition of this notable feat of Arctic navigation.
Later in 1944, Larsen's return trip was far more swift than his first. He made the trip in 86 days to sail back from Halifax, Nova Scotia to Vancouver, British Columbia. He set a record for traversing the route in a single season. The ship, after extensive upgrades, followed a more northerly, partially uncharted route.
On July 1, 1957, the United States Coast Guard Cutter "Storis" departed in company with USCGC "Bramble" and USCGC "Spar" to search for a deep-draft channel through the Arctic Ocean and to collect hydrographic information. Upon her return to Greenland waters, the "Storis" became the first U.S.-registered vessel to have circumnavigated North America. Shortly after her return in late 1957, she was reassigned to her new home port of Kodiak, Alaska.
In 1969, the made the passage, accompanied by the Canadian icebreakers CCGS "John A. Macdonald" and CCGS "Louis S. St-Laurent". The U.S. Coast Guard icebreakers "Northwind" and "Staten Island" also sailed in support of the expedition.
The "Manhattan" was a specially reinforced supertanker sent to test the viability of the passage for the transport of oil. While the "Manhattan" succeeded, the route was deemed not to be cost effective. The United States built the Alaska Pipeline instead.
In June 1977, sailor Willy de Roos left Belgium to attempt the Northwest Passage in his steel yacht "Williwaw". He reached the Bering Strait in September and after a stopover in Victoria, British Columbia, went on to round Cape Horn and sail back to Belgium, thus being the first sailor to circumnavigate the Americas entirely by ship.
In 1981 as part of the Transglobe Expedition, Ranulph Fiennes and Charles R. Burton completed the Northwest Passage. They left Tuktoyaktuk on July 26, 1981, in the open Boston Whaler and reached Tanquary Fiord on August 31, 1981. Their journey was the first open boat transit from west to east and covered around , taking a route through Dolphin and Union Strait following the south coast of Victoria and King William islands, north to Resolute Bay via Franklin Strait and Peel Sound, around the south and east coasts of Devon Island, through Hell Gate and across Norwegian Bay to Eureka, Greely Bay and the head of Tanquary Fiord. Once they reached Tanquary Fiord, they had to trek via Lake Hazen to Alert before setting up their winter base camp.
In 1984, the commercial passenger vessel MS "Explorer" (which sank in the Antarctic Ocean in 2007) became the first cruise ship to navigate the Northwest Passage.
In July 1986, Jeff MacInnis and Mike Beedell set out on an catamaran called "Perception" on a 100-day sail, west to east, through the Northwest Passage. This pair was the first to sail the passage, although they had the benefit of doing so over a couple of summers.
In July 1986, David Scott Cowper set out from England in a lifeboat, the "Mabel El Holland", and survived three Arctic winters in the Northwest Passage before reaching the Bering Strait in August 1989. He continued around the world via the Cape of Good Hope to return to England on September 24, 1990. His was the first vessel to circumnavigate the world via the Northwest Passage.
On July 1, 2000, the Royal Canadian Mounted Police patrol vessel "Nadon", having assumed the name "St Roch II", departed Vancouver on a "Voyage of Rediscovery". Nadon's mission was to circumnavigate North America via the Northwest Passage and the Panama Canal, recreating the epic voyage of her predecessor, "St. Roch." The Voyage of Rediscovery was intended to raise awareness concerning "St. Roch" and kick off the fund-raising efforts necessary to ensure the continued preservation of "St. Roch". The voyage was organized by the Vancouver Maritime Museum and supported by a variety of corporate sponsors and agencies of the Canadian government.
"Nadon" is an aluminum, catamaran-hulled, high-speed patrol vessel. To make the voyage possible, she was escorted and supported by the Canadian Coast Guard icebreaker "Simon Fraser". The Coast Guard vessel was chartered by the Voyage of Rediscovery and crewed by volunteers. Throughout the voyage, she provided a variety of necessary services, including provisions and spares, fuel and water, helicopter facilities, and ice escort; she also conducted oceanographic research during the voyage. The Voyage of Rediscovery was completed in five and a half months, with "Nadon" reaching Vancouver on December 16, 2000.
On September 1, 2001, "Northabout", an aluminium sailboat with diesel engine, built and captained by Jarlath Cunnane, completed the Northwest Passage east-to-west from Ireland to the Bering Strait. The voyage from the Atlantic to the Pacific was completed in 24 days. Cunnane cruised in the "Northabout" in Canada for two years before returning to Ireland in 2005 via the Northeast Passage; he completed the first east-to-west circumnavigation of the pole by a single sailboat. The Northeast Passage return along the coast of Russia was slower, starting in 2004, requiring an ice stop and winter over in Khatanga, Siberia. He returned to Ireland via the Norwegian coast in October 2005. On January 18, 2006, the Cruising Club of America awarded Jarlath Cunnane their Blue Water Medal, an award for "meritorious seamanship and adventure upon the sea displayed by amateur sailors of all nationalities."
On July 18, 2003, a father-and-son team, Richard and Andrew Wood, with Zoe Birchenough, sailed the yacht "Norwegian Blue" into the Bering Strait. Two months later she sailed into the Davis Strait to become the first British yacht to transit the Northwest Passage from west to east. She also became the only British vessel to complete the Northwest Passage in one season, as well as the only British sailing yacht to return from there to British waters.
In 2006 a scheduled cruise liner (the ) successfully ran the Northwest Passage, helped by satellite images telling the location of sea ice.
On May 19, 2007, a French sailor, Sébastien Roubinet, and one other crew member left Anchorage, Alaska, in "Babouche", a ice catamaran designed to sail on water and slide over ice. The goal was to navigate west to east through the Northwest Passage by sail only. Following a journey of more than , Roubinet reached Greenland on September 9, 2007, thereby completing the first Northwest Passage voyage made in one season without engine.
In April 2009, planetary scientist Pascal Lee and a team of four on the Northwest Passage Drive Expedition drove the "HMP Okarian" Humvee rover a record-setting on sea-ice from Kugluktuk to Cambridge Bay, Nunavut, the longest distance driven on sea-ice in a road vehicle. The "HMP Okarian" was being ferried from the North American mainland to the Haughton-Mars Project (HMP) Research Station on Devon Island, where it would be used as a simulator of future pressurized rovers for astronauts on the Moon and Mars. The "HMP Okarian" was eventually flown from Cambridge Bay to Resolute Bay in May 2009, and then driven again on sea-ice by Lee and a team of five from Resolute to the West coast of Devon Island in May 2010. The "HMP Okarian" reached the HMP Research Station in July 2011.
In 2009 sea ice conditions were such that at least nine small vessels and two cruise ships completed the transit of the Northwest Passage. These trips included one by Eric Forsyth on board the Westsail sailboat "Fiona", a boat he built in the 1980s. Self-financed, Forsyth, a retired engineer from the Brookhaven National Laboratory, and winner of the Cruising Club of America's Blue Water Medal, sailed the Canadian Archipelago with sailor Joey Waits, airline captain Russ Roberts and carpenter David Wilson. After successfully sailing the Passage, the 77-year-old Forsyth completed the circumnavigation of North America, returning to his home port on Long Island, New York.
On August 28, 2010, Bear Grylls and a team of five were the first rigid inflatable boat (RIB) crew to complete a point-to-point navigation between Pond Inlet on Baffin Island and Tuktoyaktuk in the Northwest Territories. Note: A Northwest Passage requires crossing the Arctic Circle twice, once each in the Atlantic and the Pacific oceans.
On August 29, 2012 the Swedish yacht "Belzebub," a 31-foot fiberglass cutter captained by Canadian Nicolas Peissel, Swede Edvin Buregren and Morgan Peissel, became the first sailboat in history to sail through McClure Strait, part of a journey of achieving the most northerly Northwest Passage in recorded history. The accomplishment is recorded in the Polar Scott Institutes record of Northwest Passage Transits and recognized by the Explorers Club
At 18:45 GMT September 18, 2012, "Best Explorer", a steel cutter , skipper Nanni Acquarone, passing between the two Diomedes, was the first Italian sailboat to complete the Northwest Passage along the classical Amundsen route. Twenty-two Italian amateur sailors took part of the trip, in eight legs from Tromsø, Norway, to King Cove, Alaska, totalling .
Setting sail from Nome, Alaska, on August 18, 2012, and reaching Nuuk, Greenland, on September 12, 2012, "The World" became the largest passenger vessel to transit the Northwest Passage. The ship, carrying 481 passengers, for 26 days and at sea, followed in the path of Captain Roald Amundsen. "The World" transit of the Northwest Passage was documented by "National Geographic" photographer Raul Touzon.
In September 2013 the became the first commercial bulk carrier to transit the Northwest Passage. She was carrying a cargo of 73,500 tons of coking coal from Port Metro Vancouver, Canada, to the Finnish Port of Pori, 15,000 more tons than would have been possible via the traditional Panama Canal route. The Northwest Passage shortened the distance by 1,000 nautical miles compared to traditional route via the Panama Canal.
International waters dispute.
The Canadian government claims that some of the waters of the Northwest Passage, particularly those in the Canadian Arctic Archipelago, are Internal waters of Canada, giving Canada the right to bar transit through these waters. Most maritime nations, including the United States and those of the European Union, classify these waters as an international strait, where foreign vessels have the right of "transit passage". In such a regime, Canada would have the right to enact fishing and environmental regulation, and fiscal and smuggling laws, as well as laws intended for the safety of shipping, but not the right to close the passage. If the passage’s deep waters become completely ice-free in summer months, they would be particularly enticing for massive supertankers that are too big to pass through the Panama Canal and must otherwise navigate around the tip of South America.
In 1985, the U.S. Coast Guard icebreaker "Polar Sea" passed through from Greenland to Alaska; the ship submitted to inspection by the Canadian Coast Guard before passing through, but the event infuriated the Canadian public and resulted in a diplomatic incident. The United States government, when asked by a Canadian reporter, indicated that they did not ask for permission as they were not legally required to. The Canadian government issued a declaration in 1986 reaffirming Canadian rights to the waters. But, the United States refused to recognize the Canadian claim. In 1988 the governments of Canada and the U.S. signed an agreement, "Arctic Cooperation", that resolved the practical issue without solving the sovereignty questions. Under the law of the sea, ships engaged in transit passage are not permitted to engage in research. The agreement states that all US Coast Guard vessels are engaged in research, and so would require permission from the Government of Canada to pass through.
In late 2005, it was reported that U.S. nuclear submarines had traveled unannounced through Canadian Arctic waters, sparking outrage in Canada. In his first news conference after the 2006 federal election, Prime Minister-designate Stephen Harper contested an earlier statement made by the U.S. ambassador that Arctic waters were international, stating the Canadian government's intention to enforce its sovereignty there. The allegations arose after the U.S. Navy released photographs of the USS "Charlotte" surfaced at the North Pole.
On April 9, 2006, Canada's Joint Task Force (North) declared that the Canadian military will no longer refer to the region as the Northwest Passage, but as the Canadian Internal Waters. The declaration came after the successful completion of Operation Nunalivut (Inuktitut for "the land is ours"), which was an expedition into the region by five military patrols.
In 2006 a report prepared by the staff of the Parliamentary Information and Research Service of Canada suggested that because of the September 11 attacks, the United States might be less interested in pursuing the international waterways claim in the interests of having a more secure North American perimeter. This report was based on an earlier paper, "The Northwest Passage Shipping Channel: Is Canada’s Sovereignty Really Floating Away?" by Andrea Charron, given to the 2004 Canadian Defence and Foreign Affairs Institute Symposium. Later in 2006 former United States Ambassador to Canada, Paul Cellucci agreed with this position; however, the succeeding ambassador, David Wilkins, stated that the Northwest Passage was in international waters.
On July 9, 2007, Prime Minister Harper announced the establishment of a deep-water port in the far North. In the government press release the Prime Minister is quoted as saying, “Canada has a choice when it comes to defending our sovereignty over the Arctic. We either use it or lose it. And make no mistake, this Government intends to use it. Because Canada’s Arctic is central to our national identity as a northern nation. It is part of our history. And it represents the tremendous potential of our future."
On July 10, 2007, Rear Admiral Timothy McGee of the United States Navy, and Rear Admiral Brian Salerno of the United States Coast Guard announced that the United States would be increasing its ability to patrol the Arctic.
Thinning ice cover and the Northwest Passage.
In the summer of 2000, two Canadian ships took advantage of thinning summer ice cover on the Arctic Ocean to make the crossing. It is thought that climate change is likely to open the passage for increasing periods of time, making it potentially attractive as a major shipping route. However the passage through the Arctic Ocean would require significant investment in escort vessels and staging ports, and it would remain seasonal. Therefore, the Canadian commercial marine transport industry does not anticipate the route as a viable alternative to the Panama Canal even within the next 10 to 20 years.
On September 14, 2007, the European Space Agency stated that ice loss that year had opened up the historically impassable passage, setting a new low of ice cover as seen in satellite measurements which went back to 1978. According to the Arctic Climate Impact Assessment, the latter part of the 20th century and the start of the 21st had seen marked shrinkage of ice cover. The extreme loss in 2007 rendered the passage "fully navigable". However, the ESA study was based only on analysis of satellite images and could in practice not confirm anything about the actual navigation of the waters of the passage. The ESA suggested the passage would be navigable "during reduced ice cover by multi-year ice pack" (namely sea ice surviving one or more summers) where previously any traverse of the route had to be undertaken during favourable seasonable climatic conditions or by specialist vessels or expeditions. The agency's report speculated that the conditions prevalent in 2007 had shown the passage may "open" sooner than expected. An expedition in May 2008 reported that the passage was not yet continuously navigable even by an icebreaker and not yet ice-free.
Scientists at a meeting of the American Geophysical Union on December 13, 2007, revealed that NASA satellites observing the western Arctic showed a 16% decrease in cloud coverage during the summer of 2007 compared to 2006. This would have the effect of allowing more sunlight to penetrate Earth's atmosphere and warm the Arctic Ocean waters, thus melting sea ice and contributing to the opening the Northwest Passage.
In 2006 the cruise liner MS "Bremen" successfully ran the Northwest Passage, helped by satellite images telling where sea ice was.
On November 28, 2008, the Canadian Broadcasting Corporation reported that the Canadian Coast Guard confirmed the first commercial ship sailed through the Northwest Passage. In September 2008, the MV "Camilla Desgagnés", owned by Desgagnés Transarctik Inc. and, along with the Arctic Cooperative, is part of Nunavut Sealift and Supply Incorporated (NSSI), transported cargo from Montreal to the hamlets of Cambridge Bay, Kugluktuk, Gjoa Haven, and Taloyoak. A member of the crew is reported to have claimed that "there was no ice whatsoever". Shipping from the east was to resume in the fall of 2009. Although sealift is an annual feature of the Canadian Arctic this is the first time that the western communities have been serviced from the east. The western portion of the Canadian Arctic is normally supplied by Northern Transportation Company Limited (NTCL) from Hay River. The eastern portion by NNSI and NTCL from Churchill and Montreal.
In January 2010, the ongoing reduction in the Arctic sea ice led telecoms cable specialist Kodiak-Kenai Cable to propose the laying of a fiberoptic cable connecting London and Tokyo, by way of the Northwest Passage, saying the proposed system would nearly cut in half the time it takes to send messages from the United Kingdom to Japan.
In September 2013 the first large sea freighter, , used the passage.
Transfer of Pacific species to North Atlantic.
Scientists believe that reduced sea ice in the Northwest Passage has permitted some new species to migrate across the Arctic Ocean. The gray whale "Eschrichtius robustus" has not been seen in the Atlantic since it was hunted to extinction there in the 18th century, but in May 2010, one such whale turned up in the Mediterranean. Scientists speculated the whale had followed its food sources through the Northwest Passage and simply kept on going.
The plankton species "Neodenticula seminae" had not been seen in the Atlantic for 800,000 years. Over the past few years, however, it has become increasingly prevalent there. Again, scientists believe that it got there through the reopened Northwest Passage.
In August 2010, two bowhead whales from West Greenland and Alaska, respectively, entered the Northwest Passage from opposite directions and spent approximately 10 days in the same area.

</doc>
<doc id="21216" url="https://en.wikipedia.org/wiki?curid=21216" title="Nevada">
Nevada

Nevada (Spanish for "snow covered") is a state in the Western, Mountain West, and Southwestern regions of the United States of America. Nevada is the 7th most extensive, the 35th most populous, and the 9th least densely populated of the 50 United States. Nearly three-quarters of Nevada's people live in Clark County, which contains the Las Vegas–Paradise metropolitan area where three of the state's four largest incorporated cities are located. Nevada's capital is Carson City. Nevada is officially known as the "Silver State" due to the importance of silver to its history and economy. It is also known as the "Battle Born State", because it achieved statehood during the Civil War (the words "Battle Born" also appear on the state flag); as the "Sage-brush State", for the native plant of the same name; and as the "Sage-hen State". Nevada borders Oregon to the northwest, Idaho to the northeast, California to the west, Arizona to the southeast and Utah to the east.
Nevada is largely desert and semiarid, much of it located within the Great Basin. Areas south of the Great Basin are located within the Mojave Desert, while Lake Tahoe and the Sierra Nevada lie on the western edge. About 86% of the state's land is managed by various jurisdictions of the U.S. federal government, both civilian and military.
Before European contact, Native Americans of the Paiute, Shoshone, and Washoe tribes inhabited the land that is now Nevada. The first Europeans to explore the region were Spanish. They called the region "Nevada" (snowy) due to the snow which covered the mountains in winter. The area formed part of the Viceroyalty of New Spain, and became part of Mexico when it gained independence in 1821. The United States annexed the area in 1848 after its victory in the Mexican–American War, and it was incorporated as part of Utah Territory in 1850. The discovery of silver at the Comstock Lode in 1859 led to a population boom that became an impetus to the creation of Nevada Territory out of western Utah Territory in 1861. Nevada became the 36th state on October 31, 1864, as the second of two states added to the Union during the Civil War (the first being West Virginia).
Nevada has a reputation for its libertarian laws. In 1900, with a population of just over 40,000 people, Nevada was by far the least populated state, with less than half the population of the next least-populated state. However, legalized gambling and lenient marriage and divorce laws transformed Nevada into a major tourist destination in the 20th century. Nevada is the only U.S. state where prostitution is legal, though it is illegal in Las Vegas (Clark County) and Reno (Washoe County) as well as Carson City, which is an independent city. The tourism industry remains Nevada's largest employer, with mining continuing as a substantial sector of the economy: Nevada is the fourth-largest producer of gold in the world.
Etymology and pronunciation.
The name "Nevada" comes from the Spanish "nevada" , meaning "snow-covered", after the Sierra Nevada ("snow-covered mountain range").
Nevadans usually pronounce the second syllable of their state name using the vowel of "bad". Many from outside the Western United States pronounce it with the vowel of "father" . Although the latter pronunciation is closer to the Spanish pronunciation, it is not the pronunciation preferred by Nevadans. State Assemblyman Harry Mortenson proposed a bill to recognize the alternate (quasi-Spanish) pronunciation of Nevada, though the bill was not supported by most legislators and never received a vote. The native pronunciation is the de facto official one, since it is the one used by the state legislature. The state's official tourism organization, TravelNevada, stylizes the name of the state as "Nevăda", with a breve accent over the "a" indicating the locally preferred pronunciation which is also available as a license plate design.
Geography.
Nevada is almost entirely within the Basin and Range Province, and is broken up by many north-south mountain ranges. Most of these ranges have endorheic valleys between them, which belies the image portrayed by the term Great Basin.
Much of the northern part of the state is within the Great Basin, a mild desert that experiences hot temperatures in the summer and cold temperatures in the winter. Occasionally, moisture from the Arizona Monsoon will cause summer thunderstorms; Pacific storms may blanket the area with snow. The state's highest recorded temperature was in Laughlin (elevation of ) on June 29, 1994. The coldest recorded temperature was set in San Jacinto in 1972, in the northeastern portion of the state.
The Humboldt River crosses the state from east to west across the northern part of the state, draining into the Humboldt Sink near Lovelock. Several rivers drain from the Sierra Nevada eastward, including the Walker, Truckee, and Carson rivers. All of these rivers are endorheic basins, ending in Walker Lake, Pyramid Lake, and the Carson Sink, respectively. However, not all of Nevada is within the Great Basin. Tributaries of the Snake River drain the far north, while the Colorado River, which also forms much of the boundary with Arizona, drains much of southern Nevada.
The mountain ranges, some of which have peaks above , harbor lush forests high above desert plains, creating sky islands for endemic species. The valleys are often no lower in elevation than , while some in central Nevada are above .
The southern third of the state, where the Las Vegas area is situated, is within the Mojave Desert. The area receives less rain in the winter but is closer to the Arizona Monsoon in the summer. The terrain is also lower, mostly below , creating conditions for hot summer days and cool to chilly winter nights (due to temperature inversion).
Nevada and California have by far the longest diagonal line (in respect to the cardinal directions) as a state boundary at just over . This line begins in Lake Tahoe nearly offshore (in the direction of the boundary), and continues to the Colorado River where the Nevada, California, and Arizona boundaries merge southwest of the Laughlin Bridge.
The largest mountain range in the southern portion of the state is the Spring Mountain Range, just west of Las Vegas. The state's lowest point is along the Colorado River, south of Laughlin.
Nevada has 172 mountain summits with of prominence. Nevada ranks second in the US, behind Alaska, and ahead of California, Montana, and Washington. Nevada is the most mountainous state in the contiguous United States.
Climate.
Nevada is the driest state in the United States. It is made up of mostly desert and semiarid climate regions, and, with the exception of the Las Vegas Valley, the average summer diurnal temperature range approaches in much of the state. While winters in northern Nevada are long and fairly cold, the winter season in the southern part of the state tends to be of short duration and mild. Most parts of Nevada receive scarce precipitation during the year. Most rain that falls in the state falls on the lee side (east and northeast slopes) of the Sierra Nevada.
The average annual rainfall per year is about ; the wettest parts get around . Nevada's highest recorded temperature is at Laughlin on June 29, 1994 and the lowest recorded temperature is at San Jacinto on January 8, 1937. Nevada's reading is the third highest statewide record high temperature of a U.S. state, just behind Arizona's reading and California's reading.
Vegetation.
The vegetation of Nevada is diverse and differs by state area. Nevada contains six biotic zones: alpine, sub-alpine, ponderosa pine, pinion-juniper, sagebrush and creosotebush.
Counties.
Nevada is divided into political jurisdictions designated as "counties". Carson City is officially a consolidated municipality; however, for many purposes under state law it is considered to be a county. As of 1919 there were 17 counties in the state, ranging from .
Lake County, one of the original nine counties formed in 1861, was renamed Roop County in 1862. Part of the county became Lassen County, California in 1864. The portion that remained in Nevada was annexed in 1883 by Washoe County.
In 1969 Ormsby County was dissolved and the Consolidated Municipality of Carson City was created by the Legislature in its place co-terminous with the old boundaries of Ormsby County.
Bullfrog County was formed in 1987 from part of Nye County. After the creation was declared unconstitutional the county was abolished in 1989.
Humboldt county was designated as a county in 1856 by Utah Territorial Legislature and again in 1861 by the new Nevada Legislature.
Clark County is the most populous county in Nevada, accounting for nearly three-quarters of its residents. Las Vegas, Nevada's most populous city, has been the county seat since the county was created. Clark County attracts numerous tourists. An estimated 44 million people visited Clark County in 2014.
Washoe County is the second most populous county of Nevada. Its county seat is Reno. Washoe County includes the Reno–Sparks metropolitan area.
Lyon County is the third most populous county. It was one of the nine original counties created in 1861. It was named after Nathaniel Lyon, the first Union General to be killed in the Civil War. Its current county seat is Yerington. Its first county seat was established at Dayton on November 29, 1861.
History.
Before 1861.
Francisco Garcés was the first European in the area, Nevada was annexed as a part of the Spanish Empire in the northwestern territory of New Spain. Administratively, the area of Nevada was part of the Commandancy General of the Provincias Internas in the Viceroyalty of New Spain. Nevada became a part of Alta California ("Upper California") province in 1804 when the Californias were split. 
With the Mexican War of Independence won in 1821, the province of Alta California became a territory (state) of Mexico, with small population. 
Jedediah Smith entered the Las Vegas Valley in 1827, and Peter Skene Ogden traveled the Humboldt River in 1828. When the Mormons created the State of Deseret in 1847, they laid claim to all of Nevada within the Great Basin and the Colorado watershed. In June 1855, William Bringhurst and 29 fellow Mormon missionaries from Utah arrived at this site just northeast of downtown Las Vegas and built a 150-foot square adobe fort, the first permanent structure erected in the valley, which remained under the control of Salt Lake City until the winter of 1858-1859.
As a result of the Mexican–American War and the Treaty of Guadalupe-Hidalgo, Mexico permanently lost Alta California in 1848. The new areas acquired by the United States continued to be administered as territories. As part of the Mexican Cession (1848) and the subsequent California Gold Rush that used Emigrant Trails through the area, the state's area evolved first as part of the Utah Territory, then the Nevada Territory (March 2, 1861; named for the Sierra Nevada).
See History of Utah, History of Las Vegas, and the discovery of the first major U.S. deposit of silver ore in Comstock Lode under Virginia City, Nevada in 1859.
Separation from Utah Territory.
On March 2, 1861, the Nevada Territory separated from the Utah Territory and adopted its current name, shortened from "Sierra Nevada" (Spanish for "snow-covered mountain range").
The 1861 southern boundary is commemorated by Nevada Historical Markers 57 and 58 in Lincoln and Nye counties.
Statehood (1864).
Eight days before the presidential election of 1864, Nevada became the 36th state in the union. Statehood was rushed to the date of October 31 to help ensure Abraham Lincoln's reelection on November 8 and post-Civil War Republican dominance in Congress, as Nevada's mining-based economy tied it to the more industrialized Union. As it turned out, however, Lincoln and the Republicans won the election handily, and did not need Nevada's help.
Nevada is one of only two states to significantly expand its borders after admission to the Union. (The other is Missouri, which acquired additional territory in 1837 due to the Platte Purchase.)
In 1866 another part of the western Utah Territory was added to Nevada in the eastern part of the state, setting the current eastern boundary.
Nevada achieved its current southern boundaries on January 18, 1867, when it absorbed the portion of Pah-Ute County in the Arizona Territory west of the Colorado River, essentially all of present-day Nevada south of the 37th parallel. The transfer was prompted by the discovery of gold in the area, and it was thought by officials that Nevada would be better able to oversee the expected population boom. This area includes most of what is now Clark County.
Mining shaped Nevada's economy for many years (see "Silver mining in Nevada"). When Mark Twain lived in Nevada during the period described in "Roughing It", mining had led to an industry of speculation and immense wealth. However, both mining and population declined in the late 19th century. However, the rich silver strike at Tonopah in 1900, followed by strikes in Goldfield and Rhyolite, again put Nevada's population on an upward trend.
Gambling and labor.
Unregulated gambling was commonplace in the early Nevada mining towns but was outlawed in 1909 as part of a nationwide anti-gambling crusade. Because of subsequent declines in mining output and the decline of the agricultural sector during the Great Depression, Nevada again legalized gambling on March 19, 1931, with approval from the legislature. Governor Fred B. Balzar's signature enacted the most liberal divorce laws in the country and open gambling. The reforms came just eight days after the federal government presented the $49 million construction contract for Boulder Dam (now Hoover Dam).
Nuclear testing.
The Nevada Test Site, northwest of the city of Las Vegas, was founded on January 11, 1951, for the testing of nuclear weapons. The site consists of about of desert and mountainous terrain. Nuclear testing at the Nevada Test Site began with a bomb dropped on Frenchman Flat on January 27, 1951. The last atmospheric test was conducted on July 17, 1962, and the underground testing of weapons continued until September 23, 1992. The location is known for having the highest concentration of nuclear-detonated weapons in the U.S.
Over 80% of the state's area is owned by the federal government. The primary reason for this is that homesteads were not permitted in large enough sizes to be viable in the arid conditions that prevail throughout desert Nevada. Instead, early settlers would homestead land surrounding a water source, and then graze livestock on the adjacent public land, which is useless for agriculture without access to water (this pattern of ranching still prevails).
Demographics.
Population.
The United States Census Bureau estimates that the population of Nevada was 2,890,845 on July 1, 2015, a 7.05% increase since the 2010 United States Census.
According to the Census Bureau's 2015 estimate, Nevada had an estimated population of 2,890,845 which is an increase of 51,746, from the prior year and an increase of 190,294, or 7.05%, since the year 2010. This includes a natural increase since the last census of 81,661 people (that is 170,451 births minus 88,790 deaths) and an increase due to net migration of 337,043 people into the state. Immigration resulted in a net increase of 66,098 people, and migration within the country produced a net increase of 270,945 people. According to the 2006 census estimate, Nevada is the eighth fastest growing state in the nation.
The center of population of Nevada is located in southern Nye County. In this county, the unincorporated town of Pahrump, located west of Las Vegas on the California state line, has grown very rapidly from 1980 to 2010. At the 2010 census, the town had 36,441 residents. Las Vegas was America's fastest-growing city and metropolitan area from 1960 to 2000, but has grown from a gulch of 100 people in 1900 to 10,000 by 1950 to 100,000 by 1970.
From about the 1940s until 2003, Nevada was the fastest-growing state in the US percentage-wise. Between 1990 and 2000, Nevada's population increased 66%, while the USA's population increased 13%. Over two thirds of the population of the state lives in the Clark County Las Vegas metropolitan area.
Henderson and North Las Vegas are among the USA's top 20 fastest-growing cities of over 100,000.
The rural community of Mesquite located northeast of Las Vegas was an example of micropolitan growth in the 1990s and 2000s. Other desert towns like Indian Springs and Searchlight on the outskirts of Las Vegas have seen some growth as well.
Large numbers of new residents in the state originate from California, which led some locals to feel that their state is being "Californicated".
Rural areas.
A small percentage of Nevada's population lives in rural areas. The culture of these places differs significantly from that of the major metropolitan areas. People in these rural counties tend to be native Nevada residents, unlike in the Las Vegas and Reno areas, where the vast majority of the population was born in another state. The rural population is also less diverse in terms of race and ethnicity. Mining plays an important role in the economies of the rural counties, with tourism being less prominent. Ranching also has a long tradition in rural Nevada.
Race.
According to the 2010 census estimates, racial distribution was as follows:
Hispanics or Latinos of any race made 26.5% of the population. In 1980, non-Hispanic whites made up 83.3% of the state's population.
The principal ancestries of Nevada's residents in 2009 have been surveyed to be the following:
Nevada is home to many cultures and nationalities. As of 2011, 63.6% of Nevada's population younger than age 1 were minorities. Las Vegas is minority majority city . Nevada also has a sizable Basque ancestry population. In Douglas, Mineral and Pershing counties, a plurality of residents are of Mexican ancestry, with Clark County (Las Vegas) alone being home to over 200,000 Mexican Americans. Nye County and Humboldt County have a plurality of Germans; and Washoe County has many Irish Americans. Americans of English descent form pluralities in Lincoln County, Churchill County, Lyon County, White Pine County and Eureka County. Las Vegas is home to rapid-growing ethnic communities, including Scandinavians, Italians, Poles, Greeks, Spaniards and Armenians. Though, Mexicans are the majority of Latinos in the state, Nevada has a relatively diverse Hispanic/Latino population.
Asian Americans lived in the state since the California Gold Rush of the 1850s brought thousands of Chinese miners to Washoe county. They were followed by a few hundred Japanese farm workers in the late 19th century. By the late 20th century, many immigrants from China, Japan, Korea, the Philippines, Bangladesh, India and Vietnam came to the Las Vegas metropolitan area. The city now has one of America's most prolific Asian American communities, with a mostly Chinese and Taiwanese area known as "Chinatown" west of I-15 on Spring Mountain Boulevard, and an "Asiatown" shopping mall for Asian customers located at Charleston Avenue and Paradise Boulevard. Filipino Americans form the largest Asian American group in the state, with a population of more than 113,000. They comprise 56.5% of the Asian American population in Nevada and constitute about 4.3% of the entire state's population.
Largely African American sections of Las Vegas ("the Meadows") and Reno can be found. Many current African-American Nevadans are newly transplanted residents from California.
According to the 2000 US Census, 16.19% of Nevada's population aged 5 and older speak Spanish at home, while 1.59% speak Filipino, and 1% speak Chinese.
At the 2010 census, 6.9% of the state's population were reported as under 5, 24.6% were under 18, and 12.0% were 65 or older. Females made up about 49.5% of the population.
Las Vegas was a major destination for immigrants from South Asia and Latin America seeking employment in the gaming and hospitality industries during the 1990s and first decade of the 21st century, but farming and construction are the biggest employers of immigrant labor.
Senior citizens (over age 65) and young children or teenagers (under age 18) form large sections of the Nevada population. The religious makeup of Nevadans includes large communities of Mormons, Roman Catholics and Evangelicals; each is known for higher birth rates and a younger than national average age. American Jews represent a large proportion of the active adult retirement community.
Data from 2000 and 2005 suggests the following figures:
Religion.
Church attendance in Nevada is among the lowest of all US states. In a 2009 Gallup poll only 30% of Nevadans said they attended church weekly or almost weekly, compared to 42% of all Americans (only four states were found to have a lower attendance rate than Nevada).
Major religious affiliations of the people of Nevada are: Roman Catholic 25%, Protestant 35%, no religion 28%, Latter-day Saint 4%, Jewish 2%, Hindu less than 1%, Buddhist 0.5% and Islam less than 0.1%.. Parts of Nevada (in the eastern parts of the state) are situated in the Mormon Corridor.
The largest denominations by number of adherents in 2010 were the Roman Catholic Church with 451,070; The Church of Jesus Christ of Latter-day Saints with 175,149; and the Southern Baptist Convention with 45,535; Buddhist congregations 14,727; Bahá'í 1,723; and Muslim 1,700. The Jewish community is represented by The Rohr Jewish Learning Institute and Chabad.
Economy.
The economy of Nevada is tied to tourism (especially entertainment and gambling related), mining, and cattle ranching. Nevada's industrial outputs are tourism, mining, machinery, printing and publishing, food processing, and electric equipment. The Bureau of Economic Analysis estimates that Nevada's total state product in 2010 was $126 billion. The state's per capita personal income in 2009 was $38,578, ranking nineteenth in the nation. Nevada's state debt in 2012 was calculated to be $7.5 billion, or $3,100 per taxpayer. As of December 2014, the state's unemployment rate was 6.8%.
Entertainment and tourism.
The economy of Nevada has long been tied to vice industries. "was founded on mining and refounded on sin—-beginning with prizefighting and easy divorce a century ago and later extending to gaming and prostitution", said the August 21, 2010 issue of "The Economist".
Resort areas like Las Vegas, Reno, Lake Tahoe, and Laughlin attract visitors from around the nation and world. In FY08 the total of 266 casinos with gaming revenue over $1m for the year, brought in revenue of $12 billion in gaming revenue, and $13 billion in non-gaming revenue. A review of gaming statistics can be found at Nevada gaming area.
Nevada has by far the most hotel rooms per capita in the United States. According to the American Hotel and Lodging Association, there were 187,301 rooms in 584 hotels (of 15 or more rooms). The state is ranked just below California, Texas, Florida, and New York in total number of rooms, but those states have much larger populations. Nevada has one hotel room for every 14 residents, far above the national average of one hotel room per 67 residents.
Prostitution is legal in parts of Nevada in licensed brothels, but only counties with populations under 400,000 residents have the option to legalize it. Although prostitution employs roughly 300 women as independent contractors, and not a major part of the Nevada economy, it is a very visible endeavor. Of the 14 counties that are permitted to legalize prostitution under state law, 8 have chosen to legalize brothels. State law prohibits prostitution in Clark County (which contains Las Vegas), and Washoe County (which contains Reno). However, prostitution is legal in Storey County, which is part of the Reno–Sparks metropolitan area.
Mining.
In portions of the state outside of the Las Vegas and Reno metropolitan areas mining plays a major economic role. By value, gold is by far the most important mineral mined. In 2004, of gold worth $2.84 billion were mined in Nevada, and the state accounted for 8.7% of world gold production (see "Gold mining in Nevada"). Silver is a distant second, with worth $69 million mined in 2004 (see "Silver mining in Nevada"). Other minerals mined in Nevada include construction aggregates, copper, gypsum, diatomite and lithium. Despite its rich deposits, the cost of mining in Nevada is generally high, and output is very sensitive to world commodity prices.
Cattle ranching.
Cattle ranching is a major economic activity in rural Nevada. Nevada's agricultural outputs are cattle, hay, alfalfa, dairy products, onions, and potatoes. As of January 1, 2006, there were an estimated 500,000 head of cattle and 70,000 head of sheep in Nevada. Most of these animals forage on rangeland in the summer, with supplemental feed in the winter. Calves are generally shipped to out-of-state feedlots in the fall to be fattened for market. Over 90% of Nevada's of cropland is used to grow hay, mostly alfalfa, for livestock feed.
Taxation.
Nevada does not have a state income tax.
The state sales tax (similar to VAT or GST) in Nevada is variable depending upon the county. The minimum statewide tax rate is 6.85%, with five counties (Elko, Esmeralda, Eureka, Humboldt, and Mineral) charging this minimum amount. All other counties assess various option taxes, making the combined state/county sales taxes rate in one county as high as 8.1%, which is the amount charged in Clark County. Sales tax in the other major counties: Carson at 7.745%, Washoe at 7.725%. The minimum Nevada sales tax rate changed on July 1, 2009.
Largest employers.
The largest employers in the state, as of the first fiscal quarter of 2011, are the following, according to the Nevada Department of Employment, Training and Rehabilitation:
Transportation.
Amtrak's "California Zephyr" train uses the Union Pacific's original transcontinental railroad line in daily service from Chicago to Emeryville, California, serving Elko, Winnemucca, and Reno. Amtrak Thruway Motorcoaches also provide connecting service from Las Vegas to trains at Needles, California, Los Angeles, and Bakersfield, California; and from Stateline, Nevada, to Sacramento, California. Las Vegas has had no passenger train service since Amtrak's Desert Wind was discontinued in 1997, although there have been a number of proposals to re-introduce service to either Los Angeles or Southern California.
The Union Pacific Railroad has some railroads in the north and in the south. Greyhound Lines provides some bus service.
Interstate 15 passes through the southern tip of the state, serving Las Vegas and other communities. I-215 and spur route I-515 also serve the Las Vegas metropolitan area. Interstate 80 crosses through the northern part of Nevada, roughly following the path of the Humboldt River from Utah in the east and the Truckee River westward through Reno into California. It has a spur route, I-580. Nevada also is served by several U.S. highways: US 6, US 50, US 93, US 95 and US 395. There are also 189 Nevada state routes. Many of Nevada's counties have a system of county routes as well, though many are not signed or paved in rural areas. Nevada is one of a few states in the U.S. that does not have a continuous interstate highway linking its two major population centers—the road connection between the Las Vegas and Reno areas is made using a combination of Interstate and U.S. highways.
The state is one of just a few in the country to allow semi-trailer trucks with three trailers—what might be called a "road train" in Australia. But American versions are usually smaller, in part because they must ascend and descend some fairly steep mountain passes.
RTC Transit is the public transit system in the Las Vegas metropolitan area. The agency is the largest transit agency in the state and operates a network of bus service across the Las Vegas Valley, including the use of The Deuce, double-decker buses, on the Las Vegas Strip and several outlying routes. RTC RIDE operates a system of local transit bus service throughout the Reno-Sparks metropolitan area. Other transit systems in the state include Carson City's JAC. Most other counties in the state do not have public transportation at all.
Additionally, a monorail system provides public transportation in the Las Vegas area. The Las Vegas Monorail line services several casino properties and the Las Vegas Convention Center on the east side of the Las Vegas Strip, running near Paradise Road, with a possible future extension to McCarran International Airport. Several hotels also run their own monorail lines between each other, which are typically several blocks in length.
McCarran International Airport in Las Vegas is the busiest airport serving Nevada. The Reno-Tahoe International Airport (formerly known as the Reno Cannon International Airport) is the other major airport in the state.
Law and government.
Government.
The government of Nevada is defined under the Constitution of Nevada as a democratic republic with three branches of government: the executive branch consisting of the Governor of Nevada and their cabinet along with the other elected constitutional officers; the legislative branch consisting of the Nevada Legislature which includes the Assembly and the Senate; and the judicial branch consisting of the Supreme Court of Nevada and lower courts.
The Governor of Nevada is the chief magistrate of Nevada, the head of the executive department of the state's government, and the commander-in-chief of the state's military forces. The current Governor of Nevada is Brian Sandoval, a Republican.
The Nevada Legislature is a bicameral body divided into an Assembly and Senate. Members of the Assembly serve for 2 years, and members of the Senate serve for 4 years. Both houses of the Nevada Legislature will be impacted by term limits starting in 2010, as Senators and Assemblymen/women will be limited to a maximum of 12 years service in each house (by appointment or election which is a lifetime limit)—a provision of the constitution which was recently upheld by the Supreme Court of Nevada in a unanimous decision. Each session of the Legislature meets for a constitutionally mandated 120 days in every odd-numbered year, or longer if the Governor calls a special session.
The Supreme Court of Nevada is the state supreme court. Original jurisdiction is divided between the District Courts (with general jurisdiction), and Justice Courts and Municipal Courts (both of limited jurisdiction).
Incorporated towns in Nevada, known as cities, are given the authority to legislate anything not prohibited by law. A recent movement has begun to permit home rule in incorporated Nevada cities to give them more flexibility and fewer restrictions from the Legislature. Town Boards for unincorporated towns are limited local governments created by either the local county commission, or by referendum, and form a purely advisory role and in no way diminish the responsibilities of the county commission that creates them.
State agencies.
State departments and agencies:
Law.
In 1900, Nevada's population was the smallest of all states and was shrinking, as the difficulties of living in a "barren desert" began to outweigh the lure of silver for many early settlers. Historian Lawrence Friedman has explained what happened next:
With the advent of air conditioning for summertime use and Southern Nevada's mild winters, the fortunes of the state began to turn around, as it did for Arizona, making these two states the fastest growing in the Union.
Prostitution.
Nevada is the only state where prostitution is legal (under the form of licensed brothels).
Prostitution is specifically illegal by state law in the state's larger jurisdictions, which include Clark County (which contains Las Vegas), Washoe County (which contains Reno), and the independent city of Carson City. Otherwise, it is legal in those counties which specifically vote to permit it. When permitted, brothels are only located in rural or isolated parts of counties.
Divorce.
Nevada's early reputation as a "divorce haven" arose from the fact that, before the no-fault divorce revolution in the 1970s, divorces were quite difficult to obtain in the United States. Already having legalized gaming and prostitution, Nevada continued the trend of boosting its profile by adopting one of the most liberal divorce statutes in the nation. This resulted in "Williams v. North Carolina (1942)", , in which the U.S. Supreme Court ruled that North Carolina had to give "full faith and credit" to a Nevada divorce.
Nevada's divorce rate tops the national average.
Taxes.
Nevada's tax laws are intended to draw new residents and businesses to the state. Nevada has no personal income tax or corporate income tax. Since Nevada does not collect income data it cannot share such information with the federal government, the IRS.
Nevada's state sales tax rate is 6.85 percent. Counties may impose additional rates via voter approval or through approval of the Legislature; therefore, the applicable sales tax will vary by county from 6.85 percent to 8.1 percent in Clark County.
Clark County, which includes Las Vegas, imposes four separate county option taxes in addition to the statewide rate – 0.25 percent for flood control, 0.50 percent for mass transit, 0.25 percent for infrastructure, and 0.25 percent for more cops. In Washoe County, which includes Reno, the sales tax rate is 7.725 percent, due to county option rates for flood control, the ReTRAC train trench project, mass transit, and an additional county rate approved under the Local Government Tax Act of 1991.
The lodging tax rate in unincorporated Clark County, which includes the Las Vegas Strip, is 12%. Within the boundaries of the cities of Las Vegas and Henderson, the lodging tax rate is 13%.
Corporations such as Apple Inc. allegedly have set up investment companies and funds in Nevada to avoid paying taxes.
Gay rights.
In 2009, the Nevada Legislature passed a bill creating a domestic partnership registry that enables gay couples to enjoy the same rights as married couples. As of 2015, gay marriage is legal in Nevada.
Incorporation.
Nevada provides friendly environment for the formation of corporations, and many (especially California) businesses have incorporated in Nevada to take advantage of the benefits of the Nevada statute. Nevada corporations offer great flexibility to the Board of Directors and simplify or avoid many of the rules that are cumbersome to business managers in some other states. In addition, Nevada has no franchise tax, although it does require businesses to have a license for which the business has to pay the state.
Financial institutions.
Similarly, many U.S. states have usury laws limiting the amount of interest a lender can charge, but federal law allows corporations to 'import' these laws from their home state.
Alcohol and other drugs.
Non-alcohol drug laws are a notable exception to Nevada's otherwise libertarian principles. It is notable for having the harshest penalties for drug offenders in the country. Nevada remains the only state to still use mandatory minimum sentencing guidelines for marijuana possession. However, it is now a misdemeanor for possession of less than one ounce but only for persons age 21 and older. In 2006, voters in Nevada defeated attempts to allow possession of 1 ounce of marijuana (for personal use) without being criminally prosecuted, (55% against legalization, 45% in favor of legalization). However, Nevada is one of the states that allows for use of marijuana for medical reasons (though this remains illegal under federal law).
Nevada has very liberal alcohol laws. Bars are permitted to remain open 24 hours, with no "last call". Liquor stores, convenience stores and supermarkets may also sell alcohol 24 hours per day, and may sell beer, wine and spirits.
Smoking.
Nevada voters enacted a smoking ban ("The Nevada Clean Indoor Air Act") in November 2006 that became effective on December 8, 2006. It outlaws smoking in most workplaces and public places. Smoking is permitted in bars, but only if the bar serves no food, or the bar is inside a larger casino. Smoking is also permitted in casinos, certain hotel rooms, tobacco shops, and brothels. However, some businesses do not obey this law and the government tends not to enforce it. In 2011, smoking restrictions in Nevada were loosened for certain places which allow only people age 21 or older inside.
Crime.
In 2006, the crime rate in Nevada was about 24% higher than the national average rate, though crime has since decreased. Property crimes accounted for about 85% of the total crime rate in Nevada, which was 21% higher than the national rate. The remaining 20.3% were violent crimes. A complete listing of crime data in the state for 2013 can be found here:
Politics.
State politics.
Due to heavy growth in the southern portion of the state, there is a noticeable divide between politics of northern and southern Nevada. The north has long maintained control of key positions in state government, even while the population of southern Nevada is larger than the rest of the state combined. The north sees the high population south becoming more influential and perhaps commanding majority rule. The south sees the north as the "old guard" trying to rule as an oligarchy. This has fostered some resentment, however, due to a term limit amendment passed by Nevada voters in 1994, and again in 1996, some of the north's hold over key positions will soon be forfeited to the south, leaving Northern Nevada with less power.
Historically, northern Nevada has been very Republican. The more rural counties of the north are among the most conservative regions of the country. Washoe County, home to Reno, has historically been strongly Republican, but has become more of a swing county at least at the federal level. Clark County, home to Las Vegas, has become increasingly Democratic.
Clark and Washoe counties have long dominated the state's politics. Between them, they cast 87 percent of Nevada's vote, and elect a substantial majority of the state legislature. The great majority of the state's elected officials are either from Las Vegas or Reno.
National politics.
Nevada has voted for the winner in every presidential election since 1912, except in 1976 when it voted for Gerald Ford over Jimmy Carter. This includes Nevada supporting Democrat Bill Clinton in 1992 and 1996, Republican George W. Bush in 2000 and 2004, and Democrat Barack Obama winning the state in both 2008 and 2012. This gives the state status as a political bellwether. Since 1912, Nevada has been carried by the presidential victor the most out of any state (25 of 26 elections). Nevada was one of only three states won by John F. Kennedy in the American West in the election of 1960, albeit narrowly.
The state's U.S. Senators are Democrat Harry Reid, the Senate Minority Leader, and Republican Dean Heller. The Governorship is held by Brian Sandoval, a Republican from Reno.
Voting.
Nevada is the only U.S. state to have a none of the above option available on its ballots. Officially called None of These Candidates, the option was first added to the ballot in 1975 and is currently used in all elections for president and 
all state constitutional positions. In the event that None of These Candidates "wins" the election, the candidate with the next-highest total is still elected.
Education.
Education in Nevada is achieved through public and private elementary, middle, and high schools, as well as colleges and universities.
A May 2015 educational reform law expanded school choice options to 450,000 Nevada students who are at up to 185% of the federal poverty level. Education savings accounts (ESAs) are enabled by the new law to help pay the tuition for private schools. Alternatively, families "can use funds in these accounts to also pay for textbooks and tutoring."
Public school districts.
Public school districts in Nevada include:
Parks and recreation areas.
Wilderness.
There are 68 designated wilderness areas in Nevada, protecting some under the jurisdiction of the National Park Service, U.S. Forest Service, and Bureau of Land Management.
State parks.
The Nevada state parks comprise protected areas managed by the state of Nevada, including state parks, state historic sites, and state recreation areas. There are currently 24 state park units, including Van Sickle Bi-State Park which opened in July 2011 and is operated in partnership with the state of California.
Sports.
Nevada is not well known for its professional sports teams, but the state takes pride in college sports, most notably its college football. College teams in the state include the Nevada Wolf Pack (representing the University of Nevada, Reno) of the Mountain West Conference (MW)—and the UNLV Rebels (representing the University of Nevada, Las Vegas), also of the MW. In 2012, The University of Nevada, Reno joined its cross-state rival in the MW.
UNLV is most remembered for its men's basketball program, which experienced its height of supremacy in the late 1980s and early 1990s. Coached by Jerry Tarkanian, the Runnin' Rebels became one of the most elite programs in the country. In 1990, UNLV won the Men's Division I Championship by defeating Duke 103–73, which set tournament records for most points scored by a team and largest margin of victory in the national title game.
In 1991, UNLV finished the regular season undefeated, a feat that would not be matched in Division I men's basketball for more than 20 years. Forward Larry Johnson won several awards, including the Naismith Award. UNLV reached the Final Four yet again, but lost their national semifinal against Duke 79–77. The Runnin' Rebels were the Associated Press pre-season No. 1 back to back (1989–90, 1990–91). North Carolina is the only other team to accomplish that (2007–08, 2008–09).
Las Vegas has hosted several professional boxing matches, most recently at the MGM Grand Garden Arena with bouts such as Mike Tyson vs. Evander Holyfield, Evander Holyfield vs. Mike Tyson II, Oscar De La Hoya vs. Floyd Mayweather and Oscar De La Hoya vs. Manny Pacquiao.
Along with significant rises in popularity in mixed martial arts (MMA), a number of fight leagues such as the UFC have taken interest in Las Vegas as a primary event location due to the number of suitable host venues. The Mandalay Bay Events Center and MGM Grand Garden Arena are among some of the more popular venues for fighting events such as MMA and have hosted several UFC and other MMA title fights. The city has held the most UFC events with 86 events.
The state is also home to the Las Vegas Motor Speedway, which hosts the Kobalt Tools 400. The Thomas & Mack Center, home to UNLV men's basketball, also hosts two major rodeo events—the National Finals Rodeo and the PBR World Finals, the latter operated by the bull riding-only Professional Bull Riders. Finally, Sam Boyd Stadium, home to the UNLV football team, also hosts the country's biggest rugby event, the USA Sevens tournament in the IRB Sevens World Series and the AMA Supercross Championship.
The state is also home to one of the most famous tennis players of all time, Andre Agassi.
Nevada sports teams.
Professional
College
The Nevada Aerospace Hall of Fame provides educational resources and promotes the aerospace and aviation history of the state.
Military.
Several United States Navy ships have been named USS "Nevada" in honor of the state. They include:
Area 51 is located near Groom Lake, a dry salt lake bed. The much smaller Creech Air Force Base is located in Indian Springs, Nevada; Hawthorne Army Depot in Hawthorne; the Tonopah Test Range near Tonopah; and Nellis AFB in the northeast part of the Las Vegas Valley. Naval Air Station Fallon in Fallon; NSAWC, (pronounced "EN-SOCK") in western Nevada. NSAWC consolidated three Command Centers into a single Command Structure under a flag officer on July 11, 1996. The Naval Strike Warfare Center (STRIKE "U") based at NAS Fallon since 1984, was joined with the Navy Fighter Weapons School (TOPGUN) and the Carrier Airborne Early Warning Weapons School (TOPDOME) which both moved from NAS Miramar as a result of a Base Realignment and Closure (BRAC) decision in 1993 which transferred that installation back to the Marine Corps as MCAS Miramar. The Seahawk Weapon School was added in 1998 to provide tactical training for Navy helicopters.
These bases host a number of activities including the Joint Unmanned Aerial Systems Center of Excellence, the Naval Strike and Air Warfare Center, Nevada Test and Training Range, Red Flag, the U.S. Air Force Thunderbirds, the United States Air Force Warfare Center, the United States Air Force Weapons School, and the United States Navy Fighter Weapons School.
Future issues.
Nevada enjoys many economic advantages, and the southern portion of the state enjoys mild winter weather, but rapid growth has led to some overcrowded roads and schools. Nevada has the nation's 5th largest school district in the Clark County School District (projected fall 2007 enrollment is 314,000 students grades K-12).
Coyote Springs is a proposed community for 240,000 inhabitants in Clark and Lincoln counties. It would be Nevada's largest planned city. The town is being developed by Harvey Whittemore and has generated some controversy because of environmental concerns and allegations of political favoritism.

</doc>
<doc id="21217" url="https://en.wikipedia.org/wiki?curid=21217" title="Native Americans in the United States">
Native Americans in the United States

In the United States, Native Americans are considered to be people whose pre-Columbian ancestors were indigenous to the lands within the nation's modern boundaries. These peoples were composed of numerous distinct tribes, bands, and ethnic groups, and many of these groups survive intact today as sovereign nations.
The terms Native Americans use to refer to themselves vary regionally and generationally, with many older Native Americans self-identifying as "Indians" or "American Indians", while younger Native Americans often identify as "Indigenous" or "Aboriginal." Which terms should be used to refer to Native Americans has at times been controversial. The term "Native American" has been adopted by major newspapers and some academic groups, but has not traditionally included Native Hawaiians or certain Alaskan Natives, such as Aleut, Yup'ik, or Inuit peoples. Indigenous American peoples from Canada are known as First Nations.
Since the end of the 15th century, the migration of Europeans to the Americas has led to centuries of exchange and adjustment between Old and New World societies. Most Native American groups had historically lived as hunter-gatherer societies and preserved their histories by oral traditions and artwork, which has resulted in the first written sources on the conflict being authored by Europeans.
At the time of first contact, the indigenous cultures were quite different from those of the proto-industrial and mostly Christian immigrants. Some of the Northeastern and Southwestern cultures in particular were matrilineal and operated on a more collective basis than the Europeans were familiar with. The majority of Indigenous American tribes maintained their hunting grounds and agricultural lands for use of the entire tribe. Europeans at that time had patriarchal cultures and had developed concepts of individual property rights with respect to land that were extremely different. The differences in cultures between the established Native Americans and immigrant Europeans, as well as shifting alliances among different nations in times of war, caused extensive political tension, ethnic violence, and social disruption. Even before the European settlement of what is now the United States, Native Americans suffered high fatalities from contact with European diseases spread throughout the Americas by the Spanish to which they had yet not acquired immunity. Smallpox epidemics are thought to have caused the greatest loss of life for indigenous populations, although estimates of the pre-Columbian population of what today constitutes the U.S. vary significantly, from one million to eighteen million.
After the thirteen colonies revolted against Great Britain and established the United States of America, President George Washington and Henry Knox conceived of the idea of "civilizing" Native Americans in preparation for assimilation as U.S. citizens. Assimilation (whether voluntary, as with the Choctaw, or forced) became a consistent policy through American administrations. During the 19th century, the ideology of manifest destiny became integral to the American nationalist movement. Expansion of European-American populations to the west after the American Revolution resulted in increasing pressure on Native American lands, warfare between the groups, and rising tensions. In 1830, the U.S. Congress passed the Indian Removal Act, authorizing the government to relocate Native Americans from their homelands within established states to lands west of the Mississippi River, accommodating European-American expansion. This resulted in the ethnic cleansing of many tribes, with the brutal, forced marches coming to be known as The Trail of Tears.
As American expansion reached into the West, settler and miner migrants came into increasing conflict with the Great Basin, Great Plains, and other Western tribes. These were complex nomadic cultures based on (introduced) horse culture and seasonal bison hunting. They carried out resistance against United States incursion in the decades after the completion of the Civil War and the Transcontinental Railroad in a series of Indian Wars, which were frequent up until the 1890s but continued into the 20th century. Over time, the United States forced a series of treaties and land cessions by the tribes and established reservations for them in many western states. U.S. agents encouraged Native Americans to adopt European-style farming and similar pursuits, but European-American agricultural technology of the time was inadequate for often dry reservation lands, leading to mass starvation. In 1924, Native Americans who were not already U.S. citizens were granted citizenship by Congress.
Contemporary Native Americans have a unique relationship with the United States because they may be members of nations, tribes, or bands with sovereignty and treaty rights. Cultural activism since the late 1960s has increased political participation and led to an expansion of efforts to teach and preserve Indigenous languages for younger generations and to establish a greater cultural infrastructure: Native Americans have founded independent newspapers and online media, recently including First Nations Experience, the first Native American television channel; established Native American studies programs, tribal schools and universities, and museums and language programs; and have increasingly been published as authors.
History.
Pre-Columbian.
"Neolithic" is not generally used to describe indigenous cultures in the Americas, see Archaeology of the Americas.
The usual theory of the settlement of the Americas is that the earliest peoples of the Americas came from Eurasia over a land bridge which connected the two continents across what is now the Bering Strait during a period of glaciation, when the sea water level was lower. The number and nature of these migrations is uncertain, but the land bridge is believed to have existed only until about 12,000 years ago, when the land bridge was flooded.
Three major migrations occurred, as traced by linguistic and genetic data; the early Paleoamericans soon spread throughout the Americas, diversifying into many hundreds of culturally distinct nations and tribes. By 8000 BCE, the North American climate was very similar to today's.
The Clovis culture, a megafauna-hunting culture, appears around 11,500–11,000 uncal RCYBP (uncalibrated radiocarbon years before present) at the end of the last glacial period. It is characterized by the manufacture of "Clovis points" and distinctive bone and ivory tools. Archaeologists' most precise determinations at present suggest that this radiocarbon age is equal to roughly 13,200 to 12,900 calendar years ago. Clovis people are considered to be the ancestors of most of the indigenous cultures of the Americas. They ranged over much of North America and also appeared in South America.
Numerous Paleoindian cultures occupied North America. According to later oral histories, Native Americans have been living on this continent since their genesis, described by a wide range of traditional creation stories. However, genetic and linguistic data connect the indigenous people of this continent with ancient northeast Asians.
The Folsom Tradition was characterized by use of Folsom points as projectile tips, and data from kill sites, where slaughter and butchering of bison took place. Folsom tools were left behind between 9000 BCE and 8000 BCE.
Na-Dené-speaking peoples entered North America starting around 8000 BCE, reaching the Pacific Northwest by 5000 BCE, and from there, migrating along the Pacific Coast and into the interior. It is believed that their ancestors comprised a separate migration into North America, later than the first Paleo-Indians. They migrated into Alaska and northern Canada, south along the Pacific Coast, into the interior of Canada, and south to the Great Plains and the American Southwest. They were the earliest ancestors of the Athabascan-speaking peoples, including the present-day and historical Navajo and Apache.
Since the 1990s, archeologists have explored and dated eleven Middle Archaic sites in present-day Louisiana and Florida at which early cultures built complexes with multiple earthwork mounds; they were societies of hunter-gatherers rather than the settled agriculturalists believed necessary according to the theory of Neolithic Revolution to sustain such large villages over long periods. The prime example is Watson Brake in northern Louisiana, whose 11-mound complex is dated to 3500 BCE, making it the oldest, dated site in the Americas for such complex construction. Construction of the mounds went on for 500 years until abandoned about 2800 BCE, probably due to changing environmental conditions. Poverty Point culture is a Late Archaic archaeological culture that inhabited the area of the lower Mississippi Valley and surrounding Gulf Coast. The culture thrived from 2200 BCE to 700 BCE, during the Late Archaic period. Artifacts show the people traded with other Native Americans located from Georgia to the Great Lakes region. This is one among numerous mound sites of complex indigenous cultures throughout the Mississippi and Ohio valleys. They were one of several succeeding cultures often referred to as mound builders.
The Woodland period of North American pre-Columbian cultures refers to the time period from roughly 1000 BCE to 1,000 CE in the eastern part of North America. The term "Woodland" was coined in the 1930s and refers to prehistoric sites dated between the Archaic period and the Mississippian cultures. The Hopewell tradition is the term for the common aspects of the Native American culture that flourished along rivers in the Northeastern and Midwestern United States from 200 BCE to 500 CE, stretching as far south as Crystal River.
Hohokam is one of the four major prehistoric archaeological traditions of the present-day American Southwest. Living as simple farmers, they raised corn and beans. The early Hohokam founded a series of small villages along the middle Gila River. The communities were located near good arable land, with dry farming common in the earlier years of this period.
The Mississippian culture, which extended throughout the Ohio and Mississippi valleys and built sites throughout the Southeast, created the largest earthworks in North America north of Mexico, most notably at Cahokia, on a tributary of the Mississippi River in present-day Illinois. The society began building at this site about 950 CE, and reached its peak population in 1,250 CE of 20,000–30,000 people, which was not equaled by any city in the present-day United States until after 1800.
Sophisticated pre-Columbian sedentary societies evolved in North America. The rise of the complex cultures was based on the people's adoption of maize agriculture, development of greater population densities, and chiefdom-level complex social organization from 1200 CE to 1650 CE. The introduction of maize from Mesoamerica allowed the accumulation of crop surpluses to support a higher density of population and led to development of specialized skills.
The Iroquois League of Nations or "People of the Long House", based in present-day upstate and western New York, had a confederacy model from the mid-15th century. It has been suggested that their culture contributed to political thinking during the development of the later United States government. Their system of affiliation was a kind of federation, different from the strong, centralized European monarchies.
Inter-tribal warfare was endemic resulting in displacement and migration of numerous tribes.
European exploration and colonization.
After 1492, European exploration and colonization of the Americas revolutionized how the Old and New Worlds perceived themselves. Many of the first major contacts were in Florida and the Gulf coast by Spanish explorers.
Impact on native populations.
From the 16th through the 19th centuries, the population of Indians sharply declined. Most mainstream scholars believe that, among the various contributing factors, epidemic disease was the overwhelming cause of the population decline of the Native Americans because of their lack of immunity to new diseases brought from Europe. It is difficult to estimate the number of pre-Columbian Native Americans who were living in what is today the United States of America. Estimates range from a low of 2.1 million to a high of 18 million (Dobyns 1983). By 1800, the Native population of the present-day United States had declined to approximately 600,000, and only 250,000 Native Americans remained in the 1890s. Chicken pox and measles, endemic but rarely fatal among Europeans (long after being introduced from Asia), often proved deadly to Native Americans. In the 100 years following the arrival of the Spanish to the Americas, large disease epidemics depopulated large parts of the eastern United States in the 16th century.
There are a number of documented cases where diseases were deliberately spread among Native Americans as a form of biological warfare. The most well known example occurred in 1763, when Sir Jeffrey Amherst, Commander-in-Chief of the Forces of the British Army, wrote praising the use of smallpox infected blankets to "extirpate" the Indian race. Blankets infected with smallpox were given to Native Americans besieging Fort Pitt. The effectiveness of the attempt is unclear.
In 1634, Fr. Andrew White of the Society of Jesus established a mission in what is now the state of Maryland, and the purpose of the mission, stated through an interpreter to the chief of an Indian tribe there, was "to extend civilization and instruction to his ignorant race, and show them the way to heaven." Fr. Andrew's diaries report that by 1640, a community had been founded which they named St. Mary's, and the Indians were sending their children there "to be educated among the English." This included the daughter of the Piscataway Indian chief Tayac, which exemplifies not only a school for Indians, but either a school for girls, or an early co-ed school. The same records report that in 1677, "a school for humanities was opened by our Society in the centre of directed by two of the Fathers; and the native youth, applying themselves assiduously to study, made good progress. Maryland and the recently established school sent two boys to St. Omer who yielded in abilities to few Europeans, when competing for the honor of being first in their class. So that not gold, nor silver, nor the other products of the earth alone, but men also are gathered from thence to bring those regions, which foreigners have unjustly called ferocious, to a higher state of virtue and cultivation."
In 1727, the Sisters of the Order of Saint Ursula founded Ursuline Academy in New Orleans, which is currently the oldest continuously operating school for girls and the oldest Catholic school in the United States. From the time of its foundation, it offered the first classes for Native American girls, and would later offer classes for female African-American slaves and free women of color.
Between 1754 and 1763, many Native American tribes were involved in the French and Indian War/Seven Years' War. Those involved in the fur trade tended to ally with French forces against British colonial militias. The British had made fewer allies, but it was joined by some tribes that wanted to prove assimilation and loyalty in support of treaties to preserve their territories. They were often disappointed when such treaties were later overturned. The tribes had their own purposes, using their alliances with the European powers to battle traditional Native enemies. Some Iroquois who were loyal to the British, and helped them fight in the American Revolution, fled north into Canada.
After European explorers reached the West Coast in the 1770s, smallpox rapidly killed at least 30% of Northwest Coast Native Americans. For the next eighty to one hundred years, smallpox and other diseases devastated native populations in the region. Puget Sound area populations, once estimated as high as 37,000 people, were reduced to only 9,000 survivors by the time settlers arrived en masse in the mid-19th century.
Smallpox epidemics in 1780–82 and 1837–38 brought devastation and drastic depopulation among the Plains Indians. By 1832, the federal government established a smallpox vaccination program for Native Americans ("The Indian Vaccination Act of 1832"). It was the first federal program created to address a health problem of Native Americans.
Animal introductions.
With the meeting of two worlds, animals, insects, and plants were carried from one to the other, both deliberately and by chance, in what is called the Columbian Exchange. In the 16th century, Spaniards and other Europeans brought horses to Mexico. Some of the horses escaped and began to breed and increase their numbers in the wild. As Native Americans adopted use of the animals, they began to change their cultures in substantial ways, especially by extending their nomadic ranges for hunting. The reintroduction of the horse to North America had a profound impact on Native American culture of the Great Plains.
King Philip's War.
King Philip's War, also called Metacom's War or Metacom's Rebellion, was the last major armed conflict between Native American inhabitants of present-day southern New England and English colonists and their Native American allies from 1675 to 1676. It continued in northern New England (primarily on the Maine frontier) even after King Philip was killed, until a treaty was signed at Casco Bay in April 1678.
Natural society.
Some European philosophers considered Native American societies to be truly "natural" and representative of a golden age known to them only in folk history.
American Revolution.
During the American Revolution, the newly proclaimed United States competed with the British for the allegiance of Native American nations east of the Mississippi River. Most Native Americans who joined the struggle sided with the British, based both on their trading relationships and hopes that colonial defeat would result in a halt to further colonial expansion onto Native American land. Many native communities were divided over which side to support in the war and others wanted to remain neutral. The first native community to sign a treaty with the new United States Government was the Lenape. For the Iroquois Confederacy, based in New York, the American Revolution resulted in civil war. The British made peace with the Americans in the Treaty of Paris (1783), through which they ceded vast Native American territories to the United States without informing or consulting with the Native Americans.
18th-century United States.
The United States was eager to expand, to develop farming and settlements in new areas, and to satisfy land hunger of settlers from New England and new immigrants. The national government initially sought to purchase Native American land by treaties. The states and settlers were frequently at odds with this policy.
United States policy toward Native Americans continued to evolve after the American Revolution. George Washington and Henry Knox believed that Native Americans were equals but that their society was inferior. Washington formulated a policy to encourage the "civilizing" process. Washington had a six-point plan for civilization which included:
In the late 18th century, reformers starting with Washington and Knox, supported educating native children and adults, in efforts to "civilize" or otherwise assimilate Native Americans to the larger society (as opposed to relegating them to reservations). The Civilization Fund Act of 1819 promoted this civilization policy by providing funding to societies (mostly religious) who worked on Native American improvement.
19th century.
As American expansion continued, Native Americans resisted settlers' encroachment in several regions of the new nation (and in unorganized territories), from the Northwest to the Southeast, and then in the West, as settlers encountered the tribes of the Great Plains. East of the Mississippi River, an intertribal army led by Tecumseh, a Shawnee chief, fought a number of engagements in the Northwest during the period 1811–12, known as Tecumseh's War. During the War of 1812, Tecumseh's forces allied themselves with the British. After Tecumseh's death, the British ceased to aid the Native Americans south and west of Upper Canada and American expansion proceeded with little resistance. Conflicts in the Southeast include the Creek War and Seminole Wars, both before and after the Indian Removals of most members of the Five Civilized Tribes. Native American nations on the plains in the west continued armed conflicts with the United States throughout the 19th century, through what were called generally "Indian Wars".
In the 1830s, President Andrew Jackson signed the Indian Removal Act of 1830, a policy of relocating Indians from their homelands to Indian Territory and reservations in surrounding areas to open their lands for non-native settlements. This resulted in the Trail of Tears.
In July 1845, the New York newspaper editor John L. O'Sullivan coined the phrase, "Manifest Destiny", as the "design of Providence" supporting the territorial expansion of the United States. Manifest Destiny had serious consequences for Native Americans, since continental expansion for the United States took place at the cost of their occupied land.
The Indian Appropriations Act of 1851 set the precedent for modern-day Native American reservations through allocating funds to move western tribes onto reservations since there were no more lands available for relocation.
Civil War.
Native Americans served in both the Union and Confederate military during the American Civil War. At the outbreak of the war, for example, the minority party of the Cherokees gave its allegiance to the Confederacy, while originally the majority party went for the North. Native Americans fought knowing they might jeopardize their independence, unique cultures, and ancestral lands if they ended up on the losing side of the Civil War. 28,693 Native Americans served in the Union and Confederate armies during the Civil War, participating in battles such as Pea Ridge, Second Manassas, Antietam, Spotsylvania, Cold Harbor, and in Federal assaults on Petersburg. A few Native American tribes, such as the Creek and the Choctaw, were slaveholders and found a political and economic commonality with the Confederacy. The Choctaw owned over 2,000 slaves.
Removals and reservations.
In the 19th century, the incessant westward expansion of the United States incrementally compelled large numbers of Native Americans to resettle further west, often by force, almost always reluctantly. Native Americans believed this forced relocation illegal, given the Treaty of Hopewell of 1785. Under President Andrew Jackson, United States Congress passed the Indian Removal Act of 1830, which authorized the President to conduct treaties to exchange Native American land east of the Mississippi River for lands west of the river.
As many as 100,000 Native Americans relocated to the West as a result of this Indian removal policy. In theory, relocation was supposed to be voluntary and many Native Americans did remain in the East. In practice, great pressure was put on Native American leaders to sign removal treaties. The most egregious violation, the Trail of Tears, was removal of the Cherokee by President Jackson to Indian Territory.
Native Americans and U.S. Citizenship.
In 1817, the Cherokee became the first Native Americans recognized as U.S. citizens. Under Article 8 of the 1817 Cherokee treaty, "Upwards of 300 Cherokees (Heads of Families) in the honest simplicity of their souls, made an election to become American citizens".
Factors establishing citizenship included:
After the American Civil War, the Civil Rights Act of 1866 states, "that all persons born in the United States, and not subject to any foreign power, excluding Indians not taxed, are hereby declared to be citizens of the United States".
Indian Appropriations Act of 1871.
In 1871, Congress added a rider to the Indian Appropriations Act, signed into law by President Ulysses S. Grant, ending United States recognition of additional Native American tribes or independent nations, and prohibiting additional treaties.
Education.
After the Indian wars in the late 19th century, the government established Native American boarding schools, initially run primarily by or affiliated with Christian missionaries. At this time, American society thought that Native American children needed to be acculturated to the general society. The boarding school experience Was a total immersion in modern American society, but it could prove traumatic to children, who were forbidden to speak their native languages. They were taught Christianity and not allowed to practice their native religions, and in numerous other ways forced to abandon their Native American identities.
Before the 1930s, schools on the reservations provided no schooling beyond the sixth grade. To obtain more, boarding school was usually necessary. Small reservations with a few hundred people usually send their children to nearby public schools. The "Indian New Deal" of the 1930s closed many of the boarding schools, and downplayed the assimilationist goals. The Indian Division of the Civilian Conservation Corps operated large-scale construction projects on the reservations, building thousands of new schools and community buildings. Under the leadership of John Collier the BIA brought in progressive educators to reshape Indian education. The Bureau of Indian Affairs (BIA) by 1938 taught 30,000 students in 377 boarding and day schools, or 40% of all Indian children in school. The Navajo largely opposed schooling of any sort, but the other tribes accepted the system. There were now high schools on larger reservations, make educated not only teenagers but also an adult audience. There were no Indian facilities for higher education. They deemphasized textbooks, emphasized self-esteem, and started teaching Indian history. They promoted traditional arts and crafts of the sort that could be conducted on the reservations, such as making jewelry. The New Deal reformers met significant resistance from parents and teachers, and had mixed results. World War II brought younger Indians in contact with the broader society through military service and work in the munitions industries. The role of schooling was changed to focus on vocational education for jobs in urban America.
Since the rise of self-determination for Native Americans, they have generally emphasized education of their children at schools near where they live. In addition, many federally recognized tribes have taken over operations of such schools and added programs of language retention and revival to strengthen their cultures. Beginning in the 1970s, tribes have also founded colleges at their reservations, controlled, and operated by Native Americans, to educate their young for jobs as well as to pass on their cultures.
20th century.
On August 29, 1911, Ishi, generally considered to have been the last Native American to live most of his life without contact with European-American culture, was discovered near Oroville, California.
On June 2, 1924, U.S. Republican President Calvin Coolidge signed the Indian Citizenship Act, which made all Native Americans born in the United States and its territories American citizens. Prior to passage of the act, nearly two-thirds of Native Americans were already U.S. citizens.
American Indians today in the United States have all the rights guaranteed in the U.S. Constitution, can vote in elections, and run for political office. Controversies remain over how much the federal government has jurisdiction over tribal affairs, sovereignty, and cultural practices.
Mid-century, the Indian termination policy and the Indian Relocation Act of 1956 marked a new direction for assimilating Native Americans into urban life. 
The census counted 332,000 Indians in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Indians averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940.
World War II.
Some 44,000 Native Americans served in the United States military during World War II: at the time, one-third of all able-bodied Indian men from eighty to fifty years of age. Described as the first large-scale exodus of indigenous peoples from the reservations since the removals of the 19th century, the men's service with the U.S. military in the international conflict was a turning point in Native American history. The overwhelming majority of Native Americans welcomed the opportunity to serve; they had a voluntary enlistment rate that was 40% higher than those drafted.
Their fellow soldiers often held them in high esteem, in part since the legend of the tough Native American warrior had become a part of the fabric of American historical legend. White servicemen sometimes showed a lighthearted respect toward Native American comrades by calling them "chief". The resulting increase in contact with the world outside of the reservation system brought profound changes to Native American culture. "The war", said the U.S. Indian Commissioner in 1945, "caused the greatest disruption of Native life since the beginning of the reservation era", affecting the habits, views, and economic well-being of tribal members. The most significant of these changes was the opportunity—as a result of wartime labor shortages—to find well-paying work in cities, and many people relocated to urban areas, particularly on the West Coast with the buildup of the defense industry.
There were also losses as a result of the war. For instance, a total of 1,200 Pueblo men served in World War II; only about half came home alive. In addition, many more Navajo served as code talkers for the military in the Pacific. The code they made, although cryptologically very simple, was never cracked by the Japanese.
Self-determination.
Military service and urban residency contributed to the rise of American Indian activism, particularly after the 1960s and the occupation of Alcatraz Island (1969–1971) by a student Indian group from San Francisco. In the same period, the American Indian Movement (AIM) was founded in Minneapolis, and chapters were established throughout the country, where American Indians combined spiritual and political activism. Political protests gained national media attention and the sympathy of the American public.
Through the mid-1970s, conflicts between governments and Native Americans occasionally erupted into violence. A notable late 20th-century event was the Wounded Knee incident on the Pine Ridge Indian Reservation. Upset with tribal government and the failures of the federal government to enforce treaty rights, about 300 Oglala Lakota and AIM activists took control of Wounded Knee on February 27, 1973.
Indian activists from around the country joined them at Pine Ridge, and the occupation became a symbol of rising American Indian identity and power. Federal law enforcement officials and the national guard cordoned off the town, and the two sides had a standoff for 71 days. During much gunfire, one United States Marshal was wounded and paralyzed. In late April, a Cherokee and local Lakota man were killed by gunfire; the Lakota elders ended the occupation to ensure no more lives were lost.
In June 1975, two FBI agents seeking to make an armed robbery arrest at Pine Ridge Reservation were wounded in a firefight, and killed at close range. The AIM activist Leonard Peltier was sentenced in 1976 to two consecutive terms of life in prison in the FBI deaths.
In 1968, the government enacted the Indian Civil Rights Act. This gave tribal members most of the protections against abuses by tribal governments that the Bill of Rights accords to all U.S. citizens with respect to the federal government. In 1975, the U.S. government passed the Indian Self-Determination and Education Assistance Act, marking the culmination of fifteen years of policy changes. It resulted from American Indian activism, the Civil Rights Movement, and community development aspects of President Lyndon Johnson's social programs of the 1960s. The Act recognized the right and need of Native Americans for self-determination. It marked the U.S. government's turn away from the 1950s policy of termination of the relationship between tribes and the government. The U.S. government encouraged Native Americans' efforts at self-government and determining their futures. Tribes have developed organizations to administer their own social, welfare and housing programs, for instance. Tribal self-determination has created tension with respect to the federal government's historic trust obligation to care for Indians; however, the Bureau of Indian Affairs has never lived up to that responsibility.
Tribal colleges.
Navajo Community College, now called Diné College, the first tribal college, was founded in Tsaile, Arizona, in 1968 and accredited in 1979. Tensions immediately arose between two philosophies: one that the tribal colleges should have the same criteria, curriculum and procedures for educational quality as mainstream colleges, the other that The faculty and curriculum should be closely adapted to the particular historical culture of the tribe. There was a great deal of turnover, exacerbated by very tight budgets. In 1994, the U.S. Congress passed legislation recognizing the tribal colleges as land-grant colleges, which provided opportunities for large-scale funding. Thirty-two tribal colleges in the United States belong to the American Indian Higher Education Consortium. By the early 21st century, tribal nations had also established numerous language revival programs in their schools.
In addition, Native American activism has led major universities across the country to establish Native American studies programs and departments, increasing awareness of the strengths of Indian cultures, providing opportunities for academics, and deepening research on history and cultures in the United States. Native Americans have entered academia; journalism and media; politics at local, state and federal levels; and public service, for instance, influencing medical research and policy to identify issues related to American Indians.
21st century.
In 2009, an "apology to Native Peoples of the United States" was included in the defense appropriations act. It states that the U.S. "apologizes on behalf of the people of the United States to all Native Peoples for the many instances of violence, maltreatment, and neglect inflicted on Native Peoples by citizens of the United States.
In 2013, jurisdiction over persons who were not tribal members under the Violence Against Women Act was extended to Indian Country. This closed a gap which prevented arrest or prosecution by tribal police or courts of abusive partners of tribal members who were not native or from another tribe.
Migration to urban areas continued to grow with 70% of Native Americans living in urban areas in 2012, up from 45% in 1970 and 8% in 1940. Urban areas with significant Native American populations include Minneapolis, Denver, Albuquerque, Phoenix, Tucson, Chicago, Oklahoma City, Houston, New York City, Los Angeles, and Rapid City. Many lived in poverty. Racism, unemployment, drugs and gangs were common problems which Indian social service organizations such as the Little Earth housing complex in Minneapolis attempted to address.
Demographics.
Historical population.
The census counted 248,000 Indians in 1890, 332,000 in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Indians averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940.
Population and distribution.
The 2010 census permitted respondents to self-identify as being of one or more races. Self-identification dates from the census of 1960; prior to that the race of the respondent was determined by opinion of the census taker. The option to select more than one race was introduced in 2000. If American Indian or Alaska Native was selected, the form requested the individual provide the name of the "enrolled or principal tribe". The 2010 Census showed that the U.S. population on April 1, 2010, was 308.7 million.
Out of the total U.S. population, 2.9 million people, or 0.9 percent, reported American Indian or Alaska Native alone. In addition, 2.3 million people, or another 0.7 percent, reported American Indian or Alaska Native in combination with one or more other races. Together, these two groups totaled 5.2 million people. Thus, 1.7 percent of all people in the United States identified as American Indian or Alaska Native, either alone or in combination with one or more other races.
The definition of American Indian or Alaska Native used in the 2010 census:According to Office of Management and Budget, "American Indian or Alaska Native" refers to a person having origins in any of the original peoples of North and South America (including Central America) and who maintains tribal affiliation or community attachment.
78% of Native Americans live outside a reservation. Full-blood individuals are more likely to live on a reservation than mixed-blood individuals. The Navajo, with 286,000 full-blood individuals, is the largest tribe if only full-blood individuals are counted; the Navajo are the tribe with the highest proportion of full-blood individuals, 86.3%. The Cherokee have a different history; it is the largest tribe with 819,000 individuals, and it has 284,000 full-blood individuals.
Urban migration.
As of 2012, 70% of American Indians live in urban areas, up from 45% in 1970 and 8% in 1940. Urban areas with significant Native American populations include Minneapolis, Denver, Phoenix, Tucson, Chicago, Oklahoma City, Houston, New York City, Los Angeles, and Rapid City. Many live in poverty. Racism, unemployment, drugs and gangs are common problems which Indian social service organizations such as the Little Earth housing complex in Minneapolis attempt to address.
Distribution by U.S. state.
According to 2003 United States Census Bureau estimates, a little over one third of the 2,786,652 Native Americans in the United States live in three states: California at 413,382, Arizona at 294,137 and Oklahoma at 279,559.
In 2010, the U.S. Census Bureau estimated that about 0.8% of the U.S. population was of American Indian or Alaska Native descent. This population is unevenly distributed across the country. Below, all fifty states, as well as the District of Columbia and Puerto Rico, are listed by the proportion of residents citing American Indian or Alaska Native ancestry, based on the 2010 U.S. Census.
In 2006, the U.S. Census Bureau estimated that about less than 1.0% of the U.S. population was of Native Hawaiian or Pacific Islander descent. This population is unevenly distributed across twenty-six states. Below, are the twenty-six states that had at least 0.1%. They are listed by the proportion of residents citing Native Hawaiian or Pacific Islander ancestry, based on 2006 estimates:
Population by tribal grouping.
Below are numbers for U.S. citizens self-identifying to selected tribal grouping, according to the 2000 U.S. census.
Current legal status.
There are 562 federally recognized tribal governments in the United States. These tribes possess the right to form their own governments, to enforce laws (both civil and criminal) within their lands, to tax, to establish requirements for membership, to license and regulate activities, to zone, and to exclude persons from tribal territories. Limitations on tribal powers of self-government include the same limitations applicable to states; for example, neither tribes nor states have the power to make war, engage in foreign relations, or coin money (this includes paper currency).
Many Native Americans and advocates of Native American rights point out that the U.S. federal government's claim to recognize the "sovereignty" of Native American peoples falls short, given that the United States wishes to govern Native American peoples and treat them as subject to U.S. law. Such advocates contend that full respect for Native American sovereignty would require the U.S. government to deal with Native American peoples in the same manner as any other sovereign nation, handling matters related to relations with Native Americans through the Secretary of State, rather than the Bureau of Indian Affairs. The Bureau of Indian Affairs reports on its website that its "responsibility is the administration and management of of land held in trust by the United States for American Indians, Indian tribes, and Alaska Natives". Many Native Americans and advocates of Native American rights believe that it is condescending for such lands to be considered "held in trust" and regulated in any fashion by other than their own tribes, whether the U.S. or Canadian governments, or any other non-Native American authority.
As of 2000, the largest groups in the United States by population were Navajo, Cherokee, Choctaw, Sioux, Chippewa, Apache, Blackfeet, Iroquois, and Pueblo. In 2000, eight of ten Americans with Native American ancestry were of mixed ancestry. It is estimated that by 2100 that figure will rise to nine out of ten.
In addition, there are a number of tribes that are recognized by individual states, but not by the federal government. The rights and benefits associated with state recognition vary from state to state.
Some tribal groups have been unable to document the cultural continuity required for federal recognition. The Muwekma Ohlone of the San Francisco bay area are pursuing litigation in the federal court system to establish recognition. Many of the smaller eastern tribes, long considered remnants of extinct peoples, have been trying to gain official recognition of their tribal status. Several tribes in Virginia and North Carolina have gained state recognition. Federal recognition confers some benefits, including the right to label arts and crafts as Native American and permission to apply for grants that are specifically reserved for Native Americans. But gaining federal recognition as a tribe is extremely difficult; to be established as a tribal group, members have to submit extensive genealogical proof of tribal descent and continuity of the tribe as a culture.
In July 2000, the Washington State Republican Party adopted a resolution recommending that the federal and legislative branches of the U.S. government terminate tribal governments. In 2007, a group of Democratic Party congressmen and congresswomen introduced a bill in the U.S. House of Representatives to "terminate" the Cherokee Nation. This was related to their voting to exclude Cherokee Freedmen as members of the tribe unless they had a Cherokee ancestor on the Dawes Rolls, although all Cherokee Freedmen and their descendants had been members since 1866.
As of 2004, various Native Americans are wary of attempts by others to gain control of their reservation lands for natural resources, such as coal and uranium in the West.
In the state of Virginia, Native Americans face a unique problem. Virginia has no federally recognized tribes but the state has recognized eight. This is related historically to the greater impact of disease and warfare on the Virginia Indian populations, as well as their intermarriage with Europeans and Africans. Some people confused the ancestry with culture, but groups of Virginia Indians maintained their cultural continuity. Most of their early reservations were ended under the pressure of early European settlement.
Some historians also note the problems of Virginia Indians in establishing documented continuity of identity, due to the work of Walter Ashby Plecker (1912–1946). As registrar of the state's Bureau of Vital Statistics, he applied his own interpretation of the one-drop rule, enacted in law in 1924 as the state's Racial Integrity Act. It recognized only two races: "white" and "colored".
Plecker, a segregationist, believed that the state's Native Americans had been "mongrelized" by intermarriage with African Americans; to him, ancestry determined identity, rather than culture. He thought that some people of partial black ancestry were trying to "pass" as Native Americans. Plecker thought that anyone with any African heritage had to be classified as colored, regardless of appearance, amount of European or Native American ancestry, and cultural/community identification. Plecker pressured local governments into reclassifying all Native Americans in the state as "colored", and gave them lists of family surnames to examine for reclassification based on his interpretation of data and the law. This led to the state's destruction of accurate records related to families and communities who identified as Native American (as in church records and daily life). By his actions, sometimes different members of the same family were split by being classified as "white" or "colored". He did not allow people to enter their primary identification as Native American in state records. In 2009, the Senate Indian Affairs Committee endorsed a bill that would grant federal recognition to tribes in Virginia.
To achieve federal recognition and its benefits, tribes must prove continuous existence since 1900. The federal government has maintained this requirement, in part because through participation on councils and committees, federally recognized tribes have been adamant about groups' satisfying the same requirements as they did.
Contemporary issues.
Native American struggles amid poverty to maintain life on the reservation or in larger society have resulted in a variety of health issues, some related to nutrition and health practices. The community suffers a vulnerability to and disproportionately high rate of alcoholism.
Societal discrimination and racism.
Most non-Native Americans admitted they rarely encountered Native Americans in their daily lives. While sympathetic toward Native Americans and expressing regret over the past, most people had only a vague understanding of the problems facing Native Americans today. For their part, Native Americans told researchers that they believed they continued to face prejudice, mistreatment, and inequality in the broader society.
Affirmative action issues.
Federal contractors and subcontractors, such as businesses and educational institutions, are legally required to adopt equal opportunity employment and affirmative action measures intended to prevent discrimination against employees or applicants for employment on the basis of "color, religion, sex, or national origin". For this purpose, a Native American is defined as "A person having origins in any of the original peoples of North and South America (including Central America), and who maintains a tribal affiliation or community attachment". However, self-reporting is permitted: "Educational institutions and other recipients should allow students and staff to self-identify their race and ethnicity unless self-identification is not practicable or feasible."
Self-reporting opens the door to "box checking" by people who, despite not having a substantial relationship to Native American culture, innocently or fraudulently check the box for Native American.
Native American mascots in sports.
American Indian activists in the United States and Canada have criticized the use of Native American mascots in sports, as perpetuating stereotypes.
Many universities and professional sports teams no longer use such images. Examples include Stanford University, which changed from Indians to Cardinal in 1972; Miami University, which switched from Redskins to RedHawks in 1997; and the NBA's Golden State Warriors, who originally used Native American-themed logos but have not since 1971.
However, controversy has remained regarding teams such as the NFL's Washington Redskins, whose name is considered to be a racial slur, and MLB's Cleveland Indians, whose usage of a caricature called Chief Wahoo has diminished but not ended altogether.
Some tribal team names have been approved by the tribe in question, such as the Seminole Tribe of Florida's approving use of their name for the teams of Florida State University.
Historical depictions in art.
Native Americans have been depicted by American artists in various ways at different periods. A number of 19th and 20th-century United States and Canadian painters, often motivated by a desire to document and preserve Native culture, specialized in Native American subjects. Among the most prominent of these were Elbridge Ayer Burbank, George Catlin, Seth Eastman, Paul Kane, W. Langdon Kihn, Charles Bird King, Joseph Henry Sharp, and John Mix Stanley.
In the 20th century, early portrayals of Native Americans in movies and television roles were first performed by European Americans dressed in mock traditional attire. Examples included "The Last of the Mohicans" (1920), "Hawkeye and the Last of the Mohicans" (1957), and "F Troop" (1965–67). In later decades, Native American actors such as Jay Silverheels in "The Lone Ranger" television series (1949–57) came to prominence. Roles of Native Americans were limited and not reflective of Native American culture. By the 1970s some Native American film roles began to show more complexity, such as those in "Little Big Man" (1970), "Billy Jack" (1971), and "The Outlaw Josey Wales" (1976), which depicted Native Americans in minor supporting roles.
For years, Native people on U.S. television were relegated to secondary, subordinate roles. During the years of the series "Bonanza" (1959–1973), no major or secondary Native characters appeared on a consistent basis. The series "The Lone Ranger" (1949–1957), "Cheyenne" (1955–1963), and "Law of the Plainsman" (1959–1963) had Native characters who were essentially aides to the central white characters. This continued in such series as "How the West Was Won". These programs resembled the "sympathetic" yet contradictory film "Dances With Wolves" of 1990, in which, according to Ella Shohat and Robert Stam, the narrative choice was to relate the Lakota story as told through a Euro-American voice, for wider impact among a general audience.
Like the 1992 remake of "The Last of the Mohicans" and "" (1993), "Dances with Wolves" employed a number of Native American actors, and made an effort to portray Indigenous languages.
In 2009 "We Shall Remain" (2009), a television documentary by Ric Burns and part of the American Experience series, presented a five-episode series "from a Native American perspective". It represented "an unprecedented collaboration between Native and non-Native filmmakers and involves Native advisors and scholars at all levels of the project". The five episodes explore the impact of King Philip's War on the northeastern tribes, the "Native American confederacy" of Tecumseh's War, the U.S.-forced relocation of Southeastern tribes known as the Trail of Tears, the pursuit and capture of Geronimo and the Apache Wars, and concludes with the Wounded Knee incident, participation by the American Indian Movement, and the increasing resurgence of modern Native cultures since.
Terminology differences.
Common usage in the United States.
Native Americans are more commonly known as Indians or American Indians. The term "Native American" was introduced in the United States in preference to the older term "Indian" to distinguish the indigenous peoples of the Americas from the people of India, and to avoid negative stereotypes associated with the term "Indian". Some academics believe that the term "Indian" should be considered outdated or offensive. Many indigenous Americans, however, prefer the term "American Indian".
Criticism of the neologism "Native American" comes from diverse sources. Russell Means, an American Indian activist, opposed the term "Native American" because he believed it was imposed by the government without the consent of American Indians. He has also argued that the use of the word "Indian" derives not from a confusion with India but from a Spanish expression "En Dio", meaning "in God".
A 1995 U.S. Census Bureau survey found that more Native Americans in the United States preferred "American Indian" to "Native American". Most American Indians are comfortable with "Indian", "American Indian", and "Native American", and the terms are often used interchangeably. The traditional term is reflected in the name chosen for the National Museum of the American Indian, which opened in 2004 on the Mall in Washington, D.C.
Gambling industry.
Gambling has become a leading industry. Casinos operated by many Native American governments in the United States are creating a stream of gambling revenue that some communities are beginning to use as leverage to build diversified economies. Although many Native American tribes have casinos, the impact of Native American gaming is widely debated. Some tribes, such as the Winnemem Wintu of Redding, California, feel that casinos and their proceeds destroy culture from the inside out. These tribes refuse to participate in the gambling industry.
Crime on reservations.
Prosecution of serious crime, historically endemic on reservations, was required by the 1885 Major Crimes Act, 18 U.S.C. §§1153, 3242, and court decisions to be investigated by the federal government, usually the Federal Bureau of Investigation, and prosecuted by United States Attorneys of the United States federal judicial district in which the reservation lies.
A December 13, 2009" New York Times" article about growing gang violence on the Pine Ridge Indian Reservation estimated that there were 39 gangs with 5,000 members on that reservation alone. Navajo country recently reported 225 gangs in its territory.
As of 2012, a high incidence of rape continued to impact Native American women and Alaskan native women. According to the Department of Justice, 1 in 3 Native women have suffered rape or attempted rape, more than twice the national rate. About 46 percent of Native American women have been raped, beaten, or stalked by an intimate partner, according to a 2010 study by the Centers for Disease Control. According to Professor N. Bruce Duthu, "More than 80 percent of Indian victims identify their attacker as non-Indian".
Society, language, and culture.
Though cultural features, language, clothing, and customs vary enormously from one tribe to another, there are certain elements which are encountered frequently and shared by many tribes. Early European American scholars described the Native Americans as having a society dominated by clans.
Early hunter-gatherer tribes made stone weapons from around 10,000 years ago; as the age of metallurgy dawned, newer technologies were used and more efficient weapons produced. Prior to contact with Europeans, most tribes used similar weaponry. The most common implements were the bow and arrow, the war club, and the spear. Quality, material, and design varied widely. Native American use of fire both helped provide and prepare for food and altered the landscape of the continent to help the human population flourish.
Large mammals like mammoths and mastodons were largely extinct by around 8000 BCE. Native Americans switched to hunting other large game, such as bison. The Great Plains tribes were still hunting the bison when they first encountered the Europeans. The Spanish reintroduction of the horse to North America in the 17th century and Native Americans' learning to use them greatly altered the Native Americans' culture, including changing the way in which they hunted large game. Horses became such a valuable, central element of Native lives that they were counted as a measure of wealth.
Ethno-linguistic classification.
Native Americans were divided into several hundred ethno-linguistic groups.
A number of English words have been derived from Native American languages.
Language education.
To evade a shift to English, some Native American tribes have initiated language immersion schools for children, where a native Indian language is the medium of instruction. For example, the Cherokee Nation instigated a 10-year language preservation plan that involved growing new fluent speakers of the Cherokee language from childhood on up through school immersion programs as well as a collaborative community effort to continue to use the language at home. This plan was part of an ambitious goal that in 50 years, 80% or more of the Cherokee people will be fluent in the language. The Cherokee Preservation Foundation has invested $3 million into opening schools, training teachers, and developing curricula for language education, as well as initiating community gatherings where the language can be actively used. Formed in 2006, the Kituwah Preservation & Education Program (KPEP) on the Qualla Boundary focuses on language immersion programs for children from birth to fifth grade, developing cultural resources for the general public and community language programs to foster the Cherokee language among adults.
There is also a Cherokee language immersion school in Tahlequah, Oklahoma that educates students from pre-school through eighth grade. Because Oklahoma's official language is English, Cherokee immersion students are hindered when taking state-mandated tests because they have little competence in English. The Department of Education of Oklahoma said that in 2012 state tests: 11% of the school's sixth-graders showed proficiency in math, and 25% showed proficiency in reading; 31% of the seventh-graders showed proficiency in math, and 87% showed proficiency in reading; 50% of the eighth-graders showed proficiency in math, and 78% showed proficiency in reading. The Oklahoma Department of Education listed the charter school as a Targeted Intervention school, meaning the school was identified as a low-performing school but has not so that it was a Priority School. Ultimately, the school made a C, or a 2.33 grade point average on the state's A-F report card system. The report card shows the school getting an F in mathematics achievement and mathematics growth, a C in social studies achievement, a D in reading achievement, and an A in reading growth and student attendance. "The C we made is tremendous," said school principal Holly Davis, "here is no English instruction in our school's younger grades, and we gave them this test in English." She said she had anticipated the low grade because it was the school's first year as a state-funded charter school, and many students had difficulty with English. Eighth graders who graduate from the Tahlequah immersion school are fluent speakers of the language, and they usually go on to attend Sequoyah High School where classes are taught in both English and Cherokee.
Society and art.
The Iroquois, living around the Great Lakes and extending east and north, used strings or belts called "wampum" that served a dual function: the knots and beaded designs mnemonically chronicled tribal stories and legends, and further served as a medium of exchange and a unit of measure. The keepers of the articles were seen as tribal dignitaries.
Pueblo peoples crafted impressive items associated with their religious ceremonies. "Kachina" dancers wore elaborately painted and decorated masks as they ritually impersonated various ancestral spirits. Sculpture was not highly developed, but carved stone and wood fetishes were made for religious use. Superior weaving, embroidered decorations, and rich dyes characterized the textile arts. Both turquoise and shell jewelry were created, as were high-quality pottery and formalized pictorial arts.
Navajo spirituality focused on the maintenance of a harmonious relationship with the spirit world, often achieved by ceremonial acts, usually incorporating sandpainting. The colors—made from sand, charcoal, cornmeal, and pollen—depicted specific spirits. These vivid, intricate, and colorful sand creations were erased at the end of the ceremony.
Agriculture.
An early crop the Native Americans grew was squash. Other early crops included cotton, sunflower, pumpkins, tobacco, goosefoot, knotgrass, and sump weed.
Agriculture in the southwest started around 4,000 years ago when traders brought cultigens from Mexico. Due to the varying climate, some ingenuity was needed for agriculture to be successful. The climate in the southwest ranged from cool, moist mountains regions, to dry, sandy soil in the desert. Some innovations of the time included irrigation to bring water into the dry regions and the selection of seed based on the traits of the growing plants that bore them.
In the southwest, they grew beans that were self-supported, much like the way they are grown today. In the east, however, they were planted next to corn in order for the vines to be able to "climb" the cornstalks.
The most important crop the Native Americans raised was maize. It was first started in Mesoamerica and spread north. About 2,000 years ago it reached eastern America. This crop was important to the Native Americans because it was part of their everyday diet; it could be stored in underground pits during the winter, and no part of it was wasted. The husk was made into art crafts, and the cob was used as fuel for fires.
By 800 CE the Native Americans had established three main crops — beans, squash, and corn — called the three sisters.
The agriculture gender roles of the Native Americans varied from region to region. In the southwest area, men prepared the soil with hoes. The women were in charge of planting, weeding, and harvesting the crops. In most other regions, the women were in charge of doing everything, including clearing the land. Clearing the land was an immense chore since the Native Americans rotated fields frequently. There is a tradition that Squanto showed the Pilgrims in New England how to put fish in fields to act like a fertilizer, but the truth of this story is debated.
Native Americans did plant beans next to corn; the beans would replace the nitrogen which the corn took from the ground, as well as using corn stalks for support for climbing. Native Americans used controlled fires to burn weeds and clear fields; this would put nutrients back into the ground. If this did not work, they would simply abandon the field to let it be fallow, and find a new spot for cultivation.
Europeans in the eastern part of the continent observed that Native Americans cleared large areas for cropland. Their fields in New England sometimes covered hundreds of acres. Colonists in Virginia noted thousands of acres under cultivation by Native Americans.
Native Americans commonly used tools such as the hoe, maul, and dibber. The hoe was the main tool used to till the land and prepare it for planting; then it was used for weeding. The first versions were made out of wood and stone. When the settlers brought iron, Native Americans switched to iron hoes and hatchets. The dibber was a digging stick, used to plant the seed. Once the plants were harvested, women prepared the produce for eating. They used the maul to grind the corn into mash. It was cooked and eaten that way or baked as corn bread.
Religion.
Traditional Native American ceremonies are still practiced by many tribes and bands, and the older theological belief systems are still held by many of the native people. These spiritualities may accompany adherence to another faith, or can represent a person's primary religious identity. While much Native American spiritualism exists in a tribal-cultural continuum, and as such cannot be easily separated from tribal identity itself, certain other more clearly defined movements have arisen among "traditional" Native American practitioners, these being identifiable as "religions" in the prototypical sense familiar in the industrialized Western world.
Traditional practices of some tribes include the use of sacred herbs such as tobacco, sweetgrass or sage. Many Plains tribes have sweatlodge ceremonies, though the specifics of the ceremony vary among tribes. Fasting, singing and prayer in the ancient languages of their people, and sometimes drumming are also common.
The Midewiwin Lodge is a traditional medicine society inspired by the oral traditions and prophesies of the Ojibwa (Chippewa) and related tribes.
Another significant religious body among Native peoples is known as the Native American Church. It is a syncretistic church incorporating elements of Native spiritual practice from a number of different tribes as well as symbolic elements from Christianity. Its main rite is the peyote ceremony. Prior to 1890, traditional religious beliefs included Wakan Tanka. In the American Southwest, especially New Mexico, a syncretism between the Catholicism brought by Spanish missionaries and the native religion is common; the religious drums, chants, and dances of the Pueblo people are regularly part of Masses at Santa Fe's Saint Francis Cathedral. Native American-Catholic syncretism is also found elsewhere in the United States. (e.g., the National Kateri Tekakwitha Shrine in Fonda, New York, and the National Shrine of the North American Martyrs in Auriesville, New York).
The eagle feather law (Title 50 Part 22 of the Code of Federal Regulations) stipulates that only individuals of certifiable Native American ancestry enrolled in a federally recognized tribe are legally authorized to obtain eagle feathers for religious or spiritual use. The law does not allow Native Americans to give eagle feathers to non-Native Americans.
Gender roles.
Gender roles are differentiated in many Native American tribes. Whether a particular tribe is predominantly matrilineal or patrilineal, usually both sexes have some degree of decision-making power within the tribe. Many Nations, such as the Haudenosaunee Five Nations and the Southeast Muskogean tribes, have matrilineal or Clan Mother systems, in which property and hereditary leadership are controlled by and passed through the maternal lines. The children are considered to belong to the mother's clan. In Cherokee culture, women own the family property. When traditional young women marry, their husbands may join them in their mother's household.
Matrilineal structures enable young women to have assistance in childbirth and rearing, and protect them in case of conflicts between the couple. If a couple separates or the man dies, the woman has her family to assist her. In matrilineal cultures the mother's brothers are usually the leading male figures in her children's lives; fathers have no standing in their wife and children's clan, as they still belong to their own mother's clan. Hereditary clan chief positions pass through the mother's line and chiefs have historically been selected on recommendation of women elders, who also could disapprove of a chief.
In the patrilineal tribes, such as the Omaha, Osage and Ponca, hereditary leadership passes through the male line, and children are considered to belong to the father and his clan. In patrilineal tribes, if a woman marries a non-Native, she is no longer considered part of the tribe, and her children are considered to share the ethnicity and culture of their father.
In some tribes, men have historically hunted, traded and made war while, as life-givers, women have primary responsibility for the survival and welfare of the families (and future of the tribe). In many tribes women gather and cultivate plants, use plants and herbs to treat illnesses, care for the young and the elderly, make all the clothing and instruments, and process and cure meat and skins from the game. Some mothers use cradleboards to carry an infant while working or traveling. In other tribes, the gender roles are not so clear-cut, and are even less so in the modern era.
At least several dozen tribes allowed polygyny to sisters, with procedural and economic limits.
Apart from making homes, women have many additional tasks that are also essential for the survival of the tribes. Historically they have made weapons and tools, they take care of the roofs of their homes and often help the men hunt and fish. In many tribes, medicine women gather herbs and cure the ill, while in others men may also be healers.
Lakota, Dakota, and Nakota girls are encouraged to learn to ride, hunt and fight. Though fighting in war has mostly been left to the boys and men, occasionally women fought as well - both in battles and in defense of the home - especially if the tribe was severely threatened.
Sports.
Native American leisure time led to competitive individual and team sports. Jim Thorpe, Joe Hipp, Notah Begay III, Chris Wondolowski, Jacoby Ellsbury, Joba Chamberlain, Kyle Lohse, Sam Bradford, Jack Brisco, Tommy Morrison, Billy Mills, and Shoni Schimmel are well known professional athletes.
Team based.
Native American ball sports, sometimes referred to as lacrosse, stickball, or baggataway, was often used to settle disputes, rather than going to war, as a civil way to settle potential conflict. The Choctaw called it "isitoboli" ("Little Brother of War"); the Onondaga name was "dehuntshigwa'es" ("men hit a rounded object"). There are three basic versions, classified as Great Lakes, Iroquoian, and Southern.
The game is played with one or two rackets/sticks and one ball. The object of the game is to land the ball on the opposing team's goal (either a single post or net) to score and to prevent the opposing team from scoring on your goal. The game involves as few as 20 or as many as 300 players with no height or weight restrictions and no protective gear. The goals could be from around apart to about ; in Lacrosse the field is . A Jesuit priest referenced stickball in 1729, and George Catlin painted the subject.
Women's basketball.
Currently in the WNBA, there are 2 women who are of Native ancestry and enrolled in federally recognized tribes.
Angel Goodrich, Cherokee, was selected in the third round of the WNBA draft (29th pick overall) by the Tulsa Shock. At the time she was the highest-drafted Native American player in the history of the WNBA. During the 2013-14 off-season, she played for Chevakata Vologda in the Russian Premier League. In 2014, she completed her second and final season for the Shock. In 2015, she was picked up on waivers by the Seattle Storm
Shoni Schimmel, Confederated Tribes of the Umatilla Indian Reservation, is an American professional basketball player. She was an All-American college player at the University of Louisville and a first round draft pick of the WNBA's Atlanta Dream. She also earned recognition as the 2014 WNBA All-Star Game Most Valuable Player on July 19, 2014 in Phoenix, Arizona.
Individual based.
Chunkey was a game that consisted of a stone shaped disk that was about 1–2 inches in diameter. The disk was thrown down a corridor so that it could roll past the players at great speed. The disk would roll down the corridor, and players would throw wooden shafts at the moving disk. The object of the game was to strike the disk or prevent your opponents from hitting it.
U.S. Olympics.
Jim Thorpe, a Sauk and Fox Native American, was an all-round athlete playing football and baseball in the early 20th century. Future President Dwight Eisenhower injured his knee while trying to tackle the young Thorpe. In a 1961 speech, Eisenhower recalled Thorpe: "Here and there, there are some people who are supremely endowed. My memory goes back to Jim Thorpe. He never practiced in his life, and he could do anything better than any other football player I ever saw."
In the 1912 Olympics, Thorpe could run the 100-yard dash in 10 seconds flat, the 220 in 21.8 seconds, the 440 in 51.8 seconds, the 880 in 1:57, the mile in 4:35, the 120-yard high hurdles in 15 seconds, and the 220-yard low hurdles in 24 seconds. He could long jump 23 ft 6 in and high-jump 6 ft 5 in. He could pole vault , put the shot , throw the javelin , and throw the discus . Thorpe entered the U.S. Olympic trials for the pentathlon and the decathlon.
Louis Tewanima, Hopi people, was an American two-time Olympic distance runner and silver medalist in the 10,000 meter run in 1912. He ran for the Carlisle Indian School where he was a teammate of Jim Thorpe. His silver medal in 1912 remained the best U.S. achievement in this event until another Indian, Billy Mills, won the gold medal in 1964. Tewanima also competed at the 1908 Olympics, where he finished in ninth place in the marathon.
Ellison Brown, of the Narragansett people from Rhode Island, better known as "Tarzan" Brown, won two Boston Marathons (1936, 1939) and competed on the United States Olympic team in the 1936 Olympic Games in Berlin, Germany, but did not finish due to injury. He qualified for the 1940 Olympic Games in Helsinki, Finland, but the games were canceled due to the outbreak of World War II.
Billy Mills, a Lakota and USMC officer, won the gold medal in the 10,000 meter run at the 1964 Tokyo Olympics. He was the only American ever to win the Olympic gold in this event. An unknown before the Olympics, Mills finished second in the U.S. Olympic trials.
Billy Kidd, part Abenaki from Vermont, became the first American male to medal in alpine skiing in the Olympics, taking silver at age 20 in the slalom in the 1964 Winter Olympics at Innsbruck, Austria. Six years later at the 1970 World Championships, Kidd won the gold medal in the combined event and took the bronze medal in the slalom.
Music and art.
Traditional Native American music is almost entirely monophonic, but there are notable exceptions. Native American music often includes drumming and/or the playing of rattles or other percussion instruments but little other instrumentation. Flutes and whistles made of wood, cane, or bone are also played, generally by individuals, but in former times also by large ensembles (as noted by Spanish conquistador de Soto). The tuning of modern flutes is typically pentatonic.
Performers with Native American parentage have occasionally appeared in American popular music such as Rita Coolidge, Wayne Newton, Gene Clark, Buffy Sainte-Marie, Blackfoot, Tori Amos, Redbone (members are also of Mexican descent), and CocoRosie. Some, such as John Trudell, have used music to comment on life in Native America. Other musicians such as R. Carlos Nakai, Joanne Shenandoah and Robert "Tree" Cody integrate traditional sounds with modern sounds in instrumental recordings, whereas the music by artist Charles Littleleaf is derived from ancestral heritage as well as nature. A variety of small and medium-sized recording companies offer an abundance of recent music by Native American performers young and old, ranging from pow-wow drum music to hard-driving rock-and-roll and rap. In the International world of ballet dancing Maria Tallchief was considered America's first major prima ballerina, and was the first person of Native American descent to hold the rank. along with her sister Marjorie Tallchief both became star ballerinas.
The most widely practiced public musical form among Native Americans in the United States is that of the pow-wow. At pow-wows, such as the annual Gathering of Nations in Albuquerque, New Mexico, members of drum groups sit in a circle around a large drum. Drum groups play in unison while they sing in a native language and dancers in colorful regalia dance clockwise around the drum groups in the center. Familiar pow-wow songs include honor songs, intertribal songs, crow-hops, sneak-up songs, grass-dances, two-steps, welcome songs, going-home songs, and war songs. Most indigenous communities in the United States also maintain traditional songs and ceremonies, some of which are shared and practiced exclusively within the community.
Native American art comprises a major category in the world art collection. Native American contributions include pottery, paintings, jewellery, weavings, sculpture, basketry, and carvings. Franklin Gritts was a Cherokee artist who taught students from many tribes at Haskell Institute (now Haskell Indian Nations University) in the 1940s, the "Golden Age" of Native American painters. The integrity of certain Native American artworks is protected by the Indian Arts and Crafts Act of 1990, that prohibits representation of art as Native American when it is not the product of an enrolled Native American artist. Attorney Gail Sheffield and others claim that this law has had "the unintended consequence of sanctioning discrimination against Native Americans whose tribal affiliation was not officially recognized." Native artists such as Jeanne Rorex Bridges (Cherokee) who are not enrolled run the risk of fines or imprisonment if they continue to sell their art while affirming their Indian heritage.
Traditional economy.
The Inuit, or Eskimo, prepared and buried large amounts of dried meat and fish. Pacific Northwest tribes crafted seafaring dugouts long for fishing. Farmers in the Eastern Woodlands tended fields of maize with hoes and digging sticks, while their neighbors in the Southeast grew tobacco as well as food crops. On the Plains, some tribes engaged in agriculture but also planned buffalo hunts in which herds were driven over bluffs.
Dwellers of the Southwest deserts hunted small animals and gathered acorns to grind into flour with which they baked wafer-thin bread on top of heated stones. Some groups on the region's mesas developed irrigation techniques, and filled storehouses with grain as protection against the area's frequent droughts.
In the early years, as these native peoples encountered European explorers and settlers and engaged in trade, they exchanged food, crafts, and furs for blankets, iron and steel implements, horses, trinkets, firearms, and alcoholic beverages.
Contemporary barriers to economic development.
Today, other than tribes successfully running casinos, many tribes struggle, as they are often located on reservations isolated from the main economic centers of the country. The estimated 2.1 million Native Americans are the most impoverished of all ethnic groups. According to the 2000 Census, an estimated 400,000 Native Americans reside on reservation land. While some tribes have had success with gaming, only 40% of the 562 federally recognized tribes operate casinos. According to a 2007 survey by the U.S. Small Business Administration, only 1% of Native Americans own and operate a business.
Native Americans rank at the bottom of nearly every social statistic: highest teen suicide rate of all minorities at 18.5 per 100,000, highest rate of teen pregnancy, highest high school drop-out rate at 54%, lowest per capita income, and unemployment rates between 50% and 90%.
The barriers to economic development on Native American reservations have been identified by Joseph Kalt and Stephen Cornell of the Harvard Project on American Indian Economic Development at Harvard University, in their report: "What Can Tribes Do? Strategies and Institutions in American Indian Economic Development" (2008), are summarized as follows:
A major barrier to development is the lack of entrepreneurial knowledge and experience within Indian reservations. "A general lack of education and experience about business is a significant challenge to prospective entrepreneurs", was the report on Native American entrepreneurship by the Northwest Area Foundation in 2004. "Native American communities that lack entrepreneurial traditions and recent experiences typically do not provide the support that entrepreneurs need to thrive. Consequently, experiential entrepreneurship education needs to be embedded into school curricula and after-school and other community activities. This would allow students to learn the essential elements of entrepreneurship from a young age and encourage them to apply these elements throughout life". "Rez Biz" magazine addresses these issues.
Native Americans, Europeans, and Africans.
Interracial relations between Native Americans, Europeans, and Africans is a complex issue that has been mostly neglected with "few in-depth studies on interracial relationships". Some of the first documented cases of European/Native American intermarriage and contact were recorded in Post-Columbian Mexico. One case is that of Gonzalo Guerrero, a European from Spain, who was shipwrecked along the Yucatan Peninsula, and fathered three Mestizo children with a Mayan noblewoman. Another is the case of Hernán Cortés and his mistress La Malinche, who gave birth to another of the first multi-racial people in the Americas.
Assimilation.
European impact was immediate, widespread, and profound—more than any other race that had contact with Native Americans during the early years of colonization and nationhood. Europeans living among Native Americans were often called "white indians". They "lived in native communities for years, learned native languages fluently, attended native councils, and often fought alongside their native companions".
Early contact was often charged with tension and emotion, but also had moments of friendship, cooperation, and intimacy. Marriages took place in English, Spanish, and French colonies between Native Americans and Europeans. Given the preponderance of men among the colonists in the early years, generally European men married American Indian women.
In 1528, Isabel de Moctezuma, an heir of Moctezuma II, was married to Alonso de Grado, a Spanish Conquistador. After his death, the widow married Juan Cano de Saavedra. Together they had five children. Many heirs of Emperor Moctezuma II were acknowledged by the Spanish crown, who granted them titles including Duke of Moctezuma de Tultengo.
On April 5, 1614, Pocahontas married the Englishman John Rolfe. They had a child called Thomas Rolfe. Intimate relations among Native American and Europeans were widespread, beginning with the French and Spanish explorers and trappers. For instance, in the early 19th century, the Native American woman Sacagawea, who would help translate for the Lewis and Clark Expedition, was married to the French-Canadian trapper Toussaint Charbonneau. They had a son named Jean Baptiste Charbonneau. This was the most typical pattern among the traders and trappers.
There was fear on both sides, as the different peoples realized how different their societies were. The whites regarded the Indians as "savage" because they were not Christian. They were suspicious of cultures which they did not understand. The Native American author, Andrew J. Blackbird, wrote in his "History of the Ottawa and Chippewa Indians of Michigan," (1897), that white settlers introduced some immoralities into Native American tribes. Many Indians suffered because the Europeans introduced alcohol and the whiskey trade resulted in alcoholism among the people, who were alcohol-intolerant.
Blackbird wrote:
The U.S. government had two purposes when making land agreements with Native Americans: to open it up more land for white settlement, and to ease tensions between whites and Native Americans by forcing the Native Americans to use the land in the same way as did the whites—for subsistence farms. The government used a variety of strategies to achieve these goals; many treaties required Native Americans to become farmers in order to keep their land. Government officials often did not translate the documents which Native Americans were forced to sign, and native chiefs often had little or no idea what they were signing.
For a Native American man to marry a white woman, he had to get consent of her parents, as long as "he can prove to support her as a white woman in a good home". In the early 19th century, the Shawnee Tecumseh and blonde hair, blue-eyed Rebbecca Galloway had an interracial affair. In the late 19th century, three European-American middle-class women teachers at Hampton Institute married Native American men whom they had met as students.
As European-American women started working independently at missions and Indian schools in the western states, there were more opportunities for their meeting and developing relationships with Native American men. For instance, Charles Eastman, a man of European and Lakota descent whose father sent both his sons to Dartmouth College, got his medical degree at Boston University and returned to the West to practice. He married Elaine Goodale, whom he met in South Dakota. He was the grandson of Seth Eastman, a military officer from Maine, and a chief's daughter. Goodale was a young European-American teacher from Massachusetts and a reformer, who was appointed as the U.S. superintendent of Native American education for the reservations in the Dakota Territory. They had six children together.
European enslavement.
When Europeans arrived as colonists in North America, Native Americans changed their practice of slavery dramatically. Native Americans began selling war captives to Europeans rather than integrating them into their own societies as they had done before. As the demand for labor in the West Indies grew with the cultivation of sugar cane, Europeans enslaved Native Americans for the Thirteen Colonies, and some were exported to the "sugar islands". The British settlers, especially those in the southern colonies, purchased or captured Native Americans to use as forced labor in cultivating tobacco, rice, and indigo. Accurate records of the numbers enslaved do not exist. Scholars estimate tens of thousands of Native Americans may have been enslaved by the Europeans, being sold by Native Americans themselves.
Slaves became a caste of people who were foreign to the English (Native Americans, Africans and their descendants) and non-Christians. The Virginia General Assembly defined some terms of slavery in 1705:
The slave trade of Native Americans lasted only until around 1730. It gave rise to a series of devastating wars among the tribes, including the Yamasee War. The Indian Wars of the early 18th century, combined with the increasing importation of African slaves, effectively ended the Native American slave trade by 1750. Colonists found that Native American slaves could easily escape, as they knew the country. The wars cost the lives of numerous colonial slave traders and disrupted their early societies. The remaining Native American groups banded together to face the Europeans from a position of strength. Many surviving Native American peoples of the southeast strengthened their loose coalitions of language groups and joined confederacies such as the Choctaw, the Creek, and the Catawba for protection.
Native American women were at risk for rape whether they were enslaved or not; during the early colonial years, settlers were disproportionately male. They turned to Native women for sexual relationships. Both Native American and African enslaved women suffered rape and sexual harassment by male slaveholders and other white men.
Traditions of Native American slavery.
The majority of Native American tribes did practice some form of slavery before the European introduction of African slavery into North America, but none exploited slave labor on a large scale. Most Native American tribes did not barter captives in the pre-colonial era, although they sometimes exchanged enslaved individuals with other tribes in peace gestures or in exchange for their own members.
The conditions of enslaved Native Americans varied among the tribes. In many cases, young enslaved captives were adopted into the tribes to replace warriors killed during warfare or by disease. Other tribes practiced debt slavery or imposed slavery on tribal members who had committed crimes; but, this status was only temporary as the enslaved worked off their obligations to the tribal society.
Native American and African relations.
African and Native Americans have interacted for centuries. The earliest record of Native American and African contact occurred in April 1502, when Spanish colonists transported the first Africans to Hispaniola to serve as slaves.
Sometimes Native Americans resented the presence of African Americans. The "Catawaba tribe in 1752 showed great anger and bitter resentment when an African American came among them as a trader". To gain favor with Europeans, the Cherokee exhibited the strongest color prejudice of all Native Americans. He contends that because of European fears of a unified revolt of Native Americans and African Americans, the colonists encouraged hostility between the ethnic groups: "Whites sought to convince Native Americans that African Americans worked against their best interests." In 1751, South Carolina law stated:
In addition, in 1758 the governor of South Carolina James Glen wrote:
Europeans considered both races inferior and made efforts to make both Native Americans and Africans enemies. Native Americans were rewarded if they returned escaped slaves, and African Americans were rewarded for fighting in the late 19th-century Indian Wars.
"Native Americans, during the transitional period of Africans becoming the primary race enslaved, were enslaved at the same time and shared a common experience of enslavement. They worked together, lived together in communal quarters, produced collective recipes for food, shared herbal remedies, myths and legends, and in the end they intermarried." Because of a shortage of men due to warfare, many tribes encouraged marriage between the two groups, to create stronger, healthier children from the unions.
In the 18th century, many Native American women married freed or runaway African men due to a decrease in the population of men in Native American villages. Records show that many Native American women bought African men but, unknown to the European sellers, the women freed and married the men into their tribe. When African men married or had children by a Native American woman, their children were born free, because the mother was free (according to the principle of "partus sequitur ventrem", which the colonists incorporated into law).
European colonists often required the return of runaway slaves to be included as a provision in treaties with American Indians. In 1726, the British Governor of New York exacted a promise from the Iroquois to return all runaway slaves who had joined up with them. In the mid-1760s, the government requested the Huron and Delaware to return runaway slaves, but there was no record of slaves having been returned. Colonists placed ads about runaway slaves.
While numerous tribes used captive enemies as servants and slaves, they also often adopted younger captives into their tribes to replace members who had died. In the Southeast, a few Native American tribes began to adopt a slavery system similar to that of the American colonists, buying African American slaves, especially the Cherokee, Choctaw, and Creek. Though less than 3% of Native Americans owned slaves, divisions grew among the Native Americans over slavery. Among the Cherokee, records show that slave holders in the tribe were largely the children of European men that had shown their children the economics of slavery. As European colonists took slaves into frontier areas, there were more opportunities for relationships between African and Native American peoples.
Based on the work of geneticists, a PBS series on African Americans explained that while most African Americans are racially mixed, it is relatively rare that they have Native American ancestry. According to the PBS series, the most common "non-black" mix is English and Scots-Irish. However, the Y-chromosome and mtDNA (mitochondrial DNA) testing processes for direct-line male and female ancestors can fail to pick up the heritage of many ancestors. (Some critics thought the PBS series did not sufficiently explain the limitations of DNA testing for assessment of heritage.)
Another study suggests that relatively few Native Americans have African-American heritage. A study reported in "The American Journal of Human Genetics" stated, "We analyzed the European genetic contribution to 10 populations of African descent in the United States (Maywood, Illinois; Detroit; New York; Philadelphia; Pittsburgh; Baltimore; Charleston, South Carolina; New Orleans; and Houston) ... mtDNA haplogroups analysis shows no evidence of a significant maternal Amerindian contribution to any of the 10 populations." A few writers persist in the myth that most African Americans have Native American heritage.
DNA testing has limitations and should not be depended on by individuals to answer all their questions about heritage. So far, such testing cannot distinguish among the many distinct Native American tribes. No tribes accept DNA testing to satisfy their differing qualifications for membership, usually based on documented blood quantum or descent from ancestor(s) listed on the Dawes Rolls.
Native American adoption of African slavery.
Native Americans interacted with enslaved Africans and African Americans on many levels. Over time all the cultures interacted. Native Americans began slowly to adopt white culture. Native Americans in the South shared some experiences with Africans, especially during the period, primarily in the 17th century, when both were enslaved. The colonists along the Atlantic Coast had begun enslaving Native Americans to ensure a source of labor. At one time the slave trade was so extensive that it caused increasing tensions with the various Algonquian tribes, as well as the Iroquois. Based in New York and Pennsylvania, they had threatened to attack colonists on behalf of the related Iroquoian Tuscarora before they migrated out of the South in the early 1700s.
In the 1790s, Benjamin Hawkins was assigned as the U.S. agent to the southeastern tribes, who became known as the Five Civilized Tribes for their adoption of numerous Anglo-European practices. He advised the tribes to take up slaveholding to aid them in European-style farming and plantations. He thought their traditional form of slavery, which had looser conditions, was less efficient than chattel slavery. In the 19th century, some members of these tribes who were more closely associated with settlers, began to purchase African-American slaves for workers. They adopted some European-American ways to benefit their people.
From the late 1700s to the 1860s, the Five Civilized Tribes were involved in the institution of African slavery as planters. For example, Cherokee leader Joseph Vann owned more than 100 slaves. The proportion of Cherokee families who owned slaves did not exceed ten percent, and was comparable to the percentage among white families across the South, where a slaveholding elite owned most of the laborers.
The writer William Loren Katz contends that Native Americans treated their slaves better than did the typical white American in the Deep South. Though less than 3% of Native Americans owned slaves, bondage created destructive cleavages among those who were slaveholders. Among the Five Civilized Tribes, mixed-race slaveholders were generally part of an elite hierarchy, often based on their mothers' clan status, as the societies had matrilineal systems. As did Benjamin Hawkins, European fur traders and colonial officials tended to marry high-status women, in strategic alliances seen to benefit both sides. The Choctaw, Creek and Cherokee believed they benefited from stronger alliances with the traders and their societies. The women's sons gained their status from their mother's families; they were part of hereditary leadership lines who exercised power and accumulated personal wealth in their changing Native American societies. The historian Greg O'Brien calls them the Creole generation to show that they were part of a changing society. The chiefs of the tribes believed that some of the new generation of mixed-race, bilingual chiefs would lead their people into the future and be better able to adapt to new conditions influenced by European Americans.
Proposals for Indian Removal heightened the tensions of cultural changes, due to the increase in the number of mixed-race Native Americans in the South. Full bloods, who tended to live in areas less affected by colonial encroachment, generally worked to maintain traditional ways, including control of communal lands. While the traditional members often resented the sale of tribal lands to Anglo-Americans, by the 1830s they agreed it was not possible to go to war with the colonists on this issue.
Who are Native Americans?
Admixture and genetics.
Intertribal mixing was common among many Native American tribes prior to European contact, as they would adopt captives taken in warfare. Individuals often had ancestry from more than one tribe, particularly after tribes lost so many members from disease in the colonial era and after. Bands or entire tribes occasionally split or merged to form more viable groups in reaction to the pressures of climate, disease and warfare.
A number of tribes traditionally adopted captives into their group to replace members who had been captured or killed in battle. Such captives were from rival tribes and later were taken from raids on European settlements. Some tribes also sheltered or adopted white traders and runaway slaves, and others owned slaves of their own. Tribes with long trading histories with Europeans show a higher rate of European admixture, reflecting years of intermarriage between Native American women and European men, often seen as advantageous to both sides. A number of paths to genetic and ethnic diversity among Native Americans have occurred.
In recent years, genetic genealogists have been able to determine the proportion of Native American ancestry carried by the African-American population. The literary and history scholar Henry Louis Gates, Jr., had experts on his TV programs who discussed African-American ancestry. They stated that 5% of African Americans have at least 12.5% Native American ancestry, or the equivalent to one great-grandparent, which may represent more than one distant ancestor. A greater percentage could have a smaller proportion of Indian ancestry, but their conclusions show that popular estimates of Native American admixture may have been too high.
DNA testing is not sufficient to qualify a person for specific tribal membership, as it cannot distinguish among Native American tribes.
Native American identity has historically been based on culture, not just biology, as many American Indian peoples adopted captives from their enemies and assimilated them into their tribes. The Indigenous Peoples Council on Biocolonialism (IPCB) notes that:
"Native American markers" are not found solely among Native Americans. While they occur more frequently among Native Americans, they are also found in people in other parts of the world.
Geneticists state:Not all Native Americans have been tested; especially with the large number of deaths due to disease such as smallpox, it is unlikely that Native Americans only have the genetic markers they have identified far, even when their maternal or paternal bloodline does not include a non-Native American.
Tribal membership.
To receive tribal services, a Native American must be a certified (or enrolled) member of a federally recognized tribal organization. Each tribal government makes its own rules for eligibility of citizens or tribal members. Among tribes, qualification for enrollment may be based upon a required percentage of Native American "blood" (or the "blood quantum") of an individual seeking recognition, or documented descent from an ancestor on the Dawes Rolls or other registers. But, the federal government has its own standards related to who qualifies for services available to certified Native Americans. For instance, federal scholarships for Native Americans require the student both to be enrolled in a federally recognized tribe "and" to be of at least one-quarter Native American descent (equivalent to one grandparent), attested to by a Certificate of Degree of Indian Blood (CDIB) card issued by the federal government.
Some tribes have begun requiring genealogical DNA testing of individuals' applying for membership, but this is usually related to an individual's proving parentage or direct descent from a certified member. Requirements for tribal membership vary widely by tribe. The Cherokee require documented direct genealogical descent from a Native American listed on the early 1906 Dawes Rolls. Tribal rules regarding recognition of members who have heritage from multiple tribes are equally diverse and complex.
Tribal membership conflicts have led to a number of legal disputes, court cases, and the formation of activist groups. One example of this are the Cherokee Freedmen. Today, they include descendants of African Americans once enslaved by the Cherokees, who were granted, by federal treaty, citizenship in the historic Cherokee Nation as freedmen after the Civil War. The modern Cherokee Nation, in the early 1980s, passed a law to require that all members must prove descent from a Cherokee Native American (not Cherokee Freedmen) listed on the Dawes Rolls, resulting in the exclusion of some individuals and families who had been active in Cherokee culture for years.
Increased self-identification.
Since the census of 2000, people may identify as being of more than one race. Since the 1960s, the number of people claiming Native American ancestry has grown significantly and by the 2000 census, the number had more than doubled. Sociologists attribute this dramatic change to "ethnic shifting" or "ethnic shopping"; they believe that it reflects a willingness of people to question their birth identities and adopt new ethnicities which they find more compatible.
The author Jack Hitt writes:
The journalist Mary Annette Pember notes that identifying with Native American culture may be a result of a person's increased interest in genealogy, the romanticization of the lifestyle, and a family tradition of Native American ancestors in the distant past. There are different issues if a person wants to pursue enrollment as a member of a tribe. Different tribes have different requirements for tribal membership; in some cases persons are reluctant to enroll, seeing it as a method of control initiated by the federal government; and there are individuals who are 100% Native American but, because of their mixed tribal heritage, do not qualify to belong to any individual tribe. Pember concludes:
Genetics.
The genetic history of indigenous peoples of the Americas primarily focuses on human Y-chromosome DNA haplogroups and human mitochondrial DNA haplogroups. "Y-DNA" is passed solely along the patrilineal line, from father to son, while "mtDNA" is passed down the matrilineal line, from mother to offspring of both sexes. Neither recombines, and thus Y-DNA and mtDNA change only by chance mutation at each generation with no intermixture between parents' genetic material. Autosomal "atDNA" markers are also used, but differ from mtDNA or Y-DNA in that they overlap significantly. Autosomal DNA is generally used to measure the average continent-of-ancestry genetic admixture in the entire human genome and related isolated populations.
The genetic pattern indicates Indigenous Americans experienced two very distinctive genetic episodes; first with the initial-peopling of the Americas, and secondly with European colonization of the Americas. The former is the determinant factor for the number of gene lineages, zygosity mutations and founding haplotypes present in today's Indigenous Amerindian populations.
Human settlement of the New World occurred in stages from the Bering sea coast line, with an initial 15,000 to 20,000-year layover on Beringia for the small founding population. The micro-satellite diversity and distributions of the Y lineage specific to South America indicates that certain Amerindian populations have been isolated since the initial colonization of the region. The Na-Dené, Inuit and Indigenous Alaskan populations exhibit haplogroup Q-M242 (Y-DNA) mutations, however, that are distinct from other indigenous Amerindians, and that have various mtDNA and atDNA mutations. This suggests that the paleo-Indian migrants into the northern extremes of North America and Greenland were descended from a later, independent migrant population.

</doc>
<doc id="21218" url="https://en.wikipedia.org/wiki?curid=21218" title="Nights into Dreams...">
Nights into Dreams...

Development began soon after the release of "Sonic & Knuckles" in 1994, although the concept originated during the development of "Sonic the Hedgehog 2" (1992). Development was led by Sonic Team veterans Yuji Naka, Naoto Ohshima, and Takashi Iizuka. Naka began the project with the central idea being flight, and Oshima designed the character Nights to resemble an angel that could fly like a bird. Oshima also designed Nights specifically as an androgynous character. The team conducted research on dreaming and REM sleep, and was influenced by the works and theories of psychoanalysts Carl Jung and Sigmund Freud. An analogue controller, known as the Saturn 3D controller, was designed alongside the game and was exclusively included with some retail copies sold.
"Nights into Dreams..." received positive reviews upon release; critics praised the graphics, gameplay, soundtrack, and atmosphere. It has been included on multiple lists as being the best Sega Saturn game of all time and among the best games ever made. An abbreviated version of the game with a Christmas theme, titled "Christmas Nights", was released in December 1996. The game was ported to the PlayStation 2 in 2008 exclusively in Japan and a high-definition version was released worldwide for PlayStation 3, Xbox 360, and Microsoft Windows in 2012. A direct sequel, "", was released for the Wii in 2007.
Gameplay.
"Nights into Dreams..." is split into seven levels, referred to as "Dreams". The levels are distributed equally between the two teenage characters; three are unique to Claris, three to Elliot, and each play through an identical final seventh level, "Twin Seeds". Initially, only Claris' Spring Valley and Elliot's Splash Garden levels are available, and successful completion of one of these unlocks the next level in that character's path. Previously completed stages may be revisited to improve the player's high scores; a grade between A and F will be given to the player upon completion, however, a "C" grade in all the selected character's levels must be achieved to unlock the relevant Twin Seeds stage for that character. Points are accumulated depending on how fast the player completes a level, and extra points are awarded when the player flies through rings.
Each level is split up into four "Mares" set in Nightopia and a boss fight which takes place in Nightmare. In each level, players initially control Claris or Elliot, who immediately have their Ideyas (spherical objects that contain emotions) of hope, growth, intelligence and purity stolen from them by Wizeman's minions, leaving behind only their Ideya of courage. The goal of each Mare is to recover one of the stolen Ideya by collecting 20 blue chips and delivering them to the cage holding the Ideyas, which will overload and release the orb it holds. If the player walks around the landscape for too long, they will be pursued by a sentient alarm clock which will awaken the character and end the level if it comes in contact with the player. The majority of the gameplay centres on flying sequences, which is triggered by walking into the Ideya Palace near the start of each level so that the character merges with the imprisoned Nights, the protagonist. Once the flying sequence is initiated, the time limit will begin.
In the flying sections, the player controls Nights' flight along a predetermined route through each Mare, resembling that of a 2D platformer. The player has only a limited period of time available before Nights falls to the ground and transforms back into Claris or Elliot, and each collision with an enemy subtracts five seconds from the time remaining. Whilst flying, Nights can use a boost to travel faster, as well as defeat certain reverie enemies littered across the field. Grabbing onto certain enemies causes Nights to spin around, which will launch both Nights and the enemy in the direction the boost is pressed. Various acrobatic manoeuvres can be performed, including the "Paraloop", whereby flying around in a complete circle and connecting the trail of stars left in Nights' wake will cause any items within the loop to be attracted towards Nights. The game features a combo system known as "Linking", whereby actions such as collecting items and flying through rings are worth more points when performed in quick succession. Power-ups may be gained by flying through a number of predetermined rings, which will be indicated by a bonus barrel. The power-ups include a speed boost, point multiplier and an air pocket.
At the end of each Mare, players are given a grade based on their score, and after all four Mares are cleared an overall grade for the level is displayed. Nights is then transported to Nightmare for a boss fight against one of Wizeman's "Level Two" Nightmarens. Each boss fight has a time limit, and the game will end if the player runs out of time during the battle. Upon winning the boss fight, the player is awarded a score multiplier based on how quickly the boss was defeated, which is then applied to the score earned in the Nightopia section to produce the player's final score for that Dream. The game also features a multiplayer mode, which allows two players to battle each other by using a splitscreen. One player controls Nights, whereas the other controls the secondary antagonist, Reala. The winner is determined by the first player to defeat the other, which is accomplished by hitting the other player three times.
The game features an artificial life system known as "A-Life", which involves entities called Nightopians and keeps track of their moods. It is possible to have them mate with other Nightopians, which will result in creating hybrids known as "Superpians". The more the game is played, the more inhabitants appear and will consequently change environmental features and cultures. The A-Life system features an evolving music engine, allowing tempo, pitch, and melody to alter depending on the state of Nightopians within the level. The feature also runs from the Sega Saturn's internal clock, which will also alter features in the A-Life system depending on the time.
Plot.
Setting.
Every night, all human dreams are played out in Nightopia and Nightmare, the two parts of the dream world. In Nightopia, distinct aspects of dreamers' personalities are represented by luminous coloured spheres known as "Ideya". However, the evil ruler of Nightmare, Wizeman the Wicked, is stealing this dream energy from sleeping visitors in order to gather power and take control of Nightopia and eventually the real world. To achieve this, he creates five beings called "Nightmaren"; jester-like, flight-capable beings, which include Jackle, Clawz, Gulpo, Gillwing and Puffy, as well as many minor maren. In addition, he creates two "Level One" Nightmaren; Nights and Reala. However, Nights rebels against Wizeman's plans, and is punished by being imprisoned inside an Ideya palace, a gazebo-like container for dreamers' Ideya.
Synopsis.
One day, Elliot Edwards and Claris Sinclair, two teenagers from the city of Twin Seeds, go through failures. Elliot is a basketball player who enjoys a game with his friends. However, he is challenged by a group of older school students and suffers a humiliating defeat on the court. Claris is a talented singer and her ambition is to perform on stage. She auditions for a part in the events commemorating the centenary of the city of Twin Seeds. However, upon standing in front of the judges, she is overcome by stage fright and does not perform well, which causes her to lose all hopes of getting the role. On the night they go to sleep, both Elliot and Claris suffer nightmares that replay the events. They escape into Nightopia and find that they both possess the rare Red Ideya of Courage, the only type that Wizeman cannot steal.
Once in Nightopia, they discover and release Nights, who tells them about dreams and Wizeman and his plans; the three begin a journey to stop Wizeman and restore peace to Nightopia. Upon defeating Wizeman and Reala, peace is returned to Nightopia and the world of Nightmare is suppressed. The next day, a ceremony of the centenary of Twin Seeds commences. Elliot is seen walking through the parade until he sees a vision of Nights looking at him through a hoarding. Realising that Claris is performing in a stage hall, Elliot runs through the crowd and sees Claris singing well in front of a large audience. The two look at each other, and are transitioned to a spring valley in Nightopia, which leaves ambiguity if what they achieved was real or just a dream.
Development.
The game was developed by Sonic Team in the United States, in contrast to their previous titles being developed in Japan. The concept for the game originated during the development of "Sonic the Hedgehog 2" in 1992, however, actual development did not begin until after the release of "Sonic & Knuckles" in late 1994. The programming of the game began in April 1995 and total development spanned over a period of six months. The development team consisted of staff who worked on previous "Sonic the Hedgehog" titles; Yuji Naka (the former head of AM8) was selected to act as lead programmer and producer, whilst Naoto Oshima and Takashi Iizuka undertook the roles of director and lead designer, respectively. According to Naka, the initial development team consisted of seven people at the start of production, and gradually grew to 20 people once more programmers started to arrive.
"Sonic" creator and project director Oshima created the character of Nights based on his impressions of travelling around Europe and western Asia. He eventually came to the conclusion that the main character of the game should resemble an angel and fly like a bird. Naka originally settled on making "Nights into Dreams..." a slow paced game, however, as development progressed the gameplay pace gradually increased, in similar vein to "Sonic the Hedgehog" titles. The initial concept of the game featured the flying character in a rendered 2D sprite art, with side-scrolling features similar to "Sonic the Hedgehog". The team were at first hesitant to switch the game from 2D to 3D, as Naka was sceptical over creating "appealing" characters with polygons, in contrast to traditional pixel art sprites, which Sonic Team's designers found "more expressive". According to Izuka, the game design and story took two years to finalise.
The game was developed using Silicon Graphics for graphical designs and Hewlett-Packard emulators for programming. Izuka stated that the game faced problems during early stages of development due to the lack of games to use as reference; the team were consequently tasked with redesigning the Spring Valley numerous times and building "everything from scratch". Due to the Sonic Team offices not accommodating any sound-proof studios, anonymous team members were forced to record vocal sound effects for the game at night. Naka revealed that every phrase in the game has a meaning, for example "abayo" is Japanese slang for "goodbye". Regarding the design of the 3D cutscenes, the team felt that the global market would be "less resistant" to a game featuring full 3D CGI in comparison to 2D anime. Norihiro Nishiyama, the designer of the in-game movie, stated that the use of 3D cutscenes in the game was necessary as he felt the movies were a good method to show the different concepts of dreaming and waking up into reality. Naka stated that the in-game movies incorporate realism in order to make it more difficult for the player to disambiguate the boundary between dreams and reality.
At the end of development, Naka admitted that the cycle took longer than expected, due to the team's inexperience with Saturn hardware and the uncertainty of utilising the full 560 megabyte space on the CD-ROM. The team initially thought that the game would consume around 100 megabytes of data, and at one point considered releasing the game on two separate discs. In a retrospective interview, Iizuka declared that the most difficult part of development was finding a way of handling the "contradiction" of using 2D sidescroller controls in a fully 3D game. Naka also purposely limited the game's flying mechanic to "invisible 2D tracks" because early beta testing revealed that the game was too difficult to play in full 3D.
In a 2007 interview, Iizuka stated that the Christmas-themed add-on "Christmas Nights" was conceptualised in order to increase the sales of the Saturn consoles, and that the game was inspired by various titles and characters from Japanese anime and Cirque du Soleil's "Mystère" performance. The development team performed various research into dream sequences and REM sleep; which included the works of the psychoanalysts Carl Jung, Sigmund Freud and Friedrich Holtz. Iizuka analysed Jung's theories of dream archetypes, and spent a considerable amount of time studying dreams and theories associated with them. Naka said that the main protagonist, Nights, is reflective of Jung's analytical "shadow" theory, whereas the two central characters, Claris and Elliot were inspired from Jung's Animus and Anima.
Release.
"Nights into Dreams..." was introduced alongside an optional game controller, the Saturn 3D controller, which was included with most copies of the game. The gamepad features an analogue stick and analogue triggers which was designed specifically for the game, in order to make movement easier. During development, director Steven Spielberg visited the Sonic Team studio and became the first person outside the development team to play the game. Naka asked him to play with an experimental version of the Saturn 3D controller, and was jokingly referred to as the "Spielberg controller" throughout development. The controller was designed with the game specifically in mind; the development team compared the success of the Nintendo 64 controller with "Super Mario 64" (1996), and realised that the default Saturn controller was more suited for arcade games in contrast to the gameplay of "Nights into Dreams...". The game was marketed with an advertising budget of $10 million, which included the use of television and print advertisements in the United States.
Ports.
Remakes.
Sega released a remake of "Nights into Dreams..." for the PlayStation 2 exclusively in Japan on 21 February 2008. It includes 16:9 wide screen support, an illustration gallery and features the ability to play the game in classic Saturn graphics. The game was also featured in a bundle named the Nightopia Dream Pack, which includes a reprint of a picture book that was released in Japan alongside the original Saturn game. The "Christmas Nights" levels are included as an unlockable bonus once the main game has been completed. A "Nights into Dreams..." handheld electronic game was released by Tiger Electronics in 1997, and a port of it was later released for Tiger's unsuccessful R-Zone console.
HD version.
A high definition remaster of the PlayStation 2 version was released for PlayStation Network on 2 October 2012 and for Xbox Live Arcade on 5 October 2012. A Microsoft Windows version was released via Steam on 17 December 2012. This version of the game introduces online high score leaderboards and includes the option to play either with enhanced graphics or with the original Saturn version's graphics. The HD version also includes "Christmas Nights" and the original game's two player versus mode. However, the "Sonic the Hedgehog" level of "Christmas Nights" was removed.
Related games.
Christmas Nights.
, or Christmas NiGHTS into Dreams..., is a Christmas-themed two-level game of "Nights into Dreams..." that was released in December 1996. It was introduced in Japan as part of a Christmas Sega Saturn bundle, whereas elsewhere it was given away with the purchase of certain Saturn games such as "Daytona USA Championship Circuit Edition" (1996), and was also bundled in with issues of "Sega Saturn Magazine" and "Next Generation Magazine". In the United Kingdom, "Christmas Nights" was not included with the "Sega Saturn Magazine" until December 1997.
The story of "Christmas Nights" follows Elliot and Claris during the holiday season following their adventures with Nights. Though they both enjoy the Christmas season, they feel as if something is missing. Realising that the Christmas Star that usually sits at the top of the Twin Seeds Christmas tree is absent, the pair travel to Nightopia to find it. There, they reunite with Nights and re-explore Spring Valley, which has now been decorated due to their dreams of the holiday season. The remainder of the story revolves around their attempts of defeating a revived Gillwing and retrieving the Christmas Star from his lair.
The "Christmas Nights" disc contains the full version of Claris' Spring Valley dream level from "Nights into Dreams...", which allows both Claris and Elliot to play through the area. The game uses the Saturn's internal clock to change elements of the game according to the date and time. During December, "Christmas Nights" mode is activated, resulting in further Christmas-themed alterations, such as item boxes becoming Christmas presents, greenery becoming snow, Nightopians dressing in elf costumes, and Christmas trees replacing Ideya captures. During the "Winter Nights" period, the weather in Spring Valley will change according to the hour. Other cosmetic changes are visible on New Year's Day and Halloween, and loading the game on April Fool's Day results in Reala replacing Nights as the playable character.
Development of "Christmas Nights" began in July 1996 and took "three to four" months to complete, according to Naka. The disc features a number of unlockable bonuses such as being able to play the game's soundtrack. Further extra modes allow players to observe the status of the A-life system, experiment with the game's music mixer, time attack one Mare, or play the demo stage as Sega's mascot Sonic the Hedgehog. In the minigame "Sonic the Hedgehog: Into Dreams", Sonic may only play through the stage of Spring Valley on foot, and must defeat the boss: an inflatable Dr. Robotnik. The music is a remixed version of "Final Fever", the final boss battle music from the Japanese and European version of "Sonic CD" (1993). The "Christmas Nights" content is playable in the HD version after the game has been cleared once.
Sequel.
A game with the working title ""Air Nights"" was intended to use a tilt sensor in the Saturn Mission Stick, and development later planned for the Dreamcast. In an August 1999 interview with the American "Official Dreamcast Magazine", Yuji Naka confirmed that a sequel was in development, however, the project was eventually abandoned a year later. Aside from a handheld electronic game released by Tiger Electronics and small minigames featured in several Sega titles, no full sequel was released for a Sega console. Yuji Naka expressed his enthusiasm to develop a sequel, and also noted that he was interested in using "Nights into Dreams..." as a licence "to reinforce Sega's identity".
On 1 April 2007, a sequel called "" was officially announced for the Wii. The official announcement followed items on the game published in several magazines and websites. The sequel is a Wii exclusive, making use of Wii Remote, resembling the concept of motion control from "Air Nights". The gameplay involves the use of various masks, and features a multiplayer mode for two players in addition to Nintendo Wi-Fi Connection online functions. The game was developed by Sega Studio USA, with Iizuka, one of the designers of the original game, serving as producer. It was released in Japan and the United States in December 2007, and in Europe and Australia on 18 January 2008.
In 2010, Iizuka commented that he would be interested in making a third "Nights into Dreams..." game, should the management of Sega decide to commission one.
Reception.
The game received positive reviews upon release. It holds an average score of 89% at GameRankings, based on an aggregate of nine reviews. In Japan, "Nights into Dreams..." was the best-selling game for the Sega Saturn and the 21st highest-selling game during 1996.
The graphics and flight mechanics were the most praised aspects of the game. Tom Guise of "Computer and Video Games" heralded the game's flight system and freedom as captivating, whilst stating that "Nights into Dreams..." is the "perfect evolution" of a "Sonic" game. "GamePro" said flying using the analogue joystick "is a breeze" and that the gameplay is "fun, enjoyable, and impressive". "Entertainment Weekly" said its "graceful acrobatic stunts" offer "a more compelling sensation of soaring than most flight simulators". "Edge" stated that the game was overall well designed and the graphics unrivalled, however, they noted that the game had a low number of levels and extras in comparison to "Super Mario 64". Martin Robinson of Eurogamer opined that the flight mechanics were a "giddy thrill", in contrast to "Sonic" games. Colin Ferris of Game Revolution praised the graphics and speed of the game as breathtaking and awe-inspiring, whilst summarising that it offered the "best qualities of the generation machines". "GameFan" praised the "great combination of lush graphics, amazing music, and totally unique gameplay". A reviewer of "Next Generation" criticised the fast tempo of the game, saying that the only disappointing aspect was the way "it all rushes by so fast". However, the magazine praised the two-player mode and the innovative method of "grading" the player once they completed a level.
Levi Buchanan of IGN added scepticism over the Saturn's ability to portray graphics, stating that the console "was not built to handle "Nights"" due to the game occasionally clipping and warping, though he admitted that the graphics were "pretty darn good". A reviewer of "Mean Machines Sega" praised the game's graphically vibrant colours and detailed textures, whilst comparing its smooth animation "as fluid as water". However, the reviewer noted occasional build up and glitching during the game. Sam Hickman of the British "Sega Saturn Magazine" praised the visuals and colour scheme as rich in both texture and detail, whilst suggesting that "Nights into Dreams..." is one of the most captivating games on the Saturn. A reviewer of "Next Generation" similarly commended the game's visuals, stating that they were "beyond a doubt" the most fluid and satisfying for any game on any system. Upon release, the Japanese "Sega Saturn Magazine" opined that the game would have a significant impact on the video game industry, particularly that in the action game genre. The reviewer also stated that the game "felt better" through the use of the analogue pad, in contrast to the conventional controller, and also praised the "light" and smooth feeling the analogue pad portrayed during gameplay.
Reviewers also praised the game's soundtrack and audio effects. Paul Davies of "Computer and Video Games" cited the game as having "the best music ever", whilst in the same review, Tom Guise attributed the music to creating a "hypnotically magical" atmosphere. Ferris stated that the music and sound effects were that of a dream world, and asserted that they were fitting for a game like "Nights into Dreams...". IGN's Buchanan praised the game's soundtrack, stating that each stage's soundtrack is "quite good" and that the sound effects "fit in perfectly with the dream universe". A reviewer of "Mean Machines Sega" similarly praised the music and sound effects as "awesome" and impressive.
Awards.
Since its release, "Nights into Dreams..." has appeared on several best game of all time lists. In a January 2000 poll by "Computer and Video Games", readers placed the game 15th on their "100 Greatest Games" list, directly behind "Super Mario 64". IGN ranked the game as the 94th best game of all time in their "Top 100 Games" list in 2007, and in 2008, Levi Buchanan ranked it fourth in his list of the top 10 Sega Saturn games. "Next Generation Magazine" ranked the game 25th in its list of the "100 Greatest Games of All-Time" in their September 1996 issue. 1UP ranked the game third in its "Top Ten Cult Classics" list. In 2014, GamesRadar listed "Nights into Dreams..." as the best Sega Saturn game of all time, stating that the game "tapped into a new kind of platform gameplay for its era".
Legacy.
In other media.
Claris and Eliot make a cameo appearance in Sonic Team's "Burning Rangers" (1998), with both Claris and Eliot sending the Rangers emails thanking them for their help. "Nights into Dreams..."-themed pinball areas feature in "Sonic Adventure" (1998) and "Sonic Pinball Party" (2003), with soundtrack being featured in the latter game. The PlayStation 2 titles ' (2003) and "Sega SuperStars" (2004) both feature minigames based on "Nights into Dreams...", in which Nights is controlled using the player's body. Nights is also an unlockable character in "Sonic Riders" (2006) and ' (2008).
A minigame version of "Nights into Dreams..." is playable through utilising the Nintendo GameCube – Game Boy Advance link cable connectivity with "Phantasy Star Online Episode I & II" (2000) and "Billy Hatcher and the Giant Egg" (2003). Following a successful fan campaign by a "Nights into Dreams..." fansite, the character Nights was integrated into "Sonic & Sega All-Stars Racing" (2010) as a traffic guard. Nights and Reala also appear as playable characters in "Sega Superstars Tennis" (2008) and "Sonic & All-Stars Racing Transformed" (2012), the latter of which also features a "Nights into Dreams..."-themed racetrack. The limited Deadly Six edition of "Sonic Lost World" (2013) features a "Nights into Dreams..."-inspired stage, named "Nightmare Zone", as downloadable content.
Comics.
In February 1998, Archie Comics adapted "Nights into Dreams..." into a three-issue comic book miniseries to test whether or not a Nights comic would sell well in North America. The first miniseries was loosely based on the game, with Nights being specifically identified as a male despite the character's androgynous design. The company later released a second three-issue miniseries, continuing the story of the first, however, the series did not gain enough sales to warrant an ongoing series. The series would later be added to a list of guest franchises featured in Archie Comics' "Worlds Unite" crossover between its "Sonic the Hedgehog" and "Mega Man" titles.
References.
Citations
Bibliography

</doc>
<doc id="21220" url="https://en.wikipedia.org/wiki?curid=21220" title="Negligence per se">
Negligence per se

Negligence "per se" is a doctrine within the law of United States of America whereby an act is considered negligent because it violates a statute (or regulation).
Elements.
In order to prove negligence "per se", the plaintiff usually must show that:
In some jurisdictions, negligence "per se" creates merely a rebuttable presumption of negligence.
A typical example is one in which a contractor violates a building code when constructing a house. The house then collapses, injuring somebody. The violation of the building code establishes negligence "per se" and the contractor will be found liable, so long as the contractor's breach of the code was the cause (proximate cause and actual cause) of the injury.
History.
A famous early case in negligence "per se" is "Gorris v. Scott" (1874), a Court of Exchequer case that established that the harm in question must be of the kind that the statute was intended to prevent. "Gorris" involved a shipment of sheep that was washed overboard but would not have been washed overboard had the shipowner complied with the regulations established pursuant to the Contagious Diseases (Animals) Act 1869, which required that livestock be transported in pens to segregate potentially-infected animal populations from uninfected ones. Chief Baron Fitzroy Kelly held that as the statute was intended to prevent the spread of disease, rather than the loss of livestock in transit, the plaintiff could not claim negligence "per se".
A subsequent New York Court of Appeals case, "Martin v. Herzog" (1920), penned by Judge Benjamin N. Cardozo, first presented the notion that negligence "per se" could be absolute evidence of negligence in certain cases.

</doc>
<doc id="21221" url="https://en.wikipedia.org/wiki?curid=21221" title="Neuromyotonia">
Neuromyotonia

Neuromyotonia (NMT), also known as Isaacs Syndrome and Isaacs-Merton syndrome, is a form of peripheral nerve hyperexcitability that causes spontaneous muscular activity resulting from repetitive motor unit action potentials of peripheral origin. Prevalence is unknown but 100—200 cases have been reported so far.
Causes.
The three causes of NMT are:
The acquired form is the most common accounting for up to 80 percent of all cases and is suspected to be autoimmune-mediated, which is usually caused by antibodies against the neuromuscular junction.
The exact cause is unknown. However, autoreactive antibodies can be detected in a variety of peripheral (e.g. myasthenia gravis, Lambert-Eaton myasthenic syndrome) and central nervous system (e.g. paraneoplastic cerebellar degeneration, paraneoplastic limbic encephalitis) disorders. Their causative role has been established in some of these diseases but not all. Neuromyotonia is considered to be one of these with accumulating evidence for autoimmune origin over the last few years. Autoimmune neuromyotonia is typically caused by antibodies that bind to potassium channels on the motor nerve resulting in continuous/hyper-excitability. Onset is typically seen between the ages of 15-60, with most experiencing symptoms before the age of 40. Some neuromyotonia cases do not only improve after plasma exchange but they may also have antibodies in their serum samples against voltage-gated potassium channels. Moreover, these antibodies have been demonstrated to reduce potassium channel function in neuronal cell lines.
Symptoms.
NMT is a diverse disorder. As a result of muscular hyperactivity patients may present with muscle cramps, stiffness, myotonia-like symptoms (slow relaxation), associated walking difficulties, hyperhidrosis (excessive sweating), myokymia (quivering of a muscle), fasciculations (muscle twitching), fatigue, exercise intolerance, myoclonic jerks and other related symptoms. The symptoms (especially the stiffness and fasciculations) are most prominent in the calves, legs, trunk, and sometimes the face and neck, but can also affect other body parts. NMT symptoms may fluctuate in severity and frequency. Symptoms range from mere inconvenience to debilitating. At least a third of people also experience sensory symptoms.
Types.
There are three main types of NMT:
Diagnosis.
Diagnosis is clinical and initially consists of ruling out more common conditions, disorders, and diseases, and usually begins at the general practitioner level. A doctor may conduct a basic neurological exam, including coordination, strength, reflexes, sensation, etc. A doctor may also run a series of tests that include blood work and MRIs.
From there, a patient is likely to be referred to a neurologist or a neuromuscular specialist. The neurologist or specialist may run a series of more specialized tests, including needle electromyography EMG/ and nerve conduction studies (NCS) (these are the most important tests), chest CT (to rule out paraneoplastic) and specific blood work looking for voltage-gated potassium channel antibodies, acetylcholine receptor antibody, and serum immunofixation, TSH, ANA ESR, EEG etc. Neuromyotonia is characterized electromyographically by doublet, triplet or multiplet single unit discharges that have a high, irregular intraburst frequency. Fibrillation potentials and fasciculations are often also present with electromyography.
Because the condition is so rare, it can often be years before a correct diagnosis is made.
NMT is not fatal and many of the symptoms can be controlled. However, because NMT mimics some symptoms of motor neuron disease (ALS) and other more severe diseases, which may be fatal, there can often be significant anxiety until a diagnosis is made. In some rare cases, acquired neuromyotonia has been misdiagnosed as amyotrophic lateral sclerosis (ALS) particularly if fasciculations may be evident in the absence of other clinical features of ALS. However, fasciculations are rarely the first sign of ALS as the hallmark sign is weakness. Similarly, Multiple Sclerosis has been the initial misdiagnosis in some NMT patients. In order to get an accurate diagnosis see a trained neuromuscular specialist.
Peripheral Nerve Hyperexcitability.
Neuromyotonia is a type of peripheral nerve hyperexcitability. Peripheral Nerve Hyperexcitability is an umbrella diagnosis that includes (in order of severity of symptoms from least severe to most severe) Benign Fasciculation Syndrome, Cramp Fasciculation Syndrome, and Neuromyotonia. Some doctors will only give the diagnosis of Peripheral Nerve Hyperexcitability as the differences between the three are largely a matter of the severity of the symptoms and can be subjective. However, some objective EMG criteria have been established to help distinguish between the three.
Moreover, the generic use of the term "Peripheral Nerve Hyperexcitability syndromes" to describe the aforementioned conditions is recommended and endorsed by several prominent researchers and practitioners in the field. 
Treatments.
There is no known cure for Neuromyotonia, but the condition is treatable. Anticonvulsants, including phenytoin and carbamazepine, usually provide significant relief from the stiffness, muscle spasms, and pain associated with Neuromyotonia. Plasma exchange and IVIg treatment may provide short-term relief for patients with some forms of the acquired disorder. It is speculated that the plasma exchange causes an interference with the function of the voltage-dependent potassium channels, one of the underlying issues of hyper-excitability in autoimmune Neuromyotonia. Botox injections also provide short-term relief. Immunosuppressants, such as Prednisone may provide long term relief for patients with some forms of the acquired disorder.
Prognosis.
The long-term prognosis for individuals with the disorder is uncertain, and this has mostly to do with the etiology (underlying cause; i.e. autoimmune, paraneoplastic, etc.) and lack of research for this disorder. However, in recent years our increased understanding of the basic mechanisms of NMT and autoimmunity has led to the development of novel treatment strategies. NMT disorders are now amenable to treatment and their prognoses are good. Many patients respond well to treatment, which usually provide significant relief of symptoms. Some cases of spontaneous remission have been noted, including Isaac's original two patients when followed up 14 years later.
While NMT symptoms may fluctuate, they generally don't deteriorate into anything more serious and with the correct treatment the symptoms are manageable.
A very small proportion of cases with NMT may develop central nervous system findings in their clinical course, causing a disorder called Morvan's syndrome and they may also have antibodies against potassium channels in their serum samples. Sleep disorder is only one of a variety of clinical conditions observed in Morvan's syndrome cases ranging from confusion and memory loss to hallucinations and delusions. However, this is a separate disorder.
Some studies have linked NMT with certain types of cancers, mostly lung and thymus, suggesting that NMT may be paraneoplastic in some cases. In these cases, the underlying cancer will determine prognosis. However, most examples of NMT are autoimmune and not associated with cancer.

</doc>
<doc id="21224" url="https://en.wikipedia.org/wiki?curid=21224" title="Napoleon (disambiguation)">
Napoleon (disambiguation)

Napoleon (1769–1821) also known as Napoleon Bonaparte or Napoleon I, was a French military leader and emperor.
Napoleon (English) or Napoléon (French) or Napoleón (Spanish) or Napoleone (Italian) may also refer to:

</doc>
<doc id="21226" url="https://en.wikipedia.org/wiki?curid=21226" title="Neurology">
Neurology

Neurology (from , "neuron", and the suffix
A neurologist is a physician specializing in neurology and trained to investigate, or diagnose and treat neurological disorders. Neurologists may also be involved in clinical research, clinical trials, and basic or translational research. While neurology is a non-surgical specialty, its corresponding surgical specialty is neurosurgery.
Scope.
A large number of neurological disorders have been described as listed. These can affect the central nervous system (brain and spinal cord), the peripheral nervous system, the autonomic nervous system and the muscular system.
Training.
In the United States and Canada, neurologists are physicians having completed postgraduate training in neurology after graduation from medical school. Neurologists complete, on average, at least 10–13 years of college education and clinical training. This training includes obtaining a four-year undergraduate degree, a medical degree (D.O. or M.D.), which comprises an additional four years of study, and then completing a one-year internship and a three-year residency in neurology. The four-year residency consists of one year of internal medicine internship training followed by three years of training in neurology.
Some neurologists receive additional subspecialty training focusing on a particular area of neurology. These training programs are called fellowships, and are one to two years in duration. Sub-specialties include: 
brain injury medicine, clinical neurophysiology, epilepsy, hospice and palliative medicine, neurodevelopmental disabilities, neuromuscular medicine, pain medicine and sleep medicine, neurocritical care, vascular neurology (stroke), behavioral neurology, child neurology, headache, multiple sclerosis, neuroimaging, neurorehabilitation, and interventional neurology.
In Germany, a compulsory year of psychiatry must be done to complete a residency of neurology.
In the United Kingdom and Ireland, neurology is a subspecialty of general (internal) medicine. After five to nine years of medical school and a year as a pre-registration house officer (or two years on the Foundation Programme), a neurologist must pass the examination for Membership of the Royal College of Physicians (or the Irish equivalent) before completing two years of core medical training and then entering specialist training in neurology. A generation ago, some neurologists would have also spent a couple of years working in psychiatric units and obtain a Diploma in Psychological Medicine. However, this requirement has become uncommon, and, now that a basic psychiatric qualification takes three years to obtain, the requirement is no longer practical. A period of research is essential, and obtaining a higher degree aids career progression: Many found it was eased after an attachment to the Institute of Neurology at Queen Square, London. Some neurologists enter the field of rehabilitation medicine (known as physiatry in the US) to specialise in neurological rehabilitation, which may include stroke medicine as well as brain injuries.
Physical examination.
During a neurological examination, the neurologist reviews the patient's health history with special attention to the current condition. The patient then takes a neurological exam. Typically, the exam tests mental status, function of the cranial nerves (including vision), strength, coordination, reflexes, and sensation. This information helps the neurologist determine whether the problem exists in the nervous system and the clinical localization. Localization of the pathology is the key process by which neurologists develop their differential diagnosis. Further tests may be needed to confirm a diagnosis and ultimately guide therapy and appropriate management.
Clinical tasks.
Neurologists examine patients who have been referred to them by other physicians in both the inpatient and outpatient settings. A neurologist will begin their interaction with a patient by taking a comprehensive medical history, and then perform a physical examination focusing on evaluating the nervous system. Components of the neurological examination include assessment of the patient's cognitive function, cranial nerves, motor strength, sensation, reflexes, coordination, and gait.
In some instances, neurologists may order additional diagnostic tests as part of the evaluation. Commonly employed tests in neurology include imaging studies such as computed axial tomography (CAT) scans, magnetic resonance imaging (MRI), and ultrasound of major blood vessels of the head and neck. Neurophysiologic studies, including electroencephalography (EEG), needle electromyography (EMG), nerve conduction studies (NCSs) and evoked potentials are also commonly ordered. Neurologists frequently perform lumbar punctures in order to assess characteristics of a patient's cerebrospinal fluid. Advances in genetic testing has made genetic testing and important tool in the classification of inherited neuromuscular disease. The role of genetic influences on the development of acquired neuromuscular diseases is an active area of research.
Some of the commonly encountered conditions treated by neurologists include headaches, radiculopathy, neuropathy, stroke, dementia, seizures and epilepsy, Alzheimer's Disease, Attention deficit/hyperactivity disorder, Parkinson's Disease, Tourette's syndrome, multiple sclerosis, head trauma, sleep disorders, neuromuscular diseases, and various infections and tumors of the nervous system. Neurologists are also asked to evaluate unresponsive patients on life support in order to confirm brain death.
Treatment options vary depending on the neurological problem. They can include everything from referring the patient to a physiotherapist, to prescribing medications, to recommending a surgical procedure.
Some neurologists specialize in certain parts of the nervous system or in specific procedures. For example, clinical neurophysiologists specialize in the use of EEG and intraoperative monitoring in order to diagnose certain neurological disorders. Other neurologists specialize in the use of electrodiagnostic medicine studies - needle EMG and NCSs. In the US, physicians do not typically specialize in all the aspects of clinical neurophysiology - i.e. sleep, EEG, EMG, and NCSs. The American Board of Clinical Neurophysiology certifies US physicians in general clinical neurophysiology, epilepsy, and intraoperative monitoring. The American Board of Electrodiagnostic Medicine certifies US physicians in electrodiagnostic medicine and certifies technologists in nerve conduction studies. Sleep medicine is a subspecialty field in the US under several medical specialties including anesthesiology, internal medicine, family medicine, and neurology. Neurosurgery is a distinct specialty that involves a different training path, and emphasizes the surgical treatment of neurological disorders.
There are also many non-medical doctors, those with PhD degrees in subjects such as biology and chemistry, who study and research the nervous system. Working in labs in universities, hospitals, and private companies, these neuroscientists perform clinical and laboratory experiments and tests in order to learn more about the nervous system and find cures or new treatments for diseases and disorders.
There is a great deal of overlap between neuroscience and neurology. A large number of neurologists work in academic training hospitals, where they conduct research as neuroscientists in addition to treating patients and teaching neurology to medical students.
General caseload.
Neurologists are responsible for the diagnosis, treatment, and management of all the conditions mentioned above. When surgical intervention is required, the neurologist may refer the patient to a neurosurgeon. In some countries, additional legal responsibilities of a neurologist may include making a finding of brain death when it is suspected that a patient has died. Neurologists frequently care for people with hereditary (genetic) diseases when the major manifestations are neurological, as is frequently the case. Lumbar punctures are frequently performed by neurologists. Some neurologists may develop an interest in particular subfields, such as stroke, dementia, movement disorders, neurointensive care, headaches, epilepsy, sleep disorders, chronic pain management, multiple sclerosis, or neuromuscular diseases.
Overlapping areas.
There is some overlap with other specialties, varying from country to country and even within a local geographic area. Acute head trauma is most often treated by neurosurgeons, whereas sequelae of head trauma may be treated by neurologists or specialists in rehabilitation medicine. Although stroke cases have been traditionally managed by internal medicine or hospitalists, the emergence of vascular neurology and interventional neurologists has created a demand for stroke specialists. The establishment of Joint Commission certified stroke centers has increased the role of neurologists in stroke care in many primary as well as tertiary hospitals. Some cases of nervous system infectious diseases are treated by infectious disease specialists. Most cases of headache are diagnosed and treated primarily by general practitioners, at least the less severe cases. Likewise, most cases of sciatica and other mechanical radiculopathies are treated by general practitioners, though they may be referred to neurologists or a surgeon (neurosurgeons or orthopedic surgeons). Sleep disorders are also treated by pulmonologists and psychiatrists. Cerebral palsy is initially treated by pediatricians, but care may be transferred to an adult neurologist after the patient reaches a certain age. Physical medicine and rehabilitation physicians also in the US diagnosis and treat patients with neuromuscular diseases through the use of electrodiagnostic studies (needle EMG and nerve conduction studies) and other diagnostic tools. In the United Kingdom and other countries, many of the conditions encountered by older patients such as movement disorders including Parkinson's Disease, stroke, dementia or gait disorders are managed predominantly by specialists in geriatric medicine.
Clinical neuropsychologists are often called upon to evaluate brain-behavior relationships for the purpose of assisting with differential diagnosis, planning rehabilitation strategies, documenting cognitive strengths and weaknesses, and measuring change over time (e.g., for identifying abnormal aging or tracking the progression of a dementia).
Relationship to clinical neurophysiology.
In some countries, e.g. USA and Germany, neurologists may subspecialize in clinical neurophysiology, the field responsible for EEG and intraoperative monitoring, or in electrodiagnostic medicine nerve conduction studies, EMG and evoked potentials. In other countries, this is an autonomous specialty (e.g., United Kingdom, Sweden, Spain).
Overlap with psychiatry.
Although mental illnesses are believed by many to be neurological disorders affecting the central nervous system, traditionally they are classified separately, and treated by psychiatrists. In a 2002 review article in the American Journal of Psychiatry, Professor Joseph B. Martin, Dean of Harvard Medical School and a neurologist by training, wrote that "the separation of the two categories is arbitrary, often influenced by beliefs rather than proven scientific observations. And the fact that the brain and mind are one makes the separation artificial anyway".
Neurological disorders often have psychiatric manifestations, such as post-stroke depression, depression and dementia associated with Parkinson's disease, mood and cognitive dysfunctions in Alzheimer's disease and Huntington disease, to name a few. Hence, there is not always a sharp distinction between neurology and psychiatry on a biological basis. The dominance of psychoanalytic theory in the first three quarters of the 20th century has since then been largely replaced by a focus on pharmacology. Despite the shift to a medical model, brain science has not advanced to the point where scientists or clinicians can point to readily discernible pathologic lesions or genetic abnormalities that in and of themselves serve as reliable or predictive biomarkers of a given mental disorder.
Neurological enhancement.
The emerging field of neurological enhancement highlights the potential of therapies to improve such things as workplace efficacy, attention in school, and overall happiness in personal lives. However, this field has also given rise to questions about neuroethics and the psychopharmacology of lifestyle drugs.

</doc>
<doc id="21227" url="https://en.wikipedia.org/wiki?curid=21227" title="Nu">
Nu

Nu or NU may refer to:
Burmese people.
"Nu" is a common Burmese name and may refer to:

</doc>
