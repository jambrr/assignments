<doc id="23479" url="https://en.wikipedia.org/wiki?curid=23479" title="Physicalism">
Physicalism

In philosophy, physicalism is the ontological thesis that "everything is physical", that there is "nothing over and above" the physical, or that everything supervenes on the physical. Physicalism is a form of ontological monism—a "one substance" view of the nature of reality as opposed to a "two-substance" (dualism) or "many-substance" (pluralism) view. Both the definition of "physical" and the meaning of physicalism have been debated.
Physicalism is closely related to materialism. Physicalism grew out of materialism with the success of the physical sciences in explaining observed phenomena. The terms are often used interchangeably, although they are sometimes distinguished, for example on the basis of physics describing more than just matter (including energy and physical law). Common arguments against physicalism include both the philosophical zombie argument and the multiple observers argument, that the existence of a physical being may imply zero or more distinct conscious entities.
Definition of physical.
The use of "physical" in physicalism is a philosophical concept and can be distinguished from alternative definitions found in the literature (e.g. Popper defined a physical proposition to be one which can at least in theory be denied by observation). A "physical property", in this context, may be a metaphysical or logical combination of properties which are physical in the ordinary sense. It is common to express the notion of "metaphysical or logical combination of properties" using the notion of supervenience: A property "A" is said to supervene on a property "B" if any change in "A" necessarily implies a change in "B". Since any change in a combination of properties must consist of a change in at least one component property, we see that the combination does indeed supervene on the individual properties. The point of this extension is that physicalists usually suppose the existence of various abstract concepts which are non-physical in the ordinary sense of the word; so physicalism cannot be defined in way that denies the existence of these abstractions. Also, physicalism defined in terms of supervenience does not entail that all properties in the actual world are type identical to physical properties. It is, therefore, compatible with multiple realizability.
From the notion of supervenience, we see that, assuming that mental, social, and biological properties supervene on physical properties, it follows that two hypothetical worlds cannot be identical in their physical properties but differ in their mental, social or biological properties.
Two common approaches to defining "physicalism" are the theory-based and object-based approaches. The theory-based conception of physicalism proposes that "a property is physical if and only if it either is the sort of property that physical theory tells us about or else is a property which metaphysically (or logically) supervenes on the sort of property that physical theory tells us about". Likewise, the object-based conception claims that "a property is physical if and only if: it either is the sort of property required by a complete account of the intrinsic nature of paradigmatic physical objects and their constituents or else is a property which metaphysically (or logically) supervenes on the sort of property required by a complete account of the intrinsic nature of paradigmatic physical objects and their constituents".
Physicalists have traditionally opted for a "theory-based" characterization of the physical either in terms of current physics, or a future (ideal) physics. These two theory-based conceptions of the physical represent both horns of Hempel's dilemma (named after the late philosopher of science and logical empiricist Carl Gustav Hempel): an argument against theory-based understandings of the physical. Very roughly, Hempel's dilemma is that if we define the physical by reference to current physics, then physicalism is very likely to be false, as it is very likely (by pessimistic meta-induction) that much of current physics is false. But if we instead define the physical in terms of a future (ideal) or completed physics, then physicalism is hopelessly vague or indeterminate.
While the force of Hempel's dilemma against theory-based conceptions of the physical remains contested, alternative "non-theory-based" conceptions of the physical have also been proposed. Frank Jackson (1998) for example, has argued in favour of the aforementioned "object-based" conception of the physical. An objection to this proposal, which Jackson himself noted in 1998, is that if it turns out that panpsychism or panprotopsychism is true, then such a non-materialist understanding of the physical gives the counterintuitive result that physicalism is, nevertheless, also true since such properties will figure in a complete account of paradigmatic examples of the physical.
David Papineau and Barbara Montero have advanced and subsequently defended a "via negativa" characterization of the physical. The gist of the via negativa strategy is to understand the physical in terms of what it is not: the mental. In other words, the via negativa strategy understands the physical as "the non-mental". An objection to the via negativa conception of the physical is that (like the object-based conception) it doesn't have the resources to distinguish neutral monism (or panprotopsychism) from physicalism.
Supervenience-based definitions of physicalism.
Adopting a supervenience-based account of the physical, the definition of physicalism as "all properties are physical" can be unravelled to:
1) Physicalism is true at a possible world "w" if and only if any world that is a physical duplicate of "w" is also a duplicate of "w simpliciter".
Applied to the actual world (our world), statement 1 above is the claim that physicalism is true at the actual world if and only if at "every possible world" in which the physical properties and laws of the actual world are instantiated, the non-physical (in the ordinary sense of the word) properties of the actual world are instantiated as well. To borrow a metaphor from Saul Kripke (1972), the truth of physicalism at the actual world entails that once God has instantiated or "fixed" the physical properties and laws of our world, then God's work is done; the rest comes "automatically".
Unfortunately, statement 1 fails to capture even a necessary condition for physicalism to be true at a world "w". To see this, imagine a world in which there are "only" physical properties—if physicalism is true at any world it is true at this one. But one can conceive physical duplicates of such a world that are "not" also duplicates simpliciter of it: worlds that have the same physical properties as our imagined one, but with some additional property or properties. A world might contain "epiphenomenal ectoplasm", some additional pure experience that does not interact with the physical components of the world and is not necessitated by them (does not supervene on them). To handle the epiphenomenal ectoplasm problem, statement 1 can be modified to include a "that's-all" or "totality" clause or be restricted to "positive" properties. Adopting the former suggestion here, we can reformulate statement 1 as follows:
2) Physicalism is true at a possible world "w" if and only if any world that is a "minimal" physical duplicate of "w" is a duplicate of "w simpliciter".
Applied in the same way, statement 2 is the claim that physicalism is true at a possible world "w" if and only if any world that is a physical duplicate of "w" (without any further changes), is duplicate of "w" without qualification. This allows a world in which there are only physical properties to be counted as one at which physicalism is true, since worlds in which there is some extra stuff are "not" "minimal" physical duplicates of such a world, nor are they minimal physical duplicates of worlds that contain some non-physical properties that are metaphysically necessitated by the physical.
But while statement 2 overcomes the problem of worlds at which there is some extra stuff (sometimes referred to as the "epiphenomenal ectoplasm problem") it faces a different challenge: the so-called "blockers problem". Imagine a world where the relation between the physical and non-physical properties at this world (call the world "w1") is slightly weaker than metaphysical necessitation, such that a certain kind of non-physical intervener—"a blocker"—could, were it to exist at "w1," prevent the non-physical properties in "w1" from being instantiated by the instantiation of the physical properties at "w1." Since statement 2 rules out worlds which are physical duplicates of "w1" that also contain non-physical interveners by virtue of the minimality, or that's-all clause, statement 2 gives the (allegedly) incorrect result that physicalism is true at "w1." One response to this problem is to abandon statement 2 in favour of the alternative possibility mentioned earlier in which supervenience-based formulations of physicalism are restricted to what David Chalmers (1996) calls "positive properties". A positive property is one that "...if instantiated in a world W, is also instantiated by the corresponding individual in all worlds that contain W as a proper part." Following this suggestion, we can then formulate physicalism as follows:
3) Physicalism is true at a possible world "w" if and only if any world that is a physical duplicate of "w" is a positive duplicate of "w".
On the face of it, statement 3 seems able to handle both the epiphenomenal ectoplasm problem and the blockers problem. With regard to the former, statement 3 gives the correct result that a purely physical world is one at which physicalism is true, since worlds in which there is some extra stuff are positive duplicates of a purely physical world. With regard to the latter, statement 3 appears to have the consequence that worlds in which there are blockers are worlds where positive non-physical properties of "w1" will be absent, hence "w1" will not be counted as a world at which physicalim is true. Daniel Stoljar (2010) objects to this response to the blockers problem on the basis that since the non-physical properties of "w1" aren't instantiated at a world in which there is a blocker, they are not positive properties in Chalmers' (1996) sense, and so statement 3 will count "w1" as a world at which physicalism is true after all.
A further problem for supervenience-based formulations of physicalism is the so-called "necessary beings problem". A necessary being in this context is a non-physical being that exists in all possible worlds (for example what theists refer to as God). A necessary being is compatible with all the definitions provided, because it is supervenient on everything; yet it is usually taken to contradict the notion that everything is physical. So any supervenience-based formulation of physicalism will at best state a necessary but not sufficient condition for the truth of physicalism.
Additional objections have been raised to the above definitions provided for supervenience physicalism: one could imagine an alternate world that differs only by the presence of a single ammonium molecule (or physical property), and yet based on statement 1, such a world might be completely different in terms of its distribution of mental properties. Furthermore, there are differences expressed concerning the modal status of physicalism; whether it is a necessary truth, or is only true in a world which conforms to certain conditions (i.e. those of physicalism).
Realisation physicalism.
Closely related to supervenience physicalism, is realisation physicalism, the thesis that every instantiated property is either physical or is realised by a physical property.
Token physicalism.
Token physicalism is the proposition that "for every actual particular (object, event or process) x, there is some physical particular y such that x = y". It is intended to capture the idea of "physical mechanisms". Token physicalism is compatible with property dualism, in which all substances are "physical", but physical objects may have mental properties as well as physical properties. Token physicalism is not however equivalent to supervenience physicalism. Firstly, token physicalism does not imply supervenience physicalism because the former does not rule out the possibility of non-supervenient properties (provided that they are associated only with physical particulars). Secondarily, supervenience physicalism does not imply token physicalism, for the former allows supervenient objects (such as a "nation", or "soul") that are not equal to any physical object.
Reductionism and emergentism.
Reductionism.
There are multiple versions of reductionism. In the context of physicalism, the reductions referred to are of a "linguistic" nature, allowing discussions of, say, mental phenomena to be translated into discussions of physics. In one formulation, every concept is analysed in terms of a physical concept. One counter-argument to this supposes there may be an additional class of expressions which is non-physical but which increases the expressive power of a theory. Another version of reductionism is based on the requirement that one theory (mental or physical) be logically derivable from a second.
A common argument against reductive physicalism is multiple realizability, the possibility that a psychological process (say) could be instantiated by many different neurological processes (even non-neurological processes, in the case of machine or alien intelligence). For in this case, the neurological terms translating a psychological term must be disjunctions over the possible instantiations, and it is argued that no physical law can use these disjunctions as terms.
Emergentism.
Supervenience physicalism has been seen as a form of emergentism, in which the subject's psychological experience is considered genuinely novel. While some forms of emergentism appear either incompatible with physicalism or equivalent to it (e.g. posteriori physicalism), others appear to merge both dualism and supervenience. Such emergentism claims that mental facts and physical facts are metaphysically distinct while maintaining the supervenience of mental properties on the physical. This proposition however contradicts supervenience physicalism, which asserts a denial of dualism.
Type physicalism.
Type physicalism is a form of reductive physicalism which asserts that "for every actually instantiated property F, there is some physical property G such that F=G". Unlike token physicalism, type physicalism entails supervenience physicalism. It was the original target of the multiple realizability argument.
A priori versus a posteriori physicalism.
Physicalists hold that physicalism is true. A natural question for physicalists, then, is whether the truth of physicalism is deducible a priori from the nature of the physical world (i.e., the inference is justified independently of experience, even though the nature of the physical world can itself only be determined through experience) or can only be deduced a posteriori (i.e., the justification of the inference itself is dependent upon experience). So-called "a priori physicalists" hold that from knowledge of the conjunction of all physical truths, a totality or that's-all truth (to rule out non-physical epiphenomena, and enforce the closure of the physical world), and some primitive indexical truths such as "I am A" and "now is B", the truth of physicalism is knowable a priori. Let "P" stand for the conjunction of all physical truths and laws, "T" for a that's-all truth, "I" for the indexical "centering" truths, and "N" for any non-physical truth at the actual world. We can then, using the material conditional "→", represent a priori physicalism as the thesis that PTI → N is knowable a priori. An important wrinkle here is that the concepts in N must be possessed non-deferentially in order for PTI → N to be knowable a priori. The suggestion, then, is that possession of the concepts in the consequent, plus the empirical information in the antecedent is sufficient for the consequent to be knowable a priori.
An "a posteriori physicalist", on the other hand, will reject the claim that PTI → N is knowable a priori. Rather, they would hold that the inference from PTI to N is justified by metaphysical considerations that in turn can be derived from experience. So the claim then is that "PTI and not N" is metaphysically impossible.
One commonly issued challenge to a priori physicalism and to physicalism in general is the "conceivability argument", or zombie argument. At a rough approximation, the conceivability argument runs as follows:
P1) PTI and not Q (where "Q" stands for the conjunction of all truths about consciousness, or some "generic" truth about someone being "phenomenally" conscious there is "something it is like" to be a person x ) is conceivable (i.e., it is not knowable a priori that PTI and not Q is false).
P2) If PTI and not Q is conceivable, then PTI and not Q is metaphysically possible.
P3) If PTI and not Q is metaphysically possible then physicalism is false.
C) Physicalism is false.
Here proposition P3 is a direct application of the supervenience of consciousness, and hence of any supervenience-based version of physicalism: If PTI and not Q is possible, there is some possible world where it is true. This world differs from relevant indexing on our world, where PTIQ is true. But the other world is a minimal physical duplicate of our world, because PT is true there. So there is a possible world which is a minimal physical duplicate of our world, but not a full duplicate; this contradicts the definition of physicalism that we saw above.
Since a priori physicalists hold that PTI → N is a priori, they are committed to denying P1) of the conceivability argument. The a priori physicalist, then, must argue that PTI and not Q, on ideal rational reflection, is incoherent or contradictory.
A posteriori physicalists, on the other hand, generally accept P1) but deny P2)--the move from "conceivability to metaphysical possibility". Some a posteriori physicalists think that unlike the possession of most, if not all other empirical concepts, the possession of consciousness has the special property that the presence of PTI and the absence of consciousness will be conceivable—even though, according to them, it is knowable a posteriori that PTI and not Q is not metaphysically possible. These a posteriori physicalists endorse some version of what Daniel Stoljar (2005) has called "the phenomenal concept strategy". Roughly speaking, the phenomenal concept strategy is a label for those a posteriori physicalists who attempt to show that it is only the "concept" of consciousness—not the "property"—that is in some way "special" or sui generis. Other a posteriori physicalists eschew the phenomenal concept strategy, and argue that even ordinary macroscopic truths such as "water covers 60% of the earth's surface" are not knowable a priori from PTI and a non-deferential grasp of the concepts "water" and "earth" "et cetera". If this is correct, then we should (arguably) conclude that conceivability does not entail metaphysical possibility, and P2) of the conceivability argument against physicalism is false.
Other views.
Strawsonian physicalism.
Galen Strawson's "realistic physicalism" (or "realistic monism") entails panpsychism – or at least micropsychism. Strawson argues that "many—perhaps most—of those who call themselves physicalists or materialists mistakenly committed to the thesis that physical stuff is, in itself, in its fundamental nature, something wholly and utterly non-experiential... even when they are prepared to admit with Eddington that physical stuff has, in itself, ‘a nature capable of manifesting itself as mental activity’, i.e. as experience or consciousness". Because experiential phenomena allegedly cannot be emergent from wholly non-experiential phenomena, philosophers are driven to substance dualism, property dualism, eliminative materialism and "all other crazy attempts at wholesale mental-to-non-mental reduction".
External links.
Daniel Stoljar's SEP entry on Physicalism: http://plato.stanford.edu/entries/physicalism/

</doc>
<doc id="23482" url="https://en.wikipedia.org/wiki?curid=23482" title="Parallelism">
Parallelism

Parallelism may refer to:

</doc>
<doc id="23483" url="https://en.wikipedia.org/wiki?curid=23483" title="Philosophy of perception">
Philosophy of perception

The philosophy of perception is concerned with the nature of perceptual experience and the status of perceptual data, in particular how they relate to beliefs about, or knowledge of, the world. Any explicit account of perception requires a commitment to one of a variety of ontological or metaphysical views. Philosophers distinguish internalist accounts, which assume that perceptions of objects, and knowledge or beliefs about them, are aspects of an individual's mind, and externalist accounts, which state that they constitute real aspects of the world external to the individual. The position of naïve realism—the 'everyday' impression of physical objects constituting what is perceived—is to some extent contradicted by the occurrence of perceptual illusions and hallucinations and the relativity of perceptual experience as well as certain insights in science. Realist conceptions include phenomenalism and direct and indirect realism. Anti-realist conceptions include idealism and skepticism.
Categories of perception.
We may categorize perception as "internal" or "external".
The philosophy of perception is mainly concerned with exteroception.
Scientific accounts of perception.
. An object at some distance from an observer will reflect light from the sun (or from an artificial source) in all directions, some of which will fall upon the corneae of the eyes where it will be focussed upon each retina, forming an image. The disparity between the electrical output of these two slightly different images is resolved either at the level of the lateral geniculate nucleus or in a part of the visual cortex called 'V1'. The resolved data is further processed in the visual cortex where some areas have specialised functions, for instance area V5 is involved in the modelling of motion and V4 in adding colour. The resulting single image that subjects report as their experience is called a 'percept'. Studies involving rapidly changing scenes show the percept derives from numerous processes that involve time delays. show that dreams, imaginings and perceptions of things such as faces are accompanied by activity in many of the same areas of brain as are involved with physical sight. Imagery that originates from the senses and internally generated imagery may have a shared ontology at higher levels of cortical processing.
Sound is analyzed in term of pressure waves sensed by the cochlea in the ear. Data from the eyes and ears is combined to form a 'bound' percept. The problem of how this is produced, known as the binding problem, .
Perception is analyzed as a cognitive process in which information processing is used to transfer information into the mind where it is related to other information. Some psychologists propose that this processing gives rise to particular mental states (cognitivism) whilst others envisage a direct path back into the external world in the form of action (radical behaviourism). Behaviourists such as John B. Watson and B.F. Skinner have proposed that perception acts largely as a process between a stimulus and a response but have noted that Gilbert Ryle's "ghost in the machine of the brain" still seems to exist. "The objection to inner states is not that they do not exist, but that they are not relevant in a functional analysis". This view, in which experience is thought to be an incidental by-product of information processing, is known as epiphenomenalism.
Contrary to the behaviouralist approach to understanding the elements of cognitive processes, gestalt psychology sought to understand their organization as a whole, studying perception as a process of figure and ground.
Philosophical accounts of perception.
Important philosophical problems derive from the epistemology of perception—how we can gain knowledge via perception—such as the question of the nature of qualia. Within the biological study of perception naive realism is unusable. However, outside biology modified forms of naive realism are defended. Thomas Reid, the eighteenth-century founder of the Scottish School of Common Sense, formulated the idea that sensation was composed of a set of data transfers but also declared that there is still a direct connection between perception and the world. This idea, called direct realism, has again become popular in recent years with the rise of postmodernism.
The succession of data transfers involved in perception suggests that sense data are somehow available to a perceiving subject that is the substrate of the percept. Indirect realism, the view held by John Locke and Nicolas Malebranche, proposes that we can only be aware of mental representations of objects. however this may imply an infinite regress (a perceiver within a perceiver within a perceiver...), though a finite regress is perfectly possible. It also assumes that perception is entirely due to data transfer and information processing, an argument that can be avoided by proposing that the percept does not depend wholly upon the transfer and rearrangement of data. This still involves basic ontological issues of the sort raised by Leibniz Locke, Hume, Whitehead and others, which remain outstanding particularly in relation to the binding problem, the question of how different perceptions (e.g. color and contour in vision) are "bound" to the same object when they are processed by separate areas of the brain.
Indirect realism (representational views) provides an account of issues such as perceptual contents, qualia, dreams, imaginings, hallucinations, illusions, the resolution of binocular rivalry, the resolution of multistable perception, the modelling of motion that allows us to watch TV, the sensations that result from direct brain stimulation, the update of the mental image by saccades of the eyes and the referral of events backwards in time. Direct realists must either argue that these experiences do not occur or else refuse to define them as perceptions.
Idealism holds that reality is limited to mental qualities while skepticism challenges our ability to know anything outside our minds. One of the most influential proponents of idealism was George Berkeley who maintained that everything was mind or dependent upon mind. Berkeley's idealism has two main strands, phenomenalism in which physical events are viewed as a special kind of mental event and subjective idealism. David Hume is probably the most influential proponent of skepticism.
A fourth theory of perception in opposition to naive realism, enactivism, attempts to find a middle path between direct realist and indirect realist theories, positing that cognition arises as a result of the dynamic interplay between an organism's sensory-motor capabilities and its environment. Instead of seeing perception as a passive process determined entirely by the features of an independently existing world, enactivism suggests that organism and environment are structurally coupled and co-determining. The theory was first formalized by Francisco Varela, Evan Thompson, and Eleanor Rosch in "The Embodied Mind".
Spatial representation.
An aspect of perception that is common to both realists and anti-realists is the idea of mental or perceptual space. David Hume concluded that things appear extended because they have attributes of colour and solidity. A popular modern philosophical view is that the brain cannot contain images so our sense of space must be due to the actual space occupied by physical things. However, as René Descartes noticed, perceptual space has a projective geometry, things within it appear as if they are viewed from a point. The phenomenon of perspective was closely studied by artists and architects in the Renaissance, who relied mainly on the 11th century polymath, Alhazen (Ibn al-Haytham), who affirmed the visibility of perceptual space in geometric structuring projections. Mathematicians now know of many types of projective geometry such as complex Minkowski space that might describe the layout of things in perception (see Peters (2000)) and it has also emerged that parts of the brain contain patterns of electrical activity that correspond closely to the layout of the retinal image (this is known as retinotopy). How or whether these become conscious experience is still unknown (see McGinn (1995)).

</doc>
<doc id="23484" url="https://en.wikipedia.org/wiki?curid=23484" title="Proper name (philosophy)">
Proper name (philosophy)

In the philosophy of language a proper name, for example the names of persons or places, is a name which is ordinarily taken to uniquely identify its referent in the world. As such it presents particular challenges for theories of meaning and it has become a central problem in analytical philosophy. The common sense view was originally formulated by John Stuart Mill in "A System of Logic" where he defines it as "a word that answers the purpose of showing what thing it is that we are talking about but not of telling anything about it". This view was criticized when philosophers applied principles of formal logic to linguistic propositions. Gottlob Frege pointed out that proper names may apply to imaginary and inexistent entities without becoming meaningless, and he showed that sometimes more than one proper name may identify the same entity without having the same "sense", so that the phrase "Homer believed the morning star was the evening star" could be meaningful and not tautological in spite of the fact that the morning star and the evening star identifies the same referent. This example became known as Frege's Puzzle and is a central issue in the theory of proper names.
Bertrand Russell was the first to propose a Descriptivist theory of names, which held that a proper name refers not to a referent, but to a set of true propositions that uniquely describe a referent - for example "Aristotle" refers to "the teacher of Alexander the Great". Rejecting descriptivism Saul Kripke and Keith Donnellan instead advanced causal-historical theories of reference which hold that names come to be associated with individual referents because social groups who links the name to its reference in a naming event (e.g. a baptism) which henceforth fixes the value of the name to the specific referent within that community. Today a direct reference theory is common, which holds that proper names refer to their referents without attributing any additional information, connotative or of sense, about them.
The problem.
The problem of proper names arise within a theory of meaning that is based on truth values and propositional logic, when trying to ascertain the criteria with which to determine if propositions that include proper names are true or false.
For example in the proposition "Cicero is Roman" it is unclear what semantic content the proper name "Cicero" provides to the proposition. One may intuitively assume that the name refers to a person who may or may not be Roman, and that the truth value depends on whether that is the case or not. But from the point of view of a theory of meaning the question is "how" the word Cicero establishes its referent.
Another problem known as Frege's Puzzle, asks why it can be the case that the two names can refer to the same referent, yet not necessarily be considered entirely synonymous. His example is that the proposition "Hesperus is Hesperus" (Hesperus being the Greek name of the morning star) is tautological and vacuous while the proposition "Hesperus is Phosphorus" (Phosphorus being the Greek name of the evening star) conveys information. This puzzle suggests that there is something more to the meaning of the proper name than simply pointing out its referent.
Theories.
Many theories have been proposed about proper names, each attempting to solve the problems of reference and identity inherent in the concept.
Millian theory.
John Stuart Mill distinguished between connotative and denotative meaning, and argued that proper names included no other semantic content to a proposition than identifying the referent of the name and were hence purely denotative. Some contemporary proponents of a Millian theory of naming argue that the process through which something becomes a proper name is exactly the gradual loss of connotation for pure denotation - such as the process that turned the descriptive propositions "long island" into the proper name Long Island.
Sense based theory of names.
Frege argued that one had to distinguish between the sense ("Sinn") and the reference of the name. And that different names for the same entity might identify the same referent without being formally synonymous. For example although the Morning star and the evening star is the same astronomical object, the proposition "the morning star is the evening star" is not a tautology but provides actual information to someone who did not know this. Hence to Frege the two names for the object must have a different sense. Philosophers such as John McDowell have elaborated on Frege's theory of proper names.
Descriptive theory.
The "descriptive" theory of proper names is the view that the meaning of a given use of a proper name is a set of properties that can be expressed as a description that picks out an object that satisfies the description. 
Bertrand Russell espoused such a view arguing that the name refers to a description, and that description, like a definition, "picks out" the bearer of the name. The description then functions as an abbreviation or a truncated form of the description. The distinction between the embedded description and the bearer itself is similar to that between the "extension" and the "intension" (Frege's terms) of a general term, or between connotation and denotation (Mill's terms).
John Searle elaborated Russell's theory suggesting that the proper name refers to a cluster of propositions that in combination pick out a unique referent. This was meant to deal with the objection by some critics of Russell's theory that a descriptive theory of meaning would make the referent of a name dependent on the knowledge that the person saying the name has about the referent.
In 1973 Tyler Burge proposed a metalinguistic descriptivist theory of proper names which holds that names have the meaning that corresponds to the description of the individual entities to whom the name is applied. This however opens up for the possibility that names are not proper, when for example more than one person shares the same name. This leads Burge to argue that plural usages of names, such as "all the Alfreds I know have red hair", support this view.
Causal theory of names.
The causal-historical theory originated by Saul Kripke in "Naming and Necessity", building on work by among others Keith Donnellan, combines the referential view with the idea that the name's referent is fixed by a baptismal act, whereupon the name becomes a rigid designator of the referent. Kripke did not emphasize causality, but rather the historical relation between the naming event and the community of speakers within which it circulates, but in spite of this the theory is often called "a causal theory of naming".
The pragmatic naming theory of Charles Sanders Peirce is sometimes considered a precursor of causal-historical naming theory. He described proper names in the following terms: "A proper name, when one meets with it for the first time, is existentially connected with some percept or other equivalent individual knowledge of the individual it names. It is then, and then only, a genuine Index. The next time one meets with it, one regards it as an Icon of that Index. The habitual acquaintance with it having been acquired, it becomes a Symbol whose Interpretant represents it as an Icon of an Index of the Individual named." Here he notes out that the baptismal event takes place for each person when a proper name is first associated with a referent (for example by pointing and saying "this is John", establishing an indexical relation between the name and the person) who is henceforth considered to be a conventional ("symbolic" in Peircean terms) references to the referent.
Direct reference theories.
Rejecting sense-based, descriptivist and causal-historical theories of naming, theories of direct reference hold that names together with demonstratives are a class of words that refer directly to their referent.
In the "Tractatus Logico Philosophicus" Ludwig Wittgenstein also held a direct reference position, arguing that names refer to a particular directly, and that this referent is its only meaning. In his later work however he has been attributed a cluster-descriptivist position based on the idea of family resemblances (for example by Kripke), although it has been argued that this misconstrues Wittgenstein's argument. Particularly his later view has been compared to that of Kripke's own view which recognizes names as stemming from a social convention and pragmatic principles of understanding others utterances.
Direct reference theory is similar to Mill's theory in that it proposes that the only meaning of a proper name is its referent. Modern proposals such as those by David Kaplan, which distinguish between Fregean and non-Fregean terms, the former which have both sense and reference and the latter which include proper names and have only reference.
Continental philosophy.
Outside of the analytic tradition few continental philosophers have approached the proper name as a philosophical problem. In "Of Grammatology" Jacques Derrida specifically refutes the idea that proper names stand outside of the social construct of language as a binary relation between referent and sign. Rather he argues, the proper name as all words are caught up in a context of social, spatial and temporal differences that make it meaningful. He also notes that there are subjective elements of meaning in proper names, since they connect the bearer of a name with the sign of their own identity.

</doc>
<doc id="23485" url="https://en.wikipedia.org/wiki?curid=23485" title="Prolog">
Prolog

Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.
Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is declarative: the program logic is expressed in terms of relations, represented as facts and rules. A computation is initiated by running a "query" over these relations.
The language was first conceived by a group around Alain Colmerauer in Marseille, France, in the early 1970s and the first Prolog system was developed in 1972 by Colmerauer with Philippe Roussel.
Prolog was one of the first logic programming languages, and remains the most popular among such languages today, with several free and commercial implementations available. The language has been used for theorem proving, expert systems, as well as its original intended field of use, natural language processing. Modern Prolog environments support creating graphical user interfaces, as well as administrative and networked applications.
Prolog is well-suited for specific tasks that benefit from rule-based logical queries such as searching databases, voice control systems, and filling templates.
Syntax and semantics.
In Prolog, program logic is expressed in terms of relations, and a computation is initiated by running a "query" over these relations. Relations and queries are constructed using Prolog's single data type, the "term". Relations are defined by "clauses". Given a query, the Prolog engine attempts to find a resolution refutation of the negated query. If the negated query can be refuted, i.e., an instantiation for all free variables is found that makes the union of clauses and the singleton set consisting of the negated query false, it follows that the original query, with the found instantiation applied, is a logical consequence of the program. This makes Prolog (and other logic programming languages) particularly useful for database, symbolic mathematics, and language parsing applications. Because Prolog allows impure predicates, checking the truth value of certain special predicates may have some deliberate side effect, such as printing a value to the screen. Because of this, the programmer is permitted to use some amount of conventional imperative programming when the logical paradigm is inconvenient. It has a purely logical subset, called "pure Prolog", as well as a number of extralogical features.
Data types.
Prolog's single data type is the "term". Terms are either "atoms", "numbers", "variables" or "compound terms".
Special cases of compound terms:
Rules and facts.
Prolog programs describe relations, defined by means of clauses. Pure Prolog is restricted to Horn clauses. There are two types of clauses: facts and rules. A rule is of the form
<syntaxhighlight lang="prolog">Head :- Body.</syntaxhighlight>
and is read as "Head is true if Body is true". A rule's body consists of calls to predicates, which are called the rule's goals. The built-in predicate codice_10 (meaning a 2-arity operator with name codice_11) denotes conjunction of goals, and codice_12 denotes disjunction. Conjunctions and disjunctions can only appear in the body, not in the head of a rule.
Clauses with empty bodies are called facts. An example of a fact is:
<syntaxhighlight lang="prolog">cat(tom).</syntaxhighlight>
which is equivalent to the rule:
<syntaxhighlight lang="prolog">cat(tom) :- true.</syntaxhighlight>
The built-in predicate codice_13 is always true.
Given the above fact, one can ask:
"is tom a cat?"
<syntaxhighlight lang="prolog">
</syntaxhighlight>
"what things are cats?"
<syntaxhighlight lang="prolog">
</syntaxhighlight>
Clauses with bodies are called rules. An example of a rule is:
<syntaxhighlight lang="prolog">animal(X) :- cat(X).</syntaxhighlight>
If we add that rule and ask "what things are animals?"
<syntaxhighlight lang="prolog">
</syntaxhighlight>
Due to the relational nature of many built-in predicates, they can typically be used in several directions. For example, codice_14 can be used to determine the length of a list (codice_15, given a list codice_16) as well as to generate a list skeleton of a given length (codice_17), and also to generate both list skeletons and their lengths together (codice_18). Similarly, codice_19 can be used both to append two lists (codice_20 given lists codice_21 and codice_22) as well as to split a given list into parts (codice_23, given a list codice_16). For this reason, a comparatively small set of library predicates suffices for many Prolog programs.
As a general purpose language, Prolog also provides various built-in predicates to perform routine activities like input/output, using graphics and otherwise communicating with the operating system. These predicates are not given a relational meaning and are only useful for the side-effects they exhibit on the system. For example, the predicate codice_25 displays a term on the screen.
Execution.
Execution of a Prolog program is initiated by the user's posting of a single goal, called the query. Logically, the Prolog engine tries to find a resolution refutation of the negated query. The resolution method used by Prolog is called SLD resolution. If the negated query can be refuted, it follows that the query, with the appropriate variable bindings in place, is a logical consequence of the program. In that case, all generated variable bindings are reported to the user, and the query is said to have succeeded. Operationally, Prolog's execution strategy can be thought of as a generalization of function calls in other languages, one difference being that multiple clause heads can match a given call. In that case, the system creates a choice-point, unifies the goal with the clause head of the first alternative, and continues with the goals of that first alternative. If any goal fails in the course of executing the program, all variable bindings that were made since the most recent choice-point was created are undone, and execution continues with the next alternative of that choice-point. This execution strategy is called chronological backtracking. For example:
This results in the following query being evaluated as true:
<syntaxhighlight lang="prolog">
</syntaxhighlight>
This is obtained as follows: Initially, the only matching clause-head for the query codice_26 is the first one, so proving the query is equivalent to proving the body of that clause with the appropriate variable bindings in place, i.e., the conjunction codice_27. The next goal to be proved is the leftmost one of this conjunction, i.e., codice_28. Two clause heads match this goal. The system creates a choice-point and tries the first alternative, whose body is codice_29. This goal can be proved using the fact codice_30, so the binding codice_31 is generated, and the next goal to be proved is the second part of the above conjunction: codice_32. Again, this can be proved by the corresponding fact. Since all goals could be proved, the query succeeds. Since the query contained no variables, no bindings are reported to the user. A query with variables, like:
<syntaxhighlight lang="prolog">?- father_child(Father, Child).</syntaxhighlight>
enumerates all valid answers on backtracking.
Notice that with the code as stated above, the query codice_33 also succeeds. One would insert additional goals to describe the relevant restrictions, if desired.
Loops and recursion.
Iterative algorithms can be implemented by means of recursive predicates.
Negation.
The built-in Prolog predicate codice_34 provides negation as failure, which allows for non-monotonic reasoning. The goal codice_35 in the rule
<syntaxhighlight lang="prolog">legal(X) :- \+ illegal(X).</syntaxhighlight>
is evaluated as follows: Prolog attempts to prove the codice_36. If a proof for that goal can be found, the original goal (i.e., codice_35) fails. If no proof can be found, the original goal succeeds. Therefore, the codice_34 prefix operator is called the "not provable" operator, since the query codice_39 succeeds if Goal is not provable. This kind of negation is sound if its argument is "ground" (i.e. contains no variables). Soundness is lost if the argument contains variables and the proof procedure is complete. In particular, the query codice_40 can now not be used to enumerate all things that are legal.
Programming in Prolog.
In Prolog, loading code is referred to as "consulting". Prolog can be used interactively by entering queries at the Prolog prompt codice_41. If there is no solution, Prolog writes codice_42. If a solution exists then it is printed. If there are multiple solutions to the query, then these can be requested by entering a semi-colon codice_43. There are guidelines on good programming practice to improve code efficiency, readability and maintainability.
Here follow some example programs written in Prolog.
Hello world.
An example of a query:
Compiler optimization.
Any computation can be expressed declaratively as a sequence of state transitions. As an example, an optimizing compiler with three optimization passes could be implemented as a relation between an initial program and its optimized form:
or equivalently using DCG notation:
Quicksort.
The quicksort sorting algorithm, relating a list to its sorted version:
Design patterns.
A design pattern is a general reusable solution to a commonly occurring problem in software design. In Prolog, design patterns go under various names: skeletons and techniques, cliches, program schemata, and logic description schemata. An alternative to design patterns is higher order programming.
Higher-order programming.
A higher-order predicate is a predicate that takes one or more other predicates as arguments. Although support for higher-order programming takes Prolog outside the domain of first-order logic, which does not allow quantification over predicates, ISO Prolog now has some built-in higher-order predicates such as codice_44, codice_45, codice_46, codice_47, codice_48, and codice_49. Furthermore, since arbitrary Prolog goals can be constructed and evaluated at run-time, it is easy to write higher-order predicates like codice_50, which applies an arbitrary predicate to each member of a given list, and codice_51, which filters elements that satisfy a given predicate, also allowing for currying.
To convert solutions from temporal representation (answer substitutions on backtracking) to spatial representation (terms), Prolog has various all-solutions predicates that collect all answer substitutions of a given query in a list. This can be used for list comprehension. For example, perfect numbers equal the sum of their proper divisors:
This can be used to enumerate perfect numbers, and also to check whether a number is perfect.
As another example, the predicate codice_52 applies a predicate codice_53 to all corresponding positions in a pair of lists:
When codice_53 is a predicate that for all codice_55, codice_56 unifies codice_57 with a single unique value, codice_58 is equivalent to applying the map function in functional programming as codice_59.
Higher-order programming style in Prolog was pioneered in HiLog and λProlog.
Modules.
For programming in the large, Prolog provides a module system. The module system is standardised by ISO. However, not all Prolog compilers support modules, and there are compatibility problems between the module systems of the major Prolog compilers. Consequently, modules written on one Prolog compiler will not necessarily work on others.
Parsing.
There is a special notation called definite clause grammars (DCGs). A rule defined via codice_60 instead of codice_61 is expanded by the preprocessor (codice_62, a facility analogous to macros in other languages) according to a few straightforward rewriting rules, resulting in ordinary Prolog clauses. Most notably, the rewriting equips the predicate with two additional arguments, which can be used to implicitly thread state around, analogous to monads in other languages. DCGs are often used to write parsers or list generators, as they also provide a convenient interface to difference lists.
Meta-interpreters and reflection.
Prolog is a homoiconic language and provides many facilities for reflection. Its implicit execution strategy makes it possible to write a concise meta-circular evaluator (also called "meta-interpreter") for pure Prolog code:
<syntaxhighlight lang="prolog">
solve(true).
solve((Subgoal1,Subgoal2)) :- 
solve(Head) :- 
</syntaxhighlight>
where codice_63 represents an empty conjunction, and codice_64 unifies with clauses in the database of the form codice_65.
Since Prolog programs are themselves sequences of Prolog terms (codice_61 is an infix operator) that are easily read and inspected using built-in mechanisms (like codice_67), it is possible to write customized interpreters that augment Prolog with domain-specific features. For example, Sterling and Shapiro present a meta-interpreter that performs reasoning with uncertainty, reproduced here with slight modifications:
<syntaxhighlight lang="prolog">
solve(true, 1) :- !.
solve((Subgoal1,Subgoal2), Certainty) :-
solve(Goal, 1) :-
solve(Head, Certainty) :-
</syntaxhighlight>
This interpreter uses a table of built-in Prolog predicates of the form
<syntaxhighlight lang="prolog">
builtin(A is B).
builtin(read(X)).
% etc.
</syntaxhighlight>
and clauses represented as codice_68. Given those, it can be called as codice_69 to execute codice_70 and obtain a measure of certainty about the result.
Turing completeness.
Pure Prolog is based on a subset of first-order predicate logic, Horn clauses, which is Turing-complete. Turing completeness of Prolog can be shown by using it to simulate a Turing machine:
A simple example Turing machine is specified by the facts:
This machine performs incrementation by one of a number in unary encoding: It loops over any number of "1" cells and appends an additional "1" at the end. Example query and result:
This illustrates how any computation can be expressed declaratively as a sequence of state transitions, implemented in Prolog as a relation between successive states of interest.
Implementation.
ISO Prolog.
The ISO Prolog standard consists of two parts. ISO/IEC 13211-1, published in 1995, aims to standardize the existing practices of the many implementations of the core elements of Prolog. It has clarified aspects of the language that were previously ambiguous and leads to portable programs. There are two corrigenda: Cor.1:2007 and Cor.2:2012. ISO/IEC 13211-2, published in 2000, adds support for modules to the standard. The standard is maintained by the ISO/IEC JTC1/SC22/WG17 working group. ANSI X3J17 is the US Technical Advisory Group for the standard.
Compilation.
For efficiency, Prolog code is typically compiled to abstract machine code, often influenced by the register-based Warren Abstract Machine (WAM) instruction set. Some implementations employ abstract interpretation to derive type and mode information of predicates at compile time, or compile to real machine code for high performance. Devising efficient implementation methods for Prolog code is a field of active research in the logic programming community, and various other execution methods are employed in some implementations. These include clause binarization and stack-based virtual machines.
Tail recursion.
Prolog systems typically implement a well-known optimization method called tail call optimization (TCO) for deterministic predicates exhibiting tail recursion or, more generally, tail calls: A clause's stack frame is discarded before performing a call in a tail position. Therefore, deterministic tail-recursive predicates are executed with constant stack space, like loops in other languages.
Term indexing.
Finding clauses that are unifiable with a term in a query is linear in the number of clauses. Term indexing uses a data structure that enables sub-linear-time lookups. Indexing only affects program performance, it does not affect semantics. Most Prologs only use indexing on the first term, as indexing on all terms is expensive, but techniques based on "field-encoded words" or "superimposed codewords" provide fast indexing across the full query and head.
Hashing.
Some Prolog systems, such as LPA Prolog and SWI-Prolog, now implement hashing to help handle large datasets more efficiently. This tends to yield very large performance gains when working with large corpora such as WordNet.
Tabling.
Some Prolog systems, (BProlog, XSB Yap, B-Prolog and Ciao), implement a memoization method called "tabling", which frees the user from manually storing intermediate results.
Subgoals encountered in a query evaluation are maintained in a table, along with answers to these subgoals. If a subgoal is re-encountered, the evaluation reuses information from the table rather than re-performing resolution against program clauses.
Tabling is a space-time tradeoff; execution time can be reduced by using more memory to store intermediate results.
Implementation in hardware.
During the Fifth Generation Computer Systems project, there were attempts to implement Prolog in hardware with the aim of achieving faster execution with dedicated architectures. Furthermore, Prolog has a number of properties that may allow speed-up through parallel execution. A more recent approach has been to compile restricted Prolog programs to a field programmable gate array. However, rapid progress in general-purpose hardware has consistently overtaken more specialised architectures.
Limitations.
Although Prolog is widely used in research and education, Prolog and other logic programming languages have not had a significant impact on the computer industry in general. Most applications are small by industrial standards, with few exceeding 100,000 lines of code. Programming in the large is considered to be complicated because not all Prolog compilers support modules, and there are compatibility problems between the module systems of the major Prolog compilers. Portability of Prolog code across implementations has also been a problem, but developments since 2007 have meant: "the portability within the family of Edinburgh/Quintus derived Prolog implementations is good enough to allow for maintaining portable real-world applications."
Software developed in Prolog has been criticised for having a high performance penalty compared to conventional programming languages. In particular, Prolog's non-deterministic evaluation strategy can be problematic when programming deterministic computations, or when even using "don't care non-determinism" (where a single choice is made instead of backtracking over all possibilities). Cuts and other language constructs may have to be used to achieve desirable performance, destroying one of Prolog's main attractions, the ability to run programs "backwards and forwards".
Prolog is not purely declarative: because of constructs like the cut operator, a procedural reading of a Prolog program is needed to understand it. The order of clauses in a Prolog program is significant, as the execution strategy of the language depends on it. Other logic programming languages, such as Datalog, are truly declarative but restrict the language. As a result, many practical Prolog programs are written to conform to Prolog's depth-first search order, rather than as purely declarative logic programs.
Extensions.
Various implementations have been developed from Prolog to extend logic programming capabilities in numerous directions. These include types, modes, constraint logic programming (CLP), object-oriented logic programming (OOLP), concurrency, linear logic (LLP), functional and higher-order logic programming capabilities, plus interoperability with knowledge bases:
Types.
Prolog is an untyped language. Attempts to introduce types date back to the 1980s, and as of 2008 there are still attempts to extend Prolog with types. Type information is useful not only for type safety but also for reasoning about Prolog programs.
Modes.
The syntax of Prolog does not specify which arguments of a predicate are inputs and which are outputs. However, this information is significant and it is recommended that it be included in the comments. Modes provide valuable information when reasoning about Prolog programs
Constraints.
Constraint logic programming extends Prolog to include concepts from constraint satisfaction. A constraint logic program allows constraints in the body of clauses, such as: codice_71 It is suited to large-scale combinatorial optimisation problems. and is thus useful for applications in industrial settings, such as automated time-tabling and production scheduling. Most Prolog systems ship with at least one constraint solver for finite domains, and often also with solvers for other domains like rational numbers.
Object-orientation.
Flora-2 is an object-oriented knowledge representation and reasoning system based on F-logic and incorporates HiLog, Transaction logic, and defeasible reasoning.
Logtalk is an object-oriented logic programming language that can use most Prolog implementations as a back-end compiler. As a multi-paradigm language, it includes support for both prototypes and classes.
Oblog is a small, portable, object-oriented extension to Prolog by Margaret McDougall of EdCAAD, University of Edinburgh.
Objlog was a frame-based language combining objects and Prolog II from CNRS, Marseille, France.
Prolog++ was developed by Logic Programming Associates and first released in 1989 for MS-DOS PCs. Support for other platforms was added, and a second version was released in 1995. A book about Prolog++ by Chris Moss was published by Addison-Wesley in 1994.
Graphics.
Prolog systems that provide a graphics library are SWI-prolog, Visual-prolog, LPA Prolog for Windows and B-Prolog.
Concurrency.
Prolog-MPI is an open-source SWI-Prolog extension for distributed computing over the Message Passing Interface. Also there are various concurrent Prolog programming languages.
Web programming.
Some Prolog implementations, notably SWI-Prolog and Ciao, support server-side web programming with support for web protocols, HTML and XML. There are also extensions to support semantic web formats such as RDF and OWL. Prolog has also been suggested as a client-side language.
Adobe Flash.
Cedar is a free and basic Prolog interpreter. From version 4 and above Cedar has a FCA (Flash Cedar App) support. This provides a new platform to programming in Prolog through ActionScript.
Interfaces to other languages.
Frameworks exist which can bridge between Prolog and other languages:
History.
The name "Prolog" was chosen by Philippe Roussel as an abbreviation for "" (French for "programming in logic"). It was created around 1972 by Alain Colmerauer with Philippe Roussel, based on Robert Kowalski's procedural interpretation of Horn clauses. It was motivated in part by the desire to reconcile the use of logic as a declarative knowledge representation language with the procedural representation of knowledge that was popular in North America in the late 1960s and early 1970s. According to Robert Kowalski, the first Prolog system was developed in 1972 by Colmerauer and Phillipe Roussel. The first implementations of Prolog were interpreters. However, David H. D. Warren created the Warren Abstract Machine, an early and influential Prolog compiler which came to define the "Edinburgh Prolog" dialect which served as the basis for the syntax of most modern implementations.
European AI researchers favored Prolog while Americans favored Lisp, reportedly causing many nationalistic debates on the merits of the languages. Much of the modern development of Prolog came from the impetus of the Fifth Generation Computer Systems project (FGCS), which developed a variant of Prolog named "Kernel Language" for its first operating system.
Pure Prolog was originally restricted to the use of a resolution theorem prover with Horn clauses of the form:
The application of the theorem-prover treats such clauses as procedures:
Pure Prolog was soon extended, however, to include negation as failure, in which negative conditions of the form not(Bi) are shown by trying and failing to solve the corresponding positive conditions Bi.
Subsequent extensions of Prolog by the original team introduced constraint logic programming abilities into the implementations.
Use in industry.
Prolog has been used in Watson. Watson uses IBM's DeepQA software and the Apache UIMA (Unstructured Information Management Architecture) framework. The system was written in various languages, including Java, C++, and Prolog, and runs on the SUSE Linux Enterprise Server 11 operating system using Apache Hadoop framework to provide distributed computing. Prolog is used for pattern matching over natural language parse trees. The developers have stated: "We required a language in which we could conveniently express pattern matching rules over the parse trees and other annotations (such as named entity recognition results), and a technology that could execute these rules very efficiently. We found that Prolog was the ideal choice for the language due to its simplicity and expressiveness."

</doc>
<doc id="23486" url="https://en.wikipedia.org/wiki?curid=23486" title="Phil Zimmermann">
Phil Zimmermann

Philip R. "Phil" Zimmermann, Jr. (born February 12, 1954) is the creator of Pretty Good Privacy (PGP), the most widely used email encryption software in the world. He is also known for his work in VoIP encryption protocols, notably ZRTP and Zfone. Zimmermann is co-founder and Chief Scientist of the global encrypted communications firm, Silent Circle.
Background.
He was born in Camden, New Jersey. His father was a concrete mixer truck driver. Zimmermann received a B.S. degree in computer science from Florida Atlantic University in Boca Raton, Florida in 1978, and thereafter moved to the San Francisco Bay Area. In the 1980s, Zimmermann worked in Boulder, Colorado as a software engineer and was a part of the Nuclear Weapons Freeze Campaign as a military policy analyst.
PGP.
In 1991, he wrote the popular Pretty Good Privacy (PGP) program, and made it available (together with its source code) through public FTP for download, the first widely available program implementing public-key cryptography. Shortly thereafter, it became available overseas via the Internet, though Zimmermann has said he had no part in its distribution outside the United States.
The very first version of PGP included an encryption algorithm, BassOmatic, developed by Zimmermann.
Arms Export Control Act investigation.
After a report from RSA Security, who were in a licensing dispute with regard to the use of the RSA algorithm in PGP, the United States Customs Service started a criminal investigation of Zimmermann, for allegedly violating the Arms Export Control Act. The United States Government had long regarded cryptographic software as a munition, and thus subject to arms trafficking export controls. At that time, the boundary between what cryptography was permitted ("low-strength") and impermissible ("high-strength") for export from the United States was placed such that PGP fell on the too-strong-to-export side of the boundary. The boundary for legal export has since been raised and now allows PGP to be exported. The investigation lasted three years, but was finally dropped without filing charges.
After the government dropped its case without indictment in early 1996, Zimmermann founded PGP Inc. and released an updated version of PGP and some additional related products. That company was acquired by Network Associates (NAI) in December 1997, and Zimmermann stayed on for three years as a Senior Fellow. NAI decided to drop the product line and in 2002, PGP was acquired from NAI by a new company called PGP Corporation. Zimmermann served as a special advisor and consultant to that firm until Symantec acquired PGP Corporation in 2010. Zimmermann is also a fellow at the Stanford Law School's Center for Internet and Society. He was a principal designer of the cryptographic key agreement protocol (the "association model") for the Wireless USB standard.
Along with Mike Janke, in 2012 he created Silent Circle.
Zimmermann's Law.
In 2013, an article on "Zimmermann's Law" quoted Phil Zimmermann as saying "The natural flow of technology tends to move in the direction of making surveillance easier", and "the ability of computers to track us doubles every eighteen months."
Awards and other recognition.
Zimmermann has received numerous technical and humanitarian awards for his pioneering work in cryptography:
Simon Singh's "The Code Book" devotes an entire chapter to Zimmermann and PGP.

</doc>
<doc id="23490" url="https://en.wikipedia.org/wiki?curid=23490" title="Political spectrum">
Political spectrum

A political spectrum is a system of classifying different political positions upon one or more geometric axes that symbolize independent political dimensions.
Most long-standing spectra include a right wing and left wing, which originally referred to seating arrangements in the French parliament after the Revolution (1789–99). According to the simplest left–right axis, communism and socialism are usually regarded internationally as being on the left, opposite fascism and conservatism on the right. Liberalism can mean different things in different contexts, sometimes on the left (social liberalism), sometimes on the right (economic liberalism). Politics that rejects the conventional left–right spectrum is known as syncretic politics. Those with an intermediate outlook are classified as centrists or moderates.
Political scientists have frequently noted that a single left–right axis is insufficient for describing the existing variation in political beliefs, and often include other axes. Though the descriptive words at polar opposites may vary, often in popular biaxial spectra the axes are split between sociocultural issues and economic issues, each scaling from some form of individualism (or government for the freedom of the individual) to some form of communitarianism (or government for the welfare of the community). In this context, the contemporary American left is often considered individualist (or libertarian) on sociocultural issues and communitarian (or populist) on economic issues, while the contemporary American right is often considered communitarian (or populist) on sociocultural issues and individualist (or libertarian) on economic issues.
Historical origin of the terms.
The terms "Right" and "Left" refer to political affiliations originating early in the French Revolutionary era of 1789–1799, and referred originally to the seating arrangements in the various legislative bodies of France. As seen from the Speaker's seat at the front of the Assembly, the aristocracy sat on the right (traditionally the seat of honor) and the commoners sat on the left, hence the terms Right-wing politics and Left-wing politics.
Originally, the defining point on the ideological spectrum was the "Ancien Régime" ("old order"). "The Right" thus implied support for aristocratic or royal interests, and the church, while "The Left" implied support for republicanism, secularism, and civil liberties. Because the political franchise at the start of the revolution was relatively narrow, the original "Left" represented mainly the interests of the bourgeoisie, the rising capitalist class (with notable exceptions such as the proto-communist Gracchus Babeuf). Support for laissez-faire commerce and free markets were expressed by politicians sitting on the left, because these represented policies favorable to capitalists rather than to the aristocracy; but outside of parliamentary politics, these views are often characterized as being on the Right.
The reason for this apparent contradiction lies in the fact that those "to the left" of the parliamentary left, outside of official parliamentary structures (such as the "sans-culottes" of the French Revolution), typically represent much of the working class, poor peasantry, and the unemployed. Their political interests in the French Revolution lay with opposition to the aristocracy, and so they found themselves allied with the early capitalists. However, this did not mean that their economic interests lay with the "laissez-faire" policies of those representing them politically.
As capitalist economies developed, the aristocracy became less relevant and were mostly replaced by capitalist representatives. The size of the working class increased as capitalism expanded, and began to find expression partly through trade unionist, socialist, anarchist, and communist politics, rather than being confined to the capitalist policies expressed by the original "left". This evolution has often pulled parliamentary politicians away from laissez-faire economic policies, although this has happened to different degrees in different countries.
Thus, the word "left" in American political parlance may refer to "liberalism" and be identified with the Democratic Party, whereas in a country such as France these positions would be regarded as relatively more right-wing, and "left" is more likely to refer to "socialist" positions rather than "liberal" ones.
Academic investigation.
For almost a century, social scientists have considered the problem of how best to describe political variation.
Leonard W. Ferguson.
In 1950, Leonard W. Ferguson analyzed political values using ten scales measuring attitudes toward: birth control, capital punishment, censorship, communism, evolution, law, patriotism, theism, treatment of criminals, and war. Submitting the results to factor analysis, he was able to identify three factors, which he named "Religionism", "Humanitarianism", and "Nationalism". He defined "Religionism" as belief in God and negative attitudes toward evolution and birth control; "Humanitarianism" as being related to attitudes opposing war, capital punishment and harsh treatment of criminals; and "Nationalism" as describing variation in opinions on censorship, law, patriotism and communism.
This system was derived empirically; rather than devising a political model on purely theoretical grounds and testing it, Ferguson's research was exploratory. As a result of this method, care must be taken in the interpretation of Ferguson's three factors, as factor analysis will output an abstract 'factor' whether an objectively real factor exists or not. Although replication of the "Nationalism" factor was inconsistent, the finding of "Religionism" and "Humanitarianism" had a number of replications by Ferguson and others.
Hans Eysenck.
Shortly afterward, Hans Eysenck began researching political attitudes in Great Britain. He believed that there was something essentially similar about the National Socialists (Nazis) on the one hand, and the Communists on the other, despite their opposite positions on the left–right axis. As Hans Eysenck described in his 1956 book "Sense and Nonsense in Psychology", Eysenck compiled a list of political statements found in newspapers and political tracts and asked subjects to rate their agreement or disagreement with each. Submitting this value questionnaire to the same process of factor analysis used by Ferguson, Eysenck drew out two factors, which he named "Radicalism" (R-factor) and "Tender-Mindedess" (T-factor).
Such analysis produces a factor whether or not it corresponds to a real-world phenomenon, and so caution must be exercised in its interpretation. While Eysenck's R-factor is easily identified as the classical "left–right" dimension, the T-factor (representing a factor drawn at right angles to the R-factor) is less intuitive; high-scorers favored pacifism, racial equality, religious education, and restrictions on abortion, while low-scorers had attitudes more friendly to militarism, harsh punishment, easier divorce laws, and companionate marriage.
Despite the difference in methodology, location, and theory, the results attained by Eysenck and Ferguson matched; simply rotating Eysenck's two factors 45 degrees renders the same factors of "Religionism" and "Humanitarianism" identified by Ferguson in America.
Eysenck's dimensions of R and T were found by factor analyses of values in Germany and Sweden, France, and Japan.
One interesting result Eysenck noted in his 1956 work was that in the United States and Great Britain, most of the political variance was subsumed by the left/right axis, while in France, the T-axis was larger, and in the Middle East, the only dimension to be found was the T-axis: "Among mid-Eastern Arabs it has been found that while the tough-minded/tender-minded dimension is still clearly expressed in the relationships observed between different attitudes, there is nothing that corresponds to the radical-conservative continuum."
Relationship between Eysenck's political views and political research.
Eysenck's political views related to his research: Eysenck was an outspoken opponent of what he perceived as the authoritarian abuses of the left and right, and accordingly he believed that, with this T axis, he had found the link between nazism and communism. According to Eysenck, members of both ideologies were tough-minded. Central to Eysenck's thesis was the claim that tender-minded ideologies were democratic and friendly to human freedoms, while tough-minded ideologies were aggressive and authoritarian, a claim that is open to political criticism. In this context, Eysenck carried out studies on nazism and communist groups, claiming to find members of both groups to be more "dominant" and more "aggressive" than control groups.
Eysenck left Nazi Germany to live in Britain, and was not shy in attacking Stalinist 'communism' (which he regarded as representative of communist ideology), noting the anti-Semitic prejudices of the Russian government, the luxurious lifestyles of the USSR's leaders despite their talk about equality and the poverty of their people, and the Orwellian "doublethink" of East Germany's naming itself the "German Democratic Republic" despite being "one of the most undemocratic regimes in the world today." While Eysenck was an opponent of Nazism, his relationship with fascist organizations was more complex. Eysenck himself lent theoretical support to the English National Party (which also opposed "Hitlerite" Nazism), and was interviewed in the first issue of their journal "The Beacon" in relation to his controversial views on relative intelligence between different races. At one point during the interview, Eysenck was asked whether or not he was of Jewish origin before the interviewer proceeded. His political allegiances were called into question by other researchers, notably Steven Rose, who alleged that his scientific research was used for political purposes.
Subsequent criticism of Eysenck's research.
Eysenck's conception of 'tough-mindedness' has been criticized for a number of reasons.
Milton Rokeach.
Dissatisfied with Hans J. Eysenck's work, Milton Rokeach developed his own two-axis model of political values in 1973, basing this on the ideas of "freedom" and "equality", which he described in his book, "The Nature of Human Values."
Milton Rokeach claimed that the defining difference between the left and right was that the left stressed the importance of equality more than the right. Despite his criticisms of Eysenck's tough-tender axis, Rokeach also postulated a basic similarity between communism and nazism, claiming that these groups would not value freedom as greatly as more conventional social democrats, democratic socialists and capitalists would, and he wrote that "the two value model presented here most resembles Eysenck's hypothesis."
To test this model, Milton Rokeach and his colleagues used content analysis on works exemplifying nazism (written by Adolf Hitler), communism (written by V.I. Lenin), capitalism (by Barry Goldwater) and socialism (written by various socialist authors). This method has been criticized for its reliance on the experimenter's familiarity with the content under analysis, and its dependence on the researcher's particular political outlooks.
Multiple raters made frequency counts of sentences containing synonyms for a number of values identified by Rokeach, including freedom and equality, and Rokeach analyzed these results by comparing the relative frequency rankings of all the values for each of the four texts:
"In excerpts from..."
Later studies using samples of American ideologues and American presidential inaugural addresses attempted to apply this model.
Later research.
In further research, Hans J. Eysenck refined his methodology to include more questions on economic issues. Doing this, he revealed a split in the left–right axis between social policy and economic policy, with a previously undiscovered dimension of "socialism-capitalism" (S-factor).
While factorially distinct from Eysenck's previous R factor, the S-factor did positively correlate with the R-factor, indicating that a basic left–right or right–left tendency underlies both social values and economic values, although S tapped more into items discussing economic inequality and big business, while R relates more to the treatment of criminals, and to sexual issues and military issues.
Most research and political theory since this time has replicated the factors shown above.
Another replication came from Ronald Inglehart's research into national opinions based on the World Values Survey, although Inglehart's research described the values of "countries" rather than individuals or groups of individuals within nations. Inglehart's two-factor solution took the form of Ferguson's original Religionism and Humanitarianism dimensions; Inglehart labelled them "secularism–traditionalism", which covered issues of tradition and religion, like patriotism, abortion, euthanasia and the importance of obeying the law and authority figures, and "survivalism – self expression", which measured issues like everyday conduct and dress, acceptance of diversity (including foreigners) and innovation, and attitudes towards people with specific controversial lifestyles such as homosexuality and vegetarianism, as well as willingness to engage in political activism. See for Inglehart's national chart.
Other proposed dimensions.
Numerous alternatives exist, usually developed by those that feel their views are not fairly represented on the traditional right-left spectrum.
One alternative spectrum offered by the conservative "American Federalist Journal" accounts for only the "Degree of Government Control" without consideration for any other social or political variable, and thus places "Fascism" (totalitarianism) at one extreme and "Anarchism" (no government at all) at the other extreme.
In 1998, political author Virginia Postrel, in her book "The Future and Its Enemies", offered another single-axis spectrum that measures views of the future, contrasting "stasists", who allegedly fear the future and wish to control it, and "dynamists", who want the future to unfold naturally and without attempts to plan and control. The distinction corresponds to the utopian versus dystopian spectrum used in some theoretical assessments of liberalism, and the book's title is borrowed from the work of the anti-utopian classic-liberal theorist Karl Popper.
Other proposed axes include:
Other multi-axis models.
Nolan: economic freedom, personal freedom.
The Nolan chart was created by libertarian David Nolan. This chart shows what he considers as "economic freedom" (issues like taxation, free trade and free enterprise) on the horizontal axis and what he considers as "personal freedom" (issues like drug legalization, abortion and the draft) on the vertical axis. This puts left-wingers in the left quadrant, libertarians in the top, right-wingers in the right, and what Nolan originally named populists in the bottom. It is possible to consider the Nolan chart to be an Eysenck model that has been rotated 45 degrees. The popular "diamond" presentation of the Nolan chart makes this particular comparison readily apparent.
The traditional left–right spectrum forms a diagonal across the Nolan chart, with communism and fascism both in the ultra-populist corner, an assignment hotly disputed by more liberal-minded communists who do not advocate state control over matters of personal freedom. There are some discrepancies among various forms of the model. In some, the bottom section is labeled with neutral, non-pejorative terms (such as 'communitarian') whereas others use emotional, loaded terms such as 'statist', 'authoritarian', or 'totalitarian'.
The Nolan chart has been reoriented and visually represented in many forms since David Nolan first created it, and has been the inspiration for an endless array of political self-quizzes, perhaps the most famous of these being the World's Smallest Political Quiz, which places one on the Diamond Chart with "statist" at the bottom. this quiz is being used in 150 schools. It can be found in at least a dozen popular textbooks that feature the Quiz as part of their enhanced digital content. In August 2000 Portrait of America did a telephone survey that was done using the same questions and scale. More recently, The Institute for Humane Studies has created Politopia, a similar quiz. The Institute found that most applicants fell into the lower, populist section.
Three axis variants of Nolan Chart.
There are two three-axis models based on the Nolan Chart. The Friesian Institute has suggested a model that combines the economic liberty and personal liberty axes with positive liberty, creating a cube. The Vosem Chart (from Russian восемь "vosem" 'eight') splits the economic axis of the Nolan chart into two axes, corporate economics (z-axis) and individual economics (y-axis), which combine with the civil liberty axis (x-axis) to form a cube. Max Barry's NationStates uses the Nolan chart to classify each nation, but adds political freedom along the z-axis.
Political compass.
The political compass has two axes. One represents economic issues as right-vs-left. The other represents issues of freedom, or social issues, as authoritarian-vs-libertarian. One can determine their position on the political compass through an online quiz by the same name.
Greenberg & Jonas: left–right, ideological rigidity.
In a 2003 "Psychological Bulletin" paper, Jeff Greenberg and Eva Jonas posit a model comprising the standard left–right axis and an axis representing ideological rigidity. For Greenberg and Jonas, ideological rigidity has "much in common with the related concepts of dogmatism and authoritarianism" and is characterized by "believing in strong leaders and submission, preferring one’s own in-group, ethnocentrism and nationalism, aggression against dissidents, and control with the help of police and military." Greenberg and Jonas posit that high ideological rigidity can be motivated by "particularly strong needs to reduce fear and uncertainty" and is a primary shared characteristic of "people who subscribe to any extreme government or ideology, whether it is right-wing or left-wing."
Pournelle: liberty-control, irrationalism-rationalism.
This very distinct two-axis model was created by Jerry Pournelle in 1963 for his doctoral dissertation in political science. The Pournelle chart has liberty on one axis, with those on the left seeking freedom from control or protections for social deviance and those on the right emphasizing state authority or protections for norm enforcement (farthest right being state worship, farthest left being the idea of a state as the "ultimate evil"). The other axis is rationalism, defined here as the belief in planned social progress, with those higher up believing that there are problems with society that can be rationally solved, and those lower down skeptical of such approaches.
Inglehart: traditionalist–secular, and self expressionist – survivalist.
In its 4 January 2003 issue, "The Economist" discussed a chart, proposed by Dr. Ronald Inglehart and supported by the World Values Survey (associated with the University of Michigan), to plot cultural ideology onto two dimensions. On the y-axis it covered issues of tradition and religion, like patriotism, abortion, euthanasia and the importance of obeying the law and authority figures. At the bottom of the chart is the "traditionalist" position on issues like these (with loyalty to country and family and respect for life considered important), while at the top is the "secular" position. The x-axis deals with self-expression, issues like everyday conduct and dress, acceptance of diversity (including foreigners) and innovation, and attitudes towards people with specific controversial lifestyles such as vegetarianism, as well as willingness to engage in political activism. At the right of the chart is the open "self-expressionist" position, while at the left is its opposite position, which Dr. Inglehart calls "survivalist". This chart not only has the power to map the values of individuals, but also to compare the values of people in different countries. Placed on this chart, EU countries in continental Europe come out on the top right, Anglophone countries on the middle right, Latin American countries on the bottom right, African, Middle Eastern and South Asian countries on the bottom left, and ex-Communist countries on the top left.
Mitchell: Eight Ways to Run the Country.
Brian Patrick Mitchell identifies four main political traditions in Anglo-American history. Mitchell analyzed modern American political perspectives according to their regard for "kratos" (defined as the use of force) and "archē" or “archy” (defined as the recognition of rank), grounding this distinction of archy and kratos in the West's historical experience of church and state and crediting the collapse of the Christian consensus on church and state with the appearance of four main divergent traditions in Western political thought:
Mitchell charts these traditions graphically using a vertical axis as a scale of kratos/akrateia and a horizontal axis as a scale of archy/anarchy. He places democratic progressivism in the lower left, plutocratic nationalism in the lower right, republican constitutionalism in the upper right, and libertarian individualism in the upper left. The political left is therefore distinguished by its rejection of archy, while the political right is distinguished by its acceptance of archy. For Mitchell, anarchy is not the absence of government but the rejection of rank. Thus there can be both anti-government anarchists (Mitchell’s "libertarian individualists") and pro-government anarchists (Mitchell's "democratic progressives", who favor the use of government force against social hierarchies such as patriarchy). Mitchell also distinguishes between left-wing anarchists and right-wing anarchists, whom Mitchell renames "akratists" for their opposition to the government’s use of force. Anthony Gregory of the Independent Institute credits Mitchell with "the best explanation of the political spectrum," saying he "makes sense of all the major mysteries."
Political-spectrum-based forecasts.
As shown by Russian political scientist Stepan S. Sulakshin, political spectra can be used as a forecasting tool. Sulakshin offered mathematical evidence that stable development (positive dynamics of the vast number of statistic indices) depends on the width of the political spectrum: if it is too narrow or too wide, stagnation or political disasters will result. Sulakshin also showed that, in the short run, the political spectrum determines the statistic indices dynamic and not vice versa.

</doc>
<doc id="23496" url="https://en.wikipedia.org/wiki?curid=23496" title="Pregnancy (mammals)">
Pregnancy (mammals)

In mammals, pregnancy is the period of reproduction during which a female carries one or more live offspring from implantation in the uterus through gestation. It begins when a fertilized zygote implants in the female's uterus; and ends once it leaves the uterus.
Fertilization and implantation.
A male and female copulate, the male inseminating the female. The spermatozoon fertilizes an ovum or various ova in the uterus or fallopian tubes, and this results in one or multiple zygotes. Sometimes, a zygote can be created by humans outside of the animal's body in the artificial process of in-vitro fertilization. After fertilization, the newly formed zygote then begins to divide through mitosis, forming an embryo, which implants in the female's endometrium. At this time, the embryo usually consists of 50 cells.
Development.
After implantation.
A blastocoele is a small cavity on the center of the embryo, and the developing embryonary cells will grow around it. Then, a flat layer cell forms on the exterior of this cavity, and the zona pellucida, the blastocyst's barrier, remains the same size as before. Cells grow increasingly smaller to fit in. This new structure with a cavity in the center and the developing cells around it is known as a blastocyst.
The presence of the blastocyst means that two types of cells are forming, an inner-cell mass growing on the interior of the blastocele and cells growing on the exterior of it. In 24 to 48 hours, the zona pellucida breaches. The cells on the exterior of the blastocyst begin excreting an enzyme which erodes epithelial uterine lining and creates a site for implantation.
Placental circulation system.
The cells surrounding the blastocyst now destroy cells in the uterine lining, forming small pools of blood, which in turn stimulate the production of capillaries. This is the first stage in the growth of the placenta. The inner cell mass of the blastocyst divides rapidly, forming two layers. The top layer becomes the embryo, and cells from there occupy the amniotic cavity. At the same time, the bottom layer forms a small sac (if the cells begin developing in an abnormal position, an ectopic gestation may also occur at this point).
Several days later, chorionic villi in the forming placenta anchor the implantation site to the uterus. A system of blood and blood vessels now develops at the point of the newly forming placenta, growing near the implantation site. The small sac inside the blastocyst begins producing red blood cells. For the next 24 hours, connective tissue develops between the developing placenta and the growing embryo. This later develops into the umbilical cord.
Cellular differentiation.
Following this, a narrow line of cells appears on the surface on the embryo. Its growth makes the embryo undergo gastrulation, in which the three primary tissue layers of the fetus, the ectoderm, mesoderm, and endoderm, develop. The narrow line of cells begin to form the endoderm and mesoderm. The ectoderm begins to grow rapidly as a result of chemicals being produced by the mesoderm. These three layers give rise to all the various types of tissue in the body.
The endoderm later forms the lining of the tongue, digestive tract, lungs, bladder and several glands. The mesoderm forms muscle, bone, and lymph tissue, as well as the interior of the lungs, heart, and reproductive and excretory systems. It also gives rise to the spleen, and produces blood cells. The ectoderm forms the skin, nails, hair, cornea, lining of the internal and external ear, nose, sinuses, mouth, anus, teeth, pituitary gland, mammary glands, eyes, and all parts of the nervous system.
Approximately 18 days after fertilization, the embryo has divided to form much of the tissue it will need. It is shaped like a pear, where the head region is larger than the tail. The embryo's nervous system is one of the first organic systems to grow. It begins growing in a concave area known as the neural groove.
The blood system continues to grow networks which allow the blood to flow around the embryo. Blood cells are already being produced and are flowing through these developing networks. Secondary blood vessels also begin to develop around the placenta, to supply it with more nutrients. Blood cells begin to form on the sac in the center of the embryo, as well as cells which begin to differentiate into blood vessels. Endocardial cells begin to form the myocardium.
At about 24 days past fertilization, there is a primitive S-shaped tubule heart which begins beating. The flow of fluids throughout the embryo begins at this stage.
Gestation period by species.
The following table shows the gestation or incubation periods for a range of mammals.

</doc>
<doc id="23497" url="https://en.wikipedia.org/wiki?curid=23497" title="Paroxysmal attack">
Paroxysmal attack

Paroxysmal attacks or paroxysms (from Greek παροξυσμός) are a sudden recurrence or intensification of symptoms, such as a spasm or seizure. These short, frequent, and stereotyped symptoms can be observed in various clinical conditions. They are usually associated with multiple sclerosis or pertussis, but they may also be observed in other disorders such as encephalitis, head trauma, stroke, asthma, trigeminal neuralgia, breath-holding spells, epilepsy, malaria, tabes dorsalis, and Behçet's disease, paroxysmal nocturnal hemoglobinuria (PNH). It has also been noted as a symptom of gratification disorder in children.
The word paroxysm means "sudden attack, outburst", and comes from the Greek παροξυσμός ("paroxusmos"), "irritation, exasperation".
Paroxysmal attacks in various disorders have been reported extensively and ephaptic coupling of demyelinated nerves has been presumed as one of the underlying mechanisms of this phenomenon. This is supported by the presence of these attacks in multiple sclerosis and tabes dorsalis, which both involve demyelination of spinal cord neurons. Exercise, tactile stimuli, hot water, anxiety and neck flexion may provoke paroxysmal attacks. Most reported paroxysmal attacks are painful tonic spasms, dysarthria and ataxia, numbness and hemiparesis. They are typically different from other transient symptoms by their brevity (lasting no more than 2 minutes), frequency (from 1-2 times/day up to a few hundred times/day), stereotyped fashion and excellent response to drugs (usually carbamazepine). Withdrawal of symptoms without any residual neurological finding is another key feature in their recognition.

</doc>
<doc id="23501" url="https://en.wikipedia.org/wiki?curid=23501" title="Potato">
Potato

The potato is a starchy, tuberous crop from the perennial nightshade "Solanum tuberosum" L. The word "potato" may refer either to the plant itself or to the edible tuber. In the Andes, where the species is indigenous, there are some other closely related cultivated potato species. Potatoes were introduced outside the Andes region approximately four centuries ago, and have since become an integral part of much of the world's food supply. It is the world's fourth-largest food crop, following maize, wheat, and rice. The green leaves and green skins of tubers exposed to the light are toxic.
Wild potato species can be found throughout the Americas from the United States to southern Chile. The potato was originally believed to have been domesticated independently in multiple locations, but later genetic testing of the wide variety of cultivars and wild species proved a single origin for potatoes in the area of present-day southern Peru and extreme northwestern Bolivia (from a species in the "Solanum brevicaule" complex), where they were domesticated approximately 7,000–10,000 years ago. Following centuries of selective breeding, there are now over a thousand different types of potatoes. Over 99% of the presently cultivated potatoes worldwide descended from varieties that originated in the lowlands of south-central Chile, which have displaced formerly popular varieties from the Andes.
However, the local importance of the potato is variable and changing rapidly. It remains an essential crop in Europe (especially eastern and central Europe), where per capita production is still the highest in the world, but the most rapid expansion over the past few decades has occurred in southern and eastern Asia. As of 2007 China led the world in potato production, and nearly a third of the world's potatoes were harvested in China and India.
Etymology.
The English word "potato" comes from Spanish "patata" (the name used in Spain). The Spanish Royal Academy says the Spanish word is a compound of the Taíno "batata" called in India speically in Gujarat as "બટેટા" (sweet potato) and the Quechua "papa" (potato). The name potato originally referred to a type of sweet potato although the two plants are not closely related; in many of the chronicles detailing agriculture and plants, no distinction is made between the two. The 16th-century English herbalist John Gerard used the terms "bastard potatoes" and "Virginia potatoes" for this species, and referred to sweet potatoes as "common potatoes". Potatoes are occasionally referred to as "Irish potatoes" or "white potatoes" in the United States, to distinguish them from sweet potatoes.
The name spud for a small potato comes from the digging of soil (or a hole) prior to the planting of potatoes. The word has an unknown origin and was originally (c. 1440) used as a term for a short knife or dagger, probably related to Dutch "spyd" or the Latin "spad-" a word root meaning "sword"; cf. Spanish "espada", English "spade" and "spadroon". The word spud traces back to the 16th century. It subsequently transferred over to a variety of digging tools. Around 1845, the name transferred to the tuber itself. The origin of the word "spud" has erroneously been attributed to a 19th-century activist group dedicated to keeping the potato out of Britain, calling itself The Society for the Prevention of an Unwholesome Diet (S.P.U.D.). It was Mario Pei's 1949 "The Story of Language" that can be blamed for the word's false origin. Pei writes, "the potato, for its part, was in disrepute some centuries ago. Some Englishmen who did not fancy potatoes formed a Society for the Prevention of Unwholesome Diet. The initials of the main words in this title gave rise to spud." Like most other pre-20th century acronymic origins, this is false.
Characteristics.
Potato plants are herbaceous perennials that grow about high, depending on variety, with the leaves dying back after flowering, fruiting and tuber formation. They bear white, pink, red, blue, or purple flowers with yellow stamens. In general, the tubers of varieties with white flowers have white skins, while those of varieties with colored flowers tend to have pinkish skins. Potatoes are mostly cross-pollinated by insects such as bumblebees, which carry pollen from other potato plants, though a substantial amount of self-fertilizing occurs as well. Tubers form in response to decreasing day length, although this tendency has been minimized in commercial varieties.
After flowering, potato plants produce small green fruits that resemble green cherry tomatoes, each containing about 300 seeds. Like all parts of the plant except the tubers, the fruit contain the toxic alkaloid solanine and are therefore unsuitable for consumption. All new potato varieties are grown from seeds, also called "true potato seed", "TPS" or "botanical seed" to distinguish it from seed tubers. New varieties grown from seed can be propagated vegetatively by planting tubers, pieces of tubers cut to include at least one or two eyes, or cuttings, a practice used in greenhouses for the production of healthy seed tubers. Plants propagated from tubers are clones of the parent, whereas those propagated from seed produce a range of different varieties.
Genetics.
There are about 5,000 potato varieties worldwide. Three thousand of them are found in the Andes alone, mainly in Peru, Bolivia, Ecuador, Chile, and Colombia. They belong to eight or nine species, depending on the taxonomic school. Apart from the 5,000 cultivated varieties, there are about 200 wild species and subspecies, many of which can be cross-bred with cultivated varieties. Cross-breeding has been done repeatedly to transfer resistances to certain pests and diseases from the gene pool of wild species to the gene pool of cultivated potato species. Genetically modified varieties have met public resistance in the United States and in the European Union.
The major species grown worldwide is "Solanum tuberosum" (a tetraploid with 48 chromosomes), and modern varieties of this species are the most widely cultivated. There are also four diploid species (with 24 chromosomes): "S. stenotomum", "S. phureja", "S. goniocalyx", and "S. ajanhuiri". There are two triploid species (with 36 chromosomes): "S. chaucha" and "S. juzepczukii". There is one pentaploid cultivated species (with 60 chromosomes): "S. curtilobum". There are two major subspecies of "Solanum tuberosum": "andigena", or Andean; and "tuberosum", or Chilean. The Andean potato is adapted to the short-day conditions prevalent in the mountainous equatorial and tropical regions where it originated; the Chilean potato, however, native to the Chiloé Archipelago, is adapted to the long-day conditions prevalent in the higher latitude region of southern Chile.
The International Potato Center, based in Lima, Peru, holds an ISO-accredited collection of potato germplasm. The international Potato Genome Sequencing Consortium announced in 2009 that they had achieved a draft sequence of the potato genome. The potato genome contains 12 chromosomes and 860 million base pairs, making it a medium-sized plant genome. More than 99 percent of all current varieties of potatoes currently grown are direct descendants of a subspecies that once grew in the lowlands of south-central Chile. Nonetheless, genetic testing of the wide variety of cultivars and wild species affirms that all potato subspecies derive from a single origin in the area of present-day southern Peru and extreme northwestern Bolivia (from a species in the "Solanum brevicaule" complex).
Most modern potatoes grown in North America arrived through European settlement and not independently from the South American sources; however, at least one wild potato species, "Solanum fendleri", is found as far north as Texas and is used in breeding for resistance to a nematode species that attacks cultivated potatoes. A secondary center of genetic variability of the potato is Mexico, where important wild species that have been used extensively in modern breeding are found, such as the hexaploid "Solanum demissum", as a source of resistance to the devastating late blight disease. Another relative native to this region, "Solanum bulbocastanum", has been used to genetically engineer the potato to resist potato blight.
Potatoes yield abundantly with little effort, and adapt readily to diverse climates as long as the climate is cool and moist enough for the plants to gather sufficient water from the soil to form the starchy tubers. Potatoes do not keep very well in storage and are vulnerable to molds that feed on the stored tubers and quickly turn them rotten, however: crops such as grain can be stored for several years with a low risk of rot. The yield of Calories per acre (about 9.2 million) is higher than that of maize (7.5 million), rice (7.4 million), wheat (3 million), or soybean (2.8 million).
History.
The potato was first domesticated in the region of modern-day southern Peru and extreme northwestern Bolivia between 8000 and 5000 BC. It has since spread around the world and become a staple crop in many countries.
The earliest archaeologically verified potato tuber remains have been found at the coastal site of Ancon (central Peru), dating to 2500 BC.
According to conservative estimates, the introduction of the potato was responsible for a quarter of the growth in Old World population and urbanization between 1700 and 1900. Following the Spanish conquest of the Inca Empire, the Spanish introduced the potato to Europe in the second half of the 16th century. The staple was subsequently conveyed by European mariners to territories and ports throughout the world. The potato was slow to be adopted by distrustful European farmers, but soon enough it became an important food staple and field crop that played a major role in the European 19th century population boom. However, lack of genetic diversity, due to the very limited number of varieties initially introduced, left the crop vulnerable to disease. In 1845, a plant disease known as late blight, caused by the fungus-like oomycete "Phytophthora infestans", spread rapidly through the poorer communities of western Ireland, resulting in the crop failures that led to the Great Irish Famine. Thousands of varieties still persist in the Andes however, where over 100 cultivars might be found in a single valley, and a dozen or more might be maintained by a single agricultural household.
Role in world food supply.
The Food and Agriculture Organization of the United Nations reports that the world production of potatoes in 2013 was about 368 million tonnes. Just over two thirds of the global production is eaten directly by humans with the rest being fed to animals or used to produce starch. This means that the annual diet of an average global citizen in the first decade of the 21st century included about 33 kg (or 73 lb) of potato. However, the local importance of potato is extremely variable and rapidly changing. It remains an essential crop in Europe (especially eastern and central Europe), where per capita production is still the highest in the world, but the most rapid expansion over the past few decades has occurred in southern and eastern Asia. As of 2007, China led the world in potato production, and nearly a third of the world's potatoes were harvested in China and India. The geographic shift of potato production has been away from wealthier countries toward lower-income areas of the world, although the degree of this trend is ambiguous.
In 2008, several international organizations highlighted the potato's role in world food production, in the face of developing economic problems. They cited its potential derived from its status as a cheap and plentiful crop that grows in a wide variety of climates and locales.
International Year of the Potato.
Due to perishability, only about 5% of the world's potato crop is traded internationally; its minimal presence in world financial markets contributed to its stable pricing during the 2007–2008 world food price crisis. Thus, the United Nations officially declared 2008 as the "International Year of the Potato", to raise its profile in developing nations, calling the crop a "hidden treasure". This followed the International Rice Year in 2004.
Nutrition.
The potato contains vitamins and minerals, as well as an assortment of phytochemicals, such as carotenoids and natural phenols. Chlorogenic acid constitutes up to 90% of the potato tuber natural phenols. Others found in potatoes are 4-O-caffeoylquinic acid (crypto-chlorogenic acid), 5-O-caffeoylquinic (neo-chlorogenic acid), 3,4-dicaffeoylquinic and 3,5-dicaffeoylquinic acids. A medium-size potato with the skin provides 27 mg of vitamin C (45% of the Daily Value (DV)), 620 mg of potassium (18% of DV), 0.2 mg vitamin B6 (10% of DV) and trace amounts of thiamin, riboflavin, folate, niacin, magnesium, phosphorus, iron, and zinc.
The potato is best known for its carbohydrate content (approximately 26 grams in a medium potato). The predominant form of this carbohydrate is starch. A small but significant portion of this starch is resistant to digestion by enzymes in the stomach and small intestine, and so reaches the large intestine essentially intact. This resistant starch is considered to have similar physiological effects and health benefits as fiber: It provides bulk, offers protection against colon cancer, improves glucose tolerance and insulin sensitivity, lowers plasma cholesterol and triglyceride concentrations, increases satiety, and possibly even reduces fat storage. The amount of resistant starch in potatoes depends much on preparation methods. Cooking and then cooling potatoes significantly increases resistant starch. For example, cooked potato starch contains about 7% resistant starch, which increases to about 13% upon cooling.
The storage and cooking method used can significantly affect the nutrient availability of the potato.
Potatoes are often broadly classified as high on the glycemic index (GI) and so are often excluded from the diets of individuals trying to follow a low-GI diet. In fact, the GI of potatoes can vary considerably depending on type (such as red, russet, white, or King Edward), origin (where it was grown), preparation methods (i.e., cooking method, whether it is eaten hot or cold, whether it is mashed or cubed or consumed whole, etc.), and with what it is consumed (i.e., the addition of various high-fat or high-protein toppings).
In the UK, potatoes are not considered by the NHS as counting towards the recommended daily five portions of fruit and vegetables.
Comparison to other major staple foods.
The following table shows the nutrient content of potato and other major staple foods, each in respective raw form. Staple foods are not commonly eaten raw and are usually sprouted or cooked before eating. In sprouted and cooked form, the relative nutritional and anti-nutritional contents of each of these grains may be different from the values reported in this table. 
Toxicity.
Potatoes contain toxic compounds known as glycoalkaloids, of which the most prevalent are solanine and chaconine. Solanine is also found in other plants in the family Solanaceae, which includes such plants as the deadly nightshade ("Atropa belladonna"), henbane ("Hyoscyamus niger"), tobacco ("Nicotiana"), as well as eggplant and tomato.
These compounds, which protect the plant from its predators, are, in general, concentrated in its leaves, stems, sprouts, and fruits. In a summary of several studies, the glycoalkaloid content was highest in flowers and sprouts and lowest in the tuber flesh (in order from highest to lowest content, generally: flowers, sprouts, leaves, skin, roots, berries, peel plus outer cortex of tuber flesh, stems, and tuber flesh). Exposure to light, physical damage, and age increase glycoalkaloid content within the tuber. Cooking at high temperatures—over —partly destroys these. The concentration of glycoalkaloid in wild potatoes suffices to produce toxic effects in humans. Glycoalkaloids may cause headaches, diarrhea, cramps, and in severe cases coma and death; however, poisoning from potatoes occurs very rarely. Light exposure causes greening from chlorophyll synthesis, thus giving a visual clue as to areas of the tuber that may have become more toxic; however, this does not provide a definitive guide, as greening and glycoalkaloid accumulation can occur independently of each other. Varieties contain different levels of glycoalkaloids. Lenape was released in 1967 but was withdrawn in 1970 as it contained high levels of glycoalkaloids. Since then breeders developing new varieties test for this, and sometimes have to discard an otherwise promising cultivar.
Breeders try to keep solanine levels below 200 mg/kg (200 ppmw). However, when these commercial varieties turn green, even they can approach concentrations of solanine of 1000 mg/kg (1000 ppmw). In normal potatoes, analysis has shown solanine levels may be as little as 3.5% of the breeders' maximum, with 7–187 mg/kg being found. While a normal potato has 12–20 mg/kg of glycoalkaloid content, a green tuber contains 250–280 mg/kg, and green skin 1500–2200 mg/kg.
The U.S. National Toxicology Program suggests that the average American consume at most 12.5 mg/day of solanine from potatoes (the toxic dose is actually several times this, depending on body weight). Douglas L. Holt, the State Extension Specialist for Food Safety at the University of Missouri, notes that no reported cases of potato-source solanine poisoning have occurred in the U.S. in the last 50 years, and most cases involved eating green potatoes or drinking potato-leaf tea.
Growth and cultivation.
Potatoes are generally grown from seed potatoes – these are tubers specifically grown to be disease free and provide consistent and healthy plants. To be disease free, the areas where seed potatoes are grown are selected with care. In the USA this restricts production of seed potatoes to only 15 states out of the 50 states that grow potatoes. These locations are selected for their cold hard winters that kill pests and long sunshine hours in the summer for optimum growth. In the UK, most seed potatoes originate in Scotland in areas where westerly winds prevent aphid attack and thus prevent spread of potato virus pathogens.
Potato growth has been divided into five phases. During the first phase, sprouts emerge from the seed potatoes and root growth begins. During the second, photosynthesis begins as the plant develops leaves and branches. In the third phase stolons develop from lower leaf axils on the stem and grow downwards into the ground and on these stolons new tubers develop as swellings of the stolon. This phase is often (but not always) associated with flowering. Tuber formation halts when soil temperatures reach ; hence potatoes are considered a cool-season crop. Tuber bulking occurs during the fourth phase, when the plant begins investing the majority of its resources in its newly formed tubers. At this stage, several factors are critical to yield: optimal soil moisture and temperature, soil nutrient availability and balance, and resistance to pest attacks. The final phase is maturation: The plant canopy dies back, the tuber skins harden, and their sugars convert to starches.
New tubers may arise at the soil surface. Since exposure to light leads to greening of the skins and the development of solanine, growers are interested in covering such tubers. Commercial growers usually address this problem by piling additional soil around the base of the plant as it grows ("hilling", or in British English "earthing up"). An alternative method used by home gardeners and smaller-scale growers involves covering the growing area with organic mulches such as straw or with plastic sheets.
Correct potato husbandry can be an arduous task in some circumstances. Good ground preparation, harrowing, plowing, and rolling are always needed, along with a little grace from the weather and a good source of water. Three successive plowings, with associated harrowing and rolling, are desirable before planting. Eliminating all root-weeds is desirable in potato cultivation. In general, the potatoes themselves are grown from the eyes of another potato and not from seed. Home gardeners often plant a piece of potato with two or three eyes in a hill of mounded soil. Commercial growers plant potatoes as a row crop using seed tubers, young plants or microtubers and may mound the entire row. Seed potato crops are 'rogued' in some countries to eliminate diseased plants or those of a different variety from the seed crop.
Potatoes are sensitive to heavy frosts, which damage them in the ground. Even cold weather makes potatoes more susceptible to bruising and possibly later rotting, which can quickly ruin a large stored crop.
At harvest time, gardeners usually dig up potatoes with a long-handled, three-prong "grape" (or graip), i.e., a spading fork, or a potato hook, which is similar to the graip but with tines at a 90° angle to the handle. In larger plots, the plow is the fastest implement for unearthing potatoes. Commercial harvesting is typically done with large potato harvesters, which scoop up the plant and surrounding earth. This is transported up an apron chain consisting of steel links several feet wide, which separates some of the dirt. The chain deposits into an area where further separation occurs. Different designs use different systems at this point. The most complex designs use vine choppers and shakers, along with a blower system to separate the potatoes from the plant. The result is then usually run past workers who continue to sort out plant material, stones, and rotten potatoes before the potatoes are continuously delivered to a wagon or truck. Further inspection and separation occurs when the potatoes are unloaded from the field vehicles and put into storage.
Immature potatoes may be sold as "new potatoes" and are particularly valued for taste. These are often harvested by the home gardener or farmer by "grabbling", i.e. pulling out the young tubers by hand while leaving the plant in place.
Potatoes are usually cured after harvest to improve skin-set. Skin-set is the process by which the skin of the potato becomes resistant to skinning damage. Potato tubers may be susceptible to skinning at harvest and suffer skinning damage during harvest and handling operations. Curing allows the skin to fully set and any wounds to heal. Wound-healing prevents infection and water-loss from the tubers during storage. Curing is normally done at relatively warm temperatures with high humidity and good gas-exchange if at all possible.
Storage.
Storage facilities need to be carefully designed to keep the potatoes alive and slow the natural process of decomposition, which involves the breakdown of starch. It is crucial that the storage area is dark, well ventilated and for long-term storage maintained at temperatures near . For short-term storage before cooking, temperatures of about are preferred.
On the other hand, temperatures below convert potatoes' starch into sugar, which alters their taste and cooking qualities and leads to higher acrylamide levels in the cooked product, especially in deep-fried dishesthe discovery of acrylamides in starchy foods in 2002 has led to many international health concerns as they are believed to be possible carcinogens and their occurrence in cooked foods is currently under study as a possible influence in potential health problems.
Under optimum conditions possible in commercial warehouses, potatoes can be stored for up to ten to twelve months. When stored in homes, the shelf life is usually only a few weeks. If potatoes develop green areas or start to sprout, these areas should be trimmed before using. Trimming or peeling green areas are inadequate to remove copresent toxins, and such potatoes are no longer suitable as animal food.
Commercial storage of potatoes involves several phases: drying of surface moisture; a wound healing phase at 85% to 95% relative humidity and temperatures below ; a staged cooling phase; a holding phase; and a reconditioning phase, during which the tubers are slowly warmed. Mechanical ventilation is used at various points during the process to prevent condensation and accumulation of carbon dioxide.
Yield.
The world dedicated 18.6 million hectares in 2010 for potato cultivation. The average world farm yield for potato was 17.4 tonnes per hectare, in 2010. Potato farms in the United States were the most productive in 2010, with a nationwide average of 44.3 tonnes per hectare. United Kingdom was a close second.
New Zealand farmers have demonstrated some of the best commercial yields in the world, ranging between 60 and 80 tonnes per hectare, some reporting yields of 88 tonnes potatoes per hectare.
There is a big gap among various countries between high and low yields, even with the same variety of potato. Average potato yields in developed economies ranges between 38–44 tonnes per hectare. China and India accounted for over a third of world's production in 2010, and had yields of 14.7 and 19.9 tonnes per hectare respectively. The yield gap between farms in developing economies and developed economies represents an opportunity loss of over 400 million tonnes of potato, or an amount greater than 2010 world potato production. Potato crop yields are determined by factors such as the crop breed, seed age and quality, crop management practices and the plant environment. Improvements in one or more of these yield determinants, and a closure of the yield gap, can be a major boost to food supply and farmer incomes in the developing world.
Varieties.
While there are close to 4000 different varieties of potato, it has been bred into many standard or well-known varieties, each of which has particular agricultural or culinary attributes. In general, varieties are categorized into a few main groups, such as russets, reds, whites, yellows (also called Yukons) and purples—based on common characteristics. Around 80 varieties are commercially available in the UK. For culinary purposes, varieties are often differentiated by their waxiness. Floury, or mealy (baking) potatoes have more starch (20–22%) than waxy (boiling) potatoes (16–18%). The distinction may also arise from variation in the comparative ratio of two potato starch compounds: amylose and amylopectin. Amylose, a long-chain molecule, diffuses from the starch granule when cooked in water, and lends itself to dishes where the potato is mashed. Varieties that contain a slightly higher amylopectin content, a highly branched molecule, help the potato retain its shape when boiled.
The European Cultivated Potato Database (ECPD) is an online collaborative database of potato variety descriptions, updated and maintained by the Scottish Agricultural Science Agency within the framework of the European Cooperative Programme for Crop Genetic Resources Networks (ECP/GR)—which is run by the International Plant Genetic Resources Institute (IPGRI).
Popular varieties (cultivars) include:
Blue varieties.
The blue or purple potato originated in South America. It has purple skin and flesh, which becomes blue once cooked. It has a slight whitish scab that seems to be present in all samples. The variety, called "Cream of the Crop", has been introduced into Ireland and has proved popular. To preserve the purple color best, microwaving is the best way to cook it; however, it can also be baked and steamed. The purple or blue potato tends to have a more nutty flavor than its relatives of other colors.
A mutation in the varieties' P locus causes production of the antioxidant anthocyanin.
Genetically modified potatoes.
Genetic research has produced several genetically modified varieties. 'New Leaf', owned by Monsanto Company, incorporates genes from "Bacillus thuringiensis", which confers resistance to the Colorado potato beetle; 'New Leaf Plus' and 'New Leaf Y', approved by US regulatory agencies during the 1990s, also include resistance to viruses. McDonald's, Burger King, Frito-Lay, and Procter & Gamble announced they would not use genetically modified potatoes, and Monsanto published its intent to discontinue the line in March 2001.
Waxy potato varieties produce two main kinds of potato starch, amylose and amylopectin, the latter of which is most industrially useful. The German chemical company BASF created the Amflora potato, which has been modified to contain antisense against the enzyme that drives synthesis of amylose, namely granule bound starch synthase. This resulting potato almost exclusively produces amylopectin, and thus is more useful for the starch industry. In 2010, the European Commission cleared the way for 'Amflora' to be grown in the European Union for industrial purposes only—not for food. Nevertheless, under EU rules, individual countries have the right to decide whether they will allow this potato to be grown on their territory. Commercial planting of 'Amflora' was expected in the Czech Republic and Germany in the spring of 2010, and Sweden and the Netherlands in subsequent years. Another GM potato variety developed by BASF is 'Fortuna' which was made resistant to late blight by adding two resistance genes, blb1 and blb2, which originate from the Mexican wild potato Solanum bulbocastanum. In October 2011 BASF requested cultivation and marketing approval as a feed and food from the EFSA. In 2012, GMO development in Europe was stopped by BASF.
In November 2014, the USDA approved a genetically modified potato developed by J.R. Simplot Company, which contains genetic modifications that prevent bruising and produce less acrylamide when fried than conventional potatoes; the modifications do not cause new proteins to be made, but rather prevent proteins from being made via RNA interference.
Pests.
The historically significant "Phytophthora infestans" (late blight) remains an ongoing problem in Europe and the United States. Other potato diseases include "Rhizoctonia", "Sclerotinia", black leg, powdery mildew, powdery scab and leafroll virus.
Insects that commonly transmit potato diseases or damage the plants include the Colorado potato beetle, the potato tuber moth, the green peach aphid ("Myzus persicae"), the potato aphid, beet leafhoppers, thrips, and mites. The potato cyst nematode is a microscopic worm that thrives on the roots, thus causing the potato plants to wilt. Since its eggs can survive in the soil for several years, crop rotation is recommended.
Pesticides.
During the crop year 2008, many of the certified organic potatoes produced in the United Kingdom and certified by the Soil Association as organic were sprayed with a copper pesticide to control potato blight ("Phytophthora infestans"). According to the Soil Association, the total copper that can be applied to organic land is 6 kg/ha/year.
According to an Environmental Working Group analysis of USDA and FDA pesticide residue tests performed from 2000 through 2008, 84% of the 2,216 tested potato samples contained detectable traces of at least one pesticide. A total of 36 unique pesticides were detected on potatoes over the 2,216 samples, though no individual sample contained more than 6 unique pesticide traces, and the average was 1.29 detectable unique pesticide traces per sample. The average quantity of all pesticide traces found in the 2,216 samples was 1.602 ppm. While this was a very low value of pesticide residue, it was the highest amongst the 50 vegetables analyzed.
Uses.
Culinary uses.
Potatoes are prepared in many ways: skin-on or peeled, whole or cut up, with seasonings or without. The only requirement involves cooking to swell the starch granules. Most potato dishes are served hot, but some are first cooked, then served cold, notably potato salad and potato chips/crisps.
Common dishes are: mashed potatoes, which are first boiled (usually peeled), and then mashed with milk or yogurt and butter; whole baked potatoes; boiled or steamed potatoes; French-fried potatoes or chips; cut into cubes and roasted; scalloped, diced, or sliced and fried (home fries); grated into small thin strips and fried (hash browns); grated and formed into dumplings, Rösti or potato pancakes. Unlike many foods, potatoes can also be easily cooked in a microwave oven and still retain nearly all of their nutritional value, provided they are covered in ventilated plastic wrap to prevent moisture from escaping; this method produces a meal very similar to a steamed potato, while retaining the appearance of a conventionally baked potato. Potato chunks also commonly appear as a stew ingredient.
Potatoes are boiled between 10 and 25 minutes, depending on size and type, to become soft.
Grading.
In the U.S., potato grading for Idaho potatoes is performed in which No. 1 potatoes are the highest quality and No. 2 are rated as lower in quality due to their appearance (e.g. blemishes or bruises, pointy ends). Potato density assessment can be performed by floating them in brines. High-density potatoes are desirable in the production of dehydrated mashed potatoes, potato crisps and french fries.
Latin America.
Peruvian cuisine naturally contains the potato as a primary ingredient in many dishes, as around 3,000 varieties of this tuber are grown there.
Some of the more notable dishes include boiled potato as a base for several dishes or with ají-based sauces like in Papa a la Huancaína or ocopa, diced potato for its use in soups like in cau cau, or in Carapulca with dried potato (papa seca). Smashed condimented potato is used in causa Limeña and papa rellena. French-fried potatoes are a typical ingredient in Peruvian stir-fries, including the classic dish lomo saltado.
Chuño is a freeze-dried potato product traditionally made by Quechua and Aymara communities of Peru and Bolivia, and is known in various countries of South America, including Peru, Bolivia, Argentina, and Chile. In Chile's Chiloé Archipelago, potatoes are the main ingredient of many dishes, including milcaos, chapaleles, curanto and chochoca. In Ecuador, the potato, as well as being a staple with most dishes, is featured in the hearty "locro de papas", a thick soup of potato, squash, and cheese.
European cuisine.
In the UK, potatoes form part of the traditional staple fish and chips. Roast potatoes are commonly served with a Sunday roast, and mashed potatoes form a major component of several other traditional dishes such as shepherd's pie, bubble and squeak, and bangers and mash. New potatoes are often cooked with mint and served with a little melted butter.
The Tattie scone is a popular Scottish dish containing potatoes. Colcannon is a traditional Irish food made with mashed potato, shredded kale or cabbage, and onion; champ is a similar dish. Boxty pancakes are eaten throughout Ireland, although associated especially with the north, and in Irish diaspora communities; they are traditionally made with grated potatoes, soaked to loosen the starch and mixed with flour, buttermilk and baking powder. A variant eaten and sold in Lancashire, especially Liverpool, is made with cooked and mashed potatoes.
"Bryndzové halušky" is the Slovakian national dish, made of a batter of flour and finely grated potatoes that is boiled to form dumplings. These are then mixed with regionally varying ingredients.
In Northern and Eastern Europe, especially in Scandinavian countries, Poland, Russia, Belarus and Ukraine, newly harvested, early ripening varieties are considered a special delicacy. Boiled whole and served un-peeled with dill, these "new potatoes" are traditionally consumed with Baltic herring. Puddings made from grated potatoes (kugel, kugelis, and potato babka) are popular items of Ashkenazi, Lithuanian, and Belarusian cuisine.
Cepelinai is Lithuanian national dish. They are a type of dumpling made from riced potatoes (see Potato ricer) and usually stuffed with minced meat, although sometimes dry cottage cheese (curd) or mushrooms are used instead.
In Western Europe, especially in Belgium, sliced potatoes are fried to create "frieten", the original French fried potatoes. "Stamppot", a traditional Dutch meal, is based on mashed potatoes mixed with vegetables.
In France, the most notable potato dish is the "Hachis Parmentier", named after Antoine-Augustin Parmentier, a French pharmacist, nutritionist, and agronomist who, in the late 18th century, was instrumental in the acceptance of the potato as an edible crop in the country. The "pâté aux pommes de terre" is a regional potato dish from the central Allier and Limousin regions.
In the north of Italy, in particular, in the Friuli region of the northeast, potatoes serve to make a type of pasta called gnocchi. Similarly, cooked and mashed potatoes or potato flour can be used in the Knödel or dumpling eaten with or added to meat dishes all over central and Eastern Europe, but especially in Bavaria and Luxembourg. Potatoes form one of the main ingredients in many soups such as the vichyssoise and Albanian potato and cabbage soup. In western Norway, komle is popular.
A traditional Canary Islands dish is Canarian wrinkly potatoes or "papas arrugadas". "Tortilla de patatas" (potato omelete) and "patatas bravas" (a dish of fried potatoes in a spicy tomato sauce) are near-universal constituent of Spanish tapas.
North America.
In the United States, potatoes have become one of the most widely consumed crops and thus have a variety of preparation methods and condiments. French fries and often hash browns are commonly found in typical American fast-food burger joints and cafeterias. One popular favorite involves a baked potato with cheddar cheese (or sour cream and chives) on top, and in New England "smashed potatoes" (a chunkier variation on mashed potatoes, retaining the peel) have great popularity. Potato flakes are popular as an instant variety of mashed potatoes, which reconstitute into mashed potatoes by adding water, with butter or oil and salt to taste. A regional dish of Central New York, salt potatoes are bite-size new potatoes boiled in water saturated with salt then served with melted butter. At more formal dinners, a common practice includes taking small red potatoes, slicing them, and roasting them in an iron skillet. Among American Jews, the practice of eating latkes (fried potato pancakes) is common during the festival of Hanukkah.
A traditional Acadian dish from New Brunswick is known as "poutine râpée". The Acadian poutine is a ball of grated and mashed potato, salted, sometimes filled with pork in the center, and boiled. The result is a moist ball about the size of a baseball. It is commonly eaten with salt and pepper or brown sugar. It is believed to have originated from the German "Klöße", prepared by early German settlers who lived among the Acadians.
"Poutine", by contrast, is a hearty serving of French fries, fresh cheese curds and hot gravy. Tracing its origins to Quebec in the 1950s, it has become a widespread and popular dish throughout Canada.
South Asia.
In South Asia, Potato is very popular traditional staple. In India, the most popular potato dishes are "aloo ki sabzi", batata vada, and samosa, which is spicy mashed potato mixed with a small amount of vegetable stuffed in conical dough, and deep fried. Potatoes are also a major ingredient as fast food items, such as aloo chaat, where they are deep fried and served with chutney. In Northern India, alu dum and alu paratha are a favorite part of the diet; the first is a spicy curry of boiled potato, the second is a type of stuffed chapati.
A dish called masala dosa from South India is very notable all over India. It is a thin pancake of rice and pulse paste rolled over spicy smashed potato and eaten with sambhar and chutney. Poori in south India in particular in Tamil Nadu is almost always taken with smashed potato masal. Other favorite dishes are alu tikki and pakoda items.
Vada pav is a popular vegetarian fast food dish in Mumbai and other regions in the Maharashtra in India.
Aloo posto (a curry with potatoes and poppy seeds) is immensely popular in East India, especially Bengal. Although potatoes are not native to India, it has become a vital part of food all over the country especially North Indian food preparations. In Tamil Nadu this tuber acquired a name based on its appearance 'urulai-k-kizhangu' (உருளைக் கிழங்கு) meaning cylindrical tuber.
The Aloo gosht, Potato and meat curry, is one of the popular dishes in South Asia, especially in Pakistan.
East Asia.
In East Asia, particularly Southeast Asia, rice is by far the predominant starch crop, with potatoes a secondary crop, especially in China and Japan. However, it is used in northern China where rice is not easily grown, with a popular dish being 青椒土豆丝 (qīng jiāo tǔ dòu sī), made with green pepper, vinegar and thin slices of potato. In the winter, roadside sellers in northern China will also sell roasted potatoes. It is also occasionally seen in Korean and Thai cuisines.
Art.
The potato has been an essential crop in the Andes since the pre-Columbian Era. The Moche culture from Northern Peru made ceramics from earth, water, and fire. This pottery was a sacred substance, formed in significant shapes and used to represent important themes. Potatoes are represented anthropomorphically as well as naturally.
During the late 19th century, numerous images of potato harvesting appeared in European art, including the works of Willem Witsen and Anton Mauve. Van Gogh's 1885 painting "The Potato Eaters" portrays a family eating potatoes.
Invented in 1949 and marketed and sold commercially by Hasbro in 1952, Mr. Potato Head is an American toy that consists of a plastic potato and attachable plastic parts such as ears and eyes to make a face. It was the first toy ever advertised on television.

</doc>
<doc id="23503" url="https://en.wikipedia.org/wiki?curid=23503" title="Portland, Oregon">
Portland, Oregon

Portland () is the largest city in the U.S. state of Oregon and the seat of Multnomah County. It is located in the Willamette Valley region of the Pacific Northwest, at the confluence of the Willamette and Columbia rivers. The city covers 145 square miles (376 km²) and had an estimated population of 619,360 in 2014, making it the 28th most populous city in the United States. Approximately 2,348,247 people live in the Portland metropolitan statistical area (MSA), the 24th most populous MSA in the United States. Its Combined Statistical Area (CSA) ranks 17th with a population of 3,022,178. Roughly 60 percent of Oregon's population resides within the Portland metropolitan area.
Named after the city on the coast of Maine (which was named after the English Isle of Portland), the Oregon settlement began to be populated in the 1830s near the end of the Oregon Trail. Its water access provided convenient transportation of goods, and the timber industry was a major force in the city's early economy. At the turn of the 20th century, the city had developed a reputation as one of the most dangerous port cities in the world, a hub for organized crime and racketeering. After the city's economy experienced an industrial boom during World War II, its hard-edged reputation began to dissipate. Beginning in the 1960s, Portland became noted for its growing liberal political values, and the city has earned a reputation as a bastion of counterculture, a view which has proceeded into the 21st century. According to a 2009 Pew Research Center study, Portland ranks as the 8th most popular American city, based on where people want to live.
The city operates with a commission-based government guided by a mayor and four commissioners as well as Metro, the only directly elected metropolitan planning organization in the United States. The city government is notable for its land-use planning and investment in public transportation. Portland is frequently recognized as one of the most environmentally conscious cities in the world because of its high walkability, large community of bicyclists, farm-to-table dining, expansive network of public transportation options, and 10,000+ acres of public parks. Its climate is marked by warm, dry summers and chilly, rainy winters. This climate is ideal for growing roses, and Portland has been called the "City of Roses" for over a century. "Keep Portland Weird" is an unofficial slogan for the city.
History.
Pre-history and natives.
During the prehistoric period, the land that would become Portland was flooded after the collapse of glacial dams from Lake Missoula, located in what would later become Montana. These massive floods occurred during the last ice age and filled the Willamette Valley with of water.
Before American pioneers began arriving in the 1800s, the land that eventually became Portland and surrounding Multnomah County was inhabited for many centuries by two bands of indigenous Chinook people— the Multnomah and the Clackamas peoples. The Chinook people occupying the land which would become Portland were first documented by Meriwether Lewis and William Clark in 1805. Before its European settlement, the Portland Basin of the lower Columbia River and Willamette River valleys had been one of the most densely populated regions on the Pacific Coast.
Settlement.
Significant numbers of pioneer settlers began arriving in the Willamette Valley in the 1830s via the Oregon Trail, though life was originally centered in nearby Oregon City. In the early 1840s a new settlement began emerging ten miles from the mouth of the Willamette River, roughly halfway between Oregon City and Fort Vancouver. This community was initially referred to as "Stumptown" and "The Clearing" because of the many trees being cut down to allow for its growth. In 1843 William Overton saw potential in the new settlement but lacked the funds necessary to file an official land claim. For 25 cents Overton agreed to share half of the site with Asa Lovejoy of Boston, Massachusetts.
In 1845 Overton sold his remaining half of the claim to Francis W. Pettygrove of Portland, Maine. Both Pettygrove and Lovejoy wished to rename "The Clearing" after their respective hometowns (Lovejoy's being Boston, and Pettygrove's, Portland). This controversy was settled with a coin toss which Pettygrove won in a series of two out of three tosses, thereby providing Portland with its namesake. The coin used for this decision, now known as the Portland Penny, is on display in the headquarters of the Oregon Historical Society. At the time of its incorporation on February 8, 1851, Portland had over 800 inhabitants, a steam sawmill, a log cabin hotel, and a newspaper, the "Weekly Oregonian". A major fire swept through downtown in August 1873, destroying twenty blocks on the west side of the Willamette along Yamhill and Morrison Streets, and causing $1.3 million in damage. By 1879, the population had grown to 17,500 and by 1890 it had grown to 46,385. In 1888, the city constructed the first steel bridge built on the West Coast.
Portland's access to the Pacific Ocean via the Willamette and the Columbia rivers, as well as its easy access to the agricultural Tualatin Valley via the "Great Plank Road" (the route of current-day U.S. Route 26), provided the pioneer city with an advantage over other nearby ports, and it grew very quickly. Portland remained the major port in the Pacific Northwest for much of the 19th century, until the 1890s, when Seattle's deepwater harbor was connected to the rest of the mainland by rail, affording an inland route without the treacherous navigation of the Columbia River. The lumber industry also became a prominent economical presence, due to the area's large population of Douglas Firs, Western Hemlocks, Red Cedars, and Big Leaf Maple trees.
Portland developed a reputation early on in its history as a hard-edged and gritty port town. Some historians have described the city's early establishment as being a "scion of New England; an ends-of-the-earth home for the exiled spawn of the eastern established elite." In 1889, "The Oregonian" called Portland "the most filthy city in the Northern States," due to the unsanitary sewers and gutters, and, at the turn of the 20th century, it was considered one of the most dangerous port cities in the world. The city housed a large number of saloons, bordellos, gambling dens, and boardinghouses which were populated with miners after the California Gold Rush, as well as the multitude of sailors passing through the port. By the early 20th century, the city had lost its reputation as a "sober frontier city" and garnered a reputation for being violent and dangerous.
Post-war development.
Between 1900 and 1930, the population of the city tripled from nearly 100,000 to 301,815. Following this population boom, Portland became a notorious hub for underground criminal activity and organized crime between the 1940s and 1950s. In 1957, "LIFE" Magazine published an article detailing the city's history of government corruption and crime, specifically its gambling rackets and illegal nightclubs. The article, which focused on crime boss Jim Elkins, became the basis of a fictionalized film titled "Portland Exposé" (1957). In spite of the city's seedier undercurrent of criminal activity, Portland was experiencing an economic and industrial surge during World War II. Ship builder Henry J. Kaiser had been awarded contracts to construct Liberty ships and aircraft carrier escorts, and chose sites in Portland and Vancouver, Washington for work yards. During this time, Portland's population rose by over 150,000, largely attributed to recruited laborers.
During the 1960s, an influx of hippie subculture began to take root in the city in the wake of San Francisco's burgeoning countercultural scene. The city's Crystal Ballroom became a hub for the city's psychedelic culture, while food cooperatives and listener-funded media and radio stations were established. A large social activist presence evolved during this time as well, specifically concerning Native American rights, environmentalist causes, and gay rights. By the 1970s, Portland had well established itself as a progressive city, and experienced an economic boom for the majority of the decade; however, the slowing of the housing market in 1979 caused demand for the city and state timber industries to drop significantly.
1990s to present.
In the 1990s, the technology industry began to emerge in Portland, specifically with the establishment of companies like Intel, which brought more than $10 billion in investments in 1995 alone. After the year 2000, Portland experienced significant growth, with a population rise of over 90,000 between the years 2000 and 2014. The city's increased presence within the cultural lexicon has established it a popular city for young people, and it was second only to Louisville, Kentucky as one of the cities to attract and retain the highest number of college-educated people in the United States. Between 2001 and 2012, Portland's gross domestic product per person grew fifty percent, more than any other city in the country.
The city has acquired a diverse range of nicknames throughout its history, though it is most frequently called "Rose City" or "The City of Roses", the latter of which being its unofficial nickname since 1888 and its official nickname since 2003. Another widely utilized nickname by local residents in everyday speech is "PDX", which is also the airport code for Portland International Airport. Other nicknames include Bridgetown, Stumptown, Rip City, Soccer City, P-Town, Portlandia, and the more antiquated Little Beirut.
Geography.
Topography.
Portland is located 60 miles east of the Pacific Ocean at the northern end of Oregon's most populated region, the Willamette Valley. Downtown Portland straddles the banks of the Willamette River which flows north through the city center and consequently separates the east and west neighborhoods of the city. Less than 10 miles from downtown the Willamette River flows into the Columbia River, the fourth-largest river in the United States, which divides Oregon from Washington state. Portland is about 100 miles upriver from the Pacific Ocean on the Columbia.
Though much of downtown Portland is relatively flat, the foothills of the Tualatin Mountains, more commonly referred to locally as the "West Hills", pierce through the Northwest and Southwest reaches of the city. Council Crest Park, the tallest point within city limits, is located in the West Hills and rises to an elevation of 1,073 feet. The highest point east of the river is Mt. Tabor, an extinct volcanic cinder cone, which rises to 636 feet. Nearby Powell Butte and Rocky Butte rise to 614 feet and 612 feet, respectively. To the west of the Tualatin Mountains lies the Oregon Coast Range, and to the east lies the actively volcanic Cascade Range. On clear days Mt. Hood and Mt St. Helens dominate the horizon while Mt. Adams and Mt. Rainier can also be visible in the distance.
According to the United States Census Bureau, the city has a total area of , of which, is land and is water. Although almost all of Portland lies within Multnomah County, small portions of the city lie within Clackamas and Washington counties with populations estimated at 785 and 1,455, respectively.
Portland lies on top of an extinct volcanic field known as the Boring Lava Field. The Boring Lava Field contains at least 32 cinder cones such as Mount Tabor, and its center lies in Southeast Portland. Mount St. Helens, a highly active volcano 50 miles northeast of the city in Washington state, is easily visible on clear days and is close enough to have dusted the city with volcanic ash after its eruption on May 18, 1980.
Cityscape.
Portland's cityscape derives much of its character from the numerous bridges that span the Willamette River downtown, several of which are historical landmarks, and Portland has been nicknamed "Bridgetown" for many decades as a result. Three of downtown's most heavily-utilized bridges are more than 100 years old and are designated historic landmarks: Hawthorne Bridge (1910), Steel Bridge (1912), and Broadway Bridge (1913). Portland's newest bridge in the downtown area, Tilikum Crossing, opened in 2015 and is the first new bridge to span the Willamette in Portland since the 1973 opening of the Fremont Bridge.
Neighborhoods.
The Willamette River, which flows north through downtown, serves as the natural boundary between east and west Portland. The denser and earlier-developed west side extends into the lap of the West Hills, while the flatter east side fans out for roughly 180 blocks until it meets the suburb of Gresham. In 1891 the cities of Portland, Albina, and East Portland were consolidated, creating inconsistent patterns of street names and addresses. The "great renumbering" on September 2, 1931 standardized street naming patterns, divided Portland into five official quadrants, and changed house numbers from 20 per block to 100 per block.
The five quadrants of Portland have come to develop distinctive identities over time, with mild cultural differences and friendly rivalries between their residents, especially between those who live east of the Willamette River versus west of the river. The official quadrants of Portland are: North, Northwest, Northeast, Southwest, and Southeast, with downtown Portland being located in the SW quadrant. The Willamette River divides the east and west quadrants while Burnside Street, which traverses the entire city lengthwise, divides the north and south quadrants. All addresses within the city are denoted as belonging to one of these specific quadrants with the prefixes: N, NW, NE, SW or SE.
Though officially located in SW Portland, the RiverPlace, John's Landing and South Waterfront neighborhoods lie in a so-called (but unofficial) "sixth quadrant" called South Portland, where addresses rise higher from west to east toward the river. This "sixth quadrant" is roughly bounded by Naito Parkway and Barbur Boulevard to the west, Montgomery Street to the north and Nevada Street to the south. East-West addresses in this area are denoted with a leading zero (instead of a minus sign). This means 0246 SW California St. is not the same as 246 SW California St. Many mapping programs are unable to distinguish between the two.
The Pearl District in Northwest Portland, which was largely occupied by warehouses, light industry and railroad classification yards in the early-mid twentieth century, now houses upscale art galleries, restaurants, and retail stores, and is one of the wealthiest neighborhoods in the city. Areas further west of the Pearl District include neighborhoods known as Uptown and Nob Hill, as well as the Alphabet District and NW 23rd Ave., a major shopping street lined with trendy clothing boutiques and other upscale retail, mixed with cafes and restaurants.
Northeast Portland is home to the Lloyd District, Alberta Arts District, and the Hollywood District. The northernmost point of the city, known simply as North Portland, is also largely residential; it contains the St. Johns neighborhood, which is historically one of the most ethnically-diverse and poorest neighborhoods in the city.
Old Town Chinatown is located adjacent to the Pearl District in Northwest Portland, while Southwest Portland consists largely of the downtown district, made up of commercial businesses, museums, skyscrapers, and public landmarks. Southeast Portland is largely residential, and consists of the Hawthorne District, Belmont, Brooklyn, and Mount Tabor.
Climate.
Portland experiences a temperate climate with both oceanic and Mediterranean features. This climate is characterized by warm, dry summers and cool, rainy winters. The precipitation pattern is distinctly Mediterranean, with little to no rainfall occurring during the summer months and more than half of annual precipitation falling between November and February. Portland experiences a much more temperate climate than one would expect of its latitude, with snowfall and freezing temperatures uncommon. Out of the three most populated cities within the Pacific Northwest (Seattle, Vancouver and Portland, respectively) Portland has the warmest average temperature, the highest number of sunshine hours, and the fewest inches of rainfall and snowfall among the three cities. According to the Köppen climate classification, Portland falls within the dry-summer mild temperate zone ("Csb"), also referred to as a warm-summer Mediterranean climate with a USDA Plant Hardiness Zones between 8b and 9a. Other climate systems, such as the Trewartha climate classification, places it within the oceanic zone ("Do"), like much of the Pacific Northwest and Western Europe.
Summers in Portland are warm to hot, dry, and sunny. The months of June, July, August and September account for a combined of total rainfall only 12% of the of the precipitation that falls throughout the year. The warmest month is August, with an average high temperature of . Because of its inland location from the coast, as well as the protective nature of the Oregon Coast Range to its west, Portland summers are less susceptible to the moderating influence of the nearby Pacific Ocean. Consequently, Portland experiences heat waves with temperatures rising well above for days at a time, and sometimes above . On average, temperatures reach or exceed 56 days per year, of which 12 days will reach and 1.4 days will reach . The most 90 degree days ever recorded in one year is 29, which happened in 2015. The highest temperature ever recorded was , on July 30, 1965, as well as August 8 and 10, 1981. The warmest recorded overnight low was on July 28, 2009. A temperature of has been recorded in all five months from May through September.
Spring and fall can bring variable weather including warm fronts that send temperatures surging above and cold snaps that plunge daytime temperatures into the 40s °F (4–9 °C). However, consistently mild temperatures in the 50s and 60s °F (12−19 °C) are the norm with lengthy stretches of cloudy or partly cloudy days beginning in mid fall and continuing into mid spring. Rain often falls as a light drizzle for several consecutive days at a time, contributing to 155 days on average with measurable (≥) precipitation annually. Temperatures have reached as early as May 3 and as late as October 5, while has been reached as early as April 1 and as late as October 21. Severe weather, such as thunder and lightning, is uncommon and tornadoes are exceptionally rare.
Winters are cool, cloudy, and rainy. The coldest month is December with an average daily high of , although overnight lows usually remain above freezing. Evening temperatures fall to or below freezing 33 nights per year on average, but very rarely to or below . There are only 2.1 days per year where the daytime high temperature fails to rise above freezing. The lowest overnight temperature ever recorded was , on February 2, 1950 while the coldest daytime high temperature ever recorded was on December 30, 1968. The average window for freezing temperatures to potentially occur is between November 15 and March 19, allowing a growing season of 240 days.
Snowfall is uncommon with a normal yearly accumulation of , which usually falls during only two or three days per year. Portland has one of the warmest and least snowy winters of any non-Sun Belt city in the United States, with more than 25 percent of its winters receiving no snow whatsoever. The city of Portland avoids snow more frequently than its suburbs, due in part to its low elevation and urban heat island effect. Neighborhoods outside of the downtown core, especially in slightly higher elevations near the West Hills and Mount Tabor, can experience a dusting of snow while downtown receives no accumulation at all. The city has experienced a few major snow and ice storms in its past with extreme totals having reached at the airport in 1949–50 and at downtown in 1892–93.
Demographics.
The census reported the city as 76.1% White (444,254 people), 7.1% Asian (41,448), 6.3% Black or African American (36,778), 1.0% Native American (5,838), 0.5% Pacific Islander (2,919), 4.7% belonging to two or more racial groups (24,437) and 5.0% from other races (28,987). 9.4% were Hispanic or Latino, of any race (54,840). Whites not of Hispanic origin made up 72.2% of the total population.
In 1940, Portland's African-American population was approximately 2,000 and largely consisted of railroad employees and their families. During the war-time liberty ship construction boom, the need for workers drew many blacks to the city. The new influx of blacks settled in specific neighborhoods, such as the Albina district and Vanport. The May 1948 flood which destroyed Vanport eliminated the only integrated neighborhood, and an influx of blacks into the northeast quadrant of the city continued. Portland's longshoremen racial mix was described as being "lily-white" in the 1960s, when the local International Longshore and Warehouse Union declined to represent grain handlers since some were black.
At 6.3%, Portland's African American population is three times the state average. Over two thirds of Oregon's African-American residents live in Portland. As of the 2000 census, three of its high schools (Cleveland, Lincoln and Wilson) were over 70% white, reflecting the overall population, while Jefferson High School was 87% non-white. The remaining six schools have a higher number of non-whites, including blacks and Asians. Hispanic students average from 3.3% at Wilson to 31% at Roosevelt.
Portland residents identifying solely as Asian Americans account for 7.1% of the population; an additional 1.8% is partially of Asian heritage. Vietnamese Americans make up 2.2% of Portland's population, and make up the largest Asian ethnic group in the city, followed by Chinese (1.7%), Filipinos (0.6%), Japanese (0.5%), Koreans (0.4%), Laotians (0.4%), Hmong (0.2%), and Cambodians (0.1%). There is a small population of Yao people that live in Portland. Portland has two Chinatowns, with New Chinatown located along SE 82nd Avenue and bustling with Chinese supermarkets, Hong Kong style noodle houses, dim sum, and Vietnamese phở restaurants.
With about 12,000 Vietnamese residing in the city proper, Portland has one of the largest Vietnamese populations in America per capita. According to statistics there are 21,000 Pacific Islanders in Portland, making up 4% of the population.
Portland's population has been and remains predominantly white. In 1940, whites were over 98% of the city's population. In 2009, Portland had the fifth-highest percentage of white residents among the 40 largest U.S. metropolitan areas. A 2007 survey of the 40 largest cities in the U.S. concluded that Portland's urban core has the highest percentage of white residents. Some scholars have noted the Pacific Northwest as a whole is "one of the last Caucasian bastions of the United States". While Portland's diversity was historically comparable to metro Seattle and Salt Lake City, those areas grew more diverse in the late 1990s and 2000s. Portland not only remains white, but migration to Portland is disproportionately white.
The Oregon Territory banned African American settlement in 1849. In the 19th century, certain laws allowed the immigration of Chinese laborers but prohibited them from owning property or bringing their families. The early 1920s saw the rapid growth of the Ku Klux Klan, which became very influential in Oregon politics, culminating in the election of Walter M. Pierce as governor.
The largest influxes of minority populations occurred during World War II, as the African American population grew by a factor of 10 for wartime work. After World War II, the Vanport flood in 1948 displaced many African Americans. As they resettled, redlining directed the displaced workers from the wartime settlement to neighboring Albina. There and elsewhere in Portland, they experienced police hostility, lack of employment, and mortgage discrimination, leading to half the black population leaving after the war.
In the 1980s and 1990s, radical skinhead groups flourished in Portland. In 1988, Mulugeta Seraw, an Ethiopian immigrant, was killed by three skinheads. The response to his murder involved a community-driven series of rallies, campaigns, nonprofits and events designed to address Portland's racial history, leading to a city considered significantly more tolerant than in 1988 at Seraw's death.
During redevelopment of north Portland along the MAX Yellow Line, displacement of minorities occurred at a drastic rate. Out of 29 census tracts in north and northeast Portland, ten were majority nonwhite in 2000. By 2010, none of these tracts were majority nonwhite as gentrification drove the cost of living up. Today, Portland's African-American community is concentrated in the north and northeast section of the city, mainly in the King neighborhood.
Households.
As of the 2010 census, there are 583,776 people residing in the city, organized into 235,508 households. The population density is 4,375.2 people per square mile. There are 265,439 housing units at an average density of 1989.4 per square mile (1,236.3/km²). Population growth in Portland increased 10.3% between 2000 and 2010. Population growth in the Portland metropolitan area has outpaced the national average during the last decade, and this is expected to continue over the next 50 years.
Out of 223,737 households, 24.5% have children under the age of 18 living with them, 38.1% are married couples living together, 10.8% have a female householder with no husband present, and 47.1% are non-families. 34.6% of all households are made up of individuals and 9% have someone living alone who is 65 years of age or older. The average household size is 2.3 and the average family size is 3. The age distribution was 21.1% under the age of 18, 10.3% from 18 to 24, 34.7% from 25 to 44, 22.4% from 45 to 64, and 11.6% who are 65 years of age or older. The median age is 35 years. For every 100 females there are 97.8 males. For every 100 females age 18 and over, there are 95.9 males.
The median income for a household in the city is $40,146, and the median income for a family is $50,271. Males have a reported median income of $35,279 versus $29,344 reported for females. The per capita income for the city is $22,643. 13.1% of the population and 8.5% of families are below the poverty line. Out of the total population, 15.7% of those under the age of 18 and 10.4% of those 65 and older are living below the poverty line. Figures delineating the income levels based on race are not available at this time. According to the Modern Language Association, in 2010 80.92% (539,885) percent of Multnomah County residents ages 5 and over spoke English as their primary language at home. 8.10% of the population spoke Spanish (54,036), with Vietnamese speakers making up 1.94%, and Russian 1.46%.
Social demographics.
The Portland metropolitan area has historically had a significant LGBT population throughout the late twentieth and twenty-first century. In 2015, the city metro had the second highest percentage of LGBT residents in the United States with 5.4% of residents identifying as gay, lesbian, bisexual, or transgender, second only to San Francisco. In 2006, it was reported to have the seventh highest LGBT population in the country, with 8.8% of residents identifying as gay, lesbian, or bisexual, and the metro ranking fourth in the nation at 6.1%. The city held its first pride festival in 1975 on the Portland State University campus.
Portland has been cited as the least religious city in the United States, with over 42% of residents identifying as religiously "unaffiliated," according to the nonpartisan and nonprofit Public Religion Research Institute’s American Values Atlas. Of the 35.89% of the city's residents who do identify as religious, Roman Catholics make up the largest group, at 15.8%. The second highest religious group in the city are Evangelical Christians at 6.04%, with Baptists following behind at 2.5%. Latter Day Saints make up 2.3% of the city's religiously affiliated population, with Lutheran and Pentecostal following behind. 1.48% of religiously affiliated persons identified themselves as following Eastern religions, while 0.86% of the religiously affiliated population identified as Jewish, and 0.29% as Muslim.
Economy.
Portland's location is beneficial for several industries. Relatively low energy cost, accessible resources, north–south and east–west Interstates, international air terminals, large marine shipping facilities, and both west coast intercontinental railroads are all economic advantages. The US consulting firm Mercer, in a 2009 assessment "conducted to help governments and major companies place employees on international assignments", ranked Portland 42nd worldwide in quality of living; the survey factored in political stability, personal freedom, sanitation, crime, housing, the natural environment, recreation, banking facilities, availability of consumer goods, education, and public services including transportation. In 2012, the city was listed among the 10 best places to retire in the U.S. by CBS MoneyWatch.
The city's marine terminals alone handle over 13 million tons of cargo per year, and the port is home to one of the largest commercial dry docks in the country. The Port of Portland is the third largest export tonnage port on the west coast of the U.S., and being located about upriver, it is the largest fresh-water port. The city of Portland is largest shipper of wheat in the United States, and is the second largest port for wheat in the world.
The steel industry's history in Portland predates World War II. By the 1950s, the steel industry became the city's number one industry for employment. The steel industry thrives in the region, with Schnitzer Steel Industries, a prominent steel company, shipping a record 1.15 billion tons of scrap metal to Asia during 2003. Other heavy industry companies include ESCO Corporation and Oregon Steel Mills.
Technology is a major component of the city's economy, with more than 1,200 technology companies existing within the metro. This high density of technology companies has led to the nickname Silicon Forest being used to describe the Portland area, a reference to the abundance of trees in the region and to the Silicon Valley region in Northern California. The area also hosts facilities for software companies and online startup companies, some supported by local seed funding organizations and business incubators. Computer components manufacturer Intel is the Portland area's largest employer, providing jobs for more than 15,000 people, with several campuses to the west of central Portland in the city of Hillsboro.
The Portland metro area has become a business cluster for athletic and footwear manufacturers. The area is home to the global, North American or US headquarters of Nike, Adidas, Columbia Sportswear, LaCrosse Footwear, Dr. Martens, Li-Ning, Keen, and Hi-Tec Sports. While headquartered elsewhere, Merrell, Amer Sports and Under Armour have design studios and local offices in the Portland area. Portland-based Precision Castparts is one of two Fortune 500 companies headquartered in Oregon, the other being Nike. Other notable Portland-based companies include film animation studio Laika; commercial vehicle manufacturer Daimler Trucks North America; advertising firm Wieden+Kennedy; bankers Umpqua Holdings; and retailers Fred Meyer, New Seasons and Storables.
Culture.
Music, film, and performing arts.
Portland is home to famous bands such as The Kingsmen and Paul Revere & the Raiders, both famous for their association with the song "Louie Louie" (1963). Other widely known musical groups include The Dandy Warhols, Quarterflash, Everclear, Pink Martini, Sleater-Kinney, The Shins, Blitzen Trapper, The Decemberists, and the late Elliott Smith. In the 1980s, the city was home to a burgeoning punk scene, which included bands such as the Wipers and Dead Moon. The city's now-demolished Satyricon nightclub was a punk venue that is notorious for being the place where Nirvana frontman Kurt Cobain first encountered future wife and Hole frontwoman Courtney Love in 1990. Love was a native of Portland, and started several bands there with Kat Bjelland, later of Babes in Toyland. Multi-Grammy award winning jazz artist Esperanza Spalding is from Portland, and performed with the Chamber Music Society of Oregon at a young age.
In 2013, "The Guardian" named the city's music scene as one of the "most vibrant" in the United States. According to the "New York Times", the dozens of karaoke bars in Portland make it not just "the capital of karaoke" in the United States, but "one of the most exciting music scenes in America. Portland also has a range of classical performing arts institutions which include the Oregon Symphony, Portland Opera and the Portland Youth Philharmonic. The city is also home to several theaters and performing arts institutions, including the Oregon Ballet Theatre, Northwest Children's Theatre, Portland Center Stage, Artists Repertory Theatre, Miracle Theatre, and Tears of Joy Theatre.
A wide range of films have been shot in Portland, from various independent features to major big-budget productions (see "List of films shot in Oregon" for a complete list). Director Gus Van Sant has notably set and shot many of his films in the city. The IFC sketch comedy series "Portlandia", starring Fred Armisen and Sleater-Kinney member Carrie Brownstein, shoots on location in Portland, satirizing the city as a hub of liberal politics, organic food, alternative lifestyles and anti-establishment attitudes. MTV's long-time running reality show, "The Real World", was shot in Portland for the show's 29th season. "" premiered on MTV on March 27, 2013 and was filmed in a loft in the Pearl District. The show featured the cast members taking part in several Portland activities, such as hiking in the Columbia River Gorge. The cast members worked at a local frozen yogurt shop and the local Pizza Schmizza. Other TV shows which have shot in the city include "Leverage", "The Librarians", "Under Suspicion", "Grimm", "Nowhere Man" and "Life Unexpected".
An unusual feature of Portland entertainment is the large number of movie theaters serving beer, often with second-run or revival films. Notable examples of these "brew and view" theaters includes The Bagdad Theater and Pub, a former vaudeville theater built in 1927 by Universal Studios; Cinema 21, and the Laurelhurst Theater, in operation since 1923. Portland hosts the world's longest-running HP Lovecraft Film Festival at the Hollywood Theatre.
Museums.
Portland museums offer a variety of educational programs. The Oregon Museum of Science and Industry (OMSI) includes many hands-on activities for adults and children. It consists of five main halls, most of which consist of smaller laboratories: Earth Science Hall, Life Science Hall, Turbine Hall, Science Playground, and Featured Exhibit Hall. The Featured Exhibit Hall has a new exhibit every few months. The laboratories are Chemistry, Physics, Technology, Life, Paleontology, and Watershed. OMSI has many other unique attractions, such as the submarine, used in the film "The Hunt for Red October", the OMNIMAX Dome Theater, and OMSI's Kendall Planetarium.
The OMNIMAX Dome Theater is a variant of the IMAX motion picture format, where the movie is projected onto a domed projection surface. The projection surface at OMSI's OMNIMAX Dome Theater is . The OMNIMAX Theater uses the largest frame in the motion picture industry and the frames are ten times the size of the standard 35mm film. OMSI's Kendall Planetarium is the largest and most technologically advanced planetarium in the Pacific Northwest. OMSI is located at 1945 SE Water Ave. OMSI is built right up next to the river and is also conveniently located near the entrance to the Springwater Corridor and Eastbank Esplanade pedestrian and bike trails.
The Portland Art Museum owns the city's largest art collection and presents a variety of touring exhibitions each year and with the recent addition of the Modern and Contemporary Art wing it became one of the United States' 25 largest museums.
The Oregon History Museum was founded in 1898. The Oregon History Museum has a variety of books, film, pictures, artifacts, and maps dating back throughout Oregon's history. The Oregon History Museum has one of the most extensive collections of state history materials in the USA.
The Portland Children's Museum is a museum specifically geared for early childhood development. This museum has many topics, and many of their exhibits rotate, to keep the information fresh. The Portland Children's Museum also supports a small charter school for elementary children.
Cuisine and breweries.
Portland has been named the best city in the world for street food by several publications, including the "U.S. News & World Report" and CNN. Food carts are extremely popular within the city, with over 600 licensed carts, making Portland one of the most robust street food scenes in North America.
Portland has the most total breweries and independent microbreweries of any city in the world, with 58 active breweries within city limits, and 70+ within the surrounding metro area. The city receives frequent acclaim as the best beer city in the United States and is consistently ranked as one of the top-5 beer destinations in the world. Portland has played a prominent role in the microbrewery revolution in the U.S. and is nicknamed "Beertown" and "Beervana" as a result. Portland's modern microbrewery boom dates to the 1980s when Oregon state law was changed to allow the consumption of beer on brewery premises. Brewery innovation is further supported by the abundance of beer ingredients produced locally, including two-row barley, over a dozen varieties of local Cascade hops, as well as pure mountain water from the Bull Run Watershed.
The beer culture is partially responsible for CNBC naming Portland the best city for happy hour in the U.S. The McMenamin brothers alone have over thirty brewpubs, distilleries, and wineries scattered throughout the metropolitan area, several in renovated cinemas and other historically significant buildings otherwise destined for demolition. Other notable Portland brewers include Widmer Brothers, BridgePort, Hair of the Dog, and Hopworks Urban Brewery. In 1999, author Michael "Beerhunter" Jackson called Portland a candidate for the "beer capital of the world" because the city boasted more breweries than Cologne, Germany.
Portland is also known as a leader in specialty coffee. The city is home to Stumptown Coffee Roasters as well as dozens of other micro-roasteries and cafes.
Portland has an emerging restaurant scene nationally, and among three nominees, was recognized by the Food Network Awards as their "Delicious Destination of the Year: A rising city with a fast-growing food scene" for 2007. In 2014, "The Washington Post" called Portland the fourth best city for food in the United States. "Travel + Leisure" ranked Portland's food and bar scene #5 in the nation in 2012. The city is also known for being among the most vegetarian-friendly cities in America.
Recreation.
Portland is home to a diverse array of artists and arts organizations, and was named in 2006 by "American Style" magazine as the tenth best Big City Arts Destination in the country. Portland's public art is managed by the Regional Arts & Culture Council. Art galleries abound downtown and in the Pearl District, as well as in the Alberta Arts District and other neighborhoods throughout the city.
"Portlandia", a statue on the west side of the Portland Building, is the second-largest hammered-copper statue in the U.S. (after the Statue of Liberty). Portland's public art is managed by the Regional Arts & Culture Council.
Powell's City of Books claims to be the largest independent new and used bookstore in the world, occupying a multistory building on an entire city block in the Pearl District. In 2010, Powell's Technical Books was relocated to Powell's Books Bldg. 2 across the street from the flagship store. In 2012, the city was named by "Travel & Leisure" magazine as the number 1 best city in the country for book stores.
The Portland Rose Festival takes place annually in June and includes two parades, dragon boat races, carnival rides at Tom McCall Waterfront park, and dozens of other events. The city is home to the Rosebud and Thorn Pageant, started in 1975 and modeled after the Imperial Sovereign Rose Court of Oregon. Washington Park, in the West Hills, is home to some of Portland's most popular recreational sites, including the Oregon Zoo, the World Forestry Center, and the Hoyt Arboretum. Oaks Amusement Park, located in the Sellwood district of Southeast Portland, is the city's only amusement park, and is also one of the longest-running amusement parks in the country. It has been in operation since 1905, and was known as the "Coney Island of the Northwest" upon its opening.
Portland hosts a number of festivals throughout the year in celebration of beer and brewing, including the Oregon Brewers Festival, held in Tom McCall Waterfront Park. Held each summer during the last full weekend of July, it is the largest outdoor craft beer festival in North America with over 70,000 attendees in 2008. Other major beer festivals throughout the calendar year include the Spring Beer and Wine Festival in April, the North American Organic Brewers Festival in June, the Portland International Beerfest in July, and the Holiday Ale Festival in December. Pioneer Courthouse Square also holds an annual Christmas tree lighting on the Friday after Thanksgiving, and the tree remains in the square for five to six weeks through the holiday season. The residents of Peacock Lane, a street in the Sunnyside neighborhood, annually decorate their houses for the Christmas season and provide rides through the neighborhood in horse drawn carriage, a tradition established in the 1920s.
Sustainability.
Portland is often awarded "Greenest City in America" and "most green cities" designations. "Popular Science" awarded Portland the title of the Greenest City in America in 2008, and "Grist" magazine listed it in 2007 as the second greenest city in the world. The city became a pioneer of state-directed metropolitan planning, a program which was instated statewide in 1969 to compact the urban growth boundaries of the city.
Sports.
Portland is home to two major league sports franchises: the Portland Trail Blazers of the NBA and the Portland Timbers of Major League Soccer. The Portland Thorns of the National Women's Soccer League also play in Portland. In 2015, the Timbers won the MLS Cup, which was the first professional sports championship for a team from Portland since the Trail Blazers won the NBA championship in 1977. Despite being the 19th most populated metro area in the United States, Portland contains only one franchise from the NFL, NBA, NHL, or MLB, making it America's most populated metro area with that distinction. The city has been frequently rumored to receive an additional franchise although efforts to acquire a team have failed due to stadium funding issues.
Portland sports fans are characterized by their passionate support. The Trail Blazers sold out every home game between 1977 and 1995, a span of 814 consecutive games, the second longest streak in American sports history. The Timbers joined MLS in 2011 and have sold out every home match since joining the league, a streak that has now reached 70+ matches. The Timbers season ticket waiting list has reached 10,000+, the longest waiting list in MLS. In 2015, they became the first team in the Northwest to win the MLS Cup. Player Diego Valeri marked a new record for fastest goal in MLS Cup history at 27-seconds into the game.
Two rival universities exist within Portland city limits: the University of Portland Pilots and the Portland State University Vikings, both of whom field teams in popular spectator sports including soccer, baseball, basketball, and football. Additionally, the University of Oregon Ducks and the Oregon State University Beavers both receive substantial attention and support from many Portland residents, despite their campuses being located 110 and 84 miles from the city, respectively.
Running is a popular activity in Portland and every year the city hosts the Portland Marathon as well as parts of the Hood to Coast Relay, the world's largest long-distance relay race (by number of participants). Portland serves as the center to two elite running groups, the Nike Oregon Project and Oregon Track Club, and is the current residence of several elite runners including British 2012 Olympic 10,000m and 5,000m champion Mo Farah, American record holder at 10,000m Galen Rupp, and 2008 American Olympic bronze medalist at 10,000m Shalane Flanagan.
Portland also has one of the most active bicycle scenes in the United States and has become an elite bicycle racing destination. The Oregon Bicycle Racing Association supports hundreds of official bicycling events every year. Weekly events at Alpenrose Velodrome and Portland International Raceway allow for racing nearly every night of the week from March through September. Cyclocross races, such as the Cross Crusade, can attract over 1,000 riders and spectators.
Skiing and snowboarding are also highly popular, with a number of nearby resorts on Mount Hood, including year-round Timberline Lodge.
Parks and gardens.
Parks and greenspace planning date back to John Charles Olmsted's 1903 "Report to the Portland Park Board". In 1995, voters in the Portland metropolitan region passed a regional bond measure to acquire valuable natural areas for fish, wildlife, and people. Ten years later, more than of ecologically valuable natural areas had been purchased and permanently protected from development.
Portland is one of only four cities in the U.S. with extinct volcanoes within its boundaries (along with Pilot Butte in Bend, Oregon, Jackson Volcano in Jackson, Mississippi, and Diamond Head in Honolulu, Hawaii). Mount Tabor Park is known for its scenic views and historic reservoirs.
Forest Park is the largest wilderness park within city limits in the United States, covering more than . Portland is also home to Mill Ends Park, the world's smallest park (a two-foot-diameter circle, the park's area is only about 0.3 m2). Washington Park is just west of downtown, and is home to the Oregon Zoo, the Portland Japanese Garden, and the International Rose Test Garden. Portland is also home to Lan Su Chinese Garden (formerly the Portland Classical Chinese Garden), an authentic representation of a Suzhou-style walled garden.
Portland's downtown features two groups of contiguous city blocks dedicated for park space: the North and South Park Blocks. The Tom McCall Waterfront Park was built in 1974 along the length of the downtown waterfront after Harbor Drive was removed; it now hosts large events throughout the year. The nearby historically significant Burnside Skatepark and five indoor skateparks give Portland a reputation as possibly "the most skateboard-friendly town in America."
Tryon Creek State Natural Area is one of three Oregon State Parks in Portland and the most popular; its creek has a run of steelhead. The other two State Parks are Willamette Stone State Heritage Site located in the West Hills and the Government Island State Recreation Area located in the Columbia River near Portland International Airport.
Portland's city park system has been proclaimed one of the best in America. In its 2013 ParkScore ranking, The Trust for Public Land reported that Portland had the 7th best park system among the 50 most populous U.S. cities. ParkScore ranks city park systems by a formula that analyzes the city's median park size, park acres as percent of city area, the percent of city residents within a half-mile of a park, spending of park services per resident, and the number of playgrounds per 10,000 residents. The survey revealed that 80% of Portlanders live within a half-mile to a park and over 16% of Portland's city area is parkland.
Law and government.
The city of Portland is governed by the Portland City Council, which includes the Mayor, four Commissioners, and an auditor. Each is elected citywide to serve a four-year term. The auditor provides checks and balances in the commission form of government and accountability for the use of public resources. In addition, the auditor provides access to information and reports on various matters of city government.
The city's Office of Neighborhood Involvement serves as a conduit between city government and Portland's 95 officially recognized neighborhoods. Each neighborhood is represented by a volunteer-based neighborhood association which serves as a liaison between residents of the neighborhood and the city government. The city provides funding to neighborhood associations through seven district coalitions, each of which is a geographical grouping of several neighborhood associations. Most (but not all) neighborhood associations belong to one of these district coalitions.
Portland and its surrounding metropolitan area are served by Metro, the United States' only directly elected metropolitan planning organization. Metro's charter gives it responsibility for land use and transportation planning, solid waste management, and map development. Metro also owns and operates the Oregon Convention Center, Oregon Zoo, Portland Center for the Performing Arts, and Portland Metropolitan Exposition Center.
The Multnomah County government provides many services to the Portland area, as do Washington and Clackamas counties to the west and south.
Politics.
Portland is a territorial charter city, and strongly favors the Democratic Party. All city offices are technically non-partisan.
Portland's delegation to the Oregon Legislative Assembly is entirely Democratic. In the current 76th Oregon Legislative Assembly, which first convened in 2011, four state Senators represent Portland in the state Senate: Diane Rosenbaum (District 21), Chip Shields (District 22), Jackie Dingfelder (District 23), and Rod Monroe (District 24). Portland sends six Representatives to the state House of Representatives: Jules Bailey (District 42), Lew Frederick (District 43), Tina Kotek (District 44), Michael Dembrow (District 45), Alissa Keny-Guyer (District 46), and Jefferson Smith (District 47).
Federally, Portland is split among three congressional districts. Most of the city is in the 3rd District, represented by Earl Blumenauer, who served on the city council from 1986 until his election to Congress in 1996. Most of the city west of the Willamette River is part of the 1st District, represented by Suzanne Bonamici. A small portion of southwestern Portland is in the 5th District, represented by Kurt Schrader. All three are Democrats; a Republican has not represented a significant portion of Portland in the U.S. House of Representatives since 1975. Both of Oregon's senators, Ron Wyden and Jeff Merkley, are from Portland and are also both Democrats.
In the 2008 presidential election, Democratic candidate Barack Obama easily carried Portland, winning 245,464 votes from city residents to 50,614 for his Republican rival, John McCain. In the 2012 presidential election, Democratic candidate Barack Obama again easily carried Portland, winning 256,925 votes from Multnomah county residents to 70,958 for his Republican rival, Mitt Romney.
Sam Adams, the former mayor of Portland, became the city's first openly gay mayor in 2009. In 2004, 59.7 percent of Multnomah County voters cast ballots against Oregon Ballot Measure 36, which amended the Oregon Constitution to prohibit recognition of same-sex marriages. The measure passed with 56.6% of the statewide vote. Multnomah County is one of two counties where a majority voted against the initiative; the other is Benton County, which includes Corvallis, home of Oregon State University. On April 28, 2005, Portland became the only city in the nation to withdraw from a Joint Terrorism Task Force. As of February 19, 2015, the Portland city council approved permanently staffing the JTTF with two of its city's police officers.
Planning and development.
The city consulted with urban planners as far back as 1903. Development of Washington Park and one of the country's finest greenways, the 40 Mile Loop, which interconnects many of the city's parks, began.
Portland is often cited as an example of a city with strong land use planning controls. This is largely the result of statewide land conservation policies adopted in 1973 under Governor Tom McCall, in particular the requirement for an urban growth boundary (UGB) for every city and metropolitan area. The opposite extreme, a city with few or no controls, is typically illustrated by Houston, Texas.
Portland's urban growth boundary, adopted in 1979, separates urban areas (where high-density development is encouraged and focused) from traditional farm land (where restrictions on non-agricultural development are very strict). This was atypical in an era when automobile use led many areas to neglect their core cities in favor of development along interstate highways, in suburbs, and satellite cities.
The original state rules included a provision for expanding urban growth boundaries, but critics felt this wasn't being accomplished. In 1995, the State passed a law requiring cities to expand UGBs to provide enough undeveloped land for a 20-year supply of future housing at projected growth levels.
Oregon's 1973 "urban growth boundary" law limits the boundaries for large-scale development in each metropolitan area in Oregon. This limits access to utilities such as sewage, water and telecommunications, as well as coverage by fire, police and schools. Originally this law mandated that the city must maintain enough land within the boundary to provide an estimated 20 years of growth; however, in 2007 the legislature altered the law to require the maintenance of an estimated 50 years of growth within the boundary, as well as the protection of accompanying farm and rural lands.
The growth boundary, along with efforts of the PDC to create economic development zones, has led to the development of a large portion of downtown, a large number of mid- and high-rise developments, and an overall increase in housing and business density.
The Portland Development Commission is a semi-public agency that plays a major role in downtown development; it was created by city voters in 1958 to serve as the city's urban renewal agency. It provides housing and economic development programs within the city, and works behind the scenes with major local developers to create large projects.
In the early 1960s, the PDC led the razing of a large Italian-Jewish neighborhood downtown, bounded roughly by I-405, the Willamette River, 4th Avenue and Market street.
Mayor Neil Goldschmidt took office in 1972 as a proponent of bringing housing and the associated vitality back to the downtown area, which was seen as emptying out after 5 pm. The effort has had dramatic effects in the 30 years since, with many thousands of new housing units clustered in three areas: north of Portland State University (between I-405, SW Broadway, and SW Taylor St.); the RiverPlace development along the waterfront under the Marquam (I-5) bridge; and most notably in the Pearl District (between I-405, Burnside St., NW Northrup St., and NW 9th Ave.).
The Urban Greenspaces Institute, housed in Portland State University Geography Department's Center for Mapping Research, promotes better integration of the built and natural environments. The institute works on urban park, trail, and natural areas planning issues, both at the local and regional levels. In October 2009, the Portland City Council unanimously adopted a climate action plan that will cut the city's greenhouse gas emissions to 80% below 1990 levels by 2050.
According to "Grist" magazine, Portland is the second most eco-friendly or "green" city in the world trailing only Reykjavík, Iceland. In 2010, Move, Inc. placed Portland in its "top 10 greenest cities" list.
As of 2012 Portland was the largest city in the United States that did not add fluoride to its public water supply, and fluoridation has historically been a subject of controversy in the city. Portland voters have four times voted against fluoridation, in 1956, 1962, 1980 (repealing a 1978 vote in favor), and 2013. Most recently, in 2012 the city council, responding to advocacy from public health organizations and others, voted unanimously to begin fluoridation by 2014. Fluoridation opponents forced a public vote on the issue, and on May 21, 2013, city voters again rejected fluoridation.
Free speech.
Strong free speech protections of the Oregon Constitution upheld by the Oregon Supreme Court in "State v. Henry", 732 P.2d 9 (Or. 1987) specifically found that full nudity and lap dances in strip clubs are protected speech. Portland has the highest number of strip clubs per-capita in a city in the United States, and Oregon ranks as the highest state for per-capita strip clubs. Portland has been titled as "Pornland" for its strip clubs, erotic massage parlors, and high rate of child sex trafficking. The term was heavily used in 2010, but the term was referenced by Chuck Palahniuk in 2003.
A judge dismissed charges against a nude bicyclist in November 2008 on the grounds that the city's annual World Naked Bike Ride was "a well-established tradition" in Portland. The first instance occurred sometime around 1999 and had less than 7 participants. Participants would 'purchase' a bike from a local chain department store and then return it the next morning. It used to take place at midnight and lasted until the participants were stopped/arrested. The prankster aspect of it came in when the arresting officers didn't want to touch the naked cyclists in order to arrest them. The 2009 Naked Bike Ride occurred without significant incident. City police managed traffic intersections. There were an estimated 3,000 to 5,000 participants. In June 2010, Portland's World Naked Bike Ride had an estimated 13,000 people.
A state law prohibiting publicly insulting a person in a way likely to provoke a violent response was tested in Portland and struck down unanimously by the State Supreme Court as violating protected free speech and being overly broad.
Crime.
According to the Federal Bureau of Investigation's Uniform Crime Report in 2009, Portland ranked 53rd in violent crime out of the top 75 U.S. cities with a population greater than 250,000. The murder rate in Portland in 2013 averaged 2.3 murders per 100,000 people per year, which was lower than the national average. In October 2009, "Forbes" magazine rated Portland as the third safest city in America.
Below is a sortable table containing violent crime data from each Portland neighborhood during the calendar year of 2014.
Education.
Primary and secondary education.
Portland is served by six public school districts and many private schools. Portland Public Schools is the largest school district, operating 85 public schools. Grant High School located in the Grant Park neighborhood has the largest enrollment of any public high school in the city. Other high schools include Benson Polytechnic High School, Cleveland High School, and Roosevelt High School. Established in 1869, Lincoln High School is the city's oldest public education institution, and is one of two of the oldest high schools west of the Mississippi River.
Former public schools in the city included Washington High School, which operated from 1906 until 1981, as well as Jackson High School, which also closed the same year.
Private schools in the area include The Northwest Academy, Rosemary Anderson High School, Portland Adventist Academy, Portland Lutheran School, the Portland Waldorf School, and Trinity Academy.
The city and surrounding metropolitan area is also home to a large number of Roman Catholic-affiliated private schools, including St. Mary's Academy, an all-girls school; De La Salle North Catholic High School; the co-educational Jesuit High School; La Salle High School; and Central Catholic High School, the only archdiocesan high school in the Roman Catholic Archdiocese of Portland.
Higher education.
Portland State University, located in downtown Portland, has the second largest enrollment rate of any university in the state (after Oregon State University), with a student body of nearly 30,000. It has been named among the top fifteen percentile of American universities by The Princeton Review for undergraduate education, and has been internationally recognized for its degrees in Masters of Business Administration and urban planning. The city is also home to the Oregon Health & Science University, as well as Portland Community College.
Notable private universities include the University of Portland, a Roman Catholic university affiliated with the Congregation of Holy Cross; Reed College, a rigorous liberal arts college, ranked by "Forbes" as the 52nd best college in the country; and Lewis & Clark College.
Other institutions of higher learning within the city's borders are:
Media.
"The Oregonian" is the only daily general-interest newspaper serving Portland. It also circulates throughout the state and in Clark County, Washington.
Smaller local newspapers, distributed free of charge in newspaper boxes and at venues around the city, include the "Portland Tribune" (general-interest paper published on Tuesdays and Thursdays), "Willamette Week" (general-interest alternative weekly published on Wednesdays), "The Portland Mercury" (another alt-weekly, targeted at younger urban readers published on Thursdays), "The Asian Reporter" (a weekly covering Asian news, both international and local) and The Skanner (a weekly African-American newspaper covering both local and national news).
Portland Indymedia is one of the oldest and largest Independent Media Centers. The "Portland Alliance", a largely anti-authoritarian progressive monthly, is the largest radical print paper in the city. "Just Out", published in Portland twice monthly until the end of 2011, was the region's foremost LGBT publication. A biweekly paper, "Street Roots", is also sold within the city by members of the homeless community.
"The Portland Business Journal", a weekly, covers business-related news, as does "The Daily Journal of Commerce". "Portland Monthly" is a monthly news and culture magazine. "The Bee", over 105 years old, is another neighborhood newspaper serving the inner southeast neighborhoods.
Infrastructure.
Healthcare.
Legacy Health, a non-profit healthcare system in Portland, operates multiple facilities in the city and surrounding suburbs. These include Legacy Emanuel, founded in 1912, located in Northeast Portland; and Legacy Good Samaritan, founded in 1875, and located in Northwest Portland. Randall's Children's Hospital operates at the Legacy Emanuel Campus. Good Samaritan has centers for breast health, cancer, and stroke, and is home to the Legacy Devers Eye Institute, the Legacy Obesity and Diabetes Institute, the Legacy Diabetes and Endocrinology Center, the Legacy Rehabilitation Clinic of Oregon, and the Linfield-Good Samaritan School of Nursing.
The Catholic-affiliated Providence Health & Services operates Providence Portland Medical Center in the North Tabor neighborhood of the city. Oregon Health & Science University is a university hospital formed in 1974. The Veterans Affairs Medical Center operates adjacent to the Oregon Health & Science University main campus. Adventist Medical Center also serves the city. Shriners Hospital for Children is a small children's hospital established in 1923.
Transportation.
The Portland metropolitan area has transportation services common to major US cities, though Oregon's emphasis on proactive land-use planning and transit-oriented development within the urban growth boundary means that commuters have multiple well-developed options. In 2014, "Travel + Leisure" magazine rated Portland as the #1 most pedestrian and transit-friendly city in the United States. A 2011 study by Walk Score ranked Portland 12th most walkable of fifty largest US cities.
In 2008, 12.6% of all commutes in Portland were on public transit. TriMet operates most of the region's buses and the MAX (short for Metropolitan Area Express) light rail system, which connects the city and suburbs. The 1986-opened MAX system has expanded to five lines, with the latest being the Orange Line to Milwaukie, in service as of September 2015. WES Commuter Rail opened in February 2009 in Portland's western suburbs, linking Beaverton and Wilsonville.
The city-owned Portland Streetcar serves two routes in the Central City – downtown and adjacent districts. The first line, which opened in 2001 and was extended in 2005–2007, operates from the South Waterfront District through Portland State University and north through the West End of downtown, to shopping areas and dense residential districts north and northwest of downtown. The second line opened in 2012 and added of tracks on the east side of the Willamette River and across the Broadway Bridge to a connection with the original line. The east-side line completed a loop to the tracks on the west side of the river upon completion of the new Tilikum Crossing in 2015, and, in anticipation of that, had already been named the Central Loop line in 2012. However, it was renamed the Loop Service, with an A Loop (clockwise) and B Loop (counterclockwise), when it became a complete loop with the opening of the Tilikum Crossing bridge.
Fifth and Sixth avenues within downtown comprise the Portland Transit Mall, two streets devoted primarily to bus and light rail traffic with limited automobile access. Opened in 1977 for buses, the transit mall was renovated and rebuilt in 2007–09, with light rail added. Starting in 1975 and lasting nearly four decades, all transit service within downtown Portland was free, the area being known by TriMet as Fareless Square, but a need for minor budget cuts and funding needed for expansion prompted the agency to limit free rides to rail service only in 2010, and subsequently to discontinue the fare-free zone entirely in 2012.
TriMet provides real-time tracking of buses and trains with its TransitTracker, and makes the data available to software developers so they can create customized tools of their own.
I-5 connects Portland with the Willamette Valley, Southern Oregon, and California to the south and with Washington to the north. I-405 forms a loop with I-5 around the central downtown area of the city and I-205 is a loop freeway route on the east side which connects to the Portland International Airport. US 26 supports commuting within the metro area and continues to the Pacific Ocean westward and Mount Hood and Central Oregon eastward. US 30 has a main, bypass, and business route through the city extending to Astoria to the west; through Gresham, Oregon, and the eastern exurbs, and connects to I-84, traveling towards Boise, Idaho. Portland ranks 13th in traffic congestion of all American cities, and is 16th among all North American cities.
Portland's main airport is Portland International Airport, located about 20 minutes by car (40 minutes by MAX) northeast of downtown. In addition Portland is home to Oregon's only public use heliport, the Portland Downtown Heliport.
Amtrak, the national passenger rail system, provides service to Portland at Union Station on three routes. Long-haul train routes include the "Coast Starlight" (with service from Los Angeles to Seattle) and the "Empire Builder" (with service from Seattle/Portland to Chicago.) The "Amtrak Cascades" state-supported trains operate between Vancouver, British Columbia and Eugene, Oregon, and serve Portland several times daily. The city is also served by Greyhound Lines intercity bus service which operates BoltBus an express bus service. The bus depot is about one block from the Portland Union Station. The city's first airport was the Swan Island Municipal Airport which was closed in the 1940s.
Portland is the only city in the United States that owns operating mainline steam locomotives, donated to the city in 1958 by the railroads that ran them. Spokane, Portland & Seattle 700 and the world-famous Southern Pacific 4449 can be seen several times a year pulling a special excursion train, either locally or on an extended trip. The "Holiday Express", pulled over the tracks of the Oregon Pacific Railroad on weekends in December, has become a Portland tradition over its several years running. These trains and others are operated by volunteers of the Oregon Rail Heritage Foundation, an amalgamation of rail preservation groups which collaborated on the finance and construction of the Oregon Rail Heritage Center, a permanent and publicly accessible home for the locomotives, which opened in 2012 adjacent to OMSI.
In Portland, cycling is a significant mode of transportation. As the city has been particularly supportive of urban bicycling it now ranks highly among the most bicycle-friendly cities in the world.
Approximately 8% of commuters bike to work, the highest proportion of any major U.S. city and about 10 times the national average.
By July 2016 through a 4-0 city council vote, Portland will have a bike share program running with 600 bikes. The new bikes will be provided by Social Bicycles, and will be operated by Motivate. For its achievements in promoting cycling as an everyday means of transportation, Portland has been recognized by the League of American Bicyclists and other cycling organizations for its network of on-street bicycling facilities and other bicycle-friendly services, being one of only three US cities to have earned a Platinum-level rating.
Car sharing through Zipcar, Car2Go, Getaround, and Uhaul Car Share is available to residents of the city and some inner suburbs. Portland has a commuter aerial cableway, the Portland Aerial Tram, which connects the South Waterfront district on the Willamette River to the Oregon Health & Science University campus on Marquam Hill above.
Sister cities.
Portland has ten sister cities and one "friendship city" (Utrecht); each city is required to maintain long term involvement and participation:
External links.
Portland websites that are also wikis

</doc>
<doc id="23506" url="https://en.wikipedia.org/wiki?curid=23506" title="Pan and scan">
Pan and scan

Pan and scan is a method of adjusting widescreen film images so that they can be shown within the proportions of a standard definition 4:3 aspect ratio television screen, often cropping off the sides of the original widescreen image to focus on the composition's most important aspects. Some film directors and film enthusiasts disapprove of pan and scan cropping, because it can remove up to 45% of the original image on 2.35:1 films or up to 53% on earlier 2.55:1 presentations, changing the director or cinematographer's original vision and intentions. The worst examples remove up to 75% of the original picture on such aspect ratios as 2.75:1 or even 3:1 in epics such as "Ben-Hur", "King of Kings" or "Lawrence of Arabia".
The vertical equivalent is known as "tilt and scan" or "reverse pan and scan". The method was most common in the days of VHS, before widescreen home media such as DVD and Blu-ray.
Center cut is similar with the difference as the name suggests that it is simply a direct cut of the material from the center of the image with no horizontal panning or vertical tilting involved. This method doesn't require the permission or availability of the film maker or director to identify the most important part of each frame. Most video displays have three options for 16:9 widescreen frame formatting, which are either center cut, letterbox or full frame. The first two options are reliant on the video stream's aspect ratio flag being set correctly.
Background.
For the first several decades of television broadcasting, sets displayed images with a 4:3 aspect ratio in which the width is 1.33 times the height—similar to most theatrical films prior to 1960. This was fine for pre-1953 films such as "The Wizard of Oz" or "Casablanca". Meanwhile, in order to compete with television and lure audiences away from their sets, producers of theatrical motion pictures began to use "widescreen" formats such as CinemaScope and Todd-AO in the early to mid-1950s, which enable more panoramic vistas and present other compositional opportunities. Films with these formats might be twice as wide as a TV screen when televised. To present a widescreen movie on such a television requires one of two techniques to accommodate this difference: One is "letterboxing", which preserves the original theatrical aspect ratio, but is not as tall as a standard television screen, leaving black bars at the top and bottom of the screen; the other more common technique is to "pan and scan", filling the full height of the screen, but cropping it horizontally. Pan and scan cuts out as much as half of the image.
In the 1990s (before Blu-ray Disc or HDTV), when so-called televisions offered a wider 16:9 aspect ratio (1.78 times the height instead of 1.33), they allowed films made at 1.66:1 and 1.85:1 to fill most or all of the screen, with only small letterboxing or cropping required. DVD packaging began to use the expression, "16:9 – Enhanced for Widescreen TVs."
However, films shot at aspect ratios of 2.20:1, 2.35:1, 2.39:1, 2.55:1, and especially 2.76:1 ("Ben-Hur" for example) might still be problematic when displayed on televisions of any type. But when the DVD is "anamorphically enhanced for widescreen", or the film is telecast on a high-definition channel seen on a widescreen TV, the black spaces are smaller, and the effect is still much like watching a film on a theatrical wide screen. Though 16:9 (and occasionally 16:10, mostly for computers and tablets) remain standard as of 2012, wider-screen consumer TVs in 21:9 have been released to the market by multiple brands.
Techniques.
During the "pan and scan" process, an editor selects the parts of the original filmed composition that seem to be the focus of the shot and makes sure that these are copied (i.e. "scanned"). When the important action shifts to a new position in the frame, the operator moves the scanner to follow it, creating the effect of a "pan" shot. In a scene in which the focus does not gradually shift from one horizontal position to another—such as actors at each extreme engaging in rapid conversation with each other—the editor may choose to "cut" from one to the other rather than rapidly panning back and forth. If the actors are closer together on the screen, the editor may pan slightly, alternately cropping one or the other partially. This method allows the maximum resolution of the image, since it uses all the available vertical video scan lines—which is especially important for NTSC televisions, having a rather low number of lines available. It also gives a full-screen image on a traditional television set; hence pan-and-scan versions of films on videotape or DVD are often known as "". However, it also has several drawbacks. Some visual information is necessarily cropped out. It can also change a shot in which the camera was originally stationary to one in which it is frequently panning, or change a single continuous shot into one with frequent cuts. In a shot which was originally panned to show something new, or one in which something enters the shot from off-camera, it changes the timing of these appearances to the audience. As an example, in the film "Oliver!", made in Panavision, the criminal Bill Sikes commits a murder. The murder takes place mostly offscreen, behind a staircase wall, and Oliver is a witness to it. As Sikes steps back from behind the wall, we see Oliver from the back watching him in terror. In the pan-and-scan version of the film, we see Oliver's reaction as the murder is being committed, but not when Sikes steps backward from the wall having done it.
Another example, as Martin Scorsese has noted on television, is in the 1959 film "Ben-Hur". During the chariot race, Ben-Hur drives four horses, but in the pan and scan version of the film, the audience sometimes sees only two.
Yet another instance of this occurs in early TV versions of the film "MASH". In the scene where 'Hawkeye' Pierce (Donald Sutherland) arrives at Kimpo Air Base, the formatting was set to where the left half of the screen was occupied by a building, on which appeared quotes of Korean War generals, and from behind which Hawkeye enters the scene. However, in the pan and scan version, the camera pans all of the way to the left, centering on the text, but cutting off Sutherland's entire right-hand side.
Often in a pan and scan telecast, a character will seem to be speaking offscreen, when what has really happened is that the pan and scan technique has cut his image out of the screen.
As television screenings of feature films became more common and more financially important, cinematographers began to work for compositions that would keep the vital information within the "TV safe area" of the frame. For example, the BBC suggested programme makers who were recording in 16:9 frame their shots in a aspect ratio which was then broadcast on analogue services with small black bars at the top and bottom of the picture, while owners of widescreen TV sets receiving digital broadcasts would see the full 16:9 picture (this is known as Shoot and protect). Film makers may also reverse this process, creating an original image that includes visual information that extends above and below the widescreen theatrical image; this is called "open matte". This may still be pan-and-scanned, but gives the compositor the freedom to "zoom out" or "uncrop" the image to include not only the full width of the wide-format image, but additional visual content at the top and/or bottom of the screen, not included in the widescreen version. As a general rule (prior to the adoption of DVD), special effects would be done within the theatrical aspect ratio, but not the full-frame thereof; also the expanded image area can include extraneous objects—such as cables, microphone booms, jet vapor trails, or overhead telephone wires—not intended to be included in the frame. Changes in screen angle (panning) may be necessary to prevent closeups between two speakers where only one person is visible in the pan-and-scan version and both participants seem to speak alternately to persons off camera; this comes at the cost of losing the smoothness of scenes. Inversely, the cropping of a film originally shown in the standard ratio to fit widescreen televisions may cut off foreground or background, such as a tap-dance scene in which much attention is directed appropriately at a dancer's feet. This situation will commonly occur whenever a widescreen TV is set to display full images without stretching (often called the zoom setting) on images with an aspect ratio of 1.78:1 or less. The solution is to pillar box the image by adding black bars on either side of the image, which maintains the full picture height. In Europe, where the PAL TV format offers more resolution to begin with, "pan-and-scan" broadcasts and "pan-and-scan" DVDs of movies originally shown in widescreen are relatively rare, unless it is of programming broadcasts aimed for family viewing times like "A Bug's Life". However, on some channels in some countries (such as the United Kingdom), films with an aspect ratio of more than 1.85:1 are panned and scanned to fit the broadcast 1.78:1 ratio. One modern alternative to pan and scan is to directly adjust the source material. This is very rare: the only known uses are computer-generated features, such as those produced by Pixar and video games such as BioShock. They call their approach to full-screen versions "reframing": some shots are pan and scan, while others are transferred open matte (a full widescreen image extended with added image above and below). Another method is to keep the camera angle as tight as a pan shot, but move the location of characters, objects, or the camera, so that the subjects fit in the frame. The advent of DVDs and their use of anamorphic presentation, coupled with the increasing popularity of widescreen televisions and computer monitors, have rendered pan and scan less important. Fullscreen versions of films originally produced in widescreen are still available in the United States.
Reactions.
Some directors still balk at the use of "pan and scan" because they feel it compromises the directorial vision with which their movies were created. For instance, Sydney Pollack brought a lawsuit against Danish TV after a screening of his 1975 film "Three Days of the Condor" in pan-and-scan in 1991 (The court ruled that the pan scanning conducted by Danish television was a 'mutilation' of the film and a violation of Pollack's 'Droit Moral', his legal right as an artist to maintain his reputation by protecting the integrity of his work. Nonetheless, the court ruled in favor of the defendant on a technicality). Steven Spielberg initially refused to release a pan-and-scan version of "Raiders of the Lost Ark" but eventually gave in (although he successfully ordered the letterboxed format for the home video releases of "The Color Purple" and "Always"); Woody Allen refused altogether to release one of "Manhattan", the letterbox version is therefore the only version available on VHS and DVD even though one VHS release includes the typical pan-and-scan disclaimer on the cover.

</doc>
<doc id="23508" url="https://en.wikipedia.org/wiki?curid=23508" title="Plymouth">
Plymouth

Plymouth () is a city on the south coast of Devon, England, about south-west of Exeter and west-south-west of London, between the mouths of the rivers Plym to the east and Tamar to the west where they join Plymouth Sound to form the boundary with Cornwall.
Plymouth's early history extends to the Bronze Age, when a first settlement emerged at Mount Batten. This settlement continued as a trading post for the Roman Empire, until it was surpassed by the more prosperous village of Sutton, now called Plymouth. In 1620, the Pilgrim Fathers departed Plymouth for the New World and established Plymouth Colony – the second English settlement in what is now the United States of America. During the English Civil War the town was held by the Parliamentarians and was besieged between 1642 and 1646.
Throughout the Industrial Revolution, Plymouth grew as a commercial shipping port, handling imports and passengers from the Americas, and exporting local minerals (tin, copper, lime, china clay and arsenic) while the neighbouring town of Devonport became a strategic Royal Naval shipbuilding and dockyard town. In 1914 three neighbouring independent towns, viz., the county borough of Plymouth, the county borough of Devonport, and the urban district of East Stonehouse were merged to form a single County Borough. The combined town took the name of Plymouth which, in 1928, achieved city status. The city's naval importance later led to its targeting and partial destruction during World War II, an act known as the Plymouth Blitz. After the war the city centre was completely rebuilt and subsequent expansion led to the incorporation of Plympton and Plymstock along with other outlying suburbs in 1967.
The city is home to () people, making it the 30th most populous built-up area in the United Kingdom. It is governed locally by Plymouth City Council and is represented nationally by three MPs. Plymouth's economy remains strongly influenced by shipbuilding and seafaring including ferry links to Brittany (Roscoff and St Malo) and Spain (Santander), but has tended toward a service-based economy since the 1990s. It has the largest operational naval base in Western Europe – HMNB Devonport and is home to Plymouth University.
History.
Early history.
Upper Palaeolithic deposits, including bones of Homo sapiens, have been found in local caves, and artefacts dating from the Bronze Age to the Middle Iron Age have been found at Mount Batten showing that it was one of the main trading ports of the country at that time. An unidentified settlement named 'TAMARI OSTIA' (mouth/estuaries of the Tamar) is listed in Ptolemy's "Geographia" and is presumed to be located in the area of the modern city.
The settlement of Plympton, further up the River Plym than the current Plymouth, was also an early trading port, but the river silted up in the early 11th century and forced the mariners and merchants to settle at the current day Barbican near the river mouth. At the time this village was called Sutton, meaning "south town" in Old English. The name "Plym Mouth", meaning "mouth of the River Plym" was first mentioned in a Pipe Roll of 1211.
The name "Plymouth" first officially replaced Sutton in a charter of King Henry VI in 1440. See Plympton for the derivation of the name "Plym".
Early defence and Renaissance.
During the Hundred Years' War a French attack (1340) burned a manor house and took some prisoners, but failed to get into the town. In 1403 the town was burned by Breton raiders. In the late fifteenth century, Plymouth Castle, a "castle quadrate", was constructed close to the area now known as The Barbican; it included four round towers, one at each corner, as featured on the city coat of arms. The castle served to protect Sutton Pool, which is where the fleet was based in Plymouth prior to the establishment of Plymouth Dockyard. In 1512 an Act of Parliament was passed for further fortifying Plymouth, and a series of fortifications were then built, including defensive walls at the entrance to Sutton Pool (across which a chain would be extended in time of danger). Defences on St Nicholas Island also date from this time, and a string of six artillery blockhouses were built, including one on Fishers Nose at the south-eastern corner of the Hoe. This location was further strengthened by the building of a fort (later known as Drake's Fort) in 1596, which itself went on to provide the site for the Citadel, established in the 1660s (see below).
During the 16th century locally produced wool was the major export commodity. Plymouth was the home port for successful maritime traders, among them Sir John Hawkins, who led England's first foray into the Atlantic slave trade, as well as Sir Francis Drake, Mayor of Plymouth in 1581 and 1593. According to legend, Drake insisted on completing his game of bowls on the Hoe before engaging the Spanish Armada in 1588. In 1620 the Pilgrim Fathers set sail for the New World from Plymouth, establishing Plymouth Colony – the second English colony in what is now the United States of America.
During the English Civil War Plymouth sided with the Parliamentarians and was besieged for almost four years by the Royalists. The last major attack by the Royalist was by Sir Richard Grenville leading thousands of soldiers towards Plymouth, but they were defeated by the Plymothians at Freedom Fields Park. The civil war ended as a Parliamentary win, but monarchy was restored by King Charles II in 1660, who imprisoned many of the Parliamentary heroes on Drake's Island. Construction of the Royal Citadel began in 1665, after the Restoration; it was armed with cannon facing both out to sea and into the town, rumoured to be a reminder to residents not to oppose the Crown. Mount Batten tower also dates from around this time.
Plymouth Dock, naval power and Foulston.
Throughout the 17th century Plymouth had gradually lost its pre-eminence as a trading port. By the mid-17th century commodities manufactured elsewhere in England cost too much to transport to Plymouth and the city had no means of processing sugar or tobacco imports, although it did play a relatively small part in the Atlantic slave trade during the early 18th century.
In the nearby parish of Stoke Damerel the first dockyard, HMNB Devonport, opened in 1690 on the eastern bank of the River Tamar. Further docks were built here in 1727, 1762 and 1793. The settlement that developed here was called "Dock" or "Plymouth Dock" at the time, and a new town, separate from Plymouth, grew up. In 1712 there were 318 men employed and by 1733 it had grown to a population of 3,000 people.
Before the latter half of the 18th century, grain, timber and then coal were Plymouth's main imports. During this time the real source of wealth was from the neighbouring town of Plymouth Dock (renamed in 1824 to Devonport) and the major employer in the entire region was the dockyard. The "Three Towns" conurbation of Plymouth, Stonehouse and Devonport enjoyed some prosperity during the late 18th and early 19th century and were enriched by a series of neo-classical urban developments designed by London architect John Foulston. Foulston was important for both Devonport and Plymouth and was responsible for several grand public buildings, many now destroyed, including the Athenaeum, the Theatre Royal and Royal Hotel, and much of Union Street.
Local chemist William Cookworthy established his short-lived Plymouth Porcelain venture in 1768 to exploit the deposits of china clay that he had discovered in Cornwall. He was acquainted with engineer John Smeaton, the builder of the third Eddystone Lighthouse.
The Breakwater in Plymouth Sound was designed by John Rennie in order to protect the fleet moving in and out of Devonport; work started in 1812. Numerous technical difficulties and repeated storm damage meant that it was not completed until 1841, twenty years after Rennie's death. In the 1860s, a ring of Palmerston forts was constructed around the outskirts of Devonport, to protect the dockyard from attack from any direction.
Some of the greatest imports to Plymouth from the Americas and Europe during the latter half of the 19th century included maize, wheat, barley, sugar cane, guano, sodium nitrate and phosphate Aside from the dockyard in the town of Devonport, industries in Plymouth such as the gasworks, the railways and tramways and a number of small chemical works had begun to develop in the 19th century, continuing into the 20th century.
"Plan for Plymouth" 1943.
During the First World War, Plymouth was the port of entry for many troops from around the Empire and also developed as a facility for the manufacture of munitions. Although major units of the Royal Navy moved to the safety of Scapa Flow, Devonport was an important base for escort vessels and repairs. Flying boats operated from Mount Batten.
In the First World War, Devonport was the headquarters of Western Approaches Command until 1941 and Sunderland flying boats were operated by the Royal Australian Air Force. It was an important embarkation point for US troops for D-Day. The city was heavily bombed by the Luftwaffe, in a series of 59 raids known as the Plymouth Blitz. Although the dockyards were the principal targets, much of the city centre and over 3,700 houses were completely destroyed and more than 1,000 civilians lost their lives. This was largely due to Plymouth's status as a major port. Charles Church was hit by incendiary bombs and partially destroyed in 1941 during the Blitz, but has not been demolished, as it is now an official permanent monument to the bombing of Plymouth during World War II.
The redevelopment of the city was planned by Sir Patrick Abercrombie in his 1943 "Plan for Plymouth" whilst simultaneously working on the reconstruction plan for London. Between 1951 and 1957 over 1000 homes were completed every year mostly using innovative prefabricated systems of just three main types; by 1964 over 20,000 new homes had been built transforming the dense overcrowded and unsanitary slums of the pre-war city into a low density, dispersed suburbia. Most of the city centre shops had been destroyed and those that remained were cleared to enable a zoned reconstruction according to his plan. In 1962 the modernist high rise of the Civic Centre was constructed, an architecturally significant example of mid twentieth century civic slab-and-tower set piece allowed to fall into disrepair by its owner Plymouth City Council but recently grade II listed by English Heritage to prevent its demolition.
Post-war, Devonport Dockyard was kept busy refitting aircraft carriers such as the and, later, nuclear submarines while new light industrial factories were constructed in the newly zoned industrial sector attracting rapid growth of the urban population. The army had substantially left the city by 1971, with barracks pulled down in the 1960s, however the city remains home to the 42 Commando of the Royal Marines.
Government.
Local government history.
The first record of the existence of a settlement at Plymouth was in the Domesday Book in 1086 as "Sudtone", Saxon for south farm, located at the present day Barbican. From Saxon times, it was in the hundred of Roborough. In 1254 it gained status as a town and in 1439, became the first town in England to be granted a Charter by Parliament. Between 1439 and 1934, Plymouth had a Mayor. In 1914 the county boroughs of Plymouth and Devonport, and the urban district of East Stonehouse merged to form a single county borough of Plymouth. Collectively they were referred to as "The Three Towns".
In 1919 Nancy Astor was elected the first ever female member of parliament to take office in the British Houses of Parliament for the constituency of Plymouth Sutton. Taking over office from her husband Waldorf Astor, Lady Astor was a vibrantly active campaigner for her resident constituents . Plymouth was granted city status on 18 October 1928. The city's first Lord Mayor was appointed in 1935 and its boundaries further expanded in 1967 to include the town of Plympton and the parish of Plymstock.
In 1945, Plymouth-born Michael Foot was elected Labour MP for the war-torn constituency of Plymouth Devonport and after serving as Secretary of State for Education and responsible for the 1974 Health and Safety at Work Act, went on to become one of the most distinguished leaders of the Labour party.
The 1971 Local Government White Paper proposed abolishing county boroughs, which would have left Plymouth, a town of 250,000 people, being administered from a council based at the smaller Exeter, on the other side of the county. This led to Plymouth lobbying for the creation of a Tamarside county, to include Plymouth, Torpoint, Saltash, and the rural hinterland. The campaign was not successful, and Plymouth ceased to be a county borough on 1 April 1974 with responsibility for education, social services, highways and libraries transferred to Devon County Council. All powers returned when the city become a unitary authority on 1 April 1998 under recommendations of the Banham Commission.
In the Parliament of the United Kingdom, Plymouth is represented by the three constituencies of Plymouth Moor View, Plymouth Sutton and Devonport and South West Devon and within the European Parliament as South West England. In the 2015 general election all three constituencies returned Conservative MPs, who were Oliver Colvile (for Devon South West), Gary Streeter (for Sutton and Devonport) and Johnny Mercer for Moor View.
City Council.
The City of Plymouth is divided into 20 wards, 17 of which elect three councillors and the other three electing two councillors, making up a total council of 57. Each year a third of the council is up for election for three consecutive years – there are no elections on the following "fourth" year, which is when County Council elections take place. The total for Plymouth was 188,924 in April 2015. The local election of 7 May 2015 resulted in a political composition of 28 Labour councillors, 26 Conservative and 3 UKIP resulting in a Labour administration. Plymouth City Council is formally twinned with: Brest, France (1963), Gdynia, Poland (1976), Novorossiysk, Russia (1990) San Sebastián, Spain (1990) and Plymouth, United States (2001).
Plymouth was granted the dignity of Lord Mayor by King George V in 1935. The position is elected each year by a group of six councillors. It is traditional that the position of the Lord Mayor alternates between the Conservative Party and the Labour Party annually and that the Lord Mayor chooses the Deputy Lord Mayor. Conservative councillor Dr John Mahony is the incumbent for 2015–16.
The Lord Mayor's official residence is 3 Elliot Terrace, located on the Hoe. Once a home of Waldorf and Nancy Astor, it was given by Lady Astor to the City of Plymouth as an official residence for future Lord Mayors and is also used today for civic hospitality, as lodgings for visiting dignitaries and High Court judges and it is also available to hire for private events. The Civic Centre municipal office building in Armada Way became a listed building in June 2007 because of its quality and period features, but has become the centre of a controversy as the council planned for its demolition estimating that it could cost £40m to refurbish it, resulting in possible job losses.
Geography.
Plymouth lies between the River Plym to the east and the River Tamar to the west; both rivers flow into the natural harbour of Plymouth Sound. Since 1967, the unitary authority of Plymouth has included the, once independent, towns of Plympton and Plymstock which lie along the east of the River Plym. The River Tamar forms the county boundary between Devon and Cornwall and its estuary forms the Hamoaze on which is sited Devonport Dockyard.
The River Plym, which flows off Dartmoor to the north-east, forms a smaller estuary to the east of the city called Cattewater. Plymouth Sound is protected from the sea by the Plymouth Breakwater, in use since 1814. In the Sound is Drake's Island which is seen from Plymouth Hoe, a flat public area on top of limestone cliffs. The Unitary Authority of Plymouth is . The topography rises from sea level to a height, at Roborough, of about above Ordnance Datum (AOD).
Geologically, Plymouth has a mixture of limestone, Devonian slate, granite and Middle Devonian limestone. Plymouth Sound, Shores and Cliffs is a Site of Special Scientific Interest, because of its geology. The bulk of the city is built upon Upper Devonian slates and shales and the headlands at the entrance to Plymouth Sound are formed of Lower Devonian slates, which can withstand the power of the sea.
A band of Middle Devonian limestone runs west to east from Cremyll to Plymstock including the Hoe. Local limestone may be seen in numerous buildings, walls and pavements throughout Plymouth. To the north and north east of the city is the granite mass of Dartmoor; the granite was mined and exported via Plymouth. Rocks brought down the Tamar from Dartmoor include ores containing tin, copper, tungsten, lead and other minerals. There is evidence that the middle Devonian limestone belt at the south edge of Plymouth and in Plymstock was quarried at West Hoe, Cattedown and Radford.
Urban Form.
On 27 April 1944 Sir Patrick Abercrombie's "Plan for Plymouth" to rebuild the bomb-damaged city was published; it called for demolition of the few remaining pre-War buildings in the city centre to make way for their replacement with wide, parallel, modern boulevards aligned east–west linked by a north–south avenue (Armada Way) linking the railway station with the vista of Plymouth Hoe. A peripheral road system connecting the historic Barbican on the east and Union Street to the west determines the principal form of the city centre, even following pedestrianisation of the shopping centre in the late 1980s, and continues to inform the present 'Vision for Plymouth' developed by a team led by Barcelona-based architect David MacKay in 2003 which calls for revivification of the city centre with mixed-use and residential. In suburban areas, post-War prefabs had already begun to appear by 1946, and over 1,000 permanent council houses were built each year from 1951–57 according to the Modernist zoned low-density garden city model advocated by Abercrombie. By 1964 over 20,000 new homes had been built, more than 13,500 of them permanent council homes and 853 built by the Admiralty. Plymouth is home to 28 parks with an average size of . Its largest park is Central Park, with other sizeable green spaces including Victoria Park, Freedom Fields Park, Alexandra Park, Devonport Park and the Hoe.
Climate.
Along with the rest of South West England, Plymouth has a temperate oceanic climate (Köppen "Cfb") which is generally wetter and milder than the rest of England. This means a wide range of exotic plants can be grown. The annual mean temperature is approximately . Due to the modifying effect of the sea the seasonal range is less than in most other parts of the UK. As a result of this summer highs are lower than its southerly latitude should warrant, but as a contrast the coldest month of February has mean minimum temperatures as mild as between . Snow is rare, not usually equating to more than a few flakes, but there have been exclusions, namely the European winter storms of 2009-10 which, in early January, covered Plymouth in at least of snow; more on higher ground. Another period of notable snow occurred from 17–19 December 2010 when up to of snow fell through the period – though only would lie at any one time due to melt. Over the 1961–1990 period, annual snowfall accumulation averaged less than per year. July and August are the warmest months with mean daily maxima over .
South West England has a favoured location when the Azores High pressure area extends north-eastwards towards the UK, particularly in summer. Coastal areas have average annual sunshine totals over 1,600 hours.
Rainfall tends to be associated with Atlantic depressions or with convection. The Atlantic depressions are more vigorous in autumn and winter and most of the rain which falls in those seasons in the south-west is from this source. Average annual rainfall is around . November to March have the highest mean wind speeds, with June to August having the lightest winds. The predominant wind direction is from the south-west.
Typically, the warmest day of the year (1971–2000) will achieve a temperature of , although in June 1976 the temperature reached , the site record. On average, 4.25 days of the year will report a maximum temperature of or above. During the winter half of the year, the coldest night will typically fall to although in January 1979 the temperature fell to . Typically, 18.6 nights of the year will register an air frost.
Education.
The University of Plymouth enrolls total students as of ( largest in the UK out of ). It also employs 3,000 staff with an annual income of around £160 million. It was founded in 1992 from Polytechnic South West (formerly Plymouth Polytechnic) following the Further and Higher Education Act 1992. It has courses in maritime business, marine engineering, marine biology and Earth, ocean and environmental sciences, surf science, shipping and logistics. The university formed a joint venture with the fellow Devonian University of Exeter in 2000, establishing the Peninsula College of Medicine and Dentistry. The college is ranked 8th out of 30 universities in the UK in 2011 for medicine. Its dental school was established in 2006, which also provides free dental care in an attempt to improve access to dental care in the South West.
The University of St Mark & St John (known as "Marjon" or "Marjons") specialises in teacher training, and offers training across the country and abroad.
The city is also home to two large colleges. The City College Plymouth provides courses from the most basic to Foundation degrees for approximately 26,000 students. Plymouth College of Art offers a selection of courses including media. It was started 153 years ago and is now one of only four independent colleges of art and design in the UK.
Plymouth also has 71 state primary phase schools, 13 state secondary schools, eight special schools and three selective state grammar schools, Devonport High School for Girls, Devonport High School for Boys and Plymouth High School for Girls. There is also an independent school Plymouth College.
The city was also home to the Royal Naval Engineering College; opened in 1880 in Keyham, it trained engineering students for five years before they completed the remaining two years of the course at Greenwich. The college closed in 1910, but in 1940 a new college opened at Manadon. This was renamed "Dockyard Technical College" in 1959 before finally closing in 1994; training was transferred to the University of Southampton.
Plymouth is home to the Marine Biological Association of the United Kingdom (MBA) which conducts research in all areas of the marine sciences. The Plymouth Marine Laboratory is an offshoot of the MBA. Together with the National Marine Aquarium, the Sir Alister Hardy Foundation for Ocean Sciences, Plymouth University's Marine Institute and the Diving Diseases Research Centre, these marine-related organisations form the Plymouth Marine Sciences Partnership. The Plymouth Marine Laboratory, which focuses on global issues of climate change and sustainability. It monitors the effects of ocean acidity on corals and shellfish and reports the results to the UK government. It also cultivates algae that could be used to make biofuels or in the treatment of waste water by using technology such as photo-bioreactors. It works alongside the Boots Group to investigate the use of algae in skin care protects, taking advantage of the chemicals they contain that adapt to protect themselves from the sun.
Demography.
From the 2011 Census, the Office for National Statistics published that Plymouth's unitary authority area population was 256,384; 15,664 more people than that of the last census from 2001, which indicated that Plymouth had a population of 240,720. The Plymouth urban area had a population of 260,203 in 2011 (the urban sprawl which extends outside the authority's boundaries). The city's average household size was 2.3 persons. At the time of the 2011 UK census, the ethnic composition of Plymouth's population was 96.2% White (of 92.9% was White British), with the largest minority ethnic group being Chinese at 0.5%. The white Irish ethnic group saw the largest decline in its share of the population since the 2001 Census (-24%), while the "Other Asian" and Black African had the largest increases (360% and 351% respectively). This excludes the two new ethnic groups added to the 2011 census of Gypsy or Irish Traveller and Arab. The population rose rapidly during the second half of the 19th century, but declined by over 1.6% from 1931 to 1951.
Plymouth's gross value added (a measure of the size of its economy) was 5,169 million GBP in 2013 making up 25% of Devon's GVA. Its GVA per person was £19,943 and compared to the national average of £23,755, was £3,812 lower. Plymouth's unemployment rate was 7.0% in 2014 which was 2.0 points higher than the South West average and 0.8 points higher than the average for Great Britain (England, Wales and Scotland).
A 2014 profile by the National Health Service showed Plymouth had higher than average levels of poverty and deprivation (26.2% of population among the poorest 20.4% nationally). Life expectancy, at 78.3 years for men and 82.1 for women, was the lowest of any region in the South West of England.
Economy.
Because of its coastal location, the economy of Plymouth has traditionally been , in particular the defence sector with over 12,000 people employed and approximately 7,500 in the armed forces. The Plymouth Gin Distillery has been producing Plymouth Gin since 1793, which was exported around the world by the Royal Navy. During the 1930s, it was the most widely distributed gin and has a controlled term of origin. Since the 1980s, employment in the defence sector has decreased substantially and the public sector is now prominent particularly in administration, health, education, medicine and engineering.
Devonport Dockyard is the UK's only naval base that refits nuclear submarines and the Navy estimates that the Dockyard generates about 10% of Plymouth's income. Plymouth has the largest cluster of marine and maritime businesses in the south west with 270 firms operating within the sector. Other substantial employers include the university with almost 3,000 staff, as well as the Tamar Science Park employing 500 people in 50 companies. Several employers have chosen to locate their headquarters in Plymouth, including Hemsley Fraser.
Plymouth has a post-war shopping area in the city centre with substantial pedestrianisation. At the west end of the zone inside a grade II listed building is the Pannier Market that was completed in 1959 – "pannier" meaning "basket" from French, so it translates as "basket market". In terms of retail floorspace, Plymouth is ranked in the top five in the South West, and 29th nationally. Plymouth was one of the first ten British cities to trial the new Business Improvement District initiative. The Tinside Pool is situated at the foot of the Hoe and became a grade II listed building in 1998 before being restored to its 1930s look for £3.4 million.
Plymouth 2020.
As of 2003, Plymouth Council has been undertaking a project of urban redevelopment called the "Vision for Plymouth" launched by the architect David Mackay and backed by both Plymouth City Council and the Plymouth Chamber of Commerce (PCC). Its projects range from shopping centres, a cruise terminal, a boulevard and to increase the population to 300,000 and build 33,000 dwellings.
In 2004 the old Drake Circus shopping centre and Charles Cross car park were demolished and replaced by the latest Drake Circus Shopping Centre, which opened in October 2006. It received negative feedback before opening when David Mackay said it was already "ten years out of date". In contrast, the Theatre Royal's production and education centre, TR2, which was built on wasteland at Cattedown, was a runner-up for the RIBA Stirling Prize for Architecture in 2003.
There is a project involving the future relocation of Plymouth City Council's headquarters, the civic centre, to the current location of the Bretonside bus station; it would involve both the bus station and civic centre being demolished and a rebuilt together at the location with the land from the civic centre being sold off. Other suggestions include the demolition of the Plymouth Pavilions entertainment arena to create a canal "boulevard" linking Millbay to the city centre. Millbay is being regenerated with mixed residential, retail and office space alongside the ferry port.
Transport.
The A38 dual-carriageway runs from east to west across the north of the city. Within the city it is designated as 'The Parkway' and represents the boundary between the urban parts of the city and the generally more recent suburban areas. Heading east, it connects Plymouth to the M5 motorway about away near Exeter; and heading west it connects Cornwall and Devon via the Tamar Bridge. Regular bus services are provided by Plymouth Citybus, First South West and Target Travel. There are three Park and ride services located at Milehouse, Coypool (Plympton) and George Junction (Plymouth City Airport), which are operated by First South West.
A regular international ferry service provided by Brittany Ferries operates from Millbay taking cars and foot passengers directly to France (Roscoff) and Spain (Santander) on the three ferries, "MV Armorique", "MV Bretagne" and "MV Pont-Aven". There is a passenger ferry between Stonehouse and the Cornish hamlet of Cremyll, which is believed to have operated continuously since 1204. There is also a pedestrian ferry from the Mayflower Steps to Mount Batten, and an alternative to using the Tamar Bridge via the Torpoint Ferry (vehicle and pedestrian) across the River Tamar.
The city's airport was Plymouth City Airport about north of the city centre.
The airport was home to the local airline Air Southwest,
which operated flights across the United Kingdom and Ireland. In June 2003, a report by the South West RDA was published looking at the future of aviation in the south-west and the possible closure of airports. It concluded that the best option for the south-west was to close Plymouth City Airport and expand Exeter International Airport and Newquay Cornwall Airport, although it did conclude that this was not the best option for Plymouth. In April 2011, it was announced that the airport would close, which it did on 23 December. However, FlyPlymouth plans to reopen the city airport by 2018, which will provide daily services to various destinations including London.
Plymouth railway station, which opened in 1877, is managed by Great Western Railway and also sees trains on the CrossCountry network. Smaller stations are served by local trains on the Tamar Valley Line and Cornish Main Line. First Great Western have come under fire recently, due to widespread rail service cuts across the south-west, which affect Plymouth greatly. Three MPs from the three main political parties in the region have lobbied that the train services are vital to its economy.
The Exeter to Plymouth railway of the LSWR needs to be reopened to connect Cornwall and Plymouth to the rest of the UK railway system on an all weather basis. There are proposals to reopen the line from Tavistock to Bere Alston for a through service to Plymouth. On the night of 4 February 2014, amid high winds and extremely rough seas, part of the sea wall at Dawlish was breached washing away around of the wall and the ballast under the railway immediately behind. The line was closed. Network Rail began repair work and the line reopened on 4 April 2014. In the wake of widespread disruption caused by damage to the mainline track at Dawlish by coastal storms in February 2014, Network Rail are considering reopening the Tavistock to Okehampton and Exeter section of the line as an alternative to the coastal route.
Religion.
Plymouth has about 150 churches and its Roman Catholic cathedral (1858) is in Stonehouse. The city's oldest church is St Andrew's (Anglican) located at the top of Royal Parade—it is the largest parish church in Devon and has been a site of gathering since AD 800. The city also includes five Baptist churches, over twenty Methodist chapels, and thirteen Roman Catholic churches. In 1831 the first Brethren assembly in England, a movement of conservative non-denominational Evangelical Christians, was established in the city, so that Brethren are often called Plymouth Brethren, although the movement did not begin locally.
Plymouth has the first known reference to Jews in the South West from Sir Francis Drake's voyages in 1577 to 1580, as his log mentioned "Moses the Jew" – a man from Plymouth. The Plymouth Synagogue is a Listed Grade II* building, built in 1762 and is the oldest Ashkenazi Synagogue in the English speaking world. There are also places of worship for Islam, Bahá'í, Buddhism, Unitarianism, Chinese beliefs and Humanism.
58.1% of the population described themselves in the 2011 census return as being at least nominally Christian and 0.8% as Muslim with all other religions represented by less than 0.5% each. The portion of people without a religion is 32.9%; above the national average of 24.7%. 7.1% did not state their religious belief. Since the 2001 Census, the number of Christians and Jews has decreased (-16% and -7% respectively), while all other religions have increased and non-religious people have almost doubled in number.
Culture.
Built in 1815, Union Street was at the heart of Plymouth's historical culture. It became known as "the servicemen's playground", as it was where sailors from the Royal Navy would seek entertainment of all kinds. During the 1930s, there were 30 pubs and it attracted such performers as Charlie Chaplin to the New Palace Theatre. It is now the late-night hub of Plymouth's entertainment strip, but has a reputation for trouble at closing hours.
Outdoor events and festivals are held including the annual British Firework Championships in August, which attracts tens of thousands of people across the waterfront. In August 2006 the world record for the most amount of simultaneous fireworks was surpassed, by Roy Lowry of the University of Plymouth, over Plymouth Sound. Since 1992 the Music of the Night has been performed in the Royal Citadel by the 29 Commando Regiment and local performers to raise money for local and military charities.
The city's main theatres are the Theatre Royal (1,315 capacity), its Drum Theatre (200 capacity), and its production and creative learning centre, The TR2. The Plymouth Pavilions has multiple uses for the city staging music concerts, basketball matches and stand-up comedy. There are also three cinemas: Reel Cinema at Derrys Cross, Plymouth Arts Centre at Looe Street and a Vue cinema at the Barbican Leisure Park. The Plymouth City Museum and Art Gallery is operated by Plymouth City Council allowing free admission – it has six galleries. The Plymouth Athenaeum, which includes a local interest library, is a society dedicated to the promotion of learning in the fields of science, technology, literature and art. From 1961 to 2009 it also housed a theatre.
Plymouth is the regional television centre of BBC South West. A team of journalists are headquartered at Plymouth for the ITV West Country regional station, after a merger with ITV West forced ITV Westcountry to close on 16 February 2009. The main local newspapers serving Plymouth are "The Herald" and "Western Morning News" with Radio Plymouth , BBC Radio Devon, Heart South West , and Pirate FM being the main local radio stations.
Sport.
Plymouth is home to Plymouth Argyle F.C., who play in the fourth tier of English football league known as Football League Two. The team's home ground is called Home Park and is located in Central Park. It links itself with the group of English non-conformists that left Plymouth for the New World in 1620: its nickname is "The Pilgrims". The city also has four Non-League football clubs; Plymouth Parkway F.C. who play at Bolitho Park, Elburton Villa F.C. who play at Haye Road, Vospers Oak Villa F.C. who play at Weston Mill and Plymstock United F.C. who play at Deans Cross. All four clubs play in the South West Peninsula League.
Other sports clubs include Plymouth Albion R.F.C. and the Plymouth Raiders basketball club. Plymouth Albion Rugby Football Club is a rugby union club that was founded in 1875 and are currently competing in the third tier of Professional English Rugby . They play at the Brickfields. Plymouth Raiders play in the British Basketball League – the top tier of British basketball. They play at the Plymouth Pavilions entertainment arena and were founded in 1983. Plymouth cricket club was formed in 1843, the current 1st XI play in the Devon Premier League. Plymouth Devils are a speedway team in the British Premier League. Plymouth was home to an American football club, the Plymouth Admirals until 2010. Plymouth is also home to Plymouth Marjons Hockey Club, with their 1st XI playing in the National League last season.
Plymouth is an important centre for watersports, especially scuba diving and sailing. The Port of Plymouth Regatta is one of the oldest regattas in the world, and has been held regularly since 1823. In September 2011, Plymouth hosted the America's Cup World Series for nine days.
Public services.
Since 1973 Plymouth has been supplied water by South West Water. Prior to the 1973 take over it was supplied by Plymouth County Borough Corporation. Before the 19th century two leats were built in order to provide drinking water for the town. They carried water from Dartmoor to Plymouth. A watercourse, known as Plymouth or Drake's Leat, was opened on 24 April 1591 to tap the River Meavy. The Devonport Leat was constructed to carry fresh drinking water to the expanding town of Devonport and its ever growing dockyard. It was fed by three Dartmoor rivers: The West Dart, Cowsic and Blackabrook. It seems to have been carrying water since 1797, but it was officially completed in 1801. It was originally designed to carry water to Devonport town, but has since been shortened and now carries water to Burrator Reservoir, which feeds most of the water supply of Plymouth. Burrator Reservoir is located about north of the city and was constructed in 1898 and expanded in 1928.
Plymouth City Council is responsible for waste management throughout the city and South West Water is responsible for sewerage. Plymouth's electricity is supplied from the National Grid and distributed to Plymouth via Western Power Distribution. On the outskirts of Plympton a combined cycle gas-powered station, the Langage Power Station, which started to produce electricity for Plymouth at the end of 2009.
Her Majesty's Courts Service provide a Magistrates' Court and a Combined Crown and County Court in the city. The Plymouth Borough Police, formed in 1836, eventually became part of Devon and Cornwall Constabulary. There are police stations at Charles Cross and Crownhill (the Divisional HQ) and smaller stations at Plympton and Plymstock. The city has one of the Devon and Cornwall Area Crown Prosecution Service Divisional offices. Plymouth has five fire stations located in Camel's Head, Crownhill, Greenbank, Plympton and Plymstock which is part of Devon and Somerset Fire and Rescue Service. The Royal National Lifeboat Institution have an Atlantic 85 class lifeboat and Severn class lifeboat stationed at Millbay Docks.
Plymouth is served by Plymouth Hospitals NHS Trust and the city's NHS hospital is Derriford Hospital north of the city centre. The Royal Eye Infirmary is located at Derriford Hospital. South Western Ambulance Service NHS Foundation Trust operates in Plymouth and the rest of the south west; its headquarters are in Exeter.
The mid-19th century burial ground at Ford Park Cemetery was reopened in 2007 by a successful trust and the City council operate two large early 20th century cemeteries at Weston Mill and Efford both with crematoria and chapels. There is also a privately owned cemetery on the outskirts of the city, Drake Memorial Park which does not allow headstones to mark graves, but a brass plaque set into the ground.
Landmarks and tourist attractions.
After the English Civil War the Royal Citadel was built in 1666 on the east end of Plymouth Hoe, to defend the port from naval attacks, suppress Plymothian Parliamentary leanings and to train the armed forces. Guided tours are available in the summer months. Further west is Smeaton's Tower, which was built in 1759 as a lighthouse on rocks off shore, but dismantled and the top two thirds rebuilt on the Hoe in 1877. It is open to the public and has views over the Plymouth Sound and the city from the lantern room. Plymouth has 20 war memorials of which nine are on The Hoe including: Plymouth Naval Memorial, to remember those killed in World Wars I and II, and the Armada Memorial, to commemorate the defeat of the Spanish Armada.
The early port settlement of Plymouth, called "Sutton", approximates to the area now referred to as the Barbican and has 100 listed buildings and the largest concentration of cobbled streets in Britain. The Pilgrim Fathers left for the New World in 1620 near the commemorative Mayflower Steps in Sutton Pool. Also on Sutton Pool is the National Marine Aquarium which displays 400 marine species and includes Britain's deepest aquarium tank.
On the northern outskirts of the city, Crownhill Fort is a well restored example of a "Palmerston's Folly". It is owned by the Landmark Trust and is open to the public.
To the west of the city is Devonport, one of Plymouth's historic quarters. As part of Devonport's millennium regeneration project, the Devonport Heritage Trail has been introduced, complete with over 70 waymarkers outlining the route.
Plymouth is often used as a base by visitors to Dartmoor, the Tamar Valley and the beaches of south-east Cornwall. Kingsand, Cawsand and Whitsand Bay are popular.
The Roland Levinsky building, the landmark building of the University of Plymouth, is located in the city's central quarter. Designed by leading architect Henning Larsen, the building was opened in 2008 and houses the University's Arts faculty. It has been consistently considered one of the UK's most beautiful university buildings.
Notable people.
People from Plymouth are known as Plymothians or less formally as Janners. Its meaning is described as a person from Devon, deriving from Cousin Jan (the Devon form of John), but more particularly in naval circles anyone from the Plymouth area.
The Elizabethan navigator, Sir Francis Drake was born in the nearby town of Tavistock and was the mayor of Plymouth. He was the first Englishman to circumnavigate the world and was known by the Spanish as "El Draco" meaning "The Dragon" after he raided many of their ships. He died of dysentery in 1596 off the coast of Puerto Rico. In 2002 a mission to recover his body and bring it to Plymouth was allowed by the Ministry of Defence. His cousin and contemporary John Hawkins was a Plymouth man. Painter Sir Joshua Reynolds, founder and first president of the Royal Academy was born and educated in nearby Plympton, now part of Plymouth. William Cookworthy born in Kingsbridge set up his successful porcelain business in the city and was a close friend of John Smeaton designer of the Eddystone Lighthouse. On 26 January 1786, Benjamin Robert Haydon, an English painter who specialised in grand historical pictures, was born here. The naturalist Dr William Elford Leach FRS, who did much to pave the way in Britain for Charles Darwin, was born at Hoe Gate in 1791.
Antarctic explorers Robert Falcon Scott and Frank Bickerton both lived in the city. Artists include Beryl Cook whose paintings depict the culture of Plymouth and Robert Lenkiewicz, whose paintings investigated themes of vagrancy, sexual behaviour and suicide, lived in the city from the 1960s until his death in 2002. Illustrator and creator of children's series Mr Benn and King Rollo, David McKee, was born and brought up in South Devon and trained at Plymouth College of Art. Jazz musician John Surman, born in nearby Tavistock, has close connections to the area, evidenced by his 2012 album Saltash Bells. The avant garde prepared guitarist Keith Rowe was born in the city before establishing the jazz free improvisation band AMM in London in 1965 and MIMEO in 1997. The musician and film director Cosmo Jarvis has lived in several towns in South Devon and has filmed videos in and around Plymouth. In addition, actors Sir Donald Sinden and Judi Trott. George Passmore of Turner Prize winning duo Gilbert and George was born in the city, as was Labour politician Michael Foot whose family reside at nearby Trematon Castle.
Notable athletes include swimmer Sharron Davies, diver Tom Daley, dancer Wayne Sleep, and footballer Trevor Francis. Other past residents include composer journalist and newspaper editor William Henry Wills, Ron Goodwin, and journalist Angela Rippon and comedian Dawn French. Canadian politician and legal scholar Chris Axworthy hails from Plymouth. America based actor Donald Moffat, whose roles include American Vice President Lyndon B. Johnson in the film "The Right Stuff", and fictional President Bennett in "Clear and Present Danger", was born in Plymouth.

</doc>
<doc id="23511" url="https://en.wikipedia.org/wiki?curid=23511" title="Point-to-Point Protocol">
Point-to-Point Protocol

In computer networking, Point-to-Point Protocol (PPP) is a data link (layer 2) protocol used to establish a direct connection between two nodes. It can provide connection authentication, transmission encryption (using ECP, RFC 1968), and compression.
PPP is used over many types of physical networks including serial cable, phone line, trunk line, cellular telephone, specialized radio links, and fiber optic links such as SONET. 
PPP is also used over Internet access connections.
Internet service providers (ISPs) have used PPP for customer dial-up access to the Internet, since IP packets cannot be transmitted over a modem line on their own, without some data link protocol. 
Two derivatives of PPP, Point-to-Point Protocol over Ethernet (PPPoE) and Point-to-Point Protocol over ATM (PPPoA), are used most commonly by Internet Service Providers (ISPs) to establish a Digital Subscriber Line (DSL) Internet service connection with customers.
PPP is commonly used as a data link layer protocol for connection over synchronous and asynchronous circuits, where it has largely superseded the older Serial Line Internet Protocol (SLIP) and telephone company mandated standards (such as Link Access Protocol, Balanced (LAPB) in the X.25 protocol suite). The only requirement for PPP is that the circuit provided be duplex. PPP was designed to work with numerous network layer protocols, including Internet Protocol (IP), TRILL, Novell's Internetwork Packet Exchange (IPX), NBF, DECnet and AppleTalk.
Description.
PPP was designed somewhat after the original HDLC specifications. The designers of PPP included many additional features that had been seen only in proprietary data-link protocols up to that time.
RFC 2516 describes Point-to-Point Protocol over Ethernet (PPPoE) as a method for transmitting PPP over Ethernet that is sometimes used with DSL. RFC 2364 describes Point-to-Point Protocol over ATM (PPPoA) as a method for transmitting PPP over ATM Adaptation Layer 5 (AAL5), which is also a common alternative to PPPoE used with DSL.
PPP is a layered protocol that has three components:
PPP is specified in RFC 1661.
Automatic self configuration.
Link Control Protocol (LCP) initiates and terminates connections gracefully, allowing hosts to negotiate connection options. It is an integral part of PPP, and is defined in the same standard specification. LCP provides automatic configuration of the interfaces at each end (such as setting datagram size, escaped characters, and magic numbers) and for selecting optional authentication. The LCP protocol runs on top of PPP (with PPP protocol number 0xC021) and therefore a basic PPP connection has to be established before LCP is able to configure it.
RFC 1994 describes Challenge-handshake authentication protocol (CHAP), which is preferred for establishing dial-up connections with ISPs.
Although deprecated, Password authentication protocol (PAP) is still sometimes used.
Another option for authentication over PPP is Extensible Authentication Protocol (EAP) described in RFC 2284.
After the link has been established, additional network (layer 3) configuration may take place. Most commonly, the Internet Protocol Control Protocol (IPCP) is used, although Internetwork Packet Exchange Control Protocol (IPXCP) and AppleTalk Control Protocol (ATCP) were once very popular. Internet Protocol Version 6 Control Protocol (IPv6CP) will see extended use in the future, when IPv6 replaces IPv4 as the dominant layer-3 protocol.
Multiple network layer protocols.
PPP permits multiple network layer protocols to operate on the same communication link. For every network layer protocol used, a separate Network Control Protocol (NCP) is provided in order to encapsulate and negotiate options for the multiple network layer protocols. It negotiates network-layer information, e.g. network address or compression options, after the connection has been established.
For example, Internet Protocol (IP) uses the IP Control Protocol (IPCP), and Internetwork Packet Exchange (IPX) uses the Novell IPX Control Protocol (IPX/SPX). NCPs include fields containing standardized codes to indicate the network layer protocol type that the PPP connection encapsulates.
The following NCPs may be used with PPP:
Looped link detection.
PPP detects looped links using a feature involving magic numbers. When the node sends PPP LCP messages, these messages may include a magic number. If a line is looped, the node receives an LCP message with its own magic number, instead of getting a message with the peer's magic number.
PPP Configuration Options.
The previous section introduced the use of LCP options to meet specific WAN connection requirements. PPP may include the following LCP options:
PPP frame.
Structure of a PPP frame.
PPP frames are variants of HDLC frames:
If both peers agree to Address field and Control field compression during LCP, then those fields are omitted. Likewise if both peers agree to Protocol field compression, then the 0x00 byte can be omitted.
The Protocol field indicates the type of payload packet: 0xC021 for LCP, 0x80xy for various NCPs, 0x0021 for IP, 0x0029 AppleTalk, 0x002B for IPX, 0x003D for Multilink, 0x003F for NetBIOS, 0x00FD for MPPC and MPPE, etc. PPP is limited, and cannot contain general Layer 3 data, unlike EtherType.
The Information field contains the PPP payload; it has a variable length with a negotiated maximum called the Maximum Transmission Unit. By default, the maximum is 1500 octets. It might be padded on transmission; if the information for a particular protocol can be padded, that protocol must allow information to be distinguished from padding.
Encapsulation.
PPP frames are encapsulated in a lower-layer protocol that provides framing and may provide other functions such as a checksum to detect transmission errors. PPP on serial links is usually encapsulated in a framing similar to HDLC, described by IETF RFC 1662.
The Flag field is present when PPP with HDLC-like framing is used.
The Address and Control fields always have the value hex FF (for "all stations") and hex 03 (for "unnumbered information"), and can be omitted whenever PPP LCP Address-and-Control-Field-Compression (ACFC) is negotiated.
The frame check sequence (FCS) field is used for determining whether an individual frame has an error. It contains a checksum computed over the frame to provide basic protection against errors in transmission. This is a CRC code similar to the one used for other layer two protocol error protection schemes such as the one used in Ethernet. According to RFC 1662, it can be either 16 bits (2 bytes) or 32 bits (4 bytes) in size (default is 16 bits - Polynomial "x"16 + "x"12 + "x"5 + 1).
The FCS is calculated over the Address, Control, Protocol, Information and Padding fields after the message has been encapsulated.
PPP line activation and phases.
The phases of the Point to Point Protocol according to RFC 1661 are listed below:
PPP over several links.
Multilink PPP.
Multilink PPP (also referred to as MLPPP, MP, MPPP, MLP, or Multilink) provides a method for spreading traffic across multiple distinct PPP connections. It is defined in RFC 1990. It can be used, for example, to connect a home computer to an Internet Service Provider using two traditional 56k modems, or to connect a company through two leased lines.
On a single PPP line frames cannot arrive out of order, but this is possible when the frames are divided among multiple PPP connections. Therefore, Multilink PPP must number the fragments so they can be put in the right order again when they arrive.
Multilink PPP is an example of a link aggregation technology. Cisco IOS Release 11.1 and later supports Multilink PPP. 
Multiclass PPP.
With PPP, one cannot establish several simultaneous distinct PPP connections over a single link.
That's not possible with Multilink PPP either. Multilink PPP uses contiguous numbers for all the fragments of a packet, and as a consequence it is not possible to suspend the sending of a sequence of fragments of one packet in order to send another packet. This prevents from running Multilink PPP multiple times on the same links.
Multiclass PPP is a kind of Multilink PPP where each "class" of traffic uses a separate sequence number space and reassembly buffer. Multiclass PPP is defined in RFC 2686.
PPP and tunnels.
Derived protocols.
PPTP is a form of PPP between two hosts via GRE using encryption (MPPE) and compression (MPPC).
PPP as a layer 2 protocol between both ends of a tunnel.
Many protocols can be used to tunnel data over IP networks. Some of them, like SSL, SSH, or L2TP create virtual network interfaces and give the impression of a direct physical connections between the tunnel endpoints. On a Linux host for example, these interfaces would be called tun0.
As there are only two endpoints on a tunnel, the tunnel is a point-to-point connection and PPP is a natural choice as a data link layer protocol between the virtual network interfaces. PPP can assign IP addresses to these virtual interfaces, and these IP addresses can be used, for example, to route between the networks on both sides of the tunnel.
IPsec in tunneling mode does not create virtual physical interfaces at the end of the tunnel, since the tunnel is handled directly by the TCP/IP stack. L2TP can be used to provide these interfaces, this technique is called L2TP/IPsec. In this case too, PPP provides IP addresses to the extremities of the tunnel.
References.
RFCs.
PPP is defined in RFC 1661 (The Point-to-Point Protocol, July 1994). RFC 1547 (Requirements for an Internet Standard Point-to-Point Protocol, December 1993) provides historical information about the need for PPP and its development. A series of related RFCs have been written to define how a variety of network control protocols-including TCP/IP, DECnet, AppleTalk, IPX, and others-work with PPP.

</doc>
<doc id="23512" url="https://en.wikipedia.org/wiki?curid=23512" title="Patterson–Gimlin film">
Patterson–Gimlin film

The Patterson–Gimlin film (also known as the Patterson film or the PGF) is a famous short motion picture of an unidentified subject the filmmakers said was a Bigfoot. The footage was shot in 1967, and has since been subjected to many attempts to authenticate or debunk it. Several university based studies and professional evaluations have concluded the subject cannot possibly be a man in an ape suit. However, others have judged it to be a hoax staged with a person in an ape suit.
The footage was filmed alongside Bluff Creek, a tributary of the Klamath River, about 25 logging-road miles northwest of Orleans, California, in Del Norte County. The film site is roughly 38 miles south of Oregon and 18 miles east of the Pacific Ocean. For decades, the exact location of the site was lost, primarily because of re-growth of foliage in the streambed after the flood of 1964. It was rediscovered in 2011.
The filmmakers were Roger Patterson (February 14, 1933 – January 15, 1972) and Robert "Bob" Gimlin (born October 18, 1931). Patterson died of cancer in 1972 and "maintained right to the end that the creature on the film was real." Patterson's friend, Gimlin, has always denied being involved in any part of a hoax with Patterson. Gimlin mostly avoided publicly discussing the subject from at least the early 1970s until about 2005 (except for three appearances), when he began giving interviews and appearing at Bigfoot conferences.
The film is 23.85 feet long (preceded by 76.15 feet of "horseback" footage), has 954 frames, and runs for 59.5 seconds at 16 frames per second. If the film was shot at 18 fps, as Grover Krantz believes, the event lasted 53 seconds. The date was October 20, 1967, according to the filmmakers, although some critics believe it was shot earlier.
Background.
Patterson said he became interested in Bigfoot after reading an article about the creature by Ivan T. Sanderson in "True" magazine in December 1959. In 1961 Sanderson published his encyclopedic "Abominable Snowmen: Legend Come to Life," a worldwide survey of accounts of Bigfoot-type creatures, including recent track finds, etc. in the Bluff Creek area, which heightened his interest. Thereafter, Marian Place wrote:
Patterson's book, "Do Abominable Snowmen of America Really Exist?", was self-published in 1966. The book has been characterized as "little more than a collection of newspaper clippings laced together with Patterson's circus-poster style prose." The book, however, actually contains 20 pages of previously unpublished interviews and letters, 17 drawings by Patterson of the encounters described in the text, 5 hand-drawn maps (rare in subsequent Bigfoot books), and almost 20 photos and illustrations from other sources. It was first reprinted in 1996 by Chris Murphy, and then again re-issued by Murphy in 2005 under the title "The Bigfoot Film Controversy," with 81 pages of additional material by Murphy.
In May/June 1967 Patterson began filming a docudrama or pseudo-documentary about cowboys being led by an old miner and a wise Indian tracker on a hunt for Bigfoot. The storyline called for Patterson, his Indian guide (Gimlin in a wig), and the cowboys to recall in flashbacks the stories of Fred Beck (of the 1924 Ape Canyon incident) and others as they tracked the beast on horseback. For actors and cameraman, Patterson used at least nine volunteer acquaintances, including Gimlin and Bob Heironimus, for three days of shooting, perhaps over the Memorial Day weekend. Lacking a cooperative Bigfoot, Patterson would have needed a costume to represent one, if the time came to shoot such climactic scenes.
Prior to the October 1967 filming, Patterson apparently visited Los Angeles on these occasions:
Merritt soon moved back to Yakima and became Patterson's neighbor, and later his collaborator on his Bigfoot documentary. 
Both Patterson and Gimlin had been rodeo riders and amateur boxers — and local champions in their weight classes. Patterson had played high school football.
In October 1967, Patterson and his friend Gimlin set out for the Six Rivers National Forest in far northern California. They drove in Gimlin's truck, carrying his provisions and three horses, positioned sideways. Patterson chose the area because of intermittent reports of the creatures in the past, and of their enormous footprints since 1958. (His familiarity with the area and its residents from prior visits may also have been a factor.)
The most recent of these reports was the nearby Blue Creek Mountain track find, which was investigated by journalist John Green, Bigfoot hunter René Dahinden, and archaeologist Don Abbott on and after August 28, 1967. This find was reported to Patterson (via his wife) soon thereafter by Al Hodgson, owner of the Willow Creek Variety Store, a five and dime at the time.
Though Gimlin says he doubted the existence of Sasquatch-like creatures, he agreed to Patterson's insistence that they should not attempt to shoot one.
The encounter.
As their stories went, in the early afternoon of Friday, October 20, Patterson and Gimlin were riding generally northeast (upstream) on horseback along the east bank of Bluff Creek. At sometime between 1:15 and 1:40 PM they "came to an overturned tree with a large root system at a turn in the creek, almost as high as a room." When they rounded it, "there was a logjam—a 'crow's nest'—left over from the flood of '64," and then they spotted the figure behind it nearly simultaneously. It was either "crouching beside the creek to their left" or "standing" there, on the opposite bank. Gimlin later described himself as in a mild state of shock after first seeing the figure.
Patterson initially estimated its height at six and one-half to seven feet, and later raised his estimate to about seven and one-half feet. Some later analysts, anthropologist Grover Krantz among them, have suggested Patterson's later estimate was about one foot too tall. Gimlin's estimate was six feet even.
The film shows what Patterson and Gimlin claimed was a large, hairy, bipedal, apelike figure with short, "silvery brown" or "dark reddish-brown" or "black" hair covering most of its body, including its prominent breasts. The figure in the Patterson–Gimlin film generally matches the descriptions of Bigfoot offered by others who claim to have seen one.
Patterson estimated he was about 25 ft (7.6 m) away from the creature at his closest. Patterson said that his horse reared upon sensing the figure, and he spent about twenty seconds extricating himself from the saddle, controlling his horse, getting around to its other side, and getting his camera from a saddlebag before he could run toward the figure while operating his camera. He yelled "Cover me" to Gimlin, "meaning to get the gun out." He crossed the creek on horseback after Patterson had run well beyond it, riding on a path somewhat to the left of Patterson's and somewhat beyond his position. Perez estimates he came within 60–90 feet of "Patty." Then, rifle in hand, he dismounted, but did not point his rifle at the creature.
The figure had walked away from them to a distance of about 120 ft (36.5 m) before Patterson began to run after it. The resulting film (about 59.5 seconds long at 16 fps) is initially quite shaky until Patterson got about 80 ft (24.4 m) from the figure. At that point, the figure glanced over its right shoulder at the men and Patterson fell to his knees; on Krantz's map this corresponds to frame 264. To researcher John Green, Patterson would later characterize the creature's expression as one of "contempt and disgust...you know how it is when the umpire tells you 'one more word and you're out of the game.' That's the way it felt."
Shortly after this point the steady, middle portion of the film begins, containing the famous look-back frame 352. Patterson said, "it turned a total of I think three times," the other times therefore being before the filming began and/or while he was running with his finger off the trigger. Shortly after glancing over its shoulder on film, the creature disappeared behind a grove of trees for 14 seconds, then reappeared in the film's final 15 seconds after Patterson moved ten feet to a better vantage point, fading into the trees again and being lost to view at a distance of 265 feet as the reel of film ran out.
Gimlin remounted and followed it on horseback, keeping his distance, until it disappeared around a bend in the road three hundred yards away. Patterson called him back at that point, feeling vulnerable on foot without a rifle, because he feared the creature's mate might approach. The entire encounter had lasted less than two minutes.
Next, Gimlin and Patterson rounded up Patterson's horses, which had run off in the opposite direction, downstream, before the filming began. Patterson got his second roll of film from his saddlebag and filmed the tracks. Then the men tracked "Patty" for either one mile or three miles (5 km), but "lost it in the heavy undergrowth." They went to their campsite three miles south, picked up plaster, returned to the initial site, measured the creature's step-length, and made two plaster casts, one each of the best-quality right and left prints.
Details.
According to Patterson and Gimlin, they were the only witnesses to their brief encounter with what they claimed was a Sasquatch. Their statements agree in general, but author Greg Long notes a number of inconsistencies. They offered somewhat different sequences in describing how they and the horses reacted upon seeing the creature. Patterson in particular increased his estimates of the creature's size in subsequent retellings of the encounter. In a different context, Long argues, these discrepancies would probably be considered minor, but given the extraordinary claims made by Patterson and Gimlin, any apparent disagreements in perception or memory are worth noting.
The film's defenders have responded by saying that commercially motivated hoaxers would have "got their stories straight" beforehand so they wouldn't have disagreed immediately upon being interviewed, and on so many points, and so they wouldn't have created a suit and a creature with foreseeably objectionable features and behaviors.
A more serious objection concerns the film's "timeline." This is important because Kodachrome II movie film, as far as is known, could only be developed by a lab containing a $60,000+ machine, and the few West Coast labs known to possess one did not do developing over weekends. Patterson's brother-in-law Al DeAtley claims not to remember where he took the film for development—or even where he picked it up.
Another timeline problem is that critics claim that too much happened between the filming (at 1:15 at the earliest) and the filmmakers' arrival in Willow Creek (at 6:30 at the latest). Daegling wrote, "All of the problems with the timeline disappear if the film is shot a few days or hours beforehand. If that it the case, one has to wonder what other details of this story are wrong." The film's defenders retort that although the time window was tight, it was do-able.
Chris Murphy wrote, "I have confirmed with Bob Gimlin that Patterson definitely rode a small quarter horse (which he owned), not his Welsh pony 'Peanuts'. Also, that Patterson had arranged to borrow a horse by the name of 'Chico' from Bob Heironimus for Gimlin to use. . . . Gimlin did not have a horse that was suitable (old enough) for the expedition." Heironimus stated that Chico (a middle-aged gelding) "wouldn't jump or buck. . . ."
The immediate aftermath.
At approximately 6:30 PM, Patterson and Gimlin met up with Al Hodgson at his variety store in Willow Creek, approximately 54.3 miles south by road. (It's about 28.8 miles by Bluff Creek Road from their camp to the 1967 roadhead right next to Bluff Creek, and 25.5 miles down California State Route 96 to Willow Creek.) Patterson intended to drive on to Eureka to ship his film. Either at that time, or when he arrived in the Eureka/Arcata area, he called Al DeAtley (his brother-in-law in Yakima) and told him to expect the film he was shipping. He requested Hodgson to call Donald Abbott, whom Grover Krantz described as "the only scientist of any stature to have demonstrated any serious interest in the subject," hoping he would help them search for the creature by bringing a tracking dog. Hodgson called, but Abbott declined. Krantz argued that this call the same day of the encounter is evidence against a hoax, at least on Patterson's part.
After shipping the film, they headed back toward their camp, where they had left their horses. On their way they "stopped at the Lower Trinity Ranger Station, as planned, arriving about 9:00 p.m. Here they met with Syl McCoy friend and Al Hodgson." At this point Patterson called the daily "Times-Standard" newspaper in Eureka and related his story. They arrived back at their campsite at about midnight. At either 5 or 5:30 the next morning, after it started to rain heavily, Gimlin returned to the filmsite from the camp and covered the other prints with bark to protect them. The cardboard boxes he had been given by Al Hodgson for this purpose and had left outside were so soggy they were useless, so he left them.
When he returned to the camp he and Patterson aborted their plan to remain looking for more evidence and departed for home, fearing the rain would wash out their exit. After attempting to go out along "the low road"—Bluff Creek Road—and finding it blocked by a mudslide, they went instead up the steep Onion Mountain Road, off whose shoulder their truck slipped; extracting it required the (unauthorized) borrowing of a nearby front-end loader. The drive home from their campsite covered about 580 miles, the initial 28.8 miles on a low-speed logging road, and then about 110 miles on twisty Route 96. Driving a truck with three horses, and allowing for occasional stops, it would have taken 13 hours to get home Saturday evening, at an average speed of 45 mph; it would have taken 14.5 hours at a 40 mph average speed.
US Forestry Service "Timber Management Assistant" Lyle Laverty said, "I his team of three, in a Jeep passed the site on either Thursday the 19th or Friday the 20th" and noticed no tracks. After reading the news of Patterson's encounter on their weekend break, Laverty and his team returned to the site on Monday, the 23rd, and made six photos of the tracks. (Laverty later served as an Assistant Secretary of the Interior under George W. Bush.) Taxidermist and outdoorsman Robert Titmus went to the site with his sister and brother-in-law nine days later. Titmus made plaster casts of ten successive prints of the creature and, as best he could, plotted Patterson's and the creature's movements on a map.
The long-term aftermath.
Film-related.
Grover Krantz writes that "Patterson had the film developed as soon as possible. At first he thought he had brought in proof of Bigfoot's existence and really expected the scientists to accept it. But only a few scientists were willing to even look at the film," usually at showings at scientific organizations. These were usually arranged at the behest of zoologist, author, and media figure Ivan Sanderson, a supporter of Patterson's film. Seven showings occurred, in Vancouver, Manhattan, The Bronx, Washington, D.C., Atlanta, and Washington, D.C. again (all by the end of 1968); then, later, in Beaverton, Oregon. Of those who were quoted, most expressed various reservations, although some were willing to say they were intrigued by it.
Christopher Murphy wrote, "Dahinden traveled to Europe the film in 1971. He visited England, Finland, Sweden, Switzerland and Russia. Although scientists in these countries were somewhat more open-minded than those in North America, their findings were basically the same ... A real glimmer of hope, however, emerged Russia, where he met Bayanov, Bourtsev, and their associates."
Though there was little scientific interest in the film, Patterson was still able to capitalize on it. He made a deal with the BBC, allowing it to use his footage in a docudrama they made in return for letting him tour with their docudrama, into which he melded material from his own documentary and additional material he and Al DeAtley filmed. This film was shown in local movie houses around the Pacific Northwest and Midwest. A technique commonly used for nature films called "four-walling" was employed, involving heavy local advertising, mostly on TV, of a few days of showings. It was a modest financial success. Al DeAtley estimated that his 50% of the film's profits amounted to $75,000.
The film generated a fair amount of national publicity. Patterson appeared on a few popular TV talk shows to promote the film and belief in Bigfoot by showing excerpts from it: for instance, on the "Joe Pyne Show" in Los Angeles, in 1967, which covered most of the western US; on Merv Griffin's program, with Krantz offering his analysis of the film; on Joey Bishop's talk show, and also on Johnny Carson's "Tonight Show". Articles on the film appeared in "Argosy", "National Wildlife Magazine", and "Reader's Digest".
One radio interview, with Gimlin, by Vancouver-based Jack Webster in November 1967, was partly recorded by John Green and reprinted in Loren Coleman's "Bigfoot!" Patterson also appeared on broadcast interviews on local stations near where his film would be shown during his four-walling tour in 1968.
Patterson subsequently sold overlapping distribution rights for the film to several parties, which resulted in costly legal entanglements.
After Patterson's death, Michael McLeod wrote, "With the consent of Al DeAtley and Patricia Patterson, the film distributor Ron Olson took over the operation of Northwest Research . . . and changed its name to the North American Wildlife Research Association. . . . He worked full-time compiling reports, soliciting volunteers to join the hunt, and organizing several small expeditions. A Bigfoot trap Olson and his crew built still survives . . . . Olson . . . continued to lobby the company National Enterprises to produce a Bigfoot film. . . . In 1974 . . . ANE finally agreed. . . . was released in 1975, titled "Bigfoot: Man or Beast". e devised a storyline involving members of a Bigfoot research party . . . . The film comes to a frightful end when a Bigfoot terrorized the expedition at night. Olson spent several years exhibiting the film around the country. He planned to make millions with the film, but says it lost money." Olson is profiled in Barbara Wasson's "Sasquatch Apparitions".
Joshua Buhs wrote, "During the Thanksgiving holiday of 1974, CBS aired "Mysterious Monsters", a documentary about the Loch Ness Monster and Bigfoot. (It was co-produced by the Smithsonian Institution...) The show attracted sixty million viewers, making it the highest-rated program of the week. Sunn Films took the documentary on a four-wall tour ..." This show included footage from the Patterson–Gimlin film. John Green wrote of this show's popularity, "Almost everyone is interested in monsters, only most people don't like to admit it... But let some official organization show an interest in a monster, and the situation reverses. When it has somehow been made respectable, a monster story attracts a tremendous public response."
Filmmaker-related.
Patterson's expensive ($369) 16mm camera had been rented on May 13, but he had kept it longer than the contract had stipulated, and an arrest warrant had been issued for him on October 17; he was actually arrested within weeks of his return from Bluff Creek. After Patterson returned the camera in working order, this charge was ultimately dismissed, in 1969.
While Patterson sought publicity, Gimlin was conspicuous by his absence. He only briefly helped to promote the film and avoided discussing his Bigfoot encounter publicly for many subsequent years; he turned down requests for interviews. He later reported that he had avoided publicity after Patterson and promoter Al DeAtley had broken their agreement to pay him a one-third share of any profits generated by the film. Another factor was that his wife objected to publicity.
Daegling wrote, "Bigfoot advocates emphasize that Patterson remained an active Bigfoot hunter up until his death." For instance, in 1969, he hired a pair of brothers to travel around in a truck chasing down leads to Bigfoot witnesses and interviewing them. Later, in December of that year, he was one of those present in Bossburg, Washington, in the aftermath of the cripplefoot tracks found there. Krantz reports that "few years after the film was made, Patterson received a letter from a man ["a US airman stationed in Thailand" who assured him a Sasquatch was being held in a Buddhist monastery. Patterson spent most of his remaining money preparing an expedition to retrieve this creature" only to learn it was a hoax. He learned this only after having sent Dennis Jenson fruitlessly to Thailand (where he concluded that the airman was "mentally unbalanced") and then, after receiving a second untrue letter from the man, going himself to Thailand with Jenson.
To obtain money to travel to Thailand, "Patterson called Ron, who had returned to ANE, and sold the company the theatrical rights to the clip for what Olson described as a pretty good sum of money."
Patterson died of Hodgkin's lymphoma in 1972. According to Michael McLeod, Greg Long, and Bill Munns, "A few days before Roger died, he told author Peter Byrne that in retrospect, . . . he he would have shot the thing and brought out a body instead of a reel of film." According to Grover Krantz and Robert Pyle, years later, Patterson and Gimlin both agreed they should have tried to shoot the creature, both for financial gain and to silence naysayers.
In 1995, almost three decades after the Patterson–Gimlin filming, Greg Long, a technical writer for a technology firm who had a hobby of investigating and writing about Northwest mysteries, started years of interviewing people who knew Patterson, some of whom described him as a liar and a conman. 
Legal status.
Greg Long reports that a 1978 legal "settlement gave Dahinden controlling rights—51 percent of the film footage, 51 percent of video cassette rights, and 100 percent of all 952 frames of the footage. Patty Patterson had 100 percent of all TV rights and 49 percent rights in the film footage. Dahinden had ... bought out Gimlin, who himself had received nothing from Patterson; and Mason and Radford, promised part of the profits by Patterson, had nothing to show for their investment or efforts."
Frame 352, the well-known look-back image, is in the public domain, having long been reprinted by others without protest by the copyright holder.
Ownership of the physical films.
The first reel.
The whereabouts of the original is unknown, although there are several speculations, mostly online, as to what happened to it. 
At least seven copies were made of the original film
Bill Munns listed four other missing reels of derivative works that would be helpful to film analysts.
The second reel.
The second reel, showing Patterson and Gimlin making and displaying plaster casts of some footprints, was not shown in conjunction with the first reel at Al DeAtley's house, according to those who were there. Chris Murphy wrote, "I believe the screening of this roll at the University of British Columbia on October 26, 1967, was the first and last major screening." It has subsequently been lost. 
A ten-foot strip from that reel, or from a copy of that reel, from which still images were taken by Chris Murphy, still exists, but it too has gone missing.
Filming speed.
One factor that complicates discussion of the Patterson film is that Patterson said he normally filmed at 24 frames per second, but in his haste to capture the Bigfoot on film, he did not note the camera's setting. His Cine-Kodak K-100 camera had markings on its continuously variable dial at 16, 24, 32, 48, and 64 frames per second, but no click-stops, and was capable of filming at any frame speed within this range. Grover Krantz wrote, "Patterson clearly told John Green that he found, after the filming, that the camera was set on 18 frames per second (fps) . . . ." It has been suggested that Patterson simply misread "16" as "18".
Analysis.
The Patterson–Gimlin film has seen relatively little interest from mainstream scientists. Statements of scientists who viewed the film at a screening, or who conducted a study, are reprinted in Chris Murphy's "Bigfoot Film Journal". Typical objections include: Neither humans nor chimpanzees have hairy breasts as does the figure in the film, and Napier has noted that a sagittal crest is "only very occasionally seen, to an insignificant extent, in chimpanzees females." Critics have argued these features are evidence against authenticity. Krantz countered the latter point, saying "a sagittal crest ... is a consequence of absolute size alone."
As anthropologist David Daegling writes, "skeptics have not felt compelled to offer much of a detailed argument against the film; the burden of proof, rightly enough, should lie with the advocates." Yet, without a detailed argument against authenticity, Daegling notes that "the film has not gone away." Similarly, Krantz argues that of the many opinions offered about the Patterson film, "[only a few of these opinions are based on technical expertise and careful study of the film itself."
Regarding the quality of the film, second-generation copies or copies from TV and DVD productions are inferior to first-generation copies. Many early frames are blurry due to camera shake, and the quality of subsequent frames varies for the same reason. Stabilization of the film (e.g., by M.K. Davis) to counter the effect of camera shake has improved viewers' ability to analyze it. Regarding "graininess," Bill Munns writes, "Based on transparencies taken off the camera original, . . . the PGF original is as fine grain as any color 16mm film can achieve." He adds that graininess increases as images are magnified.
Scientific studies favorable to the film.
Dmitri Bayanov et al..
Dimitri Bayanov, Igor Bourtsev, and René Dahinden authored "Analysis of the Patterson–Gimlin Film, Why We Find It Authentic." It is a study with sections examining the technical characteristics of the footage, the filming speed, the morphology of the creature, and the specimen's movements. It ended with an assessment and a conclusion favorable to the film subject's reality. The most notable sentences are these: "Nikita Lavinsky argues that the better a costume from the anatomical point of view, the worse it would be from the viewpoint of biomechanics. A clever costume on a moving hoaxer would "expose, not conceal" a fraud." Later, Bayanov "felt the North American monster enthusiasts did not pursue the issues with enough vigor" and more funding from the ISC.
Dmitri Donskoy.
A formal academic study of the Patterson film was conducted by Dmitri Donskoy, Chief of the Dept. of Biomechanics at the USSR Central Institute of Physical Culture, and later associated with Moscow's Darwin Museum.
Donskoy concluded the creature was non-human on the basis of its weight, and especially its gait, which Donskoy judged would be difficult, if not impossible, for a human to replicate. He inferred the film's subject was weighty from the ponderous momentum he observed in the movements of its arms and legs, in the sagging of the knee as weight came onto it, and in the flatness of the foot. Its gait he considered non-artificial because it was confident and unwavering, "neatly expressive," and well-coordinated—and yet non-human because its arm motion and glide resembled a cross-country skier's. Krantz describes Donskoy's conclusion as being that the film depicts "a very massive animal that is definitely not a human being."
Reuben Steindorf.
Jeff Meldrum wrote, "Animator and computer-generated effects expert Reuben Steindorf, of Vision Realm, created a computer model of 'Patty,' as the film subject has been nicknamed. ... Steindorf reconstructed Patty's skeletal anatomy from the ground up, using 'reverse kinematics.'" Among other findings, "Steindorf confirmed ... that the upper extremity was rather long compared to the lower. This ratio can be expressed as an intermembral index (IM). ... The intermembral index for the film subject was approximately 88. ... An average human IM is 71."
Gordon Strasenburgh.
Gordon Strasenburgh authored "The Crested Australopithecus Robustus and the Patterson–Gimlin Film". After an introduction, it reviews four other studies and opinions by scientists on the film, then goes on with the following sections: Testing Hypotheses, Anthropological Attitudes, Alternatives to "A. robustus", The Basic Problem, and Conclusion. The last section begins, "The Patterson–Gimlin film and its analytical results to date are the best data."
Jeff Glickman.
Glickman is a Certified Forensic Examiner who "performed intensive computer analysis on the Patterson/Gimlin film over a period of three years." His 43-page study, written in a scientific format, contains 13 pages about the film. He gave estimated measurements of the creature, including a very high weight estimate that few have accepted. He was unable to find evidence of fakery, but noted several indications of authenticity. His conclusions are summed up on one page in two of Christopher Murphy's books. Background information on Glickman's project, sponsored by the North American Science Institute (NASI), can be found in another of Murphy's books.
Grover Krantz.
Anthropologist Grover Krantz was originally skeptical of the Patterson film, based on the still photos in Argosy Magazine, but changed his mind in 1969 after seeing the film because "the realism of the creature's locomotion impressed him." He later offered an in-depth examination of the Patterson film. He concluded that the film depicts a genuine unknown creature. Primarily, Krantz's argument is based on a detailed analysis of the figure's stride, center of gravity, and biomechanics. Krantz argues that the creature's leg and foot motions are quite different from a human's and could not have been duplicated by a person wearing a gorilla suit. Krantz wrote, "the knee is regularly bent more than 90°, while the human leg bends less than 70°." Daniel Perez brought out the implication of this, writing, "The subject['s] ... toes lift off the soil at least ten inches in every walking cycle. ... René Dahinden ... filmed and studied how modern man walks, finding ... a maximum of 2–3 inches of distance between toes and the surface it is walking over . . . ." No human has yet replicated this 10"-high lower leg lift while maintaining the smoothness, posture, and stride length (41") of the creature.
Krantz pointed out the tremendous width of the creature's shoulders, which (after deducting 1" for hair) he estimated at 28.2 inches, or 35.1% of its full standing height of 78", or a higher percentage of its 72" "walking height," which was a bit stooped, crouched, and sunk into the sand. The creature's shoulders are almost 50% wider than the human mean. (For comparison, André the Giant had a typical human ratio of 24%. Wide-shouldered Bob Heironimus (see below) has 27.4%. Only very rarely do humans have a shoulder breadth of 30%.) Krantz argued that a suited person could not mimic this breadth and still have the naturalistic hand and arm motions present on the film.
Krantz and others have noted natural-looking musculature visible as the creature moved, arguing this would be highly difficult or impossible to fake. Hunter and Dahinden also note that "the bottom of the figure's head seems to become part of the heavy back and shoulder muscles... the muscles of the buttocks were distinct."
Jeffrey Meldrum.
Jeffrey Meldrum of Idaho State University cites efforts by John Green as important in his own studies of the Patterson film. "It has been obvious to even the casual viewer that the film subject possesses arms that are disproportionately long for its stature." Meldrum writes that "Anthropologists typically express limb proportions as an intermembral index (IM)" and notes that humans have an average IM index of 72, gorillas an average IM index of 117 and chimpanzees an average IM index of 106.
In determining an IM index for the figure in the Patterson film, Meldrum concludes the figure has "an IM index somewhere between 80 and 90, intermediate between humans and African apes. In spite of the imprecision of this preliminary estimate, it is well beyond the mean for humans and effectively rules out a man-in-a-suit explanation for the Patterson–Gimlin film without invoking an elaborate, if not inconceivable, prosthetic contrivance to account for the appropriate positions and actions of wrist and elbow and finger flexion visible on the film. This point deserves further examination and may well rule out the probability of hoaxing."
In his book, Meldrum says, "Steindorf tracked the joint centers through 116 frames of the film, yielding a reliable estimate of the film subject's limb proportions. ... The combination of these proportions with the exceptional breadth dimensions argue[s compellingly against the simplistic hypothesis of an average man, even one wearing shoulder pads ... or using artificial arm extensions."
However, scientist Esteban Sarmiento (see below) disagrees that the subject has a non-human IMI.
Dr. Scott Lynn, Associate Professor of Kinesiology, California State University, was another scientist who reached a favorable conclusion.
Scientific studies unfavorable to the film.
Bernard Heuvelmans.
Bernard Heuvelmans—a zoologist and the so-called "father of cryptozoology"—thought the creature in the Patterson film was a suited human. He objected to the film subject's hair-flow pattern as being too uniform; to the hair on the breasts as not being like a primate; to its buttocks as being insufficiently separated; and to its too-calm retreat from the pursuing men. However, Brian Regal writes, "The father of cryptozoology, who believed the Patterson film a hoax, hesitated in his original assessment of the film because of Krantz's analysis," but nevertheless remained a disbeliever in it.
D.W. Grieve.
Anatomist D.W. Grieve of the Royal Free Hospital School of Medicine studied a copy of the film in 1971, and wrote a detailed analysis. He notes, "The possibility of a very clever fake cannot be ruled out on the evidence of the film" but also writes that his analysis hinges largely on the question of filming speed.
Grieve concluded that "the possibility of fakery is ruled out if the speed of the film was 16 or 18 frames per second. In these conditions a normal human being could not duplicate the observed pattern, which would suggest that the Sasquatch must possess a very different locomotor system to that of man." If filmed at the higher speed, Grieve concluded that the creature "walked with a gait pattern very similar in most respects to a man walking at high speed."
Grieve stated, "I can see the muscle masses in the appropriate places... If it is a fake, it is an extremely clever one." Like Krantz, Grieve thought the figure's shoulders were quite broad. Also like Krantz, Grieve thought Patterson's estimate of the figure's height was inaccurate. Grieve concluded the figure in the Patterson film revealed "an estimated standing height for the subject of not more than ." He notes that a tall human is consistent with the figure's height but also notes that for a tall human "he shoulder breadth however would be difficult to achieve without giving an unnatural appearance to the arm swing and shoulder contours."
Grieve notes that his "subjective impressions have oscillated between total acceptance of the Sasquatch based on the grounds that the film would be difficult to fake, to one of irrational rejection based on an emotional response to the possibility that the Sasquatch actually exists. This seems worth stating because others have reacted similarly to the film."
Krantz claimed Grieve made errors in his measurements and reference points. And John Green criticized his measurements and his reasoning.
John Napier.
Prominent primate expert John Napier (one-time director of the Smithsonian's Primate Biology Program) was one of the few mainstream scientists not only to critique the Patterson–Gimlin film but also to study then-available Bigfoot evidence in a generally sympathetic manner, in his 1973 book, "Bigfoot: The Sasquatch and Yeti in Myth and Reality".
Napier conceded the likelihood of Bigfoot as a real creature, stating, "I am convinced that Sasquatch exists." But he argued against the film being genuine: "There is little doubt that the scientific evidence taken collectively points to a hoax of some kind. The creature shown in the film does not stand up well to functional analysis." Napier gives several reasons for his and other's skepticism that are commonly raised, but apparently his main reasons are original with him. First, the length of "the footprints are totally at variance with its calculated height." Second, the footprints are of the "hourglass" type, which he is suspicious of. (In response, Barbara Wasson criticized Napier's logic at length.)
He adds, "I could not see the zipper; and I still can't. There I think we must leave the matter. Perhaps it was a man dressed up in a monkey-skin; if so it was a brilliantly executed hoax and the unknown perpetrator will take his place with the great hoaxers of the world. Perhaps it was the first film of a new type of hominid, quite unknown to science, in which case Roger Patterson deserves to rank with Dubois, the discoverer of "Pithecanthropus erectus", or Raymond Dart of Johannesburg, the man who introduced the world to its immediate human ancestor, "Australopithecus africanus"."
The skeptical views of Grieve and Napier are summarized favorably by Kenneth Wylie (and those of Bayanov and Donskoy negatively) in Appendix A of his 1980 book, "Bigfoot: A Personal Inquiry into a Phenomenon".
Esteban Sarmiento.
Esteban Sarmiento is a specialist in physical anthropology at the American Museum of Natural History. He has 25 years of experience with great apes in the wild. He writes, "I did find some inconsistencies in appearance and behavior that might suggest a fake ... but nothing that conclusively shows that this is the case." His most original criticism is this: "The plantar surface of the feet is decidedly pale, but the palm of the hand seems to be dark. There is no mammal I know of in which the plantar sole differs so drastically in color from the palm." (But see Meldrum, 170–71.) His most controversial statements are these: "The gluteals, although large, fail to show a humanlike cleft (or crack)." "Body proportions: ... In all of the above relative values, bigfoot is well within the human range and differs markedly from any living ape and from the 'australopithecine' fossils." (E.g., the IM index is in the normal human range.) And: "I estimate bigfoot's weight to be between 190 and 240 lbs."
David J. Daegling and Daniel O. Schmitt.
When anthropologists David J. Daegling of the University of Florida and Daniel O. Schmitt examined the film, they concluded it was impossible to conclusively determine if the subject in the film is nonhuman, and additionally argued that flaws in the studies by Krantz and others invalidated their claims. Daegling and Schmitt noted problems of uncertainties in the subject and camera positions, camera movement, poor image quality, and artifacts of the subject. They concluded: "Based on our analysis of gait and problems inherent in estimating subject dimensions, it is our opinion that it is not possible to evaluate the identity of the film subject with any confidence."
Daegling has asserted that the creature's odd walk could be replicated: "Supposed peculiarities of subject speed, stride length, and posture are all reproducible by a human being employing this type of locomotion "compliant gait"."
Daegling notes that in 1967, movie and television special effects were primitive compared to the more sophisticated effects in later decades, and allows that if the Patterson film depicts a man in a suit that "it is not unreasonable to suggest that it is better than some of the tackier monster outfits that got thrown together for television at that time."
Jessica Rose and James Gamble.
Jessica Rose and James Gamble are authors of "the definitive text on human gait", "Human Walking". They operate the Motion and Gait Analysis Lab at Stanford University. They conducted a high-tech human-replication attempt of "Patty's" gait, in cooperation with Jeff Meldrum. Rose was certain their subject had matched Patty's gait, while Gamble was not quite as sure. Meldrum was impressed and acknowledged that "some aspects" of the creature's walk had been replicated, but not all. The narrator said, "even the experts can see the gait test could not replicate all parameters of the gait." It was shown in an episode of the Discovery Channel's "Best Evidence" series.
Other analysts.
M.K. Davis.
Developments in computer technology permitted enhancements of the Patterson–Gimlin films to be made. Bigfoot enthusiast M.K. Davis created a version that removes the shakiness of the camera, permitting the creature to be seen from a more stable perspective. Davis has produced a second stabilized version incorporating enlargements of specific elements that he believes are significant.
Nike researcher Gordon Valient.
Krantz also showed the film to Gordon Valient, a researcher for Nike shoes, who he says "made some rather useful observations about some rather unhuman movements he could see."
"MonsterQuest".
A first-season episode of "MonsterQuest" focuses on the Bigfoot phenomenon. One pair of scientists, Jurgen Konczak (Director, Human Sensorimotor Control Laboratory, University of Minnesota) and Esteban Sarmiento, attempts and fails to get a mime outfitted with LEDs on his joints to mimic the Patterson Bigfoot's gait. A second pair, Daris Swindler and Owen Caddy, employs digital enhancement and observes facial movements, such as moving eyelids, lips that compress like an upset chimp's, and a mouth that is lower than it appears, due to a false-lip anomaly like that of a chimp's. (Unfortunately, the show's narrator falsely claims, three times, that the original film shot by Patterson was used.) The episode concludes, "the new findings are intriguing but inconclusive, until a body is found."
Film industry personnel.
Bill Munns.
Bill Munns, retired, was a special effects and make-up artist, cameraman, and film editor. He argues that Universal and Disney were not the most knowledgeable studios to consult with. He says that Fox, MGM, and special effects artist Stuart Freeborn in England, "who had just completed his groundbreaking ape suits for ""," would have been preferable.
Munns started posting his online analysis of the film in 2009 and summarizing it in the online Munns Report. In 2013 he and Jeff Meldrum co-authored three papers in Meldrum's online magazine, "Relict Hominoid Inquiry". In 2014, Munns self-published "When Roger Met Patty", a 488-page book incorporating material from those articles that analyses the film and film subject from various perspectives.
He argues the film depicts a non-human animal, not a man in a fur suit. He proposes a new diagnostic test of authenticity, at the armpit: natural concave skin fold vs. artificial vertical crease. Munns' analysis has been featured in an episode of the History Channel series "MonsterQuest."
Hoax allegations.
The major hoax allegations are summarized and criticized in:
Philip Morris.
In 2002, Philip Morris, owner of Morris Costumes (a North Carolina-based company offering costumes, props and stage products) claimed that he made a gorilla costume that was used in the Patterson film. Morris says he discussed his role in the hoax "at costume conventions, lectures, magician conventions" in the 1980s, but first addressed the public at large on August 16, 2002, on Charlotte, North Carolina, radio station WBT-AM. His story was also printed in "The Charlotte Observer". Morris claims he was reluctant to expose the hoax earlier for fear of harming his business: giving away a performer's secrets, he said, would be widely regarded as disreputable.
Morris said that he sold an ape suit to Patterson via mail order in 1967, thinking it was going to be used in what Patterson described as a "prank." (Ordinarily the gorilla suits he sold were used for a popular sideshow routine that depicted an attractive woman changing into a gorilla.) After the initial sale, Morris said that Patterson telephoned him asking how to make the "shoulders more massive" and the "arms longer." Morris says he suggested that whoever wore the suit should wear football shoulder pads and hold sticks in his hands within the suit.
As for the creature's walk, Morris said:
The Bigfoot researchers say that no human can walk that way in the film. Oh, yes they can! When you're wearing long clown's feet, you can't place the ball of your foot down first. You have to put your foot down flat. Otherwise, you'll stumble. Another thing, when you put on the gorilla head, you can only turn your head maybe a quarter of the way. And to look behind you, you've got to turn your head and your shoulders and your hips. Plus, the shoulder pads in the suit are in the way of the jaw. That's why the Bigfoot turns and looks the way he does in the film. He has to twist his entire upper body.
Morris' wife and business partner Amy had vouched for her husband and claims to have helped frame the suit. Morris offered no evidence apart from his own testimony to support his account, the most conspicuous shortcoming being the absence of a gorilla suit or documentation that would match the detail evidenced in the film and could have been produced in 1967.
A re-creation of the PGF was undertaken in October 6, 2004, at "Cow Camp," near Rimrock Lake, a location 41 miles west of Yakima. This was six months after the publication of Long's book and 11 months after Long had first contacted Morris. Bigfooter Daniel Perez wrote, ""National Geographic's" Noel Dockster . . . noted the suit used in the re-creation ... was in no way similar to what was depicted in the P–G film."
Morris wouldn't consent to release the video to National Geographic, the re-creation's sponsor, claiming he hadn't had adequate time to prepare and that the month was in the middle of his busy season. However, he has not attempted to create a suit more to his liking since that time.
Bob Heironimus.
Bob Heironimus claims to have been the figure depicted in the Patterson film. Heironimus says he had not previously publicly discussed his role in the hoax because he hoped to be paid eventually and was afraid of being convicted of fraud had he confessed. After speaking with his lawyer he was told that since he had not been paid for his involvement in the hoax, he could not be held accountable.
A month after watching the December 28, 1998, Fox-television special "World's Greatest Hoaxes: Secrets Finally Revealed?", he went public, via a January 30 press release by his lawyer, Barry Woodard, in a Yakima newspaper story. He stated, "I'm telling the truth. I'm tired after thirty-seven years." Five days later, a second newspaper story reported that his "lawyer's office has been inundated with calls from media outlets . . . . 'We're just sort of waiting for the dust to settle,' he said, explaining he and his client are evaluating offers." He also said, "We anticipate that we will be telling the full story to somebody rather quickly."
Heironimus's name was first publicly revealed, and his allegations first publicly detailed, five years later, in Greg Long's book, "The Making of Bigfoot", which includes testimony that corroborates Heironimus' claims: 
Long argues that the suit Morris says he sold to Patterson was the same suit Heironimus claims to have worn in the Patterson film. However, Long quotes Heironimus and Morris describing different ape suits in many respects. Among the notable differences are:
Long speculates that Patterson modified the costume, but only by attaching Morris's loose hands and feet to the costume, and by replacing Morris's mask. However, there's nothing he wrote on "suit" modification. There's no evidence or testimony that Patterson changed the Morris suit to horsehide, or dyed it a darker color, or cut it in half at the waist to agree with Heironimus's description.
Some film proponents say that Heironimus' arms are too short to match that of a Bigfoot and that he was a few inches shorter than the creature on the film (up to 14 inches shorter).
It has also been said that Heironimus was not as bulky as the creature, but film critics claim that a suit could correct for that (and for height). However, Heironimus did not mention there being padding in the torso, either when questioned by Long about the suit or when specifically asked about padding by Rob McConnell in his 2nd "XZone" radio interview, on August 6, 2007.
Polygraph tests regarding their claims have been passed by both Heironimus and Patterson.
Ray Wallace.
After the death of Ray Wallace in 2002, following a request by Loren Coleman to "The Seattle Times" reporter Bob Young to investigate, the family of Wallace went public with claims that he had started the Bigfoot phenomenon with fake footprints (made from a wooden foot-shaped cutout) left in Californian sites in 1958.

</doc>
<doc id="23513" url="https://en.wikipedia.org/wiki?curid=23513" title="Producer">
Producer

Producer or producers may refer to:

</doc>
<doc id="23517" url="https://en.wikipedia.org/wiki?curid=23517" title="List of Polish-language poets">
List of Polish-language poets

List of poets who have written much of their poetry in the Polish language. See also Discussion Page for additional poets not listed here.
There have been four Polish Nobel Prize laureates in literature: Henryk Sienkiewicz, Władysław Reymont, Czesław Miłosz, Wisława Szymborska. The last two have been poets.

</doc>
<doc id="23519" url="https://en.wikipedia.org/wiki?curid=23519" title="Paul Valéry">
Paul Valéry

Ambroise-Paul-Toussaint-Jules Valéry (; ; 30 October 1871 – 20 July 1945) was a French poet, essayist, and philosopher. In addition to his poetry and fiction (drama and dialogues), his interests included aphorisms on art, history, letters, music, and current events. Valéry was nominated for the Nobel Prize in Literature in 12 different years.
Biography.
Valéry was born to a Corsican father and Genoese-Istrian mother in Sète, a town on the Mediterranean coast of the Hérault, but he was raised in Montpellier, a larger urban center close by. After a traditional Roman Catholic education, he studied law at university, then resided in Paris for most of the remainder of his life, where he was, for a while, part of the circle of Stéphane Mallarmé.
In 1900, he married Jeannie Gobillard, a friend of Stéphane Mallarmé's family, who was also a niece of the painter Berthe Morisot. The wedding was a double ceremony in which the bride's cousin, Morisot's daughter, Julie Manet married the painter, Ernest Rouart. Valéry and Gobillard had three children: Claude, Agathe, and François.
Valéry served as a juror with Florence Meyer Blumenthal in awarding the Prix Blumenthal, a grant given between 1919 and 1954 to young French painters, sculptors, decorators, engravers, writers, and musicians.
Though his earliest publications date from his mid-twenties, Valéry did not become a full-time writer until 1920, when the man for whom he worked as private secretary, a former chief executive of the Havas news agency, Edouard Lebey, died of Parkinson's disease. Until then, Valéry had, briefly, earned his living at the Ministry of War before assuming the relatively flexible post as assistant to the increasingly impaired Lebey, a job he held for some twenty years.
After his election to the Académie française in 1925, Valéry became a tireless public speaker and intellectual figure in French society, touring Europe and giving lectures on cultural and social issues as well as assuming a number of official positions eagerly offered to him by an admiring French nation. He represented France on cultural matters at the League of Nations, and he served on several of its committees. "The Outlook for Intelligence" (1989) contains English translations of a dozen essays resulting from these activities.
In 1931, he founded the Collège International de Cannes, a private institution teaching French language and civilization. The Collège is still operating today, offering professional courses for native speakers (for educational certification, law and business) as well as courses for foreign students.
He gave the keynote address at the 1932 German national celebration of the 100th anniversary of the death of Johann Wolfgang Goethe. This was a fitting choice, as Valéry shared Goethe's fascination with science (specifically, biology and optics).
In addition to his activities as a member of the Académie française, he was also a member of the Academy of Sciences of Lisbon, and of the "Front national des Ecrivains". In 1937, he was appointed chief executive of what later became the University of Nice. He was the inaugural holder of the Chair of Poetics at the Collège de France.
During World War II, the Vichy regime stripped him of some of these jobs and distinctions because of his quiet refusal to collaborate with Vichy and the German occupation, but Valéry continued, throughout these troubled years, to publish and to be active in French cultural life, especially as a member of the Académie française.
Valéry died in Paris in 1945. He is buried in the cemetery of his native town, Sète, the same cemetery celebrated in his famous poem, "le Cimetière marin".
Work.
The great silence.
Valéry is best known as a poet, and he is sometimes considered to be the last of the French symbolists. However, he published fewer than a hundred poems, and none of them drew much attention. On the night of 4 October 1892, during a heavy storm, Paul Valéry underwent an existential crisis, an event that made a huge impact on his writing career. Eventually, around 1898, he quit writing altogether, publishing not a word for nearly twenty years. This hiatus was in part due to the death of his mentor, Stéphane Mallarmé. When, in 1917, he finally broke his 'great silence' with the publication of "La Jeune Parque"; he was forty-six years of age.
"La Jeune Parque".
This obscure, but sublimely musical, masterpiece, of 512 alexandrine lines in rhyming couplets, had taken him four years to complete, and it immediately secured his fame. With "Le Cimetière marin" and "L'Ebauche d'un serpent," it is often considered one of the greatest French poems of the twentieth century.
The title was chosen late in the poem's gestation; it refers to the youngest of the three "Parcae" (the minor Roman deities also called "The Fates"), though for some readers the connection with that mythological figure is tenuous and problematic.
The poem is written in the first person, and is the soliloquy of a young woman contemplating life and death, engagement and withdrawal, love and estrangement, in a setting dominated by the sea, the sky, stars, rocky cliffs, and the rising sun. However, it is also possible to read the poem as an allegory on the way fate moves human affairs or as an attempt to comprehend the horrific violence in Europe at the time of the poem's composition. The poem is not about World War I, but it does try to address the relationships between destruction and beauty, and, in this sense, it resonates with ancient Greek meditations on these matters, especially in the plays of Sophocles and Aeschylus. There are, therefore, evident links with "le Cimetière marin", which is also a seaside meditation on comparably large themes.
Other works.
Before "la Jeune Parque", Valéry's only publications of note were dialogues, articles, some poems, and a study of Leonardo da Vinci. In 1920 and 1922, he published two slim collections of verses. The first, "Album des vers anciens" (Album of ancient verses), was a revision of early but beautifully wrought smaller poems, some of which had been published individually before 1900. The second, "Charmes" (from the Latin "carmina", meaning "songs" and also "incantations"), further confirmed his reputation as a major French poet. The collection includes "le Cimetière marin", and many smaller poems with diverse structures. 'Le Cimetière marin' is mentioned or indirectly implied or referred to in at least four of Iris Murdoch's novels, The Unicorn, The Time of the Angels, The Nice and the Good and The Sea, The Sea.
Technique.
Valéry's technique is quite orthodox in its essentials. His verse rhymes and scans in conventional ways, and it has much in common with the work of Mallarmé. His poem, "Palme", inspired James Merrill's celebrated 1974 poem "Lost in Translation", and his cerebral lyricism also influenced the American poet, Edgar Bowers.
Prose works.
His far more ample prose writings, peppered with many aphorisms and "bons mots", reveal a conservative and skeptical outlook on human nature, verging on the cynical. However, he never said or wrote anything giving aid or comfort to any form of totalitarianism popular during his lifetime.
Raymond Poincaré, Louis de Broglie, André Gide, Henri Bergson, and Albert Einstein all respected Valéry's thinking and became friendly correspondents. Valéry was often asked to write articles on topics not of his choosing; the resulting intellectual journalism was collected in five volumes titled "Variétés".
The notebooks.
Valéry's most striking achievement is perhaps his monumental intellectual diary, called the "Cahiers" (Notebooks). Early every morning of his adult life, he contributed something to the "Cahiers", prompting him to write: "Having dedicated those hours to the life of the mind, I thereby earn the right to be stupid for the rest of the day."
The subjects of his "Cahiers" entries often were, surprisingly, reflections on science and mathematics. In fact, arcane topics in these domains appear to have commanded far more of his considered attention than his celebrated poetry. The "Cahiers" also contain the first drafts of many aphorisms he later included in his books. To date, the "Cahiers" have been published in their entirety only as photostatic reproductions, and only since 1980 have they begun to receive scholarly scrutiny. The "Cahiers" have been translated into English in five volumes published by Peter Lang with the title "Cahiers/Notebooks".
Valéry is currently considered a touchstone for those interested in constructivist epistemology, for instance, in Jean-Louis Le Moigne's description of constructivist history.
In popular culture.
Oscar-winning Japanese director Hayao Miyazaki's 2013 film "The Wind Rises" takes its title from Valery's verse "Le vent se lève!... Il faut tenter de vivre!" ("The wind rises… We must try to live!") in the poem "Le Cimetière marin" ("The Graveyard By The Sea").
Selected works.
In English translation:

</doc>
<doc id="23528" url="https://en.wikipedia.org/wiki?curid=23528" title="Pianist">
Pianist

A pianist ( , ) is an individual musician who plays the piano. Most forms of Western music can make use of the piano. Consequently, pianists have a wide variety of repertoire and styles to choose from, including traditionally classical music, jazz, blues and all sorts of popular music, including rock music. Most pianists can, to a certain extent, play other keyboard-related instruments such as the synthesizer, harpsichord, celesta and the organ.
Pianists past and present.
Modern classical pianists dedicate their careers to performing, recording, teaching, researching as well as learning new works/expanding their repertoire. They generally do not write or transcribe music as pianists did in the 19th century. Some classical pianists might specialize in accompaniment and chamber music while others (relatively few) will perform as full-time piano soloists.
Classical.
Mozart could be considered the first "concert pianist" as he performed widely on the piano. Composers Beethoven and Clementi from the classical era were also famed for their playing, as were, from the romantic era, Liszt, Brahms, Chopin, Mendelssohn and Rachmaninoff. From that era, leading performers less known as composers were Clara Schumann and Hans von Bülow. However, as we do not have modern audio recordings of most of these pianists, we rely mainly on written commentary to give us an account of their technique and style.
Jazz.
Jazz pianists almost always perform with other musicians. Their playing is freer than that of classical pianists and they create an air of spontaneity in their performances. They generally do not write down their compositions; improvisation is a significant part of their work. Well known Jazz pianists include Art Tatum, Duke Ellington, Thelonious Monk, Oscar Peterson and Herbie Hancock.
Popular pianists might work as live performers (concert, theatre, etc.), session musicians, arrangers most likely feel at home with synthesizers and other electronic keyboard instruments. Notable popular pianists include Victor Borge who performed as a comedian; Richard Clayderman, who is known for his covers of popular tunes; and singer and entertainer Liberace, who at the height of his fame, was one of the highest paid entertainers in the world.
Well known pianists.
A single listing of pianists in all genres would be impractical, given the multitude of musicians noted for their performances on the instrument. Below are links to lists of well-known or influential pianists divided by genres:
Pianists-composers.
Many important composers were also virtuoso pianists. The following is an incomplete list of such musicians.
Amateur pianists.
Some people, having received a solid piano training in their youth, decide not to continue their musical careers but choose nonmusical ones. As a result, there are prominent communities of "amateur pianists" all over the world that play at quite a high level and give concerts just because of their love to music, but not to earn money. The International Piano Competition for Outstanding Amateurs, held annually in Paris, attracts about one thousand listeners each year and is broadcast on French radio. It is also a notable fact that Jon Nakamatsu, the Gold Medal winner of the prestigious Van Cliburn International Piano Competition for professional pianists in Fort Worth, Texas (1997) was at the moment of his victory technically an amateur: he never attended a music conservatory or major in music, and worked as a high school German teacher then. It was only after the competition that he started pursuing a career as a classical pianist.
The German pianist Davide Martello is known for travelling around conflict zones to play his moving piano. Martello has previously been recognised by the European parliament for his “outstanding contribution to European cooperation and the promotion of common values”.

</doc>
<doc id="23529" url="https://en.wikipedia.org/wiki?curid=23529" title="Proverb">
Proverb

A proverb (from ) is a simple and concrete saying, popularly known and repeated, that expresses a truth based on common sense or experience. They are often metaphorical. A proverb that describes a basic rule of conduct may also be known as a maxim. Proverbs fall into the category of formulaic language.
Proverbs are often borrowed from similar languages and cultures, and sometimes come down to the present through more than one language. Both the Bible (including, but not limited to the Book of Proverbs) and medieval Latin (aided by the work of Erasmus) have played a considerable role in distributing proverbs across Europe. Mieder has concluded that cultures that treat the Bible as their "major spiritual book contain between three hundred and five hundred proverbs that stem from the Bible." However, almost every culture has examples of its own unique proverbs.
Definitions.
Defining a “proverb” is a difficult task. Proverb scholars often quote Archer Taylor’s classic “The definition of a proverb is too difficult to repay the undertaking... An incommunicable quality tells us this sentence is proverbial and that one is not. Hence no definition will enable us to identify positively a sentence as proverbial”. Another common definition is from Lord John Russell (c. 1850) “A proverb is the wit of one, and the wisdom of many.” 
More constructively, Mieder has proposed the following definition, “A proverb is a short, generally known sentence of the folk which contains wisdom, truth, morals, and traditional views in a metaphorical, fixed, and memorizable form and which is handed down from generation to generation.” Norrick created a table of distinctive features to distinguish proverbs from idioms, cliches, etc. Prahlad distinguishes proverbs from some other, closely related types of sayings, “True proverbs must further be distinguished from other types of proverbial speech, e.g. proverbial phrases, Wellerisms, maxims, quotations, and proverbial comparisons.” Based on Persian proverbs, Zolfaghari and Ameri propose the following definition: "A proverb is a short sentence, which is well-known and at times rhythmic, including advice, sage themes and ethnic experiences, comprising simile, metaphor or irony which is well-known among people for its fluent wording, clarity of expression, simplicity, expansiveness and generality and is used either with or without change"
There are many sayings in English that are commonly referred to as “proverbs”, such as weather sayings. Alan Dundes, however, rejects including such sayings among truly proverbs: “Are weather proverbs proverbs? I would say emphatically 'No!'” The definition of “proverb” has also changed over the years. For example, the following was labeled “A Yorkshire proverb” in 1883, but would not be categorized as a proverb by most today, “as throng as Throp's wife when she hanged herself with a dish-cloth.” The changing of the definition of "proverb" is also noted in Turkish.
In other languages and cultures, the definition of “proverb” also differs from English. In the Chumburung language of Ghana, ""aŋase" are literal proverb and "akpare" are metaphoric ones.” Among the Bini of Nigeria, there are three words that are used to translate "proverb": "ere, ivbe", and "itan". The first relates to historical events, the second relates to current events, and the third was “linguistic ornamentation in formal discourse”. Among the Balochi of Pakistan and Afghanistan, there is a word "batal" for ordinary proverbs and "bassīttuks" for "proverbs with background stories".
All of this makes it difficult to come up with a definition of "proverb" that is universally applicable, which brings us back to Taylor's observation, "An incommunicable quality tells us this sentence is proverbial and that one is not.".
Paremiology.
The study of proverbs is called paremiology which has a variety of uses in the study of such topics as philosophy, linguistics, and folklore. There are several types and styles of proverbs which are analyzed within Paremiology as is the use and misuse of familiar expressions which are not strictly 'proverbial' in the dictionary definition of being fixed sentences.
Grammatical structures.
Proverbs in various languages are found with a wide variety of grammatical structures. In English, for example, we find the following structures (in addition to others):
However, people will often quote only a fraction of a proverb to invoke an entire proverb, e.g. "All is fair" instead of "All is fair in love and war", and "A rolling stone" for "A rolling stone gathers no moss."
The grammar of proverbs is not always the typical grammar of the spoken language, often elements are moved around, to achieve rhyme or focus.
Use in conversation.
Proverbs are used in conversation by adults more than children, partially because adults have learned more proverbs than children. Also, using proverbs well is a skill that is developed over years. Additionally, children have not mastered the patterns of metaphorical expression that are invoked in proverb use. Proverbs, because they are indirect, allow a speaker to disagree or give advice in a way that may be less offensive. Studying actual proverb use in conversation, however, is difficult since the researcher must wait for proverbs to happen. An Ethiopian researcher, Tadesse Jaleta Jirata, made headway in such research by attending and taking notes at events where he knew proverbs were expected to be part of the conversations.
Use in literature.
Many authors have used proverbs in their writings. Probably the most famous user of proverbs in novels is J. R. R. Tolkien in his "The Hobbit" and "The Lord of the Rings" series. Also, C. S. Lewis created a dozen proverbs in "The Horse and His Boy". These books are notable for not only using proverbs as integral to the development of the characters and the story line, but also for creating proverbs.
Among medieval literary texts, Geoffrey Chaucer's Troilus and Criseyde plays a special role because Chaucer's usage seems to challenge the truth value of proverbs by exposing their epistemological unreliability.
Proverbs (or portions of them) have been the inspiration for titles of books: "The Bigger they Come" by Erle Stanley Gardner, and "Birds of a Feather" (several books with this title), "Devil in the Details" (multiple books with this title). Sometimes a title alludes to a proverb, but does not actually quote it, such as "The Gift Horse's Mouth" by Robert Campbell. Some stories have been written with a proverb overtly as an opening, such as "A stitch in time saves nine" at the beginning of "Kitty's Class Day", one of Louisa May Alcott's "Proverb Stories". Other times, a proverb appears at the end of a story, summing up a moral to the story, frequently found in Aesop's Fables, such as "Heaven helps those who help themselves" from "Hercules and the Wagoner".
Proverbs have also been used strategically by poets. Sometimes proverbs (or portions of them or anti-proverbs) are used for titles, such as "A bird in the bush" by Lord Kennet and his stepson Peter Scott and "The blind leading the blind" by Lisa Mueller. Sometimes, multiple proverbs are important parts of poems, such as Paul Muldoon's "Symposium", which begins "You can lead a horse to water but you can't make it hold its nose to the grindstone and hunt with the hounds. Every dog has a stitch in time..." The Turkish poet Refiki wrote an entire poem by stringing proverbs together, which has been translated into English poetically yielding such verses as "Be watchful and be wary, / But seldom grant a boon; / The man who calls the piper / Will also call the tune."
Because proverbs are familiar and often pointed, they have been used by a number of hip-hop poets. This has been true not only in the USA, birthplace of hip-hop, but also in Nigeria. Since Nigeria is so multilingual, hip-hop poets there use proverbs from various languages, mixing them in as it fits there need, sometimes translating the original. For example,
"They forget say ogbon ju agbaralo
They forget that wisdom is greater than power"
Some authors have bent and twisted proverbs, creating anti-proverbs, for a variety of literary effects. For example, in the Harry Potter novels, J. K. Rowling reshapes a standard English proverb into “It’s no good crying over spilt potion” and Dumbledore
advises Harry not to “count your owls before they are delivered”. In a slightly different use of reshaping proverbs, in the Aubrey–Maturin series of historical naval novels by Patrick O'Brian, Capt. Jack Aubrey humorously mangles and mis-splices proverbs, such as “Never count the bear’s skin before it is hatched” and “There’s a good deal to be said for making hay while the iron is hot.”
Because proverbs are so much a part of the language and culture, authors have sometimes used proverbs in historical fiction effectively, but anachronistically, before the proverb was actually known. For example, the novel "Ramage and the Rebels", by Dudley Pope is set in approximately 1800. Captain Ramage reminds his adversary "You are supposed to know that it is dangerous to change horses in midstream" (p. 259), with another allusion to the same proverb three pages later. However, the proverb about changing horses in midstream is reliably dated to 1864, so the proverb could not have been known or used by a character from that period.
Some authors have used so many proverbs that there have been entire books written cataloging their proverb usage, such as Charles Dickens, Agatha Christie, George Bernard Shaw, and Friedrich Nietzsche.
On the non-fiction side, proverbs have also been used by authors. Some have been used as the basis for book titles, e.g. "I Shop, Therefore I Am: Compulsive Buying and the Search for Self" by April Lane Benson. Some proverbs been used as the basis for article titles, "All our eggs in a broken basket: How the Human Terrain System is undermining sustainable military cultural competence." Proverbs have been noted as common in subtitles of articles. Many authors have cited proverbs as epigrams at the beginning of their articles, e.g. "'If you want to dismantle a hedge, remove one thorn at a time' Somali proverb" in an article on peacemaking in Somalia. An article about research among the Maori used a Maori proverb as a title, then began the article with the Maori form of the proverb as an epigram "Set the overgrown bush alight and the new flax shoots will spring up", followed by three paragraphs about how the proverb served as a metaphor for the research and the present context.
Interpretations.
Interpreting proverbs is often complex, but is best done in a context. Interpreting proverbs from other cultures is much more difficult than interpreting proverbs in ones own culture. Even within English-speaking cultures, there is difference of opinion on how to interpret the proverb A rolling stone gathers no moss. Some see it as condemning a person that keeps moving, seeing moss as a positive thing, such as profit; others see it the proverb as praising people that keep moving and developing, seeing moss as a negative thing, such as negative habits.
In an extreme example, one researcher working in Ghana found that for a single Akan proverb, twelve different interpretations were given. Though this is extreme, proverbs can often have multiple interpretations.
Children will sometimes interpret proverbs in a literal sense, not yet knowing how to understand the conventionalized metaphor. Interpretation of proverbs is also affected by injuries and diseases of the brain, "A hallmark of schizophrenia is impaired proverb interpretation."
Counter proverbs.
There are often proverbs that contradict each other, such as "Look before you leap" and "He who hesitates is lost." These have been labeled "counter proverbs" or "antonymous proverbs". When there are such counter proverbs, each can be used in its own appropriate situation, and neither is intended to be a universal truth.
The concept of "counter proverb" is more about pairs of contradictory proverbs than about the use of proverbs to counter each other in an argument. For example, from the Tafi language of Ghana, the following pair of proverbs are counter to each other but are each used in appropriate contexts, "A co-wife who is too powerful for you, you address her as your mother" and "Do not call your mother’s co-wife your mother..." In Nepali, there is a set of totally contradictory proverbs: "Religion is victorious and sin erodes" and "Religion erodes and sin is victorious".
Also, the following pair are counter proverbs from the Kasena of Ghana "It is the patient person who will milk a barren cow" and "The person who would milk a barren cow must prepare for a kick on the forehead" The two contradict each other, whether they are used in an argument or not (though indeed they were used in an argument). But the same work contains an appendix with many examples of proverbs used in arguing for contrary positions, but proverbs that are not inherently contradictory, such as "One is better off with hope of a cow's return than news of its death" countered by "If you don't know a goat its death you mock at its skin". Though this pair was used in a contradictory way in a conversation, they are not a set of "counter proverbs".
Discussing counter proverbs in the Badaga language, Hockings explained that in his large collection "a few proverbs are mutually contradictory... we can be sure that the Badagas do not see the matter that way, and would explain such apparent contradictions by reasoning that proverb "x" is used in one context, while "y" is used in quite another." Comparing Korean proverbs, "when you compare two proverbs, often they will be contradictory." They are used for "a particular situation".
"Counter proverbs" are not the same as a "paradoxical proverb", a proverb that contains a seeming paradox.
Proverbs in drama and film.
Similarly to other forms of literature, proverbs have also been used as important units of language in drama and films. This is true from the days of classical Greek works to old French to Shakespeare, to 19th Century Spanish, to today. The use of proverbs in drama and film today is still found in languages around the world, such as Yorùbá.
A film that makes rich use of proverbs is "Forrest Gump", known for both using and creating proverbs. Other studies of the use of proverbs in film include work by Kevin McKenna on the Russian film "Aleksandr Nevsky", Haase's study of an adaptation of Little Red Riding Hood, Elias Dominguez Barajas on the film "Viva Zapata!", and Aboneh Ashagrie on "The Athlete" (a movie in Amharic about Abebe Bikila).
In the case of "Forrest Gump", the screenplay by Eric Roth had more proverbs than the novel by Winston Groom, but for "The Harder They Come", the reverse is true, where the novel derived from the movie by Michael Thelwell has many more proverbs than the movie.
Éric Rohmer, the French film director, directed a series of films, the "Comedies and Proverbs", where each film was based on a proverb: "The Aviator's Wife", "The Perfect Marriage", "Pauline at the Beach", "Full Moon in Paris" (the film's proverb was invented by Rohmer himself: "The one who has two wives loses his soul, the one who has two houses loses his mind."), "The Green Ray", "Boyfriends and Girlfriends".
Movie titles based on proverbs include "Murder Will Out (1939 film)", "Try, Try Again", and "The Harder They Fall". The title of an award-winning Turkish film, Three Monkeys, also invokes a proverb, though the title does not fully quote it.
They have also been used as the titles of plays: "Baby with the Bathwater" by Christopher Durang, "Dog Eat Dog" by Mary Gallagher, and "The Dog in the Manger" by Charles Hale Hoyt. The use of proverbs as titles for plays is not, of course, limited to English plays: "Il faut qu'une porte soit ouverte ou fermée" (A door must be open or closed) by Paul de Musset. Proverbs have also been used in musical dramas, such as "The Full Monty", which has been shown to use proverbs in clever ways.
Proverbs and music.
Proverbs are often poetic in and of themselves, making them ideally suited for adapting into songs. Proverbs have been used in music from opera to country to hip-hop. Proverbs have also been used in music in other languages, such as the Akan language the Igede language, and Spanish.
English examples of using proverbs in music include Elvis Presley's "Easy come, easy go", Harold Robe's "Never swap horses when you're crossing a stream", Arthur Gillespie's "Absence makes the heart grow fonder", Bob Dylan's "Like a rolling stone", Cher's "Apples don't fall far from the tree". Lynn Anderson made famous a song full of proverbs, "I never promised you a rose garden" (written by Joe South). In choral music, we find Michael Torke's "Proverbs" for female voice and ensemble. A number of Blues musicians have also used proverbs extensively. The frequent use of proverbs in Country music has led to published studies of proverbs in this genre. The Reggae artist Jahdan Blakkamoore has recorded a piece titled "Proverbs Remix". The opera "Maldobrìe" contains careful use of proverbs. An extreme example of many proverbs used in composing songs is a song consisting almost entirely of proverbs performed by Bruce Springsteen, "My best was never good enough". The Mighty Diamonds recorded a song called simply "Proverbs".
The band Fleet Foxes used the proverb painting Netherlandish Proverbs for the cover of their eponymous album Fleet Foxes.
In addition to proverbs being used in songs themselves, some rock bands have used parts of proverbs as their names, such as the Rolling Stones, Bad Company, The Mothers of Invention, Feast or Famine, Of Mice and Men. There have been at least two groups that called themselves "The Proverbs", and there is a hip-hop performer in South Africa known as "Proverb". In addition, many albums have been named with allusions to proverbs, such as "Spilt milk" (a title used by Jellyfish and also Kristina Train), "The more things change" by Machine Head, "Silk purse" by Linda Ronstadt, "Another day, another dollar" by DJ Scream Roccett, "The blind leading the naked" by Vicious Femmes, "What's good for the goose is good for the gander" by Bobby Rush, "Resistance is Futile" by Steve Coleman, "Murder will out" by Fan the Fury. The proverb "Feast or famine" has been used as an album title by Chuck Ragan, Reef the Lost Cauze, Indiginus, and DaVinci. Whitehorse mixed two proverbs for the name of their album "Leave no bridge unburned". The band Splinter Group released an album titled "When in Rome, Eat Lions". The band Downcount used a proverb for the name of their tour, "Come and take it".
Sources.
Proverbs come from a variety of sources. Some are, indeed, the result of people pondering and crafting language, such as some by Confucius, Plato, Baltasar Gracián, etc. Others are taken from such diverse sources as poetry, songs, commercials, advertisements, movies, literature, etc. A number of the well known sayings of Jesus, Shakespeare, and others have become proverbs, though they were original at the time of their creation, and many of these sayings were not seen as proverbs when they were first coined. Many proverbs are also based on stories, often the end of a story. For example, the proverb "Who will bell the cat?" is from the end of a story about the mice planning how to be safe from the cat.
Some authors have created proverbs in their writings, such a J.R.R. Tolkien, and some of these proverbs have made their way into broader society, such as the bumper sticker pictured here. Similarly, C.S. Lewis' created proverb about a lobster in a pot, from the "Chronicles of Narnia", has also gained currency. In cases like this, deliberately created proverbs for fictional societies have become proverbs in real societies. In a fictional story set in a real society, the movie "Forrest Gump" introduced "Life is like a box of chocolates" into broad society.
Though many proverbs are ancient, they were all newly created at some point by somebody. Sometimes it is easy to detect that a proverb is newly coined by a reference to something recent, such as the Haitian proverb "The fish that is being microwaved doesn't fear the lightning". Also, there is a proverb in the Kafa language of Ethiopia that refers to the forced military conscription of the 1980s, "...the one who hid himself lived to have children." A Mongolian proverb also shows evidence of recent origin, "A beggar who sits on gold; Foam rubber piled on edge." A political candidate in Kenya popularised a new proverb in his 1995 campaign, "Chuth ber" "Immediacy is best". "The proverb has since been used in other contexts to prompt quick action." Over 1,400 new English proverbs are said to have been coined in the 20th century. This process of creating proverbs is always ongoing, so that possible new proverbs are being created constantly. Those sayings that are adopted and used by an adequate number of people become proverbs in that society.
Paremiological minimum.
Grigorii Permjakov developed the concept of the core set of proverbs that full members of society know, what he called the "paremiological minimum" (1979). For example, an adult American is expected to be familiar with "Birds of a feather flock together", part of the American paremiological minimum. However, an average adult American is not expected to know "Fair in the cradle, foul in the saddle", an old English proverb that is not part of the current American paremiological minimum. Thinking more widely than merely proverbs, Permjakov observed "every adult Russian language speaker (over 20 years of age) knows no fewer than 800 proverbs, proverbial expressions, popular literary quotations and other forms of cliches". Studies of the paremiological minimum have been done for a limited number of languages, including Russian, Hungarian, Czech, Somali, Nepali, Gujarati, Spanish, Esperanto, and Polish. Two noted examples of attempts to establish a paremiological minimum in America are by Haas (2008) and Hirsch, Kett, and Trefil (1988), the latter more prescriptive than descriptive. There is not yet a recognized standard method for calculating the paremiological minimum, as seen by comparing the various efforts to establish the paremiological minimum in a number of languages.
Proverbs in visual form.
From ancient times, people around the world have recorded proverbs in visual form. This has been done in two ways. First, proverbs have been "written" to be displayed, often in a decorative manner, such as on pottery, cross-stitch, murals, kangas (East African women's wraps), and quilts.
Secondly, proverbs have often been visually depicted in a variety of media, including paintings, etchings, and sculpture. Jakob Jordaens painted a plaque with a proverb about drunkenness above a drunk man wearing a crown, titled "The King Drinks". Probably the most famous examples of depicting proverbs are the different versions of the paintings "Netherlandish Proverbs" by the father and son Pieter Bruegel the Elder and Pieter Brueghel the Younger, the proverbial meanings of these paintings being the subject of a 2004 conference, which led to a published volume of studies (Mieder 2004a). The same father and son also painted versions of The Blind Leading the Blind, a Biblical proverb. These and similar paintings inspired another famous painting depicting some proverbs and also idioms (leading to a series of additional paintings) "Proverbidioms" by T. E. Breitenbach. Another painting inspired by Bruegel's work is by the Chinese artist, Ah To, who created a painting illustrating 81 Cantonese sayings. Corey Barksdale has produced a book of paintings with specific proverbs and pithy quotations. The British artist Chris Gollon has painted a major work entitled "Big Fish Eat Little Fish", a title echoing Bruegel's painting Big Fishes Eat Little Fishes.
Sometimes well-known proverbs are pictured on objects, without a text actually quoting the proverb, such as the three wise monkeys who remind us "Hear no evil, see no evil, speak no evil". When the proverb is well known, viewers are able to recognize the proverb and understand the image appropriately, but if viewers do not recognize the proverb, much of the effect of the image is lost. For example, there is a Japanese painting in the Bonsai museum in Saitama city that depicted flowers on a dead tree, but only when the curator learned the ancient (and no longer current) proverb "Flowers on a dead tree" did the curator understand the deeper meaning of the painting.
A bibliography on proverbs in visual form has been prepared by Mieder and Sobieski (1999). Interpreting visual images of proverbs is subjective, but familiarity with the depicted proverb helps.
In an abstract non-representational visual work, sculptor Mark di Suvero has created a sculpture titled "Proverb", which is located in Dallas, TX, near the Morton H. Meyerson Symphony Center.
Some artists have used proverbs and anti-proverbs for titles of their paintings, alluding to a proverb rather than picturing it. For example, Vivienne LeWitt painted a piece titled "If the shoe doesn’t fit, must we change the foot?", which shows neither foot nor shoe, but a woman counting her money as she contemplates different options when buying vegetables.
Proverbs in cartoons.
Cartoonists, both editorial and pure humorists, have often used proverbs, sometimes primarily building on the text, sometimes primarily on the situation visually, the best cartoons combining both. Not surprisingly, cartoonists often twist proverbs, such as visually depicting a proverb literally or twisting the text as an anti-proverb. An example with all of these traits is a cartoon showing a waitress delivering two plates with worms on them, telling the customers, "Two early bird specials... here ya go."
The traditional Three wise monkeys were depicted in Bizarro with different labels. Instead of the negative imperatives, the one with ears covered bore the sign “See and speak evil”, the one with eyes covered bore the sign “See and hear evil”, etc. The caption at the bottom read “The power of positive thinking.” Another cartoon showed a customer in a pharmacy telling a pharmacist, “I'll have an ounce of prevention.” The comic strip The Argyle Sweater showed an Egyptian archeologist loading a mummy on the roof of a vehicle, refusing the offer of a rope to tie it on, with the caption “A fool and his mummy are soon parted.” The comic One Big Happy showed a conversation where one person repeatedly posed part of various proverb and the other tried to complete each one, resulting in such humorous results as “Don't change horses... unless you can lift those heavy diapers.”
Editorial cartoons can use proverbs to make their points with extra force as they can invoke the wisdom of society, not just the opinion of the editors. In an example that invoked a proverb only visually, when a US government agency (GSA) was caught spending money extravagantly, a cartoon showed a black pot labeled “Congress” telling a black kettle labeled “GSA”, “Stop wasting the taxpayers' money!” It may have taken some readers a moment of pondering to understand it, but the impact of the message was the stronger for it.
Cartoons with proverbs are so common that Wolfgang Mieder has published a collected volume of them, many of them editorial cartoons. For example, a German editorial cartoon linked a current politician to the Nazis, showing him with a bottle of swastika-labeled wine and the caption “In vino veritas.” 
One cartoonist very self-consciously drew and wrote cartoons based on proverbs for the University of Vermont student newspaper "The Water Tower", under the title "Proverb place".
Applications.
There is a growing interest in deliberately using proverbs to achieve goals, usually to support and promote changes in society. On the negative side, this was deliberately done by the Nazis. On the more positive side, proverbs have also been used for constructive purposes. For example, proverbs have been used for teaching foreign languages at various levels. In addition, proverbs have been used for public health promotion, such as promoting breast feeding with a shawl bearing a Swahili proverb “Mother’s milk is sweet”. Proverbs have also been applied for helping people manage diabetes, to combat prostitution, and for community development., to resolve conflicts, and to slow the transmission of HIV.
The most active field deliberately using proverbs is Christian ministry, where Joseph G. Healey and others have deliberately worked to catalyze the collection of proverbs from smaller languages and the application of them in a wide variety of church-related ministries, resulting in publications of collections and applications. This attention to proverbs by those in Christian ministries is not new, many pioneering proverb collections having been collected and published by Christian workers.
U.S. Navy Captain Edward Zellem pioneered the use of Afghan proverbs as a positive relationship-building tool during the war in Afghanistan, and in 2012 he published two bilingual collections of Afghan proverbs in Dari and English, part of an effort of nationbuilding, followed by a volume of Pashto proverbs in 2014.
Borrowing and spread of proverbs.
Proverbs are often and easily translated and transferred from one language into another. “There is nothing so uncertain as the derivation of proverbs, the same proverb being often found in all nations, and it is impossible to assign its paternity.”
Proverbs are often borrowed across lines of language, religion, and even time. For example, a proverb of the approximate form “No flies enter a mouth that is shut” is currently found in Spain, France, Ethiopia, and many countries in between. It is embraced as a true local proverb in many places and should not be excluded in any collection of proverbs because it is shared by the neighbors. However, though it has gone through multiple languages and millennia, the proverb can be traced back to an ancient Babylonian proverb (Pritchard 1958:146).
In the Alaaba and Gurage languages of south central Ethiopia, there is a proverb, “The she-dog , because she is in extreme hurry gives birth to blind (ones).” It is also found in Pashto language of Afghanistan. Erasmus also gave a Latin form of it in his "Adagia", "Canis festinans caecos parit catulos". This proverb is also well attested in ancient Greek and even Akkadian texts, where Moran gives it as “The bitch by her acting too hastily brought forth the blind”. Alster, documenting an Akkadian inscription, classified this proverb as having “a longer history than any other recorded proverb in the world”, going back to “around 1800 BC”.
Another example of a widely spread proverb is “A drowning person clutches at foam”, found in Peshai of Afghanistan and Orma of Kenya, and presumably places in between.
Proverbs about one hand clapping are common across Asia, from Dari in Afghanistan to Japan.
Some studies have been done devoted to the spread of proverbs in certain regions, such as India and her neighbors and Europe.
An extreme example of the borrowing and spread of proverbs was the work done to create a corpus of proverbs for Esperanto, where all the proverbs were translated from other languages.
It is often not possible to trace the direction of borrowing a proverb between languages. This is complicated by the fact that the borrowing may have been through plural languages. In some cases, it is possible to make a strong case for discerning the direction of the borrowing based on an artistic form of the proverb in one language, but a prosaic form in another language. For example, in Ethiopia there is a proverb “Of mothers and water, there is none evil.” It is found in Amharic, Alaaba language, and Oromo, three languages of Ethiopia:
The Oromo version uses poetic features, such as the initial "ha" in both clauses with the final "-aa" in the same word, and both clauses ending with "-an". Also, both clauses are built with the vowel "a" in the first and last words, but the vowel "i" in the one syllable central word. In contrast, the Amharic and Alaaba versions of the proverb show little evidence of sound-based art. Based on the verbal artistry of the Oromo, it appears that the Oromo form is prior to the Alaaba or Amharic, though it could be borrowed from yet another language.
Are cultural values reflected in proverbs?
There is a longstanding debate among proverb scholars as to whether the cultural values of specific language communities are reflected (to varying degree) in their proverbs. Many claim that the proverbs of a particular culture reflect the values of that specific culture, at least to some degree. Many writers have asserted that the proverbs of their cultures reflect their culture and values; this can be seen in such titles as the following: "An introduction to Kasena society and culture through their proverbs", Prejudice, power, and poverty in Haiti: a study of a nation's culture as seen through its proverbs, Proverbiality and worldview in Maltese and Arabic proverbs, Fatalistic traits in Finnish proverbs, "Vietnamese cultural patterns and values as expressed in proverbs", "The Wisdom and Philosophy of the Gikuyu proverbs: The Kihooto worldview", "Spanish Grammar and Culture through Proverbs," and "How Russian Proverbs Present the Russian National Character". Kohistani has written a thesis to show how understanding Afghan Dari proverbs will help Europeans understand Afghan culture.
However, a number of scholars argue that such claims are not valid. They have used a variety of arguments. Grauberg argues that since many proverbs are so widely circulated they are reflections of broad human experience, not any one culture's unique viewpoint. Related to this line of argument, from a collection of 199 American proverbs, Jente showed that only 10 were coined in the USA, so that most of these proverbs would not reflect uniquely American values. Giving another line of reasoning that proverbs should not be trusted as a simplistic guide to cultural values, Mieder once observed “proverbs come and go, that is, antiquated proverbs with messages and images we no longer relate to are dropped from our proverb repertoire, while new proverbs are created to reflect the mores and values of our time”, so old proverbs still in circulation might reflect past values of a culture more than its current values. Also, within any language’s proverb repertoire, there may be “counter proverbs”, proverbs that contradict each other on the surface (see section above). When examining such counter proverbs, it is difficult to discern an underlying cultural value. With so many barriers to a simple calculation of values directly from proverbs, some feel "one cannot draw conclusions about values of speakers simply from the texts of proverbs".
Many outsiders have studied proverbs to discern and understand cultural values and world view of cultural communities. These outsider scholars are confident that they have gained insights into the local cultures by studying proverbs, but this is not universally accepted.
Seeking empirical evidence to evaluate the question of whether proverbs reflect a culture’s values, some have counted the proverbs that support various values. For example, Moon lists what he sees as the top ten core cultural values of the Builsa society of Ghana, as exemplified by proverbs. He found that 18% of the proverbs he analyzed supported the value of being a member of the community, rather than being independent. This was corroboration to other evidence that collective community membership is an important value among the Builsa. In studying Tajik proverbs, Bell notes that the proverbs in his corpus “Consistently illustrate Tajik values” and “The most often observed proverbs reflect the focal and specific values” discerned in the thesis 
A study of English proverbs created since 1900 showed in the 1960s a sudden and significant increase in proverbs that reflected more casual attitudes toward sex. Since the 1960s was also the decade of the Sexual revolution, this shows a strong statistical link between the changed values of the decades and a change in the proverbs coined and used. Another study mining the same volume counted Anglo-American proverbs about religion to show that proverbs indicate attitudes toward religion are going downhill.
There are many examples where cultural values have been explained and illustrated by proverbs. For example, from India, the concept that birth determines one's nature "is illustrated in the oft-repeated proverb: there can be no friendship between grass-eaters and meat-eaters, between a food and its eater". Proverbs have been used to explain and illustrate the Fulani cultural value of "pulaaku". But using proverbs to "illustrate" a cultural value is not the same as using a collection of proverbs to "discern" cultural values. In a comparative study between Spanish and Jordanian proverbs it is defined the social imagination for the mother as an archetype in the context of role transformation and in contrast with the roles of husband, son and brother, in two societies which might be occasionally associated with sexist and /or rural ideologies.
Some scholars have adopted a cautious approach, acknowledging at least a genuine, though limited, link between cultural values and proverbs: “The cultural portrait painted by proverbs may be fragmented, contradictory, or otherwise at variance with reality... but must be regarded not as accurate renderings but rather as tantalizing shadows of the culture which spawned them.” There is not yet agreement on the issue of whether, and how much, cultural values are reflected in a culture's proverbs.
It is clear that the Soviet Union believed that proverbs had a direct link to the values of a culture, as they used them to try to create changes in the values of cultures within their sphere of domination. Sometimes they took old Russian proverbs and altered them into socialist forms. These new proverbs promoted Socialism and its attendant values, such as atheism and collectivism, e.g. “Bread is given to us not by Christ, but by machines and collective farms” and “A good harvest is had only by a collective farm.” They did not limit their efforts to Russian, but also produced “newly coined proverbs that conformed to socialist thought” in Tajik and other languages of the USSR.
Proverbs and religion.
Many proverbs from around the world address matters of ethics and expected of behavior. Therefore, it is not surprising that proverbs are often important texts in religions. The most obvious example is the Book of Proverbs in the Bible. Additional proverbs have also been coined to support religious values, such as the following from Dari of Afghanistan: "In childhood you're playful, In youth you're lustful, In old age you're feeble, So when will you before God be worshipful?"
Clearly proverbs in religion are not limited to monotheists; among the Badaga of India (Sahivite Hindus), there is a traditional proverb "Catch hold of and join with the man who has placed sacred ash himself." Proverbs are widely associated with large religions that draw from sacred books, but they are also used for religious purposes among groups with their own traditional religions, such as the Guji Oromo. The broadest comparative study of proverbs across religions is "The eleven religions and their proverbial lore, a comparative study. A reference book to the eleven surviving major religions of the world" by Selwyn Gurney Champion, from 1945. Some sayings from sacred books also become proverbs, even if they were not obviously proverbs in the original passage of the sacred book. For example, many quote "Be sure your sin will find you out" as a proverb from the Bible, but there is no evidence it was proverbial in its original usage (Numbers 32:23).
Not all religious references in proverbs are positive, some are cynical, such as the Tajik, "Do as the mullah says, not as he does." Also, note the Italian proverb, "One barrel of wine can work more miracles than a church full of saints". An Indian proverb is cynical about devotees of Hinduism, " When in distress, a man calls on Rama". In the context of Tibetan Buddhism, some Ladakhi proverbs mock the lamas, e.g. "If the lama's own head does not come out cleanly, how will he do the drawing upwards of the dead?... used for deriding the immoral life of the lamas."
Dammann thought "The influence of Islam manifests itself in African proverbs... Christian influences, on the contrary, are rare." If widely true in Africa, this is likely due to the longer presence of Islam in many parts of Africa. Reflection of Christian values is common in Amharic proverbs of Ethiopia, an area that has had a presence of Christianity for well over 1,000 years. The Islamic proverbial reproduction may also be shown in the image of some animals such as the dog. Although dog is portrayed in many European proverbs as the most faithful friend of man, it is represented in some Islamic countries as impure, dirty, vile, cowardly, ungrateful and treacherous, in addition to link it to negative human superstitions such as loneliness, indifference and bad luck.
Proverbs and psychology.
Though much proverb scholarship is done by literary scholars, those studying the human mind have used proverbs in a variety of studies. One of the earliest studies in this field is the "Proverbs Test" by Gorham, developed in 1956. A similar test is being prepared in German. Proverbs have been used to evaluate dementia, study the cognitive development of children, measure the results of brain injuries, and study how the mind processes figurative language.
Proverbs in advertising.
Proverbs are frequently used in advertising, often in slightly modified form.
Ford once advertised its Thunderbird with, "One drive is worth a thousand words" (Mieder 2004b: 84). This is doubly interesting since the underlying proverb behind this, "One picture is worth a thousand words," was originally introduced into the English proverb repertoire in an ad for televisions (Mieder 2004b: 83).
A few of the many proverbs adapted and used in advertising include:
The GEICO company has created a series of television ads that are built around proverbs, such as "A bird in the hand is worth two in the bush", and "The pen is mightier than the sword", "Pigs may fly/When pigs fly", "If a tree falls in the forest...", and "Words can never hurt you". Doritos made a commercial based on the proverb, "When pigs fly.".
Use of proverbs in advertising is not limited to the English language. Seda Başer Çoban has studied the use of proverbs in Turkish advertising. Tatira has given a number of examples of proverbs used in advertising in Zimbabwe. However, unlike the examples given above in English, all of which are anti-proverbs, Tatira's examples are standard proverbs. Where the English proverbs above are meant to make a potential customer smile, in one of the Zimbabwean examples "both the content of the proverb and the fact that it is phrased as a proverb secure the idea of a secure time-honored relationship between the company and the individuals". When newer buses were imported, owners of older buses compensated by painting a traditional proverb on the sides of their buses, "Going fast does not assure safe arrival".
Conservative language.
Because many proverbs are both poetic and traditional, they are often passed down in fixed forms. Though spoken language may change, many proverbs are often preserved in conservative, even archaic, form. In English, for example, "betwixt" is not used by many, but a form of it is still heard (or read) in the proverb "There is many a slip 'twixt the cup and the lip." The conservative form preserves the meter and the rhyme. This conservative nature of proverbs can result in archaic words and grammatical structures being preserved in individual proverbs, as has been documented in Amharic, Greek, Nsenga, and Polish.
In addition, proverbs may still be used in languages which were once more widely known in a society, but are now no longer so widely known. For example, English speakers use some non-English proverbs that are drawn from languages that used to be widely understood by the educated class, e.g. "C'est la vie" from French and "Carpe diem" from Latin.
Proverbs are often handed down through generations. Therefore, "many proverbs refer to old measurements, obscure professions, outdated weapons, unknown plants, animals, names, and various other traditional matters."
Therefore, it is common that they preserve words that become less common and archaic in broader society. For example, English has a proverb "The cobbler's children have no shoes". The word "cobbler", meaning a maker of shoes, is now unknown among many English speakers, but it is preserved in the proverb.
Sources for proverb study.
A seminal work in the study of proverbs is Archer Taylor's "The Proverb" (1931), later republished by Wolfgang Mieder with Taylor's Index included (1985/1934). A good introduction to the study of proverbs is Mieder's 2004 volume, "Proverbs: A Handbook". Mieder has also published a series of bibliography volumes on proverb research, as well as a large number of articles and other books in the field. Stan Nussbaum has edited a large collection on proverbs of Africa, published on a CD, including reprints of out-of-print collections, original collections, and works on analysis, bibliography, and application of proverbs to Christian ministry (1998). Paczolay has compared proverbs across Europe and published a collection of similar proverbs in 55 languages (1997). Mieder edits an academic journal of proverb study, "Proverbium" (), many back issues of which are available online. A volume containing articles on a wide variety of topics touching on proverbs was edited by Mieder and Alan Dundes (1994/1981). "Paremia" is a Spanish-language journal on proverbs, with articles available online. There are also papers on proverbs published in conference proceedings volumes from the annual Interdisciplinary Colloquium on Proverbs in Tavira, Portugal. Mieder has published a two-volume "International Bibliography of Paremiology and Phraseology", with a topical, language, and author index. Mieder has published a bibliography of collections of proverbs from around the world. A broad introduction to proverb study, "Introduction to Paremiology", edited by Hrisztalina Hrisztova-Gotthardt and Melita Aleksa Varga has been published in both hardcover and free open access, with articles by a dozen different authors.
External links.
Serious websites related to the study of proverbs, and some that list regional proverbs:

</doc>
<doc id="23531" url="https://en.wikipedia.org/wiki?curid=23531" title="Portability (social security)">
Portability (social security)

The portability of social security benefits is the ability of workers to preserve, maintain, and transfer acquired social security rights and social security rights in the process of being acquired from one private, occupational, or public social security scheme to another. Social security rights refer to rights stemming from pension schemes (old age, survivor, disability), unemployment insurance, health insurance, workers' compensation, and sickness benefits. 
Hence, if social security benefits are portable, contributors to, for example, old-age pension schemes do not experience any disadvantage such as the loss of contributions and benefits associated with these contributions when moving from one job to another, from one occupation to another, or from the public to the private sector or vice versa.
International portability of social security rights allows international migrants, who have contributed to a social security scheme for some time in a particular country, to maintain acquired benefits or benefits in the process of being acquired when moving to another country. International portability of social security benefits is therefore understood as the migrant's ability to preserve, maintain, and transfer acquired social security rights independent of nationality and country of residence.
International portability of social security benefits is achieved through bilateral or multilateral social security agreements between countries. These agreements guarantee the totalization of periods of contribution to the social security systems of both countries and the extraterritorial payment of benefits. Currently it is estimated that approximately 23 per cent of migrants worldwide are covered by bilateral social security agreements.

</doc>
<doc id="23534" url="https://en.wikipedia.org/wiki?curid=23534" title="Percopsiformes">
Percopsiformes

The Percopsiformes are a small order of ray-finned fishes, comprising the trout-perch and its allies. It contains just ten extant species, grouped into seven genera and three families. Five of these genera are monotypic
They are generally small fish, ranging from in adult body length. They inhabit freshwater habitats in North America. They are grouped together because of technical characteristics of their internal anatomy, and the different species may appear quite different externally.

</doc>
<doc id="23535" url="https://en.wikipedia.org/wiki?curid=23535" title="Photon">
Photon

A photon is an elementary particle, the quantum of all forms of electromagnetic radiation including light. It is the force carrier for electromagnetic force, even when static via virtual photons. The photon has zero rest mass and as a result, the interactions of this force with matter at long distance are observable at the microscopic and macroscopic levels. Like all elementary particles, photons are currently best explained by quantum mechanics but exhibit wave–particle duality, exhibiting properties of both waves and particles. For example, a single photon may be refracted by a lens and exhibit wave interference with itself, and it can behave as a particle with definite and finite measurable position and momentum. The photon's wave and quanta qualities are two observable aspects of a single phenomenon, and cannot be described by any mechanical model; a representation of this dual property of light, which assumes certain points on the wavefront to be the seat of the energy, is not possible. The quanta in a light wave cannot be spatially localized. Some defined physical parameters of a photon are listed.
The modern concept of the photon was developed gradually by Albert Einstein in the early 20th century to explain experimental observations that did not fit the classical wave model of light. The benefit of the photon model was that it accounted for the frequency dependence of light's energy, and explained the ability of matter and electromagnetic radiation to be in thermal equilibrium. The photon model accounted for anomalous observations, including the properties of black-body radiation, that others (notably Max Planck) had tried to explain using "semiclassical models". In that model, light was described by Maxwell's equations, but material objects emitted and absorbed light in "quantized" amounts (i.e., they change energy only by certain particular discrete amounts). Although these semiclassical models contributed to the development of quantum mechanics, many further experiments beginning with the phenomenon of Compton scattering of single photons by electrons, validated Einstein's hypothesis that "light itself" is quantized. In 1926 the optical physicist Frithiof Wolfers and the chemist Gilbert N. Lewis coined the name photon for these particles. After Arthur H. Compton won the Nobel Prize in 1927 for his scattering studies, most scientists accepted that light quanta have an independent existence, and the term "photon" was accepted.
In the Standard Model of particle physics, photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles, such as charge, mass and spin, are determined by this gauge symmetry. The photon concept has led to momentous advances in experimental and theoretical physics, including as lasers, Bose–Einstein condensation, quantum field theory, and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry, high-resolution microscopy, and measurements of molecular distances. Recently, photons have been studied as elements of quantum computers, and for applications in optical imaging and optical communication such as quantum cryptography.
Nomenclature.
In 1900, the German physicist Max Planck was studying black-body radiation and suggested that the energy carried by electromagnetic waves could only be released in "packets" of energy. In his 1901 article in Annalen der Physik he called these packets "energy elements". The word "quanta" (singular "quantum," Latin for "how much") was used before 1900 to mean particles or amounts of different quantities, including electricity. In 1905, Albert Einstein suggested that electromagnetic waves could only exist as discrete wave-packets. He called such a wave-packet "the light quantum" (German: "das Lichtquant"). The name "photon" derives from the Greek word for light, "" (transliterated "phôs"). Arthur Compton used "photon" in 1928, referring to Gilbert N. Lewis. The same name was used earlier, by the American physicist and psychologist Leonard T. Troland, who coined the word in 1916, in 1921 by the Irish physicist John Joly, in 1924 by the French physiologist René Wurmser (1890-1993) and in 1926 by the French physicist Frithiof Wolfers (1891-1971). The name was suggested initially as a unit related to the illumination of the eye and the resulting sensation of light and was used later in a physiological context. Although Wolfers's and Lewis's theories were contradicted by many experiments and never accepted, the new name was adopted very soon by most physicists after Compton used it.
In physics, a photon is usually denoted by the symbol "γ" (the Greek letter gamma). This symbol for the photon probably derives from gamma rays, which were discovered in 1900 by Paul Villard, named by Ernest Rutherford in 1903, and shown to be a form of electromagnetic radiation in 1914 by Rutherford and Edward Andrade. In chemistry and optical engineering, photons are usually symbolized by "hν", the energy of a photon, where "h" is Planck's constant and the Greek letter "ν" (nu) is the photon's frequency. Much less commonly, the photon can be symbolized by "hf", where its frequency is denoted by "f".
Physical properties.
A photon is massless, has no electric charge, and is a stable particle. A photon has two possible polarization states. In the momentum representation of the photon, which is preferred in quantum field theory, a photon is described by its wave vector, which determines its wavelength "λ" and its direction of propagation. A photon's wave vector may not be zero and can be represented either as a spatial 3-vector or as a (relativistic) four-vector; in the latter case it belongs to the light cone (pictured). Different signs of the four-vector denote different circular polarizations, but in the 3-vector representation one should account for the polarization state separately; it actually is a spin quantum number. In both cases the space of possible wave vectors is three-dimensional.
The photon is the gauge boson for electromagnetism, and therefore all other quantum numbers of the photon (such as lepton number, baryon number, and flavour quantum numbers) are zero. Also, the photon does not obey the Pauli exclusion principle.
Photons are emitted in many natural processes. For example, when a charge is accelerated it emits synchrotron radiation. During a molecular, atomic or nuclear transition to a lower energy level, photons of various energy will be emitted, ranging from radio waves to gamma rays. A photon can also be emitted when a particle and its corresponding antiparticle are annihilated (for example, electron–positron annihilation).
In empty space, the photon moves at "c" (the speed of light) and its energy and momentum are related by , where "p" is the magnitude of the momentum vector p. This derives from the following relativistic relation, with :
The energy and momentum of a photon depend only on its frequency ("ν") or inversely, its wavelength ("λ"):
where k is the wave vector (where the wave number ), is the angular frequency, and is the reduced Planck constant.
Since p points in the direction of the photon's propagation, the magnitude of the momentum is
The photon also carries a quantity called spin angular momentum that does not depend on its frequency. The magnitude of its spin is formula_5 and the component measured along its direction of motion, its helicity, must be ±ħ. These two possible helicities, called right-handed and left-handed, correspond to the two possible circular polarization states of the photon.
To illustrate the significance of these formulae, the annihilation of a particle with its antiparticle in free space must result in the creation of at least "two" photons for the following reason. In the center of momentum frame, the colliding antiparticles have no net momentum, whereas a single photon always has momentum (since, as we have seen, it is determined by the photon's frequency or wavelength, which cannot be zero). Hence, conservation of momentum (or equivalently, translational invariance) requires that at least two photons are created, with zero net momentum. (However, it is possible if the system interacts with another particle or field for the annihilation to produce one photon, as when a positron annihilates with a bound atomic electron, it is possible for only one photon to be emitted, as the nuclear Coulomb field breaks translational symmetry.) The energy of the two photons, or, equivalently, their frequency, may be determined from conservation of four-momentum. Seen another way, the photon can be considered as its own antiparticle. The reverse process, pair production, is the dominant mechanism by which high-energy photons such as gamma rays lose energy while passing through matter. That process is the reverse of "annihilation to one photon" allowed in the electric field of an atomic nucleus.
The classical formulae for the energy and momentum of electromagnetic radiation can be re-expressed in terms of photon events. For example, the pressure of electromagnetic radiation on an object derives from the transfer of photon momentum per unit time and unit area to that object, since pressure is force per unit area and force is the change in momentum per unit time.
Experimental checks on photon mass.
Current commonly accepted physical theories imply or assume the photon to be strictly massless. If the photon is not a strictly massless particle, it would not move at the exact speed of light, "c" in vacuum. Its speed would be lower and depend on its frequency. Relativity would be unaffected by this; the so-called speed of light, "c", would then not be the actual speed at which light moves, but a constant of nature which is the maximum speed that any object could theoretically attain in space-time. Thus, it would still be the speed of space-time ripples (gravitational waves and gravitons), but it would not be the speed of photons.
If a photon did have non-zero mass, there would be other effects as well. Coulomb's law would be modified and the electromagnetic field would have an extra physical degree of freedom. These effects yield more sensitive experimental probes of the photon mass than the frequency dependence of the speed of light. If Coulomb's law is not exactly valid, then that would allow the presence of an electric field to exist within a hollow conductor when it is subjected to an external electric field. This thus allows one to test Coulomb's law to very high precision. A null result of such an experiment has set a limit of "m" ≲ 10−14 eV/c2.
Sharper upper limits on the speed of light have been obtained in experiments designed to detect effects caused by the galactic vector potential. Although the galactic vector potential is very large because the galactic magnetic field exists on very great length scales, only the magnetic field would be observable if the photon is massless. In the case that the photon has mass, the mass term formula_6 would affect the galactic plasma. The fact that no such effects are seen implies an upper bound on the photon mass of "m" < . The galactic vector potential can also be probed directly by measuring the torque exerted on a magnetized ring. Such methods were used to obtain the sharper upper limit of 10−18eV/c2 (the equivalent of ) given by the Particle Data Group.
These sharp limits from the non-observation of the effects caused by the galactic vector potential have been shown to be model dependent. If the photon mass is generated via the Higgs mechanism then the upper limit of "m"≲10−14 eV/c2 from the test of Coulomb's law is valid.
Photons inside superconductors do develop a nonzero effective rest mass; as a result, electromagnetic forces become short-range inside superconductors.
Historical development.
In most theories up to the eighteenth century, light was pictured as being made up of particles. Since particle models cannot easily account for the refraction, diffraction and birefringence of light, wave theories of light were proposed by René Descartes (1637), Robert Hooke (1665), and Christiaan Huygens (1678); however, particle models remained dominant, chiefly due to the influence of Isaac Newton. In the early nineteenth century, Thomas Young and August Fresnel clearly demonstrated the interference and diffraction of light and by 1850 wave models were generally accepted. In 1865, James Clerk Maxwell's prediction that light was an electromagnetic wave—which was confirmed experimentally in 1888 by Heinrich Hertz's detection of radio waves—seemed to be the final blow to particle models of light.
The Maxwell wave theory, however, does not account for "all" properties of light. The Maxwell theory predicts that the energy of a light wave depends only on its intensity, not on its frequency; nevertheless, several independent types of experiments show that the energy imparted by light to atoms depends only on the light's frequency, not on its intensity. For example, some chemical reactions are provoked only by light of frequency higher than a certain threshold; light of frequency lower than the threshold, no matter how intense, does not initiate the reaction. Similarly, electrons can be ejected from a metal plate by shining light of sufficiently high frequency on it (the photoelectric effect); the energy of the ejected electron is related only to the light's frequency, not to its intensity.
At the same time, investigations of blackbody radiation carried out over four decades (1860–1900) by various researchers culminated in Max Planck's hypothesis that the energy of "any" system that absorbs or emits electromagnetic radiation of frequency "ν" is an integer multiple of an energy quantum . As shown by Albert Einstein, some form of energy quantization "must" be assumed to account for the thermal equilibrium observed between matter and electromagnetic radiation; for this explanation of the photoelectric effect, Einstein received the 1921 Nobel Prize in physics.
Since the Maxwell theory of light allows for all possible energies of electromagnetic radiation, most physicists assumed initially that the energy quantization resulted from some unknown constraint on the matter that absorbs or emits the radiation. In 1905, Einstein was the first to propose that energy quantization was a property of electromagnetic radiation itself. Although he accepted the validity of Maxwell's theory, Einstein pointed out that many anomalous experiments could be explained if the "energy" of a Maxwellian light wave were localized into point-like quanta that move independently of one another, even if the wave itself is spread continuously over space. In 1909 and 1916, Einstein showed that, if Planck's law of black-body radiation is accepted, the energy quanta must also carry momentum , making them full-fledged particles. This photon momentum was observed experimentally by Arthur Compton, for which he received the Nobel Prize in 1927. The pivotal question was then: how to unify Maxwell's wave theory of light with its experimentally observed particle nature? The answer to this question occupied Albert Einstein for the rest of his life, and was solved in quantum electrodynamics and its successor, the Standard Model (see Second quantization and The photon as a gauge boson, below).
Einstein's light quantum.
Unlike Planck, Einstein entertained the possibility that there might be actual physical quanta of light—what we now call photons. He noticed that a light quantum with energy proportional to its frequency would explain a number of troubling puzzles and paradoxes, including an unpublished law by Stokes, the ultraviolet catastrophe, and of course the photoelectric effect. Stokes's law said simply that the frequency of fluorescent light cannot be greater than the frequency of the light (usually ultraviolet) inducing it. Einstein eliminated the ultraviolet catastrophe by imagining a gas of photons behaving like a gas of electrons that he had previously considered. He was advised by a colleague to be careful how he wrote up this paper, in order to not challenge Planck, a powerful figure in physics, too directly, and indeed the warning was justified, as Planck never forgave him for writing it.
Early objections.
Einstein's 1905 predictions were verified experimentally in several ways in the first two decades of the 20th century, as recounted in Robert Millikan's Nobel lecture. However, before Compton's experiment showed that photons carried momentum proportional to their wave number (1922), most physicists were reluctant to believe that electromagnetic radiation itself might be particulate. (See, for example, the Nobel lectures of Wien, Planck and Millikan.) Instead, there was a widespread belief that energy quantization resulted from some unknown constraint on the matter that absorbed or emitted radiation. Attitudes changed over time. In part, the change can be traced to experiments such as Compton scattering, where it was much more difficult not to ascribe quantization to light itself to explain the observed results.
Even after Compton's experiment, Niels Bohr, Hendrik Kramers and John Slater made one last attempt to preserve the Maxwellian continuous electromagnetic field model of light, the so-called BKS model. To account for the data then available, two drastic hypotheses had to be made:
However, refined Compton experiments showed that energy–momentum is conserved extraordinarily well in elementary processes; and also that the jolting of the electron and the generation of a new photon in Compton scattering obey causality to within 10 ps. Accordingly, Bohr and his co-workers gave their model "as honorable a funeral as possible". Nevertheless, the failures of the BKS model inspired Werner Heisenberg in his development of matrix mechanics.
A few physicists persisted in developing semiclassical models in which electromagnetic radiation is not quantized, but matter appears to obey the laws of quantum mechanics. Although the evidence from chemical and physical experiments for the existence of photons was overwhelming by the 1970s, this evidence could not be considered as "absolutely" definitive; since it relied on the interaction of light with matter, and a sufficiently complete theory of matter could in principle account for the evidence. Nevertheless, "all" semiclassical theories were refuted definitively in the 1970s and 1980s by photon-correlation experiments. Hence, Einstein's hypothesis that quantization is a property of light itself is considered to be proven.
Wave–particle duality and uncertainty principles.
Photons, like all quantum objects, exhibit wave-like and particle-like properties. Their dual wave–particle nature can be difficult to visualize. The photon displays clearly wave-like phenomena such as diffraction and interference on the length scale of its wavelength. For example, a single photon passing through a double-slit experiment exhibits interference phenomena but only if no measure was made at the slit. A single photon passing through a double-slit experiment lands on the screen with a probability distribution given by its interference pattern determined by Maxwell's equations. However, experiments confirm that the photon is "not" a short pulse of electromagnetic radiation; it does not spread out as it propagates, nor does it divide when it encounters a beam splitter. Rather, the photon seems to be a point-like particle since it is absorbed or emitted "as a whole" by arbitrarily small systems, systems much smaller than its wavelength, such as an atomic nucleus (≈10−15 m across) or even the point-like electron. Nevertheless, the photon is "not" a point-like particle whose trajectory is shaped probabilistically by the electromagnetic field, as conceived by Einstein and others; that hypothesis was also refuted by the photon-correlation experiments cited above. According to our present understanding, the electromagnetic field itself is produced by photons, which in turn result from a local gauge symmetry and the laws of quantum field theory (see the Second quantization and Gauge boson sections below).
A key element of quantum mechanics is Heisenberg's uncertainty principle, which forbids the simultaneous measurement of the position and momentum of a particle along the same direction. Remarkably, the uncertainty principle for charged, material particles "requires" the quantization of light into photons, and even the frequency dependence of the photon's energy and momentum. 
An elegant illustration of the uncertainty principle is Heisenberg's thought experiment for locating an electron with an ideal microscope. The position of the electron can be determined to within the resolving power of the microscope, which is given by a formula from classical optics
where θ is the aperture angle of the microscope and λ is the wavelength of the light used to observe the electron. Thus, the position uncertainty formula_8 can be made arbitrarily small by reducing the wavelength λ. Even if the momentum of the electron is initially known, the light impinging on the electron will give it a momentum "kick" formula_9 of some unknown amount, rendering the momentum of the electron uncertain. If light were "not" quantized into photons, the uncertainty formula_9 could be made arbitrarily small by reducing the light's intensity. In that case, since the wavelength and intensity of light can be varied independently, one could simultaneously determine the position and momentum to arbitrarily high accuracy, violating the uncertainty principle. By contrast, Einstein's formula for photon momentum preserves the uncertainty principle; since the photon is scattered anywhere within the aperture, the uncertainty of momentum transferred equals
giving the product formula_12, which is Heisenberg's uncertainty principle. Thus, the entire world is quantized; both matter and fields must obey a consistent set of quantum laws, if either one is to be quantized.
The analogous uncertainty principle for photons forbids the simultaneous measurement of the number formula_13 of photons (see Fock state and the Second quantization section below) in an electromagnetic wave and the phase formula_14 of that wave
See coherent state and squeezed coherent state for more details.
Both photons and electrons create analogous interference patterns when passed through a double-slit experiment. For photons, this corresponds to the interference of a Maxwell light wave whereas, for material particles (electron), this corresponds to the interference of the Schrödinger wave equation. Although this similarity might suggest that Maxwell's equations describing the photon's electromagnetic wave are simply Schrödinger's equation for photons, most physicists do not agree. For one thing, they are mathematically different; most obviously, Schrödinger's one equation for the electron solves for a complex field, whereas Maxwell's four equations solve for real fields. More generally, the normal concept of a Schrödinger probability wave function cannot be applied to photons. As photons are massless, they cannot be localized without being destroyed; technically, photons cannot have a position eigenstate formula_16, and, thus, the normal Heisenberg uncertainty principle formula_17 does not pertain to photons. A few substitute wave functions have been suggested for the photon, but they have not come into general use. Instead, physicists generally accept the second-quantized theory of photons described below, quantum electrodynamics, in which photons are quantized excitations of electromagnetic modes.
Another interpretation, that avoids duality, is the De Broglie–Bohm theory: known also as the "pilot-wave model". In that theory, the photon is both, wave and particle. ""This idea seems to me so natural and simple, to resolve the wave-particle dilemma in such a clear and ordinary way, that it is a great mystery to me that it was so generally ignored"", J.S.Bell.
Bose–Einstein model of a photon gas.
In 1924, Satyendra Nath Bose derived Planck's law of black-body radiation without using any electromagnetism, but rather by using a modification of coarse-grained counting of phase space. Einstein showed that this modification is equivalent to assuming that photons are rigorously identical and that it implied a "mysterious non-local interaction", now understood as the requirement for a symmetric quantum mechanical state. This work led to the concept of coherent states and the development of the laser. In the same papers, Einstein extended Bose's formalism to material particles (bosons) and predicted that they would condense into their lowest quantum state at low enough temperatures; this Bose–Einstein condensation was observed experimentally in 1995. It was later used by Lene Hau to slow, and then completely stop, light in 1999 and 2001.
The modern view on this is that photons are, by virtue of their integer spin, bosons (as opposed to fermions with half-integer spin). By the spin-statistics theorem, all bosons obey Bose–Einstein statistics (whereas all fermions obey Fermi–Dirac statistics).
Stimulated and spontaneous emission.
In 1916, Einstein showed that Planck's radiation law could be derived from a semi-classical, statistical treatment of photons and atoms, which implies a link between the rates at which atoms emit and absorb photons. The condition follows from the assumption that functions of the emission and absorption of radiation by the atoms are independent of each other, and that thermal equilibrium is made by way of the radiation's interaction with the atoms. Consider a cavity in thermal equilibrium with all parts of itself and filled with electromagnetic radiation and that the atoms can emit and absorb that radiation. Thermal equilibrium requires that the energy density formula_18 of photons with frequency formula_19 (which is proportional to their number density) is, on average, constant in time; hence, the rate at which photons of any particular frequency are "emitted" must equal the rate at which they "absorb" them.
Einstein began by postulating simple proportionality relations for the different reaction rates involved. In his model, the rate formula_20 for a system to "absorb" a photon of frequency formula_19 and transition from a lower energy formula_22 to a higher energy formula_23 is proportional to the number formula_24 of atoms with energy formula_22 and to the energy density formula_18 of ambient photons of that frequency,
where formula_28 is the rate constant for absorption. For the reverse process, there are two possibilities: spontaneous emission of a photon, or the emission of a photon initiated by the interaction of the atom with a passing photon and the return of the atom to the lower-energy state. Following Einstein's approach, the corresponding rate formula_29 for the emission of photons of frequency formula_19 and transition from a higher energy formula_23 to a lower energy formula_22 is
where formula_34 is the rate constant for emitting a photon spontaneously, and formula_35 is the rate constant for emissions in response to ambient photons (induced or stimulated emission). In thermodynamic equilibrium, the number of atoms in state i and those in state j must, on average, be constant; hence, the rates formula_20 and formula_29 must be equal. Also, by arguments analogous to the derivation of Boltzmann statistics, the ratio of formula_38 and formula_24 is formula_40 where formula_41 are the degeneracy of the state i and that of j, respectively, formula_42 their energies, k the Boltzmann constant and T the system's temperature. From this, it is readily derived that
formula_43 and
The A and Bs are collectively known as the "Einstein coefficients".
Einstein could not fully justify his rate equations, but claimed that it should be possible to calculate the coefficients formula_34, formula_28 and formula_35 once physicists had obtained "mechanics and electrodynamics modified to accommodate the quantum hypothesis". In fact, in 1926, Paul Dirac derived the formula_35 rate constants by using a semiclassical approach, and, in 1927, succeeded in deriving "all" the rate constants from first principles within the framework of quantum theory. Dirac's work was the foundation of quantum electrodynamics, i.e., the quantization of the electromagnetic field itself. Dirac's approach is also called "second quantization" or quantum field theory; earlier quantum mechanical treatments only treat material particles as quantum mechanical, not the electromagnetic field.
Einstein was troubled by the fact that his theory seemed incomplete, since it did not determine the "direction" of a spontaneously emitted photon. A probabilistic nature of light-particle motion was first considered by Newton in his treatment of birefringence and, more generally, of the splitting of light beams at interfaces into a transmitted beam and a reflected beam. Newton hypothesized that hidden variables in the light particle determined which of the two paths a single photon would take. Similarly, Einstein hoped for a more complete theory that would leave nothing to chance, beginning his separation from quantum mechanics. Ironically, Max Born's probabilistic interpretation of the wave function was inspired by Einstein's later work searching for a more complete theory.
Second quantization and high energy photon interactions.
In 1910, Peter Debye derived Planck's law of black-body radiation from a relatively simple assumption. He correctly decomposed the electromagnetic field in a cavity into its Fourier modes, and assumed that the energy in any mode was an integer multiple of formula_49, where formula_19 is the frequency of the electromagnetic mode. Planck's law of black-body radiation follows immediately as a geometric sum. However, Debye's approach failed to give the correct formula for the energy fluctuations of blackbody radiation, which were derived by Einstein in 1909.
In 1925, Born, Heisenberg and Jordan reinterpreted Debye's concept in a key way. As may be shown classically, the Fourier modes of the electromagnetic field—a complete set of electromagnetic plane waves indexed by their wave vector k and polarization state—are equivalent to a set of uncoupled simple harmonic oscillators. Treated quantum mechanically, the energy levels of such oscillators are known to be formula_51, where formula_19 is the oscillator frequency. The key new step was to identify an electromagnetic mode with energy formula_51 as a state with formula_13 photons, each of energy formula_49. This approach gives the correct energy fluctuation formula.
Dirac took this one step further. He treated the interaction between a charge and an electromagnetic field as a small perturbation that induces transitions in the photon states, changing the numbers of photons in the modes, while conserving energy and momentum overall. Dirac was able to derive Einstein's formula_34 and formula_35 coefficients from first principles, and showed that the Bose–Einstein statistics of photons is a natural consequence of quantizing the electromagnetic field correctly (Bose's reasoning went in the opposite direction; he derived Planck's law of black-body radiation by "assuming" B–E statistics). In Dirac's time, it was not yet known that all bosons, including photons, must obey Bose–Einstein statistics.
Dirac's second-order perturbation theory can involve virtual photons, transient intermediate states of the electromagnetic field; the static electric and magnetic interactions are mediated by such virtual photons. In such quantum field theories, the probability amplitude of observable events is calculated by summing over "all" possible intermediate steps, even ones that are unphysical; hence, virtual photons are not constrained to satisfy formula_58, and may have extra polarization states; depending on the gauge used, virtual photons may have three or four polarization states, instead of the two states of real photons. Although these transient virtual photons can never be observed, they contribute measurably to the probabilities of observable events. Indeed, such second-order and higher-order perturbation calculations can give apparently infinite contributions to the sum. Such unphysical results are corrected for using the technique of renormalization.
Other virtual particles may contribute to the summation as well; for example, two photons may interact indirectly through virtual electron–positron pairs. In fact, such photon-photon scattering (see two-photon physics), as well as electron-photon scattering, is meant to be one of the modes of operations of the planned particle accelerator, the International Linear Collider.
In modern physics notation, the quantum state of the electromagnetic field is written as a Fock state, a tensor product of the states for each electromagnetic mode
where formula_60 represents the state in which formula_61 photons are in the mode formula_62. In this notation, the creation of a new photon in mode formula_62 (e.g., emitted from an atomic transition) is written as formula_64. This notation merely expresses the concept of Born, Heisenberg and Jordan described above, and does not add any physics.
The hadronic properties of the photon.
Measurements of the interaction between energetic photons and hadrons show that the interaction is much more intense than expected by the interaction of merely photons with the hadron's electric charge. Furthermore, the interaction of energetic photons with protons is similar to the interaction of photons with neutrons in spite of the fact that the electric charge structures of protons and neutrons are substantially different. A theory called Vector Meson Dominance (VMD) was developed to explain this effect. According to VMD, the photon is a superposition of the pure electromagnetic photon which interacts only with electric charges and vector meson. However, if experimentally probed at very short distances, the intrinsic structure of the photon is recognized as a flux of quark and gluon components, quasi-free according to asymptotic freedom in QCD and described by the photon structure function. A comprehensive comparison of data with theoretical predictions is presented in a recent review.
The photon as a gauge boson.
The electromagnetic field can be understood as a gauge field, i.e., as a field that results from requiring that a gauge symmetry holds independently at every position in spacetime. For the electromagnetic field, this gauge symmetry is the Abelian U(1) symmetry of complex numbers of absolute value 1, which reflects the ability to vary the phase of a complex field without affecting observables or real valued functions made from it, such as the energy or the Lagrangian.
The quanta of an Abelian gauge field must be massless, uncharged bosons, as long as the symmetry is not broken; hence, the photon is predicted to be massless, and to have zero electric charge and integer spin. The particular form of the electromagnetic interaction specifies that the photon must have spin ±1; thus, its helicity must be formula_65. These two spin components correspond to the classical concepts of right-handed and left-handed circularly polarized light. However, the transient virtual photons of quantum electrodynamics may also adopt unphysical polarization states.
In the prevailing Standard Model of physics, the photon is one of four gauge bosons in the electroweak interaction; the other three are denoted W+, W− and Z0 and are responsible for the weak interaction. Unlike the photon, these gauge bosons have mass, owing to a mechanism that breaks their SU(2) gauge symmetry. The unification of the photon with W and Z gauge bosons in the electroweak interaction was accomplished by Sheldon Glashow, Abdus Salam and Steven Weinberg, for which they were awarded the 1979 Nobel Prize in physics. Physicists continue to hypothesize grand unified theories that connect these four gauge bosons with the eight gluon gauge bosons of quantum chromodynamics; however, key predictions of these theories, such as proton decay, have not been observed experimentally.
Contributions to the mass of a system.
The energy of a system that emits a photon is "decreased" by the energy formula_66 of the photon as measured in the rest frame of the emitting system, which may result in a reduction in mass in the amount formula_67. Similarly, the mass of a system that absorbs a photon is "increased" by a corresponding amount. As an application, the energy balance of nuclear reactions involving photons is commonly written in terms of the masses of the nuclei involved, and terms of the form formula_67 for the gamma photons (and for other relevant energies, such as the recoil energy of nuclei).
This concept is applied in key predictions of quantum electrodynamics (QED, see above). In that theory, the mass of electrons (or, more generally, leptons) is modified by including the mass contributions of virtual photons, in a technique known as renormalization. Such "radiative corrections" contribute to a number of predictions of QED, such as the magnetic dipole moment of leptons, the Lamb shift, and the hyperfine structure of bound lepton pairs, such as muonium and positronium.
Since photons contribute to the stress–energy tensor, they exert a gravitational attraction on other objects, according to the theory of general relativity. Conversely, photons are themselves affected by gravity; their normally straight trajectories may be bent by warped spacetime, as in gravitational lensing, and their frequencies may be lowered by moving to a higher gravitational potential, as in the Pound–Rebka experiment. However, these effects are not specific to photons; exactly the same effects would be predicted for classical electromagnetic waves.
Photons in matter.
Light that travels through transparent matter does so at a lower speed than "c", the speed of light in a vacuum. For example, photons engage in so many collisions on the way from the core of the sun that radiant energy can take about a million years to reach the surface; however, once in open space, a photon takes only 8.3 minutes to reach Earth. The factor by which the speed is decreased is called the refractive index of the material. In a classical wave picture, the slowing can be explained by the light inducing electric polarization in the matter, the polarized matter radiating new light, and that new light interfering with the original light wave to form a delayed wave. In a particle picture, the slowing can instead be described as a blending of the photon with quantum excitations of the matter to produce quasi-particles known as polariton (other quasi-particles are phonons and excitons); this polariton has a nonzero effective mass, which means that it cannot travel at "c". Light of different frequencies may travel through matter at different speeds; this is called dispersion (not to be confused with scattering). In some cases, it can result in extremely slow speeds of light in matter. The effects of photon interactions with other quasi-particles may be observed directly in Raman scattering and Brillouin scattering.
Photons can also be absorbed by nuclei, atoms or molecules, provoking transitions between their energy levels. A classic example is the molecular transition of retinal (C20H28O), which is responsible for vision, as discovered in 1958 by Nobel laureate biochemist George Wald and co-workers. The absorption provokes a cis-trans isomerization that, in combination with other such transitions, is transduced into nerve impulses. The absorption of photons can even break chemical bonds, as in the photodissociation of chlorine; this is the subject of photochemistry.
Technological applications.
Photons have many applications in technology. These examples are chosen to illustrate applications of photons "per se", rather than general optical devices such as lenses, etc. that could operate under a classical theory of light. The laser is an extremely important application and is discussed above under stimulated emission.
Individual photons can be detected by several methods. The classic photomultiplier tube exploits the photoelectric effect: a photon of sufficient energy strikes a metal plate and knocks free an electron, initiating an ever-amplifying avalanche of electrons. S emiconductor charge-coupled device chips use a similar effect: an incident photon generates a charge on a microscopic capacitor that can be detected. Other detectors such as Geiger counters use the ability of photons to ionize gas molecules contained in the device, causing a detectable change of conductivity of the gas.
Planck's energy formula formula_69 is often used by engineers and chemists in design, both to compute the change in energy resulting from a photon absorption and to determine the frequency of the light emitted from a given photon emission. For example, the emission spectrum of a gas-discharge lamp can be altered by filling it with (mixtures of) gases with different electronic energy level configurations.
Under some conditions, an energy transition can be excited by "two" photons that individually would be insufficient. This allows for higher resolution microscopy, because the sample absorbs energy only in the spectrum where two beams of different colors overlap significantly, which can be made much smaller than the excitation volume of a single beam (see two-photon excitation microscopy). Moreover, these photons cause less damage to the sample, since they are of lower energy.
In some cases, two energy transitions can be coupled so that, as one system absorbs a photon, another nearby system "steals" its energy and re-emits a photon of a different frequency. This is the basis of fluorescence resonance energy transfer, a technique that is used in molecular biology to study the interaction of suitable proteins.
Several different kinds of hardware random number generators involve the detection of single photons. In one example, for each bit in the random sequence that is to be produced, a photon is sent to a beam-splitter. In such a situation, there are two possible outcomes of equal probability. The actual outcome is used to determine whether the next bit in the sequence is "0" or "1".
Recent research.
Much research has been devoted to applications of photons in the field of quantum optics. Photons seem well-suited to be elements of an extremely fast quantum computer, and the quantum entanglement of photons is a focus of research. Nonlinear optical processes are another active research area, with topics such as two-photon absorption, self-phase modulation, modulational instability and optical parametric oscillators. However, such processes generally do not require the assumption of photons "per se"; they may often be modeled by treating atoms as nonlinear oscillators. The nonlinear process of spontaneous parametric down conversion is often used to produce single-photon states. Finally, photons are essential in some aspects of optical communication, especially for quantum cryptography.
Additional references.
By date of publication:
Education with single photons:

</doc>
<doc id="23537" url="https://en.wikipedia.org/wiki?curid=23537" title="Philipp Franz von Siebold">
Philipp Franz von Siebold

Philipp Franz Balthasar von Siebold (17 February 1796 – 18 October 1866) was a German physician, botanist, and traveler. He achieved prominence by his studies of Japanese flora and fauna and the introduction of Western medicine in Japan. He was the father of the first female Japanese doctor, Kusumoto Ine.
Career.
Early life.
Born into a family of doctors and professors of medicine in Würzburg (then in the Bishopric of Würzburg, later part of Bavaria), Siebold initially studied medicine at University of Würzburg from November 1815, where he became a member of the Corps Moenania Würzburg. One of his professors was Franz Xaver Heller (1775–1840), author of the "" ("Flora of the Grand Duchy of Würzburg", 1810–1811). Ignaz Döllinger (1770–1841), his professor of anatomy and physiology, however, most influenced him. Döllinger was one of the first professors to understand and treat medicine as a natural science. Siebold stayed with Döllinger, where he came in regular contact with other scientists. He read the books of Humboldt, a famous naturalist and explorer, which probably raised his desire to travel to distant lands. Philipp Franz von Siebold became a physician by earning his M.D. degree in 1820. He initially practiced medicine in Heidingsfeld, in the Kingdom of Bavaria, now part of Würzburg.
Invited to Holland by an acquaintance of his family, Siebold applied for a position as a military physician, which would enable him to travel to the Dutch colonies. He entered the Dutch military service on June 19, 1822, and was appointed as ship's surgeon on the frigate "Adriana", sailing from Rotterdam to Batavia (present-day Jakarta) in the Dutch East Indies (now called Indonesia). On his trip to Batavia on the frigate "Adriana", Siebold practiced his knowledge of the Dutch language and also rapidly learned Malay, and during the long voyage he began a collection of marine fauna. He arrived in Batavia on February 18, 1823.
As an army medical officer, Siebold was posted to an artillery unit. However, he was given a room for a few weeks at the residence of the Governor-General of the Dutch East Indies, Baron Godert van der Capellen, to recover from an illness. With his erudition, he impressed the Governor-General, and also the director of the botanical garden at Buitenzorg (now Bogor), Caspar Georg Carl Reinwardt. These men sensed in Siebold a worthy successor to Engelbert Kaempfer and Carl Peter Thunberg, two former resident physicians at Dejima, a Dutch trading post in Japan, the latter of whom was the author of "". The Batavian Academy of Arts and Sciences soon elected Siebold as a member.
Arrival in Japan.
On 28 June 1823, after only a few months in the Dutch East Indies, Siebold was posted as resident physician and scientist to Dejima, a small artificial island and trading post at Nagasaki, and arrived there on 11 August 1823. During an eventful voyage to Japan he only just escaped drowning during a typhoon in the East China Sea. As only a very small number of Dutch personnel were allowed to live on this island, the posts of physician and scientist had to be combined. Dejima had been in the possession of the Dutch East India Company (known as the VOC) since the 17th century, but the Company had gone bankrupt in 1798, after which a trading post was operated there by the Dutch state for political considerations, with notable benefits to the Japanese.
The European tradition of sending doctors with botanical training to Japan was a long one. Sent on a mission by the Dutch East India Company, Engelbert Kaempfer (1651–1716), a German physician and botanist who lived in Japan from 1690 until 1692, ushered in this tradition of a combination of physician and botanist. The Dutch East India Company did not, however, actually employ the Swedish botanist and physician Carl Peter Thunberg (1743–1828), who had arrived in Japan in 1775.
Medical practice.
Japanese scientists invited Siebold to show them the marvels of western science, and he learned in return through them much about the Japanese and their customs. After curing an influential local officer, Siebold gained the permission to leave the trade post. He used this opportunity to treat Japanese patients in the greater area around the trade post. Siebold is credited with the introduction of vaccination and pathological anatomy for the first time in Japan.
In 1824, Siebold started a medical school in Nagasaki, the "Narutaki-juku", that grew into a meeting place for around fifty "students". They helped him in his botanical and naturalistic studies. The Dutch language became the "lingua franca" (common spoken language) for these academic and scholarly contacts for a generation, until the Meiji Restoration.
His patients paid him in kind with a variety of objects and artifacts that would later gain historical significance. These everyday objects later became the basis of his large ethnographic collection, which consisted of everyday household goods, woodblock prints, tools and hand-crafted objects used by the Japanese people.
Japanese family.
During his stay in Japan, Siebold "lived together" with Kusumoto Taki (楠本滝), who gave birth to their daughter Kusumoto (O-)Ine in 1827. Siebold used to call his wife "Otakusa" (probably derived from O-Taki-san) and named a "Hydrangea" after her. Kusumoto Ine eventually became the first Japanese woman known to have received a physician's training and became a highly regarded practicing physician and court physician to the Empress in 1882. She died at court in 1903.
Studies of Japanese fauna and flora.
His main interest, however, focused on the study of Japanese fauna and flora. He collected as much material as he could. Starting a small botanical garden behind his home (there was not much room on the small island) Siebold amassed over 1,000 native plants. In a specially built glasshouse he cultivated the Japanese plants to endure the Dutch climate. Local Japanese artists like Kawahara Keiga drew and painted images of these plants, creating botanical illustrations but also images of the daily life in Japan, which complemented his ethnographic collection. He hired Japanese hunters to track rare animals and collect specimens. Many specimens were collected with the help of his Japanese collaborators Keisuke Ito (1803–1901), Mizutani Sugeroku (1779–1833), Ōkochi Zonshin (1796–1882) and Katsuragawa Hoken (1797–1844), a physician to the Shogun. As well, Siebold's assistant and later successor, Heinrich Bürger (1806–1858), proved to be indispensable in carrying on Siebold's work in Japan.
Siebold first introduced to Europe such familiar garden-plants as the "Hosta" and the "Hydrangea otaksa". Unknown to the Japanese, he was also able to smuggle out germinative seeds of tea plants to the botanical garden "" in Batavia. Through this single act, he started the tea culture in Java, a Dutch colony at the time. Until then Japan had strictly guarded the trade in tea plants. Remarkably, in 1833, Java already could boast a half million tea plants.
He also introduced Japanese knotweed ("Fallopia japonica"), which has become a highly invasive weed in Europe and North America. All derive from a single female plant collected by Siebold.
During his stay at Dejima, Siebold sent three shipments with an unknown number of herbarium specimens to Leiden, Ghent, Brussels and Antwerp. The shipment to Leiden contained the first specimens of the Japanese giant salamander ("Andrias japonicus") to be sent to Europe.
In 1825 the government of the Dutch-Indies provided him with two assistants: apothecary and mineralogist Heinrich Bürger (his later successor) and the painter Carl Hubert de Villeneuve. Each would prove to be useful to Siebold's efforts that ranged from ethnographical to botanical to horticultural, when attempting to document the exotic Eastern Japanese experience. De Villeneuve taught Kawahara the techniques of Western painting.
Reportedly, Siebold was not the easiest man to deal with. He was in continuous conflict with his Dutch superiors who felt he was arrogant. This threat of conflict resulted in his recall in July 1827 back to Batavia. But the ship, the "Cornelis Houtman", sent to carry him back to Batavia, was thrown ashore by a typhoon in Nagasaki bay. The same storm badly damaged Dejima and destroyed Siebold's botanical garden. Repaired, the "Cornelis Houtman" was refloated. It left for Batavia with 89 crates of Siebold's salvaged botanical collection, but Siebold himself remained behind in Dejima.
Siebold Incident.
In 1826 Siebold made the court journey to Edo. During this long trip he collected many plants and animals. But he also obtained from the court astronomer Takahashi Kageyasu several detailed maps of Japan and Korea (written by Inō Tadataka), an act strictly forbidden by the Japanese government. When the Japanese discovered, by accident, that Siebold had a map of the northern parts of Japan, the government accused him of high treason and of being a spy for Russia.
The Japanese placed Siebold under house arrest and expelled him from Japan on 22 October 1829. Satisfied that his Japanese collaborators would continue his work, he journeyed back on the frigate "Java" to his former residence, Batavia, in possession of his enormous collection of thousands of animals and plants, his books and his maps. The botanical garden of "" would soon house Siebold's surviving, living flora collection of 2,000 plants. He arrived in the Netherlands on 7 July 1830. His stay in Japan and Batavia had lasted for a period of eight years.
Return to Europe.
Philipp Franz von Siebold arrived in the Netherlands in 1830, just at a time when political troubles erupted in Brussels, leading soon to Belgian independence. Hastily he salvaged his ethnographic collections in Antwerp and his herbarium specimens in Brussels and took them to Leiden. He left behind his botanical collections of living plants that were sent to the University of Ghent. The consequent expansion of this collection of rare and exotic plants led to the horticultural fame of Ghent. In gratitude the University of Ghent presented him in 1841 with specimens of every plant from his original collection.
Siebold settled in Leiden, taking with him the major part of his collection. The "Philipp Franz von Siebold collection", containing many type specimens, was the earliest botanical collection from Japan. Even today, it still remains a subject of ongoing research, a testimony to the depth of work undertaken by Siebold. It contained about 12,000 specimens, from which he could describe only about 2,300 species. The whole collection was purchased for a handsome amount by the Dutch government. Siebold was also granted a substantial annual allowance by the Dutch King William II and was appointed "Advisor to the King for Japanese Affairs". In 1842, the King even raised Siebold to the nobility as an esquire.
The "Siebold collection" opened to the public in 1831. He founded a museum in his home in 1837. This small, private museum would eventually evolve into the National Museum of Ethnology in Leiden. Siebold's successor in Japan, Heinrich Bürger, sent Siebold three more shipments of herbarium specimens collected in Japan. This flora collection formed the basis of the Japanese collections of the National Herbarium of the Netherlands in Leiden and the natural history museum Naturalis ("National Natuurhistorisch Museum").
In 1845 Siebold married Helene von Gagern (1820–1877), they had three sons and two daughters.
Writings.
During his stay in Leiden, Siebold wrote "Nippon" in 1832, the first part of a volume of a richly illustrated ethnographical and geographical work on Japan. The 'Archiv zur Beschreibung Nippons' also contained a report of his journey to the Shogunate Court at Edo. He wrote six further parts, the last ones published posthumously in 1882; his sons published an edited and lower-priced reprint in 1887.
The "" appeared between 1833 and 1841. This work was co-authored by Joseph Hoffmann and Kuo Cheng-Chang, a Javanese of Chinese extraction, who had journeyed along with Siebold from Batavia. It contained a survey of Japanese literature and a Chinese, Japanese and Korean dictionary.
The zoologists Coenraad Temminck (1777–1858), Hermann Schlegel (1804–1884), and Wilhem de Haan (1801–1855) scientifically described and documented Siebold's collection of Japanese animals. The ', a series of monographs published between 1833 and 1850, was mainly based on Siebold's collection, making the Japanese fauna the best-described non-European fauna – "a remarkable feat". A significant part of the ' was also based on the collections of Siebold's successor on Dejima, Heinrich Bürger.
Siebold wrote his "" in collaboration with the German botanist Joseph Gerhard Zuccarini (1797–1848). It first appeared in 1835, but the work was not completed until after his death, finished in 1870 by F.A.W. Miquel (1811–1871), director of the Rijksherbarium in Leiden. This work expanded Siebold's scientific fame from Japan to Europe.
From the Hortus Botanicus Leiden – the botanical garden of Leiden – many of Siebold's plants spread to Europe and from there to other countries. "Hosta" and "Hortensia", "Azalea", and the Japanese butterbur and the coltsfoot as well as the Japanese larch began to inhabit gardens across the world.
International endeavours.
After his return to Europe, Siebold tried to exploit his knowledge of Japan. Whilst living in Boppard, from 1852 he corresponded with Russian diplomats such as Baron von Budberg-Bönninghausen, the Russian ambassador to Prussia, which resulted in an invitation to go to St Petersburg to advise the Russian government how to open trade relations with Japan. Though still employed by the Dutch government he did not inform the Dutch of this voyage until after his return. American Naval Commodore Matthew C. Perry consulted Siebold in advance of his voyage to Japan in 1854.
In 1858, the Japanese government lifted the banishment of Siebold. He returned to Japan in 1859 as an adviser to the Agent of the Dutch Trading Society (Nederlandsche Handel-Maatschappij) in Nagasaki, Albert Bauduin. After two years the connection with the Trading Society was severed as the advice of Siebold was of no value. In Nagasaki he fathered another child with one of his female servants. In 1861 Siebold organised his appointment as an adviser to the Japanese government and went in that function to Edo. There he tried to obtain a position between the foreign representatives and the Japanese government. As he had been specially admonished by the Dutch authorities before going to Japan that he was to abstain from all interference in politics, the Dutch Consul General in Japan, J.K. de Wit, was ordered to ask Siebold's removal. Siebold was ordered to return to Batavia and from there he returned to Europe. After his return he asked the Dutch government to employ him as Consul General in Japan but the Dutch government severed all relations with Siebold who had a huge debt because of loans given to him, except for the payment of his pension.
Siebold kept trying to organise an other voyage to Japan. After he did not succeed in gaining employment with the Russian government, he went to Paris in 1865 to try to interest the French government in funding another expedition to Japan, but failed. He died in Munich on 18 October 1866.
Plants named after Siebold.
The botanical and horticultural spheres of influence have honored Philipp Franz von Siebold by naming some of the very garden-worthy plants that he studied after him. Examples include:
Also a type of abalone, "Nordotis gigantea", is known as Siebold's abalone, and is prized for sushi.
Though he is well known in Japan, where he is called "Shiboruto-san", and although mentioned in the relevant schoolbooks, Siebold is almost unknown elsewhere, except among gardeners who admire the many plants whose names incorporate "sieboldii" and "sieboldiana". The Hortus Botanicus in Leiden has recently laid out the "Von Siebold Memorial Garden", a Japanese garden with plants sent by Siebold. The garden was laid out under a 150-year-old "Zelkova serrata" tree dating from Siebold's lifetime. Japanese visitors come and visit this garden, to pay their respect for him.
Siebold museums.
Although he was disillusioned by what he perceived as a lack of appreciation for Japan and his contributions to its understanding, a testimony of the remarkable character of Siebold is found in museums that honor him.
His collections laid the foundation for the ethnographic museums of Munich and Leiden. Alexander von Siebold, his son by his European wife, donated much of the material left behind after Siebold's death in Würzburg to the British Museum in London. The Royal Scientific Academy of St. Petersburg purchased 600 colored plates of the "".
Another son, Heinrich (or Henry) von Siebold (1852–1908), continued part of his father's research. He is recognized, together with Edward S. Morse, as one of the founders of modern archaeological efforts in Japan.
Published works.
The standard author abbreviation Siebold is used to indicate Philipp Franz von Siebold as the author when citing a botanical name.

</doc>
<doc id="23538" url="https://en.wikipedia.org/wiki?curid=23538" title="Probability interpretations">
Probability interpretations

The word probability has been used in a variety of ways since it was first applied to the mathematical study of games of chance. Does probability measure the real, physical tendency of something to occur or is it a measure of how strongly one believes it will occur, or does it draw on both these elements? In answering such questions, mathematicians interpret the probability values of probability theory.
There are two broad categories of probability interpretations which can be called "physical" and "evidential" probabilities. Physical probabilities, which are also called objective or frequency probabilities, are associated with random physical systems such as roulette wheels, rolling dice and radioactive atoms. In such systems, a given type of event (such as a die yielding a six) tends to occur at a persistent rate, or "relative frequency", in a long run of trials. Physical probabilities either explain, or are invoked to explain, these stable frequencies. The two main kinds of theory of physical probability are frequentist accounts (such as those of Venn, Reichenbach and von Mises) and propensity accounts (such as those of Popper, Miller, Giere and Fetzer).
Evidential probability, also called Bayesian probability, can be assigned to any statement whatsoever, even when no random process is involved, as a way to represent its subjective plausibility, or the degree to which the statement is supported by the available evidence. On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap). There are also evidential interpretations of probability covering groups, which are often labelled as 'intersubjective' (proposed by Gillies and Rowbottom).
Some interpretations of probability are associated with approaches to statistical inference, including theories of estimation and hypothesis testing. The physical interpretation, for example, is taken by followers of "frequentist" statistical methods, such as Ronald Fisher, Jerzy Neyman and Egon Pearson. Statisticians of the opposing Bayesian school typically accept the existence and importance of physical probabilities, but also consider the calculation of evidential probabilities to be both valid and necessary in statistics. This article, however, focuses on the interpretations of probability rather than theories of statistical inference.
The terminology of this topic is rather confusing, in part because probabilities are studied within a variety of academic fields. The word "frequentist" is especially tricky. To philosophers it refers to a particular theory of physical probability, one that has more or less been abandoned. To scientists, on the other hand, "frequentist probability" is just another name for physical (or objective) probability. Those who promote Bayesian inference view "frequentist statistics" as an approach to statistical inference that recognises only physical probabilities. Also the word "objective", as applied to probability, sometimes means exactly what "physical" means here, but is also used of evidential probabilities that are fixed by rational constraints, such as logical and epistemic probabilities.
Philosophy.
The philosophy of probability presents problems chiefly in matters of epistemology and the uneasy interface between mathematical concepts and ordinary language as it is used by non-mathematicians.
Probability theory is an established field of study in mathematics. It has its origins in correspondence discussing the mathematics of games of chance between Blaise Pascal and Pierre de Fermat in the seventeenth century, and was formalized and rendered axiomatic as a distinct branch of mathematics by Andrey Kolmogorov in the twentieth century. In its axiomatic form, mathematical statements about probability theory carry the same sort of epistemological confidence shared by other mathematical statements in the philosophy of mathematics.
The mathematical analysis originated in observations of the behaviour of game equipment such as playing cards and dice, which are designed specifically to introduce random and equalized elements; in mathematical terms, they are subjects of indifference. This is not the only way probabilistic statements are used in ordinary human language: when people say that ""it will probably rain"", they typically do not mean that the outcome of rain versus not-rain is a random factor that the odds currently favor; instead, such statements are perhaps better understood as qualifying their expectation of rain with a degree of confidence. Likewise, when it is written that "the most probable explanation" of the name of Ludlow, Massachusetts "is that it was named after Roger Ludlow", what is meant here is not that Roger Ludlow is favored by a random factor, but rather that this is the most plausible explanation of the evidence, which admits other, less likely explanations.
Thomas Bayes attempted to provide a logic that could handle varying degrees of confidence; as such, Bayesian probability is an attempt to recast the representation of probabilistic statements as an expression of the degree of confidence by which the beliefs they express are held.
Though probability initially had somewhat mundane motivations, its modern influence and use is widespread ranging from evidence-based medicine, through Six sigma, all the way to the Probabilistically checkable proof and the String theory landscape.
Classical definition.
The first attempt at mathematical rigour in the field of probability, championed by Pierre-Simon Laplace, is now known as the classical definition. Developed from studies of games of chance (such as rolling dice) it states that probability is shared equally between all the possible outcomes, provided these outcomes can be deemed equally likely. (3.1)
This can be represented mathematically as follows:
If a random experiment can result in N mutually exclusive and equally likely outcomes and if NA of these outcomes result in the occurrence of the event A, the probability of A is defined by
There are two clear limitations to the classical definition. Firstly, it is applicable only to situations in which there is only a 'finite' number of possible outcomes. But some important random experiments, such as tossing a coin until it rises heads, give rise to an infinite set of outcomes. And secondly, you need to determine in advance that all the possible outcomes are equally likely without relying on the notion of probability to avoid circularity—for instance, by symmetry considerations.
Frequentism.
Frequentists posit that the probability of an event is its relative frequency over time, (3.4) i.e., its relative frequency of occurrence after repeating a process a large number of times under similar conditions. This is also known as aleatory probability. The events are assumed to be governed by some random physical phenomena, which are either phenomena that are predictable, in principle, with sufficient information (see Determinism); or phenomena which are essentially unpredictable. Examples of the first kind include tossing dice or spinning a roulette wheel; an example of the second kind is radioactive decay. In the case of tossing a fair coin, frequentists say that the probability of getting a heads is 1/2, not because there are two equally likely outcomes but because repeated series of large numbers of trials demonstrate that the empirical frequency converges to the limit 1/2 as the number of trials goes to infinity.
If we denote by formula_2 the number of occurrences of an event formula_3 in formula_4 trials, then if formula_5 we say that "formula_6
The frequentist view has its own problems. It is of course impossible to actually perform an infinity of repetitions of a random experiment to determine the probability of an event. But if only a finite number of repetitions of the process are performed, different relative frequencies will appear in different series of trials. If these relative frequencies are to define the probability, the probability will be slightly different every time it is measured. But the real probability should be the same every time. If we acknowledge the fact that we only can measure a probability with some error of measurement attached, we still get into problems as the error of measurement can only be expressed as a probability, the very concept we are trying to define. This renders even the frequency definition circular; see for example “What is the Chance of an Earthquake?” 
Logical, epistemic, and inductive probability.
It is widely recognised that the term "probability" is sometimes used in contexts where it has nothing to do with physical randomness. Consider, for example, the claim that the extinction of the dinosaurs was probably caused by a large meteorite hitting the earth. Statements such as "Hypothesis H is probably true" have been interpreted to mean that the (presently available) empirical evidence (E, say) supports H to a high degree. This degree of support of H by E has been called the logical probability of H given E, or the epistemic probability of H given E, or the inductive probability of H given E.
The differences between these interpretations are rather small, and may seem inconsequential. One of the main points of disagreement lies in the relation between probability and belief. Logical probabilities are conceived (for example in Keynes' Treatise on Probability) to be objective, logical relations between propositions (or sentences), and hence not to depend in any way upon belief. They are degrees of (partial) entailment, or degrees of logical consequence, not degrees of belief. (They do, nevertheless, dictate proper degrees of belief, as is discussed below.) Frank P. Ramsey, on the other hand, was skeptical about the existence of such objective logical relations and argued that (evidential) probability is "the logic of partial belief". (p 157) In other words, Ramsey held that epistemic probabilities simply "are" degrees of rational belief, rather than being logical relations that merely "constrain" degrees of rational belief.
Another point of disagreement concerns the "uniqueness" of evidential probability, relative to a given state of knowledge. Rudolf Carnap held, for example, that logical principles always determine a unique logical probability for any statement, relative to any body of evidence. Ramsey, by contrast, thought that while degrees of belief are subject to some rational constraints (such as, but not limited to, the axioms of probability) these constraints usually do not determine a unique value. Rational people, in other words, may differ somewhat in their degrees of belief, even if they all have the same information.
Propensity.
Propensity theorists think of probability as a physical propensity, or disposition, or tendency of a given type of physical situation to yield an outcome of a certain kind or to yield a long run relative frequency of such an outcome. This kind of objective probability is sometimes called 'chance'.
Propensities, or chances, are not relative frequencies, but purported causes of the observed stable relative frequencies. Propensities are invoked to explain why repeating a certain kind of experiment will generate given outcome types at persistent rates, which are known as propensities or chances. Frequentists are unable to take this approach, since relative frequencies do not exist for single tosses of a coin, but only for large ensembles or collectives. In contrast, a propensitist is able to use the law of large numbers to explain the behaviour of long-run frequencies. This law, which is a consequence of the axioms of probability, says that if (for example) a coin is tossed repeatedly many times, in such a way that its probability of landing heads is the same on each toss, and the outcomes are probabilistically independent, then the relative frequency of heads will be close to the probability of heads on each single toss. This law allows that stable long-run frequencies are a manifestation of invariant "single-case" probabilities. In addition to explaining the emergence of stable relative frequencies, the idea of propensity is motivated by the desire to make sense of single-case probability attributions in quantum mechanics, such as the probability of decay of a particular atom at a particular time.
The main challenge facing propensity theories is to say exactly what propensity means. (And then, of course, to show that propensity thus defined has the required properties.) At present, unfortunately, none of the well-recognised accounts of propensity comes close to meeting this challenge.
A propensity theory of probability was given by Charles Sanders Peirce. A later propensity theory was proposed by philosopher Karl Popper, who had only slight acquaintance with the writings of C. S. Peirce, however. Popper noted that the outcome of a physical experiment is produced by a certain set of "generating conditions". When we repeat an experiment, as the saying goes, we really perform another experiment with a (more or less) similar set of generating conditions. To say that a set of generating conditions has propensity "p" of producing the outcome "E" means that those exact conditions, if repeated indefinitely, would produce an outcome sequence in which "E" occurred with limiting relative frequency "p". For Popper then, a deterministic experiment would have propensity 0 or 1 for each outcome, since those generating conditions would have same outcome on each trial. In other words, non-trivial propensities (those that differ from 0 and 1) only exist for genuinely indeterministic experiments.
A number of other philosophers, including David Miller and Donald A. Gillies, have proposed propensity theories somewhat similar to Popper's.
Other propensity theorists (e.g. Ronald Giere) do not explicitly define propensities at all, but rather see propensity as defined by the theoretical role it plays in science. They argue, for example, that physical magnitudes such as electrical charge cannot be explicitly defined either, in terms of more basic things, but only in terms of what they do (such as attracting and repelling other electrical charges). In a similar way, propensity is whatever fills the various roles that physical probability plays in science.
What roles does physical probability play in science? What are its properties? One central property of chance is that, when known, it constrains rational belief to take the same numerical value. David Lewis called this the "Principal Principle", (3.3 & 3.5) a term that philosophers have mostly adopted. For example, suppose you are certain that a particular biased coin has propensity 0.32 to land heads every time it is tossed. What is then the correct price for a gamble that pays $1 if the coin lands heads, and nothing otherwise? According to the Principal Principle, the fair price is 32 cents.
Subjectivism.
Subjectivists, also known as Bayesians or followers of epistemic probability, give the notion of probability a subjective status by regarding it as a measure of the 'degree of belief' of the individual assessing the uncertainty of a particular situation. Epistemic or subjective probability is sometimes called credence, as opposed to the term chance for a propensity probability.
Some examples of epistemic probability are to assign a probability to the proposition that a proposed law of physics is true, and to determine how probable it is that a suspect committed a crime, based on the evidence presented.
Gambling odds don't reflect the bookies' belief in a likely winner, so much as the other bettors' belief, because the bettors are actually betting against one another. The odds are set based on how many people have bet on a possible winner, so that even if the high odds players always win, the bookies will always make their percentages anyway.
The use of Bayesian probability raises the philosophical debate as to whether it can contribute valid justifications of belief.
Bayesians point to the work of Ramsey (p 182) and de Finetti (p 103) as proving that subjective beliefs must follow the laws of probability if they are to be coherent. Evidence casts doubt that humans will have coherent beliefs.
The use of Bayesian probability involves specifying a prior probability. This may be obtained from consideration of whether the required prior probability is greater or lesser than a reference probability associated with an urn model or a thought experiment. The issue is that for a given problem, multiple thought experiments could apply, and choosing one is a matter of judgement: different people may assign different prior probabilities, known as the reference class problem.
The "sunrise problem" provides an example.
Prediction.
An alternative account of probability emphasizes the role of "prediction" – predicting future observations on the basis of past observations, not on unobservable parameters. In its modern form, it is mainly in the Bayesian vein. This was the main function of probability before the 20th century,
but fell out of favor compared to the parametric approach, which modeled phenomena as a physical system that was observed with error, such as in celestial mechanics.
The modern predictive approach was pioneered by Bruno de Finetti, with the central idea of exchangeability – that future observations should behave like past observations. This view came to the attention of the Anglophone world with the 1974 translation of de Finetti's book, and has
since been propounded by such statisticians as Seymour Geisser.
Axiomatic probability.
The mathematics of probability can be developed on an entirely axiomatic basis that is independent of any interpretation: see the articles on probability theory and probability axioms for a detailed treatment.

</doc>
<doc id="23539" url="https://en.wikipedia.org/wiki?curid=23539" title="Probability axioms">
Probability axioms

In Kolmogorov's probability theory, the probability "P" of some event "E", denoted formula_1, is usually defined such that "P" satisfies the Kolmogorov axioms, named after the famous Russian mathematician Andrey Kolmogorov, which are described below.
These assumptions can be summarised as follows: Let (Ω, "F", "P") be a measure space with "P"(Ω)=1. Then (Ω, "F", "P") is a probability space, with sample space Ω, event space "F" and probability measure "P".
An alternative approach to formalising probability, favoured by some Bayesians, is given by Cox's theorem.
Axioms.
First axiom.
The probability of an event is a non-negative real number:
where formula_3 is the event space. In particular, formula_1 is always finite, in contrast with more general measure theory. Theories which assign negative probability relax the first axiom.
Second axiom.
This is the assumption of unit measure: that the probability that at least one of the elementary events in the entire sample space will occur is 1. More specifically, there are no elementary events outside the sample space.
This is often overlooked in some mistaken probability calculations; if you cannot precisely define the whole sample space, then the probability of any subset cannot be defined either.
Third axiom.
This is the assumption of σ-additivity:
Some authors consider merely finitely additive probability spaces, in which case one just needs an algebra of sets, rather than a σ-algebra. Quasiprobability distributions in general relax the third axiom.
Consequences.
From the Kolmogorov axioms, one can deduce other useful rules for calculating probabilities.
The numeric bound.
It immediately follows from the monotonicity property that
Proofs.
The proofs of these properties are both interesting and insightful. They illustrate the power of the third axiom,
and its interaction with the remaining two axioms. When studying axiomatic probability theory, many deep consequences follow from merely these three axioms.
In order to verify the monotonicity property, we set formula_11 and formula_12,
where formula_13 for formula_14. It is easy to see that the sets formula_15
are pairwise disjoint and formula_16. Hence,
we obtain from the third axiom that
Since the left-hand side of this equation is a series of non-negative numbers, and that it converges to
formula_18 which is finite, we obtain both formula_19 and formula_20.
The second part of the statement is seen by contradiction: if formula_21 then the left hand side is not less than infinity
If formula_23 then we obtain a contradiction, because the sum does not exceed formula_18 which is finite. Thus, formula_25. We have shown as a byproduct of the proof of monotonicity that formula_20.
Further consequences.
Another important property is:
This is called the addition law of probability, or the sum rule.
That is, the probability that "A" "or" "B" will happen is the sum of the
probabilities that "A" will happen and that "B" will happen, minus the
probability that both "A" "and" "B" will happen. The proof of this is as follows:
now, formula_29.
Eliminating formula_30 from both equations gives us the desired result.
This can be extended to the inclusion-exclusion principle.
That is, the probability that any event will "not" happen (or the event's complement) is 1 minus the probability that it will.
Simple example: coin toss.
Consider a single coin-toss, and assume that the coin will either land heads (H) or tails (T) (but not both). No assumption is made as to whether the coin is fair.
We may define:
Kolmogorov's axioms imply that:
The probability of "neither" heads "nor" tails, is 0.
The probability of "either" heads "or" tails, is 1.
The sum of the probability of heads and the probability of tails, is 1.

</doc>
<doc id="23542" url="https://en.wikipedia.org/wiki?curid=23542" title="Probability theory">
Probability theory

Probability theory is the branch of mathematics concerned with probability, the analysis of random phenomena. The central objects of probability theory are random variables, stochastic processes, and events: mathematical abstractions of non-deterministic events or measured quantities that may either be single occurrences or evolve over time in an apparently random fashion.
It is not possible to predict precisely results of random events. However, if a sequence of individual events, such as coin flipping or the roll of dice, is influenced by other factors, such as friction, it will exhibit certain patterns, which can be studied and predicted. Two representative mathematical results describing such patterns are the law of large numbers and the central limit theorem.
As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of large sets of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics. A great discovery of twentieth century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics.
History.
The mathematical theory of probability has its roots in attempts to analyze games of chance by Gerolamo Cardano in the sixteenth century, and by Pierre de Fermat and Blaise Pascal in the seventeenth century (for example the "problem of points"). Christiaan Huygens published a book on the subject in 1657 and in the 19th century a big work was done by Laplace in what can be considered today as the classic interpretation.
Initially, probability theory mainly considered discrete events, and its methods were mainly combinatorial. Eventually, analytical considerations compelled the incorporation of continuous variables into the theory.
This culminated in modern probability theory, on foundations laid by Andrey Nikolaevich Kolmogorov. Kolmogorov combined the notion of sample space, introduced by Richard von Mises, and measure theory and presented his axiom system for probability theory in 1933. Fairly quickly this became the mostly undisputed axiomatic basis for modern probability theory but alternatives exist, in particular the adoption of finite rather than countable additivity by Bruno de Finetti.
Treatment.
Most introductions to probability theory treat discrete probability distributions and continuous probability distributions separately. The more mathematically advanced measure theory based treatment of probability covers both the discrete, the continuous, any mix of these two and more.
Motivation.
Consider an experiment that can produce a number of outcomes. The set of all outcomes is called the "sample space" of the experiment. The "power set" of the sample space (or equivalently, the event space) is formed by considering all different collections of possible results. For example, rolling an honest die produces one of six possible results. One collection of possible results corresponds to getting an odd number. Thus, the subset {1,3,5} is an element of the power set of the sample space of die rolls. These collections are called "events". In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, that event is said to have occurred.
Probability is a way of assigning every "event" a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) be assigned a value of one. To qualify as a probability distribution, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events that contain no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that one of the events will occur is given by the sum of the probabilities of the individual events.
The probability that any one of the events {1,6}, {3}, or {2,4} will occur is 5/6. This is the same as saying that the probability of event {1,2,3,4,6} is 5/6. This event encompasses the possibility of any number except five being rolled. The mutually exclusive event {5} has a probability of 1/6, and the event {1,2,3,4,5,6} has a probability of 1, that is, absolute certainty.
Discrete probability distributions.
Discrete probability theory deals with events that occur in countable sample spaces.
Examples: Throwing dice, experiments with decks of cards, random walk, and tossing coins
Classical definition:
Initially the probability of an event to occur was defined as number of cases favorable for the event, over the number of total outcomes possible in an equiprobable sample space: see Classical definition of probability.
For example, if the event is "occurrence of an even number when a die is rolled", the probability is given by formula_1, since 3 faces out of the 6 have even numbers and each face has the same probability of appearing.
Modern definition:
The modern definition starts with a finite or countable set called the sample space, which relates to the set of all "possible outcomes" in classical sense, denoted by formula_2. It is then assumed that for each element formula_3, an intrinsic "probability" value formula_4 is attached, which satisfies the following properties:
That is, the probability function "f"("x") lies between zero and one for every value of "x" in the sample space "Ω", and the sum of "f"("x") over all values "x" in the sample space "Ω" is equal to 1. An event is defined as any subset formula_7 of the sample space formula_8. The probability of the event formula_7 is defined as
So, the probability of the entire sample space is 1, and the probability of the null event is 0.
The function formula_4 mapping a point in the sample space to the "probability" value is called a probability mass function abbreviated as pmf. The modern definition does not try to answer how probability mass functions are obtained; instead it builds a theory that assumes their existence.
Continuous probability distributions.
Continuous probability theory deals with events that occur in a continuous sample space.
Classical definition:
The classical definition breaks down when confronted with the continuous case. See Bertrand's paradox.
Modern definition:
If the outcome space of a random variable "X" is the set of real numbers (formula_12) or a subset thereof, then a function called the cumulative distribution function (or cdf) formula_13 exists, defined by formula_14. That is, "F"("x") returns the probability that "X" will be less than or equal to "x".
The cdf necessarily satisfies the following properties.
If formula_13 is absolutely continuous, i.e., its derivative exists and integrating the derivative gives us the cdf back again, then the random variable "X" is said to have a probability density function or pdf or simply density formula_19
For a set formula_20, the probability of the random variable "X" being in formula_7 is
In case the probability density function exists, this can be written as
Whereas the "pdf" exists only for continuous random variables, the "cdf" exists for all random variables (including discrete random variables) that take values in formula_24
These concepts can be generalized for multidimensional cases on formula_25 and other continuous sample spaces.
Measure-theoretic probability theory.
The "raison d'être" of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.
An example of such distributions could be a mix of discrete and continuous distributions—for example, a random variable that is 0 with probability 1/2, and takes a random value from a normal distribution with probability 1/2. It can still be studied to some extent by considering it to have a pdf of formula_26, where formula_27 is the Dirac delta function.
Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density. The modern approach to probability theory solves these problems using measure theory to define the probability space:
Given any set formula_8, (also called sample space) and a σ-algebra formula_29 on it, a measure formula_30 defined on formula_29 is called a probability measure if formula_32
If formula_29 is the Borel σ-algebra on the set of real numbers, then there is a unique probability measure on formula_29 for any cdf, and vice versa. The measure corresponding to a cdf is said to be induced by the cdf. This measure coincides with the pmf for discrete variables and pdf for continuous variables, making the measure-theoretic approach free of fallacies.
The "probability" of a set formula_7 in the σ-algebra formula_29 is defined as
where the integration is with respect to the measure formula_38 induced by formula_39
Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside formula_25, as in the theory of stochastic processes. For example, to study Brownian motion, probability is defined on a space of functions.
When it's convenient to work with a dominating measure, the Radon-Nikodym theorem is used to define a density as the Radon-Nikodym derivative of the probability distribution of interest with respect to this dominating measure. Discrete densities are usually defined as this derivative with respect to a counting measure over the set of all possible outcomes. Densities for absolutely continuous distributions are usually defined as this derivative with respect to the Lebesgue measure. If a theorem can be proved in this general setting, it holds for both discrete and continuous distributions as well as others; separate proofs are not required for discrete and continuous distributions.
Classical probability distributions.
Certain random variables occur very often in probability theory because they well describe many natural or physical processes. Their distributions therefore have gained "special importance" in probability theory. Some fundamental "discrete distributions" are the discrete uniform, Bernoulli, binomial, negative binomial, Poisson and geometric distributions. Important "continuous distributions" include the continuous uniform, normal, exponential, gamma and beta distributions.
Convergence of random variables.
In probability theory, there are several notions of convergence for random variables. They are listed below in the order of strength, i.e., any subsequent notion of convergence in the list implies convergence according to all of the preceding notions.
As the names indicate, weak convergence is weaker than strong convergence. In fact, strong convergence implies convergence in probability, and convergence in probability implies weak convergence. The reverse statements are not always true.
Law of large numbers.
Common intuition suggests that if a fair coin is tossed many times, then "roughly" half of the time it will turn up "heads", and the other half it will turn up "tails". Furthermore, the more often the coin is tossed, the more likely it should be that the ratio of the number of "heads" to the number of "tails" will approach unity. Modern probability theory provides a formal version of this intuitive idea, known as the law of large numbers. This law is remarkable because it is not assumed in the foundations of probability theory, but instead emerges from these foundations as a theorem. Since it links theoretically derived probabilities to their actual frequency of occurrence in the real world, the law of large numbers is considered as a pillar in the history of statistical theory and has had widespread influence.
The law of large numbers (LLN) states that the sample average
of a sequence of independent and
identically distributed random variables formula_57 converges towards their common expectation formula_58, provided that the expectation of formula_59 is finite.
It is in the different forms of convergence of random variables that separates the "weak" and the "strong" law of large numbers
It follows from the LLN that if an event of probability "p" is observed repeatedly during independent experiments, the ratio of the observed frequency of that event to the total number of repetitions converges towards "p".
For example, if formula_61 are independent Bernoulli random variables taking values 1 with probability "p" and 0 with probability 1-"p", then formula_62 for all "i", so that formula_63 converges to "p" almost surely.
Central limit theorem.
"The central limit theorem (CLT) is one of the great results of mathematics." (Chapter 18 in)
It explains the ubiquitous occurrence of the normal distribution in nature.
The theorem states that the average of many independent and identically distributed random variables with finite variance tends towards a normal distribution "irrespective" of the distribution followed by the original random variables. Formally, let formula_48 be independent random variables with mean formula_65 and variance formula_66 Then the sequence of random variables
converges in distribution to a standard normal random variable.
For some classes of random variables the classic central limit theorem works rather fast (see Berry–Esseen theorem), for example the distributions with finite first, second and third moment from the exponential family; on the other hand, for some random variables of the heavy tail and fat tail variety, it works very slowly or may not work at all: in such cases one may use the Generalized Central Limit Theorem (GCLT).

</doc>
<doc id="23543" url="https://en.wikipedia.org/wiki?curid=23543" title="Probability distribution">
Probability distribution

In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures.
In applied probability, a probability distribution can be specified in a number of different ways, often chosen for mathematical convenience:
A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector—a set of two or more random variables—taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.
Introduction.
To define probability distributions for the simplest cases, one needs to distinguish between discrete and continuous random variables. In the discrete case, one can easily assign a probability to each possible value: for example, when throwing a fair dice, each of the six values "1" to "6" has the probability 1/6. In contrast, when a random variable takes values from a continuum then, typically, probabilities can be nonzero only if they refer to intervals: in quality control one might demand that the probability of a "500 g" package containing between 490 g and 510 g should be no less than 98%.
If the random variable is real-valued (or more generally, if a total order is defined for its possible values), the cumulative distribution function (CDF) gives the probability that the random variable is no larger than a given value; in the real-valued case, the CDF is the integral of the probability density function (pdf) provided that this function exists.
Terminology.
As probability theory is used in quite diverse applications, terminology is not uniform and sometimes confusing. The following terms are used for non-cumulative probability distribution functions:
The following terms are somewhat ambiguous as they can refer to non-cumulative or cumulative distributions, depending on authors' preferences:
Finally,
Cumulative distribution function.
Because a probability distribution Pr on the real line is determined by the probability of a scalar random variable "X" being in a half-open interval (-∞, "x"], the probability distribution is completely characterized by its cumulative distribution function:
Discrete probability distribution.
A discrete probability distribution is a "probability distribution" characterized by a probability mass function. Thus, the distribution of a random variable "X" is discrete, and "X" is called a discrete random variable, if
as "u" runs through the set of all possible values of "X". Hence, a random variable can assume only a finite or countably infinite number of values—the random variable is a discrete variable. For the number of potential values to be countably infinite, even though their probabilities sum to 1, the probabilities have to decline to zero fast enough. for example, if formula_3 for "n" = 1, 2, ..., we have the sum of probabilities 1/2 + 1/4 + 1/8 + ... = 1.
Well-known discrete probability distributions used in statistical modeling include the Poisson distribution, the Bernoulli distribution, the binomial distribution, the geometric distribution, and the negative binomial distribution. Additionally, the discrete uniform distribution is commonly used in computer programs that make equal-probability random selections between a number of choices.
Measure theoretic formulation.
A measurable function formula_4 between a probability space formula_5 and a measurable space formula_6 is
called a discrete random variable provided its image is a countable set and the pre-image of singleton sets are measurable, i.e., formula_7 for all formula_8.
The latter requirement induces a probability mass function formula_9 via formula_10. Since the pre-images of disjoint sets
are disjoint
This recovers the definition given above.
Cumulative density.
Equivalently to the above, a discrete random variable can be defined as a random variable whose cumulative distribution function (cdf) increases only by jump discontinuities—that is, its cdf increases only where it "jumps" to a higher value, and is constant between those jumps. The points where jumps occur are precisely the values which the random variable may take.
Delta-function representation.
Consequently, a discrete probability distribution is often represented as a generalized probability density function involving Dirac delta functions, which substantially unifies the treatment of continuous and discrete distributions. This is especially useful when dealing with probability distributions involving both a continuous and a discrete part.
Indicator-function representation.
For a discrete random variable "X", let "u"0, "u"1, ... be the values it can take with non-zero probability. Denote
These are disjoint sets, and by formula (1)
It follows that the probability that "X" takes any value except for "u"0, "u"1, ... is zero, and thus one can write "X" as
except on a set of probability zero, where formula_15 is the indicator function of "A". This may serve as an alternative definition of discrete random variables.
Continuous probability distribution.
A continuous probability distribution is a "probability distribution" that has a cumulative distribution function that is continuous. Most often they are generated by having a probability density function. Mathematicians call distributions with probability density functions absolutely continuous, since their cumulative distribution function is absolutely continuous with respect to the Lebesgue measure "λ". If the distribution of "X" is continuous, then "X" is called a continuous random variable. There are many examples of continuous probability distributions: normal, uniform, chi-squared, and others.
Intuitively, a continuous random variable is the one which can take a continuous range of values—as opposed to a discrete distribution, where the set of possible values for the random variable is at most countable. While for a discrete distribution an event with probability zero is impossible (e.g., rolling 3 on a standard die is impossible, and has probability zero), this is not so in the case of a continuous random variable. For example, if one measures the width of an oak leaf, the result of 3½ cm is possible; however, it has probability zero because uncountably many other potential values exist even between 3 cm and 4 cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval is nonzero. This apparent paradox is resolved by the fact that the probability that "X" attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero.
Formally, if "X" is a continuous random variable, then it has a probability density function "ƒ"("x"), and therefore its probability of falling into a given interval, say is given by the integral
In particular, the probability for "X" to take any single value "a" (that is ) is zero, because an integral with coinciding upper and lower limits is always equal to zero.
The definition states that a continuous probability distribution must possess a density, or equivalently, its cumulative distribution function be absolutely continuous. This requirement is stronger than simple continuity of the cumulative distribution function, and there is a special class of distributions, singular distributions, which are neither continuous nor discrete nor a mixture of those. An example is given by the Cantor distribution. Such singular distributions however are never encountered in practice.
Note on terminology: some authors use the term "continuous distribution" to denote the distribution with continuous cumulative distribution function. Thus, their definition includes both the (absolutely) continuous and singular distributions.
By one convention, a probability distribution formula_17 is called "continuous" if its cumulative distribution function formula_18 is continuous and, therefore, the probability measure of singletons formula_19 for all formula_20.
Another convention reserves the term "continuous probability distribution" for absolutely continuous distributions. These distributions can be characterized by a probability density function: a non-negative Lebesgue integrable function formula_21 defined on the real numbers such that
Discrete distributions and some continuous distributions (like the Cantor distribution) do not admit such a density.
Kolmogorov definition.
In the measure-theoretic formalization of probability theory, a random variable is defined as a measurable function "X" from a probability space formula_23 to measurable space formula_24. A probability distribution of "X" is the pushforward measure "X"*P  of "X" , which is a probability measure on formula_24 satisfying "X"*P = P"X" −1.
Random number generation.
A frequent problem in statistical simulations (the Monte Carlo method) is the generation of pseudo-random numbers that are distributed in a given way. Most algorithms are based on a pseudorandom number generator that produces numbers "X" that are uniformly distributed in the interval [0,1). These random variates "X" are then transformed via some algorithm to create a new random variate having the required probability distribution.
Applications.
The concept of the probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory, and the science of statistics. There is spread or variability in almost any value that can be measured in a population (e.g. height of people, durability of a metal, sales growth, traffic flow, etc.); almost all measurements are made with some intrinsic error; in physics many processes are described probabilistically,from the kinetic properties of gases to the quantum mechanical description of fundamental particles. For these and many other reasons, simple numbers are often inadequate for describing a quantity, while probability distributions are often more appropriate.
As a more specific example of an application, the cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.
Common probability distributions.
The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, continuous, multivariate, etc.)
Note also that all of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.

</doc>
<doc id="23545" url="https://en.wikipedia.org/wiki?curid=23545" title="Psychological statistics">
Psychological statistics

Psychological statistics is the application of formulas, theorems, numbers and laws to psychology. Some of the more common applications include:
Some of the more commonly used statistical tests in psychology are:
Parametric tests
Non-parametric tests

</doc>
