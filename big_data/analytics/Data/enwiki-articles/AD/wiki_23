<doc id="19870" url="https://en.wikipedia.org/wiki?curid=19870" title="Meson">
Meson

In particle physics, mesons ( or ) are hadronic subatomic particles composed of one quark and one antiquark, bound together by the strong interaction. Because mesons are composed of sub-particles, they have a physical size, with a diameter of roughly one fermi, which is about the size of a proton or neutron. All mesons are unstable, with the longest-lived lasting for only a few hundredths of a microsecond. Charged mesons decay (sometimes through intermediate particles) to form electrons and neutrinos. Uncharged mesons may decay to photons.
Mesons are not produced by radioactive decay, but appear in nature only as short-lived products of very high-energy interactions in matter, between particles made of quarks. In cosmic ray interactions, for example, such particles are ordinary protons and neutrons. Mesons are also frequently produced artificially in high-energy particle accelerators that collide protons, anti-protons, or other particles.
In nature, the importance of lighter mesons is that they are the associated quantum-field particles that transmit the nuclear force, in the same way that photons are the particles that transmit the electromagnetic force. The higher energy (more massive) mesons were created momentarily in the Big Bang, but are not thought to play a role in nature today. However, such particles are regularly created in experiments, in order to understand the nature of the heavier types of quark that compose the heavier mesons.
Mesons are part of the hadron particle family, and are defined simply as particles composed of two quarks. The other members of the hadron family are the baryons: subatomic particles composed of three quarks rather than two. Some experiments show evidence of exotic mesons, which do not have the conventional valence quark content of one quark and one antiquark.
Because quarks have a spin of , the difference in quark-number between mesons and baryons results in conventional two-quark mesons being bosons, whereas baryons are fermions.
Each type of meson has a corresponding antiparticle (antimeson) in which quarks are replaced by their corresponding antiquarks and vice versa. For example, a positive pion () is made of one up quark and one down antiquark; and its corresponding antiparticle, the negative pion (), is made of one up antiquark and one down quark.
Because mesons are composed of quarks, they participate in both the weak and strong interactions. Mesons with net electric charge also participate in the electromagnetic interaction. They are classified according to their quark content, total angular momentum, parity and various other properties, such as C-parity and G-parity. Although no meson is stable, those of lower mass are nonetheless more stable than the most massive mesons, and are easier to observe and study in particle accelerators or in cosmic ray experiments. They are also typically less massive than baryons, meaning that they are more easily produced in experiments, and thus exhibit certain higher energy phenomena more readily than baryons composed of the same quarks would. For example, the charm quark was first seen in the J/Psi meson () in 1974, and the bottom quark in the upsilon meson () in 1977.
History.
From theoretical considerations, in 1934 Hideki Yukawa predicted the existence and the approximate mass of the "meson" as the carrier of the nuclear force that holds atomic nuclei together. If there were no nuclear force, all nuclei with two or more protons would fly apart because of the electromagnetic repulsion. Yukawa called his carrier particle the meson, from μέσος "mesos", the Greek word for "intermediate," because its predicted mass was between that of the electron and that of the proton, which has about 1,836 times the mass of the electron. Yukawa had originally named his particle the "mesotron", but he was corrected by the physicist Werner Heisenberg (whose father was a professor of Greek at the University of Munich). Heisenberg pointed out that there is no "tr" in the Greek word "mesos".
The first candidate for Yukawa's meson, now known in modern terminology as the muon, was discovered in 1936 by Carl David Anderson and others in the decay products of cosmic ray interactions. The mu meson had about the right mass to be Yukawa's carrier of the strong nuclear force, but over the course of the next decade, it became evident that it was not the right particle. It was eventually found that the "mu meson" did not participate in the strong nuclear interaction at all, but rather behaved like a heavy version of the electron, and was eventually classed as a lepton like the electron, rather than a meson. Physicists in making this choice decided that properties other than particle mass should control their classification.
There were years of delays in the subatomic particle research during World War II in 1939–45, with most physicists working in applied projects for wartime necessities. When the war ended in August 1945, many physicists gradually returned to peacetime research. The first true meson to be discovered was what would later be called the "pi meson" (or pion). This discovery was made in 1947, by Cecil Powell, César Lattes, and Giuseppe Occhialini, who were investigating cosmic ray products at the University of Bristol in England, based on photographic films placed in the Andes mountains. Some mesons in these films had about the same mass as the already-known meson, yet seemed to decay into it, leading physicist Robert Marshak to hypothesize in 1947 that it was actually a new and different meson. Over the next few years, more experiments showed that the pion was indeed involved in strong interactions. The pion (as a virtual particle) is the primary force carrier for the nuclear force in atomic nuclei. Other mesons, such as the rho mesons are involved in mediating this force as well, but to lesser extents. Following the discovery of the pion, Yukawa was awarded the 1949 Nobel Prize in Physics for his predictions.
The word "meson" has at times been used to mean "any" force carrier, such as the "Z0 meson", which is involved in mediating the weak interaction. However, this spurious usage has fallen out of favor. Mesons are now defined as particles composed of pairs of quarks and antiquarks.
Overview.
Spin, orbital angular momentum, and total angular momentum.
Spin (quantum number S) is a vector quantity that represents the "intrinsic" angular momentum of a particle. It comes in increments of  ħ. The ħ is often dropped because it is the "fundamental" unit of spin, and it is implied that "spin 1" means "spin 1 ħ". (In some systems of natural units, ħ is chosen to be 1, and therefore does not appear in equations).
Quarks are fermions—specifically in this case, particles having spin ("S" = ). Because spin projections vary in increments of 1 (that is 1 ħ), a single quark has a spin vector of length , and has two spin projections ("S"z = + and "S"z = −). Two quarks can have their spins aligned, in which case the two spin vectors add to make a vector of length "S" = 1 and three spin projections ("S"z = +1, "S"z = 0, and "S"z = −1), called the spin-1 triplet. If two quarks have unaligned spins, the spin vectors add up to make a vector of length S = 0 and only one spin projection ("S"z = 0), called the spin-0 singlet. Because mesons are made of one quark and one antiquark, they can be found in triplet and singlet spin states.
There is another quantity of quantized angular momentum, called the orbital angular momentum (quantum number "L"), that comes in increments of 1 ħ, which represent the angular momentum due to quarks orbiting around each other. The total angular momentum (quantum number "J") of a particle is therefore the combination of intrinsic angular momentum (spin) and orbital angular momentum. It can take any value from to , in increments of 1.
Particle physicists are most interested in mesons with no orbital angular momentum ("L" = 0), therefore the two groups of mesons most studied are the "S" = 1; "L" = 0 and "S" = 0; "L" = 0, which corresponds to "J" = 1 and "J" = 0, although they are not the only ones. It is also possible to obtain "J" = 1 particles from "S" = 0 and "L" = 1. How to distinguish between the "S" = 1, "L" = 0 and "S" = 0, "L" = 1 mesons is an active area of research in meson spectroscopy.
Parity.
If the universe were reflected in a mirror, most of the laws of physics would be identical—things would behave the same way regardless of what we call "left" and what we call "right". This concept of mirror reflection is called parity ("P"). Gravity, the electromagnetic force, and the strong interaction all behave in the same way regardless of whether or not the universe is reflected in a mirror, and thus are said to conserve parity (P-symmetry). However, the weak interaction does" "distinguish "left" from "right", a phenomenon called parity violation (P-violation).
Based on this, one might think that, if the wavefunction for each particle (more precisely, the quantum field for each particle type) were simultaneously mirror-reversed, then the new set of wavefunctions would perfectly satisfy the laws of physics (apart from the weak interaction). It turns out that this is not quite true: In order for the equations to be satisfied, the wavefunctions of certain types of particles have to be multiplied by −1, in addition to being mirror-reversed. Such particle types are said to have "negative" or "odd" parity ("P" = −1, or alternatively "P" = –), whereas the other particles are said to have "positive" or "even" parity ("P" = +1, or alternatively "P" = +).
For mesons, the parity is related to the orbital angular momentum by the relation:
where the "L" is a result of the parity of the corresponding spherical harmonic of the wavefunction. The '+1' in the exponent comes from the fact that, according to the Dirac equation, a quark and an antiquark have opposite intrinsic parities. Therefore, the intrinsic parity of a meson is the product of the intrinsic parities of the quark (+1) and antiquark (−1). As these are different, their product is −1, and so it contributes a +1 in the exponent.
As a consequence, mesons with no orbital angular momentum ("L" = 0) all have odd parity ("P" = −1).
C-parity.
C-parity is only defined for mesons that are their own antiparticle (i.e. neutral mesons). It represents whether or not the wavefunction of the meson remains the same under the interchange of their quark with their antiquark. If
then, the meson is "C even" (C = +1). On the other hand, if
then the meson is "C odd" (C = −1).
C-parity rarely is studied on its own, but more commonly in combination with P-parity into CP-parity. CP-parity was thought to be conserved, but was later found to be violated in weak interactions.
G-parity.
G parity is a generalization of the C-parity. Instead of simply comparing the wavefunction after exchanging quarks and antiquarks, it compares the wavefunction after exchanging the meson for the corresponding antimeson, regardless of quark content. In the case of neutral meson, G-parity is equivalent to C-parity because neutral mesons are their own antiparticles.
If
then, the meson is "G even" (G = +1). On the other hand, if
then the meson is "G odd" (G = −1).
Isospin and charge.
[[File:Meson nonet - spin 0.svg|thumb|200px|
Combinations of one u, d or s quarks and one u, d, or s antiquark in configuration form a nonet.]]
The concept of isospin was first proposed by Werner Heisenberg in 1932 to explain the similarities between protons and neutrons under the strong interaction. Although they had different electric charges, their masses were so similar that physicists believed that they were actually the same particle. The different electric charges were explained as being the result of some unknown excitation similar to spin. This unknown excitation was later dubbed "isospin" by Eugene Wigner in 1937. When the first mesons were discovered, they too were seen through the eyes of isospin and so the three pions were believed to be the same particle, but in different isospin states.
This belief lasted until Murray Gell-Mann proposed the quark model in 1964 (containing originally only the u, d, and s quarks). The success of the isospin model is now understood to be the result of the similar masses of the u and d quarks. Because the u and d quarks have similar masses, particles made of the same number of them also have similar masses. The exact specific u and d quark composition determines the charge, because u quarks carry charge + whereas d quarks carry charge −. For example, the three pions all have different charges ( (), (a quantum superposition of and states), ()), but have similar masses (~) as they are each made of a same number of total of up and down quarks and antiquarks. Under the isospin model, they were considered to be a single particle in different charged states.
The mathematics of isospin was modeled after that of spin. Isospin projections varied in increments of 1 just like those of spin, and to each projection was associated a "charged state". Because the "pion particle" had three "charged states", it was said to be of isospin "I" = 1. Its "charged states" , , and , corresponded to the isospin projections "I"3 = +1, "I"3 = 0, and "I"3 = −1 respectively. Another example is the "rho particle", also with three charged states. Its "charged states" , , and , corresponded to the isospin projections "I"3 = +1, "I"3 = 0, and "I"3 = −1 respectively. It was later noted that the isospin projections were related to the up and down quark content of particles by the relation
where the "n"'s are the number of up and down quarks and antiquarks.
In the "isospin picture", the three pions and three rhos were thought to be the different states of two particles. However, in the quark model, the rhos are excited states of pions. Isospin, although conveying an inaccurate picture of things, is still used to classify hadrons, leading to unnatural and often confusing nomenclature. Because mesons are hadrons, the isospin classification is also used, with "I"3 = + for up quarks and down antiquarks, and "I"3 = − for up antiquarks and down quarks.
Flavour quantum numbers.
The strangeness quantum number "S" (not to be confused with spin) was noticed to go up and down along with particle mass. The higher the mass, the lower the strangeness (the more s quarks). Particles could be described with isospin projections (related to charge) and strangeness (mass) (see the uds nonet figures). As other quarks were discovered, new quantum numbers were made to have similar description of udc and udb nonets. Because only the u and d mass are similar, this description of particle mass and charge in terms of isospin and flavour quantum numbers only works well for the nonets made of one u, one d and one other quark and breaks down for the other nonets (for example ucb nonet). If the quarks all had the same mass, their behaviour would be called "symmetric", because they would all behave in exactly the same way with respect to the strong interaction. However, as quarks do not have the same mass, they do not interact in the same way (exactly like an electron placed in an electric field will accelerate more than a proton placed in the same field because of its lighter mass), and the symmetry is said to be broken.
It was noted that charge ("Q") was related to the isospin projection ("I"3), the baryon number ("B") and flavour quantum numbers ("S", "C", "B"′, "T") by the Gell-Mann–Nishijima formula:
where "S", "C", "B"′, and "T" represent the strangeness, charm, bottomness and topness flavour quantum numbers respectively. They are related to the number of strange, charm, bottom, and top quarks and antiquark according to the relations:
meaning that the Gell-Mann–Nishijima formula is equivalent to the expression of charge in terms of quark content:
Classification.
Mesons are classified into groups according to their isospin ("I"), total angular momentum ("J"), parity ("P"), G-parity ("G") or C-parity ("C") when applicable, and quark (q) content. The rules for classification are defined by the Particle Data Group, and are rather convoluted. The rules are presented below, in table form for simplicity.
Types of meson.
Mesons are classified into types according to their spin configurations. Some specific configurations are given special names based on the mathematical properties of their spin configuration.
Nomenclature.
Flavourless mesons.
Flavourless mesons are mesons made of pair of quark and antiquarks of the same flavour (all their flavour quantum numbers are zero: "S" = 0, "C" = 0, "B"′ = 0, "T" = 0). The rules for flavourless mesons are:
† The C parity is only relevant to neutral mesons.<br>
†† For "J""PC"=1−−, the ψ is called the 
In addition:
Flavoured mesons.
Flavoured mesons are mesons made of pair of quark and antiquarks of different flavours. The rules are simpler in this case: the main symbol depends on the heavier quark, the superscript depends on the charge, and the subscript (if any) depends on the lighter quark. In table form, they are:
In addition:
Exotic mesons.
There is experimental evidence for particles that are hadrons (i.e., are composed of quarks) and are color-neutral with zero baryon number, and thus by conventional definition are mesons. Yet, these particles do not consist of a single quark-antiquark pair, as all the other conventional mesons discussed above do. A tentative category for these particles is exotic mesons.
There are at least five exotic meson resonances that have been experimentally confirmed to exist by two or more independent experiments. The most statistically significant of these is the Z(4430), discovered by the Belle experiment in 2007 and confirmed by LHCb in 2014. It is a candidate for being a tetraquark: a particle composed of two quarks and two antiquarks. See the main article above for other particle resonances that are candidates for being exotic mesons.

</doc>
<doc id="19871" url="https://en.wikipedia.org/wiki?curid=19871" title="Marvel Super Heroes (role-playing game)">
Marvel Super Heroes (role-playing game)

Marvel Superheroes (MSHRPG), aka "the FASERIP system," is a role playing game set in the Marvel Universe, first published by TSR under license from Marvel Comics in 1984. In 1986, TSR published an expanded edition, entitled the "Marvel Superheroes Advanced Game". Jeff Grubb designed both editions, and Steve Winter wrote both editions. Both use the same game system.
The basic game was designed to let players assume the roles of superheroes from Marvel Comics, such as Spider-Man, Daredevil, Hulk, Captain America, the Fantastic Four, the X-Men, and 
others. 
The game was designed to be easy to understand, and the simplest version, found in the 16-page "Battle Book" of the Basic Set, contains a bare-bones combat system sufficient to resolve comic book style superheroic fights.
System.
Attributes.
Most game situations are resolved by rolling percentile dice and comparing the results against a column of the colorful "Universal Results Table". The column used is determined by the attribute used; different tasks are resolved by reference to different attributes. All characters have seven basic attributes:
Fighting, which determines hit probability in and defense against hand-to-hand attacks.
Agility, which determines hit probability in and defense against ranged attacks, feats of agility vs. the environment, and similar acrobatics.
Strength, which determines damage inflicted by hand-to-hand attacks as well as the success of tasks such as grappling or the lifting and breaking of heavy objects.
Endurance, which determines resistance to physical damage (e.g., poison, disease, death) it also determined how long a character can fight and how fast a character could move at top speed by exerting themselves.
Reason, which determines the success of tasks relating to knowledge, puzzle-solving, and advanced technology.
Intuition, which determines the success of tasks relating to awareness, perception, and instinct.
Psyche, which determines the success of tasks relating to willpower, psionics, and magic.
Players sometimes refer to this set of attributes, or the game system as a whole, by the acronym "FASERIP". Attribute scores for the majority of characters range from 1 to 100, where normal human ability is Typical (6), and peak (non-superheroic) human ability is Excellent (20). However, the designers minimize use of the numerical figures, instead preferring adjectives in the Marvel Comics tradition, such as "Incredible" (scores from 36-45) and "Amazing" (46-62). A "Typical" (5-7) attribute has a 50% base chance for success at most tasks relating to that attribute. For example, a character with "Typical" fighting skill has a base chance of 50% to connect with a punch. As an attribute increases, the chance of success increases by about 5% per 10 points. Thus a character with an "Amazing" (50) attribute has a 75% chance of success at tasks relating to that attribute.
Superpowers.
Beyond the seven attributes, characters possessed superpowers, such as Spider-Man's wall crawling, or Mister Fantastic's elasticity. The powers function on a mostly "ad hoc" basis, and thus each character's description gives considerable space to a description of how his or her powers work in the game.
Each character had an origin, which put ceilings on a character's abilities and superpowers. The origins included: Altered Humans (normal people who acquired powers, such as Spider-Man or the Fantastic Four), High-Tech Wonders (normal people whose powers come from devices, e.g., Iron Man), Mutants (persons born with superpowers, such as the X-Men), Robots (created beings such as the Vision and Ultron), and Aliens (a blanket term used to cover non-humans, including extra-dimensional beings such as Thor and Hercules).
Talents.
The game also featured a simple skill system, referred to as Talents. Talents had to be learned and covered a wide range of knowledges from Archery to Zoology. A Talent raised a character's ability by one rank when attempting actions related to that Talent. For example, a character uses his Agility score when attempting ranged attacks. A character with an Agility of Excellent would normally roll on that column when attacking with a rifle. However, if he had the "Guns" Talent he would treat his Agility as the next higher power rank (Remarkable). The GM was free to determine if a character would be unable to attempt an action without the appropriate Talent (such as a character with no medical background attempting to make a pill that can cure a rare disease).
Resources and Popularity.
Characters also had two variable attributes: Resources and Popularity. These attributes were described using the same terms as the character's seven attributes ("Poor," "Amazing," "Unearthly," etc.). But unlike the seven physical and mental attributes which changed very slowly, if at all, Resources and Popularity could change very quickly.
The first of the variables, Resources, represented the character's wealth and ability to obtain goods or services. Rather than have the player keep track of how much money the character had in the bank or with him, the Advanced Game assumed the character had enough money coming in to cover his basic living expenses. The Resources ability was used when the character wished to purchase something out of the ordinary like a new car or house. For example, the referee might decide a character with Typical resources would probably be unable to purchase a brand new sports car, but with a Yellow Resources roll might be able to afford a used car in good condition. The game books note that a character's Resources score can change for a variety of reasons, such as winning the lottery or having a major business transaction go bad.
The second variable, Popularity, reflected how much the character was liked (or disliked) in the Marvel Universe. Popularity could be used to influence non-player characters. A superhero with a high rating, like Captain America (whose popularity is Unearthly-the highest most characters can achieve), might be able to use his Popularity to gain entrance to a club because the general population of the Marvel Universe admires him. If he were to try the same thing as his secret identity Steve Rogers (whose Popularity is only Typical), he would probably be unable to do it. Villains also had a Popularity score, which was usually negative (a bouncer might let Doctor Doom or Magneto into the aforementioned club simply out of fear). There were several ways Popularity could change. For example, if Doctor Doom defeated Spider-Man in front of the general public, Spidey's Popularity would go down for a short time. But if everyone's favorite web-slinger managed to foil one of Doctor Doom's plans and the word got out, he would enjoy a temporary Popularity boost. Since mutants were generally feared and distrusted in the Marvel Universe, these characters start with a Popularity of 0 and have a hard time improving this attribute.
Character creation.
The game was intended to be played using existing Marvel characters as the heroes. The and Advanced Sets both contained fairly simple systems for creating original superheroes, based on random ability rolls (as in Dungeons & Dragons). In addition, the Basic Set Campaign Book also allowed players to create original heroes by simply describing the desired kind of hero, and working together with the GM to assign the appropriate abilities, powers, and talents.
"The Ultimate Powers Book", by David Edward Martin, expanded and organized the game's list of powers, making a fairly comprehensive survey of comic book-style super-powers. Players were given a wide variety of body types, secret origins, weaknesses, and powers. The "UPB" gave a much greater range to characters one could create. Additionally, the book suffered from editing problems and omissions; several errata and partial revisions were released in the pages of TSR's publication Dragon Magazine in issue #122 "The Ultimate Addenda to the Ultimate Powers Book", issue #134 "The Ultimate Addenda's Addenda", issue #150 "Death Effects on Superheroes", and issue #151 "Son of the Ultimate Addenda". The expanded, corrected version of the book is available for free on the Web, and was compiled by Zan of Heroplay.
Karma.
The game's equivalent of experience points was "Karma", a pool of points initially determined as the sum of a character's three mental attributes (Reason, Intuition, and Psyche).
The basic system allowed players to increase their chances of success at most tasks by spending points of Karma. For example, a player who wanted to make sure he would hit a villain in a critical situation could spend however many Karma points were necessary to raise the dice roll to the desired result. Additional Karma points were distributed by the referee at the end of game sessions, typically as rewards for accomplishing heroic goals, such as defeating villains, saving innocents, and foiling crimes. Conversely, Karma could be lost for unheroic actions such as fleeing from a villain, or failing to stop a crime: in fact, in a notable departure from many RPGs (but strongly in keeping with the genre), all Karma was lost if a hero killed someone or allowed someone to die.
In the Advanced Game, Karma points could also be spent to permanently increase character attributes and powers (at a relatively moderate cost, ten times the attribute number raised, powers were steeper, at twenty times the number). The Karma system thus united two RPG mechanics—"Action" or "Hero" points (which allow players to control random outcomes) and character advancement (e.g., "experience points")—in one system. Though this system could frustrate both referees and players (the former because a player willing and able to spend Karma could effectively overcome any challenge at least once; the latter because advancement was slow compared with most other RPGs), it had the virtue of emulating two central features of super-hero comics, namely, that heroes almost always win, even in improbable circumstances, and that heroes' power levels remain mostly static. Furthermore, the system encouraged players to keep their characters' behavior to the equivalent concept of their alignment by giving an incentive to behave heroically and morally correct.
Game mechanics.
Marvel Superheroes was driven by two primary game mechanics: column shifts and colored results. Both essentially influenced the difficulty of an action.
A column shift is used when a character is attempting an exceptionally hard or easy action. A column shift to the left indicates a penalty, while a shift to the right indicates a bonus. For example, Reed Richards (Mr. Fantastic) has an Intuition of Excellent, making him significantly more perceptive than the average person whose Intuition is Typical (two ranks lower). The GM might determine that spotting a trap hidden beneath a few sticks and leaves will be fairly easy, and give the player running Mr. Fantastic a +1 column shift. His Intuition will be treated as Remarkable (the next column to the right). However, a trap buried underground might be considerably harder to spot, and the GM might give the player a -1 column shift penalty. In this case, Mr. Fantastic's Intuition will only be treated as Good (the column to the left).
The column for each ability is divided into four colors: white, green, yellow, and red. A white result is always a failure or unfavorable outcome. In most cases, getting a green result was all that was needed to succeed at a particular action. Yellow and red results usually indicated more favorable results that could knock back, stun, or even kill an opponent. However, the GM could determine that succeeding at an exceptionally hard task might require a yellow or red result.
Additional rules in the "Campaign Book" of the , and the subsequent Advanced Set, used the same game mechanic to resolve non-violent tasks. For example, if a superhero needs to figure out how to operate a piece of alien technology, the hero would have to succeed at a Reason roll, where the chance of success is modified by the complexity of the device.
Official game supplements.
The original Marvel Super Heroes game received extensive support from TSR, covering a wide variety of Marvel Comics characters and settings, including a "Gamer's Handbook of the Marvel Universe" patterned after Marvel's "Official Handbook of the Marvel Universe". MSH even received its own column in the (at the time) TSR-published gaming magazine, Dragon, called "The Marvel-phile", which usually spotlighted a character or group of characters that hadn't yet appeared in a published game product.
Reception.
Steve Kenson commented that "it's a testament to the game's longevity that it still has enthusiastic fan support on the Internet and an active play community more than a decade after its last product was published. Even more so that it continues to set a standard by which new superhero roleplaying games are measured. Like modern comic book writers and artists following the greats of the Silver Age, modern RPG designers have a tough act to follow."
Later Marvel RPGs.
Before losing the MSH license back to Marvel Comics, TSR published a different game using their SAGA System game engine, called the "Marvel Super Heroes Adventure Game". This version, written by Mike Selinker, was published in the late 1990s as a card-based version of the Marvel role-playing game (though a method of converting characters from the prior format to the SAGA System was included in the core rules). Though critically praised in various reviews at the time, it never reached a large market and has since faded into obscurity. 
In 2003, after the gaming license had reverted to Marvel Comics, the "Marvel Universe Roleplaying Game" was published by Marvel Comics. This edition uses mechanics that are totally different from any previous versions, using a diceless game mechanic that incorporated a Karma-based resolution system of "stones" (or tokens) to represent character effort. Since its initial publication, a few additional supplements were published by Marvel Comics. However, Marvel stopped supporting the game a little over a year after its initial release, despite going through several printings of the core rulebook. Some fans continue to create material for it. 
In August 2011, Margaret Weis Productions acquired the licence to publish RPG based on Marvel superheroes, and the first book in their series, titled "Marvel Heroic Roleplaying Basic Game", was released in February 28, 2012. Margaret Weis Productions, however, found that although the game was critically acclaimed, winning two Origins Awards, Marvel Heroic Roleplaying: Civil War "didn’t garner the level of sales necessary to sustain the rest of the line" so they brought the game to a close at the end of April 2013.

</doc>
<doc id="19873" url="https://en.wikipedia.org/wiki?curid=19873" title="Measure (mathematics)">
Measure (mathematics)

In mathematical analysis, a measure on a set is a systematic way to assign a number to each suitable subset of that set, intuitively interpreted as its size. In this sense, a measure is a generalization of the concepts of length, area, and volume. A particularly important example is the Lebesgue measure on a Euclidean space, which assigns the conventional length, area, and volume of Euclidean geometry to suitable subsets of the -dimensional Euclidean space . For instance, the Lebesgue measure of the interval in the real numbers is its length in the everyday sense of the word – specifically, 1.
Technically, a measure is a function that assigns a non-negative real number or +∞ to (certain) subsets of a set ("see" Definition below). It must assign 0 to the empty set and be (countably) additive: the measure of a 'large' subset that can be decomposed into a finite (or countable) number of 'smaller' disjoint subsets, is the sum of the measures of the "smaller" subsets. In general, if one wants to associate a "consistent" size to "each" subset of a given set while satisfying the other axioms of a measure, one only finds trivial examples like the counting measure. This problem was resolved by defining measure only on a sub-collection of all subsets; the so-called "measurable" subsets, which are required to form a -algebra. This means that countable unions, countable intersections and complements of measurable subsets are measurable. Non-measurable sets in a Euclidean space, on which the Lebesgue measure cannot be defined consistently, are necessarily complicated in the sense of being badly mixed up with their complement. Indeed, their existence is a non-trivial consequence of the axiom of choice.
Measure theory was developed in successive stages during the late 19th and early 20th centuries by Émile Borel, Henri Lebesgue, Johann Radon, and Maurice Fréchet, among others. The main applications of measures are in the foundations of the Lebesgue integral, in Andrey Kolmogorov's axiomatisation of probability theory and in ergodic theory. In integration theory, specifying a measure allows one to define integrals on spaces more general than subsets of Euclidean space; moreover, the integral with respect to the Lebesgue measure on Euclidean spaces is more general and has a richer theory than its predecessor, the Riemann integral. Probability theory considers measures that assign to the whole set the size 1, and considers measurable subsets to be events whose probability is given by the measure. Ergodic theory considers measures that are invariant under, or arise naturally from, a dynamical system.
Definition.
Let be a set and a -algebra over . A function from to the extended real number line is called a measure if it satisfies the following properties:
One may require that at least one set has finite measure. Then the empty set automatically has measure zero because of countable additivity, because formula_3, so formula_4.
If only the second and third conditions of the definition of measure above are met, and takes on at most one of the values , then is called a signed measure.
The pair is called a measurable space, the members of are called measurable sets. If formula_5 and formula_6 are two measurable spaces, then a function formula_7 is called measurable if for every -measurable set formula_8, the inverse image is -measurable – i.e.: formula_9. The composition of measurable functions is measurable, making the measurable spaces and measurable functions a category, with the measurable spaces as objects and the set of measurable functions as arrows.
A triple is called a . A probability measure is a measure with total measure one – i.e. . A probability space is a measure space with a probability measure.
For measure spaces that are also topological spaces various compatibility conditions can be placed for the measure and the topology. Most measures met in practice in analysis (and in many cases also in probability theory) are Radon measures. Radon measures have an alternative definition in terms of linear functionals on the locally convex space of continuous functions with compact support. This approach is taken by Bourbaki (2004) and a number of other sources. For more details, see the article on Radon measures.
Examples.
Some important measures are listed here.
Other 'named' measures used in various theories include: Borel measure, Jordan measure, ergodic measure, Euler measure, Gaussian measure, Baire measure, Radon measure, Young measure, and strong measure zero.
In physics an example of a measure is spatial distribution of mass (see e.g., gravity potential), or another non-negative extensive property, conserved (see conservation law for a list of these) or not. Negative values lead to signed measures, see "generalizations" below.
Liouville measure, known also as the natural volume form on a symplectic manifold, is useful in classical statistical and Hamiltonian mechanics.
Gibbs measure is widely used in statistical mechanics, often under the name canonical ensemble.
Properties.
Several further properties can be derived from the definition of a countably additive measure.
Monotonicity.
A measure is monotonic: If and are measurable sets with then
Measures of infinite unions of measurable sets.
A measure is countably subadditive: For any countable sequence of sets in (not necessarily disjoint):
A measure μ is continuous from below: If are measurable sets and is a subset of for all , then the union of the sets is measurable, and
Measures of infinite intersections of measurable sets.
A measure is continuous from above: If , are measurable sets and for all , then the intersection of the sets is measurable; furthermore, if at least one of the has finite measure, then
This property is false without the assumption that at least one of the has finite measure. For instance, for each , let , which all have infinite Lebesgue measure, but the intersection is empty.
Sigma-finite measures.
A measure space is called finite if is a finite real number (rather than ∞). Nonzero finite measures are analogous to probability measures in the sense that any finite measure is proportional to the probability measure formula_14. A measure is called "σ-finite" if can be decomposed into a countable union of measurable sets of finite measure. Analogously, a set in a measure space is said to have a "σ-finite measure" if it is a countable union of sets with finite measure.
For example, the real numbers with the standard Lebesgue measure are σ-finite but not finite. Consider the closed intervals for all integers ; there are countably many such intervals, each has measure 1, and their union is the entire real line. Alternatively, consider the real numbers with the counting measure, which assigns to each finite set of reals the number of points in the set. This measure space is not σ-finite, because every set with finite measure contains only finitely many points, and it would take uncountably many such sets to cover the entire real line. The σ-finite measure spaces have some very convenient properties; σ-finiteness can be compared in this respect to the Lindelöf property of topological spaces. They can be also thought of as a vague generalization of the idea that a measure space may have 'uncountable measure'.
Completeness.
A measurable set is called a "null set" if . A subset of a null set is called a "negligible set". A negligible set need not be measurable, but every measurable negligible set is automatically a null set. A measure is called "complete" if every negligible set is measurable.
A measure can be extended to a complete one by considering the σ-algebra of subsets which differ by a negligible set from a measurable set , that is, such that the symmetric difference of and is contained in a null set. One defines to equal .
Additivity.
Measures are required to be countably additive. However, the condition can be strengthened as follows.
For any set and any set of nonnegative , formula_15 define:
That is, we define the sum of the to be the supremum of all the sums of finitely many of them.
A measure on is -additive if for any and any family formula_17, the following hold:
Note that the second condition is equivalent to the statement that the ideal of null sets is -complete.
Non-measurable sets.
If the axiom of choice is assumed to be true, not all subsets of Euclidean space are Lebesgue measurable; examples of such sets include the Vitali set, and the non-measurable sets postulated by the Hausdorff paradox and the Banach–Tarski paradox.
Generalizations.
For certain purposes, it is useful to have a "measure" whose values are not restricted to the non-negative reals or infinity. For instance, a countably additive set function with values in the (signed) real numbers is called a "signed measure", while such a function with values in the complex numbers is called a "complex measure". Measures that take values in Banach spaces have been studied extensively. A measure that takes values in the set of self-adjoint projections on a Hilbert space is called a "projection-valued measure"; these are used in functional analysis for the spectral theorem. When it is necessary to distinguish the usual measures which take non-negative values from generalizations, the term positive measure is used. Positive measures are closed under conical combination but not general linear combination, while signed measures are the linear closure of positive measures.
Another generalization is the "finitely additive measure", which are sometimes called contents. This is the same as a measure except that instead of requiring "countable" additivity we require only "finite" additivity. Historically, this definition was used first. It turns out that in general, finitely additive measures are connected with notions such as Banach limits, the dual of "L"∞ and the Stone–Čech compactification. All these are linked in one way or another to the axiom of choice.
A charge is a generalization in both directions: it is a finitely additive, signed measure.

</doc>
<doc id="19876" url="https://en.wikipedia.org/wiki?curid=19876" title="Motorcycle">
Motorcycle

A motorcycle (also called a motorbike, bike, or cycle) is a two- or three-wheeled motor vehicle. Motorcycle design varies greatly to suit a range of different purposes: long distance travel, commuting, cruising, sport including racing, and off-road riding. Motorcycling is riding a motorcycle and related social activity such as joining a motorcycle club and attending motorcycle rallies.
In 1894, Hildebrand & Wolfmüller became the first series production motorcycle, and the first to be called a motorcycle. In 2014, the three top motorcycle producers globally by volume were Honda, Yamaha (both from Japan), and Hero MotoCorp (India).
Motorcycles are usually a luxury good in the developed world, where they are used mostly for recreation, as a lifestyle accessory or a symbol of personal identity. In developing countries, motorcycles are overwhelmingly utilitarian due to lower prices and greater fuel economy. Of all the motorcycles in the world, 58% are in the Asia Pacific and Southern and Eastern Asia regions, excluding car-centric Japan.
According to the United States Department of Transportation the number of fatalities per vehicle mile traveled was 37 times higher for motorcycles than for cars.
Types.
The term motorcycle has different legal definitions depending on jurisdiction (see #Legal definitions and restrictions).
There are three major types of motorcycle: street, off-road, and dual purpose. Within these types, there are many sub-types of motorcycles for different purposes. There is often a racing counterpart to each type, such as road racing and street bikes, or motocross and dirt bikes.
Street bikes include cruisers, sportbikes, scooters and mopeds, and many other types. Off-road motorcycles include many types designed for dirt-oriented racing classes such as motocross and are not street legal in most areas. Dual purpose machines like the dual-sport style are made to go off-road but include features to make them legal and comfortable on the street as well.
Each configuration offers either specialised advantage or broad capability, and each design creates a different riding posture.
History.
Experimentation and invention.
The first internal combustion, petroleum fueled motorcycle was the Daimler "Reitwagen". It was designed and built by the German inventors Gottlieb Daimler and Wilhelm Maybach in Bad Cannstatt, Germany in 1885. This vehicle was unlike either the safety bicycles or the boneshaker bicycles of the era in that it had zero degrees of steering axis angle and no fork offset, and thus did not use the principles of bicycle and motorcycle dynamics developed nearly 70 years earlier. Instead, it relied on two outrigger wheels to remain upright while turning.
The inventors called their invention the "Reitwagen" ("riding car"). It was designed as an expedient testbed for their new engine, rather than a true prototype vehicle.
The first commercial design for a self-propelled cycle was a three-wheel design called the Butler Petrol Cycle, conceived of Edward Butler in England in 1884. He exhibited his plans for the vehicle at the Stanley Cycle Show in London in 1884. The vehicle was built by the Merryweather Fire Engine company in Greenwich, in 1888.
The Butler Petrol Cycle was a three-wheeled vehicle, with the rear wheel directly driven by a 5/8hp (466W) 600 cc (40 in3; 2¼×5-inch {57×127-mm}) flat twin four stroke engine (with magneto ignition replaced by coil and battery) equipped with rotary valves and a float-fed carburettor (five years before Maybach) and Ackermann steering, all of which were state of the art at the time. Starting was by compressed air. The engine was liquid-cooled, with a radiator over the rear driving wheel. Speed was controlled by means of a throttle valve lever. No braking system was fitted; the vehicle was stopped by raising and lowering the rear driving wheel using a foot-operated lever; the weight of the machine was then borne by two small castor wheels. The driver was seated between the front wheels. It wasn't, however, a success, as Butler failed to find sufficient financial backing.
Many authorities have excluded steam powered, electric motorcycles or diesel-powered two-wheelers from the definition of a 'motorcycle', and credit the Daimler "Reitwagen" as the world's first motorcycle. Given the rapid rise in use of electric motorcycles worldwide, defining only internal-combustion powered two-wheelers as 'motorcycles' is increasingly problematic.
If a two-wheeled vehicle with steam propulsion is considered a motorcycle, then the first motorcycles built seem to be the French Michaux-Perreaux steam velocipede which patent application was filled in December 1868, constructed around the same time as the American Roper steam velocipede, built by Sylvester H. Roper Roxbury, Massachusetts,
who demonstrated his machine at fairs and circuses in the eastern U.S. in 1867, and built a total of 10 examples.
First motorcycle companies.
In 1894, Hildebrand & Wolfmüller became the first series production motorcycle, and the first to be called a motorcycle (). Excelsior Motor Company, originally a bicycle manufacturing company based in Coventry, England, began production of their first motorcycle model in 1896. The first production motorcycle in the US was the Orient-Aster, built by Charles Metz in 1898 at his factory in Waltham, Massachusetts.
In the early period of motorcycle history, many producers of bicycles adapted their designs to accommodate the new internal combustion engine. As the engines became more powerful and designs outgrew the bicycle origins, the number of motorcycle producers increased. Many of the nineteenth century inventors who worked on early motorcycles often moved on to other inventions. Daimler and Roper, for example, both went on to develop automobiles.
At the turn of the century the first major mass-production firms were set up. In 1898, Triumph Motorcycles in England began producing motorbikes, and by 1903 it was producing over 500 bikes. Other British firms were Royal Enfield, Norton and Birmingham Small Arms Company who began motorbike production in 1899, 1902 and 1910, respectively. Indian began production in 1901 and Harley-Davidson was established two years later. By the outbreak of the First World War, the largest motorcycle manufacturer in the world was Indian,
producing over 20,000 bikes per year.
First World War.
During the First World War, motorbike production was greatly ramped up for the war effort to supply effective communications with front line troops. Messengers on horses were replaced with despatch riders on motorcycles carrying messages, performing reconnaissance and acting as a military police. American company Harley-Davidson was devoting over 50% of its factory output toward military contract by the end of the war. The British company Triumph Motorcycles sold more than 30,000 of its Triumph Type H model to allied forces during the war. With the rear wheel driven by a belt, the Model H was fitted with a air-cooled four-stroke single-cylinder engine. It was also the first Triumph without pedals.
The Model H in particular, is regarded by many as having been the first "modern motorcycle". Introduced in 1915 it had a 550 cc side-valve four-stroke engine with a three-speed gearbox and belt transmission. It was so popular with its users that it was nicknamed the "Trusty Triumph."
Postwar.
By 1920, Harley-Davidson was the largest manufacturer, with their motorcycles being sold by dealers in 67 countries.
By the late 1920s or early 1930s, DKW in Germany took over as the largest manufacturer.
In the 1950s, streamlining began to play an increasing part in the development of racing motorcycles and the "dustbin fairing" held out the possibility of radical changes to motorcycle design. NSU and Moto Guzzi were in the vanguard of this development, both producing very radical designs well ahead of their time.
NSU produced the most advanced design, but after the deaths of four NSU riders in the 1954–1956 seasons, they abandoned further development and quit Grand Prix motorcycle racing.
Moto Guzzi produced competitive race machines, and by 1957 nearly all the Grand Prix races were being won by streamlined machines. The following year, 1958, full enclosure fairings were banned from racing by the FIM in the light of the safety concerns.
From the 1960s through the 1990s, small two-stroke motorcycles were popular worldwide, partly as a result of East German Walter Kaaden's engine work in the 1950s.
Today.
In the 21st century, the motorcycle industry is mainly dominated by the Chinese motorcycle industry and by Japanese motorcycle companies. In addition to the large capacity motorcycles, there is a large market in smaller capacity (less than 300 cc) motorcycles, mostly concentrated in Asian and African countries and produced in China and India. A Japanese example is the 1958 Honda Super Cub, which went on to become the biggest selling vehicle of all time, with its 60 millionth unit produced in April 2008.
Today, this area is dominated by mostly Indian companies with Hero MotoCorp emerging as the world's largest manufacturer of two wheelers. Its Splendor model has sold more than 8.5 million to date. Other major producers are Bajaj and TVS Motors.
Technical aspects.
Construction.
Motorcycle construction is the engineering, manufacturing, and assembly of components and systems for a motorcycle which results in the performance, cost, and aesthetics desired by the designer. With some exceptions, construction of modern mass-produced motorcycles has standardised on a steel or aluminium frame, telescopic forks holding the front wheel, and disc brakes. Some other body parts, designed for either aesthetic or performance reasons may be added. A petrol powered engine typically consisting of between one and four cylinders (and less commonly, up to eight cylinders) coupled to a manual five- or six-speed sequential transmission drives the swingarm-mounted rear wheel by a chain, driveshaft or belt.
Fuel economy.
Motorcycle fuel economy varies greatly with engine displacement and riding style ranging from a low of reported by a Honda VTR1000F rider,
to reported for the Verucci Nitro 50 cc scooter.
A specially designed Matzu Matsuzawa Honda XL125 achieved "on real highways in real conditions."
Due to low engine displacements (100 cc–200 cc), and high power-to-mass ratios, motorcycles offer good fuel economy. Under conditions of fuel scarcity like 1950s Britain and modern developing nations, motorcycles claim large shares of the vehicle market.
Electric motorcycles.
Very high fuel economy equivalents are often derived by electric motorcycles. Electric motorcycles are nearly silent, zero-emission electric motor-driven vehicles. Operating range and top speed are limited by battery technology. Fuel cells and petroleum-electric hybrids are also under development to extend the range and improve performance of the electric drive system.
Reliability.
A 2013 survey of 4,424 readers of the US "Consumer Reports" magazine collected reliability data on 4,680 motorcycles purchased new from 2009 to 2012. The most common problem areas were accessories, brakes, electrical (including starters, charging, ignition), and fuel systems, and the types of motorcycles with the greatest problems were touring, off road/dual sport, sport-touring, and cruisers. There were not enough sport bikes in the survey for a statistically significant conclusion, though the data hinted at reliability as good as cruisers. These results may be partially explained by accessories including such equipment as fairings, luggage, and auxiliary lighting, which are frequently added to touring, adventure touring/dual sport and sport touring bikes. Trouble with fuel systems is often the result of improper winter storage, and brake problems may also be due to poor maintenance. Of the five brands with enough data to draw conclusions, Honda, Kawasaki and Yamaha were statistically tied, with 11 to 14% of those bikes in the survey experiencing major repairs. Harley-Davidsons had a rate of 24%, while BMWs did worst, with 30% of those needing major repairs. There were not enough Triumph and Suzuki motorcycles surveyed for a statistically sound conclusion, though it appeared Suzukis were as reliable as the other three Japanese brands while Triumphs were comparable to Harley-Davidson and BMW. Three fourths of the repairs in the survey cost less than US$200 and two thirds of the motorcycles were repaired in less than two days. In spite of their relatively worse reliability in this survey, Harley-Davidson and BMW owners showed the greatest owner satisfaction, and three fourths of them said they would buy the same bike again, followed by 72% of Honda owners and 60 to 63% of Kawasaki and Yamaha owners.
Dynamics.
Different types of motorcycles have different dynamics and these play a role in how a motorcycle performs in given conditions. For example, one with a longer wheelbase provides the feeling of more stability by responding less to disturbances. Motorcycle tyres have a large influence over handling.
Motorcycles must be leaned in order to make turns. This lean is induced by the method known as countersteering, in which the rider momentarily steers the handlebars in the direction opposite of the desired turn. This practice is counterintuitive and therefore often confusing to novices and even many experienced motorcyclists.
With such short wheelbase, motorcycles can generate enough torque at the rear wheel, and enough stopping force at the front wheel, to lift the opposite wheel off the road. These actions, if performed on purpose, are known as wheelies and stoppies (or endos) respectively.
Accessories.
Various features and accessories may be attached to a motorcycle either as OEM (factory-fitted) or aftermarket. Such accessories are selected by the owner to enhance the motorcycle's appearance, safety, performance, or comfort, and may include anything from mobile electronics to sidecars and trailers.
Safety.
Motorcycles have a higher rate of fatal accidents than automobiles or trucks and buses. United States Department of Transportation data for 2005 from the Fatality Analysis Reporting System show that for passenger cars, 18.62 fatal crashes occur per 100,000 registered vehicles. For motorcycles this figure is higher at 75.19 per 100,000 registered vehicles four times higher than for cars.
The same data shows that 1.56 fatalities occur per 100 million vehicle miles travelled for passenger cars, whereas for motorcycles the figure is 43.47 which is 28 times higher than for cars (37 times more deaths per mile travelled in 2007).
Furthermore, for motorcycles the accident rates have increased significantly since the end of the 1990s, while the rates have dropped for passenger cars.
The two major causes of motorcycle accidents in the United States are: motorists pulling out or turning in front of motorcyclists and violating their rights-of-way, and motorcyclists running wide through turns. The former is sometimes called a , an acronym formed from the motorists' common response of "Sorry mate, I didn't see you". The latter is more commonly caused by operating a motorcycle while intoxicated.
Motorcyclists can anticipate and avoid some of these crashes with proper training, increasing their visibility to other traffic, keeping the speed limits, and not consuming alcohol or other drugs before riding.
The United Kingdom has several organisations dedicated to improving motorcycle safety by providing advanced rider training beyond what is necessary to pass the basic motorcycle licence test. These include the Institute of Advanced Motorists (IAM) and the Royal Society for the Prevention of Accidents (RoSPA). Along with increased personal safety, riders with these advanced qualifications may benefit from reduced insurance costs.
In South Africa, the Think Bike campaign is dedicated to increasing both motorcycle safety and the awareness of motorcycles on the country's roads. The campaign, while strongest in the Gauteng province, has representation in Western Cape, KwaZulu Natal and the Free State. It has dozens of trained marshals available for various events such as cycle races and is deeply involved in numerous other projects such as the annual Motorcycle Toy Run.
Motorcycle safety education is offered throughout the United States by organisations ranging from state agencies to non-profit organisations to corporations. Most states use the courses designed by the Motorcycle Safety Foundation (MSF), while Oregon and Idaho developed their own. All of the training programs include a Basic Rider Course, an Intermediate Rider Course and an Advanced Rider Course.
In Ireland, since 2010, in the UK and some Australian jurisdictions, such as Victoria, New South Wales,
the Australian Capital Territory, Tasmania
and the Northern Territory, it is compulsory to complete a basic rider training course before being issued a Learners Licence, after which they can ride on public roads.
In Canada, motorcycle rider training is compulsory in Quebec and Manitoba only, but all provinces and territories have graduated licence programs which place restrictions on new drivers until they have gained experience. Eligibility for a full motorcycle licence or endorsement for completing a Motorcycle Safety course varies by province. The Canada Safety Council, a non-profit safety organisation, offers the Gearing Up program across Canada and is endorsed by the Motorcycle and Moped Industry Council. Training course graduates may qualify for reduced insurance premiums.
Motorcycle rider postures.
The motorcyclist's riding position depends on rider body-geometry (anthropometry) combined with the geometry of the motorcycle itself. These factors create a set of three basic postures.
Factors of a motorcycle's ergonomic geometry that determine the seating posture include the height, angle and location of footpegs, seat and handlebars. Factors in a rider's physical geometry that contribute to seating posture include torso, arm, thigh and leg length, and overall rider height.
Legal definitions and restrictions.
A motorcycle is broadly defined by law in most countries for the purposes of registration, taxation and rider licensing as a powered two-wheel motor vehicle. Most countries distinguish between mopeds of 49 cc and the more powerful, larger vehicles (scooters do not count as a separate category). Many jurisdictions include some forms of three-wheeled cars as motorcycles.
Environmental impact.
Motorcycles and scooters' low fuel consumption has attracted interest in the United States from environmentalists and those whom increased fuel prices affect.
Piaggio Group Americas supported this interest with the launch of a "Vespanomics" website and platform, claiming lower per-mile carbon emissions of 0.4 lb/mile (113 g/km) less than the average car, a 65% reduction, and better fuel economy.
However, a motorcycle's exhaust emissions may contain 10–20 times more oxides of nitrogen (NOx), carbon monoxide, and unburned hydrocarbons than exhaust from a similar-year passenger car or SUV.
This is because many motorcycles lack a catalytic converter, and the emission standard is much more permissive for motorcycles than for other vehicles. While catalytic converters have been installed in most gasoline-powered cars and trucks since 1975 in the United States, they can present fitment and heat difficulties in motorcycle applications. 
United States Environmental Protection Agency 2007 certification result reports for all vehicles versus on highway motorcycles (which also includes scooters), the average certified emissions level for 12,327 vehicles tested was 0.734. The average "Nox+Co End-Of-Useful-Life-Emissions" for 3,863 motorcycles tested was 0.8531. 54% of the tested 2007-model motorcycles were equipped with a catalytic converter.
United States emissions limits.
The following table shows maximum acceptable legal emissions of the combination of hydrocarbons, oxides of nitrogen, and carbon monoxide for new motorcycles sold in the United States with 280 cc or greater piston displacement.
The maximum acceptable legal emissions of hydrocarbon and carbon monoxide for new Class I and II motorcycles (50 cc–169 cc and 170 cc–279 cc respectively) sold in the United States are as follows:
Europe.
European emission standards for motorcycles are similar to those for cars. New motorcycles must meet Euro III standards,
while cars must meet Euro V standards. Motorcycle emission controls are being updated and it has been proposed to update to Euro IV in 2012 and Euro V in 2015.

</doc>
<doc id="19877" url="https://en.wikipedia.org/wiki?curid=19877" title="Map">
Map

A map is a symbolic depiction highlighting relationships between elements of some space, such as objects, regions, and themes.
Many maps are static two-dimensional, geometrically accurate (or approximately accurate) representations of three-dimensional space, while others are dynamic or interactive, even three-dimensional. Although most commonly used to depict geography, maps may represent any space, real or imagined, without regard to context or scale; e.g. brain mapping, DNA mapping and extraterrestrial mapping.
Although the earliest maps known are of the heavens, geographic maps of territory have a very long tradition and exist from ancient times. The word "map" comes from the medieval Latin "Mappa mundi", wherein "mappa" meant napkin or cloth and "mundi" the world. Thus, "map" became the shortened term referring to a two-dimensional representation of the surface of the world.
Geographic maps.
Cartography or "map-making" is the study and practice of crafting representations of the Earth upon a flat surface (see History of cartography), and one who makes maps is called a cartographer.
Road maps are perhaps the most widely used maps today, and form a subset of navigational maps, which also include aeronautical and nautical charts, railroad network maps, and hiking and bicycling maps. In terms of quantity, the largest number of drawn map sheets is probably made up by local surveys, carried out by municipalities, utilities, tax assessors, emergency services providers, and other local agencies. Many national surveying projects have been carried out by the military, such as the British Ordnance Survey: a civilian government agency, internationally renowned for its comprehensively detailed work.
In addition to location information maps may also be used to portray contour lines indicating constant values of elevation, temperature, rainfall, etc.
Orientation of maps.
The orientation of a map is the relationship between the directions on the map and the corresponding compass directions in reality. The word "orient" is derived from Latin "oriens", meaning East. In the Middle Ages many maps, including the T and O maps, were drawn with East at the top (meaning that the direction "up" on the map corresponds to East on the compass). Today, the most common – but far from universal – cartographic convention is that North is at the top of a map. Several kinds of maps are often traditionally not oriented with North at the top:
Scale and accuracy.
Many, but not all, maps are drawn to a scale, expressed as a ratio such as 1:10,000, meaning that 1 of any unit of measurement on the map corresponds exactly, to 10,000 of that same unit on the ground. The scale statement may be taken as exact when the region mapped is small enough for the curvature of the Earth to be neglected, for example in a town planner's city map. Over larger regions where the curvature cannot be ignored we must use map projections from the curved surface of the Earth (sphere or ellipsoid) to the plane. The impossibility of flattening the sphere to the plane implies that no map projection can have constant scale: on most projections the best we can achieve is accurate scale on one or two lines (not necessarily straight) on the projection. Thus for map projections we must introduce the concept of point scale, which is a function of position, and strive to keep its variation within narrow bounds. Although the scale statement is nominal it is usually accurate enough for all but the most precise of measurements.
Large scale maps, say 1:10,000, cover relatively small regions in great detail and small scale maps, say 1:10,000,000, cover large regions such as nations, continents and the whole globe. The large/small terminology arose from the practice of writing scales as numerical fractions: 1/10,000 is larger than 1/10,000,000. There is no exact dividing line between large and small but 1/100,000 might well be considered as a medium scale. Examples of large scale maps are the 1:25,000 maps produced for hikers; on the other hand maps intended for motorists at 1:250,000 or 1:1,000,000 are small scale.
It is important to recognize that even the most accurate maps sacrifice a certain amount of accuracy in scale to deliver a greater visual usefulness to its user. For example, the width of roads and small streams are exaggerated when they are too narrow to be shown on the map at true scale; that is, on a printed map they would be narrower than could be perceived by the naked eye. The same applies to computer maps where the smallest unit is the pixel. A narrow stream say must be shown to have the width of a pixel even if at the map scale it would be a small fraction of the pixel width.
Some maps, called cartograms, have the scale deliberately distorted to reflect information other than land area or distance. For example, this map (at the right) of Europe has been distorted to show population distribution, while the rough shape of the continent is still discernible.
Another example of distorted scale is the famous London Underground map. The basic geographical structure is respected but the tube lines (and the River Thames) are smoothed to clarify the relationships between stations. Near the center of the map stations are spaced out more than near the edges of map.
Further inaccuracies may be deliberate. For example, cartographers may simply omit military installations or remove features solely in order to enhance the clarity of the map. For example, a road map may not show railroads, smaller waterways or other prominent non-road objects, and even if it does, it may show them less clearly (e.g. dashed or dotted lines/outlines) than the main roads. Known as decluttering, the practice makes the subject matter that the user is interested in easier to read, usually without sacrificing overall accuracy. Software-based maps often allow the user to toggle decluttering between ON, OFF and AUTO as needed. In AUTO the degree of decluttering is adjusted as the user changes the scale being displayed.
Map types and projections.
Maps of the world or large areas are often either 'political' or 'physical'. The most important purpose of the political map is to show territorial borders; the purpose of the physical is to show features of geography such as mountains, soil type or land use including infrastructure such as roads, railroads and buildings. Topographic maps show elevations and relief with contour lines or shading. Geological maps show not only the physical surface, but characteristics of the underlying rock, fault lines, and subsurface structures.
Maps that depict the surface of the Earth also use a projection, a way of translating the three-dimensional real surface of the geoid to a two-dimensional picture. Perhaps the best-known world-map projection is the Mercator projection, originally designed as a form of nautical chart.
Aeroplane pilots use aeronautical charts based on a Lambert conformal conic projection, in which a cone is laid over the section of the earth to be mapped. The cone intersects the sphere (the earth) at one or two parallels which are chosen as standard lines. This allows the pilots to plot a great-circle route approximation on a flat, two-dimensional chart.
Electronic maps.
From the last quarter of the 20th century, the indispensable tool of the cartographer has been the computer. Much of cartography, especially at the data-gathering survey level, has been subsumed by Geographic Information Systems (GIS). The functionality of maps has been greatly advanced by technology simplifying the superimposition of spatially located variables onto existing geographical maps. Having local information such as rainfall level, distribution of wildlife, or demographic data integrated within the map allows more efficient analysis and better decision making. In the pre-electronic age such superimposition of data led Dr. John Snow to identify the location of an outbreak of cholera. Today, it is used by agencies of the human kind, as diverse as wildlife conservationists and militaries around the world.
Even when GIS is not involved, most cartographers now use a variety of computer graphics programs to generate new maps.
Interactive, computerised maps are commercially available, allowing users to "zoom in" or "zoom out" (respectively meaning to increase or decrease the scale), sometimes by replacing one map with another of different scale, centered where possible on the same point. In-car global navigation satellite systems are computerised maps with route-planning and advice facilities which monitor the user's position with the help of satellites. From the computer scientist's point of view, zooming in entails one or a combination of:
For example:
"See also: Webpage (Graphics), PDF (Layers), MapQuest, Google Maps, Google Earth, OpenStreetMap or Yahoo! Maps."
Climatic maps.
The maps that reflect the territorial distribution of climatic conditions based on the results of long-term observations. Climatic maps can be compiled both for individual climatic features (temperature, precipitation, humidity) and for combinations of them at the earth’s surface and in the upper layers of the atmosphere. Climatic maps afford a very convenient overview of the climatic features in a large region and permit values of climatic features to be compared in different parts of the region. Through interpolation the maps can be used to determine the values of climatic features in any particular spot.
Climatic maps generally apply to individual months and to the year as a whole, sometimes to the four seasons, to the growing period, and so forth. On maps compiled from the observations of ground meteorological stations, atmospheric pressure is converted to sea level. Air temperature maps are compiled both from the actual values observed on the surface of the earth and from values converted to sea level. The pressure field in free atmosphere is represented either by maps of the distribution of pressure at different standard altitudes—for example, at every kilometer above sea level—or by maps of baric topography on which altitudes (more precisely geopotentials) of the main isobaric surfaces (for example, 900, 800, and 700 millibars) counted off from sea level are plotted. The temperature, humidity, and wind on aeroclimatic maps may apply either to standard altitudes or to the main isobaric surfaces.
Isolines are drawn on maps of such climatic features as the long-term mean values (of atmospheric pressure, temperature, humidity, total precipitation, and so forth) to connect points with equal values of the feature in question—for example, isobars for pressure, isotherms for temperature, and isohyets for precipitation. Isoamplitudes are drawn on maps of amplitudes (for example, annual amplitudes of air temperature—that is, the differences between the mean temperatures of the warmest and coldest month). Isanomals are drawn on maps of anomalies (for example, deviations of the mean temperature of each place from the mean temperature of the entire latitudinal zone). Isolines of frequency are drawn on maps showing the frequency of a particular phenomenon (for example, annual number of days with a thunderstorm or snow cover). Isochrones are drawn on maps showing the dates of onset of a given phenomenon (for example, the first frost and appearance or disappearance of the snow cover) or the date of a particular value of a meteorological element in the course of a year (for example, passing of the mean daily air temperature through zero). Isolines of the mean numerical value of wind velocity or isotachs are drawn on wind maps (charts); the wind resultants and directions of prevailing winds are indicated by arrows of different length or arrows with different plumes; lines of flow are often drawn. Maps of the zonal and meridional components of wind are frequently compiled for the free atmosphere. Atmospheric pressure and wind are usually combined on climatic maps. Wind roses, curves showing the distribution of other meteorological elements, diagrams of the annual course of elements at individual stations, and the like are also plotted on climatic maps.
Maps of climatic regionalization, that is, division of the earth’s surface into climatic zones and regions according to some classification of climates, are a special kind of climatic map.
Climatic maps are often incorporated into climatic atlases of varying geographic range (globe, hemispheres, continents, countries, oceans) or included in comprehensive atlases. Besides general climatic maps, applied climatic maps and atlases have great practical value. Aeroclimatic maps, aeroclimatic atlases, and agroclimatic maps are the most numerous.
Conventional signs.
The various features shown on a map are represented by conventional signs or symbols. For example, colors can be used to indicate a classification of roads. Those signs are usually explained in the margin of the map, or on a separately published characteristic sheet.
Some cartographers prefer to make the map cover practically the entire screen or sheet of paper, leaving no room "outside" the map for information about the map as a whole.
These cartographers typically place such information in an otherwise "blank" region "inside" the map -- cartouche, map legend, title, compass rose, bar scale, etc.
In particular, some maps contain smaller "sub-maps" in otherwise blank regions—often one at a much smaller scale showing the whole globe and where the whole map fits on that globe, and a few showing "regions of interest" at a larger scale in order to show details that wouldn't otherwise fit.
Occasionally sub-maps use the same scale as the large map—a few maps of the contiguous United States include a sub-map to the same scale for each of the two non-contiguous states.
Labeling.
To communicate spatial information effectively, features such as rivers, lakes, and cities need to be labeled. Over centuries cartographers have developed the art of placing names on even the densest of maps. Text placement or name placement can get mathematically very complex as the number of labels and map density increases. Therefore, text placement is time-consuming and labor-intensive, so cartographers and GIS users have developed automatic label placement to ease this process.
Non-geographical spatial maps.
Maps exist of the Solar System, and other cosmological features such as star maps. In addition maps of other bodies such as the Moon and other planets are technically not "geo"graphical maps.
Non spatial maps.
Diagrams such as schematic diagrams and Gantt charts and treemaps display logical relationships between items, and do not display spatial relationships at all.
Some maps, for example the London Underground map, are topological maps. Topological in nature, the distances are completely unimportant; only the connectivity is significant.
General-purpose maps.
General-purpose maps provide many types of information on one map. Most atlas maps, wall maps, and road maps fall into this category. The following are some features that might be shown on a general-purpose maps: bodies of water, roads, railway lines, parks, elevations, towns and cities, political boundaries, latitude and longitude, national and provincial parks. These maps give a broad understanding of location and features of an area. The reader may gain an understanding of the type of landscape, the location of urban places, and the location of major transportation routes all at once.
Legal regulation.
Some countries required that all published maps represent their national claims regarding border disputes. For example:
In 2010, the People's Republic of China began requiring that all online maps be served from within China, to enforce compliance with its laws.

</doc>
<doc id="19881" url="https://en.wikipedia.org/wiki?curid=19881" title="Management">
Management

Management in businesses and organizations is the function that coordinates the efforts of people to accomplish goals and objectives by using available resources efficiently and effectively.
Management includes planning, organizing, staffing, leading or directing, and controlling an organization to accomplish the goal or target. Resourcing encompasses the deployment and manipulation of human resources, financial resources, technological resources, and natural resources. Management is also an academic discipline, a social science whose objective is to study social organization.
Etymology.
The English verb "manage" comes from the Italian "maneggiare" (to handle, especially tools or a horse), which derives from the two Latin words "manus" (hand) and "agere" (to act).
The French word for housekeeping, "ménagerie", derived from "ménager" ("to keep house"; compare "ménage" for "household"), also encompasses taking care of domestic animals.
The French word "mesnagement" (or "ménagement") influenced the semantic development of the English word "management" in the 17th and 18th centuries.
Note that "Ménagerie" is the French translation of Xenophon's famous book "Oeconomicus"
() on household matters and husbandry.
While the Italian word "maneggiare" refers to subaltern responsibilities, the modern Italian language would characterize the work of an executive as "gestire".
Definitions.
Views on the definition and scope of management include:
Theoretical scope.
Management involves identifying the mission, objective, procedures, rules and manipulation
of the human capital of an enterprise to contribute to the success of the enterprise. This implies effective communication: an enterprise environment (as opposed to a physical or mechanical mechanism) implies human motivation and implies some sort of successful progress or system outcome. As such, management is not the manipulation of a mechanism (machine or automated program), not the herding of animals, and can occur either in a legal or in an illegal enterprise or environment. Management does not need to be seen from enterprise point of view alone, because management is an essential function to improve one's life and relationships. Management is therefore everywhere and it has a wider range of application. Based on this, management must have humans, communication, and a positive enterprise endeavor. Plans, measurements, motivational psychological tools, goals, and economic measures (profit, etc.) may or may not be necessary components for there to be management. At first, one views management functionally, such as measuring quantity, adjusting plans, meeting goals. This applies even in situations where planning does not take place. From this perspective, Henri Fayol (1841–1925) 
considers management to consist of six functions:
In another way of thinking, Mary Parker Follett (1868–1933), allegedly defined management as "the art of getting things done through people".
She described management as philosophy.
Critics, however, find this definition useful but far too narrow. The phrase "management is what managers do" occurs widely,
suggesting the difficulty of defining management without circularity, the shifting nature of definitions and the connection of managerial practices with the existence of a managerial cadre or of a class.
One habit of thought regards management as equivalent to "business administration" and thus excludes management in places outside commerce, as for example in charities and in the public sector. More broadly, every organization must "manage" its work, people, processes, technology, etc. to maximize effectiveness. Nonetheless, many people refer to university departments that teach management as "business schools". Some such institutions (such as the Harvard Business School) use that name, while others (such as the Yale School of Management) employ the broader term "management".
English-speakers may also use the term "management" or "the management" as a collective word describing the managers of an organization, for example of a corporation.
Historically this use of the term often contrasted with the term "labor" - referring to those being managed.
But in the present era the concept of management is identified in the wide areas and its frontiers have been pushed to a broader range. Apart from profitable organizations even non-profitable organizations (NGOs) apply management concepts. The concept and its uses are not constrained. Management on the whole is the process of planning, organizing, staffing, leading and controlling.
Nature of work.
In profitable organizations, management's primary function is the satisfaction of a range of stakeholders. This typically involves making a profit (for the shareholders), creating valued products at a reasonable cost (for customers), and providing great employment opportunities for employees. In nonprofit management, add the importance of keeping the faith of donors. In most models of management and governance, shareholders vote for the board of directors, and the board then hires senior management. Some organizations have experimented with other methods (such as employee-voting models) of selecting or reviewing managers, but this is rare.
In the public sector of countries constituted as representative democracies, voters elect politicians to public office. Such politicians hire many managers and administrators, and in some countries like the United States political appointees lose their jobs on the election of a new president/governor/mayor.
Historical development.
Some see management (by definition) as late-modern (in the sense of late modernity) conceptualization. On those terms it cannot have a pre-modern history, only harbingers (such as stewards). Others, however, detect management-like-thought back to Sumerian traders and to the builders of the pyramids of ancient Egypt. Slave-owners through the centuries faced the problems of exploiting/motivating a dependent but sometimes unenthusiastic or recalcitrant workforce, but many pre-industrial enterprises, given their small scale, did not feel compelled to face the issues of management systematically. However, innovations such as the spread of Hindu numerals (5th to 15th centuries) and the codification of double-entry book-keeping (1494) provided tools for management assessment, planning and control.
With the changing workplaces of industrial revolutions in the 18th and 19th centuries, military theory and practice contributed approaches to managing the newly-popular factories.
Given the scale of most commercial operations and the lack of mechanized record-keeping and recording before the industrial revolution, it made sense for most owners of enterprises in those times to carry out management functions by and for themselves. But with growing size and complexity of organizations, the split between owners (individuals, industrial dynasties or groups of shareholders) and day-to-day managers (independent specialists in planning and control) gradually became more common.
Early writing.
While management (according to some definitions) has existed for millennia, several writers have created a background of works that assisted in modern management theories.
Some ancient military texts have been cited for lessons that civilian managers can gather. For example, Chinese general Sun Tzu in the 6th century BC, "The Art of War", recommends being aware of and acting on strengths and weaknesses of both a manager's organization and a foe's.
Various ancient and medieval civilizations have produced "mirrors for princes" books, which aim to advise new monarchs on how to govern. Plato described job specialization in 350 B.C., and Alfarabi listed several leadership traits in A.D. 900. Other examples include the Indian Arthashastra by Chanakya (written around 300 BCE), and "The Prince" by Italian author 
Niccolò Machiavelli (c. 1515).
Written in 1776 by Adam Smith, a Scottish moral philosopher, "The Wealth of Nations" discussed efficient organization of work through division of labour.
Smith described how changes in processes could boost productivity in the manufacture of pins. While individuals could produce 200 pins per day, Smith analyzed the steps involved in manufacture and, with 10 specialists, enabled production of 48,000 pins per day.
19th century.
Classical economists such as Adam Smith (1723–1790) and John Stuart Mill (1806–1873) provided a theoretical background to resource-allocation, production, and pricing issues. About the same time, innovators like Eli Whitney (1765–1825), James Watt (1736–1819), and Matthew Boulton (1728–1809) developed elements of technical production such as standardization, quality-control procedures, cost-accounting, interchangeability of parts, and work-planning. Many of these aspects of management existed in the pre-1861 slave-based sector of the US economy. That environment saw 4 million people, as the contemporary usages had it, "managed" in profitable quasi-mass production.
Salaried managers as an identifiable group first became prominent in the late 19th century.
20th century.
By about 1900 one finds managers trying to place their theories on what they regarded as a thoroughly scientific basis (see scientism for perceived limitations of this belief). Examples include Henry R. Towne's "Science of management" in the 1890s, Frederick Winslow Taylor's "The Principles of Scientific Management" (1911), Lillian Gilbreth's "Psychology of Management" (1914), Frank and Lillian Gilbreth's "Applied motion study" (1917), and Henry L. Gantt's charts (1910s). J. Duncan wrote the first college management-textbook in 1911. In 1912 Yoichi Ueno introduced Taylorism to Japan and became the first management consultant of the "Japanese-management style". His son Ichiro Ueno pioneered Japanese quality assurance.
The first comprehensive theories of management appeared around 1920. The Harvard Business School offered the first Master of Business Administration degree (MBA) in 1921. People like Henri Fayol (1841–1925) and Alexander Church described the various branches of management and their inter-relationships. In the early 20th century, people like Ordway Tead (1891–1973), Walter Scott and J. Mooney applied the principles of psychology to management. Other writers, such as Elton Mayo (1880–1949), Mary Parker Follett (1868–1933), Chester Barnard (1886–1961), Max Weber (1864–1920), who saw what he called the "administrator" as bureaucrat), Rensis Likert (1903–1981), and Chris Argyris (* 1923) approached the phenomenon of management from a sociological perspective.
Peter Drucker (1909–2005) wrote one of the earliest books on applied management: "Concept of the Corporation" (published in 1946). It resulted from Alfred Sloan (chairman of General Motors until 1956) commissioning a study of the organisation. Drucker went on to write 39 books, many in the same vein.
H. Dodge, Ronald Fisher (1890–1962), and Thornton C. Fry introduced statistical techniques into management-studies. In the 1940s, Patrick Blackett worked in the development of the applied-mathematics science of operations research, initially for military operations. Operations research, sometimes known as "management science" (but distinct from Taylor's scientific management), attempts to take a scientific approach to solving decision-problems, and can apply directly to multiple management problems, particularly in the areas of logistics and operations.
Some of the more developments include the Theory of Constraints, management by objectives, reengineering, Six Sigma and various information-technology-driven theories such as agile software development, as well as group-management theories such as Cog's Ladder.
As the general recognition of managers as a class solidified during the 20th century and gave perceived practitioners of the art/science of management a certain amount of prestige, so the way opened for popularised systems of management ideas to peddle their wares. In this context many management fads may have had more to do with pop psychology than with scientific theories of management.
Towards the end of the 20th century, business management came to consist of six separate branches, namely:
21st century.
In the 21st century observers find it increasingly difficult to subdivide management into functional categories in this way. More and more processes simultaneously involve several categories. Instead, one tends to think in terms of the various processes, tasks, and objects subject to management.
Branches of management theory also exist relating to nonprofits and to government: such as public administration, public management, and educational management. Further, management programs related to civil-society organizations have also spawned programs in nonprofit management and social entrepreneurship.
Note that many of the assumptions made by management have come under attack from business-ethics viewpoints, critical management studies, and anti-corporate activism.
As one consequence, workplace democracy (sometimes referred to as Workers' self-management) has become both more common and advocated to a greater extent, in some places distributing all management functions among workers, each of whom takes on a portion of the work. However, these models predate any current political issue, and may occur more naturally than does a command hierarchy. All management embraces to some degree a democratic principle—in that in the long term, the majority of workers must support management. Otherwise, they leave to find other work or go on strike. Despite the move toward workplace democracy, command-and-control organization structures remain commonplace as "de facto" organization structure. Indeed, the entrenched nature of command-and-control is evident in the way that recent layoffs have been conducted with management ranks affected far less than employees at the lower levels. In some cases, management has even rewarded itself with bonuses after laying off lower-level workers.
According to leadership academic Manfred F.R. Kets de Vries, a contemporary senior management team will almost inevitably have some personality disorders.
Topics.
Basics.
Management operates through five basic functions: planning, organizing, coordinating, commanding, and controlling.
Skills.
Management skills include:
Levels.
Most organizations have three management levels: first-level, middle-level, and top-level managers. First-line managers are the lowest level of management and manage the work of nonmanagerial individuals who are directly involved with the production or creation of the organization's products. First-line managers are often called supervisors, but may also be called line managers, office managers, or even foremen. Middle managers include all levels of management between the first-line level and the top level of the organization. These managers manage the work of first-line managers and may have titles such as department head, project leader, plant manager, or division manager. Top managers are responsible for making organization-wide decisions and establishing the plans and goals that affect the entire organization. These individuals typically have titles such as executive vice president, president, managing director, chief operating officer, chief executive officer, or chairman of the board.
Top.
The top consists of the board of directors (including non-executive directors and executive directors), president, vice-president, CEOs and other members of the C-level executives. They are responsible for controlling and overseeing the entire organization. They set a tone at the top and develop strategic plans, company policies, and make decisions on the direction of the business. In addition, top-level managers play a significant role in the mobilization of outside resources and are accountable to the shareholders and general public.
The board of directors is typically primarily composed of non-executives which owe a fiduciary duty to shareholders and are not closely involved in the day-to-day activities of the organization, although this varies depending on the type (e.g., public versus private), size and culture of the organization. These directors are theoretically liable for breaches of that duty and typically insured under directors and officers liability insurance. Fortune 500 directors are estimated to spend 4.4 hours per week on board duties, and median compensation was $212,512 in 2010. The board sets corporate strategy, makes major decisions such as major acquisitions, and hires, evaluates, and fires the top-level manager (Chief Executive Officer or CEO) and the CEO typically hires other positions. However, board involvement in the hiring of other positions such as the Chief Financial Officer (CFO) has increased. In 2013, a survey of over 160 CEOs and directors of public and private companies found that the top weaknesses of CEOs were "mentoring skills" and "board engagement", and 10% of companies never evaluated the CEO. The board may also have certain employees (e.g., internal auditors) report to them or directly hire independent contractors; for example, the board (through the audit committee) typically selects the auditor.
Helpful skills of top management vary by the type of organization but typically include a broad understanding of competition, world economies, and politics. In addition, the CEO is responsible for implementing and determining (within the board's framework) the broad policies of the organization. Executive management accomplishes the day-to-day details, including: instructions for preparation of department budgets, procedures, schedules; appointment of middle level executives such as department managers; coordination of departments; media and governmental relations; and shareholder communication.
Middle.
Consist of general managers, branch managers and department managers. They are accountable to the top management for their department's function. They devote more time to organizational and directional functions. Their roles can be emphasized as executing organizational plans in conformance with the company's policies and the objectives of the top management, they define and discuss information and policies from top management to lower management, and most importantly they inspire and provide guidance to lower level managers towards better performance.
Middle management is the midway management of a categorized organization, being secondary to the senior management but above the deepest levels of operational members. An operational manager may be well-thought-out by middle management, or may be categorized as non-management operate, liable to the policy of the specific organization. Efficiency of the middle level is vital in any organization, since they bridge the gap between top level and bottom level staffs.
Their functions include:
Lower.
Consist of supervisors, section leaders, foremen, etc. They focus on controlling and directing. They usually have the responsibility of assigning employees tasks, guiding and supervising employees on day-to-day activities, ensuring quality and quantity production, making recommendations, suggestions, and up channeling employee problems, etc. First-level managers are role models for employees that provide:
Training.
Universities around the world offer bachelor's and advanced degrees, diplomas and certificates in management, generally within their colleges of business and business schools but also in other related departments. There is also an increase in online management education and training in the form of electronic educational technology ( also called e-learning).
United States.
At the graduate level students may choose to specialize in major subareas of management such as entrepreneurship, human resources, international business, organizational behavior, organizational theory, strategic management, accounting, corporate finance, entertainment, global management, healthcare management, investment management, sustainability and real estate. A Master of Business Administration (MBA) is the most popular professional master's degree and can be obtained from many universities in the United States. MBAs provide further education in management and leadership for graduate students, but management doctorates are the most advanced terminal degrees.
Current best practices.
While management trends can change rapidly, the long term trend in management has been defined by a market embracing diversity and a rising service industry. Managers are currently being trained to encourage greater equality for minorities and women in the workplace, by offering increased flexibility in working hours, better retraining, and innovative (and usually industry-specific) performance markers. Managers destined for the service sector are being trained to use unique measurement techniques, better worker support and more charismatic leadership styles. Human resources finds itself increasingly working with management in a training capacity to help collect management data on the success (or failure) of management actions with employees.

</doc>
<doc id="19883" url="https://en.wikipedia.org/wiki?curid=19883" title="Mineralogy">
Mineralogy

Mineralogy is a subject of geology specializing in the scientific study of chemistry, crystal structure, and physical (including optical) properties of minerals. Specific studies within mineralogy include the processes of mineral origin and formation, classification of minerals, their geographical distribution, as well as their utilization.
History.
Early writing on mineralogy, especially on gemstones, comes from ancient Babylonia, the ancient Greco-Roman world, ancient and medieval China, and Sanskrit texts from ancient India and the ancient Islamic World. Books on the subject included the "Naturalis Historia" of Pliny the Elder, which not only described many different minerals but also explained many of their properties, and Kitab al Jawahir (Book of Precious Stones) by Persian scientist Al Biruni. The German Renaissance specialist Georgius Agricola wrote works such as "De re metallica" ("On Metals", 1556) and "De Natura Fossilium" ("On the Nature of Rocks", 1546) which began the scientific approach to the subject. Systematic scientific studies of minerals and rocks developed in post-Renaissance Europe. The modern study of mineralogy was founded on the principles of crystallography (the origins of geometric crystallography, itself, can be traced back to the mineralogy practiced in the eighteenth and nineteenth centuries) and to the microscopic study of rock sections with the invention of the microscope in the 17th century.
Nicholas Steno first observed the law of constancy of interfacial angles (also known as the first law of crystallography) in quartz crystals in 1669. This was later generalized and established experimentally by Jean-Baptiste L. Romé de l'Islee in 1783. René Just Haüy, the "father of modern crystallography", showed that crystals are periodic and established that the orientations of crystal faces can be expressed in terms of rational numbers, as later encoded in the Miller indices. In 1814, Jöns Jacob Berzelius introduced a classification of minerals based on their chemistry rather than their crystal structure. William Nicol developed the Nicol prism, which polarizes light, in 1827–1828 while studying fossilized wood; Henry Clifton Sorby showed that thin sections of minerals could be identified by their optical properties using a polarizing microscope. James D. Dana published his first edition of "A System of Mineralogy" in 1837, and in a later edition introduced a chemical classification that is still the standard. X-ray diffraction was demonstrated by Max von Laue in 1912, and developed into a tool for analyzing the crystal structure of minerals by the father/son team of William Henry Bragg and William Lawrence Bragg.
More recently, driven by advances in experimental technique (such as neutron diffraction) and available computational power, the latter of which has enabled extremely accurate atomic-scale simulations of the behaviour of crystals, the science has branched out to consider more general problems in the fields of inorganic chemistry and solid-state physics. It, however, retains a focus on the crystal structures commonly encountered in rock-forming minerals (such as the perovskites, clay minerals and framework silicates). In particular, the field has made great advances in the understanding of the relationship between the atomic-scale structure of minerals and their function; in nature, prominent examples would be accurate measurement and prediction of the elastic properties of minerals, which has led to new insight into seismological behaviour of rocks and depth-related discontinuities in seismograms of the Earth's mantle. To this end, in their focus on the connection between atomic-scale phenomena and macroscopic properties, the "mineral sciences" (as they are now commonly known) display perhaps more of an overlap with materials science than any other discipline.
Physical properties.
An initial step in identifying a mineral is to examine its physical properties, many of which can be measured on a hand sample. These can be classified into density (often given as specific gravity); measures of mechanical cohesion (hardness, tenacity, cleavage, fracture, parting); macroscopic visual properties (luster, color, streak, luminescence, diaphaneity); magnetic and electric properties; radioactivity and solubility in hydrogen chloride ().
If the mineral is well crystallized, it will also have a distinctive crystal habit (for example, hexagonal, columnar, botryoidal) that reflects the crystal structure or internal arrangement of atoms. It is also affected by crystal defects and twinning. Many crystals are polymorphic, having more than one possible crystal structure depending on factors such as pressure and temperature.
Crystal structure.
The crystal structure is the arrangement of atoms in a crystal. It is represented by a lattice of points which repeats a basic pattern, called a unit cell, in three dimensions. The lattice can be characterized by its symmetries and by the dimensions of the unit cell. These dimensions are represented by three "Miller indices". The lattice remains unchanged by certain symmetry operations about any given point in the lattice: reflection, rotation, inversion, and rotary inversion, a combination of rotation and reflection. Together, they make up a mathematical object called a "crystallographic point group" or "crystal class". There are 32 possible crystal classes. In addition, there are operations that displace all the points: translation, screw axis, and glide plane. In combination with the point symmetries, they form 230 possible space groups.
Most geology departments have X-ray powder diffraction equipment to analyze the crystal structures of minerals. X-rays have wavelengths that are the same order of magnitude as the distances between atoms. Diffraction, the constructive and destructive interference between waves scattered at different atoms, leads to distinctive patterns of high and low intensity that depend on the geometry of the crystal. In a sample that is ground to a powder, the X-rays sample a random distribution of all crystal orientations. Powder diffraction can distinguish between minerals that may appear the same in a hand sample, for example quartz and its polymorphs tridymite and cristobalite.
isomorphous minerals of different compositions have similar powder diffraction patterns, the main difference being in spacing and intensity of lines. For example, the (halite) crystal structure is space group "Fm3m"; this structure is shared by sylvite (), periclase (), bunsenite (), galena (), alabandite (), chlorargyrite (), and osbornite ().
Chemical.
A few minerals are chemical elements, including sulfur, copper, silver, and gold, but the vast majority are compounds. Before about 1947, the main method for identifying composition was "wet chemical analysis", which involved dissolving a mineral in an acid such as hydrochloric acid (). The elements in solution were then identified using colorimetry, volumetric analysis or gravimetric analysis. A variation on the wet methods is 
atomic absorption spectroscopy, which also requires the dissolution of the sample but is much faster and cheaper than the above methods. The solution is vaporized and its absorption spectrum is measured in the visible and ultraviolet range. Other techniques are X-ray fluorescence, electron microprobe analysis and optical emission spectrography.
Optical.
In addition to macroscopic properties such as color or lustre, minerals have properties that require a polarizing microscope to observe.
Transmitted light.
When light passes from air or a vacuum into a transparent crystal, some of it is reflected at the surface and some refracted. The latter is a bending of the light path that occurs because the speed of light changes as it goes into the crystal; Snell's law relates the bending angle to the Refractive index, the ratio of speed in a vacuum to speed in the crystal. Crystals whose point symmetry group falls in the cubic system are "isotropic": the index does not depend on direction. All other crystals are "anisotropic": light passing through them is broken up into two plane polarized rays that travel at different speeds and refract at different angles.
A polarizing microscope is similar to an ordinary microscope, but it has two plane-polarized filters, a ("polarizer") below the sample and an analyzer above it, polarized perpendicular to each other. Light passes successively through the polarizer, the sample and the analyzer. If there is no sample, the analyzer blocks all the light from the polarizer. However, an anisotropic sample will generally change the polarization so some of the light can pass through. Thin sections and powders can be used as samples.
When an isotropic crystal is viewed, it appears dark because it does not change the polarization of the light. However, when it is immersed in a calibrated liquid with a lower index of refraction and the microscope is thrown out of focus, a bright line called a "Becke line" appears around the perimeter of the crystal. By observing the presence or absence of such lines in liquids with different indices, the index of the crystal can be estimated, usually to within .
Systematic.
Systematic mineralogy is the identification and classification of minerals by their properties. Historically, mineralogy was heavily concerned with taxonomy of the rock-forming minerals. In 1959, the International Mineralogical Association formed the Commission of New Minerals and Mineral Names to rationalize the nomenclature and regulate the introduction of new names. In July 2006, it was merged with the Commission on Classification of Minerals to form the Commission on New Minerals, Nomenclature, and Classification. There are over 6,000 named and unnamed minerals, and about 100 are discovered each year. The "Manual of Mineralogy" places minerals in the following classes: native elements, sulfides, sulfosalts, oxides and hydroxides, halides, carbonates, nitrates and borates, sulfates, chromates, molybdates and tungstates, phosphates, arsenates and vanadates, and silicates.
Formation environments.
The environments of mineral formation and growth are highly varied, ranging from slow crystallization at the high temperatures and pressures of igneous melts deep within the Earth's crust to the low temperature precipitation from a saline brine at the Earth's surface.
Various possible methods of formation include:
Biomineralogy.
Biomineralogy is a cross-over field between mineralogy, paleontology and biology. It is the study of how plants and animals stabilize minerals under biological control, and the sequencing of mineral replacement of those minerals after deposition. It uses techniques from chemical mineralogy, especially isotopic studies, to determine such things as growth forms in living plants and animals as well as things like the original mineral content of fossils.
A new approach to mineralogy called "mineral evolution" explores the co-evolution of the geosphere and biosphere, including the role of minerals in the origin of life and processes as mineral-catalyzed organic synthesis and the selective adsorption of organic molecules on mineral surfaces.
Uses.
Minerals are essential to various needs within human society, such as minerals used as ores for essential components of metal products used in various commodities and machinery, essential components to building materials such as limestone, marble, granite, gravel, glass, plaster, cement, etc. Minerals are also used in fertilizers to enrich the growth of agricultural crops.
Collecting.
Mineral collecting is also a recreational study and collection hobby, with clubs and societies representing the field. Museums, such as the Smithsonian National Museum of Natural History Hall of Geology, Gems, and Minerals, the Natural History Museum of Los Angeles County, the Natural History Museum, London, and the private Mim Mineral Museum in Beirut, Lebanon, have popular collections of mineral specimens on permanent display.

</doc>
<doc id="19886" url="https://en.wikipedia.org/wiki?curid=19886" title="Maple syrup">
Maple syrup

Maple syrup is a syrup usually made from the xylem sap of sugar maple, red maple, or black maple trees, although it can also be made from other maple species. In cold climates, these trees store starch in their trunks and roots before the winter; the starch is then converted to sugar that rises in the sap in late winter and early spring. Maple trees can be tapped by drilling holes into their trunks and collecting the exuded sap, which is processed by heating to evaporate much of the water, leaving the concentrated syrup.
Maple syrup was first collected and used by the indigenous peoples, and the practice was adopted by European settlers, who gradually refined production methods. Technological improvements in the 1970s further refined syrup processing. The Canadian province of Quebec is by far the largest producer, responsible for about three-quarters of the world's output; Canadian exports of maple syrup exceed C$145 million (approximately US$130.5 million) per year. Vermont is the largest producer in the United States, generating about 5.5 percent of the global supply.
Maple syrup is graded according to the Canada, United States, or Vermont scales based on its density and translucency. Sucrose is the most prevalent sugar in maple syrup. In Canada, syrups must be made exclusively from maple sap to qualify as maple syrup and must also be at least 66 percent sugar. In the United States, a syrup must be made almost entirely from maple sap to be labelled as "maple", though states such as Vermont and New York have more restrictive definitions (see below).
Maple syrup is often eaten with pancakes, waffles, French toast, or oatmeal and porridge. It is also used as an ingredient in baking, and as a sweetener or flavouring agent. Culinary experts have praised its unique flavour, although the chemistry responsible is not fully understood.
Sources.
Three species of maple trees are predominantly used to produce maple syrup: the sugar maple ("Acer saccharum"), the black maple ("A. nigrum"), and the red maple ("A. rubrum"), because of the high sugar content (roughly two to five percent) in the sap of these species. The black maple is included as a subspecies or variety in a more broadly viewed concept of "A. saccharum", the sugar maple, by some botanists. Of these, the red maple has a shorter season because it buds earlier than sugar and black maples, which alters the flavour of the sap.
A few other (but not all) species of maple ("Acer") are also sometimes used as sources of sap for producing maple syrup, including the box elder or Manitoba maple ("Acer negundo"), the silver maple ("A. saccharinum"), and the bigleaf maple ("A. macrophyllum"). Similar syrups may also be produced from birch or palm trees, among other sources.
History.
Indigenous peoples.
Indigenous peoples living in northeastern North America were the first groups known to have produced maple syrup and maple sugar. According to aboriginal oral traditions, as well as archaeological evidence, maple tree sap was being processed into syrup long before Europeans arrived in the region. There are no authenticated accounts of how maple syrup production and consumption began, but various legends exist; one of the most popular involves maple sap being used in place of water to cook venison served to a chief. Other stories credit the development of maple syrup production to Nanabozho, Glooskap, or the squirrel. Aboriginal tribes developed rituals around sugar-making, celebrating the Sugar Moon (the first full moon of spring) with a Maple Dance. Many aboriginal dishes replaced the salt traditional in European cuisine with maple sugar or syrup.
The Algonquians recognized maple sap as a source of energy and nutrition. At the beginning of the spring thaw, they used stone tools to make V-shaped incisions in tree trunks; they then inserted reeds or concave pieces of bark to run the sap into buckets, which were often made from birch bark. The maple sap was concentrated either by dropping hot cooking stones into the buckets or by leaving them exposed to the cold temperatures overnight and disposing of the layer of ice that formed on top. While there was widespread agriculture in Mesoamerica and the Southeast and Southwest regions of the United States, the production of maple syrup is one of only a few agricultural processes in the Northeast that is not a European colonial import.
Europeans.
In the early stages of European colonization in northeastern North America, local Indigenous peoples showed the arriving colonists how to tap the trunks of certain types of maples during the spring thaw to harvest the sap. André Thevet, the "Royal Cosmographer of France", wrote about Jacques Cartier drinking maple sap during his Canadian voyages. By 1680, European settlers and fur traders were involved in harvesting maple products. However, rather than making incisions in the bark, the Europeans used the method of drilling tapholes in the trunks with augers. During the 17th and 18th centuries, processed maple sap was used primarily as a source of concentrated sugar, in both liquid and crystallized-solid form, as cane sugar had to be imported from the West Indies.
Maple sugaring parties typically began to operate at the start of the spring thaw in regions of woodland with sufficiently large numbers of maples. Syrup makers first bored holes in the trunks, usually more than one hole per large tree; they then inserted wooden spouts into the holes and hung a wooden bucket from the protruding end of each spout to collect the sap. The buckets were commonly made by cutting cylindrical segments from a large tree trunk and then hollowing out each segment's core from one end of the cylinder, creating a seamless, watertight container. Sap filled the buckets, and was then either transferred to larger holding vessels (barrels, large pots, or hollowed-out wooden logs), often mounted on sledges or wagons pulled by draft animals, or carried in buckets or other convenient containers. The sap-collection buckets were returned to the spouts mounted on the trees, and the process was repeated for as long as the flow of sap remained "sweet". The specific weather conditions of the thaw period were, and still are, critical in determining the length of the sugaring season. As the weather continues to warm, a maple tree's normal early spring biological process eventually alters the taste of the sap, making it unpalatable, perhaps due to an increase in amino acids.
The boiling process was very time-consuming. The harvested sap was transported back to the party's base camp, where it was then poured into large vessels (usually made from metal) and boiled to achieve the desired consistency. The sap was usually transported using large barrels pulled by horses or oxen to a central collection point, where it was processed either over a fire built out in the open or inside a shelter built for that purpose (the "sugar shack").
Since 1850.
Around the time of the American Civil War, syrup makers started using large, flat sheet metal pans as they were more efficient for boiling than heavy, rounded iron kettles, because of a greater surface area for evaporation. Around this time, cane sugar replaced maple sugar as the dominant sweetener in the US; as a result, producers focused marketing efforts on maple syrup. The first evaporator, used to heat and concentrate sap, was patented in 1858. In 1872, an evaporator was developed that featured two pans and a metal arch or firebox, which greatly decreased boiling time. Around 1900, producers bent the tin that formed the bottom of a pan into a series of flues, which increased the heated surface area of the pan and again decreased boiling time. Some producers also added a finishing pan, a separate batch evaporator, as a final stage in the evaporation process.
Buckets began to be replaced with plastic bags, which allowed people to see at a distance how much sap had been collected. Syrup producers also began using tractors to haul vats of sap from the trees being tapped (the sugarbush) to the evaporator. Some producers adopted motor-powered tappers and metal tubing systems to convey sap from the tree to a central collection container, but these techniques were not widely used. Heating methods also diversified: modern producers use wood, oil, natural gas, propane, or steam to evaporate sap. Modern filtration methods were perfected to prevent contamination of the syrup.
A large number of technological changes took place during the 1970s. Plastic tubing systems that had been experimental since the early part of the century were perfected, and the sap came directly from the tree to the evaporator house. Vacuum pumps were added to the tubing systems, and preheaters were developed to recycle heat lost in the steam. Producers developed reverse-osmosis machines to take a portion of water out of the sap before it was boiled, increasing processing efficiency.
Improvements in tubing and vacuum pumps, new filtering techniques, "supercharged" preheaters, and better storage containers have since been developed. Research continues on pest control and improved woodlot management. In 2009, researchers at the University of Vermont unveiled a new type of tap that prevents backflow of sap into the tree, reducing bacterial contamination and preventing the tree from attempting to heal the bore hole. Experiments show that it may be possible to use saplings in a plantation instead of mature trees dramatically boosting productivity per acre.
Processing.
Production methods have been streamlined since colonial days, yet remain basically unchanged. Sap must first be collected and boiled down to obtain pure syrup without chemical agents or preservatives. Maple syrup is made by boiling between 20 and 50 volumes of sap (depending on its concentration) over an open fire until 1 volume of syrup is obtained, usually at a temperature over the boiling point of water. As the boiling point of water varies with changes in air pressure the correct value for pure water is determined at the place where the syrup is being produced, each time evaporation is begun and periodically throughout the day. Syrup can be boiled entirely over one heat source or can be drawn off into smaller batches and boiled at a more controlled temperature.
Boiling the syrup is a tightly controlled process, which ensures appropriate sugar content. Syrup boiled too long will eventually crystallize, whereas under-boiled syrup will be watery, and will quickly spoil. The finished syrup has a density of 66° on the Brix scale (a hydrometric scale used to measure sugar solutions). The syrup is then filtered to remove sugar sand, crystals made up largely of sugar and calcium malate. These crystals are not toxic, but create a "gritty" texture in the syrup if not filtered out. The filtered syrup is graded and packaged while still hot, usually at a temperature of or greater. The containers are turned over after being sealed to sterilize the cap with the hot syrup. Packages can be made of metal, glass, or coated plastic, depending on volume and target market. The syrup can also be heated longer and further processed to create a variety of other maple products, including maple sugar, maple butter or cream, and maple candy or taffy.
Off-flavours.
Off-flavours can sometimes develop during the production of maple syrup; causes include contaminants in the boiling apparatus, such as paint or cleanser; changes in the sap, such as fermentation when it has been left sitting too long; and changes in the tree, such as "buddy sap" late in the season when budding has begun. In some circumstances it is possible to remove off-flavours through processing.
Production.
Maple syrup production is centred in northeastern North America; however, given the correct weather conditions, it can be made wherever suitable species of maple trees grow.
A maple syrup production farm is called a "sugarbush" or "sugarwood". Sap is often boiled in a "sugar house" (also known as a "sugar shack," "sugar shanty," or "cabane à sucre"), a building louvered at the top to vent the steam from the boiling sap.
Maples are usually tapped beginning at 30 to 40 years of age. Each tree can support between one and three taps, depending on its trunk diameter. The average maple tree will produce of sap per season, up to per day. This is roughly equal to 7% of its total sap. Seasons last for four to eight weeks, depending on the weather. During the day, starch stored in the roots for the winter rises through the trunk as sugary sap, allowing it to be tapped. Sap is not tapped at night because the temperature drop inhibits sap flow, although taps are typically left in place overnight. Some producers also tap in autumn, though this practice is less common than spring tapping. Maples can continue to be tapped for sap until they are over 100 years old.
Commerce.
Until the 1930s, the United States produced most of the world's maple syrup. Today, after rapid growth in the 1990s, Canada produces more than 80 percent of the world's maple syrup, producing about in 2004. The vast majority of this comes from the province of Quebec, which is the world's largest producer, with about 75 percent of global production totalling in 2005. As of 2003, Quebec had more than 7,000 producers, collectively making over of syrup. Production in Quebec is controlled through a supply management system, with producers receiving quota allotments from the Federation of Quebec Maple Syrup Producers (Fédération des producteurs acéricoles du Québec), which also maintains reserves of syrup. Canada exports more than of maple syrup per year, valued at more than C$145 million. The provinces of Ontario, Nova Scotia, New Brunswick, and Prince Edward Island produce smaller amounts of syrup.
The Canadian provinces of Manitoba and Saskatchewan produce maple syrup using the sap of the box elder or Manitoba maple ("Acer negundo"). A Manitoba maple tree's yield is usually less than half that of a similar sugar maple tree. Manitoba maple syrup has a slightly different flavour from sugar-maple syrup, because it contains less sugar and the tree's sap flows more slowly.
Vermont is the biggest US producer, with over during the 2013 season, followed by New York with and Maine with . Wisconsin, Ohio, New Hampshire, Michigan, Pennsylvania, Massachusetts, and Connecticut all produced marketable quantities of maple syrup of less than each in 2013. As of 2003, Vermont produced about 5.5 percent of the global syrup supply.
Maple syrup has been produced on a small scale in some other countries, notably Japan and South Korea. However, in South Korea in particular, it is traditional to consume maple sap, called "gorosoe", instead of processing it into syrup. 
In 2013, 65% of Canadian maple syrup exports went to the United States (a value of C$178 million), 9% to Japan (C$25 million), 8% to Germany (C$22 million) and 4.3% to the United Kingdom (C$12 million).
Grades.
Following an effort from the International Maple Syrup Institute (IMSI) and many maple syrup producer associations, both Canada and the United States have altered their laws regarding the classification of maple syrup to be uniform. Whereas in the past each state or province had their own laws on the classification of maple syrup, now those laws define a unified grading system. This had been a work in progress for several years, and most of the finalization of the new grading system was made in 2014. The Canadian Food Inspection Agency announced in the Canada Gazette on 28 June 2014 that rules for the sale of maple syrup would be amended to include new descriptors, at the request of the IMSI.
As of December 31, 2014, the Canadian Food Inspection Agency (CFIA) and as of March 2, 2015, the United States Department of Agriculture (USDA) Agricultural Marketing Service (AMS) issued revised standards on the classification of maple syrup as follows:
As long as maple syrup does not have an off-flavour and is of a uniform colour and clean and free from cloudiness, turbidity, sediment, it can be identified as one of the A grades. If it exhibits any of these problems, it does not meet Grade A requirements and must be labeled as Processing Grade maple syrup and may not be sold to the consumer. If maple syrup does not meet the requirements of Processing Grade maple syrup (including a fairly characteristic maple taste), it is classified as Substandard.
As of February 2015, this grading system has been accepted and made law by most maple-producing states and provinces, other than Ontario, Quebec, and Ohio. Vermont, in an effort to "jump-start" the new grading regulations, adopted the new grading system as of January 1, 2014, after the grade changes passed the Senate and House in 2013. Maine passed a bill to take effect as soon as both Canada and the United States adopted the new grades. They are allowing a one-year grace period. In New York, the new grade changes became law on January 1, 2015, with a one-year grace period. New Hampshire did not require legislative approval and so the new grade laws became effective as of December 16, 2014, and producer compliance is required as of January 1, 2016.
Golden and Amber grades typically have a milder flavour than Dark and Very dark, which are both dark and have an intense maple flavour. The darker grades of syrup are used primarily for cooking and baking, although some specialty dark syrups are produced for table use. Syrup harvested earlier in the season tends to yield a lighter colour. With the new grading system, the classification of maple syrup depends ultimately on its translucence. Golden has to be more than 75 percent translucent, Amber has to be 50.0 to 74.9 percent translucent, Dark has to be 25.0 to 49.9 percent translucent, and Very Dark is any product less than 25.0 percent translucent.
Old grading system.
In Canada, maple syrup was classified prior to December 31, 2014, by the Canadian Food Inspection Agency (CFIA) as one of three grades, each with several colour classes: Canada No. 1, including Extra Light, Light, and Medium; No. 2 Amber; and No. 3 Dark or any other ungraded category. Producers in Ontario or Québec may have followed either federal or provincial grading guidelines. Québec's and Ontario's guidelines differed slightly from the federal: there were two "number" categories in Québec (Number 1, with four colour classes, and 2, with five colour classes). As in Québec, Ontario's producers had two "number" grades: 1, with three colour classes; and 2, with one colour class, which was typically referred to as "Ontario Amber" when produced and sold in that province only. A typical year's yield for a maple syrup producer will be about 25 to 30 percent of each of the #1 colours, 10 percent #2 Amber, and 2 percent #3 Dark.
The United States used (some states still do, as they await state regulation) different grading standards. Maple syrup was divided into two major grades: Grade A and Grade B. Grade A was further divided into three subgrades: Light Amber (sometimes known as Fancy), Medium Amber, and Dark Amber. The Vermont Agency of Agriculture Food and Markets used a similar grading system of colour, and is roughly equivalent, especially for lighter syrups, but using letters: "AA", "A", etc. The Vermont grading system differed from the US system in maintaining a slightly higher standard of product density (measured on the Baumé scale). New Hampshire maintained a similar standard, but not a separate state grading scale. The Vermont-graded product had 0.9 percent more sugar and less water in its composition than US-graded. One grade of syrup not for table use, called commercial or Grade C, was also produced under the Vermont system.
Food and nutrition.
The basic ingredient in maple syrup is the sap from the xylem of sugar maple or various other species of maple trees. It consists primarily of sucrose and water, with small amounts of the monosaccharides glucose and fructose from the invert sugar created in the boiling process. Accordingly, sugars comprise 90% of total carbohydrates which contribute nearly all of the 261 calories per 100 g serving (right table).
Maple syrup generally is devoid of micronutrient content (right table), excepting appreciable amounts of zinc and manganese which contribute 44% and 157% of the Daily Value, respectively, per 100 g of syrup consumed (right table).
Maple syrup also contains trace amounts of amino acids which increase in content as sap flow occurs. Additionally, maple syrup contains a wide variety of volatile organic compounds, including vanillin, hydroxybutanone, and propionaldehyde. It is not yet known exactly what compounds are responsible for maple syrup's distinctive flavour, however its primary flavour contributing compounds are maple furanone, strawberry furanone, and maltol.
New compounds have been identified in maple syrup, one of which is quebecol, a natural phenolic compound created when the maple sap is boiled to create syrup.
One author described maple syrup as "a unique ingredient, smooth- and silky-textured, with a sweet, distinctive flavour – hints of caramel with overtones of toffee will not do – and a rare colour, amber set alight. Maple flavour is, well, maple flavour, uniquely different from any other." Agriculture Canada has developed a "flavour wheel" that details 91 unique flavours that can be present in maple syrup. These flavours are divided into 13 families: vanilla, empyreumatic (burnt), milky, fruity, floral, spicy, foreign deterioration or environment, maple, confectionery, plants forest-humus-cereals, herbaceous, or ligneous. These flavours are evaluated using a procedure similar to wine tasting. Other culinary experts praise its unique flavour.
Maple syrup and its various artificial imitations are widely used as toppings for pancakes, waffles, and French toast in North America. They can also be used to flavour a variety of foods, including fritters, ice cream, hot cereal, fresh fruit, and sausages. It is also used as sweetener for granola, applesauce, baked beans, candied sweet potatoes, winter squash, cakes, pies, breads, tea, coffee, and hot toddies. Maple syrup can also be used as a replacement for honey in wine (mead).
Imitations and substitutions.
In the United States, maple syrup must be made almost entirely from maple sap, although small amounts of substances such as salt may be added. "Maple-flavoured" syrups include maple syrup but may contain additional ingredients. "Pancake syrup", "waffle syrup", "table syrup", and similarly named syrups are substitutes which are less expensive than maple syrup. In these syrups, the primary ingredient is most often high fructose corn syrup flavoured with sotolon; they have no genuine maple content, and are usually thickened far beyond the viscosity of maple syrup. The fenugreek seed, a spice with high amounts of sotolon, can be prepared to have a maple-like flavour, and is used to make a very strong commercial flavouring that is similar to maple syrup, but much less expensive; one such syrup, Mapleine, was popular during the Great Depression. American labelling laws prohibit imitation syrups from having "maple" in their names.
In Canada, maple syrup must be made entirely from maple sap, and syrup must have a density of 66° on the Brix scale to be marketed as maple syrup. Québécois sometimes refer to imitation maple syrup as "sirop de poteau" ("pole syrup"), a joke referring to the syrup as having been made by tapping telephone poles.
Imitation syrups are generally cheaper than maple syrup, but tend to taste artificial. A 2009 "Cook's Illustrated" comparison between top-selling maple and imitation syrups consistently rated the real maple brands (Maple Grove Farms, Highland Sugarworks, Camp Maple, Spring Tree, and Maple Gold) above the imitation brands tested (Eggo, Aunt Jemima, Mrs. Butterworth's, Log Cabin, and Hungry Jack). In the United States, consumers generally prefer imitation syrups, likely because of the significantly lower cost.
Cultural significance.
Maple syrup and maple sugar were used during the American Civil War and by abolitionists in the years prior to the war because most cane sugar and molasses were produced by Southern slaves. Because of food rationing during the Second World War, people in the northeastern United States were encouraged to stretch their sugar rations by sweetening foods with maple syrup and maple sugar, and recipe books were printed to help housewives employ this alternative source.
Maple products are considered emblematic of Canada, in particular Quebec, and are frequently sold in tourist shops and airports as souvenirs from Canada. The sugar maple's leaf has come to symbolize Canada, and is depicted on the country's flag. Several US states, including New York, Vermont and Wisconsin, have the sugar maple as their state tree. A scene of sap collection is depicted on the Vermont state quarter, issued in 2001.

</doc>
<doc id="19888" url="https://en.wikipedia.org/wiki?curid=19888" title="Matthew">
Matthew

Matthew may refer to:

</doc>
<doc id="19890" url="https://en.wikipedia.org/wiki?curid=19890" title="Male (disambiguation)">
Male (disambiguation)

Male may refer to:
In geography:
People:
Other:

</doc>
<doc id="19891" url="https://en.wikipedia.org/wiki?curid=19891" title="Macron">
Macron

A macron () is a diacritical mark, a straight bar placed above a letter, usually a vowel. Its name derives from the Greek ("makrón"), meaning "long", and was originally used to mark long or heavy syllables in Greco-Roman metrics. It now more often marks a long vowel. In the International Phonetic Alphabet, the macron is used to indicate a mid-tone; the sign for a long vowel is instead a modified triangular colon .
The opposite is the breve , which marks a short or light syllable or a short vowel.
Uses.
Syllable weight.
In Greco-Roman metrics and in the description of the metrics of other literatures, the macron was introduced and is still widely used to mark a long (heavy) syllable. Even relatively recent classical Greek and Latin dictionaries are still concerned with indicating only the length (weight) of syllables; that is why most still do not indicate the length of vowels in syllables that are otherwise metrically determined. Many textbooks about Ancient Rome and Greece use the macron even if it was not actually used at that time.
Vowel length.
The following languages or transliteration systems use the macron to mark long vowels:
Tone.
The following languages or alphabets use the macron to mark tones:
Omission.
Sometimes the macron marks an omitted "n" or "m", like the tilde:
Letter extension.
The macron is used in the orthography of a number of vernacular languages of the Solomon Islands and Vanuatu, particularly those first transcribed by Anglican missionaries. The macron has no unique value, and is simply used to distinguish between two different phonemes.
Thus, in several languages of the Banks Islands, including Mwotlap, the simple "m" stands for , but an "m" with a macron (m̄) is a rounded labial-velar nasal ; while the simple "n" stands for the common alveolar nasal , an "n" with macron (n̄) represents the velar nasal ; the vowel ē stands for a (short) higher by contrast with plain "e" ; likewise ō contrasts with plain "o" .
In Hiw orthography, the consonant "r̄" stands for the prestopped velar lateral approximant .
In Araki, the same symbol "r̄" encodes the alveolar trill – by contrast with "r", which encodes the alveolar flap .
In Kokota, "ḡ" is used for the velar stop , but "g" without macron is the voiced velar fricative .
In Marshallese, a macron is used on four letters—'—whose pronunciations differ from the unmarked '. Marshallese uses a vertical vowel system with three to four vowel phonemes, but traditionally their allophones have been written out, so vowel letters with macron are used for some of these allophones. Though the standard diacritic involved is a macron, there are no other diacritics used "above" letters, so in practice other diacritics can and have been used in less polished writing or print, yielding nonstandard letters like "", depending on displayability of letters in computer fonts.
Other uses.
Also, in some instances, a diacritic will be written like a macron, although it represents another diacritic whose standard form is different:
Medicine.
In medical prescriptions and other handwritten notes, macrons mean:
Mathematics and science.
The overline is a typographical symbol similar to the macron, used in a number of ways in mathematics and science.
Music.
In music, the tenuto marking resembles the macron.
The macron is also used in German lute tablature to distinguish repeating alphabetic characters.
Technical notes.
In LaTeX a macron is created with the command "\=", for example: M\=aori for Māori.
In OpenOffice, if the extension Compose Special Characters is installed, a macron may be added by following the letter with a hyphen and pressing the user’s predefined shortcut key for composing special characters. A macron may also be added by following the letter with the character’s four-digit hex-code, and pressing the user’s predefined shortcut key for adding unicode characters.

</doc>
<doc id="19894" url="https://en.wikipedia.org/wiki?curid=19894" title="Mosque">
Mosque

A mosque (; from "Al-masjid") is a place of worship for followers of Islam.
There are strict and detailed requirements in Sunni "fiqh" for a place of worship to be considered a mosque, with places that do not meet these requirements regarded as "musalla"s. There are stringent restrictions on the uses of the area formally demarcated as the mosque (which is often a small portion of the larger complex), and, in the Islamic "Sharia" law, after an area is formally designated as a mosque, it remains so until the Last Day.
Many mosques have elaborate domes, minarets, and prayer halls, in varying styles of architecture. Mosques originated on the Arabian Peninsula, but are now found in all inhabited continents. The mosque serves as a place where Muslims can come together for "salat" ( "ṣalāt", meaning "prayer") as well as a center for information, education, and dispute settlement. The imam leads the congregation in prayer.
Etymology.
The word entered English from a French word that probably derived from Italian "moschea", a variant of Italian "moscheta", from either Middle Armenian մզկիթ ("mzkit‘") or Medieval Greek μασγίδιον ("masgídion") or Spanish "mezquita", from the Arabic "masjid" meaning "place of worship" or "prostration in prayer", either from Nabataean "masgĕdhā́" or from Arabic "sajada" meaning "to bow down in prayer", probably ultimately from Aramaic "sĕghēdh".
History.
The first mosque in the world is often considered to be the area around the Kaaba in Mecca now known as the Masjid al-Haram. Since as early as 638 AD, the Masjid al-Haram has been expanded on several occasions to accommodate the increasing number of Muslims who either live in the area or make the annual pilgrimage known as "hajj" to the city. Others regard the first mosque in history to be the Quba Mosque in present-day Medina since it was the first structure built by Muhammad upon his emigration from Mecca in 622.
The Islamic Prophet Muhammad went on to establish another mosque in Medina, which is now known as the Masjid an-Nabawi, or the Prophet's Mosque. Built on the site of his home, Muhammad participated in the construction of the mosque himself and helped pioneer the concept of the mosque as the focal point of the Islamic city. The Masjid al-Nabawi introduced some of the features still common in today's mosques, including the niche at the front of the prayer space known as the "mihrab" and the tiered pulpit called the "minbar". The Masjid al-Nabawi was also constructed with a large courtyard, a motif common among mosques built since then.
Diffusion and evolution.
Mosques had been built in Iraq and North Africa by the end of the 7th century, as Islam spread outside the Arabian Peninsula with early caliphates. The Imam Husayn Shrine in Karbala is reportedly one of the oldest mosques in Iraq, although its present formtypical of Persian architectureonly goes back to the 11th century. The shrine, while still operating as a mosque, remains one of the holiest sites for Shia Muslims, as it honors the death of the third Shia imam, and Prophet Muhammad's grandson, Hussein ibn Ali. The Mosque of Amr ibn al-As was reportedly the first mosque in Egypt, serving as a religious and social center for Fustat (present-day Cairo) during its prime. Like the Imam Husayn Shrine, though, nothing of its original structure remains. With the later Shia Fatimid Caliphate, mosques throughout Egypt evolved to include schools (known as "madrasas"), hospitals, and tombs.
The Great Mosque of Kairouan in present-day Tunisia was reportedly the first mosque built in northwest Africa, with its present form (dating from the 9th century) serving as a model for other Islamic places of worship in the Maghreb. It was the first to incorporate a square minaret (as opposed to the more common circular minaret) and includes naves akin to a basilica. Those features can also be found in Andalusian mosques, including the Grand Mosque of Cordoba, as they tended to reflect the architecture of the Moors instead of their Visigoth predecessors. Still, some elements of Visigothic architecture, like horseshoe arches, were infused into the mosque architecture of Spain and the Maghreb.
The first mosque in East Asia was reportedly established in the 8th century in Xi'an. However, the Great Mosque of Xi'an, whose current building dates from the 18th century, does not replicate the features often associated with mosques elsewhere. Indeed, minarets were initially prohibited by the state. Following traditional Chinese architecture, the Great Mosque of Xi'an, like many other mosques in eastern China, resembles a pagoda, with a green roof instead of the yellow roof common on imperial structures in China. Mosques in western China were more likely to incorporate elements, like domes and minarets, traditionally seen in mosques elsewhere.
A similar integration of foreign and local influences could be seen on the Indonesian islands of Sumatra and Java, where mosques, including the Demak Great Mosque, were first established in the 15th century. Early Javanese mosques took design cues from Hindu, Buddhist, and Chinese architectural influences, with tall timber, multi-level roofs similar to the pagodas of Balinese Hindu temples; the ubiquitous Islamic dome did not appear in Indonesia until the 19th century. In turn, the Javanese style influenced the styles of mosques in Indonesia's Austronesian neighbors—Malaysia, Brunei, and the Philippines.
Muslim empires were instrumental in the evolution and spread of mosques. Although mosques were first established in India during the 7th century, they were not commonplace across the subcontinent until the arrival of the Mughals in the 16th and 17th centuries. Reflecting their Timurid origins, Mughal-style mosques included onion domes, pointed arches, and elaborate circular minarets, features common in the Persian and Central Asian styles. The Jama Masjid in Delhi and the Badshahi Mosque in Lahore, built in a similar manner in the mid-17th century, remain two of the largest mosques on the Indian subcontinent.
The Umayyad Caliphate was particularly instrumental in spreading Islam and establishing mosques within the Levant, as the Umayyads constructed among the most revered mosques in the region—the al-Aqsa Mosque and Dome of the Rock in Jerusalem and the Umayyad Mosque in Damascus. The designs of the Dome of the Rock and the Umayyad Mosque were influenced by Byzantine architecture, a trend that continued with the rise of the Ottoman Empire.
Several of the early mosques in the Ottoman Empire were originally churches or cathedrals from the Byzantine Empire, with the Hagia Sophia (one of those converted cathedrals) informing the architecture of mosques from after the Ottoman conquest of Constantinople. Still, the Ottomans developed their own architectural style characterized by large central rotundas (sometimes surrounded by multiple smaller domes), pencil-shaped minarets, and open facades.
Mosques from the Ottoman period are still scattered across Eastern Europe, but the most rapid growth in the number of mosques in Europe has occurred within the past century as more Muslims have migrated to the continent. Many major European cities are home to mosques, like the Grand Mosque of Paris, that incorporate domes, minarets, and other features often found with mosques in Muslim-majority countries. The first mosque in North America was founded by Albanian Americans in 1915, but the continent's oldest surviving mosque, the Mother Mosque of America, only dates back to the 1930s. As in Europe, the number of American mosques has rapidly increased in recent decades as Muslim immigrants, particularly from South Asia, have come in the United States. Greater than forty percent of mosques in the United States were constructed after 2000.
Conversion of places of worship.
According to early Muslim historians, towns that surrendered without resistance and made treaties with the Muslims gave the Muslims permission to take their churches and synagogues. One of the earliest examples of these kinds of conversions was in Damascus, Syria, where in 705 Umayyad caliph Al-Walid I bought the church of St. John from the Christians and had it rebuilt as a mosque in exchange for building a number of new churches for the Christians in Damascus. Overall, Abd al-Malik ibn Marwan (Al-Waleed's father) is said to have transformed 10 churches in Damascus into mosques.
The process of turning churches into mosques was especially intensive in the villages where most of the inhabitants converted to Islam. The Abbasid caliph al-Ma'mun turned many churches into mosques. Ottoman Turks converted nearly all churches, monasteries, and chapels in Constantinople, including the famous Hagia Sophia, into mosques immediately after capturing the city in 1453. In some instances mosques have been established on the places of Jewish or Christian sanctuaries associated with Biblical personalities who were also recognized by Islam.
Mosques have also been converted for use by other religions, notably in southern Spain, following the conquest of the Moors in 1492. The most prominent of them is the Great Mosque of Cordoba. Outside of the Iberian Peninsula, such instances also occurred in southeastern Europe once regions were no longer under Muslim rule.
Religious functions.
The "masjid jāmi"', a central mosque, can play a role in religious activities such as teaching the Quran and educating future imams.
Prayers.
There are two holidays ("Eids") in the Islamic calendar, Eid ul-Fitr and Eid al-Adha, during which there are special prayers held at mosques in the morning. These Eid prayers are supposed to be offered in large groups, and so, in the absence of an outdoor "eidgah" larger mosques will normally host them for their congregants as well as the congregants of smaller local mosques. Some mosques will even rent convention centers or other large public buildings to hold the large number of Muslims who attend. Mosques, especially those in countries where Muslims are the majority, will also host Eid prayers outside in courtyards, town squares or on the outskirts of town in an "Eidgah".
Ramadan events.
Islam's holiest month, "Ramadan", is observed through many events. As Muslims must fast during the day during Ramadan, mosques will host "iftar" dinners after sunset and the fourth required prayer of the day, "maghrib". Food is provided, at least in part, by members of the community, thereby creating nightly potluck dinners. Because of the community contribution necessary to serve iftar dinners, mosques with smaller congregations may not be able to host the "iftar" dinners daily. Some mosques will also hold "suhoor" meals before dawn to congregants attending the first required prayer of the day, "fajr". As with iftar dinners, congregants usually provide the food for suhoor, although able mosques may provide food instead. Mosques will often invite poorer members of the Muslim community to share in beginning and breaking the fasts, as providing charity during Ramadan is regarded in Islam as especially honorable.
Following the last obligatory daily prayer ("Isha"') special, optional "tarawih" prayers are offered in larger mosques. During each night of prayers, which can last for up to two hours each night, usually one member of the community who has memorized the entire Quran (a Hafiz) will recite a segment of the book. Sometimes, several such people (not necessarily of the local community) take turns to do this. During the last ten days of Ramadan, larger mosques will host all-night programs to observe Laylat al-Qadr, the night Muslims believe that Muhammad first received Quranic revelations. On that night, between sunset and sunrise, mosques employ speakers to educate congregants in attendance about Islam. Mosques or the community usually provide meals periodically throughout the night
During the last ten days of Ramadan, larger mosques within the Muslim community will host "Iʿtikāf", a practice in which at least one Muslim man from the community must participate. Muslims performing itikaf are required to stay within the mosque for ten consecutive days, often in worship or learning about Islam. As a result, the rest of the Muslim community is responsible for providing the participants with food, drinks, and whatever else they need during their stay.
Charity.
The third of the Five Pillars of Islam states that Muslims are required to give approximately one-fortieth of their wealth to charity as "zakat". Since mosques form the center of Muslim communities, they are where Muslims go to both give "zakat" and, if necessary, collect it. Before the holiday of Eid ul-Fitr, mosques also collect a special zakat that is supposed to assist in helping poor Muslims attend the prayers and celebrations associated with the holiday.
Contemporary political roles.
The late 20th century saw an increase in the number of mosques used for political purposes. Today, civic participation is commonly promoted in mosques in the Western world. Because of the importance in the community, mosques are used for preaching peaceful coexistence with non-believers, even in times of adversity.
Large mosques sometimes play a political role as well. In Islamic countries like Bangladesh, Pakistan, Iran, and Saudi Arabia, political subjects are preached by imams at Friday congregations on a regular basis. In other Islamic countries, imams are usually banned from mentioning political issues.
Advocacy.
Countries with a minority Muslim population are more likely than Muslim-majority countries of the Greater Middle East to use mosques as a way to promote civic participation. American mosques host voter registration and civic participation drives that promote involving Muslims, who are often first- or second-generation immigrants, in the political process. As a result of these efforts as well as attempts at mosques to keep Muslims informed about the issues facing the Muslim community, regular mosque attendants are more likely to participate in protests, sign petitions, and otherwise be involved in politics.
Nevertheless, a link between political views and mosque attendance can still be seen in other parts of the world. Following the al-Askari Mosque bombing in February 2006, imams and other Islamic leaders used mosques and Friday prayers as vehicles to call for calm and peace in the midst of widespread violence.
Social conflict.
As they are considered important to the Muslim community, mosques, like other places of worship, can be at the heart of social conflicts. The Babri Mosque was the subject of such a conflict up until the early 1990s when it was demolished. Before a mutual solution could be devised, the mosque was destroyed on December 6, 1992 as the mosque was built by Babur allegedly on the site of a previous Hindu temple marking the birthplace of Rama. The controversy surrounded the mosque was directly linked to rioting in Bombay (present-day Mumbai) as well as bombings in 1993 that killed 257 people.
Bombings in February 2006 and June 2007 seriously damaged Iraq's al-Askari Mosque and exacerbated existing tensions. Other mosque bombings in Iraq, both before and after the February 2006 bombing, have been part of the conflict between the country's groups of Muslims. However, mosque bombings have not been exclusive to Iraq; in June 2005, a suicide bomber killed at least 19 people at an Afghan Shia mosque near Jade Maivand. In April 2006, two explosions occurred at India's Jama Masjid.
Following the September 11 attacks, several American mosques were targeted in attacks ranging from simple vandalism to arson. Furthermore, the Jewish Defense League was suspected of plotting to bomb the King Fahd Mosque in Culver City, California. Similar attacks occurred throughout the United Kingdom following the 7 July 2005 London bombings. Outside the Western world, in June 2001, the Hassan Bek Mosque was the target of vandalism and attacks by hundreds of Israelis after a suicide bomber killed 19 people in a night club in Tel Aviv. Although mosquegoing is highly encouraged for men, it is permitted to stay at home when one feels at risk from Islamophobic persecution.
Saudi influence.
Although the Saudi involvement in Sunni mosques around the world can be traced back to the 1960s, it was not until later in the 20th century that the government of Saudi Arabia became a large influence in foreign Sunni mosques.. Beginning in the 1980s, the Saudi Arabian government began to finance the construction of Sunni mosques in countries around the world. An estimated US$45 billion has been spent by the Saudi Arabian government financing mosques and Sunni Islamic schools in foreign countries. "Ain al-Yaqeen", a Saudi newspaper, reported in 2002 that Saudi funds may have contributed to building as many as 1,500 mosques and 2,000 other Islamic centers.
Saudi citizens have also contributed significantly to mosques in the Islamic world, especially in countries where they see Muslims as poor and oppressed. Following the fall of the Soviet Union, in 1992, mosques in war-torn Afghanistan saw many contributions from Saudi citizens. The King Fahd Mosque in Culver City, California and the Islamic Cultural Center of Italy in Rome represent two of Saudi Arabia's largest investments in foreign mosques as former Saudi king Fahd bin Abdul Aziz al-Saud contributed US$8 million and US$50 million to the two mosques, respectively.
Architecture.
Styles.
"Arab-plan" or hypostyle mosques are the earliest type of mosques, pioneered under the Umayyad Dynasty. These mosques have square or rectangular plans with an enclosed courtyard and covered prayer hall. Historically, in the warm Middle Eastern and Mediterranean climates, the courtyard served to accommodate the large number of worshippers during Friday prayers. Most early hypostyle mosques had flat roofs on prayer halls, which required the use of numerous columns and supports. One of the most notable hypostyle mosques is the Great Mosque of Cordoba in Spain, the building being supported by over 850 columns. Frequently, hypostyle mosques have outer arcades so that visitors can enjoy the shade. Arab-plan mosques were constructed mostly under the Umayyad and Abbasid dynasties; subsequently, however, the simplicity of the Arab plan limited the opportunities for further development, the mosques consequently losing popularity.
The first separate brand within mosque designs started appearing in Persia (Iran). The Persians had inherited a rich architectural legacy from the earlier Persian dynasties, and they started incorporating elements from earlier Parthian and Sassanid palace-designs into their mosques, influenced by buildings such as the Palace of Ardashir and the Sarvestan Palace. Thus, Islamic architecture started witnessing the introduction of such structures as domes and large, arched entrances, referred to as "iwans". During Seljuq rule, as Islamic mysticism was on the rise, the four-iwan arrangement took form. The four-iwan format, finalized by the Seljuqs, and later inherited by the Safavids, firmly established the courtyard facade of such mosques, with the towering gateways at every side, as more important than the actual buildings themselves, and they typically took the form of a square-shaped, central courtyard with large entrances at each side, giving the impression of being gateways to the spiritual world. Soon, a distinctly Persian style of mosques started appearing that would significantly influence the designs of later Timurid, and also Mughal, mosque designs.
The Ottomans introduced central dome mosques in the 15th century. These mosques have a large dome centered over the prayer hall. In addition to having a large central dome, a common feature is smaller domes that exist off-center over the prayer hall or throughout the rest of the mosque, where prayer is not performed. This style was heavily influenced by Byzantine architecture with its use of large central domes. Hajja Soad's mosque took a pyramid shape that is a creative style in Islamic architecture.
The Faisal Mosque in Islamabad, Pakistan in a relatively unusual design fuses contemporary lines with the more traditional look of an Arab Bedouin's tent, with its large triangular prayer hall and four minarets. However, unlike traditional mosque design, it lacks a dome. The mosque's architecture is a departure from the long history of South Asian Islamic architecture. It is one of the most outstanding and modern Islamic architecture examples in the world.
Mosques built in Southeast Asia often represent the Indonesian-Javanese style architecture, which are different from the ones found throughout the Greater Middle East. The ones found in Europe and North America appear to have various styles but most are built on Western artchitectural designs, some are former churches or other buildings that were used by non-Muslims. In Africa, most mosques are old but the new ones are built to give it a look of the Greater Middle East. This can be seen in the Abuja National Mosque in Nigeria and others.
Minarets.
A common feature in mosques is the minaret, the tall, slender tower that usually is situated at one of the corners of the mosque structure. The top of the minaret is always the highest point in mosques that have one, and often the highest point in the immediate area. The tallest minaret in the world is located at the Hassan II Mosque in Casablanca, Morocco. It has a height of and completed in 1993, it was designed by Michel Pinseau.
The first mosques had no minarets, and even nowadays the most conservative Islamic movements, like Wahhabis, avoid building minarets, seeing them as ostentatious and hazardous in case of collapse. The first minaret was constructed in 665 in Basra during the reign of the Umayyad caliph Muawiyah I. Muawiyah encouraged the construction of minarets, as they were supposed to bring mosques on par with Christian churches with their bell towers. Consequently, mosque architects borrowed the shape of the bell tower for their minarets, which were used for essentially the same purpose—calling the faithful to prayer. The oldest standing minaret in the world is the minaret of the Great Mosque of Kairouan in Tunisia, built between the 8th and the 9th century, it is a massive square tower consisting of three superimposed tiers of gradual size and decor.
Before the five required daily prayers, a "muezzin" calls the worshippers to prayer from the minaret. In many countries like Singapore where Muslims are not the majority, mosques are prohibited from loudly broadcasting the call to prayer ("adhan"), although it is supposed to be said loudly to the surrounding community. The "adhan" is required before every prayer. However, nearly every mosque assigns a "muezzin" for each prayer to say the "adhan" as it is a recommended practice or "sunnah" of the Islamic prophet Muhammad. At mosques that do not have minarets, the "adhan" is called instead from inside the mosque or somewhere else on the ground. The "iqama", which is similar to the "adhan" and said immediately before the start of prayer, is usually not said from the minaret even if a mosque has one.
Mihrab.
A mihrab is a semicircular niche in the wall of a mosque that indicates the "qibla". That is, the direction of the Kaaba in Mecca and hence the direction that Muslims should face when praying. The wall in which a "mihrab" appears is thus the ""qibla" wall."
"Mihrab"s should not be confused with the "minbar", which is the raised platform from which an Imam (leader of prayer) addresses the congregation.
Domes.
The domes, often placed directly above the main prayer hall, may signify the vaults of heaven and the sky. As time progressed, domes grew, from occupying a small part of the roof near the mihrab to encompassing the whole roof above the prayer hall. Although domes normally took on the shape of a hemisphere, the Mughals in India popularized onion-shaped domes in South Asia. Some mosques have multiple, often smaller, domes in addition to the main large dome that resides at the center.
Prayer hall.
The prayer hall, also known as the "musallah", rarely has furniture; chairs and pews are generally absent from the prayer hall so as to allow as many worshipers as possible to line the room. Some mosques have Islamic calligraphy and Quranic verses on the walls to assist worshippers in focusing on the beauty of Islam and its holiest book, the Quran, as well as for decoration.
Often, a limited part of the prayer hall is sanctified formally as a masjid in the sharia sense (although the term masjid is also used for the larger mosque complex as well). Once designated, there are onerous limitations on the use of this formally designated masjid, and it may not be used for any purpose other than worship; restrictions that do not necessarily apply to the rest of the prayer area, and to the rest of the mosque complex (although such uses may be restricted by the conditions of the "waqf" that owns the mosque).
In many mosques, especially the early congregational mosques, the prayer hall is in the hypostyle form (the roof held up by a multitude of columns). One of the finest examples of the hypostyle-plan mosques is the Great Mosque of Kairouan (also known as the Mosque of Uqba) in Tunisia.
Usually opposite the entrance to the prayer hall is the "qiblah" wall, the visually emphasized area inside the prayer hall. The qiblah wall should, in a properly oriented mosque, be set perpendicular to a line leading to Mecca, the location of the "Kaaba". Congregants pray in rows parallel to the qiblah wall and thus arrange themselves so they face Mecca. In the qiblah wall, usually at its center, is the mihrab, a niche or depression indicating the direction of Mecca. Usually the mihrab is not occupied by furniture either. Sometimes, especially during Friday prayers, a raised "minbar" or pulpit is located to the side of the mihrab for a "khatib" or some other speaker to offer a sermon ("khutbah"). The mihrab serves as the location where the imam leads the five daily prayers on a regular basis.
Ablution facilities.
As ritual purification precedes all prayers, mosques often have ablution fountains or other facilities for washing in their entryways or courtyards. However, worshippers at much smaller mosques often have to use restrooms to perform their ablutions. In traditional mosques, this function is often elaborated into a freestanding building in the center of a courtyard. This desire for cleanliness extends to the prayer halls where shoes are disallowed to be worn anywhere other than the cloakroom. Thus, foyers with shelves to put shoes and racks to hold coats are commonplace among mosques.
Contemporary features.
Modern mosques have a variety of amenities available to their congregants. As mosques are supposed to appeal to the community, they may also have additional facilities, from health clinics to libraries to gymnasiums, to serve the community.
Makeshift and temporary structures for Islamic worship.
A temporary place set aside for Islamic worship is called a musalla ("Jama'at Khana" in South Asia). A musallah is often not part of a permanent endowment (waqf), or it is otherwise not intended to become a permanent legal masjid (as defined in the Sharia). Often musallas are used while a community looks for a piece of land for a permanent masjid, or the establishment of a masjid is not practical at the time. They could be located in rented apartments, industrial units or store fronts.
Rules and etiquette.
Mosques, in accordance with Islamic practices, institute a number of rules intended to keep Muslims focused on worshipping God. While there are several rules, such as those regarding not allowing shoes in the prayer hall, that are universal, there are many other rules that are dealt with and enforced in a variety of ways from mosque to mosque.
Prayer leader.
Appointment of a prayer leader is considered desirable, but not always obligatory. The permanent prayer leader (imam) must be a free honest individual and is authoritative in religious matters. In mosques constructed and maintained by the government, the prayer leader is appointed by the ruler; in private mosques, however, appointment is made by members of the congregation through majority voting. According to the Hanafi school of Islamic jurisprudence, the individual who built the mosque has a stronger claim to the title of imam, but this view is not shared by the other schools.
Leadership at prayer falls into three categories, depending on the type of prayer: five daily prayers, Friday prayer, or optional prayers. According to the Hanafi and Maliki school of Islamic jurisprudence, appointment of a prayer leader for Friday service is mandatory because otherwise the prayer is invalid. The Shafi'i and Hanbali schools, however, argue that the appointment is not necessary and the prayer is valid as long as it is performed in a congregation. A slave may lead a Friday prayer, but Muslim authorities disagree over whether the job can be done by a minor. An imam appointed to lead Friday prayers may also lead at the five daily prayers; Muslim scholars agree to the leader appointed for five daily services may lead the Friday service as well.
All Muslim authorities hold the consensus opinion that only men may lead prayer for men. Nevertheless, women prayer leaders are allowed to lead prayer in front of all-female congregations.
Cleanliness.
All mosques have rules regarding cleanliness, as it is an essential part of the worshippers' experience. Muslims before prayer are required to cleanse themselves in an ablution process known as "wudu". However, even to those who enter the prayer hall of a mosque without the intention of praying, there are still rules that apply. Shoes must not be worn inside the carpeted prayer hall. Some mosques will also extend that rule to include other parts of the facility even if those other locations are not devoted to prayer. Congregants and visitors to mosques are supposed to be clean themselves. It is also undesirable to come to the mosque after eating something that smells, such as garlic.
Dress.
Islam requires that its adherents wear clothes that portray modesty. Men are supposed to come to the mosque wearing loose and clean clothes that do not reveal the shape of the body. Likewise, it is recommended that women at a mosque wear loose clothing that covers to the wrists and ankles, and cover their heads with a "hijab" or other covering. Many Muslims, regardless of their ethnic background, wear Middle Eastern clothing associated with Arabic Islam to special occasions and prayers at mosques.
Concentration.
As mosques are places of worship, those within the mosque are required to remain respectful to those in prayer. Loud talking within the mosque, as well as discussion of topics deemed disrespectful, is forbidden in areas where people are praying. In addition, it is disrespectful to walk in front of or otherwise disturb Muslims in prayer. The walls within the mosque have few items, except for possibly Islamic calligraphy, so Muslims in prayer are not distracted. Muslims are also discouraged from wearing clothing with distracting images and symbols so as not to divert the attention of those standing behind them during prayer. In many mosques, even the carpeted prayer area has no designs, its plainness helping worshippers to focus.
Gender separation.
There is nothing written in the Qurʼan about the issue of space in mosques and gender separation. However, traditional rules have segregated women and men. By traditional rules, women are most often told to occupy the rows behind the men. In part, this was a practical matter as the traditional posture for prayerkneeling on the floor, head to the groundmade mixed-gender prayer uncomfortably revealing for many women and distracting for some men. Traditionalists try to argue that Muhammad preferred women to pray at home rather than at a mosque, and they cite a "hadith" in which Muhammad supposedly said: "The best mosques for women are the inner parts of their houses," although women were active participants in the mosque started by Muhammad. Muhammad told Muslims not to forbid women from entering mosques. They are allowed to go in. The second Sunni caliph ʻUmar at one time prohibited women from attending mosques especially at night because he feared they may be sexually harassed or assaulted by men, so he required them to pray at home. Sometimes a special part of the mosque was railed off for women; for example, the governor of Mecca in 870 had ropes tied between the columns to make a separate place for women.
Many mosques today will put the women behind a barrier or partition or in another room. Mosques in South and Southeast Asia put men and women in separate rooms, as the divisions were built into them centuries ago. In nearly two-thirds of American mosques, women pray behind partitions or in separate areas, not in the main prayer hall; some mosques do not admit women at all due to the lack of space and the fact that some prayers, such as the Friday Jumuʻah, are mandatory for men but optional for women. Although there are sections exclusively for women and children, the Grand Mosque in Mecca is desegregated.
Non-Muslims in mosques.
Under most interpretations of "sharia", non-Muslims are permitted to enter mosques provided that they respect the place and the people inside it. Prophet Muhammad once allowed a group of Christians to pray inside Al-Masjid al-Nabawi for their Sunday worship. A dissenting opinion and minority view is presented by followers of the Maliki school of Islamic jurisprudence, who argue that non-Muslims may not be allowed into mosques under any circumstances.
The Quran addresses the subject of non-Muslims, and particularly polytheists, in mosques in two verses in its ninth chapter, Sura At-Tawba. The seventeenth verse of the chapter prohibits those who "join gods with Allah"—polytheists—from entering mosques:
The twenty-eighth verse of the same chapter is more specific as it only considers polytheists in the Sacred Mosque, the Masjid al-Haram in Mecca:
According to Ahmad ibn Hanbal, these verses were followed to the letter at the times of Muhammad, when Jews and Christians, considered monotheists, were still allowed to the Masjid al-Haram. However, the Umayyad caliph Umar II later forbade non-Muslims from entering mosques, and his ruling remains in practice in present-day Saudi Arabia. Today, the decision on whether non-Muslims should be allowed to enter mosques varies. With few exceptions, mosques in the Arabian Peninsula as well as Morocco do not allow entry to non-Muslims. For example, the Hassan II Mosque in Casablanca is one of only two mosques in Morocco currently open to non-Muslims.
However, there are also many other places in the West as well as the Islamic world where non-Muslims are welcome to enter mosques. Most mosques in the United States, for example, report receiving non-Muslim visitors every month. Many mosques throughout the United States welcome non-Muslims as a sign of openness to the rest of the community as well as to encourage conversions to Islam.
In modern-day Saudi Arabia, the Grand Mosque and all of Mecca are open only to Muslims. Likewise, the Al-Masjid al-Nabawi and the city of Medina that surrounds it are also off-limits to those who do not practice Islam. For mosques in other areas, it has most commonly been taken that non-Muslims may only enter mosques if granted permission to do so by Muslims and if they have a legitimate reason. All entrants regardless of religious affiliation are expected to respect the rules and decorum for mosques.
In modern Turkey, non-Muslim tourists are allowed to enter any mosque, but there are some strict rules. Visiting a mosque is allowed only between prayers; visitors are required to wear long trousers and not to wear shoes, women must cover their heads; visitors are not allowed to interrupt praying Muslims, especially by taking photos of them; no loud talk is allowed; and no references to other religions are allowed (no crosses on necklaces, no cross gestures, etc.) Similar rules apply to mosques in Malaysia, where larger mosques that are also tourist attractions (such as the Masjid Negara) provide robes and headscarves for visitors who are deemed inappropriately attired.
In certain times and places, non-Muslims were expected to behave a certain way in the vicinity of a mosque: in some Moroccan cities, Jews were required to remove their shoes when passing by a mosque; in 18th-century Egypt, Jews and Christians had to dismount before several mosques in veneration of their sanctity.
The mosque played a major part in the spread of education in the Muslim World, and the association of the mosque with education remained one of its main characteristics throughout history, and, the school became an indispensable appendage to the mosque. From the earliest days of Islam, the mosque was the centre of the Muslim community, a place for prayer, meditation, religious instruction, political discussion, and a school. And anywhere Islam took hold, mosques were established, and basic religious and educational instruction began. Once established, mosques developed into well-known places of learning, often with hundreds, even thousands, of students, and frequently contained important libraries
In Iraq, pharmacology, engineering, astronomy and other subjects were taught in the mosques of Baghdad, and students came from Syria, Persia and India to learn these sciences. While at the Qarawiyin Mosque, there were courses on grammar, rhetoric, logic, mathematics, and astronomy, and possibly history, geography and chemistry

</doc>
<doc id="19895" url="https://en.wikipedia.org/wiki?curid=19895" title="Molecular cloud">
Molecular cloud

A molecular cloud, sometimes called a stellar nursery (if star formation is occurring within), is a type of interstellar cloud, the density and size of which permit the formation of molecules, most commonly molecular hydrogen (H2). This is in contrast to other areas of the interstellar medium that contain predominantly ionized gas.
Molecular hydrogen is difficult to detect by infrared and radio observations, so the molecule most often used to determine the presence of H2 is carbon monoxide (CO). The ratio between CO luminosity and H2 mass is thought to be constant, although there are reasons to doubt this assumption in observations of some other galaxies.
Occurrence.
Within the Milky Way, molecular gas clouds account for less than one percent of the volume of the interstellar medium (ISM), yet it is also the densest part of the medium, comprising roughly half of the total gas mass interior to the Sun's galactic orbit. The bulk of the molecular gas is contained in a ring between from the center of the Milky Way (the Sun is about 8.5 kiloparsecs from the center). Large scale CO maps of the galaxy show that the position of this gas correlates with the spiral arms of the galaxy. That molecular gas occurs predominantly in the spiral arms suggests that molecular clouds must form and dissociate on a timescale shorter than 10 million years—the time it takes for material to pass through the arm region.
Vertically to the plane of the galaxy, the molecular gas inhabits the narrow midplane of the galactic disc with a characteristic scale height, "Z", of approximately 50 to 75 parsecs, much thinner than the warm atomic ("Z "from 130 to 400 parsecs) and warm ionized ("Z "around 1000 parsecs) gaseous components of the ISM. The exception to the ionized-gas distribution are H II regions, which are bubbles of hot ionized gas created in molecular clouds by the intense radiation given off by young massive stars and as such they have approximately the same vertical distribution as the molecular gas.
This distribution of molecular gas is averaged out over large distances; however, the small scale distribution of the gas is highly irregular with most of it concentrated in discrete clouds and cloud complexes.
Types of molecular cloud.
Giant molecular clouds.
A vast assemblage of molecular gas with a mass of approximately 103 to 107 times the mass of the Sun is called a giant molecular cloud (GMC). GMCs are around 15 to 600 light-years in diameter (5 to 200 parsecs). Whereas the average density in the solar vicinity is one particle per cubic centimetre, the average density of a GMC is a hundred to a thousand times as great. Although the Sun is much more dense than a GMC, the volume of a GMC is so great that it contains much more mass than the Sun. The substructure of a GMC is a complex pattern of filaments, sheets, bubbles, and irregular clumps.
The densest parts of the filaments and clumps are called "molecular cores", while the densest molecular cores are called "dense molecular cores" and have densities in excess of 104 to 106 particles per cubic centimeter. Observationally, typical molecular cores are traced with CO and dense molecular cores are traced with ammonia. The concentration of dust within molecular cores is normally sufficient to block light from background stars so that they appear in silhouette as dark nebulae.
GMCs are so large that "local" ones can cover a significant fraction of a constellation; thus they are often referred to by the name of that constellation, e.g. the Orion Molecular Cloud (OMC) or the Taurus Molecular Cloud (TMC). These local GMCs are arrayed in a ring in the neighborhood of the Sun coinciding with the Gould Belt. The most massive collection of molecular clouds in the galaxy forms an asymmetrical ring about the galactic center at a radius of 120 parsecs; the largest component of this ring is the Sagittarius B2 complex. The Sagittarius region is chemically rich and is often used as an exemplar by astronomers searching for new molecules in interstellar space.
Small molecular clouds.
Isolated gravitationally-bound small molecular clouds with masses less than a few hundred times that of the Sun are called Bok globules. The densest parts of small molecular clouds are equivalent to the molecular cores found in GMCs and are often included in the same studies.
High-latitude diffuse molecular clouds.
In 1984 IRAS identified a new type of diffuse molecular cloud. These were diffuse filamentary clouds that are visible at high galactic latitudes. These clouds have a typical density of 30 particles per cubic centimeter.
Processes.
Star formation.
The formation of stars occurs exclusively within molecular clouds. This is a natural consequence of their low temperatures and high densities, because the gravitational force acting to collapse the cloud must exceed the internal pressures that are acting "outward" to prevent a collapse. There is observed evidence that the large, star-forming clouds are confined to a large degree by their own gravity (like stars, planets, and galaxies) rather than by external pressure. The evidence comes from the fact that the "turbulent" velocities inferred from CO linewidth scale in the same manner as the orbital velocity (a virial relation).
Physics.
The physics of molecular clouds are poorly understood and much debated. Their internal motions are governed by turbulence in a cold, magnetized gas, for which the turbulent motions are highly supersonic but comparable to the speeds of magnetic disturbances. This state is thought to lose energy rapidly, requiring either an overall collapse or a steady reinjection of energy. At the same time, the clouds are known to be disrupted by some process—most likely the effects of massive stars—before a significant fraction of their mass has become stars.
Molecular clouds, and especially GMCs, are often the home of astronomical masers.

</doc>
<doc id="19897" url="https://en.wikipedia.org/wiki?curid=19897" title="Minoru Yamasaki">
Minoru Yamasaki

Minoru Yamasaki (December 1, 1912February 6, 1986) was an American architect, best known for designing the original World Trade Center in New York City and several other large-scale projects. Yamasaki was one of the most prominent architects of the 20th century. He and fellow architect Edward Durell Stone are generally considered to be the two master practitioners of "New Formalism".
Early life and education.
Yamasaki was born in Seattle, Washington, a second-generation Japanese American, son of John Tsunejiro Yamasaki and Hana Yamasaki. He grew up in the slums of that city and was bullied for his ethnicity. The family moved to Auburn, Washington and he graduated from Garfield Senior High School in Seattle. He enrolled in the University of Washington program in architecture in 1929, and graduated with a Bachelor of Architecture (B.Arch.) in 1934. During his college years, he was strongly encouraged by faculty member Lionel Pries. He earned money to pay for his tuition by working at an Alaskan salmon cannery.
After moving to New York City in the 1930s, he enrolled at New York University for a master's degree in architecture and got a job with the architecture firm Shreve, Lamb & Harmon, designers of the Empire State Building. In 1945, Yamasaki moved to Detroit, where he was hired by Smith, Hinchman & Grylls. The firm helped Yamasaki avoid internment as a Japanese-American during World War II, and he himself sheltered his parents in New York City. Yamasaki left the firm in 1949, and started his own partnership. One of the first projects he designed at his own firm was Ruhl's Bakery at 7 Mile Road and Monica Street in Detroit. In 1964, Yamasaki received a D.F.A. from Bates College.
His firm, Yamasaki & Associates, closed on December 31, 2009.
Career.
His first internationally recognized design, the Pacific Science Center with its iconic arches, was constructed by the City of Seattle for the 1962 Seattle World's Fair. His first significant project was the Pruitt–Igoe housing project in St. Louis, Missouri, 1955. Despite his love of Japanese traditional design, this was a stark, modernist concrete structure. The housing project experienced so many problems that it was demolished in 1972, less than twenty years after its completion. Its destruction is considered by some to be the beginning of postmodern architecture.
In 1955, he also designed the "sleek" terminal at Lambert–St. Louis International Airport which led to his 1959 commission to design the Dhahran International Airport in Saudi Arabia. During this period, he created a number of office buildings which led to his innovative design of the towers of the World Trade Center in 1964, which began construction March 21, 1966. The first of the towers was finished in 1970. Many of his buildings feature superficial details inspired by the pointed arches of Gothic architecture, and make use of extremely narrow vertical windows. This narrow-windowed style arose from his own personal fear of heights. One particular design challenge of the World Trade Center's design related to the efficacy of the elevator system, which was unique in the world. Yamasaki was able to integrate the fastest elevators created at the time, running at 1,700 feet per minute. Then, instead of placing a large traditional elevator shaft in the core of each tower, Yamasaki created the Twin Towers' "Skylobby" system. The Skylobby design created three separate, connected elevator systems which would serve different segments of the building, depending on which floor was chosen, saving approximately 70% of the space that would have been used for a traditional shaft; the space saved was then used for office space.
It was in 1978 that Yamasaki also designed the Federal Reserve Bank tower in Richmond, Virginia. The work was designed with a similar appearance as the World Trade Center complex, with its narrow fenestration, and now stands at .
Yamasaki was a member of the Pennsylvania Avenue Commission, created in 1961 to restore the grand avenue in Washington, D.C., but resigned after disagreements and disillusionment with the design by committee approach.
After teaming up with Emery Roth and Sons on the design of the World Trade Center, they teamed up again on other projects including new defense buildings at Bolling Air Force Base in Washington, D.C.
Yamasaki designed two notable synagogues during this period, North Shore Congregation Israel in Glencoe, Illinois in 1964 and Temple Beth El, in Bloomfield Hills, Michigan, just north of Detroit in 1973. He also designed a number of buildings on the campus of Carleton College in Northfield, Minnesota between 1958 and 1968.
Personal life.
Yamasaki was first married in 1941 and had two other wives before marrying his first wife again in 1969. He died of stomach cancer in 1986. His son, Taro Yamasaki, is a Pulitzer Prize-winning photographer.

</doc>
<doc id="19898" url="https://en.wikipedia.org/wiki?curid=19898" title="Madeira">
Madeira

Madeira ( or ; or ) is a Portuguese archipelago situated in the north Atlantic Ocean, west and slightly south of Portugal. Its total population was estimated in 2011 at 267,785. The capital of Madeira is Funchal on the main island's south coast.
It is just under north of Tenerife, Canary Islands. Since 1976, the archipelago has been one of the two Autonomous regions of Portugal (the other being the Azores, located to the northwest). It includes the islands of Madeira, Porto Santo, and the Desertas, administered together with the separate archipelago of the Savage Islands. It is an outermost region of the European Union.
Madeira was claimed by Portuguese sailors in the service of Prince Henry the Navigator in 1419 and settled after 1420. The archipelago is considered to be the first territorial discovery of the exploratory period of the Portuguese Age of Discovery, which extended from 1415 to 1542.
Its southerly marine position renders the warmest year-round subtropical climate in Portugal, with winters being extremely mild and summers long but with relatively modest heat.
Today, it is a popular year-round resort, being visited every year by about one million tourists. The region is noted for its Madeira wine, gastronomy, historical and cultural value, its endemic flora and fauna, landscapes (Laurel forest) which are classified as a UNESCO World Heritage Site and embroidery artisans. Its annual New Year celebrations feature the largest fireworks show in the world, as officially recognised by "Guinness World Records" in 2006. The main harbour in Funchal is the leading Portuguese port in cruise liner dockings, being an important stopover for commercial and trans-Atlantic passenger cruises between Europe, the Caribbean and North Africa. Madeira is the second richest region of Portugal by GDP per capita, being only surpassed by Lisbon.
History.
Exploration.
Pliny mentioned certain "Purple Islands", their position corresponding to the location of the Fortunate Isles (or Canary Islands), that may have referred to islands of Madeira. Plutarch ("Sertorius", 75 AD) referring to the military commander Quintus Sertorius (d. 72 BC), relates that after his return to Cádiz: "The islands are said to be two in number separated by a very narrow strait and lie 10,000 furlongs (2,011.68 km) from Africa. They are called the Isles of the Blessed...". The estimated distance from Africa, , and the closeness of the two islands, seem to describe the similar position of the islands of Madeira and Porto Santo. 
Archeological evidence suggests that the islands may have been visited by the Vikings sometime between 900-1030 AD.
Legend.
During the reign of King Edward III of England, lovers Robert Machim and Anna d'Arfet were said to flee from England to France in 1346. They were driven off their course by a violent storm and their ship went aground along the coast of an island, that may have been Madeira. Later this legend was the basis of the naming of the city of Machico, in memory of the young lovers. Its name was transliterated from the name of the young man in the tale.
Discovery.
Knowledge of some Atlantic islands, such as Madeira, existed before their formal discovery and settlement, as the islands were shown on maps as early as 1339. From a portolan dating to 1351, and preserved in Florence, Italy, the islands of Madeira appeared to have been discovered long before Portuguese explorers reached them. 
In 1418, two captains under service to Prince Henry the Navigator, João Gonçalves Zarco and Tristão Vaz Teixeira, were driven off-course by a storm to an island which they named Porto Santo (English: "holy harbour") in gratitude for divine deliverance from a shipwreck. The following year, an organised expedition, under the captaincy of Zarco, Vaz Teixeira, and Bartolomeu Perestrello, traveled to the island to claim it on behalf of the Portuguese Crown. Subsequently, the new settlers observed "a heavy black cloud suspended to the southwest." Their investigation revealed it to be the larger island they called Madeira.
Settlement.
The first Portuguese settlers began colonizing the islands around 1420 or 1425. The three Captains-major had led the first settlement, along with their respective families, a small group of minor nobility, people of modest conditions, and some prisoners who could be trusted to work the lands. To make the island suitable for agriculture, they had to rough-hew a part of the dense forest of laurisilva and to construct a large number of canals (levadas). In some parts of the island there was excess water, while in others water was scarce. 
On 23 September 1433, the name "Ilha da Madeira" (English: "Madeira Island", or literally "island of wood") was first used in a document, followed by other papers and maps. 
Grain production began to fall and the ensuing crisis forced Henry the Navigator to order other commercial crops to be planted so that the islands could be profitable. The planting of sugarcane, and later Sicilian sugar beet, allowed the introduction of the "sweet salt" (as sugar was known) into Europe, where it was a rare and popular spice. These specialised plants, and their associated industrial technology, created one of the major revolutions on the islands and fuelled Portuguese industry. The expansion of sugar plantations in Madeira began in 1455, using advisers from Sicily and financed by Genoese capital. (Genoa acted as an integral part of the island economy until the 17th century). The accessibility of Madeira attracted Genoese and Flemish traders, who were keen to bypass Venetian monopolies.
Sugarcane production was the primary engine of the island's economy, increasing the demand for labour. African slaves were used during portions of the island's history to cultivate sugar cane, and the proportion of imported slaves reached 10% of the total population of Madeira by the 16th century.
Barbary corsairs from North Africa, who enslaved Europeans from ships and coastal communities throughout the Mediterranean region, captured 1,200 people in Porto Santo in 1617. After the 17th century, as Portuguese sugar production was shifted to Brazil, São Tomé and Príncipe and elsewhere, Madeira's most important commodity product became its wine. 
The British occupied Madeira as a result of the Napoleonic Wars, an agreed-upon occupation starting in 1807 and concluding in 1814 when the island was returned to Portugal. The island was designated as a British Crown Colony for four months, and Britain had intentions of keeping it after the Napoleonic Wars, owing to its strategic position, but plans for its permanent annexation were abandoned shortly after the start of the occupation.
After the death of King John VI of Portugal, his usurper son Miguel of Portugal seized power from the rightful heir, his niece Maria II, and proclaimed himself 'Absolute King.' Madeira held out for the queen under the governor José Travassos Valdez. Miguel sent an expeditionary force that overwhelmed the defence of the island. Valdez was forced to flee to England under the protection of the Royal Navy (September 1828). 
World War I.
On 31 December 1916 during the Great War, the German U-boat, "SM U-38," captained by Max Valentiner, entered Funchal harbour on Madeira; it torpedoed and sank three ships: "CS Dacia" (1,856 tons), "SS Kanguroo" (2,493 tons) and "Surprise" (680 tons), bringing the war to Portugal by extension. The commander of the French gunboat "Surprise" and 34 of her crew (including 7 Portuguese) died in the attack. The "Dacia," a British cable-laying vessel, had previously undertaken war work off the coast of Casablanca and Dakar. It was in the process of diverting the German South American cable into Brest, France. Following the attack on the ships, the Germans proceeded to bombard Funchal for two hours from a range of about . Batteries on Madeira returned fire and eventually forced the Germans to withdraw. 
On 12 December 1917, 2 German U-boats, "SM U-156" and "SM U-157" (captained by Max Valentiner) again bombarded Funchal. This time the attack lasted around 30 minutes. Forty, shells were fired. There were 3 fatalities and 17 wounded; a number of houses and Santa Clara church were hit.
Charles I, the last Emperor of the Austro-Hungarian Empire, went into exile in Madeira, after his second unsuccessful coup d'état in Hungary. He died there on 1 April 1922 and is buried in Monte. Charles had tried in 1917 to secretly enter into peace negotiations with France. Although his foreign minister, Ottokar Czernin, was interested only in negotiating a general peace to include Germany, Charles independently pursued a separate peace. He negotiated with the French using his brother-in-law, Prince Sixtus of Bourbon-Parma, an officer in the Belgian Army, as intermediary. When news of the overture leaked in April 1918, Charles denied involvement until the French Prime Minister Georges Clemenceau published letters signed by him. Czernin resigned and Austria-Hungary became more dependent in relation to its seemingly wronged German ally. Determined to prevent an attempt to restore Charles to the throne, the Council of Allied Powers agreed he could go into exile on Madeira because it was isolated in the Atlantic and easily guarded.
Autonomy and recent history.
On 1 July 1976, following the democratic revolution of 1974, Portugal granted political autonomy to Madeira, celebrated on Madeira Day. The region now has its own government and legislative assembly. 
In October 2012 it was reported that there was a dengue fever epidemic on the island. There was a total of 2,168 cases reported of dengue fever since the start in October 2012. The number of cases was on the decline since mid November 2012 and by 4 February 2013, no new cases had been reported.
Geography.
The archipelago of Madeira is located from the African coast and from the European continent (approximately a one-and-a-half hour flight from the Portuguese capital of Lisbon). It is found in the extreme south of the Tore-Madeira Ridge, a bathymetric structure of great dimensions oriented along a north-northeast to south-southwest axis that extends for . This submarine structure consists of long geomorphological relief that extends from the abyssal plain to 3500 metres; its highest submersend point is at a depth of about 150 metres (around latitude 36ºN). The origins of the Tore-Madeira Ridge are not clearly established, but may have resulted from a morphological "buckling" of the lithosphere.
Islands and islets.
Madeira (740.7 km2), including Ilhéu de Agostinho, Ilhéu de São Lourenço, Ilhéu Mole (northwest);
Porto Santo (42.5 km2), including Ilhéu de Baixo ou da Cal, Ilhéu de Ferro, Ilhéu das Cenouras, Ilhéu de Fora, Ilhéu de Cima;
Desertas Islands (14.2 km2), including the three uninhabited islands: Deserta Grande Island, Bugio Island and Ilhéu de Chão;
Savage Islands (3.6 km2), archipelago 280 km south-southeast of Madeira Island including three main islands and 16 uninhabited islets in two groups: the Northwest Group (Selvagem Grande Island, Ilhéu de Palheiro da Terra, Ilhéu de Palheiro do Mar) and the Southeast Group (Selvagem Pequena Island, Ilhéu Grande, Ilhéu Sul, Ilhéu Pequeno, Ilhéu Fora, Ilhéu Alto, Ilhéu Comprido, Ilhéu Redondo, Ilhéu Norte).
Madeira Island.
The island of Madeira is at the top of a massive shield volcano that rises about from the floor of the Atlantic Ocean, on the Tore underwater mountain range. The volcano formed atop an east-west rift in the oceanic crust along the African Plate, beginning during the Miocene epoch over 5 million years ago, continuing into the Pleistocene until about 700,000 years ago. This was followed by extensive erosion, producing two large amphitheatres open to south in the central part of the island. Volcanic activity later resumed, producing scoria cones and lava flows atop the older eroded shield. The most recent volcanic eruptions were on the west-central part of the island only 6,500 years ago, creating more cinder cones and lava flows.
Madeira Island represents 93% of the archipelago's area, with 90% of the landmass above 500 m. It is the largest island of the group with an area of , a length of (from Ponte de São Lourenço to Ponte do Pargo), while approximately at its widest point (from Ponte da Cruz to Ponte São Jorge), with a coastline of . It has a mountain ridge that extends along the centre of the island, reaching at its highest point (Pico Ruivo), while much lower (below 200 meters) along its eastern extent. The primitive volcanic foci responsible for the central mountainous area, consisted of the peaks: Ruivo (1862 meters), Torres (1851 meters), Arieiro (1818 meters), Cidrão (1802 meters), Cedro (1759 meters), Casado (1725 meters), Grande (1657 meters), Ferreiro (1582 meters). At the end of this eruptive phase, an island circled by reefs was formed, its marine vestiges are evident in a calcareous layer in the area of Lameiros, in São Vicente (which was later explored for calcium oxide production). Sea cliffs, such as Cabo Girão, valleys and ravines extend from this central spine, making the interior generally inaccessible. Daily life is concentrated in the many villages at the mouths of the ravines, through which the heavy rains of autumn and winter usually travel to the sea.
Climate.
Madeira has been classified as a Mediterranean climate (Köppen climate classification: "Csa"/"Csb"). Based on differences in sun exposure, humidity, and annual mean temperature, there are clear variations between north- and south-facing regions, as well as between some islands. Other microclimates are expected to exist, from the constantly humid wettest points of the mountains to the desert and arid Selvagens islands. The islands are strongly influenced by the Gulf Stream and Canary Current, giving mild year-round temperatures; according to the Instituto de Meteorologia (IM), the average annual temperature at Funchal weather station is for the 1980–2010 period. For the 1960–1990 period, IM published an article showing that some regions in the South Coastline surpass in annual average. Porto Santo has at least one weather station with a semiarid climate ("BSh"). On the highest windward slopes of Madeira, rainfall exceeds 50 inches per year, mostly falling between October and April.
Wildfires.
Drought conditions, coupled with hot and windy weather in summer, have caused numerous wildfires in recent years. The largest of the fires in August 2010 burned through 95 percent of the Funchal Ecological Park, a 1,000-hectare preserve set aside to restore native vegetation to the island. 
In July 2012 Madeira was suffering again from severe drought. Wildfires broke out on July 18, in the midst of temperatures up to 40 degrees Celsius (more than 100 degrees Fahrenheit) and high winds. By July 20, fires had spread to the nearby island of Porto Santo, and firefighters were sent from mainland Portugal to contain the multiple blazes. Numerous residents had to be evacuated and firefighters were sent from the mainland to help battle the fires.
In August 2013 a hospital and some private homes were evacuated as a wildfire approached Funchal. A number of homes were destroyed when the fire hit Monte, a suburb of Funchal.
Biome.
The Macaronesia region harbours an important floral diversity. In fact, the archipelago's forest composition and maturity are quite similar to the forests found in the Tertiary period that covered Southern Europe and Northern Africa millions of years ago. The great biodiversity of Madeira is phytogeographically linked to the Mediterranean region, Africa, America and Australia, and interest in this phytogeography has been increasing in recent years due to the discovery of some epiphytic bryophyte species with non-adjacent distribution.
Madeira also has many endemic species of fauna – mostly invertebrates which include the extremely rare Madeiran large white butterfly but also some vertebrates such as the native bat, some lizards species, and some birds as already mentioned. The biggest tarantula of Europe is found on Desertas islands of Madeira and can be as wide as a man's hand. These islands have more than 250 species of land molluscs (snails and slugs), some with very unusual shell shape and colours, most of which are endemic and vulnerable.
Madeira has three endemic bird species: Zino's petrel, the Trocaz pigeon and the Madeira firecrest, while the Madeiran chaffinch is an endemic subspecies. It is also important for breeding seabirds, including the Madeiran storm-petrel, Barolo shearwater and Cory's shearwater.
Native birds gallery.
In the south, there is very little left of the indigenous subtropical rainforest which once covered the whole island (the original settlers set fire to the island to clear the land for farming) and gave it the name it now bears ("Madeira" means "wood" in Portuguese). However, in the north, the valleys contain native trees of fine growth. These "laurisilva" forests, called "lauraceas madeirense", notably the forests on the northern slopes of Madeira Island, are designated as a World Heritage Site by UNESCO. The critically endangered vine "Jasminum azoricum" is one of the plant species that is endemic to Madeira.
Levadas.
The island of Madeira is wet in the northwest but dry in the southeast. In the 16th century the Portuguese started building levadas or aqueducts to carry water to the agricultural regions in the south. The most recent were built in the 1940s. Madeira is very mountainous, and building the levadas was difficult and often sentenced criminals or slaves were used. Many are cut into the sides of mountains, and it was also necessary to dig of tunnels, some of which are still accessible.
Today the levadas not only supply water to the southern parts of the island but provide hydro-electric power. There are over of levadas and they provide a network of walking paths. Some provide easy and relaxing walks through the countryside, but others are narrow, crumbling ledges where a slip could result in serious injury or death.
Two of the most popular levadas to hike are the "Levada do Caldeirão Verde" and the "Levada do Caldeirão do Inferno" which should not be attempted by hikers prone to vertigo or without torches and helmets. The "Levada do Caniçal" is a much easier walk, running from Maroços to the "Caniçal Tunnel". It is known as the "mimosa levada" because mimosa trees are found all along the route.
Governance.
Administratively, Madeira (with a population of 267,302 inhabitants in 2011) and covering an area of is organised into eleven municipalities:
Funchal.
Funchal is the capital and principal city of the Madeira Autonomous Region, located along the southern coast of the island of Madeira. It is a modern city, located within a natural geological "amphitheatre" composed of vulcanological structure and fluvial hydrological forces. Beginning at the harbour (Porto de Funchal), the neighbourhoods and streets rise almost , along gentle slopes that helped to provide a natural shelter to the early settlers.
Population.
Demographics.
When the Portuguese discovered the island of Madeira in 1419, it was uninhabited by humans, with no aboriginal population. The island was settled by Portuguese people, especially farmers from the Minho region, meaning that "Madeirans" (), as they are called, are ethnically Portuguese, though they have developed their own distinct regional identity and cultural traits.
The region has a total population of just under 270,000, the majority of whom live on the main island of Madeira where the population density is 337/km2; meanwhile only around 5,000 live on the Porto Santo Island where the population density is 112/km2.
Diaspora.
Madeiran immigrants in North America mostly clustered in the New England and mid-Atlantic states, Toronto, Northern California, and Hawaii. They also settled in Rhode Island and Massachusetts to participate in the flourishing American whaling industry. By 1980, the U.S. Census registered more than a million Americans of Portuguese descent, a large portion Madeirans. The city of New Bedford is especially rich in Madeirans, hosting the Museum of Madeira Heritage, as well as the annual Madeiran and Luso-American celebration, the Feast of the Blessed Sacrament, the world's largest celebration of Madeiran heritage, regularly drawing crowds of tens of thousands to the city's Madeira Field.
In 1846 when a famine struck Madeira over 6,000 of the inhabitants migrated to British Guiana. In 1891 they numbered 4.3% of the population. In 1902 in Honolulu, Hawaii there were 5,000 Portuguese people, mostly Madeirans. In 1910 this grew to 21,000.
1849 saw an emigration of Protestant religious exiles from Madeira to the United States, by way of Trinidad and other locations in the West Indies. Most of them settled in Illinois with financial and physical aid of the American Protestant Society, headquartered in New York City. In the late 1830s the Reverend Robert Reid Kalley, from Scotland, a Presbyterian minister as well as a physician, made a stop at Funchal, Madeira on his way to a mission in China, with his wife, so that she could recover from an illness. The Rev. Kalley and his wife stayed on Madeira where he began preaching the Protestant gospel and converting islanders from Catholicism. Eventually, the Rev. Kalley was arrested for his religious conversion activities and imprisoned. Another missionary from Scotland, William Hepburn Hewitson, took on Protestant ministerial activities in Madeira. By 1846, about 1,000 Protestant Madeirenses, who were discriminated against and the subjects of mob violence because of their religious conversions, chose to immigrate to Trinidad and other locations in the West Indies in answer for a call for sugar plantation workers. The Madeirenses exiles did not fare well in the West Indies. The tropical climate was unfamiliar and they found themselves in serious economic difficulties. By 1848, the American Protestant Society raised money and sent the Rev. Manuel J. Gonsalves, a Baptist minister and a naturalized U.S. citizen from Madeira, to work with the Rev. Arsenio da Silva, who had emigrated with the exiles from Madeira, to arrange to resettle those who wanted to come to the United States. The Rev. da Silva died in early 1849. Later in 1849, the Rev. Gonsalves was then charged with escorting the exiles from Trinidad to be settled in Sangamon and Morgan counties in Illinois on land purchased with funds raised by the American Protestant Society. Accounts state that anywhere from 700 to 1,000 exiles came to the United States at this time.
There are several large Madeiran communities around the world, such is the great number in the UK, including Jersey, the Portuguese British community mostly made up of Madeirans celebrate Madeira Day.
Economy.
The setting-up of a free trade zone has led to the installation, under more favourable conditions, of infrastructure, production shops and essential services for small and medium-sized industrial enterprises. The Madeira Free Trade Zone, also called the Madeira International Business Centre, being a tax-privileged economic area, provides an incentive for companies, offering them financial and tax advantages via a whole range of activities exercised in the Industrial Free Zone, the Off-Shore Financial Centre, the International Shipping Register organisation, and the International Service Centre.
The services sector makes the largest contribution to the formation of the regional gross value added as opposed to the agricultural sector, for which the share has continuously declined in the regional economy. 
The largest industries are by sector food, beverages (especially Madeira wine), and construction. 
Tourism.
Tourism is an important sector in the region's economy since it contributes 20% to the region's GDP, providing support throughout the year for commercial, transport and other activities and constituting a significant market for local products. The share in Gross Value Added of hotels and restaurants (9%) also highlights this phenomenon. The island of Porto Santo, with its long beach and its climate, is entirely devoted to tourism.
Visitors are mainly from the European Union, with German, British, Scandinavian and Portuguese tourists providing the main contingents. The average annual occupancy rate was 60.3% in 2008, reaching its maximum in March and April, when it exceeds 70%.
Whale watching.
Whale watching has become very popular in recent years. Many species of dolphins such as common dolphin, spotted dolphin, striped dolphin, bottlenose dolphin, short-finned pilot whale, and whales such as Bryde's whale, Sei whale, fin whale, sperm whale, beaked whales can be spotted near the coast or offshore.
Immigration.
Madeira is part of the Schengen Area. European Union and EFTA citizens can enter the islands freely, while those from other regions need identification.
There were in 2009, 7,105 legal immigrants living in Madeira Islands. They come mostly from Brazil (1,300), the United Kingdom (912), Venezuela (732) and Ukraine (682), according to SEF. But in 2013, that number dropped to 5,829, also according to SEF.
Transport.
The Islands have two airports, Madeira Airport and Porto Santo Airport, on the islands of Madeira and Porto Santo respectively. From Madeira Airport the most frequent flights are to Lisbon. There are also direct flights to over 30 other airports in Europe and nearby islands.
Transport between the two main islands is by plane, or ferries from the Porto Santo Line, the latter also carrying vehicles. Visiting the interior of the islands is now easy thanks to construction of the "Vias Rápidas", major roads built during Portugal's economic boom. Modern roads reach all points of interest on the islands.
Funchal has an extensive public transportation system. Bus companies, including Horários do Funchal which has been operating for over a hundred years, have regularly scheduled routes to all points of interest on the island.
Culture.
Music.
Folklore music in Madeira is widespread and mainly uses local musical instruments such as the machete, rajao, brinquinho and cavaquinho, which are used in traditional folkloric dances like the "bailinho da Madeira".
Emigrants from Madeira also influenced the creation of new musical instruments. In the 1880s, the ukulele was created, based on two small guitar-like instruments of Madeiran origin, the cavaquinho and the rajao. The ukulele was introduced to the Hawaiian Islands by Portuguese immigrants from Madeira and Cape Verde. Three immigrants in particular, Madeiran cabinet makers Manuel Nunes, José do Espírito Santo, and Augusto Dias, are generally credited as the first ukulele makers. Two weeks after they disembarked from the "SS Ravenscrag" in late August 1879, the "Hawaiian Gazette" reported that "Madeira Islanders recently arrived here, have been delighting the people with nightly street concerts."
Cuisine.
Because of the geographic situation of Madeira in the Atlantic Ocean, the island has an abundance of fish of various kinds. The species that are consumed the most are espada (black scabbardfish), blue fin tuna, white marlin, blue marlin, albacore, bigeye tuna, wahoo, spearfish, skipjack tuna and many others are found in the local dishes as they are found up and down the coast of Madeira. Espada is often served with banana. Bacalhau is also popular just like in Portugal.
There are many meat dishes on Madeira, one of the most popular being espetada. Espetada is traditionally made of large chunks of beef rubbed in garlic, salt and bay leaf and marinated for 4 to 6 hours in Madeira wine, red wine vinegar and olive oil then skewered onto a bay laurel stick and left to grill over smouldering wood chips. These are so integral a part of traditional eating habits that a special iron stand is available with a T-shaped end, each branch of the "T" having a slot in the middle to hold a brochette (espeto in Portuguese); a small plate is then placed underneath to collect the juices. The brochettes are very long and have a V-shaped blade in order to pierce the meat more easily. It is usually accompanied with the local bread called bolo do caco.
Other popular dishes in Madeira include açorda, feijoada, carne de vinha d'alhos.
Traditional pastries in Madeira usually contain local ingredients, one of the most common being "mel de cana", literally "sugarcane honey" (molasses). The traditional cake of Madeira is called "Bolo de Mel", which translates as (Sugarcane) "Honey Cake" and according to custom, is never cut with a knife, but broken into pieces by hand. It is a rich and heavy cake. The cake commonly well known as "Madeira Cake" in England also finds its naming roots in the Island of Madeira.
Malasadas are a Madeiran creation which were taken around the world by emigrants to places such as Hawaii. In Madeira, Malasadas are mainly consumed during the Carnival of Madeira. Pastéis de nata, as in the rest of Portugal, are also very popular.
Milho frito is a very popular dish in Madeira which is very similar to the Italian dish polenta. Açorda Madeirense is another popular local dish.
Beverages.
Madeira is a fortified wine, produced in the Madeira Islands; varieties may be sweet or dry. It has a history dating back to the Age of Exploration when Madeira was a standard port of call for ships heading to the New World or East Indies. To prevent the wine from spoiling, neutral grape spirits were added. However, wine producers of Madeira discovered, when an unsold shipment of wine returned to the islands after a round trip, that the flavour of the wine had been transformed by exposure to heat and movement. Today, Madeira is noted for its unique winemaking process which involves heating the wine and deliberately exposing the wine to some levels of oxidation. Most countries limit the use of the term "Madeira" to those wines that come from the Madeira Islands, to which the European Union grants Protected Designation of Origin (PDO) status.
A local beer called Coral is produced by the Madeira Brewery, which dates from 1872. Other alcoholic drinks are also popular in Madeira, such as the locally created Poncha, Niquita, Pé de Cabra, Aniz, as well as Portuguese drinks such as Macieira Brandy, Licor Beirão.
Laranjada is a type of carbonated soft drink with an orange flavour, its name being derived from the Portuguese word "laranja" ("orange"). Launched in 1872 it was the first soft drink to be produced in Portugal, and remains very popular to the present day. Brisa drinks, a brand name, are also very popular and come in a range of flavours.
There is also a huge coffee culture in Madeira. Like in mainland Portugal, popular coffee-based drinks include Garoto, Galão, Bica, Café com Cheirinho, Mazagran, Chinesa and many more.
Sister provinces.
Madeira Island has the following sister provinces:
Postage stamps.
Portugal has issued postage stamps for Madeira during several periods, beginning in 1868.
Notable citizens.
The following people were either born or have lived part of their lives in Madeira:

</doc>
<doc id="19901" url="https://en.wikipedia.org/wiki?curid=19901" title="M16 rifle">
M16 rifle

The M16 rifle, officially designated "Rifle, Caliber 5.56 mm, M16", is a United States military adaptation of the ArmaLite AR-15 rifle. The original M16 was a select-fire, 5.56×45mm rifle with a 20-round magazine.
In 1963, the M16 entered American military service and was deployed for jungle warfare operations during the Vietnam War. In 1969, the M16A1 replaced the M14 rifle to become the U.S. military's standard service rifle. The M16A1 improvements include a bolt-assist, chromed plated bore and a new 30-round magazine. In 1983, the USMC adopted the M16A2 rifle and the U.S. Army adopted it in 1986. The M16A2 fires the improved 5.56×45mm NATO (M855/SS109) cartridge and has a new adjustable rear sight, case deflector, heavy barrel, improved handguard, pistol grip and buttstock, as well as a semi-auto and three-round burst only fire selector. Adopted in 1998, the M16A4 is the fourth generation of the M16 series. It is equipped with a removable carrying handle and Picatinny rail for mounting optics and other ancillary devices.
The M16 has also been widely adopted by other militaries around the world. Total worldwide production of M16s has been approximately 8 million, making it the most-produced firearm of its 5.56 mm caliber. The U.S. Army has largely replaced the M16 in combat units with the shorter and lighter M4 carbine, and the U.S. Marine Corps approved a similar change in October 2015.
History.
Background.
After World War II, the United States military started looking for a single automatic rifle to replace the M1 Garand, M1/M2 Carbines, M1918 Browning Automatic Rifle, M3 "Grease Gun" and Thompson submachine gun. However, early experiments with select-fire versions of the M1 Garand proved disappointing. During the Korean War, the select-fire M2 Carbine largely replaced the submachine gun in US service and became the most widely used Carbine variant. However, combat experience suggested that the .30 Carbine round was under-powered. American weapons designers concluded that an intermediate round was necessary, and recommended a small-caliber, high-velocity cartridge.
However, senior American commanders having faced fanatical enemies and experienced major logistical problems during WWII and the Korean War, insisted that a single powerful .30 caliber cartridge be developed, that could not only be used by the new automatic rifle, but by the new general-purpose machine gun (GPMG) in concurrent development. This culminated in the development of the 7.62×51mm NATO cartridge and the M14 rifle which was an improved M1 Garand with a 20-round magazine and automatic fire capability. The U.S. also adopted the M60 general purpose machine gun (GPMG). Its NATO partners adopted the FN FAL and HK G3 rifles, as well as the FN MAG and Rheinmetall MG3 GPMGs.
The first confrontations between the AK-47 and the M14 came in the early part of the Vietnam War. Battlefield reports indicated that the M14 was uncontrollable in full-auto and that soldiers could not carry enough ammo to maintain fire superiority over the AK-47. And, while the M2 Carbine offered a high rate of fire, it was under-powered and ultimately outclassed by the AK-47. A replacement was needed: A medium between the traditional preference for high-powered rifles such as the M14, and the lightweight firepower of the M2 Carbine.
As a result, the Army was forced to reconsider a 1957 request by General Willard G. Wyman, commander of the U.S. Continental Army Command (CONARC) to develop a .223 caliber (5.56 mm) select-fire rifle weighing 6 lb (2.7 kg) when loaded with a 20-round magazine. The 5.56mm round had to penetrate a standard U.S. helmet at 500 yards (460 meters) and retain a velocity in excess of the speed of sound, while matching or exceeding the wounding ability of the .30 Carbine cartridge.
This request ultimately resulted in the development of a scaled-down version of the Armalite AR-10, called AR-15 rifle. However, despite overwhelming evidence that the AR-15 could bring more firepower to bear than the M14, the Army opposed the adoption of the new rifle. In January 1963, Secretary of Defense Robert McNamara concluded that the AR-15 was the superior weapon system and ordered a halt to M14 production. At the time, the AR-15 was the only rifle available that could fulfill the requirement of a universal infantry weapon for issue to all services.
After modifications (most notably, the charging handle was re-located from under the carrying handle like AR-10 to the rear of the receiver), the new redesigned rifle was subsequently adopted as the M16 Rifle. "(The M16) was much lighter compared to the M14 it replaced, ultimately allowing Soldiers to carry more ammunition. The air-cooled, gas-operated, magazine-fed assault rifle was made of steel, aluminum alloy and composite plastics, truly cutting-edge for the time. Designed with full and semi-automatic capabilities, the weapon initially did not respond well to wet and dirty conditions, sometimes even jamming in combat. After a few minor modifications, the weapon gained in popularity among troops on the battlefield."
Adoption.
In July 1960, General Curtis LeMay was impressed by a demonstration of the ArmaLite AR-15. In the summer of 1961, General LeMay was promoted to United States Air Force, Chief of Staff, and requested 80,000 AR-15s. However, General Maxwell D. Taylor, Chairman of the Joint Chiefs of Staff, advised President John F. Kennedy that having "two" different calibers within the military system at the same time would be problematic and the request was rejected. In October 1961, William Godel, a senior man at the Advanced Research Projects Agency, sent 10 AR-15s to South Vietnam. The reception was enthusiastic, and in 1962, another 1,000 AR-15s were sent. United States Army Special Forces personnel filed battlefield reports lavishly praising the AR-15 and the stopping-power of the 5.56 mm cartridge, and pressed for its adoption.
The damage caused by the 5.56 mm bullet was originally believed to be caused by "tumbling" due to the slow 1 in rifling twist rate. However, any pointed lead core bullet will "tumble" after penetration in flesh, because the center of gravity is towards the rear of the bullet. The large wounds observed by soldiers in Vietnam were actually caused by bullet fragmentation, which was created by a combination of the bullet's velocity and construction. These wounds were so devastating, that the photographs remained classified into the 1980s.
U.S. Secretary of Defense Robert McNamara now had two conflicting views: the ARPA report favoring the AR-15 and the Army's position favoring the M14. Even President Kennedy expressed concern, so McNamara ordered Secretary of the Army Cyrus Vance to test the M14, the AR-15 and the AK-47. The Army reported that only the M14 was suitable for service, but Vance wondered about the impartiality of those conducting the tests. He ordered the Army Inspector General to investigate the testing methods used; the Inspector General confirmed that the testers were biased towards the M14.
In January 1963, Secretary McNamara received reports that M14 production was insufficient to meet the needs of the armed forces and ordered a halt to M14 production. At the time, the AR-15 was the only rifle that could fulfill a requirement of a "universal" infantry weapon for issue to all services. McNamara ordered its adoption, despite receiving reports of several deficiencies, most notably the lack of a chrome-lined barrel.
After modifications (most notably, the charging handle was re-located from under the carrying handle like AR-10 to the rear of the receiver), the new redesigned rifle was renamed the "Rifle, Caliber 5.56 mm, M16". Inexplicably, the modification to the new M16 did not include a chrome-lined barrel. Meanwhile, the Army relented and recommended the adoption of the M16 for jungle warfare operations. However, the Army insisted on the inclusion of a forward assist to help push the bolt into battery in the event that a cartridge failed to seat into the chamber. The Air Force, Colt and Eugene Stoner believed that the addition of a forward assist was an unjustified expense. As a result, the design was split into two variants: the Air Force's M16 without the forward assist, and the XM16E1 with the forward assist for the other service branches.
In November 1963, McNamara approved the U.S. Army's order of 85,000 XM16E1s; and to appease General LeMay, the Air Force was granted an order for another 19,000 M16s. In 1964, the Army accepted delivery of the first batch of 2129 rifles and an additional 57,240 rifles the following year.
In 1964, the Army was informed that DuPont could not mass-produce the IMR 4475 stick powder to the specifications demanded by the M16. Therefore, Olin Mathieson Company provided a high-performance ball propellant. While the Olin WC 846 powder achieved the desired per second muzzle velocity, it produced much more fouling, that quickly jammed the M16s action (unless the rifle was cleaned well and often).
In March 1965, the Army began to issue the XM16E1 to infantry units. However, the rifle was initially delivered without adequate cleaning kits or instructions because Colt had claimed the M16 was self-cleaning. As a result, reports of stoppages in combat began to surface. The most severe problem, was known as "failure to extract"—the spent cartridge case remained lodged in the chamber after the rifle was fired. Documented accounts of dead U.S. troops found next to disassembled rifles eventually led to a Congressional investigation.
In February 1967, the improved XM16E1 was standardized as the M16A1. The new rifle was given a chrome-lined barrel to eliminate corrosion and stuck cartridges, as well as other minor modifications. New cleaning kits, powder solvents and lubricants were also issued. Intensive training programs in weapons cleaning were instituted including a comic book-style operations manual. As a result, reliability problems quickly diminished and the M16A1 rifle achieved widespread acceptance by U.S. troops in Vietnam. However the M16 rifle continued to suffer from a reputation of unreliability for some time.
In 1969, the M16A1 officially replaced the M14 rifle to become the U.S. military's standard service rifle. In 1970, the new WC 844 powder was introduced to reduce fouling.
Reliability.
During the early part of its career, the M16 had a reputation for poor reliability and a malfunction rate of two per 1000 rounds fired. The M16 uses a unique gas powered operating system. This gas operating system works by passing high pressure propellant gasses tapped from the barrel down a tube and into the carrier group within the upper receiver, and is commonly but incorrectly referred to as a "direct impingement gas system" system. The gas expands within a donut shaped gas cylinder within the carrier. Because the bolt is prevented from moving forward by the barrel, the carrier is driven to the rear by the expanding gasses and thus converts the energy of the gas to movement of the rifle’s parts. The bolt bears a piston head and the cavity in the bolt carrier is the piston sleeve. It is more correct to call it an "internal piston" system." This design is much lighter and more compact than a gas-piston design. However, this design requires that combustion byproducts from the discharged cartridge be blown into the receiver as well. This accumulating carbon and vaporized metal build-up within the receiver and bolt-carrier negatively affects reliability and necessitates more intensive maintenance on the part of the individual soldier. The channeling of gasses into the bolt carrier during operation increases the amount of heat that is deposited in the receiver while firing the M16 and causes essential lubricant to be "burned off". This requires frequent and generous applications of appropriate lubricant. Lack of proper lubrication is the most common source of weapon stoppages or jams.
The original M16 fared poorly in the jungles of Vietnam and was infamous for reliability problems in the harsh environment. As a result, it became the target of a Congressional investigation. The investigation found that:
When these issues were addressed and corrected by the M16A1, the reliability problems decreased greatly. According to a 1968 Department of Army report, the M16A1 rifle achieved widespread acceptance by U.S. troops in Vietnam. "Most men armed with the M16 in Vietnam rated this rifle's performance high, however, many men entertained some misgivings about the M16's reliability. When asked what weapon they preferred to carry in combat, 85 percent indicated that they wanted either the M16 or its submachine gun version, the XM177E2. (The M14 was preferred by 15 percent, while less than one percent wished to carry either the Stoner rifle, the AK-47, the carbine or a pistol.)" In March 1970, the "President’s Blue Ribbon Defense Panel" concluded that the issuance of the M16 saved the lives of 20,000 U.S. servicemen during the Vietnam War, who would have otherwise died had the M14 remained in service. However the M16 rifle continued to suffer from a reputation of unreliability for some time.
After the introduction of the M4 Carbine, it was found that the shorter barrel length of 14.5 inches also has a negative effect on reliability, as the gas port is located closer to the chamber than the gas port of the standard length M16 rifle: 7.5 inches instead of 13 inches. This affects the M4’s timing and increases the amount of stress and heat on the critical components, thereby reducing reliability. In a 2002 assessment the USMC found that the M4 malfunctioned three times more often than the M16A4 (the M4 failed 186 times for 69,000 rounds fired, while the M16A4 failed 61 times). Thereafter, the Army and Colt worked to make modifications to the M4s and M16A4s in order to address the problems found. In tests conducted in 2005 and 2006 the Army found that on average, the new M4s and M16s fired approximately 5,000 rounds between stoppages.
In December 2006, the Center for Naval Analyses (CNA) released a report on U.S. small arms in combat. The CNA conducted surveys on 2,608 troops returning from combat in Iraq and Afghanistan over the past 12 months. Only troops who fired their weapons at enemy targets were allowed to participate. 1,188 troops were armed with M16A2 or A4 rifles, making up 46 percent of the survey. 75 percent of M16 users (891 troops) reported they were satisfied with the weapon. 60 percent (713 troops) were satisfied with handling qualities such as handguards, size, and weight. Of the 40 percent dissatisfied, most were with its size. Only 19 percent of M16 users (226 troops) reported a stoppage, while 80 percent of those that experienced a stoppage said it had little impact on their ability to clear the stoppage and re-engage their target. Half of the M16 users never experienced failures of their magazines to feed. 83 percent (986 troops) did not need their rifles repaired while in theater. 71 percent (843 troops) were confident in the M16's reliability, defined as level of soldier confidence their weapon will fire without malfunction, and 72 percent (855 troops) were confident in its durability, defined as level of soldier confidence their weapon will not break or need repair. Both factors were attributed to high levels of soldiers performing their own maintenance. 60 percent of M16 users offered recommendations for improvements. Requests included greater bullet lethality, new-built rifles instead of rebuilt, better quality magazines, decreased weight, and a collapsible stock. Some users recommended shorter and lighter weapons such as M4 carbine. Some issues have been addressed with the issuing of the Improved STANAG magazine in March 2009, and the M855A1 Enhanced Performance Round in June 2010.
In early 2010, two journalists from the New York Times spent three months with soldiers and Marines in Afghanistan. While there, they questioned around 100 infantry troops about the reliability of their M16 rifles, as well as the M4 carbine. The troops did not report reliability problems with their rifles. While only 100 troops were asked, they engaged in daily fighting in Marja, including least a dozen intense engagements in Helmand Province, where the ground is covered in fine powdered sand (called "moon dust" by troops) that can stick to firearms. Weapons were often dusty, wet, and covered in mud. Intense firefights lasted hours with several magazines being expended. Only one soldier reported a jam when his M16 was covered in mud after climbing out of a canal. The weapon was cleared and resumed firing with the next chambered round. Furthermore, the Marine Chief Warrant Officer responsible for weapons training and performance of the Third Battalion, Sixth Marines, reported that "We've had nil in the way of problems; we've had no issues." with his battalion's 350 M16s and 700 M4s.
NATO standards.
In March 1970, the U.S. recommended that all NATO forces adopt the 5.56×45mm cartridge. This shift represented a change in the philosophy of the military's long-held position about caliber size. By the mid 1970s, other armies were looking at M16-style weapons. A NATO standardization effort soon started and tests of various rounds were carried out starting in 1977. The U.S. offered the 5.56×45mm M193 round, but there were concerns about its penetration in the face of the wider introduction of body armor. In the end the Belgian 5.56×45mm SS109 round was chosen (STANAG 4172) in October 1980. The SS109 round was based on the U.S. cartridge but included a new stronger, heavier, 62 grain bullet design, with better long range performance and improved penetration (specifically, to consistently penetrate the side of a steel helmet at 600 meters). Due to its design and lower muzzle velocity (about 3110 ft/s) the Belgian SS109 round is considered more humane because it is less likely to fragment than the U.S. M193 round. The NATO 5.56×45mm standard ammunition produced for U.S. forces is designated M855.
In October 1980, shortly after NATO accepted the 5.56×45mm NATO rifle cartridge. Draft Standardization Agreement 4179 (STANAG 4179) was proposed to allow NATO members to easily share rifle ammunition and magazines down to the individual soldier level. The magazine chosen to become the "STANAG magazine" was originally designed for the U.S. M16 rifle. Many NATO member nations, but not all, subsequently developed or purchased rifles with the ability to accept this type of magazine. However, the standard was never ratified and remains a 'Draft STANAG'.
The NATO Accessory Rail STANAG 4694, or Picatinny rail STANAG 2324, or a "Tactical Rail" is a bracket used on M16 type rifles to provide a standardized mounting platform. The rail comprises a series of ridges with a T-shaped cross-section interspersed with flat "spacing slots". Scopes are mounted either by sliding them on from one end or the other; by means of a "rail-grabber" which is clamped to the rail with bolts, thumbscrews or levers; or onto the slots between the raised sections. The rail was originally for scopes. However, once established, the use of the system was expanded to other accessories, such as tactical lights, laser aiming modules, night vision devices, reflex sights, foregrips, bipods, and bayonets.
Currently, the M16 is in use by 15 NATO countries and more than 80 countries worldwide.
Design.
The M16 is a lightweight, 5.56 mm, air-cooled, gas-operated, magazine-fed assault rifle, with a rotating bolt. The M16's receivers are made of 7075 aluminum alloy, its barrel, bolt, and bolt carrier of steel, and its handguards, pistol grip, and buttstock of plastics.
The M16A1 was especially lightweight at with a loaded 30-round magazine. This was significantly less than the M14 that it replaced at with a loaded 20-round magazine. It is also lighter when compared to the AKMs with a loaded 30-round magazine.
The M16A2 weighs loaded with a 30-round magazine, because of the adoption of a thicker barrel profile. The thicker barrel is more resistant to damage when handled roughly and is also slower to overheat during sustained fire. Unlike a traditional "bull" barrel that is thick its entire length, the M16A2's barrel is only thick forward of the handguards. The barrel profile under the handguards remained the same as the M16A1 for compatibility with the M203 grenade launcher.
Barrel.
Early model M16 barrels had a rifling twist of 4 grooves, right hand twist, 1 turn in 14 inches (1:355.6 mm) bore - as it was the same rifling used by the .222 Remington sporting round. This was shown to make the light .223 Remington bullet yaw in flight at long ranges and it was soon replaced. Later models had an improved rifling with 6 grooves, right hand twist, 1 turn in 12 inches (1:304.8 mm) for increased accuracy and was optimized for use with the standard U.S. M193 cartridge. Current models are optimized for the heavier NATO SS109 bullet and have 6 grooves, right hand twist, 1 turn in 7 in (1:177.8 mm). Weapons designed to accept both the M193 or SS109 rounds (like civilian market clones) have a 6-groove, right hand twist, 1 turn in 9 inches (1:228.6 mm) bore.
Recoil.
"The (M16's) Stoner system provides a very symmetric design that allows straight line movement of the operating components. This allows recoil forces to drive straight to the rear. Instead of connecting or other mechanical parts driving the system, high pressure gas performs this function, reducing the weight of moving parts and the rifle as a whole." The M16's straight-line recoil design, where the recoil spring is located in the stock directly behind the action, and serves the dual function of operating spring and recoil buffer. The stock being in line with the bore also reduces muzzle rise, especially during automatic fire. Because recoil does not significantly shift the point of aim, faster follow-up shots are possible and user fatigue is reduced. Also, current model M16 flash-suppressors also act as compensators to reduce recoil further.
Notes: Free Recoil is mathematical equation calculated by using the rifle weight, bullet weight, muzzle velocity and charge weight. It is that which would be measured if the rifle were fired suspended from strings, free to recoil. As mentioned above, a rifles perceived recoil is also dependent on many other factors which are not readily quantified.
Sights.
The M16's most distinctive ergonomic feature is the carrying handle and rear sight assembly on top of the receiver. This is a by-product of the original design, where the carry handle served to protect the charging handle. As the line of sight is over the bore, the M16 has an inherent parallax problem. At closer ranges (typically inside 15–20 meters), the shooter must aim high in order to place shots where desired. The M16 has a 500 mm (19.75 inches) sight radius. The M16 uses an L-type flip, aperture rear sight and it is adjustable with two settings, 0 to 300 meters and 300 to 400 meters. The front sight is a post adjustable for elevation in the field. The rear sight can be adjusted in the field for windage. The sights can be adjusted with a bullet tip and soldiers are trained to zero their own rifles. The sight picture is the same as the M14, M1 Garand, M1 Carbine and the M1917 Enfield. The M16 also has a "Low Light Level Sight System", which includes a front sight post with a small glass vial of (glow-in-the-dark) radioactive Tritium H3 and a larger aperture rear sight. The M16 can also mount a scope on the carrying handle. With the advent of the M16A2, a new fully adjustable rear sight was added, allowing the rear sight to be dialed in for specific range settings between 300 and 800 meters and to allow windage adjustments without the need of a tool or cartridge. Modern versions such as M16A4 use Picatinny rails, which allows the use of various scopes and sighting devices. The current United States Army and Air Force issue M4 Carbine comes with the M68 Close Combat Optic and Back-up Iron Sight. The United States Marine Corps uses the ACOG Rifle Combat Optic and the United States Navy uses EOTech Holographic Weapon Sight.
Range and accuracy.
Note *: The effective range of a firearm is the maximum distance at which a weapon may be expected to be accurate and achieve the desired effect.<br>Note **: The horizontal range is the distance traveled by a bullet, fired from the rifle at a height of 1.6 meters and 0° elevation, until the bullet hits the ground. This is the equivalent of the muzzle energy of a .22LR handgun.
The M16 has always enjoyed a reputation for excellent accuracy. Its light recoil, high-velocity and flat trajectory allow shooters to take head shots out to 300 meters. Newer M16s use the newer M855 cartridge increasing their effective range to 600 meters. They are also more accurate than their predecessors and are capable of shooting 1–3 inch groups at 100 yards. "In Fallujah, Marines with ACOG-equipped M16A4s created a stir by taking so many head shots that until the wounds were closely examined, some observers thought the insurgents had been executed." The newest M855A1 EPR cartridge is even more accurate and during testing "...has shown that, on average, 95 percent of the rounds will hit within an 8 x 8-inch target at 600 meters."
Terminal ballistics.
The 5.56×45mm cartridge had several advantages over the 7.62×51mm NATO round used in the M14 rifle. It enabled each soldier to carry more ammunition and was easier to control during automatic or burst fire. The 5.56×45mm NATO cartridge can also produce massive wounding effects when the bullet impacts at high speed and yaws ("tumbles") in tissue leading to fragmentation and rapid transfer of energy.
The original ammunition for the M16 was the 55-grain M193 cartridge. When fired from a 20" barrel at ranges of up to 100 meters, the thin-jacketed lead-cored round traveled fast enough (above 2900 ft/s) that the force of striking a human body would cause the round to yaw (or tumble) and fragment into about a dozen pieces of various sizes thus created wounds that were out of proportion to its caliber. These wounds were so devastating that many considered the M16 to be an inhumane weapon. As the 5.56mm round's velocity decreases, so does the number of fragments that it produces. The 5.56mm round does not normally fragment at distances beyond 200 meters or at velocities below 2500 ft/s, and its lethality becomes largely dependent on shot placement.
With the development of the M16A2, the new 62-grain M855 cartridge was adopted in 1983. The heavier bullet had more energy, and was made with a steel core to penetrate Soviet body armor. However, this caused less fragmentation on impact and reduced effects against targets without armor, both of which lessened kinetic energy transfer and wounding ability. Some soldiers and Marines coped with this through training, with requirements to shoot vital areas three times to guarantee killing the target.
However, there have been repeated and consistent reports of the M855's inability to wound effectively (i.e. fragment) when fired from the short barreled M4 carbine (even at close ranges). The M4's 14.5" barrel length reduces muzzle velocity to about 2900 ft/s. This reduced wounding ability is one reason that, despite the Army's transition to short-barrel M4's, the Marine Corps has decided to continue using the M16A4 with its 20″ barrel as the 5.56×45mm M855 is largely dependent upon high velocity in order to wound effectively.
In 2003, the U.S. Army contended that the lack of lethality of the 5.56×45mm was more a matter of perception than fact. With good shot placement to the head and chest, the target was usually defeated without issue. The majority of failures were the result of hitting the target in non-vital areas such as extremities. However, a minority of failures occurred in spite of multiple hits to the chest. In 2006, a study found that 20% of soldiers using the M4 Carbine wanted more lethality or stopping power. In June 2010, the United States Army announced it began shipping its new 5.56mm, lead-free, M855A1 Enhanced Performance Round to active combat zones. This upgrade is designed to maximize performance of the 5.56×45mm round, to extend range, improve accuracy, increase penetration and to consistently fragment in soft-tissue when fired from not only standard length M16s, but also the short-barreled M4 carbines. The U.S. Army has been so impressed with the new M855A1 EPR round that they're now developing a 7.62 NATO variant.
Magazines.
The M16's magazine was meant to be a lightweight, disposable item. As such, it is made of pressed/stamped aluminum and was not designed to be durable. The M16 originally used a 20-round magazine which was later replaced by a bent 30-round design. As a result, the magazine follower tends to rock or tilt, causing malfunctions. Many non-U.S. and commercial magazines have been developed to effectively mitigate these shortcomings (e.g., H&K's all-stainless-steel magazine, Magpul's polymer P-MAG, etc.).
Production of 30-round magazine started late 1967 but did not fully replace the 20-round magazine till the mid 1970s. Standard USGI aluminum 30-round M16 magazines weigh empty and are long. The newer plastic magazines are about a half inch longer. And, the newer steel magazines are about 0.5 inch longer and 4 ounces heavier. The M16s magazine has become the unofficial NATO STANAG magazine and is currently used by many Western Nations, in numerous weapon systems.
In 2009, the U.S. Military began fielding an "improved magazine" identified by a tan-colored follower. "The new follower incorporates an extended rear leg and modified bullet protrusion for improved round stacking and orientation. The self-leveling/anti-tilt follower minimizes jamming while a wider spring coil profile creates even force distribution. The performance gains have not added weight or cost to the magazines."
Muzzle devices.
Most M16 rifles have a barrel threaded in 1⁄2-28" threads to incorporate the use of a muzzle device such as a flash suppressor or sound suppressor. The initial flash suppressor design had three tines or prongs and was designed to preserve the shooter's night vision by disrupting the flash. Unfortunately it was prone to breakage and getting entangled in vegetation. The design was later changed to close the end to avoid this and became known as the "A1" or "bird cage" flash suppressor on the M16A1. Eventually on the M16A2 version of the rifle, the bottom port was closed to reduce muzzle climb and prevent dust from rising when the rifle was fired in the prone position. For these reasons, the U.S. military declared the A2 flash suppressor as a compensator or a muzzle brake; but it is more commonly known as the "GI" or "A2" flash suppressor.
The M16's Vortex Flash Hider weighs 3 ounces, is 2.25 inches long, and does not require a lock washer to attach to barrel. It was developed in 1984, and is one of the earliest privately designed muzzle devices. The U.S. military uses the Vortex Flash Hider on M4 carbines and M16 rifles. A version of the Vortex has been adopted by the Canadian Military for the Colt Canada C8 CQB rifle. Other flash suppressors developed for the M16 include the Phantom Flash Suppressor by Yankee Hill Machine (YHM) and the KX-3 by Noveske Rifleworks.
The threaded barrel allows sound suppressors with the same thread pattern to be installed directly to the barrel; however this can result in complications such as being unable to remove the suppressor from the barrel due to repeated firing on full auto or three-round burst. A number of suppressor manufacturers such as Advanced Armament Corporation, Gemtech, Smith Enterprise, SureFire and OPS Inc. have turned to designing "direct-connect" sound suppressors which can be installed over an existing M16's flash suppressor as opposed to using the barrel's threads.
Grenade launchers and shotguns.
All current M16 type rifles are designed to fire STANAG (NATO standard) 22 mm rifle grenades from their integral flash hiders without the use of an adapter. These 22 mm grenade types range from anti-tank rounds to simple finned tubes with a fragmentation hand grenade attached to the end. They come in the "standard" type which are propelled by a blank cartridge inserted into the chamber of the rifle. They also come in the "bullet trap" and "shoot through" types, as their names imply, they use live ammunition. The U.S. military does not generally use rifle grenades; however, they are used by other nations.
All current M16 type rifles can mount under-barrel 40 mm grenade-launchers, such as the M203 and M320. Both use the same 40 mm grenades as the older, stand-alone M79 grenade launcher. The M16 can also mount under-barrel 12 gauge shotguns such as KAC Masterkey or the M26 Modular Accessory Shotgun System.
Riot Control Launcher.
The M234 Riot Control Launcher is an M16-series rifle attachment firing an M755 blank round. The M234 mounts on the muzzle, bayonet lug and front sight post of the M16. It fires either the M734 64 mm Kinetic Riot Control or the M742 64 mm CSI Riot Control Ring Airfoil Projectiles. The latter produces a 4 to 5 foot tear gas cloud on impact. The main advantage to using Ring Airfoil Projectiles is that their design does not allow them be thrown back by rioters with any real effect. The M234 is no longer used by United States forces. It has been replaced by the M203 40mm grenade launcher and nonlethal ammunition.
Bayonet.
The M16 is 44.25 inches (1124mm) long with an M7 bayonet attached. The M7 bayonet is based on earlier designs such as the M4, M5, & M6 bayonets, all of which are direct descendants of the M3 Fighting Knife and have spear-point blade with a half sharpened secondary edge. The newer M9 bayonet has a clip-point blade with saw teeth along the spine, and can be used as a multi-purpose knife and wire-cutter when combined with its scabbard. The current USMC OKC-3S bayonet bears a resemblance to the Marines' iconic Ka-Bar fighting knife with serrations near the handle.
Bipod.
The M16 and M16A1 were able to use the XM3 bipod. The XM3 bipod was a lightweight non-adjustable support that clamped onto the barrel of the M16 rifle for more accurate firing. The design did not see full scale use and was scrapped shortly after the Vietnam War.
Variants.
Pre-Production ArmaLite AR-15.
The weapon that eventually became the M16 series was basically a scaled down AR-10 with an ambidextrous charging handle located within the carrying handle, a narrower front sight "A" frame, and no flash suppressor.
AR-15 (Colt Models 601 & 602).
Colt's first two models produced after the acquisition of the rifle from ArmaLite were the 601 and 602, and these rifles were in many ways clones of the original ArmaLite rifle (in fact, these rifles were often found stamped "Colt ArmaLite AR-15, Property of the U.S. Government caliber .223", with no reference to them being M16s). The 601 and 602 are easily identified by their flat lower receivers without raised surfaces around the magazine well and occasionally green or brown furniture. The 601 was adopted first of any of the rifles by the USAF, and was quickly supplemented with the XM16 (Colt Model 602) and later the M16 (Colt Model 604) as improvements were made. There was also a limited purchase of 602s, and a number of both of these rifles found their way to a number of Special Operations units then operating in South East Asia, most notably the U.S. Navy SEALs. The only major difference between the 601 and 602 is the switch from the original 1:14-inch rifling twist to the more common 1:12-inch twist. These weapons were equipped with a triangular charging handle and a bolt hold open device that lacked a raised lower engagement surface. The bolt hold open device had a slanted and serrated surface that had to be engaged with a bare thumb, index finger, or thumb nail because of the lack of this surface.
The United States Air Force continued to use the AR-15 marked rifles in various configurations into the 1990s.
M16.
Variant originally adopted by the U.S. Air Force. This was the first M16 adopted operationally. This variant had triangular handguards, butt stocks without a compartment for the storage of a cleaning kit, a three-pronged flash suppressor, full auto, and no forward assist. Bolt carriers were originally chrome plated and slick-sided, lacking forward assist notches. Later, the chrome plated carriers were dropped in favor of Army issued notched and parkerized carriers though the interior portion of the bolt carrier is still chrome-lined. The Air Force continued to operate these weapons until around 2001, at which time the Air Force converted all of its M16s to the M16A2 configuration.
The M16 was also adopted by the British SAS, who used it during the Falklands War.
XM16E1 and M16A1 (Colt Model 603).
The U.S. Army XM16E1 was essentially the same weapon as the M16 with the addition of a forward assist and corresponding notches in the bolt carrier. The M16A1 was the finalized production model in 1967.
To address issues raised by the XM16E1's testing cycle, a closed, bird-cage flash suppressor replaced the XM16E1's three-pronged flash suppressor which caught on twigs and leaves. Various other changes were made after numerous problems in the field. Cleaning kits were developed and issued while barrels with chrome-plated chambers and later fully lined bores were introduced.
With these and other changes, the malfunction rate slowly declined and new soldiers were generally unfamiliar with early problems. A rib was built into the side of the receiver on the XM16E1 to help prevent accidentally pressing the magazine release button while closing the ejection port cover. This rib was later extended on production M16A1s to help in preventing the magazine release from inadvertently being pressed. The hole in the bolt that accepts the cam pin was crimped inward on one side, in such a way that the cam pin may not be inserted with the bolt installed backwards, which would cause failures to eject until corrected. The M16A1 is no longer in service with the United States, but is still standard issue in many world armies.
M16A2.
The development of the M16A2 rifle was originally requested by the United States Marine Corps as a result of the USMC's combat experience in Vietnam with the XM16E1 and M16A1. It was officially adopted by US Department of Defense as the "US Rifle, 5.56mm, M16A2" in 1982. The Marines were the first branch of the U.S. Armed Forces to adopt the M16A2 in the early/mid-1980s, with the United States Army following suit in the late 1980s. Modifications to the M16A2 were extensive. In addition to the new rifling, the barrel was made with a greater thickness in front of the front sight post, to resist bending in the field and to allow a longer period of sustained fire without overheating. The rest of the barrel was maintained at the original thickness to enable the M203 grenade launcher to be attached. A new adjustable rear sight was added, allowing the rear sight to be dialed in for specific range settings between 300 and 800 meters to take full advantage of the ballistic characteristics of the new SS109 rounds and to allow windage adjustments without the need of a tool or cartridge. The weapon's reliability allowed it to be widely used around the United States Marine Corps special operations divisions as well. The flash suppressor was again modified, this time to be closed on the bottom so it would not kick up dirt or snow when being fired from the prone position, and acting as a recoil compensator. The front grip was modified from the original triangular shape to a round one, which better fit smaller hands and could be fitted to older models of the M16. The new handguards were also symmetrical so that armories need not separate left and right spares. The handguard retention ring was tapered to make it easier to install and uninstall the handguards. A notch for the middle finger was added to the pistol grip, as well as more texture to enhance the grip. The buttstock was lengthened by . The new buttstock became ten times stronger than the original due to advances in polymer technology since the early 1960s. Original M16 stocks were made from fiberglass-impregnated resin; the newer stocks were engineered from DuPont Zytel glass-filled thermoset polymers. The new stock included a fully textured polymer buttplate for better grip on the shoulder, and retained a panel for accessing a small compartment inside the stock, often used for storing a basic cleaning kit. The heavier bullet reduces muzzle velocity from , to about . The A2 uses a faster twist rifling to allow the use of a trajectory-matched tracer round. It has a 1:7 twist rate. A spent case deflector was incorporated into the upper receiver immediately behind the ejection port to prevent cases from striking left-handed users.
The action was also modified, replacing the fully automatic setting with a three-round burst setting. When using a fully automatic weapon, inexperienced troops often hold down the trigger and "spray" when under fire. The U.S. Army concluded that three-shot groups provide an optimum combination of ammunition conservation, accuracy, and firepower. The USMC has retired the M16A2 in favour of the newer M16A4, although a few M16A2s are still in use by the United States Marine Corps today. Many M16A2s remain in service with the U.S. Army, Air Force, Navy and Coast Guard.
M16A3.
The M16A3 is a modified version of the M16A2 adopted in small numbers by the U.S. Navy SEAL, Seabee, and Security units. It features the M16A1 trigger group providing "safe," "semi-automatic." and "fully automatic" modes instead of the A2's "safe," "semi-automatic," and "burst."
M16A4.
The M16A4 is the fourth generation of the M16 series. It is equipped with a removable carrying handle and a full length quad Picatinny rail for mounting optics and other ancillary devices. The FN M16A4, using safe/semi/burst selective fire, became standard issue for the U.S. Marine Corps and is the current issue to Marine Corps recruits in both MCRD San Diego and MCRD Parris Island.
Military issue rifles are also equipped with a Knight's Armament Company M5 RAS hand guard, allowing vertical grips, lasers, tactical lights, and other accessories to be attached, coining the designation M16A4 MWS (or Modular Weapon System) in U.S. Army field manuals.
Colt also produces M16A4 models for international purchases, with specifics selective fire:
A study of significant changes to Marine M16A4 rifles released in February 2015 outlined several new features that could be added from inexpensive and available components. Those features included: a muzzle compensator in place of the flash suppressor to manage recoil and allow for faster follow-on shots, though at the cost of noise and flash signature and potential overpressure in close quarters; a heavier and/or free-floating barrel to increase accuracy from 4.5 MOA to potentially 2 MOA; changing the reticle on the Rifle Combat Optic from chevron-shaped to the semi-circle with a dot at the center used in the M27 IAR's Squad Day Optic so as not to obscure the target at long distance; using a trigger group with a more consistent pull force, even a reconsideration of the burst capability; and the addition of ambidextrous charging handles and bolt catch releases for easier use with left-handed shooters.
In 2014, Marine units were provided with a limited number of adjustable stocks in place of the traditional fixed stock for their M16A4s to issue to smaller Marines that would have trouble comfortably reaching the trigger when wearing body armor. The adjustable stocks were added as a standard authorized accessory, meaning units can use operations and maintenance funds to purchase more if needed.
The Marine Corps had long maintained the full-length M16 as their standard infantry rifle, but in October 2015 the switch to the M4 carbine was approved as the standard-issue weapon, giving Marine infantrymen a smaller and more compact weapon. Enough M4s are already in the inventory to re-equip all necessary units by September 2016, and M16A4s will be moved to support and non-infantry Marines.
Derivatives.
Colt Model 655 and 656 "Sniper" variants.
With the expanding Vietnam War, Colt developed two rifles of the M16 pattern for evaluation as possible light sniper or designated marksman rifles. The Colt Model 655 M16A1 Special High Profile was essentially a standard A1 rifle with a heavier barrel and a scope bracket that attached to the rifle's carry handle. The Colt Model 656 M16A1 Special Low Profile had a special upper receiver with no carrying handle. Instead, it had a low-profile iron sight adjustable for windage and a Weaver base for mounting a scope, a precursor to the Colt and Picatinny rails. It also had a hooded front iron sight in addition to the heavy barrel. Both rifles came standard with either a Leatherwood/Realist scope 3–9× Adjustable Ranging Telescope. Some of them were fitted with a Sionics noise and flash suppressor. Neither of these rifles were ever standardized.
These weapons can be seen in many ways to be predecessors of the U.S. Army's SDM-R and the USMC's SAM-R weapons.
XM177.
In Vietnam, some soldiers were issued a carbine version of the M16 called the XM177. The XM177 had a shorter barrel and a telescoping stock, which made it substantially more compact. It also possessed a combination flash hider/sound moderator to reduce problems with muzzle flash and loud report. The Air Force's GAU-5/A (XM177) and the Army's XM177E1 variants differed over the latter’s inclusion of a forward assist, although some GAU-5s do have the forward assist. The final Air Force GAU-5/A and Army XM177E2 had an barrel with a longer flash/sound suppressor. The lengthening of the barrel was to support the attachment of Colt's own XM148 40 mm grenade launcher. These versions were also known as the Colt Commando model commonly referenced and marketed as the CAR-15. The variants were issued in limited numbers to special forces, helicopter crews, Air Force pilots, Air Force Security Police Military Working Dog (MWD) handlers, officers, radio operators, artillerymen, and troops other than front line riflemen. Some USAF GAU-5A/As were later equipped with even longer 1/12 rifled barrels as the two shorter versions were worn out. The barrel allowed the use of MILES gear and for bayonets to be used with the sub-machine guns (as the Air Force described them). By 1989, the Air Force started to replace the earlier barrels with 1/7 rifled models for use with the M855-round. The weapons were given the redesignation of GUU-5/P.
These were effectively used by the British Special Air Service during the Falklands War.
Colt Model 733.
Colt also returned to the original "Commando" idea, with its Model 733, essentially a modernized XM177E2 with many of the features introduced on the M16A2.
M231 Firing Port Weapon (FPW).
M231 Firing Port Weapon (FPW) is an adapted version of the M16 assault rifle for firing from ports on the M2 Bradley. The infantry's normal M16s are too long for use in a "buttoned up" fighting vehicle, so the FPW was developed to provide a suitable weapon for this role. Designed by the Rock Island Arsenal, the M231 FPW remains in service, although all but the rear two firing ports on the Bradley have been removed. The M231 FPW fires from the open bolt and is only configured for fully automatic fire. The open bolt configuration gives the M231 a much higher cyclic rate of fire than the closed bolt operation of the M16A1. Official doctrine discourages deploying M231 outside of the firing port role. The weapon jams easily and is known to break the bolt without warning.
Mk 4 Mod 0.
The Mk 4 Mod 0 was a variant of the M16A1 produced for the U.S. Navy SEALs during the Vietnam War and adopted in April 1970. It differed from the basic M16A1 primarily in being optimized for maritime operations and coming equipped with a sound suppressor. Most of the operating parts of the rifle were coated in Kal-Guard, a hole of was drilled through the stock and buffer tube for drainage, and an O-ring was added to the end of the buffer assembly. The weapon could reportedly be carried to the depth of 200 feet (60 m) in water without damage. The initial Mk 2 Mod 0 Blast Suppressor was based on the U.S. Army's Human Engineering Lab's (HEL) M4 noise suppressor. The HEL M4 vented gas directly from the action, requiring a modified bolt carrier. A gas deflector was added to the charging handle to prevent gas from contacting the user. Thus, the HEL M4 suppressor was permanently mounted though it allowed normal semi-automatic and automatic operation. If the HEL M4 suppressor were removed, the weapon would have to be manually loaded after each single shot. On the other hand, the Mk 2 Mod 0 blast suppressor was considered an integral part of the Mk 4 Mod 0 rifle, but it would function normally if the suppressor were removed. The Mk 2 Mod 0 blast suppressor also drained water much more quickly and did not require any modification to the bolt carrier or to the charging handle. In the late 1970s, the Mk 2 Mod 0 blast suppressor was replaced by the Mk 2 blast suppressor made by Knight's Armament Company (KAC). The KAC suppressor can be fully submerged and water will drain out in less than eight seconds. It will operate without degradation even if the rifle is fired at the maximum rate of fire. The U.S. Army replaced the HEL M4 with the much simpler Studies in Operational Negation of Insurgency and Counter-Subversion (SIONICS) MAW-A1 noise and flash suppressor.
Mark 12.
Developed to increase the effective range of soldiers in the designated marksman role, the U.S. Navy developed the Mark 12 Special Purpose Rifle (SPR). Configurations in service vary, but the core of the Mark 12 SPR is an 18" heavy barrel with muzzle brake and free float tube. This tube relieves pressure on the barrel caused by standard handguards and greatly increases the potential accuracy of the system. Also common are higher magnification optics ranging from the 6× power Trijicon ACOG to the Leupold Mark 4 Tactical rifle scopes. Firing Mark 262 Mod 0 ammunition with a 77gr Open tip Match bullet, the system has an official effective range of 600+ meters. However published reports of confirmed kills beyond 800 m from Iraq and Afghanistan are not uncommon.
M4 carbine.
The M4 carbine was developed from various outgrowths of these designs, including a number of -barreled A1 style carbines. The XM4 (Colt Model 727) started its trials in the mid-1980s, with a barrel of . Officially adopted as a replacement for the M3 "Grease Gun" (and the Beretta M9 and M16A2 for select troops) in 1994, it was used with great success in the Balkans and in more recent conflicts, including the Afghanistan and Iraq theaters. The M4 carbine has a three-round burst firing mode, while the M4A1 carbine has a fully automatic firing mode. Both have a Picatinny rail on the upper receiver, allowing the carry handle/rear sight assembly to be replaced with other sighting devices.
Heckler & Koch HK416.
The Heckler & Koch HK416 is an assault rifle designed and manufactured by Heckler & Koch. It is based on the M16 platform, and was originally conceived as an improvement based on the Colt M4 carbine family issued to the U.S. military, with the notable inclusion of an HK-proprietary short-stroke gas piston system derived from the Heckler & Koch G36. The HK416 was used by United States Navy SEALs to kill Osama bin Laden.
Diemaco C7 and C8.
The Diemaco C7 and C8 are updated variants of the M16 developed and used by the Canadian Forces and are now manufactured by Colt Canada. The C7 is a further development of the experimental M16A1E1. Like earlier M16s, it can be fired in either single shot or automatic mode, instead of the burst function selected for the M16A2. The C7 also features the structural strengthening, improved handguards, and longer stock developed for the M16A2. Diemaco changed the trapdoor in the buttstock to make it easier to access and a spacer of is available to adjust stock length to user preference. The most easily noticeable external difference between American M16A2s and Diemaco C7s is the retention of the A1 style rear sights. Not easily apparent is Diemaco's use of hammer-forged barrels. The Canadians originally desired to use a heavy barrel profile instead.
The C7 has been developed to the C7A1, with a Weaver rail on the upper receiver for a C79 optical sight, and to the C7A2, with different furniture and internal improvements. The Diemaco produced Weaver rail on the original C7A1 variants does not meet the M1913 'Picatinny' standard, leading to some problems with mounting commercial sights. This is easily remedied with minor modification to the upper receiver or the sight itself. Since Diemaco's acquisition by Colt to form Colt Canada, all Canadian produced flattop upper receivers are machined to the M1913 standard.
The C8 is the carbine version of the C7. The C7 and C8 are also used by "Hærens Jegerkommando", "Marinejegerkommandoen" and FSK (Norway), Military of Denmark (all branches), the Royal Netherlands Army and Netherlands Marine Corps as its main infantry weapon. Following trials, variants became the weapon of choice of the British SAS.
Production and users.
The M16 is the most commonly manufactured 5.56×45mm rifle in the world. Currently, the M16 is in use by 15 NATO countries and more than 80 countries worldwide. Together, numerous companies in the United States, Canada, and China have produced more than 8,000,000 rifles of all variants. Approximately 90% are still in operation. The M16 replaced the M14 and M1 carbine as standard infantry rifles of the U.S. armed forces. The M14 continues to see limited service, mostly in sniper, designated marksman, and ceremonial roles.

</doc>
<doc id="19903" url="https://en.wikipedia.org/wiki?curid=19903" title="Marlon Brando">
Marlon Brando

Marlon Brando, Jr. (April 3, 1924 – July 1, 2004) was an American actor, film director, and activist. He is hailed for bringing a gripping realism to film acting and is often cited as one of the greatest and most influential actors of all time. He is also credited with helping to popularize the Stanislavski system of acting, today more commonly referred to as method acting. A cultural icon, Brando is most famous for his Academy Award-winning performances as Terry Malloy in "On the Waterfront" (1954) and Vito Corleone in "The Godfather" (1972), as well as influential performances in "A Streetcar Named Desire" (1951), "Viva Zapata!" (1952), "Julius Caesar" (1953), "The Wild One" (1953), "Reflections in a Golden Eye" (1967), "Last Tango in Paris" (1972), and "Apocalypse Now" (1979). Brando was also an activist for many causes, notably the African-American Civil Rights Movement and various Native American movements.
He initially gained acclaim and an Academy Award nomination for reprising the role of Stanley Kowalski in the 1951 film adaptation of Tennessee Williams' play "A Streetcar Named Desire", a role that he had originated successfully on Broadway. He received further praise for his performance as Terry Malloy in "On the Waterfront", and his portrayal of the rebel motorcycle gang leader Johnny Strabler in "The Wild One" proved to be a lasting image in popular culture. Brando received Academy Award nominations for playing Emiliano Zapata in "Viva Zapata!"; Mark Antony in Joseph L. Mankiewicz's 1953 film adaptation of Shakespeare's "Julius Caesar"; and Air Force Major Lloyd Gruver in "Sayonara" (1957), an adaption of James Michener's 1954 novel. Brando was included in a list of Top Ten Money Making Stars three times in the 1950s, coming in at number 10 in 1954, number 6 in 1955, and number 4 in 1958.
The 1960s proved to be a fallow decade for Brando. He directed and starred in the cult western film "One-Eyed Jacks", a critical and commercial flop, after which he delivered a series of box-office failures, beginning with the 1962 film adaptation of the novel "Mutiny on the Bounty". After 10 years, during which he did not appear in a successful film, he won his second Academy Award for playing Vito Corleone in Francis Ford Coppola's "The Godfather", a role critics consider among his greatest. "The Godfather" was then one of the most commercially successful films of all time. With that and his Oscar-nominated performance in "Last Tango in Paris", Brando re-established himself in the ranks of top box-office stars, placing sixth and tenth in the Money Making Stars poll in 1972 and 1973, respectively. Brando took a four-year hiatus before appearing in "The Missouri Breaks" (1976). After this, he was content with being a highly paid character actor in cameo roles, such as in "Superman" (1978) and "The Formula" (1980), before taking a nine-year break from motion pictures. According to the "Guinness Book of World Records", Brando was paid a record $3.7 million ($ million in inflation-adjusted dollars) and 11.75% of the gross profits for 13 days' work on "Superman". He finished out the 1970s with his controversial performance as Colonel Kurtz in another Coppola film, "Apocalypse Now", a box-office hit for which he was highly paid and which helped finance his career layoff during the 1980s.
Brando was ranked by the American Film Institute as the fourth-greatest movie star among male movie stars whose screen debuts occurred in or before 1950. He was one of only three professional actors, along with Charlie Chaplin and Marilyn Monroe, named in 1999 by "Time" magazine as one of its . He died of respiratory failure on July 1, 2004, at age 80.
Early life.
Brando was born on April 3, 1924, in Omaha, Nebraska, to Marlon Brando, Sr., a pesticide and chemical feed manufacturer, and Dorothy Julia (née Pennebaker). Brando had two older sisters, Jocelyn Brando (1919–2005) and Frances (1922–1994). His ancestry included German, Dutch, English, and Irish. His patrilineal immigrant ancestor, Johann Wilhelm Brandau, arrived in New York in the early 1700s from the Palatinate of Germany. Brando was raised a Christian Scientist. His mother, known as Dodie, was unconventional for her time; an actress herself, she smoked, wore trousers and drove cars—all unusual for women at the time—and was even a theatre administrator, helping Henry Fonda begin his acting career. However, she was an alcoholic and often had to be brought home from Chicago bars by her husband. In his autobiography, "Songs My Mother Taught Me", Brando expressed sadness when writing about his mother: "The anguish that her drinking produced was that she preferred getting drunk to caring for us." Dodie and Brando's father eventually joined Alcoholics Anonymous. Brando harbored far more enmity for his father, stating, "I was his namesake, but nothing I did ever pleased or even interested him. He enjoyed telling me I couldn't do anything right. He had a habit of telling me I would never amount to anything." Brando's parents moved to Evanston, Illinois, when his father's work took him to Chicago, but separated when Brando was 11 years old. His mother took the three children to Santa Ana, California, where they lived with her mother. In 1937, Brando's parents reconciled and moved together to Libertyville, Illinois, a small town north of Chicago. In 1939 and 1941, he worked as an usher at the town's only movie theatre, The Liberty.
Brando, whose childhood nickname was "Bud", was a mimic from his youth. He developed an ability to absorb the mannerisms of kids he played with and display them dramatically while staying in character. In the 2007 TCM biopic, "Brando: The Documentary", childhood friend George Englund recalls Brando's earliest acting as imitating the cows and horses on the family farm as a way to distract his mother from drinking. His sister Jocelyn was the first to pursue an acting career, going to study at the American Academy of Dramatic Art in New York City. She appeared on Broadway, then films and television. Brando's sister Frances left college in California to study art in New York. Brando had been held back a year in school and was later expelled from Libertyville High School for riding his motorcycle through the corridors.
He was sent to Shattuck Military Academy, where his father had studied before him. Brando excelled at theatre and did well in the school. In his final year (1943), he was put on probation for being insubordinate to a visiting army colonel during maneuvers. He was confined to his room, but sneaked into town and was caught. The faculty voted to expel him, though he was supported by the students, who thought expulsion was too harsh. He was invited back for the following year, but decided instead to drop out of high school. Brando worked as a ditch-digger as a summer job arranged by his father. He tried to enlist in the Army, but his induction physical revealed that a football injury he had sustained at Shattuck had left him with a trick knee. He was classified 4-F and not inducted.
New York and acting.
Brando decided to follow his sisters to New York, studying at the American Theatre Wing Professional School, part of the Dramatic Workshop of the New School, with influential German director Erwin Piscator. In a 1988 documentary, "Marlon Brando: The Wild One", Brando's sister Jocelyn remembered, "He was in a school play and enjoyed it ... So he decided he would go to New York and study acting because that was the only thing he had enjoyed. That was when he was 18." In the A&E "Biography" episode on Brando, George Englund said Brando fell into acting in New York because "he was accepted there. He wasn't criticized. It was the first time in his life that he heard good things about himself."
Brando was an avid student and proponent of Stella Adler, from whom he learned the techniques of the Stanislavski System. This technique encouraged the actor to explore his own feelings and past experiences to fully realize the character being portrayed. Brando's remarkable insight and sense of realism was evident early on. Adler used to recount that when teaching Brando, she had instructed the class to act like chickens, and added that a nuclear bomb was about to fall on them. Most of the class clucked and ran around wildly, but Brando sat calmly and pretended to lay an egg. Asked by Adler why he had chosen to react this way, he said, "I'm a chicken—what do I know about bombs?" Despite being commonly regarded as a Method actor, Brando disagreed. He claimed to have abhorred Lee Strasberg's teachings:
Career.
Early career: 1944–1951.
Brando used his Stanislavski System skills for his first summer-stock roles in Sayville, New York, on Long Island. Brando established a pattern of erratic, insubordinate behavior in the few shows he had been in. His behavior had him kicked out of the cast of the New School's production in Sayville, but he was soon afterwards discovered in a locally produced play there. Then, in 1944, he made it to Broadway in the bittersweet drama "I Remember Mama", playing the son of Mady Christians. The Lunts wanted Brando to play the role of Alfred Lunt's son in "O Mistress Mine", and Lunt even coached him for his audition, but Brando's reading during the audition was so desultory that they couldn't hire him. New York Drama Critics voted him "Most Promising Young Actor" for his role as an anguished veteran in "Truckline Café", although the play was a commercial failure. In 1946, he appeared on Broadway as the young hero in the political drama "A Flag is Born", refusing to accept wages above the Actors' Equity rate. In that same year, Brando played the role of Marchbanks alongside Katharine Cornell in her production's revival of "Candida", one of her signature roles. Cornell also cast him as the Messenger in her production of Jean Anouilh's "Antigone" that same year. He was also offered the opportunity to portray one of the principal characters in the Broadway premiere of Eugene O'Neill's "The Iceman Cometh", but turned the part down after falling asleep while trying to read the massive script and pronouncing the play "ineptly written and poorly constructed".
In 1945, Brando's agent recommended he take a co-starring role in "The Eagle Has Two Heads" with Tallulah Bankhead, produced by Jack Wilson. Tallulah had turned down the role of Blanche Dubois in a "A Streetcar Named Desire", which Williams had written for her, to tour the play for the 1946-1947 season. Bankhead recognized Brando's potential, despite her disdain (which most Broadway veterans shared) for method acting, and agreed to hire him even though he auditioned poorly. The two clashed greatly during the pre-Broadway tour, with Bankhead reminding Brando of his mother, being her age and also having a drinking problem.
Jack Wilson was largely tolerant of Brando's behavior, but he reached his limit when Brando mumbled through a dress rehearsal shortly before the November 28, 1946, opening. "I don't care what your grandmother did," Wilson exclaimed, "and that Method stuff, I want to know what you're going to do!" Brando in turn raised his voice, and acted with great power and passion. "It was marvelous," a cast member recalled. "Everybody hugged him and kissed him. He came ambling offstage and said to me, ‘They don't think you can act unless you can yell." A review of Brando's performance in the opening assessed that Brando was "still building his character, but at present fails to impress". 
One Boston critic remarked of Brando's prolonged death scene, "Brando looked like a car in midtown Manhattan searching for a parking space." He received better reviews at subsequent tour stops, but what his colleagues recalled was only occasional indications of the talent he would later demonstrate. "There were a few times when he was really magnificent," Tallulah admitted to an interviewer in 1962. "He was a great young actor when he wanted to be, but most of the time I couldn't even hear him on the stage." Brando displayed his apathy for the production by demonstrating some shocking onstage manners. He "tried everything in the world to ruin it for her", Tallulah's stage-manager claimed. "He nearly drove her crazy: scratching his crotch, picking his nose, doing anything." After several weeks on the road, they reached Boston, by which time Tallulah was ready to dismiss him. This proved to be one of the greatest blessings of his career, as it freed him up to play the role of Stanley Kowalski in Tennessee Williams's 1947 play "A Streetcar Named Desire," directed by Elia Kazan. Bankhead had recommended him to Williams for the role of Stanley, thinking he was perfect for the part.
Pierpont writes that John Garfield was first choice for the role, but "made impossible demands". It was Kazan's decision to fall back on the far less experienced (and technically too young for the role) Brando. In a letter dated August 29, 1947, Williams confided to his agent Audrey Wood: "It had not occurred to me before what an excellent value would come through casting a very young actor in this part. It humanizes the character of Stanley in that it becomes the brutality and callousness of youth rather than a vicious old man ... A new value came out of Brando's reading which was by far the best reading I have ever heard." Brando based his portrayal of Kowalski on the boxer Rocky Graziano, whom he had studied at a local gymnasium. Graziano did not know who Brando was, but attended the production with tickets provided by the young man. He said, "The curtain went up and on the stage is that son of a bitch from the gym, and he's playing me."
In 1947, Brando performed a screen test for an early Warner Brothers script for the novel "Rebel Without a Cause" (1944), which bore no relation to the film eventually produced in 1955. The screen test is included as an extra in the 2006 DVD release of "A Streetcar Named Desire".
Brando's first screen role was the bitter paraplegic veteran in "The Men" (1950). He spent a month in bed at the Birmingham Army Hospital in Van Nuys to prepare for the role. "The New York Times" reviewer Bosley Crowther wrote that Brando as Ken
"is so vividly real, dynamic and sensitive that his illusion is complete" and noted, "Out of stiff and frozen silences he can lash into a passionate rage with the tearful and flailing frenzy of a taut cable suddenly cut."
By Brando's own account, it may have been because of this film that his draft status was changed from 4-F to 1-A. He had had surgery on his trick knee, and it was no longer physically debilitating enough to incur exclusion from the draft. When Brando reported to the induction center, he answered a questionnaire by saying his race was "human", his color was "Seasonal–oyster white to beige", and he told an Army doctor that he was psychoneurotic. When the draft board referred him to a psychiatrist, Brando explained that he had been expelled from military school and had severe problems with authority. Coincidentally, the psychiatrist knew a doctor friend of Brando. Brando avoided military service during the Korean War.
Early in his career, Brando began using cue cards instead of memorizing his lines. Despite the objections of several of the film directors he worked with, Brando felt that this helped bring realism and spontaneity to his performances. He felt otherwise he would appear to be reciting a writer's speech. In the TV documentary "The Making of Superman: The Movie", Brando explained: 
However, some thought Brando used the cards out of laziness or an inability to memorize his lines. Once on "The Godfather" set, Brando was asked why he wanted his lines printed out. He responded, "Because I can read them that way."
Rise to fame: 1951–1954.
Brando brought his performance as Stanley Kowalski to the screen in Tennessee William's "A Streetcar Named Desire" (1951). The role is regarded as one of Brando's greatest. The reception of Brando's performance was so positive that Brando quickly became a male sex symbol in Hollywood. The role earned him his first Academy Award nomination in the Best Actor category.
He was also nominated the next year for "Viva Zapata!" (1952), a fictionalized account of the life of Mexican revolutionary Emiliano Zapata. It recounted his peasant upbringing, his rise to power in the early 20th century, and death. The film was directed by Elia Kazan and co-starred Anthony Quinn. In the biopic "Marlon Brando: The Wild One", Sam Shaw says, "Secretly, before the picture started, he went to Mexico to the very town where Zapata lived and was born in and it was there that he studied the speech patterns of people, their behavior, movement." Most critics focused on the actor rather than the film, with "Time" and "Newsweek" publishing rave reviews.
Years later, in his autobiography, Brando remarked: "Tony Quinn, whom I admired professionally and liked personally, played my brother, but he was extremely cold to me while we shot that picture. During our scenes together, I sensed a bitterness toward me, and if I suggested a drink after work, he either turned me down or else was sullen and said little. Only years later did I learn why." Brando related that, to create on-screen tension between the two, "Gadg" (Kazan) had told Quinn—who had taken over the role of Stanley Kowalski on Broadway after Brando had finished—that Brando had been unimpressed with his work. After achieving the desired effect, Kazan never told Quinn that he had misled him. It was only many years later, after comparing notes, that Brando and Quinn realized the deception.
Brando's next film, "Julius Caesar" (1953), received highly favorable reviews. Brando portrayed Mark Antony. While most acknowledged Brando's talent, some critics felt Brando's "mumbling" and other idiosyncrasies betrayed a lack of acting fundamentals and, when his casting was announced, many remained dubious about his prospects for success. Directed by Joseph L. Mankiewicz and co-starring British stage actor John Gielgud, Brando delivered an impressive performance, especially during Antony's noted "Friends, Romans, countrymen ..." speech. Gielgud was so impressed that he offered Brando a full season at the Hammersmith Theatre, an offer he declined. In his biography on the actor, Stefan Kanfer writes, "Marlon's autobiography devotes one line to his work on that film: Among all those British professionals, 'for me to walk onto a movie set and play Mark Anthony was asinine'—yet another example of his persistent self-denigration, and wholly incorrect." Kanfer adds that after a screening of the film, director John Huston commented, "Christ! It was like a furnace door opening—the heat came off the screen. I don't know another actor who could do that." During the filming of "Julius Caesar", Brando learned that Elia Kazan had cooperated with congressional investigators, naming a whole string of "subversives" to the House Committee on Un-American Activities. By all accounts, Brando was upset by his mentor's decision, but he worked with him again in "On The Waterfront". "None of us is perfect," he later wrote in his memoir, "and I think that Gadg has done injury to others, but mostly to himself."
In 1953, Brando also starred in "The Wild One", riding his own Triumph Thunderbird 6T motorcycle. Triumph's importers were ambivalent at the exposure, as the subject matter was rowdy motorcycle gangs taking over a small town. The film was criticized for its perceived gratuitous violence at the time, with "Time" stating, "The effect of the movie is not to throw light on the public problem, but to shoot adrenaline through the moviegoer's veins." Brando allegedly did not see eye to eye with the Hungarian director László Benedek and did not get on with costar Lee Marvin.
To Brando's expressed puzzlement, the movie inspired teen rebellion and made him a role model to the nascent rock-and-roll generation and future stars such as James Dean and Elvis Presley. After the movie's release, the sales of leather jackets and blue jeans skyrocketed. Reflecting on the movie in his autobiography, Brando concluded that it had not aged very well but said:
"On the Waterfront".
In 1954, Brando starred in "On the Waterfront", a crime drama film about union violence and corruption among longshoremen. The film was directed by Elia Kazan and written by Budd Schulberg; it also stars Karl Malden, Lee J. Cobb, Rod Steiger and, in her film debut, Eva Marie Saint. When initially offered the role, Brando—still stung by Kazan's testimony to HUAC—demurred and the part of Terry Malloy nearly went to Frank Sinatra. According to biographer Stefan Kanfer, the director believed that Sinatra, who grew up in Hoboken, would work as Malloy, but eventually producer Sam Spiegel wooed Brando to the part, signing him for $100,000. "Kazan made no protest because, he subsequently confessed, 'I always preferred Brando to anybody.'"
Brando won the Oscar for his role as Terry Malloy in "On the Waterfront". His performance, spurred on by his rapport with Eva Marie Saint and Kazan's direction, was praised as a "tour de force". For the famous "I coulda been a contender" scene, he convinced Kazan that the scripted scene was unrealistic. Schulberg's script had Brando acting the entire scene with his character being held at gunpoint by his brother Charlie, played by Rod Steiger. Brando insisted on gently pushing away the gun, saying that Terry would never believe that his brother would pull the trigger and doubting that he could continue his speech while fearing a gun on him. Kazan let Brando improvise and later expressed deep admiration for Brando's instinctive understanding, saying:
Upon its release, "On the Waterfront" received glowing reviews from critics and was a commercial success, earning an estimated $4.2 million in rentals at the North American box office in 1954. In his July 29, 1954, review, "The New York Times" critic A. H. Weiler praised the film, calling it "an uncommonly powerful, exciting, and imaginative use of the screen by gifted professionals." Film critic Roger Ebert lauded the film, stating that Brando and Kazan changed acting in American films forever and added it to his "Great Movies" list. In his autobiography, Brando was typically dismissive of his performance: "On the day Gadg showed me the complete picture, I was so depressed by my performance I got up and left the screening room ... I thought I was a huge failure." After Brando won the Academy Award for Best Actor, the statue was stolen. Much later, it turned up at a London auction house, which contacted the actor and informed him of its whereabouts.
Box office successes and directorial debut: 1954–1959.
Brando's first six films—"The Men", "A Streetcar Named Desire", "Viva Zapata!", "Julius Caesar", "The Wild One" and "On the Waterfront"—laid a standard of excellence that would sustain him throughout his career—a standard that Brando himself would have difficulty reattaining. As the decade continued, Brando remained a top box office draw but critics felt his performances were half-hearted, lacking the intensity and commitment found in his earlier work, especially in his work with Kazan. He portrayed Napoleon in the 1954 film "Désirée". According to co-star Jean Simmons, Brando's contract forced him to star in the movie. He put little effort into the role, claiming he didn't like the script, and later dismissed the entire movie as "superficial and dismal". Brando was especially contemptuous of director Henry Koster.
Brando and Simmons were paired together again in the film adaptation of the musical "Guys and Dolls" (1955). "Guys and Dolls" would be Brando's first and last musical role. "Time" found the picture "false to the original in its feeling", remarking that Brando "sings in a faraway tenor that sometimes tends to be flat." Appearing in Edward Murrow's "Person to Person" interview in early 1955, he admitted to having problems with his singing voice, which he called "pretty terrible." In the 1965 documentary "Meet Marlon Brando" he revealed that the final product heard in the movie was a result of countless singing takes being cut into one and later joked, "I couldn't hit a note with a baseball bat; some notes I missed by extraordinary margins ... They sewed my words together on one song so tightly that when I mouthed it in front of the camera, I nearly asphyxiated myself". Relations between Brando and Sinatra were also frosty, with Stefan Kanfer observing, "The two men were diametrical opposites: Marlon required multiple takes; Frank detested repeating himself." Upon their first meeting Sinatra reportedly scoffed, "Don't give me any of that Actors Studio shit." Brando later famously quipped, "Frank is the kind of guy, when he dies, he's going to heaven and give God a hard time for making him bald." Frank Sinatra famously called Brando "the world's most overrated actor", and referred to him as "mumbles". The film was commercially though not critically successful, costing $5.5 million to make and grossing $13 million.
Brando played Sakini, a Japanese interpreter for the U.S. Army in postwar Japan in "The Teahouse of the August Moon" (1956). Pauline Kael was not particularly impressed by the movie, but noted "Marlon Brando starved himself to play the pixie interpreter Sakini, and he looks as if he's enjoying the stunt—talking with a mad accent, grinning boyishly, bending forward, and doing tricky movements with his legs. He's harmlessly genial (and he is certainly missed when he's offscreen), though the fey, roguish role doesn't allow him to do what he's great at and it's possible that he's less effective in it than a lesser actor might have been." In "Sayonara" (1957) he appeared as a United States Air Force officer. "Newsweek" found the film a "dull tale of the meeting of the twain" but fans disagreed and the movie became a hit. According to Stefan Kanfer's biography of the actor, Marlon's manager Jay Kanter negotiated a profitable contract with ten percent of the gross going to Brando, which put him in the millionaire category. The movie was controversial due to openly discussing interracial marriage but proved a great success, earning 10 Academy Award nominations, with Brando being nominated for Best Actor. The film went on to win four Academy Awards. "Teahouse" and "Sayonara" were the first in a string of films Brando would strive to make over the next decade which contained socially relevant messages, and he formed a partnership with Paramount to establish his own production company called Pennebaker, its declared purpose to develop films that contained "social value that would improve the world." The name was a tribute in honor of his mother, who had died in 1954. By all accounts, Brando was devastated by her death, with biographer Peter Manso telling A&E's "Biography", "She was the one who could give him approval like no one else could and, after his mother died, it seems that Marlon stops caring." Brando shocked many who knew him by appointing his hated father to run Pennebaker. In the same A&E special, George Englund claims that Brando did it because "it gave Marlon a chance to take shots at him, to demean and diminish him" while biographer David Thomson speculates that "Brando was psychologically daunted by his father."
In 1958, Brando appeared in "The Young Lions", dyeing his hair blonde and assuming a German accent for the role, which he later admitted was not convincing. It based on the novel by Irwin Shaw and Brando's portrayal of the character Christian Diestl was controversial for its time. He later wrote, "The original script closely followed the book, in which Shaw painted all Germans as evil caricatures, especially Christian, whom he portrayed as a symbol of everything that was bad about Nazism; he was mean, nasty, vicious, a cliché of evil ... I thought the story should demonstrate that there are no inherently 'bad' people in the world, but they can easily be misled." Shaw and Brando even appeared together for a televised interview with CBS correspondent David Schoenbrun and, during a bombastic exchange, Shaw charged that, like most actors, Brando was incapable of playing flat-out villainy; Brando responded by stating "Nobody creates a character but an actor. I play the role; now he exists. He is my creation." "The Young Lions" also features Brando's only appearance in a film with friend and rival Montgomery Clift (although they shared no scenes together). Brando closed out the decade by appearing in "The Fugitive Kind" opposite Anna Magnani. The film was based on another play by Tennessee Williams but was hardly the success "A Streetcar Named Desire" had been, with the "Los Angeles Times" labeling Williams's personae "psychologically sick or just plain ugly" and "The New Yorker" calling it a "cornpone melodrama".
"One-Eyed Jacks" and "Mutiny on the Bounty".
In 1962, Brando made his directorial debut in the western "One-Eyed Jacks". The picture was originally planned to be directed by Stanley Kubrick from a screenplay by Sam Peckinpah, but studio disputes led to their replacement by Brando and Guy Trosper. Brando portrays the lead character Rio, and Karl Malden plays his partner "Dad" Longworth. The supporting cast features Katy Jurado, Ben Johnson, and Slim Pickens. Brando's penchant for multiple retakes and character exploration as an actor carried over into his directing, however, and the film soon went over budget; Paramount expected the film to take three months to complete but shooting stretched to six and the cost doubled to more than six million dollars. Brando's inexperience as an editor also delayed postproduction and Paramount eventually took control of the film. Brando later wrote, "Paramount said it didn't like my version of the story; I'd had everyone lie except Karl Malden. The studio cut the movie to pieces and made him a liar, too. By then, I was bored with the whole project and walked away from it." Visually stunning, "One-Eyed Jacks" was poorly reviewed by critics. Review aggregator Rotten Tomatoes reports that only 50% of critics have given the film a positive review, making it "rotten".
Brando's revulsion with the film industry reportedly boiled over on the set of his next film, Metro-Goldwyn-Mayer's remake of "Mutiny on the Bounty", which was filmed in Tahiti. The actor was accused of deliberately sabotaging nearly every aspect of the production. On June 16, 1962, "The Saturday Evening Post" ran an article by Bill Davidson with the headline "Six million dollars down the drain: the mutiny of Marlon Brando". "Mutiny" director Lewis Milestone claimed that the executives "deserve what they get when they give a ham actor, a petulant child, complete control over an expensive picture." " Mutiny on the Bounty" nearly capsized MGM and, while the project had indeed been hampered with delays other than Brando's behavior, the accusations would dog the actor for years as studios began to fear Brando's difficult reputation. Critics also began taking note of Marlon's fluctuating weight.
Box office decline: 1963–1971.
Distracted by his personal life and becoming disillusioned with his career, Brando began to view acting as a means to an end financially. Critics protested when he started accepting roles in films many perceived as being beneath his talent, or criticized him for failing to live up to the better roles. Previously only signing short term deals with movie studios, in 1961 Brando uncharacteristically signed a five picture deal with Universal Studios that would haunt him for the rest of the decade. "The Ugly American" (1963) was the first of these films. Based on the 1958 novel of the same name that Pennebaker had optioned, the movie, which featured Brando's sister Jocelyn, was rated fairly positively but died at the box office. Brando was nominated for a Golden Globe for his performance. All of Brando's other Universal films during this period, including "Bedtime Story" (1964), "The Appaloosa" (1966), "A Countess from Hong Kong" (1967) and "The Night of the Following Day" (1969), were also critical and commercial flops. "Countess" in particular was a disappointment for Brando, who had looked forward to working with one of his heroes, director Charlie Chaplin. The experience turned out to be an unhappy one; Brando had no chemistry with his leading lady Sophia Loren and was horrified at Chaplin's didactic style of direction and his authoritarian approach. Brando had also appeared in the spy thriller "Morituri" in 1965; that, too, failed to attract an audience.
Brando acknowledged his professional decline, writing later, "Some of the films I made during the sixties were successful; some weren't. Some, like "The Night of the Following Day", I made only for the money; others, like "Candy", I did because a friend asked me to and I didn't want to turn him down ... In some ways I think of my middle age as the Fuck You Years." "Candy" was especially appalling for many; a 1968 sex farce film directed by Christian Marquand and based on the 1958 novel by Terry Southern, the movie satirizes pornographic stories through the adventures of its naive heroine, Candy, played by Ewa Aulin. It is generally regarded as the nadir of Brando's career. "The Washington Post" observed: "Brando's self-indulgence over a dozen years is costing him and his public his talents." In the March 1966 issue of "The Atlantic", Pauline Kael wrote that in his rebellious days, Marlon "was antisocial because he knew society was crap; he was a hero to youth because he was strong enough not to take the crap" but now Brando and others like him had become "buffoons, shamelessly, pathetically mocking their public reputations." In an earlier review of "The Appaloosa" in 1966, Kael wrote that the actor was "trapped in another dog of a movie ... Not for the first time, Mr. Brando gives us a heavy-lidded, adenoidally openmouthed caricature of the inarticulate, stalwart loner." Although he feigned indifference, Brando was hurt by the critical mauling, admitting in the 2015 film "Listen to Me Marlon", "They can hit you every day and you have no way of fighting back. I was very convincing in my pose of indifference, but I was very sensitive and it hurt a lot."
While Brando had lost much of his critical and commercial appeal in the 1960s, he still gave some memorable performances. Brando portrayed a repressed gay army officer in "Reflections in a Golden Eye", directed by John Huston and costarring Elizabeth Taylor. The role turned out as one of his greatest in years, with Stanley Crouch marveling, "Brando's main achievement was to portray the taciturn but stoic gloom of those pulverized by circumstances." The film overall received mixed reviews. Another notable film was "The Chase" (1966), which paired the actor with Arthur Penn, Robert Duvall, Jane Fonda and Robert Redford. The film deals with themes of racism, sexual revolution, small-town corruption, and vigilantism but is probably best remembered for a scene in which the sheriff played by Brando is brutally beaten by three of the vigilantes. The film was received mostly positively.
Brando cited "Burn!" (1970) as his personal favorite of the films he had made, writing in his autobiography, "I think I did some of the best acting I've ever done in that picture, but few people came to see it." Brando dedicated a full chapter to the film in his memoir, stating that the director, Gillo Pontecorvo, was the best director he had ever worked with next to Kazan and Bernardo Bertolucci. Brando also detailed his clashes with Pontecorvo on the set and how "we nearly killed each other." Loosely based on events in the history of Guadeloupe, the film got a hostile reception from critics despite a committed performance from Brando. In 1971, Michael Winner directed him in the British horror film "The Nightcomers" with Stephanie Beacham, Thora Hird, Harry Andrews and Anna Palk. It is a prequel to "The Turn of the Screw", which later became the 1961 film "The Innocents". Brando's performance earned him a nomination for a Best Actor BAFTA but the film bombed at the box office.
"The Godfather" and "Last Tango in Paris".
By the dawn of the 1970s, Brando was considered "unbankable". Critics were becoming increasingly dismissive of his work and he had not appeared in a box office hit since "The Young Lions" in 1958, the last year he had ranked as one of the Top Ten Box Office Stars. which was also the year of his last Academy Award nomination, for "Sayonara." Brando's performance as Vito Corleone, the "Don," in "The Godfather" (1972), Francis Ford Coppola's adaptation of Mario Puzo's 1969 best-selling novel of the same name, was a career turning point, putting him back in the Top Ten and winning him his second Best Actor Oscar.
Paramount production chief Robert Evans, who had given Puzo an advance to write "The Godfather" so that Paramount would own the movie rights, hired Coppola after many major directors had turned the movie down because he wanted an Italian-American director who could provide the movie with cultural authenticity. Coppola also came cheap. Evans was conscious of the fact that Paramount's last Mafia movie, "The Brotherhood" (1968) had been a box office bomb, and he believed it was partly due to the fact that the director, Martin Ritt, and the star, Kirk Douglas, were Jews and the film lacked an authentic Italian flavor. The studio originally intended the movie to be a low-budget production set in contemporary times without any major actors, but the phenomenal success of the novel gave Evans the clout to turn "The Godfather" into a prestige picture.
Coppola had developed a list of actors for all the roles, and his list of potential Dons included the Oscar-winning Italian-American Ernest Borgnine, the Italian-American Frank de Kova (best known for playing Chief Wild Eagle on the TV sit-com "F-Troop"), John Marley (a Best Supporting Oscar-nominee for Paramount's 1970 hit movie "Love Story" who was cast as the movie producer Woltz in the picture), the Italian-American Richard Conte (who was cast as Don Corleone's deadly rival Don Barzini), and Italian movie producer Carlo Ponti. Coppola admitted in a 1975 interview, "We finally figured we had to lure the "best" actor in the world. It was that simple. That boiled down to Laurence Olivier or Marlon Brando, who "are" the greatest actors in the world." The holographic copy of Coppola's cast list shows Brando's name underlined.
Evans told Coppola that he had been thinking of Brando for the part two years earlier, and Puzo had imagined Brando in the part when he wrote the novel and had actually written to him about the part, so Coppola and Evans narrowed it down to Brando. (Ironically, Olivier would compete with Brando for the Best Actor Oscar for his part in "Sleuth." He bested Brando at the 1972 New York Film Critics Circle Awards.) Albert S. Ruddy, who Paramount assigned to produce the film, agreed with the choice of Brando. However, Paramount studio heads were opposed to casting Brando due to his reputation for difficulty and his long string of box office flops. Brando also had "One-Eyed Jacks" working against him, a troubled production that lost money for Paramount when it was released in 1961. Paramount Pictures President Stanley Jaffe told an exasperated Coppola, "As long as I'm president of this studio, Marlon Brando will not be in this picture, and I will no longer allow you to discuss it."
Jaffe eventually set three conditions for the casting of Brando: That he would have to take a fee far below what he typically received; he'd have to agree to accept financial responsibility for any production delays his behavior cost; and he had to submit to a screen test. In order to skirt the humiliation of having Brando submit to a screen test, Coppola convinced Brando to a videotaped "make-up" test, in which Brando did his own makeup (he used cotton balls to simulate the puffed-cheek look). Coppola had feared Brando might be too young to play the Don, but was electrified by the actor's characterization as the head of a crime family. Even so, he had to fight the studio in order to cast the temperamental actor. Brando had doubts himself, stating in his autobiography, "I had never played an Italian before, and I didn't think I could do it successfully." Eventually, Charles Bluhdorn, the president of Paramount parent Gulf+Western, was won over to letting Brando have the role; when he saw the screen test, he asked in amazement, "What are we watching? Who is this old guinea?" Brando was signed for a low fee of $50,000, but in his contract, he was given a percentage of the gross on a sliding scale: 1% of the gross for each $10 million over a $10 million threshold, up to 5% if the picture exceeded $60 million. According to Evans, Brando sold back his points in the picture for $100,000 as he was in dire need of funds. "That $100,000 cost him $11 million," Evans claimed.
In a 1994 interview that can be found on the Academy of Achievement website, Coppola insisted, ""The Godfather" was a very unappreciated movie when we were making it. They were very unhappy with it. They didn't like the cast. They didn't like the way I was shooting it. I was always on the verge of getting fired." When word of this reached Brando, he threatened to walk off the picture, writing in his memoir, "I strongly believe that directors are entitled to independence and freedom to realize their vision, though Francis left the characterizations in our hands and we had to figure out what to do." In a 2010 television interview with Larry King, Al Pacino also talked about how Brando's support helped him keep the role of Michael Corleone in the movie—despite the fact director Francis Ford Coppola wanted to sack him. Brando was on his best behavior during filming, buoyed by an impressive cast that included Al Pacino, Robert Duvall, James Caan and Diane Keaton. In the "Vanity Fair" article "The Godfather Wars" Mark Seals writes, "With the actors, as in the movie, Brando served as the head of the family. He broke the ice by toasting the group with a glass of wine. 'When we were young, Brando was like the godfather of actors,' says Robert Duvall. 'I used to meet with Dustin Hoffman in Cromwell's Drugstore, and if we mentioned his name once, we mentioned it 25 times in a day.' Caan adds, 'The first day we met Brando everybody was in awe.'"
Brando's performance was glowingly reviewed by critics. "I thought it would be interesting to play a gangster, maybe for the first time in the movies, who wasn't like those bad guys Edward G. Robinson played, but who is kind of a hero, a man to be respected," Brando recalled in his autobiography. "Also, because he had so much power and unquestioned authority, I thought it would be an interesting contrast to play him as a gentle man, unlike Al Capone, who beat up people with baseball bats." Co-star Robert Duvall later marveled to A&E's "Biography", "He minimized the sense of beginning. In other words he, like, deemphasized the word "action". He would go in front of that camera just like he was before. "Cut!" It was all the same. There was really no beginning. I learned a lot from watching that." Brando won the Academy Award for Best Actor for his performance, but turned down the Oscar, becoming the second actor to refuse a Best Actor award (the first being George C. Scott for "Patton" in 1970). He boycotted the award ceremony, instead sending American Indian rights activist Sacheen Littlefeather, who appeared in full Apache attire, to state Brando's reasons, which were based on his objection to the depiction of American Indians by Hollywood and television.
The actor followed "The Godfather" with Bernardo Bertolucci's 1972 film "Last Tango in Paris" opposite Maria Schneider, but Brando's highly noted performance threatened to be overshadowed by an uproar over the sexual content of the film. Brando portrays a recent American widower named Paul, who begins an anonymous sexual relationship with a young, betrothed Parisian woman named Jeanne. As with previous films, Brando refused to memorize his lines for many scenes; instead, he wrote his lines on cue cards and posted them around the set for easy reference, leaving Bertolucci with the problem of keeping them out of the picture frame. The film features several intense, graphic scenes involving Brando, including Paul anally raping Jeanne using butter as a lubricant and Paul's angry, emotionally charged final confrontation with the corpse of his dead wife. The controversial movie was a hit, however, and Brando made the list of Top Ten Box Office Stars for the last time. The voting membership of the Academy of Motion Picture Arts & Sciences again nominated Brando for Best Actor, his seventh nomination. Although Brando won the 1973 New York Film Critics Circle Awards and the actor did not appear at the ceremony or send a representative to pick up the award if he won.
Critic Pauline Kael, in her famous "New Yorker" review, wrote "The movie breakthrough has finally come. Bertolucci and Brando have altered the face of an art form." Brando confessed in his autobiography, "To this day I can't say what "Last Tango in Paris" was about," and added the film "required me to do a lot of emotional arm wrestling with myself, and when it was finished, I decided that I wasn't ever again going to destroy myself emotionally to make a movie. I felt I had violated my innermost self and I didn't want to suffer like that anymore ... You can't fake it."
In 1973, Brando was devastated by the death of his childhood and best friend Wally Cox. Brando slept in Cox's pajamas and wrenched his ashes from his widow. She was going to sue for their return, but finally said "I think Marlon needs the ashes more than I do."
Late 1970s.
In 1976, Brando appeared in "The Missouri Breaks" with his friend Jack Nicholson. The movie also reunited the actor with director Arthur Penn. Following "The Godfather" and "Tango", Brando's performance was disappointing for some reviewers, who accused him of giving an erratic and inconsistent performance. As biographer Stefan Kanfer describes, Penn had difficulty controlling Brando, who seemed intent on going over the top with his border-ruffian-turned-contract-killer Robert E. Lee Clayton: "Marlon made him a cross-dressing psychopath. Absent for the first hour of the movie, Clayton enters on horseback, dangling upside down, caparisoned in white buckskin, Littlefeather-style. He speaks in an Irish accent for no apparent reason. Over the next hour, also for no apparent reason, Clayton assumes the intonation of a British upper-class twit and an elderly frontier woman, complete with a granny dress and matching bonnet. Penn, who believed in letting actors do their thing, indulged Marlon all the way." Critics were unkind, with "The Observer" calling Brando's performance "one of the most extravagant displays of "grandedamerie" since Sarah Bernhardt", while "The Sun" complained, "Marlon Brando at fifty-two has the sloppy belly of a sixty-two-year-old, the white hair of a seventy-two-year-old, and the lack of discipline of a precocious twelve-year-old." However, Kanfer noted: "Even though his late work was met with disapproval, a re-examination shows that often, in the middle of the most pedestrian scene, there would be a sudden, luminous occurrence, a flash of the old Marlon that showed how capable he remained."
In 1977, Brando made a rare appearance on television in the miniseries ', portraying George Lincoln Rockwell; he won a Primetime Emmy Award for Outstanding Supporting Actor in a Miniseries or a Movie for his performance. In 1978, he narrated the English version of "Raoni", a French-Belgian documentary film directed by Jean-Pierre Dutilleux and Luiz Carlos Saldanha that focused on the life of Raoni Metuktire and issues surrounding the survival of the indigenous Indian tribes of north central Brazil. Brando portrayed Superman's father Jor-El in the 1978 film "Superman". He agreed to the role only on assurance that he would be paid a large sum for what amounted to a small part, that he would not have to read the script beforehand, and that his lines would be displayed somewhere off-camera. It was revealed in a documentary contained in the 2001 DVD release of "Superman" that he was paid $3.7 million for two weeks of work. Brando also filmed scenes for the movie's sequel, "Superman II", but after producers refused to pay him the same percentage he received for the first movie, he denied them permission to use the footage. "I asked for my usual percentage," he recollected in his memoir, "but they refused, and so did I." However, after Brando's death, the footage was reincorporated into the 2006 re-cut of the film, ' and in the 2006 "loose sequel" "Superman Returns", in which both used and unused archive footage of him as Jor-El from the first two "Superman" films was remastered for a scene in the Fortress of Solitude, and Brando's voice-overs were used throughout the film. As the decade wore on, Brando was criticized more and more for only accepting what amounted to cameos in exchange for huge paydays.
Brando starred as Colonel Walter E. Kurtz in Francis Ford Coppola's Vietnam epic "Apocalypse Now" (1979). He plays a highly decorated U.S. Army Special Forces officer who goes renegade, running his own operation based in Cambodia and is feared by the U.S. military as much as the Vietnamese. Brando was paid $1 million a week for 3 weeks work. The film drew attention for its lengthy and troubled production, as Eleanor Coppola's documentary "Hearts of Darkness: A Filmmaker's Apocalypse" documents: Brando showed up on the set overweight, Martin Sheen suffered a heart attack, and severe weather destroyed several expensive sets. The film's release was also postponed several times while Coppola edited millions of feet of footage. In the documentary, Coppola talks about how astonished he was when an overweight Brando turned up for his scenes and, feeling desperate, decided to portray Kurtz, who appears emaciated in the original story, as a man who had indulged every aspect of himself. Coppola: "He was already heavy when I hired him and he promised me that he was going to get in shape and I imagined that I would, if he were heavy, I could use that. But he was "so" fat, he was very, very shy about it ... He was very, very adamant about how he didn't want to portray himself that way." Brando admitted to Coppola that he had not read the book, "Heart of Darkness", as the director had asked him to, and the pair spent days exploring the story and the character of Kurtz, much to the actor's financial benefit, according to producer Fred Roos: "The clock was ticking on this deal he had and we had to finish him within three weeks or we'd go into this very expensive overage ... And Francis and Marlon would be talking about the character and whole days would go by. And this is at Marlon's urging—and yet he's getting paid for it."
Upon release, "Apocalypse Now" earned critical acclaim, as did Brando's performance. Marlon's whispering of Kurtz's final words ""The horror! The horror!"", has become particularly famous. Roger Ebert, writing in the "Chicago Sun-Times", defended the movie's controversial "denouement", opining that the ending, "with Brando's fuzzy, brooding monologues and final violence, feels more satisfactory than any conventional ending possibly could."
Later work.
After appearing as oil tycoon Adam Steiffel in 1980's "The Formula", which was poorly received critically, Brando announced his retirement from acting. However he returned in 1989 in "A Dry White Season," based on André Brink's 1979 anti-apartheid novel. Brando agreed to do the film for free, but fell out with director Euzhan Palcy over how the film was edited; he even made a rare television appearance in an interview with Connie Chung to voice his disapproval. In his memoir, he maintained that Palcy "had cut the picture so poorly, I thought, that the inherent drama of this conflict was vague at best." Brando received praise for his performance, earning an Academy Award nomination for Best Supporting Actor and winning the Best Actor Award at the Tokyo Film Festival. Brando also scored enthusiastic reviews for his caricature of his Vito Corleone role as Carmine Sabatini in 1990's "The Freshman." In his original review, Roger Ebert wrote, "There have been a lot of movies where stars have repeated the triumphs of their parts—but has any star ever done it more triumphantly than Marlon Brando does in "The Freshman"?" Variety also praised Brando's performance as Sabatini and noted, "Marlon Brando's sublime comedy performance elevates "The Freshman" from screwball comedy to a quirky niche in film history." Brando also starred alongside his friend Johnny Depp in the box office hit "Don Juan DeMarco" (1995) and in Depp's controversial "The Brave" (1997), which was never released in the United States. Later performances, such as his appearance in "" (1992) (for which he won a Raspberry as "Worst Supporting Actor"), "The Island of Dr. Moreau" (in which he was nominated for another "Worst Actor" Raspberry) (1996), and his barely recognizable appearance in "Free Money" (1998), resulted in some of the worst reviews of his career. However, his last film, "The Score" (2001), was received generally positively. In the film, in which he portrays a fence, he starred with Robert De Niro, who had portrayed Vito Corleone in "The Godfather Part II". 
Brando conceived the idea of a novel called "Fan-Tan" with director Donald Cammell in 1979, which was not released until 2005.
Final years and death.
Brando's notoriety, his troubled family life, and his obesity attracted more attention than his late acting career. He gained a great deal of weight in the 1960s and by the early to mid-1990s he weighed over 300 lbs (136 kg) and suffered from Type 2 diabetes. He had a history of weight fluctuation throughout his career that, by and large, he attributed to his years of stress-related overeating followed by compensatory dieting. He also earned a reputation for being difficult on the set, often unwilling or unable to memorize his lines and less interested in taking direction than in confronting the film director with odd demands.
He also dabbled with some innovation in his last years. He had several patents issued in his name from the U.S. Patent and Trademark Office, all of which involve a method of tensioning drum heads, in June 2002 – November 2004. (For example, see and its equivalents).
In 2004, Brando recorded for the character Mrs. Sour in the unreleased animated film "Big Bug Man".
The actor was a longtime close friend of entertainer Michael Jackson and paid regular visits to his Neverland Ranch, resting there for weeks at a time. Brando also participated in the singer's two-day solo career 30th-anniversary celebration concerts in 2001, and starred in his 13-minute-long music video, "You Rock My World," in the same year. On Jackson's 30th anniversary concert, Brando gave a rambling speech to the audience on humanitarian work which received a poor reaction and was unaired.
The actor's son, Miko, was Jackson's bodyguard and assistant for several years, and was a friend of the singer. He stated, "The last time my father left his house to go anywhere, to spend any kind of time ... was with Michael Jackson. He loved it ... He had a 24-hour chef, 24-hour security, 24-hour help, 24-hour kitchen, 24-hour maid service." "Michael was instrumental helping my father through the last few years of his life. For that I will always be indebted to him. Dad had a hard time breathing in his final days, and he was on oxygen much of the time. He loved the outdoors, so Michael would invite him over to Neverland. Dad could name all the trees there, and the flowers, but being on oxygen it was hard for him to get around and see them all, it's such a big place. So Michael got Dad a golf cart with a portable oxygen tank so he could go around and enjoy Neverland. They'd just drive around—Michael Jackson, Marlon Brando, with an oxygen tank in a golf cart."
In April 2001, Brando was hospitalized with pneumonia.
In 2004, Brando signed with Tunisian film director Ridha Behi and began pre-production on a project to be titled "Brando and Brando". Up to a week before his death, he was working on the script in anticipation of a July/August 2004 start date. Production was suspended in July 2004 following Brando's death, at which time Behi stated that he would continue the film as an homage to Brando, with a new title of "Citizen Brando".
On July 1, 2004, Brando died of respiratory failure from pulmonary fibrosis with congestive heart failure at the UCLA Medical Center. He left behind 14 children (two of his children, Cheyenne and Dylan Brando, had predeceased him), as well as over 30 grandchildren. He was also survived by his sister Jocelyn. The cause of death was initially withheld, with his lawyer citing privacy concerns. He also suffered from failing eyesight caused by diabetes and liver cancer. Shortly before his death and despite needing an oxygen mask to breathe, he recorded his voice to appear in "", once again as Don Vito Corleone. However, Brando only recorded one line due to his health and an impersonator was hired to finish his lines. Some lines from his character were directly lifted from the film. He died three months before his "Superman" co-star Christopher Reeve.
Karl Malden—a fellow actor in "A Streetcar Named Desire", "On The Waterfront", and "One-Eyed Jacks" (the only film directed by Brando)—talks in a documentary accompanying the DVD of "A Streetcar Named Desire" about a phone call he received from Brando shortly before Brando's death. A distressed Brando told Malden he kept falling over. Malden wanted to come over, but Brando put him off, telling him there was no point. Three weeks later, Brando was dead. Shortly before his death, he had apparently refused permission for tubes carrying oxygen to be inserted into his lungs, which, he was told, was the only way to prolong his life.
Brando was cremated, and his ashes were put in with those of his childhood friend, comedian and actor Wally Cox and another longtime friend, Sam Gilman. They were then scattered partly in Tahiti and partly in Death Valley. In 2007, a 165-minute biopic of Brando for Turner Classic Movies, "Brando: The Documentary", produced by Mike Medavoy (the executor of Brando's will), was released.
Personal life.
Brando's children
Brando was known for his tumultuous personal life and his large number of wives, girlfriends and children. He was the father to sixteen known children, three of whom were adopted. Some sources claim he fathered as many as 17 children or more. In 1976 he told a French journalist "I too have had homosexual experiences, and I am not ashamed. I'd never paid much attention to what people think about me. Deep down I feel a bit ambiguous."
In "Songs My Mother Taught Me", Brando wrote he met Marilyn Monroe at a party where she played piano, unnoticed by anybody else there, that they had an affair and maintained an intermittent relationship for many years, and that he received a telephone call from her several days before she died. He also claimed numerous other romances, although he did not discuss his marriages, his wives, or his children in his autobiography. As a young man he was a family friend of Stella Adler's, whose daughter Ellen was a roommate of novelist Paula Fox. For a while, Brando and Fox lived under one roof and became close. Brando may have been the father of Fox's first child, daughter Linda Carroll, who was born in 1944. Linda was given up for adoption and is the mother of Courtney Love. Fox never named the child's father or addressed the affair.
Brando married actress Anna Kashfi in 1957. Kashfi was born in Calcutta and moved to Wales from India in 1947. She is said to have been the daughter of a Welsh steel worker of Irish descent, William O'Callaghan, who had been superintendent on the Indian State railways. However, in her book, "Brando for Breakfast", she claimed that she really is half Indian and that the press incorrectly thought that her stepfather, O'Callaghan, was her biological father. She said that her biological father was Indian and that she was the result of an "unregistered alliance" between her parents. Brando and Kashfi had a son, Christian Brando, on May 11, 1958; they divorced in 1959.
In 1960, Brando married Movita Castaneda, a Mexican-American actress seven years his senior; they were divorced in 1962. Castaneda had appeared in the first "Mutiny on the Bounty" film in 1935, some 27 years before the 1962 remake with Brando as Fletcher Christian. They had two children together: Miko Castaneda Brando (born 1961) and Rebecca Brando (born 1966).
Tahitian actress Tarita Teriipaia, who played Brando's love interest in "Mutiny on the Bounty", became his third wife on August 10, 1962. She was 20 years old, 18 years younger than Brando, who was reportedly delighted by her naïveté. Because Teriipaia was a native French speaker, Brando became fluent in the language and gave numerous interviews in French. Teriipaia became the mother of two of his children: Simon Teihotu Brando (born 1963) and Tarita Cheyenne Brando (born 1970). Brando also adopted Teriipaia's daughter, Maimiti Brando (born 1977) and niece, Raiatua Brando (born 1982). Brando and Teriipaia divorced in July 1972.
Brando had a long-term relationship with his housekeeper Maria Cristina Ruiz, by whom he had three children: Ninna Priscilla Brando (born May 13, 1989), Myles Jonathan Brando (born January 16, 1992), and Timothy Gahan Brando (born January 6, 1994). He had five more children by unidentified women: Stephen Blackehart (born 1967), Michael Gregor Gilman (born 1967), who was adopted by Brando's longtime friend Sam Gilman, and Dylan Brando (1968–1988), Warren Angelo Brando (born 1985), Angelique Brando. Brando also adopted Petra Brando-Corval (born 1972), the daughter of his assistant Caroline Barrett and novelist James Clavell.
His close friendship with Wally Cox was the subject of rumors. Brando told a journalist: "If Wally had been a woman, I would have married him and we would have lived happily ever after", and writer/editor Beauregard Houston-Montgomery has stated that while high on marijuana Brando confessed to him that Cox had been the love of his life. However, two of Cox's wives dismissed the suggestion that the love was more than platonic. 
Brando's grandson Tuki Brando (born 1990), son of Cheyenne Brando, is a fashion model. His numerous grandchildren also include Michael Brando (born 1988), son of Christian Brando, Prudence Brando and Shane Brando, children of Miko C. Brando, the children of Rebecca Brando, the three children of Teihotu Brando and the children of Michael Gregor Gilman, among others.
Death of Dag Drollet.
In May 1990, Dag Drollet, the Tahitian lover of Brando's daughter Cheyenne, died of a gunshot wound after a confrontation with Cheyenne's half-brother Christian at the family's hilltop home above Beverly Hills. Christian, then 31 years old, claimed he was drunk and the shooting was accidental. After heavily publicized pre-trial proceedings, Christian pleaded guilty to voluntary manslaughter and use of a gun. He was sentenced to 10 years in prison. Before the sentence, Brando delivered an hour of testimony, in which he said he and his former wife had failed Christian. He commented softly to members of the Drollet family: "I'm sorry ... If I could trade places with Dag, I would. I'm prepared for the consequences." Afterward, Drollet's father, Jacques, said he thought Brando was acting and his son was "getting away with murder." The tragedy was compounded in 1995, when Cheyenne, suffering from lingering effects of a serious car accident and said to still be depressed over Drollet's death, committed suicide by hanging herself in Tahiti. Christian Brando died of pneumonia at age 49 on January 26, 2008.
Lifestyle.
Brando earned a reputation as a 'bad boy' for his public outbursts and antics. According to "Los Angeles" magazine, "Brando was rock and roll before anybody knew what rock and roll was". His behavior during the filming of "Mutiny on the Bounty" (1962) seemed to bolster his reputation as a difficult star. He was blamed for a change in director and a runaway budget, though he disclaimed responsibility for either. On June 12, 1973, Brando broke paparazzo Ron Galella's jaw. Galella had followed Brando, who was accompanied by talk show host Dick Cavett, after a taping of "The Dick Cavett Show" in New York City. He reportedly paid a $40,000 out-of-court settlement and suffered an infected hand as a result. Galella wore a football helmet the next time he photographed Brando at a gala benefiting the American Indians Development Association.
The filming of "Mutiny on the Bounty" affected Brando's life in a profound way, as he fell in love with Tahiti and its people. He bought a 12-island atoll, Tetiaroa, and in 1970 hired an award-winning young Los Angeles architect, Bernard Judge, to build his home and natural village there without despoiling the environment. An environmental laboratory protecting sea birds and turtles was established and student groups welcomed there for many years. Tragically, the 1983 hurricane destroyed many of the structures including his resort. A hotel using Brando's name is currently under reconstruction under new ownership due to open in 2014. His son Simon is the only inhabitant of Tetiaroa. Brando was an active ham radio operator, with the call signs KE6PZH and FO5GJ (the latter from his island). He was listed in the Federal Communications Commission (FCC) records as Martin Brandeaux to preserve his privacy.
In the A&E "Biography" episode on Brando, biographer Peter Manso comments, "On the one hand, being a celebrity allowed Marlon to take his revenge on the world that had so deeply hurt him, so deeply scarred him. On the other hand he hated it because he knew it was false and ephemeral." In the same program biographer David Thomson relates, "Many, many people who worked with him, and came to work with him with the best intentions, went away in despair saying he's a spoiled kid. It has to be done his way or he goes away with some vast story about how he was wronged, he was offended, and I think that fits with the psychological pattern that he was a wronged kid."
Politics.
In 1946, Brando performed in Ben Hecht's Zionist play "A Flag is Born". He attended some fundraisers for John F. Kennedy in the 1960 presidential election. In August 1963, he participated in the March on Washington along with fellow celebrities Harry Belafonte, James Garner, Charlton Heston, Burt Lancaster and Sidney Poitier. Along with Paul Newman, Brando also participated in the freedom rides.
In the aftermath of the 1968 assassination of Martin Luther King, Jr., Brando made one of the strongest commitments to furthering King's work. Shortly after King's death, he announced that he was bowing out of the lead role of a major film ("The Arrangement") (1969) which was about to begin production in order to devote himself to the civil rights movement. "I felt I'd better go find out where it is; what it is to be black in this country; what this rage is all about," Brando said on the late-night ABC-TV talk show "Joey Bishop Show". In A&E's "Biography" episode on Brando actor and co-star Martin Sheen states, "I'll never forget the night that Reverend King was shot and I turned on the news and Marlon was walking through Harlem with Mayor Lindsay. And there were snipers and there was a lot of unrest and he kept walking and talking through those neighborhoods with Mayor Lindsay. It was one of the most incredible acts of courage I ever saw, and it meant a lot and did a lot."
Brando's participation in the African-American civil rights movement actually began well before King's death. In the early 1960s, he contributed thousands of dollars to both the Southern Christian Leadership Conference (S.C.L.C.) and to a scholarship fund established for the children of slain Mississippi N.A.A.C.P. leader Medgar Evers. By this time, Brando was already involved in films that carried messages about human rights: "Sayonara", which addressed interracial romance, and "The Ugly American", depicting the conduct of U.S. officials abroad and the deleterious effect on the citizens of foreign countries. For a time, he was also donating money to the Black Panther Party and considered himself a friend of founder Bobby Seale. Brando ended his financial support for the group over his perception of its increasing radicalization, specifically a passage in a Panther pamphlet put out by Eldridge Cleaver advocating indiscriminate violence, "for the Revolution."
At the 1973 Academy Awards ceremony, Brando refused to accept the Oscar for his performance in "The Godfather". Sacheen Littlefeather represented him at the ceremony. She appeared in full Apache attire and stated that owing to the "poor treatment of Native Americans in the film industry", Brando would not accept the award. This occurred while the standoff at Wounded Knee was ongoing. The event grabbed the attention of the US and the world media. This was considered a major event and victory for the movement by its supporters and participants.
Outside of his film work, Brando appeared before the California Assembly in support of a fair housing law and personally joined picket lines in demonstrations protesting discrimination in housing developments.
He was also an activist against apartheid. In 1964, he favored a boycott of his films in South Africa to prevent them from being shown to a segregated audience. He took part at a 1975 protest rally against American investments in South Africa and for the release of Nelson Mandela. In 1989, Brando also starred in the film "A Dry White Season", based upon André Brink's novel of the same name.
Comments on Jews, Hollywood and Israel.
In an interview in "Playboy" magazine in January 1979, Brando said: "You've seen every single race besmirched, but you never saw an image of the kike because the Jews were ever so watchful for that—and rightly so. They never allowed it to be shown on screen. The Jews have done so much for the world that, I suppose, you get extra disappointed because they didn't pay attention to that."
Brando made a similar comment on "Larry King Live" in April 1996, saying "Hollywood is run by Jews; it is owned by Jews, and they should have a greater sensitivity about the issue of—of people who are suffering. Because they've exploited—we have seen the—we have seen the nigger and greaseball, we've seen the chink, we've seen the slit-eyed dangerous Jap, we have seen the wily Filipino, we've seen everything, but we never saw the kike. Because they knew perfectly well, that that is where you draw the wagons around." Larry King, who is Jewish, replied, "When you say—when you say something like that, you are playing right in, though, to anti-Semitic people who say the Jews are—" Brando interrupted: "No, no, because I will be the first one who will appraise the Jews honestly and say 'Thank God for the Jews'."
Jay Kanter, Brando's agent, producer, and friend, defended him in "Daily Variety": "Marlon has spoken to me for hours about his fondness for the Jewish people, and he is a well-known supporter of Israel." Similarly, Louie Kemp, in his article for "Jewish Journal", wrote: "You might remember him as Don Vito Corleone, Stanley Kowalski or the eerie Col. Walter E. Kurtz in 'Apocalypse Now', but I remember Marlon Brando as a mensch and a personal friend of the Jewish people when they needed it most." In an interview with "NBC Today" one day after Brando's death, King also defended Brando's comments, saying that they had been blown out of proportion and taken out of context. In the 1940s, Brando had donated money to the Irgun, a Jewish paramilitary group.
Legacy.
Brando was one of the most respected actors of the post-war era. He is listed by the American Film Institute as the fourth greatest male star whose screen debut occurred before or during 1950 (it occurred in 1950). He earned respect among critics for his memorable performances and charismatic screen presence. His greatest contribution was helping to popularize Method acting. He is regarded as one of the greatest cinema actors of the 20th century.
"Encyclopedia Britannica" describes him as "the most celebrated of the method actors, and his slurred, mumbling delivery marked his rejection of classical dramatic training. His true and passionate performances proved him one of the greatest actors of his generation". It also notes the apparent paradox of his talent: "He is regarded as the most influential actor of his generation, yet his open disdain for the acting profession... often manifested itself in the form of questionable choices and uninspired performances. Nevertheless, he remains a riveting screen presence with a vast emotional range and an endless array of compulsively watchable idiosyncrasies."
Cultural influence.
Marlon Brando is a cultural icon whose popularity has endured for over six decades. His rise to national attention in the 1950s had a profound effect on American culture.
According to film critic Pauline Kael, "Brando represented a reaction against the post-war mania for security. As a protagonist, the Brando of the early fifties had no code, only his instincts. He was a development from the gangster leader and the outlaw. He was antisocial because he knew society was crap; he was a hero to youth because he was strong enough not to take the crap ... Brando represented a contemporary version of the free American ... Brando is still the most exciting American actor on the screen." Sociologist Dr. Suzanne Mcdonald-Walker states: "Marlon Brando, sporting leather jacket, jeans, and moody glare, became a cultural icon summing up 'the road' in all its maverick glory." His portrayal of the gang leader Johnny Strabler in "The Wild One" has become an iconic image, used both as a symbol of rebelliousness and a fashion accessory that includes a Perfecto style motorcycle jacket, a tilted cap, jeans and sunglasses. Johnny's haircut inspired a craze for sideburns, followed by James Dean and Elvis Presley, among others. Dean copied Brando's acting style extensively and Presley used Brando's image as a model for his role in "Jailhouse Rock". The "I coulda been a contender" scene from "On the Waterfront", according to the author of" Brooklyn Boomer", Martin H. Levinson, is "one of the most famous scenes in motion picture history, and the line itself has become part of America's cultural lexicon." Brando's powerful "Wild One" image was still, as of 2011, being marketed by the makers of his Triumph Thunderbird motorcycle, in a range of clothing inspired by his character from the film and licensed by Brando's estate.
Brando was also considered a male sex symbol. Linda Williams writes: "Marlon Brando the quintessential American male sex symbol of the late fifties and early sixties".
Views on acting.
Brando was notoriously reluctant to discuss acting in interviews, but many of his insights are on record. In his autobiography "Songs My Mother Taught Me", Brando observed:
He also confessed that, while having great admiration for the theater, he did not return to it after his initial success primarily because the work left him drained emotionally:
Brando repeatedly credited Stella Adler and her understanding of the Stanislavsky acting technique for bringing realism to American cinema, but also added:
In the 2015 documentary "Listen to Me Marlon", Brando shared his thoughts on playing a death scene, stating, "That's a tough scene to play. You have to make 'em believe that you are dying ... Try to think of the most intimate moment you've ever had in your life." Brando's favorite actors were Spencer Tracy, John Barrymore, Fredric March, James Cagney and Paul Muni. 
Financial legacy.
Upon his death in 2004, Brando left an estate valued at $21.6 million. Brando's estate still earns about $9 million a year, according to "Forbes". He was named one of the top-earning deceased celebrities in the world by the magazine.
Awards and honors.
Brando was named the fourth greatest male star whose screen debut occurred before or during 1950 by the American Film Institute, and part of "TIME" magazine's . He was also named one of the top 10 "Icons of the Century" by "Variety" magazine.
References.
Notes
Citations
Bibliography

</doc>
<doc id="19904" url="https://en.wikipedia.org/wiki?curid=19904" title="Meteorology">
Meteorology

Meteorology is the interdisciplinary scientific study of the atmosphere. The study of meteorology dates back millennia, though significant progress in meteorology did not occur until the 18th century. The 19th century saw modest progress in the field after weather observation networks were formed across broad regions. Prior attempts at prediction of weather depended on historical data. It wasn't until after the elucidation of the laws of physics and, more particularly, the development of the computer, allowing for the automated solution of the great many equations that model the weather, in the latter half of the 20th century that significant breakthroughs in weather forecasting were achieved.
Meteorological phenomena are observable weather events that are explained by the science of meteorology. Meteorological phenomena are described and quantified by the variables of Earth's atmosphere: temperature, air pressure, water vapor, mass flow, and the variations and interactions of those variables, and how they change over time. Different spatial scales are used to describe and predict weather on local, regional, and global levels.
Meteorology, climatology, atmospheric physics, and atmospheric chemistry are sub-disciplines of the atmospheric sciences. Meteorology and hydrology compose the interdisciplinary field of hydrometeorology. The interactions between Earth's atmosphere and its oceans are part of a coupled ocean-atmosphere system. Meteorology has application in many diverse fields such as the military, energy production, transport, agriculture, and construction.
The word "meteorology" is from Greek "metéōros" "lofty; high (in the sky)" (from "meta-" "above" and "aeiro" "I lift up") and "-logia" "-(o)logy", i.e. "the study of things in the air".
History.
The beginnings of meteorology can be traced back to ancient India, as the Upanishads contain serious discussion about the processes of cloud formation and rain and the seasonal cycles caused by the movement of Earth around the sun. Varāhamihira's classical work "Brihatsamhita", written about 500 AD, provides clear evidence that a deep knowledge of atmospheric processes existed even in those times.
In 350 BC, Aristotle wrote "Meteorology". Aristotle is considered the founder of meteorology. One of the most impressive achievements described in the "Meteorology" is the description of what is now known as the hydrologic cycle.
The book De Mundo (composed before 250 BC or between 350 and 200 BC) noted
The Greek scientist Theophrastus compiled a book on weather forecasting, called the "Book of Signs". The work of Theophrastus remained a dominant influence in the study of weather and in weather forecasting for nearly 2,000 years. In 25 AD, Pomponius Mela, a geographer for the Roman Empire, formalized the climatic zone system. According to Toufic Fahd, around the 9th century, Al-Dinawari wrote the "Kitab al-Nabat" ("Book of Plants"), in which he deals with the application of meteorology to agriculture during the Muslim Agricultural Revolution. He describes the meteorological character of the sky, the planets and constellations, the sun and moon, the lunar phases indicating seasons and rain, the "anwa" (heavenly bodies of rain), and atmospheric phenomena such as winds, thunder, lightning, snow, floods, valleys, rivers, lakes.
Research of visual atmospheric phenomena.
Ptolemy wrote on the atmospheric refraction of light in the context of astronomical observations. In 1021, Alhazen showed that atmospheric refraction is also responsible for twilight; he estimated that twilight begins when the sun is 19 degrees below the horizon, and also used a geometric determination based on this to estimate the maximum possible height of the Earth's atmosphere as 52,000 "passuum" (about 49 miles, or 79 km).
St. Albert the Great was the first to propose that each drop of falling rain had the form of a small sphere, and that this form meant that the rainbow was produced by light interacting with each raindrop. Roger Bacon was the first to calculate the angular size of the rainbow. He stated that a rainbow summit can not appear higher than 42 degrees above the horizon. In the late 13th century and early 14th century, Kamāl al-Dīn al-Fārisī and Theodoric of Freiberg were the first to give the correct explanations for the primary rainbow phenomenon. Theoderic went further and also explained the secondary rainbow. In 1716, Edmund Halley suggested that aurorae are caused by "magnetic effluvia" moving along the Earth's magnetic field lines.
Instruments and classification scales.
In 1441, King Sejong's son, Prince Munjong, invented the first standardized rain gauge. These were sent throughout the Joseon Dynasty of Korea as an official tool to assess land taxes based upon a farmer's potential harvest. In 1450, Leone Battista Alberti developed a swinging-plate anemometer, and was known as the first "anemometer". In 1607, Galileo Galilei constructed a thermoscope. In 1611, Johannes Kepler wrote the first scientific treatise on snow crystals: "Strena Seu de Nive Sexangula (A New Year's Gift of Hexagonal Snow)". In 1643, Evangelista Torricelli invented the mercury barometer. In 1662, Sir Christopher Wren invented the mechanical, self-emptying, tipping bucket rain gauge. In 1714, Gabriel Fahrenheit created a reliable scale for measuring temperature with a mercury-type thermometer. In 1742, Anders Celsius, a Swedish astronomer, proposed the "centigrade" temperature scale, the predecessor of the current Celsius scale. In 1783, the first hair hygrometer was demonstrated by Horace-Bénédict de Saussure. In 1802–1803, Luke Howard wrote "On the Modification of Clouds", in which he assigns cloud types Latin names. In 1806, Francis Beaufort introduced his system for classifying wind speeds. Near the end of the 19th century the first cloud atlases were published, including the "International Cloud Atlas", which has remained in print ever since. The April 1960 launch of the first successful weather satellite, TIROS-1, marked the beginning of the age where weather information became available globally.
Atmospheric composition research.
In 1648, Blaise Pascal rediscovered that atmospheric pressure decreases with height, and deduced that there is a vacuum above the atmosphere. In 1738, Daniel Bernoulli published "Hydrodynamics", initiating the Kinetic theory of gases and established the basic laws for the theory of gases. In 1761, Joseph Black discovered that ice absorbs heat without changing its temperature when melting. In 1772, Black's student Daniel Rutherford discovered nitrogen, which he called "phlogisticated air", and together they developed the phlogiston theory. In 1777, Antoine Lavoisier discovered oxygen and developed an explanation for combustion. In 1783, in Lavoisier's essay "Reflexions sur le phlogistique", he deprecates the phlogiston theory and proposes a caloric theory. In 1804, Sir John Leslie observed that a matte black surface radiates heat more effectively than a polished surface, suggesting the importance of black body radiation. In 1808, John Dalton defended caloric theory in "A New System of Chemistry" and described how it combines with matter, especially gases; he proposed that the heat capacity of gases varies inversely with atomic weight. In 1824, Sadi Carnot analyzed the efficiency of steam engines using caloric theory; he developed the notion of a reversible process and, in postulating that no such thing exists in nature, laid the foundation for the second law of thermodynamics.
Research into cyclones and air flow.
In 1494, Christopher Columbus experienced a tropical cyclone, which led to the first written European account of a hurricane. In 1686, Edmund Halley presented a systematic study of the trade winds and monsoons and identified solar heating as the cause of atmospheric motions. In 1735, an "ideal" explanation of global circulation through study of the trade winds was written by George Hadley. In 1743, when Benjamin Franklin was prevented from seeing a lunar eclipse by a hurricane, he decided that cyclones move in a contrary manner to the winds at their periphery. Understanding the kinematics of how exactly the rotation of the Earth affects airflow was partial at first. Gaspard-Gustave Coriolis published a paper in 1835 on the energy yield of machines with rotating parts, such as waterwheels. In 1856, William Ferrel proposed the existence of a circulation cell in the mid-latitudes, and the air within deflected by the Coriolis force resulting in the prevailing westerly winds. Late in the 19th century, the motion of air masses along isobars was understood to be the result of the large-scale interaction of the pressure gradient force and the deflecting force. By 1912, this deflecting force was named the Coriolis effect. Just after World War I, a group of meteorologists in Norway led by Vilhelm Bjerknes developed the Norwegian cyclone model that explains the generation, intensification and ultimate decay (the life cycle) of mid-latitude cyclones, and introduced the idea of fronts, that is, sharply defined boundaries between air masses. The group included Carl-Gustaf Rossby (who was the first to explain the large scale atmospheric flow in terms of fluid dynamics), Tor Bergeron (who first determined how rain forms) and Jacob Bjerknes.
Observation networks and weather forecasting.
In 1654, Ferdinando II de Medici established the first "weather observing" network, that consisted of meteorological stations in Florence, Cutigliano, Vallombrosa, Bologna, Parma, Milan, Innsbruck, Osnabrück, Paris and Warsaw. The collected data were sent to Florence at regular time intervals. In 1832, an electromagnetic telegraph was created by Baron Schilling. The arrival of the electrical telegraph in 1837 afforded, for the first time, a practical method for quickly gathering surface weather observations from a wide area. This data could be used to produce maps of the state of the atmosphere for a region near the Earth's surface and to study how these states evolved through time. To make frequent weather forecasts based on these data required a reliable network of observations, but it was not until 1849 that the Smithsonian Institution began to establish an observation network across the United States under the leadership of Joseph Henry. Similar observation networks were established in Europe at this time. Nineteenth century researchers in meteorology were drawn from military or medical backgrounds, rather than trained as dedicated scientists. In 1854, the United Kingdom government appointed Robert FitzRoy to the new office of "Meteorological Statist to the Board of Trade" with the task of gathering weather observations at sea. FitzRoy's office became the United Kingdom Meteorological Office in 1854, the first national meteorological service in the world. The first daily weather forecasts made by FitzRoy's Office were published in "The Times" newspaper in 1860. The following year a system was introduced of hoisting storm warning cones at principal ports when a gale was expected.
Over the next 50 years many countries established national meteorological services. The India Meteorological Department (1875) was established to follow tropical cyclone and monsoon. The Finnish Meteorological Central Office (1881) was formed from part of Magnetic Observatory of Helsinki University. Japan's Tokyo Meteorological Observatory, the forerunner of the Japan Meteorological Agency, began constructing surface weather maps in 1883. The United States Weather Bureau (1890) was established under the United States Department of Agriculture. The Australian Bureau of Meteorology (1906) was established by a Meteorology Act to unify existing state meteorological services.
Numerical weather prediction.
In 1904, Norwegian scientist Vilhelm Bjerknes first argued in his paper "Weather Forecasting as a Problem in Mechanics and Physics" that it should be possible to forecast weather from calculations based upon natural laws.
It was not until later in the 20th century that advances in the understanding of atmospheric physics led to the foundation of modern numerical weather prediction. In 1922, Lewis Fry Richardson published "Weather Prediction By Numerical Process", after finding notes and derivations he worked on as an ambulance driver in World War I. He described how small terms in the prognostic fluid dynamics equations that govern atmospheric flow could be neglected, and a numerical calculation scheme that could be devised to allow predictions. Richardson envisioned a large auditorium of thousands of people performing the calculations. However, the sheer number of calculations required was too large to complete without electronic computers, and the size of the grid and time steps used in the calculations led to unrealistic results. Though numerical analysis later found that this was due to numerical instability.
Starting in the 1950s, numerical forecasts with computers became feasible. The first weather forecasts derived this way used barotropic (single-vertical-level) models, and could successfully predict the large-scale movement of midlatitude Rossby waves, that is, the pattern of atmospheric lows and highs. In 1959, the UK Meteorological Office received its first computer, a Ferranti Mercury.
In the 1960s, the chaotic nature of the atmosphere was first observed and mathematically described by Edward Lorenz, founding the field of chaos theory. These advances have led to the current use of ensemble forecasting in most major forecasting centers, to take into account uncertainty arising from the chaotic nature of the atmosphere. Mathematical models used to predict the long term weather of the Earth (Climate models), have been developed that have a resolution today that are as coarse as the older weather prediction models. These climate models are used to investigate long-term climate shifts, such as what effects might be caused by human emission of greenhouse gases.
Meteorologists.
Meteorologists are scientists who study meteorology. The American Meteorological Society published and continually updates an authoritative electronic "Meteorology Glossary". Meteorologists work in government agencies, private consulting and research services, industrial enterprises, utilities, radio and television stations, and in education. In the United States, meteorologists held about 9,400 jobs in 2009.
Meteorologists are best known by the public for weather forecasting. Some radio and television weather forecasters are professional meteorologists, while others are reporters (weather specialist, weatherman, etc.) with no formal meteorological training. The American Meteorological Society and National Weather Association issue "Seals of Approval" to weather broadcasters who meet certain requirements.
Equipment.
Each science has its own unique sets of laboratory equipment. In the atmosphere, there are many things or qualities of the atmosphere that can be measured. Rain, which can be observed, or seen anywhere and anytime was one of the first atmospheric qualities measured historically. Also, two other accurately measured qualities are wind and humidity. Neither of these can be seen but can be felt. The devices to measure these three sprang up in the mid-15th century and were respectively the rain gauge, the anemometer, and the hygrometer. Many attempts had been made prior to the 15th century to construct adequate equipment to measure the many atmospheric variables. Many were faulty in some way or were simply not reliable. Even Aristotle noted this in some of his work; as the difficulty to measure the air.
Sets of surface measurements are important data to meteorologists. They give a snapshot of a variety of weather conditions at one single location and are usually at a weather station, a ship or a weather buoy. The measurements taken at a weather station can include any number of atmospheric observables. Usually, temperature, pressure, wind measurements, and humidity are the variables that are measured by a thermometer, barometer, anemometer, and hygrometer, respectively. Professional stations may also include air quality sensors (carbon monoxide, carbon dioxide, methane, ozone, dust, and smoke), ceilometer (cloud ceiling), falling precipitation sensor, flood sensor, lightning sensor, microphone (explosions, sonic booms, thunder), pyranometer/pyrheliometer/spectroradiometer (IR/Vis/UV photodiodes), rain gauge/snow gauge, scintillation counter (background radiation, fallout, radon), seismometer (earthquakes and tremors), transmissometer (visibility), and a GPS clock for data logging. Upper air data are of crucial importance for weather forecasting. The most widely used technique is launches of radiosondes. Supplementing the radiosondes a network of aircraft collection is organized by the World Meteorological Organization.
Remote sensing, as used in meteorology, is the concept of collecting data from remote weather events and subsequently producing weather information. The common types of remote sensing are Radar, Lidar, and satellites (or photogrammetry). Each collects data about the atmosphere from a remote location and, usually, stores the data where the instrument is located. Radar and Lidar are not passive because both use EM radiation to illuminate a specific portion of the atmosphere. Weather satellites along with more general-purpose Earth-observing satellites circling the earth at various altitudes have become an indispensable tool for studying a wide range of phenomena from forest fires to El Niño.
Spatial scales.
In the study of the atmosphere, meteorology can be divided into distinct areas that depend on both time and spatial scales. At one extreme of this scale is climatology. In the timescales of hours to days, meteorology separates into micro-, meso-, and synoptic scale meteorology. Respectively, the geospatial size of each of these three scales relates directly with the appropriate timescale.
Other subclassifications are used to describe the unique, local, or broad effects within those subclasses.
Microscale.
Microscale meteorology is the study of atmospheric phenomena on a scale of about or less. Individual thunderstorms, clouds, and local turbulence caused by buildings and other obstacles (such as individual hills) are modeled on this scale.
Mesoscale.
Mesoscale meteorology is the study of atmospheric phenomena that has horizontal scales ranging from 1 km to 1000 km and a vertical scale that starts at the Earth's surface and includes the atmospheric boundary layer, troposphere, tropopause, and the lower section of the stratosphere. Mesoscale timescales last from less than a day to the lifetime of the event, which in some cases can be weeks. The events typically of interest are thunderstorms, squall lines, fronts, precipitation bands in tropical and extratropical cyclones, and topographically generated weather systems such as mountain waves and sea and land breezes.
Synoptic scale.
Synoptic scale meteorology is generally large area dynamics referred to in horizontal coordinates and with respect to time. The phenomena typically described by synoptic meteorology include events like extratropical cyclones, baroclinic troughs and ridges, frontal zones, and to some extent jet streams. All of these are typically given on weather maps for a specific time. The minimum horizontal scale of synoptic phenomena is limited to the spacing between surface observation stations.
Global scale.
Global scale meteorology is the study of weather patterns related to the transport of heat from the tropics to the poles. Very large scale oscillations are of importance at this scale. These oscillations have time periods typically on the order of months, such as the Madden–Julian oscillation, or years, such as the El Niño–Southern Oscillation and the Pacific decadal oscillation. Global scale meteorology pushes into he range of climatology. The traditional definition of climate is pushed into larger timescales and with the understanding of the longer time scale global oscillations, their effect on climate and weather disturbances can be included in the synoptic and mesoscale timescales predictions.
Numerical Weather Prediction is a main focus in understanding air–sea interaction, tropical meteorology, atmospheric predictability, and tropospheric/stratospheric processes. The Naval Research Laboratory in Monterey, California, developed a global atmospheric model called Navy Operational Global Atmospheric Prediction System (NOGAPS). NOGAPS is run operationally at Fleet Numerical Meteorology and Oceanography Center for the United States Military. Many other global atmospheric models are run by national meteorological agencies.
Some meteorological principles.
Boundary layer meteorology.
Boundary layer meteorology is the study of processes in the air layer directly above Earth's surface, known as the atmospheric boundary layer (ABL). The effects of the surface – heating, cooling, and friction – cause turbulent mixing within the air layer. Significant movement of heat, matter, or momentum on time scales of less than a day are caused by turbulent motions. Boundary layer meteorology includes the study of all types of surface–atmosphere boundary, including ocean, lake, urban land and non-urban land for the study of meteorology.
Dynamic meteorology.
Dynamic meteorology generally focuses on the fluid dynamics of the atmosphere. The idea of air parcel is used to define the smallest element of the atmosphere, while ignoring the discrete molecular and chemical nature of the atmosphere. An air parcel is defined as a point in the fluid continuum of the atmosphere. The fundamental laws of fluid dynamics, thermodynamics, and motion are used to study the atmosphere. The physical quantities that characterize the state of the atmosphere are temperature, density, pressure, etc. These variables have unique values in the continuum.
Applications.
Weather forecasting.
Weather forecasting is the application of science and technology to predict the state of the atmosphere at a future time and given location. Humans have attempted to predict the weather informally for millennia and formally since at least the 19th century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere and using scientific understanding of atmospheric processes to project how the atmosphere will evolve.
Once an all-human endeavor based mainly upon changes in barometric pressure, current weather conditions, and sky condition, forecast models are now used to determine future conditions. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes mean that forecasts become less accurate as the difference in current time and the time for which the forecast is being made (the "range" of the forecast) increases. The use of ensembles and model consensus help narrow the error and pick the most likely outcome.
There are a variety of end uses to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to commodity traders within stock markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and the wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them.
Aviation meteorology.
Aviation meteorology deals with the impact of weather on air traffic management. It is important for air crews to understand the implications of weather on their flight plan as well as their aircraft, as noted by the Aeronautical Information Manual:
"The effects of ice on aircraft are cumulative—thrust is reduced, drag increases, lift lessens, and weight increases. The results are an increase in stall speed and a deterioration of aircraft performance. In extreme cases, 2 to 3 inches of ice can form on the leading edge of the airfoil in less than 5 minutes. It takes but 1/2 inch of ice to reduce the lifting power of some aircraft by 50 percent and increases the frictional drag by an equal percentage."
Agricultural meteorology.
Meteorologists, soil scientists, agricultural hydrologists, and agronomists are persons concerned with studying the effects of weather and climate on plant distribution, crop yield, water-use efficiency, phenology of plant and animal development, and the energy balance of managed and natural ecosystems. Conversely, they are interested in the role of vegetation on climate and weather.
Hydrometeorology.
Hydrometeorology is the branch of meteorology that deals with the hydrologic cycle, the water budget, and the rainfall statistics of storms. A hydrometeorologist prepares and issues forecasts of accumulating (quantitative) precipitation, heavy rain, heavy snow, and highlights areas with the potential for flash flooding. Typically the range of knowledge that is required overlaps with climatology, mesoscale and synoptic meteorology, and other geosciences.
The multidisciplinary nature of the branch can result in technical challenges, since tools and solutions from each of the individual disciplines involved may behave slightly differently, be optimized for different hard- and software platforms and use different data formats. There are some initiatives - such as the DRIHM project - that are trying to address this issue.
Nuclear meteorology.
Nuclear meteorology investigates the distribution of radioactive aerosols and gases in the atmosphere.
Maritime meteorology.
Maritime meteorology deals with air and wave forecasts for ships operating at sea. Organizations such as the Ocean Prediction Center, Honolulu National Weather Service forecast office, United Kingdom Met Office, and JMA prepare high seas forecasts for the world's oceans.
Military meteorology.
Military meteorology is the research and application of meteorology for military purposes. In the United States, the United States Navy's Commander, Naval Meteorology and Oceanography Command oversees meteorological efforts for the Navy and Marine Corps while the United States Air Force's Air Force Weather Agency is responsible for the Air Force and Army.
Environmental Meteorology.
Environmental meteorology mainly analyze industrial air pollution dispersion physically and chemically based on meteorological parameters such as temperature, humidity, wind, and various weather conditions.
Renewable Energy.
Meteorology applications in renewable energy includes basic research, "exploration", and potential mapping of wind power and solar radiation for wind and solar energy.
External links.
"Please see weather forecasting for weather forecast sites."

</doc>
<doc id="19908" url="https://en.wikipedia.org/wiki?curid=19908" title="Mount">
Mount

__NOTOC__
Mount may refer to:

</doc>
<doc id="19916" url="https://en.wikipedia.org/wiki?curid=19916" title="Meitnerium">
Meitnerium

Meitnerium is a chemical element with symbol Mt and atomic number 109. It is an extremely radioactive synthetic element (an element not found in nature that can be created in a laboratory). The most stable known isotope, meitnerium-278, has a half-life of 7.6 seconds. The GSI Helmholtz Centre for Heavy Ion Research near Darmstadt, Germany, first created this element in 1982. It is named for Lise Meitner.
In the periodic table, meitnerium is a d-block transactinide element. It is a member of the 7th period and is placed in the group 9 elements, although no chemical experiments have been carried out to confirm that it behaves as the heavier homologue to iridium in group 9. Meitnerium is calculated to have similar properties to its lighter homologues, cobalt, rhodium, and iridium.
History.
Discovery.
Meitnerium was first synthesized on August 29, 1982 by a German research team led by Peter Armbruster and Gottfried Münzenberg at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung) in Darmstadt. The team bombarded a target of bismuth-209 with accelerated nuclei of iron-58 and detected a single atom of the isotope meitnerium-266:
Naming.
Using Mendeleev's nomenclature for unnamed and undiscovered elements, meitnerium should be known as "eka-iridium". In 1979, during the Transfermium Wars (but before the synthesis of meitnerium), IUPAC published recommendations according to which the element was to be called "unnilennium" (with the corresponding symbol of "Une"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who either called it "element 109", with the symbol of "(109)" or even simply "109", or used the proposed name "meitnerium".
The naming of meitnerium was discussed in the element naming controversy regarding the names of elements 104 to 109, but "meitnerium" was the only proposal and thus was never disputed. The name "meitnerium" (Mt) was suggested in honor of the Austrian physicist Lise Meitner, a co-discoverer of protactinium (with Otto Hahn), and one of the discoverers of nuclear fission. In 1994 the name was recommended by IUPAC, and was officially adopted in 1997. It is thus the only element named specifically after a non-mythological woman (curium being named for both Pierre and Marie Curie).
Isotopes.
Meitnerium has no stable or naturally-occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Eight different isotopes of meitnerium have been reported with atomic masses 266, 268, 270, and 274–278, two of which, meitnerium-268 and meitnerium-270, have known but unconfirmed metastable states. Most of these decay predominantly through alpha decay, although some undergo spontaneous fission.
Stability and half-lives.
All meitnerium isotopes are extremely unstable and radioactive; in general, heavier isotopes are more stable than the lighter. The most stable known meitnerium isotope, 278Mt, is also the heaviest known; it has a half-life of 7.6 seconds. A metastable nuclear isomer, 270mMt, has been reported to also have a half-life of over a second. The isotopes 276Mt and 274Mt have half-lives of 0.72 and 0.44 seconds respectively. The remaining four isotopes have half-lives between 1 and 20 milliseconds. The undiscovered isotope 281Mt has been predicted to be the most stable towards beta decay; no known meitnerium isotope has been observed to undergo beta decay. Some unknown isotopes, such as 265Mt, 272Mt, 273Mt, and 279Mt, are predicted to have half-lives longer than the known isotopes. Before its discovery, 274Mt and 277Mt were predicted to have half-lives of 20 seconds and 1 minute respectively, but they were later found to have half-lives of only 0.44 seconds and 5 milliseconds respectively.
Predicted properties.
Chemical.
Meitnerium is the seventh member of the 6d series of transition metals. Since element 112 (copernicium) has been shown to be a transition metal, it is expected that all the elements from 104 to 112 would form a fourth transition metal series, with meitnerium as part of the platinum group metals. Calculations on its ionization potentials and atomic and ionic radii are similar to that of its lighter homologue iridium, thus implying that meitnerium's basic properties will resemble those of the other group 9 elements, cobalt, rhodium, and iridium.
Prediction of the probable chemical properties of meitnerium has not received much attention recently. Meitnerium is expected to be a noble metal. Based on the most stable oxidation states of the lighter group 9 elements, the most stable oxidation states of meitnerium are predicted to be the +6, +3, and +1 states, with the +3 state being the most stable in aqueous solutions. In comparison, rhodium and iridium show a maximum oxidation state of +6, while the most stable states are +4 and +3 for iridium and +3 for rhodium. The oxidation state +9, represented only by iridium in might be possible for its congener meitnerium in the nonafluoride (MtF9) and the [MtO4+ cation, although [IrO4]+ is expected to be more stable. The tetrahalides of meitnerium have also been predicted to have similar stabilities to those of iridium, thus also allowing a stable +4 state. It is further expected that the maximum oxidation states of elements from bohrium (element 107) to darmstadtium (element 110) may be stable in the gas phase but not in aqueous solution.
Physical and atomic.
Meitnerium is expected to be a solid under normal conditions and assume a face-centered cubic crystal structure, similarly to its lighter congener iridium. It should be a very heavy metal with a density of around 37.4 g/cm3, which would be the second-highest of any of the 118 known elements, second only to that predicted for its neighbor hassium (41 g/cm3). In comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm3. This results from meitnerium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough meitnerium to measure this quantity would be impractical, and the sample would quickly decay. Meitnerium is also predicted to be paramagnetic.
Theoreticians have predicted the covalent radius of meitnerium to be 6 to 10 pm larger than that of iridium. For example, the Mt–O bond distance is expected to be around 1.9 Å. The atomic radius of meitnerium is expected to be around 128 pm.
Experimental chemistry.
Unambiguous determination of the chemical characteristics of meitnerium has yet to have been established due to the short half-lives of meitnerium isotopes and a limited number of likely volatile compounds that could be studied on a very small scale. One of the few meitnerium compounds that are likely to be sufficiently volatile is meitnerium hexafluoride (), as its lighter homologue iridium hexafluoride () is volatile above 60 °C and therefore the analogous compound of meitnerium might also be sufficiently volatile; a volatile octafluoride () might also be possible. For chemical studies to be carried out on a transactinide, at least four atoms must be produced, the half-life of the isotope used must be at least 1 second, and the rate of production must be at least one atom per week. Even though the half-life of 278Mt, the most stable known meitnerium isotope, is 7.6 seconds, long enough to perform chemical studies, another obstacle is the need to increase the rate of production of meitnerium isotopes and allow experiments to carry on for weeks or months so that statistically significant results can be obtained. Separation and detection must be carried out continuously to separate out the meitnerium isotopes and automated systems can then experiment on the gas-phase and solution chemistry of meitnerium as the yields for heavier elements are predicted to be smaller than those for lighter elements; some of the separation techniques used for bohrium and hassium could be reused. However, the experimental chemistry of meitnerium has not received as much attention as that of the heavier elements from copernicium to livermorium.
The Lawrence Berkeley National Laboratory attempted to synthesize the isotope 271Mt in 2002–2003 for a possible chemical investigation of meitnerium because it was expected that it might be more stable than the isotopes around it as it has 162 neutrons, a magic number for deformed nuclei; its half-life was predicted to be a few seconds, long enough for a chemical investigation. However, no atoms of 271Mt were detected, and this isotope of meitnerium is currently unknown.
An experiment determining the chemical properties of a transactinide would need to compare a compound of that transactinide with analogous compounds of some of its lighter homologues: for example, in the chemical characterization of hassium, hassium tetroxide (HsO4) was compared with the analogous osmium compound, osmium tetroxide (OsO4). In a preliminary step towards determining the chemical properties of meitnerium, the GSI attempted sublimation of the rhodium compounds rhodium(III) oxide (Rh2O3) and rhodium(III) chloride (RhCl3). However, macroscopic amounts of the oxide would not sublimate until 1000 °C and the chloride would not until 780 °C, and then only in the presence of carbon aerosol particles: these temperatures are far too high for such procedures to be used on meitnerium, as most of the current methods used for the investigation of the chemistry of superheavy elements do not work above 500 °C.

</doc>
<doc id="19918" url="https://en.wikipedia.org/wiki?curid=19918" title="Megabyte">
Megabyte

The megabyte is a multiple of the unit byte for digital information. Its recommended unit symbol is MB, but sometimes "MByte" is used. The unit prefix "mega" is a multiplier of (106) in the International System of Units (SI). Therefore, one megabyte is one million bytes of information. This definition has been incorporated into the International System of Quantities.
However, in the computer and information technology fields, several other definitions are used that arose for historical reasons of convenience. A common usage has been to designate one megabyte as (220 B), a measurement that conveniently expresses the binary multiples inherent in digital computer memory architectures. However, most standards bodies have deprecated this usage in favor of a set of binary prefixes, in which this measurement is designated by the unit mebibyte (MiB). Less common is a measurement that used the megabyte to mean 1000×1024 () bytes.
Definitions.
The megabyte is commonly used to measure either 10002 bytes or 10242 bytes. The interpretation of using base 1024 originated as a compromise technical jargon for the byte multiples that needed to be expressed by the powers of 2 but lacked a convenient name. As 1024 (210) approximates 1000 (103), roughly corresponding to the SI prefix kilo-, it began to be used for binary multiples as well. In 1998 the International Electrotechnical Commission (IEC) proposed standards for binary prefixes requiring the use of megabyte to strictly denote 10002 bytes and mebibyte to denote 10242 bytes. By the end of 2009, the IEC Standard had been adopted by the IEEE, EU, ISO and NIST. Nevertheless, the term megabyte continues to be widely used with different meanings:
Semiconductor memory doubles in size for each address lane added to an integrated circuit package, which favors counts that are powers of two. The capacity of a disk drive is the product of the sector size, number of sectors per track, number of tracks per side, and the number of disk platters in the drive. Changes in any of these factors would not usually double the size. Sector sizes were set as powers of two (most common 512 bytes or 4096 bytes) for convenience in processing. It was a natural extension to give the capacity of a disk drive in multiples of the sector size, giving a mix of decimal and binary multiples when expressing total disk capacity.
Examples of use.
Depending on compression methods and file format, a megabyte of data can roughly be:
The human genome consists of DNA representing 800 MB of data. The parts that differentiate one person from another can be compressed to 4 MB.

</doc>
<doc id="19919" url="https://en.wikipedia.org/wiki?curid=19919" title="Monosaccharide">
Monosaccharide

Monosaccharides (from Greek "monos": single, "sacchar": sugar), also called simple sugars, are the most basic units of carbohydrates. They are fundamental units of carbohydrates and cannot be further hydrolised to simpler compounds. The general formula is . They are the simplest form of sugar and are usually colorless, water-soluble, and crystalline solids. Some monosaccharides have a sweet taste. Examples of monosaccharides include glucose (dextrose), fructose (levulose) and galactose. Monosaccharides are the building blocks of disaccharides (such as sucrose and lactose) and polysaccharides (such as cellulose and starch). Further, each carbon atom that supports a hydroxyl group (so, all of the carbons except for the primary and terminal carbon) is chiral, giving rise to a number of isomeric forms, all with the same chemical formula. For instance, galactose and glucose are both aldohexoses, but have different physical structures and chemical properties.
Structure and nomenclature.
With few exceptions (e.g., deoxyribose), monosaccharides have this chemical formula: C"x"(H2O)"y", where conventionally "x" ≥ 3. Monosaccharides can be classified by the number "x" of carbon atoms they contain: diose (2) triose (3) tetrose (4), pentose (5), hexose (6), heptose (7), and so on.
The most important monosaccharide, glucose, is a hexose. Examples of heptoses include the ketoses mannoheptulose and sedoheptulose. Monosaccharides with eight or more carbons are rarely observed as they are quite unstable.
Linear-chain monosaccharides.
Simple monosaccharides have a linear and unbranched carbon skeleton with one carbonyl (C=O) functional group, and one hydroxyl (OH) group on each of the remaining carbon atoms. Therefore, the molecular structure of a simple monosaccharide can be written as H(CHOH)"n"(C=O)(CHOH)"m"H, where "n" + 1 + "m" = "x"; so that its elemental formula is C"x"H2"x"O"x".
By convention, the carbon atoms are numbered from 1 to "x" along the backbone, starting from the end that is closest to the C=O group.
If the carbonyl is at position 1 (that is, "n" or "m" is zero), the molecule begins with a formyl group H(C=O)− and is technically an aldehyde. In that case, the compound is termed an aldose. Otherwise, the molecule has a keto group, a carbonyl −(C=O)− between two carbons; then it is formally a ketone, and is termed a ketose. Ketoses of biological interest usually have the carbonyl at position 2.
The various classifications above can be combined, resulting in names such as "aldohexose" and "ketotriose".
A more general nomenclature for open-chain monosaccharides combines a Greek prefix to indicate the number of carbons (tri-, tetr-, pent-, hex-, etc.) with the suffixes "-ose" for aldoses and "-ulose" for ketoses. In the latter case, if the carbonyl is not at position 2, its position is then indicated by a numeric infix. So, for example, H(C=O)(CHOH)4H is pentose, H(CHOH)(C=O)(CHOH)3H is pentulose, and H(CHOH)2(C=O)(CHOH)2H is pent-3-ulose.
Open-chain stereoisomers.
Two monosaccharides with equivalent molecular graphs (same chain length and same carbonyl position) may still be distinct stereoisomers, whose molecules differ in the three-dimensional arrangement of the bonds of certain atoms. This happens only if the molecule contains a stereogenic center, specifically a carbon atom that is chiral (connected to four distinct molecular sub-structures). Those four bonds can have any of two configurations in space distinguished by their handedness. In a simple open-chain monosaccharide, every carbon is chiral except the first and the last atoms of the chain, and (in ketoses) the carbon with the keto group.
For example, the triketose H(CHOH)(C=O)(CHOH)H (glycerone, dihydroxyacetone) has no stereogenic center, and therefore exists as a single stereoisomer. The other triose, the aldose H(C=O)(CHOH)2H (glyceraldehyde), has one chiral carbon — the central one, number 2 — which is bonded to groups −H, −OH, −C(OH)H2, and −(C=O)H. Therefore, it exists as two stereoisomers whose molecules are mirror images of each other (like a left and a right glove). Monosaccharides with four or more carbons may contain multiple chiral carbons, so they typically have more than two stereoisomers. The number of distinct stereoisomers with the same diagram is bounded by 2"c", where "c" is the total number of chiral carbons.
The Fischer projection is a systematic way of drawing the skeletal formula of an acyclic monosaccharide so that the handedness of each chiral carbon is well specified. Each stereoisomer of a simple open-chain monosaccharide can be identified by the positions (right or left) in the Fischer diagram of the chiral hydroxyls (the hydroxyls attached to the chiral carbons).
Most stereoisomers are themselves chiral (distinct from their mirror images). In the Fischer projection, two mirror-image isomers differ by having the positions of all chiral hydroxyls reversed right-to-left. Mirror-image isomers are chemically identical in non-chiral environments, but usually have very different biochemical properties and occurrences in nature.
While most stereoisomers can be arranged in pairs of mirror-image forms, there are some non-chiral stereoisomers that are identical to their mirror images, in spite of having chiral centers. This happens whenever the molecular graph is symmetrical, as in the 3-ketopentoses H(CHOH)2(CO)(CHOH)2H, and the two halves are mirror images of each other. In that case, mirroring is equivalent to a half-turn rotation. For this reason, there are only three distinct 3-ketopentose stereoisomers, even though the molecule has two chiral carbons.
Distinct stereoisomers that are not mirror-images of each other usually have different chemical properties, even in non-chiral environments. Therefore, each mirror pair and each non-chiral stereoisomer may be given a specific monosaccharide name. For example, there are 16 distinct aldohexose stereoisomers, but the name "glucose" means a specific pair of mirror-image aldohexoses. In the Fischer projection, one of the two glucose isomers has the hydroxyl at left on C3, and at right on C4 and C5; while the other isomer has the reversed pattern. These specific monosaccharide names have conventional three-letter abbreviations, like "Glu" for glucose and "Thr" for threose.
Generally, a monosaccharide with "n" asymmetrical carbons has 2"n" stereoisomers. The number of open chain stereoisomers for an aldose monosaccharide is larger by one than that of a ketose monosaccharide of the same length. Every ketose will have 2("n"−3) stereoisomers where "n" > 2 is the number of carbons. Every aldose will have 2("n"−2) stereoisomers where "n" > 2 is the number of carbons.
These are also referred to as epimers which have the different arrangement of −OH and −H groups at the asymmetric or chiral carbon atoms (this does not apply to those carbons having the carbonyl functional group).
Chirality nomenclature.
Like many chiral molecules, the two stereoisomers of glyceraldehyde will gradually rotate the polarization direction of linearly polarized light as it passes through it, even in solution. The two stereoisomers are identified with the prefixes D- and L-, according to the sense of rotation: D-glyceraldehyde is dextrorotatory (rotates the polarization axis clockwise), while L-glyceraldehyde is levorotatory (rotates it counterclockwise).
The D- and L- prefixes are also used with other monosaccharides, to distinguish two particular stereoisomers that are mirror-images of each other. For this purpose, one considers the chiral carbon that is furthest removed from the C=O group. Its four bonds must connect to −H, −OH, −C(OH)H, and the rest of the molecule. If the molecule can be rotated in space so that the directions of those four groups match those of the analog groups in D-glyceraldehyde's C2, then the isomer receives the D- prefix. Otherwise, it receives the L- prefix.
In the Fischer projection, the D- and L- prefixes specifies the configuration at the carbon atom that is second from bottom: D- if the hydroxyl is on the right side, and L- if it is on the left side.
Note that the D- and L- prefixes do not indicate the direction of rotation of polarized light, which is a combined effect of the arrangement at all chiral centers. However, the two enantiomers will always rotate the light in opposite directions, by the same amount. See also .
Cyclic isomers.
A monosaccharide often switches from the acyclic (open-chain) form to a cyclic form, through a nucleophilic addition reaction between the carbonyl group and one of the hydroxyls of the same molecule. The reaction creates a ring of carbon atoms closed by one bridging oxygen atom. The resulting molecule has an hemiacetal or hemiketal group, depending on whether the linear form was an aldose or a ketose. The reaction is easily reversed, yielding the original open-chain form.
In these cyclic forms, the ring usually has 5 or 6 atoms. These forms are called furanoses and pyranoses, respectively — by analogy with furan and pyran, the simplest compounds with the same carbon-oxygen ring (although they lack the double bonds of these two molecules). For example, the aldohexose glucose may form a hemiacetal linkage between the hydroxyl on carbon 1 and the oxygen on carbon 4, yielding a molecule with a 5-membered ring, called glucofuranose. The same reaction can take place between carbons 1 and 5 to form a molecule with a 6-membered ring, called glucopyranose. Cyclic forms with a 7-atom ring (the same of oxepane), rarely encountered, are called heptoses.
For many monosaccharides (including glucose), the cyclic forms predominate, in the solid state and in solutions, and therefore the same name commonly is used for the open- and closed-chain isomers. Thus, for example, the term "glucose" may signify glucofuranose, glucopyranose, the open-chain form, or a mixture of the three.
Cyclization creates a new stereogenic center at the carbonyl-bearing carbon. The −OH group that replaces the carbonyl's oxygen may end up in two distinct positions relative to the ring's midplane. Thus each open-chain monosaccharide yields two cyclic isomers (anomers), denoted by the prefixes α- and β-. The molecule can change between these two forms by a process called mutarotation, that consists in a reversal of the ring-forming reaction followed by another ring formation.
Haworth projection.
The stereochemical structure of a cyclic monosaccharide can be represented in a Haworth projection. In this diagram, the α-isomer has the -OH of the anomeric carbon below the plane of the carbon atoms, and the β-isomer has the -OH of the anomeric carbon above the plane. Pyranoses typically adopt a chair conformation, similar to that of cyclohexane. In this conformation, the α-isomer has the -OH of the anomeric carbon in an axial position, whereas the β-isomer has the OH- of the anomeric carbon in equatorial position.
Derivatives.
A large number of biologically important modified monosaccharides exist:

</doc>
<doc id="19924" url="https://en.wikipedia.org/wiki?curid=19924" title="Microscopium">
Microscopium

Microscopium is a minor constellation in the Southern Celestial Hemisphere, one of twelve created in the 18th century by French astronomer Nicolas Louis de Lacaille and one of several depicting scientific instruments. Its name is a Latinised form of the Greek word for microscope. Its stars are faint and hardly visible from most of the non-tropical Northern Hemisphere.
The constellation's brightest star is Gamma Microscopii of apparent magnitude 4.68, a yellow giant located around 381 light-years distant. Two star systems—WASP-7 and HD 205739—have planets, while two others—the young red dwarf star AU Microscopii and the sunlike HD 202628—have debris disks. AU Microscopii and the binary red dwarf system AT Microscopii are probably a wide triple system and members of the Beta Pictoris moving group. Nicknamed "Speedy Mic", BO Microscopii is a star with an extremely fast rotation period of 9 hours 7 minutes.
Characteristics.
Microscopium is a small constellation bordered by Capricornus to the north, Piscis Austrinus and Grus to the west, Sagittarius to the east, and Indus to the south, touching on Telescopium to the southeast. The recommended three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Mic'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of four segments ("illustrated in infobox"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −27.45° and −45.09°. The whole constellation is visible to observers south of latitude 45°N. Given that its brightest stars are of fifth magnitude, the constellation is invisible to the naked eye in areas with polluted skies.
Notable features.
Stars.
French astronomer Nicolas Louis de Lacaille charted and designated ten stars with the Bayer designations Alpha through to Iota in 1756. A star in neighbouring Indus that Lacaille had labelled Nu Indi turned out to be in Microscopium, so Gould renamed it Nu Microscopii. Francis Baily considered Gamma and Epsilon Microscopii to belong to the neighbouring constellation Piscis Austrinus, but subsequent cartographers did not follow this. In his 1725 "Catalogus Britannicus", John Flamsteed labelled the stars 1, 2, 3 and 4 Piscis Austrini, which became Gamma Microscopii, HR 8076, HR 8110 and Epsilon Microscopii respectively. Within the constellation's borders, there are 43 stars brighter than or equal to apparent magnitude 6.5.
Depicting the eyepiece of the microscope is Gamma Microscopii, which—at magnitude of 4.68—is the brightest star in the constellation. Having spent much of its 620-million-year lifespan as a blue-white main sequence star, it has swollen and cooled to become a yellow giant of spectral type G6III, with a diameter ten times that of the Sun. Measurement of its parallax yields a distance of 229±4 light years from Earth. Alpha Microscopii is also an ageing yellow giant star of spectral type G7III with an apparent magnitude of 4.90. Located 380±30 light-years away from Earth, it has swollen to 17.5 times the diameter of the Sun. Alpha has a 10th magnitude companion, visible in 7.5 cm telescopes, though this is a coincidental closeness rather than a true binary system. Epsilon Microscopii lies 182±2 light years away, and is a white star of apparent magnitude 4.7, and spectral type A1V. Theta1 and Theta2 Microscopii make up a wide double whose components are splittable to the naked eye. Both are white A-class magnetic spectrum variable stars with strong metallic lines, similar to Cor Caroli. They mark the constellation's specimen slide.
Many notable objects are too faint to be seen with the naked eye. AX Microscopii, better known as Lacaille 8760, is a red dwarf which lies only 12.9 light-years from the Solar System. At magnitude 6.68, it is the brightest red dwarf in the sky. BO Microscopii is a rapidly rotating star that has 80% the diameter of the Sun. Nicknamed "Speedy Mic", it has a rotation period of 9 hours 7 minutes. An active star, it has prominent stellar flares that average 100 times stronger than those of the Sun, and are emitting energy mainly in the X-ray and ultraviolet bands of the spectrum. AT Microscopii is a binary star system, both members of which are flare star red dwarfs. The system lies close to and may form a very wide triple system with AU Microscopii, a young star which appears to be a planetary system in the making with a debris disk. The three stars are candidate members of the Beta Pictoris moving group, one of the nearest associations of stars that share a common motion through space.
The Astronomical Society of Southern Africa in 2003 reported that observations of four of the Mira variables in Microscopium were very urgently needed as data on their light curves was incomplete. Two of them—R and S Microscopii—are challenging stars for novice amateur astronomers, and the other two U and RY Microscopii are more difficult still. Another red giant, T Microscopii, is a semiregular variable that ranges between magnitudes 7.7 and 9.6 over 344 days. Of apparent magnitude 11, DD Microscopii is a symbiotic star system composed of an orange giant of spectral type K2III and white dwarf in close orbit, with the smaller star ionizing the stellar wind of the larger star. The system has a low metallicity. Combined with its high galactic latitude, this indicates that the star system has its origin in the galactic halo of the Milky Way.
HD 205739 is a yellow-white main sequence star of spectral type F7V that is around 1.22 times as massive and 2.3 times as luminous as the Sun. It has a Jupiter-sized planet with an orbital period of 280 days that was discovered by the radial velocity method. WASP-7 is a star of spectral type F5V with an apparent magnitude of 9.54, about 1.28 times as massive as the Sun. Its hot Jupiter planet—WASP-7b—was discovered by transit method and found to orbit the star every 4.95 days. HD 202628 is a sunlike star of spectral type G2V with a debris disk that ranges from 158 to 220 AU distant. Its inner edge is sharply defined, indicating a probable planet orbiting between 86 and 158 AU from the star.
Deep sky objects.
Describing Microscopium as "totally unremarkable", astronomer Patrick Moore concluded there was nothing of interest for amateur observers. NGC 6925 is a barred spiral galaxy of apparent magnitude 11.3 which is lens-shaped, as it lies almost edge-on to observers on Earth, 3.7 degrees west-northwest of Alpha Microscopii. SN 2011ei, a Type II Supernova in NGC 6925, was discovered by Stu Parker in New Zealand in July 2011. NGC 6923 lies nearby and is a magnitude fainter still. The Microscopium Void is a roughly rectangular region of relatively empty space, bounded by incomplete sheets of galaxies from other voids. The Microscopium Supercluster is an overdensity of galaxy clusters that was first noticed in the early 1990s. The component Abell clusters 3695 and 3696 are likely to be gravitationally bound, while the relations of Abell clusters 3693 and 3705 in the same field are unclear.
Meteor showers.
The Microscopids are a minor meteor shower that appear from June to mid-July.
History.
The stars that comprise Microscopium are in a region previously considered the hind feet of Sagittarius, a neighbouring constellation. John Ellard Gore wrote that al-Sufi seems to have reported that Ptolemy had seen the stars but he (Al Sufi) did not pinpoint their positions. Microscopium itself was introduced in 1751–52 by Lacaille with the French name "le Microscope", after he had observed and catalogued 10,000 southern stars during a two-year stay at the Cape of Good Hope. He devised fourteen new constellations in uncharted regions of the Southern Celestial Hemisphere not visible from Europe. All but one honoured instruments that symbolised the Age of Enlightenment. Commemorating the compound microscope, the Microscope's name had been Latinised by Lacaille to "Microscopium" by 1763.

</doc>
<doc id="19925" url="https://en.wikipedia.org/wiki?curid=19925" title="IC 342/Maffei Group">
IC 342/Maffei Group

The IC 342/Maffei Group (also known as the "IC 342 Group" or the "Maffei 1 Group") is the nearest group of galaxies to the Local Group. The group can be described as a binary group; the member galaxies are mostly concentrated around either IC 342 or Maffei 1, both of which are the brightest galaxies within the group. The group is one of many located within the Virgo Supercluster (i.e. the Local Supercluster).
Members.
The table below lists galaxies that have been identified as associated with the IC342/Maffei 1 Group by I. D. Karachentsev. Note that Karachentsev divides this group into two subgroups centered around IC 342 and Maffei 1.
Additionally, KKH 37 is listed as possibly being a member of the IC 342 Subgroup, and KKH 6 is listed as possibly being a member of the Maffei 1 Subgroup.
Foreground dust obscuration.
As seen from Earth, the group lies near the plane of the Milky Way (a region sometimes called the Zone of Avoidance). Consequently, the light from many of the galaxies is severely affected by dust obscuration within the Milky Way. This complicates observational studies of the group, as uncertainties in the dust obscuration also affect measurements of the galaxies' luminosities and distances as well as other related quantities.
Moreover, the galaxies within the group have historically been difficult to identify. Many galaxies have only been discovered using late 20th century astronomical instrumentation. For example, while many fainter, more distant galaxies, such as the galaxies in the New General Catalogue, were already identified visually by the end of the nineteenth century, Maffei 1 and Maffei 2 were only discovered in 1968 using infrared photographic images of the region. Furthermore, it is difficult to determine whether some objects near IC 342 or Maffei 1 are galaxies associated with the IC 342/Maffei Group or diffuse foreground objects within the Milky Way that merely look like galaxies. For example, the objects MB 2 and Camelopardalis C were once thought to be dwarf galaxies in the IC 342/Maffei Group but are now known to be objects within the Milky Way.
Group formation and possible interactions with the Local Group.
Since the IC 342/Maffei Group and the Local Group are located physically close to each other, the two groups may have influenced each other's evolution during the early stages of galaxy formation. An analysis of the velocities and distances to the IC 342/Maffei Group as measured by M. J. Valtonen and collaborators suggested that IC 342 and Maffei 1 were moving faster than what could be accounted for in the expansion of the universe. They therefore suggested that IC 342 and Maffei 1 were ejected from the Local Group after a violent gravitational interaction with the Andromeda Galaxy during the early stages of the formation of the two groups.
However, this interpretation is dependent on the distances measured to the galaxies in the group, which in turn is dependent on accurately measuring the degree to which interstellar dust in the Milky Way obscures the group. More recent observations have demonstrated that the dust obscuration may have been previously overestimated, so the distances may have been underestimated. If these new distance measurements are correct, then the galaxies in the IC 342/Maffei Group appear to be moving at the rate expected from the expansion of the universe, and the scenario of a collision between the IC 342/Maffei Group and the Local Group would be implausible.

</doc>
<doc id="19926" url="https://en.wikipedia.org/wiki?curid=19926" title="M81 Group">
M81 Group

The M81 Group is a galaxy group in the constellations Ursa Major and Camelopardalis that includes the well-known galaxies Messier 81 and Messier 82, as well as several other galaxies with high apparent brightnesses. The approximate center of the group is located at a distance of 3.6 Mpc, making it one of the nearest groups to the Local Group. The group is estimated to have a total mass of (1.03 ± 0.17).
The M81 Group, the Local Group, and other nearby groups all lie within the Virgo Supercluster (i.e. the Local Supercluster).
Members.
The table below lists galaxies that have been identified as associated with the M81 Group by I. D. Karachentsev.
Note that the object names used in the above table differ from the names used by Karachentsev. NGC, IC, UGC, and PGC numbers have been used in many cases to allow for easier referencing.
Interactions within the group.
Messier 81, Messier 82, and NGC 3077 are all strongly interacting with each other. The gravitational interactions have stripped some hydrogen gas away from all three galaxies, leading to the formation of filamentary gas structures within the group. Moreover, the interactions have also caused some interstellar gas to fall into the centers of Messier 82 and NGC 3077, which has led to strong starburst activity (or the formation of many stars) within the centers of these two galaxies.

</doc>
<doc id="19929" url="https://en.wikipedia.org/wiki?curid=19929" title="Mensa">
Mensa

Mensa or Mensae (genitive form) may refer to:

</doc>
<doc id="19930" url="https://en.wikipedia.org/wiki?curid=19930" title="Metre (poetry)">
Metre (poetry)

In poetry, metre (meter in US spelling) is the basic rhythmic structure of a verse or lines in verse. Many traditional verse forms prescribe a specific verse metre, or a certain set of metres alternating in a particular order. The study and the actual use of metres and forms of versification are both known as prosody. (Within linguistics, "prosody" is used in a more general sense that includes not only poetic metre but also the rhythmic aspects of prose, whether formal or informal, that vary from language to language, and sometimes between poetic traditions.)
Qualitative vs. quantitative metre.
The metre of most poetry of the Western world and elsewhere is based on patterns of syllables of particular types. The familiar type of metre in English-language poetry is called qualitative metre, with stressed syllables coming at regular intervals (e.g. in iambic pentameters, usually every even-numbered syllable). Many Romance languages use a scheme that is somewhat similar but where the position of only one particular stressed syllable (e.g. the last) needs to be fixed. The metre of the old Germanic poetry of languages such as Old Norse and Old English was radically different, but was still based on stress patterns.
Some classical languages, in contrast, used a different scheme known as quantitative metre, where patterns were based on syllable weight rather than stress. In the dactylic hexameters of Classical Latin and Classical Greek, for example, each of the six feet making up the line was either a dactyl (long-short-short) or a spondee (long-long): a "long syllable" was literally one that took longer to pronounce than a short syllable: specifically, a syllable consisting of a long vowel or diphthong or followed by two consonants. The stress pattern of the words made no difference to the metre. A number of other ancient languages also used quantitative metre, such as Sanskrit and Classical Arabic (but not Biblical Hebrew).
Finally, non-stressed languages that have little or no differentiation of syllable length, such as French or Chinese, base their verses on the number of syllables only. The most common form in French is the Alexandrine, with twelve syllables a verse, and in classical Chinese five characters, and thus five syllables. But since each Chinese character is pronounced using one syllable in a certain tone, classical Chinese poetry also had more strictly defined rules, such as parallelism or antithesis between lines.
Feet.
In many Western classical poetic traditions, the metre of a verse can be described as a sequence of "feet", each foot being a specific sequence of syllable types — such as relatively unstressed/stressed (the norm for English poetry) or long/short (as in most classical Latin and Greek poetry).
Iambic pentameter, a common metre in English poetry, is based on a sequence of five "iambic feet" or "iambs", each consisting of a relatively unstressed syllable (here represented with "×" above the syllable) followed by a relatively stressed one (here represented with "/" above the syllable) — "da-DUM" = "× /" :
This approach to analyzing and classifying metres originates from Ancient Greek tragedians and poets such as Homer, Pindar, Hesiod, and Sappho.
However some metres have an overall rhythmic pattern to the line that cannot easily be described using feet. This occurs in Sanskrit poetry; see Vedic metre and Sanskrit metre. (Although this poetry is in fact specified using feet, each "foot" is more or less equivalent to an entire line.) It also occurs in some Western metres, such as the hendecasyllable favoured by Catullus, which can be described as:
x x — ∪ ∪ — ∪ — ∪ — —
Half-lines.
In place of using feet, alliterative verse of old Germanic languages such as Old English and Old Norse divided each line into two half-lines. Each half-line had to follow one of five or so patterns, each of which defined a sequence of stressed and unstressed syllables, typically with two stressed syllables per line. Unlike typical Western poetry, however, the number of unstressed syllables could vary somewhat. For example, the common pattern "DUM-da-DUM-da" could allow between one and five unstressed syllables between the two stresses.
The following is a famous example, taken from The Battle of Maldon:
<poem style="margin-left: 2em">
"Hige sceal þe heardra," || "heorte þe cēnre,"
"mōd sceal þe māre," || "swā ūre mægen lȳtlað"
("Will must be the harder, courage the bolder,
spirit must be the more, as our might lessens.")
</poem>
In the quoted section, the stressed syllables have been underlined. (Normally, the stressed syllable must be long if followed by another syllable in a word. However, by a rule known as "syllable resolution", two short syllables in a single word are considered equal to a single long syllable. Hence, sometimes two syllables have been underlined, as in "hige" and "mægen".) The first three half-lines have the type A pattern "DUM-da-(da-)DUM-da", while the last one has the type C pattern "da-(da-da-)DUM-DUM-da", with parentheses indicating optional unstressed syllables that have been inserted. Note also the pervasive pattern of alliteration, where the first and/or second stress alliterate with the third, but not with the fourth.
Caesurae.
Another component of a verse's metre are the caesurae (literally, "cuts"), which are not pauses but compulsory word boundaries that occur after a particular syllabic position in every line of a poem. In Latin and Greek poetry, a caesura is a break within a foot caused by the end of a word.
For example, in the verse below, each odd line has a caesura (shown by a slash /) after the fourth syllable (daily, her, won'dring, mother) while each even line is without a caesura:
A caesura would split the word "devotion" in the fourth line or the word "majesty" in the sixth line.
Metric variations.
Poems with a well-defined overall metric pattern often have a few lines that violate that pattern. A common variation is the "inversion" of a foot, which turns an iamb ("da-DUM") into a trochee ("DUM-da"). Another common variation is a "headless" verse, which lacks the first syllable of the first foot. Yet a third variation is catalexis, where the end of a line is shortened by a foot, or two or part thereof - an example of this is at the end of each verse in Keats' 'La Belle Dame sans Merci':
If the line has only one foot, it is called a "monometer"; two feet, "dimeter"; three is "trimeter"; four is "tetrameter"; five is "pentameter"; six is "hexameter", seven is "heptameter" and eight is "octameter". For example, if the feet are iambs, and if there are five feet to a line, then it is called a iambic pentameter. If the feet are primarily "dactyls" and there are six to a line, then it is a dactylic hexameter.
Meter in various languages.
Sanskrit.
Versification in Classical Sanskrit poetry is of three kinds.
Standard traditional works on metre are Pingala's Chandaḥśāstra and Kedāra's Vṛttaratnākara. The most exhaustive compilations, such as the modern ones by Patwardhan and Velankar contain over 600 metres. This is a substantially larger repertoire than in any other metrical tradition.
Greek and Latin.
The metrical "feet" in the classical languages were based on the length of time taken to pronounce each syllable, which were categorized according to their weight as either "long" syllables or "short" syllables (indicated as "daa" and "duh" below). These are also called "heavy" and "light" syllables, respectively, to distinguish from long and short vowels. The foot is often compared to a musical measure and the long and short syllables to whole notes and half notes. In English poetry, feet are determined by emphasis rather than length, with stressed and unstressed syllables serving the same function as long and short syllables in classical metre.
The basic unit in Greek and Latin prosody is a mora, which is defined as a single short syllable. A long syllable is equivalent to two morae. A long syllable contains either a long vowel, a diphthong, or a short vowel followed by two or more consonants. Various rules of elision sometimes prevent a grammatical syllable from making a full syllable, and certain other lengthening and shortening rules (such as correption) can create long or short syllables in contexts where one would expect the opposite.
The most important Classical metre is the dactylic hexameter, the metre of Homer and Virgil. This form uses verses of six feet. The word "dactyl" comes from the Greek word "daktylos" meaning "finger", since there is one long part followed by two short stretches. The first four feet are dactyls ("daa-duh-duh"), but can be spondees ("daa-daa"). The fifth foot is almost always a dactyl. The sixth foot is either a spondee or a trochee ("daa-duh"). The initial syllable of either foot is called the "ictus", the basic "beat" of the verse. There is usually a caesura after the ictus of the third foot. The opening line of the "Æneid" is a typical line of dactylic hexameter:
In this example, the first and second feet are dactyls; their first syllables, "Ar" and "rum" respectively, contain short vowels, but count as long because the vowels are both followed by two consonants. The third and fourth feet are spondees, the first of which is divided by the main caesura of the verse. The fifth foot is a dactyl, as is nearly always the case. The final foot is a spondee.
The dactylic hexameter was imitated in English by Henry Wadsworth Longfellow in his poem "Evangeline":
Notice how the first line:
Follows this pattern:
Also important in Greek and Latin poetry is the dactylic pentameter. This was a line of verse, made up of two equal parts, each of which contains two dactyls followed by a long syllable, which counts as a half foot. In this way, the number of feet amounts to five in total. Spondees can take the place of the dactyls in the first half, but never in the second. The long syllable at the close of the first half of the verse always ends a word, giving rise to a caesura.
Dactylic pentameter is never used in isolation. Rather, a line of dactylic pentameter follows a line of dactylic hexameter in the elegiac distich or elegiac couplet, a form of verse that was used for the composition of elegies and other tragic and solemn verse in the Greek and Latin world, as well as love poetry that was sometimes light and cheerful. An example from Ovid's "Tristia":
The Greeks and Romans also used a number of lyric metres, which were typically used for shorter poems than elegiacs or hexameter. In Aeolic verse, one important line was called the hendecasyllabic, a line of eleven syllables. This metre was used most often in the Sapphic stanza, named after the Greek poet Sappho, who wrote many of her poems in the form. A hendecasyllabic is a line with a never-varying structure: two trochees, followed by a dactyl, then two more trochees. In the Sapphic stanza, three hendecasyllabics are followed by an "Adonic" line, made up of a dactyl and a trochee. This is the form of Catullus 51 (itself an homage to Sappho 31):
The Sapphic stanza was imitated in English by Algernon Charles Swinburne in a poem he simply called "Sapphics":
Classical Arabic.
The metrical system of Classical Arabic poetry, like those of classical Greek and Latin, is based on the weight of syllables classified as either "long" or "short". The basic principles of Arabic poetic metre "Arūḍ" or Arud ( ') Science of Poetry ( '), were put forward by Al-Farahidi (786 - 718 AD) who did so after noticing that poems consisted of repeated syllables in each verse. In his first book, "Al-Ard" ( ""), he described 15 types of verse. Al-Akhfash described one extra, the 16th.
A short syllable contains a short vowel with no following consonants. For example, the word "kataba," which syllabifies as "ka-ta-ba", contains three short vowels and is made up of three short syllables. A long syllable contains either a long vowel or a short vowel followed by a consonant as is the case in the word "maktūbun" which syllabifies as "mak-tū-bun". These are the only syllable types possible in Classical Arabic phonology which, by and large, does not allow a syllable to end in more than one consonant or a consonant to occur in the same syllable after a long vowel. In other words, syllables of the type "-āk-" or "-akr-" are not found in classical Arabic.
Each verse consists of a certain number of metrical feet ("tafāʿīl" or "ʾaǧzāʾ") and a certain combination of possible feet constitutes a metre ("baḥr").
The traditional Arabic practice for writing out a poem's metre is to use a concatenation of various derivations of the verbal root "F-ʿ-L" (فعل). Thus, the following hemistich
Would be traditionally scanned as:
That is, Romanized and with traditional Western scansion:
The Arabic metres.
Classical Arabic has sixteen established metres. Though each of them allows for a certain amount of variation, their basic patterns are as follows, using:
Classical Persian.
The terminology for metrical system used in classical and classical-style Persian poetry is the same as that of Classical Arabic, even though these are quite different in both origin and structure. This has led to serious confusion among prosodists, both ancient and modern, as to the true source and nature of the Persian meters, the most obvious error being the assumption that they were copied from Arabic.
Classical Chinese.
Classical Chinese poetic metric may be divided into fixed and variable length line types, although the actual scansion of the metre is complicated by various factors, including linguistic changes and variations encountered in dealing with a tradition extending over a geographically extensive regional area for a continuous time period of over some two-and-a-half millennia. Beginning with the earlier recorded forms: the Classic of Poetry tends toward couplets of four-character lines, grouped in rhymed quatrains; and, the Chuci follows this to some extent, but moves toward variations in line length. Han Dynasty poetry tended towards the variable line-length forms of the folk ballads and the Music Bureau yuefu. Jian'an poetry, Six Dynasties poetry, and Tang Dynasty poetry tend towards a poetic metre based on fixed-length lines of five, seven, (or, more rarely six) characters/verbal units tended to predominate, generally in couplet/quatrain-based forms, of various total verse lengths. The Song poetry is specially known for its use of the "ci", using variable line lengths which follow the specific pattern of a certain musical song's lyrics, thus "ci" are sometimes referred to as "fixed-rhythm" forms. Yuan poetry metres continued this practice with their "qu" forms, similarly fixed-rhythm forms based on now obscure or perhaps completely lost original examples (or, ur-types). Not that Classical Chinese poetry ever lost the use of the "shi" forms, with their metrical patterns found in the "old style poetry" ("gushi") and the regulated verse forms of ("lüshi" or "jintishi"). The regulated verse forms also prescribed patterns based upon linguistic tonality. The use of caesura is important in regard to the metrical analysis of Classical Chinese poetry forms.
Old English.
The metric system of Old English poetry was different from that of modern English, and related more to the verse forms of most of older Germanic languages. It used alliterative verse, a metrical pattern involving varied numbers of syllables but a fixed number (usually four) of strong stresses in each line. The unstressed syllables were relatively unimportant, but the caesurae played a major role in Old English poetry.
Modern English.
Most English metre is classified according to the same system as Classical metre with an important difference. English is an accentual language, and therefore beats and offbeats (stressed and unstressed syllables) take the place of the long and short syllables of classical systems. In most English verse, the metre can be considered as a sort of back beat, against which natural speech rhythms vary expressively. The most common characteristic feet of English verse are the iamb in two syllables and the anapest in three. (See Foot (prosody) for a complete list of the metrical feet and their names.)
Metrical systems.
The number of metrical systems in English is not agreed upon. The four major types are: accentual verse, accentual-syllabic verse, syllabic verse and quantitative verse. The alliterative verse of Old English could also be added to this list, or included as a special type of accentual verse. Accentual verse focuses on the number of stresses in a line, while ignoring the number of offbeats and syllables; accentual-syllabic verse focuses on regulating both the number of stresses and the total number of syllables in a line; syllabic verse only counts the number of syllables in a line; quantitative verse regulates the patterns of long and short syllables (this sort of verse is often considered alien to English). It is to be noted, however, that the use of foreign metres in English is all but exceptional.
Frequently-used metres.
The most frequently encountered metre of English verse is the iambic pentameter, in which the metrical norm is five iambic feet per line, though metrical substitution is common and rhythmic variations practically inexhaustible. John Milton's "Paradise Lost", most sonnets, and much else besides in English are written in iambic pentameter. Lines of unrhymed iambic pentameter are commonly known as blank verse. Blank verse in the English language is most famously represented in the plays of William Shakespeare and the great works of Milton, though Tennyson ("Ulysses", "The Princess") and Wordsworth ("The Prelude") also make notable use of it.
A rhymed pair of lines of iambic pentameter make a heroic couplet, a verse form which was used so often in the 18th century that it is now used mostly for humorous effect (although see Pale Fire for a non-trivial case). The most famous writers of heroic couplets are Dryden and Pope.
Another important metre in English is the ballad metre, also called the "common metre", which is a four-line stanza, with two pairs of a line of iambic tetrameter followed by a line of iambic trimeter; the rhymes usually fall on the lines of trimeter, although in many instances the tetrameter also rhymes. This is the metre of most of the Border and Scots or English ballads. In hymnody it is called the "common metre", as it is the most common of the named hymn metres used to pair many hymn lyrics with melodies, such as "Amazing Grace":
Emily Dickinson is famous for her frequent use of ballad metre:
French.
In French poetry, metre is determined solely by the number of syllables in a line, because it is considered less important than rhymes. A silent 'e' counts as a syllable before a consonant, but is elided before a vowel (where "h aspiré" counts as a consonant). At the end of a line, the "e" remains unelided but is hypermetrical (outside the count of syllables, like a feminine ending in English verse), in that case, the rhyme is also called "feminine", whereas it is called "masculine" in the other cases.
The most frequently encountered metre in Classical French poetry is the alexandrine, composed of two hemistiches of six syllables each. Two famous alexandrines are
(the daughter of Minos and Pasiphae), and
Classical French poetry also had a complex set of rules for rhymes that goes beyond how words merely sound. These are usually taken into account when describing the metre of a poem.
Spanish.
In Spanish poetry the metre is determined by the number of syllables the verse has. Still it is the phonetic accent in the last word of the verse that decides the final count of the line. If the accent of the final word is at the last syllable, then the poetic rule states that one syllable shall be added to the actual count of syllables in the said line, thus having a higher number of poetic syllables than the number of grammatical syllables. If the accent lies on the second to last syllable of the last word in the verse, then the final count of poetic syllables will be the same as the grammatical number of syllables. Furthermore, if the accent lies on the third to last syllable, then one syllable is subtracted from the actual count, having then less poetic syllables than grammatical syllables.
Spanish poetry uses poetic licenses, unique to Romance languages, to change the number of syllables by manipulating mainly the vowels in the line.
Regarding these poetic licenses one must consider three kinds of phenomena: (1) syneresis, (2) dieresis and (3) hiatus
There are many types of licenses, used either to add or subtract syllables, that may be applied when needed after taking in consideration the poetic rules of the last word. Yet all have in common that they only manipulate vowels that are close to each other and not interrupted by consonants.
Some common metres in Spanish verse are:
Italian.
In Italian poetry, metre is determined solely by the position of the last accent in a line, the position of the other accents being however important for verse equilibrium. Syllables are enumerated with respect to a verse which ends with a paroxytone, so that a Septenary (having seven syllables) is defined as a verse whose last accent falls on the sixth syllable: it may so contain eight syllables ("Ei fu. Siccome immobile") or just six ("la terra al nunzio sta"). Moreover, when a word ends with a vowel and the next one starts with a vowel, they are considered to be in the same syllable (synalepha): so "Gli anni e i giorni" consists of only four syllables ("Gli an" "ni e i" "gior" "ni"). Even-syllabic verses have a fixed stress pattern. Because of the mostly trochaic nature of the Italian language, verses with an even number of syllables are far easier to compose, and the Novenary is usually regarded as the most difficult verse.
Some common metres in Italian verse are:
Turkish.
Apart from Ottoman poetry which was heavily influenced by Persian traditions and created a unique Ottoman style, original Turkish metre was based on "stop"s ("durak" in Turkish). Traditional Turkish poetry features a system in which number of syllables in each verse must be the same, most frequently 7, 8, 11, 14 syllables. These verses are then divided into syllable groups depending on the number of total syllables in a verse: 4+3 for 7 syllables, 4+4 or 5+3 for 8, 4+4+3 or 6+5 for 11 syllables. End of each group in a verse is called a "durak" (stop), and must coincide with the last syllable of a word.
Example by Faruk Nafiz Çamlıbel, one of the most devout users of traditional Turkish metre:
6+5 metre is used, last of the first six syllables in each verse coincided with the end of a word, did not "interrupt" a word.
Ottoman Turkish.
In the Ottoman Turkish language, the structures of the poetic foot (تفعل "tef'ile") and of poetic metre (وزن "vezin") were indirectly borrowed from the Arabic poetic tradition through the medium of the Persian language.
Ottoman poetry, also known as Dîvân poetry, was generally written in quantitative, mora-timed metre. The moras, or syllables, are divided into three basic types:
In writing out a poem's poetic metre, open syllables are symbolized by "." and closed syllables are symbolized by "–". From the different syllable types, a total of sixteen different types of poetic foot—the majority of which are either three or four syllables in length—are constructed, which are named and scanned as follows:
These individual poetic feet are then combined in a number of different ways, most often with four feet per line, so as to give the poetic metre for a line of verse. Some of the most commonly used metres are the following:
Portuguese.
The most commonly used ones are:
History.
Metrical texts are first attested in early Indo-European languages. The earliest known unambiguously metrical texts, and at the same time the only metrical texts with a claim of dating to the Late Bronze Age, are the hymns of the Rigveda. That the texts of the Ancient Near East (Sumerian, Egyptian or Semitic) should not exhibit metre is surprising,and may be partly due to the nature of Bronze Age writing. There were, in fact, attempts to reconstruct metrical qualities of the poetic portions of the Hebrew Bible, e.g. by Gustav Bickell or Julius Ley, but they remained inconclusive (see Biblical poetry). Early Iron Age metrical poetry is found in the Iranian Avesta and in the Greek works attributed to Homer and Hesiod.
Latin verse survives from the Old Latin period (c. 2nd century BC), in the Saturnian metre. Persian poetry arises in the Sassanid era. Tamil poetry of the early centuries AD may be the earliest known non-Indo-European
Medieval poetry was metrical without exception, spanning traditions as diverse as European Minnesang, Trouvère or Bardic poetry, Classical Persian and Sanskrit poetry, Tang dynasty Chinese poetry or the Japanese Nara period "Man'yōshū". Renaissance and Early Modern poetry in Europe is characterized by a return to templates of Classical Antiquity, a tradition begun by Petrarca's generation and continued into the time of Shakespeare and Milton.
Dissent.
Not all poets accept the idea that metre is a fundamental part of poetry. 20th-century American poets Marianne Moore, William Carlos Williams and Robinson Jeffers were poets who believed that metre was imposed upon poetry by man and not a fundamental part of its nature. In an essay titled "Robinson Jeffers, & The Metric Fallacy" Dan Schneider echoes Jeffers' sentiments: "What if someone actually said to you that all music was composed of just 2 notes? Or if someone claimed that there were just 2 colors in creation? Now, ponder if such a thing were true. Imagine the clunkiness & mechanicality of such music. Think of the visual arts devoid of not just color, but sepia tones, & even shades of gray." Jeffers called his technique "rolling stresses".
Moore went further than Jeffers, openly declaring her poetry was written in syllabic form, and wholly denying metre. These syllabic lines from her famous poem illustrate her contempt for metre and other poetic tools. Even the syllabic pattern of this poem does not remain perfectly consistent:
Williams tried to form poetry whose subject matter was centered on the lives of common people. He came up with the concept of the variable foot. Williams spurned traditional metre in most of his poems, preferring what he called "colloquial idioms." Another poet that turned his back on traditional concepts of metre was Britain's Gerard Manley Hopkins. Hopkins' major innovation was what he called sprung rhythm. He claimed most poetry was written in this older rhythmic structure inherited from the Norman side of the English literary heritage, based on repeating groups of two or three syllables, with the stressed syllable falling in the same place on each repetition. Sprung rhythm is structured around feet with a variable number of syllables, generally between one and four syllables per foot, with the stress always falling on the first syllable in a foot.

</doc>
<doc id="19932" url="https://en.wikipedia.org/wiki?curid=19932" title="Majed Moqed">
Majed Moqed

A former law student, Majed Mashaan Ghanem Moqed (, ; also transliterated as Moqued) (June 18, 1977 – September 11, 2001) was one of five hijackers of American Airlines Flight 77 as part of the September 11 attacks.
A Saudi, Moqed was studying law at a university in Saudi Arabia before joining Al-Qaeda in 1999 and being chosen to participate in the 9/11 attacks. He arrived in the United States in May 2001 and helped with the planning of how the attacks would be carried out.
On September 11, 2001, Moqed boarded American Airlines Flight 77 and assisted in the hijacking of the plane so that it could be crashed into the Pentagon.
Biography.
Moqed was a law student from the small town of Al-Nakhil, Saudi Arabia (west of Medina), studying at King Fahd University's Faculty of Administration and Economics. Before he dropped out, he was apparently recruited into al-Qaeda in 1999 along with friend Satam al-Suqami, with whom he had earlier shared a college room. 
The two trained at Khalden, a large training facility near Kabul that was run by Ibn al-Shaykh al-Libi. A friend in Saudi Arabia claimed he was last seen there in 2000, before leaving to study English in the United States. In November 2000, Moqed and Suqami flew into Iran from Bahrain together.
Some time late in 2000, Moqed traveled to the United Arab Emirates, where he purchased traveler's cheques presumed to have been paid for by 9/11 financier Mustafa Ahmed al-Hawsawi. Five other hijackers also passed through the UAE and purchased travellers cheques, including Wail al-Shehri, Saeed al-Ghamdi, Hamza al-Ghamdi, Ahmed al-Haznawi and Ahmed al-Nami.
Known as "al-Ahlaf" during the preparations, Moqed then moved in with hijackers Salem al-Hazmi, Abdulaziz al-Omari and Khalid al-Mihdhar in an apartment in Paterson, New Jersey.
2001.
According to the FBI, Moqed first arrived in the United States on May 2, 2001.
In March 2001, Moqed, Hani Hanjour, Hazmi and Ahmed al-Ghamdi rented a minivan and travelled to Fairfield, Connecticut. There they met a contact in the parking lot of a local convenience store who provided them with false IDs. (This was possibly Eyad Alrababah, a Jordanian charged with document fraud).
Moqed was one of the five hijackers who asked for a state identity card on August 2, 2001. On August 24, both Mihdhar and Moqed tried to purchase flight tickets from the American Airlines online ticket-merchant, but had technical difficulties resolving their address and gave up.
Employees at Advance Travel Service in Totowa, New Jersey later claimed that Moqed and Hanjour had both purchased tickets there. They claimed that Hani Hanjour spoke very little English, and Moqed did most of the speaking. Hanjour requested a seat in the front row of the airplane. Their credit card failed to authorize, and after being told the agency did not accept personal cheques, the pair left to withdraw cash. They returned shortly afterwards and paid $1842.25 total in cash. 
During this time, Moqed was staying in Room 343 of the "Valencia Motel". On September 2, Moqed paid cash for a $30 weekly membership at Gold's Gym in Greenbelt, Maryland.
Three days later he was seen on an ATM camera with Hani Hanjour. After the attacks, employees at an adult video store, "Adult Lingerie Center", in Beltsville claimed that Moqed had been in the store three times, although there were no transactions slips that confirmed this.
Attacks.
On September 11, 2001, Moqed arrived at Washington Dulles International Airport.
According to the 9/11 Commission Report, Moqed set off the metal detector at the airport and was screened with a hand-wand. He passed the cursory inspection, and was able board his flight at 7:50. He was seated in 12A, adjacent to Mihdhar who was in 12B. Moqed helped to hijack the plane and assisted Hani Hanjour in crashing the plane into the Pentagon at 9:37 A.M., killing 189 people (64 on the plane and 125 on the ground).
The flight was scheduled to depart at 08:10, but ended up departing 10 minutes late from Gate D26 at Dulles. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. At 08:54, Flight 77 began to deviate from its normal, assigned flight path and turned south, and then hijackers set the flight's autopilot heading for Washington, D.C. Passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the plane had been hijacked and that the assailants had box cutters and knives. At 09:37, American Airlines Flight 77 crashed into the west facade of the Pentagon, killing all 64 aboard (including the hijackers), along with 125 on the ground in the Pentagon. In the recovery process at the Pentagon, remains of all five Flight 77 hijackers were identified through a process of elimination, as not matching any DNA samples for the victims, and put into custody of the FBI.
After the attacks his family told Arab News that Moqed had been a fan of sports, and enjoyed travelling. Additionally, the U.S. announced it had found a "Kingdom of Saudi Arabia Student Identity Card" bearing Moqed's name in the rubble surrounding the Pentagon. They also stated that it appeared to have been a forgery.

</doc>
<doc id="19933" url="https://en.wikipedia.org/wiki?curid=19933" title="Matthew Perry (disambiguation)">
Matthew Perry (disambiguation)

Matthew Perry (born 1969) is Canadian-American television and film actor.
Matthew Perry or Matt Perry may also refer to:

</doc>
<doc id="19935" url="https://en.wikipedia.org/wiki?curid=19935" title="Mimeograph">
Mimeograph

The stencil duplicator or mimeograph machine (often abbreviated to mimeo) is a low-cost duplicating machine that works by forcing ink through a stencil onto paper. The mimeograph process should not be confused with the spirit duplicator process.
Mimeographs, along with spirit duplicators and hectographs, were a common technology in printing small quantities, as in office work, classroom materials, and church bulletins. Early fanzines were printed with this technology, because it was widespread and cheap. In the late 1960s, mimeographs, spirit duplicators, and hectographs began to be gradually displaced by photocopying.
Origins.
Use of stencils is an ancient art but through chemistry, papers, and presses techniques advanced rapidly in the late nineteenth century:
Papyrograph.
A description of the Papyrograph method of duplication was published by David Owen: 
A major beneficiary of the invention of synthetic dyes was a document reproduction technique known as stencil duplicating. Its earliest form was invented in 1874 by Eugenio de Zuccato, a young Italian studying law in London, who called his device the Papyrograph. Zuccato’s system involved writing on a sheet of varnished paper with caustic ink, which ate through the varnish and paper fibers, leaving holes where the writing had been. This sheet – which had now become a stencil – was placed on a blank sheet of paper, and ink rolled over it so that the ink oozed through the holes, creating a duplicate on the second sheet.
The process was commercialized and Zuccato applied for a patent in 1895 having stencils prepared by typewriting.
Electric pen.
Thomas Edison received US patent 180,857 for Autographic Printing on August 8, 1876. The patent covered the electric pen, used for making the stencil, and the flatbed duplicating press. In 1880 Edison obtained a further patent, US 224,665: "Method of Preparing Autographic Stencils for Printing," which covered the making of stencils using a file plate, a grooved metal plate on which the stencil was placed which perforated the stencil when written on with a blunt metal stylus.
The word mimeograph was first used by Albert Blake Dick when he licensed Edison's patents in 1887.
Dick received Trademark Registration no. 0356815 for the term "Mimeograph" in the US Patent Office. It is currently listed as a dead entry, but shows the A.B. Dick Company of Chicago as the owner of the name.
Over time, the term became generic and is now an example of a genericized trademark. ("Roneograph," also "Roneo machine," was another trademark used for mimeograph machines, the name being a contraction of Rotary Neostyle.)
Cyclostyle.
In 1891, David Gestetner patented his Automatic Cyclostyle. This was one of the first rotary machines that retained the flatbed, which passed back and forth under inked rollers. This invention provided for more automated, faster reproductions since the pages were produced and moved by rollers instead of pressing one single sheet at a time.
By 1900, two primary types of mimeographs had come into use: a single-drum machine and a dual-drum machine. The single-drum machine used a single drum for ink transfer to the stencil, and the dual-drum machine used two drums and silk-screens to transfer the ink to the stencils. The single drum (example Roneo) machine could be easily used for multi-color work by changing the drum - each of which contained ink of a different color. This was spot color for mastheads. Colors could not be mixed.
The mimeograph became popular because it was much cheaper than traditional print - there was neither typesetting nor skilled labor involved. One individual with a typewriter and the necessary equipment became his own printing factory, allowing for greater circulation of printed material.
Mimeography process.
The image transfer medium was originally a stencil made from waxed mulberry paper. Later this became an immersion-coated long-fibre paper, with the coating being a plasticized nitrocellulose. This flexible waxed or coated sheet is backed by a sheet of stiff card stock, with the two sheets bound at the top.
Once prepared, the stencil is wrapped around the ink-filled drum of the rotary machine. When a blank sheet of paper is drawn between the rotating drum and a pressure roller, ink is forced through the holes on the stencil onto the paper. Early flatbed machines used a kind of squeegee. The ink originally had a lanolin base. and later became an oil in water emulsion. This emulsion commonly used Turkey-Red Oil (Sulfated Castor Oil) which gives it a distinctive and heavy scent.
Preparing stencils.
For printed copy, a stencil assemblage is placed in a typewriter. The part of the mechanism which lifts the ribbon must be disabled so that the bare, sharp type element strikes the stencil directly. The impact of the type element displaces the coating, making the tissue paper permeable to the oil-based ink. This is called "cutting a stencil."
A variety of specialized styluses were used on the stencil to render lettering, illustrations, or other artistic features by hand against a textured plastic backing plate.
Mistakes can be corrected by brushing them out with a specially formulated correction fluid, and retyping once it has dried. ("Obliterine" was a popular brand of correction fluid in Australia and the United Kingdom.)
Stencils were also made with a thermal process; an infrared method similar to that used by early photocopiers. The common machine was a Thermofax.
Another device, called an electrostencil machine, sometimes was used to make mimeo stencils from a typed or printed original. It worked by scanning the original on a rotating drum with a moving optical head and burning through the blank stencil with an electric spark in the places where the optical head detected ink. It was slow and produced ozone. Text from electrostencils had lower resolution than that from typed stencils, although the process was good for reproducing illustrations. A skilled mimeo operator using an electrostencil and a very coarse halftone screen could make acceptable printed copies of a photograph.
During the declining years of the mimeograph, some people made stencils with early computers and dot-matrix impact printers.
Limitations.
Unlike spirit duplicators (where the only ink available is depleted from the master image), mimeograph technology works by forcing a replenishable supply of ink through the stencil master. In theory, the mimeograph process could be continued indefinitely, especially if a durable stencil master were used (e.g. a thin metal foil). In practice, most low-cost mimeo stencils gradually wear out over the course of producing several hundred copies. Typically the stencil deteriorates gradually, producing a characteristic degraded image quality until the stencil tears, abruptly ending the print run. If further copies are desired at this point, another stencil must be made.
Often, the stencil material covering the interiors of closed letterforms (e.g. "a", "b", "d", "e", "g", etc.) would fall away during continued printing, causing ink-filled letters in the copies. The stencil would gradually stretch, starting near the top where the mechanical forces were greatest, causing a characteristic "mid-line sag" in the textual lines of the copies, that would progress until the stencil failed completely. The Gestetner Company (and others) devised various methods to make mimeo stencils more durable.
Compared to spirit duplication, mimeography produced a darker, more legible image. Spirit duplicated images were usually tinted a light purple or lavender, which gradually became lighter over the course of some dozens of copies. Mimeography was often considered "the next step up" in quality, capable of producing hundreds of copies. Print runs beyond that level were usually produced by professional printers, or as the technology became available, xerographic copiers.
Durability.
Mimeographed images generally have much better durability than spirit duplicated images, since the inks are more resistant to ultraviolet light. The primary preservation challenge is the low-quality paper often used, which would yellow and degrade due to residual acid in the treated pulp from which the paper was made. In the worst case, old copies can literally crumble into small particles when handled. Mimeographed copies have moderate durability when acid-free paper is used.
Contemporary use.
Gestetner, Risograph, and other companies still make and sell highly automated mimeograph-like machines that are externally similar to photocopiers. The modern version of a mimeograph, called a digital duplicator, or copyprinter, contains a scanner, a thermal head for stencil cutting, and a large roll of stencil material entirely inside the unit. The stencil material consists of a very thin polymer film laminated to a long-fibre non-woven tissue. It makes the stencils and mounts and unmounts them from the print drum automatically, making it almost as easy to operate as a photocopier. The Risograph is the best known of these machines.
Although mimeographs remain more economical and energy-efficient in mid-range quantities, easier-to-use photocopying and offset printing have replaced mimeography almost entirely in developed countries. Mimeograph machines continue to be used in developing countries because it is a simple, cheap, and robust technology. Many mimeographs can be hand-cranked, requiring no electricity.
Uses and art.
Mimeographs and the closely related but distinctly different spirit duplicator process were both used extensively in schools to copy homework assignments and tests. They were also commonly used for low-budget amateur publishing, including club newsletters and church bulletins. They were especially popular with science fiction fans, who used them extensively in the production of fanzines in the middle 20th century, before photocopying became inexpensive.
Letters and typographical symbols were sometimes used to create illustrations, in a precursor to ASCII art. Because changing ink color in a mimeograph could be a laborious process, involving extensively cleaning the machine or, on newer models, replacing the drum or rollers, and then running the paper through the machine a second time, some fanzine publishers experimented with techniques for painting several colors on the pad, notably Shelby Vick, who created a kind of plaid "Vicolor".

</doc>
<doc id="19937" url="https://en.wikipedia.org/wiki?curid=19937" title="Meteorite">
Meteorite

A meteorite is a solid piece of debris from an object, such as a comet, asteroid, or meteoroid, that originates in outer space and survives its passage through the Earth's atmosphere and impact with the Earth's surface. When the object enters the atmosphere, various factors like friction, pressure, and chemical interactions with the atmospheric gases cause it to heat up and radiate that energy. It then becomes a meteor and forms a fireball, also known as a shooting/falling star; astronomers call the brightest examples "bolides." Meteorites that survive atmospheric entry and impact vary greatly in size. For geologists, a bolide is a meteorite large enough to create a crater.
Meteorites that are recovered after being observed as they transit the atmosphere or impact the Earth are called meteorite falls. All others are known as meteorite finds. , there were approximately 1,140 witnessed falls that have specimens in the world's collections. The first confirmed fall for 2016 is the Osceola meteorite which fell on January 24, 2016 in Florida, USA. There are more than 38,660 well-documented meteorite finds.
Meteorites have traditionally been divided into three broad categories: stony meteorites are rocks, mainly composed of silicate minerals; iron meteorites that are largely composed of metallic iron-nickel; and, stony-iron meteorites that contain large amounts of both metallic and rocky material. Modern classification schemes divide meteorites into groups according to their structure, chemical and isotopic composition and mineralogy. Meteorites smaller than 2mm are classified as micrometeorites. Extraterrestrial meteorites are such objects that have impacted other celestial bodies, whether or not they have passed through an atmosphere. They have been found on the moon and Mars.
Naming.
Meteorites are always named for the places they were found, usually a nearby town or geographic feature. In cases where many meteorites were found in one place, the name may be followed by a number or letter (e.g., Allan Hills 84001 or Dimmitt (b)). The name designated by the Meteoritical Society is used by scientists, catalogers, and most collectors.
Fall phenomena.
Most meteoroids disintegrate when entering the Earth's atmosphere. Usually, five to ten a year are observed to fall and are subsequently recovered and made known to scientists. Few meteorites are large enough to create large impact craters. Instead, they typically arrive at the surface at their terminal velocity and, at most, create a small pit.
Large meteoroids may strike the ground with a significant fraction of their second cosmic velocity, leaving behind a hypervelocity impact crater. The kind of crater will depend on the size, composition, degree of fragmentation, and incoming angle of the impactor. The force of such collisions has the potential to cause widespread destruction. The most frequent hypervelocity cratering events on the Earth are caused by iron meteoroids, which are most easily able to transit the atmosphere intact. Examples of craters caused by iron meteoroids include Barringer Meteor Crater, Odessa Meteor Crater, Wabar craters, and Wolfe Creek crater; iron meteorites are found in association with all of these craters. In contrast, even relatively large stony or icy bodies like small comets or asteroids, up to millions of tons, are disrupted in the atmosphere, and do not make impact craters. Although such disruption events are uncommon, they can cause a considerable concussion to occur; the famed Tunguska event probably resulted from such an incident. Very large stony objects, hundreds of meters in diameter or more, weighing tens of millions of tons or more, can reach the surface and cause large craters, but are very rare. Such events are generally so energetic that the impactor is completely destroyed, leaving no meteorites. (The very first example of a stony meteorite found in association with a large impact crater, the Morokweng crater in South Africa, was reported in May 2006.)
Several phenomena are well documented during witnessed meteorite falls too small to produce hypervelocity craters. The fireball that occurs as the meteoroid passes through the atmosphere can appear to be very bright, rivaling the sun in intensity, although most are far dimmer and may not even be noticed during daytime. Various colors have been reported, including yellow, green, and red. Flashes and bursts of light can occur as the object breaks up. Explosions, detonations, and rumblings are often heard during meteorite falls, which can be caused by sonic booms as well as shock waves resulting from major fragmentation events. These sounds can be heard over wide areas, with a radius of a hundred or more kilometers. Whistling and hissing sounds are also sometimes heard, but are poorly understood. Following passage of the fireball, it is not unusual for a dust trail to linger in the atmosphere for several minutes.
As meteoroids are heated during atmospheric entry, their surfaces melt and experience ablation. They can be sculpted into various shapes during this process, sometimes resulting in shallow thumbprint-like indentations on their surfaces called regmaglypts. If the meteoroid maintains a fixed orientation for some time, without tumbling, it may develop a conical "nose cone" or "heat shield" shape. As it decelerates, eventually the molten surface layer solidifies into a thin fusion crust, which on most meteorites is black (on some achondrites, the fusion crust may be very light colored). On stony meteorites, the heat-affected zone is at most a few mm deep; in iron meteorites, which are more thermally conductive, the structure of the metal may be affected by heat up to below the surface. Reports vary; some meteorites are reported to be "burning hot to the touch" upon landing, while others have been cold enough to condense water and form a frost. Meteorites from multiple falls, such as Bjurbole, Tagish Lake, and Buzzard Coulee, have been found having fallen on lake and sea ice, perhaps suggesting that they were not hot when they fell.
Meteoroids that experience disruption in the atmosphere may fall as meteorite showers, which can range from only a few up to thousands of separate individuals. The area over which a meteorite shower falls is known as its strewn field. Strewn fields are commonly elliptical in shape, with the major axis parallel to the direction of flight. In most cases, the largest meteorites in a shower are found farthest down-range in the strewn field.
Meteorite types.
Most meteorites are stony meteorites, classed as chondrites and achondrites. Only about 6% of meteorites are iron meteorites or a blend of rock and metal, the stony-iron meteorites. Modern classification of meteorites is complex. The review paper of Krot et al. (2007) summarizes modern meteorite taxonomy.
About 86% of the meteorites that fall on Earth are chondrites, which are named for the small, round particles they contain. These particles, or chondrules, are composed mostly of silicate minerals that appear to have been melted while they were free-floating objects in space. Certain types of chondrites also contain small amounts of organic matter, including amino acids, and presolar grains. Chondrites are typically about 4.55 billion years old and are thought to represent material from the asteroid belt that never coalesced into large bodies. Like comets, chondritic asteroids are some of the oldest and most primitive materials in the solar system. Chondrites are often considered to be "the building blocks of the planets".
About 8% of the meteorites that fall on Earth are achondrites (meaning they do not contain chondrules), some of which are similar to terrestrial igneous rocks. Most achondrites are also ancient rocks, and are thought to represent crustal material of differentiated planetesimals. One large family of achondrites (the HED meteorites) may have originated on the parent body of the Vesta Family, although this claim is disputed. Others derive from unidentified asteroids. Two small groups of achondrites are special, as they are younger and do not appear to come from the asteroid belt. One of these groups comes from the Moon, and includes rocks similar to those brought back to Earth by Apollo and Luna programs. The other group is almost certainly from Mars and constitutes the only materials from other planets ever recovered by humans.
About 5% of meteorites that have been seen to fall are iron meteorites composed of iron-nickel alloys, such as kamacite and/or taenite. Most iron meteorites are thought to come from the cores of planetesimals that were once molten. As with the Earth, the denser metal separated from silicate material and sank toward the center of the planetesimal, forming its core. After the planetesimal solidified, it broke up in a collision with another planetesimal. Due to the low abundance of iron meteorites in collection areas such as Antarctica, where most of the meteoric material that has fallen can be recovered, it is possible that the percentage of iron-meteorite falls is lower than 5%. This would be explained by a recovery bias; laypeople are more likely to notice and recover solid masses of metal than most other meteorite types. The abundance of iron meteorites relative to total Antarctic finds is 0.4% 
Stony-iron meteorites constitute the remaining 1%. They are a mixture of iron-nickel metal and silicate minerals. One type, called pallasites, is thought to have originated in the boundary zone above the core regions where iron meteorites originated. The other major type of stony-iron meteorites is the mesosiderites.
Tektites (from Greek "tektos", molten) are not themselves meteorites, but are rather natural glass objects up to a few centimeters in size that were formed—according to most scientists—by the impacts of large meteorites on Earth's surface. A few researchers have favored tektites originating from the Moon as volcanic ejecta, but this theory has lost much of its support over the last few decades.
Meteorite chemistry.
In March 2015, NASA scientists reported that, for the first time, complex organic compounds found in DNA and RNA, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.
Meteorite recovery.
Falls.
Most meteorite falls are recovered on the basis of eyewitness accounts of the fireball or the impact of the object on the ground, or both. Therefore, despite the fact that meteorites fall with virtually equal probability everywhere on Earth, verified meteorite falls tend to be concentrated in areas with high human population densities such as Europe, Japan, and northern India.
A small number of meteorite falls have been observed with automated cameras and recovered following calculation of the impact point. The first of these was the Přibram meteorite, which fell in Czechoslovakia (now the Czech Republic) in 1959. In this case, two cameras used to photograph meteors captured images of the fireball. The images were used both to determine the location of the stones on the ground and, more significantly, to calculate for the first time an accurate orbit for a recovered meteorite.
Following the Pribram fall, other nations established automated observing programs aimed at studying infalling meteorites. One of these was the "Prairie Network", operated by the Smithsonian Astrophysical Observatory from 1963 to 1975 in the midwestern US. This program also observed a meteorite fall, the "Lost City" chondrite, allowing its recovery and a calculation of its orbit. Another program in Canada, the Meteorite Observation and Recovery Project, ran from 1971 to 1985. It too recovered a single meteorite, "Innisfree", in 1977. Finally, observations by the European Fireball Network, a descendant of the original Czech program that recovered Pribram, led to the discovery and orbit calculations for the "Neuschwanstein" meteorite in 2002.
NASA has an automated system that detects meteors and calculates the orbit, magnitude, ground track, and other parameters over the southeast USA, which often detects a number of events each night.
Finds.
Until the twentieth century, only a few hundred meteorite finds had ever been discovered. More than 80% of these were iron and stony-iron meteorites, which are easily distinguished from local rocks. To this day, few stony meteorites are reported each year that can be considered to be "accidental" finds. The reason there are now more than 30,000 meteorite finds in the world's collections started with the discovery by Harvey H. Nininger that meteorites are much more common on the surface of the Earth than was previously thought.
The Great Plains of the US.
Nininger's strategy was to search for meteorites in the Great Plains of the United States, where the land was largely cultivated and the soil contained few rocks. Between the late 1920s and the 1950s, he traveled across the region, educating local people about what meteorites looked like and what to do if they thought they had found one, for example, in the course of clearing a field. The result was the discovery of over 200 new meteorites, mostly stony types.
In the late 1960s, Roosevelt County, New Mexico in the Great Plains was found to be a particularly good place to find meteorites. After the discovery of a few meteorites in 1967, a public awareness campaign resulted in the finding of nearly 100 new specimens in the next few years, with many being by a single person, Ivan Wilson. In total, nearly 140 meteorites were found in the region since 1967. In the area of the finds, the ground was originally covered by a shallow, loose soil sitting atop a hardpan layer. During the dustbowl era, the loose soil was blown off, leaving any rocks and meteorites that were present stranded on the exposed surface.
Antarctica.
[[File:ALH84001 structures.jpg|thumb|A scanning electron microscope revealed structures resembling bacteria fossils - in the meteorite ALH84001 discovered in Antarctica in 1984.
Microscopically, the features were initially interpreted as fossils of bacteria-like lifeforms. It has since been shown that similar magnetite structures can form without the presence of microbial life in hydrothermal systems.]] 
A few meteorites were found in Antarctica between 1912 and 1964. In 1969, the 10th Japanese Antarctic Research Expedition found nine meteorites on a blue ice field near the Yamato Mountains. With this discovery, came the realization that movement of ice sheets might act to concentrate meteorites in certain areas. After a dozen other specimens were found in the same place in 1973, a Japanese expedition was launched in 1974 dedicated to the search for meteorites. This team recovered nearly 700 meteorites.
Shortly thereafter, the United States began its own program to search for Antarctic meteorites, operating along the Transantarctic Mountains on the other side of the continent: the Antarctic Search for Meteorites (ANSMET) program. European teams, starting with a consortium called "EUROMET" in the late 1980s, and continuing with a program by the Italian Programma Nazionale di Ricerche in Antartide have also conducted systematic searches for Antarctic meteorites.
The Antarctic Scientific Exploration of China has conducted successful meteorite searches since 2000. A Korean program (KOREAMET) was launched in 2007 and has collected a few meteorites. The combined efforts of all of these expeditions have produced more than 23,000 classified meteorite specimens since 1974, with thousands more that have not yet been classified. For more information see the article by Harvey (2003).
Australia.
At about the same time as meteorite concentrations were being discovered in the cold desert of Antarctica, collectors discovered that many meteorites could also be found in the hot deserts of Australia. Several dozen meteorites had already been found in the Nullarbor region of Western and South Australia. Systematic searches between about 1971 and the present recovered more than 500 others, ~300 of which are currently well characterized. The meteorites can be found in this region because the land presents a flat, featureless, plain covered by limestone. In the extremely arid climate, there has been relatively little weathering or sedimentation on the surface for tens of thousands of years, allowing meteorites to accumulate without being buried or destroyed. The dark colored meteorites can then be recognized among the very different looking limestone pebbles and rocks.
The Sahara.
In 1986–87, a German team installing a network of seismic stations while prospecting for oil discovered about 65 meteorites on a flat, desert plain about southeast of Dirj (Daraj), Libya. A few years later, a desert enthusiast saw photographs of meteorites being recovered by scientists in Antarctica, and thought that he had seen similar occurrences in northern Africa. In 1989, he recovered about 100 meteorites from several distinct locations in Libya and Algeria. Over the next several years, he and others who followed found at least 400 more meteorites. The find locations were generally in regions known as regs or hamadas: flat, featureless areas covered only by small pebbles and minor amounts of sand. Dark-colored meteorites can be easily spotted in these places. In the case of several meteorite fields, such as Dar el Gani, Dhofar, and others, favorable light-colored geology consisting of basic rocks (clays, dolomites, and limestones) makes meteorites particularly easy to identify.
Although meteorites had been sold commercially and collected by hobbyists for many decades, up to the time of the Saharan finds of the late 1980s and early 1990s, most meteorites were deposited in or purchased by museums and similar institutions where they were exhibited and made available for scientific research. The sudden availability of large numbers of meteorites that could be found with relative ease in places that were readily accessible (especially compared to Antarctica), led to a rapid rise in commercial collection of meteorites. This process was accelerated when, in 1997, meteorites coming from both the Moon and Mars were found in Libya. By the late 1990s, private meteorite-collecting expeditions had been launched throughout the Sahara. Specimens of the meteorites recovered in this way are still deposited in research collections, but most of the material is sold to private collectors. These expeditions have now brought the total number of well-described meteorites found in Algeria and Libya to more than 500.
Northwest Africa.
Meteorite markets came into existence in the late 1990s, especially in Morocco. This trade was driven by Western commercialization and an increasing number of collectors. The meteorites were supplied by nomads and local people who combed the deserts looking for specimens to sell. Many thousands of meteorites have been distributed in this way, most of which lack any information about how, when, or where they were discovered. These are the so-called "Northwest Africa" meteorites. When they get classified, they are named "Northwest Africa" (abbreviated NWA) followed by a number. It is generally accepted that NWA meteorites originate in Morocco, Algeria, Western Sahara, Mali, and possibly even further afield. Nearly all of these meteorites leave Africa through Morocco. Scores of important meteorites, including Lunar and Martian ones, have been discovered and made available to science via this route. A few of the more notable meteorites recovered include Tissint and Northwest Africa 7034. Tissint was the first witnessed Martian meteorite fall in over fifty years; NWA 7034 is the oldest meteorite known to come from Mars, and is a unique water-bearing regolith breccia.
Arabian Peninsula.
In 1999, meteorite hunters discovered that the desert in southern and central Oman were also favorable for the collection of many specimens. The gravel plains in the Dhofar and Al Wusta regions of Oman, south of the sandy deserts of the Rub' al Khali, had yielded about 5,000 meteorites as of mid-2009. Included among these are a large number of lunar and Martian meteorites, making Oman a particularly important area both for scientists and collectors. Early expeditions to Oman were mainly done by commercial meteorite dealers, however international teams of Omani and European scientists have also now collected specimens.
The recovery of meteorites from Oman is currently prohibited by national law, but a number of international hunters continue to remove specimens now deemed "national treasures". This new law provoked a small international incident, as its implementation preceded any public notification of such a law, resulting in the prolonged imprisonment of a large group of meteorite hunters, primarily from Russia, but whose party also consisted of members from the US as well as several other European countries.
The Black Stone in the wall of the Kaaba in Mecca is thought to be a meteorite by some historians, but there is little support for this in the scientific literature.
The American Southwest.
Beginning in the mid-1990s, amateur meteorite hunters began scouring the arid areas of the southwestern United States. To date, meteorites numbering possibly into the thousands have been recovered from the Mojave, Sonoran, Great Basin, and Chihuahuan Deserts, with many being recovered on dry lake beds. Significant finds include the Superior Valley 014 Acapulcoite, one of two of its type found within the United States, as well as the Blue Eagle meteorite, the first Rumuruti-type chondrite yet found in the Americas. Perhaps the most notable find in recent years has been the Los Angeles meteorite, a Martian meteorite that was found by Robert Verish.
A number of finds from the American Southwest have yet to be formally submitted to the Meteorite Nomenclature Committee, as many finders think it is unwise to publicly state the coordinates of their discoveries for fear of confiscation by the federal government and competition with other hunters at published find sites. 
Several of the meteorites found recently are currently on display in the Griffith Observatory in Los Angeles.
Meteorites in history.
Meteorite falls may have been the source of cultish worship. The cult in the Temple of Artemis at Ephesus, one of the Seven Wonders of the Ancient World, possibly originated with the observation of a meteorite fall that was understood by contemporaries to have fallen to the earth from the abode of deities. There are reports that a sacred stone was enshrined at the temple that may have been a meteorite. Although the use of the metal found in meteorites is also recorded in myths of many countries and cultures where the celestial source was often acknowledged, scientific documentation only began in the last few centuries.
The oldest known iron artifacts are nine small beads hammered from meteoritic iron. They were found in northern Egypt and have been securely dated to 3200 BC.
In the 1970s, a stone meteorite was uncovered during an archaeological dig at Danebury Iron Age hillfort, Danebury England. It was found deposited part way down in an Iron Age pit (c. 1200 BC). Since it must have been deliberately placed there, this could indicate one of the first (known) human finds of a meteorite in Europe.
Some Native Americans treated meteorites as ceremonial objects. In 1915, a 135-pound iron meteorite was found in a Sinagua (c. 1100–1200 AD) burial cyst near Camp Verde, Arizona, respectfully wrapped in a feather cloth. A small pallasite was found in a pottery jar in an old burial found at Pojoaque Pueblo, New Mexico. Nininger reports several other such instances, in the Southwest US and elsewhere, such as the discovery of Native American beads of meteoric iron found in Hopewell burial mounds, and the discovery of the Winona meteorite in a Native American stone-walled crypt.
Indigenous peoples often prized iron-nickel meteorites as an easy, if limited, source of iron metal. For example, the Inuit used chips of the Cape York meteorite to form cutting edges for tools and spear tips.
Two of the oldest recorded meteorite falls in Europe are the Elbogen (1400) and Ensisheim (1492) meteorites. The German physicist, Ernst Florens Chladni, was the first to publish (in 1794) the then audacious idea that meteorites were rocks from space. His booklet was ""On the Origin of the Iron Masses Found by Pallas and Others Similar to it, and on Some Associated Natural Phenomena"". In this he compiled all available data on several meteorite finds and falls concluded that they must have their origins in outer space. The scientific community of the time responded with resistance and mockery. It took nearly ten years before a general acceptance of the origin of meteorites was achieved through the work of the French scientist Jean-Baptiste Biot and the British chemist, Edward Howard. Biot's study, initiated by the French Academy of Sciences, was compelled by a fall of thousands of meteorites on 26 April 1803 from the skies of L'Aigle, France.
One of the leading theories for the cause of the Cretaceous–Paleogene extinction event that included the dinosaurs is a large meteorite impact. The Chicxulub Crater has been identified as the site of this impact. There has been a lively scientific debate as to whether other major extinctions, including the ones at the end of the Permian and Triassic periods might also have been the result of large impact events, but the evidence is much less compelling than for the end Cretaceous extinction.
There are several reported instances of falling meteorites having killed people and livestock, but a few of these appear more credible than others. The most infamous reported fatality from a meteorite impact is that of an Egyptian dog that was killed in 1911, although this report is highly disputed. This meteorite fall was identified in the 1980s as Martian in origin. There is substantial evidence that the meteorite known as Valera (Venezuela 1972, see Meteorite fall) hit and killed a cow upon impact, nearly dividing the animal in two, and similar unsubstantiated reports of a horse being struck and killed by a stone of the New Concord fall also abound. Throughout history, many first and second-hand reports of meteorites falling on and killing both humans and other animals abound. One example is from 1490 AD in China, which purportedly killed thousands of people. John Lewis has compiled some of these reports, and summarizes, "No one in recorded history has ever been killed by a meteorite in the presence of a meteoriticist and a medical doctor" and "reviewers who make sweeping negative conclusions usually do not cite any of the primary publications in which the eyewitnesses describe their experiences, and give no evidence of having read them".
The first known modern case of a human hit by a space rock occurred on 30 November 1954 in Sylacauga, Alabama. There a stone chondrite crashed through a roof and hit Ann Hodges in her living room after it bounced off her radio. She was badly bruised. The Hodges meteorite, or Sylacauga meteorite, is currently on exhibit at the Alabama Museum of Natural History.
Another claim was put forth by a young boy who stated that he had been hit by a small (~3 gram) stone of the Mbale meteorite fall from Uganda, and who stood to gain nothing from this assertion. The stone reportedly fell through banana leaves before striking the boy on the head, causing little to no pain, as it was small enough to have been slowed by both friction with the atmosphere as well as that with banana leaves, before striking the boy.
Several persons have since claimed to have been struck by "meteorites" but no verifiable meteorites have resulted.
Meteorite weathering.
Most meteorites date from the oldest times in the solar system and are by far the oldest material available on our planet. Despite their age, they are fairly vulnerable to terrestrial environment: water, salt, oxygen attack the meteorites as soon they reach the ground.
The terrestrial alteration of meteorites is called weathering. In order to quantify the degree of alteration that a meteorite experienced, several qualitative weathering indices have been applied to Antarctic and desertic samples.
The most known weathering scale, used for ordinary chondrites, ranges from W0 (pristine state) to W6 (heavy alteration).
Notable meteorites.
Apart from meteorites fallen onto the Earth, two tiny fragments of asteroids were found among the samples collected on the Moon; these were the Bench Crater meteorite (Apollo 12, 1969) and the Hadley Rille meteorite (Apollo 15, 1971). The Opportunity rover discovered the "Heat Shield Rock" meteorite on Mars and five similar iron meteorites. Two nickel-iron meteorites were identified by the Spirit rover. (see also: )

</doc>
<doc id="19938" url="https://en.wikipedia.org/wiki?curid=19938" title="Mega-">
Mega-

Mega is a unit prefix in the metric system denoting a factor of one million (106 or 1000000 (number)). Symbol (M). It was confirmed for use in the International System of Units (SI) in 1960. "Mega" comes from the Greek , meaning "great".
Exponentiation.
When units occur in exponentiation, such as in square and cubic forms, any size prefix is considered part of the unit, and thus included in the exponentiation.
Computing.
In computing, "mega" may sometimes denote 1,048,576 (220) of information units (example: a megabyte, a megaword), but denotes (106) units of other quantities, for example, transfer rates: = . The prefix "mebi" has been suggested as a prefix for 220 to avoid ambiguity.

</doc>
<doc id="19940" url="https://en.wikipedia.org/wiki?curid=19940" title="Maciej Płażyński">
Maciej Płażyński

Maciej Płażyński (; 10 February 1958 – 10 April 2010) was a Polish liberal-conservative politician.
Płażyński was born in Młynary. He began his political career in 1980 / 1981 as one of the leaders of the Students' Solidarity; he was governor of the Gdańsk Voivodship from August 1990 to July 1996, and was elected to the Sejm (the lower house of the Polish parliament) in September 1997. To date he is longest serving Marshal of the Sejm of the Third Republic of Poland
In January 2001, he founded the Civic Platform political party with Donald Tusk and Andrzej Olechowski. He left Civic Platform for personal reasons and at the time of his death was an independent MP. He was member of Kashubian-Pomeranian Association. He was later chosen as a chairman of the Association "Polish Community".
Maciej Płażyński was married to Elżbieta Płażyńska and together they had three children: Jakub, Katarzyna, and Kacper.
He was listed on the flight manifest of the Tupolev Tu-154 of the 36th Special Aviation Regiment carrying the President of Poland Lech Kaczyński which crashed while landing at Smolensk-North airport near Pechersk near Smolensk, Russia, on 10 April 2010, killing all aboard.
Honours and awards.
In 2000, Płażyński was awarded the Order of Merit of the Italian Republic, First Class. He received the titles of honorary citizen of Młynary, Puck, Pionki and Lidzbark Warmiński.
On 16 April 2010 he was posthumously awarded the Grand Cross of the Order of Polonia Restituta. He was also awarded a Gold Medal of Gloria Artis.

</doc>
<doc id="19941" url="https://en.wikipedia.org/wiki?curid=19941" title="Mark Bingham">
Mark Bingham

Mark Kendall Bingham (May 22, 1970 – September 11, 2001) was an American public relations executive who founded his own company, the Bingham Group. During the September 11 attacks in 2001 he was a passenger on board United Airlines Flight 93. Bingham was among the passengers who, along with Todd Beamer, Tom Burnett and Jeremy Glick, formed the plan to retake the plane from the hijackers, and led the effort that resulted in the crash of the plane into a field near Shanksville, Pennsylvania, thwarting the hijackers plan to crash the plane into a building in Washington, D.C., most likely either the U.S. Capitol Building or the White House.
Both for his presence on United 93, as well as his athletic physique and masculine lifestyle, Bingham has been widely honored posthumously for having "smashed the gay stereotype mold and really opened the door to many others that came after him."
Early life.
Mark Bingham was born in 1970, the only child of mother Alice Hoagland and father Gerald Bingham. He grew up in Miami, Florida, and Southern California before moving to the San Jose area in 1983. Bingham was an aspiring filmmaker growing up, and began using a video camera as a teenager as a personal diary through which he expressed himself and documented his life and the lives of his family and friends. He accumulated hundreds of hours of video documenting the final decade and a half of his life. He graduated from Los Gatos High School as a two-year captain of his rugby team in 1988. As an undergraduate at the University of California, Berkeley, Bingham played on two of Coach Jack Clark's national-championship-winning rugby teams in the early 1990s. He also joined the Chi Psi fraternity, eventually becoming its president. Upon graduation at the age of twenty-one, Bingham came out as gay to his family and friends.
Rugby and business career.
A large athlete at 6 ft 4 in (190 cm) and , Bingham also played for the gay-inclusive rugby union team San Francisco Fog RFC. Bingham played No. 8 in their first two friendly matches. He played in their first tournament, and taught his teammates his favorite rugby songs.
Bingham had recently opened a satellite office of his public relations firm in New York City, and was spending more time on the East Coast. He discussed plans with his friend Scott Glaessgen to form a New York City rugby team, the Gotham Knights.
On September 11, 2001.
On the morning of September 11, Bingham overslept and nearly missed his flight, on his way to San Francisco to be an usher in his fraternity brother Joseph Salama's wedding. He arrived at the Terminal A at 7:40am, ran to Gate 17, and was the last passenger to board United Airlines Flight 93, taking seat 4D, next to passenger Tom Burnett.
United Flight 93 was scheduled to depart at 8:00am, but the Boeing 757 did not depart until 42 minutes later due to runway traffic delays. Four minutes later, American Airlines Flight 11 crashed into the World Trade Center's North Tower. Fifteen minutes later, at 9:03 am, as United Flight 175 crashed into the South Tower, United 93 was climbing to cruising altitude, heading west over New Jersey and into Pennsylvania. At 9:25 am, Flight 93 was above eastern Ohio, and pilots Jason Dahl and LeRoy Homer received an alert, "beware of cockpit intrusion," on the cockpit computer device ACARS (Aircraft Communications and Reporting System). Three minutes later, Cleveland controllers could hear screams over the cockpit's open microphone. Moments later, the hijackers, led by the Lebanese Ziad Samir Jarrah, took over the plane's controls, disengaged the autopilot, and told passengers, "Keep remaining sitting. We have a bomb board". Bingham and the other passengers were herded into the back of the plane. The curtain between first class and second class had been drawn, at which point the pilot and co-pilot were seen lying dead on the floor just outside the curtain, their throats having been cut. Within six minutes, the plane changed course and was heading for Washington, D.C. Several of the passengers made phone calls to loved ones, who informed them about the two planes that had crashed into the World Trade Center. Bingham phoned his mother, reporting that his plane had been hijacked and relaying his love for her.
After the hijackers veered the plane sharply south, the passengers decided to act. Bingham, along with Todd Beamer, Tom Burnett and Jeremy Glick formed a plan to take the plane back from the hijackers. They were joined by other passengers, including Lou Nacke, Rich Guadagno, Alan Beaven, Honor Elizabeth Wainio, Linda Gronlund, and William Cashman, along with flight attendants Sandra Bradshaw and Cee Cee Ross-Lyles, in discussing their options and voting on a course of action, ultimately deciding to storm the cockpit and take over the plane.
According to the "9/11 Commission Report", after the plane's voice data recorder was recovered, it revealed pounding and crashing sounds against the cockpit door and shouts and screams in English. "Let's get them!" a passenger cries. A hijacker shouts, "Allah akbar!" ("God is great"). Jarrah repeatedly pitched the plane to knock passengers off their feet, but the passengers apparently managed to invade the cockpit, where one was heard shouting, "In the cockpit. If we don't, we'll die." At 10:02 am, a hijacker ordered, "Pull it down! Pull it down!" The 9/11 Commission later reported that the plane's control wheel was turned hard to the right, causing it to roll on its back and plow into an empty field in Shanksville, Pennsylvania at 580 miles an hour, killing everyone on board. The plane was twenty minutes of flying time away from its suspected target, the White House or the U.S. Capitol Building in Washington, D.C. According to Vice President Dick Cheney, President George W. Bush had given the order to shoot the plane down had it continued its path to Washington.
Legacy.
Bingham was survived by his parents, stepmother and his former partner of six years, Paul Holm, who said Bingham had risked his life to protect the lives of others on occasions prior to 9/11, having twice successfully protected Holm from attempted muggings, one at gunpoint. Holm described Bingham as a brave, competitive man, saying, "He hated to lose—at anything." He was known to proudly display a scar he received after being gored at the Running of the Bulls in Pamplona, Spain.
U.S. Senators John McCain and Barbara Boxer honored Bingham on September 17, 2001, in a ceremony for San Francisco Bay Area victims of the attacks, presenting a folded American flag to Paul Holm.
The Mark Kendall Bingham Memorial Tournament (referred to as the Bingham Cup), a biennial international rugby union competition predominantly for gay and bisexual men, was established in 2002 in his memory.
Bingham, along with the other passengers on Flight 93, was posthumously awarded the Arthur Ashe Courage Award in 2002.
The Eureka Valley Recreation Center's Gymnasium in San Francisco was renamed the Mark Bingham Gymnasium in August 2002.
Singer Melissa Etheridge dedicated the song "Tuesday Morning" in 2004 to his memory.
Beginning in 2005, the Mark Bingham Award for Excellence in Achievement has been awarded by the California Alumni Association of the University of California, Berkeley to a young alumnus or alumna at its annual Charter Gala.
At the National 9/11 Memorial, Bingham and other passengers from Flight 93 are memorialized at the South Pool, on Panel S-67.
At the Flight 93 National Memorial in Pennsylvania, Bingham's name is located on one of the 40 8-foot-tall panels of polished, 3-inch thick granite that comprise the Memorial's Wall of Names.
The 2012 feature-length documentary "With You" focuses on Bingham and the bond he had with his mother, Alice Hoagland, a former United Airlines flight attendant who, following his death, became a nationally known authority on airline safety and a champion of LGBT rights. Directed by Scott Gracheff, the film relies on the vast amount of video footage Bingham himself shot beginning in his teens until weeks before his death. The film's title is a popular rugby term, and one of Bingham's favorite expressions.

</doc>
