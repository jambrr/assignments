<doc id="23601" url="https://en.wikipedia.org/wiki?curid=23601" title="Pi">
Pi

The number is a mathematical constant, the ratio of a circle's circumference to its diameter, commonly approximated as 3.14159. It has been represented by the Greek letter "" since the mid-18th century, though it is also sometimes spelled out as "pi" ().
Being an irrational number, cannot be expressed exactly as a fraction (equivalently, its decimal representation never ends and never settles into a permanent repeating pattern). Still, fractions such as 22/7 and other rational numbers are commonly used to approximate . The digits appear to be randomly distributed; however, to date, no proof of this has been discovered. Also, is a transcendental number – a number that is not the root of any non-zero polynomial having rational coefficients. This transcendence of implies that it is impossible to solve the ancient challenge of squaring the circle with a compass and straightedge.
Ancient civilizations needed the value of to be computed accurately for practical reasons. It was calculated to seven digits, using geometrical techniques, in Chinese mathematics and to about five in Indian mathematics in the 5th century CE. The historically first exact formula for , based on infinite series, was not available until a millennium later, when in the 14th century the Madhava–Leibniz series was discovered in Indian mathematics. In the 20th and 21st centuries, mathematicians and computer scientists discovered new approaches that, when combined with increasing computational power, extended the decimal representation of to, as of 2015, over 13.3 trillion (1013) digits. Practically all scientific applications require no more than a few hundred digits of , and many substantially fewer, so the primary motivation for these computations is the human desire to break records. However, the extensive calculations involved have been used to test supercomputers and high-precision multiplication algorithms.
Because its definition relates to the circle, is found in many formulae in trigonometry and geometry, especially those concerning circles, ellipses or spheres. It is also found in formulae used in other branches of science such as cosmology, number theory, statistics, fractals, thermodynamics, mechanics and electromagnetism. The ubiquity of makes it one of the most widely known mathematical constants both inside and outside the scientific community: Several books devoted to it have been published, the number is celebrated on Pi Day and record-setting calculations of the digits of often result in news headlines. Attempts to memorize the value of with increasing precision have led to records of over 70,000 digits.
Fundamentals.
Name.
The symbol used by mathematicians to represent the ratio of a circle's circumference to its diameter is the lowercase Greek letter , sometimes spelled out as "pi," and derived from the first letter of the Greek word "perimetros," meaning circumference. In English, is pronounced as "pie" ( , ). In mathematical use, the lowercase letter (or π in sans-serif font) is distinguished from its capital counterpart , which denotes a product of a sequence.
The choice of the symbol is discussed in the section "Adoption of the symbol ".
Definition.
The ratio is constant, regardless of the circle's size. For example, if a circle has twice the diameter of another circle it will also have twice the circumference, preserving the ratio . This definition of implicitly makes use of flat (Euclidean) geometry; although the notion of a circle can be extended to any curved (non-Euclidean) geometry, these new circles will no longer satisfy the formula .
Here, the circumference of a circle is the arc length around the perimeter of the circle, a quantity which can be formally defined independently of geometry using limits, a concept in calculus. For example, one may compute directly the arc length of the top half of the unit circle given in Cartesian coordinates by , as the integral:
An integral such as this was adopted as the definition of by Karl Weierstrass, who defined it directly as an integral in 1841.
Definitions of such as these that rely on a notion of circumference, and hence implicitly on concepts of the integral calculus, are no longer common in the literature. explains that this is because in many modern treatments of calculus, differential calculus typically precedes integral calculus in the university curriculum, so it is desirable to have a definition of that does not rely on the latter. One such definition, due to Richard Baltzer, and popularized by Edmund Landau, is the following: is twice the smallest positive number at which the cosine function equals 0. The cosine can be defined independently of geometry as a power series, or as the solution of a differential equation.
In a similar spirit, can be defined instead using properties of the complex exponential, , of a complex variable . Like the cosine, the complex exponential can be defined in one of several ways. The set of complex numbers at which is equal to one is then an (imaginary) arithmetic progression of the form:
and there is a unique positive real number with this property.
A more abstract variation on the same idea, making use of sophisticated mathematical concepts of topology and algebra, is the following theorem: there is a unique continuous isomorphism from the group formula_4 of real numbers under addition modulo integers (the circle group) onto the multiplicative group of complex numbers of absolute value one. The number is then defined as half the magnitude of the derivative of this homomorphism.
A circle encloses the largest area that can be attained within a given perimeter. Thus the number is also characterized as the best constant in the isoperimetric inequality (times one-fourth). There are many other, closely related, ways in which appears as an eigenvalue of some geometrical or physical process; see below.
Properties.
The transcendence of has two important consequences: First, cannot be expressed using any finite combination of rational numbers and square roots or "n"-th roots such as or . Second, since no transcendental number can be constructed with compass and straightedge, it is not possible to "square the circle". In other words, it is impossible to construct, using compass and straightedge alone, a square whose area is equal to the area of a given circle. Squaring a circle was one of the important geometry problems of the classical antiquity. Amateur mathematicians in modern times have sometimes attempted to square the circle and sometimes claim success despite the fact that it is impossible., p 185.</ref>
The digits of have no apparent pattern and have passed tests for statistical randomness, including tests for normality; a number of infinite length is called normal when all possible sequences of digits (of any given length) appear equally often. The conjecture that is normal has not been proven or disproven.</ref> Since the advent of computers, a large number of digits of have been available on which to perform statistical analysis. Yasumasa Kanada has performed detailed statistical analyses on the decimal digits of and found them consistent with normality; for example, the frequency of the ten digits 0 to 9 were subjected to statistical significance tests, and no evidence of a pattern was found. Despite the fact that 's digits pass statistical tests for randomness, contains some sequences of digits that may appear non-random to non-mathematicians, such as a sequence of six consecutive 9s that begins at the 762nd decimal place of the decimal representation of .
Continued fractions.
Like all irrational numbers, cannot be represented as a common fraction (also known as a simple or vulgar fraction), by the very definition of "irrational". But every irrational number, including , can be represented by an infinite series of nested fractions, called a continued fraction:
Truncating the continued fraction at any point yields a rational approximation for ; the first four of these are 3, 22/7, 333/106, and 355/113. These numbers are among the most well-known and widely used historical approximations of the constant. Each approximation generated in this way is a best rational approximation; that is, each is closer to than any other fraction with the same or a smaller denominator. Because is known to be transcendental, it is by definition not algebraic and so cannot be a quadratic irrational. Therefore, cannot have a periodic continued fraction. Although the simple continued fraction for (shown above) also does not exhibit any other obvious pattern, mathematicians have discovered several generalized continued fractions that do, such as:
Approximate value.
Some approximations of "pi" include:
Spectral characterizations.
The number appears naturally in many applications via its special role as an eigenvalue. For example, the derivative operator on the space of complex-valued functions formula_7 on the interval formula_8 with formula_9 has "i" as an eigenvalue, because the function formula_10 satisfies formula_11.
The number is in fact the "least" such eigenvalue. This follows from Wirtinger's inequality for functions, which states that for a function formula_12 such that formula_7 and formula_14 are both square integrable (see Sobolev space), satisfying formula_15,
where the inequality is in fact an equality precisely when formula_7 is a multiple of formula_18. So formula_19 appears as an optimal constant in Wirtinger's inequality, and from this it follows that it is the smallest such eigenvalue (by Rayleigh quotient methods).
The number serves a similar role in higher-dimensional analysis, appearing as eigenvalues for other similar kinds of problems. As mentioned above, the number is also characterized by its role in the best constant of the isoperimetric inequality. This inequality asserts that the area "A" enclosed by a plane Jordan curve of perimeter "P" satisfies the inequality
and equality is clearly achieved for the circle. This fact is ultimately also connected with the fact that the constant is associated with best constants of the Poincaré inequality. For example, appears as the optimal smallest eigenvalue of the Dirichlet energy, in dimensions 1 and 2, which thus characterizes the role of in many physical phenomena as well.
History.
Antiquity.
The best known approximations to dating before the Common Era were accurate to two decimal places; this was improved upon in Chinese mathematics in particular by the mid first millennium, to an accuracy of seven decimal places.
After this, no further progress was made until the late medieval period.
Some Egyptologists have claimed that the ancient Egyptians used an approximation of as from as early as the Old Kingdom.
This claim has met with skepticism., p. 30.See also .See also </ref>Skeptics: Shermer, Michael, "The Skeptic Encyclopedia of Pseudoscience", ABC-CLIO, 2002, pp 407–408, ISBN 9781576076538.See also Fagan, Garrett G., "Archaeological Fantasies: How Pseudoarchaeology Misrepresents The Past and Misleads the Public", Routledge, 2006, ISBN 9780415305938.For a list of explanations for the shape that do not involve , see </ref>
The earliest written approximations of are found in Egypt and Babylon, both within one percent of the true value. In Babylon, a clay tablet dated 1900–1600 BC has a geometrical statement that, by implication, treats as  = 3.1250. In Egypt, the Rhind Papyrus, dated around 1650 BC but copied from a document dated to 1850 BC, has a formula for the area of a circle that treats as ()2 ≈ 3.1605.
Astronomical calculations in the "Shatapatha Brahmana" (ca. 4th century BC) use a fractional approximation of  ≈ 3.139 (an accuracy of 9×10−4). Other Indian sources by about 150 BC treat as  ≈ 3.1622
Polygon approximation era.
The first recorded algorithm for rigorously calculating the value of was a geometrical approach using polygons, devised around 250 BC by the Greek mathematician Archimedes. This polygonal algorithm dominated for over 1,000 years, and as a result is sometimes referred to as "Archimedes' constant". Archimedes computed upper and lower bounds of by drawing a regular hexagon inside and outside a circle, and successively doubling the number of sides until he reached a 96-sided regular polygon. By calculating the perimeters of these polygons, he proved that (that is ). Archimedes' upper bound of may have led to a widespread popular belief that is equal to . Around 150 AD, Greek-Roman scientist Ptolemy, in his "Almagest", gave a value for of 3.1416, which he may have obtained from Archimedes or from Apollonius of Perga.</ref> Mathematicians using polygonal algorithms reached 39 digits of in 1630, a record only broken in 1699 when infinite series were used to reach 71 digits.
In ancient China, values for included 3.1547 (around 1 AD), (100 AD, approximately 3.1623), and (3rd century, approximately 3.1556). Around 265 AD, the Wei Kingdom mathematician Liu Hui created a polygon-based iterative algorithm and used it with a 3,072-sided polygon to obtain a value of of 3.1416. Liu later invented a faster method of calculating and obtained a value of 3.14 with a 96-sided polygon, by taking advantage of the fact that the differences in area of successive polygons form a geometric series with a factor of 4. The Chinese mathematician Zu Chongzhi, around 480 AD, calculated that (a fraction that goes by the name "Milü" in Chinese), using Liu Hui's algorithm applied to a 12,288-sided polygon. With a correct value for its seven first decimal digits, this value of 3.141592920... remained the most accurate approximation of available for the next 800 years.
The Indian astronomer Aryabhata used a value of 3.1416 in his "Āryabhaṭīya" (499 AD). Fibonacci in c. 1220 computed 3.1418 using a polygonal method, independent of Archimedes. Italian author Dante apparently employed the value .
The Persian astronomer Jamshīd al-Kāshī produced 9 sexagesimal digits, roughly the equivalent of 16 decimal digits, in 1424 using a polygon with 3×228 sides, which stood as the world record for about 180 years. French mathematician François Viète in 1579 achieved 9 digits with a polygon of 3×217 sides. Flemish mathematician Adriaan van Roomen arrived at 15 decimal places in 1593. In 1596, Dutch mathematician Ludolph van Ceulen reached 20 digits, a record he later increased to 35 digits (as a result, was called the "Ludolphian number" in Germany until the early 20th century). Dutch scientist Willebrord Snellius reached 34 digits in 1621, and Austrian astronomer Christoph Grienberger arrived at 38 digits in 1630 using 1040 sides, which remains the most accurate approximation manually achieved using polygonal algorithms.
Infinite series.
The calculation of was revolutionized by the development of infinite series techniques in the 16th and 17th centuries. An infinite series is the sum of the terms of an infinite sequence. Infinite series allowed mathematicians to compute with much greater precision than Archimedes and others who used geometrical techniques. Although infinite series were exploited for most notably by European mathematicians such as James Gregory and Gottfried Wilhelm Leibniz, the approach was first discovered in India sometime between 1400 and 1500 AD.</ref> The first written description of an infinite series that could be used to compute was laid out in Sanskrit verse by Indian astronomer Nilakantha Somayaji in his "Tantrasamgraha", around 1500 AD. The series are presented without proof, but proofs are presented in a later Indian work, "Yuktibhāṣā", from around 1530 AD. Nilakantha attributes the series to an earlier Indian mathematician, Madhava of Sangamagrama, who lived c. 1350 – c. 1425. Several infinite series are described, including series for sine, tangent, and cosine, which are now referred to as the Madhava series or Gregory–Leibniz series. Madhava used infinite series to estimate to 11 digits around 1400, but that value was improved on around 1430 by the Persian mathematician Jamshīd al-Kāshī, using a polygonal algorithm.
[[File:GodfreyKneller-IsaacNewton-1689.jpg|thumb|upright|alt=A formal portrait of a man, with long hair|Isaac Newton
used infinite series to compute to 15 digits, later writing "I am ashamed to tell you to how many figures I carried these computations".]]
The first infinite sequence discovered in Europe was an infinite product (rather than an infinite sum, which are more typically used in calculations) found by French mathematician François Viète in 1593:
The second infinite sequence found in Europe, by John Wallis in 1655, was also an infinite product. The discovery of calculus, by English scientist Isaac Newton and German mathematician Gottfried Wilhelm Leibniz in the 1660s, led to the development of many infinite series for approximating . Newton himself used an arcsin series to compute a 15 digit approximation of in 1665 or 1666, later writing "I am ashamed to tell you to how many figures I carried these computations, having no other business at the time."
In Europe, Madhava's formula was rediscovered by Scottish mathematician James Gregory in 1671, and by Leibniz in 1674:
This formula, the Gregory–Leibniz series, equals when evaluated with  = 1. In 1699, English mathematician Abraham Sharp used the Gregory–Leibniz series to compute to 71 digits, breaking the previous record of 39 digits, which was set with a polygonal algorithm. The Gregory–Leibniz series is simple, but converges very slowly (that is, approaches the answer gradually), so it is not used in modern calculations.
In 1706 John Machin used the Gregory–Leibniz series to produce an algorithm that converged much faster:
Machin reached 100 digits of with this formula. Other mathematicians created variants, now known as Machin-like formulae, that were used to set several successive records for calculating digits of . Machin-like formulae remained the best-known method for calculating well into the age of computers, and were used to set records for 250 years, culminating in a 620-digit approximation in 1946 by Daniel Ferguson – the best approximation achieved without the aid of a calculating device.
A record was set by the calculating prodigy Zacharias Dase, who in 1844 employed a Machin-like formula to calculate 200 decimals of in his head at the behest of German mathematician Carl Friedrich Gauss. British mathematician William Shanks famously took 15 years to calculate to 707 digits, but made a mistake in the 528th digit, rendering all subsequent digits incorrect.
Rate of convergence.
Some infinite series for converge faster than others. Given the choice of two infinite series for , mathematicians will generally use the one that converges more rapidly because faster convergence reduces the amount of computation needed to calculate to any given accuracy.</ref> A simple infinite series for is the Gregory–Leibniz series:
As individual terms of this infinite series are added to the sum, the total gradually gets closer to , and – with a sufficient number of terms – can get as close to as desired. It converges quite slowly, though – after 500,000 terms, it produces only five correct decimal digits of .
An infinite series for (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory–Leibniz series is:
The following table compares the convergence rates of these two series:
After five terms, the sum of the Gregory–Leibniz series is within 0.2 of the correct value of , whereas the sum of Nilakantha's series is within 0.002 of the correct value of . Nilakantha's series converges faster and is more useful for computing digits of . Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term.
Irrationality and transcendence.
Not all mathematical advances relating to were aimed at increasing the accuracy of approximations. When Euler solved the Basel problem in 1735, finding the exact value of the sum of the reciprocal squares, he established a connection between and the prime numbers that later contributed to the development and study of the Riemann zeta function:
Swiss scientist Johann Heinrich Lambert in 1761 proved that is irrational, meaning it is not equal to the quotient of any two whole numbers. Lambert's proof exploited a continued-fraction representation of the tangent function. French mathematician Adrien-Marie Legendre proved in 1794 that 2 is also irrational. In 1882, German mathematician Ferdinand von Lindemann proved that is transcendental, confirming a conjecture made by both Legendre and Euler. Hardy and Wright states that "the proofs were afterwards modified and simplified by Hilbert, Hurwitz, and other writers".
Adoption of the symbol.
The earliest known use of the Greek letter to represent the ratio of a circle's circumference to its diameter was by Welsh mathematician William Jones in his 1706 work "Synopsis Palmariorum Matheseos; or, a New Introduction to the Mathematics". The Greek letter first appears there in the phrase "1/2 Periphery ()" in the discussion of a circle with radius one. Jones may have chosen because it was the first letter in the Greek spelling of the word "periphery". However, he writes that his equations for are from the "ready pen of the truly ingenious Mr. John Machin", leading to speculation that Machin may have employed the Greek letter before Jones. It had indeed been used earlier for geometric concepts. William Oughtred used and δ, the Greek letter equivalents of p and d, to express ratios of periphery and diameter in the 1647 and later editions of "Clavis Mathematicae".
After Jones introduced the Greek letter in 1706, it was not adopted by other mathematicians until Euler started using it, beginning with his 1736 work "Mechanica". Before then, mathematicians sometimes used letters such as "c" or "p" instead. Because Euler corresponded heavily with other mathematicians in Europe, the use of the Greek letter spread rapidly. In 1748, Euler used in his widely read work "Introductio in analysin infinitorum" (he wrote: "for the sake of brevity we will write this number as ; thus is equal to half the circumference of a circle of radius 1") and the practice was universally adopted thereafter in the Western world.
Modern quest for more digits.
Computer era and iterative algorithms.
The development of computers in the mid-20th century again revolutionized the hunt for digits of . American mathematicians John Wrench and Levi Smith reached 1,120 digits in 1949 using a desk calculator. Using an inverse tangent (arctan) infinite series, a team led by George Reitwiesner and John von Neumann that same year achieved 2,037 digits with a calculation that took 70 hours of computer time on the ENIAC computer. The record, always relying on an arctan series, was broken repeatedly (7,480 digits in 1957; 10,000 digits in 1958; 100,000 digits in 1961) until 1 million digits were reached in 1973.
Two additional developments around 1980 once again accelerated the ability to compute . First, the discovery of new iterative algorithms for computing , which were much faster than the infinite series; and second, the invention of fast multiplication algorithms that could multiply large numbers very rapidly. Such algorithms are particularly important in modern computations, because most of the computer's time is devoted to multiplication. They include the Karatsuba algorithm, Toom–Cook multiplication, and Fourier transform-based methods.
The iterative algorithms were independently published in 1975–1976 by American physicist Eugene Salamin and Australian scientist Richard Brent. These avoid reliance on infinite series. An iterative algorithm repeats a specific calculation, each iteration using the outputs from prior steps as its inputs, and produces a result in each step that converges to the desired value. The approach was actually invented over 160 years earlier by Carl Friedrich Gauss, in what is now termed the arithmetic–geometric mean method (AGM method) or Gauss–Legendre algorithm. As modified by Salamin and Brent, it is also referred to as the Brent–Salamin algorithm.
The iterative algorithms were widely used after 1980 because they are faster than infinite series algorithms: whereas infinite series typically increase the number of correct digits additively in successive terms, iterative algorithms generally "multiply" the number of correct digits at each step. For example, the Brent-Salamin algorithm doubles the number of digits in each iteration. In 1984, the Canadian brothers John and Peter Borwein produced an iterative algorithm that quadruples the number of digits in each step; and in 1987, one that increases the number of digits five times in each step.See for details of algorithms.</ref> Iterative methods were used by Japanese mathematician Yasumasa Kanada to set several records for computing between 1995 and 2002. This rapid convergence comes at a price: the iterative algorithms require significantly more memory than infinite series.
Motivations for computing.
For most numerical calculations involving , a handful of digits provide sufficient precision. According to Jörg Arndt and Christoph Haenel, thirty-nine digits are sufficient to perform most cosmological calculations, because that is the accuracy necessary to calculate the circumference of the observable universe with a precision of one atom."39 digits of are sufficient to calculate the volume of the universe to the nearest atom." Accounting for additional digits needed to compensate for computational round-off errors, Arndt concludes that a few hundred digits would suffice for any scientific application.</ref> Despite this, people have worked strenuously to compute to thousands and millions of digits. This effort may be partly ascribed to the human compulsion to break records, and such achievements with often make headlines around the world. They also have practical benefits, such as testing supercomputers, testing numerical analysis algorithms (including high-precision multiplication algorithms); and within pure mathematics itself, providing data for evaluating the randomness of the digits of .
Rapidly convergent series.
Modern calculators do not use iterative algorithms exclusively. New infinite series were discovered in the 1980s and 1990s that are as fast as iterative algorithms, yet are simpler and less memory intensive. The fast iterative algorithms were anticipated in 1914, when the Indian mathematician Srinivasa Ramanujan published dozens of innovative new formulae for , remarkable for their elegance, mathematical depth, and rapid convergence. One of his formulae, based on modular equations, is
This series converges much more rapidly than most arctan series, including Machin's formula. Bill Gosper was the first to use it for advances in the calculation of , setting a record of 17 million digits in 1985. Ramanujan's formulae anticipated the modern algorithms developed by the Borwein brothers and the Chudnovsky brothers. The Chudnovsky formula developed in 1987 is
It produces about 14 digits of per term, and has been used for several record-setting calculations, including the first to surpass 1 billion (109) digits in 1989 by the Chudnovsky brothers, 2.7 trillion (2.7×1012) digits by Fabrice Bellard in 2009, and 10 trillion (1013) digits in 2011 by Alexander Yee and Shigeru Kondo.Bellard, Fabrice, "Computation of 2700 billion decimal digits of Pi using a Desktop Computer", 11 Feb 2010.</ref> For similar formulas, see also the Ramanujan–Sato series.
In 2006, Canadian mathematician Simon Plouffe used the PSLQ integer relation algorithm to generate several new formulas for , conforming to the following template:
where is (Gelfond's constant), is an odd number, and are certain rational numbers that Plouffe computed.
Spigot algorithms.
Two algorithms were discovered in 1995 that opened up new avenues of research into . They are called spigot algorithms because, like water dripping from a spigot, they produce single digits of that are not reused after they are calculated. This is in contrast to infinite series or iterative algorithms, which retain and use all intermediate digits until the final result is produced.
American mathematicians Stan Wagon and Stanley Rabinowitz produced a simple spigot algorithm in 1995. Its speed is comparable to arctan algorithms, but not as fast as iterative algorithms.
Another spigot algorithm, the BBP digit extraction algorithm, was discovered in 1995 by Simon Plouffe:
This formula, unlike others before it, can produce any individual hexadecimal digit of without calculating all the preceding digits. Individual binary digits may be extracted from individual hexadecimal digits, and octal digits can be extracted from one or two hexadecimal digits. Variations of the algorithm have been discovered, but no digit extraction algorithm has yet been found that rapidly produces decimal digits. An important application of digit extraction algorithms is to validate new claims of record computations: After a new record is claimed, the decimal result is converted to hexadecimal, and then a digit extraction algorithm is used to calculate several random hexadecimal digits near the end; if they match, this provides a measure of confidence that the entire computation is correct.
Between 1998 and 2000, the distributed computing project PiHex used Bellard's formula (a modification of the BBP algorithm) to compute the quadrillionth (1015th) bit of , which turned out to be 0.Bellards formula in: </ref> In September 2010, a Yahoo! employee used the company's Hadoop application on one thousand computers over a 23-day period to compute 256 bits of at the two-quadrillionth (2×1015th) bit, which also happens to be zero.
Use.
Because is closely related to the circle, it is found in many formulae from the fields of geometry and trigonometry, particularly those concerning circles, spheres, or ellipses. Formulae from other branches of science also include in some of their important formulae, including sciences such as statistics, fractals, thermodynamics, mechanics, cosmology, number theory, and electromagnetism.
Geometry and trigonometry.
The formulae above are special cases of the surface area and volume of an "n"-dimensional sphere.
In that integral the function represents the top half of a circle (the square root is a consequence of the Pythagorean theorem), and the integral computes the area between that half of a circle and the axis.
The trigonometric functions rely on angles, and mathematicians generally use radians as units of measurement. plays an important role in angles measured in radians, which are defined so that a complete circle spans an angle of 2 radians. The angle measure of 180° is equal to radians, and 1° = /180 radians.
Common trigonometric functions have periods that are multiples of ; for example, sine and cosine have period 2, so for any angle and any integer ,
Monte Carlo methods.
Monte Carlo methods, which evaluate the results of multiple random trials, can be used to create approximations of . Buffon's needle is one such technique: If a needle of length is dropped times on a surface on which parallel lines are drawn units apart, and if of those times it comes to rest crossing a line ( > 0), then one may approximate based on the counts:
Another Monte Carlo method for computing is to draw a circle inscribed in a square, and randomly place dots in the square. The ratio of dots inside the circle to the total number of dots will approximately equal .</ref>
Monte Carlo methods for approximating are very slow compared to other methods, and are never used to approximate when speed or accuracy is desired.</ref>
Complex numbers and analysis.
Any complex number, say , can be expressed using a pair of real numbers. In the polar coordinate system, one number (radius or "r") is used to represent 's distance from the origin of the complex plane and the other (angle or ) to represent a counter-clockwise rotation from the positive real line as follows:
where is the imaginary unit satisfying = −1. The frequent appearance of in complex analysis can be related to the behavior of the exponential function of a complex variable, described by Euler's formula:
where the constant is the base of the natural logarithm. This formula establishes a correspondence between imaginary powers of and points on the unit circle centered at the origin of the complex plane. Setting = in Euler's formula results in Euler's identity, celebrated by mathematicians because it contains the five most important mathematical constants:
There are different complex numbers satisfying , and these are called the "-th roots of unity". They are given by this formula:
Cauchy's integral formula governs complex analytic functions and establishes an important relationship between integration and differentiation, including the fact that the values of a complex function within a closed boundary are entirely determined by the values on the boundary:
An occurrence of in the Mandelbrot set fractal was discovered by American David Boll in 1991. He examined the behavior of the Mandelbrot set near the "neck" at (−0.75, 0). If points with coordinates (−0.75, ε) are considered, as ε tends to zero, the number of iterations until divergence for the point multiplied by ε converges to . The point (0.25, ε) at the cusp of the large "valley" on the right side of the Mandelbrot set behaves similarly: the number of iterations until divergence multiplied by the square root of ε tends to .
The gamma function extends the concept of factorial (normally defined only for non-negative integers) to all complex numbers, except the negative real integers. When the gamma function is evaluated at half-integers, the result contains ; for example formula_41 and formula_42. The gamma function can be used to create a simple approximation to for large : formula_43 which is known as Stirling's approximation.
Number theory and Riemann zeta function.
The Riemann zeta function is used in many areas of mathematics. When evaluated at it can be written as
Finding a simple solution for this infinite series was a famous problem in mathematics called the Basel problem. Leonhard Euler solved it in 1735 when he showed it was equal to . Euler's result leads to the number theory result that the probability of two random numbers being relatively prime (that is, having no shared factors) is equal to . This probability is based on the observation that the probability that any number is divisible by a prime is (for example, every 7th integer is divisible by 7.) Hence the probability that two numbers are both divisible by this prime is , and the probability that at least one of them is not is . For distinct primes, these divisibility events are mutually independent; so the probability that two numbers are relatively prime is given by a product over all primes:
This probability can be used in conjunction with a random number generator to approximate using a Monte Carlo approach.
Probability and statistics.
The fields of probability and statistics frequently use the normal distribution as a simple model for complex phenomena; for example, scientists generally assume that the observational error in most experiments follows a normal distribution. is found in the Gaussian function (which is the probability density function of the normal distribution) with mean and standard deviation :
The area under the graph of the normal distribution curve is given by the Gaussian integral:
while the related integral for the Cauchy distribution is
Outside mathematics.
Describing physical phenomena.
Although not a physical constant, appears routinely in equations describing fundamental principles of the universe, often because of 's relationship to the circle and to spherical coordinate systems. A simple formula from the field of classical mechanics gives the approximate period of a simple pendulum of length , swinging with a small amplitude ( is the earth's gravitational acceleration):
One of the key formulae of quantum mechanics is Heisenberg's uncertainty principle, which shows that the uncertainty in the measurement of a particle's position (Δ) and momentum (Δ) cannot both be arbitrarily small at the same time (where is Planck's constant):
In the domain of cosmology, appears in Einstein's field equation, a fundamental formula which forms the basis of the general theory of relativity and describes the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy:Ehlers, Jürgen, "Einstein's Field Equations and Their Physical Implications", Springer, 2000, p 7, ISBN 978-3-540-67073-5.</ref>
where formula_52 is the Ricci curvature tensor, is the scalar curvature, formula_53 is the metric tensor, is the cosmological constant, is Newton's gravitational constant, is the speed of light in vacuum, and formula_54 is the stress–energy tensor.
Coulomb's law, from the discipline of electromagnetism, describes the electric field between two electric charges ( and ) separated by distance (with representing the vacuum permittivity of free space):
The fact that is approximately equal to 3 plays a role in the relatively long lifetime of orthopositronium. The inverse lifetime to lowest order in the fine-structure constant is
where is the mass of the electron.
The field of fluid dynamics contains in Stokes' law, which approximates the frictional force exerted on small, spherical objects of radius , moving with velocity in a fluid with dynamic viscosity :
The Fourier transform, defined below, is a mathematical operation that expresses time as a function of frequency, known as its frequency spectrum. It has many applications in physics and engineering, particularly in signal processing.
Under ideal conditions (uniform gentle slope on an homogeneously erodible substrate), the sinuosity of a meandering river approaches . The sinuosity is the ratio between the actual length and the straight-line distance from source to mouth. Faster currents along the outside edges of a river's bends cause more erosion than along the inside edges, thus pushing the bends even farther out, and increasing the overall loopiness of the river. However, that loopiness eventually causes the river to double back on itself in places and "short-circuit", creating an ox-bow lake in the process. The balance between these two opposing factors leads to an average ratio of between the actual length and the direct distance between source and mouth.
The Wallis formula for pi can be obtained directly from the variational approach to the spectrum of the hydrogen atom in spaces of arbitrary dimensions greater than one, including the physical three dimensions.
Memorizing digits.
Many persons have memorized large numbers of digits of , a practice called piphilology. One common technique is to memorize a story or poem in which the word lengths represent the digits of : The first word has three letters, the second word has one, the third has four, the fourth has one, the fifth has five, and so on. An early example of a memorization aid, originally devised by English scientist James Jeans, is "How I want a drink, alcoholic of course, after the heavy lectures involving quantum mechanics." When a poem is used, it is sometimes referred to as a "piem". Poems for memorizing have been composed in several languages in addition to English.
The record for memorizing digits of , certified by "Guinness World Records", is 70,000 digits, recited in India by Rajveer Meena in 9 hours and 27 minutes on 21 March 2015. In 2006, Akira Haraguchi, a retired Japanese engineer, claimed to have recited 100,000 decimal places, but the claim was not verified by Guinness World Records. Record-setting memorizers typically do not rely on poems, but instead use methods such as remembering number patterns and the method of loci.
A few authors have used the digits of to establish a new form of constrained writing, where the word lengths are required to represent the digits of . The "Cadaeic Cadenza" contains the first 3835 digits of in this manner, and the full-length book "Not a Wake" contains 10,000 words, each representing one digit of .
In popular culture.
Perhaps because of the simplicity of its definition and its ubiquitous presence in formulae, has been represented in popular culture more than other mathematical constructs.
In the 2008 Open University and BBC documentary co-production, The Story of Maths, aired in October 2008 on BBC Four, British mathematician Marcus du Sautoy shows a visualization of the - historically first exact - formula for calculating when visiting India and exploring its contributions to trigonometry.
In the Palais de la Découverte (a science museum in Paris) there is a circular room known as the "pi room". On its wall are inscribed 707 digits of . The digits are large wooden characters attached to the dome-like ceiling. The digits were based on an 1853 calculation by English mathematician William Shanks, which included an error beginning at the 528th digit. The error was detected in 1946 and corrected in 1949.</ref>
In Carl Sagan's novel "Contact" it is suggested that the creator of the universe buried a message deep within the digits of . The digits of have also been incorporated into the lyrics of the song "Pi" from the album "Aerial" by Kate Bush.
Many schools in the United States observe Pi Day on 14 March (written 3/14 in the US style). and its digital representation are often used by self-described "math geeks" for inside jokes among mathematically and technologically minded groups. Several college cheers at the Massachusetts Institute of Technology include "3.14159". Pi Day in 2015 was particularly significant because the date and time 3/14/15 9:26:53 reflected many more digits of pi. Pi Day in 2016 is also significant because the date including the year - 3/14/16 - is the value of pi rounded to 4 decimal places: 3.1416
During the 2011 auction for Nortel's portfolio of valuable technology patents, Google made a series of unusually specific bids based on mathematical and scientific constants, including .
In 1958 Albert Eagle proposed replacing by = /2 to simplify formulas. However, no other authors are known to use in this way. Some people use a different value, = 6.283185... = 2, arguing that , as the number of radians in one turn or as the ratio of a circle's circumference to its radius rather than its diameter, is more natural than and simplifies many formulas. Celebrations of this number, because it approximately equals 6.28, by making 28 June "Tau Day" and eating "twice the pie", have been reported in the media. However this use of has not made its way into mainstream mathematics.
In 1897, an amateur American mathematician attempted to persuade the Indiana legislature to pass the Indiana Pi Bill, which described a method to square the circle and contained text that implied various incorrect values for , including 3.2. The bill is notorious as an attempt to establish a value of scientific constant by legislative fiat. The bill was passed by the Indiana House of Representatives, but rejected by the Senate.</ref>

</doc>
<doc id="23603" url="https://en.wikipedia.org/wiki?curid=23603" title="Postmodernism">
Postmodernism

Postmodernism describes a broad late-20th century movement that occurred across philosophy, the arts, architecture, and criticism and marked a departure from modernism. While encompassing a broad range of ideas and projects, postmodernism is typically defined by an attitude of skepticism or distrust toward grand narratives, ideologies, and various tenets of Enlightenment rationality, including the existence of objective reality and absolute truth, as well as notions of rationality, human nature, and progress. Instead, it asserts that all knowledge and "truth" are the product of unique systems of social, historical, and political discourse, and are therefore contextual and constructed. Accordingly, postmodern thought is broadly characterized by tendencies to epistemological and moral relativism, pluralism, and focus on subjectivity.
The term "postmodernism" has been applied both to the era following modernity, and to a host of movements within that era (mainly in art, music, and literature) that reacted against tendencies in modernism. Postmodernism includes skeptical critical interpretations of culture, literature, art, philosophy, history, linguistics, economics, architecture, fiction, and literary criticism. It is often associated with deconstruction and post-structuralism because its usage as a term gained significant popularity at the same time as twentieth-century post-structural thought. 
History.
The term "postmodern" was first used around the 1880s. John Watkins Chapman suggested "a Postmodern style of painting" as a way to depart from French Impressionism. J. M. Thompson, in his 1914 article in "The Hibbert Journal" (a quarterly philosophical review), used it to describe changes in attitudes and beliefs in the critique of religion: "The raison d'etre of Post-Modernism is to escape from the double-mindedness of Modernism by being thorough in its criticism by extending it to religion as well as theology, to Catholic feeling as well as to Catholic tradition."
In 1921 and 1925, postmodernism had been used to describe new forms of art and music. In 1942 H. R. Hays described it as a new literary form. However, as a general theory for a historical movement it was first used in 1939 by Arnold J. Toynbee: "Our own Post-Modern Age has been inaugurated by the general war of 1914–1918".
In 1949 the term was used to describe a dissatisfaction with modern architecture, and led to the postmodern architecture movement, perhaps also a response to the modernist architectural movement known as the International Style. Postmodernism in architecture is marked by a re-emergence of surface ornament, reference to surrounding buildings in urban architecture, historical reference in decorative forms (eclecticism), and non-orthogonal angles.
Peter Drucker suggested the transformation into a post modern world happened between 1937 and 1957 (when he was writing). He described an as yet "nameless era" which he characterised as a shift to conceptual world based on pattern purpose and process rather than mechanical cause, outlined by four new realities: the emergence of Educated Society, the importance of international development, the decline of the nation state, and the collapse of the viability of non-Western cultures.
In 1971, in a lecture delivered at the Institute of Contemporary Art, London, Mel Bochner described "post-modernism" in art as having started with Jasper Johns, "who first rejected sense-data and the singular point-of-view as the basis for his art, and treated art as a critical investigation."
More recently, Walter Truett Anderson described postmodernism as belonging to one of four typological world views, which he identifies as either (a) Postmodern-ironist, which sees truth as socially constructed, (b) Scientific-rational, in which truth is found through methodical, disciplined inquiry, (c) Social-traditional, in which truth is found in the heritage of American and Western civilization, or (d) Neo-romantic, in which truth is found through attaining harmony with nature and/or spiritual exploration of the inner self.
Postmodernist ideas in philosophy and the analysis of culture and society expanded the importance of critical theory and has been the point of departure for works of literature, architecture, and design, as well as being visible in marketing/business and the interpretation of history, law and culture, starting in the late 20th century. These developments—re-evaluation of the entire Western value system (love, marriage, popular culture, shift from industrial to service economy) that took place since the 1950s and 1960s, with a peak in the Social Revolution of 1968—are described with the term "Postmodernity", as opposed to "Postmodernism", a term referring to an opinion or movement. Postmodernism has also been used interchangeably with the term post-structuralism out of which postmodernism grew, a proper understanding of postmodernism or doing justice to the postmodernist thought demands an understanding of the poststructuralist movement and the ideas of its advocates. Post-structuralism resulted similarly to postmodernism by following a time of structuralism. It is characterized by new ways of thinking through structuralism, contrary to the original form. "Postmodernist" describes part of a movement; "Postmodern" places it in the period of time since the 1950s, making it a part of contemporary history.
Influence on art.
Architecture.
The idea of Postmodernism in architecture began as a response to the perceived blandness and failed Utopianism of the Modern movement. Modern Architecture, as established and developed by Walter Gropius and Le Corbusier, was focused on the pursuit of a perceived ideal perfection, and attempted harmony of form and function, and dismissal of "frivolous ornament." Critics of modernism argued that the attributes of perfection and minimalism themselves were subjective, and pointed out anachronisms in modern thought and questioned the benefits of its philosophy. Definitive postmodern architecture such as the work of Michael Graves and Robert Venturi rejects the notion of a 'pure' form or 'perfect' architectonic detail, instead conspicuously drawing from all methods, materials, forms and colors available to architects.
Modernist Ludwig Mies van der Rohe is associated with the phrase "less is more"; in contrast Venturi famously said, "Less is a bore." Postmodernist architecture was one of the first aesthetic movements to openly challenge Modernism as antiquated and "totalitarian", favoring personal preferences and variety over objective, ultimate truths or principles.
It is this atmosphere of criticism, skepticism, and emphasis on difference over and against unity that distinguishes the postmodernism aesthetic. Among writers defining the terms of this discourse is Charles Jencks, described by Architectural Design Magazine as "the definer of Post-Modernism for thirty years" and the "internationally acclaimed critic..., whose name became synonymous with Post-modernism in the 80s".
Urban planning.
Postmodernism is a rejection of 'totality', of the notion that planning could be 'comprehensive', widely applied regardless of context, and rational. In this sense, Postmodernism is a rejection of its predecessor: Modernism. From the 1920s onwards, the Modern movement sought to design and plan cities which followed the logic of the new model of industrial mass production; reverting to large-scale solutions, aesthetic standardisation and prefabricated design solutions (Goodchild 1990). Postmodernism also brought a break from the notion that planning and architecture could result in social reform, which was an integral dimension of the plans of Modernism (Simonsen 1990). Furthermore, Modernism eroded urban living by its failure to recognise differences and aim towards homogenous landscapes (Simonsen 1990, 57). Within Modernism, urban planning represented a 20th-century move towards establishing something stable, structured, and rationalised within what had become a world of chaos, flux and change (Irving 1993, 475). The role of planners predating Postmodernism was one of the 'qualified professional' who believed they could find and implement one single 'right way' of planning new urban establishments (Irving 1993). In fact, after 1945, urban planning became one of the methods through which capitalism could be managed and the interests of developers and corporations could be administered (Irving 1993, 479).
Considering Modernism inclined urban planning to treat buildings and developments as isolated, unrelated parts of the overall urban ecosystems created fragmented, isolated, and homogeneous urban landscapes (Goodchild, 1990). One of the greater problems with Modernist-style of planning was the disregard of resident or public opinion, which resulted in planning being forced upon the majority by a minority consisting of affluent professionals with little to no knowledge of real 'urban' problems characteristic of post-Second World War urban environments: slums, overcrowding, deteriorated infrastructure, pollution and disease, among others (Irving 1993). These were precisely the 'urban ills' Modernism was meant to 'solve', but more often than not, the types of 'comprehensive', 'one size fits all' approaches to planning made things worse., and residents began to show interest in becoming involved in decisions which had once been solely entrusted to professionals of the built environment. Advocacy planning and participatory models of planning emerged in the 1960s to counter these traditional elitist and technocratic approaches to urban planning (Irving 1993; Hatuka & D'Hooghe 2007). Furthermore, an assessment of the 'ills' of Modernism among planners during the 1960s, fuelled development of a participatory model that aimed to expand the range of participants in urban interventions (Hatuka & D'Hooghe 2007, 21).
Jane Jacobs's 1961 book "The Death and Life of Great American Cities" was a sustained critique of urban planning as it had developed within Modernism and marked a transition from modernity to postmodernity in thinking about urban planning (Irving 1993, 479). However, the transition from Modernism to Postmodernism is often said to have happened at 3:32pm on 15 July in 1972, when Pruitt Igoe; a housing development for low-income people in St. Louis designed by architect Minoru Yamasaki, which had been a prize-winning version of Le Corbusier's 'machine for modern living' was deemed uninhabitable and was torn down (Irving 1993, 480). Since then, Postmodernism has involved theories that embrace and aim to create diversity, and it exalts uncertainty, flexibility and change (Hatuka & D'Hooghe 2007). Postmodern planning aims to accept pluralism and heighten awareness of social differences in order to accept and bring to light the claims of minority and disadvantaged groups (Goodchild 1990). It is important to note that urban planning discourse within Modernity and Postmodernity has developed in different contexts, even though they both grew within a capitalist culture. Modernity was shaped by a capitalist ethic of Fordist-Keynesian paradigm of mass, standardized production and consumption, while postmodernity was created out of a more flexible form of capital accumulation, labor markets and organisations (Irving 1993, 60). Also, there is a distinction between a postmodernism of 'reaction' and one of 'resistance'. A postmodernism of 'reaction' rejects Modernism and seeks to return to the lost traditions and history in order to create a new cultural synthesis, while Postmodernity of 'resistance' seeks to deconstruct Modernism and is a critique of the origins without necessarily returning to them (Irving 1993, 60). As a result of Postmodernism, planners are much less inclined to lay a firm or steady claim to there being one single 'right way' of engaging in urban planning and are more open to different styles and ideas of 'how to plan' (Irving 474).
Literature.
Literary postmodernism was officially inaugurated in the United States with the first issue of "boundary 2", subtitled "Journal of Postmodern Literature and Culture", which appeared in 1972. David Antin, Charles Olson, John Cage, and the Black Mountain College school of poetry and the arts were integral figures in the intellectual and artistic exposition of postmodernism at the time. "boundary 2" remains an influential journal in postmodernist circles today.
Jorge Luis Borges's (1939) short story "Pierre Menard, Author of the Quixote", is often considered as predicting postmodernism and conceiving the ideal of the ultimate parody. Samuel Beckett is sometimes seen as an important precursor and influence. Novelists who are commonly connected with postmodern literature include Vladimir Nabokov, William Gaddis, Umberto Eco, John Hawkes, William Burroughs, Giannina Braschi, Kurt Vonnegut, John Barth, Jean Rhys, Donald Barthelme, E.L. Doctorow, Richard Kalich, Jerzy Kosinski, Don DeLillo, Thomas Pynchon (Pynchon's work has also been described as "high modern"), Ishmael Reed, Kathy Acker, Ana Lydia Vega, Jachym Topol and Paul Auster.
In 1971, the Arab-American scholar Ihab Hassan published "The Dismemberment of Orpheus: Toward a Postmodern Literature," an early work of literary criticism from a postmodern perspective, in which the author traces the development of what he calls "literature of silence" through Marquis de Sade, Franz Kafka, Ernest Hemingway, Beckett, and many others, including developments such as the Theatre of the Absurd and the nouveau roman. In 'Postmodernist Fiction' (1987), Brian McHale details the shift from modernism to postmodernism, arguing that the former is characterized by an epistemological dominant, and that postmodern works have developed out of modernism and are primarily concerned with questions of ontology. In "Constructing Postmodernism" (1992), McHale's second book, he provides readings of postmodern fiction and of some of the contemporary writers who go under the label of cyberpunk. McHale's "What Was Postmodernism?" (2007), follows Raymond Federman's lead in now using the past tense when discussing postmodernism.
Music.
Postmodern music is either music of the postmodern era, or music that follows aesthetic and philosophical trends of postmodernism. As the name suggests, the postmodernist movement formed partly in reaction to the ideals of the modernist. Because of this, postmodern music is mostly defined in opposition to modernist music, and a work can either be modernist, or postmodern, but not both. Jonathan Kramer posits the idea (following Umberto Eco and Jean-François Lyotard) that postmodernism (including "musical" postmodernism) is less a surface style or historical period (i.e., condition) than an "attitude".
The postmodern impulse in classical music arose in the 1960s with the advent of musical minimalism. Composers such as Terry Riley, Henryk Górecki, Bradley Joseph, John Adams, Steve Reich, Philip Glass, Michael Nyman, and Lou Harrison reacted to the perceived elitism and dissonant sound of atonal academic modernism by producing music with simple textures and relatively consonant harmonies, whilst others, most notably John Cage challenged the prevailing narratives of beauty and objectivity common to Modernism. Some composers have been openly influenced by popular music and world ethnic musical traditions.
Postmodern classical music as well is not a musical "style", but rather refers to music of the postmodern era. It bears the same relationship to postmodernist music that postmodernity bears to postmodernism. Postmodern music, on the other hand, shares characteristics with postmodernist art—that is, art that comes "after" and reacts "against" modernism.
Though representing a general return to certain notions of music-making that are often considered to be classical or romantic, not all postmodern composers have eschewed the experimentalist or academic tenets of modernism. The works of Dutch composer Louis Andriessen, for example, exhibit experimentalist preoccupation that is decidedly anti-romantic. Eclecticism and freedom of expression, in reaction to the rigidity and aesthetic limitations of modernism, are the hallmarks of the postmodern influence in musical composition.
Graphic design.
Postmodern designers were in the beginning stages of what we now refer to as "graphic design". They created works beginning in the 1970s without any set adherence to rational order and formal organization. They also seemed to entirely pay no attention to traditional conventions such as legibility. Another characteristic of postmodern graphic design is that "retro, techno, punk, grunge, beach, parody, and pastiche were all conspicuous trends. Each had its own sites and venues, detractors and advocates". Yet, while postmodern design did not consist of one unified graphic style, the movement was an expressive and playful time for designers who searched for more and more ways to go against the system. Key influential postmodern graphic designers include Wolfgang Weingart, April Greiman, Tibor Kalman, and Jamie Reid.
Deconstruction.
One of the most well-known postmodernist concerns is "deconstruction," a theory for philosophy, literary criticism, and textual analysis developed by Jacques Derrida. The notion of a "deconstructive" approach implies an analysis that questions the already evident understanding of a text in terms of presuppositions, ideological underpinnings, hierarchical values, and frames of reference. A deconstructive approach further depends on the techniques of close reading without reference to cultural, ideological, moral opinions or information derived from an authority over the text such as the author. At the same time Derrida famously writes: "Il n'y a pas d'hors-texte ("there is no such thing as outside-of-the-text")." Derrida implies that the world follows the grammar of a text undergoing its own deconstruction. Derrida's method frequently involves recognizing and spelling out the different, yet similar interpretations of the meaning of a given text and the problematic implications of binary oppositions within the meaning of a text. Derrida's philosophy inspired a postmodern movement called deconstructivism among architects, characterized by the intentional fragmentation, distortion, and dislocation of architectural elements in designing a building. Derrida discontinued his involvement with the movement after the publication of his collaborative project with architect Peter Eisenmann in "Chora L Works: Jacques Derrida and Peter Eisenman".
Postmodernism and structuralism.
Structuralism was a philosophical movement developed by French academics in the 1950s, partly in response to French Existentialism. It has been seen variously as an expression of Modernism, High modernism, or postmodernism . "Post-structuralists" were thinkers who moved away from the strict interpretations and applications of structuralist ideas. Many American academics consider post-structuralism to be part of the broader, less well-defined postmodernist movement, even though many post-structuralists insisted it was not. Thinkers who have been called structuralists include the anthropologist Claude Lévi-Strauss, the linguist Ferdinand de Saussure, the Marxist philosopher Louis Althusser, and the semiotician Algirdas Greimas. The early writings of the psychoanalyst Jacques Lacan and the literary theorist Roland Barthes have also been called structuralist. Those who began as structuralists but became post-structuralists include Michel Foucault, Roland Barthes, Jean Baudrillard, Gilles Deleuze. Other post-structuralists include Jacques Derrida, Pierre Bourdieu, Jean-François Lyotard, Julia Kristeva, Hélène Cixous, and Luce Irigaray. The American cultural theorists, critics and intellectuals whom they influenced include Judith Butler, John Fiske, Rosalind Krauss, Avital Ronell, and Hayden White.
Post-structuralism is not defined by a set of shared axioms or methodologies, but by an emphasis on how various aspects of a particular culture, from its most ordinary, everyday material details to its most abstract theories and beliefs, determine one another. Post-structuralist thinkers reject Reductionism and Epiphenomenalism and the idea that cause-and-effect relationships are top-down or bottom-up. Like structuralists, they start from the assumption that people's identities, values and economic conditions determine each other rather than having "intrinsic" properties that can be understood in isolation.Lévi-Strauss, quoting D'Arcy Westworth Thompson states: "To those who question the possibility of defining the interrelations between entities whose nature is not completely understood, I shall reply with the following comment by a great naturalist: In a very large part of morphology, our essential task lies in the comparison of related forms rather than in the precise definition of each; and the deformation of a complicated figure may be a phenomenon easy of comprehension, though the figure itself has to be left unanalyzed and undefined."</ref> Thus the French structuralists considered themselves to be espousing Relativism and Constructionism. But they nevertheless tended to explore how the subjects of their study might be described, reductively, as a set of "essential" relationships, schematics, or mathematical symbols. (An example is Claude Lévi-Strauss's algebraic formulation of mythological transformation in "The Structural Study of Myth"Lévi-Strauss, Claude. "Structural Anthropology". Trans. Claire Jacobson and Brooke Grundfest Schoepf (New York: Basic Books, 1963), 228.</ref>). Post-structuralists thinkers went further, questioning the existence of any distinction between the nature of a thing and its relationship to other things.
Post-postmodernism.
Recently metamodernism, post-postmodernism and the "death of postmodernism" have been widely debated: in 2007 Andrew Hoberek noted in his introduction to a special issue of the journal "Twentieth Century Literature" titled "After Postmodernism" that "declarations of postmodernism's demise have become a critical commonplace". A small group of critics has put forth a range of theories that aim to describe culture or society in the alleged aftermath of postmodernism, most notably Raoul Eshelman (performatism), Gilles Lipovetsky (hypermodernity), Nicolas Bourriaud (altermodern), and Alan Kirby (digimodernism, formerly called pseudo-modernism). None of these new theories and labels have so far gained very widespread acceptance. The exhibition "Postmodernism - Style and Subversion 1970–1990" at the Victoria and Albert Museum (London, 24 September 2011 – 15 January 2012) was billed as the first show to document postmodernism as a historical movement.
Criticisms.
Criticisms of postmodernism are intellectually diverse, including the assertions that postmodernism is meaningless and promotes obscurantism. For example, Noam Chomsky has argued that postmodernism is meaningless because it adds nothing to analytical or empirical knowledge. He asks why postmodernist intellectuals do not respond like people in other fields when asked, "what are the principles of their theories, on what evidence are they based, what do they explain that wasn't already obvious, etc.?...If requests can't be met, then I'd suggest recourse to Hume's advice in similar circumstances: 'to the flames'."
Christian apologist William Lane Craig has noted "The idea that we live in a postmodern culture is a myth. In fact, a postmodern culture is an impossibility; it would be utterly unliveable. People are not relativistic when it comes to matters of science, engineering, and technology; rather, they are relativistic and pluralistic in matters of religion and ethics. But, of course, that's not postmodernism; that's modernism!"
Formal, academic critiques of postmodernism can also be found in works such as "Beyond the Hoax" and "Fashionable Nonsense".
However, as for continental philosophy, American academics have tended to label it "postmodernist", especially practitioners of "French Theory". Such a trend might derive from U.S. departments of Comparative Literature. It is interesting to note that Félix Guattari, often considered a "postmodernist", rejected its theoretical assumptions by arguing that the structuralist and postmodernist visions of the world were not flexible enough to seek explanations in psychological, social and environmental domains at the same time.
Philosopher Daniel Dennett declared, "Postmodernism, the school of 'thought' that proclaimed 'There are no truths, only interpretations' has largely played itself out in absurdity, but it has left behind a generation of academics in the humanities disabled by their distrust of the very idea of truth and their disrespect for evidence, settling for 'conversations' in which nobody is wrong and nothing can be confirmed, only asserted with whatever style you can muster."

</doc>
<doc id="23604" url="https://en.wikipedia.org/wiki?curid=23604" title="Photography">
Photography

Photography is the science, art and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.
Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically "developed" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.
Photography is employed in many fields of science, manufacturing (e.g., photolithography) and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.
Etymology.
The word "photography" was created from the Greek roots φωτός ("phōtos"), genitive of φῶς ("phōs"), "light" and γραφή ("graphé") "representation by means of lines" or "drawing", together meaning "drawing with light".
Several people may have coined the same new term from these roots independently. Hercules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, "photographie", in private notes which a Brazilian photography historian believes were written in 1834. Johann von Maedler, a Berlin astronomer, is credited in a 1932 German history of photography as having used it in an article published on 25 February 1839 in the German newspaper "Vossische Zeitung". Both of these claims are now widely reported but apparently neither has ever been independently confirmed as beyond reasonable doubt. Credit has traditionally been given to Sir John Herschel both for coining the word and for introducing it to the public. His uses of it in private correspondence prior to 25 February 1839 and at his Royal Society lecture on the subject in London on 14 March 1839 have long been amply documented and accepted as settled facts.
History.
Precursor technologies.
Photography is the result of combining several technical discoveries. Long before the first photographs were made, Chinese philosopher Mo Di and Greek mathematicians Aristotle and Euclid described a pinhole camera in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments, Ibn al-Haytham (Alhazen) (965–1040) studied the camera obscura and pinhole camera, Albertus Magnus (1193–1280) discovered silver nitrate, and Georg Fabricius (1516–71) discovered silver chloride. Techniques described in the Book of Optics are capable of producing primitive photographs using medieval materials.
Daniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. The fiction book "Giphantie", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.
The discovery of the camera obscura that provides an image of a scene dates back to ancient China. Leonardo da Vinci mentions natural camerae obscurae that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. So the birth of photography was primarily concerned with inventing means to fix and retain the image produced by the camera obscura.
Renaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western Art. The camera obscura literally means "dark chamber" in Latin. It is a box with a hole in it which allows light to go through and create an image onto the piece of paper.
Invention of photography.
Around the year 1800, Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance. He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow-copies of paintings on glass, it was reported in 1802 that "the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver." The shadow images eventually darkened all over.
The first permanent photoetching was an image produced in 1822 by the French inventor Nicéphore Niépce, but it was destroyed in a later attempt to make prints from it. Niépce was successful again in 1825. In 1826 or 1827, he made the "View from the Window at Le Gras", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).
Because Niépce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy.
Niépce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Niépce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements- a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and "fixed" with sodium hyposulfite- were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the approximately ten-minute-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839.
Meanwhile, in Brazil, Hercules Florence had apparently started working out a silver-salt-based paper process in 1832, later naming it "Photographie", and an English inventor, William Fox Talbot, succeeded in making crude but reasonably light-fast silver images on paper as early as 1834 but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his method and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, with exposures to a freshly-sensitized damp paper comparable to the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies, the basis of most chemical photography up to the present day. Daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.
John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the "blueprint". He was the first to use the terms "photography", "negative" and "positive". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to "fix" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.
In the March 1851 issue of "The Chemist", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.
Many advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize for Physics in 1908.
Glass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography it has persisted into the 2010s.
Film photography.
Hurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.
The first flexible photographic roll film was marketed by George Eastman in 1885, but this original "film" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose ("celluloid"), now usually called "nitrate film".
Although cellulose acetate or "safety film" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16 mm and 8 mm home movies, nitrate film remained standard for theatrical 35 mm motion pictures until it was finally discontinued in 1951.
Films remained the dominant form of photography until the early 21st century, when advances in digital photography drew consumers to digital formats. Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctively "look" of film based photographs compared to digital images is likely due to a combination of factors, including: (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors (2) resolution and (3) continuity of tone.
Black-and-white.
Originally, all photography was monochrome, or "black-and-white". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost and its "classic" photographic look. The tones and contrast between light and dark areas define black-and-white photography. It is important to note that monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray, but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process, first used more than years ago, produces brownish tones.
Many photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.
Color.
Color photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not "fix" the photograph to prevent the color from quickly fading when exposed to white light.
The first permanent color photograph was taken in 1861 using the three-color-separation principle first published by physicist James Clerk Maxwell in 1855. Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.
Russian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color "fringes" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.
Implementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.
Autochrome, the first commercially successful color process, was introduced by the Lumière brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.
Kodachrome, the first modern "integral tripack" (or "monopack") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.
Agfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product.
Instant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.
Color photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995–2005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive "look".
Digital photography.
In 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. In 1991, Kodak unveiled the DCS 100, the first commercially available digital single lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.
Digital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.
Digital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.
Synthesis photography.
Synthesis photography is part of computer-generated imagery (CGI) where the shooting process is modeled on real photography. The CGI, creating digital copies of real universe, requires a visual representation process of these universes. Synthesis photography is the application of analog and digital photography in digital space. With the characteristics of the real photography but not being constrained by the physical limits of real world, synthesis photography allows to get away from real photography.
Technical aspects.
The camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.
Photographers control the camera and lens to "expose" the light-recording material to the required amount of light to form a "latent image" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on a paper.
The camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).
As soon as photographic materials became "fast" (sensitive) enough for taking candid or surreptitious pictures, small "detective" cameras were made, some actually disguised as a book or handbag or pocket watch (the "Ticka" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.
The movie camera is a type of photographic camera which takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a "frame". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the "frame rate" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures together to create the illusion of motion.
Camera controls.
In all but certain specialized cameras, the process of obtaining a usable exposure must involve the use, manually or automatically, of a few controls to ensure the photograph is clear, sharp and well illuminated. The controls usually include but are not limited to the following:
Many other elements of the imaging device itself may have a pronounced effect on the quality and/or aesthetic effect of a given photograph; among them are:
Exposure and rendering.
Camera controls are interrelated. The total amount of light reaching the film plane (the 'exposure') changes with the duration of exposure, aperture of the lens, and on the effective focal length of the lens (which in variable focal length lenses, can force a change in aperture as the lens is zoomed). Changing any of these controls can alter the exposure. Many cameras may be set to adjust most or all of these controls automatically. This automatic functionality is useful for occasional photographers in many situations.
The duration of an exposure is referred to as shutter speed, often even in cameras that do not have a physical shutter, and is typically measured in fractions of a second. It is quite possible to have exposures from one up to several seconds, usually for still-life subjects, and for night scenes exposure times can be several hours. However, for a subject that is in motion use a fast shutter speed. This will prevent the photograph from coming out blurry.
The effective aperture is expressed by an f-number or f-stop (derived from "focal ratio"), which is proportional to the ratio of the focal length to the diameter of the aperture. Longer focal length lenses will pass less light through the same aperture diameter due to the greater distance the light has to travel; shorter focal length lenses will transmit more light through the same diameter of aperture.
The smaller the f/number, the larger the effective aperture. The present system of f/numbers to give the effective aperture of a lens was standardized by an international convention in 1963, and is referred to as the British Standard (BS-1013). Other aperture measurement scales had been used through the early 20th century, including the European Scale, Intermediate settings, and the 1881 Uniform System proposed by the Royal Photographic Society, which are all now largely obsolete. "T-stops" have been used for color motion picture lenses, to account for differences in "light transmission" through compound lenses, are calculated as T-number = f/number x . 
If the f-number is decreased by a factor of , the aperture diameter is increased by the same factor, and its area is increased by a factor of 2. The f-stops that might be found on a typical lens include 2.8, 4, 5.6, 8, 11, 16, 22, 32, where going up "one stop" (using lower f-stop numbers) doubles the amount of light reaching the film, and stopping down one stop halves the amount of light.
Image capture can be achieved through various combinations of shutter speed, aperture, and film or sensor speed. Different (but related) settings of aperture and shutter speed enable photographs to be taken under various conditions of film or sensor speed, lighting and motion of subjects and/or camera, and desired depth of field. A slower speed film will exhibit less "grain", and a slower speed setting on an electronic sensor will exhibit less "noise", while higher film and sensor speeds allow for a faster shutter speed, which reduces motion blur or allows the use of a smaller aperture to increase the depth of field.
For example, a wider aperture is used for lower light and a lower aperture for more light. If a subject is in motion, then a high shutter speed may be needed. A tripod can also be helpful in that it enables a slower shutter speed to be used.
For example, f/8 at 8 ms (1/125 of a second) and f/5.6 at 4 ms (1/250 of a second) yield the same amount of light. The chosen combination affects the final result. The aperture and focal length of the lens determine the depth of field, which refers to the range of distances from the lens that will be in focus. A longer lens or a wider aperture will result in "shallow" depth of field (i.e., only a small plane of the image will be in sharp focus). This is often useful for isolating subjects from backgrounds as in individual portraits or macro photography.
Conversely, a shorter lens, or a smaller aperture, will result in more of the image being in focus. This is generally more desirable when photographing landscapes or groups of people. With very small apertures, such as pinholes, a wide range of distance can be brought into focus, but sharpness is severely degraded by diffraction with such small apertures. Generally, the highest degree of "sharpness" is achieved at an aperture near the middle of a lens's range (for example, f/8 for a lens with available apertures of f/2.8 to f/16). However, as lens technology improves, lenses are becoming capable of making increasingly sharp images at wider apertures.
Image capture is only part of the image forming process. Regardless of material, some process must be employed to render the latent image captured by the camera into a viewable image. With slide film, the developed film is just mounted for projection. Print film requires the developed film negative to be printed onto photographic paper or transparency. Prior to the advent of laser jet and inkjet printers, celluloid photographic negative images had to be mounted in an enlarger which projected the image onto a sheet of light-sensitive paper for a certain length of time (usually measured in seconds or fractions of a second). This sheet then was soaked in a chemical bath of developer (to bring out the image) followed immediately by a stop bath (to neutralize the progression of development and prevent the image from changing further once exposed to normal light). After this, the paper was hung until dry enough to safely handle. This post-production process allowed the photographer to further manipulate the final image beyond what had already been captured on the negative, adjusting the length of time the image was projected by the enlarger and the duration of both chemical baths to change the image's intensity, darkness, clarity, etc. This process is still employed by both amateur and professional photographers, but the advent of digital imagery means that the vast majority of modern photographic work is captured digitally and rendered via printing processes that are no longer dependent on chemical reactions to light. Such digital images may be uploaded to an image server (e.g., a photo-sharing web site), viewed on a television, or transferred to a computer or digital photo frame. Every type can then be produced as a hard copy on regular paper or photographic paper via a printer.
Prior to the rendering of a viewable image, modifications can be made using several controls. Many of these controls are similar to controls during image capture, while some are exclusive to the rendering process. Most printing controls have equivalent digital concepts, but some create different effects. For example, dodging and burning controls are different between digital and film processes. Other printing modifications include:
Other photographic techniques.
Stereoscopic.
Photographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as "3-D" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film, and more recently in digital electronic methods (including cellphone cameras).
Full-spectrum, ultraviolet and infrared.
Ultraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.
Modified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350 nm to 1000 nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400 nm to 700 nm.
Replacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).
Uses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.
Light field photography.
Digital methods of image capture and display processing have enabled the new technology of "light field photography" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected "after" the photograph has been captured. As explained by Michael Faraday in 1846, the "light field" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point.
These additional vector attributes can be captured optically through the use of microlenses at each pixel-point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.
Other imaging techniques.
Besides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.
Modes of production.
Amateur.
An amateur photographer is one who practices photography as a hobby / passion and not for profit. The quality of some amateur work is comparable to that of many professionals and may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera.
Commercial.
Commercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. Wholesale, retail, and professional uses of photography would fall under this definition. The commercial photographic world could include:
The market for photographic services demonstrates the aphorism "A picture is worth a thousand words", which has an interesting basis in the history of photography. Magazines and newspapers, companies putting up Web sites, advertising agencies and other groups pay for photography.
Many people take photographs for commercial purposes. Organizations with a budget and a need for photography have several options: they can employ a photographer directly, organize a public competition, or obtain rights to stock photographs. Photo stock can be procured through traditional stock giants, such as Getty Images or Corbis; smaller microstock agencies, such as Fotolia; or web marketplaces, such as Cutcaster.
Art.
During the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.
At first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.
The aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images "written with light"; Nicéphore Niépce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.
Clive Bell in his classic essay "Art" states that only "significant form" can distinguish art from what is not art.
On 14 February 2004, Sotheby's London sold the 2001 photograph "99 Cent II Diptychon" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.
Conceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.
Science and forensics.
The camera has a long and distinguished history as a means of recording phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are taken from three vantage point. The vantage points are overview, mid-range, and close up.
By 1853, Charles Brooke had invented a technology for the automatic registration of instruments by photography. These instruments included barometers, thermometers, psychrometers, and magnetometers, which recorded their readings by means of an automated photographic process.
Science uses image technology that has derived from the design of the Pin Hole camera. X-Ray machine are similar in design to Pin Hole cameras with high grade filters and laser radiation.
Photography has become ubiquitous in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.
Social and cultural implications.
There are many ongoing questions about different aspects of photography. In her writing "On Photography" (1977), Susan Sontag discusses concerns about the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, "To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power." Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines it can be argued that photography is a subjective form of representation.
Modern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's "Rear Window" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.
The camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate – all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.
Digital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures, or are forbidden from combining elements of multiple photos to make "photomontages", passing them as "real" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allows digital fingerprinting of photos to detect tampering for purposes of forensic photography.
Photography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that "to photograph is to turn people into objects that can be symbolically possessed." Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.
One of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a "tourist gaze"
in which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a "reverse gaze" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.
Additionally, photography has been the topic of many songs in popular culture.
Law.
Photography is both restricted as well as protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a first amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view. In the UK a recent law (Counter-Terrorism Act 2008) increases the power of the police to prevent people, even press photographers, from taking pictures in public places.

</doc>
<doc id="23607" url="https://en.wikipedia.org/wiki?curid=23607" title="Pentateuch (disambiguation)">
Pentateuch (disambiguation)

The Pentateuch is the first part of the Bible, consisting of Genesis, Exodus, Leviticus, Numbers, and Deuteronomy.
It can also refer to:

</doc>
<doc id="23612" url="https://en.wikipedia.org/wiki?curid=23612" title="Postmodern philosophy">
Postmodern philosophy

Postmodern philosophy is a philosophical direction which is critical of certain foundational assumptions of Western philosophy and especially of the 18th-century Enlightenment. It emphasizes the importance of power relationships, personalization and discourse in the "construction" of truth and world views. Postmodernists deny that an objective reality exists, and deny that there are objective moral values.
Postmodern philosophy is often particularly skeptical about simple binary oppositions characteristic of structuralism, emphasizing the problem of the philosopher cleanly distinguishing knowledge from ignorance, social progress from reversion, dominance from submission, good from bad, and presence from absence.
Postmodern philosophy has strong relations with the substantial literature of critical theory.
Characteristic claims.
According to Brian Duignan of the Encyclopedia Britannica, many postmodern claims are a deliberate repudiation of certain 18th-century Enlightenment values. A postmodernist might believe that there is no objective natural reality, and that logic and reason are mere conceptual constructs that are not universally valid. Two other characteristic anti-Enlightenment postmodern practices are a denial that human nature exists, and a (sometimes moderate) skepticism toward claims that science and technology will change society for the better. Postmodernists also believe there are no objective moral values. Postmodern writings often focus on deconstructing the role that power and ideology play in shaping discourse and belief. Postmodern philosophy shares ontological similarities with classical skeptical and relativistic belief systems, and shares political similarities with modern identity politics.
Definitional issues.
Philosopher John Deely has argued for the contentious claim that the label "postmodern" for thinkers such as Derrida "et al." is "premature". Insofar as the "so-called" postmoderns follow the thoroughly "modern" trend of idealism, it is more an "ultra"modernism than anything else. A postmodernism that lives up to its name, therefore, must no longer confine itself to the premodern preoccupation with "things" nor with the modern confinement to "ideas," but must come to terms with the way of signs embodied in the semiotic doctrines of such thinkers as the Portuguese philosopher John Poinsot and the American philosopher Charles Sanders Peirce. Writes Deely,
The epoch of Greek and Latin philosophy was based on "being" in a quite precise sense: the existence exercised by things independently of human apprehension and attitude. The much briefer epoch of modern philosophy based itself rather on the instruments of human knowing, but in a way that unnecessarily compromised being. As the 20th century ends, there is reason to believe that a new philosophical epoch is dawning along with the new century, promising to be the richest epoch yet for human understanding. The postmodern era is positioned to synthesize at a higher level—the level of experience, where the being of things and the activity of the finite knower compenetrate one another and provide the materials whence can be derived knowledge of nature and knowledge of culture in their full symbiosis—the achievements of the ancients and the moderns in a way that gives full credit to the preoccupations of both. The postmodern era has for its distinctive task in philosophy the exploration of a new path, no longer the ancient way of things nor the modern way of ideas, but the way of signs, whereby the peaks and valleys of ancient and modern thought alike can be surveyed and cultivated by a generation which has yet further peaks to climb and valleys to find.
History.
Precursors.
While the idea of postmodernity had been around since the 1940s, postmodern philosophy originated primarily in France during the mid-20th century. However, several philosophical antecedents inform many of postmodern philosophy's concerns.
It was greatly influenced by the writings of Søren Kierkegaard and Friedrich Nietzsche in the 19th century and other early-to-mid 20th-century philosophers, including phenomenologists Edmund Husserl and Martin Heidegger, psychoanalyst Jacques Lacan, structuralist Roland Barthes, Georges Bataille, and the later work of Ludwig Wittgenstein. Postmodern philosophy also drew from the world of the arts and architecture, particularly Marcel Duchamp, John Cage and artists who practiced collage, and the architecture of Las Vegas and the Pompidou Centre.
Early postmodern philosophers.
The most influential early postmodern philosophers were Jean Baudrillard, Jean-François Lyotard, and Jacques Derrida. Michel Foucault is also often cited as an early postmodernist although he personally rejected that label. Following Nietzsche, Foucault argued that knowledge is produced through the operations of "power", and changes fundamentally in different historical periods.
The writings of Lyotard were largely concerned with the role of narrative in human culture, and particularly how that role has changed as we have left modernity and entered a "postindustrial" or postmodern condition. He argued that modern philosophies legitimized their truth-claims not (as they themselves claimed) on logical or empirical grounds, but rather on the grounds of accepted stories (or "metanarratives") about knowledge and the world—comparing these with Wittgenstein's concept of language-games. He further argued that in our postmodern condition, these metanarratives no longer work to legitimize truth-claims. He suggested that in the wake of the collapse of modern metanarratives, people are developing a new "language-game"—one that does not make claims to absolute truth but rather celebrates a world of ever-changing relationships (among people and between people and the world).
Derrida, the father of deconstruction, practiced philosophy as a form of textual criticism. He criticized Western philosophy as privileging the concept of presence and "logos", as opposed to absence and markings or writings.
In America, the most famous pragmatist and self-proclaimed postmodernist was Richard Rorty. An analytic philosopher, Rorty believed that combining Willard Van Orman Quine's criticism of the analytic-synthetic distinction with Wilfrid Sellars's critique of the "Myth of the Given" allowed for an abandonment of the view of the thought or language as a mirror of a reality or external world. Further, drawing upon Donald Davidson's criticism of the dualism between conceptual scheme and empirical content, he challenges the sense of questioning whether our particular concepts are related to the world in an appropriate way, whether we can justify our ways of describing the world as compared with other ways. He argued that truth was not about getting it right or representing reality, but was part of a social practice and language was what served our purposes in a particular time; ancient languages are sometimes untranslatable into modern ones because they possess a different vocabulary and are unuseful today. Donald Davidson is not usually considered a postmodernist, although he and Rorty have both acknowledged that there are few differences between their philosophies.

</doc>
<doc id="23613" url="https://en.wikipedia.org/wiki?curid=23613" title="Postmodern music">
Postmodern music

Postmodern music is either simply music of the postmodern era, or music that follows aesthetical and philosophical trends of postmodernism. As the name suggests, the postmodernist movement formed partly in reaction to modernism. Even so, postmodern music still does not primarily define itself in opposition to modernist music; this label is applied instead by critics and theorists.
Postmodern music is not a distinct musical style, but rather refers to music of the postmodern era. The terms "postmodern", "postmodernism", "postmodernist", and "postmodernity" are exasperating terms . Indeed, postmodernists question the tight definitions and categories of academic disciplines, which they regard simply as the remnants of modernity .
The postmodernist musical attitude.
Postmodernism in music is not a distinct musical style, but rather refers to music of the postmodern era. Postmodernist music, on the other hand, shares characteristics with postmodernist art—that is, art that comes after and reacts against modernism (see Modernism in Music).
Fredric Jameson, a major figure in the thinking on postmodernism and culture, calls postmodernism "the cultural dominant of the logic of late capitalism" , meaning that, through globalization, postmodern culture is tied inextricably with capitalism (Mark Fisher, writing 20 years later, goes further, essentially calling it the sole cultural possibility ). Drawing from Jameson and other theorists, David Beard and Kenneth Gloag argue that, in music, postmodernism is not just an attitude but also an inevitability in the current cultural climate of fragmentation . As early as 1938, Theodor Adorno had already identified a trend toward the dissolution of "a culturally dominant set of values" , citing the commodification of all genres as beginning of the end of genre or value distinctions in music .
In some respects, Postmodern music could be categorized as simply the music of the postmodern era, or music that follows aesthetic and philosophical trends of postmodernism, but with Jameson in mind, it is clear these definitions are inadequate. As the name suggests, the postmodernist movement formed partly in reaction to the ideals of modernism, but in fact postmodern music is more to do with functionality and the effect of globalization than it is with a specific reaction, movement, or attitude . In the face of capitalism, Jameson says, "It is safest to grasp the concept of the postmodern as an attempt to think the present historically in an age that has forgotten how to think historically in the first place" .
Jonathan Kramer posits the idea (following Umberto Eco and Jean-François Lyotard) that postmodernism (including "musical" postmodernism) is less a surface style or historical period (i.e., condition) than an "attitude". Kramer enumerates 16 (arguably subjective) "characteristics of postmodern music, by which I mean music that is understood in a postmodern manner, or that calls forth postmodern listening strategies, or that provides postmodern listening experiences, or that exhibits postmodern compositional practices." According to , postmodern music:
Daniel Albright summarizes the main tendencies of musical postmodernism as :
Timescale.
One author has suggested that the emergence of postmodern music in popular music occurred in the late 1960s, influenced in part by psychedelic rock and one or more of the later Beatles albums . Beard and Gloag support this position, citing Jameson's theory that "the radical changes of musical styles and languages throughout the 1960s now seen as a reflection of postmodernism" (; see also ). Others have placed the beginnings of postmodernism in the arts, with particular reference to music, at around 1930 (; ).

</doc>
<doc id="23615" url="https://en.wikipedia.org/wiki?curid=23615" title="Protocol">
Protocol

Protocol may refer to:

</doc>
<doc id="23617" url="https://en.wikipedia.org/wiki?curid=23617" title="Pump">
Pump

A pump is a device that moves fluids (liquids or gases), or sometimes slurries, by mechanical action. Pumps can be classified into three major groups according to the method they use to move the fluid: "direct lift", "displacement", and "gravity" pumps.
Pumps operate by some mechanism (typically reciprocating or rotary), and consume energy to perform mechanical work by moving the fluid. Pumps operate via many energy sources, including manual operation, electricity, engines, or wind power, come in many sizes, from microscopic for use in medical applications to large industrial pumps.
Mechanical pumps serve in a wide range of applications such as pumping water from wells, aquarium filtering, pond filtering and aeration, in the car industry for water-cooling and fuel injection, in the energy industry for pumping oil and natural gas or for operating cooling towers. In the medical industry, pumps are used for biochemical processes in developing and manufacturing medicine, and as artificial replacements for body parts, in particular the artificial heart and penile prosthesis.
Single stage pump - When in a casing only one impeller is revolving then it is called single stage pump.
Double/ Multi stage pump - When in a casing two or more than two impellers are revolving then it is called double/ multi stage pump.
In biology, many different types of chemical and bio-mechanical pumps have evolved, and biomimicry is sometimes used in developing new types of mechanical pumps.
Types.
Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.
Pumps can be classified by their method of displacement into positive displacement pumps, impulse pumps, velocity pumps, gravity pumps, steam pumps and valveless pumps. There are two basic types of pumps: positive displacement and centrifugal. Although axial-flow pumps are frequently classified as a separate type, they have essentially the same operating principles as centrifugal pumps.
Positive displacement pumps.
A positive displacement pump makes a fluid move by trapping a fixed amount and forcing (displacing) that trapped volume into the discharge pipe.
Some positive displacement pumps use an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pump as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant through each cycle of operation.
Positive displacement pump behavior and safety.
Positive displacement pumps, unlike centrifugal or roto-dynamic pumps, theoretically can produce the same flow at a given speed (RPM) no matter what the discharge pressure. Thus, positive displacement pumps are "constant flow machines". However, a slight increase in internal leakage as the pressure increases prevents a truly constant flow rate.
A positive displacement pump must not operate against a closed valve on the discharge side of the pump, because it has no shutoff head like centrifugal pumps. A positive displacement pump operating against a closed discharge valve continues to produce flow and the pressure in the discharge line increases until the line bursts, the pump is severely damaged, or both.
A relief or safety valve on the discharge side of the positive displacement pump is therefore necessary. The relief valve can be internal or external. The pump manufacturer normally has the option to supply internal relief or safety valves. The internal valve is usually only used as a safety precaution. An external relief valve in the discharge line, with a return line back to the suction line or supply tank provides increased safety.
Positive displacement types.
A positive displacement pump can be further classified according to the mechanism used to move the fluid:
Rotary positive displacement pumps.
These pumps move fluid using a rotating mechanism that creates a vacuum that captures and draws in the liquid.
"Advantages:" Rotary pumps are very efficient because they naturally remove air from the lines, eliminating the need to bleed the air from the lines manually.
"Drawbacks:" The nature of the pump requires very close clearances between the rotating pump and the outer edge, making it rotate at a slow, steady speed. If rotary pumps are operated at high speeds, the fluids cause erosion, which eventually causes enlarged clearances that liquid can pass through, which reduces efficiency.
Rotary positive displacement pumps fall into three main types: 
Reciprocating positive displacement pumps.
Reciprocating pumps move the fluid using one or more oscillating pistons, plungers, or membranes (diaphragms), while valves restrict fluid motion to the desired direction.
Pumps in this category range from "simplex", with one cylinder, to in some cases "quad" (four) cylinders, or more. Many reciprocating-type pumps are "duplex" (two) or "triplex" (three) cylinder. They can be either "single-acting" with suction during one direction of piston motion and discharge on the other, or "double-acting" with suction and discharge in both directions. The pumps can be powered manually, by air or steam, or by a belt driven by an engine. This type of pump was used extensively in the 19th century—in the early days of steam propulsion—as boiler feed water pumps. Now reciprocating pumps typically pump highly viscous fluids like concrete and heavy oils, and serve in special applications that demand low flow rates against high resistance. Reciprocating hand pumps were widely used to pump water from wells. Common bicycle pumps and foot pumps for inflation use reciprocating action.
These positive displacement pumps have an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pumps as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant given each cycle of operation.
Typical reciprocating pumps are:
Various positive displacement pumps.
The positive displacement principle applies in these pumps:
Gear pump.
This is the simplest of rotary positive displacement pumps. It consists of two meshed gears that rotate in a closely fitted casing. The tooth spaces trap fluid and force it around the outer periphery. The fluid does not travel back on the meshed part, because the teeth mesh closely in the center. Gear pumps see wide use in car engine oil pumps and in various hydraulic power packs.
Screw pump.
A screw pump is a more complicated type of rotary pump that uses two or three screws with opposing thread — e.g., one screw turns clockwise and the other counterclockwise. The screws are mounted on parallel shafts that have gears that mesh so the shafts turn together and everything stays in place. The screws turn on the shafts and drive fluid through the pump. As with other forms of rotary pumps, the clearance between moving parts and the pump's casing is minimal.
Progressing cavity pump.
Widely used for pumping difficult materials, such as sewage sludge contaminated with large particles, this pump consists of a helical rotor, about ten times as long as its width. This can be visualized as a central core of diameter "x" with, typically, a curved spiral wound around of thickness half "x", though in reality it is manufactured in single casting. This shaft fits inside a heavy duty rubber sleeve, of wall thickness also typically "x". As the shaft rotates, the rotor gradually forces fluid up the rubber sleeve. Such pumps can develop very high pressure at low volumes.
Roots-type pumps.
Named after the Roots brothers who invented it, this lobe pump displaces the liquid trapped between two long helical rotors, each fitted into the other when perpendicular at 90°, rotating inside a triangular shaped sealing line configuration, both at the point of suction and at the point of discharge. This design produces a continuous flow with equal volume and no vortex. It can work at low pulsation rates, and offers gentle performance that some applications require.
Applications include:
Peristaltic pump.
A "peristaltic pump" is a type of positive displacement pump. It contains fluid within a flexible tube fitted inside a circular pump casing (though linear peristaltic pumps have been made). A number of "rollers", "shoes", or "wipers" attached to a rotor compresses the flexible tube. As the rotor turns, the part of the tube under compression closes (or "occludes"), forcing the fluid through the tube. Additionally, when the tube opens to its natural state after the passing of the cam it draws ("restitution") fluid into the pump. This process is called peristalsis and is used in many biological systems such as the gastrointestinal tract.
Plunger pumps.
"Plunger pumps" are reciprocating positive displacement pumps.
These consist of a cylinder with a reciprocating plunger. The suction and discharge valves are mounted in the head of the cylinder. In the suction stroke the plunger retracts and the suction valves open causing suction of fluid into the cylinder. In the forward stroke the plunger pushes the liquid out of the discharge valve.
Efficiency and common problems: With only one cylinder in plunger pumps, the fluid flow varies between maximum flow when the plunger moves through the middle positions, and zero flow when the plunger is at the end positions. A lot of energy is wasted when the fluid is accelerated in the piping system. Vibration and "water hammer" may be a serious problem. In general the problems are compensated for by using two or more cylinders not working in phase with each other.
Triplex-style plunger pumps.
Triplex plunger pumps use three plungers, which reduces the pulsation of single reciprocating plunger pumps. Adding a pulsation dampener on the pump outlet can further smooth the "pump ripple", or ripple graph of a pump transducer. The dynamic relationship of the high-pressure fluid and plunger generally requires high-quality plunger seals. Plunger pumps with a larger number of plungers have the benefit of increased flow, or smoother flow without a pulsation dampener. The increase in moving parts and crankshaft load is one drawback.
Car washes often use these triplex-style plunger pumps (perhaps without pulsation dampeners). In 1968, William Bruggeman significantly reduced the size of the triplex pump and increased the lifespan so that car washes could use equipment with smaller footprints. Durable high pressure seals, low pressure seals and oil seals, hardened crankshafts, hardened connecting rods, thick ceramic plungers and heavier duty ball and roller bearings improve reliability in triplex pumps. Triplex pumps now are in a myriad of markets across the world.
Triplex pumps with shorter lifetimes are commonplace to the home user. A person who uses a home pressure washer for 10 hours a year may be satisfied with a pump that lasts 100 hours between rebuilds. Industrial-grade or continuous duty triplex pumps on the other end of the quality spectrum may run for as much as 2,080 hours a year.
The oil and gas drilling industry uses massive semi trailer-transported triplex pumps called mud pumps to pump drilling mud, which cools the drill bit and carries the cuttings back to the surface.
Drillers use triplex or even quintuplex pumps to inject water and solvents deep into shale in the extraction process called "fracking".
Compressed-air-powered double-diaphragm pumps.
One modern application of positive displacement pumps is compressed-air-powered double-diaphragm pumps. Run on compressed air these pumps are intrinsically safe by design, although all manufacturers offer ATEX certified models to comply with industry regulation. These pumps are relatively inexpensive and can perform a wide variety of duties, from pumping water out of bunds, to pumping hydrochloric acid from secure storage (dependent on how the pump is manufactured – elastomers / body construction). Lift is normally limited to roughly 6m although heads can reach almost .
Rope pumps.
Devised in China as chain pumps over 1000 years ago, these pumps can be made from very simple materials: A rope, a wheel and a PVC pipe are sufficient to make a simple rope pump. For this reason they have become extremely popular around the world since the 1980s. Rope pump efficiency has been studied by grass roots organizations and the techniques for making and running them have been continuously improved.
Impulse pumps.
Impulse pumps use pressure created by gas (usually air). In some impulse pumps the gas trapped in the liquid (usually water), is released and accumulated somewhere in the pump, creating a pressure that can push part of the liquid upwards.
Conventional impulse pumps include:
Instead of a gas accumulation and releasing cycle, the pressure can be created by burning of hydrocarbons. Such combustion driven pumps directly transmit the impulse form a combustion event through the actuation membrane to the pump fluid. In order to allow this direct transmission, the pump needs to be almost entirely made of an elastomer (e.g. silicone rubber). Hence, the combustion causes the membrane to expand and thereby pumps the fluid out of the adjacent pumping chamber. The first combustion-driven soft pump was developed by ETH Zurich. 
Hydraulic ram pumps.
A hydraulic ram is a water pump powered by hydropower.
It takes in water at relatively low pressure and high flow-rate and outputs water at a higher hydraulic-head and lower flow-rate. The device uses the water hammer effect to develop pressure that lifts a portion of the input water that powers the pump to a point higher than where the water started.
The hydraulic ram is sometimes used in remote areas, where there is both a source of low-head hydropower, and a need for pumping water to a destination higher in elevation than the source. In this situation, the ram is often useful, since it requires no outside source of power other than the kinetic energy of flowing water.
Velocity pumps.
Rotodynamic pumps (or dynamic pumps) are a type of velocity pump in which kinetic energy is added to the fluid by increasing the flow velocity. This increase in energy is converted to a gain in potential energy (pressure) when the velocity is reduced prior to or as the flow exits the pump into the discharge pipe. This conversion of kinetic energy to pressure is explained by the "First law of thermodynamics", or more specifically by "Bernoulli's principle".
Dynamic pumps can be further subdivided according to the means in which the velocity gain is achieved.
These types of pumps have a number of characteristics:
A practical difference between dynamic and positive displacement pumps is how they operate under closed valve conditions. Positive displacement pumps physically displace fluid, so closing a valve downstream of a positive displacement pump produces a continual pressure build up that can cause mechanical failure of pipeline or pump. Dynamic pumps differ in that they can be safely operated under closed valve conditions (for short periods of time).
Radial-flow pumps.
These are also referred to as "centripetal design" pumps. The fluid enters along the axis or center, is accelerated by the impeller and exits at right angles to the shaft(radially). Radial-flow pumps operate at higher pressures and lower flow rates than axial- and mixed-flow pumps.
Axial-flow pumps.
These are also referred to as All fluid pumps. The fluid is pushed outward or inward and move fluid axially. They operate at much lower pressures and higher flow rates than radial-flow (centripetal) pumps.
Mixed-flow pumps.
Mixed-flow pumps function as a compromise between radial and axial-flow pumps. The fluid experiences both radial acceleration and lift and exits the impeller somewhere between 0 and 90 degrees from the axial direction. As a consequence mixed-flow pumps operate at higher pressures than axial-flow pumps while delivering higher discharges than radial-flow pumps. The exit angle of the flow dictates the pressure head-discharge characteristic in relation to radial and mixed-flow.
Eductor-jet pump.
This uses a jet, often of steam, to create a low pressure. This low pressure sucks in fluid and propels it into a higher pressure region.
Gravity pumps.
Gravity pumps include the "syphon" and "Heron's fountain". The "hydraulic ram" is also sometimes called a gravity pump; in a gravity pump the water is lifted by gravitational force.
Steam pumps.
Steam pumps have been for a long time mainly of historical interest. They include any type of pump powered by a steam engine and also pistonless pumps such as Thomas Savery's or the Pulsometer steam pump.
Recently there has been a resurgence of interest in low power solar steam pumps for use in smallholder irrigation in developing countries. Previously small steam engines have not been viable because of escalating inefficiencies as vapour engines decrease in size. However the use of modern engineering materials coupled with alternative engine configurations has meant that these types of system are now a cost effective opportunity.
Valveless pumps.
Valveless pumping assists in fluid transport in various biomedical and engineering systems. In a valveless pumping system, no valves (or physical occlusions) are present to regulate the flow direction. The fluid pumping efficiency of a valveless system, however, is not necessarily lower than that having valves. In fact, many fluid-dynamical systems in nature and engineering more or less rely upon valveless pumping to transport the working fluids therein. For instance, blood circulation in the cardiovascular system is maintained to some extent even when the heart’s valves fail. Meanwhile, the embryonic vertebrate heart begins pumping blood long before the development of discernible chambers and valves. In microfluidics, valveless impedance pumps have been fabricated, and are expected to be particularly suitable for handling sensitive biofluids. Ink jet printers operating on the Piezoelectric transducer principle also use valveless pumping. The pump chamber is emptied through the printing jet due to reduced flow impedance in that direction and refilled by capillary action..
Pump repairs.
Examining pump repair records and mean time between failures (MTBF) is of great importance to responsible and conscientious pump users. In view of that fact, the preface to the 2006 Pump User’s Handbook alludes to "pump failure" statistics. For the sake of convenience, these failure statistics often are translated into MTBF (in this case, installed life before failure).
In early 2005, Gordon Buck, John Crane Inc.’s chief engineer for Field Operations in Baton Rouge, LA, examined the repair records for a number of refinery and chemical plants to obtain meaningful reliability data for centrifugal pumps. A total of 15 operating plants having nearly 15,000 pumps were included in the survey. The smallest of these plants had about 100 pumps; several plants had over 2000. All facilities were located in the United States. In addition, considered as "new", others as "renewed" and still others as "established". Many of these plants—but not all—had an alliance arrangement with John Crane. In some cases, the alliance contract included having a John Crane Inc. technician or engineer on-site to coordinate various aspects of the program.
Not all plants are refineries, however, and different results occur elsewhere. In chemical plants, pumps have traditionally been "throw-away" items as chemical attack limits life. Things have improved in recent years, but the somewhat restricted space available in "old" DIN and ASME-standardized stuffing boxes places limits on the type of seal that fits. Unless the pump user upgrades the seal chamber, the pump only accommodates more compact and simple versions. Without this upgrading, lifetimes in chemical installations are generally around 50 to 60 percent of the refinery values.
Unscheduled maintenance is often one of the most significant costs of ownership, and failures of mechanical seals and bearings are among the major causes. Keep in mind the potential value of selecting pumps that cost more initially, but last much longer between repairs. The MTBF of a better pump may be one to four years longer than that of its non-upgraded counterpart. Consider that published average values of avoided pump failures range from US$2600 to US$12,000. This does not include lost opportunity costs. One pump fire occurs per 1000 failures. Having fewer pump failures means having fewer destructive pump fires.
As has been noted, a typical pump failure based on actual year 2002 reports, costs US$5,000 on average. This includes costs for material, parts, labor and overhead. Extending a pump's MTBF from 12 to 18 months would save US$1,667 per year — which might be greater than the cost to upgrade the centrifugal pump's reliability.
Applications.
Pumps are used throughout society for a variety of purposes. Early applications includes the use of the windmill or watermill to pump water. Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.
Because of the wide variety of applications, pumps have a plethora of shapes and sizes: from very large to very small, from handling gas to handling liquid, from high pressure to low pressure, and from high volume to low volume.
Priming a pump.
Typically, a liquid pump can't simply draw air. The feed line of the pump and the internal body surrounding the pumping mechanism must first be filled with the liquid that requires pumping: An operator must introduce liquid into the system to initiate the pumping. This is called "priming" the pump. Loss of prime is usually due to ingestion of air into the pump. The clearances and displacement ratios in pumps for liquids, whether thin or more viscous, usually cannot displace air due to its compressibility. This is the case with most velocity (rotodynamic) pumps — for example, centrifugal pumps.
Positive–displacement pumps, however, tend to have sufficiently tight sealing between the moving parts and the casing or housing of the pump that they can be described as "self-priming". Such pumps can also serve as "priming pumps", so called when they are used to fulfill that need for other pumps in lieu of action taken by a human operator.
Pumps as public water supplies.
One sort of pump once common worldwide was a hand-powered water pump, or 'pitcher pump'. It was commonly installed over community water wells in the days before piped water supplies.
In parts of the British Isles, it was often called "the parish pump". Though such community pumps are no longer common, people still used the expression "parish pump" to describe a place or forum where matters of local interest are discussed.
Because water from pitcher pumps is drawn directly from the soil, it is more prone to contamination. If such water is not filtered and purified, consumption of it might lead to gastrointestinal or other water-borne diseases. A notorious case is the 1854 Broad Street cholera outbreak. At the time it was not known how cholera was transmitted, but physician John Snow suspected contaminated water and had the handle of the public pump he suspected removed; the outbreak then subsided.
Modern hand-operated community pumps are considered the most sustainable low-cost option for safe water supply in resource-poor settings, often in rural areas in developing countries. A hand pump opens access to deeper groundwater that is often not polluted and also improves the safety of a well by protecting the water source from contaminated buckets. Pumps such as the Afridev pump are designed to be cheap to build and install, and easy to maintain with simple parts. However, scarcity of spare parts for these type of pumps in some regions of Africa has diminished their utility for these areas.
Sealing multiphase pumping applications.
Multiphase pumping applications, also referred to as tri-phase, have grown due to increased oil drilling activity. In addition, the economics of multiphase production is attractive to upstream operations as it leads to simpler, smaller in-field installations, reduced equipment costs and improved production rates. In essence, the multiphase pump can accommodate all fluid stream properties with one piece of equipment, which has a smaller footprint. Often, two smaller multiphase pumps are installed in series rather than having just one massive pump.
For midstream and upstream operations, multiphase pumps can be located onshore or offshore and can be connected to single or multiple wellheads. Basically, multiphase pumps are used to transport the untreated flow stream produced from oil wells to downstream processes or gathering facilities. This means that the pump may handle a flow stream (well stream) from 100 percent gas to 100 percent liquid and every imaginable combination in between. The flow stream can also contain abrasives such as sand and dirt. Multiphase pumps are designed to operate under changing or fluctuating process conditions. Multiphase pumping also helps eliminate emissions of greenhouse gases as operators strive to minimize the flaring of gas and the venting of tanks where possible.
Types and features of multiphase pumps.
Helico-axial pumps (centrifugal).
A rotodynamic pump with one single shaft that requires two mechanical seals, this pump uses an open-type axial impeller. It's often called a "Poseidon pump", and can be described as a cross between an axial compressor and a centrifugal pump.
Twin-screw (positive-displacement).
The twin-screw pump is constructed of two inter-meshing screws that move the pumped fluid. Twin screw pumps are often used when pumping conditions contain high gas volume fractions and fluctuating inlet conditions. Four mechanical seals are required to seal the two shafts.
Progressive cavity (positive-displacement).
Progressive cavity pumps are single-screw types typically used in shallow wells or at the surface. This pump is mainly used on surface applications where the pumped fluid may contain a considerable amount of solids such as sand and dirt.
Electric submersible (centrifugal).
These pumps are basically multistage centrifugal pumps and are widely used in oil well applications as a method for artificial lift. These pumps are usually specified when the pumped fluid is mainly liquid.
"Buffer tank"
A buffer tank is often installed upstream of the pump suction nozzle in case of a slug flow. The buffer tank breaks the energy of the liquid slug, smooths any fluctuations in the incoming flow and acts as a sand trap.
As the name indicates, multiphase pumps and their mechanical seals can encounter a large variation in service conditions such as changing process fluid composition, temperature variations, high and low operating pressures and exposure to abrasive/erosive media. The challenge is selecting the appropriate mechanical seal arrangement and support system to ensure maximized seal life and its overall effectiveness.
Specifications.
Pumps are commonly rated by horsepower, flow rate, outlet pressure in metres (or feet) of head, inlet suction in suction feet (or metres) of head.
The head can be simplified as the number of feet or metres the pump can raise or lower a column of water at atmospheric pressure.
From an initial design point of view, engineers often use a quantity termed the specific speed to identify the most suitable pump type for a particular combination of flow rate and head.
Pumping power.
The power imparted into a fluid increases the energy of the fluid per unit volume. Thus the power relationship is between the conversion of the mechanical energy of the pump mechanism and the fluid elements within the pump. In general, this is governed by a series of simultaneous differential equations, known as the Navier–Stokes equations. However a more simple equation relating only the different energies in the fluid, known as Bernoulli's equation can be used. Hence the power, P, required by the pump:
where Δp is the change in total pressure between the inlet and outlet (in Pa), and Q, the volume flow-rate of the fluid is given in m3/s. The total pressure may have gravitational, static pressure and kinetic energy components; i.e. energy is distributed between change in the fluid's gravitational potential energy (going up or down hill), change in velocity, or change in static pressure. η is the pump efficiency, and may be given by the manufacturer's information, such as in the form of a pump curve, and is typically derived from either fluid dynamics simulation (i.e. solutions to the Navier–Stokes for the particular pump geometry), or by testing. The efficiency of the pump depends upon the pump's configuration and operating conditions (such as rotational speed, fluid density and viscosity etc.)
For a typical "pumping" configuration, the work is imparted on the fluid, and is thus positive. For the fluid imparting the work on the pump (i.e. a turbine), the work is negative. Power required to drive the pump is determined by dividing the output power by the pump efficiency. Furthermore, this definition encompasses pumps with no moving parts, such as a siphon.
Efficiency.
Pump efficiency is defined as the ratio of the power imparted on the fluid by the pump in relation to the power supplied to drive the pump. Its value is not fixed for a given pump, efficiency is a function of the discharge and therefore also operating head. For centrifugal pumps, the efficiency tends to increase with flow rate up to a point midway through the operating range (peak efficiency) and then declines as flow rates rise further. Pump performance data such as this is usually supplied by the manufacturer before pump selection. Pump efficiencies tend to decline over time due to wear (e.g. increasing clearances as impellers reduce in size).
When a system includes a centrifugal pump, an important design issue is matching the "head loss-flow characteristic" with the pump so that it operates at or close to the point of its maximum efficiency.
Pump efficiency is an important aspect and pumps should be regularly tested. Thermodynamic pump testing is one method.

</doc>
<doc id="23618" url="https://en.wikipedia.org/wiki?curid=23618" title="Progressive">
Progressive

Progressive may refer to:

</doc>
<doc id="23619" url="https://en.wikipedia.org/wiki?curid=23619" title="Pressure">
Pressure

Pressure (symbol: "p" or "P") is the force applied perpendicular to the surface of an object per unit area over which that force is distributed. Gauge pressure (also spelled "gage" pressure) is the pressure relative to the ambient pressure.
Various units are used to express pressure. Some of these derive from a unit of force divided by a unit of area; the SI unit of pressure, the pascal (Pa), for example, is one newton per square metre; similarly, the pound-force per square inch (psi) is the traditional unit of pressure in the imperial and US customary systems. Pressure may also be expressed in terms of standard atmospheric pressure; the atmosphere (atm) is equal to this pressure and the torr is defined as of this. Manometric units such as the centimetre of water, millimetre of mercury, and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.
Definition.
Pressure is the amount of force acting per unit area. The symbol for pressure is "p" or "P".
The IUPAC recommendation for pressure is a lower-case "p".
However, upper-case "P" is widely used. The usage of "P" vs "p" depends on the field in which one is working, on the nearby presence of other symbols for quantities such as power and momentum, and on writing style.
Formula.
Mathematically:
where:
Pressure is a scalar quantity. It relates the vector surface element (a vector normal to the surface) with the normal force acting on it. The pressure is the scalar proportionality constant that relates the two normal vectors:
The minus sign comes from the fact that the force is considered towards the surface element, while the normal vector points outward. The equation has meaning in that, for any surface S in contact with the fluid, the total force exerted by the fluid on that surface is the surface integral over S of the right-hand side of the above equation.
It is incorrect (although rather usual) to say "the pressure is directed in such or such direction". The pressure, as a scalar, has no direction. The force given by the previous relationship to the quantity has a direction, but the pressure does not. If we change the orientation of the surface element, the direction of the normal force changes accordingly, but the pressure remains the same.
Pressure is transmitted to solid boundaries or across arbitrary sections of fluid "normal to" these boundaries or sections at every point. It is a fundamental parameter in thermodynamics, and it is conjugate to volume.
Units.
The SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N/m2 or kg·m−1·s−2). This name for the unit was added in 1971; before that, pressure in SI was expressed simply in newtons per square metre.
Other units of pressure, such as pounds per square inch and bar, are also in common use. The CGS unit of pressure is the barye (Ba), equal to 1 dyn·cm−2 or 0.1 Pa. Pressure is sometimes expressed in grams-force or kilograms-force per square centimetre (g/cm2 or kg/cm2) and the like without properly identifying the force units. But using the names kilogram, gram, kilogram-force, or gram-force (or their symbols) as units of force is expressly forbidden in SI. The technical atmosphere (symbol: at) is 1 kgf/cm2 (98.0665 kPa or 14.223 psi).
Since a system under pressure has potential to perform work on its surroundings, pressure is a measure of potential energy stored per unit volume. It is therefore related to energy density and may be expressed in units such as joules per cubic metre (J/m3, which is equal to Pa).
Some meteorologists prefer the hectopascal (hPa) for atmospheric air pressure, which is equivalent to the older unit millibar (mbar). Similar pressures are given in kilopascals (kPa) in most other fields, where the hecto- prefix is rarely used. The inch of mercury is still used in the United States. Oceanographers usually measure underwater pressure in decibars (dbar) because pressure in the ocean increases by approximately one decibar per metre depth.
The standard atmosphere (atm) is an established constant. It is approximately equal to typical air pressure at earth mean sea level and is defined as .
Because pressure is commonly measured by its ability to displace a column of liquid in a manometer, pressures are often expressed as a depth of a particular fluid (e.g., centimetres of water, millimetres of mercury or inches of mercury). The most common choices are mercury (Hg) and water; water is nontoxic and readily available, while mercury's high density allows a shorter column (and so a smaller manometer) to be used to measure a given pressure. The pressure exerted by a column of liquid of height "h" and density "ρ" is given by the hydrostatic pressure equation , where "g" is the gravitational acceleration. Fluid density and local gravity can vary from one reading to another depending on local factors, so the height of a fluid column does not define pressure precisely. When millimetres of mercury or inches of mercury are quoted today, these units are not based on a physical column of mercury; rather, they have been given precise definitions that can be expressed in terms of SI units. One millimetre of mercury is approximately equal to one torr. The water-based units still depend on the density of water, a measured, rather than defined, quantity. These "manometric units" are still encountered in many fields. Blood pressure is measured in millimetres of mercury in most of the world, and lung pressures in centimetres of water are still common.
Underwater divers use the metre sea water (msw or MSW) and foot sea water (fsw or FSW) units of pressure, and these are the standard units for pressure gauges used to measure pressure exposure in diving chambers and personal decompression computers. A msw is defined as 0.1 bar, and is not the same as a linear metre of depth, and 33.066 fsw = 1 atm. Note that the pressure conversion from msw to fsw is different from the length conversion: 10 msw = 32.6336 fsw, while 10 m = 32.8083 ft
Gauge pressure is often given in units with 'g' appended, e.g. 'kPag', 'barg' or 'psig', and units for measurements of absolute pressure are sometimes given a suffix of 'a', to avoid confusion, for example 'kPaa', 'psia'. However, the US National Institute of Standards and Technology recommends that, to avoid confusion, any modifiers be instead applied to the quantity being measured rather than the unit of measure For example, rather than .
Differential pressure is expressed in units with 'd' appended; this type of measurement is useful when considering sealing performance or whether a valve will open or close.
Presently or formerly popular pressure units include the following:
Examples.
As an example of varying pressures, a finger can be pressed against a wall without making any lasting impression; however, the same finger pushing a thumbtack can easily damage the wall. Although the force applied to the surface is the same, the thumbtack applies more pressure because the point concentrates that force into a smaller area. Pressure is transmitted to solid boundaries or across arbitrary sections of fluid "normal to" these boundaries or sections at every point. Unlike stress, pressure is defined as a scalar quantity. The negative gradient of pressure is called the force density.
Another example is of a common knife. If we try to cut a fruit with the flat side it obviously will not cut. But if we take the thin side, it will cut smoothly. The reason is that the flat side has a greater surface area (less pressure) and so it does not cut the fruit. When we take the thin side, the surface area is reduced and so it cuts the fruit easily and quickly. This is one example of a practical application of pressure.
For gases, pressure is sometimes measured not as an "absolute pressure", but relative to atmospheric pressure; such measurements are called "gauge pressure". An example of this is the air pressure in an automobile tire, which might be said to be "220 kPa (32 psi)", but is actually 220 kPa (32 psi) above atmospheric pressure. Since atmospheric pressure at sea level is about 100 kPa (14.7 psi), the absolute pressure in the tire is therefore about 320 kPa (46.7 psi). In technical work, this is written "a gauge pressure of 220 kPa (32 psi)". Where space is limited, such as on pressure gauges, name plates, graph labels, and table headings, the use of a modifier in parentheses, such as "kPa (gauge)" or "kPa (absolute)", is permitted. In non-SI technical work, a gauge pressure of 32 psi is sometimes written as "32 psig" and an absolute pressure as "32 psia", though the other methods explained above that avoid attaching characters to the unit of pressure are preferred.
Gauge pressure is the relevant measure of pressure wherever one is interested in the stress on storage vessels and the plumbing components of fluidics systems. However, whenever equation-of-state properties, such as densities or changes in densities, must be calculated, pressures must be expressed in terms of their absolute values. For instance, if the atmospheric pressure is 100 kPa, a gas (such as helium) at 200 kPa (gauge) (300 kPa is 50% denser than the same gas at 100 kPa (gauge) (200 kPa [absolute). Focusing on gauge values, one might erroneously conclude the first sample had twice the density of the second one.
Scalar nature.
In a static gas, the gas as a whole does not appear to move. The individual molecules of the gas, however, are in constant random motion. Because we are dealing with an extremely large number of molecules and because the motion of the individual molecules is random in every direction, we do not detect any motion. If we enclose the gas within a container, we detect a pressure in the gas from the molecules colliding with the walls of our container. We can put the walls of our container anywhere inside the gas, and the force per unit area (the pressure) is the same. We can shrink the size of our "container" down to a very small point (becoming less true as we approach the atomic scale), and the pressure will still have a single value at that point. Therefore, pressure is a scalar quantity, not a vector quantity. It has magnitude but no direction sense associated with it. Pressure acts in all directions at a point inside a gas. At the surface of a gas, the pressure force acts perpendicular (at right angle) to the surface.
A closely related quantity is the stress tensor "σ", which relates the vector force formula_6 to the 
vector area formula_7 via the linear relation formula_8.
This tensor may be expressed as the sum of the viscous stress tensor minus the hydrostatic pressure. The negative of the stress tensor is sometimes called the pressure tensor, but in the following, the term "pressure" will refer only to the scalar pressure.
According to the theory of general relativity, pressure increases the strength of a gravitational field (see stress–energy tensor) and so adds to the mass-energy cause of gravity. This effect is unnoticeable at everyday pressures but is significant in neutron stars, although it has not been experimentally tested.
Types.
Fluid pressure.
Fluid pressure is the pressure at some point within a fluid, such as water or air (for more information specifically about liquid pressure, see section below).
Fluid pressure occurs in one of two situations:
Pressure in open conditions usually can be approximated as the pressure in "static" or non-moving conditions (even in the ocean where there are waves and currents), because the motions create only negligible changes in the pressure. Such conditions conform with principles of fluid statics. The pressure at any given point of a non-moving (static) fluid is called the hydrostatic pressure. 
Closed bodies of fluid are either "static", when the fluid is not moving, or "dynamic", when the fluid can move as in either a pipe or by compressing an air gap in a closed container. The pressure in closed conditions conforms with the principles of fluid dynamics.
The concepts of fluid pressure are predominantly attributed to the discoveries of Blaise Pascal and Daniel Bernoulli. Bernoulli's equation can be used in almost any situation to determine the pressure at any point in a fluid. The equation makes some assumptions about the fluid, such as the fluid being ideal and incompressible. An ideal fluid is a fluid in which there is no friction, it is inviscid, zero viscosity. The equation for all points of a system filled with a constant-density fluid is
where:
Explosion or deflagration pressures.
Explosion or deflagration pressures are the result of the ignition of explosive gases, mists, dust/air suspensions, in unconfined and confined spaces.
Negative pressures.
While pressures are, in general, positive, there are several situations in which negative pressures may be encountered:
Stagnation pressure.
Stagnation pressure is the pressure a fluid exerts when it is forced to stop moving. Consequently, although a fluid moving at higher speed will have a lower static pressure, it may have a higher stagnation pressure when forced to a standstill. Static pressure and stagnation pressure are related by:
where 
The pressure of a moving fluid can be measured using a Pitot tube, or one of its variations such as a Kiel probe or Cobra probe, connected to a manometer. Depending on where the inlet holes are located on the probe, it can measure static pressures or stagnation pressures.
Surface pressure and surface tension.
There is a two-dimensional analog of pressure – the lateral force per unit length applied on a line perpendicular to the force.
Surface pressure is denoted by π and shares many similar properties with three-dimensional pressure. Properties of surface chemicals can be investigated by measuring pressure/area isotherms, as the two-dimensional analog of Boyle's law, , at constant temperature.
Surface tension is another example of surface pressure, but with a reversed sign, because "tension" is the opposite to "pressure".
Pressure of an ideal gas.
In an ideal gas, molecules have no volume and do not interact. According to the ideal gas law, pressure varies linearly with temperature and quantity, and inversely with volume. 
where:
Real gases exhibit a more complex dependence on the variables of state.
Vapor pressure.
Vapor pressure is the pressure of a vapor in thermodynamic equilibrium with its condensed phases in a closed system. All liquids and solids have a tendency to evaporate into a gaseous form, and all gases have a tendency to condense back to their liquid or solid form.
The atmospheric pressure boiling point of a liquid (also known as the normal boiling point) is the temperature at which the vapor pressure equals the ambient atmospheric pressure. With any incremental increase in that temperature, the vapor pressure becomes sufficient to overcome atmospheric pressure and lift the liquid to form vapor bubbles inside the bulk of the substance. Bubble formation deeper in the liquid requires a higher pressure, and therefore higher temperature, because the fluid pressure increases above the atmospheric pressure as the depth increases.
The vapor pressure that a single component in a mixture contributes to the total pressure in the system is called partial vapor pressure.
Liquid pressure.
When a person swims under the water, water pressure is felt acting on the person's eardrums. The deeper that person swims, the greater the pressure. The pressure felt is due to the weight of the water above the person. As someone swims deeper, there is more water above the person and therefore greater pressure. The pressure a liquid exerts depends on its depth.
Liquid pressure also depends on the density of the liquid. If someone was submerged in a liquid more dense than water, the pressure would be correspondingly greater. The pressure due to a liquid in liquid columns of constant density or at a depth within a substance is represented by the following formula:
where:
Another way of saying this same formula is the following:
The pressure a liquid exerts against the sides and bottom of a container depends on the density and the depth of the liquid. If atmospheric pressure is neglected, liquid pressure against the bottom is twice as great at twice the depth; at three times the depth, the liquid pressure is threefold; etc. Or, if the liquid is two or three times as dense, the liquid pressure is correspondingly two or three times as great for any given depth. Liquids are practically incompressible – that is, their volume can hardly be changed by pressure (water volume decreases by only 50 millionths of its original volume for each atmospheric increase in pressure). Thus, except for small changes produced by temperature, the density of a particular liquid is practically the same at all depths.
Atmospheric pressure pressing on the surface of a liquid must be taken into account when trying to discover the "total" pressure acting on a liquid. The total pressure of a liquid, then, is "ρgh" plus the pressure of the atmosphere. When this distinction is important, the term "total pressure" is used. Otherwise, discussions of liquid pressure refer to pressure without regard to the normally ever-present atmospheric pressure.
It is important to recognize that the pressure does not depend on the "amount" of liquid present. Volume is not the important factor – depth is. The average water pressure acting against a dam depends on the average depth of the water and not on the volume of water held back. For example, a wide but shallow lake with a depth of exerts only half the average pressure that a small deep pond does (note that the "total force" applied to the longer dam will be greater, due to the greater total surface area for the pressure to act upon, but for a given 5 foot section of each dam, the 10ft deep water will apply half the force of 20ft deep water). A person will feel the same pressure whether his/her head is dunked a metre beneath the surface of the water in a small pool or to the same depth in the middle of a large lake. If four vases contain different amounts of water but are all filled to equal depths, then a fish with its head dunked a few centimetres under the surface will be acted on by water pressure that is the same in any of the vases. If the fish swims a few centimetres deeper, the pressure on the fish will increase with depth and be the same no matter which vase the fish is in. If the fish swims to the bottom, the pressure will be greater, but it makes no difference what vase it is in. All vases are filled to equal depths, so the water pressure is the same at the bottom of each vase, regardless of its shape or volume. If water pressure at the bottom of a vase were greater than water pressure at the bottom of a neighboring vase, the greater pressure would force water sideways and then up the narrower vase to a higher level until the pressures at the bottom were equalized. Pressure is depth dependent, not volume dependent, so there is a reason that water seeks its own level.
Restating this as energy equation, the energy per unit volume in an ideal, incompressible liquid is constant throughout its vessel. At the surface, gravitational potential energy is large but liquid pressure energy is low. At the bottom of the vessel, all the gravitational potential energy is converted to pressure energy. The sum of pressure energy and gravitational potential energy per unit volume is constant throughout the volume of the fluid and the two energy components change linearly with the depth. Mathematically, it is described by Bernoulli's equation where velocity head is zero and comparisons per unit volume in the vessel are:
Terms have the same meaning as in section Fluid pressure.
Direction of liquid pressure.
An experimentally determined fact about liquid pressure is that it is exerted equally in all directions. If someone is submerged in water, no matter which way that person tilts his/her head, the person will feel the same amount of water pressure on his/her ears. Because a liquid can flow, this pressure isn't only downward. Pressure is seen acting sideways when water spurts sideways from a leak in the side of an upright can. Pressure also acts upward, as demonstrated when someone tries to push a beach ball beneath the surface of the water. The bottom of a boat is pushed upward by water pressure (buoyancy).
When a liquid presses against a surface, there is a net force that is perpendicular to the surface. Although pressure doesn't have a specific direction, force does. A submerged triangular block has water forced against each point from many directions, but components of the force that are not perpendicular to the surface cancel each other out, leaving only a net perpendicular point. This is why water spurting from a hole in a bucket initially exits the bucket in a direction at right angles to the surface of the bucket in which the hole is located. Then it curves downward due to gravity. If there are three holes in a bucket (top, bottom, and middle), then the force vectors perpendicular to the inner container surface will increase with increasing depth – that is, a greater pressure at the bottom makes it so that the bottom hole will shoot water out the farthest. The force exerted by a fluid on a smooth surface is always at right angles to the surface. The speed of liquid out of the hole is formula_21, where "h" is the depth below the free surface. Interestingly, this is the same speed the water (or anything else) would have if freely falling the same vertical distance "h".
Kinematic pressure.
is the kinematic pressure, where formula_2 is the pressure and formula_24 constant mass density. The SI unit of "P" is m2/s2. Kinematic pressure is used in the same manner as kinematic viscosity formula_25 in order to compute Navier–Stokes equation without explicitly showing the density formula_24.

</doc>
<doc id="23621" url="https://en.wikipedia.org/wiki?curid=23621" title="Polygon">
Polygon

In elementary geometry, a polygon is a plane figure that is bounded by a finite chain of straight line segments closing in a loop to form a closed chain or "circuit". These segments are called its "edges" or "sides", and the points where two edges meet are the polygon's "vertices" (singular: vertex) or "corners". The interior of the polygon is sometimes called its "body". An "n"-gon is a polygon with "n" sides. A polygon is a 2-dimensional example of the more general polytope in any number of dimensions. One type of polygon is a triangle and there are many others.
The basic geometrical notion of a polygon has been adapted in various ways to suit particular purposes. Mathematicians are often concerned only with the bounding closed polygonal chain and with simple polygons which do not self-intersect, and they often define a polygon accordingly. A polygonal boundary may be allowed to intersect itself, creating star polygons. Geometrically two edges meeting at a corner are required to form an angle that is not straight (180°); otherwise, the line segments may be considered parts of a single edge; however mathematically, such corners may sometimes be allowed. These and other generalizations of polygons are described below.
Etymology.
The word "polygon" derives from the Greek adjective πολύς ("polús") "much", "many" and γωνία ("gōnía") "corner" or "angle". It has been suggested that γόνυ ("gónu") "knee" may be the origin of “gon”,
Classification.
Number of sides.
Polygons are primarily classified by the number of sides. See table below.
Convexity and non-convexity.
Polygons may be characterized by their convexity or type of non-convexity:
Properties and formulas.
Euclidean geometry is assumed throughout.
Angles.
Any polygon has as many corners as it has sides. Each corner has several angles. The two most important ones are:
Area and centroid.
Simple polygons.
For a non-self-intersecting (simple) polygon with "n" vertices "xi, yi" ( "i" = 1 to "n"), the signed area and the Cartesian coordinates of the centroid are given by:
where formula_8 is the squared distance between formula_9 and formula_10 and
To close the polygon, the first and last vertices are the same, i.e., "xn", "yn" = "x"0, "y"0. The vertices must be ordered according to positive or negative orientation (counterclockwise or clockwise, respectively); if they are ordered negatively, the value given by the area formula will be negative but correct in absolute value, but when calculating formula_13 and formula_14, the signed value of formula_15 (which in this case is negative) should be used. This is commonly called the shoelace formula or Surveyor's formula.
The area "A" of a simple polygon can also be computed if the lengths of the sides, "a"1, "a"2, ..., "an" and the exterior angles, "θ"1, "θ"2, ..., "θn" are known, from:
The formula was described by Lopshits in 1963.
If the polygon can be drawn on an equally spaced grid such that all its vertices are grid points, Pick's theorem gives a simple formula for the polygon's area based on the numbers of interior and boundary grid points: the former number plus one-half the latter number, minus 1.
In every polygon with perimeter "p" and area "A ", the isoperimetric inequality formula_17 holds.
If any two simple polygons of equal area are given, then the first can be cut into polygonal pieces which can be reassembled to form the second polygon. This is the Bolyai–Gerwien theorem.
The area of a regular polygon is also given in terms of the radius "r" of its inscribed circle and its perimeter "p" by
This radius is also termed its apothem and is often represented as "a".
The area of a regular "n"-gon with side "s" inscribed in a unit circle is
The area of a regular "n"-gon in terms of the radius "R" of its circumscribed circle and its perimeter "p" is given by
The area of a regular "n"-gon inscribed in a unit-radius circle, with side "s" and interior angle formula_21 can also be expressed trigonometrically as
The lengths of the sides of a polygon do not in general determine the area. However, if the polygon is cyclic the sides "do" determine the area. 
Of all "n"-gons with given sides, the one with the largest area is cyclic. Of all "n"-gons with a given perimeter, the one with the largest area is regular (and therefore cyclic).
Self-intersecting polygons.
The area of a self-intersecting polygon can be defined in two different ways, each of which gives a different answer:
Generalizations of polygons.
The idea of a polygon has been generalized in various ways. Some of the more important include:
Naming polygons.
The word "polygon" comes from Late Latin "polygōnum" (a noun), from Greek πολύγωνον ("polygōnon/polugōnon"), noun use of neuter of πολύγωνος ("polygōnos/polugōnos", the masculine adjective), meaning "many-angled". Individual polygons are named (and sometimes classified) according to the number of sides, combining a Greek-derived numerical prefix with the suffix "-gon", e.g. "pentagon", "dodecagon". The triangle, quadrilateral and nonagon are exceptions.
Beyond decagons (10-sided) and dodecagons (12-sided), mathematicians generally use numerical notation, for example 17-gon and 257-gon.
Exceptions exist for side counts that are more easily expressed in verbal form (e.g. 20 and 30), or are used by non-mathematicians. Some special polygons also have their own names; for example the regular star pentagon is also known as the pentagram.
Constructing higher names.
To construct the name of a polygon with more than 20 and less than 100 edges, combine the prefixes as follows. The "kai" term applies to 13-gons and higher was used by Kepler, and advocated by John H. Conway for clarity to concatenated prefix numbers in the naming of quasiregular polyhedra.
History.
Polygons have been known since ancient times. The regular polygons were known to the ancient Greeks, with the pentagram, a non-convex regular polygon (star polygon), appearing as early as the 7th century B.C. on a krater by Aristonothos, found at Caere and now in the Capitoline Museum.
The first known systematic study of non-convex polygons in general was made by Thomas Bradwardine in the 14th century.
In 1952, Geoffrey Colin Shephard generalized the idea of polygons to the complex plane, where each real dimension is accompanied by an imaginary one, to create complex polygons.
Polygons in nature.
Polygons appear in rock formations, most commonly as the flat facets of crystals, where the angles between the sides depend on the type of mineral from which the crystal is made.
Regular hexagons can occur when the cooling of lava forms areas of tightly packed columns of basalt, which may be seen at the Giant's Causeway in Northern Ireland, or at the Devil's Postpile in California.
In biology, the surface of the wax honeycomb made by bees is an array of hexagons, and the sides and base of each cell are also polygons.
Polygons in computer graphics.
A polygon in a computer graphics (image generation) system is a two-dimensional shape that is modelled and stored within its database. A polygon can be colored, shaded and textured, and its position in the database is defined by the coordinates of its vertices (corners).
Naming conventions differ from those of mathematicians:
Any surface is modelled as a tessellation called polygon mesh. If a square mesh has points (vertices) per side, there are "n" squared squares in the mesh, or 2"n" squared triangles since there are two triangles in a square. There are vertices per triangle. Where "n" is large, this approaches one half. Or, each vertex inside the square mesh connects four edges (lines).
The imaging system calls up the structure of polygons needed for the scene to be created from the database. This is transferred to active memory and finally, to the display system (screen, TV monitors etc.) so that the scene can be viewed. During this process, the imaging system renders polygons in correct perspective ready for transmission of the processed data to the display system. Although polygons are two-dimensional, through the system computer they are placed in a visual scene in the correct three-dimensional orientation.
In computer graphics and computational geometry, it is often necessary to determine whether a given point "P" = ("x"0,"y"0) lies inside a simple polygon given by a sequence of line segments. This is called the Point in polygon test.

</doc>
<doc id="23622" url="https://en.wikipedia.org/wiki?curid=23622" title="Player character">
Player character

A player character or playable character (PC) is a fictional character in a role-playing or video game whose actions are directly controlled by a player of the game rather than the rules of the game. The characters that are not controlled by a player are called non-player characters (NPCs). The actions of non-player characters are typically handled by the game itself in video games, or according to rules followed by a gamemaster refereeing tabletop role-playing games. The player character functions as a fictional, alternate body for the player controlling it. 
Video games typically have one player character for each person playing the game. Some games offer a group of player characters for the player to choose from, allowing the player to control one of them at a time. Where more than one player character is available, the characters may have different abilities, strengths, and weaknesses to make the game play style different.
Overview.
Avatars.
A player character may sometimes be based on a real person, especially in sports games that use the names and likenesses of real sports people. Historical people and leaders may sometimes appear as characters too, particularly in strategy or empire building games such as in Sid Meier's "Civilization" series. Curiously, in the case of Civilization, a player's chosen historical character is the same throughout the course of the game despite the fact that a campaign can last several hundred years before and after the lifetime of the real historical persona. Such a player character is more properly an avatar as the player character's name and image typically have little bearing on the game itself. Avatars are also commonly seen in casino game simulations.
Role-playing games.
In role playing games such as "Dungeons and Dragons" or "Final Fantasy," a player typically creates or takes on the identity of a character that may have nothing in common with the player. The character is usually of a certain (often fictional) race and class (such as zombie, berserker, rifleman, elf, or cleric), each with strengths and weaknesses. The attributes of the characters (such as magic and fighting ability) are given as numerical values which can be increased as the gamer progresses and gains rank and experience points through accomplishing goals or fighting enemies.
Blank characters.
In many video games, and especially first-person shooters, the player character is a "blank slate" without any notable characteristics or even backstory. "Crono", "Link" and "Chell" are examples of such characters. These characters are generally silent protagonists.
Some games will go even further, never showing or naming the player-character at all. This is somewhat common in first-person videogames, such as in "Myst", but is more often done in strategy video games such as "Dune 2000" and "". In such games, the only real indication that the player has a character (instead of an omnipresent status), is from the cutscenes during which the character is being given a mission briefing or debriefing; the player is usually addressed as "general", "commander", or another military rank. Typically, this is done so that the player may imagine himself in the adventure without being required to play a character who is of a different age, race, gender, or background.
In gaming culture, such a character was called Ageless, Faceless, Gender-Neutral, Culturally Ambiguous Adventure Person, abbreviated as AFGNCAAP; a term that originated in "" where it is used satirically to refer to the player. 
Fighting games.
Fighting games typically have a larger amount of player characters to choose from, with some basic moves available to all or most characters and some unique moves only available to one or a few characters. Having many different characters to play as and against, all possessing different moves and abilities, is necessary to create a larger gameplay variety in such games.
Secret characters.
A secret or unlockable character may be a playable character in a video game available after completing the game or meeting other requirements. In some video games, characters that are not secret but appear only as non-player characters like bosses or enemies become playable characters after completing certain requirements, or sometimes cheating.

</doc>
<doc id="23623" url="https://en.wikipedia.org/wiki?curid=23623" title="Parish">
Parish

A parish is a church territorial unit constituting a division within a diocese. A parish is under the pastoral care and clerical jurisdiction of a parish priest, who might be assisted by one or more curates, and who operates from a parish church. Historically, a parish often covered the same geographical area as a manor (its association with the parish church remaining paramount).
By extension the term "parish" refers not only to the territorial unit but to the people of its community or congregation as well as to church property within it. In England this church property was technically in ownership of the parish priest "ex-officio", vested in him on his institution to that parish.
Etymology and use.
First attested in English in the late 13th century, the word "parish" comes from the Old French "paroisse", in turn from , the latinisation of the , "sojourning in a foreign land", itself from ("paroikos"), "dwelling beside, stranger, sojourner", which is a compound of ("pará"), "beside, by, near" and οἶκος ("oîkos"), "house".
As an ancient concept, the term "parish" occurs in the long-established Christian denominations: Roman Catholic, Anglican Communion, the Eastern Orthodox Church, and Lutheran churches, and in some Methodist and Presbyterian administrations.
The eighth Archbishop of Canterbury Theodore of Tarsus (c. 602–690) applied to the Anglo-Saxon township unit, where it existed, the ecclesiastical term "parish".
Church territorial structure.
Broadly speaking, the parish is the standard unit in episcopal polity of church administration, although parts of a parish may be subdivided as a "chapelry", with a chapel of ease or filial church serving as the local place of worship in cases of difficulty to access the main parish church.
In the wider picture of ecclesiastical polity, a "parish" comprises a division of a diocese or see. Parishes within a diocese may be grouped into a deanery or "vicariate forane" (or simply "vicariate"), overseen by a dean or "vicar forane", or in some cases by an archpriest. Some churches of the Anglican Communion have deaneries as units of an archdeaconry.
Roman Catholic Church.
In the Roman Catholic Church, each parish normally has its own parish priest (in some countries called pastor), who has responsibility and canonical authority over the parish.
What in most English-speaking countries is termed the "parish priest" is referred to as the "pastor" in the United States, where the term "parish priest" is used of any priest assigned to a parish even in a subordinate capacity. These are called "assistant priests", "parochial vicars", "curates", or, in the United States, "associate pastors" and "assistant pastors".
Each diocese (administrative region) is divided into parishes, each with their own central church called the parish church, where religious services take place. Some larger parishes or parishes that have been combined under one pastor may have two or more such churches, or the parish may be responsible for chapels (or chapels of ease) located at some distance from the mother church for the convenience of distant parishioners.
Normally, a parish comprises all Catholics living in its area, but parishes can also be established within a defined geographical region on a personal basis for Catholics of a particular rite, language, nationality or the like. An example is that of personal parishes established in accordance with the 7 July 2007 motu proprio "Summorum Pontificum" for those attached to the older form of the Roman Rite.
Most Catholic parishes are part of Latin Rite dioceses, which together cover the whole territory of a country. There can also be overlapping parishes of eparchies of Eastern Catholic Churches, personal ordinariates or military ordinariates.
Church of England.
The Church of England geographical structure uses the local parish church as its basic unit. The parish system survived the Reformation with the Anglican Church's secession from Rome remaining largely untouched, thus it shares its roots with the Church of Rome's system described above. Parishes may extend into different counties or hundreds and historically many parishes comprised extra outlying portions in addition to its principal district, usually being described as 'detached' and intermixed with the lands of other parishes. Church of England parishes nowadays all lie within one of 44 dioceses divided between the provinces of Canterbury, 30 and York, 14.
Each parish normally has its own parish priest (either a vicar or rector, owing to the vagaries of the feudal tithe system: rectories usually having had greater income) and perhaps supported by one or more curates or deacons - although as a result of ecclesiastical pluralism some parish priests might have held more than one parish living, placing a curate in charge of those where they do not reside. Now, however, it is common for a number of neighbouring parishes to be placed under one benefice in the charge of a priest who conducts services by rotation, with additional services being provided by lay readers or other non-ordained members of the church community.
A chapelry was a subdivision of an ecclesiastical parish in England, and parts of Lowland Scotland up to the mid 19th century. It had a similar status to a township but was so named as it had a chapel which acted as a subsidiary place of worship to the main parish church. 
In England civil parishes and their governing parish councils evolved in the 19th century as ecclesiastical parishes began to be relieved of what became considered to be civic responsibilities. Thus their boundaries began to diverge. The word "parish" acquired a secular usage. Since 1895, a parish council elected by public vote or a (civil) parish meeting administers a civil parish and is formally recognised as the level of local government below a district council.
The traditional structure of the Church of England with the parish as the basic unit has been exported to other countries and churches throughout the Anglican Communion and Commonwealth but does not necessarily continue to be administered in the same way.
Church of Scotland.
The parish is also the basic level of church administration in the Church of Scotland. Spiritual oversight of each parish church in Scotland is responsibility of the congregation's Kirk Session. Patronage was regulated in 1711 (Patronage Act) and abolished in 1874, with the result that ministers must be elected by members of the congregation. Many parish churches in Scotland today are "linked" with neighbouring parish churches served by a single minister. Since the abolition of parishes as a unit of civil government in Scotland in 1929, Scottish parishes have purely ecclesiastical significance and the boundaries may be adjusted by the local Presbytery.
Methodist Church.
Although they are more often simply called congregations and have no geographic boundaries, in the United Methodist Church congregations are called parishes. A prominent example of this usage comes in "The Book of Discipline of The United Methodist Church", in which the committee of every local congregation that handles staff support is referred to as the committee on Pastor-Parish Relations. This committee gives recommendations to the bishop on behalf of the parish/congregation since it is the United Methodist Bishop of the Episcopal Area that appoints a pastor to each congregation. The same is true in the African Methodist Episcopal Church and the Christian Methodist Episcopal Church.
In New Zealand, a local grouping of Methodist churches that share one or more ministers (which in the United Kingdom would be called a circuit) is referred to as a parish.

</doc>
<doc id="23624" url="https://en.wikipedia.org/wiki?curid=23624" title="Procopius">
Procopius

Procopius of Caesarea (, ; c. AD 500 – c. AD 554) was a prominent late antique scholar from "Palaestina Prima". Accompanying the Roman general Belisarius in the wars of the Emperor Justinian, he became the principal historian of the 6th century, writing the "Wars" (or "Histories"), the "Buildings of Justinian" and the celebrated (and infamous) "Secret History". He is commonly held to be the last major historian of the ancient Western world.
Life.
Apart from his own writings, the main source for Procopius' life is an entry in the "Suda", a 10th-century Byzantine encyclopaedia that tells everything about his early life. He was a native of Caesarea in the Roman Province "Palaestina Prima". He would have received a conventional élite education in the Greek classics and then rhetoric, perhaps at the famous School of Gaza, may have attended law school, possibly at Berytus (modern Beirut) or Constantinople, and became a "rhetor" (barrister or advocate). He evidently knew Latin, as was natural for a man with legal training. In 527, the first year of Eastern Roman Emperor Justinian's reign, he became the "adsessor" (legal adviser) for Belisarius, Justinian's chief military commander who was then beginning a brilliant career.
Procopius was with Belisarius on the eastern front until the latter was defeated at the Battle of Callinicum in 531 and recalled to Constantinople. Procopius witnessed the Nika riots of January, 532, which Belisarius and his fellow general Mundus repressed with a massacre in the Hippodrome. In 533, he accompanied Belisarius on his victorious expedition against the Vandal kingdom in North Africa, took part in the capture of Carthage, and remained in Africa with Belisarius' successor Solomon the Eunuch when Belisarius returned to Constantinople. Procopius recorded a few of the extreme weather events of 535–536, although these were presented as a backdrop to Roman (Byzantine) military activities, such as a mutiny, in and near Carthage. He rejoined Belisarius for his campaign against the Ostrogothic kingdom in Italy and experienced the Gothic siege of Rome that lasted a year and nine days, ending in mid-March 538. He witnessed Belisarius' entry into the Gothic capital, Ravenna, in 540. Book Eight of the "Wars of Justinian", and the "Secret History", suggest that his relationship with Belisarius seems to have cooled thereafter. When Belisarius was sent back to Italy in 544 to cope with a renewal of the war with the Goths, now led by the able king Totila, Procopius appears to have no longer been on Belisarius' staff. 
As "magister militum", Belisarius was a "vir illustris", and Procopius, as his "adsessor", must, therefore, have had at least the rank of a "vir spectabilis". He thus belonged to the middle-ranking group of the "ordo senatorius". However, the "Suda", which is usually well informed in such matters, also describes Procopius himself as ἰλλούστριος . Should this information be correct, then Procopius had a seat in the senate of Constantinople, which was restricted to the "illustres" under Justinian.
It is not known when Procopius himself died, and many historians (James Howard-Johnson, Averil Cameron, Geoffrey Greatrex) date his death to 554, but in 562 there was an urban prefect of Constantinople ("praefectus urbi Constantinopolitanae") who happened to be called Procopius. In that year, Belisarius was implicated in a conspiracy and was brought before this urban prefect.
Writings.
The writings of Procopius are the primary source of information for the rule of the Roman emperor Justinian. Procopius was the author of a history in eight books of the wars fought by Justinian I, a panegyric on Justinian's public works throughout the empire, and a book known as the "Secret History" that claims to report the scandals that Procopius could not include in his published history.
"The Wars of Justinian".
Procopius' "Wars of Justinian" ( "Hypèr tōn polémon lógoi", , "About the Wars") is clearly his most important work, although it is not as well known as the "Secret History". The first seven books, which may have been published as a unit, seem to have been largely completed by 545, but were updated to mid-century before publication, for the latest event mentioned belongs to early 551. The first two books (often known as the "Persian War", Latin "De Bello Persico") deal with the conflict between the Romans and Sassanid Persia in Mesopotamia, Syria, Armenia, Lazica and Caucasian Iberia (roughly modern-day Georgia). It details the campaigns of the Sasanian Shah Kavadh I, the 'Nika' revolt in Constantinople in 532, the war by Kavadh's successor, Khosrau I, in 540 and his destruction of Antioch and the transportation of its inhabitants to Mesopotamia, and the great plague that devastated Constantinople in 542. They also cover the early career of the Roman general Belisarius, Procopius' patron, in some detail. The next two books, the "Vandal War" (Latin "De Bello Vandalico"), cover Belisarius' successful campaign against the Vandal kingdom in Roman Africa. The remaining books cover the "Gothic War" (Latin "De Bello Gothico"), the campaigns by Belisarius and others to recapture Italy, then under the rule of the Ostrogoths. This includes accounts of the sieges of Naples and Rome.
Later, Procopius added an eighth book ("Wars VIII" or "Gothic War IV"), which brings the history to 552/553, when a Roman army led by the eunuch Narses finally destroyed the Ostrogothic kingdom. This eighth book covers campaigns both in Italy and on the Eastern frontier.
The "Wars of Justinian" was influential on later Byzantine history-writing. A continuation of Procopius' work was written after his death by the poet and historian Agathias of Myrina.
"Secret History".
The famous "Secret History" ( "Apókryphe Historía", ) was discovered centuries later in the Vatican Library and published by Niccolò Alamanni in 1623 at Lyons. Its existence was already known from the "Suda", which referred to it as the "Anekdota" (, Latin "Anecdota", "unpublished writings"). The "Secret History" covers roughly the same years as the first seven books of the "History of Justinian's Wars" and appears to have been written after they were published. Current consensus generally dates it to 550 or 558, or maybe even as late as 562.
In the eyes of many scholars, the "Secret History" reveals an author who had become deeply disillusioned with the emperor Justinian and his wife, Empress Theodora, as well as Belisarius, his former commander and patron, and Antonina, Belisarius' wife. The anecdotes claim to expose the secret springs of their public actions, as well as the private lives of the emperor, his wife and their entourage. Justinian is portrayed as cruel, venal, prodigal and incompetent; as for Theodora, the reader is treated to the most detailed and titillating portrayals of vulgarity and insatiable lust combined with shrewish and calculating mean-spiritedness. However, it has been argued that Procopius knew that a conspiracy could overthrow the imperial power, and therefore prepared an exaggerated document in order to clear himself of all accusations of proximity with the future-former imperial power; if this hypothesis is correct, the "Secret History" cannot be seen as proof that Procopius hated Justinian and Theodora.
Among the more titillating (and doubtful) revelations in the "Secret History" is Procopius' account of Theodora's thespian accomplishments:
Her husband Justinian, meanwhile, was a monster whose head could suddenly vanish—at least according to this passage:
"The Buildings of Justinian".
The "Buildings of Justinian" ( "Perì Ktismáton", , "On Buildings") is a panegyric on Justinian's building activity in the empire. The first book may date to before the collapse of the first dome of Hagia Sophia in 557, but some scholars think that it is possible that the work postdates the building of the bridge over the Sangarius in the late 550s. The "Peri ktismaton" (or "De Aedificiis") tells us nothing further about Belisarius, but it takes a sharply different attitude towards Justinian. He is presented as an idealised Christian emperor who built churches for the glory of God and defenses for the safety of his subjects and who showed particular concern for the water supply. He built new aqueducts as well as restoring those that had fallen into disuse.
Historians consider "Buildings" to be an incomplete work, due to evidence of the surviving version being a draft with two possible redactions.
Theodora, who was dead when this panegyric was written, is mentioned only briefly, but Procopius' praise of her beauty is fulsome. The panegyric was likely written at Justinian's behest, however, and it is doubtful that the sentiments expressed are sincere.
Due to the panegyrical nature of the "The Buildings", historians have discovered in several occasions discrepancies between claims made by Procopius and other primary sources. A primary example is in Procopius starting the reign of Justinian in 518, which was actually the start of the reign of Justin I, Justinian's predecessor and uncle. This discrepancy can be seen as part of Procopius' panegyric method, as it allowed him to credit buildings constructed under the rule Justin I as Justinian's accomplishments. In this context can be mentioned the renovations to the walls of Edessa after a flood in 525, along with several churches in the region, all of which were completed under Justinian's uncle. Similarly, Procopius falsely credits Justinian for the extensive re-fortifications made in the cities of Tomis and Histria in Scythia Minor, along the Danubian frontier, actual accomplishments of Anastasius I, predecessor of Justin I.
Context.
Procopius belongs to the school of late antique secular historians who continued the traditions of the Second Sophistic; they wrote in Attic Greek, their models were Herodotus and especially Thucydides, and their subject matter was secular history. They avoided vocabulary unknown to Attic Greek and inserted an explanation when they had to use contemporary words. Thus Procopius explains to his readers that "ekklesia", meaning a Christian church, is the equivalent of a temple or shrine and that monks are "the most temperate of Christians ... whom men are accustomed to call monks". ("Wars" 2.9.14; 1.7.22) In classical Athens, monks had been unknown and an "ekklesia" was the assembly of Athenian citizens that passed the laws.
The secular historians eschewed the history of the Christian church, which they left to ecclesiastical history—a genre that was founded by Eusebius of Caesarea. However, Averil Cameron has argued convincingly that Procopius' works reflect the tensions between the classical and Christian models of history in 6th century Byzantium. This is supported by Mary Whitby's analysis of Procopius' depiction of Constantinople and the Church of Hagia Sophia in comparison to contemporary pagan panegyrics ("Buildings", Book I). Procopius can be seen as depicting Justinian as essentially God's Vicegerent, making the case for buildings being a primarily religious panegyric.
Procopius indicated ("Secret History" 26.18) that he planned to write an ecclesiastical history himself and, if he had, he would probably have followed the rules of that genre. But, as far as it is known, the ecclesiastical history remained unwritten.
A number of historical novels based on Procopius' works (along with other sources) have been written, one of which, "Count Belisarius", was written by poet and novelist Robert Graves in 1938. Procopius himself appears as a minor character in Felix Dahn's "A Struggle for Rome" and in L. Sprague de Camp's alternate history novel "Lest Darkness Fall". The novel's main character, archaeologist Martin Padway, derives most of his knowledge of historical events from the "Secret History".

</doc>
<doc id="23626" url="https://en.wikipedia.org/wiki?curid=23626" title="Property">
Property

In the abstract, property is that which belongs to or with something, whether as an attribute or as a component of said thing. In the context of this article, property is one or more components (rather than attributes), whether physical or incorporeal, of a person's estate; or so belonging to, as in being owned by, a person or jointly a group of people or a legal entity like a corporation or even a society. (Given such meaning, the word property is uncountable, and as such, is not described with an indefinite article or as plural.) Depending on the nature of the property, an owner of property has the right to consume, alter, share, redefine, rent, mortgage, pawn, sell, exchange, transfer, give away or destroy it, or to exclude others from doing these things, as well as to perhaps abandon it; whereas regardless of the nature of the property, the owner thereof has the right to properly use it (as a durable, mean or factor, or whatever), or at the very least exclusively keep it.
In economics and political economy, there are three broad forms of property: private property, public property, and collective property (also called cooperative property).
Property that jointly belongs to more than one party may be possessed or controlled thereby in very similar or very distinct ways, whether simply or complexly, whether equally or unequally. However, there is an expectation that each party's will (rather discretion) with regard to the property be clearly defined and unconditional, so as to distinguish ownership and easement from rent. The parties might expect their wills to be unanimous, or alternately every given one of them, when no opportunity for or possibility of dispute with any other of them exists, may expect his, her, its or their own will to be sufficient and absolute.
The Restatement (First) of Property defines property as anything, tangible or intangible whereby a legal relationship between persons and the state enforces a possessory interest or legal title in that thing. This mediating relationship between individual, property and state is called a property regime.
In sociology and anthropology, property is often defined as a relationship between two or more individuals and an object, in which at least one of these individuals holds a bundle of rights over the object. The distinction between "collective property" and "private property" is regarded as a confusion since different individuals often hold differing rights over a single object.
Important widely recognized types of property include real property (the combination of land and any improvements to or on the land), personal property (physical possessions belonging to a person), private property (property owned by legal persons, business entities or individual natural persons), public property (state owned or publicly owned and available possessions) and intellectual property (exclusive rights over artistic creations, inventions, etc.), although the last is not always as widely recognized or enforced. An article of property may have physical and incorporeal parts. A title, or a right of ownership, establishes the relation between the property and other persons, assuring the owner the right to dispose of the property as the owner sees fit.
Overview.
Often property is defined by the code of the local sovereignty, and protected wholly or more usually partially by such entity, the owner being responsible for any remainder of protection. The standards of proof concerning proofs of ownerships are also addressed by the code of the local sovereignty, and such entity plays a role accordingly, typically somewhat managerial. Some philosophers assert that property rights arise from social convention, while others find justifications for them in morality or in natural law.
Property, in the first instance, is a thing-in-itself. When a person finds a thing and takes that thing into that person's possession and control, then that thing becomes a thing-for-you for that person. Once the person has that thing in that person's possession, that thing becomes that person's property by reason of discovery and conquest, and that person has the individual right to defend that property (property interest) against all others by reason of self-help. Typically, persons join together to form a political state which may develop a formal legal system which enforces and protects property rights so that the individual can go to court to get protection and enforcement of that person's property rights, rather than having to use self-help. It is possible that when a person has constructive possession of personal property, but another person has actual possession, then the person having constructive possession has bare legal title, while the other person has actual possession. Generally, the ground and any buildings which are permanently attached are considered real property, while movable goods and intangibles such as a copyright are considered personal property. Also, property cannot be considered a reified concept, because in the first instance, property is very concrete as a physical thing-in-itself.
Various scholarly disciplines (such as law, economics, anthropology or sociology) may treat the concept more systematically, but definitions vary, most particularly when involving contracts. Positive law defines such rights, and the judiciary can adjudicate and enforce property rights.
According to Adam Smith, the expectation of profit from "improving one's stock of capital" rests on private property rights. Capitalism has as a central assumption that property rights encourage their holders to develop the property, generate wealth, and efficiently allocate resources based on the operation of markets. From this has evolved the modern conception of property as a right enforced by positive law, in the expectation that this will produce more wealth and better standards of living. However, Smith also expressed a very critical view on the effects of property laws on inequality:
In his text "The Common Law", Oliver Wendell Holmes describes property as having two fundamental aspects. The first, possession, can be defined as control over a resource based on the practical inability of another to contradict the ends of the possessor. The second, title, is the expectation that others will recognize rights to control resource, even when it is not in possession. He elaborates the differences between these two concepts, and proposes a history of how they came to be attached to persons, as opposed to families or to entities such as the church.
Both communism and some kinds of socialism have also upheld the notion that private ownership of capital is inherently illegitimate. This argument centers mainly on the idea that private ownership of capital always benefits one class over another, giving rise to domination through the use of this privately owned capital. Communists do not oppose personal property that is "hard-won, self-acquired, self-earned" (as the Communist Manifesto puts it) by members of the proletariat. Both socialism and communism distinguish carefully between private ownership of capital (land, factories, resources, etc.) and private property (homes, material objects and so forth).
Types of property.
Most legal systems distinguish between different types of property, especially between land (immovable property, estate in land, real estate, real property) and all other forms of property—goods and chattels, movable property or personal property, including the value of legal tender if not the legal tender itself, as the manufacturer rather than the possessor might be the owner. They often distinguish tangible and intangible property. One categorization scheme specifies three species of property: land, improvements (immovable man-made things), and personal property (movable man-made things).
Dead Investment property, it is a property from which you cannot derive any profits. Scenario the property might be not in a suitable condition to use or cannot be used to earn profits. Investors also say that the properties where we stay are Dead Investments.
In common law, real property (immovable property) is the combination of interests in land and improvements thereto, and personal property is interest in movable property. Real property rights are rights relating to the land. These rights include ownership and usage. Owners can grant rights to persons and entities in the form of leases, licenses and easements.
Throughout the last centuries of the second millennium, with the development of more complex theories of property, the concept of personal property had become divided into tangible property (such as cars and clothing) and intangible property (such as financial instruments—including stocks and bonds—intellectual property—including patents, copyrights and trademarks—digital files, communication channels, and certain forms of identifier—including Internet domain names, some forms of network address, some forms of handle and again trademarks).
Treatment of intangible property is such that an article of property is, by law or otherwise by traditional conceptualization, subject to expiration even when inheritable, which is a key distinction from tangible property. Upon expiration, the property, if of the intellectual category, becomes a part of public domain, to be used by but not owned by anybody, and possibly used by more than one party simultaneously due the inapplicability of scarcity to intellectual property. Whereas things such as communications channels and pairs of electromagnetic spectrum band and signal transmission power can only be used by a single party at a time, or a single party in a divisible context, if owned or used at all. Thus far or usually those are not considered property, or at least not private property, even though the party bearing right of exclusive use may transfer that right to another.
Related concepts.
Of the following, only sale and at-will sharing involve no encumbrance.
Issues in property theory.
What can be property?
The two major justifications given for original property, or the homestead principle, are effort and scarcity. John Locke emphasized effort, "mixing your labor" with an object, or clearing and cultivating virgin land. Benjamin Tucker preferred to look at the telos of property, i.e. What is the purpose of property? His answer: to solve the scarcity problem. Only when items are relatively scarce with respect to people's desires do they become property. For example, hunter-gatherers did not consider land to be property, since there was no shortage of land. Agrarian societies later made arable land property, as it was scarce. For something to be economically scarce it must necessarily have the "exclusivity property"—that use by one person excludes others from using it. These two justifications lead to different conclusions on what can be property. Intellectual property—incorporeal things like ideas, plans, orderings and arrangements (musical compositions, novels, computer programs)—are generally considered valid property to those who support an effort justification, but invalid to those who support a scarcity justification, since the things don't have the exclusivity property (however, those who support a scarcity justification may still support other "intellectual property" laws such as Copyright, as long as these are a subject of contract instead of government arbitration). Thus even ardent propertarians may disagree about IP. By either standard, one's body is one's property.
From some anarchist points of view, the validity of property depends on whether the "property right" requires enforcement by the state. Different forms of "property" require different amounts of enforcement: intellectual property requires a great deal of state intervention to enforce, ownership of distant physical property requires quite a lot, ownership of carried objects requires very little, while ownership of one's own body requires absolutely no state intervention. Some anarchists don't believe in property at all.
Many things have existed that did not have an owner, sometimes called the commons. The term "commons," however, is also often used to mean something quite different: "general collective ownership"—i.e. common ownership. Also, the same term is sometimes used by statists to mean government-owned property that the general public is allowed to access (public property). Law in all societies has tended to develop towards reducing the number of things not having clear owners. Supporters of property rights argue that this enables better protection of scarce resources, due to the tragedy of the commons, while critics argue that it leads to the 'exploitation' of those resources for personal gain and that it hinders taking advantage of potential network effects. These arguments have differing validity for different types of "property"—things that are not scarce are, for instance, not subject to the tragedy of the commons. Some apparent critics advocate general collective ownership rather than ownerlessness.
Things that do not have owners include: ideas (except for intellectual property), seawater (which is, however, protected by anti-pollution laws), parts of the seafloor (see the United Nations Convention on the Law of the Sea for restrictions), gases in Earth's atmosphere, animals in the wild (although in most nations, animals are tied to the land. In the United States and Canada wildlife are generally defined in statute as property of the state. This public ownership of wildlife is referred to as the North American Model of Wildlife Conservation and is based on The Public Trust Doctrine.), celestial bodies and outer space, and land in Antarctica.
The nature of children under the age of majority is another contested issue here. In ancient societies children were generally considered the property of their parents. Children in most modern societies theoretically own their own bodies but are not considered competent to exercise their rights, and their parents or guardians are given most of the actual rights of control over them.
Questions regarding the nature of ownership of the body also come up in the issue of abortion, drugs and euthanasia.
In many ancient legal systems (e.g. early Roman law), religious sites (e.g. temples) were considered property of the God or gods they were devoted to. However, religious pluralism makes it more convenient to have religious sites owned by the religious body that runs them.
Intellectual property and air (airspace, no-fly zone, pollution laws, which can include tradable emissions rights) can be property in some senses of the word.
Ownership of land can be held separately from the ownership of rights over that land, including sporting rights, mineral rights, development rights, air rights, and such other rights as may be worth segregating from simple land ownership.
Who can be an owner?
Ownership laws may vary widely among countries depending on the nature of the property of interest (e.g. firearms, real property, personal property, animals). Persons can own property directly. In most societies legal entities, such as corporations, trusts and nations (or governments) own property.
In many countries women have limited access to property following restrictive inheritance and family laws, under which only men have actual or formal rights to own property.
In the Inca empire, the dead emperors, who were considered gods, still controlled property after death.
Whether and to what extent the state may interfere with property.
Under United States law the principal limitations on whether and the extent to which the State may interfere with property rights are set by the Constitution. The "Takings" clause requires that the government (whether state or federal—for the 14th Amendment's due process clause imposes the 5th Amendment's takings clause on state governments) may take private property only for a public purpose, after exercising due process of law, and upon making "just compensation." If an interest is not deemed a "property" right or the conduct is merely an intentional tort, these limitations do not apply and the doctrine of sovereign immunity precludes relief. Moreover, if the interference does not almost completely make the property valueless, the interference will not be deemed a taking but instead a mere regulation of use. On the other hand, some governmental regulations of property use have been deemed so severe that they have been considered "regulatory takings." Moreover, conduct sometimes deemed only a nuisance or other tort has been held a taking of property where the conduct was sufficiently persistent and severe.
Theories of property.
There exist many theories of property. One is the relatively rare first possession theory of property, where ownership of something is seen as justified simply by someone seizing something before someone else does. Perhaps one of the most popular is the natural rights definition of property rights as advanced by John Locke. Locke advanced the theory that God granted dominion over nature to man through Adam in the book of Genesis. Therefore, he theorized that when one mixes one’s labor with nature, one gains a relationship with that part of nature with which the labor is mixed, subject to the limitation that there should be "enough, and as good, left in common for others." (see Lockean proviso)
From the RERUM NOVARUM, Pope Leo XIII wrote "It is surely undeniable that, when a man engages in remunerative labor, the impelling reason and motive of his work is to obtain property, and thereafter to hold it as his very own."
Anthropology studies the diverse systems of ownership, rights of use and transfer, and possession under the term "theories of property." Western legal theory is based, as mentioned, on the owner of property being a legal person. However, not all property systems are founded on this basis.
In every culture studied ownership and possession are the subject of custom and regulation, and "law" where the term can meaningfully be applied. Many tribal cultures balance individual ownership with the laws of collective groups: tribes, families, associations and nations. For example, the 1839 Cherokee Constitution frames the issue in these terms:
Communal property systems describe ownership as belonging to the entire social and political unit. Such arrangements can under certain conditions erode open access resources. This development has been critiqued by the tragedy of the commons.
Corporate systems describe ownership as being attached to an identifiable group with an identifiable responsible individual. The Roman property law was based on such a corporate system.
Different societies may have different theories of property for differing types of ownership. Pauline Peters argued that property systems are not isolable from the social fabric, and notions of property may not be stated as such, but instead may be framed in negative terms: for example the taboo system among Polynesian peoples.
Property in philosophy.
In medieval and Renaissance Europe the term "property" essentially referred to land. After much rethinking, land has come to be regarded as only a special case of the property genus. This rethinking was inspired by at least three broad features of early modern Europe: the surge of commerce, the breakdown of efforts to prohibit interest (then called "usury"), and the development of centralized national monarchies.
Ancient philosophy.
Urukagina, the king of the Sumerian city-state Lagash, established the first laws that forbade compelling the sale of property.
The Ten Commandments shown in Exodus 20:2-17 and Deuteronomy 5:6-21 stated that the Israelites were not to steal, but the connection between Bronze Age concepts of theft and modern concepts of property is suspect.
Aristotle, in "Politics," advocates "private property." He argues that self-interest leads to neglect of the commons. "hat which is common to the greatest number has the least care bestowed upon it. Every one thinks chiefly of his own, hardly at all of the common interest; and only when he is himself concerned as an individual."
In addition he says that when property is common, there are natural problems that arise due to differences in labor: "If they do not share equally enjoyments and toils, those who labor much and get little will necessarily complain of those who labor little and receive or consume much. But indeed there is always a difficulty in men living together and having all human relations in common, but especially in their having common property." ("Politics, 1261b34")
Cicero held that there is no private property under natural law but only under human law. Seneca viewed property as only becoming necessary when men become avarice. St. Ambrose latter adopted this view and St. Augustine even derided heretics for complaining the Emperor could not confiscate property they had labored for.
Medieval philosophy.
Thomas Aquinas (13th century).
The canon law "Decretum Gratiani" maintained that mere human law creates property, repeating the phrases used by St. Augustine. St. Thomas Aquinas agreed with regard to the private consumption of property but modified patristic theory in finding that the private possession of property is necessary. Thomas Aquinas concludes that, given certain detailed provisions,
Modern philosophy.
Thomas Hobbes (17th century).
The principal writings of Thomas Hobbes appeared between 1640 and 1651—during and immediately following the war between forces loyal to King Charles I and those loyal to Parliament. In his own words, Hobbes' reflection began with the idea of "giving to every man his own," a phrase he drew from the writings of Cicero. But he wondered: How can anybody call anything his own? He concluded: My own can only truly be mine if there is one unambiguously strongest power in the realm, and that power treats it as mine, protecting its status as such.
James Harrington (17th century).
A contemporary of Hobbes, James Harrington, reacted to the same tumult in a different way: he considered property natural but not inevitable. The author of "Oceana", he may have been the first political theorist to postulate that political power is a consequence, not the cause, of the distribution of property. He said that the worst possible situation is one in which the commoners have half a nation's property, with crown and nobility holding the other half—a circumstance fraught with instability and violence. A much better situation (a stable republic) will exist once the commoners own most property, he suggested.
In later years, the ranks of Harrington's admirers included American revolutionary and founder John Adams.
Robert Filmer (17th century).
Another member of the Hobbes/Harrington generation, Sir Robert Filmer, reached conclusions much like Hobbes', but through Biblical exegesis. Filmer said that the institution of kingship is analogous to that of fatherhood, that subjects are but children, whether obedient or unruly, and that property rights are akin to the household goods that a father may dole out among his children—his to take back and dispose of according to his pleasure.
John Locke (17th century).
In the following generation, John Locke sought to answer Filmer, creating a rationale for a balanced constitution in which the monarch had a part to play, but not an overwhelming part. Since Filmer's views essentially require that the Stuart family be uniquely descended from the patriarchs of the Bible, and since even in the late 17th century that was a difficult view to uphold, Locke attacked Filmer's views in his First Treatise on Government, freeing him to set out his own views in the Second Treatise on Civil Government. Therein, Locke imagined a pre-social world, each of the unhappy residents of which are willing to create a social contract because otherwise "the enjoyment of the property he has in this state is very unsafe, very unsecure," and therefore the "great and chief end, therefore, of men's uniting into commonwealths, and putting themselves under government, is the preservation of their property." They would, he allowed, create a monarchy, but its task would be to execute the will of an elected legislature. "To this end" (to achieve the previously specified goal), he wrote, "it is that men give up all their natural power to the society they enter into, and the community put the legislative power into such hands as they think fit, with this trust, that they shall be governed by declared laws, or else their peace, quiet, and property will still be at the same uncertainty as it was in the state of nature."
Even when it keeps to proper legislative form, though, Locke held that there are limits to what a government established by such a contract might rightly do.
"It cannot be supposed that hypothetical contractors they should intend, had they a power so to do, to give any one or more an absolute arbitrary power over their persons and estates, and put a force into the magistrate's hand to execute his unlimited will arbitrarily upon them; this were to put themselves into a worse condition than the state of nature, wherein they had a liberty to defend their right against the injuries of others, and were upon equal terms of force to maintain it, whether invaded by a single man or many in combination. Whereas by supposing they have given up themselves to the absolute arbitrary power and will of a legislator, they have disarmed themselves, and armed him to make a prey of them when he pleases..."
Note that both "persons "and" estates" are to be protected from the arbitrary power of any magistrate, inclusive of the "power and will of a legislator." In Lockean terms, depredations against an estate are just as plausible a justification for resistance and revolution as are those against persons. In neither case are subjects required to allow themselves to become prey.
To explain the ownership of property Locke advanced a labor theory of property.
David Hume (18th century).
In contrast to the figures discussed in this section thus far David Hume lived a relatively quiet life that had settled down to a relatively stable social and political structure. He lived the life of a solitary writer until 1763 when, at 52 years of age, he went off to Paris to work at the British embassy.
In contrast, one might think, to his polemical works on religion and his empiricism-driven skeptical epistemology, Hume's views on law and property were quite conservative.
He did not believe in hypothetical contracts, or in the love of mankind in general, and sought to ground politics upon actual human beings as one knows them. "In general," he wrote, "it may be affirmed that there is no such passion in human mind, as the love of mankind, merely as such, independent of personal qualities, or services, or of relation to ourselves." Existing customs should not lightly be disregarded, because they have come to be what they are as a result of human nature. With this endorsement of custom comes an endorsement of existing governments, because he conceived of the two as complementary: "A regard for liberty, though a laudable passion, ought commonly to be subordinate to a reverence for established government."
Therefore, Hume's view was that there are property rights because of and to the extent that the existing law, supported by social customs, secure them. He offered some practical home-spun advice on the general subject, though, as when he referred to avarice as "the spur of industry," and expressed concern about excessive levels of taxation, which "destroy industry, by engendering despair."
Adam Smith.
"The property which every man has in his own labour, as it is the original foundation of all other property, so it is the most sacred and inviolable. The patrimony of a poor man lies in the strength and dexterity of his hands; and to hinder him from employing this strength and dexterity in what manner he thinks proper without injury to his neighbour, is a plain violation of this most sacred property. It is a manifest encroachment upon the just liberty both of the workman, and of those who might be disposed to employ him. As it hinders the one from working at what he thinks proper, so it hinders the others from employing whom they think proper. To judge whether he is fit to be employed, may surely be trusted to the discretion of the employers whose interest it so much concerns. The affected anxiety of the law-giver lest they should employ an improper person, is evidently as impertinent as it is oppressive." 
- (Source: Adam Smith, "The Wealth of Nations", 1776, Book I, Chapter X, Part II.)
By the mid 19th century, the industrial revolution had transformed England and the United States, and had begun in France. The established conception of what constitutes property expanded beyond land to encompass scarce goods in general. In France, the revolution of the 1790s had led to large-scale confiscation of land formerly owned by church and king. The restoration of the monarchy led to claims by those dispossessed to have their former lands returned.
Karl Marx.
Section VIII, "Primitive Accumulation" of Capital involves a critique of Liberal Theories of property rights. Marx notes that under Feudal Law, peasants were legally as entitled to their land as the aristocracy was to its manors. Marx cites several historical events in which large numbers of the peasantry were removed from their lands, which were then seized by the aristocracy. This seized land was then used for commercial ventures (sheep heading). Marx sees this "Primitive Accumulation as integral to the creation of English Capitalism. This event created a large unlanded class which had to work for wages in order to survive. Marx asserts that Liberal theories of property are "idyllic" fairy tales that hide a violent historical process.
Charles Comte – legitimate origin of property.
Charles Comte, in "Traité de la propriété" (1834), attempted to justify the legitimacy of private property in response to the Bourbon Restoration. According to David Hart, Comte had three main points: "firstly, that interference by the state over the centuries in property ownership has had dire consequences for justice as well as for economic productivity; secondly, that property is legitimate when it emerges in such a way as not to harm anyone; and thirdly, that historically some, but by no means all, property which has evolved has done so legitimately, with the implication that the present distribution of property is a complex mixture of legitimately and illegitimately held titles."
Comte, as Proudhon later did, rejected Roman legal tradition with its toleration of slavery. He posited a communal "national" property consisting of non-scarce goods, such as land in ancient hunter-gatherer societies. Since agriculture was so much more efficient than hunting and gathering, private property appropriated by someone for farming left remaining hunter-gatherers with more land per person, and hence did not harm them. Thus this type of land appropriation did not violate the Lockean proviso – there was "still enough, and as good left." Comte's analysis would be used by later theorists in response to the socialist critique on property.
Pierre Proudhon – property is theft.
In his 1849 treatise "What is Property?", Pierre Proudhon answers with "Property is theft!" In natural resources, he sees two types of property, "de jure" property (legal title) and "de facto" property (physical possession), and argues that the former is illegitimate. Proudhon's conclusion is that "property, to be just and possible, must necessarily have equality for its condition."
His analysis of the product of labor upon natural resources as property (usufruct) is more nuanced. He asserts that land itself cannot be property, yet it should be held by individual possessors as stewards of mankind with the product of labor being the property of the producer. Proudhon reasoned that any wealth gained without labor was stolen from those who labored to create that wealth. Even a voluntary contract to surrender the product of labor to an employer was theft, according to Proudhon, since the controller of natural resources had no moral right to charge others for the use of that which he did not labor to create and therefore did not own.
Proudhon's theory of property greatly influenced the budding socialist movement, inspiring anarchist theorists such as Mikhail Bakunin who modified Proudhon's ideas, as well as antagonizing theorists like Karl Marx.
Frédéric Bastiat – property is value.
Frédéric Bastiat's main treatise on property can be found in chapter 8 of his book "Economic Harmonies" (1850). In a radical departure from traditional property theory, he defines property not as a physical object, but rather as a relationship between people with respect to an object. Thus, saying one owns a glass of water is merely verbal shorthand for "I may justly gift or trade this water to another person". In essence, what one owns is not the object but the value of the object. By "value," Bastiat apparently means "market value"; he emphasizes that this is quite different from utility. ""In our relations with one another, we are not owners of the utility of things, but of their value, and value is the appraisal made of reciprocal services.""
Bastiat theorized that, as a result of technological progress and the division of labor, the stock of communal wealth increases over time; that the hours of work an unskilled laborer expends to buy e.g. 100 liters of wheat decreases over time, thus amounting to "gratis" satisfaction. Thus, private property continually destroys itself, becoming transformed into communal wealth. The increasing proportion of communal wealth to private property results in a tendency toward equality of mankind. ""Since the human race started from the point of greatest poverty, that is, from the point where there were the most obstacles to be overcome, it is clear that all that has been gained from one era to the next has been due to the spirit of property.""
This transformation of private property into the communal domain, Bastiat points out, does not imply that private property will ever totally disappear. This is because man, as he progresses, continually invents new and more sophisticated needs and desires.
Andrew J. Galambos – A Precise Definition of Property.
Andrew J. Galambos (1924–1997) was an astrophysicist and philosopher who innovated a social structure that seeks to maximize human peace and freedom. Galambos’ concept of property was basic to his philosophy. He defined property as a man’s life and all non-procreative derivatives of his life. (Because the English language is deficient in omitting the feminine from “man” when referring to humankind, it is implicit and obligatory that the feminine is included in the term “man”.)
Galambos taught that property is essential to a non-coercive social structure. That is why he defined freedom as follows: “Freedom is the societal condition that exists when every individual has full (100%) control over his own property.” Galambos defines property as having the following elements:
Property includes all non-procreative derivatives of an individual’s life; this means children are not the property of their parents. and "primary property" (a person's own ideas).
Galambos emphasized repeatedly that true government exists to protect property and that the state attacks property.
For example, the state requires payment for its services in the form of taxes whether or not people desire such services. Since an individual’s money is his property, the confiscation of money in the form of taxes is an attack on property. Military conscription is likewise an attack on a person’s primordial property.
Contemporary views.
Contemporary political thinkers who believe that natural persons enjoy rights to own property and to enter into contracts espouse two views about John Locke. On the one hand, some admire Locke, such as W.H. Hutt (1956), who praised Locke for laying down the "quintessence of individualism". On the other hand, those such as Richard Pipes regard Locke's arguments as weak, and think that undue reliance thereon has weakened the cause of individualism in recent times. Pipes has written that Locke's work "marked a regression because it rested on the concept of Natural Law" rather than upon Harrington's sociological framework.
Hernando de Soto has argued that an important characteristic of capitalist market economy is the functioning state protection of property rights in a formal property system which clearly records ownership and transactions. These property rights and the whole formal system of property make possible:
All of the above, according to de Soto, enhance economic growth.
See also.
Property-giving (legal)
Property-taking (legal)
Property-taking (illegal)
Exclusive or discretionary relational constructs

</doc>
<doc id="23627" url="https://en.wikipedia.org/wiki?curid=23627" title="Police">
Police

A police force is a constituted body of persons empowered by the state to enforce the law, protect property, and limit civil disorder. Their powers include the legitimized use of force. The term is most commonly associated with police services of a sovereign state that are authorized to exercise the police power of that state within a defined legal or territorial area of responsibility. Police forces are often defined as being separate from military or other organizations involved in the defense of the state against foreign aggressors; however, gendarmerie are military units charged with civil policing.
Law enforcement, however, constitutes only part of policing activity. Policing has included an array of activities in different situations, but the predominant ones are concerned with the preservation of order. In some societies, in the late 18th and early 19th centuries, these developed within the context of maintaining the class system and the protection of private property. Many police forces suffer from police corruption to a greater or lesser degree. The police force is usually a public sector service, meaning they are paid through taxes.
Alternative names for police force include constabulary, gendarmerie, police department, police service, crime prevention, protective services, law enforcement agency, civil guard or civic guard. Members may be referred to as police officers, troopers, sheriffs, constables, rangers, peace officers or civic/civil guards.
As police are often interacting with individuals, slang terms are numerous. Many slang terms for police officers are decades or centuries old with lost etymology.
Etymology.
First attested in English in the early 15th century, initially in a range of senses encompassing '(public) policy; state; public order', the word "police" comes from Middle French "police" ('public order, administration, government'), in turn from Latin "politia", which is the Latinisation of the Greek πολιτεία ("politeia"), "citizenship, administration, civil polity". This is derived from πόλις ("polis"), "city".
History.
Ancient policing.
Law enforcement in Ancient China was carried out by "prefects" for thousands of years since it developed in both the Chu and Jin kingdoms of the Spring and Autumn period. In Jin, dozens of prefects were spread across the state, each having limited authority and employment period. They were appointed by local magistrates, who reported to higher authorities such as governors, who in turn were appointed by the emperor, and they oversaw the civil administration of their "prefecture", or jurisdiction. Under each prefect were "subprefects" who helped collectively with law enforcement in the area. Some prefects were responsible for handling investigations, much like modern police detectives. Prefects could also be women. The concept of the "prefecture system" spread to other cultures such as Korea and Japan.
In Ancient Greece, publicly owned slaves were used by magistrates as police. In Athens, a group of 300 Scythian slaves (the , "rod-bearers") was used to guard public meetings to keep order and for crowd control, and also assisted with dealing with criminals, handling prisoners, and making arrests. Other duties associated with modern policing, such as investigating crimes, were left to the citizens themselves.
In the Roman Empire, the Army, rather than a dedicated police organization, provided security. Local watchmen were hired by cities to provide some extra security. Magistrates such as "procurators fiscal" and "quaestors" investigated crimes. There was no concept of public prosecution, so victims of crime or their families had to organize and manage the prosecution themselves.
Under the reign of Augustus, when the capital had grown to almost one million inhabitants, 14 wards were created; the wards were protected by seven squads of 1,000 men called ""vigiles"", who acted as firemen and nightwatchmen. Their duties included apprehending thieves and robbers and capturing runaway slaves. The vigiles were supported by the Urban Cohorts who acted as a heavy-duty anti-riot force and even the Praetorian Guard if necessary.
Medieval policing.
In Medieval Spain, "hermandades", or "brotherhoods", peacekeeping associations of armed individuals, were a characteristic of municipal life, especially in Castile. As medieval Spanish kings often could not offer adequate protection, protective municipal leagues began to emerge in the 12th century against bandits and other rural criminals, and against the lawless nobility or to support one or another claimant to a crown.
These organizations were intended to be temporary, but became a long-standing fixture of Spain. The first recorded case of the formation of an "hermandad" occurred when the towns and the peasantry of the north united to police the pilgrim road to Santiago de Compostela in Galicia, and protect the pilgrims against robber knights.
Throughout the Middle Ages such alliances were frequently formed by combinations of towns to protect the roads connecting them, and were occasionally extended to political purposes. Among the most powerful was the league of North Castilian and Basque ports, the Hermandad de las marismas: Toledo, Talavera, and Villarreal.
As one of their first acts after end of the War of the Castilian Succession in 1479, Ferdinand and Isabella established the centrally organized and efficient "Holy Brotherhood" ("Santa Hermandad") as a national police force. They adapted an existing brotherhood to the purpose of a general police acting under officials appointed by themselves, and endowed with great powers of summary jurisdiction even in capital cases. The original brotherhoods continued to serve as modest local police-units until their final suppression in 1835.
The Fehmic courts of Germany provided some policing in the absence of strong state institutions.
In France during the Middle Ages, there were two Great Officers of the Crown of France with police responsibilities: The Marshal of France and the Constable of France. The military policing responsibilities of the Marshal of France were delegated to the Marshal's provost, whose force was known as the Marshalcy because its authority ultimately derived from the Marshal. The marshalcy dates back to the Hundred Years' 'War, and some historians trace it back to the early 12th century. Another organisation, the Constabulary (French: Connétablie), was under the command of the Constable of France. The constabulary was regularised as a military body in 1337. Under King Francis I (who reigned 1515–1547), the Maréchaussée was merged with the Constabulary. The resulting force was also known as the Maréchaussée, or, formally, the Constabulary and Marshalcy of France.
The English system of maintaining public order since the Norman conquest was a private system of tithings, led by a constable, which was based on a social obligation for the good conduct of the others; more common was that local lords and nobles were responsible for maintaining order in their lands, and often appointed a constable, sometimes unpaid, to enforce the law. There was also a system investigative "juries".
The Assize of Arms of 1252, which required the appointment of constables to summon men to arms, quell breaches of the peace, and to deliver offenders to the sheriffs or reeves, is cited as one of the earliest creation of the English police. The Statute of Winchester of 1285 is also cited as the primary legislation regulating the policing of the country between the Norman Conquest and the Metropolitan Police Act 1829.
From about 1500, private watchmen were funded by private individuals and organisations to carry out police functions. They were later nicknamed 'Charlies', probably after the reigning monarch King Charles II. Thief-takers were also rewarded for catching thieves and returning the stolen property.
The first use of the word police ("Polles") in English comes from the book "The Second Part of the Institutes of the Lawes of England" published in 1642.
Early modern policing.
The first centrally organised police force was created by the government of King Louis XIV in 1667 to police the city of Paris, then the largest city in Europe. The royal edict, registered by the "Parlement" of Paris on March 15, 1667 created the office of "lieutenant général de police" ("lieutenant general of police"), who was to be the head of the new Paris police force, and defined the task of the police as "ensuring the peace and quiet of the public and of private individuals, purging the city of what may cause disturbances, procuring abundance, and having each and everyone live according to their station and their duties".
This office was first held by Gabriel Nicolas de la Reynie, who had 44 "commissaires de police" (police commissioners) under his authority. In 1709, these commissioners were assisted by "inspecteurs de police" (police inspectors). The city of Paris was divided into 16 districts policed by the "commissaires", each assigned to a particular district and assisted by a growing bureaucracy. The scheme of the Paris police force was extended to the rest of France by a royal edict of October 1699, resulting in the creation of lieutenants general of police in all large French cities and towns.
After the French Revolution, Napoléon I reorganized the police in Paris and other cities with more than 5,000 inhabitants on February 17, 1800 as the Prefecture of Police. On March 12, 1829, a government decree created the first uniformed police in France, known as "sergents de ville" ("city sergeants"), which the Paris Prefecture of Police's website claims were the first uniformed policemen in the world.
In 1737, George II began paying some London and Middlesex watchmen with tax monies, beginning the shift to government control. In 1749 Henry Fielding began organizing a force of quasi-professional constables known as the Bow Street Runners. The Macdaniel affair added further impetus for a publicly salaried police force that did not depend on rewards. Nonetheless, In 1828, there were privately financed police units in no fewer than 45 parishes within a 10-mile radius of London.
The word "police" was borrowed from French into the English language in the 18th century, but for a long time it applied only to French and continental European police forces. The word, and the concept of police itself, were "disliked as a symbol of foreign oppression" (according to "Britannica 1911"). Before the 19th century, the first use of the word "police" recorded in government documents in the United Kingdom was the appointment of Commissioners of Police for Scotland in 1714 and the creation of the Marine Police in 1798.
Policing in London.
In 1797, Patrick Colquhoun was able to persuade the West Indies merchants who operated at the Pool of London on the River Thames, to establish a police force at the docks to prevent rampant theft that was causing annual estimated losses of £500,000 worth of cargo. The idea of a police, as it then existed in France, was considered as a potentially undesirable foreign import. In building the case for the police in the face of England's firm anti-police sentiment, Colquhoun framed the political rationale on economic indicators to show that a police dedicated to crime prevention was "perfectly congenial to the principle of the British constitution." Moreover, he went so far as to praise the French system, which had reached "the greatest degree of perfection" in his estimation. 
With the initial investment of £4,200, the new trial force of the Thames River Police began with about 50 men charged with policing 33,000 workers in the river trades, of whom Colquhoun claimed 11,000 were known criminals and "on the game." The force was a success after its first year, and his men had "established their worth by saving £122,000 worth of cargo and by the rescuing of several lives." Word of this success spread quickly, and the government passed the Marine Police Bill on 28 July 1800, transforming it from a private to public police agency; now the oldest police force in the world. Colquhoun published a book on the experiment, "The Commerce and Policing of the River Thames". It found receptive audiences far outside London, and inspired similar forces in other cities, notably, New York City, Dublin, and Sydney.
Colquhoun's utilitarian approach to the problem – using a cost-benefit argument to obtain support from businesses standing to benefit – allowed him to achieve what Henry and John Fielding failed for their Bow Street detectives. Unlike the stipendiary system at Bow Street, the river police were full-time, salaried officers prohibited from taking private fees. His other contribution was the concept of preventive policing; his police were to act as a highly visible deterrent to crime by their permanent presence on the Thames. Colquhoun's innovations were a critical development leading up to Robert Peel's "new" police three decades later.
Meanwhile, the authorities in Glasgow, Scotland successfully petitioned the government to pass the Glasgow Police Act establishing the City of Glasgow Police in 1800. Other Scottish towns soon followed suit and set up their own police forces through acts of parliament. In Ireland, the Irish Constabulary Act of 1822 marked the beginning of the Royal Irish Constabulary. The Act established a force in each barony with chief constables and inspectors general under the control of the civil administration at Dublin Castle. By 1841 this force numbered over 8,600 men.
Metropolitan police force.
London was fast reaching a size unprecedented in world history, due to the onset of the Industrial Revolution. It became clear that the locally maintained system of volunteer constables and "watchmen" was ineffective, both in detecting and preventing crime. A parliamentary committee was appointed to investigate the system of policing in London. Upon Sir Robert Peel being appointed as Home Secretary in 1822, he established a second and more effective committee, and acted upon its findings.
Royal Assent to the Metropolitan Police Act was given, and the Metropolitan Police Service was established on September 29, 1829 in London as the first modern and professional police force in the world.
Peel, widely regarded as the father of modern policing, was heavily influenced by the social and legal philosophy of Jeremy Bentham, who called for a strong and centralized, but politically neutral, police force for the maintenance of social order, for the protection of people from crime and to act as a visible deterrent to urban crime and disorder. Peel decided to standardise the police force as an official paid profession, to organise it in a civilian fashion, and to make it answerable to the public.
Due to public fears concerning the deployment of the military in domestic matters, Peel organised the force along civilian lines, rather than paramilitary. To appear neutral, the uniform was deliberately manufactured in blue, rather than red which was then a military colour, along with the officers being armed only with a wooden truncheon and a rattle to signal the need for assistance. Along with this, police ranks did not include military titles, with the exception of Sergeant.
To distance the new police force from the initial public view of it as a new tool of government repression, Peel publicised the so-called 'Peelian Principles', which set down basic guidelines for ethical policing:
The 1829 Metropolitan Police Act created a modern police force by limiting the purview of the force and its powers, and envisioning it as merely an organ of the judicial system. Their job was apolitical; to maintain the peace and apprehend criminals for the courts to process according to the law. This was very different to the 'Continental model' of the police force that had been developed in France, where the police force worked within the parameters of the absolutist state as an extension of the authority of the monarch and functioned as part of the governing state.
In 1863, the Metropolitan Police were issued with the distinctive Custodian helmet, and in 1884 they switched to the use of whistles that could be heard from much further away. The Metropolitan Police became a model for the police forces in most countries, such as the United States, and most of the British Empire. Bobbies can still be found in many parts of the Commonwealth of Nations.
Other countries.
Australia.
In Australia the first police force having centralised command as well as jurisdiction over an entire colony was the South Australia Police, formed in 1838 under Henry Inman.
However, whilst the New South Wales Police Force was established in 1862, it was made up from a large number of policing and military units operating within the then Colony of New South Wales and traces its links back to the Royal Marines. The passing of the Police Regulation Act of 1862 essentially tightly regulated and centralised all of the police forces operating throughout the Colony of New South Wales.
The New South Wales Police Force remains the largest police force in Australia in terms of personnel and physical resources. It is also the only police force that requires its recruits to undertake university studies at the recruit level and has the recruit pay for their own education.
Brazil.
In 1566, the first police investigator of Rio de Janeiro was recruited. By the 17th century, most captaincies already had local units with law enforcement functions. On July 9, 1775 a Cavalry Regiment was created in the state of Minas Gerais for maintaining law and order. In 1808, the Portuguese royal family relocated to Brazil, because of the French invasion of Portugal. King João VI established the "Intendência Geral de Polícia" (General Police Intendancy) for investigations. He also created a Royal Police Guard for Rio de Janeiro in 1809. In 1831, after independence, each province started organizing its local "military police", with order maintenance tasks. The Federal Railroad Police was created in 1852.
Canada.
In Canada, the Royal Newfoundland Constabulary was founded in 1729, making it the first police force in present-day Canada. It was followed in 1834 by the Toronto Police, and in 1838 by police forces in Montreal and Quebec City. A national force, the Dominion Police, was founded in 1868. Initially the Dominion Police provided security for parliament, but its responsibilities quickly grew. The famous Royal Northwest Mounted Police was founded in 1873.
The merger of these two police forces in 1920 formed the world-famous Royal Canadian Mounted Police.
Lebanon.
In Lebanon, modern police were established in 1861, with creation of the Gendarmerie.
India.
In India, the police is under the control of respective States and union territories and is known to be under State Police Services (SPS). The candidates selected for the SPS are usually posted as Deputy Superintendent of Police or Assistant Commissioner of Police once their probationary period ends. On prescribed satisfactory service in the SPS, the officers are nominated to the Indian Police Service. The service color is usually dark blue and red, while the uniform color is "Khaki".
United States.
In British North America, policing was initially provided by local elected officials. For instance, the New York Sheriff's Office was founded in 1626, and the Albany County Sheriff's Department in the 1660s. In the colonial period, policing was provided by elected sheriffs and local militias.
In 1789 the U.S. Marshals Service was established, followed by other federal services such as the U.S. Parks Police (1791) and U.S. Mint Police (1792). The first city police services were established in Philadelphia in 1751, Richmond, Virginia in 1807, Boston in 1838, and New York in 1845. The U.S. Secret Service was founded in 1865 and was for some time the main investigative body for the federal government.
In the American Old West, policing was often of very poor quality. The Army often provided some policing alongside poorly resourced sheriffs and temporarily organized posses. Public organizations were supplemented by private contractors, notably the Pinkerton National Detective Agency, which was hired by individuals, businessmen, local governments and the federal government. At its height, the Pinkerton Agency's numbers exceeded those of the United States Army.
In recent years, in addition to federal, state, and local forces, some special districts have been formed to provide extra police protection in designated areas. These districts may be known as neighborhood improvement districts, crime prevention districts, or security districts.
In 2005, the Supreme Court of the United States ruled that police do not have a constitutional duty to protect a person from harm.
Development of theory.
Michel Foucault claims that the contemporary concept of police as a paid and funded functionary of the state was developed by German and French legal scholars and practitioners in Public administration and Statistics in the 17th and early 18th centuries, most notably with Nicolas Delamare's "Traité de la Police" ("Treatise on the Police"), first published in 1705. The German "Polizeiwissenschaft" (Science of Police) first theorized by Philipp von Hörnigk a 17th-century Austrian Political economist and civil servant and much more famously by Johann Heinrich Gottlob Justi who produced an important theoretical work known as Cameral science on the formulation of police. Foucault cites Magdalene Humpert author of "Bibliographie der Kameralwissenschaften" (1937) in which the author makes note of a substantial bibliography was produced of over 4000 pieces of the practice of Polizeiwissenschaft however, this maybe a mistranslation of Foucault's own work the actual source of Magdalene Humpert states over 14,000 items were produced from the 16th century dates ranging from 1520-1850.
As conceptualized by the "Polizeiwissenschaft",according to Foucault the police had an administrative,economic and social duty ("procuring abundance"). It was in charge of demographic concerns and needed to be incorporated within the western political philosophy system of raison d'état and therefore giving the superficial appearance of empowering the population (and unwittingly supervising the population), which, according to mercantilist theory, was to be the main strength of the state. Thus, its functions largely overreached simple law enforcement activities and included public health concerns, urban planning (which was important because of the miasma theory of disease; thus, cemeteries were moved out of town, etc.), and surveillance of prices.
The concept of preventive policing, or policing to deter crime from taking place, gained influence in the late 18th century. Police Magistrate John Fielding, head of the Bow Street Runners, argued that "...it is much better to prevent even one man from being a rogue than apprehending and bringing forty to justice."
The Utilitarian philosopher, Jeremy Bentham, promoted the views of Italian Marquis Cesare Beccaria, and disseminated a translated version of "Essay on Crime in Punishment". Bentham espoused the guiding principle of "the greatest good for the greatest number:
Patrick Colquhoun's influential work, "A Treatise on the Police of the Metropolis" (1797) was heavily influenced by Benthamite thought. Colquhoun's Thames River Police was founded on these principles, and in contrast to the Bow Street Runners, acted as a deterrent by their continual presence on the riverfront, in addition to being able to intervene if they spotted a crime in progress.
Edwin Chadwick's 1829 article, "Preventive police" in the "London Review", argued that prevention ought to be the "primary" concern of a police body, which was not the case in practice. The reason, argued Chadwick, was that "A preventive police would act more immediately by placing difficulties in obtaining the objects of temptation." In contrast to a deterrent of punishment, a preventive police force would deter criminality by making crime cost-ineffective - "crime doesn't pay". In the second draft of his 1829 Police Act, the "object" of the new Metropolitan Police, was changed by Robert Peel to the "principal object," which was the "prevention of crime." Later historians would attribute the perception of England's "appearance of orderliness and love of public order" to the preventive principle entrenched in Peel's police system.
Development of modern police forces around the world was contemporary to the formation of the state, later defined by sociologist Max Weber as achieving a "monopoly on the legitimate use of physical force" and which was primarily exercised by the police and the military. Marxist theory situates the development of the modern state as part of the rise of capitalism, in which the police are one component of the bourgeoisie's repressive apparatus for subjugating the working class.
Personnel and organization.
Police forces include both preventive (uniformed) police and detectives. Terminology varies from country to country. Police functions include protecting life and property, enforcing criminal law, criminal investigations, regulating traffic, crowd control, and other public safety duties.
Uniformed police.
Preventive Police, also called Uniform Branch, Uniformed Police, Uniform Division, Administrative Police, Order Police, or Patrol, designates the police that patrol and respond to emergencies and other incidents, as opposed to detective services. As the name "uniformed" suggests, they wear uniforms and perform functions that require an immediate recognition of an officer's legal authority, such as traffic control, stopping and detaining motorists, and more active crime response and prevention.
Preventive police almost always make up the bulk of a police service's personnel. In Australia and Britain, patrol personnel are also known as "general duties" officers. Atypically, Brazil's preventive police are known as Military Police.
Detectives.
Police detectives are responsible for investigations and detective work. Detectives may be called Investigations Police, Judiciary/Judicial Police, and Criminal Police. In the UK, they are often referred to by the name of their department, the Criminal Investigation Department (CID). Detectives typically make up roughly 15%-25% of a police service's personnel.
Detectives, in contrast to uniformed police, typically wear 'business attire' in bureaucratic and investigative functions where a uniformed presence would be either a distraction or intimidating, but a need to establish police authority still exists. "Plainclothes" officers dress in attire consistent with that worn by the general public for purposes of blending in.
In some cases, police are assigned to work "undercover", where they conceal their police identity to investigate crimes, such as organized crime or narcotics crime, that are unsolvable by other means. In some cases this type of policing shares aspects with espionage.
Despite popular conceptions promoted by movies and television, many US police departments prefer not to maintain officers in non-patrol bureaus and divisions beyond a certain period of time, such as in the detective bureau, and instead maintain policies that limit service in such divisions to a specified period of time, after which officers must transfer out or return to patrol duties. This is done in part based upon the perception that the most important and essential police work is accomplished on patrol in which officers become acquainted with their beats, prevent crime by their presence, respond to crimes in progress, manage crises, and practice their skills.
Detectives, by contrast, usually investigate crimes after they have occurred and after patrol officers have responded first to a situation. Investigations often take weeks or months to complete, during which time detectives spend much of their time away from the streets, in interviews and courtrooms, for example. Rotating officers also promotes cross-training in a wider variety of skills, and serves to prevent "cliques" that can contribute to corruption or other unethical behavior.
Auxiliary.
Police may also take on auxiliary administrative duties, such as issuing firearms licenses. The extent that police have these functions varies among countries, with police in France, Germany, and other continental European countries handling such tasks to a greater extent than British counterparts.
Specialized units.
Specialized preventive and detective groups, or Specialist Investigation Departments exist within many law enforcement organizations either for dealing with particular types of crime, such as traffic law enforcement and crash investigation, homicide, or fraud; or for situations requiring specialized skills, such as underwater search, aviation, explosive device disposal ("bomb squad"), and computer crime.
Most larger jurisdictions also employ specially selected and trained quasi-military units armed with military-grade weapons for the purposes of dealing with particularly violent situations beyond the capability of a patrol officer response, including high-risk warrant service and barricaded suspects. In the United States these units go by a variety of names, but are commonly known as SWAT (Special Weapons And Tactics) teams.
In counterinsurgency-type campaigns, select and specially trained units of police armed and equipped as light infantry have been designated as police field forces who perform paramilitary-type patrols and ambushes whilst retaining their police powers in areas that were highly dangerous.
Because their situational mandate typically focuses on removing innocent bystanders from dangerous people and dangerous situations, not violent resolution, they are often equipped with non-lethal tactical tools like chemical agents, "flashbang" and concussion grenades, and rubber bullets. The London Metropolitan police's Specialist Firearms Command (CO19) is a group of armed police used in dangerous situations including hostage taking, armed robbery/assault and terrorism.
Military police.
Military police may refer to:
Religious police.
Some Islamic societies have religious police, who enforce the application of Islamic Sharia law. Their authority may include the power to arrest unrelated men and women caught socializing, anyone engaged in homosexual behavior or prostitution; to enforce Islamic dress codes, and store closures during Islamic prayer time.
They enforce Muslim dietary laws, prohibit the consumption or sale of alcoholic beverages and pork, and seize banned consumer products and media regarded as un-Islamic, such as CDs/DVDs of various Western musical groups, television shows and film. In Saudi Arabia, the Mutaween actively prevent the practice or proselytizing of non-Islamic religions within Saudi Arabia, where they are banned.
Varying jurisdictions.
Police forces are usually organized and funded by some level of government. The level of government responsible for policing varies from place to place, and may be at the national, regional or local level. In some places there may be multiple police forces operating in the same area, with different ones having jurisdiction according to the type of crime or other circumstances.
For example, in the UK, policing is primarily the responsibility of a regional police force; however specialist units exist at the national level. In the US, there is typically a state police force, but crimes are usually handled by local police forces that usually only cover a few municipalities. National agencies, such as the FBI, only have jurisdiction over federal crimes or those with an interstate component.
In addition to conventional urban or regional police forces, there are other police forces with specialized functions or jurisdiction. In the United States, the federal government has a number of police forces with their own specialized jurisdictions.
Some examples are the Federal Protective Service, which patrols and protects government buildings; the postal police, which protect postal buildings, vehicles and items; the Park Police, which protect national parks, or Amtrak Police which patrol Amtrak stations and trains.
There are also some government agencies that perform police functions in addition to other duties. The U.S. Coast Guard carries out many police functions for boaters.
In major cities, there may be a separate police agency for public transit systems, such as the New York City Port Authority Police or the MTA police, or for major government functions, such as sanitation, or environmental functions.
International policing.
The terms international policing, transnational policing, and/or global policing began to be used from the early 1990s onwards to describe forms of policing that transcended the boundaries of the sovereign nation-state (Nadelmann, 1993), (Sheptycki, 1995). These terms refer in variable ways to practices and forms for policing that, in some sense, transcend national borders. This includes a variety of practices, but international police cooperation, criminal intelligence exchange between police agencies working in different nation-states, and police development-aid to weak, failed or failing states are the three types that have received the most scholarly attention.
Historical studies reveal that policing agents have undertaken a variety of cross-border police missions for many years (Deflem, 2002). For example, in the 19th century a number of European policing agencies undertook cross-border surveillance because of concerns about anarchist agitators and other political radicals. A notable example of this was the occasional surveillance by Prussian police of Karl Marx during the years he remained resident in London. The interests of public police agencies in cross-border co-operation in the control of political radicalism and ordinary law crime were primarily initiated in Europe, which eventually led to the establishment of Interpol before the Second World War. There are also many interesting examples of cross-border policing under private auspices and by municipal police forces that date back to the 19th century (Nadelmann, 1993). It has been established that modern policing has transgressed national boundaries from time to time almost from its inception. It is also generally agreed that in the post–Cold War era this type of practice became more significant and frequent (Sheptycki, 2000).
Not a lot of empirical work on the practices of inter/transnational information and intelligence sharing has been undertaken. A notable exception is James Sheptycki's study of police cooperation in the English Channel region (2002), which provides a systematic content analysis of information exchange files and a description of how these transnational information and intelligence exchanges are transformed into police case-work. The study showed that transnational police information sharing was routinized in the cross-Channel region from 1968 on the basis of agreements directly between the police agencies and without any formal agreement between the countries concerned. By 1992, with the signing of the Schengen Treaty, which formalized aspects of police information exchange across the territory of the European Union, there were worries that much, if not all, of this intelligence sharing was opaque, raising questions about the efficacy of the accountability mechanisms governing police information sharing in Europe (Joubert and Bevers, 1996).
Studies of this kind outside of Europe are even rarer, so it is difficult to make generalizations, but one small-scale study that compared transnational police information and intelligence sharing practices at specific cross-border locations in North America and Europe confirmed that low visibility of police information and intelligence sharing was a common feature (Alain, 2001). Intelligence-led policing is now common practice in most advanced countries (Ratcliffe, 2007) and it is likely that police intelligence sharing and information exchange has a common morphology around the world (Ratcliffe, 2007). James Sheptycki has analyzed the effects of the new information technologies on the organization of policing-intelligence and suggests that a number of 'organizational pathologies' have arisen that make the functioning of security-intelligence processes in transnational policing deeply problematic. He argues that transnational police information circuits help to "compose the panic scenes of the security-control society". The paradoxical effect is that, the harder policing agencies work to produce security, the greater are feelings of insecurity.
Police development-aid to weak, failed or failing states is another form of transnational policing that has garnered attention. This form of transnational policing plays an increasingly important role in United Nations peacekeeping and this looks set to grow in the years ahead, especially as the international community seeks to develop the rule of law and reform security institutions in States recovering from conflict (Goldsmith and Sheptycki, 2007) With transnational police development-aid the imbalances of power between donors and recipients are stark and there are questions about the applicability and transportability of policing models between jurisdictions (Hills, 2009).
Perhaps the greatest question regarding the future development of transnational policing is: in whose interest is it? At a more practical level, the question translates into one about how to make transnational policing institutions democratically accountable (Sheptycki, 2004). For example, according to the Global Accountability Report for 2007 (Lloyd, et al. 2007) Interpol had the lowest scores in its category (IGOs), coming in tenth with a score of 22% on overall accountability capabilities (p. 19). As this report points out, and the existing academic literature on transnational policing seems to confirm, this is a secretive area and one not open to civil society involvement.
Equipment.
Weapons.
In many jurisdictions, police officers carry firearms, primarily handguns, in the normal course of their duties. In the United Kingdom (except Northern Ireland), Iceland, Ireland, Norway, New Zealand, and Malta, with the exception of specialist units, officers do not carry firearms as a matter of course.
Police often have specialist units for handling armed offenders, and similar dangerous situations, and can (depending on local laws), in some extreme circumstances, call on the military (since Military Aid to the Civil Power is a role of many armed forces). Perhaps the most high-profile example of this was, in 1980 the Metropolitan Police handing control of the Iranian Embassy Siege to the Special Air Service.
They can also be armed with non-lethal (more accurately known as "less than lethal" or "less-lethal") weaponry, particularly for riot control. Non-lethal weapons include batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons. Police officers often carry handcuffs to restrain suspects. The use of firearms or deadly force is typically a last resort only to be used when necessary to save human life, although some jurisdictions (such as Brazil) allow its use against fleeing felons and escaped convicts. A "shoot-to-kill" policy was recently introduced in South Africa, which allows police to use deadly force against any person who poses a significant threat to them or civilians. With the country having one of the highest rates of violent crime, president Jacob Zuma states that South Africa needs to handle crime differently from other countries.
Communications.
Modern police forces make extensive use of radio communications equipment, carried both on the person and installed in vehicles, to co-ordinate their work, share information, and get help quickly. In recent years, vehicle-installed computers have enhanced the ability of police communications, enabling easier dispatching of calls, criminal background checks on persons of interest to be completed in a matter of seconds, and updating officers' daily activity log and other, required reports on a real-time basis. Other common pieces of police equipment include flashlights/torches, whistles, police notebooks and "ticket books" or citations.
Vehicles.
Police vehicles are used for detaining, patrolling and transporting. The average police patrol vehicle is a specially modified, four door sedan (saloon in British English). Police vehicles are usually marked with appropriate logos and are equipped with sirens and flashing light bars to aid in making others aware of police presence.
Unmarked vehicles are used primarily for sting operations or apprehending criminals without alerting them to their presence. Some police forces use unmarked or minimally marked cars for traffic law enforcement, since drivers slow down at the sight of marked police vehicles and unmarked vehicles make it easier for officers to catch speeders and traffic violators. This practice is controversial, with for example, New York State banning this practice in 1996 on the grounds that it endangered motorists who might be pulled over by people impersonating police officers.
Motorcycles are also commonly used, particularly in locations that a car may not be able to reach, to control potential public order situations involving meetings of motorcyclists and often in escort duties where motorcycle police officers can quickly clear a path for escorted vehicles. Bicycle patrols are used in some areas because they allow for more open interaction with the public. In addition, their quieter operation can facilitate approaching suspects unawares and can help in pursuing them attempting to escape on foot.
Police forces use an array of specialty vehicles such as helicopters, airplanes, watercraft, mobile command posts, vans, trucks, all-terrain vehicles, motorcycles, and armored vehicles.
Other safety equipment.
Police cars may also contain fire extinguishers or defibrillators.
Strategies.
The advent of the police car, two-way radio, and telephone in the early 20th century transformed policing into a reactive strategy that focused on responding to calls for service. With this transformation, police command and control became more centralized.
In the United States, August Vollmer introduced other reforms, including education requirements for police officers. O.W. Wilson, a student of Vollmer, helped reduce corruption and introduce professionalism in Wichita, Kansas, and later in the Chicago Police Department. Strategies employed by O.W. Wilson included rotating officers from community to community to reduce their vulnerability to corruption, establishing of a non-partisan police board to help govern the police force, a strict merit system for promotions within the department, and an aggressive recruiting drive with higher police salaries to attract professionally qualified officers. During the professionalism era of policing, law enforcement agencies concentrated on dealing with felonies and other serious crime, rather than broader focus on crime prevention.
The Kansas City Preventive Patrol study in the 1970s found this approach to policing to be ineffective. Patrol officers in cars were disconnected from the community, and had insufficient contact and interaction with the community. In the 1980s and 1990s, many law enforcement agencies began to adopt community policing strategies, and others adopted problem-oriented policing.
Broken windows policing was another, related approach introduced in the 1980s by James Q. Wilson and George L. Kelling, who suggested that police should pay greater attention to minor "quality of life" offenses and disorderly conduct. This method was first introduced and made popular by New York City Mayor, Rudy Giuliani, in the early 1990s.
The concept behind this method is simple: broken windows, graffiti, and other physical destruction or degradation of property, greatly increases the chances of more criminal activities and destruction of property. When criminals see the abandoned vehicles, trash, and deplorable property, they assume that authorities do not care and do not take active approaches to correct problems in these areas. Therefore, correcting the small problems prevents more serious criminal activity.
Building upon these earlier models, intelligence-led policing has emerged as the dominant philosophy guiding police strategy. Intelligence-led policing and problem-oriented policing are complementary strategies, both which involve systematic use of information. Although it still lacks a universally accepted definition, the crux of intelligence-led policing is an emphasis on the collection and analysis of information to guide police operations, rather than the reverse.
Power restrictions.
In many nations, criminal procedure law has been developed to regulate officers' discretion, so that they do not arbitrarily or unjustly exercise their powers of arrest, search and seizure, and use of force. In the United States, "Miranda v. Arizona" led to the widespread use of Miranda warnings or constitutional warnings.
In "Miranda" the court created safeguards against self-incriminating statements made after an arrest. The court held that "The prosecution may not use statements, whether exculpatory or inculpatory, stemming from questioning initiated by law enforcement officers after a person has been taken into custody or otherwise deprived of his freedom of action in any significant way, unless it demonstrates the use of procedural safeguards effective to secure the Fifth Amendment's privilege against self-incrimination"
Police in the United States are also prohibited from holding criminal suspects for more than a reasonable amount of time (usually 24–48 hours) before arraignment, using torture, abuse or physical threats to extract confessions, using excessive force to effect an arrest, and searching suspects' bodies or their homes without a warrant obtained upon a showing of probable cause. The four exceptions to the constitutional requirement of a search warrant are:
In Terry v. Ohio (1968) the court divided seizure into two parts, the investigatory stop and arrest. The court further held that during an investigatory stop a police officer's search " confined to what [is minimally necessary to determine whether suspect is armed, and the intrusion, which made for the sole purpose of protecting himself and others nearby, [is confined to ascertaining the presence of weapons" (U.S. Supreme Court). Before Terry, every police encounter constituted an arrest, giving the police officer the full range of search authority. Search authority during a Terry stop (investigatory stop) is limited to weapons only.
Using deception for confessions is permitted, but not coercion. There are exceptions or exigent circumstances such as an articulated need to disarm a suspect or searching a suspect who has already been arrested (Search Incident to an Arrest). The Posse Comitatus Act severely restricts the use of the military for police activity, giving added importance to police SWAT units.
British police officers are governed by similar rules, such as those introduced to England and Wales under the Police and Criminal Evidence Act 1984 (PACE), but generally have greater powers. They may, for example, legally search any suspect who has been arrested, or their vehicles, home or business premises, without a warrant, and may seize anything they find in a search as evidence.
All police officers in the United Kingdom, whatever their actual rank, are 'constables' in terms of their legal position. This means that a newly appointed constable has the same arrest powers as a Chief Constable or Commissioner. However, certain higher ranks have additional powers to authorize certain aspects of police operations, such as a power to authorize a search of a suspect's house (section 18 PACE in England and Wales) by an officer of the rank of Inspector, or the power to authorize a suspect's detention beyond 24 hours by a Superintendent.
Conduct, accountability and public confidence.
Police services commonly include units for investigating crimes committed by the police themselves. These units are typically called Inspectorate-General, or in the US, "internal affairs". In some countries separate organizations outside the police exist for such purposes, such as the British Independent Police Complaints Commission.
Likewise, some state and local jurisdictions, for example, Springfield, Illinois have similar outside review organizations. The Police Service of Northern Ireland is investigated by the Police Ombudsman for Northern Ireland, an external agency set up as a result of the Patten report into policing the province. In the Republic of Ireland the Garda Síochána is investigated by the Garda Síochána Ombudsman Commission, an independent commission that replaced the Garda Complaints Board in May 2007.
The Special Investigations Unit of Ontario, Canada, is one of only a few civilian agencies around the world responsible for investigating circumstances involving police and civilians that have resulted in a death, serious injury, or allegations of sexual assault. The agency has made allegations of insufficient cooperation from various police services hindering their investigations.
In Hong Kong, any allegations of corruption within the police will be investigated by the Independent Commission Against Corruption and the Independent Police Complaints Council, two agencies which are independent of the police force.
Due to a long-term decline in public confidence for law enforcement in the United States, body cameras worn by police officers are under consideration.
Use of force.
Police forces also find themselves under criticism for their use of force, particularly deadly force. Specifically, tension increases when a police officer of one ethnic group harms or kills a suspect of another one. In the United States, such events occasionally spark protests and accusations of racism against police and allegations that police departments practice racial profiling.
In the United States since the 1960s, concern over such issues has increasingly weighed upon law enforcement agencies, courts and legislatures at every level of government. Incidents such as the 1965 Watts Riots, the videotaped 1991 beating by Los Angeles Police officers of Rodney King, and the riot following their acquittal have been suggested by some people to be evidence that U.S. police are dangerously lacking in appropriate controls.
The fact that this trend has occurred contemporaneously with the rise of the US civil rights movement, the "War on Drugs", and a precipitous rise in violent crime from the 1960s to the 1990s has made questions surrounding the role, administration and scope of police authority increasingly complicated.
Police departments and the local governments that oversee them in some jurisdictions have attempted to mitigate some of these issues through community outreach programs and community policing to make the police more accessible to the concerns of local communities, by working to increase hiring diversity, by updating training of police in their responsibilities to the community and under the law, and by increased oversight within the department or by civilian commissions.
In cases in which such measures have been lacking or absent, civil lawsuits have been brought by the United States Department of Justice against local law enforcement agencies, authorized under the 1994 Violent Crime Control and Law Enforcement Act. This has compelled local departments to make organizational changes, enter into consent decree settlements to adopt such measures, and submit to oversight by the Justice Department.
Protection of individuals.
Since 1855, the Supreme Court of the United States has consistently ruled that law enforcement officers have no duty to protect any individual, despite the motto "protect and serve". Their duty is to enforce the law in general. The first such case was in 1855 ("") and the most recent in 2005 ("Town of Castle Rock v. Gonzales").
In contrast, the police are entitled to protect private rights in some jurisdictions. To ensure that the police would not interfere in the regular competencies of the courts of law, some police acts require that the police may only interfere in such cases where protection from courts cannot be obtained in time, and where, without interference of the police, the realization of the private right would be impeded. This would, for example, allow police to establish a restaurant guest's identity and forward it to the innkeeper in a case where the guest cannot pay the bill at nighttime because his wallet had just been stolen from the restaurant table.
In addition, there are Federal law enforcement agencies in the United States whose mission includes providing protection for executives such as the President and accompanying family members, visiting foreign dignitaries, and other high-ranking individuals. Such agencies include the United States Secret Service and the United States Park Police.
International forces.
In many countries, particularly those with a federal system of government, there may be several police or police like organizations, each serving different levels of government and enforcing different subsets of the applicable law. The United States has a highly decentralized and fragmented system of law enforcement, with over 17,000 state and local law enforcement agencies.
Some countries, such as Chile, Israel, the Philippines, France, Austria, New Zealand and South Africa, use a centralized system of policing. Other countries have multiple police forces, but for the most part their jurisdictions do not overlap. In the United States however, several different law enforcement agencies may have authority in a particular jurisdiction at the same time, each with their own command.
Other countries where jurisdiction of multiple police agencies overlap, include Guardia Civil and the Policía Nacional in Spain, the Polizia di Stato and Carabinieri in Italy and the Police Nationale and National Gendarmerie in France.
Most countries are members of the International Criminal Police Organization (Interpol), established to detect and fight transnational crime and provide for international co-operation and co-ordination of other police activities, such as notifying relatives of the death of foreign nationals. Interpol does not conduct investigations or arrests by itself, but only serves as a central point for information on crime, suspects and criminals. Political crimes are excluded from its competencies.

</doc>
<doc id="23628" url="https://en.wikipedia.org/wiki?curid=23628" title="PDP-10">
PDP-10

The PDP-10 is a discontinued mainframe computer family manufactured by Digital Equipment Corporation (DEC) from 1966 into the 1980s.
The PDP-10 architecture is almost identical to the earlier PDP-6 architecture, sharing the same 36-bit word length and slightly extending the instruction set (but with improved hardware implementation). Some aspects of the instruction set are unusual, most notably the "byte" instructions, which operated on bit fields of any size from 1 to 36 bits inclusive according to the general definition of a byte as "a contiguous sequence of a fixed number of bits".
The PDP-10 is the machine that made time-sharing common, and this and other features made it a common fixture in many university computing facilities and research labs during the 1970s, the most notable being Harvard's Aiken Computer Center, MIT's AI Lab and Project MAC, Stanford's SAIL, Computer Center Corporation (CCC), and Carnegie Mellon University. Its main operating systems, TOPS-10 and TENEX, were used to build out the early ARPANET. For these reasons, the PDP-10 looms large in early hacker folklore.
Projects to extend the PDP-10 line were eclipsed by the success of the unrelated VAX superminicomputer, and the cancellation of the PDP-10 line was announced in 1983.
Models and technical evolution.
The original PDP-10 processor is the KA10, introduced in 1968. It uses discrete transistors packaged in DEC's Flip-Chip technology, with backplanes wire wrapped via a semi-automated manufacturing process. Its cycle time is 1 μs and its add time 2.1 μs. In 1973, the KA10 was replaced by the KI10, which uses TTL SSI. This was joined in 1975 by the higher-performance KL10 (later faster variants), which is built from ECL, microprogrammed, and has cache memory. A smaller, less expensive model, the KS10, was introduced in 1978, using TTL and Am2901 bit-slice components and including the PDP-11 Unibus to connect peripherals.
KA10.
The KA10 has a maximum main memory capacity (both virtual and physical) of 256 kilowords (equivalent to 1152 kilobytes). As supplied by DEC, it did not include paging hardware; memory management consisted of two sets of protection and relocation registers, called "base and bounds" registers. This allows each half of a user's address space to be limited to a set section of main memory, designated by the base physical address and size. This allows the model of separate read-only shareable code segment (normally the high segment) and read-write data/stack segment (normally the low segment) used by TOPS-10 and later adopted by Unix. Some KA10 machines, first at MIT, and later at Bolt, Beranek and Newman (BBN), were modified to add virtual memory and support for demand paging, as well as more physical memory.
KI10 and KL10.
The KI10 and later processors offer paged memory management, and also support a larger physical address space of 4 megawords. KI10 models include 1060, 1070 and 1077, the latter incorporating two CPUs.
The original KL10 PDP-10 (also marketed as DECsystem-10) models (1080, 1088, etc.) use the original PDP-10 memory bus, with external memory modules. Module in this context meant a cabinet, dimensions roughly (WxHxD) 30 x 75 x 30 in. with a capacity of 32 to 256 kWords of magnetic core memory (the picture on the right hand side of the introduction shows six of these cabinets). The processors used in the DECSYSTEM-20 (2040, 2050, 2060, 2065), commonly but incorrectly called "KL20", use internal memory, mounted in the same cabinet as the CPU. The 10xx models also have different packaging; they come in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20. The differences between the 10xx and 20xx models are more cosmetic than real; some 10xx systems have "20-style" internal memory and I/O, and some 20xx systems have "10-style" external memory and an I/O bus. In particular, all ARPAnet TOPS-20 systems had an I/O bus because the AN20 IMP interface was an I/O bus device. Both could run either TOPS-10 or TOPS-20 microcode and thus the corresponding operating system.
MASSbus.
The I/O architecture of the 20xx series KL machines is based on a DEC bus design called the MASSbus. While many attributed the success of the PDP-11 to DEC's decision to make the PDP-11 Unibus an open architecture, DEC reverted to prior philosophy with the KL, making MASSbus both unique and proprietary. Consequently, there were no aftermarket peripheral manufacturers who made devices for the MASSbus, and DEC chose to price their own MASSbus devices, notably the RP06 disk drive, at a substantial premium above comparable IBM-compatible devices. CompuServe for one, designed its own alternative disk controller that could operate on the MASSbus, but connect to IBM style 3330 disk subsystems.
Model B.
Later, the "Model B" version of the 2060 processors removed the 256 kiloword limitation on the virtual address space, by allowing the use of up to 32 "sections" of up to 256 kilowords each, along with substantial changes to the instruction set. "Model A" and "Model B" KL10 processors can be thought of as being different CPUs. The first operating system that took advantage of the Model B's capabilities was TOPS-20 release 3, and user mode extended addressing was offered in TOPS-20 release 4. TOPS-20 versions after release 4.1 would only run on a Model B.
TOPS-10 versions 7.02 and 7.03 also use extended addressing when run on a 1090 (or 1091) Model B processor running TOPS-20 microcode.
MCA25.
The final upgrade to the KL10 was the MCA25 upgrade of a 2060 to 2065 (or a 1091 to 1095), which gave some performance increases for programs which run in multiple sections.
KS10.
The KS10 design was crippled to be a Model A even though most of the necessary data paths needed to support the Model B architecture were present. This was no doubt intended to segment the market, but it greatly shortened the KS10's product life.
Frontend systems.
The KL class machines can not be started without the assistance of a PDP-11/40 frontend computer installed in every system. The PDP-11 is booted from a dual-ported RP06 disk drive (or alternatively from an 8" floppy disk drive or DECtape), and then commands can be given to the PDP-11 to start the main processor, which is typically booted from the same RP06 disk drive as the PDP-11. The PDP-11 performs watchdog functions once the main processor is running.
The KS system uses a similar boot procedure. An 8080 CPU loads the microcode from an RM03, RM80, or RP06 disk or magnetic tape and then starts the main processor. The 8080 switches modes after the operating system boots and controls the console and remote diagnostic serial ports.
Instruction set architecture.
From the first PDP-6s to the Model A KL-10s, the user-mode instruction set architecture is largely the same. This section covers that architecture. (Multi-section extended addressing is covered in the "DECsystem-10/DECSYSTEM-20 Processor Reference Manual".)
Addressing.
The PDP-10 has 36-bit words and 18-bit word addresses. In supervisor mode, instruction addresses correspond directly to physical memory. In user mode, addresses are translated to physical memory. Earlier models give a user process a "high" and a "low" memory: addresses with a 0 top bit used one base register, and higher addresses used another. Each segment is contiguous. Later architectures have paged memory access, allowing non-contiguous address spaces. The CPU's general-purpose registers can also be addressed as memory locations 0-15.
Registers.
There are 16 general-purpose, 36-bit registers. The right half of these registers (other than register 0) may be used for indexing. A few instructions operate on pairs of registers. The "PC Word" consists of a 13-bit condition register (plus 5 always zero bits) in the left half and an 18-bit Program Counter in the right half. The condition register, which records extra bits from the results of arithmetic operations ("e.g." overflow), can be accessed by only a few instructions.
Supervisor mode.
There are two operational modes, supervisor and user mode. Besides the difference in memory referencing described above, supervisor-mode programs can execute input/output operations.
Communication from user-mode to supervisor-mode is done through Unimplemented User Operations (UUOs): instructions which are not defined by the hardware, and are trapped by the supervisor. This mechanism is also used to emulate operations which may not have hardware implementations in cheaper models.
Data types.
The major datatypes which are directly supported by the architecture are two's complement 36-bit integer arithmetic (including bitwise operations), 36-bit floating-point, and halfwords. Extended, 72-bit, floating point is supported through special instructions designed to be used in multi-instruction sequences. Byte pointers are supported by special instructions. A word structured as a "count" half and a "pointer" half facilitates the use of bounded regions of memory, notably stacks.
Instructions.
The instruction set is very symmetrical. Every instruction consists of a 9-bit opcode, a 4-bit register code, and a 23-bit effective address field, which consists in turn of a 1-bit indirect bit, a 4-bit register code, and an 18-bit offset. Instruction execution begins by calculating the effective address. It adds the contents of the given register (if non-zero) to the offset; then, if the indirect bit is 1, fetches the word at the calculated address and repeats the effective address calculation until an effective address with a zero indirect bit is reached. The resulting effective address can be used by the instruction either to fetch memory contents, or simply as a constant. Thus, for example, MOVEI A,3(C) adds 3 to the 18 lower bits of register C and puts the result in register A, without touching memory.
There are three main classes of instruction: arithmetic, logical, and move; conditional jump; conditional skip (which may have side effects). There are also several smaller classes.
The arithmetic, logical, and move operations include variants which operate immediate-to-register, memory-to-register, register-to-memory, register-and-memory-to-both or memory-to-memory. Since registers may be addressed as part of memory, register-to-register operations are also defined. (Not all variants are useful, though they are well-defined.) For example, the ADD operation has as variants ADDI (add an 18-bit "I"mmediate constant to a register), ADDM (add register contents to a "M"emory location), ADDB (add to "B"oth, that is, add register contents to memory and also put the result in the register). A more elaborate example is HLROM ("H"alf "L"eft to "R"ight, "O"nes to "M"emory), which takes the Left half of the register contents, places them in the Right half of the memory location, and replaces the left half of the memory location with Ones.
The conditional jump operations examine register contents and jump to a given location depending on the result of the comparison. The mnemonics for these instructions all start with JUMP, JUMPA meaning "jump always" and JUMP meaning "jump never" - as a consequence of the symmetrical design of the instruction set, it contains several no-ops such as JUMP. For example, JUMPN A,LOC jumps to the address LOC if the contents of register A is non-zero. There are also conditional jumps based on the processor's condition register using the JRST instruction. On the KA10 and KI10, JRST is faster than JUMPA, so the standard unconditional jump is JRST.
The conditional skip operations compare register and memory contents and skip the next instruction (which is often an unconditional jump) depending on the result of the comparison. A simple example is CAMN A,LOC which compares the contents of register A with the contents of location LOC and skips the next instruction if they are not equal. A more elaborate example is TLCE A,LOC (read "Test Left Complement, skip if Equal"), which using the contents of LOC as a mask, selects the corresponding bits in the left half of register A. If all those bits are "E"qual to zero, skip the next instruction; and in any case, replace those bits by their boolean complement.
Some smaller instruction classes include the shift/rotate instructions and the procedure call instructions. Particularly notable are the stack instructions PUSH and POP, and the corresponding stack call instructions PUSHJ and POPJ. The byte instructions use a special format of indirect word to extract and store arbitrary-sized bit fields, possibly advancing a pointer to the next unit.
Software.
The original PDP-10 operating system was simply called "Monitor", but was later renamed TOPS-10. Eventually the PDP-10 system itself was renamed the DECsystem-10. Early versions of Monitor and TOPS-10 formed the basis of Stanford's WAITS operating system and the Compuserve time-sharing system.
Over time, some PDP-10 operators began running operating systems assembled from major components developed outside DEC. For example, the main Scheduler might come from one university, the Disk Service from another, and so on. The commercial timesharing services such as CompuServe, On-Line Systems (OLS), and Rapidata maintained sophisticated inhouse systems programming groups so that they could modify the operating system as needed for their own businesses without being dependent on DEC or others. There are also strong user communities such as DECUS through which users can share software that they have developed.
BBN developed their own alternative operating system, TENEX, which fairly quickly became the de facto standard in the research community. DEC later ported Tenex to the KL10, enhanced it considerably, and named it TOPS-20, forming the DECSYSTEM-20 line. MIT also had developed their own influential system, the Incompatible Timesharing System (named in parody of the Compatible Time-Sharing System, developed at MIT for a modified IBM 7094).
Tymshare developed TYMCOM-X, derived from TOPS-10 but using a page-based file system like TOPS-20.
Clones.
In 1971 to 1972 researchers at Xerox PARC were frustrated by top company management's refusal to let them purchase a PDP-10. Xerox had just bought Scientific Data Systems in 1969, and wanted PARC to use an SDS machine.
Instead, a group led by Charles P. Thacker designed and constructed two PDP-10 clone systems named "MAXC" (pronounced "Max", in honour of Max Palevsky, who had sold SDS to Xerox) for their own use. MAXC was also a backronym for Multiple Access Xerox Computer.
MAXC ran a modified version of TENEX.
Third-party attempts to sell PDP-10 clones were relatively unsuccessful; see Foonly, Systems Concepts, and XKL.
Use by CompuServe.
One of the largest collections of DECsystem-10 architecture systems ever assembled was at CompuServe, which at its peak operated over 200 loosely coupled systems in three data centers in Columbus, Ohio. CompuServe used these systems as 'hosts', providing access to commercial applications as well as the CompuServe Information Service. While the first such systems were purchased from DEC, when DEC abandoned the PDP-10 architecture in favor of the VAX, CompuServe and other PDP-10 customers began purchasing plug compatible computers from Systems Concepts. As of January 2007, CompuServe continues to operate a small number of PDP-10 architecture machines to perform some billing and routing functions.
The main power supplies used in the KL-series machines were so inefficient that CompuServe engineers designed a replacement power supply that consumed about half the energy. CompuServe offered to license the design for its KL power supply to DEC for free if DEC would promise that any new KL purchased by CompuServe would have the more efficient power supply installed. DEC declined the offer.
Another modification made to the PDP-10 by CompuServe engineers was the replacement of the hundreds of incandescent indicator lamps on the KI10 processor cabinet with LED lamp modules. The cost of the conversion was easily offset by the cost savings in electric consumption, the reduction of heat, and the manpower required to replace burned-out lamps. Digital followed this step all over the world. The picture on the right hand side shows the light panel of the MF10 memory which is contemporaneous with the KI10 CPU. This item is part of a computer museum, and was populated with LEDs in 2008 for demonstration purposes only. There were no similar banks of indicator lamps on KL and KS processors.
Cancellation and influence.
The PDP-10 was eventually eclipsed by the VAX superminicomputer machines (descendants of the PDP-11) when DEC recognized that the PDP-10 and VAX product lines were competing with each other and decided to concentrate its software development effort on the more profitable VAX. The PDP-10 product line cancellation was announced in 1983, including cancelling the ongoing Jupiter project to produce a new high-end PDP-10 processor (despite that project being in good shape at the time of the cancellation) and the Minnow project to produce a desktop PDP-10, which may then have been at the prototyping stage.
This event spelled the doom of ITS and the technical cultures that had spawned the original jargon file, but by the 1990s it had become something of a badge of honor among old-time hackers to have cut one's teeth on a PDP-10.
The PDP-10 assembly language instructions LDB and DPB (load/deposit byte) live on as functions in the programming language Common Lisp. See the "References" section on the LISP article — the 36-bit word size of the PDP-6 and PDP-10 was influenced by the programming convenience of having 2 LISP pointers, each 18 bits, in one word.
Will Crowther created "Adventure", the prototypical computer adventure game, for a PDP-10. Don Daglow created the first computer baseball game (1971) and "Dungeon" (1975), the first role-playing video game on a PDP-10. Walter Bright originally created "Empire" for the PDP-10. Roy Trubshaw and Richard Bartle created the first MUD on a PDP-10. In addition, "Zork" was written on the PDP-10, and Infocom used several PDP-10s for game development and testing.
Bill Gates and Paul Allen originally wrote Altair BASIC using an Intel 8080 emulator running on a PDP-10 at Harvard University. They founded Microsoft shortly after.
Emulation or simulation.
The software for simulation of historical computers SIMH contains a module to emulate the KS10 CPU on a Windows or Unix-based machine. Copies of DEC's original distribution tapes are available as downloads from the Internet so that a running TOPS-10 or TOPS-20 system may be established. ITS is also available for SIMH.
Ken Harrenstien's KLH10 software for Unix-like systems emulates a KL10B processor with extended addressing and 4 MW of memory or a KS10 processor with 512 KW of memory. The KL10 emulation supports v.442 of the KL10 microcode, which enables it to run the final versions of both TOPS-10 and TOPS-20. The KS10 emulation supports both ITS v.262 microcode for the final version of KS10 ITS and DEC v.130 microcode for the final versions of KS TOPS-10 and TOPS-20.

</doc>
<doc id="23629" url="https://en.wikipedia.org/wiki?curid=23629" title="DECSYSTEM-20">
DECSYSTEM-20

The DECSYSTEM-20 was a 36-bit Digital Equipment Corporation PDP-10 mainframe computer running the TOPS-20 operating system (products introduced in 1977).
PDP-10 computers running the TOPS-10 operating system were labeled "DECsystem-10" as a way of differentiating them from the PDP-11. Later on, those systems running TOPS-20 (on the KL10 PDP-10 processors) were labeled "DECSYSTEM-20" (the block capitals being the result of a lawsuit brought against DEC by Singer, which once made a computer called "system-10"). The DECSYSTEM-20 was sometimes called PDP-20, although this designation was never used by DEC.
The following models were produced:
The only significant difference the user could see between a DECsystem-10 and a DECSYSTEM-20 was the operating system and the color of the paint. Most (but not all) machines sold to run TOPS-10 were painted "Blasi Blue", whereas most TOPS-20 machines were painted "Terracotta" (often mistakenly called "Chinese Red" or orange; the actual name of the color on the paint cans was Terracotta).
There were some significant internal differences between the earlier KL10 Model A processors, used in the earlier DECsystem-10s running on KL10 processors, and the later KL10 Model Bs, used for the DECSYSTEM-20s. Model As used the original PDP-10 memory bus, with external memory modules. The later Model B processors used in the DECSYSTEM-20 used internal memory, mounted in the same cabinet as the CPU. The Model As also had different packaging; they came in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20.
The last released implementation of DEC's 36-bit architecture was the single cabinet DECSYSTEM-2020, using a KS10 processor.
The DECSYSTEM-20 was primarily designed and used as a small mainframe for timesharing. That is, multiple users would concurrently log on to individual user accounts and share use of the main processor to compile and run applications. Separate disk allocations were maintained for all users by the operating system, and various levels of protection could be maintained by for System, Owner, Group, and World users. A model 2060, for example, could typically host up to 40 to 60 simultaneous users before exhibiting noticeably reduced response time.
Remaining machines.
The Living Computer Museum of Seattle, Washington maintains a 2065 running TOPS-10, which is available to interested parties via telnet upon registration (at no cost) at their website.

</doc>
<doc id="23630" url="https://en.wikipedia.org/wiki?curid=23630" title="Programmed Data Processor">
Programmed Data Processor

Programmed Data Processor (PDP) was a series of minicomputers made and marketed by the Digital Equipment Corporation from 1957 to 1990. The name "PDP" intentionally avoided the use of the term "computer" because, at the time of the first PDPs, computers had a reputation of being large, complicated, and expensive machines, and the venture capitalists behind Digital (especially Georges Doriot) would not support Digital's attempting to build a "computer"; the word "minicomputer" had not yet been coined. So instead, Digital used their existing line of logic modules to build a "Programmed Data Processor" and aimed it at a market that could not afford the larger computers.
The various PDP machines can generally be grouped into families based on word length.
PDP series.
Members of the PDP series include:
External links.
Various sites list documents by Charles Lasner, the creator of the alt.sys.pdp8 discussion group, and related documents by various members of the alt.sys.pdp8 readership with even more authoritative information about the various models, especially detailed focus upon the various members of the PDP-8 "family" of computers both made and not made by DEC.

</doc>
<doc id="23631" url="https://en.wikipedia.org/wiki?curid=23631" title="Primary mirror">
Primary mirror

A primary mirror (or primary) is the principal light-gathering surface (the objective) of a reflecting telescope.
Description.
The primary mirror of a reflecting telescope is a spherical or parabolic shaped disks of polished reflective metal (speculum metal up to the mid 19th century), or in later telescopes, glass or other material coated with a reflective layer. One of the first known reflecting telescopes, Newton's reflector of 1668, used a 3.3 cm polished metal primary mirror. The next major change was to use silver on glass rather than metal, in the 19th century such was with the Crossley reflector. This was changed to vacuum deposited aluminum on glass, used on the 200-inch Hale telescope.
Solid primary mirrors have to sustain their own weight and not deform under gravity, which limits the maximum size for a single piece primary mirror.
Segmented mirror configurations are used to get around the size limitation on single primary mirrors. For example, the Giant Magellan Telescope will have seven 8.4 meter primary mirrors, with the resolving power equivalent to a optical aperture.
Superlative primary mirrors.
The largest optical telescope in the world as of 2009 to use a non-segmented single-mirror as its primary mirror is the 8.2 m (8.7 yards) Subaru telescope of the National Astronomical Observatory of Japan, located in Mauna Kea Observatory on Hawaii since 1997; however, this is not the largest diameter single mirror in a telescope, the U.S./German/Italian Large Binocular Telescope has two 8.4 m (9.2 yards) mirrors (which can be used together for interferometric mode). Both of these are smaller than the 10 m segmented primary mirrors on the two Keck telescope. The Hubble Space Telescope has a 2.4 m (7 ft 10 in) primary mirror.
Radio and submillimeter telescopes use much larger dishes or antennae, which do not have to be made as precisely as the mirrors used in optical telescopes. The Arecibo Observatory uses a 305 m dish, which is the world largest single-dish radio telescope fixed to the ground. The Green Bank Telescope has the world's largest steerable single radio dish with 100 m in diameter. There are largest radio arrays, composed of multiple dishes which have better image resolution but less sensitivity.

</doc>
<doc id="23632" url="https://en.wikipedia.org/wiki?curid=23632" title="Platonic idealism">
Platonic idealism

Platonic idealism usually refers to Plato's theory of forms or doctrine of ideas.
Overview.
Some commentators hold Plato argued that truth is an abstraction. In other words, we are urged to believe that Plato's theory of ideas is an abstraction, divorced from the so-called external world, of modern European philosophy, despite the fact Plato taught that ideas are ultimately real, and different from non-ideal things—indeed, he argued for a distinction between the ideal and non-ideal realm.
These commentators speak thus: for example, a particular tree, with a branch or two missing, possibly alive, possibly dead, and with the initials of two lovers carved into its bark, is distinct from the abstract form of Tree-ness. A Tree is the ideal that each of us holds that allows us to identify the imperfect reflections of trees all around us.
Plato gives the divided line as an outline of this theory. At the top of the line, the Form of the Good
is found, directing everything underneath.
Some contemporary linguistic philosophers construe "Platonism" to mean the proposition that universals exist independently of particulars (a universal is anything that can be predicated of a particular).
Platonism is an ancient school of philosophy, founded by Plato; at the beginning, this school had a physical existence at a site just outside the walls of Athens called the Academy, as well as the intellectual unity of a shared approach to philosophizing.
Platonism is usually divided into three periods:
Plato's students used the hypomnemata as the foundation to his philosophic approach to knowledge. The hypomnemata constituted a material memory of things read, heard, or thought, thus offering these as an accumulated treasure for rereading and later meditation. For the Neoplatonist they also formed a raw material for the writing of more systematic treatises in which were given arguments and means by which to struggle against some defect (such as anger, envy, gossip, flattery) or to overcome some difficult circumstance (such as a mourning, an exile, downfall, disgrace).
Platonism is considered to be, in mathematics departments the world over, the predominant philosophy of mathematics, especially regarding the foundations of mathematics.
One statement of this philosophy is the thesis that mathematics is not created but discovered.
A lucid statement of this is found in an essay written by the British mathematician G. H. Hardy in defense of pure mathematics.
The absence in this thesis of clear distinction between mathematical and nonmathematical "creation" leaves open the inference that it applies to allegedly creative endeavors in art, music, and literature.
It is unknown if Plato's ideas of idealism have some earlier origin, but Plato held Pythagoras in high regard, and Pythagoras as well as his followers in the movement known as Pythagoreanism claimed the world was literally built up from numbers, an abstract, absolute form.

</doc>
<doc id="23633" url="https://en.wikipedia.org/wiki?curid=23633" title="List of physicists">
List of physicists

Following is a list of physicists who are for their achievements.

</doc>
<doc id="23634" url="https://en.wikipedia.org/wiki?curid=23634" title="Protein">
Protein

Proteins ( or ) are large biomolecules, or macromolecules, consisting of one or more long chains of amino acid residues. Proteins perform a vast array of functions within living organisms, including catalyzing metabolic reactions, DNA replication, responding to stimuli, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in protein folding into a specific three-dimensional structure that determines its activity.
A linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than 20-30 residues, are rarely considered to be proteins and are commonly called peptides, or sometimes oligopeptides. The individual amino acid residues are bonded together by peptide bonds and adjacent amino acid residues. The sequence of amino acid residues in a protein is defined by the sequence of a gene, which is encoded in the genetic code. In general, the genetic code specifies 20 standard amino acids; however, in certain organisms the genetic code can include selenocysteine and—in certain archaea—pyrrolysine. Shortly after or even during synthesis, the residues in a protein are often chemically modified by posttranslational modification, which alters the physical and chemical properties, folding, stability, activity, and ultimately, the function of the proteins. Sometimes proteins have non-peptide groups attached, which can be called prosthetic groups or cofactors. Proteins can also work together to achieve a particular function, and they often associate to form stable protein complexes.
Once formed, proteins only exist for a certain period of time and are then degraded and recycled by the cell's machinery through the process of protein turnover. A protein's lifespan is measured in terms of its half-life and covers a wide range. They can exist for minutes or years with an average lifespan of 1–2 days in mammalian cells. Abnormal and or misfolded proteins are degraded more rapidly either due to being targeted for destruction or due to being unstable.
Like other biological macromolecules such as polysaccharides and nucleic acids, proteins are essential parts of organisms and participate in virtually every process within cells. Many proteins are enzymes that catalyze biochemical reactions and are vital to metabolism. Proteins also have structural or mechanical functions, such as actin and myosin in muscle and the proteins in the cytoskeleton, which form a system of scaffolding that maintains cell shape. Other proteins are important in cell signaling, immune responses, cell adhesion, and the cell cycle. Proteins are also necessary in animals' diets, since animals cannot synthesize all the amino acids they need and must obtain essential amino acids from food. Through the process of digestion, animals break down ingested protein into free amino acids that are then used in metabolism.
Proteins may be purified from other cellular components using a variety of techniques such as ultracentrifugation, precipitation, electrophoresis, and chromatography; the advent of genetic engineering has made possible a number of methods to facilitate purification. Methods commonly used to study protein structure and function include immunohistochemistry, site-directed mutagenesis, X-ray crystallography, nuclear magnetic resonance and mass spectrometry.
Biochemistry.
Most proteins consist of linear polymers built from series of up to 20 different L-α-amino acids. All proteinogenic amino acids possess common structural features, including an α-carbon to which an amino group, a carboxyl group, and a variable side chain are bonded. Only proline differs from this basic structure as it contains an unusual ring to the N-end amine group, which forces the CO–NH amide moiety into a fixed conformation. The side chains of the standard amino acids, detailed in the list of standard amino acids, have a great variety of chemical structures and properties; it is the combined effect of all of the amino acid side chains in a protein that ultimately determines its three-dimensional structure and its chemical reactivity.
The amino acids in a polypeptide chain are linked by peptide bonds. Once linked in the protein chain, an individual amino acid is called a "residue," and the linked series of carbon, nitrogen, and oxygen atoms are known as the "main chain" or "protein backbone."
The peptide bond has two resonance forms that contribute some double-bond character and inhibit rotation around its axis, so that the alpha carbons are roughly coplanar. The other two dihedral angles in the peptide bond determine the local shape assumed by the protein backbone. The end of the protein with a free carboxyl group is known as the C-terminus or carboxy terminus, whereas the end with a free amino group is known as the N-terminus or amino terminus.
The words "protein", "polypeptide," and "peptide" are a little ambiguous and can overlap in meaning. "Protein" is generally used to refer to the complete biological molecule in a stable conformation, whereas "peptide" is generally reserved for a short amino acid oligomers often lacking a stable three-dimensional structure. However, the boundary between the two is not well defined and usually lies near 20–30 residues. "Polypeptide" can refer to any single linear chain of amino acids, usually regardless of length, but often implies an absence of a defined conformation.
Synthesis.
Biosynthesis.
Proteins are assembled from amino acids using information encoded in genes. Each protein has its own unique amino acid sequence that is specified by the nucleotide sequence of the gene encoding this protein. The genetic code is a set of three-nucleotide sets called codons and each three-nucleotide combination designates an amino acid, for example AUG (adenine-uracil-guanine) is the code for methionine. Because DNA contains four nucleotides, the total number of possible codons is 64; hence, there is some redundancy in the genetic code, with some amino acids specified by more than one codon. Genes encoded in DNA are first transcribed into pre-messenger RNA (mRNA) by proteins such as RNA polymerase. Most organisms then process the pre-mRNA (also known as a "primary transcript") using various forms of Post-transcriptional modification to form the mature mRNA, which is then used as a template for protein synthesis by the ribosome. In prokaryotes the mRNA may either be used as soon as it is produced, or be bound by a ribosome after having moved away from the nucleoid. In contrast, eukaryotes make mRNA in the cell nucleus and then translocate it across the nuclear membrane into the cytoplasm, where protein synthesis then takes place. The rate of protein synthesis is higher in prokaryotes than eukaryotes and can reach up to 20 amino acids per second.
The process of synthesizing a protein from an mRNA template is known as translation. The mRNA is loaded onto the ribosome and is read three nucleotides at a time by matching each codon to its base pairing anticodon located on a transfer RNA molecule, which carries the amino acid corresponding to the codon it recognizes. The enzyme aminoacyl tRNA synthetase "charges" the tRNA molecules with the correct amino acids. The growing polypeptide is often termed the "nascent chain". Proteins are always biosynthesized from N-terminus to C-terminus.
The size of a synthesized protein can be measured by the number of amino acids it contains and by its total molecular mass, which is normally reported in units of "daltons" (synonymous with atomic mass units), or the derivative unit kilodalton (kDa). Yeast proteins are on average 466 amino acids long and 53 kDa in mass. The largest known proteins are the titins, a component of the muscle sarcomere, with a molecular mass of almost 3,000 kDa and a total length of almost 27,000 amino acids.
Chemical synthesis.
Short proteins can also be synthesized chemically by a family of methods known as peptide synthesis, which rely on organic synthesis techniques such as chemical ligation to produce peptides in high yield. Chemical synthesis allows for the introduction of non-natural amino acids into polypeptide chains, such as attachment of fluorescent probes to amino acid side chains. These methods are useful in laboratory biochemistry and cell biology, though generally not for commercial applications. Chemical synthesis is inefficient for polypeptides longer than about 300 amino acids, and the synthesized proteins may not readily assume their native tertiary structure. Most chemical synthesis methods proceed from C-terminus to N-terminus, opposite the biological reaction.
Structure.
Most proteins fold into unique 3-dimensional structures. The shape into which a protein naturally folds is known as its native conformation. Although many proteins can fold unassisted, simply through the chemical properties of their amino acids, others require the aid of molecular chaperones to fold into their native states. Biochemists often refer to four distinct aspects of a protein's structure:
Proteins are not entirely rigid molecules. In addition to these levels of structure, proteins may shift between several related structures while they perform their functions. In the context of these functional rearrangements, these tertiary or quaternary structures are usually referred to as "conformations", and transitions between them are called "conformational changes." Such changes are often induced by the binding of a substrate molecule to an enzyme's active site, or the physical region of the protein that participates in chemical catalysis. In solution proteins also undergo variation in structure through thermal vibration and the collision with other molecules.
Proteins can be informally divided into three main classes, which correlate with typical tertiary structures: globular proteins, fibrous proteins, and membrane proteins. Almost all globular proteins are soluble and many are enzymes. Fibrous proteins are often structural, such as collagen, the major component of connective tissue, or keratin, the protein component of hair and nails. Membrane proteins often serve as receptors or provide channels for polar or charged molecules to pass through the cell membrane.
A special case of intramolecular hydrogen bonds within proteins, poorly shielded from water attack and hence promoting their own dehydration, are called dehydrons.
Structure determination.
Discovering the tertiary structure of a protein, or the quaternary structure of its complexes, can provide important clues about how the protein performs its function. Common experimental methods of structure determination include X-ray crystallography and NMR spectroscopy, both of which can produce information at atomic resolution. However, NMR experiments are able to provide information from which a subset of distances between pairs of atoms can be estimated, and the final possible conformations for a protein are determined by solving a distance geometry problem. Dual polarisation interferometry is a quantitative analytical method for measuring the overall protein conformation and conformational changes due to interactions or other stimulus. Circular dichroism is another laboratory technique for determining internal β-sheet / α-helical composition of proteins. Cryoelectron microscopy is used to produce lower-resolution structural information about very large protein complexes, including assembled viruses; a variant known as electron crystallography can also produce high-resolution information in some cases, especially for two-dimensional crystals of membrane proteins. Solved structures are usually deposited in the Protein Data Bank (PDB), a freely available resource from which structural data about thousands of proteins can be obtained in the form of Cartesian coordinates for each atom in the protein.
Many more gene sequences are known than protein structures. Further, the set of solved structures is biased toward proteins that can be easily subjected to the conditions required in X-ray crystallography, one of the major structure determination methods. In particular, globular proteins are comparatively easy to crystallize in preparation for X-ray crystallography. Membrane proteins, by contrast, are difficult to crystallize and are underrepresented in the PDB. Structural genomics initiatives have attempted to remedy these deficiencies by systematically solving representative structures of major fold classes. Protein structure prediction methods attempt to provide a means of generating a plausible structure for proteins whose structures have not been experimentally determined.
Cellular functions.
Proteins are the chief actors within the cell, said to be carrying out the duties specified by the information encoded in genes. With the exception of certain types of RNA, most other biological molecules are relatively inert elements upon which proteins act. Proteins make up half the dry weight of an "Escherichia coli" cell, whereas other macromolecules such as DNA and RNA make up only 3% and 20%, respectively. The set of proteins expressed in a particular cell or cell type is known as its proteome.
The chief characteristic of proteins that also allows their diverse set of functions is their ability to bind other molecules specifically and tightly. The region of the protein responsible for binding another molecule is known as the binding site and is often a depression or "pocket" on the molecular surface. This binding ability is mediated by the tertiary structure of the protein, which defines the binding site pocket, and by the chemical properties of the surrounding amino acids' side chains. Protein binding can be extraordinarily tight and specific; for example, the ribonuclease inhibitor protein binds to human angiogenin with a sub-femtomolar dissociation constant (<10−15 M) but does not bind at all to its amphibian homolog onconase (>1 M). Extremely minor chemical changes such as the addition of a single methyl group to a binding partner can sometimes suffice to nearly eliminate binding; for example, the aminoacyl tRNA synthetase specific to the amino acid valine discriminates against the very similar side chain of the amino acid isoleucine.
Proteins can bind to other proteins as well as to small-molecule substrates. When proteins bind specifically to other copies of the same molecule, they can oligomerize to form fibrils; this process occurs often in structural proteins that consist of globular monomers that self-associate to form rigid fibers. Protein–protein interactions also regulate enzymatic activity, control progression through the cell cycle, and allow the assembly of large protein complexes that carry out many closely related reactions with a common biological function. Proteins can also bind to, or even be integrated into, cell membranes. The ability of binding partners to induce conformational changes in proteins allows the construction of enormously complex signaling networks.
Importantly, as interactions between proteins are reversible, and depend heavily on the availability of different groups of partner proteins to form aggregates that are capable to carry out discrete sets of function, study of the interactions between specific proteins is a key to understand important aspects of cellular function, and ultimately the properties that distinguish particular cell types.
Enzymes.
The best-known role of proteins in the cell is as enzymes, which catalyze chemical reactions. Enzymes are usually highly specific and accelerate only one or a few chemical reactions. Enzymes carry out most of the reactions involved in metabolism, as well as manipulating DNA in processes such as DNA replication, DNA repair, and transcription. Some enzymes act on other proteins to add or remove chemical groups in a process known as posttranslational modification. About 4,000 reactions are known to be catalyzed by enzymes. The rate acceleration conferred by enzymatic catalysis is often enormous—as much as 1017-fold increase in rate over the uncatalyzed reaction in the case of orotate decarboxylase (78 million years without the enzyme, 18 milliseconds with the enzyme).
The molecules bound and acted upon by enzymes are called substrates. Although enzymes can consist of hundreds of amino acids, it is usually only a small fraction of the residues that come in contact with the substrate, and an even smaller fraction—three to four residues on average—that are directly involved in catalysis. The region of the enzyme that binds the substrate and contains the catalytic residues is known as the active site.
Dirigent proteins are members of a class of proteins that dictate the stereochemistry of a compound synthesized by other enzymes.
Cell signaling and ligand binding.
Many proteins are involved in the process of cell signaling and signal transduction. Some proteins, such as insulin, are extracellular proteins that transmit a signal from the cell in which they were synthesized to other cells in distant tissues. Others are membrane proteins that act as receptors whose main function is to bind a signaling molecule and induce a biochemical response in the cell. Many receptors have a binding site exposed on the cell surface and an effector domain within the cell, which may have enzymatic activity or may undergo a conformational change detected by other proteins within the cell.
Antibodies are protein components of an adaptive immune system whose main function is to bind antigens, or foreign substances in the body, and target them for destruction. Antibodies can be secreted into the extracellular environment or anchored in the membranes of specialized B cells known as plasma cells. Whereas enzymes are limited in their binding affinity for their substrates by the necessity of conducting their reaction, antibodies have no such constraints. An antibody's binding affinity to its target is extraordinarily high.
Many ligand transport proteins bind particular small biomolecules and transport them to other locations in the body of a multicellular organism. These proteins must have a high binding affinity when their ligand is present in high concentrations, but must also release the ligand when it is present at low concentrations in the target tissues. The canonical example of a ligand-binding protein is haemoglobin, which transports oxygen from the lungs to other organs and tissues in all vertebrates and has close homologs in every biological kingdom. Lectins are sugar-binding proteins which are highly specific for their sugar moieties. Lectins typically play a role in biological recognition phenomena involving cells and proteins. Receptors and hormones are highly specific binding proteins.
Transmembrane proteins can also serve as ligand transport proteins that alter the permeability of the cell membrane to small molecules and ions. The membrane alone has a hydrophobic core through which polar or charged molecules cannot diffuse. Membrane proteins contain internal channels that allow such molecules to enter and exit the cell. Many ion channel proteins are specialized to select for only a particular ion; for example, potassium and sodium channels often discriminate for only one of the two ions.
Structural proteins.
Structural proteins confer stiffness and rigidity to otherwise-fluid biological components. Most structural proteins are fibrous proteins; for example, collagen and elastin are critical components of connective tissue such as cartilage, and keratin is found in hard or filamentous structures such as hair, nails, feathers, hooves, and some animal shells. Some globular proteins can also play structural functions, for example, actin and tubulin are globular and soluble as monomers, but polymerize to form long, stiff fibers that make up the cytoskeleton, which allows the cell to maintain its shape and size.
Other proteins that serve structural functions are motor proteins such as myosin, kinesin, and dynein, which are capable of generating mechanical forces. These proteins are crucial for cellular motility of single celled organisms and the sperm of many multicellular organisms which reproduce sexually. They also generate the forces exerted by contracting muscles and play essential roles in intracellular transport.
Methods of study.
The activities and structures of proteins may be examined "in vitro," "in vivo, and in silico". In vitro studies of purified proteins in controlled environments are useful for learning how a protein carries out its function: for example, enzyme kinetics studies explore the chemical mechanism of an enzyme's catalytic activity and its relative affinity for various possible substrate molecules. By contrast, in vivo experiments can provide information about the physiological role of a protein in the context of a cell or even a whole organism. In silico studies use computational methods to study proteins.
Protein purification.
To perform "in vitro" analysis, a protein must be purified away from other cellular components. This process usually begins with cell lysis, in which a cell's membrane is disrupted and its internal contents released into a solution known as a crude lysate. The resulting mixture can be purified using ultracentrifugation, which fractionates the various cellular components into fractions containing soluble proteins; membrane lipids and proteins; cellular organelles, and nucleic acids. Precipitation by a method known as salting out can concentrate the proteins from this lysate. Various types of chromatography are then used to isolate the protein or proteins of interest based on properties such as molecular weight, net charge and binding affinity. The level of purification can be monitored using various types of gel electrophoresis if the desired protein's molecular weight and isoelectric point are known, by spectroscopy if the protein has distinguishable spectroscopic features, or by enzyme assays if the protein has enzymatic activity. Additionally, proteins can be isolated according their charge using electrofocusing.
For natural proteins, a series of purification steps may be necessary to obtain protein sufficiently pure for laboratory applications. To simplify this process, genetic engineering is often used to add chemical features to proteins that make them easier to purify without affecting their structure or activity. Here, a "tag" consisting of a specific amino acid sequence, often a series of histidine residues (a "His-tag"), is attached to one terminus of the protein. As a result, when the lysate is passed over a chromatography column containing nickel, the histidine residues ligate the nickel and attach to the column while the untagged components of the lysate pass unimpeded. A number of different tags have been developed to help researchers purify specific proteins from complex mixtures.
Cellular localization.
The study of proteins "in vivo" is often concerned with the synthesis and localization of the protein within the cell. Although many intracellular proteins are synthesized in the cytoplasm and membrane-bound or secreted proteins in the endoplasmic reticulum, the specifics of how proteins are targeted to specific organelles or cellular structures is often unclear. A useful technique for assessing cellular localization uses genetic engineering to express in a cell a fusion protein or chimera consisting of the natural protein of interest linked to a "reporter" such as green fluorescent protein (GFP). The fused protein's position within the cell can be cleanly and efficiently visualized using microscopy, as shown in the figure opposite.
Other methods for elucidating the cellular location of proteins requires the use of known compartmental markers for regions such as the ER, the Golgi, lysosomes or vacuoles, mitochondria, chloroplasts, plasma membrane, etc. With the use of fluorescently tagged versions of these markers or of antibodies to known markers, it becomes much simpler to identify the localization of a protein of interest. For example, indirect immunofluorescence will allow for fluorescence colocalization and demonstration of location. Fluorescent dyes are used to label cellular compartments for a similar purpose.
Other possibilities exist, as well. For example, immunohistochemistry usually utilizes an antibody to one or more proteins of interest that are conjugated to enzymes yielding either luminescent or chromogenic signals that can be compared between samples, allowing for localization information. Another applicable technique is cofractionation in sucrose (or other material) gradients using isopycnic centrifugation. While this technique does not prove colocalization of a compartment of known density and the protein of interest, it does increase the likelihood, and is more amenable to large-scale studies.
Finally, the gold-standard method of cellular localization is immunoelectron microscopy. This technique also uses an antibody to the protein of interest, along with classical electron microscopy techniques. The sample is prepared for normal electron microscopic examination, and then treated with an antibody to the protein of interest that is conjugated to an extremely electro-dense material, usually gold. This allows for the localization of both ultrastructural details as well as the protein of interest.
Through another genetic engineering application known as site-directed mutagenesis, researchers can alter the protein sequence and hence its structure, cellular localization, and susceptibility to regulation. This technique even allows the incorporation of unnatural amino acids into proteins, using modified tRNAs, and may allow the rational design of new proteins with novel properties.
Proteomics.
The total complement of proteins present at a time in a cell or cell type is known as its proteome, and the study of such large-scale data sets defines the field of proteomics, named by analogy to the related field of genomics. Key experimental techniques in proteomics include 2D electrophoresis, which allows the separation of a large number of proteins, mass spectrometry, which allows rapid high-throughput identification of proteins and sequencing of peptides (most often after in-gel digestion), protein microarrays, which allow the detection of the relative levels of a large number of proteins present in a cell, and two-hybrid screening, which allows the systematic exploration of protein–protein interactions. The total complement of biologically possible such interactions is known as the interactome. A systematic attempt to determine the structures of proteins representing every possible fold is known as structural genomics.
Bioinformatics.
A vast array of computational methods have been developed to analyze the structure, function, and evolution of proteins.
The development of such tools has been driven by the large amount of genomic and proteomic data available for a variety of organisms, including the human genome. It is simply impossible to study all proteins experimentally, hence only a few are subjected to laboratory experiments while computational tools are used to extrapolate to similar proteins. Such homologous proteins can be efficiently identified in distantly related organisms by sequence alignment. Genome and gene sequences can be searched by a variety of tools for certain properties. Sequence profiling tools can find restriction enzyme sites, open reading frames in nucleotide sequences, and predict secondary structures. Phylogenetic trees can be constructed and evolutionary hypotheses developed using special software like ClustalW regarding the ancestry of modern organisms and the genes they express. The field of bioinformatics is now indispensable for the analysis of genes and proteins.
Structure prediction and simulation.
Complementary to the field of structural genomics, protein structure prediction seeks to develop efficient ways to provide plausible models for proteins whose structures have not yet been determined experimentally. The most successful type of structure prediction, known as homology modeling, relies on the existence of a "template" structure with sequence similarity to the protein being modeled; structural genomics' goal is to provide sufficient representation in solved structures to model most of those that remain. Although producing accurate models remains a challenge when only distantly related template structures are available, it has been suggested that sequence alignment is the bottleneck in this process, as quite accurate models can be produced if a "perfect" sequence alignment is known. Many structure prediction methods have served to inform the emerging field of protein engineering, in which novel protein folds have already been designed. A more complex computational problem is the prediction of intermolecular interactions, such as in molecular docking and protein–protein interaction prediction.
The processes of protein folding and binding can be simulated using such technique as molecular mechanics, in particular, molecular dynamics and Monte Carlo, which increasingly take advantage of parallel and distributed computing (Folding@home project; molecular modeling on GPU). The folding of small α-helical protein domains such as the villin headpiece and the HIV accessory protein have been successfully simulated "in silico", and hybrid methods that combine standard molecular dynamics with quantum mechanics calculations have allowed exploration of the electronic states of rhodopsins.
Protein disorder and unstructure prediction.
Many proteins (between 20 and 40% of many proteomes) contain large unstructured but biologically functional segments and can be classified as intrinsically disordered proteins.
Predicting protein disorder is, therefore, an increasingly important part of protein structure characterisation.
Nutrition.
Most microorganisms and plants can biosynthesize all 20 standard amino acids, while animals (including humans) must obtain some of the amino acids from the diet. The amino acids that an organism cannot synthesize on its own are referred to as essential amino acids. Key enzymes that synthesize certain amino acids are not present in animals — such as aspartokinase, which catalyzes the first step in the synthesis of lysine, methionine, and threonine from aspartate. If amino acids are present in the environment, microorganisms can conserve energy by taking up the amino acids from their surroundings and downregulating their biosynthetic pathways.
In animals, amino acids are obtained through the consumption of foods containing protein. Ingested proteins are then broken down into amino acids through digestion, which typically involves denaturation of the protein through exposure to acid and hydrolysis by enzymes called proteases. Some ingested amino acids are used for protein biosynthesis, while others are converted to glucose through gluconeogenesis, or fed into the citric acid cycle. This use of protein as a fuel is particularly important under starvation conditions as it allows the body's own proteins to be used to support life, particularly those found in muscle. Amino acids are also an important dietary source of nitrogen.
History and etymology.
Proteins were recognized as a distinct class of biological molecules in the eighteenth century by Antoine Fourcroy and others, distinguished by the molecules' ability to coagulate or flocculate under treatments with heat or acid. Noted examples at the time included albumin from egg whites, blood serum albumin, fibrin, and wheat gluten.
Proteins were first described by the Dutch chemist Gerardus Johannes Mulder and named by the Swedish chemist Jöns Jacob Berzelius in 1838. Mulder carried out elemental analysis of common proteins and found that nearly all proteins had the same empirical formula, C400H620N100O120P1S1. He came to the erroneous conclusion that they might be composed of a single type of (very large) molecule. The term "protein" to describe these molecules was proposed by Mulder's associate Berzelius; protein is derived from the Greek word πρώτειος ("proteios"), meaning "primary", "in the lead", or "standing in front", + "-in". Mulder went on to identify the products of protein degradation such as the amino acid leucine for which he found a (nearly correct) molecular weight of 131 Da.
Early nutritional scientists such as the German Carl von Voit believed that protein was the most important nutrient for maintaining the structure of the body, because it was generally believed that "flesh makes flesh." Karl Heinrich Ritthausen extended known protein forms with the identification of glutamic acid. At the Connecticut Agricultural Experiment Station a detailed review of the vegetable proteins was compiled by Thomas Burr Osborne. Working with Lafayette Mendel and applying Liebig's law of the minimum in feeding laboratory rats, the nutritionally essential amino acids were established. The work was continued and communicated by William Cumming Rose. The understanding of proteins as polypeptides came through the work of Franz Hofmeister and Hermann Emil Fischer. The central role of proteins as enzymes in living organisms was not fully appreciated until 1926, when James B. Sumner showed that the enzyme urease was in fact a protein.
The difficulty in purifying proteins in large quantities made them very difficult for early protein biochemists to study. Hence, early studies focused on proteins that could be purified in large quantities, e.g., those of blood, egg white, various toxins, and digestive/metabolic enzymes obtained from slaughterhouses. In the 1950s, the Armour Hot Dog Co. purified 1 kg of pure bovine pancreatic ribonuclease A and made it freely available to scientists; this gesture helped ribonuclease A become a major target for biochemical study for the following decades.
Linus Pauling is credited with the successful prediction of regular protein secondary structures based on hydrogen bonding, an idea first put forth by William Astbury in 1933. Later work by Walter Kauzmann on denaturation, based partly on previous studies by Kaj Linderstrøm-Lang, contributed an understanding of protein folding and structure mediated by hydrophobic interactions.
The first protein to be sequenced was insulin, by Frederick Sanger, in 1949. Sanger correctly determined the amino acid sequence of insulin, thus conclusively demonstrating that proteins consisted of linear polymers of amino acids rather than branched chains, colloids, or cyclols. He won the Nobel Prize for this achievement in 1958.
The first protein structures to be solved were hemoglobin and myoglobin, by Max Perutz and Sir John Cowdery Kendrew, respectively, in 1958. , the Protein Data Bank has over 115,000 atomic-resolution structures of proteins. In more recent times, cryo-electron microscopy of large macromolecular assemblies and computational protein structure prediction of small protein domains are two methods approaching atomic resolution.

</doc>
<doc id="23635" url="https://en.wikipedia.org/wiki?curid=23635" title="Physical chemistry">
Physical chemistry

Physical chemistry is the study of macroscopic, atomic, subatomic, and particulate phenomena in chemical systems in terms of laws and concepts of physics. It applies the principles, practices and concepts of physics such as motion, energy, force, time, thermodynamics, quantum chemistry, statistical mechanics and dynamics, equilibrium.
Physical chemistry, in contrast to chemical physics, is predominantly (but not always) a macroscopic or supra-molecular science, as the majority of the principles on which physical chemistry was founded are concepts related to the bulk rather than on molecular/atomic structure alone (for example, chemical equilibrium and colloids).
Some of the relationships that physical chemistry strives to resolve include the effects of:
Key concepts.
The key concepts of physical chemistry are the ways in which pure physics is applied to chemical problems.
One of the key concepts in classical chemistry is that all chemical compounds can be described as groups of atoms bonded together and chemical reactions can be described as the making and breaking of those bonds. Predicting the properties of chemical compounds from a description of atoms and how they bond is one of the major goals of physical chemistry. To describe the atoms and bonds precisely, it is necessary to know both where the nuclei of the atoms are, and how electrons are distributed around them. how nuclei move, and how light can be absorbed or emitted by a chemical compound. Spectroscopy is the related sub-discipline of physical chemistry which is specifically concerned with the interaction of electromagnetic radiation with matter.
Another set of important questions in chemistry concerns what kind of reactions can happen spontaneously and which properties are possible for a given chemical mixture. This is studied in chemical thermodynamics, which sets limits on quantities like how far a reaction can proceed, or how much energy can be converted into work in an internal combustion engine, and which provides links between properties like the thermal expansion coefficient and rate of change of entropy with pressure for a gas or a liquid. It can frequently be used to assess whether a reactor or engine design is feasible, or to check the validity of experimental data. To a limited extent, quasi-equilibrium and non-equilibrium thermodynamics can describe irreversible changes. However, classical thermodynamics is mostly concerned with systems in equilibrium and reversible changes and not what actually does happen, or how fast, away from equilibrium.
Which reactions do occur and how fast is the subject of chemical kinetics, another branch of physical chemistry. A key idea in chemical kinetics is that for reactants to react and form products, most chemical species must go through transition states which are higher in energy than either the reactants or the products and serve as a barrier to reaction. In general, the higher the barrier, the slower the reaction. A second is that most chemical reactions occur as a sequence of elementary reactions, each with its own transition state. Key questions in kinetics include how the rate of reaction depends on temperature and on the concentrations of reactants and catalysts in the reaction mixture, as well as how catalysts and reaction conditions can be engineered to optimize the reaction rate.
The fact that how fast reactions occur can often be specified with just a few concentrations and a temperature, instead of needing to know all the positions and speeds of every molecule in a mixture, is a special case of another key concept in physical chemistry, which is that to the extent an engineer needs to know, everything going on in a mixture of very large numbers (perhaps of the order of the Avogadro constant, 6 x 1023) of particles can often be described by just a few variables like pressure, temperature, and concentration. The precise reasons for this are described in statistical mechanics, a specialty within physical chemistry which is also shared with physics. Statistical mechanics also provides ways to predict the properties we see in everyday life from molecular properties without relying on empirical correlations based on chemical similarities.
History.
The term "physical chemistry" was coined by Mikhail Lomonosov in 1752, when he presented a lecture course entitled "A Course in True Physical Chemistry" (Russian: «Курс истинной физической химии») before the students of Petersburg University. In the preamble to these lectures he gives definition: "Physical chemistry is the science that must explain under provisions of physical experiments the reason for what is happening in complex bodies through chemical operations".
Modern physical chemistry originated in the 1860s to 1880s with work on chemical thermodynamics, electrolytes in solutions, chemical kinetics and other subjects. One milestone was the publication in 1876 by Josiah Willard Gibbs of his paper, "On the Equilibrium of Heterogeneous Substances". This paper introduced several of the cornerstones of physical chemistry, such as Gibbs energy, chemical potentials, and Gibbs' phase rule. Other milestones include the subsequent naming and accreditation of enthalpy to Heike Kamerlingh Onnes and to macromolecular processes. 
The first scientific journal specifically in the field of physical chemistry was the German journal, "Zeitschrift für Physikalische Chemie", founded in 1887 by Wilhelm Ostwald and Jacobus Henricus van 't Hoff. Together with Svante August Arrhenius, these were the leading figures in physical chemistry in the late 19th century and early 20th century. All three were awarded with the Nobel Prize in Chemistry between 1901-1909.
Developments in the following decades include the application of statistical mechanics to chemical systems and work on colloids and surface chemistry, where Irving Langmuir made many contributions. Another important step was the development of quantum mechanics into quantum chemistry from the 1930s, where Linus Pauling was one of the leading names. Theoretical developments have gone hand in hand with developments in experimental methods, where the use of different forms of spectroscopy, such as infrared spectroscopy, microwave spectroscopy, EPR spectroscopy and NMR spectroscopy, is probably the most important 20th century development.
Further development in physical chemistry may be attributed to discoveries in nuclear chemistry, especially in isotope separation (before and during World War II), more recent discoveries in astrochemistry, as well as the development of calculation algorithms in the field of "additive physicochemical properties" (practically all physicochemical properties, such as boiling point, critical point, surface tension, vapor pressure, etc. - more than 20 in all - can be precisely calculated from chemical structure alone, even if the chemical molecule remains unsynthesized), and in this area is concentrated practical importance of contemporary physical chemistry.
See Group contribution method, Lydersen method, Joback method, Benson group increment theory, QSPR, QSAR
Journals.
Some journals that deal with physical chemistry include:
Historical journals that covered both chemistry and physics include Annales de chimie et de physique (started in 1789, published under the name given here from 1815–1914).

</doc>
<doc id="23636" url="https://en.wikipedia.org/wiki?curid=23636" title="Perimeter">
Perimeter

A perimeter is a path that surrounds a two-dimensional shape. The word comes from the Greek "peri" (around) and "meter" (measure). The term may be used either for the path or its length - it can be thought of as the length of the outline of a shape. The perimeter of a circle or ellipse is called its circumference.
Calculating the perimeter has considerable practical applications. The perimeter can be used to calculate the length of fence required to surround a yard or garden. The perimeter of a wheel (its circumference) describes how far it will roll in one revolution. Similarly, the amount of string wound around a spool is related to the spool's perimeter.
Formulas.
The perimeter is the distance around a shape. Perimeters for more general shapes can be calculated as any path with formula_1 where formula_2 is the length of the path and formula_3 is an infinitesimal line element. Both of these must be replaced with other algebraic forms in order to be solved: an advanced notion of perimeter, which includes hypersurfaces bounding volumes in formula_4-dimensional Euclidean spaces can be found in the theory of Caccioppoli sets.
Polygons.
Polygons are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by approximating them with sequences of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is Archimedes, who approximated the perimeter of a circle by surrounding it with regular polygons.
The perimeter of a polygon equals the sum of the lengths of its edges. In particular, the perimeter of a rectangle which width is formula_5 and length formula_6 is equal to formula_7. 
An equilateral polygon is a polygon which has all sides of the same length (for example, a rhombus is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.
A regular polygon may be defined by the number of its sides and by its circumradius, that is to say, the constant distance between its centre and each of its vertices. One can calculate the length of its sides using trigonometry. If "R" is a regular polygon's radius and "n" is the number of its sides, then its perimeter is 
A splitter of a triangle is a cevian (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the semiperimeter of the triangle. The three splitters of a triangle all intersect each other at the Nagel point of the triangle.
A cleaver of a triangle is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths. The three cleavers of a triangle all intersect each other at the triangle's Spieker center.
Circumference of a circle.
The perimeter of a circle, often called the circumference, is proportional to its diameter and its radius. That is to say, there exists a constant number pi, (the Greek "p" for perimeter), such that if "P" is the circle's perimeter and "D" its diameter then:
In terms of the radius "r" of the circle, this formula becomes:
To calculate a circle's perimeter, knowledge of its radius or diameter and of the number is sufficient. The problem is that is not rational (it cannot be expressed as the quotient of two integers), nor is it algebraic (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of is important for the calculation. The search for the digits of is relevant to many fields, such as mathematical analysis, algorithmics and computer science.
Perception of perimeter.
The perimeter and the area are the main two measures of geometric figures. Confusing them is frequent, as well as believing that the greater one of them is, the greater is the other. Indeed, an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/ scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by . The real area is times the area of the shape on the map.
Nevertheless there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas equal to 1.
Proclus (5th century) reported that Greek peasants "fairly" parted fields relying on their perimeters. But a field's production is proportional to its area, not to its perimeter: many naive peasants may have got fields with long perimeters but low areas (thus, low crops).
If one removes a piece from a figure, its area decreases but its perimeter may not. In the case of very irregular shapes, some people may confuse perimeter with convex hull. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. On the animated picture on the left, all the figures have the same convex hull: the big, first hexagon.
Isoperimetry.
The isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive: it is the circle. In particular, that is why drops of fat on a broth surface are circular.
This problem may seem simple, but its mathematical proof needs sophisticated theorems. The isoperimetric problem is sometimes simplified: to find the quadrilateral, or the triangle or another particular figure, with the largest area amongst those having a given perimeter. The solution to the quadrilateral isoperimetric problem is the square, and the solution to the triangle problem is the equilateral triangle. In general, the polygon with "n" sides having the largest area and a given perimeter is the regular polygon, which is closer to being a circle than is an irregular polygon.

</doc>
<doc id="23637" url="https://en.wikipedia.org/wiki?curid=23637" title="Phase (matter)">
Phase (matter)

In the physical sciences, a phase is a region of space (a thermodynamic system), throughout which all physical properties of a material are essentially uniform. Examples of physical properties include density, index of refraction, magnetization and chemical composition. A simple description is that a phase is a region of material that is chemically uniform, physically distinct, and (often) mechanically separable. In a system consisting of ice and water in a glass jar, the ice cubes are one phase, the water is a second phase, and the humid air over the water is a third phase. The glass of the jar is another separate phase. (See state of matter#Glass)
The term "phase" is sometimes used as a synonym for state of matter, but there can be several immiscible phases of the same state of matter. Also, the term "phase" is sometimes used to refer to a set of equilibrium states demarcated in terms of state variables such as pressure and temperature by a phase boundary on a phase diagram. Because phase boundaries relate to changes in the organization of matter, such as a change from liquid to solid or a more subtle change from one crystal structure to another, this latter usage is similar to the use of "phase" as a synonym for state of matter. However, the state of matter and phase diagram usages are not commensurate with the formal definition given above and the intended meaning must be determined in part from the context in which the term is used.
Types of phases.
Distinct phases may be described as different states of matter such as gas, liquid, solid, plasma or Bose–Einstein condensate. Useful mesophases between solid and liquid form other states of matter.
Distinct phases may also exist within a given state of matter. As shown in the diagram for iron alloys, several phases exist for both the solid and liquid states. Phases may also be differentiated based on solubility as in polar (hydrophilic) or non-polar (hydrophobic). A mixture of water (a polar liquid) and oil (a non-polar liquid) will spontaneously separate into two phases. Water has a very low solubility (is insoluble) in oil, and oil has a low solubility in water. Solubility is the maximum amount of a solute that can dissolve in a solvent before the solute ceases to dissolve and remains in a separate phase. A mixture can separate into more than two liquid phases and the concept of phase separation extends to solids, i.e., solids can form solid solutions or crystallize into distinct crystal phases. Metal pairs that are mutually soluble can form alloys, whereas metal pairs that are mutually insoluble cannot.
As many as eight immiscible liquid phases have been observed. Mutually immiscible liquid phases are formed from water (aqueous phase), hydrophobic organic solvents, perfluorocarbons (fluorous phase), silicones, several different metals, and also from molten phosphorus. Not all organic solvents are completely miscible, e.g. a mixture of ethylene glycol and toluene may separate into two distinct organic phases.
Phases do not need to macroscopically separate spontaneously. Emulsions and colloids are examples of immiscible phase pair combinations that do not physically separate.
Phase equilibrium.
Left to equilibration, many compositions will form a uniform single phase, but depending on the temperature and pressure even a single substance may separate into two or more distinct phases. Within each phase, the properties are uniform but between the two phases properties differ.
Water in a closed jar with an air space over it forms a two phase system. Most of the water is in the liquid phase, where it is held by the mutual attraction of water molecules. Even at equilibrium molecules are constantly in motion and, once in a while, a molecule in the liquid phase gains enough kinetic energy to break away from the liquid phase and enter the gas phase. Likewise, every once in a while a vapor molecule collides with the liquid surface and condenses into the liquid. At equilibrium, evaporation and condensation processes exactly balance and there is no net change in the volume of either phase.
At room temperature and pressure, the water jar reaches equilibrium when the air over the water has a humidity of about 3%. This percentage increases as the temperature goes up. At 100 °C and atmospheric pressure, equilibrium is not reached until the air is 100% water. If the liquid is heated a little over 100 °C, the transition from liquid to gas will occur not only at the surface, but throughout the liquid volume: the water boils.
Number of phases.
For a given composition, only certain phases are possible at a given temperature and pressure. The number and type of phases that will form is hard to predict and is usually determined by experiment. The results of such experiments can be plotted in phase diagrams.
The phase diagram shown here is for a single component system. In this simple system, which phases that are possible depends only on pressure and temperature. The markings show points where two or more phases can co-exist in equilibrium. At temperatures and pressures away from the markings, there will be only one phase at equilibrium.
In the diagram, the blue line marking the boundary between liquid and gas does not continue indefinitely, but terminates at a point called the critical point. As the temperature and pressure approach the critical point, the properties of the liquid and gas become progressively more similar. At the critical point, the liquid and gas become indistinguishable. Above the critical point, there are no longer separate liquid and gas phases: there is only a generic fluid phase referred to as a supercritical fluid. In water, the critical point occurs at around 647 K (374 °C or 705 °F) and 22.064 MPa.
An unusual feature of the water phase diagram is that the solid–liquid phase line (illustrated by the dotted green line) has a negative slope. For most substances, the slope is positive as exemplified by the dark green line. This unusual feature of water is related to ice having a lower density than liquid water. Increasing the pressure drives the water into the higher density phase, which causes melting.
Another interesting though not unusual feature of the phase diagram is the point where the solid–liquid phase line meets the liquid–gas phase line. The intersection is referred to as the triple point. At the triple point, all three phases can coexist.
Experimentally, the phase lines are relatively easy to map due to the interdependence of temperature and pressure that develops when multiple phases forms. See Gibbs' phase rule. Consider a test apparatus consisting of a closed and well insulated cylinder equipped with a piston. By charging the right amount of water and applying heat, the system can be brought to any point in the gas region of the phase diagram. If the piston is slowly lowered, the system will trace a curve of increasing temperature and pressure within the gas region of the phase diagram. At the point where gas begins to condense to liquid, the direction of the temperature and pressure curve will abruptly change to trace along the phase line until all of the water has condensed.
Interfacial phenomena.
Between two phases in equilibrium there is a narrow region where the properties are not that of either phase. Although this region may be very thin, it can have significant and easily observable effects, such as causing a liquid to exhibit surface tension. In mixtures, some components may preferentially move toward the interface. In terms of modeling, describing, or understanding the behavior of a particular system, it may be efficacious to treat the interfacial region as a separate phase.
Crystal phases.
A single material may have several distinct solid states capable of forming separate phases. Water is a well-known example of such a material. For example, water ice is ordinarily found in the hexagonal form ice Ih, but can also exist as the cubic ice Ic, the rhombohedral ice II, and many other forms. Polymorphism is the ability of a solid to exist in more than one crystal form. For pure chemical elements, polymorphism is known as allotropy. For example, diamond, graphite, and fullerenes are different allotropes of carbon.
Phase transitions.
When a substance undergoes a phase transition (changes from one state of matter to another) it usually either takes up or releases energy. For example, when water evaporates, the increase in kinetic energy as the evaporating molecules escape the attractive forces of the liquid is reflected in a decrease in temperature. The energy required to induce the phase transition is taken from the internal thermal energy of the water, which cools the liquid to a lower temperature; hence evaporation is useful for cooling. See Enthalpy of vaporization. The reverse process, condensation, releases heat. The heat energy, or enthalpy, associated with a solid to liquid transition is the enthalpy of fusion and that associated with a solid to gas transition is the enthalpy of sublimation.

</doc>
<doc id="23638" url="https://en.wikipedia.org/wiki?curid=23638" title="Outline of physical science">
Outline of physical science

Physical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a "physical science", together called the "physical sciences". However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena and other branches of chemistry such as organic chemistry.
What is physical science?
Physical science can be described as all of the following:
History of physical science.
History of physical science – history of the branch of natural science that studies non-living systems, in contrast to the biological sciences. It in turn has many branches, each referred to as a "physical science", together called the "physical sciences". However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena (organic chemistry, for example).
General principles of the physical sciences.
Basic principles of physics.
Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the "fundamental sciences" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:
Basic principles of astronomy.
Astronomy – science of celestial bodies and their interactions in space. Its studies includes the following:
Basic principles of chemistry.
Chemistry – branch of science that studies the composition, structure, properties and change of matter. Chemistry is chiefly concerned with atoms and molecules and their interactions and transformations, for example, the properties of the chemical bonds formed between atoms to create chemical compounds. As such, chemistry studies the involvement of electrons and various forms of energy in photochemical reactions, oxidation-reduction reactions, changes in phases of matter, and separation of mixtures. Preparation and properties of complex substances, such as alloys, polymers, biological molecules, and pharmaceutical agents are considered in specialized fields of chemistry.
Basic principles of earth science.
Earth science – the science of the planet Earth, the only identified life-bearing planet. Its studies include the following:

</doc>
<doc id="23639" url="https://en.wikipedia.org/wiki?curid=23639" title="Gasoline">
Gasoline

Gasoline , also known as petrol outside North America, is a transparent, petroleum-derived liquid that is used primarily as a fuel in internal combustion engines. It consists mostly of organic compounds obtained by the fractional distillation of petroleum, enhanced with a variety of additives.
On average, a 42-gallon barrel of crude oil (159 L) yields about of gasoline when processed in an oil refinery, though this can and does vary based on the crude oil source's assay.
The characteristic of a particular gasoline blend to resist igniting too early (which causes knocking and reduces efficiency in reciprocating engines) is measured by its octane rating. Gasoline is produced in several grades of octane rating. Tetraethyllead and other lead compounds are no longer used in most areas to regulate and increase octane-rating, but many other additives are put into gasoline to improve its chemical stability, control corrosiveness and provide fuel system 'cleaning,' and determine performance characteristics under intended use. Sometimes, gasoline also contains ethanol as an alternative fuel, for economic or environmental reasons.
Gasoline, as used worldwide in the vast number of internal combustion engines used in transport and industry, has a significant impact on the environment, both in local effects (e.g., smog) and in global effects (e.g., effect on the climate). Gasoline may also enter the environment uncombusted, as liquid and as vapors, from leakage and handling during production, transport and delivery, from storage tanks, from spills, etc. As an example of efforts to control such leakage, many (underground) storage tanks are required to have extensive measures in place to detect and prevent such leaks. Gasoline contains benzene and other known carcinogens.
Octane rating.
Spark ignition engines are designed to burn gasoline in a controlled process called deflagration. However, the unburned mixture may autoignite by detonating from pressure and heat alone, rather than ignite from the spark plug at exactly the right time. This causes a rapid pressure rise which can damage the engine. This is often referred to as engine knocking or end-gas knock. Knocking can be reduced by increasing the gasoline's resistance to autoignition, which is expressed by its octane rating.
Octane rating is measured relative to a mixture of 2,2,4-trimethylpentane (an isomer of octane) and n-heptane. There are different conventions for expressing octane ratings, so the same physical fuel may have several different octane ratings based on the measure used. One of the best known is the research octane number (RON).
The octane rating of typical commercially available gasoline varies by country. In Finland, Sweden, and Norway, 95 RON is the standard for regular unleaded gasoline and 98 RON is also available as a more expensive option. In the UK, ordinary regular unleaded gasoline is 95 RON (commonly available), premium unleaded gasoline is always 97 RON, and super unleaded is usually 97-98 RON. However, both Shell and BP produce fuel at 102 RON for cars with high-performance engines and in 2006 the supermarket chain Tesco began to sell super unleaded gasoline rated at 99 RON. In the US, octane ratings in unleaded fuels can vary between 85 and 87 AKI (91-92 RON) for regular, through 89-90 AKI (94-95 RON) for mid-grade (equivalent to European regular), up to 90-94 AKI (95-99 RON) for premium (European premium).
South Africa's largest city, Johannesburg, is located on the Highveld at above sea level. So the South African AA recommends 95 octane gasoline (petrol) at low altitude and 93 octane for use in Johannesburg because "The higher the altitude the lower the air pressure, and the lower the need for a high octane fuel as there is no real performance gain".
The octane rating became important as the military sought higher output for aircraft engines in the late 1930s and the 1940s. A higher octane rating allows a higher compression ratio or supercharger boost, and thus higher temperatures and pressures, which translate to higher power output. Some scientists even predicted that a nation with a good supply of high octane gasoline would have the advantage in air power. In 1943, the Rolls Royce Merlin aero engine produced 1320 horsepower (984 kW) using 100 RON fuel from a modest 27 liter displacement. Towards the end of the second world war, experiments were conducted using 150 RON fuel (100/150 avgas), obtained by adding 2.5% aniline to 100 octane avgas.
Stability.
Quality gasoline should be stable for six months if stored properly but gasoline will break down slowly over time due to the separation of the components. Gasoline stored for a year will most likely be able to be burned in an internal combustion engine without too much trouble but the effects of long term storage will become more noticeable with each passing month until a time comes when the gasoline should be diluted with ever increasing amounts of freshly made fuel so that the older gasoline may be used up. If left undiluted, improper operation will occur and this may include engine damage from misfiring and/or the lack of proper action of the fuel within a fuel injection system and from an onboard computer attempting to compensate (if applicable to the vehicle). Storage should be in an airtight container (to prevent oxidation or water vapors mixing in with the gas) that can withstand the vapor pressure of the gasoline without venting (to prevent the loss of the more volatile fractions) at a stable cool temperature (to reduce the excess pressure from liquid expansion, and to reduce the rate of any decomposition reactions). When gasoline is not stored correctly, gums and solids may be created, which can corrode system components and accumulate on wetted surfaces, resulting in a condition called "stale fuel". Gasoline containing ethanol is especially subject to absorbing atmospheric moisture, then forming gums, solids, or two phases (a hydrocarbon phase floating on top of a water-alcohol phase).
The presence of these degradation products in the fuel tank, fuel lines plus a carburetor or fuel injection components makes it harder to start the engine or causes reduced engine performance. On resumption of regular engine use, the buildup may or may not be eventually cleaned out by the flow of fresh gasoline. The addition of a fuel stabilizer to gasoline can extend the life of fuel that is not or cannot be stored properly though removal of all fuel from a fuel system is the only real solution to the problem of long term storage of an engine or a machine or vehicle. Some typical fuel stabilizers are proprietary mixtures containing mineral spirits, isopropyl alcohol, 1,2,4-trimethylbenzene, or other additives. Fuel stabilizer is commonly used for small engines, such as lawnmower and tractor engines, especially when their use is seasonal (low to no use for one or more seasons of the year). Users have been advised to keep gasoline containers more than half full and properly capped to reduce air exposure, to avoid storage at high temperatures, to run an engine for ten minutes to circulate the stabilizer through all components prior to storage, and to run the engine at intervals to purge stale fuel from the carburetor.
Gasoline stability requirements are set in standard ASTM D4814-14b. The standard describes the various characteristics and requirements of automotive fuels for use over a wide range of operating conditions in ground vehicles equipped with spark-ignition engines.
Energy content.
Energy is obtained from the combustion of gasoline by the conversion of a hydrocarbon to carbon dioxide and water. The combustion of octane follows this reaction:
Gasoline contains about 42.4 MJ/kg (120 MJ/US gal, 33.3 kWh/US gal, 120,000 BTU/US gal) quoting the lower heating value.
A high-octane-rated fuel, such as liquefied petroleum gas (LPG) has an overall lower power output at the typical 10:1 compression ratio of a gasoline engine. However, with an engine tuned to the use of LPG (i.e. via higher compression ratios, such as 12:1 instead of 10:1), the power output can be improved. This is because higher-octane fuels allow for a higher compression ratio without knocking, resulting in a higher cylinder temperature, which improves efficiency. Also, increased mechanical efficiency is created by a higher compression ratio through the concomitant higher expansion ratio on the power stroke, which is by far the greater effect. The higher expansion ratio extracts more work from the high-pressure gas created by the combustion process. An Atkinson cycle engine uses the timing of the valve events to produce the benefits of a high expansion ratio without the disadvantages, chiefly detonation, of a high compression ratio. A high expansion ratio is also one of the two key reasons for the efficiency of diesel engines, along with the elimination of pumping losses due to throttling of the intake air flow.
The lower energy content (per liter) of LPG in comparison to gasoline is due mainly to its lower density. Energy content per kilogram is higher than for gasoline (higher hydrogen to carbon ratio, for an example see ).
Molecular weights of the above reagents are C8H18 114, O2 32, CO2 44, H2O 18; therefore 1 kg of fuel reacts with 3.51 kg of oxygen to produce 3.09 kg of carbon dioxide and 1.42 kg of water.
Density.
The density of gasoline ranges from 0.71–0.77 kg/L ( ; 0.026 lb/in3; 6.073 lb/US gal; 7.29 lb/imp gal), higher densities having a greater volume of aromatics. Since gasoline floats on water, water cannot generally be used to extinguish a gasoline fire unless used in a fine mist.
Finished marketable gasoline is traded with a standard reference of 0.755 kg/L, and its price is escalated/de-escalated according to its actual density.
Chemical analysis and production.
Gasoline is produced in oil refineries. Roughly 19 US gallons (72 L) of gasoline is derived from a 42-gallon (159 L) barrel of crude oil. Material separated from crude oil via distillation, called virgin or straight-run gasoline, does not meet specifications for modern engines (particularly the octane rating, see below), but can be pooled to the gasoline blend.
The bulk of a typical gasoline consists of hydrocarbons with between 4 and 12 carbon atoms per molecule (commonly referred to as C4-C12). It is a mixture of paraffins (alkanes), cycloalkanes (naphthenes), and olefins (alkenes), where the usage of the terms paraffin and olefin is particular to the oil industry. The actual ratio depends on:
The various refinery streams blended to make gasoline have different characteristics. Some important streams are:
The terms above are the jargon used in the oil industry and terminology varies.
Currently, many countries set limits on gasoline aromatics in general, benzene in particular, and olefin (alkene) content. Such regulations led to increasing preference for high octane pure paraffin (alkane) components, such as alkylate, and is forcing refineries to add processing units to reduce benzene content. In the EU the benzene limit is set at 1% volume for all grade of automotive gasoline.
Gasoline can also contain other organic compounds, such as organic ethers (deliberately added), plus small levels of contaminants, in particular organosulfur compounds, but these are usually removed at the refinery.
Additives.
Inhaled (huffed) gasoline vapor is a common intoxicant that has become epidemic in some poorer communities and indigenous groups in Australia, Canada, New Zealand, and some Pacific Islands.
Antiknock additives.
Almost all countries in the world have phased out automotive leaded fuel. In 2011 six countries in the world were still using leaded gasoline: Afghanistan, Myanmar, North Korea, Algeria, Iraq and Yemen. It was expected that by the end of 2013 those countries would ban leaded gasoline, but it has not occurred. Algeria will replace leaded with unleaded automotive fuel only in 2015. 
Different additives have replaced the lead compounds. The most popular additives include aromatic hydrocarbons, ethers and alcohol (usually ethanol or methanol).
For technical reasons the use of leaded additives is still permitted world-wide for the formulation of some grades of aviation gasoline such as 100LL, because the required octane rating would be technically infeasible to reach without the use of leaded additives.
Tetraethyllead.
Gasoline, when used in high-compression internal combustion engines, tends to autoignite ("detonate") causing damaging "engine knocking" (also called "pinging" or "pinking") noise. To address this problem, tetraethyllead (TEL) was widely adopted as an additive for gasoline in the 1920s. With the discovery of the extent of environmental and health damage caused by the lead, however, and the incompatibility of lead with catalytic converters, leaded gasoline was phased out in the USA beginning in 1973. By 1995, leaded fuel accounted for only 0.6% of total gasoline sales and under 2000 short tons (1814 t) of lead per year in the USA. From 1 January 1996, the U.S. Clean Air Act banned the sale of leaded fuel for use in on-road vehicles in the USA. The use of TEL also necessitated other additives, such as dibromoethane.
First European countries started replacing lead by the end of the 1980s and by the end of the 1990s leaded gasoline was banned within the entire European Union. Reduction in the average blood lead level is believed to have been a major cause for falling violent crime rates in the United States and South Africa. A statistically significant correlation has been found between the usage rate of leaded gasoline and violent crime: taking into account a 22-year time lag, the violent crime curve virtually tracks the lead exposure curve.
MMT.
Methylcyclopentadienyl manganese tricarbonyl (MMT) is used in Canada and in Australia to boost octane. It also helps old cars designed for leaded fuel run on unleaded fuel without need for additives to prevent valve problems. Its use in the US has been restricted by regulations.
Fuel stabilizers (antioxidants and metal deactivators).
Gummy, sticky resin deposits result from oxidative degradation of gasoline upon long term storage. These harmful deposits arise from the oxidation of alkenes and other minor components in gasoline (see drying oils). Improvements in refinery techniques have generally reduced the susceptibility of gasolines to these problems. Previously, catalytically or thermally cracked gasolines are most susceptible to oxidation. The formation of these gums is accelerated by copper salts, which can be neutralized by additives called metal deactivators.
This degradation can be prevented through the addition of 5–100 ppm of antioxidants, such as phenylenediamines and other amines. Hydrocarbons with a bromine number of 10 or above can be protected with the combination of unhindered or partially hindered phenols and oil-soluble strong amine bases, such as hindered phenols. "Stale" gasoline can be detected by a colorimetric enzymatic test for organic peroxides produced by oxidation of the gasoline.
Gasolines are also treated with metal deactivators, which are compounds that sequester (deactivate) metal salts that otherwise accelerate the formation of gummy residues. The metal impurities might arise from the engine itself or as contaminants in the fuel.
Detergents.
Gasoline, as delivered at the pump, also contains additives to reduce internal engine carbon buildups, improve combustion, and to allow easier starting in cold climates. High levels of detergent can be found in Top Tier Detergent Gasolines. The specification for Top Tier Detergent gasolines was developed by four automakers: GM, Honda, Toyota and BMW. According to the bulletin, the minimal EPA requirement is not sufficient to keep engines clean. Typical detergents include alkylamines and alkyl phosphates at the level of 50-100 ppm.
Ethanol.
European Union.
In the EU, 5% ethanol can be added within the common gasoline spec (EN 228). Discussions are ongoing to allow 10% blending of ethanol (available in Finnish, French and German gas stations). In Finland most gasoline stations sell 95E10, which is 10% of ethanol; and 98E5, which is 5% ethanol. Most gasoline sold in Sweden has 5-15% ethanol added.
Brazil.
In Brazil, the Brazilian National Agency of Petroleum, Natural Gas and Biofuels (ANP) requires gasoline for automobile use to have 27.5% of ethanol added to its composition. Pure hydrated ethanol is also available as a fuel.
Australia.
Legislation requires retailers to label fuels containing ethanol on the dispenser, and limits ethanol use to 10% of gasoline in Australia. Such gasoline is commonly called E10 by major brands, and it is cheaper than regular unleaded gasoline.
United States.
The federal Renewable Fuel Standard (RFS) effectively requires refiners and blenders to blend renewable biofuels (mostly ethanol) with gasoline, sufficient to meet a growing annual target of total gallons blended. Although the mandate does not require a specific percentage of ethanol, annual increases in the target combined with declining gasoline consumption has caused the typical ethanol content in gasoline to approach 10%. Most fuel pumps display a sticker that states that the fuel may contain up to 10% ethanol, an intentional disparity that reflects the varying actual percentage. Until late 2010, fuels retailers were only authorized to sell fuel containing up to 10 percent ethanol (E10), and most vehicle warranties (except for flexible fuel vehicles) authorize fuels that contain no more than 10 percent ethanol. In parts of the United States, ethanol is sometimes added to gasoline without an indication that it is a component.
India.
The Government of India in October 2007 decided to make 5% ethanol blending (with gasoline) mandatory. Currently, 10% Ethanol blended product (E10) is being sold in various parts of the country.
Ethanol has been found in at least one study to damage catalytic converters.
Dye.
In Australia, the lowest grade of gasoline (RON 91) is dyed a light shade of red/orange and the medium grade (RON 95) is dyed yellow.
In the United States, aviation gasoline (avgas) is dyed to identify its octane rating and to distinguish it from kerosene-based jet fuel, which is clear.
In Canada the gasoline for marine and farm use is dyed red and is not subject to road tax .
Oxygenate blending.
Oxygenate blending adds oxygen-bearing compounds such as MTBE, ETBE, ethanol, and biobutanol. The presence of these oxygenates reduces the amount of carbon monoxide and unburned fuel in the exhaust gas. In many areas throughout the US, oxygenate blending is mandated by EPA regulations to reduce smog and other airborne pollutants. For example, in Southern California, fuel must contain 2% oxygen by weight, resulting in a mixture of 5.6% ethanol in gasoline. The resulting fuel is often known as reformulated gasoline (RFG) or oxygenated gasoline, or in the case of California, California reformulated gasoline. The federal requirement that RFG contain oxygen was dropped on 6 May 2006 because the industry had developed VOC-controlled RFG that did not need additional oxygen.
MTBE was phased out in the US due to ground water contamination and the resulting regulations and lawsuits. Ethanol and, to a lesser extent, the ethanol-derived ETBE are common replacements. A common ethanol-gasoline mix of 10% ethanol mixed with gasoline is called gasohol or E10, and an ethanol-gasoline mix of 85% ethanol mixed with gasoline is called E85. The most extensive use of ethanol takes place in Brazil, where the ethanol is derived from sugarcane. In 2004, over 3.4 billion US gallons (2.8 billion imp gal/13 million m³) of ethanol was produced in the United States for fuel use, mostly from corn, and E85 is slowly becoming available in much of the United States, though many of the relatively few stations vending E85 are not open to the general public. The use of bioethanol, either directly or indirectly by conversion of such ethanol to bio-ETBE, is encouraged by the European Union Directive on the Promotion of the use of biofuels and other renewable fuels for transport. Since producing bioethanol from fermented sugars and starches involves distillation, though, ordinary people in much of Europe cannot legally ferment and distill their own bioethanol at present (unlike in the US, where getting a BATF distillation permit has been easy since the 1973 oil crisis).
Safety.
Environmental considerations.
Combustion of of gasoline produces of carbon dioxide (2.3 kg/l), a greenhouse gas.
The main concern with gasoline on the environment, aside from the complications of its extraction and refining, is the potential effect on the climate. Unburnt gasoline and evaporation from the tank, when in the atmosphere, reacts in sunlight to produce photochemical smog. Vapor pressure initially rises with some addition of ethanol to gasoline, but the increase is greatest at 10% by volume. At higher concentrations of ethanol above 10%, the vapor pressure of the blend starts to decrease. At a 10% ethanol by volume, the rise in vapor pressure may potentially increase the problem of photochemical smog. This rise in vapor pressure could be mitigated by increasing or decreasing the percentage of ethanol in the gasoline mixture.
The chief risks of such leaks come not from vehicles, but from gasoline delivery truck accidents and leaks from storage tanks. Because of this risk, most (underground) storage tanks now have extensive measures in place to detect and prevent any such leaks, such as monitoring systems (Veeder-Root, Franklin Fueling).
Toxicity.
The safety data sheet for unleaded gasoline shows at least 15 hazardous chemicals occurring in various amounts, including benzene (up to 5% by volume), toluene (up to 35% by volume), naphthalene (up to 1% by volume), trimethylbenzene (up to 7% by volume), methyl "tert"-butyl ether (MTBE) (up to 18% by volume, in some states) and about ten others. Hydrocarbons in gasoline generally exhibit low acute toxicities, with LD50 of 700 – 2700 mg/kg for simple aromatic compounds. Benzene and many antiknocking additives are carcinogenic.
People can be exposed to gasoline in the workplace by swallowing it, breathing in vapors, skin contact, and eye contact. The National Institute for Occupational Safety and Health (NIOSH) has designated gasoline as a carcinogen.
Inhalation.
Huffed gasoline is a common intoxicant that has become epidemic in some poorer communities and indigenous groups in Australia, New Zealand, and some Pacific Islands. In response, Opal fuel has been developed by the BP Kwinana refinery in Australia, and contains only 5% aromatics (unlike the usual 25%), which weakens the effects of inhalation.
Flammability.
Like other hydrocarbons, gasoline burns in a limited range of its vapor phase and, coupled with its volatility, this makes leaks highly dangerous when sources of ignition are present. Gasoline has a lower explosion limit of 1.4% by volume and an upper explosion limit of 7.6%. If the concentration is below 1.4%, the air-gasoline mixture is too lean and does not ignite. If the concentration is above 7.6%, the mixture is too rich and also does not ignite. However, gasoline vapor rapidly mixes and spreads with air, making unconstrained gasoline quickly flammable.
Use and pricing.
The United States accounts for about 44% of the world’s gasoline consumption. In 2003 The US consumed , which equates to of gasoline each day. The US used about of gasoline in 2006, of which 5.6% was mid-grade and 9.5% was premium grade.
Europe.
Unlike the US, countries in Europe impose substantial taxes on fuels such as gasoline. The price of gasoline in Europe is typically about three times that in the US.
United States.
From 1998 to 2004, the price of gasoline fluctuated between $1 and $2 USD per U.S. gallon. After 2004, the price increased until the average gas price reached a high of $4.11 per U.S. gallon in mid-2008, but receded to approximately $2.60 per U.S. gallon by September 2009. More recently, the U.S. experienced an upswing in gas prices through 2011, and by 1 March 2012, the national average was $3.74 per gallon.
In the United States, most consumer goods bear pre-tax prices, but gasoline prices are posted with taxes included. Taxes are added by federal, state, and local governments. As of 2009, the federal tax is 18.4¢ per gallon for gasoline and 24.4¢ per gallon for diesel (excluding red diesel). Among states, the highest gasoline tax rates, including the federal taxes as of 2005, are New York (62.9¢/gal), Hawaii (60.1¢/gal), and California (60¢/gal). However, many states' taxes are a percentage and thus vary in amount depending on the cost of the gasoline.
About 9% of all gasoline sold in the US in May 2009 was premium grade, according to the Energy Information Administration. "Consumer Reports" magazine says, "If owner’s manual says to use regular fuel, do so—there’s no advantage to a higher grade." The Associated Press said premium gas—which is a higher octane and costs more per gallon than regular unleaded—should be used only if the manufacturer says it is "required". Cars with turbocharged engines and high compression ratios often specify premium gas because higher octane fuels reduce the incidence of "knock", or fuel pre-detonation.
The price of gas varies during the summer and winter months.
History.
The first automotive combustion engines, so-called Otto engines, were developed in the last quarter of the 19th century in Germany. The fuel was a relatively volatile hydrocarbon obtained from coal gas. With a boiling point near 85 °C (octanes boil about 40 °C higher), it was well suited for early carburetors (evaporators). The development of a "spray nozzle" carburetor enabled the use of less volatile fuels. Further improvements in engine efficiency were attempted at higher compression ratios, but early attempts were blocked by knocking (premature explosion of fuel). In the 1920s, antiknock compounds were introduced by Thomas Midgley, Jr. and Boyd, specifically tetraethyllead (TEL). This innovation started a cycle of improvements in fuel efficiency that coincided with the large-scale development of oil refining to provide more products in the boiling range of gasoline. In the 1950s oil refineries started to focus on high octane fuels, and then detergents were added to gasoline to clean the jets in carburetors. The 1970s witnessed greater attention to the environmental consequences of burning gasoline. These considerations led to the phasing out of TEL and its replacement by other antiknock compounds. Subsequently, low-sulfur gasoline was introduced, in part to preserve the catalysts in modern exhaust systems.
Etymology and terminology.
"Gasoline" is cited (under the spelling "gasolene") from 1863 in the "Oxford English Dictionary". It was never a trademark, although it may have been derived from older trademarks such as "Cazeline" and "Gazeline".
Variant spellings of "gasoline" have been used to refer to raw petroleum since the 16th century. "Petrol" was first used as the name of a refined petroleum product around 1870 by British wholesaler Carless, Capel & Leonard, who marketed it as a solvent. When the product later found a new use as a motor fuel, Frederick Simms, an associate of Gottlieb Daimler, suggested to Carless that they register the trade mark "petrol", but by this time the word was already in general use, possibly inspired by the French "pétrole", and the registration was not allowed. Carless registered a number of alternative names for the product, while their competitors used the term "motor spirit" until the 1930s.
In many countries, gasoline has a colloquial name derived from that of the chemical benzene ("e.g.", German "Benzin", Czech "benzina", Dutch "benzine", Italian "benzina", Polish "benzyna", Chilean Spanish "bencina", Thai เบนซิน "bayn sin ", Greek βενζίνη "venzini", Romanian "benzină", Swedish "bensin", Arabic بنزين "binzīn"). Argentina, Uruguay, Paraguay and Italy use the colloquial name "nafta" derived from that of the chemical naphtha.
The terms "mogas", short for motor gasoline, or "autogas", short for automobile gasoline, are used to distinguish automobile fuel from aviation fuel, or "avgas".
Comparison with other fuels.
Volumetric and mass energy density of some fuels compared with gasoline (in the rows with gross and net, they are from):
(*) Diesel fuel is not used in a gasoline engine, so its low octane rating is not an issue; the relevant metric for diesel engines is the cetane number
External links.
Images

</doc>
<doc id="23640" url="https://en.wikipedia.org/wiki?curid=23640" title="Pentose">
Pentose

A pentose is a monosaccharide with five carbon atoms. Pentoses are organized into two groups. Aldopentoses have an aldehyde functional group at position 1. Ketopentoses have a ketone functional group in position 2 or 3.
Aldopentoses.
The aldopentoses have three chiral centers and therefore eight different (2^3) stereoisomers are possible.
Ketopentoses.
The 2-ketopentoses have two chiral centers, and therefore four different stereoisomers are possible (2^2). The 3-ketopentoses are rare.
Properties.
The aldehyde and ketone functional groups in these carbohydrates react with neighbouring hydroxyl functional groups to form intramolecular hemiacetals and hemiketals, respectively. The resulting ring structure is related to furan, and is termed a furanose. The ring spontaneously opens and closes, allowing rotation to occur about the bond between the carbonyl group and the neighbouring carbon atom — yielding two distinct configurations (α and β). This process is termed mutarotation.
Ribose is a constituent of RNA, and the related deoxyribose of DNA.
A polymer composed of pentose sugars is called a pentosan.
Tollens’ test for pentoses.
The Tollens’ test for pentoses relies on reaction of the furfural with phloroglucinol to produce a colored compound with high molar absorptivity.

</doc>
<doc id="23643" url="https://en.wikipedia.org/wiki?curid=23643" title="Propane">
Propane

Propane () is a three-carbon alkane with the molecular formula , a gas, at standard temperature and pressure, but compressible to a transportable liquid. A by-product of natural gas processing and petroleum refining, it is commonly used as a fuel for engines, oxy-gas torches, portable stoves, and residential central heating. Propane is one of a group of liquefied petroleum gases (LP gases). The others include butane, propylene, butadiene, butylene, isobutylene and mixtures thereof.
History.
Propane was first identified as a volatile component in gasoline by Walter O. Snelling of the U.S. Bureau of Mines in 1910. The volatility of these lighter hydrocarbons caused them to be known as "wild" because of the high vapor pressures of unrefined gasoline. On March 31, the "New York Times" reported on Snelling's work with liquefied gas and that "a steel bottle will carry enough gas to light an ordinary home for three weeks."
It was during this time that Snelling, in cooperation with Frank P. Peterson, Chester Kerr, and Arthur Kerr, created ways to liquefy the LP gases during the refining of natural gasoline. Together, they established American Gasol Co., the first commercial marketer of propane. Snelling had produced relatively pure propane by 1911, and on March 25, 1913, his method of processing and producing LP gases was issued patent #1,056,845. A separate method of producing LP gas through compression was created by Frank Peterson and its patent granted on July 2, 1912.
The 1920s saw increased production of LP gas, with the first year of recorded production totaling in 1922. In 1927, annual marketed LP gas production reached , and by 1935, the annual sales of LP gas had reached . Major industry developments in the 1930s included the introduction of railroad tank car transport, gas odorization, and the construction of local bottle-filling plants. The year 1945 marked the first year that annual LP gas sales reached a billion gallons. By 1947, 62% of all U.S. homes had been equipped with either natural gas or propane for cooking.
In 1950, 1,000 propane-fueled buses were ordered by the Chicago Transit Authority, and by 1958, sales in the U.S. had reached annually. In 2004 it was reported to be a growing $8-billion to $10-billion industry with over of propane being used annually in the U.S.
The "prop-" root found in "propane" and names of other compounds with three-carbon chains was derived from "propionic acid".
Sources.
Propane is produced as a by-product of two other processes, natural gas processing and petroleum refining. The processing of natural gas involves removal of butane, propane, and large amounts of ethane from the raw gas, in order to prevent condensation of these volatiles in natural gas pipelines. Additionally, oil refineries produce some propane as a by-product of cracking petroleum into gasoline or heating oil. 
The supply of propane cannot easily be adjusted to meet increased demand, because of the by-product nature of propane production. About 90% of U.S. propane is domestically produced. The United States imports about 10% of the propane consumed each year, with about 70% of that coming from Canada via pipeline and rail. The remaining 30% of imported propane comes to the United States from other sources via ocean transport.
After it is produced, North American propane is stored in huge salt caverns. Examples of these are Fort Saskatchewan, Alberta; Mont Belvieu, Texas and Conway, Kansas. These salt caverns were hollowed out in the 1940s, and they can store or more of propane. When the propane is needed, much of it is shipped by pipelines to other areas of the United States. Propane is also shipped by truck, ship, barge, and railway to many U.S. areas.
Propane can also be produced as a biofuel.
Properties and reactions.
Propane undergoes combustion reactions in a similar fashion to other alkanes. In the presence of excess oxygen, propane burns to form water and carbon dioxide.
When not enough oxygen is present for complete combustion, incomplete combustion occurs, allowing carbon monoxide and/or soot (carbon) to be formed as well:
Unlike natural gas, propane is heavier than air (1.5 times as dense). In its raw state, propane sinks and pools at the floor. Liquid propane will flash to a vapor at atmospheric pressure and appears white due to moisture condensing from the air.
When properly combusted, propane produces about 50 MJ/kg of heat. The gross heat of combustion of one normal cubic meter of propane is around 91 megajoules.
Propane is nontoxic; however, when abused as an inhalant, it poses several risks: sudden cardiac arrest, sudden sniffing death syndrome, Cardiac arrhythmia, loss of inhibitions and brain damage from long term use. Additionally there is a risk of asphyxiation usually through hyperventilation which leads to oxygen deprivation, hypoxia and sudden loss of consciousness. Propane is heavier than air, and can be difficult to exhale while unconscious. Because C02 concentration in the blood causes the body's urge to breathe, and hyperventilation lowers the body's carbon dioxide concentration significantly; upon loss of consciousness there may be no urge for the body to breathe. Commercial products contain hydrocarbons beyond propane (with more than 3 carbons), which may increase risk. Commonly stored under pressure at room temperature, propane and its mixtures expand and cool when released and may cause mild frostbite.
Propane combustion is much cleaner than gasoline combustion, though not as clean as natural gas combustion. The presence of C–C bonds, plus the multiple bonds of propylene and butylene, create organic exhausts besides carbon dioxide and water vapor during typical combustion. These bonds also cause propane to burn with a visible flame.
Energy content.
The enthalpy of combustion of propane gas where all products return to standard state, for example where water returns to its liquid state at standard temperature (known as higher heating value), is (−2219.2 ± 0.5) kJ/mol, or (50.33 ± 0.01) MJ/kg.
The enthalpy of combustion of propane gas where products do not return to standard state, for example where the hot gases including water vapor exit a chimney, (known as lower heating value) is −2043.455 kJ/mol. The lower heat value is the amount of heat available from burning the substance where the combustion products are vented to the atmosphere. For example, the heat from a fireplace when the flue is open.
Density.
The density of liquid propane at 25 °C (77 °F) is 0.493 g/cm3, which is equivalent to 4.11 pounds per U.S. liquid gallon or 493 kg/m3. Propane expands at 1.5% per 10 °F. Thus, liquid propane has a density of approximately 4.2 pounds per gallon (504 kg/m3) at 60 °F (15.6 °C).
Uses.
Propane is a popular choice for barbecues and portable stoves because the low boiling point of makes it vaporize as soon as it is released from its pressurized container. Therefore, no carburetor or other vaporizing device is required; a simple metering nozzle suffices. Propane powers some locomotives, buses, forklifts, taxis and ice resurfacing machines and is used for heat and cooking in recreational vehicles and campers. Since it can be transported easily, it is a popular fuel for home heat and backup electrical generation in sparsely populated areas that do not have natural gas pipelines.
Propane is generally stored and transported in steel cylinders as a liquid with a vapor space above the liquid. The vapor pressure in the cylinder is a function of temperature. When gaseous propane is drawn at a high rate, the latent heat of vaporisation required to create the gas will cause the bottle to cool. (This is why water often condenses on the sides of the bottle and then freezes). In addition, the lightweight, high-octane compounds vaporize before the heavier, low-octane ones. Thus, the ignition properties change as the cylinder empties. For these reasons, the liquid is often withdrawn using a dip tube. Propane is used as fuel in furnaces for heat, in cooking, as an energy source for water heaters, laundry dryers, barbecues, portable stoves, and motor vehicles.
Commercially available "propane" fuel, or LPG, is not pure. Typically in the United States and Canada, it is primarily propane (at least 90%), with the rest mostly ethane, propylene, butane, and odorants including ethyl mercaptan. This is the HD-5 standard, (Heavy Duty-5% maximum allowable propylene content, and no more than 5% butanes and ethane) defined by the American Society for Testing and Materials by its Standard 1835 for internal combustion engines. Not all products labeled "LPG" conform to this standard however. In Mexico, for example, gas labeled "LPG" may consist of 60% propane and 40% butane. "The exact proportion of this combination varies by country, depending on international prices, on the availability of components and, especially, on the climatic conditions that favor LPG with higher butane content in warmer regions and propane in cold areas".
Domestic and industrial fuel.
Propane use is growing rapidly in non-industrialized areas of the world. Propane has replaced many older other traditional fuel sources. The "propane" sold outside North America is actually a mixture of propane and butane. The warmer the country, the higher the butane content, commonly 50/50 and sometimes reaching 75% butane. Usage is calibrated to the different-sized nozzles found in non-U.S. grills. Americans who take their grills overseas — such as military personnel — can find U.S.-specification propane at AAFES military post exchanges.
North American industries using propane include glass makers, brick kilns, poultry farms and other industries that need portable heat.
In rural areas of North America, as well as northern Australia and some parts of southern India propane is used to heat livestock facilities, in grain dryers, and other heat-producing appliances. When used for heating or grain drying it is usually stored in a large, permanently placed cylinder which is recharged by a propane-delivery truck. , 9.7 million American households use propane as their primary heating fuel.
In North America, local delivery trucks with an average cylinder size of , fill up large cylinders that are permanently installed on the property, or other service trucks exchange empty cylinders of propane with filled cylinders. Large tractor-trailer trucks, with an average cylinder size of , transport the propane from the pipeline or refinery to the local bulk plant. The bobtail and transport are not unique to the North American market, though the practice is not as common elsewhere, and the vehicles are generally called "tankers". In many countries, propane is delivered to consumers via small or medium-sized individual cylinders, while empty cylinders are removed for refilling at a central location.
Propene (also called proplylene) can be a contaminant of commercial propane. Propane containing too much propene is not suited for most vehicle fuels. HD-5 is a specification that establishes a maximum concentration of 5% propene in propane. Propane and other LP gas specifications are established in ASTM D-1835. All propane fuels include an odorant, almost always ethanethiol, so that people can easily smell the gas in case of a leak. Propane as HD-5 was originally intended for use as vehicle fuel. HD-5 is currently being used in all propane applications.
Refrigeration.
Propane is also instrumental in providing off-the-grid refrigeration, usually by means of a gas absorption refrigerator.
Blends of pure, dry "isopropane" (R-290a) (isobutane/propane mixtures) and isobutane (R-600a) have negligible ozone depletion potential and very low Global Warming Potential (having a value of 3.3 times the GWP of carbon dioxide) and can serve as a functional replacement for R-12, R-22, R-134a, and other chlorofluorocarbon or hydrofluorocarbon refrigerants in conventional stationary refrigeration and air conditioning systems.
In motor vehicles.
Such substitution is widely prohibited or discouraged in motor vehicle air conditioning systems, on the grounds that using flammable hydrocarbons in systems originally designed to carry non-flammable refrigerant presents a significant risk of fire or explosion.
Vendors and advocates of hydrocarbon refrigerants argue against such bans on the grounds that there have been very few such incidents relative to the number of vehicle air conditioning systems filled with hydrocarbons.
Motor fuel.
Propane is also being used increasingly for vehicle fuels. In the U.S., over 190,000 on-road vehicles use propane, and over 450,000 forklifts use it for power. It is the third most popular vehicle fuel in the world, behind gasoline and Diesel fuel. In other parts of the world, propane used in vehicles is known as autogas. In 2007, approximately 13 million vehicles worldwide use autogas.
The advantage of propane in cars is its liquid state at a moderate pressure. This allows fast refill times, affordable fuel cylinder construction, and price ranges typically just over half that of gasoline. Meanwhile, it is noticeably cleaner (both in handling, and in combustion), results in less engine wear (due to carbon deposits) without diluting engine oil (often extending oil-change intervals), and until recently was a relative bargain in North America. The octane rating of propane is relatively high at 110. In the United States the propane fueling infrastructure is the most developed of all alternative vehicle fuels. Many converted vehicles have provisions for topping off from "barbecue bottles". Purpose-built vehicles are often in commercially owned fleets, and have private fueling facilities. A further saving for propane fuel vehicle operators, especially in fleets, is that pilferage is much more difficult than with gasoline or Diesel fuels.
Propane is also used as fuel for small engines, especially those used indoors or in areas with insufficient fresh air and ventilation to carry away the more toxic exhaust of an engine running on gasoline or Diesel fuel. More recently, there have been lawn care products like string trimmers, lawn mowers and leaf blowers intended for outdoor use, but fueled by propane to reduce air pollution.
Improvised explosive devices.
Propane and propane cylinders have been used as improvised explosive devices in attacks and attempted attacks against schools and terrorist targets such as the Columbine High School massacre, 2012 Brindisi school bombing, the Discovery Communications headquarters hostage crisis and in car bombs.
Propane risks.
Propane is denser than air. If a leak in a propane fuel system occurs, the gas will have a tendency to sink into any enclosed area and thus poses a risk of explosion and fire. The typical scenario is a leaking cylinder stored in a basement; the propane leak drifts across the floor to the pilot light on the furnace or water heater, and results in an explosion or fire. This property makes propane generally unsuitable as a fuel for boats.
A risk associated with propane storage and transport is known as a BLEVE or boiling liquid expanding vapor explosion. The Kingman Explosion involved a railroad tank car in Kingman, Arizona in 1973 during a propane transfer. The fire and subsequent explosions resulted in twelve fatalities and numerous injuries.
Comparison with natural gas.
Propane is bought and stored in a liquid form (LPG), and thus fuel energy can be stored in a relatively small space. Compressed Natural Gas (CNG), largely methane, is another gas used as fuel, but it cannot be liquefied by compression at normal temperatures, as these are well above its critical temperature. As a gas, very high pressure is required to store useful quantities. This poses the hazard that, in an accident, just as with any compressed gas cylinder (such as a CO2 cylinder used for a soda concession) a CNG cylinder may burst with great force, or leak rapidly enough to become a self-propelled missile. Therefore, CNG is much less efficient to store, due to the large cylinder volume required. An alternative means of storing natural gas is as a cryogenic liquid in an insulated container as Liquefied Natural Gas (LNG). This form of storage is at low pressure and is around 3.5 times as efficient as storing it as CNG. Unlike propane, if a spill occurs, CNG will evaporate and dissipate harmlessly because it is lighter than air. Propane is much more commonly used to fuel vehicles than is natural gas because the equipment required costs less. Propane requires just of pressure to keep it liquid at .
Retail cost.
United States.
, the retail cost of propane was approximately $2.37 per gallon, or roughly $25.95 per 1 million BTUs. This means that filling a 500-gallon propane tank, which is what households that use propane as their main source of energy usually require, costs $948 (80% of 500 gallons or 400 gallons), a 7.5% increase on the 2012–2013 winter season average US price. However, propane costs per gallon change significantly from one state to another: the Energy Information Administration (EIA) quotes a $2.995 per gallon average on the East Coast for October 2013, while the figure for the Midwest was $1.860 for the same period.

</doc>
<doc id="23645" url="https://en.wikipedia.org/wiki?curid=23645" title="Precambrian">
Precambrian

The Precambrian or Pre-Cambrian, sometimes abbreviated pЄ, is the largest span of time in Earth's history before the current Phanerozoic Eon, and is a Supereon divided into several eons of the geologic time scale. It spans from the formation of Earth about 4.6 billion years ago (Ga) to the beginning of the Cambrian Period, about million years ago (Ma), when hard-shelled creatures first appeared in abundance. The Precambrian is so named because it precedes the Cambrian, the first period of the Phanerozoic Eon, which is named after Cambria, the classical name for Wales, where rocks from this age were first studied. The Precambrian accounts for 88% of geologic time.
Overview.
Relatively little is known about the Precambrian, despite it making up roughly seven-eighths of the Earth's history, and what is known has largely been discovered from the 1960s onwards. The Precambrian fossil record is poorer than that of the succeeding Phanerozoic, and those fossils present (e.g. stromatolites) are of limited biostratigraphic use. This is because many Precambrian rocks have been heavily metamorphosed, obscuring their origins, while others have been destroyed by erosion, or remain deeply buried beneath Phanerozoic strata.
It is thought that the Earth itself coalesced from material in orbit around the Sun roughly 4500 Ma, or 4.5 billion years ago (Ga), and may have been struck by a very large (Mars-sized) planetesimal shortly after it formed, splitting off material that formed the Moon (see Giant impact hypothesis). A stable crust was apparently in place by 4400 Ma, since zircon crystals from Western Australia have been dated at 4404 Ma.
The term "Precambrian" is recognized by the International Commission on Stratigraphy as a general term including the Archean and Proterozoic eons. It is still used by geologists and paleontologists for general discussions not requiring the more specific eon names. It was briefly also called the "Cryptozoic" eon.
Life forms.
It is not known when life originated, but carbon in 3.8 billion year old rocks from islands off western Greenland may be of organic origin. Well-preserved bacteria older than 3.46 billion years have been found in Western Australia. Probable fossils 100 million years older have been found in the same area. There is a fairly solid record of bacterial life throughout the remainder of the Precambrian.
Excluding a few contested reports of much older forms from USA and India, the first complex multicellular life forms seem to have appeared roughly 600 Ma. The oldest fossil evidence of complex life comes from the Lantian formation, at least 580 million years ago. A quite diverse collection of soft-bodied forms is known from a variety of locations worldwide between 542 and 600 Ma. These are referred to as Ediacaran or Vendian biota. Hard-shelled creatures appeared toward the end of that time span. By the middle of the later Cambrian period a very diverse fauna is recorded in the Burgess Shale, including some which may represent stem groups of modern taxa. The rapid radiation of lifeforms during the early Cambrian is called the Cambrian explosion of life.
While land seems to have been devoid of plants and animals, cyanobacteria and other microbes formed prokaryotic mats that covered terrestrial areas.
Planetary environment and the oxygen catastrophe.
Evidence illuminating the details of plate motions and other tectonic functions in the Precambrian has been poorly preserved. It is generally believed that small proto-continents existed prior to 3000 Ma, and that most of the Earth's landmasses collected into a single supercontinent around 1000 Ma. The supercontinent, known as Rodinia, broke up around 600 Ma. A number of glacial periods have been identified going as far back as the Huronian epoch, roughly 2200 Ma. One of the most delved into is the Sturtian-Varangian glaciation, around 600 Ma, which may have brought glacial conditions all the way to the equator, resulting in a "Snowball Earth".
The atmosphere of the early Earth is not well understood. Most geologists believe it was composed primarily of nitrogen, carbon dioxide, and other relatively inert gases, lacking in free oxygen. This has been disputed with evidence in support of an oxygen-rich atmosphere since the early Archean.
Molecular oxygen was not present as a significant fraction of Earth's atmosphere until after photosynthetic life forms evolved and began to produce it in large quantities as a byproduct of their metabolism. This radical shift from an inert to an oxidizing atmosphere caused an ecological crisis sometimes called the oxygen catastrophe. At first, oxygen would quickly combine with other elements in Earth's crust, primarily iron, as it was produced. After the supply of oxidizable surfaces ran out, oxygen began to accumulate in the atmosphere, and the modern high-oxygen atmosphere developed. Evidence for this lies in older rocks that contain massive banded iron formations, laid down as iron and oxygen first combined.
Subdivisions.
An established terminology has evolved covering the early years of the Earth's existence, as radiometric dating allows plausible real dates to be assigned to specific formations and features. The Precambrian Supereon is divided into three eons: the Hadean (4500–3950 Ma), Archean (- Ma) and Proterozoic (- Ma). See Timetable of the Precambrian.
It has been proposed that the Precambrian should be divided into eons and eras that reflect stages of planetary evolution, rather than the current scheme based upon numerical ages. Such a system could rely on events in the stratigraphic record and be demarcated by GSSPs. The Precambrian could be divided into five "natural" eons, characterized as follows.
Precambrian super-continents.
The movement of plates has caused the formation and break-up of continents over time, including occasional formation of a super-continent containing most or all of the continents. The earliest known super-continent was Vaalbara. It formed from proto-continents and was a super-continent by 3.1 billion years ago (3.1 Ga). Vaalbara broke up c. 2.8 Ga ago. The super-continent Kenorland was formed c. 2.7 Ga ago and then broke sometime after 2.5 Ga into the proto-continent cratons called Laurentia, Baltica, Australia, and Kalahari. The super-continent Columbia or Nuna formed during a period of 2.0–1.8 billion years and broke up about 1.5–1.3 billion years ago. The super-continent Rodinia is thought to have formed about 1 billion years ago, to have embodied most or all of Earth's continents and to have broken up into eight continents around 600 million years ago.

</doc>
<doc id="23647" url="https://en.wikipedia.org/wiki?curid=23647" title="Polymerase chain reaction">
Polymerase chain reaction

The polymerase chain reaction (PCR) is a process used in molecular biology to amplify a single copy or a few copies of a piece of DNA across several orders of magnitude, generating thousands to millions of copies of a particular DNA sequence.
Developed in 1983 by Kary Mullis, PCR is now a common and often indispensable technique used in medical and biological research labs for a variety of applications. These include DNA cloning for sequencing, DNA-based phylogeny, or functional analysis of genes; the diagnosis of hereditary diseases; the identification of genetic fingerprints (used in forensic sciences and DNA paternity testing); and the detection and diagnosis of infectious diseases. In 1993, Mullis was awarded the Nobel Prize in Chemistry along with Michael Smith for his work on PCR.
The method relies on thermal cycling, consisting of cycles of repeated heating and cooling of the reaction for DNA melting and enzymatic replication of the DNA. Primers (short DNA fragments) containing sequences complementary to the target region along with a DNA polymerase, which the method is named after, are key components to enable selective and repeated amplification. As PCR progresses, the DNA generated is itself used as a template for replication, setting in motion a chain reaction in which the DNA template is exponentially amplified. PCR can be extensively modified to perform a wide array of genetic manipulations.
Almost all PCR applications employ a heat-stable DNA polymerase, such as Taq polymerase (an enzyme originally isolated from the bacterium "Thermus aquaticus"). This DNA polymerase enzymatically assembles a new DNA strand from DNA building-blocks, the nucleotides, by using single-stranded DNA as a template and DNA oligonucleotides (also called DNA primers), which are required for initiation of DNA synthesis. The vast majority of PCR methods use thermal cycling, i.e., alternately heating and cooling the PCR sample through a defined series of temperature steps.
In the first step, the two strands of the DNA double helix are physically separated at a high temperature in a process called DNA melting. In the second step, the temperature is lowered and the two DNA strands become templates for DNA polymerase to selectively amplify the target DNA. The selectivity of PCR results from the use of primers that are complementary to the DNA region targeted for amplification under specific thermal cycling conditions.
Principles and procedure.
PCR amplifies a specific region of a DNA strand (the DNA target). Most PCR methods typically amplify DNA fragments of between 0.1 and 10 kilo base pairs (kbp), although some techniques allow for amplification of fragments up to 40 kbp in size. The amount of amplified product is determined by the available substrates in the reaction, which become limiting as the reaction progresses.
A basic PCR set up requires several components and reagents. These components include:
The PCR is commonly carried out in a reaction volume of 10–200 μl in small reaction tubes (0.2–0.5 ml volumes) in a thermal cycler. The thermal cycler heats and cools the reaction tubes to achieve the temperatures required at each step of the reaction (see below). Many modern thermal cyclers make use of the Peltier effect, which permits both heating and cooling of the block holding the PCR tubes simply by reversing the electric current. Thin-walled reaction tubes permit favorable thermal conductivity to allow for rapid thermal equilibration. Most thermal cyclers have heated lids to prevent condensation at the top of the reaction tube. Older thermocyclers lacking a heated lid require a layer of oil on top of the reaction mixture or a ball of wax inside the tube.
Procedure.
Typically, PCR consists of a series of 20–40 repeated temperature changes, called cycles, with each cycle commonly consisting of 2–3 discrete temperature steps, usually three (Figure below). The cycling is often preceded by a single temperature step at a high temperature (>90 °C), and followed by one hold at the end for final product extension or brief storage. The temperatures used and the length of time they are applied in each cycle depend on a variety of parameters. These include the enzyme used for DNA synthesis, the concentration of divalent ions and dNTPs in the reaction, and the melting temperature (Tm) of the primers.
To check whether the PCR generated the anticipated DNA fragment (also sometimes referred to as the amplimer or amplicon), agarose gel electrophoresis is employed for size separation of the PCR products. The size(s) of PCR products is determined by comparison with a DNA ladder (a molecular weight marker), which contains DNA fragments of known size, run on the gel alongside the PCR products (see Fig. 3).
Stages.
The PCR process can be divided into three stages:
"Exponential amplification": At every cycle, the amount of product is doubled (assuming 100% reaction efficiency). The reaction is very sensitive: only minute quantities of DNA must be present.
"Leveling off stage": The reaction slows as the DNA polymerase loses activity and as consumption of reagents such as dNTPs and primers causes them to become limiting.
"Plateau": No more product accumulates due to exhaustion of reagents and enzyme.
Optimization.
In practice, PCR can fail for various reasons, in part due to its sensitivity to contamination causing amplification of spurious DNA products. Because of this, a number of techniques and procedures have been developed for optimizing PCR conditions. Contamination with extraneous DNA is addressed with lab protocols and procedures that separate pre-PCR mixtures from potential DNA contaminants. This usually involves spatial separation of PCR-setup areas from areas for analysis or purification of PCR products, use of disposable plasticware, and thoroughly cleaning the work surface between reaction setups. Primer-design techniques are important in improving PCR product yield and in avoiding the formation of spurious products, and the usage of alternate buffer components or polymerase enzymes can help with amplification of long or otherwise problematic regions of DNA. Addition of reagents, such as formamide, in buffer systems may increase the specificity and yield of PCR. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.
Applications.
Selective DNA isolation.
PCR allows isolation of DNA fragments from genomic DNA by selective amplification of a specific region of DNA. This use of PCR augments many methods, such as generating hybridization probes for Southern or northern hybridization and DNA cloning, which require larger amounts of DNA, representing a specific DNA region. PCR supplies these techniques with high amounts of pure DNA, enabling analysis of DNA samples even from very small amounts of starting material.
Other applications of PCR include DNA sequencing to determine unknown PCR-amplified sequences in which one of the amplification primers may be used in Sanger sequencing, isolation of a DNA sequence to expedite recombinant DNA technologies involving the insertion of a DNA sequence into a plasmid, phage, or cosmid (depending on size) or the genetic material of another organism. Bacterial colonies "(such as E. coli)" can be rapidly screened by PCR for correct DNA vector constructs. PCR may also be used for genetic fingerprinting; a forensic technique used to identify a person or organism by comparing experimental DNAs through different PCR-based methods.
Some PCR 'fingerprints' methods have high discriminative power and can be used to identify genetic relationships between individuals, such as parent-child or between siblings, and are used in paternity testing (Fig. 4). This technique may also be used to determine evolutionary relationships among organisms when certain molecular clocks are used (i.e., the 16S rRNA and recA genes of microorganisms). 
Amplification and quantification of DNA.
Because PCR amplifies the regions of DNA that it targets, PCR can be used to analyze extremely small amounts of sample. This is often critical for forensic analysis, when only a trace amount of DNA is available as evidence. PCR may also be used in the analysis of ancient DNA that is tens of thousands of years old. These PCR-based techniques have been successfully used on animals, such as a forty-thousand-year-old mammoth, and also on human DNA, in applications ranging from the analysis of Egyptian mummies to the identification of a Russian tsar and the body of English king Richard III.
Quantitative PCR methods allow the estimation of the amount of a given sequence present in a sample—a technique often applied to quantitatively determine levels of gene expression. Quantitative PCR is an established tool for DNA quantification that measures the accumulation of DNA product after each round of PCR amplification.
Disease diagnosis.
PCR permits early diagnosis of malignant diseases such as leukemia and lymphomas, which is currently the highest-developed in cancer research and is already being used routinely. PCR assays can be performed directly on genomic DNA samples to detect translocation-specific malignant cells at a sensitivity that is at least 10,000 fold higher than that of other methods.
PCR allows for rapid and highly specific diagnosis of infectious diseases, including those caused by bacteria or viruses. PCR also permits identification of non-cultivatable or slow-growing microorganisms such as mycobacteria, anaerobic bacteria, or viruses from tissue culture assays and animal models. The basis for PCR diagnostic applications in microbiology is the detection of infectious agents and the discrimination of non-pathogenic from pathogenic strains by virtue of specific genes.
Viral DNA can likewise be detected by PCR. The primers used must be specific to the targeted sequences in the DNA of a virus, and PCR can be used for diagnostic analyses or DNA sequencing of the viral genome. The high sensitivity of PCR permits virus detection soon after infection and even before the onset of disease. Such early detection may give physicians a significant lead time in treatment. The amount of virus ("viral load") in a patient can also be quantified by PCR-based DNA quantitation techniques (see below).
Limitations.
DNA polymerase is prone to error, which in turn causes mutations in the PCR fragments that are made. Additionally, the specificity of the PCR fragments can mutate to the template DNA, due to nonspecific binding of primers. Furthermore, prior information on the sequence is necessary in order to generate the primers.
History.
A 1971 paper in the Journal of Molecular Biology by Kjell Kleppe and co-workers in the laboratory of H. Gobind Khorana first described a method using an enzymatic assay to replicate a short DNA template with primers "in vitro". However, this early manifestation of the basic PCR principle did not receive much attention at the time, and the invention of the polymerase chain reaction in 1983 is generally credited to Kary Mullis.
When Mullis developed the PCR in 1983, he was working in Emeryville, California for Cetus Corporation, one of the first biotechnology companies. There, he was responsible for synthesizing short chains of DNA. Mullis has written that he conceived of PCR while cruising along the Pacific Coast Highway one night in his car. He was playing in his mind with a new way of analyzing changes (mutations) in DNA when he realized that he had instead invented a method of amplifying any DNA region through repeated cycles of duplication driven by DNA polymerase. In "Scientific American", Mullis summarized the procedure: "Beginning with a single molecule of the genetic material DNA, the PCR can generate 100 billion similar molecules in an afternoon. The reaction is easy to execute. It requires no more than a test tube, a few simple reagents, and a source of heat." He was awarded the Nobel Prize in Chemistry in 1993 for his invention, seven years after he and his colleagues at Cetus first put his proposal to practice. However, some controversies have remained about the intellectual and practical contributions of other scientists to Mullis' work, and whether he had been the sole inventor of the PCR principle (see below).
At the core of the PCR method is the use of a suitable DNA polymerase able to withstand the high temperatures of > required for separation of the two DNA strands in the DNA double helix after each replication cycle. The DNA polymerases initially employed for in vitro experiments presaging PCR were unable to withstand these high temperatures. So the early procedures for DNA replication were very inefficient and time-consuming, and required large amounts of DNA polymerase and continuous handling throughout the process.
The discovery in 1976 of Taq polymerase — a DNA polymerase purified from the thermophilic bacterium, "Thermus aquaticus", which naturally lives in hot () environments such as hot springs — paved the way for dramatic improvements of the PCR method. The DNA polymerase isolated from "T. aquaticus" is stable at high temperatures remaining active even after DNA denaturation, thus obviating the need to add new DNA polymerase after each cycle. This allowed an automated thermocycler-based process for DNA amplification.
Patent disputes.
The PCR technique was patented by Kary Mullis and assigned to Cetus Corporation, where Mullis worked when he invented the technique in 1983. The "Taq" polymerase enzyme was also covered by patents. There have been several high-profile lawsuits related to the technique, including an unsuccessful lawsuit brought by DuPont. The pharmaceutical company Hoffmann-La Roche purchased the rights to the patents in 1992 and currently holds those that are still protected.
A related patent battle over the Taq polymerase enzyme is still ongoing in several jurisdictions around the world between Roche and Promega. The legal arguments have extended beyond the lives of the original PCR and Taq polymerase patents, which expired on March 28, 2005.

</doc>
<doc id="23648" url="https://en.wikipedia.org/wiki?curid=23648" title="Polymerase">
Polymerase

A polymerase is an enzyme (EC 2.7.7.6/7/19/48/49) that synthesizes long chains or polymers of nucleic acids. DNA polymerase and RNA polymerase are used to assemble DNA and RNA molecules, respectively, by copying a DNA or RNA template strand using base-pairing interactions.
A polymerase from the thermophilic bacterium, "Thermus aquaticus" ("Taq") (PDB 1BGX, EC 2.7.7.7) is used in the polymerase chain reaction, an important technique of molecular biology.
Other well-known polymerases include:

</doc>
<doc id="23649" url="https://en.wikipedia.org/wiki?curid=23649" title="Pacific Scandal">
Pacific Scandal

The Pacific Scandal was a political scandal in Canada involving allegations of bribes being accepted by 150 members of the Conservative government in the attempts of private interests to influence the bidding for a national rail contract. As part of British Columbia's 1871 agreement to join Canadian Confederation, the government had agreed to build a transcontinental railway linking the Pacific Province to the eastern provinces. The proposed rail project, when completed, was the most intensive and ambitious of its kind ever undertaken to date. However, as a new nation with limited capital resources, financing for the project was sought after both at home and abroad, naturally attracting interest from Great Britain and the United States.
The scandal ultimately led to the resignation of Canada's first Prime Minister, Sir John A. Macdonald, and a transfer of power from his Conservative government to a Liberal government led by Alexander Mackenzie. One of the new government's first measures was to introduce secret ballots in an effort to improve the integrity of future elections.
Background.
For a young and loosely defined nation, the building of a national railway was an active attempt at state-making, as well as an aggressive capitalist venture. Canada, a nascent country with a population of 3.5 million in 1871, lacked the means to exercise meaningful "de facto" control within the "de jure" political boundaries of the recently acquired Rupert's Land; building a transcontinental railway was national policy of high order to change this situation. Moreover, after the American Civil War the American frontier rapidly expanded west with land-hungry settlers, exacerbating talk of annexation. Indeed, sentiments of Manifest Destiny were abuzz in this time: in 1867, year of Confederation, US Secretary of State W.H. Seward surmised that the whole North American continent "shall be, sooner or later, within the magic circle of the American Union." Therefore, preventing American investment into the project was considered as being in Canada's national interest. Thus the federal government favoured an "all Canadian route" through the rugged Canadian Shield of northern Ontario, refusing to consider a less costly route passing south through Wisconsin and Minnesota.
However, a route across the Canadian Shield was highly unpopular with potential investors, not only in the United States but also in Canada and especially Great Britain, the only other viable source of financing. For would-be investors, the objections were not primarily based on politics or nationalism but economics. At the time, national governments lacked the finances needed to undertake such large projects. For the First Transcontinental Railroad, the United States government had made extensive grants of public land to the railway's builders, inducing private financiers to fund the railway on the understanding that they would acquire rich farmland along the route, which could then be sold for a large profit. However, the eastern terminus of the proposed Canadian Pacific route, unlike that of the First Transcontinental, was not in rich Nebraskan farmland, but deep within the Canadian Shield. Copying the American financing model whilst insisting on an all-Canadian route would require the railway's backers to build hundreds of miles of track across rugged shield terrain (with little economic value) at considerable expense before they could expect to access lucrative farmland in Manitoba, which then was part of the newly created Northwest Territories. Many financiers, who had expected to make a relatively quick profit, were not willing to make this sort of long-term commitment.
Nevertheless, the Montreal capitalist Sir Hugh Allan, with his syndicate Canada Pacific Railway Company, sought the potentially lucrative charter for the project. The problem lay in that Allan and Sir John A. Macdonald highly, and secretly, were in cahoots with American financiers such as George W. McMullen and Jay Cooke, men who were deeply interested in the rival American undertaking, the Northern Pacific Railroad.
Scandal.
Two groups competed for the contract to build the railway, Sir Hugh Allan's Canada Pacific Railway Company and David Lewis Macpherson's Inter-Oceanic Railway Company. On April 2, 1873, Lucius Seth Huntington, a Liberal Member of Parliament, created an uproar in the House of Commons. He announced he had uncovered evidence that Sir Hugh Allan and his associates had been granted the Canadian Pacific Railway contract in return for political donations of $360,000.
In 1873, it became known that Allan had contributed a large sum of money to the Conservative government's re-election campaign of 1872; some sources quote a sum over $360,000. Allan had promised to keep American capital out of the railway deal, but had lied to Macdonald over this vital point, and Macdonald later discovered the lie. The Liberal party, at this time the opposition party in Parliament, accused the Conservatives of having made a tacit agreement to give the contract to Hugh Allan in exchange for money.
In making such allegations, the Liberals and their allies in the press (in particular, George Brown's newspaper the Globe) presumed that most of the money had been used to bribe voters in the 1872 election. The secret ballot, then considered a novelty, had not yet been introduced in Canada. Although it was illegal to offer, solicit or accept bribes in exchange for votes, effective enforcement of this prohibition proved impossible.
Despite Macdonald's claims that he was innocent, evidence came to light showing receipts of money from Allan to Macdonald and some of his political colleagues. Perhaps even more damaging to Macdonald was when the Liberals discovered a telegram, through a former employee of Sir Hugh Allan, which was thought to have been stolen from the safe of Allan's lawyer, Sir John Abbott.
The scandal proved fatal to Macdonald's government. Macdonald's control of Parliament was already tenuous following the 1872 election. In a time when party discipline was not as strong as it is today, once Macdonald's culpability in the scandal became known he could no longer expect to retain the confidence of the House of Commons.
Macdonald resigned as prime minister on 5 November 1873. He also offered his resignation as the head of the Conservative party, but it was not accepted and he was convinced to stay. Perhaps as a direct result of this scandal, the Conservative party fell in the eyes of the public and was relegated to being the Official Opposition in the federal election of 1874. This election, in which secret ballots were used for the first time, gave Alexander Mackenzie a firm mandate to succeed Macdonald as the new prime minister of Canada.
Despite the short-term defeat, the scandal was not a mortal wound to Macdonald, the Conservative Party, or the Canadian Pacific Railway. An economic depression gripped Canada shortly after Macdonald left office, and although the causes of the depression were largely external to Canada many Canadians nevertheless blamed Mackenzie for the ensuing hard times. Macdonald would return as prime minister in the 1878 election thanks to his National Policy. He would hold the office of prime minister to his death in 1891, and the Canadian Pacific would be completed by 1885 with Macdonald still in office.

</doc>
<doc id="23650" url="https://en.wikipedia.org/wiki?curid=23650" title="Primer (molecular biology)">
Primer (molecular biology)

A primer is a strand of short nucleic acid sequences (generally about 10 base pairs) that serves as a starting point for DNA synthesis. It is required for DNA replication because the enzymes that catalyze this process, DNA polymerases, can only add new nucleotides to an existing strand of DNA. The polymerase starts replication at the 3'-end of the primer, and copies the opposite strand.
In most cases of natural DNA replication, the primer for DNA synthesis and replication is a short strand of RNA (which can be made "de novo").
Many of the laboratory techniques of biochemistry and molecular biology that involve DNA polymerase, such as DNA sequencing and the polymerase chain reaction (PCR), require DNA primers. These primers are usually short, chemically synthesized oligonucleotides, with a length of about twenty bases. They are hybridized to a target DNA, which is then copied by the polymerase.
Mechanism "in vivo".
The lagging strand of DNA is that strand of the DNA double helix that is orientated in a 5' to 3' manner. Therefore, its complement must be synthesized in a 3'→5' manner. Because DNA polymerase III cannot synthesize in the 3'→5' direction, the lagging strand is synthesized in short segments known as Okazaki fragments. Along the lagging strand's template, primase builds RNA primers in short bursts. DNA polymerases are then able to use the free 3'-OH groups on the RNA primers to synthesize DNA in the 5'→3' direction.
The RNA fragments are then removed by DNA polymerase I for prokaryotes or DNA polymerase δ for eukaryotes (different mechanisms are used in eukaryotes and prokaryotes) and new deoxyribonucleotides are added to fill the gaps where the RNA was present. DNA ligase then joins the deoxyribonucleotides together, completing the synthesis of the lagging strand.
Primer removal.
In eukaryotic primer removal, DNA polymerase δ extends the Okazaki fragment in 5' to 3' direction, and when it encounters the RNA primer from the previous Okazaki fragment, it displaces the 5′ end of the primer into a single-stranded RNA flap, which is removed by nuclease cleavage. Cleavage of the RNA flaps involves either endonuclease 1 (FEN1) cleavage of short flaps, or coating of long flaps by the single-stranded DNA binding protein replication protein A (RPA) and sequential cleavage by Dna2 nuclease and FEN1.
This mechanism is a potential explanation of how the HIV virus can transform its genome into double-stranded DNA from the RNA-DNA formed after reverse transcription of its RNA. However, the HIV-encoded reverse transcriptase has its own ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that copies the sense cDNA strand into "antisense" DNA to form a double-stranded DNA intermediate.
Uses of synthetic primers.
DNA sequencing is used to determine the nucleotides in a DNA strand. The Sanger chain termination method of sequencing uses a primer to start the chain reaction.
In PCR, primers are used to determine the DNA fragment to be amplified by the PCR process. The length of primers is usually not more than 30 (usually 18–24) nucleotides, and they need to match the beginning and the end of the DNA fragment to be amplified. They direct replication towards each other – the extension of one primer by polymerase then becomes the template for the other, leading to an exponential increase in the target segment.
It is worth noting that primers are not always for DNA synthesis, but can in fact be used by viral polymerases, e.g. influenza, for RNA synthesis.
PCR primer design.
Pairs of primers should have similar melting temperatures since annealing in a PCR occurs for both simultaneously. A primer with a "T"m (melting temperature) significantly higher than the reaction's annealing temperature may mishybridize and extend at an incorrect location along the DNA sequence, while "T"m significantly lower than the annealing temperature may fail to anneal and extend at all.
Primer sequences need to be chosen to uniquely select for a region of DNA, avoiding the possibility of mishybridization to a similar sequence nearby. A commonly used method is BLAST search whereby all the possible regions to which a primer may bind can be seen. Both the nucleotide sequence as well as the primer itself can be BLAST searched. The free NCBI tool Primer-BLAST integrates primer design and BLAST search into one application, as do commercial software products such as ePrime and Beacon Designer. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.
Many online tools are freely available for primer design, some of which focus on specific applications of PCR. The popular tools Primer3Plus and PrimerQuest can be used to find primers matching a wide variety of specifications. Highly degenerate primers for targeting a wide variety of DNA templates can be interactively designed using GeneFISHER. Primers with high specificity for a subset of DNA templates in the presence of many similar variants can be designed using DECIPHER.
Mononucleotide and dinucleotide repeats should be avoided, as loop formation can occur and contribute to mishybridization. Primers should not easily anneal with other primers in the mixture (either other copies of same or the reverse direction primer); this phenomenon can lead to the production of 'primer dimer' products contaminating the mixture. Primers should also not anneal strongly to themselves, as internal hairpins and loops could hinder the annealing with the template DNA.
When designing a primer for use in TA cloning, efficiency can be increased by adding AG tails to the 5' and the 3' end.
The reverse primer has to be the reverse complement of the given cDNA sequence. The reverse complement can be easily determined, e.g. with online calculators.
Degenerate primers.
Sometimes "degenerate primers" are used. These are actually mixtures of similar, but not identical primers. They may be convenient if the same gene is to be amplified from different organisms, as the genes themselves are probably similar but not identical. The other use for degenerate primers is when primer design is based on protein sequence. As several different codons can code for one amino acid, it is often difficult to deduce which codon is used in a particular case. Therefore, primer sequence corresponding to the amino acid isoleucine might be "ATH", where A stands for adenine, T for thymine, and H for adenine, thymine, or cytosine, according to the genetic code for each codon, using the IUPAC symbols for degenerate bases. Use of degenerate primers can greatly reduce the specificity of the PCR amplification. The problem can be partly solved by using touchdown PCR.
"Degenerate primers" are widely used and extremely useful in the field of microbial ecology. They allow for the amplification of genes from thus far uncultivated microorganisms or allow the recovery of genes from organisms where genomic information is not available. Usually, degenerate primers are designed by aligning gene sequencing found in GenBank. Differences among sequences are accounted for by using IUPAC degeneracies for individual bases. PCR primers are then synthesized as a mixture of primers corresponding to all permutations.
There are a number of programs available to perform these primer predictions; 

</doc>
<doc id="23652" url="https://en.wikipedia.org/wiki?curid=23652" title="Purine">
Purine

A purine is a heterocyclic aromatic organic compound. It consists of a pyrimidine ring fused to an imidazole ring. Purines, which include substituted purines and their tautomers, are the most widely occurring nitrogen-containing heterocycle in nature.
Purines and pyrimidines make up the two groups of nitrogenous bases, including the two groups of nucleotide bases. Two of the four deoxyribonucleotides and two of the four ribonucleotides, the respective building-blocks of DNA and RNA, are purines. In order to form DNA and RNA, both purines and pyrimidines are needed by the cell in approximately equal quantities. Both purine and pyrimidine are self-inhibiting and activating. When purines are formed, they inhibit the enzymes required for more purine formation. This self-inhibiting occurs as they also activate the enzymes needed for pyrimidine formation. Pyrimidine simultaneously self-inhibits and activates purine in similar manner. Because of this, there is nearly an equal amount of both substances in the cell at all times.
Notable purines.
There are many naturally occurring purines. Two of the five bases in nucleic acids, adenine (2) and guanine (3), are purines. In DNA, these bases form hydrogen bonds with their complementary pyrimidines thymine and cytosine, respectively. This is called complementary base pairing. In RNA, the complement of adenine is uracil instead of thymine.
Other notable purines are hypoxanthine (4), xanthine (5), theobromine (6), caffeine (7), uric acid (8) and isoguanine (9).
Functions.
Aside from the crucial roles of purines (adenine and guanine) in DNA and RNA, purines are also significant components in a number of other important biomolecules, such as ATP, GTP, cyclic AMP, NADH, and coenzyme A. Purine (1) itself, has not been found in nature, but it can be produced by organic synthesis.
They may also function directly as neurotransmitters, acting upon purinergic receptors. Adenosine activates adenosine receptors.
History.
The word "purine" ("pure urine") was coined by the German chemist Emil Fischer in 1884. He synthesized it for the first time in 1898. The starting material for the reaction sequence was uric acid (8), which had been isolated from kidney stones by Scheele in 1776. Uric acid (8) was reacted with PCl5 to give 2,6,8-trichloropurine (10), which was converted with HI and PH4I to give 2,6-diiodopurine (11). The product was reduced to purine (1) using zinc-dust.
Metabolism.
Many organisms have metabolic pathways to synthesize and break down purines.
Purines are biologically synthesized as nucleosides (bases attached to ribose).
Accumulation of modified purine nucleotides is defective to various cellular processes, especially those involving DNA and RNA. To be viable, organisms possess a number of (deoxy)purine phosphohydrolases, which hydrolyze these purine derivatives removing them from the active NTP and dNTP pools. Deamination of purine bases can result in accumulation of such nucleotides as ITP, dITP, XTP and dXTP.
Defects in enzymes that control purine production and breakdown can severely alter a cell’s DNA sequences, which may explain why people who carry certain genetic variants of purine metabolic enzymes have a higher risk for some types of cancer.
Higher levels of meat and seafood consumption are associated with an increased risk of gout, whereas a higher level of consumption of dairy products is associated with a decreased risk. Moderate intake of purine-rich vegetables or protein is not associated with an increased risk of gout.
Purine sources.
Purines are found in high concentration in meat and meat products, especially internal organs such as liver and kidney. In general, plant-based diets are low in purines.
Examples of high-purine sources include: sweetbreads, anchovies, sardines, liver, beef kidneys, brains, meat extracts (e.g., Oxo, Bovril), herring, mackerel, scallops, game meats, beer (from the yeast) and gravy.
A moderate amount of purine is also contained in beef, pork, poultry, other fish and seafood, asparagus, cauliflower, spinach, mushrooms, green peas, lentils, dried peas, beans, oatmeal, wheat bran, wheat germ, and haws.
Laboratory synthesis.
In addition to in vivo synthesis of purines in purine metabolism, purine can also be created artificially.
Purine (1) is obtained in good yield when formamide is heated in an open vessel at 170 °C for 28 hours.
This remarkable reaction and others like it have been discussed in the context of the origin of life.
Oro, Orgel and co-workers have shown that four molecules of HCN tetramerize to form diaminomaleodinitrile (12), which can be converted into almost all natural-occurring purines. For example, five molecules of HCN condense in an exothermic reaction to make Adenine, especially in the presence of ammonia.
The Traube purine synthesis (1900) is a classic reaction (named after Wilhelm Traube) between an amine-substituted pyrimidine and formic acid.

</doc>
<doc id="23653" url="https://en.wikipedia.org/wiki?curid=23653" title="Pyrimidine">
Pyrimidine

Pyrimidine is an aromatic heterocyclic organic compound similar to pyridine. One of the three diazines (six-membered heterocyclics with two nitrogen atoms in the ring), it has the nitrogen atoms at positions 1 and 3 in the ring. The other diazines are pyrazine (nitrogen atoms at the 1 and 4 positions) and pyridazine (nitrogen atoms at the 1 and 2 positions). In nucleic acids, three types of nucleobases are pyrimidine derivatives: cytosine (C), thymine (T), and uracil (U).
Occurrence and history.
The pyrimidine ring system has wide occurrence in nature
as substituted and ring fused compounds and derivatives, including the nucleotides, thiamine (vitaminB1) and alloxan. It is also found in many synthetic compounds such as barbiturates and the HIV drug, zidovudine. Although pyrimidine derivatives such as uric acid and alloxan were known in the early 19th century, a laboratory synthesis of a pyrimidine was not carried out until 1879, when Grimaux reported the preparation of barbituric acid from Ivy urea and malonic acid in the presence of phosphorus oxychloride.
The systematic study of pyrimidines began in 1884 with Pinner,
who synthesized derivatives by condensing ethyl acetoacetate with amidines. Pinner first proposed the name “pyrimidin” in 1885. The parent compound was first prepared by Gabriel & Colman in 1900,
by conversion of barbituric acid to 2,4,6-trichloropyrimidine followed by reduction using zinc dust in hot water.
Nomenclature.
The nomenclature of pyrimidines is straightforward. However, like other heterocyclics, tautomeric hydroxyl groups yield complications since they exist primarily in the cyclic amide form. For example, 2-hydroxypyrimidine is more properly named 2-pyrimidone . A partial list of trivial names of various pyrimidines exists.
Physical properties.
Physical properties are shown in the data box. A more extensive discussion, including spectra, can be found in Brown "et al."
Chemical properties.
Per the classification by Albert six-membered heterocyclics can be described as π-deficient. Substitution by electronegative groups or additional nitrogen atoms in the ring significantly increase the π-deficiency. These effects also decrease the basicity.
Like pyridines, in pyrimidines the π-electron density is decreased to an even greater extent. Therefore, electrophilic aromatic substitution is more difficult while nucleophilic aromatic substitution is facilitated. An example of the last reaction type is the displacement of the amino group in 2-aminopyrimidine by chlorine and its reverse.
Electron lone pair availability (basicity) is decreased compared to pyridine. Compared to pyridine, N-alkylation and N-oxidation are more difficult. The pKa value for protonated pyrimidine is 1.23 compared to 5.30 for pyridine. Protonation and other electrophilic additions will occur at only one nitrogen due to further deactivation by the second nitrogen. The 2-, 4-, and 6- positions on the pyrimidine ring are electron deficient analogous to those in pyridine and nitro- and dinitrobenzene. The 5-position is less electron deficient and substitutents there are quite stable. However, electrophilic substitution is relatively facile at the 5-position, including nitration and halogenation.
Reduction in resonance stabilization of pyrimidines may lead to addition and ring cleavage reactions rather than substitutions. One such manifestation is observed in the Dimroth rearrangement.
Pyrimidine is also found in meteorites, but scientists still do not know its origin. Pyrimidine also photolytically decomposes into uracil under UV light.
Synthesis.
As is often the case with parent heterocyclic ring systems, the synthesis of pyrimidine is not that common and is usually performed by removing functional groups from derivatives. Primary syntheses in quantity involving formamide have been reported.
As a class, pyrimidines are typically synthesized by the “Principal Synthesis” involving cyclization of beta-dicarbonyl compounds with N-C-N compounds. Reaction of the former with amidines to give 2-substituted pyrimidines, with urea to give 2-pyrimidiones, and guanidines to give 2-aminopyrimidines are typical.
Pyrimidines can be prepared via the Biginelli reaction. Many other methods rely on condensation of carbonyls with diamines for instance the synthesis of 2-Thio-6-methyluracil from thiourea and ethyl acetoacetate or the synthesis of 4-methylpyrimidine with 4,4-dimethoxy-2-butanone and formamide.
A novel method is by reaction of N-vinyl and N-aryl amides with carbonitriles under electrophilic activation of the amide with 2-chloro-pyridine and trifluoromethanesulfonic anhydride:
Reactions.
Because of the decreased basicity compared to pyridine, electrophilic substitution of pyrimidine is less facile. Protonation or alkylation typically takes place at only one of the ring nitrogen atoms. Mono N-oxidation occurs by reaction with peracids.
Electrophilic C-substitution of pyrimidine occurs at the 5-position, the least electron deficient. Nitration, nitrosation, azo coupling, halogenation, sulfonation, formylation, hydroxymethylation, and aminomethylation have been observed with substituted pyrimidines.
Nucleophilic C-substitution should be facilitated at the 2-, 4-, and 6-positions but there are only a few examples. Amination and hydroxylation has been observed for substituted pyrimidines. Reactions with Grignard or alkyllithium reagents yield 4-alkyl- or 4-aryl pyrimidine after aromatization.
Free radical attack has been observed for pyrimidine and photochemical reactions have been observed for substituted pyrimidines. Pyrimidine can be hydrogenated to give tetrahydropyrimidine.
Nucleotides.
Three nucleobases found in nucleic acids, cytosine (C), thymine (T), and
uracil (U), are pyrimidine derivatives:
In DNA and RNA, these bases form hydrogen bonds with their complementary purines. Thus, in DNA, the purines adenine (A) and guanine (G) pair up with the pyrimidines thymine (T) and cytosine (C), respectively.
In RNA, the complement of adenine (A) is uracil (U) instead of thymine (T), so the pairs that form are adenine:uracil and guanine:cytosine.
Very rarely, thymine can appear in RNA, or uracil in DNA,but when other three major pyrimidine bases presented, some minor pyrimidine bases can also occur in nucleic acids. These minor pyrimidines are usually methylated versions of major ones and are postulated to have regulatory functions.
These hydrogen bonding modes are for classical Watson-Crick base pairing. Other hydrogen bonding modes ("wobble pairings") are available in both DNA and RNA, although the additional 2'-hydroxyl group of RNA expands the configurations, through which RNA can form hydrogen bonds.
Theoretical aspects.
In March 2015, NASA Ames scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.

</doc>
<doc id="23654" url="https://en.wikipedia.org/wiki?curid=23654" title="Play-by-mail game">
Play-by-mail game

Play-by-mail games, or play-by-post games, are games, of any type, played through postal mail or email.
Correspondence chess has been played by mail for centuries. The boardgame "Diplomacy" has been played by mail since the 1960s, starting with a printed newsletter (a fanzine) written by John Boardman. More complex games, moderated entirely or partially by computer programs, were pioneered by Rick Loomis of Flying Buffalo in 1970. The first such game offered via major e-mail services was "WebWar II" (based on Starweb and licensed from Flying Buffalo) from Neolithic Enterprises who accepted e-mail turns from all of the major e-mail services including CompuServe in 1983.
Play by mail games are often referred to as PBM games, and play by email is sometimes abbreviated PBeM—as opposed to face to face (FTF) or over the board (OTB) games which are played in person. Another variation on the name is Play-by-Internet (PBI) or Play-by-Web (PBW). In all of these examples, player instructions can be either executed by a human moderator, a computer program, or a combination of the two.
In the 1980s, play-by-mail games reached their peak of popularity with the advent of Gaming Universal, Paper Mayhem and Flagship magazine, the first professional magazines devoted to play-by-mail games. (An earlier fanzine, Nuts & Bolts of PBM, was the first publication to exclusively cover the hobby.) Bob McLain, the publisher and editor of Gaming Universal, further popularized the hobby by writing articles that appeared in many of the leading mainstream gaming magazines of the time. Flagship later bought overseas right to Gaming Universal, making it the leading magazine in the field. Flagship magazine was founded by Chris Harvey and Nick Palmer. The magazine still exists, under a new editor, but health concerns have led to worries over the publication's long term viability.
In the late 1990s, computer and Internet games marginalized play-by-mail conducted by actual postal mail, but the postal hobby still exists with an estimated 2000–3000 adherents worldwide.
Postal gaming.
Postal gaming developed as a way for geographically separated gamers to compete with each other. It was especially useful for those living in isolated areas and those whose tastes in games were uncommon.
In the case of a two player game such as chess, players would simply send their moves to each other alternately. In the case of a multi-player game such as Diplomacy, a central game master would run the game, receiving the moves and publishing adjudications. Such adjudications were often published in postal game zines, some of which contained far more than just games.
The commercial market for play-by-mail games grew to involve computer servers set up to host potentially thousands of players at once. Players would typically be split up into parallel games in order to keep the number of players per game at a reasonable level, with new games starting as old games ended. A typical closed game session might involve one to two dozen players, although some games claimed to have as many as five hundred people simultaneously competing in the same game world. While the central company was responsible for feeding in moves and mailing the processed output back to players, players were also provided with the mailing addresses of others so that direct contact could be made and negotiations performed. With turns being processed every few weeks (a two-week turnaround being standard), more advanced games could last over a year.
Game themes are heavily varied, and may range from those based on historical or real events to those taking place in alternate or fictional worlds.
Inevitably, the onset of the computer-moderated PBM game (primarily the Legends game system) meant that the human moderated games became "boutique" games with little chance of matching the gross revenues that larger, automated games could produce.
Mechanics.
The mechanics of play-by-mail games require that players think and plan carefully before making moves. Because planned actions can typically only be submitted at a fixed maximum frequency (e.g., once every few days or every few weeks), the number of discrete actions is limited compared to real-time games. As a result, players are provided with a variety of resources to assist in turn planning, including game aids, maps, and results from previous turns. Using this material, planning a single turn may take a number of hours.
Actual move/turn submission is traditionally carried out by filling in a "turn card". This card has formatted entry areas where players enter their planned actions (using some form of encoding) for the upcoming turn. Players are limited to some finite number of actions, and in some cases must split their resources between these actions (so that additional actions make each less effective). The way the card is filled in often implies an ordering between each command, so that they are processed in-order, one after another. Once completed, the card is then mailed (or, in more modern times, emailed) to the game master, where it is either processed, or held until the next turn processing window begins.
By gathering turn cards from a number of players and processing them all at the same time, games can provide simultaneous actions for all players. However, for this same reason, co-ordination between players can be difficult to achieve. For example, player A might attempt to move to player B's current location to do something with (or to) player B, while player B might simultaneously attempt to move to player A's current location. As such, the output/results of the turn can differ significantly from the submitted plan. Whatever the results, they are mailed back to the player to be studied and used as the basis for the next turn (often along with a new blank turn card).
While billing is sometimes done using a flat per-game rate (when the length of the game is known and finite), games more typically use a per-turn cost schedule. In such cases, each turn submitted depletes a pool of credit which must periodically be replenished in order to keep playing. Some games have multiple fee schedules, where players can pay more to perform advanced actions, or to take a greater number of actions in a turn.
Some role-playing PBM games also include an element whereby the player may describe actions of their characters in a free text form. The effect and effectiveness of the action is then based on the judgement of the GM who may allow or partially allow the action. This gives the player more flexibility beyond the normal fixed actions at the cost of more complexity and, usually, expense.
Play-by-email.
With the rise of the Internet, email and websites have largely replaced postal gaming and postal games zines. Play-by-mail games differ from popular online multiplayer games in that, for most computerized multiplayer games, the players have to be online at the same time - also known as synchronous play. With a play-by-mail game, the players can play whenever they choose, since responses need not be immediate; this is sometimes referred to as turn-based gaming and is common among browser-based games. Some video games can be played in turn-based mode: one makes one's "move", then play for that player stops, and the turn passes to another player who to makes his or her move in response.
Several non-commercial email games played on the Internet and BITNET predate these.
Some cards games like poker can also be played by email using cryptography (see for example a patent application from FXTOP, 
Play-by-web.
An increasingly popular format for play-by-email games is play-by-web. As with play-by-email games the players are notified by email when it becomes their turn, but they must then return to the game's website to continue playing what is essentially a browser-based game. The main advantage of this is that the players can be presented with a graphical representation of the game and an interactive interface to guide them through their turn. Since the notifications only have to remind the players that it is their turn they can just as easily be sent via instant messaging.
Some sites have extended this gaming style by allowing the players to see each other's actions as they are made. This allows for real time playing while everyone is online and active, or slower progress if not.
Increasingly, this format is being adopted by social and mobile games, often described using the term "asynchronous multiplayer."
References.
Readers Digest 1,001 Computer Hints and Tips

</doc>
<doc id="23658" url="https://en.wikipedia.org/wiki?curid=23658" title="Philip K. Dick Award">
Philip K. Dick Award

The Philip K. Dick Award is a science fiction award given annually at Norwescon sponsored by the Philadelphia Science Fiction Society and (since 2005) supported by the Philip K. Dick Trust, and named after science fiction and fantasy writer Philip K. Dick. It has been awarded since 1983, the year after Dick's death. Works that have received the award are identified on their covers as "Best Original SF Paperback". They are awarded to the best original paperback published each year in the US.
The award was founded by Thomas Disch with assistance from David G. Hartwell, Paul S. Williams, and Charles N. Brown. It is currently administered by David G. Hartwell & Gordon Van Gelder. Past administrators include Algis J. Budrys and David Alexander Smith.
Winners and nominees.
Winners are listed in bold.

</doc>
<doc id="23659" url="https://en.wikipedia.org/wiki?curid=23659" title="Plug-in (computing)">
Plug-in (computing)

In computing, a plug-in (or plugin, add-in, addin, add-on, addon, or extension) is a software component that adds a specific feature to an existing computer program. When a program supports plug-ins, it enables customization. The common examples are the plug-ins used in web browsers to add new features such as search-engines, virus scanners, or the ability to use a new file type such as a new video format. Well-known browser plug-ins include the Adobe Flash Player, the QuickTime Player, and the Java plug-in, which can launch a user-activated Java applet on a web page to its execution on a local Java virtual machine.
A theme or skin is a preset package containing additional or changed graphical appearance details, achieved by the use of a graphical user interface (GUI) that can be applied to specific software and websites to suit the purpose, topic, or tastes of different users to customize the look and feel of a piece of computer software or an operating system front-end GUI (and window managers).
Purpose and examples.
Applications support plug-ins for many reasons. Some of the main reasons include:
Types of applications and why they use plug-ins:
Mechanism.
As shown in the figure, the host application provides services which the plug-in can use, including a way for plug-ins to register themselves with the host application and a protocol for the exchange of data with plug-ins. Plug-ins depend on the services provided by the host application and do not usually work by themselves. Conversely, the host application operates independently of the plug-ins, making it possible for end-users to add and update plug-ins dynamically without needing to make changes to the host application.
Programmers typically implement plug-in functionality using shared libraries installed in a place prescribed by the host application. HyperCard supported a similar facility, but more commonly included the plug-in code in the HyperCard documents (called "stacks") themselves. Thus the HyperCard stack became a self-contained application in its own right, distributable as a single entity that end-users could run without the need for additional installation-steps. Programs may also implement plugins by loading a directory of simple script files written in a scripting language like Python or Lua.
Mozilla definition.
In Mozilla Foundation definitions, the words "add-on", "extension" and "plug-in" are not synonyms. "Add-on" can refer to anything that extends the functions of a Mozilla application. Extensions comprise a subtype, albeit the most common and the most powerful one. Mozilla applications come with integrated add-on managers that, similar to package managers, install, update and manage extensions. The term, "Plug-in", however, strictly refers to NPAPI-based web content renderers. Plug-ins are being deprecated. 
History.
Plug-ins appeared as early as the mid 1970s, when the EDT text editor running on the Unisys VS/9 operating system using the UNIVAC Series 90 mainframe computers provided the ability to run a program from the editor and to allow such a program to access the editor buffer, thus allowing an external program to access an edit session in memory. The plug-in program could make calls to the editor to have it perform text-editing services upon the buffer that the editor shared with the plug-in. The Waterloo Fortran compiler used this feature to allow interactive compilation of Fortran programs edited by EDT.
Very early PC software applications to incorporate plug-in functionality included HyperCard and QuarkXPress on the Macintosh, both released in 1987. In 1988, Silicon Beach Software included plug-in functionality in Digital Darkroom and SuperPaint, and Ed Bomke coined the term "plug-in".

</doc>
