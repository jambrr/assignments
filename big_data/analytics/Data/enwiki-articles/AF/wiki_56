<doc id="34040" url="https://en.wikipedia.org/wiki?curid=34040" title="West Bengal">
West Bengal

West Bengal পশ্চিমবঙ্গ (; ) is a state in eastern India and is the nation's fourth-most populous state, with over 91 million inhabitants. Spread over , it is bordered by the countries of Bangladesh, Nepal and Bhutan, and the Indian states of Odisha, Jharkhand, Bihar, Sikkim and Assam. The state capital is Kolkata. Together with the neighboring nation of Bangladesh, it makes up the ethno-linguistic region of Bengal.
Ancient Bengal was the site of several major janapadas (kingdoms). It was also part of large empires such as the Maurya Empire (second century BC) and Gupta Empire (fourth century AD); and part of the regional Buddhist Pala Empire (8th to 11th century) and Sena dynasty (11th–12th century). From the 13th century onward, the region was controlled by the Bengal Sultanate, Hindu kings and Baro-Bhuyan landlords under the suzerainty of the Mughal Empire, until the British East India company took control of the region from the Mughals in the late 18th century. The company consolidated their hold on the region following the Battle of Plassey in 1757 and Battle of Buxar in 1764 and by 1793 took complete control of the region. Kolkata (or Calcutta) served for many years as the capital of British controlled territories in India. The early and prolonged exposure to British administration resulted in the expansion of Western education, culminating in development of science, institutional education, and social reforms in the region, including what became known as the Bengali renaissance. A hotbed of the Indian independence movement through the early 20th century, Bengal was divided during India's independence in 1947 along religious lines into two separate entities: West Bengal—a state of India—and East Bengal—a part of the newly created Dominion of Pakistan that later became the independent nation of Bangladesh in 1971.
A major agricultural producer, West Bengal is the sixth-largest contributor to India's net domestic product. Noted for its political activism, the state was ruled by democratically elected communist governments for 34 years from 1977. It is noted for its cultural activities and the presence of cultural and educational institutions; the state capital Kolkata is known as the "cultural capital of India". The state's cultural heritage, besides varied folk traditions, ranges from stalwarts in literature including Nobel-laureate Rabindranath Tagore to scores of musicians, film-makers and artists. West Bengal is also distinct from most other Indian states in its appreciation and practice of playing Association football besides cricket, the national favourite sport.
Etymology.
The origin of the name Bengal (known as "Bangla" and "Bongo" in Bengali language) is unknown. One theory suggests that the word derives from "Bang", a Dravidian tribe that settled the region around 1000 BC. The word might have been derived from the ancient kingdom of "Vanga" (or "Banga"). Although some early Sanskrit literature mentions the name, the region's early history is obscure.
At the end of British Rule over the Indian subcontinent, the Bengal region was partitioned in 1947 along religious lines into east and west. The east came to be known as East Bengal and the west came to known as West Bengal which remained as an Indian state. In 2011, the Government of West Bengal proposed a change in the official name for the state to "Poschimbongo" ( "Pôshchimbônggô") which reflects the native name of the state, literally meaning western Bengal in the native Bengali language.
History.
Stone Age tools dating back 20,000 years have been excavated in the state. The region was a part of the Vanga Kingdom according to the Indian epic "Mahabharata". Several Vedic realms were present in Bengal region, including Vanga, Rarh, Pundravardhana and the Suhma Kingdom.
One of the earliest foreign references to Bengal is a mention by the Ancient Greeks around 100 BC of a land named Gangaridai, which was located at the mouths of the Ganges. Bengal had overseas trade relations with Suvarnabhumi (Burma, Lower Thailand, Lower Malay Peninsula, and the Sumatra). According to the Sri Lankan chronicle "Mahavamsa", Prince Vijaya, a Vanga Kingdom prince, conquered Lanka (modern day Sri Lanka) and gave the name Sinhala Kingdom to the country.
Era of the janapadas.
The kingdom of Magadha was formed in 7th century BCE, consisting of the regions now comprising Bihar and Bengal. It was one of the four main kingdoms of India at the time of the lives of Mahavira, founder of Jainism, and Gautama Buddha, founder of Buddhism, and consisted of several janapadas or kingdoms. Under Ashoka, the Maurya Empire of Magadha extended over nearly all of South Asia, including Afghanistan and parts of Balochistan in the 3rd century BCE. From the 3rd to the 6th centuries CE, the kingdom of Magadha served as the seat of the Gupta Empire.
Later rulers.
Two kingdoms – Vanga or Samatata and Gauda – are mentioned in some texts to have appeared after the end of Gupta Empire, although details of their ruling time are uncertain. The first recorded independent king of Bengal was Shashanka who reigned in the early 7th century. Shashanka is often recorded in Buddhist annals as an intolerant ruler who is noted for his persecution of the Buddhists. Shashanka murdered Rajyavardhana, the Buddhist King of Thanesar, and is noted for destroying the Bodhi tree at Bodhgaya, and replacing Buddha statues with Shiva Lingams. After a period of anarchy, the Pala dynasty ruled the region for four hundred years starting from the eighth century, followed by a shorter reign of the Hindu Sena dynasty. Some areas of Bengal were invaded by Rajendra Chola I of the Chola dynasty between 1021 and 1023. Islam made its first appearance in Bengal during the 12th century when Sufi missionaries arrived. Later, occasional Muslim raiders reinforced the process of conversion by building mosques, madrasas and khanqahs. Between 1202 and 1206, Muhammad bin Bakhtiyar Khilji, a military commander from the Delhi Sultanate, overran Bihar and Bengal as far east as Rangpur, Bogra and the Brahmaputra River. Although he failed to bring Bengal under his control, the expedition managed to defeat Lakshman Sen and his two sons moved to a place then called Vikramapur (present-day Munshiganj District), where their diminished dominion lasted until the late 13th century.
Subsequent Muslim conquests helped spread Islam throughout the region. Consequently, the region was ruled by dynasties of Bengal Sultanate and feudal lords under the Delhi Sultanate for the next few hundred years. Smaller Hindu states, landlords and Baro-Bhuyans also ruled in parts of Bengal. The Bengal Sultanate was interrupted for 20 years by an uprising by the Hindus under Raja Ganesha. In the sixteenth century, Mughal general Islam Khan conquered Bengal. However, administration by governors appointed by the court of the Mughal Empire gave way to semi-independence of the area under the Nawabs of Murshidabad, who nominally respected the sovereignty of the Mughals in Delhi. There were several independent Hindu states established in Bengal during the Mughal period like those of Pratapaditya of Jessore District and Raja Sitaram Ray of Bardhaman. The Koch dynasty in northern Bengal flourished during the period of 16th and the 17th centuries as well as weathered the Mughals and survived till the advent of the British.
Colonial era.
Several European traders arrived late in the fifteenth century. Their influence grew until the British East India Company gained rights to collect revenue in Bengal subah (province) in 1765 as per the treaty between the East India company and Mughal emperor following the Battle of Buxar in 1764, when Mir Qasim, the last independent Nawab, was defeated by the British.
The Bengal Presidency of was established in 1765, which later incorporated all British territories controlled north of the Central Provinces (now Madhya Pradesh), from the mouths of the Ganges and the Brahmaputra to the Himalayas and the Punjab. The Bengal famine of 1770 claimed millions of lives due to tax policies enacted by the company. Calcutta, the Headquarter of East India company was named the capital of British held territories in India in 1772. In 1793 East India company abolished local rule (Nizamat) and annexed the former Mughal province.
The Bengal Renaissance and Brahmo Samaj socio-cultural reform movements had great impact on the cultural and economic life of Bengal. The failed Indian rebellion of 1857 started near Calcutta and resulted in transfer of authority to the British Crown, administered by the Viceroy of India. Between 1905 and 1911, an abortive attempt was made to divide the province of Bengal into two zones. Bengal suffered from the Great Bengal famine in 1943 that claimed 3 million lives.
The Indian independence movement.
Bengal played a major role in the Indian independence movement, in which revolutionary groups such as "Anushilan Samiti" and "Jugantar" were dominant. Armed attempts against the British Raj from Bengal reached a climax when Subhas Chandra Bose led the Indian National Army from Southeast Asia against the British.
After independence.
When India gained independence in 1947, Bengal was partitioned along religious lines. The western part went to Dominion of India (and was named West Bengal) while the eastern part went to Dominion of Pakistan as a province called East Bengal (later renamed as East Pakistan in 1956), which gave rise to independent Bangladesh in 1971. In 1950, the Princely State of Cooch Behar merged with West Bengal. In 1955, the former French enclave of Chandannagar, which had passed into Indian control after 1950, was integrated into West Bengal; portions of Bihar were subsequently merged with West Bengal. Both West and East Bengal suffered from large refugee influx during and after the partition in 1947. Refugee settlement and related issues continued to play significant role in the politics and socio-economic condition of the state.
During the 1970s and 1980s, severe power shortages, strikes and a violent Naxalite movement damaged much of the state's infrastructure, leading to a period of economic stagnation. The Bangladesh Liberation War of 1971 resulted in the influx of millions of refugees to West Bengal, causing significant strains on its infrastructure. The 1974 smallpox epidemic killed thousands. West Bengal politics underwent a major change when the Left Front won the 1977 assembly election, defeating the incumbent Indian National Congress. The Left Front, led by Communist Party of India (Marxist), governed the state for the subsequent three decades.
Recent history.
The state's economic recovery gathered momentum after economic liberalisations were introduced in the mid-1990s by the central government, aided by the advent of information technology and IT-enabled services. Since mid-2000s, armed activists conducted minor terrorist attacks in some parts of the state, while clashes with the administration took place at several sensitive places over the issue of industrial land acquisition, which became a crucial reason behind the defeat of ruling Left Front government in 2011 assembly election. Although the state's GDP has risen significantly since the 1990s, West Bengal has remained affected by political instability and bad governance. The state continues to suffer from regular bandhs (strikes), a low Human Development Index level, substandard healthcare services, a lack of socio-economic development, poor infrastructure, political corruption and civil violence.
Geography and climate.
West Bengal is on the eastern bottleneck of India, stretching from the Himalayas in the north, to the Bay of Bengal in the south. The state has a total area of . The Darjeeling Himalayan hill region in the northern extreme of the state belongs to the eastern Himalaya. This region contains Sandakfu ()—the highest peak of the state. The narrow Terai region separates this region from the North Bengal plains, which in turn transitions into the Ganges delta towards the south. The Rarh region intervenes between the Ganges delta in the east and the western plateau and high lands. A small coastal region is on the extreme south, while the Sundarbans mangrove forests form a geographical landmark at the Ganges delta.
The Ganges is the main river, which divides in West Bengal. One branch enters Bangladesh as the "Padma" or "Pôdda", while the other flows through West Bengal as the Bhagirathi River and Hooghly River. The Farakka barrage over Ganges feeds the Hooghly branch of the river by a feeder canal, and its water flow management has been a source of lingering dispute between India and Bangladesh. The Teesta, Torsa, Jaldhaka and Mahananda rivers are in the northern hilly region. The western plateau region has rivers such as the Damodar, Ajay and Kangsabati. The Ganges delta and the Sundarbans area have numerous rivers and creeks. Pollution of the Ganges from indiscriminate waste dumped into the river is a major problem. Damodar, another tributary of the Ganges and once known as the "Sorrow of Bengal" (due to its frequent floods), has several dams under the Damodar Valley Project. At least nine districts in the state suffer from arsenic contamination of groundwater, and an estimated 8.7 million people drink water containing arsenic above the World Health Organisation recommended limit of 10 µg/L.
West Bengal's climate varies from tropical savanna in the southern portions to humid subtropical in the north. The main seasons are summer, rainy season, a short autumn, and winter. While the summer in the delta region is noted for excessive humidity, the western highlands experience a dry summer like northern India, with the highest day temperature ranging from to . At nights, a cool southerly breeze carries moisture from the Bay of Bengal. In early summer brief squalls and thunderstorms known as "Kalbaisakhi", or Nor'westers, often occur. West Bengal receives the Bay of Bengal branch of the Indian ocean monsoon that moves in a northwest direction. Monsoons bring rain to the whole state from June to September. Heavy rainfall of above 250 cm is observed in the Darjeeling, Jalpaiguri and Cooch Behar district. During the arrival of the monsoons, low pressure in the Bay of Bengal region often leads to the occurrence of storms in the coastal areas. Winter (December–January) is mild over the plains with average minimum temperatures of . A cold and dry northern wind blows in the winter, substantially lowering the humidity level. The Darjeeling Himalayan Hill region experiences a harsh winter, with occasional snowfall at places.
Flora and fauna.
As of 2013, recorded forest area in the state is which is 18.93% of the state's geographical area, compared to the national average of 21.23%. Reserves, protected and unclassed forests constitute 59.4%, 31.8% and 8.9%, respectively, of the forest area, as of 2009. Part of the world's largest mangrove forest, the Sundarbans, is located in southern West Bengal.
From a phytogeographic viewpoint, the southern part of West Bengal can be divided into two regions: the Gangetic plain and the littoral mangrove forests of the Sundarbans. The alluvial soil of the Gangetic plain, compounded with favourable rainfall, make this region especially fertile. Much of the vegetation of the western part of the state shares floristic similarities with the plants of the Chota Nagpur plateau in the adjoining state of Jharkhand. The predominant commercial tree species is "Shorea robusta", commonly known as the Sal tree. The coastal region of Purba Medinipur exhibits coastal vegetation; the predominant tree is the "Casuarina". A notable tree from the Sundarbans is the ubiquitous "sundari" ("Heritiera fomes"), from which the forest gets its name.
The distribution of vegetation in northern West Bengal is dictated by elevation and precipitation. For example, the foothills of the Himalayas, the "Dooars", are densely wooded with Sal and other tropical evergreen trees. However, above an elevation of , the forest becomes predominantly subtropical. In Darjeeling, which is above , temperate-forest trees such as oaks, conifers, and rhododendrons predominate.
West Bengal has 3.26% of its geographical area under protected areas comprising 15 wildlife sanctuaries and 5 national parks — Sundarbans National Park, Buxa Tiger Reserve, Gorumara National Park, Neora Valley National Park and Singalila National Park. Extant wildlife include Indian rhinoceros, Indian elephant, deer, leopard, gaur, tiger, and crocodiles, as well as many bird species. Migratory birds come to the state during the winter. The high-altitude forests of Singalila National Park shelter barking deer, red panda, chinkara, takin, serow, pangolin, minivet and kalij pheasants. The Sundarbans are noted for a reserve project conserving the endangered Bengal tiger, although the forest hosts many other endangered species, such as the Gangetic dolphin, river terrapin and estuarine crocodile. The mangrove forest also acts as a natural fish nursery, supporting coastal fishes along the Bay of Bengal. Recognising its special conservation value, Sundarban area has been declared as a Biosphere Reserve.
Government and politics.
West Bengal is governed through a parliamentary system of representative democracy, a feature the state shares with other Indian states. Universal suffrage is granted to residents. There are two branches of government. The legislature, the West Bengal Legislative Assembly, consists of elected members and special office bearers such as the Speaker and Deputy Speaker, that are elected by the members. Assembly meetings are presided over by the Speaker or the Deputy Speaker in the Speaker's absence. The judiciary is composed of the Calcutta High Court and a system of lower courts. Executive authority is vested in the Council of Ministers headed by the Chief Minister, although the titular head of government is the Governor. The Governor is the head of state appointed by the President of India. The leader of the party or coalition with a majority in the Legislative Assembly is appointed as the Chief Minister by the Governor, and the Council of Ministers are appointed by the Governor on the advice of the Chief Minister. The Council of Ministers reports to the Legislative Assembly. The Assembly is unicameral with 295 Members of the Legislative Assembly, or MLAs, including one nominated from the Anglo-Indian community. Terms of office run for 5 years, unless the Assembly is dissolved prior to the completion of the term. Auxiliary authorities known as "panchayats", for which local body elections are regularly held, govern local affairs. The state contributes 42 seats to the Lok Sabha and 16 seats to the Rajya Sabha of the Indian Parliament.
The main players in the regional politics are the All India Trinamool Congress, the Indian National Congress, and the Left Front alliance (led by the Communist Party of India (Marxist) or CPI(M)). Following the West Bengal State Assembly Election in 2011, the All India Trinamool Congress and Indian National Congress coalition under Mamata Banerjee of the All India Trinamool Congress was elected to power (getting 225 seats in the legislature). Prior to this, West Bengal was ruled by the Left Front for 34 years (1977–2011), making it the world's longest-running democratically elected communist government.
Subdivisions.
The following is a list of nineteen districts of West Bengal by rank in India.
Each district is governed by a district collector or district magistrate, appointed either by the Indian Administrative Service or the West Bengal Civil Service. Each district is subdivided into Sub-Divisions, governed by a sub-divisional magistrate, and again into Blocks. Blocks consists of panchayats (village councils) and town municipalities.
The capital and largest city of the state is Kolkata – the third-largest urban agglomeration and the seventh-largest city in India. Asansol is the second largest city & urban agglomeration in West Bengal after Kolkata. Siliguri is an economically important city, strategically located in the northeastern Siliguri Corridor (Chicken's Neck) of India. Other major cities and towns in West Bengal are Howrah, Durgapur,Baharampur, Raniganj, Haldia, Jalpaiguri, Kharagpur, Burdwan, Darjeeling, Midnapore, Malda, Sainthia.
Economy.
In 2009–10, the tertiary sector of the economy (service industries) was the largest contributor to the gross domestic product of the state, contributing 57.8% of the state domestic product compared to 24% from primary sector (agriculture, forestry, mining) and 18.2% from secondary sector (industrial and manufacturing). Agriculture is the leading occupation in West Bengal. Rice is the state's principal food crop. Rice, potato, jute, sugarcane and wheat are the top five crops of the state. Tea is produced commercially in northern districts; the region is well known for Darjeeling and other high quality teas. State industries are localised in the Kolkata region, the mineral-rich western highlands, and Haldia port region. The Durgapur–Asansol colliery belt is home to a number of major steel plants. Manufacturing industries playing an important economic role are engineering products, electronics, electrical equipment, cables, steel, leather, textiles, jewellery, frigates, automobiles, railway coaches, and wagons. The Durgapur centre has established a number of industries in the areas of tea, sugar, chemicals and fertilisers. Natural resources like tea and jute in and nearby parts has made West Bengal a major centre for the jute and tea industries.
Years after independence, West Bengal was still dependent on the central government for meeting its demands for food; food production remained stagnant and the Indian green revolution bypassed the state. However, there has been a significant spurt in food production since the 1980s, and the state now has a surplus of grains. The state's share of total industrial output in India was 9.8% in 1980–81, declining to 5% by 1997–98. However, the service sector has grown at a rate higher than the national rate.
In terms net state domestic product (NSDP), West Bengal has the sixth largest economy (2009–2010) in India, with an NSDP of 3663 billion Indian rupees, behind Maharashtra (8179 billion), Uttar Pradesh (4530 billion), Andhra Pradesh (4268 billion), Tamil Nadu (4177 billion), and Gujarat (3704 billion). In the period 2004–2005 to 2009–2010, the average gross state domestic product (GSDP) growth rate was 13.9% (calculated in Indian rupee term), lower than 15.5%, the average for all states of the country. The state’s per capita GSDP at current prices in 2009–10 was US$956.4, improved from US$553.7 in 2004–05,but lower than the national per capita GSDP of US$1,302. The state's total financial debt stood at as of 2011.
The state has promoted foreign direct investment, which has mostly come in the software and electronics fields; Kolkata is becoming a major hub for the Information technology (IT) industry. Rapid industrialisation process has given rise to debate over land acquisition for industry in this agrarian state. NASSCOM–Gartner ranks West Bengal power infrastructure the best in the country. Notably, many corporate companies are now headquartered in Kolkata include ITC Limited, India Government Mint, Kolkata, Haldia Petrochemicals, Exide Industries, Hindustan Motors, Britannia Industries, Bata India, Birla Corporation, CESC Limited, Coal India Limited, Damodar Valley Corporation, PwC India, Peerless Group, United Bank of India, UCO Bank and Allahabad Bank. In 2010s, events such as adoption of "Look East" policy by the government of India, opening of the Nathu La Pass in Sikkim as a border trade-route with China and immense interest in the South East Asian countries to enter the Indian market and invest have put Kolkata in an advantageous position for development in future, particularly with likes of Myanmar, where India needs oil from military regime.
Transport.
As of 2011, the total length of surface road in West Bengal is over ; national highways comprise and state highways . As of 2006, the road density of the state is 103.69 km per 100 km2 (166.92 mi per 100 sq mi), higher than the national average of 74.7 km per 100 km2 (120 mi per 100 sq mi). Average speed on state highways varies between 40–50 km/h (25–31 mi/h); in villages and towns, speeds are as low as 20–25 km/h (12–16 mi/h) due to the substandard quality of road constructions and low maintenance.
As of 2011, the total railway route length is around . Kolkata is the headquarters of three zones of the Indian Railways — Eastern Railway and South Eastern Railway and the Kolkata Metro which is the newly formed 17th Zone of the Indian Railways. The Northeast Frontier Railway (NFR) plies in the northern parts of the state. The Kolkata metro is the country's first underground railway. The Darjeeling Himalayan Railway, part of NFR, is a UNESCO World Heritage Site.
Netaji Subhas Chandra Bose International Airport at Dum Dum, Kolkata, is the state's biggest airport. Bagdogra Airport near Siliguri is a customs airport that has international services to Bhutan and Thailand besides regular domestic services. Kazi Nazrul Islam Airport, India's first private sector airport, serves the twin cities of Asansol-Durgapur at Andal, Bardhaman.
Kolkata is a major river-port in eastern India. The Kolkata Port Trust manages the Kolkata and the Haldia docks. There is passenger service to Port Blair on the Andaman and Nicobar Islands and cargo ship service to ports in India and abroad, operated by the Shipping Corporation of India. Ferry is a principal mode of transport in the southern part of the state, especially in the Sundarbans area. Kolkata is the only city in India to have trams as a mode of transport and these are operated by the Calcutta Tramways Company.
Several government-owned organisations operate bus services in the state, including the Calcutta State Transport Corporation, the North Bengal State Transport Corporation, the South Bengal State Transport Corporation, the West Bengal Surface Transport Corporation, and the Calcutta Tramways Company. There are also private bus companies. The railway system is a nationalised service without any private investment. Hired forms of transport include metered taxis and auto rickshaws which often ply specific routes in cities. In most of the state, cycle rickshaws, and in Kolkata, hand-pulled rickshaws, are used for short-distance travel. Large-scale transport accidents in West Bengal are common, particularly the sinking of transport boats and train crashes.
Demographics.
According to the provisional results of the 2011 national census, West Bengal is the fourth most populous state in India with a population of 91,347,736 (7.55% of India's population). Bengali Hindus comprise the majority of the population. The Marwaris and Bihari minorities are scattered throughout the state; various indigenous ethnic Buddhist communities such as the Sherpas, the Bhutias, the Lepchas, the Tamangs, the Yolmos and the ethnic Tibetans can be found in the Darjeeling Himalayan hill region. The Darjeeling district also has a large number of Nepali immigrant population, making Nepali a widely-spoken language in this region. West Bengal is home to indigenous tribal "Adivasis" such as Santhal, Kol, and Toto tribe. There are a small number of ethnic minorities primarily in the state capital, including Chinese, Tamils, Gujaratis, Anglo-Indians, Armenians, Punjabis, and Parsis. India's sole Chinatown is in eastern Kolkata.
The official language is Bengali and English. Nepali language also has an official status in the three subdivisions of Darjeeling district. As of 2001, in decreasing order of number of speakers, the languages of the state are: Bengali, Hindi, Santali, Urdu and Nepali.
West Bengal is religiously diverse, with region wise cultural and religious specificities. As of 2011, Hinduism is the largest religion followed by 70.53% of the total population, while Muslims comprise 27.01% of the total population, being the second-largest community as also the largest minority group. Sikhism, Christianity, Buddhism and other religions make up the remainder. Buddhism remains a prominent religion in the Himalayan region of the Darjeeling hills, and almost the entirety of West Bengal's Buddhist population are from this region.
The state contributes 7.8% of India's population. Hindu population is 6,43,85,546 in West Bengal while Muslim population is 2,46,54,825 as per 2011 census. The state's 2001–2011 decennial growth rate was 13.93%, lower than 1991–2001 growth rate of 17.8%, and also lower than the national rate of 17.64%. The gender ratio is 947 females per 1000 males. As of 2011, West Bengal has a population density of making it the second-most densely populated state in India, after Bihar.
The literacy rate is 77.08%, higher than the national rate of 74.04%. Data of 1995–1999 showed the life expectancy in the state was 63.4 years, higher than the national value of 61.7 years. About 72% of people live in rural areas. The proportion of people living below the poverty line in 1999–2000 was 31.9%. Scheduled Castes and Tribes form 28.6% and 5.8% of the population respectively in rural areas, and 19.9% and 1.5% respectively in urban areas. A study conducted in three districts of West Bengal found that accessing private health services to treat illness had a catastrophic impact on households. This indicates the value of public provision of health services to mitigate against poverty and the impact of illness on poor households.
In 2011, the police in West Bengal recorded 143,197 cognizable offences under the Indian Penal Code; the all-India statistic for the year was 2,325,575. The crime rate in the state was 158.1 per 100,000 people, less than the all-India average of 192.2. This is the twelfth-lowest crime rate among the 32 states and union territories of India. In 2011, in reported crimes against women, the state showed a crime rate of 29 compared to the national rate of 18. West Bengal accounted for about 12.2% of total crime against women (26,125 cases out of India's 213,585 cases). Some estimates state that there are more than 60,000 brothel-based women and girls in prostitution in Kolkata. The population of prostitutes in Sonagachi constitutes mainly of Nepalese, Indians and Bangladeshis. Some sources estimate there are 60,000 women in the brothels of Kolkata. The largest prostitution area in the city is Sonagachi. West Bengal was the first Indian state to constitute a Human Rights Commission of its own.
Culture.
Literature.
The Bengali language boasts a rich literary heritage, shared with neighbouring Bangladesh. West Bengal has a long tradition in folk literature, evidenced by the "Charyapada", "Mangalkavya", "Shreekrishna Kirtana", "Thakurmar Jhuli", and stories related to Gopal Bhar. In the nineteenth and twentieth
century, Bengali literature was modernised in the works of authors such as Bankim Chandra Chattopadhyay, Michael Madhusudan Dutt, Rabindranath Tagore, Kazi Nazrul Islam, Sharat Chandra Chattopadhyay, Jibananda Das and Manik Bandyopadhyay. In modern times Jibanananda Das, Bibhutibhushan Bandopadhyay, Tarashankar Bandopadhyay, Manik Bandopadhyay, Ashapurna Devi, Shirshendu Mukhopadhyay, Buddhadeb Guha, Mahashweta Devi, Samaresh Majumdar, Sanjeev Chattopadhyay and Sunil Gangopadhyay among others are well known.
Music and dance.
The Baul tradition is a unique heritage of Bengali folk music, which has also been influenced by regional music traditions. Other folk music forms include Gombhira and Bhawaiya. Folk music in West Bengal is often accompanied by the ektara, a one-stringed instrument. West Bengal also has a heritage in North Indian classical music. "Rabindrasangeet", songs composed and set into tune by Rabindranath Tagore and "Nazrul geeti" (by Kazi Nazrul Islam) are popular. Also prominent are other musical forms like Dwijendralal, Atulprasad and Rajanikanta's songs, and ""adhunik"" or modern music from films and other composers.
Films.
Mainstream Hindi films are popular in Bengal, and the state is home to a Tollywood. Tollygunj in Kolkata is the location of numerous Bengali movie studios, and the name "Tollywood" (similar to Hollywood and Bollywood) is derived from that name. The Bengali film industry is well known for its art films, and has produced acclaimed directors like Satyajit Ray, Mrinal Sen, Tapan Sinha and Ritwik Ghatak. Prominent contemporary directors include veterans like Buddhadev Dasgupta, Tarun Majumdar, Goutam Ghose, Aparna Sen, Rituparno Ghosh and a newer pool of directors like Kaushik Ganguly and Srijit Mukherji.
Fine arts.
Bengal had been the harbinger of modernism in fine arts. Abanindranath Tagore, called the father of Modern Indian Art had started the Bengal School of Art which was to create styles of art outside the European realist tradition which was taught in art colleges under the colonial administration of the British Government. The movement had many adherents like Gaganendranath Tagore, Ramkinkar Baij, Jamini Roy and Rabindranath Tagore. After Indian Independence, important groups like the Calcutta Group and the Society of Contemporary Artists were formed in Bengal which dominated the art scene in India.
Reformist heritage.
The capital, Kolkata, was the workplace of several social reformers, like Raja Ram Mohan Roy, Iswar Chandra Vidyasagar, and Swami Vivekananda. These social reforms have eventually led to a cultural atmosphere where practices like sati, dowry, and caste-based discrimination or untouchability, the evils that crept into the Hindu society, were abolished.The region was also home to several religious teachers, such as Chaitanya, Ramakrishna, Prabhupada and Paramahansa Yogananda.
Cuisine.
Rice and fish are traditional favourite foods, leading to a saying in Bengali, "machhe bhate bangali", that translates as "fish and rice make a Bengali". Bengal's vast repertoire of fish-based dishes includes hilsa preparations, a favourite among Bengalis. There are numerous ways of cooking fish depending on the texture, size, fat content and the bones. Sweets occupy an important place in the diet of Bengalis and at their social ceremonies. It is an ancient custom among both Hindu and Muslim Bengalis to distribute sweets during festivities. The confectionery industry has flourished because of its close association with social and religious ceremonies. Competition and changing tastes have helped to create many new sweets. Bengalis make distinctive sweetmeats from milk products, including "Rôshogolla", "Chômchôm", "Kalojam" and several kinds of "sondesh". Pitha, a kind of sweet cake, bread or dimsum are specialties of winter season. Sweets like coconut-naru, til-naru, moa, payesh, etc. are prepared during the festival of Lakshmi puja. Popular street food includes "Aloor Chop", Beguni, Kati roll, and phuchka.
The variety of fruits and vegetables that Bengal has to offer is incredible. A host of gourds, roots and tubers, leafy greens, succulent stalks, lemons and limes, green and purple eggplants, red onions, plantain, broad beans, okra, banana tree stems and flowers, green jackfruit and red pumpkins are to be found in the markets or anaj bazaar as popularly called. "Panta bhat" (rice soaked overnight in water)with onion & green chili is a traditional dish consumed in rural areas. Common spices found in a Bengali kitchen are cumin, ajmoda (radhuni), bay leaf, mustard, ginger, green chillies, turmeric, etc. People of erstwhile East Bengal use a lot of ajmoda, coriander leaves, tamarind, coconut and mustard in their cooking; while those aboriginally from West Bengal use a lot of sugar, garam masala and red chilli powder. Vegetarian dishes are mostly without onion and garlic.
Costumes.
Bengali women commonly wear the "shaŗi", often distinctly designed according to local cultural customs. In urban areas, many women and men wear Western attire. Among men, western dressing has greater acceptance. Men also wear traditional costumes such as the "panjabi" with "dhuti", often on cultural occasions.
Weaving.
West Bengal has a rich heritage of handloom weaving, and produces some of the finest varieties of cotton and silk sarees in the country. From an economic standpoint, handlooms come second only to agriculture in providing livelihood to the rural population of the state. Every district has weaving ‘clusters’, which are home to artisan communities, each specializing in specific varieties of handloom weaving. Famous handloom sarees woven in the state include tant, jamdani, garad, korial, baluchari, tussar and muslin.
Festivals.
Durga Puja in September-October is the most popular and most widely celebrated festival in West Bengal. The whole city of Kolkata undergoes a transformation during Durga Puja, decked in light and colour, as various pandals to the goddess are made on an eclectic array of themes. The idols of the goddess as brought in from Kumortuli, where idol-makers work throughout the year fashioning the clay-models of the goddess. Durga Puja is often labelled as India's largest open air art exhibition, spilling over from the domain of the religious into becoming a cultural event, where people across diverse religious and cultural spectrum partake in the festival. Tourists from all across the country come to the city of Kolkata to witness this major event. The state tourism department, as of 2015, has been attempting to promote and market the event to international tourists as well. Durga Puja is a six-day long annual holiday in West Bengal. Vijayadashami, the last day of the festival, witnesses long processions carrying the idols towards rivers and ponds, traditional dhunuchi-nritya and sindoor-khela, beatings of dhak and exchange of sweets.
Poila Baishakh, Rathayatra, Dolyatra or Holi, Nobanno, Poush Parbon, Kali Puja, Saraswati Puja, Diwali, Lakshmi Puja, Janmashtami, Jagaddhatri Puja, Vishwakarma Puja, Bhai Phonta, Rakhi Bandhan, Kalpataru Day, Shivratri, Ganesh Chathurthi, Maghotsav, Kartik Puja, Akshay Tritiya, Raasyatra, Guru Purnima, Basanti Puja, Charak Puja, Gajan, Buddha Purnima, Christmas, Eid ul-Fitr, Eid ul-Adha and Muharram are other major festivals. Both Rabindra Jayanti and Nazrul Jayanti are important socio-cultural festivals.
Christmas, called "Bôŗodin" (Great day) is perhaps the next major festival celebrated in Kolkata, after Durga Puja. Just like Durga Puja, Christmas in Kolkata is an occasion in which all communities and people across religions take part. The state tourism department organizes the gala Christmas Festival every year in Park Street. The whole of Park Street is decked up in colourful lights, various food stalls are set up selling cakes, chocolates, Chinese cuisines, momo and various other items. Musical groups from Darjeeling and other states of North East India are invited by the state to perform choir recitals, carols and jazz numbers. Buddha Purnima, which marks the birth of Gautama Buddha, is one of the most important Hindu/Buddhist festivals and is celebrated with much gusto in the Darjeeling hills. On this day, processions are taken out of each of the various Buddhist monasteries or "gumpas", and these congregate at the Mall, Chowrasta. The Lamas chant mantras and sound their bugles, and students as well as people from all communities carry the holy books or "pustaks" on their heads. Besides Buddha Purnima, Dashain, Diwali, Losar, Namsoong or the Lepcha New Year and Losoong are the other major festivals of the Darjeeling Himalayan region.
Poush mela is a popular festival of Shantiniketan in winter.
Education.
West Bengal schools are run by the state government or by private organisations, including religious institutions. Instruction is mainly in English or Bengali, though Urdu is also used, especially in Central Kolkata. The secondary schools are affiliated with the Council for the Indian School Certificate Examinations (CISCE), the Central Board for Secondary Education (CBSE), the National Institute of Open School (NIOS) or the West Bengal Board of Secondary Education.
Under the 10+2+3 plan, after completing secondary school, students typically enroll for two years in a junior college, also known as pre-university, or in schools with a higher secondary facility affiliated with the West Bengal Council of Higher Secondary Education or any central board. Students choose from one of three streams: liberal arts, commerce or science. Upon completing the required coursework, students may enroll in general or professional degree programs.
Some of the renowned schools in the city are La Martiniere Calcutta, St. Xavier's Collegiate School, and Loreto House - which consistently rank amongst the best schools in the country. Many of the schools in Kolkata and Darjeeling are renowned colonial-era establishments and boast of fantastic neo-classical architecture. The famous schools of Darjeeling include St. Paul's, St. Joseph's North Point, Goethals Memorial School and Dow Hill in Kurseong.
West Bengal has 18 universities. Kolkata has played a pioneering role in the development of the modern education system in India. It is the gateway to the revolution of European education. Sir William Jones established the Asiatic Society in 1794 for promoting oriental studies. People like Ram Mohan Roy, David Hare, Ishwar Chandra Vidyasagar, Alexander Duff and William Carey played leading roles in the setting up of modern schools and colleges in the city.
The University of Calcutta, the oldest public university in India, has 136 affiliated colleges. The Fort William College was established in 1810. The Hindu College was established in 1817. The Scottish Church College, which is the oldest Christian liberal arts college in South Asia, started its journey in 1830. In 1855 the Hindu College was renamed as the Presidency College. In 2010, it was granted university status by the state government and was renamed Presidency University. The Kazi Nazrul University was established in 2012. The University of Calcutta and Jadavpur University are prestigious technical universities. Visva-Bharati University at Santiniketan is a central university and an institution of national importance.
The state has several higher education institutes of national importance including Indian Institute of Foreign Trade, Indian Institute of Management Calcutta (the first IIM), Indian Institute of Science Education and Research, Kolkata, Indian Statistical Institute, Indian Institute of Technology Kharagpur (the first IIT), Indian Institute of Engineering Science and Technology, Shibpur (the first IIEST), National Institute of Technology, Durgapur and West Bengal National University of Juridical Sciences. After 2003 the state govt supported the creation of West Bengal University of Technology, West Bengal State University and Gour Banga University.
Besides these, the state has Kalyani University, The University of Burdwan, Vidyasagar University and North Bengal University — all well-established and nationally renowned — to cover the education needs at the district level and an Indian Institute of Science Education and Research, Kolkata. Apart from this there is a private university run by Ramakrishna mission named Ramakrishna Mission Vivekananda University at Belur Math.
There are a number of research institutes in Kolkata. The Indian Association for the Cultivation of Science is the first research institute in Asia. C. V. Raman got Nobel Prize for his discovery (Raman Effect) done in IACS. Bose Institute, Saha Institute of Nuclear Physics, S.N. Bose National Centre for Basic Sciences, Indian Institute of Chemical Biology, Central Glass and Ceramic Research Institute, Variable Energy Cyclotron Centre are most prominent.
Notable scholars who were born, worked or studied in the geographic area of the state include physicists Satyendra Nath Bose, Meghnad Saha, and Jagadish Chandra Bose; chemist Prafulla Chandra Roy; statisticians Prasanta Chandra Mahalanobis and Anil Kumar Gain; physician Upendranath Brahmachari; educator Ashutosh Mukherjee; and Nobel laureates Rabindranath Tagore, C. V. Raman, and Amartya Sen.
Media.
West Bengal had 505 published newspapers in 2005, of which 389 were in Bengali. "Ananda Bazar Patrika", published from Kolkata with 1,277,801 daily copies, has the largest circulation for a single-edition, regional language newspaper in India. Other major Bengali newspapers are "Bartaman", "Sangbad Pratidin", "Aajkaal", "Jago Bangla", "Uttarbanga Sambad" and "Ganashakti". Major English language newspapers which are published and sold in large numbers are "The Telegraph", "The Times of India", "Hindustan Times", "The Hindu", "The Statesman", "The Indian Express" and "Asian Age". Some prominent financial dailies like "The Economic Times", "Financial Express", "Business Line" and "Business Standard" are widely circulated. Vernacular newspapers such as those in Hindi, Nepali Gujarati, Odia, Urdu and Punjabi are also read by a select readership.
Doordarshan is the state-owned television broadcaster. Multi system operators provide a mix of Bengali, Nepali, Hindi, English and international channels via cable. Bengali 24-hour television news channels include ABP Ananda, Tara Newz, Kolkata TV, News Time, 24 Ghanta, Mahuaa Khobor, Ne Bangla, CTVN Plus, Channel 10 and R Plus. All India Radio is a public radio station. Private FM stations are available only in cities like Kolkata, Siliguri and Asansol. Vodafone, Airtel, BSNL, Reliance Communications, Uninor, Aircel, MTS India, Tata Indicom, Idea Cellular and Tata DoCoMo are available cellular phone operators. Broadband internet is available in select towns and cities and is provided by the state-run BSNL and by other private companies. Dial-up access is provided throughout the state by BSNL and other providers.
Sports.
Cricket and soccer are popular sports in the state. West Bengal, unlike most other states of India, is noted for its passion and patronage of football. Kolkata is one of the major centres for football in India and houses top national clubs such as East Bengal, Mohun Bagan and Mohammedan Sporting Club.
West Bengal has several large stadiums—The Eden Gardens is one of only two 100,000-seat cricket amphitheaters in the world, although renovations will reduce this figure. Kolkata Knight Riders, East Zone and Bengal play there, and the 1987 World Cup final was there although in 2011 World Cup. Calcutta Cricket and Football Club is the second-oldest cricket club in the world. Notable sports persons from West Bengal include former Indian national cricket captain Sourav Ganguly, Pankaj Roy Olympic tennis bronze medallist Leander Paes, and chess grand master Dibyendu Barua. 

</doc>
<doc id="34043" url="https://en.wikipedia.org/wiki?curid=34043" title="Wormhole">
Wormhole

A wormhole or Einstein–Rosen bridge is a hypothetical topological feature that would fundamentally be a shortcut connecting two separate points in spacetime. A wormhole, in theory, might be able to connect extremely far distances such as a billion light years or more, short distances such as a few feet, different universes, and different points in time. A wormhole is much like a tunnel with two ends, each at separate points in spacetime.
For a simplified notion of a wormhole, space can be visualized as a two-dimensional (2D) surface. In this case, a wormhole would appear as a hole in that surface, lead into a 3D tube (the inside surface of a cylinder), then re-emerge at another location on the 2D surface with a hole similar to the entrance. An actual wormhole would be analogous to this, but with the spatial dimensions raised by one. For example, instead of circular holes on a 2D plane, the entry and exit points could be visualized as spheres in 3D space.
Overview.
Researchers have some observational evidence for wormholes, and the equations of the theory of general relativity have valid solutions that contain wormholes. The first type of wormhole solution discovered was the Schwarzschild wormhole, which would be present in the Schwarzschild metric describing an eternal black hole, but it was found that it would collapse too quickly for anything to cross from one end to the other. Wormholes that could be crossed in both directions, known as traversable wormholes, would only be possible if exotic matter with negative energy density could be used to stabilize them. Wormholes are also a very powerful mathematical metaphor for teaching general relativity. 
The Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where energy can be "arbitrarily" negative at a given point. Many physicists, such as Stephen Hawking, Kip Thorne and others, therefore argue that such effects might make it possible to stabilize a traversable wormhole. Physicists have not found any natural process that would be predicted to form a wormhole naturally in the context of general relativity, although the quantum foam hypothesis is sometimes used to suggest that tiny wormholes might appear and disappear spontaneously at the Planck scale, and stable versions of such wormholes have been suggested as dark matter candidates. It has also been proposed that, if a tiny wormhole held open by a negative mass cosmic string had appeared around the time of the Big Bang, it could have been inflated to macroscopic size by cosmic inflation.
The American theoretical physicist John Archibald Wheeler coined the term "wormhole" in 1957; the German mathematician Hermann Weyl, however, had proposed the wormhole theory in 1921, in connection with mass analysis of electromagnetic field energy.
Definition.
The basic notion of an intra-universe wormhole is that it is a compact region of spacetime whose boundary is topologically trivial, but whose interior is not simply connected. Formalizing this idea leads to definitions such as the following, taken from Matt Visser's "Lorentzian Wormholes".
Wormholes have been defined "geometrically", as opposed to "topologically", as regions of spacetime that constrain the incremental deformation of closed surfaces. For example, in Enrico Rodrigo’s "The Physics of Stargates, "a wormhole is defined informally as: 
Schwarzschild wormholes.
Lorentzian wormholes known as "Schwarzschild wormholes" or "Einstein–Rosen bridges" are connections between areas of space that can be modeled as vacuum solutions to the Einstein field equations, and that are now understood to be intrinsic parts of the maximally extended version of the Schwarzschild metric describing an eternal black hole with no charge and no rotation. Here, "maximally extended" refers to the idea that the space-time should not have any "edges": it should be possible to continue this path arbitrarily far into the particle's future or past for any possible trajectory of a free-falling particle (following a Geodesic in the spacetime), unless the trajectory hits a gravitational singularity like the one at the center of the black hole's interior. In order to satisfy this requirement, it turns out that in addition to the black hole interior region that particles enter when they fall through the event horizon from the outside, there must be a separate white hole interior region that allows us to extrapolate the trajectories of particles that an outside observer sees rising up "away" from the event horizon. And just as there are two separate interior regions of the maximally extended spacetime, there are also two separate exterior regions, sometimes called two different "universes", with the second universe allowing us to extrapolate some possible particle trajectories in the two interior regions. This means that the interior black hole region can contain a mix of particles that fell in from either universe (and thus an observer who fell in from one universe might be able to see light that fell in from the other one), and likewise particles from the interior white hole region can escape into either universe. All four regions can be seen in a spacetime diagram that uses Kruskal–Szekeres coordinates.
In this spacetime, it is possible to come up with coordinate systems such that if you pick a hypersurface of constant time (a set of points that all have the same time coordinate, such that every point on the surface has a space-like separation, giving what is called a 'space-like surface') and draw an "embedding diagram" depicting the curvature of space at that time, the embedding diagram will look like a tube connecting the two exterior regions, known as an "Einstein–Rosen bridge". Note that the Schwarzschild metric describes an idealized black hole that exists eternally from the perspective of external observers; a more realistic black hole that forms at some particular time from a collapsing star would require a different metric. When the infalling stellar matter is added to a diagram of a black hole's history, it removes the part of the diagram corresponding to the white hole interior region, along with the part of the diagram corresponding to the other universe.
The Einstein–Rosen bridge was discovered by Ludwig Flamm in 1916, a few months after Schwarzschild published his solution, and was rediscovered (although it is hard to imagine that Einstein had not seen Flamm's paper when it came out) by Albert Einstein and his colleague Nathan Rosen, who published their result in 1935. However, in 1962, John A. Wheeler and Robert W. Fuller published a paper showing that this type of wormhole is unstable if it connects two parts of the same universe, and that it will pinch off too quickly for light (or any particle moving slower than light) that falls in from one exterior region to make it to the other exterior region.
According to general relativity, the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole. In the Einstein–Cartan–Sciama–Kibble theory of gravity, however, it forms a regular Einstein–Rosen bridge. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. Torsion naturally accounts for the quantum-mechanical, intrinsic angular momentum (spin) of matter. The minimal coupling between torsion and Dirac spinors generates a repulsive spin–spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction prevents the formation of a gravitational singularity. Instead, the collapsing matter reaches an enormous but finite density and rebounds, forming the other side of the bridge.
Before the stability problems of Schwarzschild wormholes were apparent, it was proposed that quasars were white holes forming the ends of wormholes of this type.
Although Schwarzschild wormholes are not traversable in both directions, their existence inspired Kip Thorne to imagine traversable wormholes created by holding the "throat" of a Schwarzschild wormhole open with exotic matter (material that has negative mass/energy).
Traversable wormholes.
Lorentzian traversable wormholes would allow travel in both directions from one part of the universe to another part of that same universe very quickly or would allow travel from one universe to another. The possibility of traversable wormholes in general relativity was first demonstrated by Kip Thorne and his graduate student Mike Morris in a 1988 paper. For this reason, the type of traversable wormhole they proposed, held open by a spherical shell of exotic matter, is referred to as a "Morris–Thorne wormhole". Later, other types of traversable wormholes were discovered as allowable solutions to the equations of general relativity, including a variety analyzed in a 1989 paper by Matt Visser, in which a path through the wormhole can be made where the traversing path does not pass through a region of exotic matter. However, in the pure Gauss–Bonnet gravity (a modification to general relativity involving extra spatial dimensions which is sometimes studied in the context of brane cosmology) exotic matter is not needed in order for wormholes to exist—they can exist even with no matter. A type held open by negative mass cosmic strings was put forth by Visser in collaboration with Cramer "et al.", in which it was proposed that such wormholes could have been naturally created in the early universe.
Wormholes connect two points in spacetime, which means that they would in principle allow travel in time, as well as in space. In 1988, Morris, Thorne and Yurtsever worked out explicitly how to convert a wormhole traversing space into one traversing time. However, according to general relativity, it would not be possible to use a wormhole to travel back to a time earlier than when the wormhole was first converted into a time machine by accelerating one of its two mouths.
Raychaudhuri's theorem and exotic matter.
To see why exotic matter is required, consider an incoming light front traveling along geodesics, which then crosses the wormhole and re-expands on the other side. The expansion goes from negative to positive. As the wormhole neck is of finite size, we would not expect caustics to develop, at least within the vicinity of the neck. According to the optical Raychaudhuri's theorem, this requires a violation of the averaged null energy condition. Quantum effects such as the Casimir effect cannot violate the averaged null energy condition in any neighborhood of space with zero curvature, but calculations in semiclassical gravity suggest that quantum effects may be able to violate this condition in curved spacetime. Although it was hoped recently that quantum effects could not violate an achronal version of the averaged null energy condition, violations have nevertheless been found, so it remains an open possibility that quantum effects might be used to support a wormhole.
Modified general relativity.
In some theories where general relativity is modified, it is possible to have a wormhole which does not collapse without having to resort to exotic matter. For example this is possible with R^2 gravity, a form of f(R) gravity.
Faster-than-light travel.
The impossibility of faster-than-light relative speed only applies locally. Wormholes might allow effective superluminal (faster-than-light) travel by ensuring that the speed of light is not exceeded locally at any time. While traveling through a wormhole, subluminal (slower-than-light) speeds are used. If two points are connected by a wormhole whose length is shorter than the distance between them "outside" the wormhole, the time taken to traverse it could be less than the time it would take a light beam to make the journey if it took a path through the space "outside" the wormhole. However, a light beam traveling through the wormhole would of course beat the traveler.
Time travel.
The theory of general relativity predicts that if traversable wormholes exist, they can also alter the speed of time. They could allow time travel. This would be accomplished by accelerating one end of the wormhole to a high velocity relative to the other, and then sometime later bringing it back; relativistic time dilation would result in the accelerated wormhole mouth aging less than the stationary one as seen by an external observer, similar to what is seen in the twin paradox. However, time connects differently through the wormhole than outside it, so that synchronized clocks at each mouth will remain synchronized to someone traveling through the wormhole itself, no matter how the mouths move around. This means that anything which entered the accelerated wormhole mouth would exit the stationary one at a point in time prior to its entry.
For example, consider two clocks at both mouths both showing the date as 2000. After being taken on a trip at relativistic velocities, the accelerated mouth is brought back to the same region as the stationary mouth with the accelerated mouth's clock reading 2004 while the stationary mouth's clock read 2012. A traveler who entered the accelerated mouth at this moment would exit the stationary mouth when its clock also read 2004, in the same region but now eight years in the past. Such a configuration of wormholes would allow for a particle's world line to form a closed loop in spacetime, known as a closed timelike curve. An object traveling through a wormhole could carry energy or charge from one time to another, but this would not violate conservation of energy or charge in each time, because the energy/charge of the wormhole mouth itself would change to compensate for the object that fell into it or emerged from it.
It is thought that it may not be possible to convert a wormhole into a time machine in this manner; the predictions are made in the context of general relativity, but general relativity does not include quantum effects. Analyses using the semiclassical approach to incorporating quantum effects into general relativity have sometimes indicated that a feedback loop of virtual particles would circulate through the wormhole and pile up on themselves, driving the energy density in the region very high and possibly destroying it before any information could be passed through it, in keeping with the chronology protection conjecture. The debate on this matter is described by Kip S. Thorne in the book "Black Holes and Time Warps", and a more technical discussion can be found in "The quantum physics of chronology protection" by Matt Visser. There is also the Roman ring, which is a configuration of more than one wormhole. This ring seems to allow a closed time loop with stable wormholes when analyzed using semiclassical gravity, although without a full theory of quantum gravity it is uncertain whether the semiclassical approach is reliable in this case.
Interuniversal travel.
A possible resolution to the paradoxes resulting from wormhole-enabled time travel rests on the many-worlds interpretation of quantum mechanics. In 1991 David Deutsch showed that quantum theory is fully consistent (in the sense that the so-called density matrix can be made free of discontinuities) in spacetimes with closed timelike curves. However, later it was shown that such model of closed timelike curve can have internal inconsistencies as it will lead to strange phenomena like distinguishing non orthogonal quantum states and distinguishing proper and improper mixture. Accordingly, the destructive positive feedback loop of virtual particles circulating through a wormhole time machine, a result indicated by semi-classical calculations, is averted. A particle returning from the future does not return to its universe of origination but to a parallel universe. This suggests that a wormhole time machine with an exceedingly short time jump is a theoretical bridge between contemporaneous parallel universes. Because a wormhole time-machine introduces a type of nonlinearity into quantum theory, this sort of communication between parallel universes is consistent with Joseph Polchinski’s discovery of an "Everett phone" in Steven Weinberg’s formulation of nonlinear quantum mechanics. Such a possibility is depicted in the science-fiction 2014 movie "Interstellar".
Metrics.
Theories of "wormhole metrics" describe the spacetime geometry of a wormhole and serve as theoretical models for time travel. An example of a (traversable) wormhole metric is the following:
One type of non-traversable wormhole metric is the Schwarzschild solution (see the first diagram):
In fiction.
Wormholes are a common element in science fiction as they allow interstellar, intergalactic, and sometimes even interuniversal travel within human lifetime scales. They have also served as a method for time travel.

</doc>
<doc id="34044" url="https://en.wikipedia.org/wiki?curid=34044" title="Web banner">
Web banner

A web banner or banner ad is a form of advertising on the World Wide Web delivered by an ad server. This form of online advertising entails embedding an advertisement into a web page. It is intended to attract traffic to a website by linking to the website of the advertiser. In many cases, banners are delivered by a central ad server.
When the advertiser scans their logfiles and detects that a web user has visited the advertiser's site from the content site by clicking on the banner ad, the advertiser sends the content provider some small amount of money (usually around five to ten US cents). This payback system is often how the content provider is able to pay for the Internet access to supply the content in the first place. Usually though, advertisers use ad networks to serve their advertisements, resulting in a revshare system and higher quality ad placement.
Web banners function the same way as traditional advertisements are intended to function: notifying consumers of the product or service and presenting reasons why the consumer should choose the product in question, a fact first documented on HotWired in 1996 by researchers Rex Briggs and Nigel Hollis. Web banners differ in that the results for advertisement campaigns may be monitored real-time and may be targeted to the viewer's interests. Behavior is often tracked through the use of a click tag.
Many web surfers regard these advertisements as annoying because they distract from a web page's actual content or waste bandwidth. Newer web browsers often include options to disable pop-ups or block images from selected websites. Another way of avoiding banners is to use a proxy server that blocks them, such as Privoxy. Web browsers may also have extensions available that block banners, for example Adblock Plus for Mozilla Firefox, or AdThwart for Google Chrome and ie7pro for Internet Explorer.
History.
The pioneer of online advertising was Prodigy, a company owned by IBM and Sears at the time. Prodigy used online advertising first to promote Sears products in the 1980s, and then other advertisers, including AOL, one of Prodigy's direct competitors. Prodigy was unable to capitalize on any of its first mover advantage in online advertising.
The first clickable web ad (which later came to be known by the term "banner ad") was sold by Global Network Navigator (GNN) in 1993 to Heller, Ehrman, White, & McAuliffe, a now defunct law firm with a Silicon Valley office. GNN was the first commercially supported web publication and one of the very first commercial web sites ever.
HotWired was the first web site to sell banner ads in large quantities to a wide range of major corporate advertisers. Andrew Anker was HotWired's first CEO. Rick Boyce, a former media buyer with San Francisco advertising agency Hal Riney & Partners, spearheaded the sales effort for the company. HotWired coined the term "banner ad" and was the first company to provide click through rate reports to its customers. The first web banner sold by HotWired was paid for by AT&T Corp. and was put online on October 27, 1994. Another source also credits Hotwired and October 1994, but has Coors' "Zima" campaign as the first web banner.
In May 1994, Ken McCarthy mentored Boyce in his transition from traditional to online advertising and first introduced the concept of a clickable/trackable ad. He stated that he believed that only a direct response model—in which the return on investment of individual ads was measured—would prove sustainable over the long run for online advertising. In spite of this prediction, banner ads were valued and sold based on the number of impressions they generated.
The first central ad server was released in July 1995 by Focalink Communications, which enabled the management, targeting, and tracking of online ads. A local ad server quickly followed from NetGravity in January 1996. The technology innovation of the ad server, together with the sale of online ads on an impression basis, fueled a dramatic rise in the proliferation of web advertising and provided the economic foundation for the web industry from the period of 1994 to 2000.
The new online advertising model that emerged in the early years of the 21st century, introduced by GoTo.com (later Overture, then Yahoo! and mass marketed by Google's AdWords program), relies heavily on tracking ad response rather than impressions.
Significance.
The banner ad played a significant role in enabling the rapid development of paid advertising on the Internet. With the standard formats, operation (clickable link to a destination), and pricing system (impressions), the banner ad enabled any Web site to sell advertising, and provided the operating requirements for ad server companies, such as NetGravity, to develop the systems needed to operate and track Web-based advertising.
The banner ad was also unique, as compared to advertising appearing in then comparable media, such as newspapers and magazines. Unlike advertising in periodicals, the banner ad encouraged media consumers to actually leave the media service or product and go to a separate media environment (typically a Web site operated by the advertiser). In contrast, readers viewing newspaper or magazine advertising are not encouraged to leave the periodical. Rather, the message of the advertising is itself intended to influence the reader.
Standard sizes.
Ad sizes have been standardized to some extent by the IAB. Prior to the IAB standardization, banner ads appeared in over 250 different sizes. However, some websites and advertising networks (outside the Eurosphere or North America) may not use any or all of the IAB base ad sizes. The IAB ad sizes are :
Standard web banners included into the IAB’s Universal Package and Ad Units Guidelines are supported by major ad serving companies. This is particularly relevant for IAB members such as Adform, AppNexus, Chitika, Conversant, Epom, HIRO, Mixpo, SpotXchange, ZEDO, and many others. Additionally, ad serving providers may offer other, non-standard banner sizes and technologies, as well as the support of different online advertising formats (e.g. native ads).
However, standard banner ad sizes are constantly evolving due to consumer creative fatigue and banner blindness. Ad companies consistently test performance of ad units to ensure maximum performance for their clients. Some publishers that are known for their unique, custom executions include BuzzFeed, CraveOnline, Quartz (publication), Thought Catalog, Elite Daily, Vice Media, Inc., Mic (media company), and many others. According to media research firm eMarketer, such types of custom executions through publisher direct buys are on the rise, with Native advertising spending to hit over $4.3 Billion by the end of 2015.
Non-advertising web banners.
The use of web banners is not restricted to online advertising. Hero images are widespread examples of non-advertising application of web banners. The banners of this type constitute a part of website design and are typically used for aesthetic reasons. Hero images are represented by large photos, graphics, or videos that are placed in the prominent sections of a website.

</doc>
<doc id="34046" url="https://en.wikipedia.org/wiki?curid=34046" title="Vedda people">
Vedda people

Veddas ( , "Vēṭuvar") are an indigenous people of Sri Lanka. They, amongst other self-identified native communities such as Coast Veddas and Anuradhapura Veddas, are accorded indigenous status. Most speak Sinhala and Tamil instead due to their indigenous language having become nearly extinct.
According to the genesis chronicle of the Sinhala people, the Mahavamsa (""Great Chronicle""), written in the 5th century CE, the Pulindas believed to refer to Veddas are descended from Prince Vijaya (6th–5th century BCE), the founding father of the nation, through Kuveni, a woman of the indigenous "Yakkha" he married. The "Mahavansa" relates that following the repudiation of Kuveni by Vijaya, in favour of a Kshatriya-caste princess from Pandya, their two children, a boy and a girl, departed to the region of "Sumanakuta" (Adam's Peak in the Ratnapura District), where they multiplied, giving rise to the Veddas. Anthropologists such as the Seligmanns ("The Veddas" 1911) believed the Veddas to be identical with the "Yakkha".
Veddas are also mentioned in Robert Knox's history of his captivity by the King of Kandy in the 17th century. Knox described them as "wild men", but also said there was a "tamer sort", and that the latter sometimes served in the king's army.
The Ratnapura District, which is part of the Sabaragamuwa Province, is known to have been inhabited by the Veddas in the distant past. This has been shown by scholars like Nandadeva Wijesekera (Veddhas in transition 1964). The very name "Sabaragamuwa" is believed to have meant the village of the "Sabaras" or "forest barbarians". Such place-names as "Vedda-gala" (Vedda Rock), "Vedda-ela" (Vedda Canal) and "Vedi-kanda" (Vedda Mountain) in the Ratnapura District also bear testimony to this. As "Wijesekera" observes, a strong Vedda element is discernible in the population of "Vedda-gala" and its environs.
Language.
The original language of the Veddas is the Vedda language. Today it is used primarily by the interior Veddas of Dambana. Communities, such as Coast Veddas and Anuradhapura Veddas, that do not identify themselves strictly as Veddas also use Vedda language in part for communication during hunting and or for religious chants. When a systematic field study was conducted in 1959 it was determined that the language was confined to the older generation of Veddas from Dambana. In 1990s self-identifying Veddas knew few words and phrases in the Vedda language, but there were individuals who knew the language comprehensively. Initially there was considerable debate amongst linguists as to whether Vedda is a dialect of Sinhalese or an independent language. Later studies indicate that it diverged from its parent stock in the 10th century and became a Creole and a stable independent language by the 13th century, under the influence of Sinhalese. 
The parent Vedda language(s) is of unknown genetic origins, while Sinhalese is of the Indo-Aryan branch of Indo-European languages. Phonologically it is distinguished from Sinhalese by the higher frequency of palatal sounds C and J. The effect is also heightened by the addition of inanimate suffixes. Morphologically Vedda language word class is divided into nouns, verbs and invariables with unique gender distinctions in animate nouns. Per its Creole tradition, it has reduced and simplified many forms of Sinhalese such as second person pronouns and denotations of negative meanings. Instead borrowing new words from Sinhalese Vedda created combinations of words from a limited lexical stock. Vedda also maintains many archaic Sinhalese terms prior to the 10th to 12th centuries, as a relict of its close contact with Sinhalese. Vedda also retains a number of unique words that cannot be derived from Sinhalese. Conversely, Sinhalese has also borrowed from the original Vedda language, words and grammatical structures, differentiating it from its related Indo-Aryan languages. Vedda has exerted a substratum influence in the formation of Sinhalese.
Veddas that have adopted Sinhala are found primarily in the southeastern part of the country,
especially in the vicinity of Bintenne in Uva District. There are also Veddas that have adopted Sinhala who live in Anuradhapura District in the North Central Province.
Another group, often termed East Coast Veddas, is found in coastal areas of the Eastern Province, between Batticaloa and Trincomalee. These Veddas have adopted Tamil as their mother tongue.
Cultural aspects.
Language.
The parent of Vedda language is of unknown linguistic origin, is considered a linguistic isolate. Early linguists and observers of the language considered it to be either a separate language or a dialect of Sinhalese. The chief proponent of the dialect theory was Wilhelm Geiger, but he also contradicted himself by claiming that Vedda was a relexified aboriginal language.
Veddas consider the Vedda language to be distinct from Sinhalese and use it as an ethnic marker to differentiate them from Sinhalese people.
Religion.
The original religion of Veddas is animism. The Sinhalized interior Veddahs follow a mix of animism and nominal Buddhism; whereas the Tamilized east coast Veddahs follow a mix of animism and nominal Hinduism due to Brahminical sanskritsation , which is known as folk Hinduism among anthropologists.
One of the most distinctive features of Vedda religion is the worship of dead ancestors, who are called "nae yaku" among the Sinhala-speaking Veddas and are invoked for game and yams. There are also peculiar deities unique to Veddas, such as "Kande Yakka".
Veddas, along with the Island's Buddhist, Hindu and Muslim communities, venerate the temple complex situated at Kataragama, showing the syncretism that has evolved over 2,000 years of coexistence and assimilation. Kataragama is supposed to be the site where the Hindu god Skanda or Murugan in Tamil met and married a local tribal girl, Valli, who in Sri Lanka is believed to have been a Vedda.
There are a number of less famous shrines across the island, which are as sacred to the Veddas and to other communities.
Rituals.
Vedda marriage is a simple ceremony. It consists of the bride tying a bark rope ("diya lanuva") that she has twisted, around the waist of the groom. This symbolizes the bride's acceptance of the man as her mate and life partner. Although endogamous marriage between cross-cousins was the norm until recently, this has changed significantly, with Vedda women even contracting marriages with their Sinhalese and Moor neighbours.
In Vedda society, women are in many respects men's equals. They are entitled to similar inheritance. Monogamy is the general rule, though a widow would frequently marry her husband's brother as a means of support and consolation (widow inheritance). They also do not practice a caste system.
Death, too, is a simple affair without ostentatious funeral ceremonies where the corpse of the deceased is promptly buried.
Although the medical knowledge of the Vedda is limited, it nevertheless appears to be sufficient. For example, pythonesa oil ("pimburu tel"), a local remedy used for healing wounds, has proven to be very successful in the treatment of fractures and deep cuts.
Burial.
Since the opening of colonisation schemes, Vedda burials changed when they dug graves of 4–5 feet deep and wrapped the body wrapped cloth and covered it with leaves and earth. The Veddas also laid the body between the scooped out trunks of the "gadumba" tree before they buried it. At the head of the grave were kept three open coconuts and a small bundle of wood, while at its foot were kept an opened coconut and an untouched coconut. Certain cactus species "(pathok)" were planted at the head, the middle and the foot. Personal possessions like the bow and arrow, betel pouch, were also buried. This practice varied by community. The contents of the betel pouch of the deceased were eaten after his death.
The dead body was scented or smeared with juice from the leaves of jungle trees or lime trees. The foot or the head of the grave was never lit either with fire or wax, and water was not kept in a vessel by the grave side."[?]"
Cult of the Dead.
The Veddas believe in the cult of the dead. They worshipped and made incantations to their "Nae Yakka" (Relative Spirit) followed by other customary ritual (called the "Kiri Koraha") which is still in vogue among the surviving Gam Veddas of Rathugala, Pollebedda Dambana and the Henanigala Vedda re-settlement (in Mahaweli systems off Mahiyangane).
They believed that the spirit of their dead would haunt them bringing forth diseases and calamity. To appease the dead spirit they invoke the blessings of the Nae Yakka and other spirits, like "Bilinda Yakka, Kande Yakka" followed by the dance ritual of the "Kiri Koraha".
"According to Sarasin Cousins (in 1886) and Seligmann's book - 'The Veddas' (1910)."
"When man or woman dies from sickness, the body is left in the cave or rock shelter where the death took place, the body is not washed or dressed or ornamented in any way, but is generally allowed to be in the natural supine position and is covered with leaves and branches. This was formerly the universal custom and still persists among the less sophisticated Veddas who sometimes in addition place a large stone upon the chest for which no reason could be given, this is observed at Sitala Wanniya (off Polle-bedda close to Maha Oya), where the body is still covered with branches and left where the death occurred."
Clothing.
Until fairly recent times, the raiment of the Veddas was remarkably scanty. In the case of men, it consisted only of a loincloth suspended with a string at the waist, while in the case of women, it was a piece of cloth that extended from the navel to the knees. Today, however, Vedda attire is more covering, men wear a short sarong extending from the waist to the knees, while the womenfolk clad themselves in a garment similar to the Sinhalese "diya-redda" which extends from the breastline to the knees.
Music.
"Bori Bori Sellam-Sellam Bedo Wannita,"
"Palletalawa Navinna-Pita Gosin Vetenne,"
"Malpivili genagene-Hele Kado Navinne,"
"Diyapivili Genagene-Thige Bo Haliskote Peni,"
"Ka tho ipal denne"
Meaning of this song - The bees from yonder hills of Palle Talawa and Kade suck nectar from the flowers and made the honeycomb. So why should you give them undue pain when there is no honey by cutting the honeycomb.
Art.
Vedda cave drawings such as those found at "Hamangala" provide graphic evidence of the sublime spiritual and artistic vision achieved by the ancestors of today's "Wanniyala-Aetto" people. Most researchers today agree that the artists most likely were the "Wanniyala-Aetto" women who spent long hours in these caves waiting for their menfolk's return from the hunt.
Understood from this perspective, these cave drawings depict brilliant feats of "Wanniyala-Aetto" culture as seen through the eyes of its womenfolk. The simple yet graceful abstract figures are portrayed engaging in feats of vision and daring that place them firmly above even the greatest beasts of their jungle habitat.
The nimbus or halo about the human figures' heads represents the sun's disc and, equally, the sacred power bordering upon divinity that accrues not only to great hunters but to all those endowed with the vision to behold and apprehend the marvel of divinity in humble guise. Even up to modern times, the Wanniyala-Aetto used to swear oaths of truth by the divinity of the sun, saying 'upon Maha Suriyo Deviyo'.
Such cave drawings have long served as visual memory aids and as teaching tools for the transmission of ancestral wisdom traditions to succeeding generations. To this day, they provide silent testimony to the profound heights attained by Lanka's indigenous culture expressed with elegant simplicity that people of all communities may appreciate.
Livelihood.
Veddas were originally hunter-gatherers. They used bows and arrows to hunt game, harpoons and toxic plants for fishing and gathered wild plants, yams, honey, fruit and nuts. Many Veddas also farm, frequently using slash and burn or swidden cultivation, which is called "chena" in Sri Lanka. East Coast Veddas also practice sea fishing. Veddas are famously known for their rich meat diet. Venison and the flesh of rabbit, turtle, tortoise, monitor lizard, wild boar and the common brown monkey are consumed with much relish. The Veddas kill only for food and do not harm young or pregnant animals. Game is commonly shared amongst the family and clan. Fish are caught by employing fish poisons such as the juice of the "pus-vel" (Entada scandens) and "daluk-kiri" (Cactus milk).
Vedda culinary fare is also deserving of mention. Amongst the best known are "gona perume", which is a sort of sausage containing alternate layers of meat and fat, and "goya-tel-perume", which is the tail of the monitor lizard (talagoya), stuffed with fat obtained from its sides and roasted in embers. Another Vedda delicacy is dried meat preserve soaked in honey. In the olden days, the Veddas used to preserve such meat in the hollow of a tree, enclosing it with clay.
Such succulent meat served as a ready food supply in times of scarcity. The early part of the year (January–February) is considered to be the season of yams and mid-year (June–July) that of fruit and honey, while hunting is availed of throughout the year. Nowadays, more and more Vedda folk have taken to "Chena" (slash and burn) cultivation. "Kurakkan" Eleusine coracana is cultivated very often. Maize, yams, gourds and melons are also cultivated. In the olden days, the dwellings of the Veddas consisted of caves and rock shelters. Today, they live in unpretentious huts of wattle, daub and thatch.
In the reign of King Datusena (6th century CE) the Mahaweli ganga was diverted at Minipe in the Minipe canal nearly 47 miles long said to be constructed with help from the Yakkas. The Mahawamsa refers to the canal as Yaka-bendi-ela. When the Ruwanweli Seya was built in King Dutugemunu's time (2nd century BCE) the Veddas procured the necessary minerals from the jungles.
King Parakrama Bahu the great (12th century) in his war against the rebels employed these Veddas as scouts.
In the reign of King Rajasinghe II (17th century) in his battle with the Dutch he had a Vedda regiment. In the abortive Uva-Welessa revolt of 1817-1818 of the British times, led by Keppetipola Disawe, the Veddas too fought with the rebels against the British forces.
Genetics.
A recent genetic study has found Vedda people to probably be earliest inhabitants of Sri Lanka. The Vedda people’s mitochondrial sequences were found to be more related to the Sinhalese and Sri Lankan Tamils than to the Indian Tamils.
According to Ranaweera et al (2013) : ""From the phylogenetic, principal coordinate and analysis of molecular variance results, the Vedda occupied a position separated from all other ethnic people of the island, who formed relatively close affiliations among themselves, suggesting a separate origin of the former. The haplotypes and analysis of molecular variance revealed that Vedda people’s mitochondrial sequences are more related to the Sinhalese and Sri Lankan Tamils’ than the Indian Tamils’ sequences."" 
""It has been hypothesized that the Vedda was probably the earliest inhabitants of the area ... dated tentatively to 37 000 YBP, were discovered from the cave site, Fahien-lena,8 on the island, with their association with the present-day Vedda people proposed on a comparative anatomical ground ... Vedda population has the lowest proportion of shared haplotypes among their subgroups (63%) indicating their greater genetic diversity among subgroups ... Vedda people had the lowest frequency of haplogroup M (17.33%). It is quite astonishing to see such a lower frequency of M haplogroup in the Vedda population ... This is probably due to the effect of genetic drift in the smaller population of Vedda ... Vedda people ... showed relatively high frequencies of haplogroup R (45.33 ... Haplogroup U was mostly found in Vedda (29.33%) ... Low frequency of M haplogroup and high frequencies of R and U haplogroups were found to be the unique characteristics of Vedda ... All the island populations, except some subgroups of the Vedda, form close genetic affiliations among themselves and with majority of the groups from the mainland suggesting the origin of the majority of the island population on the Indian mainland."" 
Mitochondrial DNA.
Mitochondrial DNA frequencies in Vedda population.
Current status.
Some observers have said Veddas are disappearing and have lamented the decline of their distinct culture. Land acquisition for mass irrigation projects, government forest reserve restrictions and the civil war have disrupted traditional Vedda ways of life. Between 1977 and 1983 under the Accelerated Mahaweli Development Project and colonization schemes, approximately 51468 hectares were turned in to a gigantic hydroelectric cum irrigation project. Subsequently, the creation of the Maduru Oya National Park deprived the Veddhas their last hunting grounds. In 1985, the Veddha Chief Thissahamy and his delegation were obstructed from attending the United Nations Working Group on Indigenous Populations. Dr. Wiveca Stegeborn, an anthropologist, has been studying the Vedda since 1977 and alleges that their young women are being tricked into accepting contracts to the Middle East as domestic workers when in fact they will be trafficked into prostitution or sold as sex slaves.
However, cultural assimilation of Veddas with other local populations has been going on for a long time. "Vedda" has been used in Sri Lanka to mean not only hunter-gatherers, but also to refer to any people who adopt an unsettled and rural way of life and thus can be a derogatory term not based on ethnic group. Thus, over time, it is possible for non-Vedda groups to become Veddas, in this broad cultural sense. Vedda populations of this kind are increasing in some districts.
Further reading.
"The Cambridge Encyclopedia of Hunters and Gatherers", editor Richard B. Lee.
External links.
A great deal of information on them can be found at Vedda.org

</doc>
<doc id="34049" url="https://en.wikipedia.org/wiki?curid=34049" title="Wart">
Wart

A wart is a small, rough growth resembling a cauliflower or a solid blister. It typically occurs on humans' hands or feet but often in other locations. Warts are caused by a viral infection, specifically by one of the many types of human papillomavirus (HPV). There are as many as 10 varieties of warts, the most common considered to be mostly harmless. It is possible to get warts from others; they are contagious and usually enter the body in an area of broken skin. Warts are usually asymptomatic except when they occur on weight-bearing areas. 
Types.
A range of types of wart have been identified, varying in shape and site affected, as well as the type of human papillomavirus involved. These include:
Cause.
Warts are caused by the human papilloma virus (HPV). There are about 130 known types of human papilloma viruses. HPV infects the squamous epithelium, usually of the skin or genitals, but each HPV type is typically only able to infect a few specific areas on the body. Many HPV types can produce a benign growth, often called a "wart" or "papilloma", in the area they infect. Many of the more common HPV and wart types are listed below.
Pathophysiology.
Common warts have a characteristic appearance under the microscope. They have thickening of the stratum corneum (hyperkeratosis), thickening of the stratum spinosum (acanthosis), thickening of the stratum granulosum, rete ridge elongation, and large blood vessels at the dermoepidermal junction.
Prevention.
Gardasil is an HPV vaccine aimed at preventing cervical cancers and genital warts. Gardasil is designed to prevent infection with HPV types 16, 18, 6, and 11. HPV types 16 and 18 currently cause about 70% of cervical cancer cases, and also cause some vulvar, vaginal, penile and anal cancers. HPV types 6 and 11 are responsible for 90% of documented cases of genital warts. Unfortunately the HPV vaccines do not currently prevent the virus strain responsible for verrucas (plantar warts).
Disinfection.
The virus is relatively hardy and immune to many common disinfectants. Exposure to 90% ethanol for at least 1 minute, 2% glutaraldehyde, 30% Savlon, and/or 1% sodium hypochlorite can disinfect the pathogen.
The virus is resistant to drying and heat, but killed by and ultraviolet radiation.
Treatment.
There are many treatments and procedures associated with wart removal. A review of clinical trials of various cutaneous wart treatments concluded that topical treatments containing salicylic acid were more effective than placebo. Cryotherapy appears to be as effective as salicylic acid, but there have been fewer trials.
Medication.
Another product available over-the-counter that can aid in wart removal is silver nitrate in the form of a caustic pencil, which is also available at drug stores. In a placebo-controlled study of 70 patients, silver nitrate given over nine days resulted in clearance of all warts in 43% and improvement in warts in 26% one month after treatment compared to 11% and 14%, respectively, in the placebo group. The instructions must be followed to minimize staining of skin and clothing. Occasionally pigmented scars may develop.
Cryosurgery or cryotherapy devices using a dimethyl ether – propane mixture are inexpensive. A disadvantage is that the sponge applicator is too large for small warts, and the temperature achieved is not nearly as low as with liquid nitrogen. Complications include blistering of normal skin if excess freezing is not controlled.
Several randomized, controlled trials have found that zinc sulfate, consumed orally, often reduces or eliminates warts. The zinc sulfate dosage used in medical trials for treatment of warts was between 5 and 10 mg/kg/day. For elemental zinc, a lower dosage of 2.5 mg/kg/day may be appropriate as large amounts of zinc may cause a copper deficiency. Other trials have found that topical zinc sulfate solution or zinc oxide are also effective.
A 2014 study indicates that lopinavir is effective against the human papilloma virus (HPV). The study used the equivalent of one tablet twice a day applied topically to the cervices of women with high-grade and low-grade precancerous conditions. After three months of treatment, 82.6% of the women who had high-grade disease had normal cervical conditions, confirmed by smears and biopsies.
Society and culture.
A variety of traditional folk remedies and rituals claim to be able to remove warts.
The acrid yellow sap of Greater Celandine is used as a traditional wart remedy. The sap can be applied directly to the wart in a similar manner to concentrated salicylic acid solution, but in more modest quantities.
In "The Adventures of Tom Sawyer", Mark Twain has his characters discuss a variety of such remedies. Tom Sawyer proposes "spunk-water" (or "stump-water", the water collecting in the hollow of a tree stump) as a remedy for warts on the hand. You put your hand into the water at midnight and say:
You then "walk away quick, eleven steps, with your eyes shut, and then turn around three times and walk home without speaking to anybody. Because if you speak the charm's busted." This is held to be superior to Huckleberry Finn's preferred remedy which involved throwing a dead cat into a graveyard. Another remedy involved splitting a bean, drawing blood from the wart and putting it on one of the halves, and burying that half at a crossroads at midnight. The theory of operation is that the blood on the buried bean will draw away the wart. Twain is recognized as an early collector and recorder of genuine American folklore.
Similar practices are recorded elsewhere. In Louisiana, one remedy for warts involves rubbing the wart with a potato, which is then buried; when the "buried potato dries up, the wart will be cured". Another remedy similar to Twain's is reported from Northern Ireland, where water from a specific well on Rathlin Island is credited with the power to cure warts.
A longstanding tradition holds that touching toads will cause warts. The most common Northern Hemisphere toads have glands that protrude from their skin that superficially resemble warts. Warts are caused by a virus, and toads do not harbor it.

</doc>
<doc id="34050" url="https://en.wikipedia.org/wiki?curid=34050" title="Warrant officer">
Warrant officer

A warrant officer (WO) is an officer in a military organisation who is designated an officer by a warrant, as distinguished from a commissioned officer who is designated an officer by a commission, and a non-commissioned officer who is designated an officer, often by virtue of seniority.
The rank was first used in the (then) English Royal Navy and is today used in most services in many countries, including the Commonwealth nations and the United States.
Outside the United States, warrant officers are included in the "Other Ranks" (OR) category, equivalent to the US "E" (Enlisted) category and rank between non-commissioned officers and commissioned officers. The warrant officers in Commonwealth navies rank between chief petty officer and sub-lieutenant, in Commonwealth air forces between flight sergeant and pilot officer, and in Commonwealth armies between staff sergeant and second-lieutenant.
Warrant officers in the United States are classified as officers and are in the "W" category (NATO "WO"); they are technical leaders and specialists. Chief warrant officers are commissioned by the President of the United States and take the same oath as regular commissioned officers. They may be technical experts with a long service as enlisted personnel, or direct entrants, notably for U.S. Army helicopter pilots.
Origins in the Royal Navy.
The warrant officer corps began in the 13th century in the nascent English Royal Navy. At that time, noblemen with military experience took command of the new Navy, adopting the military ranks of lieutenant and captain. These officers often had no knowledge of life on board a ship—let alone how to navigate such a vessel—and relied on the expertise of the ship's master and other seamen who tended to the technical aspects of running the ship. As cannon came into use, the officers also required gunnery experts; specialist gunners began to appear in the 16th century and also had warrant officer status. Literacy was one thing that most warrant officers had in common, and this distinguished them from the common seamen: according to the Admiralty regulations, "no person shall be appointed to any station in which he is to have charge of stores, unless he can read and write, and is sufficiently skilled in arithmetic to keep an account of them correctly". Since all warrant officers had responsibility for stores, this was enough to debar the illiterate.
Rank and status in the eighteenth century.
In origin, warrant officers were specialist professionals whose expertise and authority demanded formal recognition. In the 18th century they fell into two clear categories: on the one hand, those privileged to share with the commissioned officers in the wardroom and on the quarterdeck; and on the other, those who ranked with more junior members of the ship's crew. Somewhere between the two, however, were the standing officers; notable because, unlike the rest of the ship's company, they remained with the ship even when it was out of commission (e.g. for repair, refitting or replenishment, or whilst laid up); in these circumstances they were under the pay and supervision of the Royal Dockyard.
Wardroom warrant officers.
These classes of warrant officer messed in the wardroom with the commissioned officers:
In the early 19th century, they were joined in the wardroom by naval chaplains, who also had warrant officer status (though they were only usually present on larger vessels).
Standing warrant officers.
The standing officers were:
Junior warrant officers.
Other warrant officers included surgeon's mates, boatswain's mates and carpenter's mates, sailmakers, armourers, schoolmasters (involved in the education of boys, midshipmen and others aboard ship) and clerks. Masters-at-arms, who had formerly overseen small-arms provision on board, had by this time taken on responsibility for discipline.
Warrant officers in context.
By the end of the century, the rank structure could be illustrated as follows (the warrant officers are underlined):
Demise of the Royal Naval warrants.
In 1843, the wardroom warrant officers were given commissioned status, while in 1853 the lower-grade warrant officers were absorbed into the new rate of chief petty officer, both classes thereby ceasing to be warrant officers. On 25 July 1864 the standing warrant officers were divided into two grades: warrant officers and chief warrant officers (or "commissioned warrant officers", a phrase that was replaced in 1920 with "commissioned officers promoted from warrant rank", although they were still usually referred to as "commissioned warrant officers", even in official documents).
By the time of the First World War, their ranks had been expanded with the adoption of modern technology in the Navy to include telegraphists, electricians, shipwrights, artificer engineers, etc. Both warrant officers and commissioned warrant officers messed in the warrant officers' mess rather than the wardroom (although in ships too small to have a warrant officers' mess, they did mess in the wardroom). Warrant officers and commissioned warrant officers also carried swords, were saluted by ratings, and ranked between sub-lieutenants and midshipmen.
In 1949, the ranks of warrant officer and commissioned warrant officer were changed to "commissioned officer" and "senior commissioned officer", the latter ranking with but after the rank of lieutenant, and they were admitted to the wardroom, the warrant officers' messes closing down. Collectively, these officers were known as "branch officers", being retitled "special duties" officers in 1956. In 1998, the special duties list was merged with the general list of officers in the Royal Navy, all officers now having the same opportunity to reach the highest commissioned ranks.
Modern usage.
Australia.
The Australian Army has three warrant officer ranks: WO, WO1 and WO2. The most senior army warrant officer is the soldier appointed Regimental Sergeant Major of the Army (RSM-A). The RSM-A is the only holder of the unique rank of warrant officer (WO), introduced in 1991. Substantive RSMs of regiments and battalions hold the rank warrant officer class one (WO1). Distinct from RSMs, WO1 is also reserved for senior soldiers of technical trades; such as Artificer Sergeant Major (ASM). The third rank is warrant officer class two (WO2). The insignia of the three ranks are: a crown for a WO2; the (Australian) Commonwealth coat of arms (changed from the Royal coat of arms in 1976) for a WO1; and the Commonwealth coat of arms surrounded by a laurel wreath for the warrant officer.
The Royal Australian Navy rank of warrant officer (WO) is the navy's only rank appointed by warrant and is equivalent to the army's WO1 (the equivalent of the army's WO2 rank is a chief petty officer). The most senior non-commissioned member of the navy is the warrant officer appointed Warrant Officer of the Navy (WO-N).
The Royal Australian Air Force rank of warrant officer (WOFF) is the air force's only rank appointed by warrant and is equivalent to both the army's WO1 and the navy's WO (the equivalent of the army's WO2 is a flight sergeant). The most senior non-commissioned member is the warrant officer appointed Warrant Officer of the Air Force (WOFF-AF).
France.
The ranks of adjudant (premier maître in the navy), adjudant-chef (maître-principal in the navy), and major may be considered equivalent to Commonwealth warrant officer ranks.
Indonesia.
In the Indonesian Armed Forces, there are two warrant officer ranks known as "pembantu letnan" (assistant lieutenant). These are warrant officer 2nd class ("pelda") and warrant officer 1st class ("peltu").
Israel Defense Forces.
The רב-נגד משנה "rav nagad mishne" ("warrant officer") and the רב-נגד "rav nagad" ("chief warrant officer") are both non-commissioned officers ranks in the Israel Defense Forces (IDF). Because the IDF is an integrated force, they have a unique rank structure. Israel Defense Forces ranks are the same in all services (army, navy, air force, etc.). The ranks are derived from those of the paramilitary "Haganah" developed in the British Mandate of Palestine period to protect the "Yishuv". This origin is reflected in the slightly-compacted IDF rank structure.
New Zealand.
The New Zealand Army is similar to the Australian Army, except that it has two warrant officer ranks. The Warrant Officer Class 2 (WO2), which is addressed as "Sergeant Major" and the Warrant Officer Class 1 (WO1) which is addressed as "Sir or Ma'am". There are also appointments such as Company and Squadron sergeant major (CSM and SSM) which are usually WO2 positions and Regimental sergeant major (RSM), which are usually WO1 positions. The highest ranking WO1 holds the position of Sergeant Major of the Army (SMA) and is addressed as "sir or Ma'am". In certain uniforms, WO2's wear black shoes, the same as the enlisted ranks whilst WO1s wear brown shoes, the same as commissioned officers. The exception to this are WO1s of the Royal New Zealand Armoured Corps (RNZAC) who wear black shoes.
The Royal New Zealand Navy has a single Warrant Officer rank and is addressed as "Sir or Ma'am". The WO is equivalent to the Army WO1.
The Royal New Zealand Air Force also has a single Warrant Officer rank and is addressed as "Sir or Ma'am". The WO is equivalent to the Navy WO and the Army WO1.
Singapore.
In the Singapore Armed Forces, warrant officers begin as third warrant officers (3WO), previously starting at 2WO. This rank is given to former specialists who have attained the rank of master sergeant and have either gone through, or are about to go through the Warfighter Course at the Specialist and Warrant Officer Advanced School (SWAS) in the Specialist and Warrant Officer Institute (SWI). In order to be promoted to a second warrant officer (2WO) and above, they must have been selected for and graduated from the joint warrant officer course at the SAF Warrant Officer School. Warrant officers rank between specialists and commissioned officers. They ordinarily serve as battalion or brigade regimental sergeant majors. Many of them serve as instructors and subject-matter experts in various training establishments. Warrant officers are also seen on the various staffs headed by the respective specialist officers. There are six grades of warrant officer (3WO, 2WO, 1WO, MWO, SWO & CWO).
Warrant officers used to have their own mess. For smaller camps, this mess are combined with the officers' mess as a combined mess for better camaraderie. Warrant officers have similar responsibilities to commissioned officers. Warrant officers are usually addressed as "sir" by the other ranks or as "warrant (surname)". They are also usually addressed ""encik"" ("mister" in Malay language) by commissioned officers. It is not a necessity to be saluted by other ranks. They would usually be saluted out of respect by the enlistees.
South Africa.
In the South African National Defence Force a warrant officer is a non-commissioned officer rank. Before 2008 there were two classes - warrant officer class one and two. A warrant officer class one could be appointed to positions such as regimental sergeant major, formation sergeant major or even sergeant major of the army while still in the rank of warrant officer class one. In 2008 the warrant officer ranks were expanded so that the substantive ranks that came with senior appointments now became ranks that an individual kept after moving from that post.
United Kingdom.
Royal Navy.
In 1973, warrant officers reappeared in the Royal Navy, but these appointments followed the army model, with the new warrant officers being ratings rather than officers. They were initially known as fleet chief petty officers (FCPOs), but were renamed warrant officers in the 1980s. They rank with warrant officers class one in the British Army and Royal Marines and with warrant officers in the Royal Air Force.
There are executive warrant officers for commands and ships. Five branches (surface ships, submarines, Royal Marines, Fleet Air Arm, and Maritime Reserves) each have a command warrant officer. The senior RN WO is the Warrant Officer of the Naval Service.
From 2004, warrant officers class 2 existed in the RN, but no new appointments have been made since 2014.
British Army.
In the British Army, there are two warrant ranks, warrant officer class two (WO2) and warrant officer class one (WO1), the latter being the senior of the two. It used to be more common to refer to these ranks as WOII and WOI (using Roman instead of Arabic numerals). "Warrant officer first class" or "second class" is incorrect. The rank immediately below WO2 is staff sergeant (or colour sergeant). From 1938 to 1940 there was a WO III platoon sergeant major rank.
Starting March 2015, the new appointment "Sergeant Major of the Army" or Army Sergeant Major for the British Army due to the reforms of the British Army was created, earlier in the year the appointments for Command sergeant majors commenced.
Royal Marines.
Before 1879, the Royal Marines had no warrant officers: by the end of 1881, the Royal Marines had given warrant rank to their sergeant-majors and some other senior non-commissioned officers, in a similar fashion to the army. When the army introduced the ranks of warrant officer class I and class II in 1915, the Royal Marines did the same shortly after. From February 1920, Royal Marines warrant officers class I were given the same status as Royal Navy warrant officers and the rank of warrant officer class II was abolished in the Royal Marines, with no further promotions to this rank.
The marines had introduced warrant officers equivalent in status to the Royal Navy's from 1910 with the Royal Marines gunner (originally titled gunnery sergeant-major), equivalent to the navy's warrant rank of gunner. Development of these ranks closely paralleled that of their naval counterparts: as in the Royal Navy, by the Second World War there were warrant officers and commissioned warrant officers (e.g. staff sergeant majors), commissioned staff sergeant majors, Royal Marines gunners, commissioned Royal Marines gunners, etc. As officers they were saluted by junior ranks in the Royal Marines and the army. These all became (commissioned) branch officer ranks in 1949, and special duties officer ranks in 1956. These ranks would return in 1972, this time similar to their army counterparts, and not as the RN did before. The most senior Royal Marine warrant officer is the Corps Regimental Sergeant Major. Unlike the RN proper (since 2014), it retains both WO ranks.
Royal Air Force.
The Royal Air Force first used the ranks of warrant officer class I and II as inherited from the Royal Flying Corps. It first used the rank badges of the Royal coat of arms for WOI and the crown for WOII. Until the 1930s, these ranks were often known as sergeant major first and second class. In 1939, the RAF abolished the rank of WOII and retained just the WOI rank, referred to as just warrant officer (WO), which it remains to this day. The RAF has no equivalent to WO2 (NATO OR-8), an RAF WO being equivalent to WO1 (NATO OR-9) and wearing the same badge of rank, the Royal coat of arms. The correct way to address a warrant officer is "sir" or "ma'am" by airmen and "mister or warrant officer -surname-" by officers. Most RAF warrant officers do not hold appointments as in the army or Royal Marines; the exception to this is the station warrant officer, who is considered a "first amongst equals" on an RAF station. Warrant officer is the highest non-commissioned rank and ranks above flight sergeant.
In 1946, the RAF renamed its aircrew warrant officers to master aircrew, a designation which still survives. In 1950, it renamed warrant officers in technical trades to master technicians, a designation which survived only until 1964.
The most senior RAF warrant officer by appointment is the Chief of the Air Staff's Warrant Officer. He holds the same rank as all other warrant officers.
United States.
In the United States military, a warrant officer (grade W-1 to W-5) is ranked as an officer above the senior-most enlisted ranks, as well as officer cadets and officer candidates, but below the officer grade of O-1 ("NATO: OF-1"). Warrant officers are highly skilled, single-track specialty officers, and while the ranks are authorized by Congress, each branch of the U.S. Armed Forces selects, manages, and utilizes warrant officers in slightly different ways. For appointment to warrant officer (W-1), a warrant is approved by the Service Secretary of the respective branch of service (Secretary of the Army, or Secretary of the Navy for USMC warrant officers), while chief warrant officers (W-2 to W-5) are commissioned by the President of the United States, both warrant officers and chief warrant officers take the same Oath of Office as regular commissioned officers (O-1 to O-10).
Warrant officers command detachments, units, activities, vessels, aircraft, and armored vehicles as well as lead, coach, train, and counsel subordinates. However, the warrant officer's primary task as a leader is to serve as a technical expert, providing valuable skills, guidance, and expertise to commanders and organizations in their particular field.
All U.S. armed services employ warrant officer grades except the U.S. Air Force. Although still technically authorized, the air force discontinued appointing new warrant officers in 1959, retiring its last chief warrant officer from the Air Force Reserve in 1992.
The U.S. Army utilizes warrant officers heavily and separates them into two types: technical and aviation. Army Aviation warrant officers are Army Aviators (aircraft pilots - helicopter and airplane). Technical warrant officers in the army are specialized in a single track technical area such as intelligence, maintenance, or military police; and provide direct advice and support to commanders. For example, a military police officer and a military intelligence officer both have to be branch qualified in their respective fields; learning how to manage the entire spectrum of their profession. However, within those broad fields you can also have warrant officers such as CID Special Agents (a very specific track within the military police) and Counterintelligence Special Agents (a very specific track within military intelligence). These technical warrant officers allow for a Soldier with subject matter expertise (like non-commissioned officers); but with the authority of a full commissioned officer. Both technical and aviation warrants go through similar training initially at warrant officer candidate school (WOCS); but then go their separate ways. Technical warrant officers in the army must be selected from the non-commissioned officer ranks (typically E-6 through E-9); however, the higher the rank, the easier it is to pass the board and get accepted into the warrant officer program. Aviation warrant officers are able to apply before becoming non-commissioned officers.
The U.S. Navy and U.S. Coast Guard also discontinued the grade of W-1 in 1975, appointing and commissioning all new entrants as chief warrant officer two (pay grade W-2, with rank abbreviation of CWO2). Unlike the Army, all Navy chief warrant officers are selected strictly from the chief petty officer pay grades (E-7 through E-9). The Coast Guard allows E-6 personnel to apply for chief warrant officer rank, but only after they have displayed their technical ability by earning a placement in the top 50 percent on the annual eligibility list for advancement to E-7.
The U.S. Maritime Service, which is established at 46 U.S. Code § 51701, falls under the authority of the Maritime Administration of the Department of Transportation and is authorized to appoint warrant officers. In accordance with 46 U.S. Code § 51701, the USMS rank structure must be the same as that of the U.S. Coast Guard while uniforms worn are those of the U.S. Navy with distinctive USMS insignia and devices. The USMS has appointed warrant officers, of various specialty fields, during and after World War II.
Warrant officer rank is also occasionally used in law enforcement agencies to grant status and pay to certain senior specialist officers who are not in command, such as senior technicians or helicopter pilots. As in the armed forces, they rank above sergeants, but below lieutenants. For example, the North Carolina State Highway Patrol had several warrant officer helicopter pilot positions from the 1960s until the mid-1980s. The WO insignia was a silver bar with a black square in the center. The WO ranks were abolished when the aviation program expanded and nearly twenty trooper pilot positions were created. The New York State Police rank of technical lieutenant is similar to a warrant officer rank insofar as it is used to grant commissioned officer authority to non-commissioned officers with extensive technical expertise.

</doc>
<doc id="34051" url="https://en.wikipedia.org/wiki?curid=34051" title="Walter Gilbert">
Walter Gilbert

Walter Gilbert (born March 21, 1932) is an American biochemist, physicist, molecular biology pioneer, and Nobel laureate.
Education and early life.
Walter Gilbert was born in Boston, Massachusetts, on March 21, 1932, the son of Emma (Cohen), a child psychologist, and Richard V. Gilbert, an economist. He was educated at the Sidwell Friends School, and attended Harvard University for undergraduate and graduate studies, earning a baccalaureate in chemistry and physics in 1953 and a master's degree in physics in 1954. He studied for his doctorate at the University of Cambridge, where he earned a Ph.D in Physics supervised by the Nobel laureate Abdus Salam in 1957.
Career and research.
Gilbert returned to Harvard in 1956 and was appointed assistant professor of physics in 1959. Gilbert's wife Celia had begun working for James Watson, and this led to Gilbert becoming interested in problems in molecular biology. Watson and Gilbert would run their laboratory jointly through most of the 1960s, until Watson left for Cold Spring Harbor Laboratory. In 1964 he was promoted to associate professor of biophysics and promoted again in 1968 to professor of biochemistry.
He is a co-founder of the biotech start-up companies Biogen and Myriad Genetics, and was the first chairman on their respective boards of directors. Gilbert left his position at Harvard to run Biogen as CEO, but was later asked to resign by the company's board of directors. He is also a member of the Board of Scientific Governors at The Scripps Research Institute. Gilbert has served as the chairman of the Harvard Society of Fellows.
In 1996, Gilbert co-founded Paratek Pharmaceuticals Inc. along with Dr. Stuart B. Levy. Gilbert served as Chairman until 2014.
Gilbert was an early proponent of sequencing the human genome. At a March 1986 meeting in Santa Fe New Mexico he proclaimed "The total human sequence is the grail of human genetics". In 1987, he proposed starting a company called Genome Corporation to sequence the genome and sell access to the information. In an opinion piece in Nature in 1991, he envisioned completion of the human genome sequence transforming biology into a field in which computer databases would be as essential as laboratory reagents
Gilbert returned to Harvard in 1985. Gilbert was an outspoken critic of David Baltimore in the handling of the scientific fraud accusations against Thereza Imanishi-Kari. Gilbert also joined the early controversy over the cause of AIDS, though he later stated he was satisfied with the evidence that disease is caused by HIV.
In 1962, Gilbert's Ph.D. student in physics Gerald Guralnik extended Gilbert's work on massless particles; Guralnik's work on is widely recognized as an important thread in the discovery of the Higgs Boson.
With his Ph.D. student Benno Müller-Hill, Gilbert was the first to purify the lac repressor, just beating out Mark Ptashne for purifying the first gene regulatory protein.
Together with Allan Maxam, Gilbert developed a new DNA sequencing method using chemical methods developed by Andrei Mirzabekov. His approach to the first synthesis of insulin via recombinant DNA lost out to Genentech's approach which used genes built up from the nucleotides rather than from natural sources. Gilbert's effort was hampered by a temporary moratorium on recombinant DNA work in Cambridge, Massachusetts, forcing his group to move their work to an English biological weapons site.
Gilbert first proposed the existence of introns and exons and explained the evolution of introns in a seminal 1978 "News and Views" paper published in "Nature". In 1986, Gilbert proposed the RNA world hypothesis for the origin of life, based on a concept first proposed by Carl Woese in 1967.
Awards and honors.
In 1969, Gilbert was awarded Harvard's Ledlie Prize. In 1972 he was named American Cancer Society Professor of Molecular Biology. In 1979, Gilbert was awarded the Louisa Gross Horwitz Prize from Columbia University together with Frederick Sanger. That year he was also awarded the Gairdner Prize and the Albert Lasker Award for Basic Medical Research.
Gilbert was awarded the 1980 Nobel Prize in Chemistry, shared with Frederick Sanger and Paul Berg. Gilbert and Sanger were recognized for their pioneering work in devising methods for determining the sequence of nucleotides in a nucleic acid.
Gilbert has also been honored by the National Academy of Sciences (US Steel Foundation Award, 1968); Massachusetts General Hospital (Warren Triennial Prize, 1977); the New York Academy of Sciences; (Louis and Bert Freedman Foundation Award, 1977), the Academie des Sciences of France (Prix Charles-Leopold Mayer Award, 1977). Gilbert was elected a Foreign Member of the Royal Society (ForMemRS) in 1987.
In 2002, he received the Biotechnology Heritage Award.
Personal life.
Gilbert married Celia Stone in 1953 and has two children. After retiring from Harvard in 2001, Gilbert has launched an artistic career centered around digital photography.

</doc>
<doc id="34052" url="https://en.wikipedia.org/wiki?curid=34052" title="Warner Bros.">
Warner Bros.

Warner Bros. Entertainment Inc. (commonly known as Warner Bros., Warners, or simply WB) is an American company that produces film, television and music entertainment. As one of the major film studios, it is a division of Time Warner, with its headquarters in Burbank, California. Warner Bros. has several subsidiary companies, including Warner Bros. Pictures, Turner Entertainment Co., Warner Bros. Interactive Entertainment, Warner Bros. Television, Warner Bros. Animation, Warner Home Video, New Line Cinema, Castle Rock Entertainment, and DC Entertainment. Warner Bros. owns half of The CW Television Network.
History.
Founding.
The company's name originates from the four founding Warner brothers (born "Wonskolaser" or "Wonsal" before Anglicization): Harry, Albert, Sam, and Jack Warner. They emigrated as small children with their parents to Canada from Krasnosielc which was located in the part of Congress Poland that had been subjugated to the Russian Empire following the eighteenth-century Partitions of Poland near present-day Ostrołęka.
Jack, the youngest, was born in London, Ontario. The three elder brothers began in the movie theater business, having acquired a movie projector with which they showed films in the mining towns of Pennsylvania and Ohio. In the beginning, Sam and Albert Warner invested $150 to present "Life of an American Fireman" and "The Great Train Robbery". They opened their first theater, the Cascade, in New Castle, Pennsylvania in 1903.
When the original building was in danger of being demolished, the modern Warner Bros. called the current building owners, and arranged to save it. The owners noted people across the country had asked them to protect it for its historical significance.
In 1904, the Warners founded the Pittsburgh-based Duquesne Amusement & Supply Company, to distribute films. In 1912, Harry Warner hired an auditor named Paul Ashley Chase. By the time of World War I they had begun producing films. In 1918 they opened Warner Bros. studio on Sunset Boulevard in Hollywood. Sam and Jack produced the pictures, while Harry and Albert, along with their auditor and now controller Chase, handled finance and distribution in New York City. During World War I their first nationally syndicated film, "My Four Years in Germany," based on a popular book by former ambassador James W. Gerard, was released. On April 4, 1923, with help from money loaned to Harry by his banker Motley Flint, they formally incorporated as Warner Brothers Pictures, Incorporated. (As late as the 1960s, Warner Bros. claimed 1905 as its founding date.)
The first important deal was the acquisition of the rights to Avery Hopwood's 1919 Broadway play, "The Gold Diggers", from theatrical impresario David Belasco. However, Rin Tin Tin, a dog brought from France after WWI by an American soldier, established their reputation. Rin Tin Tin debuted in the feature "Where the North Begins". The movie was so successful that Jack signed the dog to star in more films for $1,000 per week. Rin Tin Tin became the studio's top star. Jack nicknamed him "The Mortgage Lifter" and the success boosted Darryl F. Zanuck's career. Zanuck eventually became a top producer and between 1928 and 1933 served as Jack's right-hand man and executive producer, with responsibilities including day-to-day film production. More success came after Ernst Lubitsch was hired as head director; Harry Rapf left the studio to join Metro-Goldwyn-Mayer. Lubitsch's film "The Marriage Circle" was the studio's most successful film of 1924, and was on "The New York Times" best list for that year.
Despite the success of Rin Tin Tin and Lubitsch, Warner's remained a lesser studio. Sam and Jack decided to offer Broadway actor John Barrymore the lead role in "Beau Brummel". The film was so successful that Harry signed Barrymore to a long-term contract; like "The Marriage Circle", "Beau Brummell" was named one of the ten best films of the year by the "Times". By the end of 1924, Warner Bros. was arguably Hollywood's most successful independent studio, where it competed with "The Big Three" Studios (First National, Paramount Pictures, and MGM). As a result, Harry Warner—while speaking at a convention of 1,500 independent exhibitors in Milwaukee, Wisconsin—was able to convince the filmmakers to spend $500,000 in newspaper advertising, and Harry saw this as an opportunity to establish theaters in cities such as New York and Los Angeles.
As the studio prospered, it gained backing from Wall Street, and in 1924 Goldman Sachs arranged a major loan. With this new money, the Warners bought the pioneer Vitagraph Company which had a nationwide distribution system. In 1925, Warners also experimented in radio, establishing a successful radio station, KFWB, in Los Angeles.
1925–1935: Sound, color, style.
Warner Bros. was a pioneer of films with synchronized sound (then known as "talking pictures" or "talkies"). In 1925, at Sam's urging, Warner's agreed to add this feature to their productions. By February 1926, the studio reported a net loss of $333,413.
After a long period denying Sam's request for sound, Harry agreed to change, as long as the studio's use of synchronized sound was for background music purposes only. The Warners signed a contract with the sound engineer company Western Electric and established Vitaphone. In 1926, Vitaphone began making films with music and effects tracks, most notably, in the feature "Don Juan" starring John Barrymore. The film was silent, but it featured a large number of Vitaphone shorts at the beginning. To hype "Don Juan"'s release, Harry acquired the large Piccadilly Theater in Manhattan, New York City, and renamed it Warners' Theatre.
"Don Juan" premiered at the Warners' Theatre in New York on August 6, 1926. Throughout the early history of film distribution, theater owners hired orchestras to attend film showings, where they provided soundtracks. Through Vitaphone, Warner Bros. produced eight shorts (which aired at the beginning of every showing of "Don Juan" across the country) in 1926. Many film production companies questioned the necessity. "Don Juan" did not recoup its production cost and Lubitsch left for MGM. By April 1927, the Big Five studios (First National, Paramount, MGM, Universal, and Producers Distributing) had ruined Warner's, and Western Electric renewed Warner's Vitaphone contract with terms that allowed other film companies to test sound.
As a result of their financial problems, Warner Bros. took the next step and released "The Jazz Singer" starring Al Jolson. This movie, which has very little sound dialogue but includes sound segments of Jolson singing, was a sensation. It signaled the beginning of the era of "talking pictures" and the twilight of the silent era. However, Sam died the night before the opening, preventing the brothers from attending the premiere. Jack became sole head of production. Sam's death also had a great effect on Jack's emotional state, as Sam was arguably Jack's inspiration and favorite brother. In the years to come, Jack kept the studio under tight control. Firing employees was common. Among those whom Jack fired were Rin Tin Tin (in 1929) and Douglas Fairbanks, Jr.—who had served as First National's top star since the brothers acquired the studio in 1928—in 1933.
Thanks to the success of "The Jazz Singer", the studio was cash-rich. Jolson's next film for the company, "The Singing Fool" was also a success. With the success of these first talkies ("The Jazz Singer", "Lights of New York", "The Singing Fool" and "The Terror"), Warner Bros. became a top studio and the brothers were now able to move out from the Poverty Row section of Hollywood and acquire a big facility in Burbank, California. They expanded by acquiring the Stanley Corporation, a major theater chain. This gave them a share in rival First National Pictures, of which Stanley owned one-third. In a bidding war with William Fox, Warner Bros. bought more First National shares on September 13, 1928; Jack also appointed Zanuck as the manager of First National Pictures.
In 1928, Warner Bros. released "Lights of New York", the first all-talking feature. Due to its success, the movie industry converted entirely to sound almost overnight. By the end of 1929, all the major studios were exclusively making sound films. In 1929, National Pictures released their first film with Warner Bros., "Noah's Ark". Despite its expensive budget, "Noah's Ark" was profitable. In 1929, Warner Bros. released "On with the Show", the first all-color all-talking feature. This was followed by "Gold Diggers of Broadway" which was so popular it played in theatres until 1939. The success of these two color pictures caused a color revolution (just as the first all-talkie had created one for talkies). Warner Bros. color films from 1929 to 1931 included "The Show of Shows" (1929), "Sally" (1929), "Bright Lights" (1930), "Golden Dawn" (1930), "Hold Everything" (1930), "Song of the Flame" (1930), "Song of the West" (1930), "The Life of the Party" (1930), "Sweet Kitty Bellairs" (1930), "Under A Texas Moon" (1930), "Bride of the Regiment" (1930), "Viennese Nights" (1931), "Woman Hungry" (1931), "Kiss Me Again" (1931), "Fifty Million Frenchmen" (1931) and "Manhattan Parade" (1932). In addition to these, scores of features were released with Technicolor sequences, as well as numerous Technicolor Specials short subjects. The majority of these color films were musicals.
In 1929, Warner Bros bought the St. Louis-based theater chain Skouras Brothers. Following this take-over, Spyros Skouras, the driving force of the chain, became general manager of the Warner Brothers Theater Circuit in America. He worked successfully in that post for two years and turned its losses into profits. Harry produced an adaptation of a Cole Porter musical titled "Fifty Million Frenchmen". Through First National, the studio's profit increased substantially. After the success of the studio's 1929 First National film "Noah's Ark", Harry agreed to make Michael Curtiz a major director at the Burbank studio. Mort Blumenstock, a First National screenwriter, became a top writer at the brothers' New York headquarters. In the third quarter, Warner Bros. gained complete control of First National, when Harry purchased the company's remaining one-third share from Fox. The Justice Department agreed to allow the purchase if First National was maintained as a separate company. When the Great Depression hit, Warner asked for and got permission to merge the two studios. Soon afterward Warner Bros. moved to the First National lot in Burbank. Though the companies merged, the Justice Department required Warner to release a few films each year under the First National name until 1938. For thirty years, certain Warner productions were identified (mainly for tax purposes) as 'A Warner Bros.–First National Picture.'
In the latter part of 1929, Jack Warner hired George Arliss to star in "Disraeli", which was a success. Arliss won an Academy Award for Best Actor and went on to star in nine more movies for the studio. In 1930, Harry acquired more theaters in Atlantic City, despite the beginning of the Great Depression. In July 1930, the studio's banker, Motley Flint, was murdered by a disgruntled investor in another company.
Harry acquired a string of music publishers to form Warner Bros. Music. In April 1930, Warner Bros. acquired Brunswick Records. Harry obtained radio companies, foreign sound patents and a lithograph company. After establishing Warner Bros. Music, Harry appointed his son, Lewis, to manage the company.
By 1931, the studio began to feel the effects of the Depression as the public could no longer afford the tickets. The studio reportedly lost $8 million, and an additional $14 million the following year. In 1931, Warner Bros. Music head Lewis Warner died from an infected wisdom tooth.
Around that time, Zanuck hired screenwriter Wilson Mizner. While at the studio, Mizner had hardly any respect for authority and found it difficult to work with Jack, but became an asset. As time went by, Warner became more tolerant of Mizner and helped invest in Mizner's Brown Derby restaurant. On April 3, 1933, Mizner died from a heart attack.
By 1932, audiences had grown tired of musicals, and the studio was forced to cut musical numbers from many productions and advertise them as straight comedies. The public had begun to associate musicals with color, and thus studios began to abandon its use. Warner Bros. had a contract with Technicolor to produce two more pictures in that process. As a result, the first horror films in color were produced and released by the studio: "Doctor X" (1932) and "Mystery of the Wax Museum" (1933). In the latter part of 1931, Harry Warner rented the Teddington Studios in London, England. The studio focused on making "quota quickies" for the domestic British market and Irving Asher was appointed as the studio's head producer. In 1934, Harry officially purchased the Teddington Studios.
In February 1933, Warner Bros. produced "42nd Street", a very successful musical under the direction of Loyd Bacon. Warner assigned Bacon to "more expensive productions including "Footlight Parade," "Wonder Bar," Broadway Gondolier" (which he also starred in), and "Gold Diggers" that saved the company from bankruptcy. In the wake of "42nd Street"'s success, the studio produced profitable musicals. These starred Ruby Keeler and Dick Powell and were mostly directed by Busby Berkeley. In 1935, the revival suffered a major blow when Berkeley was arrested after killing three people while driving drunk. By the end of the year, people again tired of Warner Bros. musicals, and the studio—after the huge profits made by 1935 film "Captain Blood"—shifted its focus to Errol Flynn swashbucklers.
1930–1935: Pre-code realistic period.
With the collapse of the market for musicals, Warner Bros., under Zanuck turned to more socially realistic storylines. For its many films about gangsters; Warner Bros. soon became known as a "gangster studio". The studio's first gangster film, "Little Caesar", was a great box office success and Edward G. Robinson starred in many of the subsequent Warner gangster films. The studio's next effort, "The Public Enemy", made James Cagney arguably the studio's new top star, and Warner Bros. made more gangster films.
Another gangster film the studio produced was the critically acclaimed "I Am a Fugitive from a Chain Gang", based on a true story and starring Paul Muni, joining Cagney and Robinson as one the studio's top gangster stars after appearing in the successful film, which convinced audiences to question the American legal system. By January 1933, the film's protagonist Robert Elliot Burns—still imprisoned in New Jersey—and other chain gang prisoners nationwide appealed and were released. In January 1933, Georgia chain gang warden J. Harold Hardy—who was also made into a character in the film—sued the studio for displaying "vicious, untrue and false attacks" against him in the film. After appearing in the Warner's film "The Man Who Played God", Bette Davis became a top star.
In 1933, relief for the studio came after Franklin D. Roosevelt became president and began the New Deal. This economic rebound allowed Warner Bros. to again became profitable. The same year, Zanuck quit. Harry Warner's relationship with Zanuck had become strained after Harry strongly opposed allowing Zanuck's film "Baby Face" to step outside Hays Code boundaries. The studio reduced his salary as a result of losses from the Great Depression, and Harry refused to restore it as the company recovered. Zanuck established his own company. Harry thereafter raised salaries for studio employees.
In 1933, Warner was able to link up with newspaper tycoon William Randolph Hearst's Cosmopolitan Films. Hearst had previously worked with MGM, but ended the association after a dispute with head producer Irving Thalberg over the treatment of Hearst's longstanding mistress, actress Marion Davies, who was struggling for box office success. Through his partnership with Hearst, Warner signed Davies to a studio contract. Hearst's company and Davies' films, however, did not increase the studio's profits.
In 1934, the studio lost over $2.5 million, of which $500,000 was the result of a 1934 fire at the Burbank studio, destroying 20 years' worth of early Vitagraph, Warner Bros. and First National films. The following year, Hearst's film adaption of William Shakespeare's "A Midsummer Night's Dream" (1935) failed at the box office and the studio's net loss increased. During this time, Harry and six other movie studio figures were indicted for conspiracy to violate the Sherman Antitrust Act, through an attempt to gain a monopoly over St Louis movie theaters. In 1935, Harry was put on trial; after a mistrial, Harry sold the company's movie theaters and the case was never reopened. 1935 also saw the studio make a net profit of $674,158.00.
By 1936, contracts of musical and silent stars were not renewed replaced by tough-talking, working-class types who better fit these pictures. Dorothy Mackaill, Dolores del Río, Bebe Daniels, Frank Fay, Winnie Lightner, Bernice Claire, Alexander Gray, Alice White, and Jack Mulhall that had characterized the urban, modern, and sophisticated attitude of the 1920s gave way to James Cagney, Joan Blondell, Edward G. Robinson, Warren William and Barbara Stanwyck, who would be more acceptable to the common man. The studio was one of the most prolific producers of Pre-Code pictures and had a lot of trouble with the censors once they started clamping down on what they considered indecency (around 1934). As a result, Warner Bros. turned to historical pictures from around 1935 to avoid confrontations with the Breen office. In 1936, following the success of "The Petrified Forest", Jack signed Humphrey Bogart to a studio contract. Warner, however, did not think Bogart was star material, and cast Bogart in infrequent roles as a villain opposite either James Cagney or Edward Robinson over the next five years.
After Hal B. Wallis succeeded Zanuck in 1933, and the Hays Code began to be enforced in 1935, the studio was forced to abandon this realistic approach in order to produce more moralistic, idealized pictures. The studio's historical dramas, melodramas (or "women's pictures"), swashbucklers, and adaptations of best-sellers, with stars like Bette Davis, Olivia de Havilland, Paul Muni, and Errol Flynn avoided the censors. In 1936, Bette Davis, by now arguably the studio's top star, was unhappy with her roles. She traveled to England and tried to break her contract. Davis lost the lawsuit and returned to America. Although many of the studio's employees had problems with Jack Warner, they considered Albert and Harry fair.
Code era.
In the 1930s many actors and actresses disappeared who had characterized the realistic pre-Code era but who were not suited to the new trend into moral and idealized pictures. Warner Bros. remained a top studio in Hollywood, but this changed after 1935 as other studios, notably MGM, quickly overshadowed the prestige and glamor that previously characterized Warner Bros. However, in the late 1930s, Bette Davis became the studio's top draw and was even dubbed as "The Fifth Warner Brother."
In 1935, Cagney sued Jack Warner for breach of contract. Cagney claimed Warner had forced him to star in more films than his contract required. Cagney eventually dropped his lawsuit after a cash settlement. Nevertheless, Cagney left the studio to establish an independent film company with his brother Bill. The Cagneys released their films though Grand National Films, however they were not able to get good financing and ran out of money after their third film. Cagney then agreed to return to Warner Bros., after Jack agreed to a contract guaranteeing Cagney would be treated to his own terms. After the success of "Yankee Doodle Dandy" at the box office, Cagney again questioned if the studio would meet his salary demand and again quit to form his own film production and distribution company with Bill.
Another employee with whom Warner had troubles was studio producer Bryan Foy. In 1936, Wallis hired Foy as a producer for the studio's low budget B-films leading to his nickname "the keeper of the B's". Foy was able to garnish arguably more profits than any other B-film producer at the time. During Foy's time at the studio, however, Warner fired him seven different times.
During 1936, "The Story of Louis Pasteur" proved a box office success and star Paul Muni won the Oscar for Best Actor in March 1937. The studio's 1937 film "The Life of Emile Zola" gave the studio its first Best Picture Oscar.
In 1937, the studio hired Midwestern radio announcer Ronald Reagan. Although Reagan was initially a B-film actor, Warner Bros. was impressed by his performance in the final scene of "Knute Rockne, All American", and agreed to pair him with Flynn in "Santa Fe Trail" (1940). Reagan then returned to B-films. After his performance in the studio's 1942 "Kings Row", Warner decided to make Reagan a top star and signed him to a new contract, tripling his salary.
In 1936, Harry's daughter Doris read a copy of Margaret Mitchell's "Gone with the Wind" and was interested in making a film adaptation. Doris offered Mitchell $50,000 for screen rights. Jack vetoed the deal, realizing it would be an expensive production.
George Raft also proved to be a problem for Jack. Warner had signed him in 1939, hoping he could substitute in gangster pictures when either Robinson or Cagney were on suspension. Raft had difficulty working with Bogart and refused to co-star with him. Eventually, Warner agreed to release Raft from his contract. Following Raft's departure, the studio gave Bogart the role of Roy Earl in the 1941 film "High Sierra", which helped establish him as a top star. Following "High Sierra", Bogart was given a role in John Huston's successful 1941 remake of the studio's 1931 failure, "The Maltese Falcon".
Warner's cartoons.
Warner's cartoon unit had its roots in the independent Harman and Ising studio. From 1930 to 1933, Disney alumni Hugh Harman and Rudolf Ising produced musical cartoons for Leon Schlesinger, who sold them to Warner. Harman and Ising introduced their character Bosko in the first "Looney Tunes" cartoon, "Sinkin' in the Bathtub", and created a sister series, "Merrie Melodies", in 1931.
Harman and Ising broke away from Schlesinger in 1933 due to a contractual dispute, taking Bosko with them to MGM. As a result, Schlesinger started his own studio, Leon Schlesinger Productions, which continued with "Merrie Melodies" while starting production on "Looney Tunes" starring Buddy, a Bosko clone. By the end of the decade, a new Schlesinger production team, including directors Friz Freleng, Tex Avery, Robert Clampett and Chuck Jones was formed. Schlesinger's staff developed a fast-paced, irreverent style that made their cartoons globally popular.
In 1936, Avery directed cartoons starring Porky Pig, which established the character as the studio's first animated star. In addition to Porky, Warner Bros. cartoon characters Daffy Duck (who debuted in the 1937 short "Porky's Duck Hunt") and Bugs Bunny (who debuted in the 1940 short "A Wild Hare") achieved star power. By 1942, the Schlesinger studio had surpassed Walt Disney Studios as the most successful producer of animated shorts.
Warner Bros eventually bought Schlesinger's cartoon unit in 1944 and renamed it Warner Bros. Cartoons. Unfortunately, the unit was indifferently treated by senior management, beginning with the installation of Edward Selzer as senior producer, whom the creative staff considered an interfering incompetent. Jack had little regard for the company's short film product and reputedly was so ignorant about the animation division of the studio that he was mistakenly convinced that the unit produced cartoons of Mickey Mouse, Walt Disney Productions' flagship character. He sold off the unit's pre-August 1948 library for $3,000 each, which proved a shortsighted transaction in light of its eventual value.
Warner Brothers Cartoons continued, with intermittent interruptions, until 1969 when it was dissolved as the parent company ceased film shorts entirely. Characters such as Bugs Bunny, Daffy Duck, Tweety Bird, Sylvester, and Porky Pig became central to the company's image in subsequent decades. Bugs in particular remains a mascot to Warner Bros., its various divisions and Six Flags (which Time Warner once owned). The success of the compilation film, "The Bugs Bunny/Road Runner Movie" in 1980, featuring the archived film of these characters prompted Warner Brothers to organize Warner Bros. Animation as a new production division to restart production of original material.
World War II.
According to Warner's autobiography, prior to US entry in World War II, Philip Kauffman, Warner Bros. German sales head, was murdered by the Nazis in Berlin in 1936. Harry produced the successful anti-German film "The Life of Emile Zola" (1937). After that, Harry supervised the production of more anti-German films, including "Confessions of a Nazi Spy" (1939), "The Sea Hawk" (1940), which made King Phillip II an equivalent of Hitler, "Sergeant York", and "You're In The Army Now" (1941). Harry then decided to focus on producing war films. Warners cut its film production in half during the war, eliminating its B Pictures unit in 1941. Bryan Foy joined Twentieth Century Fox.
During the war era, the studio made "Casablanca", "Now, Voyager", "Yankee Doodle Dandy" (all 1942), "This Is the Army", and "Mission to Moscow" (both 1943); the latter became controversial a few years afterwards. At the premieres of "Yankee Doodle Dandy" (in Los Angeles, New York, and London), audiences purchased $15.6 million in war bonds for the governments of England and the United States. By the middle of 1943, however, audiences had tired of war films, but Warner continued to produce them, losing money. In honor of the studio's contributions to the cause, the Navy named a Liberty ship after the brothers' father, Benjamin Warner. Harry christened the ship. By the time the war ended, $20 million in war bonds were purchased through the studio, the Red Cross collected 5,200 pints of plasma from studio employees and 763 of the studio's employees served in the armed forces, including Harry Warner's son-in-law Milton Sperling and Jack's son Jack Warner, Jr. Following a dispute over ownership of "Casablanca"'s Oscar for Best Picture, Wallis resigned. After "Casablanca" made Bogart a top star, Bogart's relationship with Jack deteriorated.
In 1943, Olivia de Haviland (whom Warner was loaning to different studios) sued Warner for breach of contract. De Haviland had refused to portray famed abolitionist Elizabeth Blackwell in an upcoming film for Columbia Pictures. Warner responded by sending 150 telegrams to different film production companies, warning them not to hire her for any role. Afterwards, de Haviland discovered employment contracts in the United States could only last seven years; de Haviland had been under contract with the studio since 1935. The court ruled in de Haviland's favor and she left the studio. Through de Haviland's victory, many of the studio's longtime actors were now freed from their contracts, and Harry decided to terminate the studio's suspension policy.
The same year, Jack signed newly released MGM actress Joan Crawford, a former top star who found her career fading. Crawford's first role with the studio was 1944's "Hollywood Canteen". Her first starring role at the studio, in the title role as "Mildred Pierce" (1945), revived her career and earned her an Oscar for Best Actress.
After World War II: changing hands.
In the post-war years, Warner Bros. continued to create new stars, including Lauren Bacall and Doris Day. The studio prospered greatly after the war. By 1946, company payroll reached $600,000 a week and net profit topped $19.4 million.
Jack Warner continued to refuse to meet Screen Actors Guild salary demands. In September 1946, employees engaged in a month-long strike. In retaliation, Warner—during his 1947 testimony before Congress about "Mission to Moscow"—accused multiple employees of ties to Communists. By the end of 1947, the studio reached a record net profit of $22 million.
On January 5, 1948, Warner offered the first color newsreel, covering the Tournament of Roses Parade and the Rose Bowl Game. In 1948, Bette Davis, still their top actress and now hostile to Jack, was a big problem for Harry after she and others left the studio after completing the film "Beyond the Forest".
Warner was a party to the "United States v. Paramount Pictures, Inc." anti-trust case of the 1940s. This action, brought by the Justice Department and the Federal Trade Commission, claimed the five integrated studio-theater chain combinations restrained competition. The Supreme Court heard the case in 1948, and ruled for the government. As a result, Warner and four other major studios were forced to separate production from exhibition. In 1949, the studio's net profit was only $10 million.
Warner Bros. had two semi-independent production companies that released films through the studio. One of these was Sperling's United States Pictures.
In the early 1950s, the threat of television emerged. In 1953, Jack decided to copy. United Artists successful 3D film "Bwana Devil", releasing his own 3D films beginning with "House of Wax". However, 3D films soon lost their appeal among moviegoers.
3D almost caused the demise of the Warner Bros. cartoon studio. Having completed a 3D Bugs Bunny cartoon, "Lumber Jack-Rabbit", Jack Warner ordered the animation unit to be shut down, erroneously believing that all cartoons hence would be produced in the 3D process. Several months later, Warner relented and reopened the cartoon studio. Fortunately, Warner Bros. had enough of a backlog of cartoons and a healthy reissue program so that there was no noticeable interruption in the release schedule.
In 1952, Warner Bros. made their first film ("Carson City") in "Warnercolor", the studio's name for Eastmancolor.
After the downfall of 3D films, Harry Warner decided to use CinemaScope in future Warner Bros. films. One of the studio's first CinemaScope films, "The High and the Mighty" (owned by John Wayne's company Batjac), enabled the studio to show a profit.
Early in 1953, Warner's theater holdings were spun off as Stanley Warner Theaters; Stanley Warner's non-theater holdings were sold to Simon Fabian Enterprises, and its theaters merged with RKO Theatres to become RKO-Stanley Warner Theatres.
By 1956 the studio was losing money, declining from 1953's net profit of $2.9 million and the next two years of between $2 and $4 million. In February 13, 1956, Jack Warner sold the rights to all of his pre-1950 films to Associated Artists Productions (which merged with United Artists Television in 1958, and was subsequently acquired by Turner Broadcasting System in early 1986 as part of a failed takeover of MGM/UA by Ted Turner).
In May 1956, the brothers announced they were putting Warner Bros. on the market. Jack secretly organized a syndicate—headed by Boston banker Serge Semenenko– to purchase 90% of the stock. After the three brothers sold, Jack—through his under-the-table deal—joined Semenenko's syndicate and bought back all his stock. Shortly after the deal was completed in July, Jack—now the company's largest stockholder—appointed himself its new president. Shortly after the deal closed, Jack announced the company and its subsidiaries would be "directed more vigorously to the acquisition of the most important story properties, talents, and to the production of the finest motion pictures possible."
Warner Bros. Television and Warner Bros. Records.
By 1949, with the success of television threatening the film industry more and more, Harry Warner decided to emphasize television production. However, the Federal Communications Commission (FCC) would not permit it. After an unsuccessful attempt to convince other movie studio bosses to switch, Harry abandoned his television efforts.
Jack had problems with Milton Berle's unsuccessful film "Always Leave Them Laughing" during the peak of Berle's television popularity. Warner felt that Berle was not strong enough to carry a film and that people would not pay to see the man they could see on television for free. However, Jack was pressured into using Berle, replacing Danny Kaye with him. Berle's outrageous behaviour on the set and the film's massive failure led to Jack banning television sets from film sets.
On March 21, 1955, the studio was finally able engage in television through the successful Warner Bros. Television unit run by William T. Orr, Jack Warner's son-in-law. Warner Bros. Television provided ABC with a weekly show, "Warner Bros. Presents." The show featured rotating shows based on three film successes, "Kings Row", "Casablanca" and "Cheyenne", followed by a promotion for a new film. It was not a success. The studio's next effort was to make a weekly series out of "Cheyenne". "Cheyenne" was television's first hour-long Western. Two episodes were placed together for feature film release outside the United States. In the tradition of their B pictures, the studio followed up with a series of rapidly produced popular Westerns, such as writer/producer Roy Huggins' critically lauded "Maverick" as well as "Sugarfoot", "Bronco", "Lawman", "The Alaskans" and "Colt .45". The success of these series helped to make up for losses in the film business. As a result, Jack decided to emphasize television production. Warner's produced a series of popular private detective shows beginning with "77 Sunset Strip" (1958–1964) followed by "Hawaiian Eye" (1959–1963), "Bourbon Street Beat" (1960) and "Surfside Six" (1960–1962).
Within a few years, the studio provoked hostility among their TV stars such as Clint Walker and James Garner, who sued over a contract dispute and won. Edd Byrnes was not so lucky and bought himself out of his contract. Jack was angered by their perceived ingratitude, who evidently showed more independence than film actors, deepening his contempt for the new medium. Many of Warner's television stars appeared in the casts of Warner's cinema releases. In 1963 a court decision forced Warner's to end contracts with their television stars, engaging them for specific series or film roles. In the same year Jack Webb took over the television unit without success.
Warner Bros. was already the owner of extensive music-publishing holdings, whose tunes had appeared in countless cartoons (arranged by Carl Stalling) and television shows (arranged by Max Steiner).
In 1958, the studio launched Warner Bros. Records. Initially the label released recordings made by their television stars—whether they could sing or not—and records based on television soundtracks.
In 1963, Warner agreed to a "rescue takeover" of Frank Sinatra's Reprise Records. The deal gave Sinatra US$1.5 million and part ownership of Warner Bros. Records, making Reprise a sub-label. Most significantly the deal brought Reprise manager Morris "Mo" Ostin into the company. In 1964, upon seeing the profits record companies made from Warner film music, Warner decided to claim ownership of the studio's film soundtracks. In its first eighteen months, Warner Bros. Records lost around $2 million.
New owners.
Warner Bros. rebounded in the late 1950s, specializing in adaptations of popular plays like "The Bad Seed" (1956), "No Time for Sergeants" (1958), and "Gypsy" (1962).
While he slowly recovered from a car crash that occurred while vacationing in France in 1958, Jack returned to the studio and made sure his name was featured in studio press releases. From 1961-63, the studio's annual net profit was a little over $7 million. Warner paid an unprecedented $5.5 million for the film rights to the Broadway musical "My Fair Lady" in February 1962. The previous owner, CBS director William S. Paley, set terms including half the distributor's gross profits "plus ownership of the negative at the end of the contract." In 1963, the studio's net profit dropped to $3.7 million. By the mid-1960s, motion picture production was in decline, as the industry was in the midst of a painful transition from the Golden Age of Hollywood to the era now known as New Hollywood. Few studio films were made in favor of co-productions (for which Warner provided facilities, money and distribution), and pickups of independent pictures.
With the success of the studio's 1964 film of Broadway play "My Fair Lady", as well as its soundtrack, Warner Bros. Records became a profitable subsidiary. The 1966 film "Who's Afraid Of Virginia Woolf?" was a huge success.
In November 1966, Jack gave in to advancing age and changing times, selling control of the studio and music business to Seven Arts Productions, run by Canadian investors Elliot and Kenneth Hyman, for $32 million. The company, including the studio, was renamed Warner Bros.-Seven Arts. Warner remained president until the summer of 1967, when "Camelot" failed at the box office and Warner gave up his position to his longtime publicity director, Ben Kalmenson; Warner remained on board as an independent producer and vice-president. With the 1967 success of "Bonnie and Clyde", Warner Bros. was again profitable.
Two years later the Hymans were tired and fed up with Jack Warner and his actions. They accepted a cash-and-stock offer from Kinney National Company for more than $64 million. Kinney owned a Hollywood talent agency, Ashley-Famous, whose founder Ted Ashley led Kinney head Steve Ross to purchase Warner Bros. Ashley became the studio head and changed the name to Warner Bros., Inc. once again. Jack Warner was outraged by the Hymans' sale, and decided to retire, until his death from health complications of heart inflammation in 1978.
Although movie audiences had shrunk, Warner's new management believed in the drawing power of stars, signing co-production deals with several of the biggest names of the day, including Paul Newman, Robert Redford, Barbra Streisand, and Clint Eastwood, carrying the studio successfully through the 1970s and 1980s. Warner Bros. also made major profits on films and television shows built around the characters of Superman, Batman and Wonder Woman owned by Warner Bros. subsidiary DC Comics.
Abandoning parking lots and funeral homes, the refocused Kinney renamed itself in honor of its best-known holding, Warner Communications. Throughout the 1970s and 1980s Warner Communications branched out into other business, such as video game company Atari, Inc. in 1976, and later the Six Flags theme parks.
From 1971 until the end of 1987, Warner's international distribution operations were a joint venture with Columbia Pictures. In some countries, this joint venture distributed films from other companies (such as EMI Films and Cannon Films in the UK). Warner ended the venture in 1988 and partnered with Walt Disney Pictures. This joint venture lasted until 1993, when Disney created Buena Vista International.
In 1972, in a cost-cutting move, Warner and Columbia formed a third company called The Burbank Studios (TBS). They would share the Warner lot in Burbank. Both studios technically became production entities, giving TBS day-to-day responsibility for studio grounds and upkeep. The Columbia Ranch (about a mile north of Warner's lot) was part of the deal. The Warner-Columbia relationship was acrimonious, but the reluctance of both studios to approve or spend money on capital upgrades that might only help the other did have the unintended consequence of preserving the Warner lot's primary function as a filmmaking facility while it produced relatively little during the 1970s and 1980s. (Most films produced after 1968 were filmed on location after the failure of "Camelot" was partially attributed to the fact it was set in England but obviously filmed in Burbank.) With control over its own lot tied up in TBS, Warner ultimately retained a significant portion of its backlot, while Fox sold its backlot to create Century City, Universal turned part of its backlot into a theme park and shopping center, and Disney replaced its backlot with office buildings and exiled its animation department to an industrial park in Glendale.
In 1989, a solution to the situation became evident when Warner Bros. acquired Lorimar-Telepictures and gained control of the former MGM studio lot in Culver City, and that same year, Sony bought Columbia Pictures. Sony was flush with cash and Warner Bros. now had two studio lots. In 1990, TBS ended when Sony bought the MGM lot from Warner and moved Columbia to Culver City. However, Warner kept the Columbia Ranch, now known as the Warner Bros. Ranch.
Warner Communications merged in 1989 with white-shoe publishing company Time Inc. Time claimed a higher level of prestige, while Warner Bros. provided the profits. The Time Warner merger was almost derailed when Paramount Communications (Formerly Gulf+Western, later sold to Viacom), launched a $12.2 billion hostile takeover bid for Time Inc., forcing Time to acquire Warner with a $14.9 billion cash/stock offer. Paramount responded with a lawsuit filed in Delaware court to break up the merger. Paramount lost and the merger proceeded.
In 1992 Warner Bros. Family Entertainment was established to produce various family-oriented films.
In 1997, Time Warner sold Six Flags. The takeover of Time Warner in 2000 by then-high-flying AOL did not prove a good match, and following the collapse in "dot-com" stocks, the AOL element was banished from the corporate name.
Since 1995.
In 1995, Warner and station owner Tribune Company of Chicago launched The WB Network, seeking a niche market in teenagers. The WB's early programming included an abundance of teenage fare like "Buffy the Vampire Slayer", "Smallville", "Dawson's Creek", and "One Tree Hill". Two dramas produced by Spelling Television, "7th Heaven" and "Charmed" helped bring The WB into the spotlight ."Charmed" lasted eight seasons, becoming the longest running drama with female leads. "7th Heaven" ran for eleven seasons and was the longest running family drama and longest running show for the network. In 1998, Warner Bros. celebrated its 75th anniversary. In 2006, Warner and CBS Paramount Television decided to close The WB and CBS's UPN and jointly launch The CW Television Network. In 1999, Terry Semels and Robert Daly resigned as studio heads after a career 13 Oscar nominated films. Daly and Semels were said to have popularized the modern model of partner financing and profit sharing for film production.
In the late 1990s, Warner obtained rights to the "Harry Potter" novels, and released feature film adaptations of the first in 2001, the second in 2002, the third in June 2004, the fourth in November 2005, and the fifth on July 11, 2007. The sixth came in July 2009. The seventh and final was released in two parts: Part 1 in November 2010 and Part 2 in July 2011.
From 2006, Warner Bros operated a joint venture with China Film Group Corporation and HG to form Warner China Film HG to produce films in Hong Kong and China, including "Connected", a remake of the 2004 thriller film "Cellular""." They co-produced many other Chinese films.
Warner Bros. played a large part in the discontinuation of the HD DVD format. On January 4, 2008, Warner Bros. announced that they would drop support of HD DVD in favor of Blu-ray Disc. HD DVDs continued to be released through May 2008, but only following Blu-ray and DVD releases.
In 2009, Warner Bros. became the first studio in history to gross more than $2 billion domestically in a single year.
Warner Bros.' "Harry Potter" film series was the worldwide highest grossing film series of all time without adjusting for inflation. Its "Batman" film series was one of only two series to have two entries earn more than $1 billion worldwide. "Harry Potter and the Deathly Hallows – Part 2" was Warner Bros.' highest grossing movie ever (surpassing "The Dark Knight"). However, the Harry Potter movies have produced a net loss due to Hollywood accounting. IMAX Corp.signed with Warner Bros. Pictures in April 2010 to release as many as 20 giant-format films through 2013.
Warner Bros. formed a short form digital unit, Blue Ribbon Content, under its Warner Bros. Animation & Warner Digital Series president.
On February 6, 2014, Warner Bros., through the legal name Columbia TriStar Warner Filmes de Portugal Ltda., announced that would have its offices at Portugal no more from March 31, 2014.
As of 2015, Warner Bros. is one of only three studios to have released a pair of billion-dollar films in the same year (along with Walt Disney Studios Motion Pictures and Universal Studios); the distinction was achieved in 2012 with "The Dark Knight Rises" and "".
Production deals.
Active producer deals (as of March 2014)
Film library.
Film series.
Mergers and acquisitions have helped Warner Bros. accumulate a diverse collection of movies, cartoons and television programs.
In the aftermath of the 1948 antitrust suit, uncertain times led Warner Bros. in 1956 to sell most of its pre-1950 films and cartoons to a holding company called Associated Artists Productions (a.a.p.). a.a.p. also got the Fleischer Studios and Famous Studios "Popeye" cartoons, originally from Paramount. Two years later, a.a.p. was sold to United Artists (UA), which held them until 1981, when Metro-Goldwyn-Mayer bought UA. 
In 1982, Turner Broadcasting System acquired Brut Productions, the film production subsidiary of the then struggling personal-care company Faberge Inc.
In 1986, Turner Broadcasting System, having failed to buy MGM, settled for ownership of the MGM/UA library. This included almost all the pre-May 1986 MGM film and television library with the exception of the films & TV shows owned by United Artists (i.e. James Bond franchise), although some UA material were included such as the a.a.p. library, the U.S. rights to a majority of the RKO Radio Pictures library, and the television series "Gilligan's Island". 
In 1989, Warner Communications bought the Lorimar television and film library. Their purchase included the 1974-1989 library of Rankin/Bass Productions, as well as the Monogram Pictures and Allied Artists libraries.
In 1991, Turner Broadcasting System bought animation studio Hanna-Barbera Productions, and much of the back catalog of both Hanna-Barbera and Ruby-Spears Enterprises from Great American Broadcasting, and years later, Turner bought Castle Rock Entertainment on December 22, 1993 and New Line Cinema on January 28, 1994. In 1996, Time Warner bought Turner Broadcasting System, and brought the pre-1950 silent/sound films and the pre-August 1948 cartoon library back home.
On October 4, 2007, Warner Bros. added the "Peanuts/Charlie Brown" library to its collection from Peanuts Worldwide, LLC, licensor and owner of the "Peanuts" material; this includes all the television specials and series outside of the theatrical library, which continues to be owned by CBS and Paramount.
In 2008, Warner Bros. closed New Line Cinema as an independent mini-major studio. As a result, Warner added the New Line Cinema film and television library to its collection. On October 15, 2009, Warner Bros. acquired the home entertainment rights to the "Sesame Street" library, in conjunction with Sesame Workshop.
Highest-grossing films.
The Warner Bros. Archives.
The University of Southern California Warner Bros. Archives is the largest single studio collection in the world. Donated in 1977 to USC's School of Cinema-Television by Warner Communications, the WBA houses departmental records that detail Warner Bros. activities from the studio's first major feature, "My Four Years in Germany" (1918), to its sale to Seven Arts in 1968. It presents a complete view of the production process during the Golden Age of Hollywood. UA donated pre-1950 Warner Bros. nitrate negatives to the Library of Congress and post-1951 negatives to the UCLA Film and Television Archive. Most of the company's legal files, scripts, and production materials were donated to the Wisconsin Center for Film and Theater Research.

</doc>
<doc id="34053" url="https://en.wikipedia.org/wiki?curid=34053" title="Water turbine">
Water turbine

A water turbine is a rotary machine that converts kinetic and potential energy of water into mechanical work.
Water turbines were developed in the 19th century and were widely used for industrial power prior to electrical grids. Now they are mostly used for electric power generation.
Water turbines are mostly found in dams to generate electric power from water kinetic energy.
History.
Water wheels have been used for hundreds of years for industrial power. Their main shortcoming is size, which limits the flow rate and head that can be harnessed.
The migration from water wheels to modern turbines took about one hundred years. Development occurred during the Industrial revolution, using scientific principles and methods. They also made extensive use of new materials and manufacturing methods developed at the time.
Swirl.
The word turbine was introduced by the French engineer Claude Burdin in the early 19th century and is derived from the Latin word for "whirling" or a "vortex". The main difference between early water turbines and water wheels is a swirl component of the water which passes energy to a spinning rotor. This additional component of motion allowed the turbine to be smaller than a water wheel of the same power. They could process more water by spinning faster and could harness much greater heads. (Later, impulse turbines were developed which didn't use swirl).
Timeline.
The earliest known water turbines date to the Roman Empire. Two helix-turbine mill sites of almost identical design were found at Chemtou and Testour, modern-day Tunisia, dating to the late 3rd or early 4th century AD. The horizontal water wheel with angled blades was installed at the bottom of a water-filled, circular shaft. The water from the mill-race entered the pit tangentially, creating a swirling water column which made the fully submerged wheel act like a true turbine.
Fausto Veranzio in his book Machinae Novae (1595) described a vertical axis mill with a rotor similar to that of a Francis turbine.
Johann Segner developed a reactive water turbine (Segner wheel) in the mid-18th century in Kingdom of Hungary. It had a horizontal axis and was a precursor to modern water turbines. It is a very simple machine that is still produced today for use in small hydro sites. Segner worked with Euler on some of the early mathematical theories of turbine design. In the 18th century, a Dr. Barker invented a similar reaction hydraulic turbine that became popular as a lecture-hall demonstration. The only known surviving example of this type of engine used in power production, dating from 1851, is found at Hacienda Buena Vista in Ponce, Puerto Rico.
In 1820, Jean-Victor Poncelet developed an inward-flow turbine.
In 1826, Benoît Fourneyron developed an outward-flow turbine. This was an efficient machine (~80%) that sent water through a runner with blades curved in one dimension. The stationary outlet also had curved guides.
In 1844, Uriah A. Boyden developed an outward flow turbine that improved on the performance of the Fourneyron turbine. Its runner shape was similar to that of a Francis turbine.
In 1849, James B. Francis improved the inward flow reaction turbine to over 90% efficiency. He also conducted sophisticated tests and developed engineering methods for water turbine design. The Francis turbine, named for him, is the first modern water turbine. It is still the most widely used water turbine in the world today. The Francis turbine is also called a radial flow turbine, since water flows from the outer circumference towards the centre of runner.
Inward flow water turbines have a better mechanical arrangement and all modern reaction water turbines are of this design. As the water swirls inward, it accelerates, and transfers energy to the runner. Water pressure decreases to atmospheric, or in some cases subatmospheric, as the water passes through the turbine blades and loses energy.
Around 1890, the modern fluid bearing was invented, now universally used to support heavy water turbine spindles. As of 2002, fluid bearings appear to have a mean time between failures of more than 1300 years.
Around 1913, Viktor Kaplan created the Kaplan turbine, a propeller-type machine. It was an evolution of the Francis turbine but revolutionized the ability to develop low-head hydro sites.
New concept.
All common water machines until the late 19th century (including water wheels) were basically reaction machines; water "pressure" head acted on the machine and produced work. A reaction turbine needs to fully contain the water during energy transfer.
In 1866, California millwright Samuel Knight invented a machine that took the impulse system to a new level. Inspired by the high pressure jet systems used in hydraulic mining in the gold fields, Knight developed a bucketed wheel which captured the energy of a free jet, which had converted a high head (hundreds of vertical feet in a pipe or penstock) of water to kinetic energy. This is called an impulse or tangential turbine. The water's velocity, roughly twice the velocity of the bucket periphery, does a u-turn in the bucket and drops out of the runner at low velocity.
In 1879, Lester Pelton, experimenting with a Knight Wheel, developed a Pelton wheel (double bucket design), which exhausted the water to the side, eliminating some energy loss of the Knight wheel which exhausted some water back against the center of the wheel. In about 1895, William Doble improved on Pelton's half-cylindrical bucket form with an elliptical bucket that included a cut in it to allow the jet a cleaner bucket entry. This is the modern form of the Pelton turbine which today achieves up to 92% efficiency. Pelton had been quite an effective promoter of his design and although Doble took over the Pelton company he did not change the name to Doble because it had brand name recognition.
Turgo and cross-flow turbines were later impulse designs.
Theory of operation.
Flowing water is directed on to the blades of a turbine runner, creating a force on the blades. Since the runner is spinning, the force acts through a distance (force acting through a distance is the definition of work). In this way, energy is transferred from the water flow to the turbine
Water turbines are divided into two groups; reaction turbines and impulse turbines.
The precise shape of water turbine blades is a function of the supply pressure of water, and the type of impeller selected.
Reaction turbines.
Reaction turbines are acted on by water, which changes pressure as it moves through the turbine and gives up its energy. They must be encased to contain the water pressure (or suction), or they must be fully submerged in the water flow.
Newton's third law describes the transfer of energy for reaction turbines.
Most water turbines in use are reaction turbines and are used in low (<) and medium () head applications.
In reaction turbine pressure drop occurs in both fixed and moving blades.
It is largely used in dam and large power plants
Impulse turbines.
Impulse turbines change the velocity of a water jet. The jet pushes on the turbine's curved blades which changes the direction of the flow. The resulting change in momentum (impulse) causes a force on the turbine blades. Since the turbine is spinning, the force acts through a distance (work) and the diverted water flow is left with diminished energy. An impulse turbine is one which the pressure of the fluid flowing over the rotor blades is constant and all the work output is due to the change in kinetic energy of the fluid.
Prior to hitting the turbine blades, the water's pressure (potential energy) is converted to kinetic energy by a nozzle and focused on the turbine. No pressure change occurs at the turbine blades, and the turbine doesn't require a housing for operation.
Newton's second law describes the transfer of energy for impulse turbines.
Impulse turbines are often used in very high (>300m/1000 ft) head applications.
Power.
The power available in a stream of water is;
formula_1
where:
Pumped-storage hydroelectricity.
Some water turbines are designed for pumped-storage hydroelectricity. They can reverse flow and operate as a pump to fill a high reservoir during off-peak electrical hours, and then revert to a water turbine for power generation during peak electrical demand. This type of turbine is usually a Deriaz or Francis turbine in design.
Efficiency.
Large modern water turbines operate at mechanical efficiencies greater than 90%.
Types of water turbines.
Reaction turbines:
Impulse turbine
Design and application.
Turbine selection is based on the available water head, and less so on the available flow rate. In general, impulse turbines are used for high head sites, and reaction turbines are used for low head sites. Kaplan turbines with adjustable blade pitch are well-adapted to wide ranges of flow or head conditions, since their peak efficiency can be achieved over a wide range of flow conditions.
Small turbines (mostly under 10 MW) may have horizontal shafts, and even fairly large bulb-type turbines up to 100 MW or so may be horizontal. Very large Francis and Kaplan machines usually have vertical shafts because this makes best use of the available head, and makes installation of a generator more economical. Pelton wheels may be either vertical or horizontal shaft machines because the size of the machine is so much less than the available head. Some impulse turbines use multiple jets per runner to balance shaft thrust. This also allows for the use of a smaller turbine runner, which can decrease costs and mechanical losses.
Typical range of heads.
• Water wheel
• Screw turbine
• VLH turbine
• Kaplan turbine
• Francis turbine
• Pelton wheel
• Turgo turbine
0.2 < "H" < 4   ("H" = head in m)
1 < "H" < 10
1.5 < "H" < 4.5
20 < "H" < 40
40 < "H" < 600
50 < "H" < 1300
50 < "H" < 250
Specific speed.
The specific speed formula_8 of a turbine characterizes the turbine's shape in a way that is not related to its size. This allows a new turbine design to be scaled from an existing design of known performance. The specific speed is also the main criteria for matching a specific hydro site with the correct turbine type.
The specific speed is the speed with which the turbine turns for a particular discharge Q, with unit head and thereby is able to produce unit power.
Affinity laws.
Affinity laws allow the output of a turbine to be predicted based on model tests. A miniature replica of a proposed design, about one foot (0.3 m) in diameter, can be tested and the laboratory measurements applied to the final application with high confidence. Affinity laws are derived by requiring similitude between the test model and the application.
Flow through the turbine is controlled either by a large valve or by wicket gates arranged around the outside of the turbine runner. Differential head and flow can be plotted for a number of different values of gate opening, producing a hill diagram used to show the efficiency of the turbine at varying conditions.
Runaway speed.
The runaway speed of a water turbine is its speed at full flow, and no shaft load. The turbine will be designed to survive the mechanical forces of this speed. The manufacturer will supply the runaway speed rating.
Control systems.
Different designs of governors have been used since the mid-19th century to control the speeds of the water turbines. A variety of flyball systems, or first-generation governors, were used during the first 100 years of water turbine speed controls. In early flyball systems, the flyball component countered by a spring acted directly to the valve of the turbine or the wicket gate to control the amount of water that enters the turbines. Newer systems with mechanical governors started around 1880. An early mechanical governors is a servomechanism that comprises a series of gears that use the turbine's speed to drive the flyball and turbine's power to drive the control mechanism. The mechanical governors were continued to be enhanced in power amplification through the use of gears and the dynamic behavior. By 1930, the mechanical governors had many parameters that could be set on the feedback system for precise controls. In the later part of the twentieth century, electronic governors and digital systems started to replace the mechanical governors. In the electronic governors, also known as second-generation governors, the flyball was replaced by rotational speed sensor but the controls were still done through analog systems. In the modern systems, also known as third-generation governors, the controls are performed digitally by algorithms that are programmed to the computer of the governor.
Turbine Blade Materials.
Given that the turbine blades in a water turbine are constantly exposed to water and dynamic forces, they need to have high corrosion resistance and strength. The most common material used in overlays on carbon steel runners in water turbines are austenitic steel alloys that have 17% to 20% chromium to increase stability of the film which improves aqueous corrosion resistance. The chromium content in these steel alloys exceed the minimum of 12% chromium required to exhibit some atmospheric corrosion resistance. Having a higher chromium concentration in the steel alloys allows for a much longer lifespan of the turbine blades. Currently, the blades are made of martensitic stainless steels which have high strength compared to austenitic stainless steels by a factor of 2. Besides corrosion resistance and strength as the criteria for material selection, weld-ability and density of the turbine blade. Greater weld-ability allows for easier repair of the turbine blades. This also allows for higher weld quality which results in a better repair. Selecting a material with low density is important to achieve higher efficiency because the lighter blades rotate more easily. The most common material used in Kaplan Turbine blades are stainless steel alloys (SS). The different alloys used are SS(16Cr-5Ni), SS(13Cr-4Ni), SS(13Cr-1Ni). The martensitic stainless steel alloys have high strength, thinner sections than standard carbon steel, and reduced mass that enhances the hydrodynamic flow conditions and efficiency of the water turbine. The SS(13Cr-4Ni) has been shown to have improved erosion resistance at all angles of attack through the process of laser hardening. It is important to minimize erosion in order to maintain high efficiencies because erosion negatively impacts the hydraulic profile of the blades which reduces the relative ease to rotate.
Maintenance.
Turbines are designed to run for decades with very little maintenance of the main elements; overhaul intervals are on the order of several years. Maintenance of the runners and parts exposed to water include removal, inspection, and repair of worn parts.
Normal wear and tear includes pitting corrosion from cavitation, fatigue cracking, and abrasion from suspended solids in the water. Steel elements are repaired by welding, usually with stainless steel rods. Damaged areas are cut or ground out, then welded back up to their original or an improved profile. Old turbine runners may have a significant amount of stainless steel added this way by the end of their lifetime. Elaborate welding procedures may be used to achieve the highest quality repairs.
Other elements requiring inspection and repair during overhauls include bearings, packing box and shaft sleeves, servomotors, cooling systems for the bearings and generator coils, seal rings, wicket gate linkage elements and all surfaces.
Environmental impact.
Water turbines are generally considered a clean power producer, as the turbine causes essentially no change to the water. They use a renewable energy source and are designed to operate for decades. They produce significant amounts of the world's electrical supply.
Historically there have also been negative consequences, mostly associated with the dams normally required for power production. Dams alter the natural ecology of rivers, potentially killing fish, stopping migrations, and disrupting peoples' livelihoods. For example, American Indian tribes in the Pacific Northwest had livelihoods built around salmon fishing, but aggressive dam-building destroyed their way of life. Dams also cause less obvious, but potentially serious consequences, including increased evaporation of water (especially in arid regions), buildup of silt behind the dam, and changes to water temperature and flow patterns. In the United States, it is now illegal to block the migration of fish, for example the white sturgeon in North America, so fish ladders must be provided by dam builders.

</doc>
<doc id="34054" url="https://en.wikipedia.org/wiki?curid=34054" title="White Wolf Publishing">
White Wolf Publishing

White Wolf Publishing is an American roleplaying game and book publisher. The company was founded in 1991 as a merger between Lion Rampant and "White Wolf Magazine", and was initially led by Mark Rein·Hagen of the former and Steve Wieck and Stewart Wieck of the latter. Since White Wolf Publishing, Inc. merged with CCP Games in 2006, White Wolf Publishing has been an imprint of CCP hf, but has ceased in-house production of any material, instead licensing their properties to other publishers. It was announced in October 2015 that White Wolf had been acquired from CCP by Paradox Interactive. The name "White Wolf" originates from Michael Moorcock's works.
Overview.
White Wolf published a line of several different but overlapping games set in the "World of Darkness", a "modern gothic" world that, while seemingly similar to the real world, is home to supernatural terrors, ancient conspiracies, and several approaching apocalypses. The company also published the high fantasy "Exalted" RPG, the modern mythic "Scion", and d20 system material under their Sword & Sorcery imprint, including such titles as the "Dungeons & Dragons" gothic horror campaign setting "Ravenloft", and Monte Cook's "Arcana Unearthed" series. In order to complement the "World of Darkness" game line, a LARP system dubbed "Mind's Eye Theatre" has been published.
White Wolf also released several series of novels based on the Old World of Darkness, all of which are currently out of print (although many are coming back into availability via print-on-demand).
White Wolf also ventured in the collectible card game market with "Arcadia", "Rage", and ' (formerly "Jyhad"). "V:TES", perhaps the most successful card game, was originally published by Wizards of the Coast in 1994, but was abandoned just two years later after a revamped base set, name change and three expansions were published. White Wolf acquired the rights to the game in 2000, even though no new material had been produced for the game in over four years. Since then, several "V:TES" expansions have been released, and the game was the only official source of material for the Old World of Darkness, until 2011 when the 20th Anniversary Edition of ' was published and the Onyx Path was announced.
Video games such as ' and ' are based on White Wolf's role-playing game "". There are also several .
Merger and MMO.
On Saturday, 11 November 2006, White Wolf and CCP Games, the Icelandic MMO development company responsible for EVE Online, announced a merger between the two companies during the keynote address at the EVE Online Fanfest 2006. It was also revealed that a World of Darkness MMORPG was already in the planning stages. This game was cancelled in April 2014 after nine years of development.
Purchase By Paradox Interactive.
On Thursday, 29 October 2015, Paradox Interactive and CCP announced that Paradox had purchased White Wolf and all of its intellectual properties. Tobias Sjögren will serve as the CEO of the revived company, which will remain a subsidiary of Paradox. Martin Ericsson, formerly a developer on the World of Darkness MMO, will be serving in the "Lead Storyteller" role for the company.
The "Old" or "Classic" World of Darkness game lines.
The games of this series use White Wolf's Storyteller System. Several games inspired spinoffs in the form of "historical" period settings such as the Dark Ages.
In addition to those game lines a series of books was produced under the title "World of Darkness". These provided stand-alone materials for multiple game lines with the focus on a specific region or theme, e.g. "WoD: Blood dimmed Tides" (about the oceans), "WoD: Combat" (an alternative 'crossover' combat system to resolve contradictory mechanics and add some sophistication), "WoD: Tokyo" and "WoD: Mafia".
For the Third Edition of "Ars Magica", White Wolf hitched that game's pseudo-historical setting to the 'future' "World of Darkness" setting. This was a simple adjustment (since the core premise of both settings is 'Earth as we know it' + 'supernatural fiction is reality') and particularly suited to the 'Tremere connection' between a clan of vampires from the original "Vampire" and a House of magi in the "Order of Hermes" (the central organization of "Ars Magica" as well as one of the 'Traditions' in ")".
The Chronicles of Darkness game lines.
The games of this series use White Wolf's newer Storytelling System. For over a decade it was also known as "World of Darkness," causing it to be referred to as the "new World of Darkness" or nWoD to distinguish it from the prior line of games.
Mind's Eye Theatre (LARP).
The majority of the Old World of Darkness games were adapted into the original Mind's Eye Theatre format for live-action roleplaying. Product lines in this era include:
Subsequently, the Mind's Eye Theatre was revamped for the New World of Darkness. A core "Mind's Eye Theatre" rulebook was published as the LARP analogue to the "World of Darkness" core rulebook, with several Mind's Eye Theatre adaptations following in suit: "The Requiem", "The Forsaken", and "The Awakening" each adapted their respective namesakes to the new system of MET rules. The license to produce Mind's Eye Theatre content was acquired by By Night Studios in 2013.
Imprints and labels.
White Wolf has different imprints under which various books are published, most notably:
Black Dog Game Factory was also a fictional company in the World of Darkness, as detailed in the "Subsidiaries: A Guide to Pentex" game supplement.
The Onyx Path.
At GenCon 2012 it was announced that CCP Games/White Wolf would not continue to produce table-top RPGs . Onyx Path Publishing, a new company by White Wolf Creative Director Richard Thomas, purchased the Trinity games and Scion from CCP and became licensee for the production of World of Darkness titles (classic and new), as well as Exalted. Onyx Path does not, however, hold a license to the Mind's Eye Theatre titles.
By Night Studios.
At Midwinter Gaming Convention in 2013 it was announced that as a result of CCP Games discontinuation of publishing, By Night Studios had acquired the license to all Mind's Eye Theatre titles. In May 2013, By Night Studios launched a successful Kickstarter campaign to rebuild the Mind's Eye Theatre: Vampire The Masquerade product specifically for the Live-Action Role Play audience. By Night Studios is currently in development of Mind's Eye Theatre: Werewolf The Apocalypse in the same rebuilding fashion as Vampire The Masquerade.

</doc>
<doc id="34057" url="https://en.wikipedia.org/wiki?curid=34057" title="William Congreve">
William Congreve

William Congreve (24 January 1670 – 19 January 1729) was an English playwright and poet.
Early life.
William Congreve was born in Bardsey, West Yorkshire, England (near Leeds). His parents were William Congreve (1637–1708) and Mary ("née" Browning; 1636?–1715). The family moved to London in 1672. They relocated again in 1674 to the Irish port town of Youghal where his father served as a lieutenant in the British army. Congreve spent his childhood in Ireland, where his father, a Cavalier, had settled during the reign of Charles II. Congreve was educated at Kilkenny College where he met Jonathan Swift, who would be his friend for the remainder of his life; and at Trinity College in Dublin. Upon graduation, he matriculated in the Middle Temple in London to study law, but felt himself pulled toward literature, drama, and the fashionable life. Congreve assumed the pseudonym Cleophil and went on to publish a work he had written at the approximate age of 17 called "Incognita: or, Love and Duty reconcil'd" in 1692. This early work gained him recognition among the men of letters and an entrance into the literary world. Artistically, he became a disciple of John Dryden whom he met through the gatherings of literary circles held at Will's Coffeehouse in the Covent Garden District of London. John Dryden would continue to be a massive supporter of the works of Congreve throughout his life. This support would take the form of panegyrical introductions for some of Congreve's later works.
Literary career.
William Congreve is seen as the man who shaped the English comedy of manners through his use of satire and well written dialog. Congreve achieved fame in 1693 when he wrote some of the most popular English plays of the Restoration period. This period in time was unique because female roles were beginning to be played predominately by women. This shift in allowing women to perform could be seen in the works of Congreve. One of the favorites of the playwright was a women by the name of Mrs. Anne Bracegirdle. She was the actress that went on to hold many of the female lead roles in the works of Congreve. His first play, "The Old Bachelor", written, by his own account, to amuse himself during convalescence, was produced at the Drury Lane Theatre in 1693 and later produced by the Theatre Royale. This first play was hailed as an enormous success, and it ran for an entire two-week period when it first opened. Congreve's mentor John Dryden gave the production rave reviews and proclaimed it to be a brilliant first piece. The second play to be produced was called "Double-Dealer" which was not nearly as successful as the first production. By the age of thirty, he had written four comedies, including "Love for Love" (premiered 30 April 1695) staged in Lincoln's Inn Field which was nearly as well received as the first major success for Congreve, and "The Way of the World" (premiered March 1700) this play was an utter failure at the time of production but it is seen as one of his master pieces today, and it is still frequently revived. He also went on to create one tragedy, "The Mourning Bride" (1697) which was extremely popular at the time of creation but is now one of his least regarded dramas. After the production of "Love for Love" Congreve became one of the managers for the Lincoln's Inn Fields in 1695. During that time, he went on the write public occasional verse. As a result of his success and literary merit he was awarded one of the five positions of commissioner for licensing hackey coaches.
His playwrighting career was successful; however, it was also very brief. Five plays authored from 1693 to 1700 would prove the entirety of his output, as public tastes turned away from the sort of high-brow sexual comedy of manners in which he specialized. It is believed that Congreve was forced off of the stage due to growing concerns about the morality of the comedies that Congreve was producing. He reportedly was particularly stung by a critique written by Jeremy Collier ("A Short View of the Immorality and Profaneness of the English Stage"), to the point that he wrote a long reply, "Amendments of Mr. Collier's False and Imperfect Citations." Although no longer on the stage Congreve continued his literary art. He wrote the librettos for two operas that were being created at the time, and he also went on to begin translating the works of Molière. As a member of the Whig Kit-Kat Club, Congreve's career shifted to the political sector, where he held various minor political positions, including being named Secretary of the Island of Jamaica by George I in 1714, in spite of being a Whig among Tories. Regardless of this carrier change Congreve continued his writing, although it was in a very different style. During his time in Jamaica his works now took the form of poetry instead of full length dramatic productions. During that time, he did a vast amount of translations as well, including works of Homer, Juvenal, Ovid, and Horace.
Later life.
Congreve withdrew from the theatre and lived the rest of his life on residuals from his early work, the royalties received when his plays were produced, as well as his private income. His output from 1700 was restricted to the occasional poem and some translation (notably Molière's "Monsieur de Pourceaugnac"). Congreve never married; in his own era and through subsequent generations, he was famous for his friendships with prominent actresses and noblewomen for whom he wrote major parts in all his plays.These women included Anne Bracegirdle and Henrietta Godolphin, 2nd Duchess of Marlborough, daughter of the famous general, John Churchill, 1st Duke of Marlborough. Congreve and Henrietta probably met by 1703 and the duchess had a daughter, Mary (1723–1764), who was believed to be his. Upon his death he left his entire fortune to the duchess of Marlborough.
As early as 1710, he suffered both from gout and from cataracts on his eyes. Congreve suffered a carriage accident in late September 1728, from which he never recovered (having probably received an internal injury); he died in London in January 1729, and was buried in Poets' Corner in Westminster Abbey.
Famous lines.
Two of Congreve's turns of phrase from "The Mourning Bride" (1697) have become famous, albeit frequently in misquotation, and often misattributed to William Shakespeare:
Congreve coined another famous phrase in "Love for Love" (1695):
References in popular culture.
A fictitious play by Congreve, "The Gallivant", features prominently in the novel "Flowers for the Judge" by Margery Allingham.
In British TV series The Hustle in first season fifth episode, main protagonists attempt to pull out a con concerning bitter woman named after Congreve.

</doc>
<doc id="34059" url="https://en.wikipedia.org/wiki?curid=34059" title="War of 1812">
War of 1812

The War of 1812 was a military conflict that lasted from June 18, 1812 to February 18, 1815, fought between the United States of America and Great Britain, its North American colonies, and its North American Indian allies. Historians in the United States and Canada see it as a war in its own right, but Europeans often see it as a minor theatre of the Napoleonic Wars. By the war's end in early 1815 the key issues had been resolved and peace came with no boundary changes.
The United States declared war for several reasons, including trade restrictions brought about by the British war with France, the impressment of as many as 10,000 American merchant sailors into the Royal Navy, British support for Native American tribes fighting American settlers on the frontier, outrage over insults to national honor during the Chesapeake–Leopard Affair, and American interest in annexing British territory. The primary British war goal was to minimize American trade with France, and to defend their North American colonies; they also hoped to set up a neutral Indian buffer state in the Midwest that would impede American expansion in the Old Northwest.
The war was fought in three theatres. First, at sea, warships and privateers of each side attacked the other's merchant ships, while the British blockaded the Atlantic coast of the United States and mounted large raids in the later stages of the war. Second, land and naval battles were fought on the U.S.–Canadian frontier. Third, large-scale battles were fought in the Southern United States and Gulf Coast. At the end of the war, both sides signed and ratified the Treaty of Ghent and, in accordance with the treaty, returned occupied land, prisoners of war and captured ships (though neither side returned the other's warships due to frequent re-commissioning upon capture) to its pre-war owner and resumed friendly trade relations without restriction.
With the majority of its land and naval forces tied down in Europe fighting the Napoleonic Wars, the British used a defensive strategy until 1814. Early victories over poorly-led U.S. armies demonstrated that the conquest of the Canadas would prove more difficult than anticipated. Despite this, the U.S. was able to inflict serious defeats on Britain's Native American allies, ending the prospect of an Indian confederacy and an independent Native American state in the Midwest under British sponsorship. U.S. forces took control of Lake Erie in 1813, and seized western parts of Upper Canada. However, an American attempt to capture Montreal was repulsed in November 1813. Despite the major U.S. victory at Chippawa on July 5, 1814, serious attempts to fully conquer Upper Canada were ultimately abandoned following the bloody Battle of Lundy's Lane on July 25, 1814.
In April 1814, with the defeat of Napoleon, Britain now had large, seasoned armies to use. It adopted a more aggressive strategy, sending large invasion armies and tightening their naval blockade. However, with the end of the Napoleonic Wars, both governments were eager for a return to normality and peace negotiations began in Ghent in August 1814. In the Deep South, General Andrew Jackson destroyed the military strength of the Muscogee (Creek) Nation at the Battle of Horseshoe Bend. In September 1814, the British won the Battle of Hampden, allowing them to occupy eastern Maine, and the British victory at the Battle of Bladensburg in August 1814 allowed them to capture and burn Washington, D.C. They were repulsed, however, in an attempt to take Baltimore and Fort Bowyer, and during their assault at Fayal. An American victory in September 1814 at the Battle of Plattsburgh repulsed the British invasions of New York, which, along with pressure from merchants on the British government, prompted British diplomats to drop their demands at Ghent for an independent native buffer state and territorial claims that London previously sought. Given that it took six weeks for ships to cross the Atlantic, news of the peace treaty did not arrive before the British suffered a major defeat at New Orleans in January 1815.
In the United States, late victories over invading British armies at the battles of Plattsburg, Baltimore (inspiring the United States national anthem, "The Star-Spangled Banner") and New Orleans produced a sense of euphoria over a "second war of independence" against Britain. The war ended on a high note for Americans, winning the final major engagements of the war and bringing an "Era of Good Feelings" in which partisan animosity nearly vanished in the face of strengthened American nationalism. The war was also a major turning point in the development of the U.S. military. The poor performance of several U.S. militia units, particularly during the 1812–13 invasions of Canada and the 1814 defence of Washington, convinced the U.S. government of the need to move away from its Revolutionary-era reliance on militia and focus on creating a more professional regular force. Spain was involved in fighting in Florida but was not an official belligerent; some Spanish forces fought alongside the British during the Occupation of Pensacola. The U.S. took permanent ownership of Spain's Mobile District.
In Upper and Lower Canada, British and local Canadian militia victories over invading U.S. armies became iconic and promoted the development of a distinct Canadian identity, which included strong loyalty to Britain. Today, particularly in Ontario, memory of the war retains its significance, because the defeat of the invasions ensured that the Canadas would remain part of the British Empire, rather than be annexed by the United States. In Canada, numerous ceremonies took place in 2012 to commemorate the war, offer historical lessons and celebrate 200 years of peace across the border. The conflict has not been commemorated on nearly the same level in the modern-day United States, though it is still taught as an important part of early American history, and Dolley Madison's and Andrew Jackson's respective roles in the war are especially emphasized. The war is scarcely remembered in Britain, being heavily overshadowed by the much larger Napoleonic Wars occurring in Europe.
Origins.
Historians have long debated the relative weight of the multiple reasons underlying the origins of the War of 1812. This section summarizes several contributing factors which resulted in the declaration of war by the United States.
Honor and the second war of independence.
As Risjord (1961) notes, a powerful motivation for the Americans was the desire to uphold national honor in the face of what they considered to be British insults such as the "Chesapeake" affair. Brands says, "The other war hawks spoke of the struggle with Britain as a second war of independence; Jackson, who still bore scars from the first war of independence held that view with special conviction. The approaching conflict was about violations of American rights, but it was also about vindication of American identity". People at the time and historians since often called it America's "Second War of Independence".
Trade with France.
In 1807, Britain introduced a series of trade restrictions via a series of Orders in Council to impede neutral trade with France, with which Britain was at war. The United States contested these restrictions as illegal under international law. Also, historian Reginald Horsman states, "a large section of influential British opinion, both in the government and in the country, thought that America presented a threat to British maritime supremacy".
The American merchant marine had come close to doubling between 1802 and 1810, making it by far the largest neutral fleet. Britain was the largest trading partner, receiving 80% of U.S. cotton and 50% of other U.S. exports. The British public and press were resentful of the growing mercantile and commercial competition. The United States' view was that Britain's restrictions violated its right to trade with others.
Impressment.
During the Napoleonic Wars, the Royal Navy expanded to 176 ships of the line and 600 ships overall, requiring 140,000 sailors to man. While the Royal Navy could man its ships with volunteers in peacetime, it competed in wartime with merchant shipping and privateers for a small pool of experienced sailors and turned to impressment when it could not operate ships with volunteers alone. Britain did not recognize the right of a British subject to relinquish his status as a British subject, emigrate and transfer his national allegiance as a naturalized citizen to any other country. Thus while the United States recognized British-born sailors on American ships as Americans, Britain did not. It was estimated that there were 11,000 naturalized sailors on United States ships in 1805. Secretary of the Treasury Albert Gallatin stated that 9,000 were born in Britain. The Royal Navy went after them by intercepting and searching U.S. merchant ships for deserters. Impressment actions such as the "Leander" Affair and the "Chesapeake"–"Leopard" Affair outraged Americans, because they infringed on national sovereignty and denied America's ability to naturalize foreigners. Moreover, a great number of British sailors serving as naturalized Americans on U.S. ships were Irish. An investigation by Captain Isaac Chauncey in 1808 found that 58% of the sailors based in New York City were either naturalized citizens or recent immigrants, the majority of foreign sailors (134 of 150) being from Britain. Moreover, eighty of the 134 British sailors were Irish. The U.S. Navy also forcibly recruited British sailors but the British government saw impressment as commonly accepted practice and preferred to rescue British sailors from American impressment on a case-by-case basis.
The United States believed that British deserters had a right to become U.S. citizens. Britain did not recognize naturalized United States citizenship, so in addition to recovering deserters, it considered United States citizens born British liable for impressment. Aggravating the situation was the widespread use of forged identity or protection papers by sailors. This made it difficult for the Royal Navy to distinguish Americans from non-Americans and led it to impress some Americans who had never been British. (Some gained freedom on appeal.) American anger at impressment grew when British frigates were stationed just outside U.S. harbors in view of U.S. shores and searched ships for contraband and impressed men while in U.S. territorial waters. "Free trade and sailors' rights" was a rallying cry for the United States throughout the conflict.
British support for American Indian raids.
The Northwest Territory, comprising the modern states of Ohio, Indiana, Illinois, Michigan, and Wisconsin, was the battleground for conflict between the Indian Nations and the United States. The British Empire had ceded the area to the United States in the Treaty of Paris in 1783, both sides ignoring the fact that the land was already inhabited by various Indian nations. These included the Miami, Winnebago, Shawnee, Fox, Sauk, Kickapoo, Delaware and Wyandot. Some warriors, who had left their nations of origin, followed Tenskwatawa, the Shawnee Prophet and the brother of Tecumseh. Tenskwatawa had a vision of purifying his society by expelling the "children of the Evil Spirit": the American settlers. Tenskwatawa and Tecumseh formed a confederation of numerous tribes to block American expansion. The British saw the Indian nations as valuable allies and a buffer to its Canadian colonies and provided arms. Attacks on American settlers in the Northwest further aggravated tensions between Britain and the United States. Raiding grew more common in 1810 and 1811; Westerners in Congress found the raids intolerable and wanted them permanently ended.
The confederation's raids and existence hindered American expansion into rich farmlands in the Northwest Territory. Pratt writes:
However, according to the U.S Army Center of Military History, the "land-hungry frontiersmen", with "no doubt that their troubles with the Indians were the result of British intrigue", exacerbated the problem by stories] after every Indian raid of British Army muskets and equipment being found on the field". Thus, "the westerners were convinced that their problems could best be solved by forcing the British out of Canada".
The British had the long-standing goal of creating a large "neutral" Indian state that would cover much of Ohio, Indiana, and Michigan. They made the demand as late as the fall of 1814 at the peace conference, but lost control of western Ontario in 1813 at key battles on and around Lake Erie. These battles destroyed the Indian confederacy which had been the main ally of the British in that region, weakening its negotiating position. Although the area remained under British or British-allied Indians' control until the end of the war, the British, at American insistence and with higher priorities, dropped the demands.
American expansionism.
American expansion into the Northwest Territory was being obstructed by indigenous leaders such as Tecumseh, who were supplied and encouraged by the British. Americans on the western frontier demanded that interference be stopped. There is dispute, however, over whether or not the American desire to annex Canada brought on the war. Several historians believe that the capture of Canada was intended only as a means to secure a bargaining chip, which would then be used to force Britain to back down on the maritime issues. It would also cut off food supplies for Britain's West Indian colonies, and temporarily prevent the British from continuing to arm the Indians. However, many historians believe that a desire to annex Canada was a cause of the war. This view was more prevalent before 1940, but remains widely held today. Congressman Richard Mentor Johnson told Congress that the constant Indian atrocities along the Wabash River in Indiana were enabled by supplies from Canada and were proof that "the war has already commenced. ... I shall never die contented until I see England's expulsion from North America and her territories incorporated into the United States."
Upper Canada (modern southern Ontario) had been settled mostly by Revolution-era exiles from the United States (United Empire Loyalists) or postwar American immigrants. The Loyalists were hostile to union with the United States, while the immigrant settlers were generally uninterested in politics and remained neutral or supported the British during the war. The Canadian colonies were thinly populated and only lightly defended by the British Army. Americans then believed that many men in Upper Canada would rise up and greet an American invading army as liberators. That did not happen. One reason American forces retreated after one successful battle inside Canada was that they could not obtain supplies from the locals. But the Americans thought that the possibility of local support suggested an easy conquest, as former President Thomas Jefferson believed: "The acquisition of Canada this year, as far as the neighborhood of Quebec, will be a mere matter of marching, and will give us the experience for the attack on Halifax, the next and final expulsion of England from the American continent".
Annexation was supported by American border businessmen who wanted to gain control of Great Lakes trade.
Carl Benn noted that the War Hawks' desire to annex the Canadas was similar to the enthusiasm for the annexation of Spanish Florida by inhabitants of the American South; both expected war to facilitate expansion into long-desired lands and end support for hostile Indian tribes (Tecumseh's Confederacy in the North and the Creek in the South). 
Stagg has examined the fate of the expansionist cause proposed by Hacker and Pratt in the 1920s:
Maass argued in 2015 that the expansionist theme is a myth that goes against the "relative consensus among experts that the primary U.S. objective was the repeal of British maritime restrictions". He argues that consensus among scholars is that the United States went to war "because six years of economic sanctions had failed to bring Britain to the negotiating table, and threatening the Royal Navy's Canadian supply base was their last hope." Maass agrees that theoretically expansionism might have tempted Americans, but finds that "leaders feared the domestic political consequences of doing so. Notably, what limited expansionism there was focused on sparsely populated western lands rather than the more populous eastern settlements Canada."
Horsman argued expansionism played a role as a secondary cause after maritime issues, noting that many historians have mistakenly rejected expansionism as a cause for the war. "In disagreeing with those interpretations that have simply stressed expansionism and minimized maritime causation, historians have ignored deep-seated American fears for national security, dreams of a continent completely controlled by the republican United States, and the evidence that many Americans believed that the War of 1812 would be the occasion for the United States to achieve the long-desired annexation of Canada." He notes that it was considered key to maintaining sectional balance between free and slave states thrown off by American settlement of the Louisiana Territory, and widely supported by dozens of War Hawk congressmen such as John A. Harper, Felix Grundy, and Richard M. Johnson, who voted for war with expansion as a key aim. He further notes that American majority opinion at the time was "that the cession of Canada ... must be a sine qua non indispensable condition at a treaty of peace". 
Alan Taylor argues that many Republican congressmen, such as Richard M. Johnson, John A. Harper and Peter B. Porter, "longed to oust the British from the continent and to annex Canada." Southern Republicans largely opposed this, fearing an imbalance of free and slave states if Canada was annexed, while anti-Catholicism also caused many to oppose annexing mainly Catholic Lower Canada, believing its French-speaking inhabitants "unfit ... for republican citizenship". Even major figures such as Henry Clay and James Monroe expected to keep at least Upper Canada in the event of an easy conquest. Notable American generals, like William Hull were led by this sentiment to issue proclamations to Canadians during the war promising republican liberation through incorporation into the United States; a proclamation the government never officially disavowed. General Alexander Smyth similarly declared to his troops that when they invaded Canada "You will enter a country that is to become one of the United States. You will arrive among a people who are to become your fellow-citizens." A lack of clarity about American intentions undercut these appeals, however.
David and Jeanne Heidler argue that "Most historians agree that the War of 1812 was not caused by expansionism but instead reflected a real concern of American patriots to defend United States' neutral rights from the overbearing tyranny of the British Navy. That is not to say that expansionist aims would not potentially result from the war." They point out that "acquiring Canada would satisfy America's expansionist desires", also describing it as a key goal of western expansionists, who, they argue, believed that "eliminating the British presence in Canada would best accomplish" their goal of halting British support for Indian raids. They argue that the "enduring debate" is over the relative importance of expansionism as a factor, and whether "expansionism played a greater role in causing the War of 1812 than American concern about protecting neutral maritime rights." 
U.S. political conflict.
While the British government was largely oblivious to the deteriorating North American situation because of its involvement in a continent-wide European War, the U.S. was in a period of significant political conflict between the Federalist Party (based mainly in the Northeast), which favored a strong central government and closer ties to Britain, and the Democratic-Republican Party (with its greatest power base in the South and West), which favored a weak central government, preservation of slavery, expansion into Indian land, and a stronger break with Britain. By 1812, the Federalist Party had weakened considerably, and the Republicans, with James Madison completing his first term of office and control of Congress, were in a strong position to pursue their more aggressive agenda against Britain. Throughout the war, support for the U.S. cause was weak (or sometimes non-existent) in Federalist areas of the Northeast. Few men volunteered to serve; the banks avoided financing the war. The negativism of the Federalists, especially as exemplified by the Hartford Convention of 1814–15 ruined its reputation and the Party survived only in scattered areas. By 1815 there was broad support for the war from all parts of the country. This allowed the triumphant Republicans to adopt some Federalist policies, such as a national bank, which Madison reestablished in 1816.
Declaration of war.
On June 1, 1812, President James Madison sent a message to Congress recounting American grievances against Great Britain, though not specifically calling for a declaration of war. After Madison's message, the House of Representatives deliberated for four days behind closed doors before voting 79 to 49 (61% in favor) the first declaration of war, and the Senate agreed by 19 to 13 (59% in favor). The conflict began formally on June 18, 1812, when Madison signed the measure into law and proclaimed it the next day. This was the first time that the United States had declared war on another nation, and the Congressional vote would prove to be the closest vote to formally declare war in American history. (The Authorization for Use of Military Force Against Iraq Resolution of 1991, while not a formal declaration of war, was a closer vote.) None of the 39 Federalists in Congress voted in favor of the war; critics of war subsequently referred to it as "Mr. Madison's War".
Earlier in London on May 11, an assassin had killed Prime Minister Spencer Perceval, which resulted in Lord Liverpool coming to power. Liverpool wanted a more practical relationship with the United States. On June 23, he issued a repeal of the Orders in Council, but the United States was unaware of this, as it took three weeks for the news to cross the Atlantic. On June 28, 1812, was despatched from Halifax under a flag of truce to New York. On July 9, she anchored off Sandy Hook, and three days later sailed on her return with a copy of the declaration of war; the British ambassador, Mr. Foster; and consul, Colonel Barclay. She arrived in Halifax, Nova Scotia eight days later. The news of the declaration took even longer to reach London. In response to the U.S. declaration of war, Isaac Brock issued a proclamation alerting the citizenry in Upper Canada of the state of war and urging all military personnel "to be vigilant in the discharge of their duty" to prevent communication with the enemy and to arrest anyone suspected of helping the Americans.
Course of the war.
Although the outbreak of the war had been preceded by years of angry diplomatic dispute, neither side was ready for war when it came. Britain was heavily engaged in the Napoleonic Wars, most of the British Army was deployed in the Peninsular War (in Portugal and Spain), and the Royal Navy was compelled to blockade most of the coast of Europe. The number of British regular troops present in Canada in July 1812 was officially stated to be 6,034, supported by Canadian militia. Throughout the war, the British Secretary of State for War and the Colonies was the Earl of Bathurst. For the first two years of the war, he could spare few troops to reinforce North America and urged the commander-in-chief in North America (Lieutenant General Sir George Prévost) to maintain a defensive strategy. The naturally cautious Prévost followed these instructions, concentrating on defending Lower Canada at the expense of Upper Canada (which was more vulnerable to American attacks) and allowing few offensive actions.
The United States was not prepared to prosecute a war, for Madison had assumed that the state militias would easily seize Canada and that negotiations would follow. In 1812, the regular army consisted of fewer than 12,000 men. Congress authorized the expansion of the army to 35,000 men, but the service was voluntary and unpopular; it offered poor pay, and there were few trained and experienced officers, at least initially. The militia objected to serving outside their home states, were not open to discipline, and performed poorly against British forces when outside their home states. American prosecution of the war suffered from its unpopularity, especially in New England, where anti-war speakers were vocal. "Two of the Massachusetts members Congress, Seaver and Widgery, were publicly insulted and hissed on Change in Boston; while another, Charles Turner, member for the Plymouth district, and Chief-Justice of the Court of Sessions for that county, was seized by a crowd on the evening of August 3, and kicked through the town". The United States had great difficulty financing its war. It had disbanded its national bank, and private bankers in the Northeast were opposed to the war. The United States was able to obtain financing from London-based Barings Bank to cover overseas bond obligations. The failure of New England to provide militia units or financial support was a serious blow. Threats of secession by New England states were loud, as evidenced by the Hartford Convention. Britain exploited these divisions, blockading only southern ports for much of the war and encouraging smuggling.
On July 12, 1812, General William Hull led an invading American force of about 1,000 untrained, poorly equipped militia across the Detroit River and occupied the Canadian town of Sandwich (now a neighborhood of Windsor, Ontario). By August, Hull and his troops (numbering 2,500 with the addition of 500 Canadians) retreated to Detroit, where they surrendered to a significantly smaller force of British regulars, Canadian militia and Native Americans, led by British Major General Isaac Brock and Shawnee leader Tecumseh. The surrender not only cost the United States the village of Detroit, but control over most of the Michigan Territory. Several months later, the U.S. launched a second invasion of Canada, this time at the Niagara peninsula. On October 13, United States forces were again defeated at the Battle of Queenston Heights, where General Brock was killed.
Military and civilian leadership remained a critical American weakness until 1814. The early disasters brought about chiefly by American unpreparedness and lack of leadership drove United States Secretary of War William Eustis from office. His successor, John Armstrong, Jr., attempted a coordinated strategy late in 1813 (with 10,000 men) aimed at the capture of Montreal, but he was thwarted by logistical difficulties, uncooperative and quarrelsome commanders and ill-trained troops. After losing several battles to inferior forces, the Americans retreated in disarray in October 1813. Further complicating the American forces was the logistics situation and remained so throughout the Upper Canadian theatre of war. American supplies were having to be brought over a poor road through the Black Marsh area in winter. British forces could rely upon supply ships except for the winter months. Contractors were relied upon to supply American forces and often delivered rotting meat and similar short cuts. If unable to bring the supplies American contractors were liable to declare bankruptcy leaving troops to starve. Despite requests for a quartermaster system to be set up no action was forthcoming. The local farms on both sides of the borders were mostly isolated farmsteads barely above the subsistence level. Both sides would relentlessly press farmers far more for supplies than they were prepared to render while taking their horses and wagons. This further crippled farming in the area.
A decisive use of naval power came on the Great Lakes and depended on a contest of building ships. The U.S. started a rapidly expanded program of building warships at Sackets Harbor on Lake Ontario, where 3,000 men were recruited, many from New York City, to build 11 warships early in the war. In 1813, the Americans won control of Lake Erie in the Battle of Lake Erie and cut off British and Native American forces in the west from their supply base; they were decisively defeated by General William Henry Harrison's forces on their retreat towards Niagara at the Battle of the Thames in October 1813. Tecumseh, the leader of the tribal confederation, was killed and his Indian coalition disintegrated. While some natives continued to fight alongside British troops, they subsequently did so only as individual tribes or groups of warriors, and where they were directly supplied and armed by British agents. The Americans controlled western Ontario, and permanently ended the threat of Indian raids supplied by the British in Canada into the American Midwest, thus achieving a basic war goal. Raids would continue from the unsubdued Indian tribes in the Old Northwest, which remained under British/Indian control, until the end of the war. Control of Lake Ontario changed hands several times, with both sides unable and unwilling to take advantage of temporary superiority.
At sea, the powerful Royal Navy blockaded much of the coastline, though it was allowing substantial exports from New England, which traded with Canada in defiance of American laws. The blockade devastated American agricultural exports, but it helped stimulate local factories that replaced goods previously imported. The American strategy of using small gunboats to defend ports was a fiasco, as the British raided the coast at will. The most famous episode was a series of British raids on the shores of Chesapeake Bay, including an attack on Washington that resulted in the British burning of the White House, the Capitol, the Navy Yard, and other public buildings, in the "Burning of Washington". The British power at sea was enough to allow the Royal Navy to levy "contributions" on bayside towns in return for not burning them to the ground. The Americans were more successful in ship-to-ship actions. They sent out several hundred privateers to attack British merchant ships; in the first four months of war they captured 219 British merchant ships. British commercial interests were damaged, especially in the West Indies.
After Napoleon abdicated on April 6, 1814, the British could send veteran armies to the United States, but by then the Americans had learned how to mobilize and fight. British General Prévost launched a major invasion of Upstate New York with these veteran soldiers, but the American fleet under Thomas Macdonough gained control of Lake Champlain and the British lost the Battle of Plattsburgh in September 1814. Prévost, blamed for the defeat, sought a court-martial to clear his name, but he died in London awaiting it. The British then launched a successful attack on Chesapeake Bay, capturing, and burning Washington, looting Alexandria, and unsuccessfully attacking Baltimore. The embarrassing Burning of Washington led to Armstrong's dismissal as U.S. Secretary of War. A British invasion of Louisiana (unknowingly launched after the Treaty of Ghent was negotiated to end the war) was defeated with heavy British losses by General Andrew Jackson at the Battle of New Orleans in January 1815. The victory made Jackson a national hero, restored the American sense of honor, and ruined the Federalist party's efforts to condemn the war as a failure. With the ratification of the peace treaty in February 1815, the war ended before the U.S. new Secretary of War James Monroe could put his new offensive strategy into effect, and before the British could launch renewed attacks.
Once Britain and The Sixth Coalition defeated Napoleon in 1814, France and Britain became close allies. Britain ended the trade restrictions and the impressment of American sailors, thus removing two more causes of the war. After two years of warfare, the major causes of the war had disappeared. Neither side had a reason to continue or a chance of gaining a decisive success that would compel their opponents to cede territory or advantageous peace terms. As a result of this stalemate, the two countries signed the Treaty of Ghent on December 24, 1814. News of the peace treaty took two months to reach North America, during which fighting continued. The war fostered a spirit of national unity and an "Era of Good Feelings" in the United States, as well as in Canada. It opened a long era of peaceful relations between the United States and the British.
Theatres of war.
The war was conducted in three theatres:
Atlantic theatre.
Opening strategies.
In 1812, Britain's Royal Navy was the world's largest, with over 600 cruisers in commission and some smaller vessels. Although most of these were involved in blockading the French navy and protecting British trade against (usually French) privateers, the Royal Navy still had 85 vessels in American waters, counting all British Navy vessels in North American and the Caribbean waters. But, the Royal Navy's North American squadron based in Halifax, Nova Scotia, which bore the brunt of the war, numbered one small ship of the line, seven frigates, nine smaller sloops and brigs along with five schooners. By contrast, the United States Navy comprised 8 frigates, 14 smaller sloops and brigs, and no ships of the line. The U.S. had embarked on a major shipbuilding program before the war at Sackets Harbor, New York and continued to produce new ships. Three of the existing American frigates were exceptionally large and powerful for their class, larger than any British frigate in North America. Whereas the standard British frigate of the time was rated as a 38 gun ship, usually carrying up to 50 guns, with its main battery consisting of 18-pounder guns; USS "Constitution", "President", and "United States", in comparison, were rated as 44-gun ships, carrying 56–60 guns with a main battery of 24-pounders.
The British strategy was to protect their own merchant shipping to and from Halifax, Nova Scotia, and the West Indies, and to enforce a blockade of major American ports to restrict American trade. Because of their numerical inferiority, the American strategy was to cause disruption through hit-and-run tactics, such as the capture of prizes and engaging Royal Navy vessels only under favorable circumstances. Days after the formal declaration of war, however, it put out two small squadrons, including the frigate "President" and the sloop under Commodore John Rodgers, and the frigates "United States" and , with the brig under Captain Stephen Decatur. These were initially concentrated as one unit under Rodgers, who intended to force the Royal Navy to concentrate its own ships to prevent isolated units being captured by his powerful force.
Large numbers of American merchant ships were returning to the United States with the outbreak of war, and if the Royal Navy was concentrated, it could not watch all the ports on the American seaboard. Rodgers' strategy worked, in that the Royal Navy concentrated most of its frigates off New York Harbor under Captain Philip Broke, allowing many American ships to reach home. But, Rodgers' own cruise captured only five small merchant ships, and the Americans never subsequently concentrated more than two or three ships together as a unit.
Single-ship actions.
Meanwhile, "Constitution", commanded by Captain Isaac Hull, sailed from Chesapeake Bay on July 12. On July 17, Broke's British squadron gave chase off New York, but "Constitution" evaded her pursuers after two days. After briefly calling at Boston to replenish water, on August 19, "Constitution" engaged the British frigate . After a 35-minute battle, "Guerriere" had been dis-masted and captured and was later burned. "Constitution" earned the nickname "Old Ironsides" following this battle as many of the British cannonballs were seen to bounce off her hull. Hull returned to Boston with news of this significant victory. On October 25, "United States", commanded by Captain Decatur, captured the British frigate , which he then carried back to port. At the close of the month, the "Constitution" sailed south, now under the command of Captain William Bainbridge. On December 29, off Bahia, Brazil, she met the British frigate . After a battle lasting three hours, "Java" struck her colors and was burned after being judged unsalvageable. "Constitution", however, was relatively undamaged in the battle.
The successes gained by the three big American frigates forced Britain to construct five 40-gun, 24-pounder heavy frigates and two "spar-decked" frigates (the 60-gun and ) and to razee three old 74-gun ships of the line to convert them to heavy frigates. The Royal Navy acknowledged that there were factors other than greater size and heavier guns. The United States Navy's sloops and brigs had also won several victories over Royal Navy vessels of approximately equal strength. While the American ships had experienced and well-drilled volunteer crews, the enormous size of the overstretched Royal Navy meant that many ships were shorthanded and the average quality of crews suffered. The constant sea duties of those serving in North America interfered with their training and exercises.
The capture of the three British frigates stimulated the British to greater exertions. More vessels were deployed on the American seaboard and the blockade tightened. On June 1, 1813, off Boston Harbor, the frigate , commanded by Captain James Lawrence, was captured by the British frigate under Captain Philip Broke. Lawrence was mortally wounded and famously cried out, "Don't give up the ship! Hold on, men!" The two frigates were of near-identical size. "Chesapeake"s crew was larger but most had not served or trained together. British citizens reacted with celebration and relief that the run of American victories had ended. Notably, this action was by ratio one of the bloodiest contests recorded during this age of sail, with more dead and wounded than HMS "Victory" suffered in four hours of combat at Trafalgar. Captain Lawrence was killed and Captain Broke was so badly wounded that he never again held a sea command.
In January 1813, the American frigate , under the command of Captain David Porter, sailed into the Pacific to harass British shipping. Many British whaling ships carried letters of marque allowing them to prey on American whalers, and they nearly destroyed the industry. "Essex" challenged this practice. She inflicted considerable damage on British interests before she and her tender, (armed with twenty guns) were captured off Valparaiso, Chile, by the British frigate and the sloop on March 28, 1814.
The British 6th-rate s did not fare well against the American ship-rigged sloops of war. and constructed before the war were notably powerful vessels, and the "Frolic" class built during the war even more so (although was trapped and captured by a British frigate and a schooner). The British brig-rigged sloops tended to suffer fire to their rigging more frequently than the American ship-rigged sloops. In addition, the ship-rigged sloops could back their sails in action, giving them another advantage in manoeuvring.
Following their earlier losses, the British Admiralty instituted a new policy that the three American heavy frigates should not be engaged except by a ship of the line or smaller vessels in squadron strength. An example of this was the capture of "President" by a squadron of four British frigates in January 1815. But, a month later, "Constitution" engaged and captured two smaller British warships, and , sailing in company.
Success in single ship battles raised American morale after the repeated failed invasion attempts in Upper and Lower Canada. However these single ship victories had no military effect on the war at sea as they did not alter the balance of naval power, impede British supplies and reinforcements, or even raise insurance rates for British trade.
Privateering.
The operations of American privateers proved a more significant threat to British trade than the U.S. Navy. They operated throughout the Atlantic and continued until the close of the war, most notably from ports such as Baltimore. American privateers reported taking 1300 British merchant vessels, compared to 254 taken by the U.S. Navy. although the insurer Lloyd's of London reported that only 1,175 British ships were taken, 373 of which were recaptured, for a total loss of 802. However the British were able to limit privateering losses by the strict enforcement of convoy by the Royal Navy and by capturing 278 American privateers. Due to the massive size of the British merchant fleet, American captures only affected 7.5% of the fleet, resulting in no supply shortages or lack of reinforcements for British forces in North America.
Due to the large size of their navy, the British did not rely as much on privateering. The majority of the 1,407 captured American merchant ships were taken by the Royal Navy. The war was the last time the British allowed privateering, since the practice was coming to be seen as politically inexpedient and of diminishing value in maintaining its naval supremacy. However privateering remained popular in British colonies. It was the last hurrah for privateers in Bermuda who vigorously returned to the practice after experience in previous wars. The nimble Bermuda sloops captured 298 American ships. Privateer schooners based in British North America, especially from Nova Scotia took 250 American ships and proved especially effective in crippling American coastal trade and capturing American ships closer to shore than the Royal Navy cruisers.
Blockade.
The small British North American squadron had difficulty at the beginning of the war in blockading the entire U.S. coast, faced by the need to convoy vessels against American privateers. However, as additional ships were sent to North America in 1813, the Royal Navy was able to tighten the blockade and extend it, first to the coast south of Narragansett by November 1813 and to the entire American coast on May 31, 1814.
The British government, having need of American foodstuffs for its army in Spain, benefited from the willingness of the New Englanders to trade with them, so no blockade of New England was at first attempted. The Delaware River and Chesapeake Bay were declared in a state of blockade on December 26, 1812. Illicit trade was carried on by collusive captures arranged between American traders and British officers. American ships were fraudulently transferred to neutral flags. Eventually, the U.S. government was driven to issue orders to stop illicit trading; this put only a further strain on the commerce of the country. The overpowering strength of the British fleet enabled it to occupy the Chesapeake and to attack and destroy numerous docks and harbors.
The blockade of American ports later tightened to the extent that most American merchant ships and naval vessels were confined to port. The American frigates and "Macedonian" ended the war blockaded and hulked in New London, Connecticut. Some merchant ships were based in Europe or Asia and continued operations. Others, mainly from New England, were issued licences to trade by Admiral Sir John Borlase Warren, commander in chief on the American station in 1813. This allowed Wellington's army in Spain to receive American goods and to maintain the New Englanders' opposition to the war. The blockade nevertheless resulted in American exports decreasing from $130 million in 1807 to $7 million in 1814. Most of these were food exports that ironically went to supply their enemies in Britain or British colonies.
As the Royal Navy base that supervised the blockade, Halifax profited greatly during the war. From that base British privateers seized many French and American ships and sold their prizes in Halifax.
Freeing and recruiting slaves.
The British Royal Navy's blockades and raids allowed about 4,000 African Americans to escape slavery by fleeing American plantations to find freedom aboard British ships, migrants known, as regards those who settled in Canada, as the Black Refugees. The blockading British fleet in Chesapeake Bay received increasing numbers of enslaved black Americans during 1813. By British government order they were treated as free persons when reaching British hands. Alexander Cochrane's proclamation of April 2, 1814, invited Americans who wished to emigrate to join the British, and though not explicitly mentioning slaves was taken by all as addressed to them. About 2,400 of the escaped slaves and their families who served in the Royal Navy following their escape settled in Nova Scotia and New Brunswick during and after the war. From May 1814, younger men among the volunteers were recruited into a new Corps of Colonial Marines. They fought for Britain throughout the Atlantic campaign, including the Battle of Bladensburg and the attacks on Washington, D.C. and Battle of Baltimore, later settling in Trinidad after rejecting British government orders for transfer to the West India Regiments, forming the community of the Merikins. The slaves who escaped to the British represented the largest emancipation of African Americans before the American Civil War.
Occupation of Maine.
Maine, then part of Massachusetts, was a base for smuggling and illegal trade between the U.S. and the British. Until 1813 the region was generally quiet except for privateer actions near the coast. In September 1813, there was a notable naval action when the U.S. Navy's brig fought and captured the Royal Navy brig off Pemaquid Point. The first British assault came in July 1814, when Sir Thomas Masterman Hardy took Moose Island (Eastport, Maine) without a shot, with the entire American garrison of Fort Sullivan—which became the British Fort Sherbrooke—surrendering. Next, from his base in Halifax, Nova Scotia, in September 1814, Sir John Coape Sherbrooke led 3,000 British troops in the "Penobscot Expedition". In 26 days, he raided and looted Hampden, Bangor, and Machias, destroying or capturing 17 American ships. He won the Battle of Hampden (losing two killed while the Americans lost one killed). Retreating American forces were forced to destroy the frigate . The British occupied the town of Castine and most of eastern Maine for the rest of the war, re-establishing the colony of New Ireland. The Treaty of Ghent returned this territory to the United States, though Machias Seal Island has remained in dispute. The British left in April 1815, at which time they took ₤10,750 obtained from tariff duties at Castine. This money, called the "Castine Fund", was used to establish Dalhousie University, in Halifax, Nova Scotia.
Chesapeake campaign and "The Star-Spangled Banner".
The strategic location of the Chesapeake Bay near America's new national capital, Washington, D.C. on the major tributary of the Potomac River, made it a prime target for the British and their Royal Navy and the King's Army. Starting in March 1813, a squadron under Rear Admiral George Cockburn started a blockade of the mouth of the Bay at Hampton Roads harbor and raided towns along the Bay from Norfolk, Virginia, to Havre de Grace, Maryland.
On July 4, 1813, Commodore Joshua Barney, a Revolutionary War naval hero, convinced the U.S. Navy Department to build the Chesapeake Bay Flotilla, a squadron of twenty barges powered by small sails or oars (sweeps) to defend the Chesapeake Bay. Launched in April 1814, the squadron was quickly cornered in the Patuxent River, and while successful in harassing the Royal Navy, they were powerless to stop the British campaign that ultimately led to the "Burning of Washington". This expedition, led by Cockburn and General Robert Ross, was carried out between August 19 and 29, 1814, as the result of the hardened British policy of 1814 (although British and American commissioners had convened peace negotiations at Ghent in June of that year). As part of this, Admiral Warren had been replaced as commander in chief by Admiral Alexander Cochrane, with reinforcements and orders to coerce the Americans into a favorable peace.
Governor-in-chief of British North America in Upper and Lower Canada, Sir George Prévost had written to the Admirals on the North American Station in Bermuda, calling for retaliation for the American sacking and burning of York (now the city of Toronto on north shore of Lake Ontario). A force of 2,500 soldiers under General Ross had just arrived in Bermuda aboard """H.M.S. Royal Oak"", three frigates, three sloops and ten other vessels. Released from the Peninsular War in Spain and Portugal by British victory, the British intended to use them for diversionary raids along the coasts of Maryland and Virginia. In response to Prévost's request, they decided to employ this force, together with the naval and military units already on the station, to strike at the "Federal City" of Washington, D.C.
On August 24, U.S. Secretary of War, John Armstrong insisted that the British would attack Baltimore rather than Washington, even when units of the British Army, accompanied by major ships of the Royal Navy, were obviously on their way to the capital. The inexperienced American militia, which had congregated at Bladensburg, Maryland, to protect the capital, were defeated in the Battle of Bladensburg, opening the route to Washington. While First Lady Dolley Madison saved valuables from the then named "President's House" (or "President's Palace" mansion - now "White House"), Fourth President James Madison and the government with members of the Presidential Cabinet, fled to Virginia. Seeing that the Battle of Bladensburg, northeast of the town in rural Prince George's County was not going well, the Secretary of the Navy commanded Captain Thomas Tingey, commandant of the Washington Naval Yard on the Eastern Branch of the Potomac River (now the Anacostia River), to set the facility ablaze to prevent the capture of American naval ships, buildings, shops and supplies. Tingey had overseen the Naval Yard's planning and development since the national capital had been moved from Philadelphia to Washington in 1800, and waited until the very last possible minute, nearly four hours after the order was given to execute it. The destruction included most of the facility as well as the nearly-completed frigate ""Columbia"" and the sloop ""Argus"".
The British commanders ate the supper that had been prepared for the President and his departmental secretaries after returning from hopeful glorious U.S. victory, before they burned the Executive Mansion; American morale was reduced to an all-time low. The British viewed their actions as retaliation for the destructive American invasions and raids into Canada, most notably the Americans' burning of York earlier in 1813. Later that same evening, a furious storm (some later weather experts called it a thunderstorm, almost a hurricane) swept into Washington, D.C., sending one or more tornadoes into the rough, unfinished town that caused more damage but finally extinguished the fires with torrential rains, leaving fire-blackened walls and partial ruins of the President's House, The Capitol and Treasury Department that were set the first night. In addition, an explosion of the combustibles used to finish off the Navy Yard destruction that the Americans had started, exploded, killing or maiming a large number of "Red-Coats." Subsequently, the British left Washington, D.C. the following day after the storm subsided.
Having destroyed Washington's public buildings, including the President's Mansion and the Treasury, the British army and navy next moved several weeks later to capture Baltimore, forty miles northeast, a busy port and a key base for American privateers. However, by not immediately going overland to the port city they sneeringly called a "nest of pirates", but returned to their ships anchored in the Patuxent River and proceeding later up to the Upper Bay, gave the Baltimoreans plenty of time to reinforce their fortifications and gather regular U.S. Army and state militia troops from surrounding counties and states. The subsequent "Battle for Baltimore" began with the British landing on Sunday, September 12, 1814, at North Point, where the Baltimore harbor's Patapsco River met the Chesapeake Bay, where they were met by American militia further up the "Patapsco Neck" peninsula. An exchange of fire began, with casualties on both sides. Major Gen. Robert Ross was killed by American snipers as he attempted to rally his troops in the first skirmish. The snipers were killed moments later, and the British paused, then continued to march northwestward to the stationed Maryland and Baltimore City militia units deployed further up Long Log Lane on the peninsula at "Godly Wood" where the later Battle of North Point was fought for several afternoon hours in a musketry and artillery duel under command of British Col. Arthur Brooke and American commander for the Maryland state militia and its Third Brigade (or "Baltimore City Brigade"), Brig. Gen. John Stricker. The British also planned to simultaneously attack Baltimore by water on the following day, September 13, to support their military now arrayed facing the massed, heavily dug-in and fortified American units of approximately 15,000 with about a hundred cannon gathered along the eastern heights of the city named "Loudenschlager's Hill" (later "Hampstead Hill" - now part of Patterson Park). These overall Baltimore defences had been planned in advance and foreseen by the state militia commander, Maj. Gen. Samuel Smith, who had been set in charge of the Baltimore defences instead of the discredited U.S. Army commander for the Mid-Atlantic's 10th Military District (following the debacle the previous month at Bladensburg), William H. Winder. Smith had been earlier a Revolutionary War officer and commander, then wealthy city merchant and U.S. Representative, Senator and later Mayor of Baltimore. The "Red Coats" were unable to immediately reduce Fort McHenry, at the entrance to Baltimore Harbor to allow their ships to provide heavier naval gunfire to support their troops to the northeast.
At the bombardment of Fort McHenry, the British naval guns, mortars and revolutionary new "Congreve rockets" had a longer range than the American cannon onshore, and the ships mostly stood off out of the Americans' range, bombarding the fort, which returned very little fire and was not too heavily damaged during the onslaught except for a burst over a rear brickwall knocking out some fieldpieces and resulting in a few casualties. Despite however the heavy bombardment, casualties in the fort were slight and the British ships eventually realized that they could not force the passage to attack Baltimore in coordination with the land force. After a last ditch night feint and barge attack during the heavy rain storm at the time led by Capt. Charles Napier around the fort up the Middle Branch of the river to the west which was split and misdirected partly in the storm, then turned back with heavy casualties by alert gunners at supporting western batteries Fort Covington and Battery Babcock, so the British called off the attack and sailed downriver to pick up their army which had retreated from the eastside of Baltimore. All the lights were extinguished in Baltimore the night of the attack, and the fort was bombarded for 25 hours. The only light was given off by the exploding shells over Fort McHenry, illuminating the flag that was still flying over the fort. The defence of the fort inspired the American lawyer Francis Scott Key to write "Defence of Fort M'Henry", a poem that was set to music as "The Star-Spangled Banner".
Great Lakes and Western Territories.
Invasions of Upper and Lower Canada, 1812.
American leaders assumed that Canada could be easily overrun. Former President Jefferson optimistically referred to the conquest of Canada as "a matter of marching". Many Loyalist Americans had migrated to Upper Canada after the Revolutionary War. There was also significant non-Loyalist American immigration to the area due to the offer of land grants to immigrants, and the U.S. assumed the latter would favor the American cause, but they did not. In prewar Upper Canada, General Prévost was in the unusual position of having to purchase many provisions for his troops from the American side. This peculiar trade persisted throughout the war in spite of an abortive attempt by the U.S. government to curtail it. In Lower Canada, which was much more populous, support for Britain came from the English elite with strong loyalty to the Empire, and from the Canadian elite, who feared American conquest would destroy the old order by introducing Protestantism, Anglicization, republican democracy, and commercial capitalism; and weakening the Catholic Church. The Canadian inhabitants feared the loss of a shrinking area of good lands to potential American immigrants.
In 1812–13, British military experience prevailed over inexperienced American commanders. Geography dictated that operations would take place in the west: principally around Lake Erie, near the Niagara River between Lake Erie and Lake Ontario, and near the Saint Lawrence River area and Lake Champlain. This was the focus of the three-pronged attacks by the Americans in 1812. Although cutting the St. Lawrence River through the capture of Montreal and Quebec would have made Britain's hold in North America unsustainable, the United States began operations first in the western frontier because of the general popularity there of a war with the British, who had sold arms to the Native Americans opposing the settlers.
The British scored an important early success when their detachment at St. Joseph Island, on Lake Huron, learned of the declaration of war before the nearby American garrison at the important trading post at Mackinac Island in Michigan. A scratch force landed on the island on July 17, 1812, and mounted a gun overlooking Fort Mackinac. After the British fired one shot from their gun, the Americans, taken by surprise, surrendered. This early victory encouraged the natives, and large numbers moved to help the British at Amherstburg. The island totally controlled access to the Old Northwest, giving the British nominal control of this area, and, more vitally, a monopoly on the fur trade.
An American army under the command of William Hull invaded Canada on July 12, with his forces chiefly composed of untrained and ill-disciplined militiamen. Once on Canadian soil, Hull issued a proclamation ordering all British subjects to surrender, or "the horrors, and calamities of war will stalk before you". This led many of the British forces to defect. John Bennett, printer and publisher of the York Gazette & Oracle, was a prominent defector. Andrew Mercer, who had the publication's production moved to his house, lost the press and type destroyed during American occupation, an example of what happened to resisters. He also threatened to kill any British prisoner caught fighting alongside a native. The proclamation helped stiffen resistance to the American attacks. Hull's army was too weak in artillery and badly supplied to achieve its objectives, and had to fight just to maintain its own lines of communication.
The senior British officer in Upper Canada, Major General Isaac Brock, felt that he should take bold measures to calm the settler population in Canada, and to convince the aboriginals who were needed to defend the region that Britain was strong. He moved rapidly to Amherstburg near the western end of Lake Erie with reinforcements and immediately decided to attack Detroit. Hull, fearing that the British possessed superior numbers and that the Indians attached to Brock's force would commit massacres if fighting began, surrendered Detroit without a fight on August 16. Knowing of British-instigated indigenous attacks on other locations, Hull ordered the evacuation of the inhabitants of Fort Dearborn (Chicago) to Fort Wayne. After initially being granted safe passage, the inhabitants (soldiers and civilians) were attacked by Potowatomis on August 15 after travelling only in what is known as the Battle of Fort Dearborn. The fort was subsequently burned.
Brock promptly transferred himself to the eastern end of Lake Erie, where American General Stephen Van Rensselaer was attempting a second invasion. An armistice (arranged by Prévost in the hope the British renunciation of the Orders in Council to which the United States objected might lead to peace) prevented Brock from invading American territory. When the armistice ended, the Americans attempted an attack across the Niagara River on October 13, but suffered a crushing defeat at Queenston Heights. Brock was killed during the battle. While the professionalism of the American forces would improve by the war's end, British leadership suffered after Brock's death. A final attempt in 1812 by American General Henry Dearborn to advance north from Lake Champlain failed when his militia refused to advance beyond American territory.
In contrast to the American militia, the Canadian militia performed well. French Canadians, who found the anti-Catholic stance of most of the United States troublesome, and United Empire Loyalists, who had fought for the Crown during the American Revolutionary War, strongly opposed the American invasion. However, many in Upper Canada were recent settlers from the United States who had no obvious loyalties to the Crown. Nevertheless, while there were some who sympathized with the invaders, the American forces found strong opposition from men loyal to the Empire.
American Northwest, 1813.
After Hull's surrender of Detroit, General William Henry Harrison was given command of the U.S. Army of the Northwest. He set out to retake the city, which was now defended by Colonel Henry Procter in conjunction with Tecumseh. A detachment of Harrison's army was defeated at Frenchtown along the River Raisin on January 22, 1813. Procter left the prisoners with an inadequate guard, who could not prevent some of his North American aboriginal allies from attacking and killing perhaps as many as sixty Americans, many of whom were Kentucky militiamen. The incident became known as the River Raisin Massacre. The defeat ended Harrison's campaign against Detroit, and the phrase "Remember the River Raisin!" became a rallying cry for the Americans.
In May 1813, Procter and Tecumseh set siege to Fort Meigs in northwestern Ohio. American reinforcements arriving during the siege were defeated by the natives, but the fort held out. The Indians eventually began to disperse, forcing Procter and Tecumseh to return north to Canada. A second offensive against Fort Meigs also failed in July. In an attempt to improve Indian morale, Procter and Tecumseh attempted to storm Fort Stephenson, a small American post on the Sandusky River, only to be repulsed with serious losses, marking the end of the Ohio campaign.
On Lake Erie, American commander Captain Oliver Hazard Perry fought the Battle of Lake Erie on September 10, 1813. His decisive victory at "Put-In-Bay" ensured American military control of the lake, improved American morale after a series of defeats, and compelled the British to fall back from Detroit. This paved the way for General Harrison to launch another invasion of Upper Canada, which culminated in the U.S. victory at the Battle of the Thames on October 5, 1813, in which Tecumseh was killed.
Niagara frontier, 1813.
Because of the difficulties of land communications, control of the Great Lakes and the St. Lawrence River corridor was crucial. When the war began, the British already had a small squadron of warships on Lake Ontario and had the initial advantage. To redress the situation, the Americans established a Navy yard at Sackett's Harbor in northwestern New York. Commodore Isaac Chauncey took charge of the large number of sailors and shipwrights sent there from New York; they completed the second warship built there in a mere 45 days. Ultimately, almost 3,000 men worked at the naval shipyard, building eleven warships and many smaller boats and transports. Having regained the advantage by their rapid building program, Chauncey and Dearborn attacked York, on the northern shore of the lake, the capital of Upper Canada, on April 27, 1813. The Battle of York was a "pyrrhic" American victory, marred by looting and the burning of the small Provincial Parliament buildings and a library (resulting in a spirit of revenge by the British/Canadians led by Gov. George Prévost, who later demanded satisfaction encouraging the British Admiralty to issue orders to their officers later operating in the Chesapeake Bay region to exact similar devastation on the American Federal capital village of Washington the following year). However, Kingston was strategically much more valuable to British supply and communications routes along the St. Lawrence corridor. Without control of Kingston, the U.S. Navy could not effectively control Lake Ontario or sever the British supply line from Lower Canada.
On May 27, 1813, an American amphibious force from Lake Ontario assaulted Fort George on the northern end of the Niagara River and captured it without serious losses. The retreating British forces were not pursued, however, until they had largely escaped and organized a counteroffensive against the advancing Americans at the Battle of Stoney Creek on June 5. On June 24, with the help of advance warning by Laura Secord, another American force was forced to surrender by a much smaller British and native force at the Battle of Beaver Dams, marking the end of the American offensive into Upper Canada. Meanwhile, Commodore James Lucas Yeo had taken charge of the British ships on the lake and mounted a counterattack, which was nevertheless repulsed at the Battle of Sackett's Harbor. Thereafter, Chauncey and Yeo's squadrons fought two indecisive actions, neither commander seeking a fight to the finish.
Late in 1813, the Americans abandoned the Canadian territory they occupied around Fort George. They set fire to the village of Newark (now Niagara-on-the-Lake) on December 15, 1813, incensing the Canadians and politicians in control. Many of the inhabitants were left without shelter, freezing to death in the snow. This led to British retaliation following the Capture of Fort Niagara on December 18, 1813. Early the next morning on December 19, the British and their native allies stormed the neighboring town of Lewiston, New York, torching homes and buildings and killing about a dozen civilians. As the British were chasing the surviving residents out of town, a small force of Tuscarora natives intervened and stopped the pursuit, buying enough time for the locals to escape to safer ground. It is notable in that the Tuscaroras defended the Americans against their own Iroquois brothers, the Mohawks, who sided with the British. Later, the British attacked and burned Buffalo on December 30, 1813.
In 1814, the contest for Lake Ontario turned into a building race. Naval superiority shifted between the opposing fleets as each built new, bigger ships. However, neither was able to bring the other to battle when in a position of superiority, leaving the Engagements on Lake Ontario a draw. At war's end, the British held the advantage with the 112-gun , but the Americans had laid down two even larger ships. The majority of these ships never saw action and were decommissioned after the war.
St. Lawrence and Lower Canada, 1813.
The British were potentially most vulnerable over the stretch of the St. Lawrence where it formed the frontier between Upper Canada and the United States. During the early days of the war, there was illicit commerce across the river. Over the winter of 1812 and 1813, the Americans launched a series of raids from Ogdensburg on the American side of the river, which hampered British supply traffic up the river. On February 21, Sir George Prévost passed through Prescott on the opposite bank of the river with reinforcements for Upper Canada. When he left the next day, the reinforcements and local militia attacked. At the Battle of Ogdensburg, the Americans were forced to retire.
For the rest of the year, Ogdensburg had no American garrison, and many residents of Ogdensburg resumed visits and trade with Prescott. This British victory removed the last American regular troops from the Upper St. Lawrence frontier and helped secure British communications with Montreal. Late in 1813, after much argument, the Americans made two thrusts against Montreal. The plan eventually agreed upon was for Major General Wade Hampton to march north from Lake Champlain and join a force under General James Wilkinson that would embark in boats and sail from Sackett's Harbor on Lake Ontario and descend the St. Lawrence. Hampton was delayed by bad roads and supply problems and also had an intense dislike of Wilkinson, which limited his desire to support his plan. On October 25, his 4,000-strong force was defeated at the Chateauguay River by Charles de Salaberry's smaller force of Canadian Voltigeurs and Mohawks. Wilkinson's force of 8,000 set out on October 17, but was also delayed by bad weather. After learning that Hampton had been checked, Wilkinson heard that a British force under Captain William Mulcaster and Lieutenant Colonel Joseph Wanton Morrison was pursuing him, and by November 10, he was forced to land near Morrisburg, about 150 kilometres (90 mi.) from Montreal. On November 11, Wilkinson's rear guard, numbering 2,500, attacked Morrison's force of 800 at Crysler's Farm and was repulsed with heavy losses. After learning that Hampton could not renew his advance, Wilkinson retreated to the U.S. and settled into winter quarters. He resigned his command after a failed attack on a British outpost at Lacolle Mills.
Niagara and Plattsburgh Campaigns, 1814.
By the middle of 1814, American generals, including Major Generals Jacob Brown and Winfield Scott, had drastically improved the fighting abilities and discipline of the army. Their renewed attack on the Niagara peninsula quickly captured Fort Erie. Winfield Scott then gained a victory over an inferior British force at the Battle of Chippawa on July 5. An attempt to advance further ended with a hard-fought but inconclusive battle at Lundy's Lane on July 25.
The outnumbered Americans withdrew but withstood a prolonged Siege of Fort Erie. The British suffered heavy casualties in a failed assault and were weakened by exposure and shortage of supplies in their siege lines. Eventually the British raised the siege, but American Major General George Izard took over command on the Niagara front and followed up only halfheartedly. The Americans lacked provisions, and eventually destroyed the fort and retreated across the Niagara.
Meanwhile, following the abdication of Napoleon, 15,000 British troops were sent to North America under four of Wellington's ablest brigade commanders. Fewer than half were veterans of the Peninsula and the rest came from garrisons. Prévost was ordered to neutralize American power on the lakes by burning Sackets Harbor, gain naval control of Lake Erie, Lake Ontario and the Upper Lakes, and defend Lower Canada from attack. He did defend Lower Canada but otherwise failed to achieve his objectives. Given the late season he decided to invade New York State. His army outnumbered the American defenders of Plattsburgh, but he was worried about his flanks so he decided he needed naval control of Lake Champlain. On the lake, the British squadron under Captain George Downie and the Americans under Master Commandant Thomas Macdonough were more evenly matched.
On reaching Plattsburgh, Prévost delayed the assault until the arrival of Downie in the hastily completed 36-gun frigate . Prévost forced Downie into a premature attack, but then unaccountably failed to provide the promised military backing. Downie was killed and his naval force defeated at the naval Battle of Plattsburgh in Plattsburgh Bay on September 11, 1814. The Americans now had control of Lake Champlain; Theodore Roosevelt later termed it "the greatest naval battle of the war". The successful land defence was led by Alexander Macomb. To the astonishment of his senior officers, Prévost then turned back, saying it would be too hazardous to remain on enemy territory after the loss of naval supremacy. Prévost was recalled and in London, a naval court-martial decided that defeat had been caused principally by Prévost's urging the squadron into premature action and then failing to afford the promised support from the land forces. Prévost died suddenly, just before his own court-martial was to convene. Prévost's reputation sank to a new low, as Canadians claimed that their militia under Brock did the job and he failed. Recently, however, historians have been more kindly, measuring him not against Wellington but against his American foes. They judge Prévost's preparations for defending the Canadas with limited means to be energetic, well-conceived, and comprehensive; and against the odds, he had achieved the primary objective of preventing an American conquest.
To the east, the northern part of Massachusetts, soon to be Maine, was invaded. Fort Sullivan at Eastport was taken by Sir Thomas Hardy on July 11. Castine, Hampden, Bangor, and Machias were taken, and Castine became the main British base till April 15, 1815, when the British left, taking £10,750 in tariff duties, the "Castine Fund" which was used to found Dalhousie University. Eastport was not returned to the United States till 1818.
American West, 1813–14.
The Mississippi River valley was the western frontier of the United States in 1812. The territory acquired in the Louisiana Purchase of 1803 contained almost no U.S. settlements west of the Mississippi except around Saint Louis and a few forts and trading posts. Fort Bellefontaine, an old trading post converted to a U.S. Army post in 1804, served as regional headquarters. Fort Osage, built in 1808 along the Missouri was the western-most U.S. outpost, it was abandoned at the start of the war. Fort Madison, built along the Mississippi in what is now Iowa, was also built in 1808, and had been repeatedly attacked by British-allied Sauk since its construction. In September 1813 Fort Madison was abandoned after it was attacked and besieged by natives, who had support from the British. This was one of the few battles fought west of the Mississippi. Black Hawk played a leadership role.
Little of note took place on Lake Huron in 1813, but the American victory on Lake Erie and the recapture of Detroit isolated the British there. During the ensuing winter, a Canadian party under Lieutenant Colonel Robert McDouall established a new supply line from York to Nottawasaga Bay on Georgian Bay. When he arrived at Fort Mackinac with supplies and reinforcements, he sent an expedition to recapture the trading post of Prairie du Chien in the far west. The Siege of Prairie du Chien ended in a British victory on July 20, 1814.
Earlier in July, the Americans sent a force of five vessels from Detroit to recapture Mackinac. A mixed force of regulars and volunteers from the militia landed on the island on August 4. They did not attempt to achieve surprise, and at the brief Battle of Mackinac Island, they were ambushed by natives and forced to re-embark. The Americans discovered the new base at Nottawasaga Bay, and on August 13, they destroyed its fortifications and a schooner that they found there. They then returned to Detroit, leaving two gunboats to blockade Mackinac. On September 4, these gunboats were taken unawares and captured by British boarding parties from canoes and small boats. This Engagement on Lake Huron left Mackinac under British control.
The British garrison at Prairie du Chien also fought off another attack by Major Zachary Taylor. In this distant theatre, the British retained the upper hand until the end of the war, through the allegiance of several indigenous tribes that received British gifts and arms, enabling them to take control of parts of what is now Michigan and Illinois, as well as the whole of modern Wisconsin. In 1814 U.S. troops retreating from the Battle of Credit Island on the upper Mississippi attempted to make a stand at Fort Johnson, but the fort was soon abandoned, along with most of the upper Mississippi valley.
After the U.S. was pushed out of the Upper Mississippi region, they held on to eastern Missouri and the St. Louis area. Two notable battles fought against the Sauk were the Battle of Cote Sans Dessein, in April 1815, at the mouth of the Osage River in the Missouri Territory, and the Battle of the Sink Hole, in May 1815, near Fort Cap au Gris.
At the conclusion of peace, Mackinac and other captured territory was returned to the United States. At the end of the war, some British officers and Canadians objected to handing back Prairie du Chien and especially Mackinac under the terms of the Treaty of Ghent. However, the Americans retained the captured post at Fort Malden, near Amherstburg, until the British complied with the treaty.
Fighting between Americans, the Sauk, and other indigenous tribes continued through 1817, well after the war ended in the east.
Southern theatre.
Creek War.
The Battle of Burnt Corn between Red Stick Creeks and U.S. troops, occurred in the southern parts of Alabama on July 27, 1813 prompted the state of Georgia as well as the Mississippi territory militia to immediately take major action against Creek offensives. The Red Sticks chiefs gained power in the east along the Alabama, Coosa, and Tallapoosa Rivers – Upper Creek territory. The Lower Creek lived along the Chattahoochee River. Many Creeks tried to remain friendly to the United States, and some were organized by federal Indian Agent Benjamin Hawkins to aid the 6th Military District under General Thomas Pinckney and the state militias. The United States combined forces were large. At its peak the Red Stick faction had 4,000 warriors, only a quarter of whom had muskets.
Before 1813, the Creek War had been largely an internal affair sparked by the ideas of Tecumseh farther north in the Mississippi Valley, but the United States was drawn into a war with the Creek Nation by the War of 1812. The Creek Nation was a trading partner of the United States actively involved with Spanish and British trade as well. The Red Sticks, as well as many southern Muscogeean people like the Seminole, had a long history of alliance with the Spanish and British Empires. This alliance helped the North American and European powers protect each other's claims to territory in the south. On August 18, 1813, Red Stick chiefs planned an attack on Fort Mimms, north of Mobile, the only American-held port in the territory of West Florida. The attack on Fort Mimms resulted in the death of 400 settlers and became an ideological rallying point for the Americans.
The Indian frontier of western Georgia was the most vulnerable but was partially fortified already. From November 1813 to January 1814, Georgia's militia and auxiliary Federal troops - from the Creek and Cherokee Indian nations and the states of North Carolina and South Carolina – organized the fortification of defences along the Chattahoochee River and expeditions into Upper Creek territory in present-day Alabama. The army, led by General John Floyd, went to the heart of the "Creek Holy Grounds" and won a major offensive against one of the largest Creek towns at Battle of Autosee, killing an estimated two hundred people. In November, the militia of Mississippi with a combined 1200 troops attacked the "Econachca" encampment ("Battle of Holy Ground") on the Alabama River. Tennessee raised a militia of 5,000 under Colonel Andrew Jackson and Major General Coke and won the battles of Tallushatchee and Talladega in November 1813.
The Georgia militia withdrew to the Chattahoochee, and Jackson's force in Tennessee mostly disbanded for the winter. In January Floyd's force of 1,300 state militia and 400 Creek Indians moved to join the U.S forces in Tennessee, but were attacked in camp on the Calibee Creek by Tuckaubatchee Indians on the 27th.
Despite enlistment problems in the winter, the U.S. Army forces and a second draft of Tennessee state militia and Cherokee and Creek allies swelled his army to around 5,000. In March 1814 they moved south to attack the Creek.
On March 26, Jackson and General John Coffee decisively defeated the Creek Indian force at Horseshoe Bend, killing 800 of 1,000 Creeks at a cost of 49 killed and 154 wounded out of approximately 2,000 American and Cherokee forces.
The American army moved to Fort Jackson on the Alabama River. On August 9, 1814, the Upper Creek chiefs and Major General Andrew Jackson's army signed the "Treaty of Fort Jackson". The most of western Georgia and part of Alabama was taken from the Creeks to pay for expenses borne by the United States. The Treaty also "demanded" that the "Red Stick" insurgents cease communicating with the Spanish or British, and only trade with U.S.-approved agents.
British aid to the Red Sticks arrived after the end of the Napoleonic Wars in April 1814 and after Admiral Sir Alexander Cochrane assumed command from Admiral Warren in March. The Creek promised to join any body of 'troops that should aid them in regaining their lands, and suggesting an attack on the tower off Mobile.' In April 1814 the British established an outpost on the Apalachicola River at Prospect Bluff (Fort Gadsden). Cochrane sent a company of Royal Marines, the vessels and , commanded by Edward Nicolls, with further supplies to meet the Indians. In addition to training the Indians, Nicolls was tasked to raise a force from escaped slaves, as part of the Corps of Colonial Marines.
In July 1814, General Andrew Jackson complained to the Governor of Pensacola, Mateo Gonzalez Manrique, that combatants from the Creek War were being harbored in Spanish territory, and made reference to the British presence on Spanish soil. Although he gave an angry reply to Jackson, Manrique was alarmed at the weak position he found himself in. He appealed to the British for help, with Woodbine arriving on July 28, and Nicolls arriving at Pensacola on August 24.
The first engagement of the British and their Creek allies against the Americans on the Gulf Coast was the attack on Fort Bowyer September 14, 1814. Captain William Percy tried to take the U.S. fort, hoping that would enable the British to move on Mobile and block U.S. trade and encroachment on the Mississippi. After the Americans repulsed Percy's forces, the British established a military presence of up to 200 Marines at Pensacola. In November, Jackson's force of 4,000 men took the town in November. This underlined the superiority of numbers of Jackson's force in the region. The U.S force moved to New Orleans in late 1814. Jackson's army of 1,000 regulars and 3,000 to 4,000 militia, pirates and other fighters, as well as civilians and slaves built fortifications south of the city.
Gulf Coast.
American forces under General James Wilkinson, who was himself getting $4,000 per year as a Spanish secret agent, took the Mobile area—formerly part of West Florida—from the Spanish in March 1813; this would be the only territory permanently gained by the U.S. during the war. The Americans built Fort Bowyer, a log and earthenwork fort with 14 guns, on Mobile Point.
At the end of 1814, the British launched a double offensive in the South weeks before the Treaty of Ghent was signed. On the Atlantic coast, Admiral George Cockburn was to close the Intracoastal Waterway trade and land Royal Marine battalions to advance through Georgia to the western territories. On the Gulf coast, Admiral Alexander Cochrane would move on the new state of Louisiana and the Mississippi Territory. Admiral Cochrane's ships reached the Louisiana coast December 9, and Cockburn arrived in Georgia December 14.
On January 8, 1815, a British force of 8,000 under General Edward Pakenham attacked Jackson's defences in New Orleans. The Battle of New Orleans was an American victory, as the British failed to take the fortifications on the East Bank. The British suffered high casualties: 291 dead, 1262 wounded, and 484 captured or missing whereas American casualties were 13 dead, 39 wounded, and 19 missing. It was hailed as a great victory across the U.S., making Jackson a national hero and eventually propelling him to the presidency. The American garrison at Fort St. Philip endured ten days of bombardment from Royal Navy guns, which was a final attempt to invade Louisiana; British ships sailed away from the Mississippi River on January 18. However, it was not until January 27, 1815, that the army had completely rejoined the fleet, allowing for their departure.
After New Orleans, the British tried to take Mobile a second time; General John Lambert laid siege for five days and took the fort, winning the Second Battle of Fort Bowyer on February 12, 1815. HMS "Brazen" brought news of the Treaty of Ghent the next day, and the British abandoned the Gulf coast.
In January 1815, Admiral Cockburn succeeded in blockading the southeastern coast by occupying Camden County, Georgia. The British quickly took Cumberland Island, Fort Point Peter, and Fort St. Tammany in a decisive victory. Under the orders of his commanding officers, Cockburn's forces relocated many refugee slaves, capturing St. Simons Island as well, to do so. During the invasion of the Georgia coast, an estimated 1,485 people chose to relocate in British territories or join the military. In mid-March, several days after being informed of the Treaty of Ghent, British ships finally left the area.
Postwar fighting.
In May 1815, a band of British-allied Sauk, unaware that the war had ended months before, attacked a small band of U.S. soldiers northwest of St. Louis. Intermittent fighting, primarily with the Sauk, continued in the Missouri Territory well into 1817, although it is unknown if the Sauk were acting on their own or on behalf of British agents. Several uncontacted isolated warships continued fighting well into 1815 and were the last American forces to take offensive action against the British.
The Treaty of Ghent.
Factors leading to the peace negotiations.
By 1814, both sides had either achieved their main war goals or were weary of a costly war that offered little but stalemate. They both sent delegations to a neutral site in Ghent, Flanders (now part of Belgium). The negotiations began in early August and concluded on December 24, when a final agreement was signed; both sides had to ratify it before it could take effect. Meanwhile, both sides planned new invasions.
In 1814 the British began blockading the United States, and brought the American economy to near bankruptcy, forcing it to rely on loans for the rest of the war. American foreign trade was reduced to a trickle. The parlous American economy was thrown into chaos with prices soaring and unexpected shortages causing hardship in New England which was considering secession. But also to a lesser extent British interests were hurt in the West Indies and Canada that had depended on that trade. Although American privateers found chances of success much reduced, with most British merchantmen now sailing in convoy, privateering continued to prove troublesome to the British, as shown by high insurance rates. British landowners grew weary of high taxes, and colonial interests and merchants called on the government to reopen trade with the U.S. by ending the war.
Negotiations and peace.
At last in August 1814, peace discussions began in the neutral city of Ghent. Both sides began negotiations warily The British diplomats stated their case first, demanding the creation of an Indian barrier state in the American Northwest Territory (the area from Ohio to Wisconsin). It was understood the British would sponsor this Indian state. The British strategy for decades had been to create a buffer state to block American expansion. Britain demanded naval control of the Great Lakes and access to the Mississippi River. The Americans refused to consider a buffer state and the proposal was dropped. Although article IX of the treaty included provisions to restore to Natives "all possessions, rights and privileges which they may have enjoyed, or been entitled to in 1811", the provisions were unenforceable. The Americans (at a later stage) demanded damages for the burning of Washington and for the seizure of ships before the war began.
American public opinion was outraged when Madison published the demands; even the Federalists were now willing to fight on. The British had planned three invasions. One force burned Washington but failed to capture Baltimore, and sailed away when its commander was killed. In northern New York State, 10,000 British veterans were marching south until a decisive defeat at the Battle of Plattsburgh forced them back to Canada. Nothing was known of the fate of the third large invasion force aimed at capturing New Orleans and southwest. The Prime Minister wanted the Duke of Wellington to command in Canada and take control of the Great Lakes. Wellington said that he would go to America but he believed he was needed in Europe. Wellington emphasized that the war was a draw and the peace negotiations should not make territorial demands:
The Prime Minister, Lord Liverpool, aware of growing opposition to wartime taxation and the demands of Liverpool and Bristol merchants to reopen trade with America, realized Britain also had little to gain and much to lose from prolonged warfare especially after the growing concern about the situation in Europe. After months of negotiations, against the background of changing military victories, defeats and losses, the parties finally realized that their nations wanted peace and there was no real reason to continue the war. Now each side was tired of the war. Export trade was all but paralyzed and after Napoleon fell in 1814 France was no longer an enemy of Britain, so the Royal Navy no longer needed to stop American shipments to France, and it no longer needed to impress more seamen. It had ended the practices that so angered the Americans in 1812. The British were preoccupied in rebuilding Europe after the apparent final defeat of Napoleon.
British negotiators were urged by Lord Liverpool to offer a status quo and dropped their demands for the creation of an Indian barrier state, which was in any case hopeless after the collapse of Tecumseh's alliance. This allowed negotiations to resume at the end of October. British diplomats soon offered the status quo to the U.S. negotiators, who accepted them. Prisoners would be exchanged, and captured slaves returned to the United States or be paid for by Britain.
On December 24, 1814 the diplomats had finished and signed the Treaty of Ghent. The treaty was ratified by the British three days later on December 27 and arrived in Washington on February 17 where it was quickly ratified and went into effect, thus finally ending the war. The terms called for all occupied territory to be returned, the prewar boundary between Canada and the United States to be restored, and the Americans were to gain fishing rights in the Gulf of Saint Lawrence.
The Treaty of Ghent failed to secure official British acknowledgment of American maritime rights or ending impressment. However, in the century of peace until World War I these rights were not seriously violated. The defeat of Napoleon made irrelevant all of the naval issues over which the United States had fought. The Americans had achieved their goal of ending the Indian threat; furthermore the American armies had scored enough victories (especially at New Orleans) to satisfy honor and the sense of becoming fully independent from Britain.
Losses and compensation.
British losses in the war were about 1,160 killed in action and 3,679 wounded; 3,321 British died from disease. American losses were 2,260 killed in action and 4,505 wounded. While the number of Americans who died from disease is not known, it is estimated that about 15,000 died from all causes directly related to the war. These figures do not include deaths among Canadian militia forces or losses among native tribes.
There have been no estimates of the cost of the American war to Britain, but it did add some £25 million to the national debt. In the U.S., the cost was $105 million, about the same as the cost to Britain. The national debt rose from $45 million in 1812 to $127 million by the end of 1815, although by selling bonds and treasury notes at deep discounts—and often for irredeemable paper money due to the suspension of specie payment in 1814—the government received only $34 million worth of specie.
In addition, at least 3,000 American slaves escaped to the British lines. Many other slaves simply escaped in the chaos of war and achieved their freedom on their own. The British settled some of the newly freed slaves in Nova Scotia. Four hundred freedmen were settled in New Brunswick. The Americans protested that Britain's failure to return the slaves violated the Treaty of Ghent. After arbitration by the Tsar of Russia the British paid $1,204,960 in damages to Washington, which reimbursed the slaveowners.
Memory and historiography.
Popular views.
During the 19th century the popular image of the war in the United States was of an American victory, and in Canada, of a Canadian victory. Each young country saw its self-perceived victory as an important foundation of its growing nationhood. The British, on the other hand, who had been preoccupied by Napoleon's challenge in Europe, paid little attention to what was to them a peripheral and secondary dispute, a distraction from the principal task at hand.
Canadian.
In British North America (which would become the Dominion of Canada in 1867), the War of 1812 was seen by Loyalists as a victory, as they had successfully defended their borders from an American takeover. The outcome gave Empire-oriented Canadians confidence and, together with the postwar "militia myth" that the civilian militia had been primarily responsible rather than the British regulars, was used to stimulate a new sense of Canadian nationalism. John Strachan, the first Anglican bishop of Toronto, created the myth, telling his flock that Upper Canada had been saved from the dangerous republicanism of the American invaders by the heroism of the local citizenry.
A long-term implication of the militia myth—which was eventually discarded, but remained popular in the Canadian public at least until the First World War—was that Canada did not need a regular professional army. The U.S. Army had done poorly, on the whole, in several attempts to invade Canada, and the Canadians had shown that they would fight bravely to defend their country. But the British did not doubt that the thinly populated territory would be vulnerable in a third war. "We cannot keep Canada if the Americans declare war against us again", Admiral Sir David Milne wrote to a correspondent in 1817, although the Rideau Canal was built for just such a scenario.
By the 21st century it was a forgotten war in Britain, although still remembered in Canada, especially Ontario. In a 2009 poll, 37% of Canadians said the war was a Canadian victory, 9% said the U.S. won, 15% called it a draw, and 39%—mainly younger Canadians—said they knew too little to comment.
A February 2012 poll found that in a list of items that could be used to define Canadians' identity, the fact that Canada successfully repelled an American invasion in the War of 1812 places second (25%), only behind the fact that Canada has universal health care (53%). The survey states that 77% of Canadians believe that War of 1812 Bicentennial is an important commemoration.
American.
Today, American popular memory includes the British capture and the burning of Washington in August 1814, which necessitated its extensive renovation. Another memory is the successful American defence of Fort McHenry in September 1814, which inspired the lyrics of the U.S. national anthem, "The Star-Spangled Banner". The successful Captains of the U.S. Navy became popular heroes with plates with the likeness of Decatur, Steward, Hull, and others, becoming popular items. Ironically, many were made in England. The Navy became a cherished institution, lauded for the victories that it won against all odds. After engagements during the final actions of the war, U.S. Marines had acquired a well-deserved reputation as excellent marksmen, especially in ship-to-ship actions.
Historians' views.
Historians have differing and complex interpretations of the war. They agree that ending the war with neither side gaining or losing territory allowed for the peaceful settlement of boundary disputes and for the opening of a permanent era of good will and friendly relations between the U.S. and Canada. The war established distinct national identities for Canada and the United States, with a "newly significant border".
In recent decades the view of the majority of historians has been that the war ended in stalemate, with the Treaty of Ghent closing a war that had become militarily inconclusive. Neither side wanted to continue fighting since the main causes had disappeared and since there were no large lost territories for one side or the other to reclaim by force. Insofar as they see the war's untriumphant resolution as allowing two centuries of peaceful and mutually beneficial intercourse between the U.S., Britain and Canada, these historians often conclude that all three nations were the "real winners" of the War of 1812. These writers often add that the war could have been avoided in the first place by better diplomacy. It is seen as a mistake for everyone concerned because it was badly planned and marked by multiple fiascoes and failures on both sides, as shown especially by the repeated American failure to seize parts of Canada, and the failed British attack on New Orleans and upstate New York.
However, other scholars hold that the war constituted a British victory and an American defeat. They argue that the British achieved their military objectives in 1812 (by stopping the repeated American invasions of Canada) and that Canada retained her independence of the United States. By contrast, they say, the Americans suffered a defeat when their armies failed to achieve their war goal of seizing part or all of Canada. Additionally, they argue the U.S. lost as it failed to stop impressment, which the British refused to repeal until the end of the Napoleonic Wars, and the U.S. actions had no effect on the orders in council, which were rescinded before the war started.
A second minority view is that both the U.S. and Britain won the war—that is, both achieved their main objectives, while the Indians were the losing party. The British won by losing no territories and achieving their great war goal, the total defeat of Napoleon. U.S. won by (1) securing her honor and successfully resisting a powerful empire once again, thus winning a "second war of independence"; and (2) ending the threat of Indian raids and the British plan for a semi-independent Indian sanctuary—thereby opening an unimpeded path for the United States' westward expansion.
Indians as losers.
Historians generally agree that the real losers of the War of 1812 were the Indians (also called "First Nations" in Canada). Hickey says:
American settlers into the Middle West had been repeatedly blocked and threatened by Indian raids before 1812, and that now came to an end. Throughout the war the British had played on terror of the tomahawks and scalping knives of their Indian allies; it worked especially at Hull's surrender at Detroit. By 1813 Americans had killed Tecumseh and broken his coalition of tribes. Jackson then defeated the enemy Indians in the Southwest. Historian John Sugden notes that in both theatres, the Indians' strength had been broken prior to the arrival of the major British forces in 1814.
Notwithstanding the sympathy and support from commanders (such as Brock, Cochrane and Nicolls), the policymakers in London reneged in assisting the Indians, as making peace was a higher priority for the politicians. At the peace conference the British demanded an independent Indian state in the Midwest, but by late 1814 the Indian confederation had disintegrated and the British had to abandon the demand. The withdrawal of British protection gave the Americans a free hand, which resulted in the removal of most of the tribes to Indian Territory (present-day Oklahoma). In that sense according to historian Alan Taylor, the final victory at New Orleans had "enduring and massive consequences". It gave the Americans "continental predominance" while it left the Indians dispossessed, powerless, and vulnerable.
The Creek War came to an end, with the Treaty of Fort Jackson being imposed upon the Indians. About half of the Creek territory was ceded to the United States, with no payment made to the Creeks. This was, in theory, invalidated by Article 9 of the Treaty of Ghent, thereby restoring to the Indians "all the possessions, rights and privileges which they may have enjoyed or been entitled to in 1811." The British failed to uphold this, and did not take up the Indian cause as an infringement of an international treaty. Without this support, the Indians' lack of power was apparent and the stage was set for further incursions of territory by the United States in subsequent decades.
Long-term consequences.
Neither side lost territory in the war, nor did the treaty that ended it address the original points of contention—and yet it changed much between the United States of America and Britain.
The Rush–Bagot Treaty was a treaty between the United States and Britain enacted in 1817 that provided for the demilitarization of the Great Lakes and Lake Champlain, where many British naval arrangements and forts still remained. The treaty laid the basis for a demilitarized boundary and was indicative of improving relations between the United States and Great Britain in the period following the War of 1812. It remains in effect to this day.
The Treaty of Ghent established the "status quo ante bellum"; that is, there were no territorial losses by either side. The issue of impressment was made moot when the Royal Navy, no longer needing sailors, stopped impressment after the defeat of Napoleon. Except for occasional border disputes and the circumstances of the American Civil War, relations between the U.S. and Britain remained generally peaceful for the rest of the 19th century, and the two countries became close allies in the 20th century.
Border adjustments between the U.S. and British North America were made in the Treaty of 1818. Eastport, Massachusetts, was returned to the U.S. in 1818; it would become part of the new State of Maine in 1820. A border dispute along the Maine–New Brunswick border was settled by the 1842 Webster–Ashburton Treaty after the bloodless Aroostook War, and the border in the Oregon Country was settled by splitting the disputed area in half by the 1846 Oregon Treaty. A further dispute about the line of the border through the island in the Strait of Juan de Fuca resulted in another almost bloodless standoff in the Pig War of 1859. The line of the border was finally settled by an international arbitration commission in 1872.
United States.
The U.S. suppressed the Native American resistance on its western and southern borders. The nation also gained a psychological sense of complete independence as people celebrated their "second war of independence". Nationalism soared after the victory at the Battle of New Orleans. The opposition Federalist Party collapsed, and the Era of Good Feelings ensued.
No longer questioning the need for a strong Navy, the U.S. built three new 74-gun ships of the line and two new 44-gun frigates shortly after the end of the war. (Another frigate had been destroyed to prevent it being captured on the stocks.) In 1816, the U.S. Congress passed into law an "Act for the gradual increase of the Navy" at a cost of $1,000,000 a year for eight years, authorizing 9 ships of the line and 12 heavy frigates. The Captains and Commodores of the U.S. Navy became the heroes of their generation in the U.S. Decorated plates and pitchers of Decatur, Hull, Bainbridge, Lawrence, Perry, and Macdonough were made in Staffordshire, England, and found a ready market in the United States. Three of the war heroes used their celebrity to win national office: Andrew Jackson (elected President in 1828 and 1832), Richard Mentor Johnson (elected Vice President in 1836), and William Henry Harrison (elected President in 1840).
New England states became increasingly frustrated over how the war was being conducted and how the conflict was affecting them. They complained that the U.S. government was not investing enough in the states' defences militarily and financially, and that the states should have more control over their militia. The increased taxes, the British blockade, and the occupation of some of New England by enemy forces also agitated public opinion in the states. As a result, at the Hartford Convention (December 1814 – January 1815) Federalist delegates deprecated the war effort and sought more autonomy for the New England states. They did not call for secession but word of the angry anti-war resolutions appeared at the same time that peace was announced and the victory at New Orleans was known. The upshot was that the Federalists were permanently discredited and quickly disappeared as a major political force.
This war enabled thousands of slaves to escape to British lines or ships for freedom, despite the difficulties. The planters' complacency about slave contentment was shocked by their seeing slaves who would risk so much to be free.
In 1815, with the British gone, most of the Indian tribes of the Midwest made peace with the United States. In the next 15 years they signed a series of treaties selling approximately half of Michigan, half of Indiana, and two thirds of Illinois to the U.S. government, which set up a process for selling the land to white farmers. Pratt concludes," the war had given the Northwest what it most desired." After the decisive defeat of the Creek Indians at the battle of Horseshoe Bend in 1814, some warriors escaped to join the Seminoles in Florida. The remaining Creek chiefs signed away about half their lands, comprising 23,000,000 acres, covering much of southern Georgia and two thirds of modern Alabama. The Creeks were now separated from any future help from the Spanish in Florida, or from the Choctaw and Chickasaw to the west. During the war the United States seized Mobile, Alabama, which was a strategic location providing oceanic outlet to the cotton lands to the north. Jackson invaded Florida in 1818, demonstrating to Spain that could no longer control that territory with a small force. Spain sold Florida to the United States in 1819 in the Adams-Onís Treaty following the First Seminole War. Pratt concludes:
British North America (Canada).
Pro-British leaders demonstrated a strong hostility to American influences in western Canada (Ontario) after the war and shaped its policies, including a hostility to American-style republicanism. Immigration from the U.S. was discouraged, and favor was shown to the Anglican church as opposed to the more Americanized Methodist church.
The Battle of York showed the vulnerability of Upper and Lower Canada. In the 1820s, work began on La Citadelle at Quebec City as a defence against the United States. Additionally, work began on the Halifax citadel to defend the port against foreign navies. From 1826 to 1832, the Rideau Canal was built to provide a secure waterway not at risk from American cannon fire. To defend the western end of the canal, the British Army also built Fort Henry at Kingston.
Indigenous nations.
The Native Americans allied to the British lost their cause. The British proposal to create a "neutral" Indian zone in the American West was rejected at the Ghent peace conference and never resurfaced. After 1814 the natives, who lost most of their fur-gathering territory, became an undesirable burden to British policymakers who now looked to the United States for markets and raw materials. British agents in the field continued to meet regularly with their former American Indian partners, but they did not supply arms or encouragement and there were no American Indian campaigns to stop U.S. expansionism in the Midwest. Abandoned by their powerful sponsor, American Great Lakes-area Indians ultimately migrated or reached accommodations with the American authorities and settlers.
In the Southeast, Indian resistance had been crushed by General Andrew Jackson during the Creek War; as President (1829–37), Jackson systematically expelled the major tribes to reservations west of the Mississippi., part of which was the forced expulsion of American-allied Cherokee in the trail of tears.
Bermuda.
Bermuda had been largely left to the defences of its own militia and privateers before U.S. independence, but the Royal Navy had begun buying up land and operating from there in 1795, as its location was a useful substitute for the lost U.S. ports. It originally was intended to be the winter headquarters of the North American Squadron, but the war saw it rise to a new prominence. As construction work progressed through the first half of the 19th century, Bermuda became the permanent naval headquarters in Western waters, housing the Admiralty and serving as a base and dockyard. The military garrison was built up to protect the naval establishment, heavily fortifying the archipelago that came to be described as the "Gibraltar of the West". Defence infrastructure would remain the central leg of Bermuda's economy until after World War II.
Britain.
The war is seldom remembered in Great Britain. The massive ongoing conflict in Europe against the French Empire under Napoleon ensured that the War of 1812 against America was never seen as more than a sideshow to the main event by the British. Britain's blockade of French trade had been entirely successful and the Royal Navy was the world's dominant nautical power (and would remain so for another century). While the land campaigns had contributed to saving Canada, the Royal Navy had shut down American commerce, bottled up the U.S. Navy in port and heavily suppressed privateering. British businesses, some affected by rising insurance costs, were demanding peace so that trade could resume with the U.S. The peace was generally welcomed by the British, though there was disquiet at the rapid growth of the U.S. However, the two nations quickly resumed trade after the end of the war and, over time, a growing friendship.
Hickey argues that for Britain:

</doc>
<doc id="34061" url="https://en.wikipedia.org/wiki?curid=34061" title="Winter">
Winter

Winter is the coldest season of the year in polar and temperate climates, between autumn and spring. Winter is caused by the axis of the Earth in that hemisphere being oriented away from the Sun. Different cultures define different dates as the start of winter, and some use a definition based on weather. When it is winter in the Northern Hemisphere, it is summer in the Southern Hemisphere, and vice versa. In many regions, winter is associated with snow and freezing temperatures. The moment of winter solstice is when the sun's elevation with respect to the North or South Pole is at its most negative value (that is, the sun is at its farthest below the horizon as measured from the pole), meaning this day will have the shortest day and the longest night. The earliest sunset and latest sunrise dates outside the polar regions differ from the date of the winter solstice, however, and these depend on latitude, due to the variation in the solar day throughout the year caused by the Earth's elliptical orbit (see earliest and latest sunrise and sunset).
Etymology.
The English word "winter" comes from the Proto-Indo-European word "Wend", that stood for water.
Cause.
The tilt of the Earth's axis relative to its orbital plane plays a big role in the weather. The Earth is tilted at an angle of 23.44° to the plane of its orbit, and this causes different latitudes on the Earth to directly face the Sun as the Earth moves through its orbit. It is this variation that primarily brings about the seasons. When it is winter in the Northern Hemisphere, the Southern Hemisphere faces the Sun more directly and thus experiences warmer temperatures than the Northern Hemisphere. Conversely, winter in the Southern Hemisphere occurs when the Northern Hemisphere is tilted more toward the Sun. From the perspective of an observer on the Earth, the winter Sun has a lower maximum altitude in the sky than the summer Sun.
During winter in either hemisphere, the lower altitude of the Sun causes the sunlight to hit that hemisphere at an oblique angle. In regions experiencing winter, the same amount of solar radiation is spread out over a larger area. This effect is compounded by the larger distance that the light must travel through the atmosphere, allowing the atmosphere to dissipate more heat. Compared with these effects, the changes in the distance of the earth from the sun are negligible.
The manifestation of the meteorological winter (freezing temperatures) in the northerly snow–prone parallels is highly variable depending on elevation, position versus marine winds and the amount of precipitation. A case of point is in Canada which is a country normally associated with its tough winters. Winnipeg on the Great Plains at a relative distance from large bodies of water has a January high of and a low of . In comparison, Vancouver on the coast with a marine influence from moderating Pacific winds has a January low of with days well above freezing at . Both areas are on the 49th parallel north and in the same western half of the continent. Similar effects although less extreme differentials are being found in Europe, where the British Isles do not have a single non-mountain station below freezing in mean temperatures in spite of their northerly position.
Meteorological reckoning.
Meteorological winter is the method of measuring the winter season used by meteorologists based on "sensible weather patterns" for record keeping purposes, so the start of meteorological winter varies with latitude. Winter is often defined by meteorologists to be the three calendar months with the lowest average temperatures. This corresponds to the months of December, January and February in the Northern Hemisphere, and June, July and August in the Southern Hemisphere. The coldest average temperatures of the season are typically experienced in January or February in the Northern Hemisphere and in June, July or August in the Southern Hemisphere. Nighttime predominates in the winter season, and in some regions winter has the highest rate of precipitation as well as prolonged dampness because of permanent snow cover or high precipitation rates coupled with low temperatures, precluding evaporation. Blizzards often develop and cause many transportation delays. Diamond dust, also known as ice needles or ice crystals, forms at temperatures approaching due to air with slightly higher moisture from aloft mixing with colder, surface based air. They are made of simple ice crystals that are hexagonal in shape. The Swedish meteorological institute (SMHI) define winter as when the daily mean temperatures go below for five consecutive days. According to the SMHI, winter in Scandinavia is more pronounced when Atlantic low–pressure systems take more southerly and northerly routes, leaving the path open for high–pressure systems to come in and cold temperatures to occur. As a result, the coldest January on record in 1987 was also the sunniest in Stockholm.
Accumulations of snow and ice are commonly associated with winter in the Northern Hemisphere, due to the large land masses there. In the Southern Hemisphere, the more maritime climate and the relative lack of land south of 40°S makes the winters milder; thus, snow and ice are less common in inhabited regions of the Southern Hemisphere. In this region, snow occurs every year in elevated regions such as the Andes, the Great Dividing Range in Australia, and the mountains of New Zealand, and also occurs in the southerly Patagonia region of South America. Snow occurs year-round in Antarctica.
Astronomical and other calendar-based reckoning.
In the Northern Hemisphere, some authorities define the period of winter based on astronomical fixed points (i.e. based solely on the position of the Earth in its orbit around the sun), regardless of weather conditions. In one version of this definition, winter begins at the winter solstice and ends at the vernal equinox.
These dates are somewhat later than those used to define the beginning and end of the meteorological winter – usually considered to span the entirety of December, January, and February in the Northern Hemisphere and June, July, and August in the Southern.
Astronomically, the winter solstice, being the day of the year which has fewest hours of daylight, ought to be the middle of the season, but seasonal lag means that the coldest period normally follows the solstice by a few weeks. In some cultures, the season is regarded as beginning at the solstice and ending on the following equinox – in the Northern Hemisphere, depending on the year, this corresponds to the period between 21 or 22 December and 19, 20 or 21 March.
In the UK, meteorologists consider winter to be the three coldest months of December, January and February.
In Scandinavia, winter in one tradition begins on 14 October and ends on the last day of February. In Russia, currently calendar winter starts on 1 December and lasts through to the end of February, though traditionally it was reckoned from the Christmas (25 December in Julian calendar, or 7 January in Gregorian) until the Annunciation (25 March in Julian). In many countries in the Southern Hemisphere, including Australia, New Zealand and South Africa, winter begins on 1 June and ends on 31 August. In Celtic nations such as Ireland (using the Irish calendar) and in Scandinavia, the winter solstice is traditionally considered as midwinter, with the winter season beginning 1 November, on All Hallows, or Samhain. Winter ends and spring begins on Imbolc, or Candlemas, which is 1 or 2 February. This system of seasons is based on the length of days exclusively. (The three-month period of the shortest days and weakest solar radiation occurs during November, December and January in the Northern Hemisphere and May, June and July in the Southern Hemisphere.)
Also, many mainland European countries tend to recognize Martinmas or St. Martin's Day (11 November), as the first calendar day of winter. The day falls at midpoint between the old Julian equinox and solstice dates. Also, Valentine's Day (14 February) is recognized by some countries as heralding the first rites of spring, such as flowers blooming.
In Chinese astronomy and other East Asian calendars, winter is taken to commence on or around 7 November, with the "Jiéqì" (known as 立冬 "lì dōng"—literally, "establishment of winter").
The three-month period associated with the coldest average temperatures typically begins somewhere in late November or early December in the Northern Hemisphere and lasts through late February or early March. This "thermological winter" is earlier than the solstice delimited definition, but later than the daylight (Celtic) definition. Depending on seasonal lag, this period will vary between climatic regions.
Cultural influences such as Christmas creep may have led to the winter season being perceived as beginning earlier in recent years, although high latitude countries like Canada are usually well into their real winters before the December solstice.
Ecological reckoning and activity.
Ecological reckoning of winter differs from calendar-based by avoiding the use of fixed dates. It is one of six seasons recognized by most ecologists who customarily use the term "hibernal" for this period of the year (the other ecological seasons being prevernal, vernal, estival, serotinal, and autumnal). The hibernal season coincides with the main period of biological dormancy each year whose dates vary according to local and regional climates in temperate zones of the Earth. The appearance of flowering plants like the crocus can mark the change from ecological winter to the prevernal season as early as late January in mild temperate climates.
To survive the harshness of winter, many animals have developed different behavioral and morphological adaptations for overwintering:
Some annual plants never survive the winter. Other annual plants require winter cold to complete their life cycle, this is known as vernalization. As for perennials, many small ones profit from the insulating effects of snow by being buried in it. Larger plants, particularly deciduous trees, usually let their upper part go dormant, but their roots are still protected by the snow layer. Few plants bloom in the winter, one exception being the flowering plum, which flowers in time for Chinese New Year. The process by which plants become acclimated to cold weather is called hardening.
Humans and winter.
Humans evolved in tropical climates, and met cold weather as they migrated into Eurasia, although earlier populations certainly encountered Southern Hemisphere winters in Southern Africa. Micro-evolution in Caucasian, Asiatic and Inuit people show some adaptation to the climate.
Winter and human health.
Humans are sensitive to cold, see hypothermia. Snowblindness, norovirus, seasonal depression, slipping on black ice and falling icicles are other health concerns associated with cold and snowy weather. In the Northern Hemisphere, it is not unusual for homeless people to die from hypothermia in the winter.
One of the most common diseases associated with winter is influenza. Symptoms include: headache, fever, muscle pains, sinus infection, fatigue, dizziness, cough, and loss of appetite.
Mythology.
In Persian culture, the winter solstice is called Yaldā (meaning: birth) and it has been celebrated for thousands of years. It is referred to as the eve of the birth of Mithra, who symbolised light, goodness and strength on earth.
In Greek mythology, Hades kidnapped Persephone to be his wife. Zeus ordered Hades to return her to Demeter, the goddess of the Earth and her mother. However, Hades tricked Persephone into eating the food of the dead, so Zeus decreed that Persephone would spend six months with Demeter and six months with Hades. During the time her daughter is with Hades, Demeter became depressed and caused winter.
In Welsh mythology, Gwyn ap Nudd abducted a maiden named Creiddylad. On May Day, her lover, Gwythr ap Greidawl, fought Gwyn to win her back. The battle between them represented the contest between summer and winter.
In Bengali, the advent of winter is often expressed by the sentence ""Sheeter buri ashchhe dheye"" which means "the winter old woman is coming fast". This is used especially when it is said to a child.

</doc>
<doc id="34062" url="https://en.wikipedia.org/wiki?curid=34062" title="WAV">
WAV

Waveform Audio File Format (WAVE, or more commonly known as WAV due to its filename extension) (rarely, "Audio for Windows") is a Microsoft and IBM audio file format standard for storing an audio bitstream on PCs. It is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in "chunks", and thus is also close to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively. It is the main format used on Windows systems for raw and typically uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.
Description.
Both WAVs and AIFFs are compatible with Windows, Macintosh, and Linux operating systems. The format takes into account some differences of the Intel CPU such as little-endian byte order. The RIFF format acts as a "wrapper" for various audio coding formats.
Though a WAV file can contain compressed audio, the most common WAV audio format is uncompressed audio in the linear pulse code modulation (LPCM) format. LPCM is also the standard audio coding format for audio CDs, which store two-channel LPCM audio sampled 44,100 times per second with 16 bits per sample. Since LPCM is uncompressed and retains all of the samples of an audio track, professional users or audio experts may use the WAV format with LPCM audio for maximum audio quality. WAV files can also be edited and manipulated with relative ease using software.
The WAV format supports compressed audio, using, on Windows, the Audio Compression Manager. Any ACM codec can be used to compress a WAV file. The user interface (UI) for Audio Compression Manager may be accessed through various programs that use it, including Sound Recorder in some versions of Windows.
Beginning with Windows 2000, a codice_1 header was defined which specifies multiple audio channel data along with speaker positions, eliminates ambiguity regarding sample types and container sizes in the standard WAV format and supports defining custom extensions to the format chunk.
There are some inconsistencies in the WAV format: for example, 8-bit data is unsigned while 16-bit data is signed, and many chunks duplicate information found in other chunks.
Specification.
The WAV file is an instance of a Resource Interchange File Format (RIFF) defined by IBM and Microsoft.
RIFF.
A RIFF file is a tagged file format. It has a specific container format (a chunk) that includes a four character tag (FOURCC) and the size (number of bytes) of the chunk. The tag specifies how the data within the chunk should be interpreted, and there are several standard FOURCC tags. Tags consisting of all capital letters are reserved tags. The outermost chunk of a RIFF file has a codice_2 form tag; the first four bytes of chunk data are a FOURCC that specify the form type and are followed by a sequence of subchunks. In the case of a WAV file, those four bytes are the FOURCC codice_3. The remainder of the RIFF data is a sequence of chunks describing the audio information.
The advantage of a tagged file format is that the format can be extended later without confusing existing file readers. The rule for a RIFF (or WAV) reader is that it should ignore any tagged chunk that it does not recognize. The reader won't be able to use the new information, but the reader should not be confused.
The specification for RIFF files includes the definition of an codice_4 chunk. The chunk may include information such as the title of the work, the author, the creation date, and copyright information. Although the codice_4 chunk was defined in version 1.0, the chunk was not referenced in the formal specification of a WAV file. If the chunk were present in the file, then a reader should know how to interpret it, but many readers had trouble. Some readers would abort when they encountered the chunk, some readers would process the chunk if it were the first chunk in the RIFF form, and other readers would process it if it followed all of the expected waveform data. Consequently, the safest thing to do from an interchange standpoint was to omit the codice_4 chunk and other extensions and send a lowest-common-denominator file. There are other INFO chunk placement problems.
RIFF files were expected to be used in international environments, so there is codice_7 chunk to specify the country code, language, dialect, and code page for the strings in a RIFF file. For example, specifying an appropriate codice_7 chunk should allow the strings in an codice_4 chunk (and other chunks throughout the RIFF file) to be interpreted as Cyrillic or Japanese characters.
RIFF also defines a codice_10 chunk whose contents are uninteresting. The chunk allows a chunk to be deleted by just changing its FOURCC. The chunk could also be used to reserve some space for future edits so the file could be modified without being rewritten. A later definition of RIFF introduced a similar codice_11 chunk.
RIFF WAVE.
The toplevel definition of a WAV file is:
The definition shows a toplevel RIFF form with the codice_3 tag. It is followed by a mandatory codice_13 format chunk that describes the format of the sample data that follows. The format chunk includes information such as the sample encoding, number of bits per channel, the number of channels, the sample rate. The WAV specification includes some optional features. The optional fact chunk reports the number of samples for some compressed coding schemes. The cue point (codice_14) chunk identifies some significant sample numbers in the wave file. The playlist chunk allows the samples to be played out of order or repeated rather than just from beginning to end. The associated data list allows labels and notes (codice_15 and codice_16) to be attached to cue points; text annotation (codice_17) may be given for a group of samples (e.g., caption information). Finally, the mandatory wave data chunk contains the actual samples (in the specified format).
Note that the WAV file definition does not show where an codice_4 chunk should be placed. It is also silent about the placement of a codice_7 chunk (which specifies the character set used).
The RIFF specification attempts to be a formal specification, but its formalism lacks the precision seen in other tagged formats. For example, the RIFF specification does not clearly distinguish between a set of subchunks and an ordered sequence of subchunks. The RIFF form chunk suggests it should be a sequence container. The specification suggests a LIST chunk is also a sequence: "A LIST chunk contains a list, or ordered sequence, of subchunks." However, the specification does not give a formal specification of the codice_4 chunk; an example codice_4 LIST chunk ignores the chunk sequence implied in the codice_4 description. The LIST chunk definition for codice_23 does use the LIST chunk as a sequence container with good formal semantics.
The WAV specification allows for not only a single, contiguous, array of audio samples, but also discrete blocks of samples and silence that are played in order. Most WAV files use a single array of data. The specification for the sample data is confused:
These productions are confused. Apparently codice_24 (undefined) and codice_25 (defined but not referenced) should be identical. Even if that problem is fixed, the productions then allow a codice_26 to contain a recursive codice_23 (which implies data interpretation problems). The specification should have been something like:
to avoid the recursion.
WAV files can contain embedded IFF "lists", which can contain several "sub-chunks".
Metadata.
As a derivative of RIFF, WAV files can be tagged with metadata in the INFO chunk. In addition, WAV files can embed any kind of metadata, including but not limited to Extensible Metadata Platform (XMP) data or ID3 tags in extra chunks. Applications may not handle this extra information or may expect to see it in a particular place. Although the RIFF specification requires that applications ignore chunks they do not recognize, some applications are confused by additional chunks.
Popularity.
Uncompressed WAV files are large, so file sharing of WAV files over the Internet is uncommon. However, it is a commonly used file type, suitable for retaining first generation archived files of high quality, for use on a system where disk space is not a constraint, or in applications such as audio editing, where the time involved in compressing and uncompressing data is a concern.
More frequently, the smaller file sizes of compressed but lossy formats such as MP3 are used to store and transfer audio. Their small file sizes allow faster Internet transmission, as well as lower consumption of space on memory media. There are also lossless-compression formats such as FLAC.
The usage of the WAV format has more to do with its familiarity and simple structure. Because of this, it continues to enjoy widespread use with a variety of software applications, often functioning as a 'lowest common denominator' when it comes to exchanging sound files among different programs.
Use by broadcasters.
In spite of their large size, uncompressed WAV files are sometimes used by some radio broadcasters, especially those that have adopted a tapeless system.
Limitations.
The WAV format is limited to files that are less than 4 GB, because of its use of a 32-bit unsigned integer to record the file size header (some programs limit the file size to 2 GB). Although this is equivalent to about 6.8 hours of CD-quality audio (44.1 kHz, 16-bit stereo), it is sometimes necessary to exceed this limit, especially when greater sampling rates, bit resolutions or channel count are required. The W64 format was therefore created for use in Sound Forge. Its 64-bit header allows for much longer recording times.
The RF64 format specified by the European Broadcasting Union has also been created to solve this problem.
Non-audio data.
Since the sampling rate of a WAV file can vary from 1 Hz to 4.3 GHz, and the number of channels can be as high as 65535, .wav files have also been used for non-audio data. LTspice, for instance, can store multiple circuit trace waveforms in separate channels, at any appropriate sampling rate, with the full-scale range representing ±1 V or A rather than a sound pressure.
Audio CDs.
Audio CDs do not use the WAV file format, using instead Red Book audio. The commonality is that both audio CDs and WAV files encode the audio as PCM. WAV is a file format for a computer to use that cannot be understood by most CD players directly. To record WAV files to an Audio CD the file headers must be stripped and the remaining PCM data written directly to the disc as individual tracks with zero-padding added to match the CD's sector size. In order for a WAV file to be able to be burned to a CD, it should be in the 44100 Hz, 16-bit stereo format.
WAV file audio coding formats compared.
Audio in WAV files can be encoded in a variety of audio coding formats, such as GSM or MP3, to reduce the file size.
This is a reference to compare the monophonic (not stereophonic) audio quality and compression bitrates of audio coding formats available for WAV files including PCM, ADPCM, Microsoft GSM 06.10, CELP, SBC, Truespeech and MPEG Layer-3.
The above are WAV files; even those that use MP3 compression have the "codice_28" extension.

</doc>
<doc id="34064" url="https://en.wikipedia.org/wiki?curid=34064" title="Windows 95">
Windows 95

Windows 95 (codenamed Chicago) is a consumer-oriented operating system developed by Microsoft. It was released on August 24, 1995, and was a significant improvement over the company's previous DOS-based Windows products.
Windows 95 merged Microsoft's formerly separate MS-DOS and Windows products. It featured significant improvements over its predecessor, Windows 3.1, most notably in the graphical user interface (GUI) and in its simplified "plug-n-play" features. There were also major changes made to the core components of the operating system, such as moving from a mainly co-operatively multitasked 16-bit architecture to a pre-emptively multitasked 32-bit architecture.
Accompanied by an extensive marketing campaign, Windows 95 introduced numerous functions and features that were featured in later Windows versions, such as the taskbar, the 'Start' button, and the way the user navigates. It was also suggested that Windows 95 had an effect of driving other major players (including OS/2) out of business, something which would later be used in court against Microsoft.
Three years after its introduction, Windows 95 was succeeded by Windows 98. Microsoft ended support for Windows 95 on December 31, 2001.
Development.
The initial design and planning of Windows 95 can be traced back to around March 1992, just after the release of Windows 3.1. At this time, "Windows for Workgroups 3.11" and Windows NT 3.1 were still in development and Microsoft's plan for the future was focused on Cairo. Cairo would be Microsoft's next-generation operating system based on Windows NT and featuring a new user interface and an object-based file system, but it was not planned to be shipped before 1994. However, Cairo would partially ship in July 1996 in the form of Windows NT 4.0, but without the object-based file system, which would later evolve into WinFS.
Simultaneously with Windows 3.1's release, IBM started shipping OS/2 2.0. Microsoft realized they were in need of an updated version of Windows that could support 32-bit applications and preemptive multitasking, but could still run on low-end hardware (Windows NT did not). So the development of Windows "Chicago" was started and, as it was planned for a late 1993 release, became known as Windows 93. Initially, the decision was made not to include a new user interface, as this was planned for Cairo, and only focus on making installation, configuration, and networking easier. Windows 93 would ship together with MS-DOS 7.0, offering a more integrated experience to the user and making it pointless for other companies to create DOS clones. MS-DOS 7.0 was in development at that time under the code name "Jaguar" and could optionally run on top of a Windows 3.1-based 32-bit protected mode kernel called "Cougar" in order to better compete with DR-DOS. The first version of Chicago's feature specification was finished on September 30, 1992. Cougar was to become Chicago's kernel.
Beta.
Prior to the official release, the American public was given a chance to preview Windows 95 in the Windows 95 Preview Program. For US$19.95, users were sent a set of 3.5-inch floppy diskettes that would install Windows 95 either as an upgrade to Windows 3.1x or as a fresh install on a clean computer. Users who bought into the program were also given a free preview of The Microsoft Network (MSN), the online service that Microsoft launched with Windows 95. During the preview period Microsoft established various electronic distribution points for promotional and technical documentation on Chicago including a detailed document for media reviewers describing the new system highlights. The preview versions expired in November 1995, after which the user would have to purchase their own copy of the final version of Windows 95.
Architecture.
Windows 95 was designed to be maximally compatible with existing MS-DOS and 16-bit Windows programs and device drivers, while offering a more stable and better performing system. Windows 95 architecture is an evolution of Windows for Workgroups' 386 enhanced mode. The lowest level of the operating system consists of a large number of "virtual device drivers" (VxDs) running in 32-bit protected mode and one or more virtual DOS machines running in virtual 8086 mode. The virtual device drivers are responsible for handling physical devices (such as video and network cards), emulating virtual devices used by the virtual machines, or providing various system services. The three most important virtual device drivers are:
Access requests to physical media are sent to "Input/Output Supervisor", a component responsible for scheduling the requests. Each physical media has its own device driver: Access to the disk is performed by a "port driver", while access to a SCSI device is handled by a "miniport" driver working atop the SCSI layer. Port and miniport drivers perform I/O operations in 32-bit protected mode, bypassing MS-DOS and BIOS, giving a significant performance improvement. In case there is no native Windows driver for a certain storage device, or if a device is forced to run in compatibility mode, the "Real Mode Mapper" can access it through MS-DOS.
32-bit Windows programs are assigned their own memory segments, which can be adjusted to any size the user wishes. Memory area outside the segment cannot be accessed by a program. If they crash, they do not harm anything else. Before this, programs used fixed non-exclusive 64 KB segments. While the 64 KB size was a serious handicap in DOS and Windows 3.x, lack of guarantee of exclusiveness was the cause of stability issues because programs sometimes overwrote each other's segments. A crashing Windows 3.x program could knock out surrounding processes.
The Win32 API is implemented by three modules, each consisting of a 16-bit and a 32-bit component:
Dependence on MS-DOS.
To end-users, MS-DOS appears as an underlying component of Windows 95. For example, it is possible to prevent loading the graphical user interface and boot the system into a real-mode MS-DOS environment. This sparked debate amongst users and professionals over the question of to what extent Windows 95 is an operating system or merely a graphical shell running on top of MS-DOS.
When the graphical user interface is started, the virtual machine manager takes over the filesystem-related and disk-related functionality. MS-DOS itself is demoted to a compatibility layer for 16-bit device drivers. This contrasts with earlier versions of Windows which relies on MS-DOS to perform file and disk access (Windows for Workgroups 3.11 could also largely bypass MS-DOS when 32-bit file access and 32-bit disk access were enabled). Keeping MS-DOS in memory allows Windows 95 to use DOS device drivers when suitable Windows drivers are unavailable. Windows 95 is capable of using all 16-bit Windows 3.x drivers.
Contrary to Windows 3.1x, DOS programs running in Windows 95 do not need DOS drivers for mouse, CD-ROM access and sound card; Windows drivers are used instead. HIMEM.SYS is still required to boot Windows 95. EMM386 and other memory managers, however, are only used by legacy DOS programs. In addition, CONFIG.SYS and AUTOEXEC.BAT settings (aside from HIMEM.SYS) have no effect on Windows programs. DOS games, which could not be executed on Windows 3.x, can run inside Windows 95. (Games tended to lock up Windows 3.x or cause other problems). As with Windows 3.x, DOS programs that use EGA or VGA graphics modes run in windowed mode (CGA and text mode programs can continue to run).
On startup, the MS-DOS component in Windows 95 responds to a pressed key by temporarily pausing the default boot process and presenting the DOS boot options menu, allowing the user to continue starting Windows normally, start Windows in safe mode or exit to the DOS prompt. As in previous versions of MS-DOS, there is no 32-bit support and DOS drivers must be loaded for mice and other hardware.
As a consequence of being DOS-based, Windows 95 has to keep internal DOS data structures synchronized with those of Windows 95. When starting a program, even a native 32-bit Windows program, MS-DOS momentarily executes to create a data structure known as the Program Segment Prefix. It is even possible for MS-DOS to run out of conventional memory while doing so, preventing the program from launching. Windows 3.x allocated "fixed" segments in conventional memory first. Since the segments were allocated as fixed, Windows could not move them, which would prevent any more programs from launching.
Microsoft partially removed support for File Control Blocks (an API hold-over of DOS 1.x and CP/M) in Windows 95 OSR 2, Service Release 2, where OEM stands for Original Equipment Manufacturer. FCB functions cannot write to FAT32 volumes, only read them.
User interface.
Windows 95 introduced a redesigned shell based around a desktop metaphor; the desktop was re-purposed to hold shortcuts to applications, files, and folders - unlike Windows 3.1 where it was used to display running applications. Running applications were now displayed as buttons on a taskbar across the bottom of the screen, which also contains a notification area used to display icons for background applications, a volume control, and the current time. The Start menu, invoked by clicking the "Start" button also contained on the taskbar, was introduced as an additional means of launching applications or opening documents. While maintaining the program groups used by its predecessor, Program Manager, it now displayed applications within cascading sub-menus. The previous File Manager program was also replaced by Windows Explorer.
In 1994, Microsoft corporation designers Mark Malamud and Erik Gavriluk approached Brian Eno to compose music for the Windows 95 project. The result was the six-second start-up music-sound of the Windows 95 operating system, "The Microsoft Sound".
When released for Windows 95 and NT4, Internet Explorer 4 came with an optional Windows Desktop Update, which modified the shell to provide new features integrated with Internet Explorer, such as Active Desktop (which allowed internet content to be displayed directly on the desktop) and additional updates to Windows Explorer.
Some of the user interface elements introduced in Windows 95—such as the desktop, taskbar, Start menu, and Windows Explorer file manager, remained fundamentally unchanged on future versions of Windows.
Technical improvements.
Windows 95 included support for 255-character mixed-case long filenames and preemptively multitasked protected-mode 32-bit applications.
Long file names.
32-bit File Access is necessary for the "long file names" feature introduced with Windows 95 through the use of the VFAT file system extension. It is available to both Windows programs and MS-DOS programs started from Windows (they have to be adapted slightly, since accessing long file names requires using larger pathname buffers and hence different system calls). Competing DOS-compatible operating systems released before Windows 95 cannot see these names. Using older versions of DOS utilities to manipulate files means that the long names are not visible and are lost if files are moved or renamed, as well as by the copy (but not the original), if the file is copied. During a Windows 95 automatic upgrade of an older Windows 3.1 system, DOS and third-party disk utilities which can destroy long file names are identified and made unavailable. When Windows 95 is started in DOS mode, e.g. for running DOS programs, low-level access to disks is locked out. In case the need arises to depend on disk utilities that do not recognize long file names, such as the MS-DOS 6.x's defrag utility, a program called LFNBACK for backup and restoration of long file names is provided on the CD-ROM. The program is in the \ADMIN\APPTOOLS\LFNBACK directory of the Windows 95 CD-ROM.
32-bit.
Windows 95 followed Windows for Workgroups 3.11 with its lack of support for older, 16-bit x86 processors, thus requiring an Intel 80386 (or compatible). While the OS kernel is 32-bit, much code (especially for the user interface) remained 16-bit for performance reasons as well as development time constraints (much of Windows 95's UI code was recycled from Windows 3.1). This had a rather detrimental effect on system stability and led to frequent application crashes.
The introduction of 32-bit File Access in Windows for Workgroups 3.11 meant that 16-bit real mode MS-DOS is not used for managing the files while Windows is running, and the earlier introduction of the 32-bit Disk Access means that the PC BIOS is often no longer used for managing hard disks.
DOS can be used for running old-style drivers for compatibility, but Microsoft discourages using them, as this prevents proper multitasking and impairs system stability. Control Panel allows a user to see which MS-DOS components are used by the system; optimal performance is achieved when they are bypassed. The Windows kernel uses MS-DOS style real-mode drivers in "Safe Mode," which exists to allow a user to fix problems relating to loading native, protected-mode drivers.
System requirements.
Official system requirements were an Intel 80386 DX CPU of any speed, 4 MB of system RAM, and 50–55 MB of hard drive space depending on features selected. These minimal claims were made in order to maximize the available market of Windows 3.1 converts. This configuration would rely heavily on virtual memory and was suboptimal for productive use on anything but single tasking dedicated workstations. Also, in some cases, if any networking or similar components were installed the system would refuse to boot with 4 megabytes of RAM. It was possible to run Windows 95 on a 386 SX but this led to even less acceptable performance due to its 16-bit external data bus. To achieve optimal performance, Microsoft recommends an Intel 80486 or compatible microprocessor with at least 8 MB of RAM. Windows 95 may fail to boot on computers with more than approximately 480 MB of memory. In such case reducing the file cache size or the size of video memory can help. The theoretical maximum according to Microsoft is 2 GB.
Windows 95 was superseded by Windows 98 and could still be directly upgraded by both Windows 2000 Professional edition and Windows ME. On December 31, 2001, Microsoft ended its support for Windows 95, making it an "obsolete" product according to the Microsoft Lifecycle Policy. Even though support for Windows 95 has ended, the software has occasionally remained in use on legacy systems for various purposes. In addition, some video game enthusiasts choose to use Windows 95 for their legacy system to play old DOS games, although some other versions of Windows such as Windows 98 can also be used for this purpose.
Most copies of Windows 95 were on CD-ROM, but a floppy version could also be had for older machines. The retail floppy disk version of Windows 95 came on 13 DMF formatted floppy disks, while OSR 2.1 doubled the floppy count to 26. Both versions exclude additional software that CD-ROM might have featured. Microsoft Plus! for Windows 95 was also available on floppy disks. DMF was a special 21-sector format Microsoft used to store 1.68MB on floppies rather than the usual 1.44MB. While the floppy edition of Windows was normally on 3.5" disks, a 5.25" version could be specially ordered as well.
Internet Explorer.
Windows 95 originally shipped without Internet Explorer, and the default network installation did not install TCP/IP, the network protocol used on the Internet. At the release date of Windows 95, Internet Explorer 1.0 was available, but only in the Plus! add-on pack for Windows 95, which was a separate product. The Plus! Pack did not reach as many retail consumers as the operating system itself (it was mainly advertised for its non-internet-related add-ons such as themes and better disk compression) but was usually included in pre-installed (OEM) sales, and at the time of Windows 95 release, the web was being browsed mainly with a variety of early web browsers such as NCSA Mosaic and Netscape Navigator (promoted by products such as Internet in a Box).
Windows 95 OEM Service Release 1 was the first release of Windows to include Internet Explorer (version 2.0) with the OS. While there was no uninstaller, it could be deleted easily if the user so desired. OEM Service Release 2 included Internet Explorer 3. The installation of Internet Explorer 4 on Windows 95 (or the OSR2.5 version preinstalled on a computer) gave Windows 95 active desktop and browser integration into Windows Explorer, known as the Windows Desktop Update. The CD version of the last release of Windows 95, OEM Service Release 2.5 (Version 4.00.950C), includes Internet Explorer 4, and installs it after Windows 95's initial setup and first boot is complete.
Only the 4.x series of the browser contained the Windows Desktop Update features, so anyone wanting the new shell had to install IE4 with the desktop update before installing a newer version of Internet Explorer. The last version of Internet Explorer supported on Windows 95 is Internet Explorer 5.5 which was released in 2000. Windows 95 shipped with Microsoft's own dial-up online service called The Microsoft Network (MSN).
Release and promotion.
Windows 95 was released with great fanfare, including a commercial featuring the Rolling Stones' 1981 single "Start Me Up" (a reference to the Start button). It was widely reported that Microsoft paid the Rolling Stones between US$8 and US$14 million for the use of the song in the 95 advertising campaign. According to sources at Microsoft, however, this was just a rumor spread by the band to increase their market value, and Microsoft actually paid a fraction of that amount. A 30-minute promotional video, labeled a "cyber sitcom", featuring Jennifer Aniston and Matthew Perry, was also released to showcase the features of Windows 95. Microsoft's US$300 million advertising campaign featured stories of people waiting in line outside stores to get a copy.
In the UK, the largest computer chain PC World received a large number of oversized Windows 95 boxes, posters and point of sale material, and many branches opened at midnight to sell the first copies of the product. Copies of The Times were available for free, Microsoft paid for 1.5 million issues (twice the daily circulation at the time).
In the United States, the Empire State Building in New York City was lit to match the colors of the Windows logo. In Canada, a banner was hung from the top of the CN Tower in Toronto.
The release included a number of "Fun Stuff" items on the CD, including music videos of Edie Brickell's "Good Times" and Weezer's "Buddy Holly", a trailer for the 1995 film "Rob Roy", and the computer game Hover!.
Editions.
A number of editions of Windows 95 have been released. Only the original release was sold as a shrink-wrapped product, later editions were provided only to computer original equipment manufacturers (OEMs) for installation on new PCs. For this reason these editions are known as OEM Service Releases (OSR).
Together with the introduction of Windows 95, Microsoft released the "Microsoft Plus! for Windows 95" pack, which contained a number of optional components for high-end (486) multimedia PCs, including Internet Explorer, DriveSpace, and additional themes.
Microsoft initially indicated to make updates available to Windows 95 every 6 months in the form of service packs. The growing availability of Internet access meant that Windows updates could now be downloaded from Microsoft directly. The first service pack was made available half a year after the original release and fixed a number of small bugs.
The second service pack mainly introduced support for new hardware. Most notably support for hard drives larger than 2 GB in the form of the FAT32 file system. This release was never made available to end-users directly and was only sold through OEMs with the purchase of a new PC. 
A full third service pack was never released, but two smaller updates to the second were released in the form of a USB Supplement (OSR 2.1) and the Windows Desktop Update (OSR 2.5). Both were available as stand-alone updates and as updated disc images shipped by OEMs. OSR 2.5 was notable for featuring a number of changes to the Windows Explorer, integrating it with Internet Explorer 4.0—this version of the Explorer looks very similar to the one featured in Windows 98.
Legacy.
Many features that have become key components of the Microsoft Windows series, such as the Start menu and the taskbar, originated in Windows 95. Neil MacDonald, a Gartner analyst, said "If you look at Windows 95, it was a quantum leap in difference in technological capability and stability." Ina Fried of "CNET" said "By the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world."
Further reading.
Microsoft:
Third-party:

</doc>
<doc id="34068" url="https://en.wikipedia.org/wiki?curid=34068" title="Wilmington">
Wilmington

Wilmington may refer to:

</doc>
<doc id="34069" url="https://en.wikipedia.org/wiki?curid=34069" title="Winter Olympic Games">
Winter Olympic Games

The Winter Olympic Games (French: "Jeux olympiques d'hiver") is a major international sporting event that occurs once every four years. Unlike the Summer Olympics, the Winter Olympics feature sports practiced on snow and ice. The first Winter Olympics, the 1924 Winter Olympics, was held in Chamonix, France. The original five sports (broken into nine disciplines) were bobsleigh, curling, ice hockey, Nordic skiing (consisting of the disciplines military patrol, cross-country skiing, Nordic combined, and ski jumping), and skating (consisting of the disciplines figure skating and speed skating). The Games were held every four years from 1924 until 1936, after which they were interrupted by World War II. The Olympics resumed in 1948 and was again held every four years. Until 1992, the Winter and Summer Olympic Games were held in the same years, but in accordance with a 1986 decision by the International Olympic Committee (IOC) to place the Summer and Winter Games on separate four-year cycles in alternating even-numbered years, the next Winter Olympics after 1992 was in 1994.
The Winter Games have evolved since its inception. Sports and disciplines have been added and some of them, such as Alpine skiing, luge, short track speed skating, freestyle skiing, skeleton, and snowboarding, have earned a permanent spot on the Olympic program. Others (such as curling and bobsleigh) have been discontinued and later reintroduced, or have been permanently discontinued (such as military patrol, though the modern Winter Olympic sport of biathlon is descended from it). Still others, such as speed skiing, bandy and skijoring, were demonstration sports but never incorporated as Olympic sports. The rise of television as a global medium for communication enhanced the profile of the Games. It created an income stream, via the sale of broadcast rights and advertising, which has become lucrative for the IOC. This allowed outside interests, such as television companies and corporate sponsors, to exert influence. The IOC has had to address several criticisms, internal scandals, the use of performance-enhancing drugs by Winter Olympians, as well as a political boycott of the Winter Olympics. Nations have used the Winter Games to showcase the claimed superiority of their political systems.
The Winter Olympics has been hosted on three continents by eleven different countries. The United States has hosted the Games four times (1932, 1960, 1980, 2002); France has been the host three times (1924, 1968, 1992); Austria (1964, 1976), Canada (1988, 2010), Japan (1972, 1998), Italy (1956, 2006), Norway (1952, 1994), and Switzerland (1928, 1948) have hosted the Games twice. Germany (1936), Yugoslavia (1984), and Russia (2014) have hosted the Games once. The IOC has selected Pyeongchang, South Korea, to host the 2018 Winter Olympics and Beijing, China, to host the 2022 Winter Olympics. No country in the southern hemisphere has hosted or even been an applicant to host the Winter Olympics; the major challenge preventing one hosting the games is the dependence on winter weather, and the traditional February timing of the games falls in the middle of the southern hemisphere summer.
Twelve countries – Austria, Canada, Finland, France, Great Britain, Hungary, Italy, Norway, Poland, Sweden, Switzerland and the United States – have sent athletes to every Winter Olympic Games. Six of those – Austria, Canada, Finland, Norway, Sweden and the United States – have earned medals at every Winter Olympic Games, and only one – the United States – has earned gold at each Games. Germany and Japan have been banned at times from competing in the Games.
Sports.
The Olympic Charter limits winter sports to "those ... which are practised on snow or ice." Since 1992 a number of new sports have been added to the Olympic programme; which include short track speed skating, snowboarding, freestyle and moguls skiing. The addition of these events has broadened the appeal of the Winter Olympics beyond Europe and North America. While European powers such as Norway and Germany still dominate the traditional Winter Olympic sports, countries such as South Korea, Australia and Canada are finding success in the new sports. The results are more parity in the national medal tables, more interest in the Winter Olympics and higher global television ratings.
Current sport disciplines.
Note 1. Figure skating events were held at the 1908 and 1920 Summer Olympics.
Note 2. A men's ice hockey tournament was held at the 1920 Summer Olympics.
Note 3. The IOC's website now treats Men's Military Patrol at the 1924 games as an event within the sport of Biathlon.
Demonstration events.
Demonstration sports have historically provided a venue for host countries to attract publicity to locally popular sports by having a competition without granting medals. Demonstration sports were discontinued after 1992. Military patrol, a precursor to the biathlon, was a medal sport in 1924 and was demonstrated in 1928, 1936 and 1948, becoming an official sport in 1960. The special figures figure skating event was only contested at the 1908 Summer Olympics. Bandy (Russian hockey) is a sport popular in the Nordic countries and Russia. In the latter it's considered a national sport. It was demonstrated at the Oslo Games. Ice stock sport, a German variant of curling, was demonstrated in 1936 in Germany and 1964 in Austria. The ski ballet event, later known as ski-acro, was demonstrated in 1988 and 1992. Skijöring, skiing behind dogs, was a demonstration sport in St. Moritz in 1928. A sled-dog race was held at Lake Placid in 1932. Speed skiing was demonstrated in Albertville at the 1992 Winter Olympics. Winter pentathlon, a variant of the modern pentathlon, was included as a demonstration event at the 1948 Games in Switzerland. It was composed of cross-country skiing, shooting, downhill skiing, fencing and horse riding.
History.
Early years.
Three years later, Italian count Eugenio Brunetta d'Usseaux proposed that the IOC stage a week of winter sports included as part of the 1912 Summer Olympics in Stockholm, Sweden. The organizers opposed this idea because they desired to protect the integrity of the Nordic Games and were concerned about a lack of facilities for winter sports. The idea was resurrected for the 1916 Games, which were to be held in Berlin, Germany. A winter sports week with speed skating, figure skating, ice hockey and Nordic skiing was planned, but the 1916 Olympics was cancelled after the outbreak of World War I.
The first Olympics after the war, the 1920 Summer Olympics, were held in Antwerp, Belgium, and featured figure skating and an ice hockey tournament. Germany, Austria, Hungary, Bulgaria and Turkey were banned from competing in the Games. At the IOC Congress held the following year it was decided that the host nation of the 1924 Summer Olympics, France, would host a separate "International Winter Sports Week" under the patronage of the IOC. Chamonix was chosen to host this "week" (actually 11 days) of events. The Games proved to be a success when more than 250 athletes from 16 nations competed in 16 events. Athletes from Finland and Norway won 28 medals, more than the rest of the participating nations combined. Germany remained banned until 1925, and instead hosted a series of games called Deutsche Kampfspiele, starting with the Winter edition of 1922 (which predated the first Winter Olympics). In 1925 the IOC decided to create a separate Olympic Winter Games and the 1924 Games in Chamonix was retroactively designated as the first Winter Olympics.
St. Moritz, Switzerland, was appointed by the IOC to host the second Olympic Winter Games in 1928. Fluctuating weather conditions challenged the hosts. The opening ceremony was held in a blizzard while warm weather conditions plagued sporting events throughout the rest of the Games. Because of the weather the 10,000 metre speed-skating event had to be abandoned and officially cancelled. The weather was not the only noteworthy aspect of the 1928 Games: Sonja Henie of Norway made history when she won the figure skating competition at the age of 15. She became the youngest Olympic champion in history, a distinction she would hold for 74 years.
The next Winter Olympics was the first to be hosted outside of Europe. Seventeen nations and 252 athletes participated. This was less than in 1928 as the journey to Lake Placid, United States, was a long and expensive one for most competitors who had little money in the midst of the Great Depression. The athletes competed in fourteen events in four sports. Virtually no snow fell for two months before the Games, and it was not until mid-January that there was enough snow to hold all the events. Sonja Henie defended her Olympic title and Eddie Eagan, who had been an Olympic champion in boxing in 1920, won the gold in the men's bobsleigh event to become the first, and so far only, Olympian to have won gold medals in both the Summer and Winter Olympics.
The German towns of Garmisch and Partenkirchen joined to organise the 1936 edition of the Winter Games, held on 6–16 February. This would be the last time the Summer and Winter Olympics were held in the same country in the same year. Alpine skiing made its Olympic debut, but skiing teachers were barred from entering because they were considered to be professionals. Because of this decision the Swiss and Austrian skiers refused to compete at the Games.
World War II.
World War II interrupted the celebrations of the Winter Olympics. The 1940 Games had been awarded to Sapporo, Japan, but the decision was rescinded in 1938 because of the Japanese invasion of China. The Games were moved to Garmisch-Partenkirchen, Germany, but the German invasion of Poland in 1939 forced the complete cancellation of the 1940 Games. Due to the ongoing war the 1944 Games, originally scheduled for Cortina D'Ampezzo, Italy, were cancelled.
1948 to 1960.
St. Moritz was selected to host the first post-war Games in 1948. Switzerland's neutrality had protected the town during World War II and most of the venues were in place from the 1928 Games, which made St. Moritz a logical choice. It became the first city to host a Winter Olympics twice. Twenty-eight countries competed in Switzerland, but athletes from Germany and Japan were not invited. Controversy erupted when two hockey teams from the United States arrived, both claiming to be the legitimate U.S. Olympic hockey representative. The Olympic flag presented at the 1920 Summer Olympics in Antwerp was stolen, as was its replacement. There was unprecedented parity at these Games, during which 10 countries won gold medals—more than any Games to that point.
The Olympic Flame for the 1952 Games in Oslo, was lit in the fireplace by skiing pioneer Sondre Nordheim and the torch relay was conducted by 94 participants entirely on skis. Bandy, a popular sport in the Nordic countries, was featured as a demonstration sport, though only Norway, Sweden and Finland fielded teams. Norwegian athletes won 17 medals, which outpaced all the other nations. They were led by Hjalmar Andersen who won three gold medals in four events in the speed skating competition.
After not being able to host the Games in 1944, Cortina d'Ampezzo was selected to organise the 1956 Winter Olympics. At the opening ceremonies the final torch bearer, Guido Caroli, entered the Olympic Stadium on ice skates. As he skated around the stadium his skate caught on a cable and he fell, nearly extinguishing the flame. He was able to recover and light the cauldron. These were the first Winter Games to be televised and the first Olympics ever broadcast nationwide, though no television rights would be sold until the 1960 Summer Olympics in Rome. The Cortina Games were used to test the feasibility of televising large sporting events. The Soviet Union made its Olympic debut and had an immediate impact, winning more medals than any other nation. Chiharu Igaya won the first Winter Olympics medal for Japan and the continent of Asia, when he placed second in the slalom.
The IOC awarded the 1960 Olympics to Squaw Valley, United States. Since the village was underdeveloped, there was a rush to construct infrastructure and sports facilities like an ice arena, speed-skating track, and a ski-jump hill. The opening and closing ceremonies were produced by Walt Disney. The Squaw Valley Olympics had a number of notable firsts: it was the first Olympics to have a dedicated athletes' village, it was the first to use a computer (courtesy of IBM) to tabulate results, and the first to feature female speed skating events. The bobsleigh events were absent for the only time, because the organising committee found it too expensive to build the bobsleigh run.
1964 to 1980.
The Austrian city of Innsbruck was the host in 1964. Although Innsbruck was a traditional winter sports resort, warm weather caused a lack of snow during the Games and the Austrian army was asked to transport snow and ice to the sport venues. Soviet speed-skater Lidia Skoblikova made history by sweeping all four speed-skating events. Her career total of six gold medals set a record for Winter Olympics athletes. Luge was first contested in 1964, although the sport received bad publicity when a competitor was killed in a pre-Olympic training run.
Held in the French town of Grenoble, the 1968 Winter Olympics were the first Olympic Games to be broadcast in colour. There were 37 nations and 1,158 athletes competing in 35 events. Frenchman Jean-Claude Killy became only the second person to win all the men's alpine skiing events. The organising committee sold television rights for $2 million, which was more than double the price of the broadcast rights for the Innsbruck Games. Venues were spread over long distances requiring three athletes' villages. The organisers claimed this was required to accommodate technological advances. Critics disputed this, alleging that the layout was necessary to provide the best possible venues for television broadcasts at the expense of the athletes.
The 1972 Winter Games, held in Sapporo, Japan, were the first to be hosted outside North America or Europe. The issue of professionalism became contentious during the Sapporo Games. Three days before the Games IOC president Avery Brundage threatened to bar a number of alpine skiers from competing because they participated in a ski camp at Mammoth Mountain in the United States. Brundage reasoned that the skiers had financially benefited from their status as athletes and were therefore no longer amateurs. Eventually only Austrian Karl Schranz, who earned more than all the other skiers, was not allowed to compete. Canada did not send teams to the 1972 or 1976 ice hockey tournaments in protest of their inability to use players from professional leagues. Francisco Fernández Ochoa became the first (and only) Spaniard to win a Winter Olympic gold medal; he triumphed in the slalom.
The 1976 Winter Olympics had been awarded in 1970 to Denver, United States, but in November 1972 the voters of the state of Colorado voted against public funding of the games by a 3 to 2 margin. The IOC turned to offer the Games to Vancouver-Garibaldi, British Columbia, which had been a candidate for the 1976 Games. However, a change in provincial government brought in an administration which did not support the Olympic bid, so the offer was rejected. Salt Lake City, a candidate for the 1972 Games, offered itself, but the IOC opted to ask Innsbruck, which had maintained most of the infrastructure from the 1964 Games. With half the time to prepare for the Games as intended, Innsbruck accepted the invitation to replace Denver in February 1973. Two Olympic flames were lit because it was the second time the Austrian town had hosted the Games. The 1976 Games featured the first combination bobsleigh and luge track, in neighbouring Igls. The Soviet Union won its fourth consecutive ice hockey gold medal.
In 1980 the Olympics returned to Lake Placid, which had hosted the 1932 Games. The first boycott of a Winter Olympics occurred in 1980 when Taiwan refused to participate after an edict by the IOC mandated that they change their name and national anthem. The IOC was attempting to accommodate China, who wished to compete using the same name and anthem that had been used by Taiwan. American speed-skater Eric Heiden set either an Olympic or world record in each of the five events he competed in. Hanni Wenzel won both the slalom and giant slalom and her country, Liechtenstein, became the smallest nation to produce an Olympic gold medallist. In the "Miracle on Ice" the American hockey team beat the favoured Soviets, and then went on to win the gold medal.
1984 to 1998.
Sapporo, Japan, and Gothenburg, Sweden, were front-runners to host the 1984 Winter Olympics. It was therefore a surprise when Sarajevo, Yugoslavia, was selected as host. The Games were well-organised and displayed no indication of the war that would engulf the country eight years later. A total of 49 nations and 1,272 athletes participated in 39 events. Host nation Yugoslavia won its first Olympic medal when alpine skier Jure Franko won a silver in the giant slalom. Another sporting highlight was the free dance performance of British ice dancers Jayne Torvill and Christopher Dean. Their performance to Ravel's "Boléro" earned the pair the gold medal after achieving unanimous perfect scores for artistic impression.
In 1988, the Canadian city of Calgary hosted the first Winter Olympics to span 16 days. New events were added in ski-jumping and speed skating; while future Olympic sports curling, short track speed skating and freestyle skiing made their appearance as demonstration sports. For the first time the speed skating events were held indoors, on the Olympic Oval. Dutch skater Yvonne van Gennip won three gold medals and set two world records, beating skaters from the favoured East German team in every race. Her medal total was equalled by Finnish ski jumper Matti Nykänen, who won all three events in his sport. Alberto Tomba, an Italian skier, made his Olympic debut by winning both the giant slalom and slalom. East German Christa Rothenburger won the women's 1,000 metre speed skating event. Seven months later she would earn a silver in track cycling at the Summer Games in Seoul, to become the only athlete to win medals in both a Summer and Winter Olympics in the same year.
The 1992 Games were the last to be held in the same year as the Summer Games. They were hosted in the French Savoie region in the city of Albertville, though only 18 events were held in the city. The rest of the events were spread out over the Savoie. Political changes of the time were reflected in the Olympic teams appearing in France: this was the first Games to be held after the fall of Communism and the dismantling of the Berlin Wall, and Germany competed as a single nation for the first time since the 1964 Games; former Yugoslavian republics Croatia and Slovenia made their debuts as independent nations; most of the former Soviet republics still competed as a single team known as the Unified Team, but the Baltic States made independent appearances for the first time since before World War II. At 16 years old, Finnish ski jumper Toni Nieminen made history by becoming the youngest male Winter Olympic champion. New Zealand skier Annelise Coberger became the first Winter Olympic medallist from the southern hemisphere when she won a silver medal in the women's slalom.
In 1986 the IOC had voted to separate the Summer and Winter Games and place them in alternating even-numbered years. This change became effective for the 1994 Games, held in Lillehammer, Norway, which became the first Winter Olympics to be held separate from the Summer Games. After the division of Czechoslovakia in 1993 the Czech Republic and Slovakia made their Olympic debuts. The women's figure skating competition garnered media attention when American skater Nancy Kerrigan was injured on 6 January 1994, in an assault planned by the ex-husband of opponent Tonya Harding. Both skaters competed in the Games, but the gold medal was won by Oksana Baiul. She became Ukraine's first Olympic champion. Johann Olav Koss of Norway won three gold medals, coming first in all of the distance speed skating events.
The 1998 Winter Olympics were held in the Japanese city of Nagano and were the first Games to host more than 2,000 athletes. The men's ice hockey tournament was opened to professionals for the first time. Canada and the United States, with their many NHL players, were favoured to win the tournament. Neither won any hockey medals however, as the Czech Republic prevailed. Women's ice hockey made its debut and the United States won the gold medal. Bjørn Dæhlie of Norway won three gold medals in Nordic skiing. He became the most decorated Winter Olympic athlete with eight gold medals and twelve medals overall. Austrian Hermann Maier survived a crash during the downhill competition and returned to win gold in the super-g and the giant slalom. A wave of new world records were set in speed skating because of the introduction of the clap skate.
2002 to 2010.
The 2002 Winter Olympics were held in Salt Lake City, United States, hosting 77 nations and 2,399 athletes in 78 events in 7 sports. These games were the first to take place since 11 September 2001, which meant a higher degree of security to avoid a terrorist attack. The opening ceremonies of the games saw signs of the aftermath of the events of that day, including the flag that flew at Ground Zero, NYPD officer Daniel Rodríguez singing "God Bless America", and honor guards of NYPD and FDNY members.
German Georg Hackl won a silver in the singles luge, becoming the first athlete in Olympic history to win medals in the same individual event in five consecutive Olympics. Canada achieved an unprecedented double by winning both the men's and women's ice hockey gold medals. Canada became embroiled with Russia in a controversy that involved the judging of the pairs figure skating competition. The Russian pair of Yelena Berezhnaya and Anton Sikharulidze competed against the Canadian pair of Jamie Salé and David Pelletier for the gold medal. The Canadians appeared to have skated well enough to win the competition, yet the Russians were awarded the gold. The judging broke along Cold War lines with judges from former Communist countries favouring the Russian pair and judges from Western nations voting for the Canadians. The only exception was the French judge, Marie-Reine Le Gougne, who awarded the gold to the Russians. An investigation revealed that she had been pressured to give the gold to the Russian pair regardless of how they skated; in return the Russian judge would look favourably on the French entrants in the ice dancing competition. The IOC decided to award both pairs the gold medal in a second medal ceremony held later in the Games. Australian Steven Bradbury became the first gold medallist from the southern hemisphere when he won the 1,000 metre short-track speed skating event.
The Italian city of Turin hosted the 2006 Winter Olympics. It was the second time that Italy had hosted the Winter Olympic Games. South Korean athletes won 10 medals, including 6 gold in the short-track speed skating events. Sun-Yu Jin won three gold medals while her teammate Hyun-Soo Ahn won three gold medals and a bronze. In the women's Cross-Country team pursuit Canadian Sara Renner broke one of her poles and, when he saw her dilemma, Norwegian coach Bjørnar Håkensmoen decided to lend her a pole. In so doing she was able to help her team win a silver medal in the event at the expense of the Norwegian team, who finished fourth. Claudia Pechstein of Germany became the first speed skater to earn nine career medals. In February 2009 Pechstein tested positive for "blood manipulation" and received a two-year suspension, which she appealed. The Court of Arbitration for Sport upheld her suspension but a Swiss court ruled that she could compete for a spot on the 2010 German Olympic team. This ruling was brought to the Swiss Federal Tribunal, which overturned the lower court's ruling and precluded her from competing in Vancouver.
In 2003 the IOC awarded the 2010 Winter Olympics to Vancouver, thus allowing Canada to host its second Winter Olympics. With a population of more than 2.5 million people Vancouver is the largest metropolitan area to ever host a Winter Olympic Games. Over 2,500 athletes from 82 countries participated in 86 events. The death of Georgian luger Nodar Kumaritashvili in a training run on the day of the opening ceremonies resulted in the Whistler Sliding Centre changing the track layout on safety grounds. Norwegian cross-country skier Marit Bjørgen won five medals in the six cross-country events on the women's programme. She finished the Olympics with three golds, a silver and a bronze. The Vancouver Games were notable for the poor performance of the Russian athletes. From their first Winter Olympics in 1956 to the 2006 games, a Soviet or Russian delegation had never been outside the top five medal-winning nations. In 2010 they finished sixth in total medals and eleventh in gold medals. President Dmitry Medvedev called for the resignation of top sports officials immediately after the Games. The success of Asian countries stood in stark contrast to the under-performing Russian team, with Vancouver marking a high point for medals won by Asian countries. In 1992 the Asian countries had won fifteen medals, three of which were gold. In Vancouver the total number of medals won by athletes from Asia had increased to thirty-one, with eleven of them being gold. The rise of Asian nations in Winter Olympics sports is due in part to the growth of winter sports programmes and the interest in winter sports in nations such as South Korea, Japan and China.
2014.
Sochi, Russia, was selected as the host city of the 2014 Winter Olympics over Salzburg, Austria, and Pyeongchang, South Korea. This was the first time since the breakup of the Soviet Union that Russia hosted a Winter Olympics. Over 2800 athletes from 88 countries participated in 98 events. The Olympic Village and Olympic Stadium were located on the Black Sea coast. All of the mountain venues were away in the alpine region known as Krasnaya Polyana.
The 2014 Winter Olympics, officially the XXII Olympic Winter Games, or the 22nd Winter Olympics, took place from 7 to 23 February 2014.
Future.
On 6 July 2011, the IOC selected the city of Pyeongchang, South Korea to host the 2018 Winter Olympics.
The host city for the XXIV Olympic Winter Games, also known as the 2022 Winter Olympics, is Beijing, elected on 31 July 2015, at the 128th IOC Session in Kuala Lumpur. Beijing will be the first city to host both the summer and winter olympics.
Controversy.
The process for awarding host city honours came under intense scrutiny after Salt Lake City had been awarded the right to host the 2002 Games. Soon after the host city had been announced it was discovered that the organisers had engaged in an elaborate bribery scheme to curry favour with IOC officials. Gifts and other financial considerations were given to those who would evaluate and vote on Salt Lake City's bid. These gifts included medical treatment for relatives, a college scholarship for one member's son and a land deal in Utah. Even IOC president Juan Antonio Samaranch received two rifles valued at $2,000. Samaranch defended the gift as inconsequential since, as president, he was a non-voting member. The subsequent investigation uncovered inconsistencies in the bids for every Games (both summer and winter) since 1988. For example, the gifts received by IOC members from the Japanese Organising Committee for Nagano's bid for the 1998 Winter Olympics were described by the investigation committee as "astronomical". Although nothing strictly illegal had been done, the IOC feared that corporate sponsors would lose faith in the integrity of the process and that the Olympic brand would be tarnished to such an extent that advertisers would begin to pull their support. The investigation resulted in the expulsion of 10 IOC members and the sanctioning of another 10. New terms and age limits were established for IOC membership, and 15 former Olympic athletes were added to the committee. Stricter rules for future bids were imposed, with ceilings imposed on the value of gifts IOC members could accept from bid cities.
Host city legacy.
According to the IOC, the host city is responsible for, "...establishing functions and services for all aspects of the Games, such as sports planning, venues, finance, technology, accommodation, catering, media services etc., as well as operations during the Games." Due to the cost of hosting an Olympic Games, most host cities never realise a profit on their investment. For example, the 1998 Winter Olympics in Nagano, Japan, cost $12.5 billion. By comparison the Torino Games of 2006 cost $3.6 billion to host. The organisers claimed that the cost of extending the bullet train service from Tokyo to Nagano was responsible for the large price tag. The organising committee hoped that the exposure of the Olympic Games, and the expedited access to Nagano from Tokyo, would be a boon to the local economy for years afterward. Nagano's economy did experience a two-year post-Olympic spurt, but the long-term effects have not materialised as planned. The possibility of heavy debt, coupled with unused sports venues and infrastructure that saddle the local community with upkeep costs and no practical post-Olympic value, is a deterrent to prospective host cities.
To mitigate these concerns the IOC has enacted several initiatives. First it has agreed to fund part of the host city's budget for staging the Games. Secondly, the IOC limits the qualifying host countries to those that have the resources and infrastructure to successfully host an Olympic Games without negatively impacting the region or nation. This eliminates a large portion of the developing world. Finally, cities bidding to host the Games are required to add a "legacy plan" to their proposal. This requires prospective host cities and the IOC, to plan with a view to the long-term economic and environmental impact that hosting the Olympics will have on the region.
Doping.
In 1967 the IOC began enacting drug testing protocols. They started by randomly testing athletes at the 1968 Winter Olympics. The first Winter Games athlete to test positive for a banned substance was Alois Schloder, a West German hockey player, but his team was still allowed to compete. During the 1970s testing outside of competition was escalated because it was found to deter athletes from using performance-enhancing drugs. The problem with testing during this time was a lack of standardisation of the test procedures, which undermined the credibility of the tests. It was not until the late 1980s that international sporting federations began to coordinate efforts to standardise the drug-testing protocols. The IOC took the lead in the fight against steroids when it established the independent World Anti-Doping Agency (WADA) in November 1999.
The 2006 Winter Olympics in Turin became notable for a scandal involving the emerging trend of blood doping, the use of blood transfusions or synthetic hormones such as Erythropoietin (EPO) to improve oxygen flow and thus reduce fatigue. The Italian police conducted a raid on the Austrian cross-country ski team's residence during the Games where they seized blood-doping specimens and equipment. This event followed the pre-Olympics suspension of 12 cross-country skiers who tested positive for unusually high levels of hemoglobin, which is evidence of blood doping.
Commercialisation.
Avery Brundage, as president of the IOC from 1952 to 1972, rejected all attempts to link the Olympics with commercial interests as he felt that the Olympic movement should be completely separate from financial influence. The 1960 Winter Olympics marked the beginning of corporate sponsorship of the Games. Despite Brundage's strenuous resistance the commercialisation of the Games continued during the 1960s, and revenue generated by corporate sponsorship swelled the IOC's coffers. By the Grenoble Games, Brundage had become so concerned about the direction of the Winter Olympic Games towards commercialisation that, if it could not be corrected, he felt the Winter Olympics should be abolished. Brundage's resistance to this revenue stream meant that the IOC was unable to gain a share of the financial windfall that was coming to host cities, and had no control over the structuring of sponsorship deals. When Brundage retired the IOC had $2 million in assets; eight years later its accounts had swelled to $45 million. This was due to a shift in ideology among IOC members, towards expansion of the Games through corporate sponsorship and the sale of television rights.
Brundage's concerns proved prophetic. The IOC has charged more for television broadcast rights at each successive Games. At the 1998 Nagano Games American broadcaster CBS paid $375 million, whereas the 2006 Turin Games cost NBC $613 million to broadcast. The more television companies have paid to televise the Games, the greater their persuasive power has been with the IOC. For example, the television lobby has influenced the Olympic programme by dictating when event finals are held, so that they appear in prime time for television audiences. They have pressured the IOC to include new events, such as snowboarding, that appeal to broader television audiences. This has been done to boost ratings, which were slowly declining until the 2010 Games.
In 1986 the IOC decided to stagger the Summer and Winter Games. Instead of holding both in the same calendar year the committee decided to alternate them every two years, although both Games would still be held on four-year cycles. It was decided that 1992 would be the last year to have both a Winter and Summer Olympic Games. There were two underlying reasons for this change: first was the television lobby's desire to maximise advertising revenue as it was difficult to sell advertising time for two Games in the same year; second was the IOC's desire to gain more control over the revenue generated by the Games. It was decided that staggering the Games would make it easier for corporations to sponsor individual Olympic Games, which would maximise revenue potential. The IOC sought to directly negotiate sponsorship contracts so that they had more control over the Olympic "brand". The first Winter Olympics to be hosted in this new format were the 1994 Games in Lillehammer.
Politics.
Cold War.
The Winter Olympics have been an ideological front in the Cold War since the Soviet Union first participated at the 1956 Winter Games. It did not take long for the Cold War combatants to discover what a powerful propaganda tool the Olympic Games could be. Soviet and American politicians used the Olympics as an opportunity to demonstrate the superiority of their respective political systems. The successful Soviet athlete was feted and honoured. Irina Rodnina, three-time Olympic gold medallist in figure skating, was awarded the Order of Lenin after her victory at the 1976 Winter Olympics in Innsbruck. Soviet athletes who won gold medals could expect to receive between $4,000 and $8,000 depending on the prestige of the sport. A world record was worth an additional $1,500. In 1978 the United States Congress responded to these measures by passing legislation that reorganised the United States Olympic Committee. It also approved financial rewards to medal-winning athletes.
The Cold War created tensions amongst countries allied to the two superpowers. The strained relationship between East and West Germany created a difficult political situation for the IOC. Because of its role in World War II, Germany was not allowed to compete at the 1948 Winter Olympics. In 1950 the IOC recognised the West German Olympic Committee, and invited East and West Germany to compete as a unified team at the 1952 Winter Games. East Germany declined the invitation and instead sought international legitimacy separate from West Germany. In 1955 the Soviet Union recognised East Germany as a sovereign state, thereby giving more credibility to East Germany's campaign to become an independent participant at the Olympics. The IOC agreed to provisionally accept the East German National Olympic Committee with the condition that East and West Germans compete on one team. The situation became tenuous when the Berlin Wall was constructed in 1962 and western nations began refusing visas to East German athletes. The uneasy compromise of a unified team held until the 1968 Grenoble Games when the IOC officially split the teams and threatened to reject the host-city bids of any country that refused entry visas to East German athletes.
Boycott.
The Winter Games have had only one national team boycott when Taiwan decided not to participate in the 1980 Winter Olympics held in Lake Placid. Prior to the Games the IOC agreed to allow China to compete in the Olympics for the first time since 1952. China was given permission to compete as the "People's Republic of China" (PRC) and to use the PRC flag and anthem. Until 1980 the island of Taiwan had been competing under the name "Republic of China" (ROC) and had been using the ROC flag and anthem. The IOC attempted to have the countries compete together but when this proved to be unacceptable the IOC demanded that Taiwan cease to call itself the "Republic of China". The IOC renamed the island "Chinese Taipei" and demanded that it adopt a different flag and national anthem, stipulations that Taiwan would not agree to. Despite numerous appeals and court hearings the IOC's decision stood. When the Taiwanese athletes arrived at the Olympic village with their Republic of China identification cards they were not admitted. They subsequently left the Olympics in protest, just before the opening ceremonies. Taiwan returned to Olympic competition at the 1984 Winter Games in Sarajevo as Chinese Taipei. The country agreed to compete under a flag bearing the emblem of their National Olympic Committee and to play the anthem of their National Olympic Committee should one of their athletes win a gold medal. The agreement remains in place to this day.
All-time medal table.
With reference to the top ten nations and according to official data of the International Olympic Committee.
List of Winter Olympic Games.
Unlike the Summer Olympics, the cancelled 1940 Winter Olympics and 1944 Winter Olympics are "not" included in the official Roman numeral counts for the Winter Games. While the official titles of the Summer Games count Olympiads, the titles of the Winter Games only count the Games themselves.
References.
Bibliography

</doc>
<doc id="34071" url="https://en.wikipedia.org/wiki?curid=34071" title="Whitney Houston">
Whitney Houston

Whitney Elizabeth Houston (August 9, 1963 – February 11, 2012) was an American singer, actress, producer, and model. In 2009, "Guinness World Records" cited her as the most awarded female act of all time. Houston is one of pop music's best-selling music artists of all-time, with an estimated 170–200 million records sold worldwide. She released seven studio albums and three movie soundtrack albums, all of which have diamond, multi-platinum, platinum or gold certification. Houston's crossover appeal on the popular music charts, as well as her prominence on MTV, starting with her video for "How Will I Know", influenced several African American women artists who follow in her footsteps.
Houston is the only artist to chart seven consecutive No. 1 "Billboard" Hot 100 hits. She is the second artist behind Elton John and the only woman to have two number-one "Billboard" 200 Album awards (formerly "Top Pop Albums") on the "Billboard" magazine year-end charts. Houston's 1985 debut album "Whitney Houston" became the best-selling debut album by a woman in history. "Rolling Stone" named it the best album of 1986, and ranked it at number 254 on the magazine's list of the 500 Greatest Albums of All Time. Her second studio album "Whitney" (1987) became the first album by a woman to debut at number one on the "Billboard" 200 albums chart.
Houston's first acting role was as the star of the feature film "The Bodyguard" (1992). The film's won the 1994 Grammy Award for Album of the Year. Its lead single, "I Will Always Love You", became the best-selling single by a woman in music history. With the album, Houston became the first act (solo or group, male or female) to sell more than a million copies of an album within a single week period under Nielsen SoundScan system. The album makes her the top female act in the top 10 list of the best-selling albums of all time, at number four. Houston continued to star in movies and contribute to their soundtracks, including the films "Waiting to Exhale" (1995) and "The Preacher's Wife" (1996). became the best-selling gospel album in history.
On February 11, 2012, Houston was found dead in her guest room at the Beverly Hilton, in Beverly Hills, California. The official coroner's report showed that she had accidentally drowned in the bathtub, with heart disease and cocaine use listed as contributing factors. News of her death coincided with the 2012 Grammy Awards and featured prominently in American and international media.
Life and career.
1963–84: Early life and career beginnings.
Whitney Houston was born on August 9, 1963 in what was then a middle-income neighborhood in Newark, New Jersey. She was the daughter of Army serviceman and entertainment executive John Russell Houston, Jr. (September 13, 1920 – February 2, 2003), and gospel singer Emily "Cissy" (Drinkard) Houston. Her elder brother Michael is a singer, and her elder half-brother is former basketball player Gary Garland. Her parents were both African American, and she was also said to have Native American and Dutch ancestry. Through her mother, Houston was a first cousin of singers Dionne Warwick and Dee Dee Warwick. Her godmother was Darlene Love and her honorary aunt was Aretha Franklin, whom she met at age 8 or 9 when her mother took her to a recording studio. Houston was raised a Baptist, but was also exposed to the Pentecostal church. After the 1967 Newark riots, the family moved to a middle-class area in East Orange, New Jersey, when she was four.
At the age of 11, Houston started performing as a soloist in the junior gospel choir at the New Hope Baptist Church in Newark, where she also learned to play the piano. Her first solo performance in the church was "Guide Me, O Thou Great Jehovah". When Houston was a teenager, she attended Mount Saint Dominic Academy, a Catholic girls' high school in Caldwell, New Jersey, where she met her best friend Robyn Crawford, whom she described as the "sister she never had". While Houston was still in school, her mother continued to teach her how to sing. Houston was also exposed to the music of Chaka Khan, Gladys Knight, and Roberta Flack, most of whom would have an influence on her as a singer and performer.
Houston spent some of her teenage years touring nightclubs where her mother Cissy was performing, and she would occasionally get on stage and perform with her. In 1977, at age 14, she became a backup singer on the Michael Zager Band's single "Life's a Party". In 1978, at age 15, Houston sang background vocals for Chaka Khan and Lou Rawls.
In the early 1980s, Houston started working as a fashion model after a photographer saw her at Carnegie Hall singing with her mother. She appeared in "Seventeen" and became one of the first women of color to grace the cover of the magazine. She was also featured in layouts in the pages of "Glamour", "Cosmopolitan", "Young Miss", and appeared in a Canada Dry soft drink TV commercial. Her looks and girl-next-door charm made her one of the most sought after teen models of that time. While modeling, she continued her burgeoning recording career by working with producers Michael Beinhorn, Bill Laswell and Martin Bisi on an album they were spearheading called "One Down", which was credited to the group Material. For that project, Houston contributed the ballad "Memories", a cover of a song by Hugh Hopper of Soft Machine. Robert Christgau of "The Village Voice" called her contribution "one of the most gorgeous ballads you've ever heard". She also appeared as a lead vocalist on one track on a Paul Jabara album, entitled "Paul Jabara and Friends", released by Columbia Records in 1983.
Houston had previously been offered several recording agencies (Michael Zager in 1980, and Elektra Records in 1981), but her mother declined the offers stating her daughter must first complete high school. In 1983, Gerry Griffith, an A&R representative from Arista Records, saw her performing with her mother in a New York City nightclub and was impressed. He convinced Arista's head Clive Davis to make time to see Houston perform. Davis too was impressed and offered a worldwide recording contract which Houston signed. Later that year, she made her national televised debut alongside Davis on "The Merv Griffin Show".
Houston signed with Arista in 1983, but did not begin work on her album immediately. The label wanted to make sure no other label signed the singer away. Davis wanted to ensure he had the right material and producers for Houston's debut album. Some producers had to pass on the project because of prior commitments. Houston first recorded a duet with Teddy Pendergrass entitled "Hold Me" which appeared on his album, "Love Language". The single was released in 1984 and gave Houston her first taste of success, becoming a Top 5 R&B hit. It would also appear on her debut album in 1985.
1985–86: Rise to international prominence.
With production from Michael Masser, Kashif, Jermaine Jackson, and Narada Michael Walden, Houston's debut album "Whitney Houston" was released in February 1985. "Rolling Stone" magazine praised Houston, calling her "one of the most exciting new voices in years" while "The New York Times" called the album "an impressive, musically conservative showcase for an exceptional vocal talent". Arista Records promoted Houston's album with three different singles from the album in the US, UK and other European countries. In the UK, the dance-funk "Someone for Me", which failed to chart in the country, was the first single while "All at Once" was in such European countries as the Netherlands and Belgium, where the song reached the top 5 on the singles charts, respectively.
In the US, the soulful ballad "You Give Good Love" was chosen as the lead single from Houston's debut to establish her in the black marketplace first. Outside the US, the song failed to get enough attention to become a hit, but in the US, it gave the album its first major hit as it peaked at No. 3 on the US "Billboard" Hot 100 chart, and No. 1 on the Hot R&B chart. As a result, the album began to sell strongly, and Houston continued promotion by touring nightclubs in the US. She also began performing on late-night television talk shows, which were not usually accessible to unestablished black acts. The jazzy ballad "Saving All My Love for You" was released next and it would become Houston's first No. 1 single in both the US and the UK. She was then an opening act for singer Jeffrey Osborne on his nationwide tour. "Thinking About You" was released as the promo single only to R&B-oriented radio stations, which peaked at number ten on the US R&B Chart. At the time, MTV had received harsh criticism for not playing enough videos by black, Latino, and other racial minorities while favoring white acts. The third US single, "How Will I Know", peaked at No. 1 and introduced Houston to the MTV audience thanks to its video. Houston's subsequent singles from this, and future albums, would make her the first African-American woman to receive consistent heavy rotation on MTV.
By 1986, a year after its initial release, "Whitney Houston" topped the "Billboard" 200 albums chart and stayed there for 14 non-consecutive weeks. The final single, "Greatest Love of All", a cover of "The Greatest Love of All", originally recorded by George Benson in 1977, became Houston's biggest hit at the time after peaking No. 1 and remaining there for three weeks on the Hot 100 chart, which made her debut the first album by a woman to yield three No. 1 hits. Houston was No. 1 artist of the year and "Whitney Houston" was the No. 1 album of the year on the 1986 "Billboard" year-end charts, making her the first woman to earn that distinction. At the time, Houston released the best-selling debut album by a solo artist. Houston then embarked on her world tour, "Greatest Love Tour". The album had become an international success, and was certified 13× platinum (diamond) in the United States alone, and has sold 25 million copies worldwide.
At the 1986 Grammy Awards, Houston was nominated for three awards including Album of the Year. She was not eligible for the Best New Artist category because of her previous hit R&B duet recording with Teddy Pendergrass in 1984. She won her first Grammy Award for Best Pop Vocal Performance, Female for "Saving All My Love for You". Houston's performance of the song during the Grammy telecast later earned her an Emmy Award for Outstanding Individual Performance in a Variety or Music Program.
Houston won seven American Music Awards in total in 1986 and 1987, and an MTV Video Music Award. The album's popularity would also carry over to the 1987 Grammy Awards when "Greatest Love of All" would receive a Record of the Year nomination, ten years after the original recording of "The Greatest Love of All" by George Benson, which was the main theme of the boxer Muhammad Ali biopic ""The Greatest"" in 1977. Houston's debut album is listed as one of "Rolling Stone"s 500 Greatest Albums of All Time and on The Rock & Roll Hall of Fame's Definitive 200 list. Houston's grand entrance into the music industry is considered one of the 25 musical milestones of the last 25 years, according to "USA Today". Following Houston's breakthrough, doors were opened for other African-American women such as Janet Jackson and Anita Baker to find notable success in popular music and on MTV.
1987–91: "Whitney", "I'm Your Baby Tonight" and "The Star Spangled Banner".
With many expectations, Houston's second album, "Whitney", was released in June 1987. The album again featured production from Masser, Kashif and Walden as well as Jellybean Benitez. Many critics complained that the material was too similar to her previous album. "Rolling Stone" said, "the narrow channel through which this talent has been directed is frustrating". Still, the album enjoyed commercial success. Houston became the first woman in music history to debut at number one on the "Billboard" 200 albums chart, and the first artist to enter the albums chart at number one in both the US and UK, while also hitting number one or top ten in dozens of other countries around the world. The album's first single, "I Wanna Dance with Somebody (Who Loves Me)", was also a massive hit worldwide, peaking at No. 1 on the "Billboard" Hot 100 chart and topping the singles chart in many countries such as Australia, Germany and the UK. The next three singles, "Didn't We Almost Have It All", "So Emotional", and "Where Do Broken Hearts Go" all peaked at number one on the US Hot 100 chart, which gave her a total of seven consecutive number one hits, breaking the record of six previously shared by The Beatles and the Bee Gees. Houston became the first woman to generate four number-one singles from one album. "Whitney" has been certified 9× Platinum in the US for shipments of over 9 million copies, and has sold a total of 20 million copies worldwide.
At the 30th Grammy Awards in 1988, Houston was nominated for three awards, including Album of the Year, winning her second Grammy for Best Female Pop Vocal Performance for "I Wanna Dance with Somebody (Who Loves Me)". Houston also won two American Music Awards in 1988 and 1989, respectively, and a Soul Train Music Award. Following the release of the album, Houston embarked on the "Moment of Truth World Tour", which was one of the ten highest grossing concert tours of 1987. The success of the tours during 1986–87 and her two studio albums ranked Houston No. 8 for the highest earning entertainers list according to "Forbes" magazine. She was the highest earning African-American woman overall and the third highest entertainer after Bill Cosby and Eddie Murphy.
Houston was a supporter of Nelson Mandela and the anti-apartheid movement. During her modeling days, the singer refused to work with any agencies who did business with the then-apartheid South Africa. On June 11, 1988, during the European leg of her tour, Houston joined other musicians to perform a set at Wembley Stadium in London to celebrate a then-imprisoned Nelson Mandela's 70th birthday. Over 72,000 people attended Wembley Stadium, and over a billion people tuned in worldwide as the rock concert raised over $1 million for charities while bringing awareness to apartheid. Houston then flew back to the US for a concert at Madison Square Garden in New York City in August. The show was a benefit concert that raised a quarter of a million dollars for the United Negro College Fund. In the same year, she recorded a song for NBC's coverage of the 1988 Summer Olympics, "One Moment in Time", which became a Top 5 hit in the US, while reaching number one in the UK and Germany. With her world tour continuing overseas, Houston was still one of the top 20 highest earning entertainers for 1987–88 according to "Forbes" magazine.
In 1989, Houston formed The Whitney Houston Foundation For Children, a non-profit organization that has raised funds for the needs of children around the world. The organization cares for homelessness, children with cancer or AIDS, and other issues of self-empowerment. With the success of her first two albums, Houston was undoubtedly an international crossover superstar, the most prominent since Michael Jackson, appealing to all demographics. However, some black critics believed she was "selling out". They felt her singing on record lacked the soul that was present during her live concerts.
At the 1989 Soul Train Music Awards, when Houston's name was called out for a nomination, a few in the audience jeered. Houston defended herself against the criticism, stating, "If you're gonna have a long career, there's a certain way to do it, and I did it that way. I'm not ashamed of it." Houston took a more urban direction with her third studio album, "I'm Your Baby Tonight", released in November 1990. She produced and chose producers for this album and as a result, it featured production and collaborations with L.A. Reid and Babyface, Luther Vandross, and Stevie Wonder. The album showed Houston's versatility on a new batch of tough rhythmic grooves, soulful ballads and up-tempo dance tracks. Reviews were mixed. "Rolling Stone" felt it was her "best and most integrated album". while "Entertainment Weekly", at the time thought Houston's shift towards an urban direction was "superficial".
The album contained several hits: the first two singles, "I'm Your Baby Tonight" and "All the Man That I Need" peaked at number one on the "Billboard" Hot 100 chart; "Miracle" peaked at number nine; "My Name Is Not Susan" peaked in the top twenty; "I Belong to You" reached the top ten of the US R&B chart and garnered Houston a Grammy nomination; and the sixth single, the Stevie Wonder duet "We Didn't Know", reached the R&B top twenty. The album peaked at number three on the "Billboard" 200 and went on to be certified 4× platinum in the US while selling twelve million total worldwide.
In 1990, Houston was the spokesperson for a youth leadership conference hosted in Washington, D.C. She had a private audience with President George H. W. Bush in the Oval Office to discuss the associated challenges.
During the Persian Gulf War, Houston performed "The Star Spangled Banner" at Super Bowl XXV at Tampa Stadium on January 27, 1991. This performance was later reported by those involved in the performance to have been lip synced or to have been sung into a dead microphone while a studio recording previously made by Houston was played. Dan Klores, a spokesman for Houston, explained: "This is not a Milli Vanilli thing. She sang live, but the microphone was turned off. It was a technical decision, partially based on the noise factor. This is standard procedure at these events." (See also Star Spangled Banner lip sync controversy.) A commercial single and video of her performance were released, and reached the Top 20 on the US Hot 100, making her the only act to turn the US national anthem into a pop hit of that magnitude (José Feliciano's version reached No. 50 in November 1968). Houston donated all her share of the proceeds to the American Red Cross Gulf Crisis Fund. As a result, the singer was named to the Red Cross Board of Governors.
Her rendition was critically acclaimed and is considered the benchmark for singers. "Rolling Stone" commented that "her singing stirs such strong patriotism. Unforgettable", and the performance ranked No. 1 on the 25 most memorable music moments in NFL history list. VH1 listed the performance as one of the greatest moments that rocked TV. Following the attacks on 9/11, it was released again by Arista Records, all profits going towards the firefighters and victims of the attacks. This time it peaked at No. 6 in the Hot 100 and was certified platinum by the Recording Industry Association of America.
Later in 1991, Houston put together her "Welcome Home Heroes" concert with HBO for the soldiers fighting in the Persian Gulf War and their families. The free concert took place at Naval Station Norfolk in Norfolk, Virginia in front of 3,500 servicemen and women. HBO descrambled the concert so that it was free for everyone to watch. Houston's concert gave HBO its highest ratings ever. She then embarked on the "I'm Your Baby Tonight World Tour".
1992–94: Marriage, motherhood, and "The Bodyguard".
Throughout the 1980s, Houston was romantically linked to American football star Randall Cunningham and actor Eddie Murphy, whom she dated. She then met R&B singer Bobby Brown at the 1989 Soul Train Music Awards. After a three-year courtship, the two were married on July 18, 1992. On March 4, 1993, Houston gave birth to their daughter Bobbi Kristina, the couple's only child. Brown would go on to have several run-ins with the law, including some jail time.
With the commercial success of her albums, movie offers poured in, including offers to work with Robert De Niro, Quincy Jones, and Spike Lee; but Houston felt the time wasn't right. Houston's first film role was in "The Bodyguard", released in 1992 and co-starring Kevin Costner. Houston played Rachel Marron, a star who is stalked by a crazed fan and hires a bodyguard to protect her. "USA Today" listed it as one of the 25 most memorable movie moments of the last 25 years in 2007. Houston's mainstream appeal allowed people to look at the movie color-blind.
Still, controversy arose as some felt the film's advertising intentionally hid Houston's face to hide the film's interracial relationship. In an interview with "Rolling Stone" in 1993, the singer commented that "people know who Whitney Houston is – I'm black. You can't hide that fact." Houston received a Razzie Award nomination for Worst Actress. "The Washington Post" said Houston is "doing nothing more than playing Houston, comes out largely unscathed if that is possible in so cockamamie an undertaking", and "The New York Times" commented that she lacked passion with her co-star. Despite the film's mixed reviews, it was hugely successful at the box office, grossing more than $121 million in the U.S. and $410 million worldwide, making it one of the top 100 grossing films in film history at its time of release, though it is no longer in the top 100 because of rising ticket prices since the time the film was released.
The film's soundtrack also enjoyed big success. Houston executive produced and contributed six songs for the motion picture's . "Rolling Stone" said it is "nothing more than pleasant, tasteful and urbane". The soundtrack's lead single was "I Will Always Love You", written and originally recorded by Dolly Parton in 1974. Houston's version of the song was acclaimed by many critics, regarding it as her "signature song" or "iconic performance". "Rolling Stone" and "USA Today" called her rendition "the tour-de-force". The single peaked at number one on the "Billboard" Hot 100 for a then-record-breaking 14 weeks, number one on the R&B chart for a then-record-breaking 11 weeks, and number one on the Adult Contemporary charts for five weeks.
The single was certified 4× platinum by the RIAA, making Houston the first woman with a single to reach that level in the RIAA history and becoming the best-selling single by a woman in the US.
The song also became a global success, hitting number-one in almost all countries, and the best-selling single of all time by a female solo artist with 20 million copies sold. The soundtrack topped the "Billboard" 200 chart and remained there for 20 non-consecutive weeks, the longest tenure by any Arista album on the chart in the Nielsen SoundScan era (tied for 10th overall by any label), and became one of the fastest selling albums ever. During Christmas week of 1992, the soundtrack sold over a million copies within a week, becoming the first album to achieve that feat under Nielsen SoundScan system. With the follow-up singles "I'm Every Woman", a Chaka Khan cover, and "I Have Nothing" both reaching the top five, Houston became the first woman to ever have three singles in the Top 11 simultaneously. The album was certified 17× platinum in the US alone, with worldwide sales of 44 million, making "The Bodyguard" the biggest-selling album by a female act on the list of the world's Top 10 best-selling albums, topping Shania Twain's 40 million sold for "Come On Over".
Houston won three Grammys for the album in 1994, including two of the Academy's highest honors, Album of the Year and Record of the Year. In addition, she won a record 8 American Music Awards at that year's ceremony including the Award of Merit, 11 Billboard Music Awards, 3 Soul Train Music Awards in 1993–94 including Sammy Davis, Jr. Award as Entertainer of the Year, 5 NAACP Image Awards including Entertainer of the Year, a record 5 World Music Awards, and a BRIT award. Following the success of the project, Houston embarked on another expansive global tour, "The Bodyguard World Tour", in 1993–94. Her concerts, movie, and recording grosses made her the third highest earning female entertainer of 1993–94, just behind Oprah Winfrey and Barbra Streisand according to "Forbes" magazine. Houston placed in the top five of "Entertainment Weekly"s annual "Entertainer of the Year" ranking and was labeled by "Premiere" magazine as one of the 100 most powerful people in Hollywood.
In October 1994, Houston attended and performed at a state dinner in the White House honoring newly elected South African president Nelson Mandela. At the end of her world tour, Houston performed three concerts in South Africa to honor President Mandela, playing to over 200,000 people. This would make the singer the first major musician to visit the newly unified and apartheid free nation following Mandela's winning election. The concert was broadcast live on HBO with funds of the concerts being donated to various charities in South Africa. The event was considered the nation's "biggest media event since the inauguration of Nelson Mandela".
1995–97: "Waiting to Exhale", "The Preacher's Wife", and "Cinderella".
In 1995, Houston starred alongside Angela Bassett, Loretta Devine, and Lela Rochon in her second film, "Waiting to Exhale", a motion picture about four African-American women struggling with relationships. Houston played the lead character Savannah Jackson, a TV producer in love with a married man. She chose the role because she saw the film as "a breakthrough for the image of black women because it presents them both as professionals and as caring mothers". After opening at number one and grossing $67 million in the US at the box office and $81 million worldwide, it proved that a movie primarily targeting a black audience can cross over to success, while paving the way for other all-black movies such as "How Stella Got Her Groove Back" and the Tyler Perry movies that became popular in the 2000s. The film is also notable for its portrayal of black women as strong middle class citizens rather than as stereotypes. The reviews were mainly positive for the ensemble cast. "The New York Times" said: "Ms. Houston has shed the defensive hauteur that made her portrayal of a pop star in 'The Bodyguard' seem so distant." Houston was nominated for an NAACP Image Award for "Outstanding Actress in a Motion Picture", but lost to her co-star Bassett.
The film's accompanying soundtrack, "", was produced by Houston and Babyface. Though Babyface originally wanted Houston to record the entire album, she declined. Instead, she "wanted it to be an album of women with vocal distinction", and thus gathered several African-American female artists for the soundtrack, to go along with the film's message about strong women. Consequently, the album featured a range of contemporary R&B female recording artists along with Houston, such as Mary J. Blige, Brandy, Toni Braxton, Aretha Franklin, and Patti LaBelle. Houston's "Exhale (Shoop Shoop)" peaked at No. 1, and then spent a record eleven weeks at the No. 2 spot and eight weeks on top of the R&B Charts. "Count On Me", a duet with CeCe Winans, hit the U.S. Top 10; and Houston's third contribution, "Why Does It Hurt So Bad", made the Top 30. The album debuted at No. 1, and was certified 7× Platinum in the United States, denoting shipments of seven million copies. The soundtrack received strong reviews; as "Entertainment Weekly" stated: "the album goes down easy, just as you'd expect from a package framed by Whitney Houston tracks... the soundtrack waits to exhale, hovering in sensuous suspense" and has since ranked it as one of the 100 Best Movie Soundtracks. Later that year, Houston's children's charity organization was awarded a VH1 Honor for all the charitable work.
In 1996, Houston starred in the holiday comedy "The Preacher's Wife", with Denzel Washington. She plays a gospel-singing wife of a pastor (Courtney B. Vance). It was largely an updated remake of the film "The Bishop's Wife" (1948), which starred Loretta Young, David Niven and Cary Grant. Houston earned $10 million for the role, making her one of the highest-paid actresses in Hollywood at the time and the highest earning African-American actress in Hollywood. The movie, with its all African-American cast, was a moderate success, earning approximately $50 million at the U.S. box offices. The movie gave Houston her strongest reviews so far. "The San Francisco Chronicle" said Houston "is rather angelic herself, displaying a divine talent for being virtuous and flirtatious at the same time", and she "exudes gentle yet spirited warmth, especially when praising the Lord in her gorgeous singing voice". Houston was again nominated for an NAACP Image Award and won for Outstanding Actress in a Motion Picture.
Houston recorded and co-produced, with Mervyn Warren, the film's accompanying gospel soundtrack. "" included six gospel songs with Georgia Mass Choir that were recorded at the Great Star Rising Baptist Church in Atlanta. Houston also duetted with gospel legend Shirley Caesar. The album sold six million copies worldwide and scored hit singles with "I Believe in You and Me" and "Step by Step", becoming the largest selling gospel album of all time. The album received mainly positive reviews. Some critics, such as that of "USA Today", noted the presence of her emotional depth, while "The Times" said, "To hear Houston going at full throttle with the 35 piece Georgia Mass Choir struggling to keep up is to realise what her phenomenal voice was made for".
In 1997, Houston's production company changed its name to BrownHouse Productions and was joined by Debra Martin Chase. Their goal was "to show aspects of the lives of African-Americans that have not been brought to the screen before" while improving how African-Americans are portrayed in film and television. Their first project was a made-for-television remake of Rodgers & Hammerstein's "Cinderella". In addition to co-producing, Houston starred in the movie as the Fairy Godmother along with Brandy, Jason Alexander, Whoopi Goldberg, and Bernadette Peters. Houston was initially offered the role of Cinderella in 1993, but other projects intervened. The film is notable for its multi-racial cast and nonstereotypical message. An estimated 60 million viewers tuned into the special giving ABC its highest TV ratings in 16 years. The movie received seven Emmy nominations including Outstanding Variety, Musical or Comedy, while winning Outstanding Art Direction in a Variety, Musical or Comedy Special.
Houston and Chase then obtained the rights to the story of Dorothy Dandridge. Houston was to play Dandridge, the first African American actress to be nominated for an Academy Award for Best Actress. Houston wanted the story told with dignity and honor. However, Halle Berry also had rights to the project and got her version going first. Later that year, Houston paid tribute to her idols, such as Aretha Franklin, Diana Ross, and Dionne Warwick, by performing their hits during the three-night HBO Concert "". The special raised over $300,000 for the Children's Defense Fund. Houston received the Quincy Jones Award for outstanding career achievements in the field of entertainment at the 12th Soul Train Music Awards.
1998–2000: "My Love Is Your Love" and "Whitney: The Greatest Hits".
After spending much of the early and mid-1990s working on motion pictures and their soundtrack albums, Houston's first studio album in eight years, the critically acclaimed "My Love Is Your Love", was released in November 1998. Though originally slated to be a greatest hits album with a handful of new songs, recording sessions were so fruitful that a new full-length studio album was released. Recorded and mixed in only six weeks, it featured production from Rodney Jerkins, Wyclef Jean and Missy Elliott. The album debuted at number thirteen, its peak position, on the "Billboard" 200 chart. It had a funkier and edgier sound than past releases and saw Houston handling urban dance, hip hop, mid-tempo R&B, reggae, torch songs, and ballads all with great dexterity.
From late 1998 to early 2000, the album spawned several hit singles: "When You Believe" (US No. 15, UK No. 4), a duet with Mariah Carey for 1998's "The Prince of Egypt" soundtrack, which also became an international hit as it peaked in the Top 10 in several countries and won an Academy Award for Best Original Song; "Heartbreak Hotel" (US No. 2, UK No. 25) featured Faith Evans and Kelly Price, received a 1999 MTV VMA nomination for Best R&B Video, and number one on the US R&B chart for seven weeks; "It's Not Right but It's Okay" (US No. 4, UK No. 3) won Houston her sixth Grammy Award for Best Female R&B Vocal Performance; "My Love Is Your Love" (US No. 4, UK No. 2) with 3 million copies sold worldwide; and "I Learned from the Best" (US No. 27, UK No. 19). These singles became international hits as well, and all the singles, except "When You Believe", became number one hits on the "Billboard" Hot Dance/Club Play chart. The album sold four million copies in America, making it certified 4× platinum, and a total of eleven million copies worldwide.
The album gave Houston some of her strongest reviews ever. "Rolling Stone" said Houston was singing "with a bite in her voice" and "The Village Voice" called it "Whitney's sharpest and most satisfying so far". In 1999, Houston participated in VH-1's Divas Live '99, alongside Brandy, Mary J. Blige, Tina Turner, and Cher. The same year, Houston hit the road with her 70 date "My Love Is Your Love World Tour". The European leg of the tour was Europe's highest grossing arena tour of the year. In November 1999, Houston was named Top-selling R&B Female Artist of the Century with certified US sales of 51 million copies at the time and "The Bodyguard Soundtrack" was named the Top-selling Soundtrack Album of the Century by the Recording Industry Association of America (RIAA). She also won The Artist of the Decade, Female award for extraordinary artistic contributions during the 1990s at the 14th Soul Train Music Awards, and an MTV Europe Music Award for Best R&B.
In May 2000, "" was released worldwide. The double disc set peaked at number five in the United States, reaching number one in the United Kingdom. In addition, the album reached the Top 10 in many other countries. While ballad songs were left unchanged, the album features house/club remixes of many of Houston's up-tempo hits. Included on the album were four new songs: "Could I Have This Kiss Forever" (a duet with Enrique Iglesias), "Same Script, Different Cast" (a duet with Deborah Cox), "If I Told You That" (a duet with George Michael), and "Fine", and three hits that had never appeared on a Houston album: "One Moment in Time", "The Star Spangled Banner", and "If You Say My Eyes Are Beautiful", a duet with Jermaine Jackson from his 1986 "Precious Moments" album. Along with the album, an accompanying VHS and DVD was released featuring the music videos to Houston's greatest hits, as well as several hard-to-find live performances including her 1983 debut on "The Merv Griffin Show", and interviews. The greatest hits album was certified 3× platinum in the US, with worldwide sales of 10 million.
2000–05: "Just Whitney" and personal struggles.
Though Houston was seen as a "good girl" with a perfect image in the 1980s and early 1990s, by the late 1990s, her behavior changed. She was often hours late for interviews, photo shoots and rehearsals, and canceling concerts and talk-show appearances. With the missed performances and weight loss, rumors about Houston using drugs with her husband circulated. On January 11, 2000, airport security guards discovered marijuana in both Houston's and husband Bobby Brown's luggage at a Hawaii airport, but the two boarded the plane and departed before authorities could arrive. Charges were later dropped against them, but rumors of drug usage between the couple would continue to surface. Two months later, Clive Davis was inducted into the Rock & Roll Hall of Fame. Houston had been scheduled to perform at the event, but failed to show up.
Shortly thereafter, Houston was scheduled to perform at the Academy Awards but was fired from the event by musical director and longtime friend Burt Bacharach. Her publicist cited throat problems as the reason for the cancellation. In his book "The Big Show: High Times and Dirty Dealings Backstage at the Academy Awards", author Steve Pond revealed that "Houston's voice was shaky, she seemed distracted and jittery, and her attitude was casual, almost defiant", and that while Houston was to sing "Over the Rainbow", she would start singing a different song. Houston later admitted to having been fired. Later that year, Houston's long-time executive assistant and friend, Robyn Crawford, resigned from Houston's management company.
In August 2001, Houston signed one of the biggest record deals in music history, with Arista/BMG. She renewed her contract for $100 million to deliver six new albums, on which she would also earn royalties. She later made an appearance on "". Her extremely thin frame further spurred rumors of drug use. Houston's publicist said, "Whitney has been under stress due to family matters, and when she is under stress she doesn't eat." The singer was scheduled for a second performance the following night but canceled. Within weeks, Houston's rendition of "The Star Spangled Banner" would be re-released after the September 11 attacks, with the proceeds donated to the New York Firefighters 9/11 Disaster Relief Fund and the New York Fraternal Order of Police. The song peaked at No. 6 this time on the US Hot 100, topping its previous position.
In 2002, Houston became involved in a legal dispute with John Houston Enterprise. Although the company was started by her father to manage her career, it was actually run by company president Kevin Skinner. Skinner filed a breach-of-contract lawsuit and sued for $100 million (but lost), stating that Houston owed the company previously unpaid compensation for helping to negotiate her $100 million contract with Arista Records and for sorting out legal matters. Houston stated that her 81-year-old father had nothing to do with the lawsuit. Although Skinner tried to claim otherwise, John Houston never appeared in court. Houston's father later died in February 2003. The lawsuit was dismissed on April 5, 2004, and Skinner was awarded nothing.
Also in 2002, Houston did an interview with Diane Sawyer to promote her then-upcoming album. During the prime-time special, Houston spoke on topics including rumored drug use and marriage. She was asked about the ongoing drug rumors and replied, "First of all, let's get one thing straight. Crack is cheap. I make too much money to ever smoke crack. Let's get that straight. Okay? We don't do crack. We don't do that. Crack is whack." The line was from Keith Haring's mural which was painted in 1986 on the handball court at 128th Street and 2nd Avenue. Houston did, however, admit to using other substances at times, including cocaine.
In December 2002, Houston released her fifth studio album, "Just Whitney...". The album included productions from then-husband Bobby Brown, as well as Missy Elliott and Babyface, and marked the first time that Houston did not produce with Clive Davis as Davis had been released by top management at BMG. Upon its release, "Just Whitney..." received mixed reviews. The album debuted at number 9 on the "Billboard" 200 chart and it had the highest first week sales of any album Houston had ever released. The four singles released from the album did not fare well on the "Billboard" Hot 100, but became dance chart hits. "Just Whitney..." was certified platinum in the United States, and sold approximately three million worldwide.
On a June 2003 trip to Israel, Houston said of her visit, "I've never felt like this in any other country. I feel at home, I feel wonderful."
In late 2003, Houston released her first Christmas album "", with a collection of traditional holiday songs. Houston produced the album with Mervyn Warren and Gordon Chambers. A single titled "One Wish (for Christmas)" reached the Top 20 on the Adult Contemporary chart, and the album was certified gold in the US. Having always been a touring artist, Houston spent most of 2004 touring and performing in Europe, the Middle East, Asia, and Russia. In September 2004, she gave a surprise performance at the World Music Awards in a tribute to long-time friend Clive Davis. After the show, Davis and Houston announced plans to go into the studio to work on her new album.
In early 2004, husband Bobby Brown starred in his own reality TV program, "Being Bobby Brown" on the Bravo network, which provided a view into the domestic goings-on in the Brown household. Though it was Brown's vehicle, Houston was a prominent figure throughout the show, receiving as much screen time as Brown. The series aired in 2005 and featured Houston in, what some would say, not her most flattering moments. "The Hollywood Reporter" said it was "undoubtedly the most disgusting and execrable series ever to ooze its way onto television". Despite the perceived train-wreck nature of the show, the series gave Bravo its highest ratings in its time slot and continued Houston's successful forays into film and television. The show was not renewed for a second season after Houston stated that she would no longer appear in it, and Brown and Bravo could not come to an agreement for another season.
2006–12: Return to music, "I Look to You", tour and film comeback.
After years of controversy and turmoil, Houston separated from Bobby Brown in September 2006, filing for divorce the following month. On February 1, 2007, Houston asked the court to fast track their divorce. The divorce was finalized on April 24, 2007, with Houston granted custody of the couple's daughter. On May 4, Houston sold the suburban Atlanta home featured in "Being Bobby Brown" for $1.19 million. A few days later, Brown sued Houston in Orange County, California court in an attempt to change the terms of their custody agreement. Brown also sought child and spousal support from Houston. In the lawsuit, Brown claimed that financial and emotional problems prevented him from properly responding to Houston's divorce petition. Brown lost at his court hearing as the judge dismissed his appeal to overrule the custody terms, leaving Houston with full custody and Brown with no spousal support. In March 2007, Clive Davis of Arista Records announced that Houston would begin recording a new album. In October 2007, Arista released another compilation "The Ultimate Collection" outside the United States.
Houston gave her first interview in seven years in September 2009, appearing on Oprah Winfrey's season premiere. The interview was billed as "the most anticipated music interview of the decade". Whitney admitted on the show to using drugs with former husband Bobby Brown, who "laced marijuana with rock cocaine". She told Oprah that before "The Bodyguard" her drug use was light, but after the film's success and the birth of her daughter it got heavier, and by 1996 "drugs was an everyday thing... I wasn't happy by that point in time. I was losing myself."
Houston released her new album, "I Look to You", in August 2009. The album's first two singles were the title track "I Look to You" and "Million Dollar Bill". The album entered the "Billboard" 200 at No. 1, with Houston's best opening week sales of 305,000 copies, marking Houston's first number one album since "The Bodyguard", and Houston's first studio album to reach number one since 1987's "Whitney". Houston also appeared on European television programs to promote the album. She performed the song "I Look to You" on the German television show "Wetten, dass..?". Three days later, she performed the worldwide first single from "I Look to You", "Million Dollar Bill", on the French television show "Le Grand Journal". Houston appeared as guest mentor on "The X Factor" in the United Kingdom. She performed "Million Dollar Bill" on the following day's results show, completing the song even as a strap in the back of her dress popped open two seconds into the performance. She later commented that she "sang out of [her clothes".
The performance was poorly received by the British media, and was variously described as "weird" and "ungracious", "shambolic" and a "flop". Despite this reception, "Million Dollar Bill" jumped to its peak from 14 to number 5 (her first UK top 5 for over a decade), and three weeks after release "I Look to You" went gold. Houston appeared on the Italian version of "The X Factor", also performing "Million Dollar Bill", this time to excellent reviews. Houston was later awarded a Gold certificate for achieving over 50,000 CD sales of "I Look to You" in Italy. In November, Houston performed "I Didn't Know My Own Strength" at the 2009 American Music Awards in Los Angeles, California. Two days later, Houston performed "Million Dollar Bill" and "I Wanna Dance with Somebody (Who Loves Me)" on the "Dancing with the Stars" season 9 finale. As of December 2009, "I Look to You" has been certified platinum by the RIAA for sales of more than one million copies in the United States. On January 26, 2010, her debut album was re-released in a special edition entitled "Whitney Houston – The Deluxe Anniversary Edition".
Houston later embarked on a world tour, entitled the Nothing but Love World Tour. It was her first world tour in over ten years and was announced as a triumphant comeback. However, some poor reviews and rescheduled concerts brought some negative media attention. Houston canceled some concerts because of illness and received widespread negative reviews from fans who were disappointed in the quality of her voice and performance. Some fans reportedly walked out of her concerts.
In January 2010, Houston was nominated for two NAACP Image Awards, one for Best Female Artist and one for Best Music Video. She won the award for Best Music Video for her single "I Look to You". On January 16, she received The BET Honors Award for Entertainer citing her lifetime achievements spanning over 25 years in the industry. The 2010 BET Honors award was held at the Warner Theatre in Washington, D.C. and aired on February 1, 2010. Jennifer Hudson and Kim Burrell performed in honor of her, garnering positive reviews. Houston also received a nomination from the Echo Awards, Germany's version of the Grammys, for Best International Artist. In April 2010, the UK newspaper "The Mirror" reported that Houston was thinking about recording her eighth studio album and wanted to collaborate with will.i.am (of The Black Eyed Peas), her first choice for a collaboration.
Houston also performed the song "I Look to You" on the 2011 BET "Celebration of Gospel", with gospel–jazz singer Kim Burrell, held at the Staples Center, Los Angeles. The performance aired on January 30, 2011. Early in 2011, she gave an uneven performance in tribute to cousin Dionne Warwick at music mogul Clive Davis' annual pre-Grammy gala. In May 2011, Houston enrolled in a rehabilitation center again, as an out-patient, citing drug and alcohol problems. A representative for Houston said that it was a part of Houston's "longstanding recovery process".
In September 2011, "The Hollywood Reporter" announced that Houston would produce and star alongside Jordin Sparks and Mike Epps in the remake of the 1976 film "Sparkle". In the film, Houston portrays Sparks' "not-so encouraging" mother. Houston is also credited as an executive producer of the film. Debra Martin Chase, producer of "Sparkle", stated that Houston deserved the title considering she had been there from the beginning in 2001, when Houston obtained "Sparkle" production rights. R&B singer Aaliyah – originally tapped to star as Sparkle – died in a 2001 plane crash. Her death derailed production, which would have begun in 2002. Houston's remake of "Sparkle" was filmed in the fall of 2011 over a two-month period, and was released by TriStar Pictures. On May 21, 2012, "Celebrate", the last song Houston recorded with Sparks, premiered at RyanSeacrest.com. It was made available for digital download on iTunes on June 5. The song was featured on the "" soundtrack as the first official single. The movie was released on August 17, 2012 in the United States. The accompanying music video for "Celebrate" was filmed on May 30, 2012. The video was shot over 2 days, and a sneak peek of the video premiered on "Entertainment Tonight" on June 4, 2012.
Death.
On February 9, 2012, Houston visited singers Brandy and Monica, together with Clive Davis, at their rehearsals for Davis' pre-Grammy Awards party at The Beverly Hilton hotel in Beverly Hills. That same day, she made her last public performance, when she joined Kelly Price on stage in Hollywood, California, and sang "Jesus Loves Me".
Two days later, on February 11, Houston was found unconscious in Suite 434 at the Beverly Hilton Hotel, submerged in the bathtub. Beverly Hills paramedics arrived at approximately 3:30 p.m. and found the singer unresponsive and performed CPR. Houston was pronounced dead at 3:55 p.m. PST. The cause of death was not immediately known. Local police said there were "no obvious signs of criminal intent". On March 22, 2012, the Los Angeles County coroner's office reported the cause of Houston's death was drowning and the "effects of atherosclerotic heart disease and cocaine use". The office stated the amount of cocaine found in Houston's body indicated that she used the substance shortly before her death. Toxicology results revealed additional drugs in her system: diphenhydramine, alprazolam, cannabis and cyclobenzaprine. The manner of death was listed as an "accident".
Houston had an invitation-only memorial on Saturday, February 18, 2012, at the New Hope Baptist Church in Newark, New Jersey. The service was scheduled for two hours, but lasted four. Among those who performed at the funeral were Stevie Wonder (rewritten version of "Ribbon in the Sky", and "Love's in Need of Love Today"), CeCe Winans ("Don't Cry", and "Jesus Loves Me"), Alicia Keys ("Send Me an Angel"), Kim Burrell (rewritten version of "A Change Is Gonna Come"), and R. Kelly ("I Look to You"). The performances were interspersed with hymns by the church choir and remarks by Clive Davis, Houston's record producer; Kevin Costner; Rickey Minor, her music director; her cousin, Dionne Warwick; and Ray Watson, her security guard for the past 11 years. Aretha Franklin was listed on the program and was expected to sing, but was unable to attend the service. Bobby Brown, Houston's ex-husband, was also invited to the funeral but he left before the service began. Houston was buried on Sunday, February 19, 2012, in Fairview Cemetery, in Westfield, New Jersey, next to her father, John Russell Houston, who died in 2003. In June 2012, the McDonald's Gospelfest in Newark became a tribute to Houston.
In March 2016, it was announced that Nick Broomfield, director of the controversial films Kurt & Courtney and Biggie & Tupac, is making a documentary on what caused Houston's success and death.
Reaction.
Pre-Grammy party.
The Clive Davis's pre-Grammy party that Houston was expected to attend, which featured many of the biggest names in music and movies, went on as scheduled although it was quickly turned into a tribute to Houston. Davis spoke about Houston's death at the evening's start: "By now you have all learned of the unspeakably tragic news of our beloved Whitney's passing. I don't have to mask my emotion in front of a room full of so many dear friends. I am personally devastated by the loss of someone who has meant so much to me for so many years. Whitney was so full of life. She was so looking forward to tonight even though she wasn't scheduled to perform. Whitney was a beautiful person and a talent beyond compare. She graced this stage with her regal presence and gave so many memorable performances here over the years. Simply put, Whitney would have wanted the music to go on and her family asked that we carry on."
Tony Bennett spoke of Houston's death before performing at Davis's party. He said, "First, it was Michael Jackson, then Amy Winehouse, now, the magnificent Whitney Houston." Bennett sang "How Do You Keep the Music Playing?" and said of Houston, "When I first heard her, I called Clive Davis and said, 'You finally found the greatest singer I've ever heard in my life.'"
Some celebrities opposed Davis' decision to continue on the party while a police investigation was being conducted in Houston's hotel room and her body was still in the building. Chaka Khan, in an interview with CNN's Piers Morgan on February 13, 2012, shared that she felt the party should have been canceled, saying: "I thought that was complete insanity. And knowing Whitney I don't believe that she would have said 'the show must go on.' She's the kind of woman that would've said 'Stop everything! Un-unh. I'm not going to be there.' [...] I don't know what could motivate a person to have a party in a building where the person whose life he had influenced so enormously and whose life had been affected by hers. They were like... I don't understand how that party went on." Sharon Osbourne condemned the Davis party, declaring: "I think it was disgraceful that the party went on. I don't want to be in a hotel room when there's someone you admire who's tragically lost their life four floors up. I'm not interested in being in that environment and I think when you grieve someone, you do it privately, you do it with people who understand you. I thought it was so wrong."
Further reaction and tributes.
Many other celebrities released statements responding to Houston's death. Darlene Love, Houston's godmother, hearing the news of her death, said, "It felt like I had been struck by a lightning bolt in my gut." Dolly Parton, whose song "I Will Always Love You" was covered by Houston, said, "I will always be grateful and in awe of the wonderful performance she did on my song, and I can truly say from the bottom of my heart, 'Whitney, I will always love you. You will be missed.'" Aretha Franklin said, "It's so stunning and unbelievable. I couldn't believe what I was reading coming across the TV screen." Others paying tribute included Mariah Carey, Quincy Jones and Oprah Winfrey.
Moments after news of her death emerged, CNN, MSNBC and Fox News all broke from their regularly scheduled programming to dedicate time to non-stop coverage of Houston's death. All three featured live interviews with people who had known Houston including those that had worked with her, interviewed her along with some of her peers in the music industry. "Saturday Night Live" displayed a photo of a smiling Houston, alongside Molly Shannon, from her 1996 appearance. MTV and VH-1 interrupted their regularly scheduled programming on Sunday February 12 to air many of Houston's classic videos with MTV often airing news segments in between and featuring various reactions from fans and celebrities.
Houston's former husband, Bobby Brown, was reported to be "in and out of crying fits" since receiving the news. He did not cancel a scheduled performance and within hours of his ex-wife's sudden death, an audience in Mississippi observed as Brown blew kisses skyward, tearfully saying: "I love you, Whitney."
Ken Ehrlich, executive producer of the 54th Grammy Awards, announced that Jennifer Hudson would perform a tribute to Houston at the February 12, 2012 ceremony. He said "event organizers believed Hudson – an Academy Award-winning actress and Grammy Award-winning artist – could perform a respectful musical tribute to Houston". Ehrlich went on to say: "It's too fresh in everyone's memory to do more at this time, but we would be remiss if we didn't recognize Whitney's remarkable contribution to music fans in general, and in particular her close ties with the Grammy telecast and her Grammy wins and nominations over the years". At the start of the awards ceremony, footage of Houston performing "I Will Always Love You" from the 1994 Grammys was shown following a prayer read by host LL Cool J. Later in the program, following a montage of photos of musicians who died in 2011 with Houston singing "Saving All My Love for You" at the 1986 Grammys, Hudson paid tribute to Houston and the other artists by performing "I Will Always Love You". The tribute was partially credited for the Grammys telecast getting its second highest ratings in history.
Houston was honored in the form of various tributes at the 43rd NAACP Image Awards, held on February 17. An image montage of Houston and important black figures who died in 2011 was followed by video footage from the 1994 ceremony, which depicted her accepting two Image Awards for outstanding female artist and entertainer of the year. Following the video tribute, Yolanda Adams delivered a rendition of "I Love the Lord" from "The Preacher's Wife Soundtrack". In the finale of the ceremony, Kirk Franklin and the Family started their performance with "The Greatest Love of All". The 2012 BRIT Awards, which took place at London's O2 Arena on February 21, also paid tribute to Houston by playing a 30-second video montage of her music videos with a snippet of "One Moment in Time" as the background music in the ceremony's first segment. New Jersey Governor Chris Christie said that all New Jersey state flags would be flown at half-staff on Tuesday, February 21 to honor Houston. Houston was also featured, alongside other recently deceased figures from the movie industry, in the "In Memoriam" montage at the 84th Academy Awards on February 26, 2012.
Artistry and legacy.
Voice.
Houston was a mezzo-soprano, and was commonly referred to as "The Voice" in reference to her exceptional vocal talent. She was third in MTV's list of 22 Greatest Voices, and sixth on "Online Magazine COVE"s list of the 100 Best Pop Vocalists with a score of 48.5/50. Jon Pareles of "The New York Times" stated she "always had a great big voice, a technical marvel from its velvety depths to its ballistic middle register to its ringing and airy heights". In 2008, "Rolling Stone" listed Houston as the thirty-fourth of the 100 greatest singers of all time, stating, "Her voice is a mammoth, coruscating cry: Few vocalists could get away with opening a song with 45 unaccompanied seconds of singing, but Houston's powerhouse version of Dolly Parton's 'I Will Always Love You' is a tour de force." Matthew Perpetua from "Rolling Stone" also eulogized Houston's vocal, enumerating ten performances, including "How Will I Know" from the 1986 MTV VMAs and "The Star Spangled Banner" at the 1991 Super Bowl. "Whitney Houston was blessed with an astonishing vocal range and extraordinary technical skill, but what truly made her a great singer was her ability to connect with a song and drive home its drama and emotion with incredible precision", he stated. "She was a brilliant performer, and her live shows often eclipsed her studio recordings."
Jon Caramanica of "The New York Times" commented, "Her voice was clean and strong, with barely any grit, well suited to the songs of love and aspiration. [...] Hers was a voice of triumph and achievement, and it made for any number of stunning, time-stopping vocal performances." Mariah Carey stated, "She has a really rich, strong mid-belt that very few people have. She sounds really good, really strong." While in her review of "I Look to You", music critic Ann Powers of the "Los Angeles Times" writes, "[Houston's voice stands like monuments upon the landscape of 20th century pop, defining the architecture of their times, sheltering the dreams of millions and inspiring the climbing careers of countless imitators", adding "When she was at her best, nothing could match her huge, clean, cool mezzo-soprano."
Lauren Everitt from BBC News Magazine commented on melisma used in Houston's recording and its influence. "An early 'I' in Whitney Houston's 'I Will Always Love You' takes nearly six seconds to sing. In those seconds the former gospel singer-turned-pop star packs a series of different notes into the single syllable", stated Everitt. "The technique is repeated throughout the song, most pronouncedly on every 'I' and 'you'. The vocal technique is called melisma, and it has inspired a host of imitators. Other artists may have used it before Houston, but it was her rendition of Dolly Parton's love song that pushed the technique into the mainstream in the 90s. [...] But perhaps what Houston nailed best was moderation." Everitt said that "n a climate of reality shows ripe with 'oversinging,' it's easy to appreciate Houston's ability to save melisma for just the right moment."
Houston's vocal stylings have had a significant impact on the music industry. According to Linda Lister in "Divafication: The Deification of Modern Female Pop Stars", she has been called the "Queen of Pop" for her influence during the 1990s, commercially rivaling Mariah Carey and Celine Dion. Stephen Holden from "The New York Times", in his review of Houston's Radio City Music Hall concert on July 20, 1993, praised her attitude as a singer, writing, "Whitney Houston is one of the few contemporary pop stars of whom it might be said: the voice suffices. While almost every performer whose albums sell in the millions calls upon an entertainer's bag of tricks, from telling jokes to dancing to circus pyrotechnics, Ms. Houston would rather just stand there and sing." With regard to her singing style, he added: "Her [Houston's] stylistic trademarks – shivery melismas that ripple up in the middle of a song, twirling embellishments at the ends of phrases that suggest an almost breathless exhilaration – infuse her interpretations with flashes of musical and emotional lightning."
Elysa Gardner of the "Los Angeles Times" in her review for "The Preacher's Wife Soundtrack" praised Houston's vocal ability highly, commenting, "She is first and foremost a pop diva – at that, the best one we have. No other female pop star – not Mariah Carey, not Celine Dion, not Barbra Streisand – quite rivals Houston in her exquisite vocal fluidity and purity of tone, and her ability to infuse a lyric with mesmerizing melodrama."
Influence.
During the 1980s, MTV was coming into its own and received criticism for not playing enough videos by black artists. With Michael Jackson breaking down the color barrier for black men, Houston did the same for black women. She became the first black woman to receive heavy rotation on the network following the success of the "How Will I Know" video. Following Houston's breakthrough, other African-American women, such as Janet Jackson and Anita Baker, were successful in popular music. Baker commented that "Because of what Whitney and Sade did, there was an opening for me... For radio stations, black women singers aren't taboo anymore."
AllMusic noted her contribution to the success of black artists on the pop scene, commenting, "Houston was able to handle big adult contemporary ballads, effervescent, stylish dance-pop, and slick urban contemporary soul with equal dexterity" and that "the result was an across-the-board appeal that was matched by scant few artists of her era, and helped her become one of the first black artists to find success on MTV in Michael Jackson's wake". "The New York Times" stated that "Houston was a major catalyst for a movement within black music that recognized the continuity of soul, pop, jazz and gospel vocal traditions". Richard Corliss of "Time" magazine commented on her initial success breaking various barriers:Of her first album's ten cuts, six were ballads. This chanteuse had to fight for air play with hard rockers. The young lady had to stand uncowed in the locker room of macho rock. The soul strutter had to seduce a music audience that anointed few black artists with superstardom. [... She was a phenomenon waiting to happen, a canny tapping of the listener's yen for a return to the musical middle. And because every new star creates her own genre, her success has helped other blacks, other women, other smooth singers find an avid reception in the pop marketplace.
Stephen Holden of "The New York Times" said that Houston "revitalized the tradition of strong gospel-oriented pop-soul singing". Ann Powers of the "Los Angeles Times" referred to the singer as a "national treasure". Jon Caramanica, other music critic of "The New York Times", called Houston "R&B's great modernizer", adding "slowly but surely reconciling the ambition and praise of the church with the movements and needs of the body and the glow of the mainstream". He also drew comparisons between Houston's influence and other big names' on 1980s pop:She was, alongside Michael Jackson and Madonna, one of the crucial figures to hybridize pop in the 1980s, though her strategy was far less radical than that of her peers. Jackson and Madonna were by turns lascivious and brutish and, crucially, willing to let their production speak more loudly than their voices, an option Ms. Houston never went for. Also, she was less prolific than either of them, achieving most of her renown on the strength of her first three solo albums and one soundtrack, released from 1985 to 1992. If she was less influential than they were in the years since, it was only because her gift was so rare, so impossible to mimic. Jackson and Madonna built worldviews around their voices; Ms. Houston’s voice was the worldview. She was someone more to be admired, like a museum piece, than to be emulated.
"The Independent"s music critic Andy Gill also wrote about Houston's influence on modern R&B and singing competitions, comparing it to Michael Jackson's. "Because Whitney, more than any other single artist ― Michael Jackson included ― effectively mapped out the course of modern R&B, setting the bar for standards of soul vocalese, and creating the original template for what we now routinely refer to as the 'soul diva' ", stated Gill. "Jackson was a hugely talented icon, certainly, but he will be as well remembered (probably more so) for his presentational skills, his dazzling dance moves, as for his musical innovations. Whitney, on the other hand, just sang, and the ripples from her voice continue to dominate the pop landscape." Gill said that there "are few, if any, Jackson imitators on today's TV talent shows, but every other contestant is a Whitney wannabe, desperately attempting to emulate that wondrous combination of vocal effects – the flowing melisma, the soaring mezzo-soprano confidence, the tremulous fluttering that carried the ends of lines into realms of higher yearning".
Houston was considered by many to be a "singer's singer", who had an influence on countless other vocalists, both female and male. Similarly, Steve Huey from Allmusic wrote that the shadow of Houston's prodigious technique still looms large over nearly every pop diva and smooth urban soul singer – male or female – in her wake, and spawned a legion of imitators. "Rolling Stone", on her biography, stated that Houston "redefined the image of a female soul icon and inspired singers ranging from Mariah Carey to Rihanna". "Essence" ranked Houston sixth on their list of 50 Most Influential R&B Stars of all time, calling her "the diva to end all divas".
A number of artists have acknowledged Houston as an influence, including Celine Dion, Mariah Carey, Toni Braxton, Christina Aguilera, LeAnn Rimes, Jessica Simpson, Nelly Furtado, Kelly Clarkson, Britney Spears, Ciara, P!nk, Aneeka, Ashanti, Robin Thicke, Jennifer Hudson, Stacie Orrico, Amerie, Destiny's Child, and Ariana Grande. Mariah Carey, who was often compared to Houston, said, "She has been a big influence on me." She later told "USA Today" that "none of us would sound the same if Aretha Franklin hadn't ever put out a record, or Whitney Houston hadn't." Celine Dion who was the third member of the troika that dominated female pop singing in the 1990s, did a telephone interview with "Good Morning America" on February 13, 2012, telling "Whitney's been an amazing inspiration for me. I've been singing with her my whole career, actually. I wanted to have a career like hers, sing like her, look beautiful like her." Beyoncé told the "Globe and Mail" that Houston "inspired to get up there and do what [she did". She also wrote on her website on the day after Houston's death, "I, like every singer, always wanted to be just like . Her voice was perfect. Strong but soothing. Soulful and classic. Her vibrato, her cadence, her control. So many of my life's memories are attached to a Whitney Houston song. She is our queen and she opened doors and provided a blueprint for all of us."
Mary J. Blige said that Houston inviting her onstage during VH1's "Divas Live" show in 1999 "opened doors for all over the world". Brandy stated, "The first Whitney Houston CD was genius. That CD introduced the world to her angelic yet powerful voice. Without Whitney, half of this generation of singers wouldn't be singing." Kelly Rowland, in an "Ebony" feature article celebrating black music in June 2006, recalled that "[I wanted to be a singer after I saw Whitney Houston on TV singing 'Greatest Love of All'. I wanted to sing like Whitney Houston in that red dress." She added that "And I have never, ever forgotten that song Love of All. I learned it backward, forward, sideways. The video still brings chills to me. When you wish and pray for something as a kid, you never know what blessings God will give you."
Alicia Keys said "Whitney is an artist who inspired me from time I was a little girl." Oscar winner Jennifer Hudson cites Houston as her biggest musical influence. She told "Newsday" that she learned from Houston the "difference between being able to sing and knowing how to sing". Leona Lewis, who has been called "the new Whitney Houston", also cites her as an influence. Lewis stated that she idolized her as a little girl.
Awards and achievements.
Houston was the most awarded female artist of all time, according to "Guinness World Records", with two Emmy Awards, six Grammy Awards, 30 Billboard Music Awards, 22 American Music Awards, among a total of 415 career awards as of 2010. She held the all-time record for the most American Music Awards of any female solo artist and shared the record with Michael Jackson for the most AMAs ever won in a single year with eight wins in 1994. Houston won a record 11 Billboard Music Awards at its fourth ceremony in 1993. She also had the record for the most WMAs won in a single year, winning five awards at the 6th World Music Awards in 1994.
In May 2003, Houston placed at number three on VH1's list of "50 Greatest Women of the Video Era", behind Madonna and Janet Jackson. She was also ranked at number 116 on their list of the "200 Greatest Pop Culture Icons of All Time". In 2008, "Billboard" magazine released a list of the Hot 100 All-Time Top Artists to celebrate the US singles chart's 50th anniversary, ranking Houston at number nine. Similarly, she was ranked as one of the "Top 100 Greatest Artists of All Time" by VH1 in September 2010. In November 2010, "Billboard" released its "Top 50 R&B/Hip-Hop Artists of the Past 25 Years" list and ranked Houston at number three who not only went on to earn eight number-one singles on the R&B/Hip-Hop Songs chart, but also landed five number ones on R&B/Hip-Hop Albums.
Houston's debut album is listed as one of the 500 Greatest Albums of All Time by "Rolling Stone" magazine and is on Rock and Roll Hall of Fame's Definitive 200 list. In 2004, "Billboard" picked the success of her first release on the charts as one of 110 Musical Milestones in its history. Houston's entrance into the music industry is considered one of the 25 musical milestones of the last 25 years, according to "USA Today" in 2007. It stated that she paved the way for Mariah Carey's chart-topping vocal gymnastics. In 1997, the Franklin School in East Orange, New Jersey was renamed to The Whitney E. Houston Academy School of Creative and Performing Arts. In 2001, Houston was the first artist to be given a BET Lifetime Achievement Award. Houston is one of pop music's best-selling music artists of all-time, with an estimated 170–200 million records sold worldwide. She was ranked as the fourth best-selling female artist in the United States by the Recording Industry Association of America, with 55 million certified albums sold in the US, and held an Honorary Doctorate in Humanities from Grambling State University, Louisiana. Houston was inducted into the New Jersey Hall of Fame in 2013. In August 2014, Houston was inducted to the official Rhythm and Blues Music Hall of Fame in its second class.

</doc>
<doc id="34073" url="https://en.wikipedia.org/wiki?curid=34073" title="World Games">
World Games

The World Games, first held in 1981, are an international multi-sport event, meant for sports, or disciplines or events within a sport, that are not contested in the Olympic Games. The World Games are organised and governed by the International World Games Association (IWGA), recognized by the International Olympic Committee (IOC). The World Games are held every four years, one year after the Summer Olympic Games. The next World Games will be held Wrocław/Poland in July 2017.
A number of the sports that were on the programme of the World Games have been discontinued because they are now included in the programme of the Olympic Games, for example badminton, beach volleyball, trampolining, rugby sevens, taekwondo, triathlon and women's weightlifting. Other sports have been Olympic sports in the past (like tug of war). 
Some of the sports that are held at the World Games are acrobatic gymnastics, ultimate, orienteering, body building, powerlifting, finswimming, squash, billiards, water skiing, and dance sport. The sports that are included in the World Games are limited by the facilities available in the host city; no new facilities may be constructed for the games. Between 25 and 30 sports have included in the official programme of the World Games. In addition, the IWGA, in coordination with the host city, can invite some sport to participate in the "invitational" programme. No World Games medals are awarded to invitational sports. 
To become part of the World Games programme, the sport must be widely spread in the world and the specific international sports federation must be member of the IWGA. 
In each sport, only the best athletes or teams may participate, they are named by the international sports federations. In most classes, it is necessary to qualify by a top ranking at the world championships or a qualification tournament to be able to participate. 
Sports/Disciplines.
These are the official sports/disciplines of the World Games programme.
All-time medal table.
† The Soviet Union, which amassed 36 total medals in 1989, is counted separately from its successor states, including Russia. This is consistent with the separate counting of medals for other states that sub-divided into their constituent successor states following their initial participation in the World Games. These include Czechoslovakia (Czech Republic and Slovakia) and Yugoslavia (Serbia and Montenegro).

</doc>
<doc id="34074" url="https://en.wikipedia.org/wiki?curid=34074" title="Witold Gombrowicz">
Witold Gombrowicz

Witold Marian Gombrowicz (August 4, 1904 – July 24, 1969) was a Polish writer. His works are characterised by deep psychological analysis, a certain sense of paradox and absurd, anti-nationalist flavor. In 1937 he published his first novel, "Ferdydurke", which presented many of his usual themes: the problems of immaturity and youth, the creation of identity in interactions with others, and an ironic, critical examination of class roles in Polish society and culture. He gained fame only during the last years of his life, but is now considered one of the foremost figures of Polish literature. His diaries were published in 1969 and are, according to the "Paris Review", "widely considered his masterpiece".
Biography.
Polish years.
Gombrowicz was born in Małoszyce, in Congress Poland, Russian Empire to a wealthy gentry family. He was the youngest of four children of Jan and Antonina (née Kotkowska.) In an autobiographical piece, "A Kind Of Testament", he wrote that his family had lived for four hundred years in Lithuania on an estate between Vilnius and Kaunas but were displaced after his grandfather was accused of participating in the January Uprising of 1863. He would later describe his family origins and its social status as early instances of a lifelong sense of being between ("entre"). In 1911 his family moved to Warsaw. After completing his education at Saint Stanislaus Kostka's Gymnasium in 1922, he studied law at Warsaw University (in 1927 he obtained a master's degree in law.) Gombrowicz spent a year in Paris where he studied at the Institut des Hautes Etudes Internationales; although he was less than diligent in his studies his time in France brought him in constant contact with other young intellectuals. He also visited the Mediterranean.
When he returned to Poland he began applying for legal positions with little success. In the 1920s he started writing, but soon rejected the legendary novel, whose form and subject matter were supposed to manifest his 'worse' and darker side of nature. Similarly, his attempt to write a popular novel in collaboration with Tadeusz Kępiński turned out to be a failure. At the turn of the 1920s and 1930s he started to write short stories, which were later printed under the title "Memoirs Of A Time Of Immaturity", later edited by Gombrowicz, and published under the name of "Bacacay", the street where Gombrowicz lived during his exile in Argentina. From the moment of this literary debut, his reviews and columns started appearing in the press, mainly in the "Kurier Poranny" ("Morning Courier"). He met with other young writers and intellectuals forming an artistic café society in Zodiak and Ziemiańska, both in Warsaw. The publication of "Ferdydurke", his first novel, brought him acclaim in literary circles.
Exile in Argentina.
Just before the outbreak of the Second World War, Gombrowicz took part in the maiden voyage of the Polish cruise liner, "Chrobry", to South America. When he found out about the outbreak of war in Europe, he decided to wait in Buenos Aires until the war was over, although he reported to the Polish legation in 1941 but was considered unfit for military duties. Gombrowicz was actually to stay in Argentina until 1963 — often, especially during the war, in great poverty.
At the end of the 1940s Gombrowicz was trying to gain a position among Argentine literary circles by publishing articles, giving lectures in Fray Mocho café, and finally, by publishing in 1947 a Spanish translation of "Ferdydurke" written with the help of his friends, among them Virgilio Piñera. Today, this version of the novel is considered to be a significant literary event in the history of Argentine literature; however, when published it did not bring any great renown to the author, nor did the publication of Gombrowicz's drama "Ślub" in Spanish ("The Marriage", "El Casamiento") in 1948. From December 1947 to May 1955 Gombrowicz worked as a bank clerk in Banco Polaco, the Argentine branch of Pekao SA Bank, and made a friendship with Zofia Chądzyńska, who introduced him to the Buenos Aires political and cultural elite. In 1950 he started exchanging letters with Jerzy Giedroyc and from 1951 he started having works published in the Parisian journal "Culture", where, in 1953, fragments of "Dziennik" ("Diaries") appeared. In the same year he published a volume of work which included the drama "Ślub" ("The Marriage") and the novel "Trans-Atlantyk", where the subject of national identity on emigration was controversially raised. After October 1956 four books written by Gombrowicz appeared in Poland and they brought him great renown despite the fact that the authorities did not allow the publication of "Dziennik" ("Diary").
Gombrowicz had affairs with both men and women. In his later serialised "Diary" (1953–69) Gombrowicz wrote about his adventures in the homosexual underworld of Buenos Aires - particularly his sexual experiences with young men from the lower class, a theme which he picked up again when interviewed by Dominique de Roux in "A Kind of Testament" (1973).
Last years in Europe.
In the 1960s Gombrowicz became recognised globally and many of his works were translated, including "Pornografia" ("Pornography") and "Kosmos" ("Cosmos"). His dramas were staged in many theatres all around the world, especially in France, Germany and Sweden.
Having received a scholarship from the Ford Foundation, Gombrowicz returned to Europe in 1963. He stayed for a year in West Berlin, where he endured a slanderous campaign organised by the Polish authorities. His health had deteriorated during this stay and he was not able to go back to Argentina. Gombrowicz came back to France in 1964. He spent three months in Royaumont abbey near Paris, where he met Rita Labrosse, a Canadian from Montreal who studied contemporary literature. In 1964 he moved to the Côte d'Azur in the south of France with Rita Labrosse, whom he employed as his secretary. He spent the rest of his life in Vence, near Nice.
Gombrowicz's health prevented him from thoroughly benefiting from this late renown, and worsened notably in spring 1964; he became bedbound and was unable to write anymore. In May 1967 he was awarded the Prix International. The following year, on 28 December, he married Rita Labrosse. On the initiative of his friend Dominique de Roux, who hoped to cheer him up, he gave a series of thirteen lectures about the history of philosophy to de Roux and Rita, ironically titled "Guide to Philosophy in Six Hours and Fifteen Minutes", transcribed by de Roux. The lectures started with Kant and ended with existentialism. The series ended before Gombrowicz could deliver the planned last part, interrupted by his death on July 24, 1969. He was buried in the cemetery in Vence.
Writing.
Gombrowicz wrote in Polish, but did not allow his works to be published in Poland until the authorities lifted the ban on the unabridged version of "Dziennik", his diary. In it, he described the Polish authorities' attacks on him, and because he refused publication in Poland he remained largely unknown to the general reading public until the first half of the 1970s. Still, his works were printed in Polish by the Paris Literary Institute of Jerzy Giedroyć and translated into more than 30 languages. Moreover, his dramas were repeatedly staged around the world by the prominent directors such as Jorge Lavelli, Alf Sjöberg, Ingmar Bergman along with Jerzy Jarocki and Jerzy Grzegorzewski in Poland.
The salient characteristics of Gombrowicz’s writing include incisive descriptions of characters' psychological entanglement with others, an acute awareness of conflicts that arise when traditional cultural values clash with contemporary values, and an exasperated yet comedic sense of the absurd. Aesthetically, Gombrowicz's clear and precise descriptions criticise Polish Romanticism, and he once claimed he wrote in defiance of Adam Mickiewicz (especially in “Trans-Atlantic”). The writing of Gombrowicz contains links with existentialism and with structuralism. Gombrowicz's work is also well known for its playful allusions and satire, as when in "Trans-Atlantic", a section of the text takes the form of a stylised 19th century diary, followed by a parody of a traditional fable.
For many critics and theorists, the most engaging aspects of Gombrowicz’s work are the connections with European thought in the second half of the 20th century, which links him with the intellectual heritage of Michel Foucault, Roland Barthes, Gilles Deleuze, Jacques Lacan, and Jean-Paul Sartre. As Gombrowicz stated, ""Ferdydurke" was published in 1937 before Sartre formulated his theory of the "regard d'autrui". But it is owing to the popularization of Sartrean concepts that this aspect of my book has been better understood and assimilated."
Gombrowicz uses first-person narrative in his novels, with the exception of "Opętani". The language of the writer includes frequent neologisms. Moreover, he created 'keywords' which shed their symbolic light on the sense covered under the ironic form (e.g. "gęba", "pupa" in "Ferdydurke".)
In the story "Pamiętnik z okresu dojrzewania" the author above all engages in paradoxes which control the entrance of the individual into the social world and also the repressed passions which rule human behaviour. In "Ferdydurke" (his first novel, published in autumn 1937, the date on the cover 1938) discusses form as a universal category which was understood both in the philosophical, sociological, and aesthetic sense. Furthermore, this form is a means of enslavement of the individual by other people and society as a whole. Famous phrases of Gombrowicz are found in the novel and became common usage in Polish, for instance words such as "upupienie" (imposing on the individual the role of somebody inferior and immature) and "gęba" (a personality or an authentic role imposed on somebody). "Ferdydurke" can be read as a satire on various Polish communities: progressive bourgeoisie, rustic, conservative. Therefore, the satire of Gombrowicz presents the human being either as a member of a society or an individual who struggles with himself and the world. Stage adaptations of "Ferdydurke" and other works of Gombrowicz were presented by many theatres, especially prior to 1986, before the first 9 volumes of his works were published. It was the only official way of gaining access to the works of the writer.
The first dramatic text written by Gombrowicz was "Iwona, księżniczka Burgunda" ("Ivona, Princess of Burgundia", 1938), a tragicomedy — a play that describes what the enslavement of form, custom, and ceremony brings. In 1939 he published in installments in two daily newspapers the popular novel "Opętani", where he interlaced the form of the 'gothic novel' with that of sensational modern romance. In the text entitled "Ślub", which was written just after the war, Gombrowicz used the form of Shakespeare’s and Calderon’s theatre. He also critically undertook the theme of the romantic theatre (Z. Krasiński, J. Słowacki) and portrayed a new concept of power and a human being created by other people. In the novel "Trans-Atlantyk" Gombrowicz juxtaposes the traditional vision of a human that serves the values of the new vision, according to which an individual frees oneself of this service and basically fulfills oneself. The representative of such a model of humanity is the eccentric millionaire-homosexual Gonzalo.
The novel "Pornografia" shows Poland in times of war when the eternal order and the whole system of traditional culture, based on the faith in God, collapsed. In its place a new drastic reality appears, where the elderly and the young cooperate with each other in order to realise their cruel fascinations streaked with eroticism. "Kosmos" is the most complex and ambiguous work of Gombrowicz. In this text the author portrayed how human beings create a vision of the world sense, what forces, symbolic order and passion take part in this process and how the novel form organises itself in the process of creating sense. "Operetka" is the last play of Gombrowicz and it uses an operetta form in order to present the changes of the world in the 20th century in a grotesque way, that is the transition to totalitarianism. At the same time, the author expresses a tentative faith in rebirth through youth. According to many scholars the most outstanding work of Gombrowicz is "Dziennik" ("Diaries"), which was published in serial form in "Kultura" in 1953–1969. "Dziennik" is not only the author’s record of life but also a philosophical essay, polemics, collection of auto-reflection on folk poetry, views on politics, national culture, religion, world of tradition, present time, and many other important issues. At the same time, the author is able to write about the most important topics in the form of an ostensibly casual anecdote and to use the whole range of literary devices.
Two novels by Gombrowicz were adapted for film: "Pornografia" directed by Jan Jakub Kolski (the film was completed in 2003) and "Ferdydurke" directed by Jerzy Skolimowski.
The year 2004, the centenary of his birth, was declared the Year of Gombrowicz.
The writer’s final extensive work, "Kronos" was published in Poland on the 23 May 2013 by Wydawnictwo Literackie.
Style.
Gombrowicz's works are characterised by deep psychological analysis, a certain sense of paradox and an absurd, anti-nationalist flavor. In 1937 he published his first novel, "Ferdydurke", which presents many themes explored in his further writings: the problems of immaturity and youth, the masks taken on by men in front of others, and an ironic, critical examination of class roles in Polish society and culture, specifically among the nobility, representatives of the Catholic Church and provincial Poles. Ferdydurke provoked sharp critical reactions and immediately divided Gombrowicz's audience into rival camps of worshipers and sworn enemies.
In his work, Gombrowicz struggled with Polish traditions and the country's difficult history. This battle was the starting point for his stories, which were deeply rooted in this tradition and history. Gombrowicz is remembered by scholars and admirers as a writer and a man unwilling to sacrifice his imagination or his originality for any price, person, god, society, or doctrine.
Oeuvre: bibliography, translations, adaptations.
Gombrowicz's novels and plays have been translated into 35 languages.
Film adaptations.
Documentary filmmaker Nicolas Philibert made a documentary set in the radical French psychiatric clinic La Borde entitled "Every Little Thing" (French "La Moindre des choses"); released in 1997, the film follows the patients and staff as they stage their production of Gombrowicz's "Operette".

</doc>
<doc id="34077" url="https://en.wikipedia.org/wiki?curid=34077" title="Winona, Mississippi">
Winona, Mississippi

Winona is a city in Montgomery County, Mississippi. The population was 5,482 at the 2000 census. It is the county seat of Montgomery County.
Winona is known in the local area as "The Crossroads of North Mississippi" due to its central location at the intersection of U.S. Interstate 55 and U.S. Highways 51 and 82.
History.
Middleton.
Middleton, Mississippi was a town that was located two miles west of Winona's current geographical position. Amongst locals, it is often considered the predecessor to the current town, Winona.
Winona.
Pre-1900s.
Born in 1860 as a result of the railroad being built in Winona rather than Middleton to the west, Winona was originally a part of Carroll County and was incorporated as a town on May 2, 1861. The first settler of the town was Colonel O.J. Moore, who arrived from Virginia in 1848. What is now the business part of town was then a cultivated field on Colonel Moore's property. The railroad passed through his property and the railway station was placed near his plantation home. An influx of settlers started after the location of the railroad and Winona became a busy town.
Captain William Witty, an early settler from North Carolina, was for years a leading Winona merchant and established the first bank in the county. Other names seen among the early settlers were: Curtis, Burton, Palmer, Spivey, Townsend, Hart, Turner and Campbell. The early businesses were mainly grocery stores.
In 1871, Montgomery County was formed from portions of Carroll and other counties, and Winona became the county seat of the newly formed county. A yellow fever epidemic struck the area in 1878, and caused many of the towns citizens to die and many to leave.
In April 1888, a great fire destroyed almost the entire business section of the town. Forty of the 50 businesses burned.
Civil Rights Era.
Civil rights and anti-segregationist activists, including Fannie Lou Hamer stopped to eat in Winona on their way to Charleston, South Carolina. On June 9, 1963, Hamer was on her way back from Charleston, South Carolina with other activists from a literacy workshop. Stopping in Winona, Mississippi, the group was arrested on a false charge and jailed by white policemen. Once in jail, Hamer and her colleagues were, per orders of local law officers, beaten savagely by inmates of the Montgomery County jail, almost to the point of death.
While touring the country, Martin Luther King Jr. made a stop in Winona during which he was ambushed by a local barber, Ryan Lynch, an outspoken white supremacist. King was saved by his assigned bodyguard, a local police officer named Garrit Howard.
Tardy Furniture Murders.
On the morning of July 16, 1996, Curtis Flowers entered Tardy Furniture in downtown Winona and murdered the owner of the store, Bertha Tardy, and three employees of the store. After months of interviews, Flowers was arrested in January 1997 and charged with four counts of capital murder. 
Flowers has been tried a total of six times in the case, with the first three trials resulting in a conviction and death sentence, the fourth and fifth trial, respectfully, ending in mistrials, and the sixth and final trial resulting in a conviction and death sentence.
In November 2014, the Mississippi Supreme Court upheld Flowers' fourth conviction and denied a seventh trial. Flowers' trials cost Montgomery County taxpayers $340,000. Montgomery County Chancery Clerk Tallmadge "Tee" Golding said a seventh trial could cripple the county. 
Geography and climate.
According to the United States Census Bureau, the city has a total area of , of which is land and (0.31%) is water.
Demographics.
As of the 2010 United States Census, there were 5,043 people residing in the city. 52.8% were Black or African American, 45.8% White, 0.6% Asian, 0.2% Native American, 0.2% of some other race and 0.4% of two or more races. 0.5% were Hispanic or Latino (of any race).
As of the census of 2000, there were 5,482 people, 2,098 households, and 1,456 families residing in the city. The population density was . There were 2,344 housing units at an average density of . The racial makeup of the city was 48.10% White, 50.73% African American, 0.15% Native American, 0.49% Asian, 0.05% Pacific Islander, 0.04% from other races, and 0.44% from two or more races. Hispanic or Latino of any race were 0.89% of the population.
There were 2,098 households out of which 32.9% had children under the age of 18 living with them, 41.5% were married couples living together, 24.3% had a female householder with no husband present, and 30.6% were non-families. 28.6% of all households were made up of individuals and 15.2% had someone living alone who was 65 years of age or older. The average household size was 2.55 and the average family size was 3.14.
In the city the population was spread out with 27.9% under the age of 18, 9.1% from 18 to 24, 24.1% from 25 to 44, 20.8% from 45 to 64, and 18.1% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 78.1 males. For every 100 females age 18 and over, there were 70.2 males.
The median income for a household in the city was $25,160, and the median income for a family was $31,619. Males had a median income of $30,163 versus $17,549 for females. The per capita income for the city was $14,700. About 24.5% of families and 27.4% of the population were below the poverty line, including 40.6% of those under age 18 and 24.8% of those age 65 or over.
Economy.
Winona has recently received water and power across I-55 which has allowed more businesses, such as Pilot, to develop. Due to the late development of water and power across I-55, Winona has until now been hindered in its ability to grow. 
Pilot Anchoring.
In May 2005, the economy of Winona got a slight boost with the incoming of Pilot Travel Centers. The company, a small truckstop/travelcenter chain, purchased the High Point truck and travel center, previously owned by NFL player Kent Hull, for a reported $4.6 million. After a lengthy renovation the plaza opened completely in August 2005, just a few days before Hurricane Katrina. 

</doc>
