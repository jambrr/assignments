<doc id="31741" url="https://en.wikipedia.org/wiki?curid=31741" title="Unemployment">
Unemployment

Unemployment occurs when people who are without work are actively seeking work. The unemployment rate is a measure of the prevalence of unemployment and it is calculated as a percentage by dividing the number of unemployed individuals by all individuals currently in the labor force. During periods of recession, an economy usually experiences a relatively high unemployment rate. According to International Labour Organization report, more than 200 million people globally or 6% of the world's workforce were without a job in 2012.
There remains considerable theoretical debate regarding the causes, consequences and solutions for unemployment. Classical economics, New classical economics, and the Austrian School of economics argue that market mechanisms are reliable means of resolving unemployment. These theories argue against interventions imposed on the labor market from the outside, such as unionization, bureaucratic work rules, minimum wage laws, taxes, and other regulations that they claim discourage the hiring of workers.
Keynesian economics emphasizes the cyclical nature of unemployment and recommends government interventions in the economy that it claims will reduce unemployment during recessions. This theory focuses on recurrent shocks that suddenly reduce aggregate demand for goods and services and thus reduce demand for workers. Keynesian models recommend government interventions designed to increase demand for workers; these can include financial stimuli, publicly funded job creation, and expansionist monetary policies. Its namesake, economist John Maynard Keynes, believed that the root cause of unemployment is the desire of investors to receive more money rather than produce more products, which is not possible without public bodies producing new money.
In addition to these comprehensive theories of unemployment, there are a few categorizations of unemployment that are used to more precisely model the effects of unemployment within the economic system. The main types of unemployment include structural unemployment which focuses on structural problems in the economy and inefficiencies inherent in labour markets, including a mismatch between the supply and demand of laborers with necessary skill sets. Structural arguments emphasize causes and solutions related to disruptive technologies and globalization. Discussions of frictional unemployment focus on voluntary decisions to work based on each individuals' valuation of their own work and how that compares to current wage rates plus the time and effort required to find a job. Causes and solutions for frictional unemployment often address job entry threshold and wage rates. Behavioral economists highlight individual biases in decision making, and often involve problems and solutions concerning sticky wages and efficiency wages.
Definitions, types, and theories.
Economists distinguish between various overlapping types of and theories of unemployment, including cyclical or Keynesian unemployment, frictional unemployment, structural unemployment and classical unemployment. Some additional types of unemployment that are occasionally mentioned are seasonal unemployment, hardcore unemployment, and hidden unemployment.
Though there have been several definitions of "voluntary" and "involuntary unemployment" in the economics literature, a simple distinction is often applied. Voluntary unemployment is attributed to the individual's decisions, whereas involuntary unemployment exists because of the socio-economic environment (including the market structure, government intervention, and the level of aggregate demand) in which individuals operate. In these terms, much or most of frictional unemployment is voluntary, since it reflects individual search behavior. Voluntary unemployment includes workers who reject low wage jobs whereas involuntary unemployment includes workers fired due to an economic crisis, industrial decline, company bankruptcy, or organizational restructuring.
On the other hand, cyclical unemployment, structural unemployment, and classical unemployment are largely involuntary in nature. However, the existence of structural unemployment may reflect choices made by the unemployed in the past, while classical (natural) unemployment may result from the legislative and economic choices made by labour unions or political parties. So, in practice, the distinction between voluntary and involuntary unemployment is hard to draw.
The clearest cases of involuntary unemployment are those where there are fewer job vacancies than unemployed workers even when wages are allowed to adjust, so that even if all vacancies were to be filled, some unemployed workers would still remain. This happens with cyclical unemployment, as macroeconomic forces cause microeconomic unemployment which can boomerang back and exacerbate these macroeconomic forces.
Classical unemployment.
Classical or real-wage unemployment occurs when real wages for a job are set above the market-clearing level, causing the number of job-seekers to exceed the number of vacancies. On the other hand, other economists argue that as wages fall below a livable wage many choose to drop out of the labor market and no longer seek employment. This is especially true in countries where low-income families are supported through public welfare systems. In such cases wages would have to be high enough to motivate people to choose employment over what they receive through public welfare. Wages below a livable wage are likely to result in lower labor market participation in above stated scenario. In addition it must be noted that consumption of goods and services is the primary driver of increased need for labor. Higher wages leads to workers having more income available to consume goods and services. Therefore, higher wages increase general consumption and as a result need for labor increases and unemployment decreases in the economy.
Many economists have argued that unemployment increases with increased governmental regulation. For example, minimum wage laws raise the cost of some low-skill laborers above market equilibrium, resulting in increased unemployment as people who wish to work at the going rate cannot (as the new and higher enforced wage is now greater than the value of their labor). Laws restricting layoffs may make businesses less likely to hire in the first place, as hiring becomes more risky.
However, this argument overly simplifies the relationship between wage rates and unemployment, ignoring numerous factors, which contribute to unemployment. Some, such as Murray Rothbard, suggest that even social taboos can prevent wages from falling to the market-clearing level.
In "Out of Work: Unemployment and Government in the Twentieth-Century America", economists Richard Vedder and Lowell Gallaway argue that the empirical record of wages rates, productivity, and unemployment in American validates classical unemployment theory. Their data shows a strong correlation between adjusted real wage and unemployment in the United States from 1900 to 1990. However, they maintain that their data does not take into account exogenous events.
Cyclical unemployment.
Cyclical, deficient-demand, or Keynesian unemployment, occurs when there is not enough aggregate supply in the economy to provide jobs for everyone who wants to work. Demand for most goods and services falls, less production is needed and consequently fewer workers are needed, wages are sticky and do not fall to meet the equilibrium level, and mass unemployment results. Its name is derived from the frequent shifts in the business cycle although unemployment can also be persistent as occurred during the Great Depression of the 1930s.
With cyclical unemployment, the number of unemployed workers exceeds the number of job vacancies, so that even if full employment were attained and all open jobs were filled, some workers would still remain unemployed. Some associate cyclical unemployment with frictional unemployment because the factors that cause the friction are partially caused by cyclical variables. For example, a surprise decrease in the money supply may shock rational economic factors and suddenly inhibit aggregate demand.
Keynesian economists on the other hand see the lack of supply for jobs as potentially resolvable by government intervention. One suggested interventions involves deficit spending to boost employment and demand. Another intervention involves an expansionary monetary policy that increases the supply of money which should reduce interest rates which should lead to an increase in non-governmental spending.
Marxian theory of unemployment.
Marxists also share the Keynesian viewpoint of the relationship between economic demand and employment, but with the caveat that the market system's propensity to slash wages and reduce labor participation on an enterprise level causes a requisite decrease in aggregate demand in the economy as a whole, causing crises of unemployment and periods of low economic activity before the capital accumulation (investment) phase of economic growth can continue.
According to Karl Marx, unemployment is inherent within the unstable capitalist system and periodic crises of mass unemployment are to be expected. The function of the proletariat within the capitalist system is to provide a "reserve army of labour" that creates downward pressure on wages. This is accomplished by dividing the proletariat into surplus labour (employees) and under-employment (unemployed). This reserve army of labour fight among themselves for scarce jobs at lower and lower wages.
At first glance, unemployment seems inefficient since unemployed workers do not increase profits. However, unemployment is profitable within the global capitalist system because unemployment lowers wages which are costs from the perspective of the owners. From this perspective low wages benefit the system by reducing economic rents. Yet, it does not benefit workers. Capitalist systems unfairly manipulate the market for labour by perpetuating unemployment which lowers laborers' demands for fair wages. Workers are pitted against one another at the service of increasing profits for owners.
According to Marx, the only way to permanently eliminate unemployment would be to abolish capitalism and the system of forced competition for wages and then shift to a socialist or communist economic system. For contemporary Marxists, the existence of persistent unemployment is proof of the inability of capitalism to ensure full employment.
Full employment.
In demand-based theory, it is possible to abolish cyclical unemployment by increasing the aggregate demand for products and workers. However, eventually the economy hits an "inflation barrier" imposed by the four other kinds of unemployment to the extent that they exist. Historical experience suggests that low unemployment affects inflation in the short term but not the long term. In the long term, the velocity of money supply measures such as the MZM ("money zero maturity," representing cash and equivalent demand deposits) velocity is far more predictive of inflation than low unemployment.
Some demand theory economists see the inflation barrier as corresponding to the natural rate of unemployment. The "natural" rate of unemployment is defined as the rate of unemployment that exists when the labour market is in equilibrium and there is pressure for neither rising inflation rates nor falling inflation rates. An alternative technical term for this rate is the NAIRU or the Non-Accelerating Inflation Rate of Unemployment. No matter what its name, demand theory holds that this means that if the unemployment rate gets "too low," inflation will accelerate in the absence of wage and price controls (incomes policies).
One of the major problems with the NAIRU theory is that no one knows exactly what the NAIRU is (while it clearly changes over time). The margin of error can be quite high relative to the actual unemployment rate, making it hard to use the NAIRU in policy-making.
Another, normative, definition of full employment might be called the "ideal" unemployment rate. It would exclude all types of unemployment that represent forms of inefficiency. This type of "full employment" unemployment would correspond to only frictional unemployment (excluding that part encouraging the McJobs management strategy) and would thus be very low. However, it would be impossible to attain this full-employment target using only demand-side Keynesian stimulus without getting below the NAIRU and causing accelerating inflation (absent incomes policies). Training programs aimed at fighting structural unemployment would help here.
To the extent that hidden unemployment exists, it implies that official unemployment statistics provide a poor guide to what unemployment rate coincides with "full employment".
Structural unemployment.
Structural unemployment occurs when a labour market is unable to provide jobs for everyone who wants one because there is a mismatch between the skills of the unemployed workers and the skills needed for the available jobs. Structural unemployment is hard to separate empirically from frictional unemployment, except to say that it lasts longer. As with frictional unemployment, simple demand-side stimulus will not work to easily abolish this type of unemployment.
Structural unemployment may also be encouraged to rise by persistent cyclical unemployment: if an economy suffers from long-lasting low aggregate demand, it means that many of the unemployed become disheartened, while their skills (including job-searching skills) become "rusty" and obsolete. Problems with debt may lead to homelessness and a fall into the vicious circle of poverty.
This means that they may not fit the job vacancies that are created when the economy recovers. The implication is that sustained "high" demand may "lower" structural unemployment. This theory of persistence in structural unemployment has been referred to as an example of path dependence or "hysteresis".
Much "technological unemployment", due to the replacement of workers by machines, might be counted as structural unemployment. Alternatively, technological unemployment might refer to the way in which steady increases in labour productivity mean that fewer workers are needed to produce the same level of output every year. The fact that aggregate demand can be raised to deal with this problem suggests that this problem is instead one of cyclical unemployment. As indicated by Okun's Law, the demand side must grow sufficiently quickly to absorb not only the growing labour force but also the workers made redundant by increased labour productivity.
Seasonal unemployment may be seen as a kind of structural unemployment, since it is a type of unemployment that is linked to certain kinds of jobs (construction work, migratory farm work). The most-cited official unemployment measures erase this kind of unemployment from the statistics using "seasonal adjustment" techniques. This results in substantial, permanent structural unemployment.
Frictional unemployment.
Frictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. It is sometimes called search unemployment and can be voluntary based on the circumstances of the unemployed individual. Frictional unemployment is always present in an economy, so the level of involuntary unemployment is properly the unemployment rate minus the rate of frictional unemployment, which means that increases or decreases in unemployment are normally under-represented in the simple statistics.
Frictional unemployment exists because both jobs and workers are heterogeneous, and a mismatch can result between the characteristics of supply and demand. Such a mismatch can be related to skills, payment, work-time, location, seasonal industries, attitude, taste, and a multitude of other factors. New entrants (such as graduating students) and re-entrants (such as former homemakers) can also suffer a spell of frictional unemployment.
Workers as well as employers accept a certain level of imperfection, risk or compromise, but usually not right away; they will invest some time and effort to find a better match. This is in fact beneficial to the economy since it results in a better allocation of resources. However, if the search takes too long and mismatches are too frequent, the economy suffers, since some work will not get done. Therefore, governments will seek ways to reduce unnecessary frictional unemployment through multiple means including providing education, advice, training, and assistance such as daycare centers.
The frictions in the labour market are sometimes illustrated graphically with a Beveridge curve, a downward-sloping, convex curve that shows a correlation between the unemployment rate on one axis and the vacancy rate on the other. Changes in the supply of or demand for labour cause movements along this curve. An increase (decrease) in labour market frictions will shift the curve outwards (inwards).;koklm,
Hidden unemployment.
Hidden, or covered, unemployment is the unemployment of potential workers that is not reflected in official unemployment statistics, due to the way the statistics are collected. In many countries only those who have no work but are actively looking for work (and/or qualifying for social security benefits) are counted as unemployed. Those who have given up looking for work (and sometimes those who are on Government "retraining" programs) are not officially counted among the unemployed, even though they are not employed.
The statistic also does not count the "underemployed" – those working fewer hours than they would prefer or in a job that doesn't make good use of their capabilities. In addition, those who are of working age but are currently in full-time education are usually not considered unemployed in government statistics. Traditional unemployed native societies who survive by gathering, hunting, herding, and farming in wilderness areas, may or may not be counted in unemployment statistics. Official statistics often underestimate unemployment rates because of hidden unemployment.
Long-term unemployment.
This is defined in European Union statistics, as unemployment lasting for longer than one year. The United States Bureau of Labor Statistics (BLS), which reports current long-term unemployment rate at 1.9 percent, defines this as unemployment lasting 27 weeks or longer. Long-term unemployment is a component of structural unemployment, which results in long-term unemployment existing in every social group, industry, occupation, and all levels of education. Current long- term unemployment is a result of a 6-year period of weak business hiring, which is the cause of an aggregate demand shortfall. Another factor of current long-term unemployment is the stigma attached to it that makes it harder for people seeking jobs to find employment in low and medium skill jobs because those employers care about long-term unemployment, while high skill jobs mainly focus on an applicants past experiences instead of their long-term unemployment. In response to current rates of long-term unemployment in the United States, which accounts for 31.9% of total unemployment, President Barack Obama implemented policies in January 2014 to assist those who desire to re-enter the work place but are struggling. As of 15 October 2014, the Department of Labor's H-1B funds are providing 23 grants, a total of $170 million, for programs in 20 states and Puerto Rico to help the long-term unemployed re-enter the workforce. The grants were divided between non-profits, local government, and employers to train and match long-term unemployed job seekers for in-demand jobs. The Obama Administration announced a call to action for over 300 businesses to implement best practices for hiring and recruiting the long-term unemployed to provide these candidates an equal chance throughout the hiring process. Along with businesses, the Office of Personnel Management is providing guidance to Federal agencies to establish a trend of fair treatment and consideration for people who are long-term unemployed candidates applying for employment by Federal agencies.
Measurement.
There are also different ways national statistical agencies measure unemployment. These differences may limit the validity of international comparisons of unemployment data. To some degree these differences remain despite national statistical agencies increasingly adopting the definition of unemployment by the International Labour Organization. To facilitate international comparisons, some organizations, such as the OECD, Eurostat, and International Labor Comparisons Program, adjust data on unemployment for comparability across countries.
Though many people care about the number of unemployed individuals, economists typically focus on the unemployment rate. This corrects for the normal increase in the number of people employed due to increases in population and increases in the labour force relative to the population. The unemployment rate is expressed as a percentage, and is calculated as follows:
As defined by the International Labour Organization, "unemployed workers" are those who are currently not working but are willing and able to work for pay, currently available to work, and have actively searched for work.
Individuals who are actively seeking job placement must make the effort to: be in contact with an employer, have job interviews, contact job placement agencies, send out resumes, submit applications, respond to advertisements, or some other means of active job searching within the prior four weeks. Simply looking at advertisements and not responding will not count as actively seeking job placement. Since not all unemployment may be "open" and counted by government agencies, official statistics on unemployment may not be accurate. In the United States, for example, the unemployment rate does not take into consideration those individuals who are not actively looking for employment, such as those still attending college.
The ILO describes 4 different methods to calculate the unemployment rate:
The primary measure of unemployment, U3, allows for comparisons between countries. Unemployment differs from country to country and across different time periods. For example, during the 1990s and 2000s, the United States had lower unemployment levels than many countries in the European Union, which had significant internal variation, with countries like the UK and Denmark outperforming Italy and France. However, large economic events such as the Great Depression can lead to similar unemployment rates across the globe.
European Union (Eurostat).
Eurostat, the statistical office of the European Union, defines unemployed as those persons age 15 to 74 who are not working, have looked for work in the last four weeks, and ready to start work within two weeks, which conform to ILO standards. Both the actual count and rate of unemployment are reported. Statistical data are available by member state, for the European Union as a whole (EU28) as well as for the euro area (EA19). Eurostat also includes a long-term unemployment rate. This is defined as part of the unemployed who have been unemployed for an excess of 1 year.
The main source used is the European Union Labour Force Survey (EU-LFS). The EU-LFS collects data on all member states each quarter. For monthly calculations, national surveys or national registers from employment offices are used in conjunction with quarterly EU-LFS data. The exact calculation for individual countries, resulting in harmonized monthly data, depend on the availability of the data.
United States Bureau of Labor statistics.
The Bureau of Labor Statistics measures employment and unemployment (of those over 15 years of age) using two different labor force surveys conducted by the United States Census Bureau (within the United States Department of Commerce) and/or the Bureau of Labor Statistics (within the United States Department of Labor) that gather employment statistics monthly. The Current Population Survey (CPS), or "Household Survey", conducts a survey based on a sample of 60,000 households. This Survey measures the unemployment rate based on the ILO definition.
The Current Employment Statistics survey (CES), or "Payroll Survey", conducts a survey based on a sample of 160,000 businesses and government agencies that represent 400,000 individual employers. This survey measures only civilian nonagricultural employment; thus, it does not calculate an unemployment rate, and it differs from the ILO unemployment rate definition. These two sources have different classification criteria, and usually produce differing results. Additional data are also available from the government, such as the unemployment insurance weekly claims report available from the Office of Workforce Security, within the U.S. Department of Labor Employment & Training Administration. The Bureau of Labor Statistics provides up-to-date numbers via a PDF linked here. The BLS also provides a readable concise current Employment Situation Summary, updated monthly.
The Bureau of Labor Statistics also calculates six alternate measures of unemployment, U1 through U6, that measure different aspects of unemployment:
"Note: "Marginally attached workers" are added to the total labour force for unemployment rate calculation for U4, U5, and U6." The BLS revised the CPS in 1994 and among the changes the measure representing the official unemployment rate was renamed U3 instead of U5. In 2013, Representative Hunter proposed that the Bureau of Labor Statistics use the U5 rate instead of the current U3 rate.</ref>
Statistics for the U.S. economy as a whole hide variations among groups. For example, in January 2008 U.S. unemployment rates were 4.4% for adult men, 4.2% for adult women, 4.4% for Caucasians, 6.3% for Hispanics or Latinos (all races), 9.2% for African Americans, 3.2% for Asian Americans, and 18.0% for teenagers. Also, the U.S. unemployment rate would be at least 2% higher if prisoners and jail inmates were counted.
The unemployment rate is included in a number of major economic indexes including the United States' Conference Board's Index of Leading Indicators a macroeconomic measure of the state of the economy.
Alternatives.
Limitations of the unemployment definition.
Some critics believe that current methods of measuring unemployment are inaccurate in terms of the impact of unemployment on people as these methods do not take into account the 1.5% of the available working population incarcerated in U.S. prisons (who may or may not be working while incarcerated), those who have lost their jobs and have become discouraged over time from actively looking for work, those who are self-employed or wish to become self-employed, such as tradesmen or building contractors or IT consultants, those who have retired before the official retirement age but would still like to work (involuntary early retirees), those on disability pensions who, while not possessing full health, still wish to work in occupations suitable for their medical conditions, those who work for payment for as little as one hour per week but would like to work full-time.
These people are "involuntary part-time" workers, those who are underemployed, e.g., a computer programmer who is working in a retail store until he can find a permanent job, involuntary stay-at-home mothers who would prefer to work, and graduate and Professional school students who were unable to find worthwhile jobs after they graduated with their bachelor's degrees.
Internationally, some nations' unemployment rates are sometimes muted or appear less severe due to the number of self-employed individuals working in agriculture. Small independent farmers are often considered self-employed; so, they cannot be unemployed. The impact of this is that in non-industrialized economies, such as the United States and Europe during the early 19th century, overall unemployment was approximately 3% because so many individuals were self-employed, independent farmers; yet, unemployment outside of agriculture was as high as 80%.
Many economies industrialize and experience increasing numbers of non-agricultural workers. For example, the United States' non-agricultural labour force increased from 20% in 1800, to 50% in 1850, to 97% in 2000. The shift away from self-employment increases the percentage of the population who are included in unemployment rates. When comparing unemployment rates between countries or time periods, it is best to consider differences in their levels of industrialization and self-employment.
Additionally, the measures of employment and unemployment may be "too high". In some countries, the availability of unemployment benefits can inflate statistics since they give an incentive to register as unemployed. People who do not really seek work may choose to declare themselves unemployed so as to get benefits; people with undeclared paid occupations may try to get unemployment benefits in addition to the money they earn from their work.
However, in countries such as the United States, Canada, Mexico, Australia, Japan and the European Union, unemployment is measured using a sample survey (akin to a Gallup poll). According to the BLS, a number of Eastern European nations have instituted labour force surveys as well. The sample survey has its own problems because the total number of workers in the economy is calculated based on a sample rather than a census.
It is possible to be neither employed nor unemployed by ILO definitions, i.e., to be outside of the "labour force." These are people who have no job and are not looking for one. Many of these are going to school or are retired. Family responsibilities keep others out of the labour force. Still others have a physical or mental disability which prevents them from participating in labour force activities. And of course some people simply elect not to work, preferring to be dependent on others for sustenance.
Typically, employment and the labour force include only work done for monetary gain. Hence, a homemaker is neither part of the labour force nor unemployed. Nor are full-time students nor prisoners considered to be part of the labour force or unemployment. The latter can be important. In 1999, economists Lawrence F. Katz and Alan B. Krueger estimated that increased incarceration lowered measured unemployment in the United States by 0.17% between 1985 and the late 1990s.
In particular, as of 2005, roughly 0.7% of the U.S. population is incarcerated (1.5% of the available working population). Additionally, children, the elderly, and some individuals with disabilities are typically not counted as part of the labour force in and are correspondingly not included in the unemployment statistics. However, some elderly and many disabled individuals are active in the labour market
In the early stages of an economic boom, unemployment often rises. This is because people join the labour market (give up studying, start a job hunt, etc.) because of the improving job market, but until they have actually found a position they are counted as unemployed. Similarly, during a recession, the increase in the unemployment rate is moderated by people leaving the labour force or being otherwise discounted from the labour force, such as with the self-employed.
For the fourth quarter of 2004, according to OECD, (source Employment Outlook 2005 ISBN 92-64-01045-9), normalized unemployment for men aged 25 to 54 was 4.6% in the U.S. and 7.4% in France. At the same time and for the same population the employment rate (number of workers divided by population) was 86.3% in the U.S. and 86.7% in France. This example shows that the unemployment rate is 60% higher in France than in the U.S., yet more people in this demographic are working in France than in the U.S., which is counterintuitive if it is expected that the unemployment rate reflects the health of the labour market.
Due to these deficiencies, many labour market economists prefer to look at a range of economic statistics such as labour market participation rate, the percentage of people aged between 15 and 64 who are currently employed or searching for employment, the total number of full-time jobs in an economy, the number of people seeking work as a raw number and not a percentage, and the total number of person-hours worked in a month compared to the total number of person-hours people would like to work. In particular the NBER does not use the unemployment rate but prefer various employment rates to date recessions.
Labor force participation rate.
The labor force participation rate is the ratio between the labor force and the overall size of their cohort (national population of the same age range). In the West during the later half of the 20th century, the labor force participation rate increased significantly, due to an increase in the number of women who entered the workplace.
In the United States, there have been four significant stages of women's participation in the labor force - increases in the 20th century and decreases in the 21st century. Male labor force participation decreased from 1953 until 2013. Since October 2013 men have been increasingly joining the labor force.
During the late 19th century through the 1920s, very few women worked outside the home. They were young single women who typically withdrew from the labor force at marriage unless family needed two incomes. These women worked primarily in the textile manufacturing industry or as domestic workers. This profession empowered women and allowed them to earn a living wage. At times, they were a financial help to their families.
Between 1930 and 1950, female labor force participation increased primarily due to the increased demand for office workers, women's participation in the high school movement, and due to electrification which reduced the time spent on household chores. Between the 1950s to the early 1970s, most women were secondary earners working mainly as secretaries, teachers, nurses, and librarians (pink-collar jobs).
Between the mid 1970s to the late 1990s there was a period of revolution of women in the labor force brought on by a source of different factors. Women more accurately planned for their future in the work force, investing in more applicable majors in college that prepared them to enter and compete in the labor market. In the United States, the female labor force participation rate rose from approximately 33% in 1948 to a peak of 60.3% in 2000. As of April 2015 the female labor force participation is at 56.6%, the male labor force participation rate is at 69.4% and the total is 62.8%.
A common theory in modern economics claims that the rise of women participating in the U.S. labor force in the 1950s through to the 1990s was due to the introduction of a new contraceptive technology, birth control pills, and the adjustment of age of majority laws. The use of birth control gave women the flexibility of opting to invest and advance their career while maintaining a relationship. By having control over the timing of their fertility, they were not running a risk of thwarting their career choices. However, only 40% of the population actually used the birth control pill.
This implies that other factors may have contributed to women choosing to invest in advancing their careers. One factor may be that more and more men delayed the age of marriage, allowing women to marry later in life without worrying about the quality of older men. Other factors include the changing nature of work, with machines replacing physical labor, eliminating many traditional male occupations, and the rise of the service sector, where many jobs are gender neutral.
Another factor that may have contributed to the trend was The Equal Pay Act of 1963, which aimed at abolishing wage disparity based on sex. Such legislation diminished sexual discrimination and encouraged more women to enter the labor market by receiving fair remuneration to help raising families and children.
At the turn of the 21st century the labor force participation began to reverse its long period of increase. Reasons for this change include a rising share of older workers, an increase in school enrollment rates among young workers and a decrease in female labor force participation.
The labor force participation rate can decrease when the rate of growth of the population outweighs that of the employed and unemployed together. The labor force participation rate is a key component in long-term economic growth, almost as important as productivity.
A historic shift began around the end of the great recession as women began leaving the labor force in the United States and other developed countries. The female labor force participation rate in the United States has steadily decreased since 2009 and as of April 2015 the female labor force participation rate has gone back down to 1988 levels of 56.6%.
Participation rates are defined as follows:
The labor force participation rate explains how an increase in the unemployment rate can occur simultaneously with an increase in employment. If a large amount of new workers enter the labor force but only a small fraction become employed, then the increase in the number of unemployed workers can outpace the growth in employment.
Unemployment ratio.
The unemployment ratio calculates the share of unemployed for the whole population. Particularly many young people between 15 and 24 are studying full-time and are therefore neither working nor looking for a job. This means they are not part of the labour force which is used as the denominator for calculating the unemployment rate. The youth unemployment ratios in the European Union range from 5.2 (Austria) to 20.6 percent (Spain). These are considerably lower than the standard youth unemployment rates, ranging from 7.9 (Germany) to 57.9 percent (Greece).
Effects.
High and persistent unemployment, in which economic inequality increases, has a negative effect on subsequent long-run economic growth. Unemployment can harm growth not only because it is a waste of resources, but also because it generates redistributive pressures and subsequent distortions, drives people to poverty, constrains liquidity limiting labor mobility, and erodes self-esteem promoting social dislocation, unrest and conflict. 2013 Economics Nobel prize winner Robert J. Shiller said that rising inequality in the United States and elsewhere is the most important problem.
Costs.
Individual.
Unemployed individuals are unable to earn money to meet financial obligations. Failure to pay mortgage payments or to pay rent may lead to homelessness through foreclosure or eviction. Across the United States the growing ranks of people made homeless in the foreclosure crisis are generating tent cities.
Unemployment increases susceptibility to cardiovascular disease, somatization, anxiety, depression, and suicide. In addition, unemployed people have higher rates of medication use, poor diet, physician visits, tobacco smoking, alcoholic beverage consumption, drug use, and lower rates of exercise. According to a study published in Social Indicator Research, even those who tend to be optimistic find it difficult to look on the bright side of things when unemployed. Using interviews and data from German participants aged 16 to 94 – including individuals coping with the stresses of real life and not just a volunteering student population – the researchers determined that even optimists struggled with being unemployed.
In 1979, Brenner found that for every 10% increase in the number of unemployed there is an increase of 1.2% in total mortality, a 1.7% increase in cardiovascular disease, 1.3% more cirrhosis cases, 1.7% more suicides, 4.0% more arrests, and 0.8% more assaults reported to the police.
A study by Ruhm, in 2000, on the effect of recessions on health found that several measures of health actually improve during recessions. As for the impact of an economic downturn on crime, during the Great Depression the crime rate did not decrease. The unemployed in the U.S. often use welfare programs such as Food Stamps or accumulating debt because unemployment insurance in the U.S. generally does not replace a majority of the income one received on the job (and one cannot receive such aid indefinitely).
Not everyone suffers equally from unemployment. In a prospective study of 9570 individuals over four years, highly conscientious people suffered more than twice as much if they became unemployed. The authors suggested this may be due to conscientious people making different attributions about why they became unemployed, or through experiencing stronger reactions following failure. There is also possibility of reverse causality from poor health to unemployment. 
Some hold that many of the low-income jobs are not really a better option than unemployment with a welfare state (with its unemployment insurance benefits). But since it is difficult or impossible to get unemployment insurance benefits without having worked in the past, these jobs and unemployment are more complementary than they are substitutes. (These jobs are often held short-term, either by students or by those trying to gain experience; turnover in most low-paying jobs is high.)
Another cost for the unemployed is that the combination of unemployment, lack of financial resources, and social responsibilities may push unemployed workers to take jobs that do not fit their skills or allow them to use their talents. Unemployment can cause underemployment, and fear of job loss can spur psychological anxiety. As well as anxiety, it can cause depression, lack of confidence, and huge amounts of stress. This stress is increased when the unemployed are faced with health issues, poverty, and lack of relational support. 
Another personal cost of unemployment is its impact on relationships. A 2008 study from Covizzi, which examines the relationship between unemployment and divorce, found that the rate of divorce is greater for couples when one partner is unemployed. However, a more recent study has found that people often stick together in “unhappy” or “unhealthy” marriages when unemployed to buffer financial costs. A 2014 study by Van der Meer found that the stigma that comes from being unemployed affects personal well-being, especially for men, who often feel as though their masculine identities are threatened by unemployment.
Unemployment can also bring personal costs in relation to gender. One study found that women are more likely to experience unemployment than men and that they are less likely to move from temporary positions to permanent positions. Another study on gender and unemployment found that men, however, are more likely to experience greater stress, depression, and adverse effects from unemployment, largely stemming from the perceived threat to their role as breadwinner. This study found that men actually expect themselves to be viewed as “less manly” after a job loss than they actually are, and as a result they engage in compensating behaviors, such as financial risk-taking and increased assertiveness, because of it.
Costs of unemployment also vary depending on age. The young and the old are the two largest age groups currently experiencing unemployment. A 2007 study from Jacob and Kleinert found that young people (ages 18 to 24) who have fewer resources and limited work experiences are more likely to be unemployed. Other researchers have found that today’s high school seniors place a lower value on work than those in the past, and this is likely because they recognize the limited availability of jobs. At the other end of the age spectrum, studies have found that older individuals have more barriers than younger workers to employment, require stronger social networks to acquire work, and are also less likely to move from temporary to permanent positions. Additionally, some older people see age discrimination as the reason they are not getting hired.
Social.
An economy with high unemployment is not using all of the resources, specifically labour, available to it. Since it is operating below its production possibility frontier, it could have higher output if all the workforce were usefully employed. However, there is a trade-off between economic efficiency and unemployment: if the frictionally unemployed accepted the first job they were offered, they would be likely to be operating at below their skill level, reducing the economy's efficiency.
During a long period of unemployment, workers can lose their skills, causing a loss of human capital. Being unemployed can also reduce the life expectancy of workers by about seven years.
High unemployment can encourage xenophobia and protectionism as workers fear that foreigners are stealing their jobs. Efforts to preserve existing jobs of domestic and native workers include legal barriers against "outsiders" who want jobs, obstacles to immigration, and/or tariffs and similar trade barriers against foreign competitors.
High unemployment can also cause social problems such as crime; if people have less disposable income than before, it is very likely that crime levels within the economy will increase.
A 2015 study published in "The Lancet" estimates that unemployment causes 45,000 suicides a year globally.
Socio-political.
High levels of unemployment can be causes of civil unrest, in some cases leading to revolution, and particularly totalitarianism. The fall of the Weimar Republic in 1933 and Adolf Hitler's rise to power, which culminated in World War II and the deaths of tens of millions and the destruction of much of the physical capital of Europe, is attributed to the poor economic conditions in Germany at the time, notably a high unemployment rate of above 20%; see Great Depression in Central Europe for details.
Note that the hyperinflation in the Weimar Republic is not directly blamed for the Nazi rise – the Inflation in the Weimar Republic occurred primarily in the period 1921–23, which was contemporary with Hitler's Beer Hall Putsch of 1923, and is blamed for damaging the credibility of democratic institutions, but the Nazis did not assume government until 1933, ten years after the hyperinflation but in the midst of high unemployment.
Rising unemployment has traditionally been regarded by the public and media in any country as a key guarantor of electoral defeat for any government which oversees it. This was very much the consensus in the United Kingdom until 1983, when Margaret Thatcher's Conservative government won a landslide in the general election, despite overseeing a rise in unemployment from 1,500,000 to 3,200,000 since its election four years earlier.
Benefits.
The primary benefit of unemployment is that people are available for hire, without being headhunted away from their existing employers. This permits new and old businesses to take on staff.
Unemployment is argued to be "beneficial" to the people who are not unemployed in the sense that it averts inflation, which itself has damaging effects, by providing (in Marxian terms) a reserve army of labour, that keeps wages in check. However, the direct connection between full local employment and local inflation has been disputed by some due to the recent increase in international trade that supplies low-priced goods even while local employment rates rise to full employment.
Before current levels of world trade were developed, unemployment was demonstrated to reduce inflation, following the Phillips curve, or to decelerate inflation, following the NAIRU/natural rate of unemployment theory, since it is relatively easy to seek a new job without losing one's current one. And when more jobs are available for fewer workers (lower unemployment), it may allow workers to find the jobs that better fit their tastes, talents, and needs.
As in the Marxian theory of unemployment, special interests may also benefit: some employers may expect that employees with no fear of losing their jobs will not work as hard, or will demand increased wages and benefit. According to this theory, unemployment may promote general labour productivity and profitability by increasing employers' rationale for their monopsony-like power (and profits).
Optimal unemployment has also been defended as an environmental tool to brake the constantly accelerated growth of the GDP to maintain levels sustainable in the context of resource constraints and environmental impacts. However the tool of denying jobs to willing workers seems a blunt instrument for conserving resources and the environment – it reduces the consumption of the unemployed across the board, and only in the short term. Full employment of the unemployed workforce, all focused toward the goal of developing more environmentally efficient methods for production and consumption might provide a more significant and lasting cumulative environmental benefit and reduced resource consumption. If so the future economy and workforce would benefit from the resultant structural increases in the sustainable level of GDP growth.
Some critics of the "culture of work" such as anarchist Bob Black see employment as overemphasized culturally in modern countries. Such critics often propose quitting jobs when possible, working less, reassessing the cost of living to this end, creation of jobs which are "fun" as opposed to "work," and creating cultural norms where work is seen as unhealthy. These people advocate an "anti-work" ethic for life.
Decline in work hours.
As a result of productivity the work week declined considerably over the 19th century. By the 1920s in the U.S. the average work week was 49 hours, but the work week was reduced to 40 hours (after which overtime premium was applied) as part of the National Industrial Recovery Act of 1933. At the time of the Great Depression of the 1930s it was understood that with the enormous productivity gains due to electrification, mass production and agricultural mechanization, there was no need for a large number of previously employed workers.
Controlling or reducing unemployment.
Societies try a number of different measures to get as many people as possible into work, and various societies have experienced close to full employment for extended periods, particularly during the Post-World War II economic expansion. The United Kingdom in the 1950s and 60s averaged 1.6% unemployment, while in Australia the 1945 "White Paper on Full Employment in Australia" established a government policy of full employment, which policy lasted until the 1970s when the government ran out of money.
However, mainstream economic discussions of full employment since the 1970s suggest that attempts to reduce the level of unemployment below the natural rate of unemployment will fail, resulting only in less output and more inflation.
Demand-side solutions.
Increases in the demand for labour will move the economy along the demand curve, increasing wages and employment. The demand for labour in an economy is derived from the demand for goods and services. As such, if the demand for goods and services in the economy increases, the demand for labour will increase, increasing employment and wages.
There are many ways to stimulate demand for goods and services. Increasing wages to the working class (those more likely to spend the increased funds on goods and services, rather than various types of savings, or commodity purchases) is one theory proposed. Increased wages is believed to be more effective in boosting demand for goods and services than central banking strategies that put the increased money supply mostly into the hands of wealthy persons and institutions. Monetarists suggest that increasing money supply in general will increase short-term demand. Long-term the increased demand will be negated by inflation. A rise in fiscal expenditures is another strategy for boosting aggregate demand.
Providing aid to the unemployed is a strategy used to prevent cutbacks in consumption of goods and services which can lead to a vicious cycle of further job losses and further decreases in consumption/demand. Many countries aid the unemployed through social welfare programs. These unemployment benefits include unemployment insurance, unemployment compensation, welfare and subsidies to aid in retraining. The main goal of these programs is to alleviate short-term hardships and, more importantly, to allow workers more time to search for a job.
A direct demand-side solution to unemployment is government-funded employment of the able-bodied poor. This was notably implemented in Britain from the 17th century until 1948 in the institution of the workhouse, which provided jobs for the unemployed with harsh conditions and poor wages to dissuade their use. A modern alternative is a job guarantee, where the government guarantees work at a living wage.
Temporary measures can include public works programs such as the Works Progress Administration. Government-funded employment is not widely advocated as a solution to unemployment, except in times of crisis; this is attributed to the public sector jobs' existence depending directly on the tax receipts from private sector employment.
In the U.S., the unemployment insurance allowance one receives is based solely on previous income (not time worked, family size, etc.) and usually compensates for one-third of one's previous income. To qualify, one must reside in their respective state for at least a year and, of course, work. The system was established by the Social Security Act of 1935. Although 90% of citizens are covered by unemployment insurance, less than 40% apply for and receive benefits. However, the number applying for and receiving benefits increases during recessions. In cases of highly seasonal industries the system provides income to workers during the off seasons, thus encouraging them to stay attached to the industry.
According to classical economic theory, markets reach equilibrium where supply equals demand; everyone who wants to sell at the market price can. Those who do not want to sell at this price do not; in the labour market this is classical unemployment. Monetary policy and fiscal policy can both be used to increase short-term growth in the economy, increasing the demand for labour and decreasing unemployment.
Supply-side solutions.
However, the labor market is not 100% efficient: It does not clear, though it may be more efficient than bureaucracy. Some argue that minimum wages and union activity keep wages from falling, which means too many people want to sell their labour at the going price but cannot. This assumes perfect competition exists in the labour market, specifically that no single entity is large enough to affect wage levels and that employees are similar in ability.
Advocates of supply-side policies believe those policies can solve this by making the labour market more flexible. These include removing the minimum wage and reducing the power of unions. Supply-siders argue the reforms increase long-term growth by reducing labour costs. This increased supply of goods and services requires more workers, increasing employment. It is argued that supply-side policies, which include cutting taxes on businesses and reducing regulation, create jobs, reduce unemployment and decrease labour's share of national income. Other supply-side policies include education to make workers more attractive to employers.
History.
There are relatively limited historical records on unemployment because it has not always been acknowledged or measured systematically. Industrialization involves economies of scale that often prevent individuals from having the capital to create their own jobs to be self-employed. An individual who cannot either join an enterprise or create a job is unemployed. As individual farmers, ranchers, spinners, doctors and merchants are organized into large enterprises, those who cannot join or compete become unemployed.
Recognition of unemployment occurred slowly as economies across the world industrialized and bureaucratized. Before this, traditional self sufficient native societies have no concept of unemployment. The recognition of the concept of "unemployment" is best exemplified through the well documented historical records in England. For example, in 16th century England no distinction was made between vagrants and the jobless; both were simply categorized as "sturdy beggars", to be punished and moved on.
The closing of the monasteries in the 1530s increased poverty, as the church had helped the poor. In addition, there was a significant rise in enclosure during the Tudor period. Also the population was rising. Those unable to find work had a stark choice: starve or break the law. In 1535, a bill was drawn up calling for the creation of a system of public works to deal with the problem of unemployment, to be funded by a tax on income and capital. A law passed a year later allowed vagabonds to be whipped and hanged.
In 1547, a bill was passed that subjected vagrants to some of the more extreme provisions of the criminal law, namely two years servitude and branding with a "V" as the penalty for the first offense and death for the second. During the reign of Henry VIII, as many as 72,000 people are estimated to have been executed. In the 1576 Act each town was required to provide work for the unemployed.
The Elizabethan Poor Law of 1601, one of the world's first government-sponsored welfare programs, made a clear distinction between those who were unable to work and those able-bodied people who refused employment. Under the Poor Law systems of England and Wales, Scotland and Ireland a workhouse was a place where people who were unable to support themselves, could go to live and work.
Industrial Revolution to late 19th century.
A description of the miserable living standards of the mill workers in England in 1844 was given by Fredrick Engels in "The Condition of the Working-Class in England in 1844". In the preface to the 1892 edition Engels notes that the extreme poverty he wrote about in 1844 had largely disappeared. David Ames Wells also noted that living conditions in England had improved near the end of the 19th century and that unemployment was low.
The scarcity and high price of labor in the U.S. during the 19th century was well documented by contemporary accounts, as in the following:
"The laboring classes are comparatively few in number, but this is counterbalanced by, and indeed, may be one of the causes of the eagerness by which they call in the use of machinery in almost every department of industry. Wherever it can be applied as a substitute for manual labor, it is universally and willingly resorted to ...It is this condition of the labor market, and this eager resort to machinery wherever it can be applied, to which, under the guidance of superior education and intelligence, the remarkable prosperity of the United States is due." Joseph Whitworth, 1854
Scarcity of labor was a factor in the economics of slavery in the U.S.
As new territories were opened and Federal land sales conducted, land had to be cleared and new homesteads established. Hundreds of thousands of immigrants annually came to the U.S. and found jobs digging canals and building railroads. Almost all work during most of the 19th century was done by hand or with horses, mules, or oxen, because there was very little mechanization. The workweek during most of the 19th century was 60 hours. Unemployment at times was between one and two percent.
The tight labor market was a factor in productivity gains allowing workers to maintain or increase their nominal wages during the secular deflation that caused real wages to rise at various times in the 19th century, especially in the final decades.
20th century.
There were labor shortages during WW I. Ford Motor Co. doubled wages to reduce turnover. After 1925 unemployment began to gradually rise.
Great Depression.
The decade of the 1930s saw the Great Depression impact unemployment across the globe. One Soviet trading corporation in New York averaged 350 applications a day from Americans seeking jobs in the Soviet Union. In Germany the unemployment rate reached nearly 25% in 1932.
In some towns and cities in the north east of England, unemployment reached as high as 70%; the national unemployment level peaked at more than 22% in 1932. Unemployment in Canada reached 27% at the depth of the Depression in 1933. In 1929, the U.S. unemployment rate averaged 3%. In 1933, 25% of all American workers and 37% of all nonfarm workers were unemployed.
In the U.S., the WPA (1935–43) was the largest make-work program. It hired men (and some women) off the relief roles ("dole") typically for unskilled labor.
In Cleveland, Ohio, the unemployment rate was 60%; in Toledo, Ohio, 80%. There were two million homeless people migrating across the United States. Over 3 million unemployed young men were taken out of the cities and placed into 2600+ work camps managed by the CCC.
Unemployment in the United Kingdom fell later in the 1930s as the depression eased, and remained low (in six figures) after World War II.
Fredrick Mills found that in the U.S., 51% of the decline in work hours was due to the fall in production and 49% was from increased productivity.
By 1972 unemployment in the UK had crept back up above 1,000,000, and was even higher by the end of the decade, with inflation also being high. Although the monetarist economic policies of Margaret Thatcher's Conservative government saw inflation reduced after 1979, unemployment soared in the early 1980s, exceeding 3,000,000 – a level not seen for some 50 years – by 1982. This represented one in eight of the workforce, with unemployment exceeding 20% in some parts of the United Kingdom which had relied on the now-declining industries such as coal mining.
However, this was a time of high unemployment in all major industrialised nations. By the spring of 1983, unemployment in the United Kingdom had risen by 6% in the previous 12 months; compared to 10% in Japan, 23% in the United States of America and 34% in West Germany (seven years before reunification).
Unemployment in the United Kingdom remained above 3,000,000 until the spring of 1987, by which time the economy was enjoying a boom. By the end of 1989, unemployment had fallen to 1,600,000. However, inflation had reached 7.8% and the following year it reached a nine-year high of 9.5%; leading to increased interest rates.
Another recession began during 1990 and lasted until 1992. Unemployment began to increase and by the end of 1992 nearly 3,000,000 in the United Kingdom were unemployed. Then came a strong economic recovery. With inflation down to 1.6% by 1993, unemployment then began to fall rapidly, standing at 1,800,000 by early 1997.
21st century.
The official unemployment rate in the 16 EU countries that use the euro rose to 10% in December 2009 as a result of another recession. Latvia had the highest unemployment rate in EU at 22.3% for November 2009. Europe's young workers have been especially hard hit. In November 2009, the unemployment rate in the EU27 for those aged 15–24 was 18.3%. For those under 25, the unemployment rate in Spain was 43.8%. Unemployment has risen in two-thirds of European countries since 2010.
Into the 21st century, unemployment in the United Kingdom remained low and the economy remaining strong, while at this time several other European economies – namely, France and Germany (reunified a decade earlier) – experienced a minor recession and a substantial rise in unemployment.
In 2008, when the recession brought on another increase in the United Kingdom, after 15 years of economic growth and no major rises in unemployment. Early in 2009, unemployment passed the 2,000,000 mark, by which time economists were predicting it would soon reach 3,000,000. However, the end of the recession was declared in January 2010 and unemployment peaked at nearly 2,700,000 in 2011, appearing to ease fears of unemployment reaching 3,000,000. The unemployment rate of Britain's young black people was 47.4% in 2011. 2013/2014 has seen the employment rate increase from 1,935,836 to 2,173,012 as supported by showing the UK is creating more job opportunities and forecasts the rate of increase in 2014/2015 will be another 7.2%
An 26 April 2005 "Asia Times" article notes that, "In regional giant South Africa, some 300,000 textile workers have lost their jobs in the past two years due to the influx of Chinese goods". The increasing U.S. trade deficit with China cost 2.4 million American jobs between 2001 and 2008, according to a study by the Economic Policy Institute (EPI). From 2000 to 2007, the United States lost a total of 3.2 million manufacturing jobs. 12.1% of US military veterans who had served after the September 11 attacks in 2001 were unemployed as of 2011; 29.1% of male veterans aged 18–24 were unemployed.
About 25 million people in the world's 30 richest countries will have lost their jobs between the end of 2007 and the end of 2010 as the economic downturn pushes most countries into recession. In April 2010, the U.S. unemployment rate was 9.9%, but the government's broader U-6 unemployment rate was 17.1%. In April 2012, the unemployment rate was 4.6% in Japan. In a 2012 news story, the "Financial Post" reported, "Nearly 75 million youth are unemployed around the world, an increase of more than 4 million since 2007. In the European Union, where a debt crisis followed the financial crisis, the youth unemployment rate rose to 18% last year from 12.5% in 2007, the ILO report shows."

</doc>
<doc id="31742" url="https://en.wikipedia.org/wiki?curid=31742" title="Unicode">
Unicode

Unicode is a computing industry standard for the consistent encoding, representation, and handling of text expressed in most of the world's writing systems. Developed in conjunction with the Universal Coded Character Set (UCS) standard and published as "The Unicode Standard", the latest version of Unicode contains a repertoire of more than 120,000 characters covering 129 modern and historic scripts, as well as multiple symbol sets. The standard consists of a set of code charts for visual reference, an encoding method and set of standard character encodings, a set of reference data files, and a number of related items, such as character properties, rules for normalization, decomposition, collation, rendering, and bidirectional display order (for the correct display of text containing both right-to-left scripts, such as Arabic and Hebrew, and left-to-right scripts). , the most recent version is "Unicode 8.0". The standard is maintained by the Unicode Consortium.
Unicode's success at unifying character sets has led to its widespread and predominant use in the internationalization and localization of computer software. The standard has been implemented in many recent technologies, including modern operating systems, XML, the Java programming language, and the Microsoft .NET Framework.
Unicode can be implemented by different character encodings. The most commonly used encodings are UTF-8, UTF-16 and the now-obsolete UCS-2. UTF-8 uses one byte for any ASCII character, all of which have the same code values in both UTF-8 and ASCII encoding, and up to four bytes for other characters. UCS-2 uses a 16-bit code unit (two 8-bit bytes) for each character but cannot encode every character in the current Unicode standard. UTF-16 extends UCS-2, using one 16-bit unit for the characters that were representable in UCS-2 and two 16-bit units (4 × 8 bits) to handle each of the additional characters.
Origin and development.
Unicode has the explicit aim of transcending the limitations of traditional character encodings, such as those defined by the ISO 8859 standard, which find wide usage in various countries of the world but remain largely incompatible with each other. Many traditional character encodings share a common problem in that they allow bilingual computer processing (usually using Latin characters and the local script), but not multilingual computer processing (computer processing of arbitrary scripts mixed with each other).
Unicode, in intent, encodes the underlying characters—graphemes and grapheme-like units—rather than the variant glyphs (renderings) for such characters. In the case of Chinese characters, this sometimes leads to controversies over distinguishing the underlying character from its variant glyphs (see Han unification).
In text processing, Unicode takes the role of providing a unique "code point"—a number, not a glyph—for each character. In other words, Unicode represents a character in an abstract way and leaves the visual rendering (size, shape, font, or style) to other software, such as a web browser or word processor. This simple aim becomes complicated, however, because of concessions made by Unicode's designers in the hope of encouraging a more rapid adoption of Unicode.
The first 256 code points were made identical to the content of ISO-8859-1 so as to make it trivial to convert existing western text. Many essentially identical characters were encoded multiple times at different code points to preserve distinctions used by legacy encodings and therefore, allow conversion from those encodings to Unicode (and back) without losing any information. For example, the "fullwidth forms" section of code points encompasses a full Latin alphabet that is separate from the main Latin alphabet section because in Chinese, Japanese, and Korean (CJK) fonts, these Latin characters are rendered at the same width as CJK ideographs, rather than at half the width. For other examples, see Duplicate characters in Unicode.
History.
The origins of Unicode date to 1987, when Joe Becker from Xerox and Lee Collins and Mark Davis from Apple started investigating the practicalities of creating a universal character set. In August 1988, Joe Becker published a draft proposal for an "international/multilingual text character encoding system, tentatively called Unicode". He explained that "he name 'Unicode' is intended to suggest a unique, unified, universal encoding".
In this document, entitled "Unicode 88", Becker outlined a 16-bit character model:
Unicode is intended to address the need for a workable, reliable world text encoding. Unicode could be roughly described as "wide-body ASCII" that has been stretched to 16 bits to encompass the characters of all the world's living languages. In a properly engineered design, 16 bits per character are more than sufficient for this purpose.
His original 16-bit design was based on the assumption that only those scripts and characters in modern use would need to be encoded:
Unicode gives higher priority to ensuring utility for the future than to preserving past antiquities. Unicode aims in the first instance at the characters published in modern text (e.g. in the union of all newspapers and magazines printed in the world in 1988), whose number is undoubtedly far below 214 = 16,384. Beyond those modern-use characters, all others may be defined to be obsolete or rare; these are better candidates for private-use registration than for congesting the public list of generally useful Unicodes.
In early 1989, the Unicode working group expanded to include Ken Whistler and Mike Kernaghan of Metaphor, Karen Smith-Yoshimura and Joan Aliprand of RLG, and Glenn Wright of Sun Microsystems, and in 1990 Michel Suignard and Asmus Freytag from Microsoft and Rick McGowan of NeXT joined the group. By the end of 1990, most of the work on mapping existing character encoding standards had been completed, and a final review draft of Unicode was ready.
The Unicode Consortium was incorporated on January 3, 1991, in California, and in October 1991, the first volume of the Unicode standard was published. The second volume, covering Han ideographs, was published in June 1992.
In 1996, a surrogate character mechanism was implemented in Unicode 2.0, so that Unicode was no longer restricted to 16 bits. This increased the Unicode codespace to over a million code points, which allowed for the encoding of many historic scripts (e.g., Egyptian Hieroglyphs) and thousands of rarely used or obsolete characters that had not been anticipated as needing encoding. Among the characters not originally intended for Unicode are rarely used Kanji or Chinese characters, many of which are part of personal and place names, making them rarely used, but much more essential than envisioned in the original architecture of Unicode.
Architecture and terminology.
Unicode defines a codespace of 1,114,112 code points in the range 0hex to 10FFFFhex. Normally a Unicode code point is referred to by writing "U+" followed by its hexadecimal number. For code points in the Basic Multilingual Plane (BMP), four digits are used (e.g., U+0058 for the character LATIN CAPITAL LETTER X); for code points outside the BMP, five or six digits are used, as required (e.g., U+E0001 for the character LANGUAGE TAG and U+10FFFD for the character PRIVATE USE CHARACTER-10FFFD).
Code point planes and blocks.
The Unicode codespace is divided into seventeen "planes", numbered 0 to 16:
All code points in the BMP are accessed as a single code unit in UTF-16 encoding and can be encoded in one, two or three bytes in UTF-8. Code points in Planes 1 through 16 ("supplementary planes") are accessed as surrogate pairs in UTF-16 and encoded in four bytes in UTF-8.
Within each plane, characters are allocated within named "blocks" of related characters. Although blocks are an arbitrary size, they are always a multiple of 16 code points and often a multiple of 128 code points. Characters required for a given script may be spread out over several different blocks.
Character General Category.
Each code point has a single General Category property. The major categories are: Letter, Mark, Number, Punctuation, Symbol, Separator and Other. Within these categories, there are subdivisions. The General Category is not useful for every use, since legacy encodings have used multiple characteristics per single code point. E.g., in ASCII is both a control and a formatting separator; in Unicode the General Category is "Other, Control". Often, other properties must be used to specify the characteristics and behaviour of a code point. The possible General Categories are:
Code points in the range U+D800–U+DBFF (1,024 code points) are known as high-surrogate code points, and code points in the range U+DC00–U+DFFF (1,024 code points) are known as low-surrogate code points. A high-surrogate code point (also known as a leading surrogate) followed by a low-surrogate code point (also known as a trailing surrogate) together form a surrogate pair used in UTF-16 to represent 1,048,576 code points outside BMP. High and low surrogate code points are not valid by themselves. Thus the range of code points that are available for use as characters is U+0000–U+D7FF and U+E000–U+10FFFF (1,112,064 code points). The value of these code points (i.e., excluding surrogates) is sometimes referred to as the character's scalar value.
Certain noncharacter code points are guaranteed never to be used for encoding characters, although applications may make use of these code points internally if they wish. There are sixty-six noncharacters: U+FDD0–U+FDEF and any code point ending in the value FFFE or FFFF (i.e., U+FFFE, U+FFFF, U+1FFFE, U+1FFFF, … U+10FFFE, U+10FFFF). The set of noncharacters is stable, and no new noncharacters will ever be defined.
Reserved code points are those code points which are available for use as encoded characters, but are not yet defined as characters by Unicode.
Private-use code points are considered to be assigned characters, but they have no interpretation specified by the Unicode standard so any interchange of such characters requires an agreement between sender and receiver on their interpretation. There are three private-use areas in the Unicode codespace:
Graphic characters are characters defined by Unicode to have a particular semantic, and either have a visible glyph shape or represent a visible space. As of Unicode 8.0 there are 120,520 graphic characters.
Format characters are characters that do not have a visible appearance, but may have an effect on the appearance or behavior of neighboring characters. For example, and may be used to change the default shaping behavior of adjacent characters (e.g., to inhibit ligatures or request ligature formation). There are 152 format characters in Unicode 8.0.
Sixty-five code points (U+0000–U+001F and U+007F–U+009F) are reserved as control codes, and correspond to the C0 and C1 control codes defined in ISO/IEC 6429. Of these U+0009 (Tab), U+000A (Line Feed), and U+000D (Carriage Return) are widely used in Unicode-encoded texts.
Graphic characters, format characters, control code characters, and private use characters are known collectively as "assigned characters".
Abstract characters.
The set of graphic and format characters defined by Unicode does not correspond directly to the repertoire of "abstract characters" that is representable under Unicode. Unicode encodes characters by associating an abstract character with a particular code point. However, not all abstract characters are encoded as a single Unicode character, and some abstract characters may be represented in Unicode by a sequence of two or more characters. For example, a Latin small letter "i" with an ogonek, a dot above, and an acute accent, which is required in Lithuanian, is represented by the character sequence U+012F, U+0307, U+0301. Unicode maintains a list of uniquely named character sequences for abstract characters that are not directly encoded in Unicode.
All graphic, format, and private use characters have a unique and immutable name by which they may be identified. This immutability has been guaranteed since Unicode version 2.0 by the Name Stability policy. In cases where the name is seriously defective and misleading, or has a serious typographical error, a formal alias may be defined, and applications are encouraged to use the formal alias in place of the official character name. For example, has the formal alias , and has the formal alias .
Unicode Consortium.
The Unicode Consortium is a nonprofit organization that coordinates Unicode's development. Full members include most of the main computer software and hardware companies with any interest in text-processing standards, including Adobe Systems, Apple, Google, IBM, Microsoft, Oracle Corporation, and Yahoo!.
The Consortium has the ambitious goal of eventually replacing existing character encoding schemes with Unicode and its standard Unicode Transformation Format (UTF) schemes, as many of the existing schemes are limited in size and scope and are incompatible with multilingual environments.
Versions.
Unicode is developed in conjunction with the International Organization for Standardization and shares the character repertoire with ISO/IEC 10646: the Universal Character Set. Unicode and ISO/IEC 10646 function equivalently as character encodings, but "The Unicode Standard" contains much more information for implementers, covering—in depth—topics such as bitwise encoding, collation and rendering. The Unicode Standard enumerates a multitude of character properties, including those needed for supporting bidirectional text. The two standards do use slightly different terminology.
The Consortium first published "The Unicode Standard" (ISBN 0-321-18578-1) in 1991 and continues to develop standards based on that original work. The latest version of the standard, Unicode 8.0, was released in June 2015 and is available from the consortium's website. The last of the major versions (versions x.0) to be published in book form was Unicode 5.0 (ISBN 0-321-48091-0), but since Unicode 6.0 the full text of the standard is no longer being published in book form. In 2012, however, it was announced that only the core specification for Unicode version 6.1 would be made available as a 692-page print-on-demand paperback. Unlike the previous major version printings of the Standard, the print-on-demand core specification does not include any code charts or standard annexes, but the entire standard, including the core specification, will still remain freely available on the Unicode website.
Thus far the following major and minor versions of the Unicode standard have been published. Update versions, which do not include any changes to character repertoire, are signified by the third number (e.g., "version 4.0.1") and are omitted in the table below.
Scripts covered.
Unicode covers almost all scripts (writing systems) in current use today.
A total of 129 scripts are included in the latest version of Unicode (covering alphabets, abugidas and syllabaries), although there are still scripts that are not yet encoded, particularly those mainly used in historical, liturgical, and academic contexts. Further additions of characters to the already encoded scripts, as well as symbols, in particular for mathematics and music (in the form of notes and rhythmic symbols), also occur.
The Unicode Roadmap Committee (Michael Everson, Rick McGowan, and Ken Whistler) maintain the list of scripts that are candidates or potential candidates for encoding and their tentative code block assignments on the Unicode Roadmap page of the Unicode Consortium Web site. For some scripts on the Roadmap, such as Jurchen, Nü Shu, and Tangut, encoding proposals have been made and they are working their way through the approval process. For others scripts, such as Mayan and Rongorongo, no proposal has yet been made, and they await agreement on character repertoire and other details from the user communities involved.
Some modern invented scripts which have not yet been included in Unicode (e.g., Tengwar) or which do not qualify for inclusion in Unicode due to lack of real-world use (e.g., Klingon) are listed in the ConScript Unicode Registry, along with unofficial but widely used Private Use Area code assignments.
There is also a Medieval Unicode Font Initiative focused on special Latin medieval characters. Part of these proposals have been already included into Unicode.
The Script Encoding Initiative, a project run by Deborah Anderson at the University of California, Berkeley was founded in 2002 with the goal of funding proposals for scripts not yet encoded in the standard. The project has become a major source of proposed additions to the standard in recent years.
Mapping and encodings.
Several mechanisms have been specified for implementing Unicode. The choice depends on available storage space, source code compatibility, and interoperability with other systems.
Unicode Transformation Format and Universal Coded Character Set.
Unicode defines two mapping methods: the "Unicode Transformation Format" (UTF) encodings, and the "Universal Coded Character Set" (UCS) encodings. An encoding maps (possibly a subset of) the range of Unicode "code points" to sequences of values in some fixed-size range, termed "code values". The numbers in the names of the encodings indicate the number of bits per code value (for UTF encodings) or the number of bytes per code value (for UCS encodings). UTF-8 and UTF-16 are probably the most commonly used encodings. UCS-2 is an obsolete subset of UTF-16; UCS-4 and UTF-32 are functionally equivalent.
UTF encodings include:
UTF-8 uses one to four bytes per code point and, being compact for Latin scripts and ASCII-compatible, provides the "de facto" standard encoding for interchange of Unicode text. It is used by FreeBSD and most recent Linux distributions as a direct replacement for legacy encodings in general text handling.
The UCS-2 and UTF-16 encodings specify the Unicode Byte Order Mark (BOM) for use at the beginnings of text files, which may be used for byte ordering detection (or byte endianness detection). The BOM, code point U+FEFF has the important property of unambiguity on byte reorder, regardless of the Unicode encoding used; U+FFFE (the result of byte-swapping U+FEFF) does not equate to a legal character, and U+FEFF in other places, other than the beginning of text, conveys the zero-width non-break space (a character with no appearance and no effect other than preventing the formation of ligatures).
The same character converted to UTF-8 becomes the byte sequence codice_1. The Unicode Standard allows that the BOM "can serve as signature for UTF-8 encoded text where the character set is unmarked". Some software developers have adopted it for other encodings, including UTF-8, in an attempt to distinguish UTF-8 from local 8-bit code pages. However RFC 3629, the UTF-8 standard, recommends that byte order marks be forbidden in protocols using UTF-8, but discusses the cases where this may not be possible. In addition, the large restriction on possible patterns in UTF-8 (for instance there cannot be any lone bytes with the high bit set) means that it should be possible to distinguish UTF-8 from other character encodings without relying on the BOM.
In UTF-32 and UCS-4, one 32-bit code value serves as a fairly direct representation of any character's code point (although the endianness, which varies across different platforms, affects how the code value manifests as an octet sequence). In the other encodings, each code point may be represented by a variable number of code values. UTF-32 is widely used as an internal representation of text in programs (as opposed to stored or transmitted text), since every Unix operating system that uses the gcc compilers to generate software uses it as the standard "wide character" encoding. Some programming languages, such as Seed7, use UTF-32 as internal representation for strings and characters. Recent versions of the Python programming language (beginning with 2.2) may also be configured to use UTF-32 as the representation for Unicode strings, effectively disseminating such encoding in high-level coded software.
Punycode, another encoding form, enables the encoding of Unicode strings into the limited character set supported by the ASCII-based Domain Name System (DNS). The encoding is used as part of IDNA, which is a system enabling the use of Internationalized Domain Names in all scripts that are supported by Unicode. Earlier and now historical proposals include .
GB18030 is another encoding form for Unicode, from the Standardization Administration of China. It is the official character set of the People's Republic of China (PRC). BOCU-1 and SCSU are Unicode compression schemes. The April Fools' Day RFC of 2005 specified two parody UTF encodings, UTF-9 and UTF-18.
Ready-made versus composite characters.
Unicode includes a mechanism for modifying character shape that greatly extends the supported glyph repertoire. This covers the use of combining diacritical marks. They are inserted after the main character. Multiple combining diacritics may be stacked over the same character. Unicode also contains precomposed versions of most letter/diacritic combinations in normal use. These make conversion to and from legacy encodings simpler, and allow applications to use Unicode as an internal text format without having to implement combining characters. For example, "é" can be represented in Unicode as U+0065 (LATIN SMALL LETTER E) followed by U+0301 (COMBINING ACUTE ACCENT), but it can also be represented as the precomposed character U+00E9 (LATIN SMALL LETTER E WITH ACUTE). Thus, in many cases, users have multiple ways of encoding the same character. To deal with this, Unicode provides the mechanism of canonical equivalence.
An example of this arises with Hangul, the Korean alphabet. Unicode provides a mechanism for composing Hangul syllables with their individual subcomponents, known as Hangul Jamo. However, it also provides 11,172 combinations of precomposed syllables made from the most common jamo.
The CJK ideographs currently have codes only for their precomposed form. Still, most of those ideographs comprise simpler elements (often called radicals in English), so in principle, Unicode could have decomposed them, as it did with Hangul. This would have greatly reduced the number of required code points, while allowing the display of virtually every conceivable ideograph (which might do away with some of the problems caused by Han unification). A similar idea is used by some input methods, such as Cangjie and Wubi. However, attempts to do this for character encoding have stumbled over the fact that ideographs do not decompose as simply or as regularly as Hangul does.
A set of radicals was provided in Unicode 3.0 (CJK radicals between U+2E80 and U+2EFF, KangXi radicals in U+2F00 to U+2FDF, and ideographic description characters from U+2FF0 to U+2FFB), but the Unicode standard (ch. 12.2 of Unicode 5.2) warns against using ideographic description sequences as an alternate representation for previously encoded characters:
Ligatures.
Many scripts, including Arabic and Devanagari, have special orthographic rules that require certain combinations of letterforms to be combined into special ligature forms. The rules governing ligature formation can be quite complex, requiring special script-shaping technologies such as ACE (Arabic Calligraphic Engine by DecoType in the 1980s and used to generate all the Arabic examples in the printed editions of the Unicode Standard), which became the proof of concept for OpenType (by Adobe and Microsoft), Graphite (by SIL International), or AAT (by Apple).
Instructions are also embedded in fonts to tell the operating system how to properly output different character sequences. A simple solution to the placement of combining marks or diacritics is assigning the marks a width of zero and placing the glyph itself to the left or right of the left sidebearing (depending on the direction of the script they are intended to be used with). A mark handled this way will appear over whatever character precedes it, but will not adjust its position relative to the width or height of the base glyph; it may be visually awkward and it may overlap some glyphs. Real stacking is impossible, but can be approximated in limited cases (for example, Thai top-combining vowels and tone marks can just be at different heights to start with). Generally this approach is only effective in monospaced fonts, but may be used as a fallback rendering method when more complex methods fail.
Standardized subsets.
Several subsets of Unicode are standardized: Microsoft Windows since Windows NT 4.0 supports WGL-4 with 652 characters, which is considered to support all contemporary European languages using the Latin, Greek, or Cyrillic script. Other standardized subsets of Unicode include the Multilingual European Subsets:
MES-1 (Latin scripts only, 335 characters), MES-2 (Latin, Greek and Cyrillic 1062 characters) and MES-3A & MES-3B (two larger subsets, not shown here). Note that MES-2 includes every character in MES-1 and WGL-4.
Rendering software which cannot process a Unicode character appropriately often displays it as an open rectangle, or the Unicode "replacement character" (U+FFFD, �), to indicate the position of the unrecognized character. Some systems have made attempts to provide more information about such characters. The Apple's Last Resort font will display a substitute glyph indicating the Unicode range of the character, and the SIL International's Unicode Fallback font will display a box showing the hexadecimal scalar value of the character.
Adoption.
Operating systems.
Unicode has become the dominant scheme for internal processing and storage of text. Although a great deal of text is still stored in legacy encodings, Unicode is used almost exclusively for building new information processing systems. Early adopters tended to use UCS-2 (the fixed-width two-byte precursor to UTF-16) and later moved to UTF-16 (the variable-width current standard), as this was the least disruptive way to add support for non-BMP characters. The best known such system is Windows NT (and its descendants, Windows 2000, Windows XP, Windows Vista and Windows 7), which uses UTF-16 as the sole internal character encoding. The Java and .NET bytecode environments, Mac OS X, and KDE also use it for internal representation. Unicode is available on Windows 95 through Microsoft Layer for Unicode, as well as on its descendants, Windows 98 and Windows ME.
UTF-8 (originally developed for Plan 9) has become the main storage encoding on most Unix-like operating systems (though others are also used by some libraries) because it is a relatively easy replacement for traditional extended ASCII character sets. UTF-8 is also the most common Unicode encoding used in HTML documents on the World Wide Web.
Multilingual text-rendering engines which use Unicode include Uniscribe and DirectWrite for Microsoft Windows, ATSUI and Core Text for Mac OS X, and Pango for GTK+ and the GNOME desktop.
Input methods.
Because keyboard layouts cannot have simple key combinations for all characters, several operating systems provide alternative input methods that allow access to the entire repertoire.
ISO 14755, which standardises methods for entering Unicode characters from their code points, specifies several methods. There is the "Basic method", where a "beginning sequence" is followed by the hexadecimal representation of the code point and the "ending sequence". There is also a "screen-selection entry method" specified, where the characters are listed in a table in a screen, such as with a character map program.
Email.
MIME defines two different mechanisms for encoding non-ASCII characters in email, depending on whether the characters are in email headers (such as the "Subject:"), or in the text body of the message; in both cases, the original character set is identified as well as a transfer encoding. For email transmission of Unicode the UTF-8 character set and the Base64 or the Quoted-printable transfer encoding are recommended, depending on whether much of the message consists of ASCII-characters. The details of the two different mechanisms are specified in the MIME standards and generally are hidden from users of email software.
The adoption of Unicode in email has been very slow. Some East-Asian text is still encoded in encodings such as ISO-2022, and some devices, such as mobile phones, still cannot handle Unicode data correctly. Support has been improving however. Many major free mail providers such as Yahoo, Google (Gmail), and Microsoft (Outlook.com) support it.
Web.
All W3C recommendations have used Unicode as their "document character set" since HTML 4.0. Web browsers have supported Unicode, especially UTF-8, for many years. There used to be display problems resulting primarily from font related issues; e.g. v 6 and older of Microsoft Internet Explorer did not render many code points unless explicitly told to use a font that contains them.
Although syntax rules may affect the order in which characters are allowed to appear, XML (including XHTML) documents, by definition, comprise characters from most of the Unicode code points, with the exception of:
HTML characters manifest either directly as bytes according to document's encoding, if the encoding supports them, or users may write them as numeric character references based on the character's Unicode code point. For example, the references codice_2, codice_3, codice_4, codice_5, codice_6, codice_7, codice_8, codice_9, and codice_10 (or the same numeric values expressed in hexadecimal, with codice_11 as the prefix) should display on all browsers as Δ, Й, ק ,م, ๗, あ, 叶, 葉, and 말.
When specifying URIs, for example as URLs in HTTP requests, non-ASCII characters must be percent-encoded.
Fonts.
Free and retail fonts based on Unicode are widely available, since TrueType and OpenType support Unicode. These font formats map Unicode code points to glyphs.
Thousands of fonts exist on the market, but fewer than a dozen fonts—sometimes described as "pan-Unicode" fonts—attempt to support the majority of Unicode's character repertoire. Instead, Unicode-based fonts typically focus on supporting only basic ASCII and particular scripts or sets of characters or symbols. Several reasons justify this approach: applications and documents rarely need to render characters from more than one or two writing systems; fonts tend to demand resources in computing environments; and operating systems and applications show increasing intelligence in regard to obtaining glyph information from separate font files as needed, i.e., font substitution. Furthermore, designing a consistent set of rendering instructions for tens of thousands of glyphs constitutes a monumental task; such a venture passes the point of diminishing returns for most typefaces.
Newlines.
Unicode partially addresses the newline problem that occurs when trying to read a text file on different platforms. Unicode defines a large number of characters that conforming applications should recognize as line terminators.
In terms of the newline, Unicode introduced and . This was an attempt to provide a Unicode solution to encoding paragraphs and lines semantically, potentially replacing all of the various platform solutions. In doing so, Unicode does provide a way around the historical platform dependent solutions. Nonetheless, few if any Unicode solutions have adopted these Unicode line and paragraph separators as the sole canonical line ending characters. However, a common approach to solving this issue is through newline normalization. This is achieved with the Cocoa text system in Mac OS X and also with W3C XML and HTML recommendations. In this approach every possible newline character is converted internally to a common newline (which one does not really matter since it is an internal operation just for rendering). In other words, the text system can correctly treat the character as a newline, regardless of the input's actual encoding.
Issues.
Philosophical and completeness criticisms.
Han unification (the identification of forms in the East Asian languages which one can treat as stylistic variations of the same historical character) has become one of the most controversial aspects of Unicode, despite the presence of a majority of experts from all three regions in the Ideographic Rapporteur Group (IRG), which advises the Consortium and ISO on additions to the repertoire and on Han unification.
Unicode has been criticized for failing to separately encode older and alternative forms of kanji which, critics argue, complicates the processing of ancient Japanese and uncommon Japanese names. This is often due to the fact that Unicode encodes characters rather than glyphs (the visual representations of the basic character that often vary from one language to another). Unification of glyphs leads to the perception that the languages themselves, not just the basic character representation, are being merged. There have been several attempts to create alternative encodings that preserve the stylistic differences between Chinese, Japanese, and Korean characters in opposition to Unicode's policy of Han unification. An example of one is TRON (although it is not widely adopted in Japan, there are some users who need to handle historical Japanese text and favor it).
Although the repertoire of fewer than 21,000 Han characters in the earliest version of Unicode was largely limited to characters in common modern usage, Unicode now includes more than 70,000 Han characters, and work is continuing to add thousands more historic and dialectal characters used in China, Japan, Korea, Taiwan, and Vietnam.
Modern font technology provides a means to address the practical issue of needing to depict a unified Han character in terms of a collection of alternative glyph representations, in the form of Unicode variation sequences. For example, the Advanced Typographic tables of OpenType permit one of a number of alternative glyph representations to be selected when performing the character to glyph mapping process. In this case, information can be provided within plain text to designate which alternate character form to select.
If the difference in the appropriate glyphs for two characters in the same script differ only in the italic, Unicode has generally unified them, as can be seen in the comparison between Russian (labeled standard) and Serbian characters at right, meaning that the difference had shown through smart font technology or manually changing fonts.
Mapping to legacy character sets.
Unicode was designed to provide code-point-by-code-point round-trip format conversion to and from any preexisting character encodings, so that text files in older character sets can be naïvely converted to Unicode, and then back and get back the same file. That has meant that inconsistent legacy architectures, such as combining diacritics and precomposed characters, both exist in Unicode, giving more than one method of representing some text. This is most pronounced in the three different encoding forms for Korean Hangul. Since version 3.0, any precomposed characters that can be represented by a combining sequence of already existing characters can no longer be added to the standard in order to preserve interoperability between software using different versions of Unicode.
Injective mappings must be provided between characters in existing legacy character sets and characters in Unicode to facilitate conversion to Unicode and allow interoperability with legacy software. Lack of consistency in various mappings between earlier Japanese encodings such as Shift-JIS or EUC-JP and Unicode led to round-trip format conversion mismatches, particularly the mapping of the character JIS X 0208 '～' (1-33, WAVE DASH), heavily used in legacy database data, to either (in Microsoft Windows) or (other vendors).
Some Japanese computer programmers objected to Unicode because it requires them to separate the use of and , which was mapped to 0x5C in JIS X 0201, and a lot of legacy code exists with this usage. (This encoding also replaces tilde '~' 0x7E with macron '¯', now 0xAF.) The separation of these characters exists in ISO 8859-1, from long before Unicode.
Indic scripts.
Indic scripts such as Tamil and Devanagari are each allocated only 128 code points, matching the ISCII standard. The correct rendering of Unicode Indic text requires transforming the stored logical order characters into visual order and the forming of ligatures (aka conjuncts) out of components. Some local scholars argued in favor of assignments of Unicode code points to these ligatures, going against the practice for other writing systems, though Unicode contains some Arabic and other ligatures for backward compatibility purposes only. Encoding of any new ligatures in Unicode will not happen, in part because the set of ligatures is font-dependent, and Unicode is an encoding independent of font variations. The same kind of issue arose for Tibetan script (the Chinese National Standard organization failed to achieve a similar change).
Thai alphabet support has been criticized for its ordering of Thai characters. The vowels เ, แ, โ, ใ, ไ that are written to the left of the preceding consonant are in visual order instead of phonetic order, unlike the Unicode representations of other Indic scripts. This complication is due to Unicode inheriting the Thai Industrial Standard 620, which worked in the same way, and was the way in which Thai had always been written on keyboards. This ordering problem complicates the Unicode collation process slightly, requiring table lookups to reorder Thai characters for collation. Even if Unicode had adopted encoding according to spoken order, it would still be problematic to collate words in dictionary order. E.g., the word "perform" starts with a consonant cluster "สด" (with an inherent vowel for the consonant "ส"), the vowel แ-, in spoken order would come after the ด, but in a dictionary, the word is collated as it is written, with the vowel following the ส.
Combining characters.
Characters with diacritical marks can generally be represented either as a single precomposed character or as a decomposed sequence of a base letter plus one or more non-spacing marks. For example, ḗ (precomposed e with macron and acute above) and ḗ (e followed by the combining macron above and combining acute above) should be rendered identically, both appearing as an e with a macron and acute accent, but in practice, their appearance may vary depending upon what rendering engine and fonts are being used to display the characters. Similarly, underdots, as needed in the romanization of Indic, will often be placed incorrectly. Unicode characters that map to precomposed glyphs can be used in many cases, thus avoiding the problem, but where no precomposed character has been encoded the problem can often be solved by using a specialist Unicode font such as Charis SIL that uses Graphite, OpenType, or AAT technologies for advanced rendering features.

</doc>
<doc id="31743" url="https://en.wikipedia.org/wiki?curid=31743" title="Uranium">
Uranium

Uranium is a chemical element with symbol U and atomic number 92. It is a silvery-white metal in the actinide series of the periodic table. A uranium atom has 92 protons and 92 electrons, of which 6 are valence electrons. Uranium is weakly radioactive because all its isotopes are unstable (with half-lives of the six naturally known isotopes, uranium-233 to uranium-238, varying between 69 years and 4.5 billion years). The most common isotopes in natural uranium are uranium-238 (which has 146 neutrons and accounts for over 99%) and uranium-235 (which has 143 neutrons). Uranium has the second highest atomic weight of the primordially occurring elements, lighter only than plutonium. Its density is about 70% higher than that of lead, and slightly lower than that of gold or tungsten. It occurs naturally in low concentrations of a few parts per million in soil, rock and water, and is commercially extracted from uranium-bearing minerals such as uraninite.
In nature, uranium is found as uranium-238 (99.2739–99.2752%), uranium-235 (0.7198–0.7202%), and a very small amount of uranium-234 (0.0050–0.0059%). Uranium decays slowly by emitting an alpha particle. The half-life of uranium-238 is about 4.47 billion years and that of uranium-235 is 704 million years, making them useful in dating the age of the Earth.
Many contemporary uses of uranium exploit its unique nuclear properties. Uranium-235 has the distinction of being the only naturally occurring fissile isotope. Uranium-238 is fissionable by fast neutrons, and is "fertile", meaning it can be transmuted to fissile plutonium-239 in a nuclear reactor. Another fissile isotope, uranium-233, can be produced from natural thorium and is also important in nuclear technology. Uranium-238 has a small probability for spontaneous fission or even induced fission with fast neutrons; uranium-235 and to a lesser degree uranium-233 have a much higher fission cross-section for slow neutrons. In sufficient concentration, these isotopes maintain a sustained nuclear chain reaction. This generates the heat in nuclear power reactors, and produces the fissile material for nuclear weapons. Depleted uranium (238U) is used in kinetic energy penetrators and armor plating. Uranium is used as a colorant in uranium glass, producing lemon yellow to green colors. Uranium glass fluoresces green in ultraviolet light. It was also used for tinting and shading in early photography. 
The 1789 discovery of uranium in the mineral pitchblende is credited to Martin Heinrich Klaproth, who named the new element after the planet Uranus. Eugène-Melchior Péligot was the first person to isolate the metal and its radioactive properties were discovered in 1896 by Henri Becquerel. Research by Otto Hahn, Lise Meitner, Enrico Fermi and others, such as J. Robert Oppenheimer starting in 1934 led to its use as a fuel in the nuclear power industry and in "Little Boy", the first nuclear weapon used in war. An ensuing arms race during the Cold War between the United States and the Soviet Union produced tens of thousands of nuclear weapons that used uranium metal and uranium-derived plutonium-239. The security of those weapons and their fissile material following the breakup of the Soviet Union in 1991 is an ongoing concern for public health and safety. See Nuclear proliferation.
Characteristics.
When refined, uranium is a silvery white, weakly radioactive metal. It has a Mohs hardness of 6, sufficient to scratch glass and approximately equal to that of titanium, rhodium, manganese and niobium. It is malleable, ductile, slightly paramagnetic, strongly electropositive and a poor electrical conductor. Uranium metal has a very high density of 19.1 g/cm3, denser than lead (11.3 g/cm3), but slightly less dense than tungsten and gold (19.3 g/cm3).
Uranium metal reacts with almost all non-metal elements (with an exception of the noble gases) and their compounds, with reactivity increasing with temperature. Hydrochloric and nitric acids dissolve uranium, but non-oxidizing acids other than hydrochloric acid attack the element very slowly. When finely divided, it can react with cold water; in air, uranium metal becomes coated with a dark layer of uranium oxide. Uranium in ores is extracted chemically and converted into uranium dioxide or other chemical forms usable in industry.
Uranium-235 was the first isotope that was found to be fissile. Other naturally occurring isotopes are fissionable, but not fissile. On bombardment with slow neutrons, its uranium-235 isotope will most of the time divide into two smaller nuclei, releasing nuclear binding energy and more neutrons. If too many of these neutrons are absorbed by other uranium-235 nuclei, a nuclear chain reaction occurs that results in a burst of heat or (in special circumstances) an explosion. In a nuclear reactor, such a chain reaction is slowed and controlled by a neutron poison, absorbing some of the free neutrons. Such neutron absorbent materials are often part of reactor control rods (see nuclear reactor physics for a description of this process of reactor control).
As little as 15 lb (7 kg) of uranium-235 can be used to make an atomic bomb. The first nuclear bomb used in war, "Little Boy", relied on uranium fission, but the very first nuclear explosive (the "Gadget" used at Trinity) and the bomb that destroyed Nagasaki ("Fat Man") were both plutonium bombs.
Uranium metal has three allotropic forms:
Applications.
Military.
The major application of uranium in the military sector is in high-density penetrators. This ammunition consists of depleted uranium (DU) alloyed with 1–2% other elements, such as titanium or molybdenum. At high impact speed, the density, hardness, and pyrophoricity of the projectile enable the destruction of heavily armored targets. Tank armor and other removable vehicle armor can also be hardened with depleted uranium plates. The use of depleted uranium became politically and environmentally contentious after the use of such munitions by the US, UK and other countries during wars in the Persian Gulf and the Balkans raised questions concerning uranium compounds left in the soil (see Gulf War Syndrome).
Depleted uranium is also used as a shielding material in some containers used to store and transport radioactive materials. While the metal itself is radioactive, its high density makes it more effective than lead in halting radiation from strong sources such as radium. Other uses of depleted uranium include counterweights for aircraft control surfaces, as ballast for missile re-entry vehicles and as a shielding material. Due to its high density, this material is found in inertial guidance systems and in gyroscopic compasses. Depleted uranium is preferred over similarly dense metals due to its ability to be easily machined and cast as well as its relatively low cost. The main risk of exposure to depleted uranium is chemical poisoning by uranium oxide rather than radioactivity (uranium being only a weak alpha emitter).
During the later stages of World War II, the entire Cold War, and to a lesser extent afterwards, uranium-235 has been used as the fissile explosive material to produce nuclear weapons. Initially, two major types of fission bombs were built: a relatively simple device that uses uranium-235 and a more complicated mechanism that uses plutonium-239 derived from uranium-238. Later, a much more complicated and far more powerful type of fission/fusion bomb (thermonuclear weapon) was built, that uses a plutonium-based device to cause a mixture of tritium and deuterium to undergo nuclear fusion. Such bombs are jacketed in a non-fissile (unenriched) uranium case, and they derive more than half their power from the fission of this material by fast neutrons from the nuclear fusion process.
Civilian.
The main use of uranium in the civilian sector is to fuel nuclear power plants. One kilogram of uranium-235 can theoretically produce about 20 terajoules of energy (2 joules), assuming complete fission; as much energy as 1500 tonnes of coal.
Commercial nuclear power plants use fuel that is typically enriched to around 3% uranium-235. The CANDU and Magnox designs are the only commercial reactors capable of using unenriched uranium fuel. Fuel used for United States Navy reactors is typically highly enriched in uranium-235 (the exact values are classified). In a breeder reactor, uranium-238 can also be converted into plutonium through the following reaction:
Before (and, occasionally, after) the discovery of radioactivity, uranium was primarily used in small amounts for yellow glass and pottery glazes, such as uranium glass and in Fiestaware.
The discovery and isolation of radium in uranium ore (pitchblende) by Marie Curie sparked the development of uranium mining to extract the radium, which was used to make glow-in-the-dark paints for clock and aircraft dials. This left a prodigious quantity of uranium as a waste product, since it takes three tonnes of uranium to extract one gram of radium. This waste product was diverted to the glazing industry, making uranium glazes very inexpensive and abundant. Besides the pottery glazes, uranium tile glazes accounted for the bulk of the use, including common bathroom and kitchen tiles which can be produced in green, yellow, mauve, black, blue, red and other colors.
Uranium was also used in photographic chemicals (especially uranium nitrate as a toner), in lamp filaments for stage lighting bulbs, to improve the appearance of dentures, and in the leather and wood industries for stains and dyes. Uranium salts are mordants of silk or wool. Uranyl acetate and uranyl formate are used as electron-dense "stains" in transmission electron microscopy, to increase the contrast of biological specimens in ultrathin sections and in negative staining of viruses, isolated cell organelles and macromolecules.
The discovery of the radioactivity of uranium ushered in additional scientific and practical uses of the element. The long half-life of the isotope uranium-238 (4.51 years) makes it well-suited for use in estimating the age of the earliest igneous rocks and for other types of radiometric dating, including uranium-thorium dating, uranium-lead dating and uranium-uranium dating. Uranium metal is used for X-ray targets in the making of high-energy X-rays.
History.
Pre-discovery use.
The use of uranium in its natural oxide form dates back to at least the year 79 CE, when it was used to add a yellow color to ceramic glazes. Yellow glass with 1% uranium oxide was found in a Roman villa on Cape Posillipo in the Bay of Naples, Italy, by R. T. Gunther of the University of Oxford in 1912. Starting in the late Middle Ages, pitchblende was extracted from the Habsburg silver mines in Joachimsthal, Bohemia (now Jáchymov in the Czech Republic), and was used as a coloring agent in the local glassmaking industry. In the early 19th century, the world's only known sources of uranium ore were these mines.
Discovery.
The discovery of the element is credited to the German chemist Martin Heinrich Klaproth. While he was working in his experimental laboratory in Berlin in 1789, Klaproth was able to precipitate a yellow compound (likely sodium diuranate) by dissolving pitchblende in nitric acid and neutralizing the solution with sodium hydroxide. Klaproth assumed the yellow substance was the oxide of a yet-undiscovered element and heated it with charcoal to obtain a black powder, which he thought was the newly discovered metal itself (in fact, that powder was an oxide of uranium). He named the newly discovered element after the planet Uranus, (named after the primordial Greek god of the sky), which had been discovered eight years earlier by William Herschel.
In 1841, Eugène-Melchior Péligot, Professor of Analytical Chemistry at the Conservatoire National des Arts et Métiers (Central School of Arts and Manufactures) in Paris, isolated the first sample of uranium metal by heating uranium tetrachloride with potassium.
Henri Becquerel discovered radioactivity by using uranium in 1896. Becquerel made the discovery in Paris by leaving a sample of a uranium salt, K2UO2(SO4)2 (potassium uranyl sulfate), on top of an unexposed photographic plate in a drawer and noting that the plate had become "fogged". He determined that a form of invisible light or rays emitted by uranium had exposed the plate.
Fission research.
A team led by Enrico Fermi in 1934 observed that bombarding uranium with neutrons produces the emission of beta rays (electrons or positrons from the elements produced; see beta particle). The fission products were at first mistaken for new elements of atomic numbers 93 and 94, which the Dean of the Faculty of Rome, Orso Mario Corbino, christened "ausonium" and "hesperium", respectively. The experiments leading to the discovery of uranium's ability to fission (break apart) into lighter elements and release binding energy were conducted by Otto Hahn and Fritz Strassmann in Hahn's laboratory in Berlin. Lise Meitner and her nephew, the physicist Otto Robert Frisch, published the physical explanation in February 1939 and named the process "nuclear fission". Soon after, Fermi hypothesized that the fission of uranium might release enough neutrons to sustain a fission reaction. Confirmation of this hypothesis came in 1939, and later work found that on average about 2.5 neutrons are released by each fission of the rare uranium isotope uranium-235. Further work found that the far more common uranium-238 isotope can be transmuted into plutonium, which, like uranium-235, is also fissile by thermal neutrons. These discoveries led numerous countries to begin working on the development of nuclear weapons and nuclear power.
On 2 December 1942, as part of the Manhattan Project, another team led by Enrico Fermi was able to initiate the first artificial self-sustained nuclear chain reaction, Chicago Pile-1. Working in a lab below the stands of Stagg Field at the University of Chicago, the team created the conditions needed for such a reaction by piling together 400 short tons (360 metric tons) of graphite, 58 short tons (53 metric tons) of uranium oxide, and six short tons (5.5 metric tons) of uranium metal, a majority of which was supplied by Westinghouse Lamp Plant in a makeshift production process.
Nuclear weaponry.
Two major types of atomic bombs were developed by the United States during World War II: a uranium-based device (codenamed "Little Boy") whose fissile material was highly enriched uranium, and a plutonium-based device (see Trinity test and "Fat Man") whose plutonium was derived from uranium-238. The uranium-based Little Boy device became the first nuclear weapon used in war when it was detonated over the Japanese city of Hiroshima on 6 August 1945. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings and killed approximately 75,000 people (see Atomic bombings of Hiroshima and Nagasaki). Initially it was believed that uranium was relatively rare, and that nuclear proliferation could be avoided by simply buying up all known uranium stocks, but within a decade large deposits of it were discovered in many places around the world.
Reactors.
The X-10 Graphite Reactor at Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, formerly known as the Clinton Pile and X-10 Pile, was the world's second artificial nuclear reactor (after Enrico Fermi's Chicago Pile) and was the first reactor designed and built for continuous operation. Argonne National Laboratory's Experimental Breeder Reactor I, located at the Atomic Energy Commission's National Reactor Testing Station near Arco, Idaho, became the first nuclear reactor to create electricity on 20 December 1951. Initially, four 150-watt light bulbs were lit by the reactor, but improvements eventually enabled it to power the whole facility (later, the town of Arco became the first in the world to have all its electricity come from nuclear power generated by BORAX-III, another reactor designed and operated by Argonne National Laboratory). The world's first commercial scale nuclear power station, Obninsk in the Soviet Union, began generation with its reactor AM-1 on 27 June 1954. Other early nuclear power plants were Calder Hall in England, which began generation on 17 October 1956, and the Shippingport Atomic Power Station in Pennsylvania, which began on 26 May 1958. Nuclear power was used for the first time for propulsion by a submarine, the USS "Nautilus", in 1954.
Prehistoric naturally occurring fission.
In 1972, the French physicist Francis Perrin discovered fifteen ancient and no longer active natural nuclear fission reactors in three separate ore deposits at the Oklo mine in Gabon, West Africa, collectively known as the Oklo Fossil Reactors. The ore deposit is 1.7 billion years old; then, uranium-235 constituted about 3% of the total uranium on Earth. This is high enough to permit a sustained nuclear fission chain reaction to occur, provided other supporting conditions exist. The capacity of the surrounding sediment to contain the nuclear waste products has been cited by the U.S. federal government as supporting evidence for the feasibility to store spent nuclear fuel at the Yucca Mountain nuclear waste repository.
Contamination and the Cold War legacy.
Above-ground nuclear tests by the Soviet Union and the United States in the 1950s and early 1960s and by France into the 1970s and 1980s spread a significant amount of fallout from uranium daughter isotopes around the world. Additional fallout and pollution occurred from several nuclear accidents.
Uranium miners have a higher incidence of cancer. An excess risk of lung cancer among Navajo uranium miners, for example, has been documented and linked to their occupation. The Radiation Exposure Compensation Act, a 1990 law in the USA, required $100,000 in "compassion payments" to uranium miners diagnosed with cancer or other respiratory ailments.
During the Cold War between the Soviet Union and the United States, huge stockpiles of uranium were amassed and tens of thousands of nuclear weapons were created using enriched uranium and plutonium made from uranium. Since the break-up of the Soviet Union in 1991, an estimated 600 short tons (540 metric tons) of highly enriched weapons grade uranium (enough to make 40,000 nuclear warheads) have been stored in often inadequately guarded facilities in the Russian Federation and several other former Soviet states. Police in Asia, Europe, and South America on at least 16 occasions from 1993 to 2005 have intercepted shipments of smuggled bomb-grade uranium or plutonium, most of which was from ex-Soviet sources. From 1993 to 2005 the Material Protection, Control, and Accounting Program, operated by the federal government of the United States, spent approximately US $550 million to help safeguard uranium and plutonium stockpiles in Russia. This money was used for improvements and security enhancements at research and storage facilities. "Scientific American" reported in February 2006 that in some of the facilities security consisted of chain link fences which were in severe states of disrepair. According to an interview from the article, one facility had been storing samples of enriched (weapons grade) uranium in a broom closet before the improvement project; another had been keeping track of its stock of nuclear warheads using index cards kept in a shoe box.
Occurrence.
Biotic and abiotic.
Uranium is a naturally occurring element that can be found in low levels within all rock, soil, and water. Uranium is the 51st element in order of abundance in the Earth's crust. Uranium is also the highest-numbered element to be found naturally in significant quantities on Earth and is almost always found combined with other elements. Along with all elements having atomic weights higher than that of iron, it is only naturally formed in supernovae. The decay of uranium, thorium, and potassium-40 in the Earth's mantle is thought to be the main source of heat that keeps the outer core liquid and drives mantle convection, which in turn drives plate tectonics.
Uranium's average concentration in the Earth's crust is (depending on the reference) 2 to 4 parts per million, or about 40 times as abundant as silver. The Earth's crust from the surface to 25 km (15 mi) down is calculated to contain 1017 kg (2 lb) of uranium while the oceans may contain 1013 kg (2 lb). The concentration of uranium in soil ranges from 0.7 to 11 parts per million (up to 15 parts per million in farmland soil due to use of phosphate fertilizers), and its concentration in sea water is 3 parts per billion.
Uranium is more plentiful than antimony, tin, cadmium, mercury, or silver, and it is about as abundant as arsenic or molybdenum. Uranium is found in hundreds of minerals, including uraninite (the most common uranium ore), carnotite, autunite, uranophane, torbernite, and coffinite. Significant concentrations of uranium occur in some substances such as phosphate rock deposits, and minerals such as lignite, and monazite sands in uranium-rich ores (it is recovered commercially from sources with as little as 0.1% uranium).
Some organisms, such as the lichen "Trapelia involuta" or microorganisms such as the bacterium "Citrobacter", can absorb concentrations of uranium that are up to 300 times the level of their environment. "Citrobacter" species absorb uranyl ions when given glycerol phosphate (or other similar organic phosphates). After one day, one gram of bacteria can encrust themselves with nine grams of uranyl phosphate crystals; this creates the possibility that these organisms could be used in bioremediation to decontaminate uranium-polluted water.
The proteobacterium "Geobacter" has also been shown to bioremediate uranium in ground water. The mycorrhizal fungus Glomus intraradices increases uranium content in the roots of its symbiotic plant.
In nature, uranium(VI) forms highly soluble carbonate complexes at alkaline pH. This leads to an increase in mobility and availability of uranium to groundwater and soil from nuclear wastes which leads to health hazards. However, it is difficult to precipitate uranium as phosphate in the presence of excess carbonate at alkaline pH. A "Sphingomonas" sp. strain BSAR-1 has been found to express a high activity alkaline phosphatase (PhoK) that has been applied for bioprecipitation of uranium as uranyl phosphate species from alkaline solutions. The precipitation ability was enhanced by overexpressing PhoK protein in "E. coli".
Plants absorb some uranium from soil. Dry weight concentrations of uranium in plants range from 5 to 60 parts per billion, and ash from burnt wood can have concentrations up to 4 parts per million. Dry weight concentrations of uranium in food plants are typically lower with one to two micrograms per day ingested through the food people eat.
Production and mining.
Worldwide production of U3O8 (yellowcake) in 2013 amounted to 70,015 tonnes, of which 22,451 t (32%) was mined in Kazakhstan. Other important uranium mining countries are Canada (9,331 t), Australia (6,350 t), Niger (4,518 t), Namibia (4,323 t) and Russia (3,135 t).
Uranium ore is mined in several ways: by open pit, underground, in-situ leaching, and borehole mining (see uranium mining). Low-grade uranium ore mined typically contains 0.01 to 0.25% uranium oxides. Extensive measures must be employed to extract the metal from its ore. High-grade ores found in Athabasca Basin deposits in Saskatchewan, Canada can contain up to 23% uranium oxides on average. Uranium ore is crushed and rendered into a fine powder and then leached with either an acid or alkali. The leachate is subjected to one of several sequences of precipitation, solvent extraction, and ion exchange. The resulting mixture, called yellowcake, contains at least 75% uranium oxides U3O8. Yellowcake is then calcined to remove impurities from the milling process before refining and conversion.
Commercial-grade uranium can be produced through the reduction of uranium halides with alkali or alkaline earth metals. Uranium metal can also be prepared through electrolysis of or
Uranium tetrafluoride, dissolved in molten calcium chloride () and sodium chloride (NaCl) solution. Very pure uranium is produced through the thermal decomposition of uranium halides on a hot filament.
Resources and reserves.
It is estimated that 5.5 million tonnes of uranium exists in ore reserves that are economically viable at US$59 per lb of uranium, while 35 million tonnes are classed as mineral resources (reasonable prospects for eventual economic extraction). Prices went from about $10/lb in May 2003 to $138/lb in July 2007. This has caused a big increase in spending on exploration, with US$200 million being spent worldwide in 2005, a 54% increase on the previous year. This trend continued through 2006, when expenditure on exploration rocketed to over $774 million, an increase of over 250% compared to 2004. The OECD Nuclear Energy Agency said exploration figures for 2007 would likely match those for 2006.
Australia has 31% of the world's known uranium ore reserves and the world's largest single uranium deposit, located at the Olympic Dam Mine in South Australia. There is a significant reserve of uranium
in Bakouma a sub-prefecture in the prefecture of Mbomou in Central African Republic.
Some nuclear fuel comes from nuclear weapons being dismantled, such as from the Megatons to Megawatts Program.
An additional 4.6 billion tonnes of uranium are estimated to be in sea water (Japanese scientists in the 1980s showed that extraction of uranium from sea water using ion exchangers was technically feasible). There have been experiments to extract uranium from sea water, but the yield has been low due to the carbonate present in the water. In 2012, ORNL researchers announced the successful development of a new absorbent material dubbed HiCap which performs surface retention of solid or gas molecules, atoms or ions and also effectively removes toxic metals from water, according to results verified by researchers at Pacific Northwest National Laboratory.
Supplies.
In 2005, seventeen countries produced concentrated uranium oxides, with Canada (27.9% of world production) and Australia (22.8%) being the largest producers and Kazakhstan (10.5%), Russia (8.0%), Namibia (7.5%), Niger (7.4%), Uzbekistan (5.5%), the United States (2.5%), Argentina (2.1%), Ukraine (1.9%) and China (1.7%) also producing significant amounts. Kazakhstan continues to increase production and may have become the world's largest producer of uranium by 2009 with an expected production of 12,826 tonnes, compared to Canada with 11,100 t and Australia with 9,430 t. In the late 1960s, UN geologists also discovered major uranium deposits and other rare mineral reserves in Somalia. The find was the largest of its kind, with industry experts estimating the deposits at over 25% of the world's then known uranium reserves of 800,000 tons.
The ultimate available supply is believed to be sufficient for at least the next 85 years, although some studies indicate underinvestment in the late twentieth century may produce supply problems in the 21st century.
Uranium deposits seem to be log-normal distributed. There is a 300-fold increase in the amount of uranium recoverable for each tenfold decrease in ore grade.
In other words, there is little high grade ore and proportionately much more low grade ore available.
Compounds.
Oxidation states and oxides.
Oxides.
Calcined uranium yellowcake, as produced in many large mills, contains a distribution of uranium oxidation species in various forms ranging from most oxidized to least oxidized. Particles with short residence times in a calciner will generally be less oxidized than those with long retention times or particles recovered in the stack scrubber. Uranium content is usually referenced to , which dates to the days of the Manhattan project when was used as an analytical chemistry reporting standard.
Phase relationships in the uranium-oxygen system are complex. The most important oxidation states of uranium are uranium(IV) and uranium(VI), and their two corresponding oxides are, respectively, uranium dioxide () and uranium trioxide (). Other uranium oxides such as uranium monoxide (UO), diuranium pentoxide (), and uranium peroxide () also exist.
The most common forms of uranium oxide are triuranium octoxide () and . Both oxide forms are solids that have low solubility in water and are relatively stable over a wide range of environmental conditions. Triuranium octoxide is (depending on conditions) the most stable compound of uranium and is the form most commonly found in nature. Uranium dioxide is the form in which uranium is most commonly used as a nuclear reactor fuel. At ambient temperatures, will gradually convert to . Because of their stability, uranium oxides are generally considered the preferred chemical form for storage or disposal.
Aqueous chemistry.
Salts of many oxidation states of uranium are water-soluble and may be studied in aqueous solutions. The most common ionic forms are (brown-red), (green), (unstable), and (yellow), for U(III), U(IV), U(V), and U(VI), respectively. A few solid and semi-metallic compounds such as UO and US exist for the formal oxidation state uranium(II), but no simple ions are known to exist in solution for that state. Ions of liberate hydrogen from water and are therefore considered to be highly unstable. The ion represents the uranium(VI) state and is known to form compounds such as uranyl carbonate, uranyl chloride and uranyl sulfate. also forms complexes with various organic chelating agents, the most commonly encountered of which is uranyl acetate.
Unlike the uranyl salts of uranium and polyatomic ion uranium-oxide cationic forms, the uranates, salts containing a polyatomic uranium-oxide anion, are generally not water-soluble.
Carbonates.
The interactions of carbonate anions with uranium(VI) cause the Pourbaix diagram to change greatly when the medium is changed from water to a carbonate containing solution. While the vast majority of carbonates are insoluble in water (students are often taught that all carbonates other than those of alkali metals are insoluble in water), uranium carbonates are often soluble in water. This is because a U(VI) cation is able to bind two terminal oxides and three or more carbonates to form anionic complexes.
Effects of pH.
The uranium fraction diagrams in the presence of carbonate illustrate this further: when the pH of a uranium(VI) solution increases, the uranium is converted to a hydrated uranium oxide hydroxide and at high pHs it becomes an anionic hydroxide complex.
When carbonate is added, uranium is converted to a series of carbonate complexes if the pH is increased. One effect of these reactions is increased solubility of uranium in the pH range 6 to 8, a fact that has a direct bearing on the long term stability of spent uranium dioxide nuclear fuels.
Hydrides, carbides and nitrides.
Uranium metal heated to reacts with hydrogen to form uranium hydride. Even higher temperatures will reversibly remove the hydrogen. This property makes uranium hydrides convenient starting materials to create reactive uranium powder along with various uranium carbide, nitride, and halide compounds. Two crystal modifications of uranium hydride exist: an α form that is obtained at low temperatures and a β form that is created when the formation temperature is above 250 °C.
Uranium carbides and uranium nitrides are both relatively inert semimetallic compounds that are minimally soluble in acids, react with water, and can ignite in air to form . Carbides of uranium include uranium monocarbide (UC), uranium dicarbide (), and diuranium tricarbide (). Both UC and are formed by adding carbon to molten uranium or by exposing the metal to carbon monoxide at high temperatures. Stable below 1800 °C, is prepared by subjecting a heated mixture of UC and to mechanical stress. Uranium nitrides obtained by direct exposure of the metal to nitrogen include uranium mononitride (UN), uranium dinitride (), and diuranium trinitride ().
Halides.
All uranium fluorides are created using uranium tetrafluoride (); itself is prepared by hydrofluorination of uranium dioxide. Reduction of with hydrogen at 1000 °C produces uranium trifluoride (). Under the right conditions of temperature and pressure, the reaction of solid with gaseous uranium hexafluoride () can form the intermediate fluorides of , , and .
At room temperatures, has a high vapor pressure, making it useful in the gaseous diffusion process to separate the rare uranium-235 from the common uranium-238 isotope. This compound can be prepared from uranium dioxide and uranium hydride by the following process:
The resulting , a white solid, is highly reactive (by fluorination), easily sublimes (emitting a vapor that behaves as a nearly ideal gas), and is the most volatile compound of uranium known to exist.
One method of preparing uranium tetrachloride () is to directly combine chlorine with either uranium metal or uranium hydride. The reduction of by hydrogen produces uranium trichloride () while the higher chlorides of uranium are prepared by reaction with additional chlorine. All uranium chlorides react with water and air.
Bromides and iodides of uranium are formed by direct reaction of, respectively, bromine and iodine with uranium or by adding to those element's acids. Known examples include: , , , and . Uranium oxyhalides are water-soluble and include , , , and . Stability of the oxyhalides decrease as the atomic weight of the component halide increases.
Isotopes.
Natural concentrations.
Natural uranium consists of three major isotopes: uranium-238 (99.28% natural abundance), uranium-235 (0.71%), and uranium-234 (0.0054%). All three are radioactive, emitting alpha particles, with the exception that all three of these isotopes have small probabilities of undergoing spontaneous fission, rather than alpha emission. There are also five other trace isotopes: uranium-239, which is formed when 238U undergoes spontaneous fission, releasing neutrons that are captured by another 238U atom; uranium-237, which is formed when 238U captures a neutron but emits two more, which then decays to neptunium-237; uranium-233, which is formed in the decay chain of that neptunium-237; and finally, uranium-236 and -240, which appear in the decay chain of primordial plutonium-244. It is also expected that thorium-232 should be able to undergo double beta decay, which would produce uranium-232, but this has not yet been observed experimentally.
Uranium-238 is the most stable isotope of uranium, with a half-life of about 4.468 years, roughly the age of the Earth. Uranium-235 has a half-life of about 7.13 years, and uranium-234 has a half-life of about 2.48 years.
For natural uranium, about 49% of its alpha rays are emitted by each of 238U atom, and also 49% by 234U (since the latter is formed from the former) and about 2.0% of them by the 235U. When the Earth was young, probably about one-fifth of its uranium was uranium-235, but the percentage of 234U was probably much lower than this.
Uranium-238 is usually an α emitter (occasionally, it undergoes spontaneous fission), decaying through the "Uranium Series" of nuclear decay, which has 18 members, into lead-206, by a variety of different decay paths.
The decay series of 235U, which is called the actinium series, has 15 members and eventually decays into lead-207. The constant rates of decay in these decay series makes the comparison of the ratios of parent to daughter elements useful in radiometric dating.
Uranium-234, which is a member of the "Uranium Series", decays to lead-206 through a series of relatively short-lived isotopes.
Uranium-233 is made from thorium-232 by neutron bombardment, usually in a nuclear reactor, and 233U is also fissile. Its decay series ends at bismuth-209 and thallium-205.
Uranium-235 is important for both nuclear reactors and nuclear weapons, because it is the only uranium isotope existing in nature on Earth in any significant amount that is fissile. This means that it can be split into two or three fragments (fission products) by thermal neutrons.
Uranium-238 is not fissile, but is a fertile isotope, because after neutron activation it can produce plutonium-239, another fissile isotope. Indeed, the 238U nucleus can absorb one neutron to produce the radioactive isotope uranium-239. 239U decays by beta emission to neptunium-239, also a beta-emitter, that decays in its turn, within a few days into plutonium-239. 239Pu was used as fissile material in the first atomic bomb detonated in the "Trinity test" on 15 July 1945 in New Mexico.
Enrichment.
In nature, uranium is found as uranium-238 (99.2742%) and uranium-235 (0.7204%). Isotope separation concentrates (enriches) the fissionable uranium-235 for nuclear weapons and most nuclear power plants, except for gas cooled reactors and pressurised heavy water reactors. Most neutrons released by a fissioning atom of uranium-235 must impact other uranium-235 atoms to sustain the nuclear chain reaction. The concentration and amount of uranium-235 needed to achieve this is called a 'critical mass'.
To be considered 'enriched', the uranium-235 fraction should be between 3% and 5%. This process produces huge quantities of uranium that is depleted of uranium-235 and with a correspondingly increased fraction of uranium-238, called depleted uranium or 'DU'. To be considered 'depleted', the uranium-235 isotope concentration should be no more than 0.3%. The price of uranium has risen since 2001, so enrichment tailings containing more than 0.35% uranium-235 are being considered for re-enrichment, driving the price of depleted uranium hexafluoride above $130 per kilogram in July 2007 from $5 in 2001.
The gas centrifuge process, where gaseous uranium hexafluoride () is separated by the difference in molecular weight between 235UF6 and 238UF6 using high-speed centrifuges, is the cheapest and leading enrichment process. The gaseous diffusion process had been the leading method for enrichment and was used in the Manhattan Project. In this process, uranium hexafluoride is repeatedly diffused through a silver-zinc membrane, and the different isotopes of uranium are separated by diffusion rate (since uranium 238 is heavier it diffuses slightly slower than uranium-235). The molecular laser isotope separation method employs a laser beam of precise energy to sever the bond between uranium-235 and fluorine. This leaves uranium-238 bonded to fluorine and allows uranium-235 metal to precipitate from the solution. An alternative laser method of enrichment is known as atomic vapor laser isotope separation (AVLIS) and employs visible tunable lasers such as dye lasers. Another method used is liquid thermal diffusion.
Human exposure.
A person can be exposed to uranium (or its radioactive daughters, such as radon) by inhaling dust in air or by ingesting contaminated water and food. The amount of uranium in air is usually very small; however, people who work in factories that process phosphate fertilizers, live near government facilities that made or tested nuclear weapons, live or work near a modern battlefield where depleted uranium weapons have been used, or live or work near a coal-fired power plant, facilities that mine or process uranium ore, or enrich uranium for reactor fuel, may have increased exposure to uranium. Houses or structures that are over uranium deposits (either natural or man-made slag deposits) may have an increased incidence of exposure to radon gas. The Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for uranium exposure in the workplace as 0.25 mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.2 mg/m3 over an 8-hour workday and a short-term limit of 0.6 mg/m3. At levels of 10 mg/m3, uranium is immediately dangerous to life and health.
Most ingested uranium is excreted during digestion. Only 0.5% is absorbed when insoluble forms of uranium, such as its oxide, are ingested, whereas absorption of the more soluble uranyl ion can be up to 5%. However, soluble uranium compounds tend to quickly pass through the body, whereas insoluble uranium compounds, especially when inhaled by way of dust into the lungs, pose a more serious exposure hazard. After entering the bloodstream, the absorbed uranium tends to bioaccumulate and stay for many years in bone tissue because of uranium's affinity for phosphates. Uranium is not absorbed through the skin, and alpha particles released by uranium cannot penetrate the skin.
Incorporated uranium becomes uranyl ions, which accumulate in bone, liver, kidney, and reproductive tissues. Uranium can be decontaminated from steel surfaces and aquifers.
Effects and precautions.
Normal functioning of the kidney, brain, liver, heart, and other systems can be affected by uranium exposure, because, besides being weakly radioactive, uranium is a toxic metal. Uranium is also a reproductive toxicant. Radiological effects are generally local because alpha radiation, the primary form of 238U decay, has a very short range, and will not penetrate skin. Uranyl () ions, such as from uranium trioxide or uranyl nitrate and other hexavalent uranium compounds, have been shown to cause birth defects and immune system damage in laboratory animals. While the CDC has published one study that no human cancer has been seen as a result of exposure to natural or depleted uranium, exposure to uranium and its decay products, especially radon, are widely known and significant health threats. Exposure to strontium-90, iodine-131, and other fission products is unrelated to uranium exposure, but may result from medical procedures or exposure to spent reactor fuel or fallout from nuclear weapons.
Although accidental inhalation exposure to a high concentration of uranium hexafluoride has
resulted in human fatalities, those deaths were associated with the generation of highly toxic hydrofluoric acid and uranyl fluoride rather than with uranium itself. Finely divided uranium metal presents a fire hazard because uranium is pyrophoric; small grains will ignite spontaneously in air at room temperature.
Uranium metal is commonly handled with gloves as a sufficient precaution. Uranium concentrate is handled and contained so as to ensure that people do not inhale or ingest it.

</doc>
<doc id="31744" url="https://en.wikipedia.org/wiki?curid=31744" title="Ungulate">
Ungulate

Ungulates (pronounced ) are any members of a diverse clade of primarily large mammals that includes odd-toed ungulates such as horses and rhinoceroses, and even-toed ungulates such as cattle, pigs, giraffes, camels, deer, and hippopotamuses. Most terrestrial ungulates use the tips of their toes, usually hoofed, to sustain their whole body weight while moving. The term means, roughly, "being hoofed" or "hoofed animal". As a descriptive term, "ungulate" normally excludes cetaceans (whales, dolphins, porpoises), as they do not possess most of the typical morphological characteristics of ungulates, but recent discoveries indicate that they are descended from early artiodactyls. Ungulates are typically herbivorous (though some species are omnivorous, such as pigs), and many employ specialized gut bacteria to allow them to digest cellulose, as in the case of ruminants. They inhabit a wide range of habitats, from jungles to plains to rivers.
Classifications.
History.
Ungulata, which used to be considered an order, has been split into the following: Perissodactyla (odd-toed ungulates), Artiodactyla (even-toed ungulates), Tubulidentata (aardvarks), Hyracoidea (hyraxes), Sirenia (dugongs and manatees), Proboscidea (elephants) and occasionally Cetacea (whales and dolphins), as the fossil record seemed to connect them.
However, in 2009 morphological and molecular work has found that aardvarks, hyraxes, sea cows, and elephants are more closely related to sengis, tenrecs, and golden moles than to the perissodactyls and artiodactyls, and form Afrotheria. Elephants, sea cows, and hyraxes are grouped together in the clade Paenungulata, while the aardvark has been considered as either a close relative to them or a close relative to sengis in the clade Afroinsectiphilia. This is a striking example of convergent evolution.
There is now some dispute as to whether this smaller Ungulata is a cladistic (evolution-based) group, or merely a phenetic group (form taxon) or folk taxon (similar, but not necessarily related). Some studies have indeed found the mesaxonian ungulates and paraxonian ungulates to form a monophyletic lineage, closely related to either the Ferae (the carnivorans and the pangolins) in the clade Fereuungulata or to the bats. Other studies found the two orders not that closely related, as some place the perissodactyls as close relatives to bats and Ferae in Pegasoferae and others place the artiodactyls as close relatives to bats.
Taxonomy.
Below is a simplified taxonomy (assuming that ungulates do indeed form a natural grouping) with the extant families, in order of the relationships. Keep in mind that there are still some grey areas of conflict, such as the case with relationship of the pecoran families and the baleen whale families. See each family for the relationships of the species as well as the controversies in their respective article.
Phylogeny.
Below is the general consensus of the phylogeny of the ungulate families.
Evolution.
Perissodactyla and Artiodactyla include the majority of large land mammals. These two groups first appeared during the late Paleocene, rapidly spreading to a wide variety of species on numerous continents, and have developed in parallel since that time. Some scientists believed that modern ungulates are descended an evolutionary grade of mammals known as the condylarths; the earliest known member of the group was the tiny "Protungulatum", an ungulate that co-existed with the last of non-avian dinosaurs 66 million years ago; however, many authorities do not consider it a true placental, let alone an ungulate. The enigmatic dinoceratans were among the first large herbivorous mammals, although their exact relationship with other mammals is still debated with one of the theories being that they might just be distant relatives to living ungulates; the most recent study recovers them as within the true ungulate assemblage, closest to "Carodnia".
Perissodactyl evolution.
Perissodactyls are said to have evolved from the Phenacodontidae, small, sheep-sized animals that already shown signs of anatomical features that their descendents will inherit (the reduction of digit I and V for example). By the start of the Eocene, 55 million years ago (Mya), they had diversified and spread out to occupy several continents. Horses and tapirs both evolved in North America; rhinoceroses appear to have developed in Asia from tapir-like animals and then colonised the Americas during the middle Eocene (about 45 Mya). Of the approximately 15 families, only three survive (McKenna and Bell, 1997; Hooker, 2005). These families were very diverse in form and size; they included the enormous brontotheres and the bizarre chalicotheres. The largest perissodactyl, an Asian rhinoceros called "Paraceratherium", reached , more than twice the weight of an elephant.
It has been found in a cladistic study that the anthracobunids and the desmostylians - two lineages that have been previously classified as Afrotherians (more specifically closer to elephants) - have been classified as a clade that is closely related to the perissodactyls. The desmostylians were large amphibious quadrupeds with massive limbs and a short tail. They grew to in length and are thought to have weighed more than . Their fossils are known from the northern Pacific Rim, from southern Japan through Russia, the Aleutian Islands and the Pacific coast of North America to the southern tip of Baja California. Their dental and skeletal form suggests desmostylians were aquatic herbivores dependent on littoral habitats. Their name refers to their highly distinctive molars, in which each cusp was modified into hollow columns, so that a typical molar would have resembled a cluster of pipes, or in the case of worn molars, volcanoes. They are the only marine mammals to have gone extinct.
The South American meridiungulates, which contain the somewhat tapir-like pyrotheres and astrapotheres, the mesaxonic litopterns and the diverse notoungulates; As a whole, meridiungulates are said to have evolved from animals like "Hyopsodus". For a while their relationships with other ungulates was a mystery. Some paleontologists have even challenged the monophyly of Meridiungulata by suggesting that the pyrotheres may be more closely related to other mammals, such as Embrithopoda (an African order that are related to elephants) than to other South American ungulates. A recent study based on bone collagen as also found that to suggest that at least litopterns and the notoungulates were closely related to the perissodactyls.
The oldest known fossils assigned to Equidae date from the early Eocene, 54 million years ago. They had been assigned to the genus "Hyracotherium", but the type species of that genus is now considered not a member of this family, but the other species have been split off into different genera. These early Equidae were fox-sized animals with three toes on the hind feet, and four on the front feet. They were herbivorous browsers on relatively soft plants, and already adapted for running. The complexity of their brains suggest that they already were alert and intelligent animals. Later species reduced the number of toes, and developed teeth more suited for grinding up grasses and other tough plant food.
Rhinocerotoids diverged from other perissodactyls by the early Eocene. Fossils of "Hyrachyus eximus" found in North America date to this period. This small hornless ancestor resembled a tapir or small horse more than a rhino. Three families, sometimes grouped together as the superfamily Rhinocerotoidea, evolved in the late Eocene: Hyracodontidae, Amynodontidae and Rhinocerotidae, thus creating an explosion of diversity unmatched for a while until environmental changes drastically eliminated several species.
The first tapirids, such as "Heptodon", appeared in the early Eocene. They appeared very similar to modern forms, but were about half the size, and lacked the proboscis. The first true tapirs appeared in the Oligocene. By the Miocene, such genera as "Miotapirus" were almost indistinguishable from the extant species. Asian and American tapirs are believed to have diverged around 20 to 30 million years ago; and tapirs migrated from North America to South America around 3 million years ago, as part of the Great American Interchange.
Perissodactyls were the dominant group of large terrestrial browsers right through the Oligocene. However, the rise of grasses in the Miocene (about 20 Mya) saw a major change: the artiodactyl species with their more complex stomachs were better able to adapt to a coarse, low-nutrition diet, and soon rose to prominence. Nevertheless, many perissodactyl species survived and prospered until the late Pleistocene (about 10,000 years ago) when they faced the pressure of human hunting and habitat change.
Artiodactyl evolution.
The artiodactyls are thought to have evolved from a small group of condylarths, Arctocyonidae, which were unspecialized, superficially raccoon-like to bear-like omnivores from the Early Paleocene
(about 65 to 60 million years ago). They had relatively short limbs lacking specializations associated with their relatives (e.g. reduced side digits, fused bones, and hoofs), and long, heavy tails. Their primitive anatomy makes it unlikely that they were able to run down prey, but with their powerful proportions, claws, and long canines, they may have been able to overpower smaller animals in surprise attacks. Evidently these mammals soon evolved into two separate lineages: the mesonychians and the artiodactyls.
Mesonychians are depicted as "wolves on hooves" and were the first major mammalian predators, appearing in the Paleocene. Early mesonychids had five digits on their feet, which probably rested flat on the ground during walking (plantigrade locomotion), but later mesonychids had four digits that ended in tiny hoofs on all of their toes and were increasingly well adapted to running. Like running members of the even-toed ungulates, mesonychids ("Pachyaena", for example) walked on their digits (digitigrade locomotion). Mesonychians fared very poorly at the close of the Eocene epoch, with only one genus, "Mongolestes", surviving into the Early Oligocene epoch, as the climate changed and fierce competition arose from the better adapted creodonts.
The first artiodactyls looked like today's chevrotains or pigs: small, short-legged creatures that ate leaves and the soft parts of plants. By the Late Eocene (46 million years ago), the three modern suborders had already developed: Suina (the pig group); Tylopoda (the camel group); and Ruminantia (the goat and cattle group). Nevertheless, artiodactyls were far from dominant at that time: the perissodactyls were much more successful and far more numerous. Artiodactyls survived in niche roles, usually occupying marginal habitats, and it is presumably at that time that they developed their complex digestive systems, which allowed them to survive on lower-grade food. While most artiodactyls were taking over the niches left behind by several extinct perissodactyls, one lineage of artiodactyls began to venture out into the seas.
Cetacean evolution.
The traditional theory of cetacean evolution was that cetaceans were related to the mesonychids. These animals had unusual triangular teeth very similar to those of primitive cetaceans. This is why scientists long believed that cetaceans evolved from a form of mesonychid. Today many scientists believe cetaceans evolved from the same stock that gave rise to hippopotamuses. This hypothesized ancestral group likely split into two branches around . One branch would evolve into cetaceans, possibly beginning about with the proto-whale "Pakicetus" and other early cetacean ancestors collectively known as Archaeoceti, which eventually underwent aquatic adaptation into the completely aquatic cetaceans. The other branch became the anthracotheres, a large family of four-legged beasts, the earliest of whom in the late Eocene would have resembled skinny hippopotamuses with comparatively small and narrow heads. All branches of the anthracotheres, except that which evolved into Hippopotamidae, became extinct during the Pliocene without leaving any descendants. The family Raoellidae is said to be the closest artiodactyl family to the cetaceans. Consequentially, new theories in cetacean evolution hypothesize that whales and their ancestors escaped predation, not competition, by slowly adapting to the ocean.
Characteristics.
Ungulates are in high diversity in response to sexual selection and ecological events; the majority of ungulates lack a collar bone. Terrestrial ungulates are for the most part herbivores, with some of them being grazers. However, there are exceptions to this as pigs, peccaries, hippos and duikers are known to have an omnivorous diet. Some cetaceans are the only modern ungulates that are carnivores; baleen whales consume significantly smaller animals in relation to their body size, such as small species of fish and krill; toothed whales, depending on the species, can consume a wide range of species: squid, fish, sharks, and other species of mammals such as seals and other whales. In terms of ecosystem ungulates have colonized all corners of the planets, from mountains to the ocean depths; grasslands to deserts and have been domesticated by humans.
Anatomy.
Ungulates have developed specialized adaptations, especially in the areas of cranial appendages, dentition, and leg morphology including the modification of the astragalus (one of the ankle bones at the end of the lower leg) with a short, robust head.
Hooves.
The hoof is the tip of a toe of an ungulate mammal, strengthened by a thick horny (keratin) covering. The hoof consists of a hard or rubbery sole, and a hard wall formed by a thick nail rolled around the tip of the toe. The weight of the animal is normally borne by both the sole and the edge of the hoof wall. Hooves grow continuously, and are constantly worn down by use. In most modern ungulates, the radius and ulna are fused along the length of the forelimb; early ungulates, such as the arctocyonids, did not share this unique skeletal structure. The fusion of the radius and ulna prevents an ungulate from rotating its forelimb. Since this skeletal structure has no specific function in ungulates, it is considered a homologous characteristic that ungulates share with other mammals. This trait would have been passed down from a common ancestor. While the two orders of ungulates colloquial names are based on the number of toes of their members ("odd-toed" for the perissodactyls and "even-toed" for the terrestrial artiodactyls), it is not an accurate reason they are grouped. Tapirs have four toes in the front, yet they are members of the "odd-toed" order; peccaries and modern cetaceans are members of the "even-toed" order, yet peccaries have three toes in the front and whales are an extreme example as they have flippers instead of hooves. Scientists had classified them according to the distribution of their weight to their toes.
Perissodactyls have a mesaxonic foot meaning that the weight is distributed on the third toe on all legs thanks to the plane symmetry of their feet. It should be noted that there has been reduction of toes from the common ancestor, with the classic example being horses with their single hooves. In consequence, there was an alternative name for the perissodactyls the nearly obsolete Mesaxonia. Though it should be aware that the perissodactyls were not the only lineage of mammals to have evolved this trait as the meridiungulates had evolved mesaxonic feet numerous times.
Terrestrial artiodactyls have a paraxonic foot meaning that the weight is distributed on the third and the fourth toe on all legs. The majority of these mammals have cloven hooves, with two smaller ones known as the dewclaws that are located further up on the leg. The earliest cetaceans (the archaeocetes), also have this characteristic in the addition of also having both an astragalus and cuboid bone in the ankle, which are further diagnostic traits of artiodactyls.
In modern cetaceans, the front limbs have become pectoral fins and the hind parts are internal and reduced. Occasionally, the genes that code for longer extremities cause a modern cetacean to develop miniature legs (known as atavism). The main method of moving is an up-and-down motion with the tail fin, called the fluke, is used for propulsion, while the pectoral fins together with the entire tail section provide directional control. All modern cetaceans still retain their digits despite the external appearance suggesting otherwise.
Teeth.
Most ungulates have developed reduced canine teeth and specialized molars, including bunodont (low, rounded cusps) and hypsodont (high crowned) teeth. The development of hypsodonty has been of particular interest as this adaptation was strongly associated with the spread of grasslands during the Miocene about 25 million years. As forest biomes declined, grasslands spread, opening new niches for mammals. Many ungulates switched from browsing diets to grazing diets, and possibly driven by abrasive silica in grass, hypsodonty became common. However, recent evidence ties the evolution of hyspodonty to open, gritty habitats and not the grass itself. This is termed the Grit, not grass hypothesis.
Some ungulates completely lack upper incisors and instead have a dental pad to assist in browsing. It can be found in camels, ruminants, and some toothed whales; modern baleen whales are remarkable in that they have baleen instead to filter out the krill from the water. On the other spectrum teeth have been evolved as weapons or sexual display seen in pigs and peccaries, some species of deer, musk deer, hippopotamuses, beaked whales and the Narwhal, with its long canine tooth.
Cranial appendages.
Ungulates evolved a variety of cranial appendages that today can be found in cervoids (with the exception of musk deer). In oxen and antelope, the size and shape of the horns vary greatly, but the basic structure is always a pair of simple bony protrusions without branches, often having a spiral, twisted or fluted form, each covered in a permanent sheath of keratin. The unique horn structure is the only unambiguous morphological feature of bovids that distinguishes them from other pecorans. Male horn development has been linked to sexual selection, while the presence of horns in females is likely due to natural selection. The horns of females are usually smaller than those of males, and are sometimes of a different shape. The horns of female bovids are thought to have evolved for defense against predators or to express territoriality, as nonterritorial females, which are able to use crypsis for predator defense, often do not have horns.
Rhinoceros horns, unlike those of other horned mammals, only consist of keratin. The horns rest on the nasal ridge of the animals skull.
Antlers are unique to cervids and found mostly on males: only caribou and reindeer have antlers on the females, and these are normally smaller than those of the males. Nevertheless, fertile does from other species of deer have the capacity to produce antlers on occasion, usually due to increased testosterone levels. Each antler grows from an attachment point on the skull called a pedicle. While an antler is growing, it is covered with highly vascular skin called velvet, which supplies oxygen and nutrients to the growing bone. Antlers are considered one of the most exaggerated cases of male secondary sexual traits in the animal kingdom, and grow faster than any other mammal bone. Growth occurs at the tip, and is initially cartilage, which is mineralized to become bone. Once the antler has achieved its full size, the velvet is lost and the antler's bone dies. This dead bone structure is the mature antler. In most cases, the bone at the base is destroyed by osteoclasts and the antlers fall off at some point. As a result of their fast growth rate, antlers are considered a handicap since there is an incredible nutritional demand on deer to re-grow antlers annually, and thus can be honest signals of metabolic efficiency and food gathering capability.
Ossicones are horn-like (or antler-like) protuberances that can be found on the heads of giraffes and male okapis today. They are similar to the horns of antelopes and cattle, save that they are derived from ossified cartilage, and that the ossicones remain covered in skin and fur, rather than horn. Antlers (such as on deer) are derived from bone tissue: when mature, the skin and fur covering of the antlers, termed "velvet", is sloughed and scraped off to expose the bone of the antlers.
Pronghorn are unique when compared to their relatives. Each "horn" of the pronghorn is composed of a slender, laterally flattened blade of bone that grows from the frontal bones of the skull, forming a permanent core. As in the Giraffidae, skin covers the bony cores, but in the pronghorn it develops into a keratinous sheath which is shed and regrown on an annual basis. Unlike the horns of the family Bovidae, the horn sheaths of the pronghorn are branched, each sheath possessing a forward-pointing tine (hence the name pronghorn). The horns of males are well developed.
Inbreeding in small populations.
Many of the world’s ungulate species exist only in relatively small populations in which some degree of inbreeding inevitably occurs. A study of 16 species of captive ungulates revealed that juvenile survival of inbred young is generally lower than that of non-inbred young. (Also see Inbreeding depression). These findings have implications for the genetic management of small ungulate populations.

</doc>
<doc id="31745" url="https://en.wikipedia.org/wiki?curid=31745" title="Udo of Aachen">
Udo of Aachen

Udo of Aachen (c.1200–1270) is a fictional monk, a creation of British technical writer Ray Girvan, who introduced him in an April Fool's hoax article in 1999. According to the article, Udo was an illustrator and theologian who discovered the Mandelbrot set some 700 years before Benoît Mandelbrot.
Additional details of the hoax include the rediscovery of Udo's works by the also-fictional Bob Schipke, a Harvard mathematician, who supposedly saw a picture of the Mandelbrot set in an illumination for a 13th-century carol. Girvan also attributed Udo as a mystic and poet whose poetry was set to music by Carl Orff with the haunting "O Fortuna" in Carmina Burana.
Aspects of the hoax.
The poetry of "O Fortuna" was actually the work of itinerant goliards, found in the German Benedictine monastery of Benediktbeuern Abbey.
The hoax was lent an air of credibility because often medieval monks did discover scientific and mathematical theories, only to have them hidden or shelved due to persecution or simply ignored because publication prior to the invention of the printing press was difficult at best. Mr. Girvan adds to this suggestion by associating Udo with several other more legitimate discoveries where an author was considered ahead of his time in terms of a scientific theory of some sort that is now established as a mainstream theory but was considered fringe science at the time.
Another aspect of the deception was that it was very common for pre-20th century mathematicians to spend incredible amounts of time on hand calculations such as a logarithm table or trigonometric functions. Calculating all of the points for a Mandelbrot set is a comparable activity that would seem tedious today but would be routine for people of the time.

</doc>
<doc id="31748" url="https://en.wikipedia.org/wiki?curid=31748" title="Ultra">
Ultra

Ultra was the designation adopted by British military intelligence in June 1941 for wartime signals intelligence obtained by breaking high-level encrypted enemy radio and teleprinter communications at the Government Code and Cypher School (GC&CS) at Bletchley Park. "Ultra" eventually became the standard designation among the western Allies for all such intelligence. The name arose because the intelligence thus obtained was considered more important than that designated by the highest British security classification then used ("Most Secret") and so was regarded as being "Ultra secret". Several other cryptonyms had been used for such intelligence. British intelligence first designated it "Boniface"—presumably to imply that it was the result of human intelligence. The U.S. used the codename "Magic" for its decrypts from Japanese sources.
Much of the German cipher traffic was encrypted on the Enigma machine. Used properly, the German military Enigma would have been virtually unbreakable; in practice, shortcomings in operation allowed it to be broken. The term "Ultra" has often been used almost synonymously with "Enigma decrypts". However, Ultra also encompassed decrypts of the German Lorenz SZ 40/42 machines that were used by the German High Command, and the Hagelin machine
Many observers, at the time and later, regarded Ultra as immensely valuable to the Allies.
Winston Churchill was reported to have told King George VI, when presenting to him Stewart Menzies (head of the Secret Intelligence Service and the person who controlled distribution of Ultra decrypts to the government): "It is thanks to the secret weapon of General Menzies, put into use on all the fronts, that we won the war!" F. W. Winterbotham quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at war's end describing Ultra as having been "decisive" to Allied victory. Sir Harry Hinsley, Bletchley Park veteran and official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war "by not less than two years and probably by four years"; and that, in the absence of Ultra, it is uncertain how the war would have ended.
Since Ultra was revealed in the middle 1970s, historians have altered the historiography of World War II. For example, Andrew Roberts, writing in the 21st century, states, "Because he had the invaluable advantage of being able to read Rommel's Enigma communications, Montgomery knew how short the Germans were of men, ammunition, food and above all fuel. When he put Rommel's picture up in his caravan he wanted to be seen to be almost reading his opponent's mind. In fact he was reading his mail".
Sources of intelligence.
Most Ultra intelligence was derived from reading radio messages that had been encrypted with cipher machines, complemented by material from radio communications using traffic analysis and direction finding. In the early phases of the war, particularly during the eight-month Phoney War, the Germans could transmit most of their messages using land lines and so had no need to use radio. This meant that those at Bletchley Park had some time to build up experience of collecting and starting to decrypt messages on the various radio networks. German Enigma messages were the main source, with those of the Luftwaffe predominating, as they used radio more and their operators were particularly ill-disciplined.
German.
Enigma.
"Enigma" refers to a family of electro-mechanical rotor cipher machines. These produced a polyalphabetic substitution cipher and were widely thought to be unbreakable in the 1920s, when a variant of the commercial Model D was first used by the Reichswehr. The German Army, Navy, Air Force, Nazi party, Gestapo and German diplomats used Enigma machines in several variants. Abwehr (German military intelligence) used a four-rotor machine without a plugboard and Naval Enigma used different key management from that of the army or air force, making its traffic far more difficult to cryptanalyse; each variant required different cryptanalytic treatment. The commercial versions were not as secure and Dilly Knox of GC&CS, is said to have broken one before the war.
German military Enigma was first broken in December 1932 by the Polish Cipher Bureau, using a combination of brilliant mathematics, the services of a spy in the German office responsible for administering encrypted communications, and good luck. The Poles read Enigma to the outbreak of World War II and beyond, in France. At the turn of 1939, the Germans made the systems ten times more complex, which required a tenfold increase in Polish decryption equipment, which they could not meet. On 25 July 1939, the Polish Cipher Bureau handed reconstructed Enigma machines and their techniques for decrypting ciphers to the French and British. Gordon Welchman wrote,
After the war, interrogation of German cryptographic personnel, led to the conclusion that German cryptanalysts understood that cryptanalytic attacks against Enigma were possible but were thought to require impracticable amounts of effort and investment. The Poles' early start at breaking Enigma and the continuity of their success, gave the Allies an advantage when World War II began.
Lorenz cipher.
In June 1941, the Germans started to introduce on-line stream cipher teleprinter systems for strategic point-to-point radio links, to which the British gave the code-name Fish. Several systems were used, principally the Lorenz SZ 40/42 (Tunny) and Geheimfernschreiber (Sturgeon). These cipher systems were cryptanalysed, particularly Tunny, which the British thoroughly penetrated. It was eventually attacked using Colossus, which were the first digital programme-controlled electronic computers. Although the volume of intelligence derived from this system was much smaller than that from Enigma, its importance was high because it produced primarily strategic intelligence.
Italian.
In June 1940, the Italians were using book codes for most of their military messages, except for the Italian Navy which, in early 1941 had started using a version of the Hagelin rotor-based cipher machine C-38. This was broken from June 1941 onwards by the Italian subsection of GC&CS at Bletchley Park.
Japanese.
In the Pacific theatre, a Japanese cipher machine called "Purple" by the Americans, was used for highest-level Japanese diplomatic traffic. It produced a polyalphabetic substitution cipher, but unlike Enigma, was not a rotor machine, being built around electrical stepping switches. It was broken by the US Army Signal Intelligence Service and disseminated as "MAGIC". Detailed reports by the Japanese ambassador to Germany were encrypted on the Purple machine. His reports included reviews of Germany assessments of the military situation, of strategy and intentions, reports on direct inspections (in one case, of Normandy beach defences) by the ambassador and reports of long interviews with Hitler.
The chief fleet communications code system used by the Imperial Japanese Navy was called JN-25 by the Americans and by early 1942, they had made considerable progress in decrypting Japanese naval messages. The Japanese are said to have obtained an Enigma machine in 1937, although it is debated whether they were given it by the Germans or bought a commercial version which apart from the plugboard and internal wirings, was the German "Heer/Luftwaffe" machine. The Japanese did not use it for their most secret communications, having developed a similar machine.
Distribution.
Army- and air force-related intelligence derived from signals intelligence (SIGINT) sources—mainly Enigma decrypts in Hut 6—was compiled in summaries at GC&CS (Bletchley Park) Hut 3 and distributed initially under the codeword "BONIFACE", implying that it was acquired from a well placed agent in Berlin. The volume of the intelligence reports going out to commanders in the field built up gradually. Naval Enigma decoded in Hut 8 was forwarded from Hut 4 to the Admiralty Operational Intelligence Centre (OIC), which were distributed initially under the codeword "HYDRO". The codeword "ULTRA" was adopted in June 1941. This codeword was reportedly suggested by Commander Geoffrey Colpoys, RN, who served in the RN OIC.
Army and air force.
The distribution of Ultra information to Allied commanders and units in the field involved considerable risk of discovery by the Germans, and great care was taken to control both the information and knowledge of how it was obtained. Liaison officers were appointed for each field command to manage and control dissemination.
Dissemination of Ultra intelligence to field commanders was carried out by MI6, which operated Special Liaison Units (SLU) attached to major army and air force commands. The activity was organized and supervised on behalf of MI6 by Group Captain F. W. Winterbotham. Each SLU included intelligence, communications, and cryptographic elements. It was headed by a British Army or RAF officer, usually a major, known as "Special Liaison Officer". The main function of the liaison officer or his deputy was to pass Ultra intelligence bulletins to the commander of the command he was attached to, or to other indoctrinated staff officers. In order to safeguard Ultra, special precautions were taken. The standard procedure was for the liaison officer to present the intelligence summary to the recipient, stay with him while he studied it, then take it back and destroy it.
By the end of the war, there were about 40 SLUs serving commands around the world. Fixed SLUs existed at the Admiralty, the War Office, the Air Ministry, RAF Fighter Command, the US Strategic Air Forces in Europe (Wycombe Abbey) and other fixed headquarters in the UK. An SLU was operating at the War HQ in Valletta, Malta. These units had permanent teleprinter links to Bletchley Park.
Mobile SLUs were attached to field army and air force headquarters, and depended on radio communications to receive intelligence summaries. The first mobile SLUs appeared during the French campaign of 1940. An SLU supported the British Expeditionary Force (BEF) headed by General Lord Gort. The first liaison officers were Robert Gore-Browne and Humphrey Plowden. A second SLU of the 1940 period was attached to the RAF Advanced Air Striking Force at Meaux commanded by Air Vice-Marshal P H Lyon Playfair. This SLU was commanded by Squadron Leader F.W. "Tubby" Long.
Intelligence agencies.
In 1940, special arrangements were made within the British intelligence services for handling BONIFACE and later Ultra intelligence. The Security Service started "Special Research Unit B1(b)" under Herbert Hart. In the SIS this intelligence was handled by "Section V" based at St Albans.
Radio and cryptography.
The communications system was founded by Brigadier Sir Richard Gambier-Parry, who from 1938 to 1946 was head of MI6 Section VIII, based at Whaddon Hall in Buckinghamshire, UK. Ultra summaries from Bletchley Park were sent over landline to the Section VIII radio transmitter at Windy Ridge. From there they were transmitted to the destination SLUs.
The communications element of each SLU was called a "Special Communications Unit" or SCU. Radio transmitters were constructed at Whaddon Hall workshops, while receivers were the National HRO, made in the USA. The SCUs were highly mobile and the first such units used civilian Packard cars. The following SCUs are listed: SCU1 (Whaddon Hall), SCU2 (France before 1940, India), SCU3 (RSS Hanslope Park), SCU5, SCU6 (possibly Algiers and Italy), SCU7 (training unit in the UK), SCU8 (Europe after D-day), SCU9 (Europe after D-day), SCU11 (Palestine and India), SCU12 (India), SCU13 and SCU14.
The cryptographic element of each SLU was supplied by the RAF and was based on the TYPEX cryptographic machine and one-time pad systems.
RN Ultra messages from the OIC to ships at sea were necessarily transmitted over normal naval radio circuits and were protected by one-time pad encryption.
Lucy.
An intriguing question concerns the alleged use of Ultra information by the "Lucy" spy ring, headquartered in Switzerland and apparently operated by one man, Rudolf Roessler. This was an extremely well informed, responsive ring that was able to get information "directly from German General Staff Headquarters" – often on specific request. It has been alleged that "Lucy" was in major part a conduit for the British to feed Ultra intelligence to the Soviets in a way that made it appear to have come from highly placed espionage rather than from cryptanalysis of German radio traffic. The Soviets, however, through an agent at Bletchley, John Cairncross, knew that Britain had broken Enigma. The "Lucy" ring was initially treated with suspicion by the Soviets. The information that it provided was accurate and timely, however, and Soviet agents in Switzerland (including their chief, Alexander Radó) eventually learned to take it seriously.
Use of intelligence.
Most deciphered messages, often about relative trivia, were not alone sufficient as intelligence reports for military strategists or field commanders. The organisation, interpretation and distribution of intelligence derived from messages from Enigma transmissions and other sources into intelligence was a subtle business. This was not recognised by the Americans before the attack on Pearl Harbor, but was learnt very quickly afterwards.
At Bletchley Park, extensive indexes were kept of the information in the messages decrypted. For each message the traffic analysis recorded the radio frequency, the date and time of intercept, and the preamble—which contained the network-identifying discriminant, the time of origin of the message, the callsign of the originating and receiving stations, and the indicator setting. This allowed cross referencing of a new message with a previous one. The indexes included message preambles, every person, every ship, every unit, every weapon, every technical term and of repeated phrases such as forms of address and other German military jargon that might be usable as "cribs".
The first decryption of a wartime Enigma message was achieved by the Poles at PC Bruno on 17 January 1940, albeit one that had been transmitted three months earlier. Little had been achieved by the start of the Allied campaign in Norway in April. At the start of the Battle of France on 10 May 1940, the Germans made a very significant change in the indicator procedures for Enigma messages. However, the Bletchley Park cryptanalysts had anticipated this, and were able—jointly with PC Bruno—to resume breaking messages from 22 May, although often with some delay. The intelligence that these messages yielded was of little operational use in the fast-moving situation of the German advance.
Decryption of Enigma traffic built up gradually during 1940, with the first two prototype bombes being delivered in March and August. The traffic was almost entirely limited to "Luftwaffe" messages. By the peak of the Battle of the Mediterranean in 1941, however, Bletchley Park was deciphering daily 2,000 Italian Hagelin messages. By the second half of 1941 30,000 Enigma messages a month were being deciphered, rising to 90,000 a month of Enigma and Fish decrypts combined later in the war.
Some of the contributions that Ultra intelligence made to the Allied successes are given below.
Safeguarding of sources.
The Allies were seriously concerned with the prospect of the Axis command finding out that they had broken into the Enigma traffic. The British were more disciplined about such measures than the Americans, and this difference was a source of friction between them. It was a little bit of a joke that in Delhi, the British Ultra unit was based in a large wooden hut in the grounds of Government House. Security consisted of a wooden table flat across the door with a bell on it and a sergeant sat there. This hut was ignored by all. The American unit was in a large brick building, surrounded by barbed wire and armed patrols. People may not have known what was in there, but they surely knew it was something important and secret.
To disguise the source of the intelligence for the Allied attacks on Axis supply ships bound for North Africa, "spotter" submarines and aircraft were sent to search for Axis ships. These searchers or their radio transmissions were observed by the Axis forces, who concluded their ships were being found by conventional reconnaissance. They suspected that there were some 400 Allied submarines in the Mediterranean and a huge fleet of reconnaissance aircraft on Malta. In fact, there were only 25 submarines and at times as few as three aircraft.
This procedure also helped conceal the intelligence source from Allied personnel, who might give away the secret by careless talk, or under interrogation if captured. Along with the search mission that would find the Axis ships, two or three additional search missions would be sent out to other areas, so that crews would not begin to wonder why a single mission found the Axis ships every time.
Other deceptive means were used. On one occasion, a convoy of five ships sailed from Naples to North Africa with essential supplies at a critical moment in the North African fighting. There was no time to have the ships properly spotted beforehand. The decision to attack solely on Ultra intelligence went directly to Churchill. The ships were all sunk by an attack "out of the blue", arousing German suspicions of a security breach. To distract the Germans from the idea of a signals breach (such as Ultra), the Allies sent a radio message to a fictitious spy in Naples, congratulating him for this success. According to some sources the Germans decrypted this message and believed it.
In the Battle of the Atlantic, the precautions were taken to the extreme. In most cases where the Allies knew from intercepts the location of a U-boat in mid-Atlantic, the U-boat was not attacked immediately, until a "cover story" could be arranged. For example, a search plane might be "fortunate enough" to sight the U-boat, thus explaining the Allied attack.
Some Germans had suspicions that all was not right with Enigma. Admiral Karl Dönitz received reports of "impossible" encounters between U-boats and enemy vessels which made him suspect some compromise of his communications. In one instance, three U-boats met at a tiny island in the Caribbean Sea, and a British destroyer promptly showed up. The U-boats escaped and reported what had happened. Dönitz immediately asked for a review of Enigma's security. The analysis suggested that the signals problem, if there was one, was not due to the Enigma itself. Dönitz had the settings book changed anyway, blacking out Bletchley Park for a period. However, the evidence was never enough to truly convince him that Naval Enigma was being read by the Allies. The more so, since "B-Dienst", his own codebreaking group, had partially broken Royal Navy traffic (including its convoy codes early in the war), and supplied enough information to support the idea that the Allies were unable to read Naval Enigma.
By 1945, most German Enigma traffic could be decrypted within a day or two, yet the Germans remained confident of its security. Had they known better, they could have changed systems, forcing Allied cryptanalysts to start again.
Effect on the war.
The exact influence of Ultra on the course of the war is debated; an oft-repeated assessment is that decryption of German ciphers advanced the end of the European war by two years. Hinsley is often cited as an authority for the two-year estimate, yet his assessment in "Codebreakers" is not specific:
Winterbotham's quoting of Eisenhower's "decisive" verdict is part of a letter sent by Eisenhower to Menzies after the conclusion of the European war and later found among his papers at the Eisenhower Presidential Library. It allows a contemporary, documentary view of a leader on Ultra's importance:
Postwar disclosures.
While it is obvious why Britain and the U.S. went to considerable pains to keep Ultra a secret until the end of the war, it has been a matter of some conjecture why Ultra was kept officially secret for 29 years thereafter, until 1974. During that period the important contributions to the war effort of a great many people remained unknown, and they were unable to share in the glory of what is likely one of the chief reasons the Allies won the war – or, at least, as quickly as they did.
At least three versions exist as to why Ultra was kept secret so long. Each has plausibility, and all may be true. First, as David Kahn pointed out in his 1974 "New York Times" review of Winterbotham's "The Ultra Secret", after World War II the British gathered up all the Enigma machines they could find and sold them to Third World countries, confident that they could continue reading the messages of the machines' new owners.
A second explanation relates to a misadventure of Churchill's between the World Wars, when he publicly disclosed information from decrypted Soviet communications. This had prompted the Soviets to change their ciphers, leading to a blackout.
The third explanation is given by Winterbotham, who recounts that two weeks after V-E Day, on 25 May 1945, Churchill requested former recipients of Ultra intelligence not to divulge the source or the information that they had received from it, in order that there be neither damage to the future operations of the Secret Service nor any cause for the Axis to blame Ultra for their defeat. Since it was British and, later, American message-breaking which had been the most extensive, this meant that the importance of Enigma decrypts to the prosecution of the war remained unknown. Discussion by either the Poles or the French of Enigma breaks carried out early in the war would have been uninformed regarding breaks carried out during the balance of the war. Nevertheless, the 1973 public disclosure of Enigma decryption in the book "Enigma" by French intelligence officer Gustave Bertrand generated pressure to discuss the rest of the Enigma–Ultra story.
The British ban was finally lifted in 1974, the year that a key participant on the distribution side of the Ultra project, F. W. Winterbotham, published "The Ultra Secret".
The official history of British intelligence in World War II was published in five volumes from 1979 to 1988. It was chiefly edited by Harry Hinsley, with one volume by Michael Howard. There is also a one-volume collection of reminiscences by Ultra veterans, "Codebreakers" (1993), edited by Hinsley and Alan Stripp.
After the war, surplus Enigmas and Enigma-like machines were sold to Third World countries, which remained convinced of the security of the remarkable cipher machines. Their traffic was not as secure as they believed, however, which is one reason the British made the machines available. Switzerland even developed its own version of Enigma, known as NEMA, and used it into the late 1970s.
Some information about Enigma decryption did get out earlier, however. In 1967, Polish military historian Władysław Kozaczuk in his book "Bitwa o tajemnice" ("Battle for Secrets") first revealed Enigma had been broken by Polish cryptologists before World War II. The same year, David Kahn in "The Codebreakers" described the 1944 capture of a Naval Enigma machine from "U-505" and noted, somewhat in passing, naval Enigma messages were already being read.
Ladislas Farago's 1971 best-seller "The Game of the Foxes" gave an early garbled version of the myth of the purloined Enigma. According to Farago, it was thanks to a "Polish-Swedish ring the British obtained a working model of the 'Enigma' machine, which the Germans used to encipher their top-secret messages." "It was to pick up one of these machines that Commander Denniston went clandestinely to a secluded Polish castle [!] on the eve of the war. Dilly Knox later solved its keying, exposing all Abwehr signals encoded by this system." "In 1941 he brilliant cryptologist Dillwyn Knox, working at the Government Code & Cypher School at the Bletchley centre of British code-cracking, solved the keying of the Abwehr's Enigma machine."
By 1970, newer, computer-based ciphers were becoming popular as the world increasingly turned to computerised communications, and the usefulness of Enigma copies (and rotor machines generally) rapidly decreased. It was shortly after this, in 1974, that a decision was taken to permit revelations about some Bletchley Park operations.
The United States National Security Agency (NSA) retired the last of its rotor-based encryption systems, the KL-7 series, in the 1980s.
A 2012 London Science Museum exhibit, "Code Breaker: Alan Turing's Life and Legacy", marking the centenary of his birth, includes a short film of statements by half a dozen participants and historians of the World War II Bletchley Park Ultra operations. John Agar, a historian of science and technology, states that by war's end 8,995 people worked at Bletchley Park. Iain Standen, Chief Executive of the Bletchley Park Trust, says of the work done there: "It was crucial to the survival of Britain, and indeed of the West." The Departmental Historian at GCHQ (the Government Communications Headquarters), who identifies himself only as "Tony" but seems to speak authoritatively, says that Ultra was a "major force multiplier. It was the first time that quantities of real-time intelligence became available to the British military." He further states that it is only in 2012 that Alan Turing's last two papers on Enigma decryption have been released to Britain's National Archives; the seven decades' delay had been due to their "continuing sensitivity... It wouldn't have been safe to release earlier."
Holocaust intelligence.
Historians and holocaust researchers have tried to establish when the Allies recognized the full extent of Nazi-era extermination of Jews, and specifically, the extermination-camp system. In 1999, the U.S. Government passed the Nazi War Crimes Disclosure Act (P.L. 105-246), making it policy to declassify all Nazi war crime documents in their files; this was later amended to include the Japanese Imperial Government. As a result, more than 600 decrypts and translations of intercepted messages were disclosed; NSA historian Robert Hanyok would conclude that Allied communications intelligence, "by itself, could not have provided an early warning to Allied leaders regarding the nature and scope of the holocaust."
Following Operation Barbarossa, decrypts in August 1941 alerted British authorities to the many massacres in occupied zones of the Soviet Union, including those of Jews, but specifics were not made public for security reasons. Revelations about the concentration camps were gleaned from other sources, and were publicly reported by the Polish government-in-exile, Jan Karski and the WJC offices in Switzerland a year or more later. A decrypted message referring to "Einsatz Reinhard" (the Höfle Telegram), from January 11, 1943, may have outlined the system and listed the number of Jews and others gassed at four death camps the previous year, but codebreakers did not understand the meaning of the message. In summer 1944, Arthur Schlesinger, an OSS analyst, interpreted the intelligence as an "incremental increase in persecution rather than... extermination."
Postwar consequences.
There has been controversy about the influence of Allied Enigma decryption on the course of World War II. It has also been suggested that the question should be broadened to include Ultra's influence not only on the war itself, but also on the post-war period.
F. W. Winterbotham, the first author to outline the influence of Enigma decryption on the course of World War II, likewise made the earliest contribution to an appreciation of Ultra's "postwar" influence, which now continues into the 21st Century — and not only in the postwar establishment of Britain's GCHQ (Government Communication Headquarters) and America's NSA. "Let no one be fooled," Winterbotham admonishes in chapter 3, "by the spate of television films and propaganda which has made the war seem like some great triumphant epic. It was, in fact, a very narrow shave, and the reader may like to ponder [...] whether [...] we might have won Ultra."
Debate continues on whether, had postwar political and military leaders been aware of Ultra's role in Allied victory in World War II, these leaders might have been less optimistic about post-World War II military involvements.
Knightley suggests that Ultra may have contributed to the development of the Cold War. The Soviets received disguised Ultra information, but the existence of Ultra itself was not disclosed by the western Allies. The Soviets, who had clues to Ultra's existence, possibly through Kim Philby and Anthony Blunt, may thus have felt still more distrustful of their wartime partners.
The mystery surrounding the discovery of the sunk U-869 off the coast of New Jersey by divers Richie Kohler and John Chatterton was unraveled in part through the analysis of Ultra intercepts, which demonstrated that, although U-869 had been ordered by U-boat Command to change course and proceed to North Africa, near Rabat, the submarine had missed the messages changing her assignment and had continued to the eastern coast of the U.S., her original destination.

</doc>
<doc id="31750" url="https://en.wikipedia.org/wiki?curid=31750" title="Ukraine">
Ukraine

Ukraine (; , tr. ) is a sovereign country in Eastern Europe, bordered by Russia to the east and northeast, Belarus to the northwest, Poland and Slovakia to the west, Hungary, Romania, and Moldova to the southwest, and the Black Sea and Sea of Azov to the south and southeast, respectively. Ukraine is currently in territorial dispute with Russia over the Crimean Peninsula which Russia annexed in 2014 but which Ukraine and most of the international community recognise as Ukrainian. Including Crimea, Ukraine has an area of , making it the largest country entirely within Europe and the 46th largest country in the world, and a population of about 44.5 million, making it the 32nd most populous country in the world.
The territory of modern Ukraine has been inhabited since 32,000 BC. During the Middle Ages, the area was a key centre of East Slavic culture, with the powerful state of Kievan Rus' forming the basis of Ukrainian identity. Following its fragmentation in the 13th century, the territory was contested, ruled and divided by a variety of powers, including Lithuania, Poland, the Ottoman Empire, Austria-Hungary, and Russia. A Cossack republic emerged and prospered during the 17th and 18th centuries, but its territory was eventually split between Poland and the Russian Empire, and later submerged fully into Russia. Two brief periods of independence occurred during the 20th century, once near the end of World War I and another during World War II, but both occasions would ultimately see Ukraine's territories conquered and consolidated into a Soviet republic, a situation that persisted until 1991, when Ukraine gained its independence from the Soviet Union in the aftermath of its dissolution at the end of the Cold War.
Following independence, Ukraine declared itself a neutral state, but nonetheless formed a limited military partnership with the Russian Federation, other CIS countries and a partnership with NATO since 1994. In the 2000s, the government began leaning towards NATO, and a deeper cooperation with the alliance was set by the NATO-Ukraine Action Plan signed in 2002. It was later agreed that the question of joining NATO should be answered by a national referendum at some point in the future. Former President Viktor Yanukovych considered the current level of co-operation between Ukraine and NATO sufficient, and was against Ukraine joining NATO. In 2013, protests against the government of President Yanukovych broke out in downtown Kiev after the government made the decision to suspend the Ukraine-European Union Association Agreement and seek closer economic ties with Russia. This began a several-months-long wave of demonstrations and protests known as the Euromaidan, which later escalated into the 2014 Ukrainian revolution that ultimately resulted in the overthrowing of Yanukovych and the establishment of a new government. These events precipitated the annexation of Crimea by Russia in March 2014, and the War in Donbass in March 2014; both are still ongoing . On 1 January 2016, Ukraine joined the Deep and Comprehensive Free Trade Area with the European Union.
Ukraine has long been a global breadbasket because of its extensive, fertile farmlands, and it remains one of the world's largest grain exporters. The diversified economy of Ukraine includes a large heavy industry sector, particularly in aerospace and industrial equipment.
Ukraine is a unitary republic under a semi-presidential system with separate powers: legislative, executive, and judicial branches. Its capital and largest city is Kiev. Ukraine maintains the second-largest military in Europe, after that of Russia, when reserves and paramilitary personnel are taken into account. The country is home to 45.4 million people (including Crimea), 77.8% of whom are Ukrainians by ethnicity, followed by a sizeable minority of Russians (17.3%) as well as Romanians/Moldovans, Belarusians, Crimean Tatars, and Hungarians. Ukrainian is the official language of Ukraine; its alphabet is Cyrillic. The dominant religion in the country is Eastern Orthodoxy, which has strongly influenced Ukrainian architecture, literature and music.
Etymology.
There are different hypotheses as to the etymology of the name "Ukraine". According to the older and most widespread hypothesis, it means "borderland", while more recently some linguistic studies claim a different meaning: "homeland" or "region, country". "The Ukraine" was once the usual form in English but since the Declaration of Independence of Ukraine, "the Ukraine" has become much less common in the English-speaking world, and style-guides largely recommend not using the definite article.
History.
Early history.
Neanderthal settlement in Ukraine is seen in the Molodova archaeological sites (43,000–45,000 BC) which include a mammoth bone dwelling. The territory is also considered to be the likely location for the human domestication of the horse.
Modern human settlement in Ukraine and its vicinity dates back to 32,000 BC, with evidence of the Gravettian culture in the Crimean Mountains. By 4,500 BC, the Neolithic Cucuteni-Trypillian Culture flourished in a wide area that included parts of modern Ukraine including Trypillia and the entire Dnieper-Dniester region. During the Iron Age, the land was inhabited by Cimmerians, Scythians, and Sarmatians. Between 700 BC and 200 BC it was part of the Scythian Kingdom, or Scythia.
Later, colonies of Ancient Greece, Ancient Rome and the Byzantine Empire, such as Tyras, Olbia and Chersonesus, were founded, beginning in the 6th century BC, on the northeastern shore of the Black Sea, and thrived well into the 6th century AD. The Goths stayed in the area but came under the sway of the Huns from the 370s AD. In the 7th century AD, the territory of eastern Ukraine was the centre of Old Great Bulgaria. At the end of the century, the majority of Bulgar tribes migrated in different directions, and the Khazars took over much of the land.
Golden Age of Kiev.
The Kievan Rus' was founded by the Rus' people, who settled around Ladoga and Novgorod, then gradually moved southward eventually reaching Kiev about 880. Kievan Rus' included the western part of modern Ukraine, and Belarus. The larger part was on the territory of the modern Russian Federation. According to the "Primary Chronicle" the Rus' elite initially consisted of Varangians from Scandinavia.
During the 10th and 11th centuries, it became the largest and most powerful state in Europe. It laid the foundation for the national identity of Ukrainians and Russians. Kiev, the capital of modern Ukraine, became the most important city of the Rus'.
The Varangians later assimilated into the Slavic population and became part of the first Rus' dynasty, the Rurik Dynasty. Kievan Rus' was composed of several principalities ruled by the interrelated Rurikid "knyazes" ("princes"), who often fought each other for possession of Kiev.
The Golden Age of Kievan Rus' began with the reign of Vladimir the Great (980–1015), who turned Rus' toward Byzantine Christianity. During the reign of his son, Yaroslav the Wise (1019–1054), Kievan Rus' reached the zenith of its cultural development and military power. The state soon fragmented as the relative importance of regional powers rose again. After a final resurgence under the rule of Vladimir II Monomakh (1113–1125) and his son Mstislav (1125–1132), Kievan Rus' finally disintegrated into separate principalities following Mstislav's death.
The 13th century Mongol invasion devastated Kievan Rus'. Kiev was totally destroyed in 1240. On today's Ukrainian territory, the principalities of Halych and Volodymyr-Volynskyi arose, and were merged into the state of Galicia-Volhynia.
Danylo Romanovych (Daniel I of Galicia or Danylo Halytskyi) son of Roman Mstyslavych, re-united all of south-western Rus', including Volhynia, Galicia and Rus' ancient capital of Kiev. Danylo was crowned by the papal archbishop in Dorohychyn 1253 as the first King of all Rus'. Under Danylo's reign, the Kingdom of Galicia–Volhynia was one of the most powerful states in east central Europe.
Foreign domination.
In the mid-14th century, upon the death of Bolesław Jerzy II of Mazovia, king Casimir III of Poland initiated campaigns (1340–1366) to take Galicia-Volhynia. Meanwhile, the heartland of Rus', including Kiev, became the territory of the Grand Duchy of Lithuania, ruled by Gediminas and his successors, after the Battle on the Irpen' River. Following the 1386 Union of Krewo, a dynastic union between Poland and Lithuania, much of what became northern Ukraine was ruled by the increasingly Slavicised local Lithuanian nobles as part of the Grand Duchy of Lithuania, and by 1392 the so-called Galicia–Volhynia Wars ended. Polish colonisers of depopulated lands in northern and central Ukraine founded or re-founded many towns. In 1430 Podolia was incorporated under the Crown of the Kingdom of Poland as Podolian Voivodeship. In 1441, in the southern Ukraine, especially Crimea and surrounding steppes, Genghisid prince Haci I Giray founded the Crimean Khanate.
In 1569 the Union of Lublin established the Polish–Lithuanian Commonwealth, and much Ukrainian territory was transferred from Lithuania to the Crown of the Kingdom of Poland, becoming Polish territory de jure. Under the demographic, cultural and political pressure of Polonisation begun already in the late 14th century, many landed gentry of Polish Ruthenia (another name for the land of Rus) converted to Catholicism and became indistinguishable from the Polish nobility. Deprived of native protectors among Rus nobility, the commoners (peasants and townspeople) began turning for protection to the emerging Zaporozhian Cossacks, who by the 17th century became devoutly Orthodox. The Cossacks did not shy from taking up arms against those they perceived as enemies, including the Polish state and its local representatives.
Formed from Golden Horde territory conquered after the Mongol invasion the Crimean Khanate was one of the strongest powers in Eastern Europe until the 18th century; in 1571 it even captured and devastated Moscow. The borderlands suffered annual Tatar invasions. From the beginning of the 16th century until the end of the 17th century, Crimean Tatar slave raiding bands exported about 2 million slaves from Russia and Ukraine. According to Orest Subtelny, "from 1450 to 1586, eighty-six Tatar raids were recorded, and from 1600 to 1647, seventy." In 1688, Tatars captured a record number of 60,000 Ukrainians. The Tatar raids took a heavy toll, discouraging settlement in more southerly regions where the soil was better and the growing season was longer. The last remnant of the Crimean Khanate was finally conquered by the Russian Empire in 1783. The Taurida Governorate was formed to govern this territory.
In the mid-17th century, a Cossack military quasi-state, the Zaporozhian Host, was formed by Dnieper Cossacks and by Ruthenian peasants who had fled Polish serfdom. Poland exercised little real control over this population, but found the Cossacks to be a useful opposing force to the Turks and Tatars, and at times the two were allies in military campaigns. However the continued harsh enserfment of peasantry by Polish nobility and especially the suppression of the Orthodox Church alienated the Cossacks.
The Cossacks sought representation in the Polish Sejm, recognition of Orthodox traditions, and the gradual expansion of the Cossack Registry. These were rejected by the Polish nobility, who dominated the Sejm.
In 1648, Bohdan Khmelnytsky and Petro Doroshenko led the largest of the Cossack uprisings against the Commonwealth and the Polish king John II Casimir.
The Ruin.
In 1657–1686 came "The Ruin", a devastating 30-year war amongst Russia, Poland, Turks and Cossacks for control of Ukraine, which occurred at about the same time as the Deluge of Poland. Khmelnytsky, deserted by his Tatar allies, suffered a crushing defeat at Berestechko, and turned to the Russian tsar for help. In 1654, Khmelnytsky signed the Treaty of Pereyaslav, forming a military and political alliance with Russia that acknowledged loyalty to the tsar. The wars escalated in intensity with hundreds of thousands of deaths. Defeat came in 1686 as the "Eternal Peace" between Russia and Poland divided the Ukrainian lands between them.
In 1709, Cossack Hetman Ivan Mazepa (1639–1709) defected to Sweden against Russia in the Great Northern War (1700–1721). Eventually Peter recognized that to consolidate and modernize Russia's political and economic power it was necessary to do away with the hetmanate and Ukrainian and Cossack aspirations to autonomy. Mazepa died in exile after fleeing from the Battle of Poltava (1709), where the Swedes and their Cossack allies suffered a catastrophic defeat.
The Constitution of Pylyp Orlyk or Pacts and Constitutions of Rights and Freedoms of the Zaporizhian Host was a 1710 constitutional document written by Hetman Pylyp Orlyk, a Cossack of Ukraine, then within the Polish-Lithuanian Commonwealth. It established a standard for the separation of powers in government between the legislative, executive, and judiciary branches, well before the publication of Montesquieu's "Spirit of the Laws". The Constitution limited the executive authority of the hetman, and established a democratically elected Cossack parliament called the General Council. Pylyp Orlyk's Constitution was unique for its historic period, and was one of the first state constitutions in Europe.
The hetmanate was abolished in 1764; the Zaporizhska Sich abolished in 1775, as Russia centralised control over its lands. As part of the partitioning of Poland in 1772, 1793 and 1795, the Ukrainian lands west of the Dnieper were divided between Russia and Austria. From 1737 to 1834, expansion into the northern Black Sea littoral and the eastern Danube valley was a cornerstone of Russian foreign policy.
Lithuanians and Poles controlled vast estates in Ukraine, and were a law unto themselves. Judicial rulings from Cracow were routinely flouted, while peasants were heavily taxed and practically tied to the land as serfs. Occasionally the landowners battled each other using armies of Ukrainian peasants. The Poles and Lithuanians were Roman Catholics and tried with some success to convert the Orthodox lesser nobility. In 1596, they set up the "Greek-Catholic" or Uniate Church; it dominates western Ukraine to this day. Religious differentiation left the Ukrainian Orthodox peasants leaderless, as they were reluctant to follow the Ukrainian nobles.
Cossacks led an uprising, called Koliivshchyna, starting in the Ukrainian borderlands of the Polish–Lithuanian Commonwealth in 1768. Ethnicity was one root cause of this revolt, which included Ukrainian violence that killed tens of thousands of Poles and Jews. Religious warfare also broke out among Ukrainian groups. Increasing conflict between Uniate and Orthodox parishes along the newly reinforced Polish-Russian border on the Dnieper River in the time of Catherine II set the stage for the uprising. As Uniate religious practices had become more Latinized, Orthodoxy in this region drew even closer into dependence on the Russian Orthodox Church. Confessional tensions also reflected opposing Polish and Russian political allegiances.
After the Annexation of Crimea by the Russian Empire in 1783, New Russia was settled by Ukrainians and Russians. Despite promises in the Treaty of Pereyaslav, the Ukrainian elite and the Cossacks never received the freedoms and the autonomy they were expecting. However, within the Empire, Ukrainians rose to the highest Russian state and church offices. At a later period, tsarists established a policy of Russification, suppressing the use of the Ukrainian language in print and in public.
19th century, World War I and revolution.
In the 19th century, Ukraine was a rural area largely ignored by Russia and Austria. With growing urbanization and modernization, and a cultural trend toward romantic nationalism, a Ukrainian intelligentsia committed to national rebirth and social justice emerged. The serf-turned-national-poet Taras Shevchenko (1814–1861) and the political theorist Mykhailo Drahomanov (1841–1895) led the growing nationalist movement.
After the Russo-Turkish War (1768–1774), Catherine the Great and her immediate successors encouraged German immigration into Ukraine and especially into Crimea, to thin the previously dominant Turk population and encourage agriculture.
Beginning in the 19th century, there was migration from Ukraine to distant areas of the Russian Empire. According to the 1897 census, there were 223,000 ethnic Ukrainians in Siberia and 102,000 in Central Asia. An additional 1.6 million emigrated to the east in the ten years after the opening of the Trans-Siberian Railway in 1906. Far Eastern areas with an ethnic Ukrainian population became known as Green Ukraine.
Nationalist and socialist parties developed in the late 19th century. Austrian Galicia, under the relatively lenient rule of the Habsburgs, became the centre of the nationalist movement.
Ukrainians entered World War I on the side of both the Central Powers, under Austria, and the Triple Entente, under Russia. 3.5 million Ukrainians fought with the Imperial Russian Army, while 250,000 fought for the Austro-Hungarian Army. Austro-Hungarian authorities established the Ukrainian Legion to fight against the Russian Empire. This became the Ukrainian Galician Army that fought against the Bolsheviks and Poles in the post-World War I period (1919–23). Those suspected of Russophile sentiments in Austria were treated harshly.
World War I destroyed both empires. The Russian Revolution of 1917 led to the founding of the Soviet Union under the Bolsheviks, and subsequent civil war in Russia. A Ukrainian national movement for self-determination re-emerged, with heavy Communist and Socialist influence. Several Ukrainian states briefly emerged: the internationally recognized Ukrainian People's Republic (UNR, the predecessor of modern Ukraine, was declared on 23 June 1917 proclaimed at first as a part of the Russian Republic; after the Bolshevik Revolution, the Ukrainian People's Republic proclaimed its independence on 25 January 1918), the Hetmanate, the Directorate and the pro-Bolshevik Ukrainian Soviet Socialist Republic (or Soviet Ukraine) successively established territories in the former Russian Empire; while the West Ukrainian People's Republic and the Hutsul Republic emerged briefly in the Ukrainian lands of former Austro-Hungarian territory.
Act Zluky (Unification Act) was an agreement signed on January 22, 1919 by the Ukrainian People's Republic and the West Ukrainian People's Republic on the St. Sophia Square in Kiev.
This led to civil war, and an anarchist movement called the Black Army or later The Revolutionary Insurrectionary Army of Ukraine developed in Southern Ukraine under the command of the anarchist Nestor Makhno during the Russian Civil War. They protected the operation of "free soviets" and libertarian communes in the Free Territory, an attempt to form a stateless anarchist society from 1918 to 1921 during the Ukrainian Revolution, fighting both the tsarist White Army under Denikin and later the Red Army under Trotsky, before being defeated by the latter in August 1921.
Poland defeated Western Ukraine in the Polish-Ukrainian War, but failed against the Bolsheviks in an offensive against Kiev. According to the Peace of Riga, western Ukraine was incorporated into Poland, which in turn recognised the Ukrainian Soviet Socialist Republic in March 1919. With establishment of the Soviet power, Ukraine lost half of its territory to Poland, Belarus and Russia, while on the left bank of Dniester River was created Moldavian autonomy. Ukraine became a founding member of the Union of Soviet Socialist Republics in December 1922.
Western Ukraine, Carpathian Ruthenia and Bukovina.
The war in Ukraine continued for another two years; by 1921, however, most of Ukraine had been taken over by the Soviet Union, while Galicia and Volhynia (West Ukraine) were incorporated into independent Poland. Bukovina was annexed by Romania and Carpathian Ruthenia was admitted to the Czechoslovak Republic as an autonomy.
A powerful underground Ukrainian nationalist movement arose in Poland in the 1920s and 1930s because of Polish national policies, which was led by the Ukrainian Military Organization and the Organisation of Ukrainian Nationalists (OUN). The movement attracted a militant following among students. Hostilities between Polish state authorities and the popular movement led to a substantial number of fatalities, and the autonomy which had been promised was never implemented. A number of Ukrainian parties, the Ukrainian Catholic Church, an active press, and a business sector existed in Poland. Economic conditions improved in the 1920s, but the region suffered from the Great Depression in the 1930s.
Inter-war Soviet Ukraine.
The Russian Civil War devastated the whole Russian Empire including Ukraine. It left over 1.5 million people dead and hundreds of thousands homeless in the former Russian Empire territory. Soviet Ukraine also faced the Russian famine of 1921 (primarily affecting the Russian Volga-Ural region). During the 1920s, under the Ukrainisation policy pursued by the national Communist leadership of Mykola Skrypnyk, Soviet leadership encouraged a national renaissance in the Ukrainian culture and language. Ukrainisation was part of the Soviet-wide policy of Korenisation (literally "indigenisation"). The Bolsheviks were also committed to universal health care, education and social-security benefits, as well as the right to work and housing. Women's rights were greatly increased through new laws. Most of these policies were sharply reversed by the early 1930s after Joseph Stalin became the "de facto" communist party leader.
Starting from the late 1920s with a centrally planned economy, Ukraine was involved in Soviet industrialisation and the republic's industrial output quadrupled during the 1930s. The peasantry suffered from the programme of collectivisation of agriculture which began during and was part of the first five-year plan and was enforced by regular troops and secret police. Those who resisted were arrested and deported and agricultural productivity greatly declined. As members of the collective farms were not allowed to receive any grain until sometimes unrealistic quotas were met, millions starved to death in a famine known as Holodomor or "Great Famine".
Scholars are divided as to whether this famine fits the definition of genocide, but the Ukrainian parliament and other countries have declared it as such.
The Communist leadership perceived famine as a means of class struggle and used starvation as a punishment tool to force peasants into collective farms.
Largely the same groups were responsible for the mass killing operations during the civil war, collectivisation, and the Great Terror. These groups were associated with Yefim Yevdokimov (1891–1939) and operated in the Secret Operational Division within General State Political Administration (OGPU) in 1929–31. Evdokimov transferred into Communist Party administration in 1934, when he became Party secretary for North Caucasus Krai. He appears to have continued advising Joseph Stalin and Nikolai Yezhov on security matters, and the latter relied on Evdokimov's former colleagues to carry out the mass killing operations that are known as the Great Terror in 1937–38.
On 13 January 2010, Kiev Appellate Court posthumously found Stalin, Kaganovich and other Soviet Communist Party functionaries guilty of genocide against Ukrainians during the Holodomor famine.
World War II.
Following the Invasion of Poland in September 1939, German and Soviet troops divided the territory of Poland. Thus, Eastern Galicia and Volhynia with their Ukrainian population became reunited with the rest of Ukraine. For the first time in history, the nation was united.
In 1940, the Soviets annexed Bessarabia and northern Bukovina. The Ukrainian SSR incorporated northern and southern districts of Bessarabia, northern Bukovina, and the Hertsa region. But it ceded the western part of the Moldavian Autonomous Soviet Socialist Republic to the newly created Moldavian Soviet Socialist Republic. These territorial gains of the USSR were internationally recognized by the Paris peace treaties of 1947.
German armies invaded the Soviet Union on 22 June 1941, initiating four years of total war. The Axis allies initially advanced against desperate but unsuccessful efforts of the Red Army. In the encirclement battle of Kiev, the city was acclaimed as a "Hero City", because of its fierce resistance. More than 600,000 Soviet soldiers (or one-quarter of the Soviet Western Front) were killed or taken captive there, with many suffering severe mistreatment.
Although the majority of Ukrainians fought in or alongside the Red Army and Soviet resistance, in Western Ukraine an independent Ukrainian Insurgent Army movement arose (UPA, 1942). Created as forces of Ukrainian Government in exile, it fell under the influence of the underground (Organization of Ukrainian Nationalists, OUN) which had developed in interwar Poland as a radical reaction to Polish policies towards the Ukrainian minority. Both supported the goal of an independent Ukrainian state on the territory with Ukrainian ethnic majority. Although this brought conflict with Nazi Germany, at times the Melnyk-wing of OUN allied with the Nazi forces. Some UPA divisions also carried out the massacres of ethnic Poles, which brought reprisals. After the war UPA continued to fight the USSR till the 1950s. At the same time, the Ukrainian Liberation Army, another nationalist movement, fought alongside the Nazis.
In total, the number of ethnic Ukrainians who fought in the ranks of the Soviet Army is estimated from 4.5 million to 7 million. The pro-Soviet partisan guerrilla resistance in Ukraine is estimated to number at 47,800 from the start of occupation to 500,000 at its peak in 1944; with about 50% being ethnic Ukrainians. Generally, the Ukrainian Insurgent Army's figures are unreliable, with figures ranging anywhere from 15,000 to as many as 100,000 fighters.
Most of the Ukrainian SSR was organised within the Reichskommissariat Ukraine, with the intention of exploiting its resources and eventual German settlement. Some western Ukrainians, who had only joined the Soviet Union in 1939, hailed the Germans as liberators. Brutal German rule eventually turned their supporters against the Nazi administrators, who made little attempt to exploit dissatisfaction with Stalinist policies. Instead, the Nazis preserved the collective-farm system, carried out genocidal policies against Jews, deported millions of people to work in Germany, and began a depopulation program to prepare for German colonisation. They blockaded the transport of food on the Kiev River.
The vast majority of the fighting in World War II took place on the Eastern Front. By some estimates, 93% of all German casualties took place there. The total losses inflicted upon the Ukrainian population during the war are estimated between 5 and 8 million, including an estimated one and a half million Jews killed by the Einsatzgruppen, sometimes with the help of local collaborators. Of the estimated 8.7 million Soviet troops who fell in battle against the Nazis, 1.4 million were ethnic Ukrainians. Victory Day is celebrated as one of ten Ukrainian national holidays.
Post-World War II.
The republic was heavily damaged by the war, and it required significant efforts to recover. More than 700 cities and towns and 28,000 villages were destroyed. The situation was worsened by a famine in 1946–47, which was caused by a drought and the wartime destruction of infrastructure. The death toll of this famine varies, with even the lowest estimate in the tens of thousands.
In 1945, the Ukrainian SSR became one of the founding members of the United Nations organization, part of a special agreement at the Yalta Conference.
Post-war ethnic cleansing occurred in the newly expanded Soviet Union. As of 1 January 1953, Ukrainians were second only to Russians among adult "special deportees", comprising 20% of the total. In addition, over 450,000 ethnic Germans from Ukraine and more than 200,000 Crimean Tatars were victims of forced deportations.
Following the death of Stalin in 1953, Nikita Khrushchev became the new leader of the USSR. Having served as First Secretary of the Communist Party of Ukrainian SSR in 1938–49, Khrushchev was intimately familiar with the republic; after taking power union-wide, he began to emphasize the friendship between the Ukrainian and Russian nations. In 1954, the 300th anniversary of the Treaty of Pereyaslav was widely celebrated. Crimea was transferred from the Russian SFSR to the Ukrainian SSR.
By 1950, the republic had fully surpassed pre-war levels of industry and production. During the 1946–1950 five-year plan, nearly 20% of the Soviet budget was invested in Soviet Ukraine, a 5% increase from pre-war plans. As a result, the Ukrainian workforce rose 33.2% from 1940 to 1955 while industrial output grew 2.2 times in that same period.
Soviet Ukraine soon became a European leader in industrial production, and an important centre of the Soviet arms industry and high-tech research. Such an important role resulted in a major influence of the local elite. Many members of the Soviet leadership came from Ukraine, most notably Leonid Brezhnev. He later ousted Khrushchev and became the Soviet leader from 1964 to 1982. Many prominent Soviet sports players, scientists, and artists came from Ukraine.
On 26 April 1986, a reactor in the Chernobyl Nuclear Power Plant exploded, resulting in the Chernobyl disaster, the worst nuclear reactor accident in history. This was the only accident to receive the highest possible rating of 7 by the International Nuclear Event Scale, indicating a "major accident", until the Fukushima Daiichi nuclear disaster in March 2011. At the time of the accident, 7 million people lived in the contaminated territories, including 2.2 million in Ukraine.
After the accident, the new city of Slavutych was built outside the exclusion zone to house and support the employees of the plant, which was decommissioned in 2000. A report prepared by the International Atomic Energy Agency and World Health Organization attributed 56 direct deaths to the accident and estimated that there may have been 4,000 extra cancer deaths.
Independence.
On 16 July 1990, the new parliament adopted the Declaration of State Sovereignty of Ukraine. This established the principles of the self-determination, democracy, independence, and the priority of Ukrainian law over Soviet law. A month earlier, a similar declaration was adopted by the parliament of the Russian SFSR. This started a period of confrontation with the central Soviet authorities. In August 1991, a conservative faction among the Communist leaders of the Soviet Union attempted a coup to remove Mikhail Gorbachev and to restore the Communist party's power. After it failed, on 24 August 1991 the Ukrainian parliament adopted the Act of Independence.
A referendum and the first presidential elections took place on 1 December 1991. More than 90% of the electorate expressed their support for the Act of Independence, and they elected the chairman of the parliament, Leonid Kravchuk as the first President of Ukraine. At the meeting in Brest, Belarus on 8 December, followed by the Alma Ata meeting on 21 December, the leaders of Belarus, Russia, and Ukraine formally dissolved the Soviet Union and formed the Commonwealth of Independent States (CIS).
Ukraine was initially viewed as having favourable economic conditions in comparison to the other regions of the Soviet Union. However, the country experienced deeper economic slowdown than some of the other former Soviet Republics. During the recession, Ukraine lost 60% of its GDP from 1991 to 1999, and suffered five-digit inflation rates. Dissatisfied with the economic conditions, as well as the amounts of crime and corruption in Ukraine, Ukrainians protested and organized strikes.
The Ukrainian economy stabilized by the end of the 1990s. A new currency, the hryvnia, was introduced in 1996. After 2000, the country enjoyed steady real economic growth averaging about seven percent annually. A new Constitution of Ukraine was adopted under second President Leonid Kuchma in 1996, which turned Ukraine into a semi-presidential republic and established a stable political system. Kuchma was, however, criticised by opponents for corruption, electoral fraud, discouraging free speech and concentrating too much power in his office. Ukraine also pursued full nuclear disarmament, giving up the third largest nuclear weapons stockpile in the world and dismantling or removing all strategic bombers on its territory.
Orange Revolution.
In 2004, Viktor Yanukovych, then Prime Minister, was declared the winner of the presidential elections, which had been largely rigged, as the Supreme Court of Ukraine later ruled. The results caused a public outcry in support of the opposition candidate, Viktor Yushchenko, who challenged the outcome. This resulted in the peaceful Orange Revolution, bringing Viktor Yushchenko and Yulia Tymoshenko to power, while casting Viktor Yanukovych in opposition.
Activists of the Orange Revolution were funded and trained in tactics of political organisation and nonviolent resistance by Western pollsters and professional consultants who were partly funded by Western government and non-government agencies but received most of their funding from domestic sources. According to "The Guardian", the foreign donors included the U.S. State Department and USAID along with the National Democratic Institute for International Affairs, the International Republican Institute, the NGO Freedom House and George Soros's Open Society Institute. The National Endowment for Democracy has supported democracy-building efforts in Ukraine since 1988. Writings on nonviolent struggle by Gene Sharp contributed in forming the strategic basis of the student campaigns.
Russian authorities provided support through advisers such as Gleb Pavlovsky, consulting on blackening the image of Yushchenko through the state media, pressuring state-dependent voters to vote for Yanukovich and on vote-rigging techniques such as multiple 'carousel voting' and 'dead souls' voting.
Yanukovych returned to power in 2006 as Prime Minister in the Alliance of National Unity, until snap elections in September 2007 made Tymoshenko Prime Minister again. Amid the 2008–09 Ukrainian financial crisis the Ukrainian economy plunged by 15%. Disputes with Russia briefly stopped all gas supplies to Ukraine in 2006 and again in 2009, leading to gas shortages in other countries. Viktor Yanukovych was elected President in 2010 with 48% of votes.
Euromaidan and 2014 revolution.
The Euromaidan (, literally "Eurosquare") protests started in November 2013 after the president, Viktor Yanukovych, began moving away from an association agreement that had been in the works with the European Union and instead chose to establish closer ties with the Russian Federation. Some Ukrainians took to the streets to show their support for closer ties with Europe. Meanwhile, in the predominantly Russian-speaking east, a large portion of the population opposed the "Euromaidan" protests, instead supporting the Yanukovych government. Over time, "Euromaidan" came to describe a wave of demonstrations and civil unrest in Ukraine, the scope of which evolved to include calls for the resignation of President Yanukovych and his government.
Violence escalated after 16 January 2014 when the government accepted new Anti-Protest Laws. Violent anti-government demonstrators occupied buildings in the centre of Kiev, including the Justice Ministry building, and riots left 98 dead with approximately fifteen thousand injured and 100 considered missing from 18 to 20 February. Owing to the violent protests, Members of Parliament voted on 22 February to remove the president and set an election for 25 May to select his replacement. Petro Poroshenko, running on a pro-European Union platform, won with over fifty percent of the vote, therefore not requiring a run-off election. Upon his election, Poroshenko announced that his immediate priorities would be to take action in the civil unrest in Eastern Ukraine and mend ties with Russian Federation. Poroshenko was inaugurated as president on 7 June 2014, as previously announced by his spokeswoman Irina Friz in a low-key ceremony without a celebration on Kiev's Maidan Nezalezhnosti square (the center of the Euromaidan protests) for the ceremony. In October 2014, Ukrainians voted to keep Poroshenko in power.
Civil unrest and Russian intervention.
The ousting of Yanukovich prompted Vladimir Putin to begin preparations to annex Crimea on 23 February 2014. Using the Russian naval base at Sevastopol as cover, Putin directed Russian troops and intelligence agents to disarm Ukrainian forces and take control of Crimea. After the troops entered Crimea, a controversial referendum was held on 16 March 2014 and the official result was that 97 percent wished to join with Russia. On 18 March 2014, Russia and self-proclaimed Republic of Crimea signed a treaty of accession of the Republic of Crimea and Sevastopol in the Russian Federation. The UN general assembly responded by passing resolution 68/262 that the referendum was invalid and supporting the territorial integrity of Ukraine.
Separately, in the Donetsk and Luhansk regions, armed men declaring themselves as local militia seized government buildings, police and special police stations in several cities and held unrecognised status referendums. Later it was uncovered that the insurgency was led by Russian intelligence agents such as Igor Girkin and Alexander Borodai as well as various militants from Russia such as Arseny Pavlov.
Talks in Geneva between the EU, Russia, Ukraine and USA yielded a Joint Diplomatic Statement referred to as the 2014 Geneva Pact in which the parties requested that all unlawful militias lay down the arms and vacate seized government buildings, and also establish a political dialogue that could lead to more autonomy for Ukraine's regions. When Petro Poroshenko won the presidential election held on 25 May 2014, he vowed to continue the military operations by the Ukrainian government forces to end the armed insurgency. More than 6,000 people have been killed in the military campaign. According to the United Nations, 730,000 Ukrainian refugees have fled to Russia since the beginning of 2014 and 117,000 have fled to other parts of Ukraine. As president-elect, Poroshenko promised to pursue the return of Crimea to Ukrainian sovereignty.
In August 2014, a bi-lateral commission of leading scholars from the United States and Russia issued the Boisto Agenda indicating a 24-step plan to resolve the crisis in Ukraine. The Boisto Agenda was organized into five imperative categories for addressing the crisis requiring stabilization identified as: (1) Elements of an Enduring, Verifiable Ceasefire; (2) Economic Relations; (3) Social and Cultural Issues; (4) Crimea; and, (5) International Status of Ukraine. In late 2014, Ukraine ratified the Ukraine–European Union Association Agreement, which Poroshenko described as Ukraine's "first but most decisive step" towards EU membership. Poroshenko also set 2020 as target for EU membership application.
In February 2015, after a summit hosted in Belarus, Poroshenko negotiated a ceasefire with the separatist troops. This included conditions such as the withdrawal of heavy weaponry from the front line and decentralisation of rebel regions by the end of 2015. It also included conditions such as the Ukrainian control of the border with Russia in 2015 and the withdrawal of all foreign troops from the Ukrainian territory. The ceasefire began at midnight on 15 February 2015. Participants in this ceasefire also agreed to attend regular meetings to ensure that the agreement is respected.
On January 1, 2016, Ukraine joined the Deep and Comprehensive Free Trade Area with European Union, which aims to modernize and develop Ukrainian economy, governance and rule of law to EU standards and gradually increase integration with the EU Internal market.
Historical maps of states.
Several states have existed on the territory of present-day Ukraine since its foundation. Most of these territories have been located within Eastern Europe. However, as depicted in the maps here, they have at times extended well into Eurasia and Southeastern Europe. At other times there has been no distinct Ukrainian state, its territories having been annexed by its more powerful neighbours.
Geography.
At and with a coastline of , Ukraine is the world's 46th-largest country (after South Sudan, before Madagascar). It is the largest wholly European country and the second largest country in Europe (after the European part of Russia, before metropolitan France). It lies between latitudes 44° and 53° N, and longitudes 22° and 41° E.
The landscape of Ukraine consists mostly of fertile plains (or steppes) and plateaus, crossed by rivers such as the Dnieper (), Seversky Donets, Dniester and the Southern Buh as they flow south into the Black Sea and the smaller Sea of Azov. To the southwest, the delta of the Danube forms the border with Romania. Its various regions have diverse geographic features ranging from the highlands to the lowlands. The country's only mountains are the Carpathian Mountains in the west, of which the highest is the Hora Hoverla at , and the Crimean Mountains on Crimea, in the extreme south along the coast. However Ukraine also has a number of highland regions such as the Volyn-Podillia Upland (in the west) and the Near-Dnipro Upland (on the right bank of Dnieper); to the east there are the south-western spurs of the Central Russian Uplands over which runs the border with Russian Federation. Near the Sea of Azov can be found the Donets Ridge and the Near Azov Upland. The snow melt from the mountains feeds the rivers, and natural changes in altitude form a sudden drop in elevation and create many opportunities to form waterfalls.
Significant natural resources in Ukraine include iron ore, coal, manganese, natural gas, oil, salt, sulphur, graphite, titanium, magnesium, kaolin, nickel, mercury, timber and an abundance of arable land. Despite this, the country faces a number of major environmental issues such as inadequate supplies of potable water; air and water pollution and deforestation, as well as radiation contamination in the north-east from the 1986 accident at the Chernobyl Nuclear Power Plant. Recycling toxic household waste is still in its infancy in Ukraine.
Soil.
From northwest to southeast the soils of Ukraine may be divided into three major aggregations: a zone of sandy podzolized soils; a central belt consisting of the black, extremely fertile Ukrainian (chernozems); and a zone of chestnut and salinized soils.
As much as two-thirds of the country's surface land consists of the so-called black earth (chornozem), a resource that has made Ukraine one of the most fertile regions in the world and famously called a "breadbasket." These (chornozem) soils may be divided into three broad groups: in the north a belt of the so-called deep chernozems, about thick and rich in humus; south and east of the former, a zone of prairie, or ordinary, chernozems, which are equally rich in humus but only about thick; and the southernmost belt, which is even thinner and has still less humus. Interspersed in various uplands and along the northern and western perimeters of the deep chernozems are mixtures of gray forest soils and podzolized black-earth soils, which together occupy much of Ukraine’s remaining area. All these soils are very fertile when sufficient water is available. However, their intensive cultivation, especially on steep slopes, has led to widespread soil erosion and gullying.
The smallest proportion of the soil cover consists of the chestnut soils of the southern and eastern regions. They become increasingly salinized to the south as they approach the Black Sea.
Biodiversity.
Ukraine is home to a very wide range of animals, fungi, microorganisms and plants.
Animals.
Ukraine is divided into two main zoological areas. One of these areas, in the west of the country, is made up of the borderlands of Europe, where there are species typical of mixed forests, the other is located in eastern Ukraine, where steppe-dwelling species thrive. In the forested areas of the country it is not uncommon to find lynxes, wolves, wild boar and martens, as well as many other similar species; this is especially true of the Carpathian Mountains, where a large number of predatory mammals make their home, as well as a contingent of brown bears. Around Ukraine's lakes and rivers beavers, otters and mink make their home, whilst within, carp, bream and catfish are the most commonly found species of fish. In the central and eastern parts of the country, rodents such as hamsters and gophers are found in large numbers.
Fungi.
More than 6,600 species of fungi (including lichen-forming species) have been recorded from Ukraine, but this number is far from complete. The true total number of fungal species occurring in Ukraine, including species not yet recorded, is likely to be far higher, given the generally accepted estimate that only about 7% of all fungi worldwide have so far been discovered. Although the amount of available information is still very small, a first effort has been made to estimate the number of fungal species endemic to Ukraine, and 2217 such species have been tentatively identified.
Climate.
Ukraine has a mostly temperate continental climate, although the southern coast has a humid subtropical climate. Precipitation is disproportionately distributed; it is highest in the west and north and lowest in the east and southeast. Western Ukraine receives around of precipitation annually, while Crimea receives around . Winters vary from cool along the Black Sea to cold farther inland. Average annual temperatures range from in the north, to in the south.
Politics.
Ukraine is a republic under a mixed semi-parliamentary semi-presidential system with separate legislative, executive, and judicial branches.
The Constitution of Ukraine.
With the proclamation of its independence on 24 August 1991, and adoption of a constitution on 28 June 1996, Ukraine became a semi-presidential republic. However, in 2004, deputies introduced changes to the Constitution, which tipped the balance of power in favour of a parliamentary system. From 2004 to 2010, the legitimacy of the 2004 Constitutional amendments had official sanction, both with the Constitutional Court of Ukraine, and most major political parties. Despite this, on 30 September 2010 the Constitutional Court ruled that the amendments were null and void, forcing a return to the terms of the 1996 Constitution and again making Ukraine's political system more presidential in character.
The ruling on the 2004 Constitutional amendments became a major topic of political discourse. Much of the concern was based on the fact that neither the Constitution of 1996 nor the Constitution of 2004 provided the ability to "undo the Constitution", as the decision of the Constitutional Court would have it, even though the 2004 constitution arguably has an exhaustive list of possible procedures for constitutional amendments (articles 154–159). In any case, the current Constitution could be modified by a vote in Parliament.
On 21 February 2014 an agreement between President Viktor Yanukovych and opposition leaders saw the country return to the 2004 Constitution. The historic agreement, brokered by the European Union, followed protests that began in late November 2013 and culminated in a week of violent clashes in which scores of protesters were killed. In addition to returning the country to the 2004 Constitution, the deal provided for the formation of a coalition government, the calling of early elections, and the release of former Prime Minister Yulia Tymoshenko from prison. A day after the agreement was reached the Ukraine parliament dismissed Yanukovych and installed its speaker Oleksandr Turchynov as interim president and Arseniy Yatsenyuk as the Prime Minister of Ukraine.
The president, parliament and government.
The President is elected by popular vote for a five-year term and is the formal head of state.
Ukraine's legislative branch includes the 450-seat unicameral parliament, the Verkhovna Rada. The parliament is primarily responsible for the formation of the executive branch and the Cabinet of Ministers, headed by the Prime Minister. However, the President still retains the authority to nominate the Ministers of the Foreign Affairs and of Defence for parliamentary approval, as well as the power to appoint the Prosecutor General and the head of the Security Service.
Laws, acts of the parliament and the cabinet, presidential decrees, and acts of the Crimean parliament may be abrogated by the Constitutional Court, should they be found to violate the constitution. Other normative acts are subject to judicial review. The Supreme Court is the main body in the system of courts of general jurisdiction.
Local self-government is officially guaranteed. Local councils and city mayors are popularly elected and exercise control over local budgets. The heads of regional and district administrations are appointed by the President in accordance with the proposals of the Prime Minister. This system virtually requires an agreement between the President and the Prime Minister, and has in the past led to problems, such as when President Yushchenko exploited a perceived loophole by appointing so-called 'temporarily acting' officers, instead of actual governors or local leaders, thus evading the need to seek a compromise with the Prime Minister. This practice was controversial and was subject to Constitutional Court review.
Ukraine has a large number of political parties, many of which have tiny memberships and are unknown to the general public. Small parties often join in multi-party coalitions (electoral blocs) for the purpose of participating in parliamentary elections.
Courts and law enforcement.
The courts enjoy legal, financial and constitutional freedom guaranteed by Ukrainian law since 2002. Judges are largely well protected from dismissal (except in the instance of gross misconduct). Court justices are appointed by presidential decree for an initial period of five years, after which Ukraine's Supreme Council confirms their positions for life. Although there are still problems, the system is considered to have been much improved since Ukraine's independence in 1991. The Supreme Court is regarded as an independent and impartial body, and has on several occasions ruled against the Ukrainian government. The World Justice Project ranks Ukraine 66 out of 99 countries surveyed in its annual Rule of Law Index.
Prosecutors in Ukraine have greater powers than in most European countries, and according to the European Commission for Democracy through Law 'the role and functions of the Prosecutor's Office is not in accordance with Council of Europe standards". The criminal judicial system maintains an average conviction rate of over 99%, equal to the conviction rate of the Soviet Union, with suspects often being incarcerated for long periods before trial. On 24 March 2010, President Yanukovych formed an expert group to make recommendations how to "clean up the current mess and adopt a law on court organization". One day later, he stated "We can no longer disgrace our country with such a court system." The criminal judicial system and the prison system of Ukraine remain quite punitive.
Since 1 January 2010 it has been permissible to hold court proceedings in Russian by mutual consent of the parties. Citizens unable to speak Ukrainian or Russian may use their native language or the services of a translator. "З подачі "Регіонів" Рада дозволила російську у судах". "Ukrayinska Pravda". 23 June 2009.[http://wayback.archive.org/web/20120111061236/http://novynar.com.ua/politics/126686]</ref> Previously all court proceedings had to be held in Ukrainian.
Law enforcement agencies in Ukraine are organised under the authority of the Ministry of Internal Affairs. They consist primarily of the national police force "(Мiлiцiя)" and various specialised units and agencies such as the State Border Guard and the Coast Guard services. Law enforcement agencies, particularly the police, faced criticism for their heavy handling of the 2004 Orange Revolution. Many thousands of police officers were stationed throughout the capital, primarily to dissuade protesters from challenging the state's authority but also to provide a quick reaction force in case of need; most officers were armed. Bloodshed was only avoided when Lt. Gen. Sergei Popkov heeded his colleagues' calls to withdraw.
The Ministry of Internal Affairs is also responsible for the maintenance of the State Security Service; Ukraine's domestic intelligence agency, which has on occasion been accused of acting like a secret police force serving to protect the country's political elite from media criticism. On the other hand, however, it is widely accepted that members of the service provided vital information about government plans to the leaders of the Orange Revolution to prevent the collapse of the movement.
Foreign relations.
In 1999–2001, Ukraine served as a non-permanent member of the UN Security Council. Historically, Soviet Ukraine joined the United Nations in 1945 as one of the original members following a Western compromise with the Soviet Union, which had asked for seats for all 15 of its union republics. Ukraine has consistently supported peaceful, negotiated settlements to disputes. It has participated in the quadripartite talks on the conflict in Moldova and promoted a peaceful resolution to conflict in the post-Soviet state of Georgia. Ukraine also has made a substantial contribution to UN peacekeeping operations since 1992.
Ukraine currently considers Euro-Atlantic integration its primary foreign policy objective,Ukraine abolishes its non-aligned status – law, Interfax-Ukraine (23 December 2014)Ukraine’s complicated path to NATO membership, Euronews (23 December 2014)Ukraine Takes Step Toward Joining NATO, New York Times (23 December 2014)http://www.wsj.com/articles/ukraine-ends-nonaligned-status-earning-quick-rebuke-from-russia-1419339226 Ukraine Ends 'Nonaligned' Status, Earning Quick Rebuke From Russia, The Wall Street journal (23 December 2014)</ref> but in practice it has always balanced its relationship with the European Union and the United States with strong ties to Russia. The European Union's Partnership and Cooperation Agreement (PCA) with Ukraine went into force on 1 March 1998. The European Union (EU) has encouraged Ukraine to implement the PCA fully before discussions begin on an association agreement, issued at the EU Summit in December 1999 in Helsinki, recognizes Ukraine's long-term aspirations but does not discuss association. On 31 January 1992, Ukraine joined the then-Conference on Security and Cooperation in Europe (now the Organization for Security and Cooperation in Europe (OSCE), and on 10 March 1992, it became a member of the North Atlantic Cooperation Council. Ukraine–NATO relations are close and the country has declared interest in eventual membership. This was removed from the government's foreign policy agenda upon election of Viktor Yanukovych to the presidency, in 2010. But after February 2014's Yanukovych ouster and the (denied by Russia) following Russian military intervention in Ukraine Ukraine renewed its drive for NATO membership. Ukraine is the most active member of the Partnership for Peace (PfP). All major political parties in Ukraine support full eventual integration into the European Union. The Association Agreement with the EU was expected to be signed and put into effect by the end of 2011, but the process was suspended by 2012 because of the political developments of that time.
Ukraine long had close ties with all its neighbours, but Russia–Ukraine relations became difficult in 2014 by the annexation of Crimea, energy dependence and payment disputes.
Ukraine is included in the European Union's European Neighbourhood Policy (ENP) which aims at bringing the EU and its neighbours closer.
Administrative divisions.
The system of Ukrainian subdivisions reflects the country's status as a unitary state (as stated in the country's constitution) with unified legal and administrative regimes for each unit.
Ukraine consists of 27 regions which are twenty-four oblasts (provinces) and one autonomous republic (), Crimea. Additionally, the cities of Kiev, the capital, and Sevastopol, both have a special legal status. The 24 oblasts and Crimea are subdivided into 490 (districts) and city municipalities of regional significance, or second-level administrative units. The average area of a Ukrainian raion is ; the average population of a raion is 52,000 people.
Populated places in Ukraine are split into two categories: urban and rural. Urban populated places are split further into cities and urban-type settlements (a Soviet administrative invention), while rural populated places consist of villages and settlements (a generally used term). All cities have certain degree of self-rule depending on their significance such as national significance (as in the case of Kiev and Sevastopol), regional significance (within each oblast or autonomous republic) or district significance (all the rest of cities). City's significance depends on several factors such as its population, socio-economic and historical importance, infrastructure and others.
Following the 2014 Crimean crisis, Crimea and Sevastopol became de facto administrated by the Russian Federation, which claims them as the Republic of Crimea and the federal city of Sevastopol. They are still recognised as being Ukrainian territory by the majority of the international community.
Armed forces.
After the dissolution of the Soviet Union, Ukraine inherited a 780,000-man military force on its territory, equipped with the third-largest nuclear weapons arsenal in the world. In May 1992, Ukraine signed the Lisbon Protocol in which the country agreed to give up all nuclear weapons to Russia for disposal and to join the Nuclear Non-Proliferation Treaty as a non-nuclear weapon state. Ukraine ratified the treaty in 1994, and by 1996 the country became free of nuclear weapons.
Ukraine took consistent steps toward reduction of conventional weapons. It signed the Treaty on Conventional Armed Forces in Europe, which called for reduction of tanks, artillery, and armoured vehicles (army forces were reduced to 300,000). The country plans to convert the current conscript-based military into a professional volunteer military.
Ukraine has been playing an increasingly larger role in peacekeeping operations. On Friday 3 January 2014, the Ukrainian frigate "Hetman Sagaidachniy" joined the European Union's counter piracy Operation Atalanta and will be part of the EU Naval Force off the coast of Somalia for two months. Ukrainian troops are deployed in Kosovo as part of the Ukrainian-Polish Battalion. A Ukrainian unit was deployed in Lebanon, as part of UN Interim Force enforcing the mandated ceasefire agreement. There was also a maintenance and training battalion deployed in Sierra Leone. In 2003–05, a Ukrainian unit was deployed as part of the Multinational force in Iraq under Polish command. The total Ukrainian armed forces deployment around the world is 562 servicemen.
Military units of other states participate in multinational military exercises with Ukrainian forces in Ukraine regularly, including U.S. military forces.
Following independence, Ukraine declared itself a neutral state. The country has had a limited military partnership with Russian Federation, other CIS countries and a partnership with NATO since 1994. In the 2000s, the government was leaning towards NATO, and a deeper cooperation with the alliance was set by the NATO-Ukraine Action Plan signed in 2002. It was later agreed that the question of joining NATO should be answered by a national referendum at some point in the future. Recently deposed President Viktor Yanukovych considered the current level of co-operation between Ukraine and NATO sufficient, and was against Ukraine joining NATO. During the 2008 Bucharest summit, NATO declared that Ukraine will become a member of NATO, whenever it wants and when it would correspond to the criteria for the accession.
Economy.
In Soviet times, the economy of Ukraine was the second largest in the Soviet Union, being an important industrial and agricultural component of the country's planned economy. With the dissolution of the Soviet system, the country moved from a planned economy to a market economy. The transition process was difficult for the majority of the population which plunged into poverty. Ukraine's economy contracted severely following the years after the Soviet dissolution. Day-to-day life for the average person living in Ukraine was a struggle. A significant number of citizens in rural Ukraine survived by growing their own food, often working two or more jobs and buying the basic necessities through the barter economy.
In 1991, the government liberalised most prices to combat widespread product shortages, and was successful in overcoming the problem. At the same time, the government continued to subsidise state-run industries and agriculture by uncovered monetary emission. The loose monetary policies of the early 1990s pushed inflation to hyperinflationary levels. For the year 1993, Ukraine holds the world record for inflation in one calendar year. Those living on fixed incomes suffered the most. Prices stabilised only after the introduction of new currency, the hryvnia, in 1996. The country was also slow in implementing structural reforms. Following independence, the government formed a legal framework for privatisation. However, widespread resistance to reforms within the government and from a significant part of the population soon stalled the reform efforts. A large number of state-owned enterprises were exempt from the privatisation process.
In the meantime, by 1999, the GDP had fallen to less than 40% of the 1991 level. It recovered considerably in the following years, but as at 2014 had yet to reach the historical maximum. In the early 2000s, the economy showed strong export-based growth of 5 to 10%, with industrial production growing more than 10% per year. Ukraine was hit by the economic crisis of 2008 and in November 2008, the IMF approved a stand-by loan of $16.5 billion for the country.
Ukraine's 2010 GDP (PPP), as calculated by the CIA, is ranked 38th in the world and estimated at $305.2 billion. Its GDP per capita in 2010 according to the CIA was $6,700 (in PPP terms), ranked 107th in the world. Nominal GDP (in U.S. dollars, calculated at market exchange rate) was $136 billion, ranked 53rd in the world. By July 2008 the average nominal salary in Ukraine reached 1,930 hryvnias per month. Despite remaining lower than in neighbouring central European countries, the salary income growth in 2008 stood at 36.8%
Ukraine produces nearly all types of transportation vehicles and spacecraft. Antonov airplanes and KrAZ trucks are exported to many countries. The majority of Ukrainian exports are marketed to the European Union and CIS. Since independence, Ukraine has maintained its own space agency, the National Space Agency of Ukraine (NSAU). Ukraine became an active participant in scientific space exploration and remote sensing missions. Between 1991 and 2007, Ukraine has launched six self made satellites and 101 launch vehicles, and continues to design spacecraft.
The country imports most energy supplies, especially oil and natural gas and to a large extent depends on Russia as its energy supplier. While 25% of the natural gas in Ukraine comes from internal sources, about 35% comes from Russia and the remaining 40% from Central Asia through transit routes that Russia controls. At the same time, 85% of the Russian gas is delivered to Western Europe through Ukraine.
Growing sectors of the Ukrainian economy include the information technology (IT) market, which topped all other Central and Eastern European countries in 2007, growing some 40 percent. In 2013, Ukraine ranked fourth in the world in number of certified IT professionals after the United States, India and Russia.
Ukraine's 2010 GDP, as calculated by the World Bank, was around $136 billion, 2011 GDP – around $163 billion, 2012 – $176.6 billion, 2013 – $177.4 billion. In 2014 and 2015, the Ukrainian currency was the world's worst performing currency, having dropped 80 percent of its value since April 2014 since the War in Donbass and the annexation of Crimea by Russia.
The World Bank classifies Ukraine as a middle-income state. Significant issues include underdeveloped infrastructure and transportation, corruption and bureaucracy. The public will to fight against corrupt officials and business elites culminated in a strong wave of public demonstrations against the Victor Yanukovych’s regime in November 2013. However, according to the Corruption Perceptions Index, Ukraine is still the most corrupt country in Europe being ranked 142nd out of 175 countries on the world, in the latest CPI report from 2014. In 2007 the Ukrainian stock market recorded the second highest growth in the world of 130 percent. According to the CIA, in 2006 the market capitalization of the Ukrainian stock market was $111.8 billion.
Ukraine has managed to achieve certain progress in reducing absolute poverty, ensuring access to primary and secondary education, improving maternal health and reducing child mortality.
The poverty rate according to the absolute criterion (share of the population whose daily consumption is below US$5.05 (PPP)) was reduced from 11.9 percent in 2000 to 2.3 percent in 2012, and the poverty rate according to the relative criterion (share of the population below the national poverty line) decreased at the same time from 71.2 percent to 24.0 percent.
Corporations.
Ukraine has a very large heavy-industry base and is one of the largest refiners of metallurgical products in Eastern Europe. However, the country is also well known for its production of high-technological goods and transport products, such as Antonov aircraft and various private and commercial vehicles. The country's largest and most competitive firms are components of the PFTS index, traded on the PFTS Ukraine Stock Exchange.
Well-known Ukrainian brands include Naftogaz Ukrainy, AvtoZAZ, PrivatBank, Roshen, Yuzhmash, Nemiroff, Motor Sich, Khortytsa, Kyivstar and Aerosvit.
Ukraine is regarded as a developing economy with high potential for future success, though such a development is thought likely only with new all-encompassing economic and legal reforms. Although Foreign Direct Investment in Ukraine remained relatively strong since recession of the early 1990s, the country has had trouble maintaining stable economic growth. Issues relating to current corporate governance in Ukraine were primarily linked to the large scale monopolisation of traditional heavy industries by wealthy individuals such as Rinat Akhmetov, the enduring failure to broaden the nation's economic base and a lack of effective legal protection for investors and their products. Despite all this, Ukraine's economy was still expected to grow by around 3.5% in 2010.
Transport.
In total, Ukrainian paved roads stretch for . Major routes, marked with the letter 'M' for 'International' "(Ukrainian: Міжнародний"), extend nationwide and connect all major cities of Ukraine, and provide cross-border routes to the country's neighbours. There are only two true motorway standard highways in Ukraine; a stretch of motorway from Kharkiv to Dnipropetrovsk and a section of the M03 which extends from Kiev to Boryspil, where the city's international airport is located.
Rail transport in Ukraine connects all major urban areas, port facilities and industrial centres with neighbouring countries. The heaviest concentration of railway track is the Donbas region of Ukraine. Although rail freight transport fell by 7.4% in 1995 in comparison with 1994, Ukraine is still one of the world's highest rail users. The total amount of railroad track in Ukraine extends for , of which is electrified. Currently the state has a monopoly on the provision of passenger rail transport, and all trains, other than those with cooperation of other foreign companies on international routes, are operated by its company 'Ukrzaliznytsia'.
Transport by air is developing quickly, with a visa-free programme for EU nationals and citizens of a number of other Western nations, the nation's aviation sector is handling a significantly increased number of travellers. The Euro 2012 football tournament, held in Poland and Ukraine as joint hosts, prompted the government to invest heavily in transport infrastructure, and in particular airports. The Donetsk airport, completed for Euro 2012, was destroyed by the end of 2014 because of the ongoing war between the government and the separatist movement.
Kiev Boryspil is the county's largest international airport; it has three main passenger terminals and is the base for both of Ukraine's national airlines. Other large airports in the country include those in Kharkiv, Lviv and Donetsk (now destroyed), whilst those in Dnipropetrovsk and Odessa have plans for terminal upgrades in the near future. Ukraine has a number of airlines, the largest of which are the nation's flag carriers, Aerosvit and UIA. Antonov Airlines, a subsidiary of the Antonov Aerospace Design Bureau is the only operator of the world's largest fixed wing aircraft, the An-225.
International maritime travel is mainly provided through the Port of Odessa, from where ferries sail regularly to Istanbul, Varna and Haifa. The largest ferry company presently operating these routes is Ukrferry.
Energy.
In 2014, Ukraine was ranked number 19 on the Emerging Market Energy Security Growth Prosperity Index, published by the think tank Bisignis Institute, which ranks emerging market countries using government corruption, GDP growth and oil reserve information.
Fuel resources.
Ukraine produces and processes its own natural gas and petroleum. However, the majority of these commodities are imported. Eighty percent of Ukrainian natural gas supplies are imported, mainly from Russia.
Natural gas is heavily utilised not only in energy production but also by steel and chemical industries of the country, as well as by the district heating sector. In 2012, Shell started exploration drilling for shale gas in Ukraine—a project aimed at the nation's total gas supply independence.
Ukraine has sufficient coal reserves and increases its use in electricity generation.
Power generation.
Ukraine has been a net energy exporting country, for example in 2011, 3.3% of electricity produced were exported, but also one of Europe's largest energy consumers. , 47.6% of total electricity generation was from nuclear power The largest nuclear power plant in Europe, the Zaporizhia Nuclear Power Plant, is located in Ukraine. Most of the nuclear fuel has been coming from Russia. In 2008 Westinghouse Electric Company won a five-year contract selling nuclear fuel to three Ukrainian reactors starting in 2011.
Following Euromaidan then President Viktor Yanukovich introduced a ban on Rosatom nuclear fuel shipments to Europe via Ukraine, which was in effect from 28 January until 6 March 2014. After the Russian annexation of Crimea in April 2014, the National Nuclear Energy Generating Company of Ukraine Energoatom and Westinghouse extended the contract for fuel deliveries through 2020.
Coal and gas-fired thermal power stations and hydroelectricity are the second and third largest kinds of power generation in the country.
Renewable energy use.
The share of renewables within the total energy mix is still very small, but is growing fast. Total installed capacity of renewable energy installations more than doubled in 2011 and stands at 397 MW. In 2011 several large solar power stations were opened in Ukraine, among them Europe's largest solar park in Perovo, (Crimea). Ukrainian State Agency for Energy Efficiency and Conservation forecasts that combined installed capacity of wind and solar power plants in Ukraine could increase by another 600 MW in 2012. According to Macquarie Research, by 2016 Ukraine will construct and commission new solar power stations with a total capacity of 1.8 GW, almost equivalent to the capacity of two nuclear reactors.
The Economic Bank for Reconstruction and Development estimates that Ukraine has great renewable energy potential: the technical potential for wind energy is estimated at 40 TWh/year, small hydropower stations at 8.3 TWh/year, biomass at 120 TWh/year, and solar energy at 50 TWh/year. In 2011, Ukraine's Energy Ministry predicted that the installed capacity of generation from alternative and renewable energy sources would increase to 9% (about 6 GW) of the total electricity production in the country.
Internet.
Ukraine has a large and steadily growing Internet sector, mostly uninfluenced by the financial crisis of 2007–08. As of June, 2014, there were 18.2 million desktop Internet users, which is 56% of the adult population. Gemius. The core of the audience is the 25 to 34-year-old age bracket, representing 29% of the population. Ukraine ranks 8th among the world's top ten countries with the fastest Internet access speed.
Tourism.
Ukraine occupies 8th place in Europe by the number of tourists visiting, according to the World Tourism Organisation rankings, because of its numerous tourist attractions: mountain ranges suitable for skiing, hiking and fishing: the Black Sea coastline as a popular summer destination; nature reserves of different ecosystems; churches, castle ruins and other architectural and park landmarks; various outdoor activity points. Kiev, Lviv, Odessa and Kamyanets-Podilskyi are Ukraine's principal tourist centres each offering many historical landmarks as well as formidable hospitality infrastructure. Tourism used to be the mainstay of Crimea's economy but there has been a major fall in visitor numbers following the Russian annexation in 2014.
The Seven Wonders of Ukraine and Seven Natural Wonders of Ukraine are the selection of the most important landmarks of Ukraine, chosen by the general public through an Internet-based vote.
Demographics.
According to the Ukrainian Census of 2001, Ukrainians make up 77.8% of the population. Other significant groups have identified themselves as belonging to the nationality of Russians (17.3%), Belarusians (0.6%), Moldovans (0.5%), Crimean Tatars (0.5%), Bulgarians (0.4%), Hungarians (0.3%), Romanians (0.3%), Poles (0.3%), Jews (0.2%), Armenians (0.2%), Greeks (0.2%) and Tatars (0.2%). The industrial regions in the east and southeast are the most heavily populated, and about 67.2% of the population lives in urban areas.
Population decline.
Ukraine's population has been declining since the 1990s because of its high death rate and a low birth rate. The population is shrinking by over 150,000 annually since 1993. The birth rate has recovered in recent years from a low level around 2000, and is now comparable to the European average. It would need to increase by another 50% or so to stabilize the population and offset the high mortality rate.
In 2007, the country's rate of population decline was the fourth highest in the world.
Life expectancy is falling, and Ukraine suffers a high mortality rate from environmental pollution, poor diets, widespread smoking, extensive alcoholism and deteriorating medical care.
In the years 2008 to 2010, more than 1.5 million children were born in Ukraine, compared to fewer than 1.2 million during 1999–2001 during the worst of the demographic crisis. In 2008 Ukraine posted record-breaking birth rates since its 1991 independence. Infant mortality rates have also dropped from 10.4 deaths to 8.3 per 1,000 children under one year of age. This is lower than in 153 countries of the world.
Fertility and natalist policies.
The current birth rate in Ukraine, , is 10.8 births/1,000 population, and the death rate is 15.2 deaths/1,000 population (see Ukraine demographic tables).
The phenomenon of lowest-low fertility, defined as total fertility below 1.3, is emerging throughout Europe and is attributed by many to postponement of the initiation of childbearing. Ukraine, where total fertility (a very low 1.1 in 2001), was one of the world's lowest, shows that there is more than one pathway to lowest-low fertility. Although Ukraine has undergone immense political and economic transformations during 1991–2004, it has maintained a young age at first birth and nearly universal childbearing. Analysis of official national statistics and the Ukrainian Reproductive Health Survey show that fertility declined to very low levels without a transition to a later pattern of childbearing. Findings from focus group interviews suggest explanations of the early fertility pattern. These findings include the persistence of traditional norms for childbearing and the roles of men and women, concerns about medical complications and infertility at a later age, and the link between early fertility and early marriage.
To help mitigate the declining population, the government continues to increase child support payments. Thus it provides one-time payments of 12,250 hryvnias for the first child, 25,000 Hryvnias for the second and 50,000 Hryvnias for the third and fourth, along with monthly payments of 154 hryvnias per child. The demographic trend is showing signs of improvement, as the birth rate has been steadily growing since 2001. Net population growth over the first nine months of 2007 was registered in five provinces of the country (out of 24), and population shrinkage was showing signs of stabilising nationwide. In 2007 the highest birth rates were in the western oblasts. In 2008, Ukraine emerged from lowest-low fertility, and the upward trend has continued since, except for a slight dip in 2010 because of the economic crisis of 2009 (see demographic tables).
Urbanisation.
In total, Ukraine has 457 cities, 176 of them are labelled oblast-class, 279 smaller -class cities, and two special legal status cities. These are followed by 886 urban-type settlements and 28,552 villages.
Language.
According to the constitution, the state language of Ukraine is Ukrainian. Russian is widely spoken, especially in eastern and southern Ukraine. According to the 2001 census, 67.5 percent of the population declared Ukrainian as their native language and 29.6 percent declared Russian. Most native Ukrainian speakers know Russian as a second language. Russian was the "de facto" official language of the Soviet Union but both Russian and Ukrainian were official languages in the Soviet Union and in the schools of the Ukrainian SSR learning Ukrainian was mandatory. Effective in August 2012, a new law on regional languages entitles any local language spoken by at least a 10% minority be declared official within that area. Russian was within weeks declared as a regional language in several southern and eastern oblasts (provinces) and cities. Russian can now be used in these cities'/oblasts' administrative office work and documents. On 23 February 2014, following the 2014 Ukrainian revolution, the Ukrainian Parliament voted to repeal the law on regional languages, making Ukrainian the sole state language at all levels; however, the repeal was not signed by acting President Turchynov and current President Poroshenko.
Ukrainian is mainly spoken in western and central Ukraine. In western Ukraine, Ukrainian is also the dominant language in cities (such as Lviv). In central Ukraine, Ukrainian and Russian are both equally used in cities, with Russian being more common in Kiev, while Ukrainian is the dominant language in rural communities. In eastern and southern Ukraine, Russian is primarily used in cities, and Ukrainian is used in rural areas. These details result in a significant difference across different survey results, as even a small restating of a question switches responses of a significant group of people.
For a large part of the Soviet era, the number of Ukrainian speakers declined from generation to generation, and by the mid-1980s, the usage of the Ukrainian language in public life had decreased significantly. Following independence, the government of Ukraine began restoring the image and usage of Ukrainian language through a policy of Ukrainisation. Today, all foreign films and TV programs, including Russian ones, are subtitled or dubbed in Ukrainian.
According to the Constitution of the Autonomous Republic of Crimea, Ukrainian is the only state language of the republic. However, the republic's constitution specifically recognises Russian as the language of the majority of its population and guarantees its usage 'in all spheres of public life'. Similarly, the Crimean Tatar language (the language of 12 percent of population of Crimea) is guaranteed a special state protection as well as the 'languages of other ethnicities'. Russian speakers constitute an overwhelming majority of the Crimean population (77 percent), with Crimean Tatar speakers 11.4 percent and Ukrainian speakers comprising just 10.1 percent. But in everyday life the majority of Crimean Tatars and Ukrainians in Crimea use Russian.
Religion.
Estimates compiled by the independent Razumkov Centre in a nationwide survey in 2006 found that 75.2 percent of the respondents believe in God and 22 percent said they did not believe in God. 37.4 percent said that they attended church on regular basis.
Among Ukrainians who are affiliated with an organised religion, the most common religion in Ukraine is Eastern Orthodoxy, currently split between three Church bodies: the Ukrainian Orthodox Church – Kiev Patriarchate, the Ukrainian Orthodox Church autonomous church body under the Patriarch of Moscow, and the Ukrainian Autocephalous Orthodox Church.
A distant second by the number of the followers is the Eastern Rite Ukrainian Greek Catholic Church, which practices a similar liturgical and spiritual tradition as Eastern Orthodoxy, but is in communion with the Holy See of the Roman Catholic Church and recognises the primacy of the Pope as head of the Church.
Additionally, there are 863 Latin Rite Catholic communities, and 474 clergy members serving some one million Latin Rite Catholics in Ukraine. The group forms some 2.19 percent of the population and consists mainly of ethnic Poles and Hungarians, who live predominantly in the western regions of the country. Protestants in Ukraine form around 2.19 percent of the population. Smaller groups are also present.
There are an estimated 500,000 Muslims in Ukraine and about 300,000 of them are Crimean Tatars. There are 487 registered Muslim communities, 368 of them on Crimea. In addition, some 50,000 Muslims live in Kiev; mostly foreign-born.
The Jewish population is a tiny fraction of what it was before World War II. In Tsarist times, Ukraine had been part of the Pale of Settlement, to which Jews were largely restricted in the Russian Empire. The largest Jewish communities in 1926 were in Odessa, 154,000 or 36.5% of the total population; and Kiev, 140,500 or 27.3%. Orthodox Judaism has the strongest presence in Ukraine. Smaller Reform and Conservative ("Masorti") Jewish communities exist as well.
One 2006 survey put the number of non-religious in Ukraine at approximately 11.1% of the population.
Famines and migration.
The famines of the 1930s, followed by the devastation of World War II, comprised a demographic disaster. Life expectancy at birth fell to a level as low as ten years for females and seven for males in 1933 and plateaued around 25 for females and 15 for males in the period 1941–44. According to "The Oxford companion to World War II", "Over 7 million inhabitants of Ukraine, more than one-sixth of the pre-war population, were killed during the Second World War."
Significant migration took place in the first years of Ukrainian independence. More than one million people moved into Ukraine in 1991–92, mostly from the other former Soviet republics. In total, between 1991 and 2004, 2.2 million immigrated to Ukraine (among them, 2 million came from the other former Soviet Union states), and 2.5 million emigrated from Ukraine (among them, 1.9 million moved to other former Soviet Union republics). Currently, immigrants constitute an estimated 14.7% of the total population, or 6.9 million people; this is the fourth largest figure in the world. In 2006, there were an estimated 1.2 million Canadians of Ukrainian ancestry, giving Canada the world's third-largest Ukrainian population behind Ukraine itself and Russia. There are also large Ukrainian immigrant communities in the United States, Australia, Brazil and Argentina.
Health.
The Ukrainian Red Cross Society was established in April 1918 in Kiev as an independent humanitarian society of the Ukrainian People's Republic. Its immediate tasks were to help refugees and prisoners of war, care for handicapped people and orphaned children, fight famine and epidemics, support and organize sick quarters, hospitals and public canteens. At present, society involves more than 6.3 million supporters and activists. Its Visiting Nurses Service has 3,200 qualified nurses. The organization takes part in more than 40 humanitarian programmes all over Ukraine, which are mostly funded by public donation and corporate partnerships. By its own estimates, the Society annually provides services to more than 105,000 lonely, elderly people, about 23,000 people disabled during the Second World War and handicapped workers, more than 25,000 war veterans, and more than 8,000 adults handicapped since childhood. Assistance for orphaned and disabled children is also rendered.
Ukraine's healthcare system is state subsidised and freely available to all Ukrainian citizens and registered residents. However, it is not compulsory to be treated in a state-run hospital as a number of private medical complexes do exist nationwide. The public sector employs most healthcare professionals, with those working for private medical centres typically also retaining their state employment as they are mandated to provide care at public health facilities on a regular basis.
All of the country's medical service providers and hospitals are subordinate to the Ministry of Health, which provides oversight and scrutiny of general medical practice as well as being responsible for the day-to-day administration of the healthcare system. Despite this, standards of hygiene and patient-care have fallen.
Hospitals in Ukraine are organised along the same lines as most European nations, according to the regional administrative structure; as a result most towns have their own hospital "(Міська Лікарня)" and many also have district hospitals "(Районна Лікарня)". Larger and more specialised medical complexes tend only to be found in major cities, with some even more specialised units located only in the capital, Kiev. However, all oblasts have their own network of general hospitals which are able to deal with almost all medical problems and are typically equipped with major trauma centres; such hospitals are called 'regional hospitals' "(Обласна Лікарня)".
Ukraine currently faces a number of major public health issues and is considered to be in a demographic crisis because of its high death rate and low birth rate (the current Ukrainian birth rate is 11 births/1,000 population, and the death rate is 16.3 deaths/1,000 population). A factor contributing to the high death rate is a high mortality rate among working-age males from preventable causes such as alcohol poisoning and smoking. In 2008, the country's population was one of the fastest declining in the world at −5% growth. The UN warned that Ukraine's population could fall by as much as 10 million by 2050 if trends did not improve. In addition, obesity, systemic high blood pressure and the HIV endemic are all major challenges facing the Ukrainian healthcare system.
As of March 2009 the Ukrainian government is reforming the health care system, by the creation of a national network of family doctors and improvements in the medical emergency services. former Prime Minister Yulia Tymoshenko put forward (in November 2009) an idea to start introducing a public healthcare system based on health insurance in the spring of 2010.
Education.
According to the Ukrainian constitution, access to free education is granted to all citizens. Complete general secondary education is compulsory in the state schools which constitute the overwhelming majority. Free higher education in state and communal educational establishments is provided on a competitive basis. There is also a small number of accredited private secondary and higher education institutions.
Because of the Soviet Union's emphasis on total access of education for all citizens, which continues today, the literacy rate is an estimated 99.4%. Since 2005, an eleven-year school programme has been replaced with a twelve-year one: primary education takes four years to complete (starting at age six), middle education (secondary) takes five years to complete; upper secondary then takes three years. In the 12th grade, students take Government tests, which are also referred to as school-leaving exams. These tests are later used for university admissions.
The first higher education institutions (HEIs) emerged in Ukraine during the late 16th and early 17th centuries. The first Ukrainian higher education institution was the Ostrozka School, or Ostrozkiy Greek-Slavic-Latin Collegium, similar to Western European higher education institutions of the time. Established in 1576 in the town of Ostrog, the Collegium was the first higher education institution in the Eastern Slavic territories. The oldest university was the Kyiv Mohyla Academy, first established in 1632 and in 1694 officially recognised by the government of Imperial Russia as a higher education institution. Among the oldest is also the Lviv University, founded in 1661. More higher education institutions were set up in the 19th century, beginning with universities in Kharkiv (1805), Kiev (1834), Odessa (1865) and Chernivtsi (1875) and a number of professional higher education institutions, e.g.: Nizhyn Historical and Philological Institute (originally established as the Gymnasium of Higher Sciences in 1805), a Veterinary Institute (1873) and a Technological Institute (1885) in Kharkiv, a Polytechnic Institute in Kiev (1898) and a Higher Mining School (1899) in Katerynoslav. Rapid growth followed in the Soviet period. By 1988 a number of higher education institutions increased to 146 with over 850,000 students. Most HEIs established after 1990 are those owned by private organisations.
The Ukrainian higher education system comprises higher educational establishments, scientific and methodological facilities under national, municipal and self-governing bodies in charge of education. The organisation of higher education in Ukraine is built up in accordance with the structure of education of the world's higher developed countries, as is defined by UNESCO and the UN.
Ukraine has more than 800 higher education institutions and in 2010 the number of graduates reached 654,700 people.
Ukraine produces the fourth largest number of post-secondary graduates in Europe, while being ranked seventh in population. higher education is either state funded or private. Students that study at state expense receive a standard scholarship if their average marks at the end-of-term exams and differentiated test suffice; this rule may be different in some universities. For highest grades, the scholarship is increased by 25%. For most students the government subsidy is not sufficient to cover their basic living expenses. Most universities provide subsidised housing for out-of-city students. Also, it is common for libraries to supply required books for all registered students. Ukrainian universities confer two degrees: the bachelor's degree (4 years) and the master's degree (5–6th year), in accordance with the Bologna process. Historically, Specialist degree (usually 5 years) is still also granted; it was the only degree awarded by universities in the Soviet times.
The Law of Ukraine "On Higher Education" came into force on 6 September 2014. It was approved in Ukrainian Parliament on 1 July 2014. The main changes in the system of higher education: a separate collegiate body to monitor the quality of education was established (Ukrainian: Національне агентство із забезпечення якості вищої освіти); each higher education institution has the right to implement its own educational and research programs; role of the student government was increased; higher education institution has the right freely administer own revenues; 5 following types of higher education qualifications were established: Junior Bachelor, Bachelor, Master, Doctor of Philosophy (PhD) and Doctor of Science; load on lecturers and students was reduced; academic mobility for faculty and students etc.
Regional differences.
Ukrainian is the dominant language in Western Ukraine and in Central Ukraine, while Russian is the dominant language in the cities of Eastern Ukraine and Southern Ukraine. In the Ukrainian SSR schools, learning Russian was mandatory; currently in modern Ukraine, schools with Ukrainian as the language of instruction offer classes in Russian and in the other minority languages.
On the Russian language, on Soviet Union and Ukrainian nationalism, opinion in Eastern Ukraine and Southern Ukraine tends to be the exact opposite of those in Western Ukraine; while opinions in Central Ukraine on these topics tend be less extreme.
Similar historical cleavages also remain evident at the level of individual social identification. Attitudes toward the most important political issue, relations with Russia, differed strongly between Lviv, identifying more with Ukrainian nationalism and the Ukrainian Greek Catholic Church, and Donetsk, predominantly Russian orientated and favourable to the Soviet era, while in central and southern Ukraine, as well as Kiev, such divisions were less important and there was less antipathy toward people from other regions (a poll by the Research & Branding Group held March 2010 showed that the attitude of the citizens of Donetsk to the citizens of Lviv was 79% positive and that the attitude of the citizens of Lviv to the citizens of Donetsk was 88% positive). However, all were united by an overarching Ukrainian identity based on shared economic difficulties, showing that other attitudes are determined more by culture and politics than by demographic differences. Surveys of regional identities in Ukraine have shown that the feeling of belonging to a "Soviet identity" is strongest in the Donbas (about 40%) and the Crimea (about 30%).
During elections voters of Western and Central Ukrainian oblasts (provinces) vote mostly for parties (Our Ukraine, Batkivshchyna) and presidential candidates (Viktor Yuschenko, Yulia Tymoshenko) with a pro-Western and state reform platform, while voters in Southern and Eastern oblasts vote for parties (CPU, Party of Regions) and presidential candidates (Viktor Yanukovych) with a pro-Russian and status quo platform. However, this geographical division is decreasing.
Culture.
Ukrainian customs are heavily influenced by Christianity, the dominant religion in the country. Gender roles also tend to be more traditional, and grandparents play a greater role in bringing up children, than in the West. The culture of Ukraine has also been influenced by its eastern and western neighbours, reflected in its architecture, music and art.
The Communist era had quite a strong effect on the art and writing of Ukraine. In 1932, Stalin made socialist realism state policy in the Soviet Union when he promulgated the decree "On the Reconstruction of Literary and Art Organisations". This greatly stifled creativity. During the 1980s glasnost (openness) was introduced and Soviet artists and writers again became free to express themselves as they wanted.
The tradition of the Easter egg, known as pysanky, has long roots in Ukraine. These eggs were drawn on with wax to create a pattern; then, the dye was applied to give the eggs their pleasant colours, the dye did not affect the previously wax-coated parts of the egg. After the entire egg was dyed, the wax was removed leaving only the colourful pattern. This tradition is thousands of years old, and precedes the arrival of Christianity to Ukraine. In the city of Kolomyia near the foothills of the Carpathian Mountains in 2000 was built the museum of Pysanka which won a nomination as the monument of modern Ukraine in 2007, part of the Seven Wonders of Ukraine action.
Weaving and embroidery.
Artisan textile arts play an important role in Ukrainian culture, especially in Ukrainian wedding traditions. Ukrainian embroidery, weaving and lace-making are used in traditional folk dress and in traditional celebrations. Ukrainian embroidery varies depending on the region of origin and the designs have a long history of motifs, compositions, choice of colours and types of stitches. Use of colour is very important and has roots in Ukrainian folklore. Embroidery motifs found in different parts of Ukraine are preserved in the Rushnyk Museum in Pereiaslav-Khmelnytskyi.
National dress is woven and highly decorated. Weaving with handmade looms is still practised in the village of Krupove, situated in Rivne Oblast. The village is the birthplace of two famous personalities in the scene of national crafts fabrication. Nina Myhailivna and Uliana Petrivna with international recognition. To preserve this traditional knowledge the village is planning to open a local weaving centre, a museum and weaving school.
Literature.
The history of Ukrainian literature dates back to the 11th century, following the Christianisation of the Kievan Rus'. The writings of the time were mainly liturgical and were written in Old Church Slavonic. Historical accounts of the time were referred to as "chronicles", the most significant of which was the Primary Chronicle. Literary activity faced a sudden decline during the Mongol invasion of Rus'.
Ukrainian literature again began to develop in the 14th century, and was advanced significantly in the 16th century with the introduction of print and with the beginning of the Cossack era, under both Russian and Polish dominance. The Cossacks established an independent society and popularized a new kind of epic poems, which marked a high point of Ukrainian oral literature. These advances were then set back in the 17th and early 18th centuries, when publishing in the Ukrainian language was outlawed and prohibited. Nonetheless, by the late 18th century modern literary Ukrainian finally emerged.
The 19th century initiated a vernacular period in Ukraine, led by Ivan Kotliarevsky's work , the first publication written in modern Ukrainian. By the 1830s, Ukrainian romanticism began to develop, and the nation's most renowned cultural figure, romanticist poet-painter Taras Shevchenko emerged. Where Ivan Kotliarevsky is considered to be the father of literature in the Ukrainian vernacular; Shevchenko is the father of a national revival.
Then, in 1863, use of the Ukrainian language in print was effectively prohibited by the Russian Empire. This severely curtailed literary activity in the area, and Ukrainian writers were forced to either publish their works in Russian or release them in Austrian controlled Galicia. The ban was never officially lifted, but it became obsolete after the revolution and the Bolsheviks' coming to power.
Ukrainian literature continued to flourish in the early Soviet years, when nearly all literary trends were approved (the most important literary figures of that time were Mykola Khvylovy, Valerian Pidmohylny, Mykola Kulish, Mykhayl Semenko and some others). These policies faced a steep decline in the 1930s, when prominent representatives as well as many others were killed by NKVD as part of the Great Purge. In general around 223 writers were repressed by what was known as the Executed Renaissance. These repressions were part of Stalin's implemented policy of socialist realism. The doctrine did not necessarily repress the use of the Ukrainian language, but it required that writers follow a certain style in their works.
In post-Stalinist times literary activities continued to be somewhat limited under the Communist Party. The most famous figures of Ukrainian post-war Soviet literature were Lina Kostenko, Dmytro Pavlychko, , Ivan Drach, Oles Honchar, Vasyl Stus, Vasyl Symonenko.
Literary freedom appeared in late 1980s — early 1990s with the process of collapse of the USSR and reestablishing of Ukrainian independence in 1991. Among the most famous writers of the post-Soviet period are Oksana Zabuzhko, Yurii Andrukhovych, , Serhiy Zhadan, Taras Prokhasko, Jaroslav Melnik, , Yuriy Pokalchuk, Yuriy Vynnychuk, Andrey Kurkov.
Architecture.
Ukrainian architecture is a term that describes the motifs and styles that are found in structures built in modern Ukraine, and by Ukrainians worldwide.
These include initial roots which were established in the Eastern Slavic state of Kievan Rus'. After the 12th century, the distinct architectural history continued in the principalities of Galicia-Volhynia. During the epoch of the Zaporozhian Cossacks, a new style unique to Ukraine was developed under the western influences of the Polish–Lithuanian Commonwealth. After the union with the Tsardom of Russia, many structures in the larger eastern, Russian-ruled area were built in the styles of Russian architecture of that period, whilst the western Galicia was developed under Austro-Hungarian architectural influences. Ukrainian national motifs would finally be used during the period of the Soviet Union and in modern independent Ukraine.
The great churches of the Rus', built after the adoption of Christianity in 988, were the first examples of monumental architecture in the East Slavic lands. The architectural style of the Kievan state was strongly influenced by the Byzantine. Early Eastern Orthodox churches were mainly made of wood, with the simplest form of church becoming known as a cell church. Major cathedrals often featured scores of small domes, which led some art historians to take this as an indication of the appearance of pre-Christian pagan Slavic temples.
Several examples of these churches survive; however, during the 16th, 17th and 18th centuries, many were externally rebuilt in the Ukrainian Baroque style (see below). Examples include the grand St. Sophia of Kiev – the year 1017 is the earliest record of foundation laid, Church of the Saviour at Berestove – built from 1113 to 1125 and St. Cyril's Church, circa 12th-century. All can still be found in the Ukrainian capital. Several buildings were reconstructed during the late-19th century, including the Assumption Cathedral in Volodymyr-Volynskyi, built in 1160 and reconstructed in 1896–1900, the Paraskevi church in Chernihiv, built in 1201 with reconstruction done in the late 1940s, and the Golden gates in Kiev, built in 1037 and reconstructed in 1982. The latter's reconstruction was criticised by some art and architecture historians as a revivalist fantasy. Unfortunately little secular or vernacular architecture of Kievan Rus' has survived.
As Ukraine became increasingly integrated into the Russian Empire, Russian architects had the opportunity to realise their projects in the picturesque landscape that many Ukrainian cities and regions offered. St. Andrew's Church of Kiev (1747–1754), built by Bartolomeo Rastrelli, is a notable example of Baroque architecture, and its location on top of the Kievan mountain made it a recognisable monument of the city. An equally notable contribution of Rasetrelli was the Mariyinsky Palace, which was built to be a summer residence to Russian Empress Elizabeth. During the reign of the last Hetman of Ukraine, Kirill Razumovsky, many of the Cossack Hetmanate's towns such as Hlukhiv, Baturyn and Koselets had grandiose projects built by Andrey Kvasov. Russia eventually conquered the south of Ukraine and Crimea, and renamed them as New Russia. New cities such as Nikolayev, Odessa, Kherson and Sevastopol were founded. These would contain notable examples of Imperial Russian architecture.
In 1934, the capital of Soviet Ukraine moved from Kharkiv to Kiev. Previously, the city was seen as only a regional centre, hence received little attention. All of that was to change, at great price. The first examples of Stalinist architecture were already showing, and, in light of the official policy, a new city was to be built on top of the old one. This meant that much-admired examples such as the St. Michael's Golden-Domed Monastery were destroyed. Even the St. Sophia Cathedral was under threat. Also, the Second World War contributed to the wreckage. After the war, a new project for the reconstruction of central Kiev transformed Khreshchatyk avenue into a notable example of Stalinism in Architecture. However, by 1955, the new politics of architecture once again stopped the project from fully being realised.
The task for modern Ukrainian architecture is diverse application of modern aesthetics, the search for an architect's own artistic style and inclusion of the existing historico-cultural environment. An example of modern Ukrainian architecture is the reconstruction and renewal of the Maidan Nezalezhnosti in central Kiev. Despite the limit set by narrow space within the plaza, the engineers were able to blend together the uneven landscape, and use underground space for a new shopping centre.
A major project, which may take up most of the 21st century, is the construction of the Kiev City-Centre on the Rybalskyi Peninsula, which, when finished, will include a dense skyscraper park amid the picturesque landscape of the Dnieper.
Music.
Music is a major part of Ukrainian culture, with a long history and many influences. From traditional folk music, to classical and modern rock, Ukraine has produced several internationally recognised musicians including Kirill Karabits, Okean Elzy and Ruslana. Elements from traditional Ukrainian folk music made their way into Western music and even into modern jazz.
Ukrainian music sometimes presents a perplexing mix of exotic melismatic singing with chordal harmony. The most striking general characteristic of authentic ethnic Ukrainian folk music is the wide use of minor modes or keys which incorporate augmented 2nd intervals.
During the Baroque period, music was an important discipline for those that had received a higher education in Ukraine. It had a place of considerable importance in the curriculum of the Kyiv-Mohyla Academy. Much of the nobility was well versed in music with many Ukrainian Cossack leaders such as (Mazepa, Paliy, Holovatyj, Sirko) being accomplished players of the kobza, bandura or torban.
The first dedicated musical academy was set up in Hlukhiv, Ukraine in 1738 and students were taught to sing, play violin and bandura from manuscripts. As a result, many of the earliest composers and performers within the Russian empire were ethnically Ukrainian, having been born or educated in Hlukhiv, or had been closely associated with this music school.
See: Dmytro Bortniansky, Maksym Berezovsky and Artemiy Vedel.
Ukrainian classical music falls into three distinct categories defined by whether the composer was of Ukrainian ethnicity living in Ukraine, a composer of non-Ukrainian ethnicity who was born or at some time was a citizen of Ukraine, or an ethnic Ukrainian living outside of Ukraine within the Ukrainian diaspora. The music of these three groups differs considerably, as do the audiences for whom they cater.
Since the mid-1960s, Western-influenced pop music has been growing in popularity in Ukraine. Folk singer and harmonium player Mariana Sadovska is prominent. Ukrainian pop and folk music arose with the international popularity of groups and performers like Vopli Vidoplyasova, Dakh Daughters, Dakha Brakha, Ivan Dorn and Okean Elzy.
Cinema.
Ukraine has had an influence on the history of the cinema. Ukrainian directors Alexander Dovzhenko, often cited as one of the most important early Soviet filmmakers, as well as being a pioneer of Soviet montage theory, Dovzhenko Film Studios, and Sergei Parajanov, Armenian film director and artist who made significant contributions to Ukrainian, Armenian and Georgian cinema. He invented his own cinematic style, Ukrainian poetic cinema, which was totally out of step with the guiding principles of socialist realism.
Other important directors including Kira Muratova, Larisa Shepitko, Sergei Bondarchuk, Leonid Bykov, Yuri Ilyenko, Leonid Osyka, Ihor Podolchak with his Delirium and Maryna Vroda. Many Ukrainian actors have achieved international fame and critical success, including: Vera Kholodnaya, Bohdan Stupka, Milla Jovovich, Olga Kurylenko, Mila Kunis.
Despite a history of important and successful productions, the industry has often been characterised by a debate about its identity and the level of European and Russian influence. Ukrainian producers are active in international co-productions and Ukrainian actors, directors and crew feature regularly in Russian (Soviet in past) films. Also successful films have been based on Ukrainian people, stories or events, including Battleship Potemkin, Man with a Movie Camera, Everything Is Illuminated.
Ukrainian State Film Agency owns National Oleksandr Dovzhenko Film Centre, film copying laboratory and archive, takes part in hosting of the Odessa International Film Festival, and Molodist is the only one FIAPF accredited International Film Festival held in Ukraine; competition program is devoted to student, first short and first full feature films from all over the world. Held annually in October.
Media.
Ukrayinska Pravda was founded by Georgiy Gongadze in April 2000 (the day of the Ukrainian constitutional referendum). Published mainly in Ukrainian with selected articles published in or translated to Russian and English, the newspaper has particular emphasis on the politics of Ukraine. Freedom of the press in Ukraine is considered to be among the freest of the post-Soviet states other than the Baltic states.
Freedom House classifies the Internet in Ukraine as "free" and the press as "partly free". Press freedom has significantly improved since the Orange Revolution of 2004. However, in 2010 Freedom House perceived "negative trends in Ukraine".
Kiev dominates the media sector in Ukraine: the Kyiv Post is Ukraine's leading English-language newspaper. National newspapers Den, Mirror Weekly, tabloids, such as The Ukrainian Week or Focus (Russian), and television and radio are largely based there, although Lviv is also a significant national media centre. The National News Agency of Ukraine, Ukrinform was founded here in 1918. The Ukraine publishing sector, including books, directories and databases, journals, magazines and business media, newspapers and news agencies, has a combined turnover. Sanoma publishing Ukrainian editions of such magazines as Esquire, Harpers Bazaar and National Geographic Magazine. BBC Ukrainian started its broadcasts in 1992.
Ukrainians listen to radio programming, such as Radio Ukraine or Radio Liberty, largely commercial, on average just over two-and-a-half hours a day. Several television channels operate, and many Websites are popular.
Sport.
Ukraine greatly benefited from the Soviet emphasis on physical education. Such policies left Ukraine with hundreds of stadia, swimming pools, gymnasia and many other athletic facilities. The most popular sport is football. The top professional league is the Vyscha Liha ("premier league").
Many Ukrainians also played for the Soviet national football team, most notably Ihor Belanov and Oleh Blokhin, winners of the prestigious Golden Ball Award. This award was only presented to one Ukrainian after the dissolution of the Soviet Union, Andriy Shevchenko. The national team made its debut in the 2006 FIFA World Cup, and reached the quarterfinals before losing to eventual champions, Italy. Ukrainians also fared well in boxing, where the brothers Vitali and Wladimir Klitschko have held world heavyweight championships.
Sergey Bubka held the record in the Pole vault from 1993 to 2014; with great strength, speed and gymnastic abilities, he was voted the world's best athlete on several occasions.
Basketball is becoming popular in Ukraine. In 2011, Ukraine was granted a right to organize EuroBasket 2015. Two years later the Ukraine national basketball team finished 6th in EuroBasket 2013 and qualified to FIBA World Cup for the first time in its history. Euroleague participant Budivelnyk Kyiv is the strongest professional basketball club in Ukraine.
Chess is a popular sport in Ukraine. Ruslan Ponomariov is the former world champion. There are about 85 Grandmasters and 198 International Masters in Ukraine.
Ukraine made its Olympic debut at the 1994 Winter Olympics. So far, Ukraine at the Olympics has been much more successful in Summer Olympics (115 medals in five appearances) than in the Winter Olympics. Ukraine is currently ranked 35th by number of gold medals won in the All-time Olympic Games medal count, with every country above it, except for Russia, having more appearances.
Cuisine.
The traditional Ukrainian diet includes chicken, pork, beef, fish and mushrooms. Ukrainians also tend to eat a lot of potatoes, grains, fresh, boiled or pickled vegetables. Popular traditional dishes include (boiled dumplings with mushrooms, potatoes, sauerkraut, cottage cheese, cherries or berries), nalysnyky (pancakes with cottage cheese, poppy seeds, mushrooms, caviar or meat), kapuśniak (soup made with meat, potatoes, carrots, onions, cabbage, millet, tomato paste, spices and fresh herbs), borsch (soup made of beets, cabbage and mushrooms or meat), (stuffed cabbage rolls filled with rice, carrots, onion and minced meat) and pierogi (dumplings filled with boiled potatoes and cheese or meat). Ukrainian specialties also include Chicken Kiev and Kiev cake. Ukrainians drink stewed fruit, juices, milk, buttermilk (they make cottage cheese from this), mineral water, tea and coffee, beer, wine and .
Notes.
a. Among the Ukrainians that rose to the highest offices in the Russian Empire were Aleksey Razumovsky, Alexander Bezborodko and Ivan Paskevich. Among the Ukrainians who greatly influenced the Russian Orthodox Church in this period were Stephen Yavorsky, Feofan Prokopovich and Dimitry of Rostov.
b. Estimates on the number of deaths vary. Official Soviet data is not available because the Soviet government denied the existence of the famine. See the Holodomor article for details. Sources differ on interpreting various statements from different branches of different governments as to whether they amount to the official recognition of the Famine as Genocide by the country. For example, after the statement issued by the Latvian Sejm on 13 March 2008, the total number of countries is given as 19 (according to "Ukrainian BBC": ), 16 (according to "Korrespondent", Russian edition: ), "more than 10" (according to "Korrespondent", Ukrainian edition: ) Retrieved 27 January 2008.
c. These figures are likely to be much higher, as they do not include Ukrainians from nations or Ukrainian Jews, but instead only ethnic Ukrainians, from the Ukrainian SSR.
d. This figure excludes POW deaths.
e. Russia and Kazakhstan are the first and second largest but both these figures include European and Asian territories. Russia is the only country possessing European territories larger than Ukraine.
f. According to the official 2001 census data (by nationality; by language) about 75 percent of Kiev's population responded 'Ukrainian' to the native language (ridna mova) census question, and roughly 25 percent responded 'Russian'. On the other hand, when the question 'What language do you use in everyday life?' was asked in the 2003 sociological survey, the Kievans' answers were distributed as follows: 'mostly Russian': 52 percent, 'both Russian and Ukrainian in equal measure': 32 percent, 'mostly Ukrainian': 14 percent, 'exclusively Ukrainian': 4.3 percent.
g. Such writings were also the base for Russian and Belarusian literature.

</doc>
<doc id="31752" url="https://en.wikipedia.org/wiki?curid=31752" title="Ulysses S. Grant">
Ulysses S. Grant

Ulysses S. Grant (born Hiram Ulysses Grant; April 27, 1822 – July 23, 1885) was the 18th President of the United States (1869–77). As Commanding General of the United States Army (1864–69), Grant worked closely with President Abraham Lincoln to lead the Union Army to victory over the Confederacy in the American Civil War. He implemented Congressional Reconstruction, often at odds with Lincoln's successor, Andrew Johnson. Twice elected president, Grant led the Republicans in their effort to remove the vestiges of Confederate nationalism and slavery, protect African-American citizenship, and support economic prosperity nationwide. His presidency has often come under criticism for protecting corrupt associates and in his second term leading the nation into a severe economic depression.
Grant graduated in 1843 from the U.S. Military Academy at West Point, served in the Mexican–American War and initially retired in 1854. He struggled financially in civilian life. When the Civil War began in 1861, he rejoined the U.S. Army. In 1862, Grant took control of Kentucky and most of Tennessee, and led Union forces to victory in the Battle of Shiloh, earning a reputation as an aggressive commander. He incorporated displaced African American slaves into the Union war effort. In July 1863, after a series of coordinated battles, Grant defeated Confederate armies and seized Vicksburg, giving the Union control of the Mississippi River and dividing the Confederacy in two. After his victories in the Chattanooga Campaign, Lincoln promoted him to lieutenant-general and Commanding General of the United States Army in March 1864. Grant confronted Robert E. Lee in a series of bloody battles, trapping Lee's army in their defense of Richmond. Grant coordinated a series of devastating campaigns in other theaters. In April 1865, Lee surrendered to Grant at Appomattox, effectively ending the war. Historians have hailed Grant's military genius, and his strategies are featured in military history textbooks, but a minority contend that he won by brute force rather than superior strategy.
After the Civil War, Grant led the army's supervision of Reconstruction in the former Confederate states. Elected president in 1868 and reelected in 1872, Grant stabilized the nation during the turbulent Reconstruction period, prosecuted the Ku Klux Klan, and enforced civil and voting rights laws using the army and the Department of Justice. He used the army to build the Republican Party in the South, based on black voters, Northern newcomers ("carpetbaggers"), and native Southern white supporters ("scalawags"). After the disenfranchisement of some former Confederates, Republicans gained majorities and African Americans were elected to Congress and high state offices. In his second term, the Republican coalitions in the South splintered and were defeated one by one as redeemers (conservative whites) regained control using coercion and violence. Grant's Indian peace policy initially reduced frontier violence, but is best known for the Great Sioux War of 1876, where George Custer and his regiment were killed at the Battle of the Little Bighorn. Grant responded to charges of corruption in executive offices more than any other 19th Century president. He appointed the first Civil Service Commission and signed legislation ending the corrupt moiety system.
In foreign policy, Grant sought to increase American trade and influence, while remaining at peace with the world. His administration successfully resolved the "Alabama" Claims by the Treaty of Washington with Great Britain, ending wartime tensions. Grant avoided war with Spain over the Virginius Affair, but Congress rejected his attempted annexation of the Dominican Republic. In trade policy, Grant's administration implemented a gold standard and sought to strengthen the dollar. Corruption charges escalated during his second term, while his response to the Panic of 1873 proved ineffective nationally in halting the five-year industrial depression that produced high unemployment, low prices, low profits, and bankruptcies. Grant left office in 1877 and embarked on a widely praised world tour lasting over two years.
In 1880, Grant was unsuccessful in obtaining a Republican presidential nomination for a third term. Facing severe investment reversals and dying of throat cancer, he completed his memoirs, which proved a major critical and financial success. His death in 1885 prompted an outpouring of national unity. 20th century historical evaluations were negative about his presidency before recovering somewhat beginning in the 1980s. Scholars rank his presidency below the average of other presidents. Grant's critics take a negative view of his economic mismanagement and his failed Dominican Republic annexation treaty, while admirers emphasize his concern for Native Americans and enforcement of civil and voting rights.
Early life.
Hiram Ulysses Grant was born in Point Pleasant, Ohio, on April 27, 1822, to Jesse Root Grant, a tanner, and Hannah (née Simpson) Grant. His ancestors Matthew and Priscilla Grant arrived aboard the "Mary and John" at Massachusetts Bay Colony in 1630. Grant's great-grandfather fought in the French and Indian War, and his grandfather served in the American Revolution at Bunker Hill. Grant's father was a Whig Party supporter with abolitionist sentiments. In 1823, the family moved to the village of Georgetown in Brown County, Ohio, where five more siblings were born: Simpson, Clara, Orvil, Jennie, and Mary. Young Grant regularly attended public schools and later was enrolled in private schools. While hating the tannery, he chose work on his father's farm. Unlike his siblings, Grant was not forced to attend church by his Methodist parents; for the rest of his life, he prayed privately and never officially joined any denomination. Observers, including his own son, thought he was an agnostic. In his youth, Grant developed an unusual ability to work with and control horses. As a general he rode the strongest and most challenging horse available, and was sometimes injured in riding.
When Grant was 17, Congressman Thomas L. Hamer nominated him to the United States Military Academy at West Point, New York. Hamer mistakenly wrote down the name as "Ulysses S. Grant of Ohio", and this became his adopted name. His nickname became "Sam" among army colleagues at the academy since the initials "U.S." also stood for "Uncle Sam". As he later recalled it, "a military life had no charms for me"; he was lax in his studies, but he achieved above-average grades in mathematics and geology. Quiet by nature, he established a few intimate friends, including Frederick Tracy Dent and Rufus Ingalls. Grant developed a reputation as a fearless and expert horseman known as a horse whisperer, setting an equestrian high-jump record that stood for almost 25 years. He also studied under Romantic artist Robert Walter Weir and produced nine surviving artworks. He graduated in 1843, ranking 21st in a class of 39. Glad to leave the academy, his plan was to resign his commission after his four-year term of duty. Despite his excellent horsemanship, he was not assigned to the cavalry (assignments were determined by class rank, not aptitude), but to the 4th Infantry Regiment. He was made regimental quartermaster, managing supplies and equipment, with the rank of brevet second lieutenant.
Early military career and personal life.
Grant's first assignment after graduation took him to the Jefferson Barracks near St. Louis, Missouri. It was the nation's largest military base in the west, commanded by Colonel Stephen W. Kearny. Grant was happy with his new commander, but looked forward to the end of his military service and a possible teaching career. He spent some of his time in Missouri visiting the family of his West Point classmate, Frederick Tracy Dent; he became engaged to Dent's sister, Julia, in 1844.
Amid rising tensions with Mexico, Grant's unit shifted to Louisiana as part of the Army of Observation under Major General Zachary Taylor. When the Mexican–American War broke out in 1846, the Army entered Mexico. Although a quartermaster, Grant led a cavalry charge at the Battle of Resaca de la Palma. At Monterrey he demonstrated his equestrian ability, by volunteering to carry a dispatch through sniper-lined streets while hanging off the side of his horse, keeping the animal between him and the enemy. President James K. Polk, wary of Taylor's growing popularity, divided his forces, sending some troops (including Grant's unit) to form a new army under Major General Winfield Scott. Scott's army landed at Veracruz and advanced toward Mexico City. The army met the Mexican forces at the battles of Molino del Rey and Chapultepec outside Mexico City. Grant was a quartermaster in charge of supplies and did not have a combat role, but he yearned for one and finally was allowed to take part in dangerous missions. At Chapultepec, men under Grant's direction dragged a disassembled howitzer into a church steeple, reassembled it, and bombarded nearby Mexican troops. His bravery and initiative earned him brevet promotions; he became a temporary captain while his permanent rank was lieutenant. Scott's army entered the city, and the Mexicans agreed to peace soon afterward.
During this war Grant studied the tactics and strategies of Scott and others, often second guessing their moves beforehand. In his "Memoirs", he wrote that this is how he learned about military leadership, and in retrospect identified his leadership style with Taylor's. Even so, he believed that the Mexican war was wrongful and that the territorial gains from the war were designed to expand slavery. Grant reflected in 1883, "I was bitterly opposed to the measure, and to this day, regard the war, which resulted, as one of the most unjust ever waged by a stronger against a weaker nation." He opined that the Civil War was punishment inflicted on the nation for its aggression in Mexico.
Grant's mandatory service expired during the war, but he chose to remain a soldier. Four years after becoming engaged, he married Julia on August 22, 1848. They had four children: Frederick, Ulysses Jr. ("Buck"), Ellen ("Nellie"), and Jesse. Grant's first post-war assignments took him and Julia to Detroit and then to Sackets Harbor, New York. In 1852, Grant's next assignment sent him west to Fort Vancouver in the Oregon Territory. Julia, who was eight months pregnant with Ulysses Jr., did not accompany him. While traveling overland through Panama, an outbreak of cholera among his fellow travelers caused 150 fatalities; Grant arranged makeshift transportation and hospital facilities to care for the sick. He debarked in San Francisco during the height of the California Gold Rush.
Grant's time in the Pacific Northwest followed the Cayuse War; the army was stationed there to keep peace between settlers and Indians. To supplement a military salary which was inadequate to support his family, Grant tried and failed at several business ventures, confirming Jesse Grant's belief that his son had no head for business. He was unhappy being separated from his family, and rumors circulated that he was drinking to excess. Historians overwhelmingly agree that his drunkenness, although off duty, at the time was a fact, though there are no eyewitness reports extant.
Promoted to captain in the summer of 1853, Grant was assigned to command Company F, 4th Infantry, at Fort Humboldt in California. The commanding officer at Fort Humboldt, Lieutenant Colonel Robert C. Buchanan, received reports that Grant became intoxicated off-duty while seated at the pay officer's table. In lieu of a court-martial, Buchanan gave Grant an ultimatum to resign; he did so, effective July 31, 1854, without explanation and returned to St. Louis. The War Department stated on his record, "Nothing stands against his good name." After Grant's retirement, rumors persisted in the regular army of his drinking. Years later, he said, "the vice of intemperance (drunkenness) had not a little to do with my decision to resign."
Civilian struggles and politics.
At age 32, with no civilian vocation, Grant struggled through seven financially lean years. His father initially offered him a place in the Galena, Illinois, branch of the family's tannery and leather goods business, on condition that Julia and the children stay with her parents in Missouri or with the Grants in Kentucky. Ulysses and Julia opposed another separation and declined the offer. In 1855, Grant farmed on his brother-in-law's property near St. Louis, using slaves owned by Julia's father. The farm was not successful and to earn money he sold firewood on St. Louis street corners. The next year, the Grants moved to land on Julia's father's farm, and built a home Grant called "Hardscrabble". Julia hated the rustic house, which she described as an "unattractive cabin". In 1857, Grant acquired a slave, a thirty-five-year-old man named William Jones, from his father-in-law. The Panic of 1857 devastated farmers, including Grant, who was forced to rent out Hardscrabble the following year.
Having met with no success farming, the Grants left the farm when their fourth and final child was born in 1858. The following year in 1859 Grant freed his only slave Jones, who was 35 years old and worth about $1,500, instead of selling him at a time when Grant desperately needed money. For the next year, the family took a small house in St. Louis where he worked with Julia's cousin Harry Boggs as a bill collector, again without success. In 1860, Jesse offered him the job in Galena without conditions, and Grant accepted. The leather shop, "Grant & Perkins", sold harnesses, saddles, and other leather goods, and purchased hides from farmers in the prosperous Galena area. Grant and family moved to a rental house that year.
After Grant had retired from the military, many considered him allied politically to Julia's father, Frederick Dent, a prominent Missouri Democrat. In the 1856 election, Grant cast his first presidential vote for the Democrat, James Buchanan, later saying he was really voting against John C. Frémont, the first Republican candidate, over concern that Frémont's anti-slavery position would motivate southern states to secede. In 1859, Grant's vote for Buchanan and his political affiliation to his father-in-law cost him an appointment to become county engineer. Grant's own father in Illinois, Jesse, was an outspoken Republican in Galena. In 1860, Grant was an open Democrat, favoring Democrat Stephen A. Douglas over Abraham Lincoln, and Lincoln over the Southern Democrat, John C. Breckinridge. Lacking the residency requirements in Illinois at the time, he could not vote. Grant opposed the southern states' secession at the outbreak of the Civil War and remained loyal to the Union.
Civil War.
On April 12, 1861, the American Civil War began as Confederate troops attacked Fort Sumter in Charleston, South Carolina. Two days later, President Lincoln called for 75,000 volunteers and a mass meeting was held in Galena to encourage recruitment. Recognized as a military professional, Grant was asked to lead the ensuing effort. Before the attack on Fort Sumter, Grant had not reacted strongly to Southern secession. The news of the attack came as a shock in Galena, and Grant shared his neighbors' mounting concern about the onset of war. After hearing a speech by his father's attorney, John Aaron Rawlins, Grant found renewed energy in the Union cause. Rawlins later became Grant's aide-de-camp and close friend during the war. Grant recalled with satisfaction that after that first recruitment meeting in Galena, "I never went into our leather store again."
Without any formal rank in the army, Grant helped recruit a company of volunteers and accompanied the regiment to Springfield, the state capital. During this time, Grant quickly perceived that the war would be fought for the most part by volunteers and not career soldiers. Illinois' Governor Richard Yates offered Grant a position recruiting and training volunteer units, which he accepted, but he still wanted a field command in the regular army. He made several efforts through contacts (including Major General George B. McClellan) to acquire such a position. McClellan refused to meet him, remembering Grant's earlier reputation for drinking while stationed in California. Meanwhile, he continued serving at the training camps and made a positive impression on the volunteer Union recruits.
With the aid of his advocate in Washington, Illinois congressman Elihu B. Washburne, Grant was formally promoted to Colonel on June 14, 1861, and put in charge of disciplining the unruly 21st Illinois Volunteer Infantry Regiment. To restore discipline, Grant had one troublemaker bound and gagged to a post for being drunk and disorderly. Transferred to northern Missouri, Grant was promoted by Lincoln to Brigadier General, backdated to May 17, 1861, again with Washburne's support. Believing Grant was a general of "dogged persistence" and "iron will", Major General John C. Frémont assigned Grant command of troops near Cairo, Illinois by the end of August 1861. Under Frémont's authority Grant advanced into Paducah and took the town without a fight.
Belmont, Forts Henry and Donelson.
On November 7, 1861 Grant and his troops crossed the Mississippi to attack Confederate soldiers encamped in Belmont, Missouri. They took the camp, but the reinforced Confederates under Brigadier General Gideon J. Pillow forced a retreat to Cairo. A tactical defeat, the battle nonetheless gave Grant and his volunteers confidence and experience. After Belmont, Grant asked his new commander Henry Halleck (Lincoln had relieved Frémont of command) for permission to move against Fort Henry in Tennessee, which would open the Tennessee River to Union gunboats; Halleck agreed on condition that the attack be conducted in coordination with navy Flag Officer Andrew H. Foote. Grant's troops, in close cooperation with Foote's naval forces captured Fort Henry on February 6, 1862.
Emboldened by Lincoln's call for a general advance of all Union forces, Grant ordered an immediate assault on nearby Fort Donelson, which dominated the Cumberland River (this time without Halleck's permission). On February 15, Grant and Foote met stiff resistance from Confederate forces under Pillow. Reinforced by 10,000 troops, Grant's army totaled 25,000 troops against 12,000 Confederates. Foote's first approach was repulsed, and the Confederates attempted a breakout, pushing Grant's right flank into disorganized retreat. Grant rallied his troops, resumed the offensive, retook the Union right, and attacked Pillow's left. Pillow ordered Confederate troops back into the fort and relinquished command to Brigadier General Simon Bolivar Buckner, who the next day acceded to Grant's demand for his "unconditional and immediate surrender." Lincoln promoted Grant to major-general of volunteers while the Northern press treated Grant as a hero. Playing off his initials, they took to calling him "Unconditional Surrender Grant".
Shiloh and aftermath.
Encamped on the western bank of the Tennessee River, Grant's Army of the Tennessee, now numbering about 45,000 troops, prepared to attack a Confederate army of roughly equal strength at Corinth, Mississippi, a vital railroad junction. The Confederates, led by Generals Albert Sidney Johnston and P.G.T. Beauregard, struck first on April 6, 1862, attacking five divisions of Grant's army bivouacked at Pittsburg Landing, not far from the Shiloh meetinghouse. Grant's troops were not entrenched and were taken by surprise, falling back before the Confederate onslaught. At day's end, the Confederates captured one Union division, but Grant's army was able to hold the Landing. The remaining Union army might have been destroyed, but the Confederates halted due to exhaustion, confusion, and a lack of reinforcements. Grant, bolstered by 18,000 fresh troops from the divisions of Major Generals Don Carlos Buell and Lew Wallace, counterattacked at dawn the next day The Northerners regained the field and forced the rebels to retreat back to Corinth.
In Shiloh's aftermath, the Northern press criticized Grant for high casualties and for his alleged drunkenness during the battle. Shiloh was the costliest battle in American history to that point, with total casualties of about 23,800. Halleck arrived at Pittsburg Landing on April 9, and removed Grant from field command, proceeding to capture Corinth. Discouraged and disappointed, Grant considered resigning his commission, but Brigadier General William Tecumseh Sherman, one of his division commanders, convinced him to stay. Lincoln overruled Grant's critics, saying "I can't spare this man; he fights." Ordered to Washington, Halleck on July 11, reinstated Grant as field commander of the Army of the Tennessee. On September 19, Grant's army defeated Confederates at the Battle of Iuka, then successfully defended Corinth, inflicting heavy casualties on the enemy. On October 25, Grant assumed command of the District of the Tennessee. In November, after Lincoln's preliminary Emancipation Proclamation, Grant ordered units under his command, headed by Chaplain John Eaton, to incorporate contraband slaves into the Union war effort, giving them clothes, shelter, and wages for their services.
Vicksburg campaign.
Located on the high bluffs of the Mississippi River, Vicksburg, Mississippi was the last major obstacle to Union control of that river; both Lincoln and Grant saw it as the key to victory in the West and were determined to take the rebel stronghold. Grant's Army held western Tennessee having almost 40,000 troops available to fight. Grant was aggravated to learn that Lincoln authorized Major General John A. McClernand to raise a separate army for the purpose. Halleck ordered McClernand to Memphis, and placed him under Grant's authority. Grant planned to attack Vicksburg on land from the east while Sherman attacked the fortress from the Mississippi River, but two Confederate cavalry raids, on December 11 and 20, prevented the armies from connecting. On December 29, a Confederate army led by Lieutenant General John C. Pemberton repulsed Sherman's direct approach to Vicksburg at Chickasaw Bayou. McClernand reached Sherman's army, assumed command, and independently of Grant led a campaign that captured Confederate Fort Hindman.
Along with his military responsibilities in the months following Grant's return to command, he was concerned over an expanding illicit cotton trade in his district. He believed the trade undermined the Union war effort, funded the Confederacy, and prolonged the war, while Union soldiers died in the fields. On December 17, he issued General Order No. 11, expelling "Jews, as a class," from the district, saying that Jewish merchants were violating trade regulations. Writing in 2012, historian Jonathan D. Sarna said Grant "issued the most notorious anti-Jewish official order in American history." Historians' opinions vary on Grant's motives for issuing the order. Jewish leaders complained to Lincoln while the Northern press criticized Grant. Lincoln demanded the order be revoked and Grant rescinded it within three weeks. Grant made amends with the Jewish community during his presidency.
On January 29, 1863, Grant assumed personal overall command and during the months of February and March made a series of attempts to advance his army through water-logged terrain to bypass Vicksburg's guns; these also proved ineffective, however, Union soldiers became better trained. On April 16, 1863, Grant ordered Admiral David Porter's Union gunboats south under fire from the Vicksburg batteries to meet up with his Union troops who had marched south down the west side of the Mississippi River. Grant ordered diversionary battles, confusing Pemberton and allowing Grant's army to cross east over the Mississippi, landing troops at Bruinsburg. Continuing eastward, Grant's army captured Port Gibson, Raymond, and Jackson, the state capital and Confederate railroad supply center. Advancing his army to Vicksburg, Grant defeated Pemberton's army at the Battle of Champion Hill on May 16, forcing Pemberton to retreat into Vicksburg. After Grant's men assaulted the Vicksburg entrenchments twice, suffering severe losses, they settled in for a siege lasting seven weeks. Pemberton surrendered Vicksburg to Grant on July 4, 1863.
The fall of Vicksburg gave Union forces control over the Mississippi River and split the Confederacy in two. By that time, Grant's political sympathies fully coincided with the Radical Republicans' aggressive prosecution of the war and emancipation of the slaves. Although the success at Vicksburg was a great morale boost for the Union war effort, Grant received criticism for his decisions and his alleged drunkenness. The personal rivalry between McClernand and Grant continued after Vicksburg, until Grant removed McClernand from command when he contravened Grant by publishing an order without permission. When Secretary of War Edwin M. Stanton suggested Grant be brought back east to run the Army of the Potomac, Grant demurred, writing that he knew the geography and resources of the West better and he did not want to upset the chain of command in the East.
Chattanooga and promotion.
Lincoln commissioned Grant a major general in the regular army and assigned him command of the newly formed Division of the Mississippi in October 1863, including the Armies of the Ohio, Tennessee, and Cumberland. After the Battle of Chickamauga, the Army of the Cumberland retreated into Chattanooga, where they were trapped. When informed of the situation, Grant put Major General George H. Thomas in charge of the besieged army. Taking command, Grant arrived in Chattanooga by horseback, devising plans to resupply the city and break the siege. Lincoln also sent Major General Joseph Hooker and two divisions of the Army of the Potomac to assist. Union forces captured Brown's Ferry and opened a supply line to Bridgeport. On November 23, 1863, Grant organized three armies to attack at Missionary Ridge and Lookout Mountain. Two days later in the early morning, Hooker's forces successfully took Lookout Mountain. Grant ordered Thomas and the Army of the Cumberland to advance when Sherman's army failed to take Missionary Ridge from the northeast. The Army of the Cumberland, led by Major General Philip Sheridan and Brigadier General Thomas J. Wood, charged uphill and captured the Confederate entrenchments on top of the ridge, forcing the rebels into disorganized retreat. The decisive battle gave the Union control of Tennessee and opened Georgia, the heartland of the Confederacy, to Union invasion.
On March 3, 1864 Lincoln promoted Grant to lieutenant general, giving him command of all Union Armies, under direct supervision only to the President. Grant assigned Sherman the Division of the Mississippi and traveled east to Washington D.C., meeting with Lincoln to devise a strategy of total war against the Confederacy. After settling Julia into a house in Georgetown, Grant established his headquarters with General George Meade's Army of the Potomac in Culpeper, Virginia. He devised a strategy of coordinated Union offensives, attacking the rebel armies at the same time to keep the Confederates from shifting reinforcements within their interior lines. Sherman was to pursue Joseph E. Johnston's Army of the Tennessee, while Meade would lead the Army of the Potomac, with Grant in camp, to attack Robert E. Lee's Army of Northern Virginia. Major General Benjamin Butler was to advance towards Richmond from the south, up the James River. If Lee was forced south as expected, Grant would join forces with Butler's armies and be fed supplies from the James. Major General Franz Sigel was to capture the railroad line at Lynchburg, move east, and attack from the Blue Ridge Mountains. Grant knew that Lee had limited manpower and that a war of attrition fought on a battlefield without entrenchments would lead to Lee's defeat.
Grant was now riding a rising tide of popularity, and there was talk that a Union victory early in the year could lead to his candidacy for the presidency. Grant was aware of the rumors, but had ruled out a political candidacy; the possibility would soon vanish with delays on the battlefield.
Overland Campaign and Union victory.
Sigel's and Butler's efforts sputtered, and Grant was left alone to fight Lee in a series of bloody battles known as the Overland Campaign. Grant crossed the Rapidan River on May 4, 1864, and attacked Lee in the Battle of the Wilderness, a hard-fought three-day battle with many casualties. Rather than retreat as his predecessors had done, Grant flanked Lee's army to the southeast and attempted to wedge his forces between Lee and Richmond at Spotsylvania Court House. Lee's army got to Spotsylvania first, and a costly battle ensued, lasting thirteen days. During the battle, Grant attempted to break through Lee's line of defense, resulting in one of the bloodiest assaults of the Civil War, known as the Battle of the Bloody Angle. Unable to break Lee's defenses, Grant again flanked the Confederate army to the southeast, meeting at North Anna, where a battle lasted three days. The Confederates had the defensive advantage, and Grant maneuvered his army to Cold Harbor, a vital railroad hub that linked to Richmond, but Lee's men were again able to entrench against the Union assault. During the third day of the thirteen-day battle, Grant led a costly assault on Lee's trenches. As casualty reports became known in the North, heavy criticism fell on Grant, who was castigated as "the Butcher" by the Northern press after taking 52,788 casualties in the thirty days since crossing the Rapidan. Lee's army suffered 32,907 casualties, but he was less able to replace them. The costly Union assault at Cold Harbor was the second of two battles in the war that Grant later said he regretted (the other being his initial assault on the fortifications around Vicksburg). Without being detected by Lee, Grant pulled out of Cold Harbor and moved his army south of the James River, freed Butler from the Bermuda Hundred (where the rebels had surrounded his army), and advanced toward Petersburg, Richmond's central railroad hub.
After crossing the James River undetected, Grant and the Army of the Potomac arrived at Petersburg. Confederate General P.G.T. Beauregard defended the city, and Lee's veteran reinforcements soon arrived. The result was a nine-month-long siege of Petersburg, stalling the advance. Northern resentment grew as the war dragged on, but an indirect benefit of the Petersburg siege was that Lee was forced to entrench and defend Richmond, and was unable to reinforce the Army of the Tennessee. Sheridan was assigned command of the Union Army of the Shenandoah and Grant directed him to "follow the enemy to their death". Lee had sent General Jubal Early up the Shenandoah Valley to attack the federal capital and draw troops away from the Army of the Potomac, but Sheridan defeated Early, ensuring that Washington would not be endangered. Grant then ordered Sheridan's cavalry to destroy vital Confederate supplies in the Shenandoah Valley. When Sheridan reported suffering attacks by irregular Confederate cavalry under John S. Mosby, Grant recommended rounding up their families for imprisonment as hostages at Fort McHenry.
Grant approved of a plan to blow up part of the enemy trenches from an underground tunnel. The explosion created a crater, into which poorly-led Union troops poured. Recovering from the surprise, Confederates surrounded the crater and easily picked off Union troops within it. The Union's 3500 casualties outnumbered the Confederates' by three-to-one; although the plan could have been successful if implemented correctly, Grant admitted the tactic had been a "stupendous failure". On August 9, 1864, Grant, who had just arrived at his headquarters in City Point, narrowly escaped death when Confederate spies blew up an ammunition barge in the James River. Rather than fight Lee in a full frontal attack as he had done at Cold Harbor, Grant continued to extend Lee's defenses south and west of Petersburg, to capture vital railroad links. As Grant continued to push the Union advance westward, Lee's lines became overstretched and undermanned. After the Federal army rebuilt the City Point Railroad, Grant was able to use mortars to attack Lee's entrenchments. On September 2, Sherman captured Atlanta while confederate forces retreated, ensuring Lincoln's reelection in November. Sherman convinced Grant and Lincoln to send a Union Army to march to Savannah devastating the Confederate heartland.
Once Sherman reached the East Coast and Thomas dispatched John Bell Hood's forces in Tennessee, Union victory appeared certain, and Lincoln attempted negotiations. He enlisted Francis Preston Blair to carry a message to Confederate President Jefferson Davis. Davis and Lincoln each appointed commissioners, but the conference soon stalled. Grant contacted Lincoln, who agreed to personally meet with the commissioners at Fort Monroe. The peace conference that took place near Union-controlled Fort Monroe was ultimately fruitless, but represented Grant's first foray into diplomacy.
In late March 1865, Grant's forces finally took Petersburg, then captured Richmond that April. Grant, Sherman, Admiral Porter and Lincoln held a conference on the "River Queen" to discuss Reconstruction of the South. Lee's troops began deserting in large numbers; disease and lack of supplies also diminished the remaining Confederates. Lee attempted to link up with the remnants of Joseph E. Johnston's defeated army, but Union cavalry forces led by Sheridan were able to stop the two armies from converging. Lee and his army surrendered to Grant at Appomattox Court House on April 9, 1865. Grant gave generous terms; Confederate troops surrendered their weapons and were allowed to return to their homes with their mounts, on the condition that they would not take up arms against the United States. On April 26, Johnson's Confederate army surrendered to Sherman under the same terms Grant offered to Lee. On May 26, Kirby Smith's western Confederate army surrendered and the Civil War was over ending in Union victory. 
Lincoln's assassination.
On April 14, five days after Grant's victory at Appomattox, he attended a cabinet meeting in Washington. Lincoln invited him and his wife to Ford's Theater, but they declined as they had plans to travel to Philadelphia. In a conspiracy that targeted several government leaders, Lincoln was fatally shot by John Wilkes Booth at the theater, and died the next morning. Many, including Grant himself, thought that he had been a target in the plot. Secretary of War Stanton notified him of the President's death and summoned him back to Washington. Attending Lincoln's funeral on April 19, Grant stood alone and wept openly; he later said Lincoln was "the greatest man I have ever known." Regarding the new President, Andrew Johnson, Grant told Julia that he dreaded the change in administrations; he judged Johnson's attitude toward white southerners as one that would "make them unwilling citizens", and initially thought that with President Johnson, "Reconstruction has been set back no telling how far."
Commanding General.
Transition to peacetime.
At the war's end, Grant remained commander of the army, with duties that included enforcement of Reconstruction in the former Confederate states and supervision of Indian wars on the western Plains. Grant secured a house for his family in Georgetown Heights in 1865, but instructed Elihu Washburne that for political purposes his legal residence remained in Galena, Illinois. That same year, Grant spoke at Cooper Union in New York, where the "New York Times" reported that "... the entranced and bewildered multitude trembled with extraordinary delight." Further travels that summer took the Grants to Albany, New York, back to Galena, and throughout Illinois and Ohio, with enthusiastic receptions.
In November 1865, Johnson sent Grant on a fact-finding mission to the South. Afterwards, Grant recommended continuation of a reformed Freedmen's Bureau, which Johnson opposed, but advised against the use of black troops in garrisons, which he believed encouraged an alternative to farm labor. Grant did not believe the people of the devastated South were ready for civilian self-rule, and that both whites and blacks in the South required protection by the federal government. He also warned of threats by disaffected poor people, black and white, and recommended that local decision-making be entrusted only to "thinking men of the South" (i.e., white men of property). In this respect, Grant's opinion on Reconstruction aligned with Johnson's policy of pardoning established southern leaders and restoring them to their positions of power. He joined Johnson in arguing that Congress should allow representatives from the South to take their seats. On July 25, 1866, Congress promoted Grant to the newly created rank of General of the Army of the United States.
Breach with Johnson.
Johnson favored a lenient approach to Reconstruction, calling for an immediate return of the former Confederate states into the Union without any guarantee of African American civil rights. The Radical Republican-controlled Congress opposed the idea and refused to admit Congressmen from the former Confederate states. Over Johnson's vetoes, Congress renewed the Freedmen's Bureau and passed the Civil Rights Act of 1866. During the congressional election campaign later that year, Johnson took his case to the people in his "Swing Around the Circle" speaking tour. Johnson pressured Grant, by then the most popular man in the country, to go on the tour; Grant, wishing to appear loyal, agreed. Grant believed that Johnson was purposefully agitating conservative opinion to defy Congressional Reconstruction. Finding himself increasingly at odds with Johnson, Grant confided to his wife that he thought the President's speeches were a "national disgrace". Publicly, Grant attempted to appear loyal to Johnson while not alienating Republican legislators essential to his future political career. Concerned that Johnson's differences with Congress would cause renewed insurrection in the South, he ordered Southern arsenals to ship arms north to prevent their capture by Southern state governments.
Conflict between radicals and conservatives continued after the 1866 congressional elections. Rejecting Johnson's vision for quick reconciliation with former Confederates, Congress passed the Reconstruction Acts, which divided the southern states into five military districts to protect the freedman's constitutional and congressional rights. Military district governors were to lead transitional state governments in each district. Grant, who was to select the general to govern each district from a group designated by Johnson, preferred Congress's plan for enforcement of Reconstruction. Grant was optimistic that Reconstruction Acts would help pacify the South. By complying with the Acts and instructing his subordinates to do likewise, Grant further alienated Johnson. When Sheridan removed public officials in Louisiana who impeded Reconstruction, Johnson was displeased and sought Sheridan's removal. Grant recommended a rebuke, but not a dismissal. Throughout the Reconstruction period, Grant and the military protected the rights of more than 1,500 African Americans elected to political office. In 1866, Congress renewed the Freedmen's Bureau over Johnson's vetoes and with Grant's support, and passed the first Civil Rights Act protecting African American civil rights by nullifying black codes. On July 19, 1867, Congress, again over Johnson's veto, passed a measure that authorized Grant to have oversight in enforcing congressional Reconstruction, making Southern state governments subordinate to military control.
Johnson's impeachment.
Johnson wished to replace Stanton, a Lincoln appointee who sympathized with Congressional Reconstruction. To keep Grant under control as a potential political rival, Johnson asked him to take the post. Grant recommended against the move, in light of the Tenure of Office Act, which required Senate approval for cabinet removals. Johnson believed the Act did not apply to officers appointed by the previous president, and forced the issue by making Grant an interim appointee during a Senate recess. Grant agreed to accept the post temporarily, and Stanton vacated the office until the Senate reconvened.
When the Senate reinstated Stanton, Johnson told Grant to refuse to surrender the office and let the courts resolve the matter. Grant told Johnson in private that violating the Tenure of Office Act was a federal offense, which could result in a fine or imprisonment. Believing he had no other legal alternatives, Grant returned the office to Stanton. This incurred Johnson's wrath; at a cabinet meeting immediately afterwards, Johnson accused Grant of breaking his promise to remain Secretary of War. Grant disputed that he had ever made such a promise although cabinet members later testified he had done so. Newspapers friendly to Johnson published a series of articles to discredit Grant over returning the War Department to Stanton, stating that Grant had been deceptive in the matter. This public insult infuriated Grant, and he defended himself in an angry letter to Johnson, after which the two men were confirmed foes. When Grant's statement became public, it increased his popularity among Radical Republicans and he emerged from the controversy unscathed. Although Grant favored Johnson's impeachment, he took no active role in the impeachment proceedings against Johnson, which were fueled in part by Johnson's removal of Stanton. Johnson barely survived, and none of the other Republican leaders directly involved benefited politically in their unsuccessful attempt to remove the president.
Election of 1868.
While remaining Commanding General, Grant entered the 1868 campaign season with increased popularity among the Radical Republicans following his abandonment of Johnson over the Secretary of War dispute. The Republicans chose Grant as their presidential candidate on the first ballot at the 1868 Republican National Convention in Chicago. In his letter of acceptance to the party, Grant concluded with "Let us have peace", which became his campaign slogan. For vice president, the delegates nominated House Speaker Schuyler Colfax. Grant's 1862 General Order No. 11 became an issue during the presidential campaign; he sought to distance himself from the order, saying "I have no prejudice against sect or race, but want each individual to be judged by his own merit." As President, Grant would atone for 1862's expulsion of the Jews. Historian Jonathan Sarna argues that Grant became one of the greatest friends of Jews in American history, meeting with them often and appointing them to high office. He was the first president to condemn atrocities against Jews in Europe, thus putting human rights on the American diplomatic agenda. As was expected at the time, Grant returned to his home state and left the active campaigning and speaking on his behalf to his campaign manager William E. Chandler and others. The Republican campaign focused on continuing Reconstruction and restoring the public credit. 
The Democrats nominated former New York Governor Horatio Seymour. Their campaign focused mainly on ending Reconstruction and returning control of the South to the white planter class, which alienated many War Democrats in the North. The Democrats attacked Reconstruction and the Republican Party's support of African American rights, while deriding Grant, calling him captain of the "Black Marines". Grant won the election by 300,000 votes out of 5,716,082 votes cast, receiving an electoral college landslide, of 214 votes to Seymour's 80. Grant, at the age of 46 was (at the time) the youngest president ever elected. His election was a triumph of conservative principles that included sound money, efficient government, and the restoration of Southern reconstructed states. Grant was the first president elected after the nation had outlawed slavery and granted citizenship to former slaves. Implementation of these new rights was slow to come; in the 1868 election, the black vote counted in only 16 of the 37 states, nearly all in the South. Grant lost Louisiana and Georgia primarily due to Ku Klux Klan violence against African American voters. During the election there was a noticeably large number of black citizens in Washington.
Presidency (1869–77).
On March 4, 1869, Grant was sworn in as the eighteenth President of the United States by Chief Justice Salmon P. Chase. His presidency began with a break from tradition, as Johnson did not attend Grant's inauguration at the Capitol or ride with him as he departed the White House for the last time. In his inaugural address, Grant urged the ratification of the Fifteenth Amendment and said he would approach Reconstruction "calmly, without prejudice, hate or sectional pride." Grant recommended the "proper treatment" of Native Americans be studied; advocating their civilization and eventual citizenship. Grant took an unconventional approach to choosing his cabinet, declining to consult with the Senate and keeping his choices secret until he submitted them for confirmation. In his effort to create national harmony, Grant purposely avoided choosing Republican Party leaders. Grant appointed his wartime comrade John A. Rawlins as Secretary of War and Hamilton Fish, a conservative New York statesman, as Secretary of State. Sherman earned promotion to Commanding General, but his relationship with Grant became strained when the President took Rawlins's side when the Secretary of War sought to limit Sherman's authority. Rawlins died in office a few months later, and Grant appointed William W. Belknap as his replacement.
Grant selected several non-politicians to his cabinet, including Adolph E. Borie and Alexander Turney Stewart, with limited success. Borie served briefly as Secretary of the Navy, later replaced by George M. Robeson, while Stewart was prevented from becoming Secretary of the Treasury by a 1789 statute that barred businessmen from the position (Senators Charles Sumner and Roscoe Conkling opposed amending the law.) In place of Stewart, Grant appointed Massachusetts Representative George S. Boutwell, a radical, as Treasury Secretary. Grant's other cabinet appointments—Jacob D. Cox (Interior), John Creswell (Postmaster General), and Ebenezer Rockwood Hoar (Attorney General)—were well-received and uncontroversial. Grant also appointed four Justices to the Supreme Court: William Strong, Joseph P. Bradley, Ward Hunt and Chief Justice Morrison Waite. Hunt voted to uphold Reconstruction laws while Waite did much to undermine them.
Later Reconstruction and civil rights.
Grant took office in 1869 during the middle of the Reconstruction of the South or former Confederate states. Unlike his predecessor, Grant advocated systematic federal enforcement of fundamental civil rights regardless of race. He lobbied Congress to pass the Fifteenth Amendment, guaranteeing that no state could prevent someone from voting based on race, and believed that its passage would secure freedmen's rights. Grant asked Congress to admit representatives from the remaining unrepresented Southern states in conformity with Congressional Reconstruction; they did so, passing legislation providing that Mississippi, Virginia, and Texas would be represented in Congress after they ratified the Fifteenth Amendment. Grant pressured Congress to draw up legislation that would seat African American state legislators in Georgia, who had been ousted by white conservatives. Congress responded through special legislation; the members were re-seated in the Georgia legislature, and Georgia was required to adopt the Fifteenth Amendment to gain representation in Congress. By July 1870, the four remaining states were readmitted.
To bolster the new amendment, Grant relied on the army and in 1870 he signed legistlation creating the Justice Department, primarily to enforce federal laws in the South. Where the attorney general had once been only a legal adviser to the president, he now led a cabinet department dedicated to enforcing federal law, including a solicitor general to argue on the government's behalf in court. Under Grant's first attorney general, Ebenezer R. Hoar, the administration was not especially aggressive in prosecuting white Southerners who terrorized their black neighbors, but Hoar's successor, Amos T. Akerman, was more zealous. Alarmed by a rise in terror by the Ku Klux Klan and other groups against African Americans, Congress (with Grant's encouragement) passed a series of three laws, the Enforcement Acts from 1870 to 1871, which made depriving African Americans their civil rights a federal offense and authorized the president to use the military to enforce the laws. In May 1871, Grant ordered federal troops to help marshals in arresting Klansmen. That October, on Akerman's recommendation, Grant suspended "habeas corpus" in part of South Carolina and sent federal troops to enforce the law there. After prosecutions by Akerman and his replacement, George Henry Williams, the Klan's power collapsed; by 1872, elections in the South saw African Americans voting in record numbers. That same year, Grant signed the Amnesty Act, which restored political rights to former Confederates. Lacking sufficient funding, the Justice Department stopped prosecutions of the Klan in June 1873, and Grant offered the Klan clemency in exchange for peace.
After the Klan's decline, other conservative whites formed armed groups, such as the Red Shirts and the White League who openly used violence and intimidation to take control of state governments. The Panic of 1873 and the ensuing depression contributed to public fatigue, and the North grew less concerned with Reconstruction. Supreme Court rulings in the "Slaughter-House Cases" (1873) and "United States v. Cruikshank" (1875) restricted federal enforcement of civil rights. Grant began to favor limiting the use of troops, to avoid the impression that he was acting as a military dictator; he was also concerned that increased military pressure in the South might cause conservative whites in the North to bolt the Republican Party. In 1874, Grant by proclamation ended the Brooks–Baxter War bringing Reconstruction in Arkansas to a peaceful conclusion, but that same year, he sent troops and warships under Major General William H. Emory to New Orleans in the wake of the Colfax Massacre and disputes over the election of Governor William Pitt Kellogg. Emory peacefully restored Kellogg to office and the following year the parties reached a compromise allowing Democrats to retain control of the Louisiana House. Under public pressure Grant recalled General Sheridan and most of the federal troops from Louisiana.
By 1875, Democratic "Redeemer" politicians took control of all but three Southern states. As violence against black Southerners escalated once more, Edwards Pierrepont (Grant's fourth attorney general) told Governor Adelbert Ames of Mississippi that the people were "tired of the autumnal outbreaks in the South", and declined to intervene directly, instead, sending an emissary to negotiate a peaceful election. Grant signed an ambitious Civil Rights Act of 1875, which expanded federal law enforcement by prohibiting discrimination on account of race in public lodging, public transportation, and jury service. However, it did not stop the rise of white supremacist forces in the South. In October 1876, Grant sent troops to South Carolina to aid Republican Governor Daniel Chamberlain. Even so, the remaining three Republican governments in the South fell to Redeemers after the 1876 presidential election, and the ensuing Compromise of 1877 marked the end of Reconstruction.
Indian peace policy.
Grant's attempts to live peacefully with Native Americans marked a radical reversal of what had since the 1830s been the government's policy of Indian removal. He appointed Ely S. Parker, a Seneca Indian and member of his wartime staff, as Commissioner of Indian Affairs. "My efforts in the future will be directed," Grant said in his second inaugural address, "by a humane course, to bring the aborigines of the country under the benign influences of education and civilization ... Wars of extermination ... are demoralizing and wicked." Grant's "Peace Policy" aimed to replace entrepreneurs serving as Indian agents with missionaries. In 1869, Grant signed a law establishing a Board of Indian Commissioners to oversee spending and reduce corruption in the Bureau of Indian Affairs. Two years later, he signed a bill ending the Indian treaty system; the law now treated individual Native Americans as wards of the federal government, and no longer dealt with the tribes as sovereign entities. Grant wished for Indian tribes to be protected on reservations and educated in European-style farming and culture, abandoning their hunter-gatherer way of life. Although, as biographer Jean Edward Smith wrote, Grant's peace policy was "remarkably progressive and humanitarian" for its time, it ultimately disregarded native cultures, something modern Americans see "as a grave error."
The peace policy showed some success in reducing battles between Indians and whites on the western frontier, but the increased slaughter of the buffalo, encouraged by Grant's subordinates, led to conflict with the Plains Indians. The Sioux and other Plains tribes accepted the reservation system, but encroachments by whites in search of gold in the Black Hills led to renewed war by the end of Grant's second term, ending the understanding that had developed between Grant and Sioux Chief Red Cloud. Under Major Generals Oliver Otis Howard and George Crook, Grant's policy had greater success in the Southwest. Howard, the former head of the Freedmen's Bureau, negotiated peace with the Apache in 1872, convincing their leader, Cochise, to move the tribe to a new reservation, and ending a war started the year before. In Oregon, relations were less peaceful, as war with the Modocs erupted in April 1873. The Modocs refused to move to a reservation and killed the local army commander, Major General Edward Canby. Although Grant was upset over Canby's death, he ordered restraint, disregarding Sherman's advice to seek revenge or exterminate the tribe. The army captured, tried, and executed the four Modoc warriors responsible for Canby's murder in October 1873. Grant ordered the rest of the Modoc tribe relocated to the Indian Territory.
During the Great Sioux War, fueled by the discovery of gold in the Black Hills, Grant came into conflict with Colonel George Armstrong Custer after Custer testified in 1876 about corruption in the War Department. Grant ordered Custer arrested for breach of military protocol and barred him from leading an upcoming campaign against the Sioux. Grant later relented and let Custer fight under Brigadier General Alfred Terry. Sioux warriors led by Crazy Horse killed Custer at the Battle of the Little Big Horn, the army's most famous defeat in the Indian wars. Two months later, Grant castigated Custer in the press, saying "I regard Custer's massacre as a sacrifice of troops, brought on by Custer himself, that was wholly unnecessary – wholly unnecessary." Custer's death shocked the nation, leading Congress to appropriate funds for more troops, two more Western forts and barred Indians from purchasing weapons.
Foreign affairs.
Even before Grant became president, expansionists in American politics desired control over the Caribbean islands. Andrew Johnson had recommended annexation but the early anti-imperialist Republicans in Congress rejected the plan. Grant renewed negotiations to annex the Dominican Republic, led by Orville E. Babcock, a wartime confidant. Grant was initially skeptical, but at the urging of Admiral Porter, who wanted a naval base at Samaná Bay, and Joseph W. Fabens, a New England businessman employed by the Dominican government, Grant became convinced of the plan's merit. Grant sent Babcock to the Dominican Republic and consult with Buenaventura Báez, the Dominican president who supported annexation. Although he had no official government standing, Babcock secretly negotiated an annexation treaty with Baez and returned to Washington in December 1869.
Grant believed in peaceful expansion of the nation's borders, and thought acquisition of the majority-black nation would allow new economic opportunities for African Americans in the United States while increasing American naval power in the Caribbean. Grant believed the island would offer a refuge for black Americans suffering from violent attacks in the South by white Americans during Reconstruction. Secretary of State Hamilton Fish dismissed annexation, seeing the island as politically unstable and troublesome. Grant personally lobbied Senators to pass the treaty, going so far as to visit Sumner at his home. Fish added to the effort out of loyalty to the administration, but to no avail; the Senate refused to pass the treaty. Sumner's role in leading opposition toward annexation led to political enmity between him and Grant. A congressional investigation in 1870 by Senator Carl Schurz revealed land speculators financially motivated passage of the treaty. After the Dominican initiative failed, Grant convinced Fish to stay in the cabinet and gave him greater authority to run the State Department. Unwilling to admit defeat, Grant successfully lobbied Congress to send a commission to the West Indies to investigate, including Frederick Douglass. Although Douglass and the commission approved of Grant's claims for annexation in its findings released on April 5, 1871, the Senate remained opposed while Grant was forced to abandon further annexation attempts.
Grant and Fish were more successful in their resolution of the "Alabama" claims. This dispute with the United Kingdom stemmed from the damage done to American shipping during the Civil War by the five ships built for the Confederacy in British shipyards including, most famously, the . The Americans claimed that Britain had violated neutrality by building ships for the Confederate Navy. When the war ended, the United States demanded restitution, which the British refused to pay. Negotiations continued fitfully, a sticking point being the claims of "indirect damages" on top of the harm directly caused by the five ships. Sumner opposed the Johnson administration's proposed settlement, which had been rejected by the Senate, believing that Britain should directly pay $2 billion in gold or, alternatively, cede Canada to the United States. Fish convinced Grant that peaceful relations with Britain were more important than acquisition of more territory, and the two nations agreed to negotiate along those lines. A commission in Washington produced a treaty whereby an international tribunal would settle the damage amounts; the British admitted regret, but not fault. The Senate approved the Treaty of Washington, which also settled disputes over fishing rights and maritime boundaries, by a 50–12 vote in 1871.
In 1873, a Spanish cruiser took captive a merchant ship, "Virginius", flying the U.S. flag, carrying war materials and men to aid the Cuban insurrection. The passengers and crew, including eight American citizens, were illegally traveling to Cuba to help overthrow the government. Spanish authorities executed the prisoners, and many Americans called for war with Spain. Fish, with Grant's support, worked to reach a peaceful resolution. Spain's President, Emilio Castelar y Ripoll, expressed regret for the tragedy and agreed to decide reparations through arbitration; Spain surrendered the "Virginius" and paid a cash indemnity of $80,000 to the families of the executed Americans. In June 1874, Grant's Secretary of the Navy, George M. Robeson, commissioned the reconstruction of five redesigned double-turreted monitor warships to compete with the superior Spanish Navy. The administration's diplomacy was also at work in the Pacific as, in December 1874, Grant held a state dinner at the White House for the King of Hawaii, David Kalakaua, who was seeking duty-free sugar importation to the United States. Grant and Fish were able to produce a successful free trade treaty in 1875 with the Kingdom of Hawaii, incorporating the Pacific islands' sugar industry into the United States' economic sphere.
Gold standard and the Gold Ring.
Soon after taking office, Grant took steps to return the nation's currency to a more secure footing. During the Civil War, Congress had authorized the Treasury to issue banknotes that, unlike the rest of the currency, were not backed by gold or silver. The "greenback" notes, as they were known, were necessary to pay the unprecedented war debts, but they also caused inflation and forced gold-backed money out of circulation; Grant determined to return the national economy to pre-war monetary standards. Many in Congress agreed, and they passed the Public Credit Act of 1869, which guaranteed that bondholders would be repaid in gold, not greenbacks. To strengthen the dollar, Treasury Secretary George S. Boutwell, backed by Grant, sold gold from the Treasury each month and bought back high-interest Treasury bonds issued during the war; this had the effect of reducing the deficit, but deflating the currency.
These actions had a large impact on the gold market and the national economy. Jay Gould, a Wall Street trader and railroad magnate, and financier Jim Fisk, seeking to drive up the gold price, enlisted the help of another speculator Abel Corbin, Grant's brother-in-law, who used his connection with the president to get inside information (the collaborators were later known as the ""Gold Ring".") Corbin convinced Grant to appoint a Gould associate, Daniel Butterfield, as assistant Treasurer, where he could gather information for the Ring. Meanwhile, Gould and Fisk quietly stockpiled gold. Gould convinced Corbin that a high gold price would be good for the nation's prosperity, and Corbin passed this theory on to Grant, who allowed the Treasury to act accordingly. After consulting in early September with Alexander Stewart (his erstwhile nominee for Treasury Secretary), Grant stopped the sale of gold, believing a higher gold price would help Western farmers. By mid-September, Grant warned Boutwell to be on his guard as the gold price continued to rise, while the conspirators bought ever more and the rising price affected the wider economy. Grant, seeing that the increase was unnatural, told Boutwell to sell gold, which reduced its price. Boutwell did so the next day, on September 24, 1869, later known as Black Friday. The sale of gold from the Treasury defeated Gould's scheme as the gold price plummeted, relieving the growing economic tension. Gould and Fisk managed to escape without much harm to themselves. Many brokerage firms collapsed while trade volume and agriculture prices plummeted, causing a mild recession, but by January 1870, the economy resumed its post-war recovery.
Election of 1872.
Despite his administration's many scandals, Grant continued to be personally popular. A growing number of reformers, however, were disappointed by Grant's support of Reconstruction, the Gold Ring, and corruption in the New York Customs House. To placate reformers, Grant created a Civil Service Commission authorized by Congress in 1871. The Commission, chaired by reformer George William Curtis, proposed certain reform rules and regulations, which Grant implemented by executive order in April 1872. Congress refused to fund the Commission beyond 1875 or to pass legislation to implement its recommendations. There was further division within the party between the faction most concerned with the plight of the freedmen and that concerned with the growth of industry. During the war, both factions' interests had aligned, and in 1868 both had supported Grant. As the wartime coalition began to fray, Grant's alignment with the party's pro-Reconstruction elements alienated party leaders who favored an end to federal intervention in Southern racial issues.
Many of that faction split from the party in 1872, calling themselves the Liberal Republican Party. Led by Charles Francis Adams of Massachusetts and Senator Carl Schurz of Missouri, they publicly denounced the political patronage system that Sumner called "Grantism" and demanded amnesty for Confederate soldiers. The Liberal Republicans nominated Horace Greeley, another Republican who had come to dislike Grant and his policies, and Governor Benjamin Gratz Brown of Missouri for Vice President. The Democrats, seeking to benefit from anti-Grant sentiment, nominated Greeley as well. The rest of the Republican Party nominated Grant for reelection, with Senator Henry Wilson of Massachusetts replacing Colfax as vice-presidential nominee. Wilson, viewed as a practical reformer and civil rights advocate, was meant to strengthen the Republican ticket. The Crédit Mobilier scandal revealed in September 1872, in which a railroad company bribed many members of Congress in 1868, did not involve Grant, but did ensnare Vice President Colfax and Senator Wilson, adding to the general sense of dishonesty in Washington. To the Liberals' chagrin, Greeley made Grant's Southern policy, rather than reform, the main campaign issue. The fusion effort failed and Grant was easily reelected. The Liberal Republicans were unable to deliver many votes, and Greeley was only successful in areas the Democrats would have carried without him. A strong economy, debt reduction, lowered tariffs, repeal of the income tax, and civil service reforms helped Grant defeat the Liberal Republicans. Grant won 56 percent of the popular vote and an Electoral College landslide of 286 to 66. A majority of African Americans in the South voted for Grant, while Democratic opposition remained mostly peaceful.
Panic of 1873, the Long Depression, and currency debates.
As his first term was ending, Grant continued to work for a strong dollar, signing into law the Coinage Act of 1873, which effectively ended the legal basis for bimetallism (the use of both silver and gold as money), and established the gold standard in practice. The Coinage Act discontinued the standard silver dollar and established the gold dollar as the sole monetary standard. Critics who wanted more money in circulation to facilitate easier credit later denounced the move as the "Crime of 1873", claiming the law caused deflation and helped bankers while hurting farmers.
Grant's second term saw renewed economic turmoil. In September 1873, Jay Cooke & Company, a New York brokerage house, collapsed after it failed to sell all of the bonds issued by Cooke's Northern Pacific Railway. The collapse sent ripples through Wall Street, and other banks and brokerages that owned railroad stocks and bonds were also ruined. On September 20, the New York Stock Exchange suspended trading for ten days. Grant, who knew little about finance, traveled to New York to consult leading businessmen and bankers for advice on how to resolve the crisis, which became known as the Panic of 1873. Grant believed that, as with the collapse of the Gold Ring in 1869, the panic was merely an economic fluctuation that affected bankers and brokers. He responded cautiously, instructing the treasury to buy $10 million in government bonds, thus injecting cash into the system. These purchases curbed the panic on Wall Street, but a five-year industrial depression, later called the Long Depression, nonetheless swept the nation. Many of the nation's railroads—89 out of 364—went bankrupt.
Congress hoped inflation would stimulate the economy and passed what became known as the Inflation Bill in 1874. Many farmers and workingmen favored the bill, which would have added $64 million in greenbacks to circulation, but some Eastern bankers opposed it because it would have weakened the dollar. Grant unexpectedly vetoed the bill, saying that it would destroy the credit of the nation. Grant's veto, supported by Fish, placed him securely in the conservative faction of the Republican party, and was the beginning of the party's commitment to a strong gold-backed dollar. Grant later pressured Congress for a bill to further strengthen the dollar by gradually reducing the number of greenbacks in circulation. After losing the House to the Democrats in the 1874 elections, the lame-duck Republican Congress did so. On January 14, 1875, Grant signed the Specie Payment Resumption Act into law. The Resumption Act required gradual reduction of the amount of greenbacks allowed to circulate and declared that specie payment (i.e., in gold or silver) would resume in 1879.
Gilded Age corruption and reform.
Grant served as president during the Gilded Age, a time when the economy was open to speculation and western expansion that fueled corruption in government offices. Against the harsh public revelation of the Credit Mobilier of America scandal, Grant faced charges of misconduct in nearly all federal departments, engaging his administration in constant conflict between corrupt associates and reformers. Although personally honest with his own money matters, Grant was trusting and had difficulty in spotting corruption in others. Stubbornly protective of corrupt associates, Grant often saw their prosecutions as unjust and shielded them from attack even at the cost of his own reputation, unless evidence of personal misconduct was overwhelming. No person linked any of the scandals together, except possibly Grant's personal secretary, Orville E. Babcock, who indirectly controlled many cabinet departments and delayed federal investigations.
There was additional scandal in New York. In 1871, Thomas Murphy, the Collector of the Port of New York and a member of New York Senator Roscoe Conkling's political machine, was forced to resign. Murphy, a Grant appointee to the patronage-rich position, had become embroiled in a dispute with another faction of the Republican party over the jobs at his disposal and was accused of corruption in office (a charge confirmed in an 1872 congressional investigation). In December, Grant appointed Chester A. Arthur, another Conkling man, to replace Murphy, and administration of the Customs House steadily improved.
In 1874, Grant replaced Richardson as Treasury Secretary with Benjamin H. Bristow, a man known for his honesty, who began a series of reforms in the department, including tightening up the detective force and firing the second-comptroller for inefficiency. Bristow dismissed over 700 people and implemented civil service rules. Discovering 12 to 15 million gallons of whiskey escaped taxes, and having Grant's endorsement to act ("Let no guilty man escape"), Bristow investigated and struck down the Whiskey Ring, seizing 32 installations, impounding documents, and arresting some 350 men while obtaining 176 indictments. These led to 110 convictions and $3,150,000 in tax dollars was restored to the Treasury. When Bristow's investigation implicated Babcock as part of the Whiskey Ring, Grant became defensive, believing Babcock was the innocent victim of a witch hunt. While denying immunity to minor Whiskey Ring conspirators, Grant worked to protect Babcock. In 1876, a jury acquitted Babcock at his St. Louis trial influenced by Grant's deposition in Babcock's favor. After the trial, under public pressure, Grant dismissed Babcock from the White House. Grant later pardoned several Ring members.
As the scandals increased, Congress, with the House now under Democratic control, began several investigations into corruption in the administration, the most notable of which dealt with profiteering at western trading posts. involving Secretary of War William W. Belknap which led to his resignation. Congress also investigated and reprimanded Navy Secretary George M. Robeson in 1876 for receiving bribes.
Grant's Civil Service Commission reforms had limited success, as his cabinet implemented a merit system that increased the number of qualified candidates and relied less on Congressional patronage. Interior Secretary Columbus Delano, however, exempted his department from competitive examinations, and Congress refused to enact permanent Civil Service reform. Zachariah Chandler, who succeeded Delano, made sweeping reforms in the entire Interior Department; Grant ordered Chandler to fire all corrupt clerks in the Bureau of Indian Affairs. Grant appointed reformers Edwards Pierrepont and Marshall Jewell as Attorney General and Postmaster General, respectively, who supported Bristow's investigations. In 1875, Pierrepont cleaned up corruption among the United States Attorneys and Marshals in the South. Grant suggested other reforms as well, including a proposal that states should offer free public schooling to all children; he also endorsed the Blaine Amendment, which would have forbidden government aid to schools with religious affiliations.
Election of 1876.
Even as Grant drew cheers at the opening of the Centennial Exposition in May 1876, the collected scandals of his presidency, the country's weak economy, and the Democratic gains in the House led many in the Republican party to repudiate him in June. Bristow was among the leading candidates to replace him, suggesting that a large faction desired an end to "Grantism" and feared that Grant would run for a third term. Ultimately, Grant declined to run, but Bristow also failed to capture the nomination, as the convention settled on Governor Rutherford B. Hayes of Ohio, a reformer. The Democrats nominated Governor Samuel J. Tilden of New York. Voting irregularities in three Southern states caused the election that year to remain undecided for several months. Grant told Congress to settle the matter through legislation and assured both sides that he would not use the army to force a result, except to curb violence. On January 29, 1877, Grant signed legislation passed by Congress to form an Electoral Commission to decide the matter. The result was the Compromise of 1877: the Electoral Commission ruled that the disputed votes belonged to Hayes, but the last troops were withdrawn from Southern capitals. The Republicans had won, but Reconstruction was over. According to biographer Jean Edward Smith, "Grant's calm visage in the White House reassured the nation."
Post-presidency.
World tour.
After leaving the White House, Grant and his family stayed with friends for two months, before setting out on a world tour. The trip, which would last two years, began in Liverpool in May 1877, where enormous crowds greeted the ex-president and his entourage. The Grants dined with Queen Victoria at Windsor Castle, and Grant gave several speeches in London. After a tour on the continent, the Grants spent a few months with their daughter Nellie, who had married an Englishman and moved to that country several years before. Grant and his wife journeyed to France and Italy, spending Christmas 1877 aboard USS "Vandalia", a warship docked in Palermo. A winter sojourn in the Holy Land followed, and they visited Greece before returning to Italy and a meeting with Pope Leo XIII. They toured Spain before moving on to Germany, where Grant discussed military matters with Chancellor Otto von Bismarck, telling him that in the final stages of the Civil War, the Union Army fought to preserve the nation and to "destroy slavery".
The Grants left from England by ship, sailing through the Suez Canal to India. They visited cities throughout the Raj, welcomed by colonial officials. After India, they toured Burma, Siam (where Grant met with King Chulalongkorn), Singapore, and Cochinchina (Vietnam). Traveling on to Hong Kong, Grant began to change his mind on the nature of colonization, believing that British rule was not "purely selfish" but also good for the colonial subjects. Leaving Hong Kong, the Grants visited the cities of Canton, Shanghai, and Peking, China. He declined to ask for an interview with the Guangxu Emperor, a child of seven, but did speak with the head of government, Prince Gong, and Li Hongzhang, a leading general. They discussed China's dispute with Japan over the Ryukyu Islands, and Grant agreed to help bring the two sides to agreement. After crossing over to Japan and meeting the Emperor Meiji, Grant convinced China to accept the Japanese annexation of the islands, and the two nations avoided war.
By then, the Grants had been gone two years, and were homesick. They crossed the Pacific and landed in San Francisco in September 1879, greeted by cheering crowds. After a visit to Yosemite Valley, they returned at last to Philadelphia on December 16, 1879. The voyage around the world had captured popular imagination, and Republicans—especially those of the Stalwart faction excluded from the Hayes administration—saw Grant in a new light. The Republican nomination for 1880 was wide open after Hayes forswore a second term and many Republicans thought that Grant was the man for the job.
Third term attempt.
Stalwarts, led by Grant's old political ally, Roscoe Conkling, saw the ex-president's renewed popularity as a way for their faction to regain power. Opponents denounced the idea as a violation of the two-term rule that had been the norm since George Washington. Grant said nothing publicly, but he wanted the job and encouraged his men. Elihu B. Washburne urged him to run; Grant demurred, saying he would be happy for the Republicans to win with another candidate, though he preferred James G. Blaine to John Sherman. Even so, Conkling and John A. Logan began to organize delegates in Grant's favor. When the convention convened in Chicago in June, there were more delegates pledged to Grant than to any other candidate, but he was still short of a majority vote to capture the nomination.
At the convention, Conkling nominated Grant with an elegant speech, the most famous line being: "When asked which state he hails from, our sole reply shall be, he hails from Appomattox and its famous apple tree." With 370 votes needed for nomination, the first ballot had Grant at 304, Blaine at 284, Sherman at 93, and the rest scattered to minor candidates. Subsequent ballots followed, with roughly the same result; neither Grant nor Blaine could win. After thirty-six ballots, Blaine's delegates deserted him and combined with those of other candidates to nominate a compromise candidate: Representative James A. Garfield of Ohio. The 306 votes Grant received on the last ballot was not enough to secure the nomination. A procedural motion made the vote unanimous for Garfield, who accepted the nomination.
Grant gave speeches for Garfield, but declined to criticize the Democratic nominee, Winfield Scott Hancock, a general who had served under him in the Army of the Potomac. Garfield won the popular vote by a narrow margin, but solidly won the Electoral College—214 to 155. After the election, Grant gave Garfield his public support, and pushed him to include Stalwarts in his administration.
Business ventures.
Grant's world tour had been costly. When he returned to America, Grant had depleted most of his savings and needed to earn money and find a new home. Wealthy friends bought him a home on Manhattan's Upper East Side, and to make an income, Grant, Jay Gould, and former Mexican Finance Secretary Matías Romero chartered the Mexican Southern Railroad, with plans to build a railroad from Oaxaca to Mexico City. Grant urged Chester A. Arthur, who had succeeded Garfield as president in 1881, to negotiate a free trade treaty with Mexico. Arthur and the Mexican government agreed, but the United States Senate rejected the treaty in 1883. The railroad was similarly unsuccessful, falling into bankruptcy the following year.
At the same time, Grant's son Ulysses Jr. had opened a Wall Street brokerage house with Ferdinand Ward. Regarded as a rising star, Ward, and the firm, Grant & Ward, were initially successful. In 1883, Grant joined the firm and invested $100,000 of his own money. Investors bought securities through the firm, and Ward used the securities as collateral to borrow money to buy more securities. Grant & Ward pledged that collateral to borrow more money to trade in securities on the firm's own account. The practice—called hypothecation—was legal and accepted; what was illegal was "rehypothecation", the practice of pledging the same securities as collateral for multiple loans. Ward, having colluded with the bank involved, did this for many of the firm's assets. When the trades went bad, multiple loans came due, all backed up by the same collateral. Historians agree that Grant was likely unaware of Ward's tactics, but it is unclear how much Buck Grant knew. In May 1884, enough investments went bad to convince Ward that the firm would soon be bankrupt. Ward told Grant of the impending failure, but suggested that this was a temporary shortfall. Grant approached businessman William Henry Vanderbilt, who gave him a personal loan of $150,000. Grant invested the money in the firm, but it was not enough to save the firm from failure. Essentially penniless, but compelled by a sense of personal honor, he repaid what he could with his Civil War mementos and the sale or transfer of all other assets. Although the proceeds did not cover the loan, Vanderbilt insisted the debt had been paid in full. Grant was left destitute.
Memoirs and death.
To restore his family's income, Grant wrote several articles on his Civil War campaigns for "The Century Magazine" at $500 each. The articles were well received by critics, and the editor, Robert Underwood Johnson, suggested that Grant write a book of memoirs, as Sherman and others had done. Grant's articles would serve as the basis for several chapters.
In the summer of 1884, Grant complained of a soreness in his throat, but put off seeing a doctor until late October where he finally learned it was throat cancer. Before being diagnosed, Grant was invited to a Methodist service for Civil War veterans in Ocean Grove, New Jersey, on August 4, 1884, receiving a standing ovation from more than ten thousand veterans and others; it would be his last public appearance. In March of the following year, the New York Times finally announced that Grant was dying of cancer and a nationwide public concern for the former president began. Later, Grant, who had forfeited his military pension when he assumed the presidency, was honored by his friends and the Congress when he was restored to the rank of General of the Army with full retirement pay.
Despite his debilitating illness, Grant worked diligently on his memoirs at his home in New York City, and then from a cottage on the slopes of Mount McGregor, finishing only days before he died. Grant asked his former staff officer, Adam Badeau, to help edit his work. Grant's son Fred assisted with references and proofreading. "Century" magazine offered Grant a book contract with a 10 percent royalty, but Grant accepted a better offer from his friend, Mark Twain, who proposed a 75 percent royalty. His memoir ends with the Civil War, and does not cover the post-war years, including his presidency.
The book, "Personal Memoirs of Ulysses S. Grant", was a critical and commercial success. In the end, Julia Grant received about $450,000 in royalties. The memoir has been highly regarded by the public, military historians, and literary critics. Grant portrayed himself in the persona of the honorable Western hero, whose strength lies in his honesty and straightforwardness. He candidly depicted his battles against both the Confederates and internal army foes. Twain called the "Memoirs" a "literary masterpiece." Given over a century of favorable literary analysis, reviewer Mark Perry states that the "Memoirs" are "the most significant work" of American non-fiction.
In the days preceding his death, Grant's wife, Julia, all of his children, and three grandchildren were present. After a year-long struggle with the cancer, Grant died at 8 o'clock in the morning in the Mount McGregor cottage on July 23, 1885, at the age of 63. Sheridan, then Commanding General of the Army, ordered a day-long tribute to Grant on all military posts, and President Grover Cleveland ordered a thirty-day nationwide period of mourning. After private services, the honor guard placed Grant's body on a special funeral train, which traveled to West Point and New York City. A quarter of a million people viewed it in the two days before the funeral. Tens of thousands of men, many of them veterans from the Grand Army of the Republic (GAR) or other veterans' organizations, marched with Grant's casket drawn by two dozen horses to Riverside Park in the Morningside Heights neighborhood of Upper Manhattan, New York City. His pallbearers included Union generals Sherman and Sheridan, Confederate generals Simon Bolivar Buckner and Joseph E. Johnston, Admiral David Dixon Porter, and Senator John A. Logan, the head of the GAR.
Grant's body was laid to rest in Riverside Park, first in a temporary tomb, and then—twelve years later, on April 17, 1897—in the General Grant National Memorial, also known as "Grant's Tomb". The tomb is the largest mausoleum in North America. Attendance at the New York funeral topped 1.5 million. Ceremonies were held in other major cities around the country, and those who eulogized Grant in the press likened him to George Washington and Abraham Lincoln.
Historical reputation.
No presidential reputations have changed as dramatically as Grant's. Hailed across the North as the winning general in a great war, his nomination as president seemed inevitable. Grant's popularity declined with congressional investigations into corruption in his administration and Custer's defeat at the Battle of the Little Big Horn. In 1877, there was bipartisan approval of Grant's peaceful handling of the electoral crisis. Grant's reputation soared during his well-publicized world tour.
At his death, Grant was seen as "a symbol of the American national identity and memory", when millions turned out for his funeral procession in 1885 and attended the 1897 dedication of his tomb. Grant's popularity increased in the years immediately after his death. At the same time, commentators and scholars portrayed his administration as the most corrupt in American history. As the popularity of the pro-Confederate Lost Cause movement increased early in the 20th century, a more negative view became increasingly common. As they had early in the Civil War, Grant's new critics charged that he was a reckless drunk, and in light of his presidency, that he was also corrupt. In the 1930s, biographer William B. Hesseltine noted that Grant's reputation deteriorated because his enemies were better writers than his friends. In 1931, Frederic Paxson and Christian Bach in the "Dictionary of American Biography" praised Grant's military vision and his execution of that vision in defeating the Confederacy, but of his political career, the authors were less complimentary. Speaking specifically of the scandals, they wrote that "personal scandal has not touched Grant in any plausible form, but it struck so close to him and so frequently as to necessitate the vindication of his honor by admitting his bad taste in the choice of associates."
Views of Grant reached new lows as he was seen as an unsuccessful president and an unskilled, if lucky, general. Bruce Catton and T. Harry Williams began the reassessment of Grant's military career in the 1960s, shifting the analysis of Grant as victor by brute force to that of successful and skillful strategist and commander. Even for scholars with a particular concern for the plight of former slaves and Indians, Grant left a problematic legacy and, with changing attitudes toward warfare after the end of the Vietnam War, Grant's military reputation suffered again. William S. McFeely won the Pulitzer Prize for his critical 1981 biography that emphasized the failure of Grant's presidency to carry out lasting progress and concluded that "he did not rise above limited talents or inspire others to do so in ways that make his administration a credit to American politics." John Y. Simon in 1982 responded to McFeely: "Grant's failure as President ... lies in the failure of the Indian peace policy and the collapse of Reconstruction ... But if Grant tried and failed, who could have succeeded?" Simon praised Grant's first term in office, arguing that it should be "remembered for his staunch enforcement of the rights of freedmen combined with conciliation of former Confederates, for reform in Indian policy and civil service, for successful negotiation of the "Alabama" Claims, and for delivery of peace and prosperity." According to Simon, the Liberal Republican revolt, the Panic of 1873, and the North's conservative retreat from Reconstruction weakened Grant's second term in office, although his foreign policy remained steady.
Historians' views have grown more favorable since the 1990s, appreciating Grant's protection of African Americans and his peace policy towards Indians, even where those policies failed. Grant's reputation rose further with Jean Edward Smith's 2001 biography. Smith argued that the same qualities that made Grant a success as a general carried over to his political life to make him, if not a successful president, then certainly an admirable one. Smith wrote that "the common thread is strength of character—an indomitable will that never flagged in the face of adversity ... Sometimes he blundered badly; often he oversimplified; yet he saw his goals clearly and moved toward them relentlessly." Brooks Simpson continued the trend in the first of two volumes on Grant in 2000, although the work was far from a hagiography. H. W. Brands, in his more uniformly positive 2012 book, wrote favorably of Grant's military and political careers alike, saying:
As Reconstruction scholar Eric Foner wrote, Brands gave "a sympathetic account of Grant's forceful and temporarily successful effort as president to crush the Ku Klux Klan, which had inaugurated a reign of terror against the former slaves." Foner adds that by 1875 Grant had given up rescuing the blacks, saying the public was tired of them, concluding that "Grant's unwillingness to act reflected the broader Northern retreat from Reconstruction and its ideal of racial equality."
Besides civil rights, issues of environmental protection have also attracted historiographical attention. Historian Joan Waugh, in her generally favorable book, says Grant appreciated the beauty of the West, and in 1872 signed the law establishing the country's first national park at Yellowstone. However, she argues:
Throughout the 20th century, historians ranked his presidency near the bottom. In the 21st century, his military reputation is strong, while most scholars rank his presidency well below average. His accomplishments as President have been overlooked due to corruption charges of his Cabinet members and appointees during his administration.
Memorials and presidential library.
Several memorials honor Grant. In addition to his mausoleum, the General Grant National Memorial in New York, there are the Ulysses S. Grant Memorial at the foot of Capitol Hill on the mall in Washington, D.C., the Ulysses S. Grant National Historic Site near St. Louis, and several other sites in Ohio and Illinois. There are smaller memorials in Chicago's Lincoln Park and Philadelphia's Fairmount Park. Named in his honor are Grant Park, as well as several counties in western and midwestern states. On June 3, 1891, a bronze statue of Grant, created by Danish sculptor Johannes Gelert and commissioned by publisher H. H. Kohlsaat, was dedicated at Grant Park in Galena, Illinois.
In May 2012, the Ulysses S. Grant Foundation, on the institute's fiftieth anniversary, selected Mississippi State University as the permanent location for Ulysses S. Grant's presidential library. Historian John Y. Simon edited Grant's letters into a 32-volume scholarly edition published by Southern Illinois University Press.

</doc>
<doc id="31756" url="https://en.wikipedia.org/wiki?curid=31756" title="United States Congress">
United States Congress

The United States Congress is the bicameral legislature of the federal government of the United States consisting of two houses: the Senate and the House of Representatives. The Congress meets in the Capitol in Washington, D.C. Both senators and representatives are chosen through direct election, though vacancies in the Senate may be filled by a gubernatorial appointment. Members are usually affiliated to the Republican Party or to the Democratic Party, and only rarely to a third-party or as independents. Congress has 535 voting members: 435 Representatives and 100 Senators.
The members of the House of Representatives serve two-year terms representing the people of a single constituency, known as a "district". Congressional districts are apportioned to states by population using the United States Census results, provided that each state has at least one congressional representative. Each state, regardless of population or size, has two senators. Currently, there are 100 senators representing the 50 states. Each senator is elected at-large in his or her state for a six-year term, with terms staggered, so every two years approximately one-third of the Senate is up for election.
Overview.
Article I of the Constitution states, "All legislative Powers herein granted shall be vested in a Congress of the United States, which shall consist of a Senate and House of Representatives." The House and Senate are equal partners in the legislative process—legislation cannot be enacted without the consent of both chambers. However, the Constitution grants each chamber some unique powers. The Senate ratifies treaties and approves presidential appointments while the House initiates revenue-raising bills. The House initiates impeachment cases, while the Senate decides impeachment cases. A two-thirds vote of the Senate is required before an impeached person can be forcibly removed from office.
The term "Congress" can also refer to a particular meeting of the legislature. A Congress covers two years; the current one, the 114th Congress, began on January 3, 2015, and would end on January 3, 2017. The Congress starts and ends on the third day of January of every odd-numbered year. Members of the Senate are referred to as senators; members of the House of Representatives are referred to as representatives, congressmen, or congresswomen.
Scholar and representative Lee H. Hamilton asserted that the "historic mission of Congress has been to maintain freedom" and insisted it was a "driving force in American government" and a "remarkably resilient institution". Congress is the "heart and soul of our democracy", according to this view, even though legislators rarely achieve the prestige or name recognition of presidents or Supreme Court justices; one wrote that "legislators remain ghosts in America's historical imagination". One analyst argues that it is not a solely reactive institution but has played an active role in shaping government policy and is extraordinarily sensitive to public pressure. Several academics described Congress:
Congress reflects us in all our strengths and all our weaknesses. It reflects our regional idiosyncrasies, our ethnic, religious, and racial diversity, our multitude of professions, and our shadings of opinion on everything from the value of war to the war over values. Congress is the government's most representative body ... Congress is essentially charged with reconciling our many points of view on the great public policy issues of the day.
Congress is constantly changing and is constantly in flux. In recent times, the American south and west have gained House seats according to demographic changes recorded by the census and includes more minorities and women although both groups are still underrepresented, according to one view. While power balances among the different parts of government continue to change, the internal structure of Congress is important to understand along with its interactions with so-called "intermediary institutions" such as political parties, civic associations, interest groups, and the mass media.
The Congress of the United States serves two distinct purposes that overlap: local representation to the federal government of a congressional district by representatives and a state's at-large representation to the federal government by senators.
Most incumbents seek re-election, and their historical likelihood of winning subsequent elections exceeds 90 percent.
The historical records of the House of Representatives and the Senate are maintained by the Center for Legislative Archives, which is a part of the National Archives and Records Administration.
Congress is directly responsible for the governing of the District of Columbia, the current seat of the federal government.
History.
The First Continental Congress was a gathering of representatives from twelve of the thirteen British Colonies in North America. On July 4, 1776, the Second Continental Congress adopted the Declaration of Independence, referring to the new nation as the "United States of America". The Articles of Confederation in 1781 created the Congress of the Confederation, a unicameral body with equal representation among the states in which each state had a veto over most decisions. With no executive or judiciary, and minimal authority, this government was weak and lacked authority to collect taxes, regulate commerce, or enforce laws.
Government powerlessness led to the Convention of 1787 which proposed a revised constitution with a two–chamber or "bicameral" congress. Smaller states argued for equal representation for each state. The two-chamber structure had functioned well in state governments. A compromise plan was adopted with representatives chosen by population (benefitting larger states) and exactly two senators chosen by state governments (benefitting smaller states). The ratified constitution created a federal structure with two overlapping power centers so that each citizen "as an individual" was subjected to both the power of state government and the national government. To protect against abuse of power, each branch of government—executive, legislative, and judicial—had a separate sphere of authority and could check other branches according to the principle of the separation of powers. Furthermore, there were checks and balances "within" the legislature since there were two separate chambers. The new government became active in 1789.
Political scientist Julian E. Zelizer suggested there were four main congressional eras, with considerable overlap, and included the "formative era" (1780s–1820s), the "partisan era" (1830s–1900s), the "committee era" (1910s–1960s), and the "contemporary era" (1970s–today).
The formative era (1780s–1820s).
Federalists and anti-federalists jostled for power in the early years as political parties became pronounced, surprising the Constitution's Founding Fathers of the United States. With the passage of the Constitution and the Bill of Rights, the Anti-Federalist movement was exhausted. Some activists joined the Anti-Administration Party that James Madison and Thomas Jefferson were forming about 1790–91 to oppose policies of Treasury Secretary Alexander Hamilton; it soon became the Democratic-Republican Party or the Jeffersonian Democrat Party and began the era of the First Party. Thomas Jefferson's election to the presidency marked a peaceful transition of power between the parties in 1800. John Marshall, 4th Chief Justice of the Supreme Court empowered the courts by establishing the principle of judicial review in law in the landmark case "Marbury v. Madison" in 1803, effectively giving the Supreme Court a power to nullify congressional legislation.
The partisan era (1830s–1900s).
These years were marked by growth in the power of political parties. The watershed event was the Civil War which resolved the slavery issue and unified the nation under federal authority, but weakened the power of states rights. A Gilded Age (1877–1901) was marked by Republican dominance of Congress. During this time, lobbying activity became more intense, particularly during the administration of President Ulysses S. Grant in which influential lobbies advocated for railroad subsidies and tariffs on wool. Immigration and high birth rates swelled the ranks of citizens and the nation grew at a rapid pace. The Progressive Era was characterized by strong party leadership in both houses of Congress as well as calls for reform; sometimes reformers would attack lobbyists as corrupting politics. The position of Speaker of the House became extremely powerful under leaders such as Thomas Reed in 1890 and Joseph Gurney Cannon. The Senate was effectively controlled by a half dozen men.
The committee era (1910s–1960s).
A system of seniority—in which long-time Members of Congress gained more and more power—encouraged politicians of both parties to serve for long terms. Committee chairmen remained influential in both houses until the reforms of the 1970s. Important structural changes included the direct election of senators by popular election according to the Seventeenth Amendment with positive effects (senators more sensitive to public opinion) and negative effects (undermining the authority of state governments). Supreme Court decisions based on the Constitution's commerce clause expanded congressional power to regulate the economy. One effect of popular election of senators was to reduce the difference between the House and Senate in terms of their link to the electorate. Lame duck reforms according to the Twentieth Amendment ended the power of defeated and retiring members of Congress to wield influence despite their lack of accountability.
The Great Depression ushered in President Franklin Roosevelt and strong control by Democrats and historic New Deal policies. Roosevelt's election in 1932 marked a shift in government power towards the executive branch. Numerous New Deal initiatives came from the White House rather than being initiated by Congress. The Democratic Party controlled both houses of Congress for many years. During this time, Republicans and conservative southern Democrats formed the Conservative Coalition. Democrats maintained control of Congress during World War II. Congress struggled with efficiency in the postwar era partly by reducing the number of standing congressional committees. Southern Democrats became a powerful force in many influential committees although political power alternated between Republicans and Democrats during these years. More complex issues required greater specialization and expertise, such as space flight and atomic energy policy. Senator Joseph McCarthy exploited the fear of communism and conducted televised hearings. In 1960, Democratic candidate John F. Kennedy narrowly won the presidency and power shifted again to the Democrats who dominated both houses of Congress until 1994.
The contemporary era (1970s–today).
Congress enacted Johnson's Great Society program to fight poverty and hunger. The Watergate Scandal had a powerful effect of waking up a somewhat dormant Congress which investigated presidential wrongdoing and coverups; the scandal "substantially reshaped" relations between the branches of government, suggested political scientist Bruce J. Schulman. Partisanship returned, particularly after 1994; one analyst attributes partisan infighting to slim congressional majorities which discouraged friendly social gatherings in meeting rooms such as the "Board of Education". Congress began reasserting its authority. Lobbying became a big factor despite the 1971 Federal Election Campaign Act. Political action committees or PACs could make substantive donations to congressional candidates via such means as "soft money" contributions. While soft money funds were not given to specific campaigns for candidates, the monies often benefited candidates substantially in an indirect way and helped reelect candidates. Reforms such as the 2002 McCain-Feingold act limited campaign donations but did not limit "soft money" contributions. One source suggests post-Watergate laws amended in 1974 meant to reduce the "influence of wealthy contributors and end payoffs" instead "legitimized PACs" since they "enabled individuals to band together in support of candidates". From 1974 to 1984, PACs grew from 608 to 3,803 and donations leaped from $12.5 million to $120 million along with concern over PAC influence in Congress. In 2009, there were 4,600 business, labor and special-interest PACs including ones for lawyers, electricians, and real estate brokers. From 2007 to 2008, 175 members of Congress received "half or more of their campaign cash" from PACs.
From 1970 to 2009, the House expanded delegates, along with their powers and privileges representing U.S. citizens in non-state areas, beginning with representation on committees for Puerto Rico's Resident Commissioner in 1970. In 1971, a delegate for the District of Columbia was authorized, and in 1972 new delegate positions were established for U.S. Virgin Islands and Guam. 1978 saw an additional delegate for American Samoa, and another for the Commonwealth of the Northern Mariana Islands began in 2009. These six Members of Congress enjoy floor privileges to introduce bills and resolutions, and in recent congresses they vote in permanent and select committees, in party caucuses and in joint conferences with the Senate. They have Capitol Hill offices, staff and two annual appointments to each of the four military academies. While their votes are constitutional when Congress authorizes their House Committee of the Whole votes, recent Congresses have not allowed for that, and they cannot vote when the House is meeting as the House of Representatives.
In the late 20th century, the media became more important in Congress's work. Analyst Michael Schudson suggested that greater publicity undermined the power of political parties and caused "more roads to open up in Congress for individual representatives to influence decisions". Norman Ornstein suggested that media prominence led to a greater emphasis on the negative and sensational side of Congress, and referred to this as the "tabloidization" of media coverage. Others saw pressure to squeeze a political position into a thirty-second soundbite. A report characterized Congress in 2013 as being unproductive, gridlocked, and "setting records for futility". In October 2013, with Congress unable to compromise, the government was shut down for several weeks and risked a serious default on debt payments, causing 60% of the public to say they would "fire every member of Congress" including their own representative. One report suggested Congress posed the "biggest risk to the US economy" because of its brinksmanship, "down-to-the-wire budget and debt crises" and "indiscriminate spending cuts", resulting in slowed economic activity and keeping up to two million people unemployed. There has been increasing public dissatisfaction with Congress, with extremely low approval ratings which dropped to 5% in October 2013.
Congress in the United States government.
Powers of Congress.
Overview of congressional power.
Article I of the Constitution sets forth most of the powers of Congress, which include numerous explicit powers enumerated in Section 8. Constitutional amendments have granted Congress additional powers. Congress also has implied powers derived from the Constitution's Necessary and Proper Clause.
Congress has authority over financial and budgetary policy through the enumerated power to "lay and collect Taxes, Duties, Imposts and Excises, to pay the Debts and provide for the common Defence and general Welfare of the United States". There is vast authority over budgets, although analyst Eric Patashnik suggested that much of Congress's power to manage the budget has been lost when the welfare state expanded since "entitlements were institutionally detached from Congress's ordinary legislative routine and rhythm". Another factor leading to less control over the budget was a Keynesian belief that balanced budgets were unnecessary.
The Sixteenth Amendment in 1913 extended congressional power of taxation to include income taxes without apportionment among the several States, and without regard to any census or enumeration. The Constitution also grants Congress the exclusive power to appropriate funds, and this "power of the purse" is one of Congress's primary checks on the executive branch. Congress can borrow money on the credit of the United States, regulate commerce with foreign nations and among the states, and coin money. Generally, both the Senate and the House of Representatives have equal legislative authority, although only the House may originate revenue and appropriation bills.
Congress has an important role in national defense, including the exclusive power to declare war, to raise and maintain the armed forces, and to make rules for the military. Some critics charge that the executive branch has usurped Congress's constitutionally defined task of declaring war. While historically presidents initiated the process for going to war, they asked for and received formal war declarations from Congress for the War of 1812, the Mexican–American War, the Spanish–American War, World War I, and World War II, although President Theodore Roosevelt's military move into Panama in 1903 did not get congressional approval. In the early days after the North Korean invasion of 1950, President Truman described the American response as a "police action". According to "Time" magazine in 1970, "U.S. presidents ordered troops into position or action without a formal congressional declaration a total of 149 times." In 1993, Michael Kinsley wrote that "Congress's war power has become the most flagrantly disregarded provision in the Constitution," and that the "real erosion [of Congress's war power began after World War II." Disagreement about the extent of congressional versus presidential power regarding war has been present periodically throughout the nation's history."
Congress can establish post offices and post roads, issue patents and copyrights, fix standards of weights and measures, establish Courts inferior to the Supreme Court, and "make all Laws which shall be necessary and proper for carrying into Execution the foregoing Powers, and all other Powers vested by this Constitution in the Government of the United States, or in any Department or Officer thereof." Article Four gives Congress the power to admit new states into the Union.
One of Congress's foremost non-legislative functions is the power to investigate and oversee the executive branch. Congressional oversight is usually delegated to committees and is facilitated by Congress's subpoena power. Some critics have charged that Congress has in some instances failed to do an adequate job of overseeing the other branches of government. In the Plame affair, critics including Representative Henry A. Waxman charged that Congress was not doing an adequate job of oversight in this case. There have been concerns about congressional oversight of executive actions such as warrantless wiretapping, although others respond that Congress did investigate the legality of presidential decisions. Political scientists Ornstein and Mann suggested that oversight functions do not help members of Congress win reelection. Congress also has the exclusive power of removal, allowing impeachment and removal of the president, federal judges and other federal officers. There have been charges that presidents acting under the doctrine of the unitary executive have assumed important legislative and budgetary powers that should belong to Congress. So-called signing statements are one way in which a president can "tip the balance of power between Congress and the White House a little more in favor of the executive branch," according to one account. Past presidents, including Ronald Reagan, George H. W. Bush, Bill Clinton, and George W. Bush have made public statements when signing congressional legislation about how they understand a bill or plan to execute it, and commentators including the American Bar Association have described this practice as against the spirit of the Constitution. There have been concerns that presidential authority to cope with financial crises is eclipsing the power of Congress. In 2008, George F. Will called the Capitol building a "tomb for the antiquated idea that the legislative branch matters."
Enumerated powers.
The Constitution enumerates the powers of Congress in detail. In addition, other congressional powers have been granted, or confirmed, by constitutional amendments. The Thirteenth (1865), Fourteenth (1868), and Fifteenth Amendments (1870) gave Congress authority to enact legislation to enforce rights of African Americans, including voting rights, due process, and equal protection under the law. Generally militia forces are controlled by state governments, not Congress.
Implied powers and the commerce clause.
Congress also has implied powers deriving from the Constitution's Necessary and Proper Clause which permit Congress to "make all Laws which shall be necessary and proper for carrying into Execution the foregoing Powers, and all other Powers vested by this Constitution in the Government of the United States, or in any Department or Officer thereof." Broad interpretations of this clause and of the Commerce Clause, the enumerated power to regulate commerce, in rulings such as McCulloch v Maryland, have effectively widened the scope of Congress's legislative authority far beyond that prescribed in Section 8.
Checks and balances.
Representative Lee H. Hamilton explained how Congress functions within the federal government:
To me the key to understanding it is balance. The founders went to great lengths to balance institutions against each other—balancing powers among the three branches: Congress, the president, and the Supreme Court; between the House of Representatives and the Senate; between the federal government and the states; among states of different sizes and regions with different interests; between the powers of government and the rights of citizens, as spelled out in the Bill of Rights ... No one part of government dominates the other.
The Constitution provides checks and balances among the three branches of the federal government. Its authors expected the greater power to lie with Congress as described in Article One.
The influence of Congress on the presidency has varied from period to period depending on factors such as congressional leadership, presidential political influence, historical circumstances such as war, and individual initiative by members of Congress. The impeachment of Andrew Johnson made the presidency less powerful than Congress for a considerable period afterwards. The 20th and 21st centuries have seen the rise of presidential power under politicians such as Theodore Roosevelt, Woodrow Wilson, Franklin D. Roosevelt, Richard Nixon, Ronald Reagan, and George W. Bush. However, in recent years, Congress has restricted presidential power with laws such as the Congressional Budget and Impoundment Control Act of 1974 and the War Powers Resolution. Nevertheless, the Presidency remains considerably more powerful today than during the 19th century. Executive branch officials are often loath to reveal sensitive information to members of Congress because of concern that information could not be kept secret; in return, knowing they may be in the dark about executive branch activity, congressional officials are more likely to distrust their counterparts in executive agencies. Many government actions require fast coordinated effort by many agencies, and this is a task that Congress is ill-suited for. Congress is slow, open, divided, and not well matched to handle more rapid executive action or do a good job of overseeing such activity, according to one analysis.
The Constitution concentrates removal powers in the Congress by empowering and obligating the House of Representatives to impeach both executive and judicial officials for "Treason, Bribery, or other high Crimes and Misdemeanors." Impeachment is a "formal accusation" of unlawful activity by a civil officer or government official. The Senate is constitutionally empowered and obligated to try all impeachments. A simple majority in the House is required to impeach an official; however, a two-thirds majority in the Senate is required for conviction. A convicted official is automatically removed from office; in addition, the Senate may stipulate that the defendant be banned from holding office in the future. Impeachment proceedings may not inflict more than this; however, a convicted party may face criminal penalties in a normal court of law. In the history of the United States, the House of Representatives has impeached sixteen officials, of whom seven were convicted. Another resigned before the Senate could complete the trial. Only two presidents have ever been impeached: Andrew Johnson in 1868 and Bill Clinton in 1999. Both trials ended in acquittal; in Johnson's case, the Senate fell one vote short of the two-thirds majority required for conviction. In 1974, Richard Nixon resigned from office after impeachment proceedings in the House Judiciary Committee indicated he would eventually be removed from office.
The Senate has an important check on the executive power by confirming Cabinet officials, judges, and other high officers "by and with the Advice and Consent of the Senate." It confirms most presidential nominees but rejections are not uncommon. Furthermore, treaties negotiated by the President must be ratified by a two-thirds majority vote in the Senate to take effect. As a result, presidential arm-twisting of senators can happen before a key vote; for example, President Obama's secretary of state, Hillary Rodham Clinton, urged her former senate colleagues to approve a nuclear arms treaty with Russia in 2010. The House of Representatives has no formal role in either the ratification of treaties or the appointment of federal officials, other than in in the office of the vice president; in such a case, a majority vote in each House is required to confirm a president's nomination of a vice president.
In 1803, the Supreme Court established judicial review of federal legislation in "Marbury v. Madison", holding, however, that Congress could not grant unconstitutional power to the Court itself. The Constitution does not explicitly state that the courts may exercise judicial review; however, the notion that courts could declare laws unconstitutional was envisioned by the founding fathers. Alexander Hamilton, for example, mentioned and expounded upon the doctrine in Federalist No. 78. Originalists on the Supreme Court have argued that if the constitution does not say something explicitly it is unconstitutional to infer what it should, might or could have said. Judicial review means that the Supreme Court can nullify a congressional law. It is a huge check by the courts on the legislative authority and limits congressional power substantially. In 1857, for example, the Supreme Court struck down provisions of a congressional act of 1820 in its Dred Scott decision. At the same time, the Supreme Court can extend congressional power through its constitutional interpretations.
Investigations are conducted to gather information on the need for future legislation, to test the effectiveness of laws already passed, and to inquire into the qualifications and performance of members and officials of the other branches. Committees may hold hearings, and, if necessary, compel individuals to testify when investigating issues over which it has the power to legislate by issuing subpoenas. Witnesses who refuse to testify may be cited for contempt of Congress, and those who testify falsely may be charged with perjury. Most committee hearings are open to the public (the House and Senate intelligence committees are the exception); important hearings are widely reported in the mass media and transcripts published a few months afterwards. Congress, in the course of studying possible laws and investigating matters, generates an incredible amount of information in various forms, and can be described as a publisher. Indeed, it publishes House and Senate reports and maintains databases which are updated irregularly with publications in a variety of electronic formats.
Congress also plays a role in presidential elections. Both Houses meet in joint session on the sixth day of January following a presidential election to count the electoral votes, and there are procedures to follow if no candidate wins a majority.
The main result of congressional activity is the creation of laws, most of which are contained in the United States Code, arranged by subject matter alphabetically under fifty title headings to present the laws "in a concise and usable form".
Structure.
Congress is split into two chambers—House and Senate—and manages the task of writing national legislation by dividing work into separate committees which specialize in different areas. Some members of Congress are elected by their peers to be officers of these committees. Further, Congress has ancillary organizations such as the Government Accountability Office and the Library of Congress to help provide it with information, and members of Congress have staff and offices to assist them as well. In addition, a vast industry of lobbyists helps members write legislation on behalf of diverse corporate and labor interests.
Committees.
Specializations.
The committee structure permits members of Congress to study a particular subject intensely. It is neither expected nor possible that a member be an expert on all subject areas before Congress. As time goes by, members develop expertise in particular subjects and their legal aspects. Committees investigate specialized subjects and advise the entire Congress about choices and trade-offs. The choice of specialty may be influenced by the member's constituency, important regional issues, prior background and experience. Senators often choose a different specialty from that of the other senator from their state to prevent overlap. Some committees specialize in running the business of other committees and exert a powerful influence over all legislation; for example, the House Ways and Means Committee has considerable influence over House affairs.
Power.
Committees write legislation. While procedures such as the House discharge petition process can introduce bills to the House floor and effectively bypass committee input, they are exceedingly difficult to implement without committee action. Committees have power and have been called "independent fiefdoms". Legislative, oversight, and internal administrative tasks are divided among about two hundred committees and subcommittees which gather information, evaluate alternatives, and identify problems. They propose solutions for consideration by the full chamber. In addition, they perform the function of "oversight" by monitoring the executive branch and investigating wrongdoing.
Officer.
At the start of each two-year session the House elects a speaker who does not normally preside over debates but serves as the majority party's leader. In the Senate, the Vice President is the ex officio "president" of the Senate. In addition, the Senate elects an officer called the President pro tempore. "Pro tempore" means "for the time being" and this office is usually held by the most senior member of the Senate's majority party and customarily keeps this position until there's a change in party control. Accordingly, the Senate does not necessarily elect a new president pro tempore at the beginning of a new Congress. In both the House and Senate, the actual presiding officer is generally a junior member of the majority party who is appointed so that new members become acquainted with the rules of the chamber.
Support services.
Library of Congress.
The Library of Congress was established by an act of Congress in 1800. It is primarily housed in three buildings on Capitol Hill, but also includes several other sites: the National Library Service for the Blind and Physically Handicapped in Washington, D.C.; the National Audio-Visual Conservation Center in Culpeper, Virginia; a large book storage facility located at Ft. Meade, Maryland; and multiple overseas offices. The Library had mostly law books when it was burned by a British raiding party during the War of 1812, but the library's collections were restored and expanded when Congress authorized the purchase of Thomas Jefferson's private library. One of the Library's missions is to serve the Congress and its staff as well as the American public. It is the largest library in the world with nearly 150 million items including books, films, maps, photographs, music, manuscripts, graphics, and materials in 470 languages.
Congressional Research Service.
The Congressional Research Service provides detailed, up-to-date and non-partisan research for senators, representatives, and their staff to help them carry out their official duties. It provides ideas for legislation, helps members analyze a bill, facilitates public hearings, makes reports, consults on matters such as parliamentary procedure, and helps the two chambers resolve disagreements. It has been called the "House's think tank" and has a staff of about 900 employees.
Congressional Budget Office.
The Congressional Budget Office or CBO is a federal agency which provides economic data to Congress.
It was created as an independent nonpartisan agency by the Congressional Budget and Impoundment Control Act of 1974. It helps Congress estimate revenue inflows from taxes and helps the budgeting process. It makes projections about such matters as the national debt as well as likely costs of legislation. It prepares an annual "Economic and Budget Outlook" with a mid-year update and writes "An Analysis of the President's Budgetary Proposals" for the Senate's Appropriations Committee. The Speaker of the House and the Senate's President pro tempore jointly appoint the CBO Director for a four-year term.
Lobbyists.
Lobbyists represent diverse interests and often seek to influence congressional decisions to reflect their clients' needs. Lobby groups and their members sometimes write legislation and whip bills. In 2007 there were approximately 17,000 federal lobbyists in Washington. They explain to legislators the goals of their organizations. Some lobbyists represent non-profit organizations and work pro-bono for issues in which they are personally interested.
Partisanship versus bipartisanship.
Congress has alternated between periods of constructive cooperation and compromise between parties known as bipartisanship and periods of deep political polarization and fierce infighting known as partisanship. The period after the Civil War was marked by partisanship as is the case today. It is generally easier for committees to reach accord on issues when compromise is possible. Some political scientists speculate that a prolonged period marked by narrow majorities in both chambers of Congress has intensified partisanship in the last few decades but that an alternation of control of Congress between Democrats and Republicans may lead to greater flexibility in policies as well as pragmatism and civility within the institution.
Procedures of Congress.
Sessions.
A term of Congress is divided into two "sessions", one for each year; Congress has occasionally been called into an extra or "special session". A new session commences on January 3 each year unless Congress decides differently. The Constitution requires Congress meet at least once each year and forbids either house from meeting outside the Capitol without the consent of the other house.
Joint sessions.
Joint Sessions of the United States Congress occur on special occasions that require a concurrent resolution from both House and Senate. These sessions include counting electoral votes after a presidential election and the president's State of the Union address. The constitutionally-mandated report, normally given as an annual speech, is modeled on Britain's Speech from the Throne, was written by most presidents after Jefferson but personally delivered as a spoken oration beginning with Wilson in 1913. Joint Sessions and Joint Meetings are traditionally presided over by the Speaker of the House except when counting presidential electoral votes when the vice president presides.
Bills and resolutions.
Ideas for legislation can come from members, lobbyists, state legislatures, constituents, legislative counsel, or executive agencies. Anyone can write a bill, but only members of Congress may introduce bills. Most bills are not written by Congress members, but originate from the Executive branch; interest groups often draft bills as well. The usual next step is for the proposal to be passed to a committee for review. A proposal is usually in one of these forms:
Representatives introduce a bill while the House is in session by placing it in the "hopper" on the Clerk's desk. It's assigned a number and referred to a committee which studies each bill intensely at this stage. Drafting statutes requires "great skill, knowledge, and experience" and sometimes take a year or more. Sometimes lobbyists write legislation and submit it to a member for introduction. Joint resolutions are the normal way to propose a constitutional amendment or declare war. On the other hand, concurrent resolutions (passed by both houses) and simple resolutions (passed by only one house) do not have the force of law but express the opinion of Congress or regulate procedure. Bills may be introduced by any member of either house. However, the Constitution states, "All Bills for raising Revenue shall originate in the House of Representatives." While the Senate cannot originate revenue and appropriation bills, it has power to amend or reject them. Congress has sought ways to establish appropriate spending levels.
Each chamber determines its own internal rules of operation unless specified in the Constitution or prescribed by law. In the House, a Rules Committee guides legislation; in the Senate, a Standing Rules committee is in charge. Each branch has its own traditions; for example, the Senate relies heavily on the practice of getting "unanimous consent" for noncontroversial matters. House and Senate rules can be complex, sometimes requiring a hundred specific steps before becoming a law. Members sometimes use experts such as Walter Oleszek, a senior specialist in American national government at the Congressional Research Service, to learn about proper procedures.
Each bill goes through several stages in each house including consideration by a committee and advice from the Government Accountability Office. Most legislation is considered by standing committees which have jurisdiction over a particular subject such as Agriculture or Appropriations. The House has twenty standing committees; the Senate has sixteen. Standing committees meet at least once each month. Almost all standing committee meetings for transacting business must be open to the public unless the committee votes, publicly, to close the meeting. A committee might call for public hearings on important bills. Each committee is led by a chair who belongs to the majority party and a ranking member of the minority party. Witnesses and experts can present their case for or against a bill. Then, a bill may go to what's called a "mark-up" session where committee members debate the bill's merits and may offer amendments or revisions. Committees may also amend the bill, but the full house holds the power to accept or reject committee amendments. After debate, the committee votes whether it wishes to report the measure to the full house. If a bill is "tabled" then it is rejected. If amendments are extensive, sometimes a new bill with amendments built in will be submitted as a so-called "clean bill" with a new number. Both houses have procedures under which committees can be bypassed or overruled but they are rarely used. Generally, members who have been in Congress longer have greater seniority and therefore greater power.
A bill which reaches the floor of the full house can be simple or complex and begins with an enacting formula such as "Be it enacted by the Senate and House of Representatives of the United States of America in Congress assembled." Consideration of a bill requires, itself, a "rule" which is a simple resolution specifying the particulars of debate—time limits, possibility of further amendments, and such. Each side has equal time and members can yield to other members who wish to speak. Sometimes opponents seek to "recommit" a bill which means to change part of it. Generally, discussion requires a "quorum", usually half of the total number of representatives, before discussion can begin, although there are exceptions. The house may debate and amend the bill; the precise procedures used by the House and Senate differ. A final vote on the bill follows.
Once a bill is approved by one house, it is sent to the other which may pass, reject, or amend it. For the bill to become law, both houses must agree to identical versions of the bill. If the second house amends the bill, then the differences between the two versions must be reconciled in a conference committee, an "ad hoc" committee that includes both senators and representatives sometimes by using a "reconciliation process" to limit budget bills. Both Houses use a budget enforcement mechanism informally known as "pay-as-you-go" or "paygo" which discourages members from considering acts which increase budget deficits. If both houses agree to the version reported by the conference committee, the bill passes, otherwise it fails.
The Constitution specifies that a majority of members known as a "quorum" be present before doing business in each house. However, the rules of each house assume that a quorum is present unless a quorum call demonstrates the contrary. Since representatives and senators who are present rarely demand quorum calls, debate often continues despite the lack of a majority.
Voting within Congress can take many forms, including systems using lights and bells and electronic voting. Both houses use voice voting to decide most matters in which members shout "aye" or "no" and the presiding officer announces the result. The Constitution, however, requires a recorded vote if demanded by one-fifth of the members present. If the voice vote is unclear or if the matter is controversial, a recorded vote usually happens. The Senate uses roll-call voting, in which a clerk calls out the names of all the senators, each senator stating "aye" or "no" when his or her name is announced. In the Senate, the vice president may cast the tie-breaking vote if present.
The House reserves roll-call votes for the most formal matters, as a roll call of all 435 representatives takes quite some time; normally, members vote by using an electronic device. In the case of a tie, the motion in question fails. Most votes in the House are done electronically, allowing members to vote "yea" or "nay" or "present" or "open". Members insert a voting "ID card" and can change their votes during the last five minutes if they choose; in addition, paper ballots are used on some occasions—"yea" indicated by green and "nay" by red. One member can not cast a proxy vote for another. Congressional votes are recorded on an online database.
After passage by both houses, a bill is enrolled and sent to the president for approval. The president may sign it making it law or veto it, perhaps returning it to Congress with his objections. A vetoed bill can still become law if each house of Congress votes to override the veto with a two-thirds majority. Finally, the president may do nothing—neither signing nor vetoing the bill—and then the bill becomes law automatically after ten days (not counting Sundays) according to the Constitution. But if Congress is adjourned during this period, presidents may veto legislation passed at the end of a congressional session simply by ignoring it; the maneuver is known as a pocket veto, and cannot be overridden by the adjourned Congress.
Congress and the public.
Challenges of reelection.
Citizens and representatives.
Senators face reelection every six years, and representatives every two. Reelections encourage candidates to focus their publicity efforts at their home states or districts. Running for reelection can be a grueling process of distant travel and fund-raising which distracts senators and representatives from paying attention to governing, according to some critics although others respond that the process is necessary to keep members of Congress in touch with voters.
Nevertheless, incumbent members of Congress running for reelection have strong advantages over challengers. They raise more money because donors expect incumbents to win, they give their funds to them rather than challengers, and donations are vital for winning elections. One critic compared being elected to Congress to receiving life tenure at a university. Another advantage for representatives is the practice of gerrymandering. After each ten-year census, states are allocated representatives based on population, and officials in power can choose how to draw the congressional district boundaries to support candidates from their party. As a result, reelection rates of members of Congress hover around 90 percent, causing some critics to accuse them of being a privileged class. Academics such as Princeton's Stephen Macedo have proposed solutions to fix gerrymandering. Both senators and representatives enjoy free mailing privileges called franking privileges.
Expensive campaigns.
In 1971, the cost of running for congress in Utah was $70,000 but costs have climbed. The biggest expense is television ads. Today's races cost more than a million dollars for a House seat, and six million or more for a Senate seat. Since fundraising is vital, "members of Congress are forced to spend ever-increasing hours raising money for their re-election."
Nevertheless, the Supreme Court has treated campaign contributions as a free speech issue. Some see money as a good influence in politics since it "enables candidates to communicate with voters." Few members retire from Congress without complaining about how much it costs to campaign for reelection. Critics contend that members of Congress are more likely to attend to the needs of heavy campaign contributors than to ordinary citizens.
Elections are influenced by many variables. Some political scientists speculate there is a "coattail effect" (when a popular president or party position has the effect of reelecting incumbents who win by "riding on the president's coattails"), although there is some evidence that the coattail effect is irregular and possibly declining since the 1950s. Some districts are so heavily Democratic or Republican that they are called a safe seat; any candidate winning the primary will almost always be elected, and these candidates do not need to spend money on advertising. But some races can be competitive when there is no incumbent. If a seat becomes vacant in an open district, then both parties may spend heavily on advertising in these races; in California in 1992, only four of twenty races for House seats were considered highly competitive.
Television and negative advertising.
Since members of Congress must advertise heavily on television, this usually involves negative advertising, which smears an opponent's character without focusing on the issues. Negative advertising is seen as effective because "the messages tend to stick." However, these ads sour the public on the political process in general as most members of Congress seek to avoid blame. One wrong decision or one damaging television image can mean defeat at the next election, which leads to a culture of risk avoidance, a need to make policy decisions behind closed doors, and concentrating publicity efforts in the members' home districts.
Public perceptions of Congress.
Prominent Founding Fathers writing in "The Federalist" Papers felt that elections were essential to liberty, and that a bond between the people and the representatives was particularly essential and that "frequent elections are unquestionably the only policy by which this dependence and sympathy can be effectually secured." In 2009, however, few Americans were familiar with leaders of Congress. The percentage of Americans eligible to vote who did, in fact, vote was 63% in 1960, but has been falling since, although there was a slight upward trend in the 2008 election. Public opinion polls asking people if they approve of the job Congress is doing have, in the last few decades, hovered around 25% with some variation. Scholar Julian Zeliger suggested that the "size, messiness, virtues, and vices that make Congress so interesting also create enormous barriers to our understanding the institution... Unlike the presidency, Congress is difficult to conceptualize." Other scholars suggest that despite the criticism, "Congress is a remarkably resilient institution ... its place in the political process is not threatened ... it is rich in resources" and that most members behave ethically. They contend that "Congress is easy to dislike and often difficult to defend" and this perception is exacerbated because many challengers running for Congress run "against" Congress, which is an "old form of American politics" that further undermines Congress's reputation with the public:
The rough-and-tumble world of legislating is not orderly and civil, human frailties too often taint its membership, and legislative outcomes are often frustrating and ineffective ... Still, we are not exaggerating when we say that Congress is essential to American democracy. We would not have survived as a nation without a Congress that represented the diverse interests of our society, conducted a public debate on the major issues, found compromises to resolve conflicts peacefully, and limited the power of our executive, military, and judicial institutions ... The popularity of Congress ebbs and flows with the public's confidence in government generally ... the legislative process is easy to dislike—it often generates political posturing and grandstanding, it necessarily involves compromise, and it often leaves broken promises in its trail. Also, members of Congress often appear self-serving as they pursue their political careers and represent interests and reflect values that are controversial. Scandals, even when they involve a single member, add to the public's frustration with Congress and have contributed to the institution's low ratings in opinion polls.—Smith, Roberts & Wielen
An additional factor that confounds public perceptions of Congress is that congressional issues are becoming more technical and complex and require expertise in subjects such as science, engineering and economics. As a result, Congress often cedes authority to experts at the executive branch.
In January 2013, the popularity of Congress reached an all-time low, after a survey of voters indicated just 9% of those polled approved of its performance and 85% disapproved. Since 2011, Gallup poll has reported Congress's approval rating among Americans at 10% or below three times. Public opinion of Congress plummeted further to 5% in October 2013 after parts of the U.S. government deemed 'nonessential government' shut down.
Smaller states and bigger states.
When the Constitution was ratified in 1787, the ratio of the populations of large states to small states was roughly twelve to one. The Connecticut Compromise gave every state, large and small, an equal vote in the Senate. Since each state has two senators, residents of smaller states have more clout in the Senate than residents of larger states. But since 1787, the population disparity between large and small states has grown; in 2006, for example, California had seventy times the population of Wyoming. Critics such as constitutional scholar Sanford Levinson have suggested that the population disparity works against residents of large states and causes a steady redistribution of resources from "large states to small states." However, others argue that the Connecticut compromise was deliberately intended by the Framers to construct the Senate so that each state had equal footing not based on population, and contend that the result works well on balance.
Members and constituents.
A major role for members of Congress is providing services to constituents. Constituents request assistance with problems. Providing services helps members of Congress win votes and elections and can make a difference in close races. Congressional staff can help citizens navigate government bureaucracies. One academic described the complex intertwined relation between lawmakers and constituents as "home style". 
Congressional style.
One way to categorize lawmakers, according to political scientist Richard Fenno, is by their general motivation:
Privileges and pay.
Privileges protecting members.
Members of Congress enjoy parliamentary privilege, including freedom from arrest in all cases except for treason, felony, and breach of the peace and freedom of speech in debate. This constitutionally derived immunity applies to members during sessions and when traveling to and from sessions. The term "arrest" has been interpreted broadly, and includes any detention or delay in the course of law enforcement, including court summons and subpoenas. The rules of the House strictly guard this privilege; a member may not waive the privilege on his or her own, but must seek the permission of the whole house to do so. Senate rules, however, are less strict and permit individual senators to waive the privilege as they choose.
The Constitution guarantees absolute freedom of debate in both houses, providing in the Speech or Debate Clause of the Constitution that "for any Speech or Debate in either House, they shall not be questioned in any other Place." Accordingly, a member of Congress may not be sued in court for slander because of remarks made in either house, although each house has its own rules restricting offensive speeches, and may punish members who transgress.
Obstructing the work of Congress is a crime under federal law and is known as contempt of Congress. Each branch has the power to cite individuals for contempt but can only issue a contempt citation—the judicial system pursues the matter like a normal criminal case. If convicted in court, an individual found guilty of contempt of Congress may be imprisoned for up to one year.
The franking privilege allows members of Congress to send official mail to constituents at government expense. Though they are not permitted to send election materials, borderline material is often sent, especially in the run-up to an election by those in close races. Indeed, some academics consider free mailings as giving incumbents a big advantage over challengers.
Pay and benefits.
From 1789 to 1815, members of Congress received only a daily payment of $6 while in session. Members received an annual salary of $1,500 per year from 1815 to 1817, then a per diem salary of $8 from 1818 to 1855; since then they have received an annual salary, first pegged in 1855 at $3,000. In 1907, salaries were raised to $7,500 per year, the equivalent of $173,000 in 2010. In 2006, members of Congress received a yearly salary of $165,200. Congressional leaders were paid $183,500 per year. The Speaker of the House of Representatives earns $212,100 annually. The salary of the President pro tempore for 2006 was $183,500, equal to that of the majority and minority leaders of the House and Senate. Privileges include having an office and paid staff. In 2008, non-officer members of Congress earned $169,300 annually. Some critics complain congressional pay is high compared with a median American income of $45,113 for men and $35,102 for women. Others have countered that congressional pay is consistent with other branches of government. In January 2014, it was reported that for the first time over half of the members of Congress are millionaires. Congress has been criticized for trying to conceal pay raises by slipping them into a large bill at the last minute. Others have criticized the wealth of members of Congress. Representative Jim Cooper of Tennessee told Harvard professor Lawrence Lessig that a chief problem with Congress was that members focused on lucrative careers as lobbyists after serving––that Congress was a "Farm League for K Street"––instead of on public service.
Members elected since 1984 are covered by the Federal Employees Retirement System (FERS). Like other federal employees, congressional retirement is funded through taxes and participants' contributions. Members of Congress under FERS contribute 1.3% of their salary into the FERS retirement plan and pay 6.2% of their salary in Social Security taxes. And like Federal employees, members contribute one-third of the cost of health insurance with the government covering the other two-thirds.
The size of a congressional pension depends on the years of service and the average of the highest three years of his or her salary. By law, the starting amount of a member's retirement annuity may not exceed 80% of his or her final salary. In 2006, the average annual pension for retired senators and representatives under the Civil Service Retirement System (CSRS) was $60,972, while those who retired under FERS, or in combination with CSRS, was $35,952.
Members of Congress make fact-finding missions to learn about other countries and stay informed, but these outings can cause controversy if the trip is deemed excessive or unconnected with the task of governing. For example, the "Wall Street Journal" reported lawmaker trips abroad at taxpayer expense, which included spas, $300-per-night extra unused rooms, and shopping excursions. Lawmakers respond that "traveling with spouses compensates for being away from them a lot in Washington" and justify the trips as a way to meet officials in other nations.

</doc>
<doc id="31758" url="https://en.wikipedia.org/wiki?curid=31758" title="United States congressional delegations from Alabama">
United States congressional delegations from Alabama

These are tables of congressional delegations from Alabama to the United States House of Representatives and the United States Senate.
House of Representatives.
Current Representatives.
List of members of the Alabaman United States House delegation, their terms in office, district boundaries, and the district political ratings according to the CPVI. The delegation has a total of 7 members, including 6 Republicans and 1 Democrat.
Delegation timeline (1818 – Present).
Tables showing membership in the Alabama federal House delegation throughout history of statehood in the United States.
1818 – 1819: 1 non-voting delegate.
Starting on January 29, 1818, Alabama Territory sent a non-voting delegate to the House.
1819 – 1823: 1 seat.
After statehood on December 14, 1819, Alabama had one seat in the House.
1823 – 1833: 3 seats.
Following the 1820 census, Alabama had three seats.
1833 – 1843: 5 seats.
Following the 1830 census, Alabama had five seats. During the 27th Congress, those seats were all elected statewide at-large on a general ticket.
1843 – 1863: 7 seats.
Following the 1840 census, Alabama resumed the use of districts, now increased to seven.
1863 – 1873: 6 seats.
Following the 1860 census, Alabama was apportioned six seats.
1873 – 1893: 8 seats.
Following the 1870 census, Alabama was apportioned eight seats. From 1873 to 1877, the two new seats were elected at large, statewide. After 1877, however, the entire delegation was redistricted.
1893 – 1913: 9 seats.
Following the 1890 census, Alabama was apportioned nine seats.
1913 – 1933: 10 seats.
Following the 1910 census, Alabama was apportioned ten seats. At first, the extra seat was elected at-large. Starting with the 1916 elections, the seats were redistricted and a was added.
1933 – 1963: 9 seats.
Following the 1930 census, Alabama was apportioned nine seats.
1963 – 1973: 8 seats.
Following the 1960 census, Alabama was apportioned eight seats.
1973 – Present: 7 seats.
Since the 1970 census, Alabama has been apportioned seven seats.
Living former Members of the U.S. House of Representatives from Alabama.
, there are seventeen former members of the U.S. House of Representatives from the U.S. State of Alabama who are currently living at this time.
United States Senate.
Senate delegation timeline (1819 – Present).
Tables showing membership in the Alabama federal Senate delegation throughout history of statehood in the United States.
Living former U.S. Senators from Alabama.
, there are two former U.S. Senators from the U.S. State of Alabama who are currently living at this time, two from Class 3.

</doc>
<doc id="31759" url="https://en.wikipedia.org/wiki?curid=31759" title="United States congressional delegations from Alaska">
United States congressional delegations from Alaska

These are tables of congressional delegations from Alaska to the United States Senate and United States House of Representatives.
United States Senate.
Ernest Gruening was elected to the Senate on October 6, 1955 for the 84th Congress but did not take the oath of office and was not accorded senatorial privileges, Alaska not yet being admitted as a state.
House of Representatives.
Delegates from Alaska Territory.
From May 17, 1884 to August 24, 1912, Alaska was designated as the District of Alaska. From then to January 3, 1959, it was the Alaska Territory.
Living former Members of the U.S. House of Representatives from Alaska's at-large congressional district.
, there are no former members of the U.S. House of Representatives from Alaska's at-large congressional district who are currently living at this time.
Living former U.S. Senators from Alaska.
, there are three former U.S. Senators from the U.S. State of Alaska who are currently living at this time, one from Class 2 and two in Class 3.

</doc>
<doc id="31760" url="https://en.wikipedia.org/wiki?curid=31760" title="United States congressional delegations from Hawaii">
United States congressional delegations from Hawaii

These are tables of congressional delegations from Hawaii to the United States Senate and United States House of Representatives.
House of Representatives.
Members of the House.
Starting in 1971, Hawaii's representatives were elected from districts instead of statewide At-large.
Living former Members of the U.S. House of Representatives from Hawaii.
, there are seven former members of the U.S. House of Representatives from the U.S. State of Hawaii who are currently living at this time.
Living former U.S. Senators from Hawaii.
, there is one former U.S. Senator from the U.S. State of Hawaii who are currently living at this time, one from Class 1.

</doc>
<doc id="31764" url="https://en.wikipedia.org/wiki?curid=31764" title="United States congressional delegations from Arizona">
United States congressional delegations from Arizona

These are tables of congressional delegations from Arizona to the United States House of Representatives and the United States Senate.
House of Representatives.
Current Representatives.
List of members of the Arizonian United States House delegation, their terms in office, district boundaries, and the district political ratings according to the CPVI. The delegation has a total of 9 members, with 5 Republicans, and 4 Democrats.
Delegation timeline (1863 – Present).
Tables showing membership in the Arizona federal House delegation throughout history of statehood in the United States.
1943 – 1963: 2 seats.
After the 1940 census, a second seat was added. For six years, the seats were elected at-large statewide on a general ticket. In 1949, districts were used.
2013 – Current: 9 seats.
After the 2010 Census, Arizona gained one seat.
United States Senate.
Senate delegation timeline (1912 – Present).
Tables showing membership in the Arizona federal Senate delegation throughout history of statehood in the United States.
Living former U.S. Senators from Arizona.
, there are two former U.S. Senators from the U.S. State of Arizona who are currently living at this time, two from Class 1.

</doc>
