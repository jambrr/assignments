<doc id="66275" url="https://en.wikipedia.org/wiki?curid=66275" title="Coati">
Coati

Coatis, genera "Nasua" and "Nasuella", also known as the coatimundi , Mexican "tejón, " "cholugo, " or "moncún", Guatemalan and Costa Rican "pizote", "hog-nosed coon", Colombian "cusumbo", and other names, are members of the raccoon family (Procyonidae). They are diurnal mammals native to South America, Central America, and south-western North America. The term is reported to be derived from the Tupi language (Brazil).
Physical characteristics.
Adult coatis measure from head to the base of the tail, which can be as long as their bodies. Coatis are about tall at the shoulder, and weigh between , about the size of a large house cat. Males can become almost twice as large as females and have large, sharp canine teeth. The above measurements are for the white-nosed and South America coatis. The two mountain coatis are smaller.
All coatis share a slender head with an elongated, flexible, slightly upward-turned nose, small ears, dark feet, and a long, non-prehensile tail used for balance and signaling.
Ring-tailed coatis have either a light brown or black coat, with a lighter under-part and a white-ringed tail in most cases. Coatis have a long brown tail with rings on it which are anywhere from starkly defined like a raccoon's to very faint. Like raccoons and unlike ring-tailed cats and cacomistles, the rings go completely around the tail. Coatis often hold the tail erect; it is used as such to keep troops of coatis together in tall vegetation. The tip of the tail can be moved slightly on its own, as is the case with cats, but it is not prehensile as is that of the kinkajou, another procyonid.
Coatis have bear- and raccoon-like paws, and coatis, raccoons, and bears walk plantigrade (on the soles of the feet, as do humans). Coatis have nonretractable claws. Coatis also are, in common with raccoons and other procyonids (and others in the order Carnivora and rare cases amongst other mammals), double-jointed and their ankles can rotate beyond 180°; they are therefore able to descend trees head first. Other animals living in forests have acquired some or all of these properties through convergent evolution, including members of the mongoose, civet, weasel, cat, and bear families. Some of these animals walk on the toes of the front paws and soles of the back paws.
The coati snout is long and somewhat pig-like (see Suidae) – part of the reason for its nickname 'the hog-nosed raccoon'. It is also extremely flexible – it can be rotated up to 60° in any direction. They use their noses to push objects and rub parts of their body. The facial markings include white markings around the eyes and on the ears and snout.
Coatis have strong limbs to climb and dig, and have a reputation for intelligence, like their fellow procyonid, the raccoon. They prefer to sleep or rest in elevated places and niches, like the rainforest canopy, in crudely built sleeping nests. Coatis are active day and night.
Habitat and range.
Overall, coatis are widespread, occupying habitats ranging from hot and arid areas to humid Amazonian rainforests or even cold Andean mountain slopes, including grasslands and bushy areas. Their geographical range extends from the southwestern U.S. (southern Arizona, New Mexico, and Texas) through northern Argentina. Around 10 coatis are thought to have formed a breeding population in Cumbria, UK.
Taxonomy.
The following species are recognized:
The Cozumel Island coati was formerly recognized as a species, but the vast majority of recent authorities treat it as a subspecies, "N. narica nelsoni", of the white-nosed coati.
Genetic evidence (cytochrome b sequences) has suggested that the genus "Nasuella" should be merged into "Nasua", as the latter is otherwise paraphyletic. Other genetic studies have shown that the closest relatives of the coatis are the olingos (genus "Bassaricyon"); the two lineages are thought to have diverged about 10.2 million years ago.
Lifespan.
In the wild, coatis live for about seven to eight years, while in captivity they can live for up to 15 years.
Feeding habits.
Coatis are omnivores; their diet consists mainly of ground litter invertebrates, such as tarantula, and fruit (Alves-Costa et al., 2004, 2007; Hirsch 2007). They also eat small vertebrate prey, such as lizards, rodents, small birds, birds' eggs, and crocodile eggs. The snout, with a formidable sense of smell, assists the skilled paws in a hog-like manner to unearth invertebrates.
Behavior.
Little is known about the behavior of the mountain coatis, and the following is almost entirely about the coatis of the genus "Nasua". Unlike most members of the raccoon family (Procyonidae), coatis are primarily diurnal. Nasua coati females and young males up to two years of age are gregarious and travel through their territories in noisy, loosely organized bands made up of four to 25 individuals, foraging with their offspring on the ground or in the forest canopy. Males over two years become solitary due to behavioural disposition and collective aggression from the females, and will join the female groups only during the breeding season.
When provoked, or for defense, coatis can be fierce fighters; their strong jaws, sharp canine teeth, and fast scratching paws, along with a tough hide sturdily attached to the underlying muscles, make it very difficult for potential predators (e.g., dogs or jaguars) to seize the smaller mammal.
Coatis communicate their intentions or moods with chirping, snorting, or grunting sounds. Different chirping sounds are used to express joy during social grooming, appeasement after fights, or to convey irritation or anger. Snorting while digging, along with an erect tail, states territorial or food claims during foraging. Coatis additionally use special postures or moves to convey simple messages; for example, hiding the nose between the front paws as a sign for submission; lowering the head, baring teeth, and jumping at an enemy signal an aggressive disposition. Individuals recognize other coatis by their looks, voices, and smells, the individual smell is intensified by special musk-glands on their necks and bellies.
Coatis from Panama are known to rub their own fur and that of other troop members with resin from "Trattinnickia aspera" trees, but its purpose is unclear. Some proposed possibilities are it serves as an insect repellent, a fungicide, or as a form of scent-marking.
Reproduction.
Coati breeding season mainly corresponds with the start of the rainy season to coincide with maximum availability of food, especially fruits: between January and March in some areas, and between October and February in others. During the breeding season, an adult male is accepted into the band of females and juveniles near the beginning of the breeding season, leading to a polygynous mating system.
The pregnant females separate from the group, build a nest on a tree or in a rocky niche and, after a gestation period of about 11 weeks, give birth to litters of three to seven kits. About six weeks after birth, the females and their young will rejoin the band. Females become sexually mature at two years of age, while males will acquire sexual maturity at three years of age.
Natural predators.
Coati predators include jaguarundis, boa constrictors, foxes, dogs, tayras, ocelots, and jaguars. Large raptors, such as ornate hawk-eagles, black-and-chestnut eagles, and harpy eagles, also are known to hunt them. White-headed capuchin monkeys hunt their pups.
Status.
Coatis face unregulated hunting and the serious threat of environmental destruction in Central and South America. The absence of scientifically sound population studies of "Nasua" or "Nasuella" in the wild is probably leading to a severe underestimation of the ecological problems and decline in numbers affecting the species.
Coatis in captivity.
Coatis are one of five groups of procyonids commonly kept as pets in various parts of North, Central and South America, the others being the raccoons (common and crab-eating), the kinkajou, the ring-tailed cat and cacomistle. However, while both the white-nosed and South America coatis are common in captivity, mountain coatis are extremely rare in captivity.
Coatis are small creatures that can be wild, somewhat difficult to control or train in some cases, and generally behave in a manner radically different from that of a pet dog. Optimally, they should have a spacious outdoor enclosure and a coati-proofed room in the house and/or other climate-controlled place, as well. They can be given the run of the house but need careful watching, more careful in some cases than others.
It is possible to litter or toilet train coatis; if one cannot be trained as such, it is still possible to lessen problems in that they tend to designate a latrine area, which can have a litter pan placed in it as is done with many ferrets, pet skunks, rabbits, and rodents.
Coatis generally need both dog and cat vaccines for distemper and many other diseases and a killed rabies vaccine. They can be spayed or neutered for the same reason as cats and dogs and other pets.

</doc>
<doc id="66278" url="https://en.wikipedia.org/wiki?curid=66278" title="Olaf Tryggvason">
Olaf Tryggvason

Olaf Tryggvason (960s – 1000) was King of Norway from 995 to 1000. He was the son of Tryggvi Olafsson, king of Viken (Vingulmark, and Rånrike), and, according to later sagas, the great-grandson of Harald Fairhair, first King of Norway.
Olaf played an important part in the conversion of the Norse to Christianity, often by forcible means. He is said to have built the first church in Norway (in 995) and to have founded the city of Trondheim (in 997). A statue of Olaf Tryggvason is located in the city's central plaza.
Historical information about Olaf is sparse. He is mentioned in some contemporary English sources, and some skaldic poems. The oldest narrative source mentioning him briefly is Adam of Bremen's "Gesta Hammaburgensis ecclesiae pontificum" (c. 1070). In the 1190s, two sagas of Olaf Tryggvason were written in Iceland, by Oddr Snorrason and Gunnlaugr Leifsson. Snorri Sturluson gives an extensive account of Olaf in the "Heimskringla" saga, (c. 1230), using Oddr Snorrason's saga as his main source. The accuracy of these late sources is not taken at face value by modern historians and their validity is a topic of some debate. The following account is mainly based on the late saga sources.
Birth and early life.
There is uncertainty about both the date and the place of Olaf's birth. The earliest Norwegian written source, the late 12th century "Historia Norwegiæ", states that Olaf was born in the Orkney Islands after his mother fled there to escape the killers of Olaf's father. Another late 12th-century source, "Ágrip af Nóregskonungasögum", says Olaf's mother fled to Orkney with Olaf when he was three years old, for the same reason. All the sagas agree that Olaf eventually ended up in Kievan Rus', at the court of King Valdemar.
The version in "Heimskringla" is the most elaborate, but also the latest, and introduces elements to the story that are not found in earlier sources. It states that Olaf was born shortly after the murder of his father in 963, while other sources suggest a date between 964 and 969. The later dates cast doubt over Olaf's claim to be of Harald Fairhair's kin, and the legitimacy of his claim to the throne. Snorri Sturluson claims in "Olaf Tryggvson's saga" that Olaf was born on an islet in Fjærlandsvatnet, where his mother Astrid Eiriksdottir, daughter of Eirik Bjodaskalle, was hiding from her husband's killers, led by Harald Greycloak, the son of Eirik Bloodaxe. Greycloak and his brothers had seized the throne from Haakon the Good. Astrid fled to her father's home in Oppland, then went on to Sweden where she thought she and Olaf would be safe. Harald sent emissaries to the king of Sweden, and asked for permission to take the boy back to Norway, where he would be raised by Greycloak's mother Gunhild. The Swedish king gave them men to help them claim the young boy, but to no avail. After a short scuffle Astrid (with her son) fled again. This time their destination was Gardarike (Kiev), where Astrid's brother Sigurd was in the service of King Valdemar (Vladimir I). Olaf was three years old when they set sail on a merchant ship for Novgorod. The journey was not successful: in the Baltic Sea they were captured by Estonian vikings, and the people aboard were either killed or taken as slaves. Olaf became the possession of a man named Klerkon, together with his foster father Thorolf and his son Thorgils. Klerkon considered Thorolf too old to be useful as a slave and killed him, and then sold the two boys to a man named Klerk for a ram. Olaf was then sold to a man called Reas for a fine cloak. Six years later, Sigurd Eirikson traveled to Estonia to collect taxes for King Valdemar. He saw a boy who did not appear to be a native. He asked the boy about his family, and the boy told him he was Olaf, son of Tryggve Olafson and Astrid Eiriksdattir. Sigurd then went to Reas and bought Olaf and Thorgils out from slavery, and took the boys with him to Novgorod to live under the protection of Valdemar.
Still according to "Heimskringla", one day in the Novgorod marketplace Olaf encountered Klerkon, his enslaver and the murderer of his foster father. Olaf killed Klerkon with an axe blow to the head. A mob followed the young boy as he fled to his protector Queen Allogia, with the intent of killing him for his misdeed. Only after Allogia had paid blood money for Olaf did the mob calm down. As Olaf grew older, Vladimir made him chief over his men-at-arms, but after a couple years the king became wary of Olaf and his popularity with his soldiers. Fearing he might be a threat to the safety of his reign, Vladimir stopped treating Olaf as a friend. Olaf decided that it was better for him to seek his fortune elsewhere, and set out for the Baltic.
Raiding.
"Heimskringla" states that after leaving Novgorod, Olaf raided settlements and ports with success. In 982 he was caught in a storm and made port in Wendland, where he met Queen Geira, a daughter of King Burizleif. She ruled the part of Wendland in which Olaf had landed, and Olaf and his men were given an offer to stay for the winter. Olaf accepted and after courting the Queen, they were married. Olaf began to reclaim the baronies which while under Geira's rule had refused to pay taxes. After these successful campaigns, he began raiding again both in Skåne and Gotland.
Alliance with Otto II.
Holy Roman Emperor Otto II had assembled a great army of Saxons, Franks, Frisians and Wends to fight against the Norse pagan Danes. Olaf was part of this army as his father-in-law was king of Wendland. Otto's army met the armies of King Harald Bluetooth and Haakon Jarl the ruler of Norway under the Danish king, at Danevirke, a great wall near Schleswig. Otto's army was unable to break the fortification, so he changed tactics and sailed around it, landing in Jutland with a large fleet. Otto won a large battle there, and forced Harald and Haakon with their armies to convert to Christianity. Otto's army then returned to their homelands. Harald would hold on to his new faith, but Haakon continued worshiping the old gods when he got home.
Death of Geira and conversion.
After Olaf had spent three years in Wendland, his wife Geira died. He felt so much sorrow from her death that he could no longer bear to stay in Wendland, and set out to plunder in 984. He raided from Friesland to the Hebrides. After four years he landed on one of the Scilly Isles. He heard of a seer who lived there. Desiring to test the seer, he sent one of his men to pose as Olaf. But the seer was not fooled. So Olaf went to see the hermit, now convinced he was a real fortune teller. And the seer told him:
Thou wilt become a renowned king, and do celebrated deeds. Many men wilt thou bring to faith and baptism, and both to thy own and others' good; and that thou mayst have no doubt of the truth of this answer, listen to these tokens. When thou comest to thy ships many of thy people will conspire against thee, and then a battle will follow in which many of thy men will fall, and thou wilt be wounded almost to death, and carried upon a shield to thy ship; yet after seven days thou shalt be well of thy wounds, and immediately thou shalt let thyself be baptized.
After the meeting Olaf was attacked by a group of mutineers, and what the seer had foretold happened. So Olaf let himself be baptised.
David Hugh Farmer, in the Oxford Dictionary of Saints, writes 'it is tempting' to identify the seer with Saint Lide who lived on the island of St Helen's in the Isles of Scilly.
By another account, he was baptised by St. Ælfheah of Canterbury near to Andover, Hampshire in 994. However, Henrietta Leyser, the author of Ælfheah's Oxford Nation Dictionary of Biography entry, states that Olaf was already baptised and that the 994 event at Andover was a confirmation of his faith, part of a Danegeld treaty in which he agreed to no longer raid in England.
Marriage to Gyda.
In 988, Olaf sailed to England, because a "thing" had been called by Queen Gyda, sister of Olaf Cuaran, King of Dublin. Gyda was the widow of an earl, and was searching for a new husband. A great many men had come, but Gyda singled out Olaf, though he was wearing his bad weather clothes, and the other men wore their finest clothing. They were to be married, but another man, Alfvine, took objection, and challenged Olaf and his men to the Scandinavian duel or holmgang. Olaf and his men fought Alfvine's crew and won every battle, but did not kill any of them, instead they bound them. Alfvine was told to leave the country and never come back again. Gyda and Olaf married, and spent half their time in England and the other half in Ireland.
Ascent to the throne.
In 995, rumours began to surface in Norway about a king in Ireland of Norwegian blood. This caught the ear of Jarl Haakon, who sent Thorer Klakka to Ireland, posing as a merchant, to see if he was the son of Tryggve Olafson. Haakon told Thorer that if it were him, to lure him to Norway, so Haakon could have him under his power. Thorer befriended Olaf and told him of the situation in Norway, that Haakon Jarl had become unpopular with the populace, because he often took daughters of the elite as concubines, which was his right as ruler. He quickly grew tired of them and sent them home after a week or two. He had also been weakened by his fighting with the Danish king, due to his rejection of the Christian faith.
Olaf seized this opportunity, and set sail for Norway. When he arrived many men had already started a revolt against Haakon, who was forced to hide in a hole dug in a pigsty, together with one of his slaves Kark. When Olaf met the rebels they accepted him as their king, and together they started to search for Haakon. They eventually came to the farm where Haakon and Kark were hiding, but could not find them. Olaf held a meeting just outside the swine-sty and promised a great reward for the man who killed the Jarl. The two men in the hole heard this speech, and Haakon became distrustful of Kark, fearing he would kill him to claim the price. He could not leave the sty, nor could he keep awake forever, and when he fell asleep Kark took out a knife and cut Haakon's head off. The next day the slave went to meet Olaf and presented with the head of Haakon. The king did not reward him, and instead beheaded the slave.
After his confirmation as King of Norway, Olaf traveled to the parts of Norway that had not been under the rule of Haakon, but that of the King of Denmark; they too swore allegiance to him. He then demanded that they all be baptised, and most reluctantly they agreed.
Rule as king.
In 997 Olaf founded his seat of government in Trondheim, where he had first held a "thing" with the revolters against Haakon. It was a good site because the River Nid twisted itself before going into the fjord, creating a peninsula that could be easily defended against land attacks by just one short wall.
Olaf continued to promote Christianity throughout his rule. He baptized America's discoverer Leif Ericson, and Leif took a priest with him back to Greenland to convert the rest of his kin. Olaf also converted the people and Earl of the Orkney Islands to Christianity. At that time, the Orkney Islands were part of Norway.
It has been suggested that Olaf's ambition was to rule a united Christian Scandinavia, and it is known that he made overtures of marriage to Sigrid the Haughty, queen of Sweden, but negotiations fell through due to her steadfast pagan faith. Instead he made an enemy of her, and did not hesitate to involve himself in a quarrel with King Sweyn I of Denmark by marrying Sweyn's sister Thyre, who had fled from her heathen husband Burislav, semi-legendary "king of Wends", in defiance of her brother's authority.
Both his Wendish and his Irish wife had brought Olaf wealth and good fortune, but, according to the sagas, they were his undoing, for it was on an expedition undertaken in the year 1000 to wrest her lands from Burislav that he was waylaid off the island Svolder, by the combined Swedish, Danish, and Wendish fleets, together with the ships of Earl Haakon's sons. The Battle of Svolder ended in the death of the Norwegian king. Olaf fought to the last on his great vessel "Ormrinn Langi" ("Long Serpent"), the mightiest ship in the North, and finally leapt overboard and was seen no more.
In the early 11th century a Viking chieftain named Tryggvi invaded Norway, claiming to be the son of Olaf and Gyda. His invasion was defeated by forces loyal to Cnut the Great's son Svein of Norway.
An account preserved in "Morkinskinna" relates that Tryggvi was actually killed by a farmer after the battle. Many years later, when Harald Hardrada was king of Norway, he passed by the site of the battle. The king met an old friend of his who pointed out the alleged assassin. After questioning the purported killer and hearing him confess, King Harald had the man hanged, citing the familial bond between him and Tryggvi and his duty to avenge the latter's death.
Rumours of survival.
For some time after the Battle of Svolder, there were rumors that Olaf had survived his leap into the sea and had made his way to safety. Accounts reported by Oddr Snorrason included sightings of Olaf in Rome, Jerusalem, and elsewhere in Europe and the Mediterranean. Both King Ethelred the Unready and Olaf's sister Astrid allegedly received gifts from Olaf long after he was presumed dead. The latest sighting reported by Oddr took place in 1046.
Forcible conversions.
Olaf routinely used force to compel conversions to Christianity, including executions and torture of those who refused. Several instances of Olaf's attempts led to days of remembrance amongst modern heathens similar to the feast days of martyred Christian saints. Raud the Strong (remembered 9 January) refused to convert and, after a failed attempt using a wooden pin to pry open his mouth to insert a snake, was killed by a snake goaded by a hot poker through a drinking horn into Raud's mouth and down his throat. Eyvind Kinnrifi (9 February) likewise refused and was killed by a brazier of hot coals resting on his belly. The possibly apocryphal figure, Sigrid the Haughty (9 November), was said to have refused to marry Olaf if it meant forgoing her forefathers' religion, upon which Olaf slapped her with his glove, an act that prompted her to unite his enemies against him some years later.

</doc>
<doc id="66279" url="https://en.wikipedia.org/wiki?curid=66279" title="Magnus the Good">
Magnus the Good

Magnus Olafsson (Old Norse: "Magnús Óláfsson", Norwegian and Danish: "Magnus Olavsson"; c. 1024 – 25 October 1047), better known as Magnus the Good (Old Norse: "Magnús góði", Norwegian and Danish: "Magnus den gode"), was the King of Norway from 1035 and King of Denmark from 1042, ruling over both countries until his death in 1047.
He was an illegitimate son of Olaf II of Norway, but fled with his mother when his father was dethroned in 1028. He returned to Norway in 1035 and was crowned king at the age of 11. In 1042, he was also crowned king of Denmark. Magnus ruled the two countries until 1047, when he died under unclear circumstances. After his death, his kingdom was split between Harald Hardrada in Norway and Sweyn Estridsson in Denmark.
Early life.
Magnus was an illegitimate son of King Olaf Haraldsson (later St. Olaf), by his English concubine Alfhild, originally a slave (thrall) of Olaf's queen Astrid Olofsdotter. Born prematurely, the child was weak and unable to breathe for the first few minutes, and he was probably not expected to survive. Olaf was not present at the child's birth, and his Icelandic skald Sigvatr Þórðarson became his godfather. In a hasty baptism, Sigvatr named Magnus after the greatest king he knew of, also Olaf's greatest role model, "Karla Magnus", or Charlemagne. Against the odds, Magnus went on to grow strong and healthy, and he became of vital importance to Olaf as his only son.
Olaf was dethroned by the Danish king Cnut the Great in 1028, and he went into exile with his family and court, including the young Magnus. They travelled over the mountains and through Eidskog during the winter, entered Värmland, and were given shelter by a chieftain called Sigtrygg in Närke. After a few months, they departed Närke, and by March went eastwards towards Sigtuna, where the Swedish king Anund Jacob had left them a ship. The party thereafter sailed through the Baltic Sea and into the Gulf of Finland, eventually landing in Kievan Rus' ("Garðaríki"). They made their first stop at Staraya Ladoga ("Aldeigjuborg") to organise the further journey. From there they travelled southwards to Novgorod ("Holmgard"), where Olaf sought assistance from Grand Prince Yaroslav the Wise. Yaroslav, however, did not want to become directly involved in the Scandinavian power-struggles, and declined to help. After some time, in early 1030, Olaf learned that the Earl of Lade Håkon Eiriksson, Cnut's regent in Norway, had disappeared at sea, and gathered his men to make a swift return to Norway. Magnus was left to be fostered by Yaroslav and his wife Ingegerd.
In early 1031, a party including Magnus's uncle Harald Sigurdsson (later also to be king and then known as Harald Hardrada) arrived to report the news of his father's death at the Battle of Stiklestad. For the next few years, Magnus was educated in Old Russian and some Greek and was trained as a warrior. After Cnut's death in 1035, the Norwegian noblemen did not want to be under the oppressive rule of his son Svein and his mother Ælfgifu (known as "Álfífa" in Norway) any longer. Einar Thambarskelfir and Kalf Arnesson, who had both sought to be appointed regents under Cnut after Olaf's death in 1030 (Cnut instead appointed Svein and Ælfgifu), went together to Kievan Rus' to bring the boy back to rule as the King of Norway. After receiving the approval of Ingegerd, they returned with Magnus to Sigtuna in early 1035, and received backing from the Swedish king, brother of Magnus's stepmother Astrid. Astrid immediately became an important supporter of Magnus, and an army was gathered in Sweden, headed by Einar and Kalf, to place Magnus on the Norwegian throne.
King of Norway and Denmark.
Magnus was proclaimed king in 1035, at 11 years of age, and Svein and his mother fled; Svein died shortly after. At first Magnus sought revenge against his father's enemies, but on Sigvatr's advice he stopped doing so, which is why he became known as "good" or "noble".
Another son of Cnut, Harthacnut, was on the throne of Denmark and wanted his country to reunite with Norway, while Magnus initiated a campaign against Denmark around 1040. However, the noblemen of both countries brought the two kings together at the Göta River, the border between their kingdoms. They made peace and agreed that the first of them to die would be succeeded by the other. In 1042 Harthacnut died while in England, and Magnus also became King of Denmark, in spite of a claim by Cnut's nephew Sweyn Estridsen, whom Harthacnut had left in control of Denmark when he went to England, and who had some support.
As part of consolidating his control, Magnus destroyed the Jomsborg, headquarters of the Jomsvikings. Sweyn fled east and returned as one of the leaders of an invasion by the Wends in 1043, which Magnus decisively defeated at the Battle of Lyrskov Heath, near Hedeby. In the battle, Magnus wielded Saint Olaf's battle-axe, named Hel after the goddess of death. He had dreamt of his father the night before, and the Norwegians swore that before the battle they could hear the bell that Saint Olaf had given to the Church of St. Clement in Kaupang, in Nidaros - a sign that the saint was watching over his son and the army. It was the greatest victory ever over the Wends, with up to 15,000 killed.
Sweyn continued to oppose Magnus in Denmark, although according to "Heimskringla", they reached a settlement by which Sweyn became Earl of Denmark under Magnus.
Magnus wanted to reunite Cnut the Great's entire North Sea Empire by also becoming king of England. When Harthacnut died, the English nobles had chosen as their king Æthelred the Unready's son Edward (later known as Edward the Confessor); Magnus wrote to him that he intended to attack England with combined Norwegian and Danish forces and "he will then govern it who wins the victory." The English were mostly hostile to Magnus; Sweyn was made welcome there, although Edward's mother, Emma, curiously favored Magnus and in 1043 the king confiscated her property, with which by one report she had promised to assist Magnus.
Meanwhile, Magnus' uncle Harald Sigurdsson had returned to Norway from the east and contested his rule there, while Sweyn was still a threat in Denmark; Harald allied himself with Sweyn. Magnus chose to appease Harald, and made him his co-king in Norway in 1046.
Death.
Sweyn increased the pressure on Magnus from his base in Scania, but by late 1046, Magnus had driven Sweyn out of Denmark. However, on October 25, 1047 he died suddenly while in Denmark, either in Zealand or in Jutland, either in an accident or of a disease; accounts vary. Reports include falling overboard from one of the ships he was mustering to invade England and drowning, falling off a horse, and falling ill while on board a ship. He is said to have made Sweyn his heir in Denmark, and Harald in Norway; some say in a deathbed statement. Magnus was buried with his father in the cathedral at Nidaros, modern Trondheim.
Physical appearance.
Snorri describes Magnus as:of middle height, with regular features and light complexion. He had light blond hair, was well-spoken and quick to make up his mind, was of noble character, most generous, a great warrior, and most valorous."
Descendants.
The line of Olaf II ended with Magnus' death. However, in 1280 Eric II of Norway, who was descended through his mother from Magnus' legitimate sister, Wulfhild, was crowned king of Norway.
Magnus was not married, but had a daughter out of wedlock, Ragnhild, who married Haakon Ivarsson, a Norwegian nobleman. Her great-grandson would become King Eric III of Denmark.
Ancestry.
Magnus' ancestry according to the sagas, although scholars now doubt that he was directly descended from Harald Fairhair. 

</doc>
<doc id="66283" url="https://en.wikipedia.org/wiki?curid=66283" title="Charles Ives">
Charles Ives

Charles Edward Ives (; October 20, 1874May 19, 1954) was an American modernist composer. He is one of the first American composers of international renown, though his music was largely ignored during his life, and many of his works went unperformed for many years. Over time, he came to be regarded as an "American original". He combined the American popular and church-music traditions of his youth with European art music, and was among the first composers to engage in a systematic program of experimental music, with musical techniques including polytonality, polyrhythm, tone clusters, aleatoric elements, and quarter tones, foreshadowing many musical innovations of the 20th century.
Sources of Ives' tonal imagery are hymn tunes and traditional songs, the town band at holiday parade, the fiddlers at Saturday night dances, patriotic songs, sentimental parlor ballads, and the melodies of Stephen Foster.
Biography.
Ives was born in Danbury, Connecticut in 1874, the son of George Ives, a U.S. Army bandleader in the American Civil War, and his wife, Mary Parmelee. A strong influence of his may have been sitting in the Danbury town square, listening to George's marching band and other bands on other sides of the square simultaneously. George's unique music lessons were also a strong influence on him; George took an open-minded approach to musical theory, encouraging him to experiment in bitonal and polytonal harmonizations. It was from him that Ives also learned the music of Stephen Foster. He became a church organist at the age of 14 and wrote various hymns and songs for church services, including his "Variations on "America"", which he wrote for a Fourth of July concert in Brewster, New York. It is considered challenging even by modern concert organists, but he famously spoke of it as being "as much fun as playing baseball", a commentary on his own organ technique at that age.
Ives moved to New Haven, Connecticut in 1893, enrolling in the Hopkins School, where he captained the baseball team. In September 1894, Ives entered Yale University, studying under Horatio Parker. Here he composed in a choral style similar to his mentor, writing church music and even an 1896 campaign song for William McKinley. On November 4, 1894, his father died, a crushing blow to him, but to a large degree he continued the musical experimentation he had begun with him.
At Yale, Ives was a prominent figure; he was a member of HeBoule, Delta Kappa Epsilon (Phi chapter) and Wolf's Head Society, and sat as chairman of the Ivy Committee. He enjoyed sports at Yale and played on the varsity football team. Michael C. Murphy, his coach, once remarked that it was a "crying shame" that he spent so much time at music as otherwise he could have been a champion sprinter. His works "Calcium Light Night" and "Yale-Princeton Football Game" show the influence of college and sports on Ives' composition. He wrote his Symphony No. 1 as his senior thesis under Parker's supervision.
Ives continued his work as a church organist until May 1902. Soon after he graduated from Yale, he started work in the actuarial department of the Mutual Life Insurance company of New York. In 1899, Ives moved to employment with the insurance agency Charles H. Raymond & Co., where he stayed until 1906. In 1907, upon the failure of Raymond & Co., he and his friend Julian Myrick formed their own insurance agency Ives & Co., which later became Ives & Myrick, where he remained until he retired. During his career as an insurance executive and actuary, Ives devised creative ways to structure life-insurance packages for people of means, which laid the foundation of the modern practice of estate planning. His "Life Insurance with Relation to Inheritance Tax", published in 1918, was well received. As a result of this he achieved considerable fame in the insurance industry of his time, with many of his business peers surprised to learn that he was also a composer. In his spare time he composed music and, until his marriage, worked as an organist in Danbury and New Haven as well as Bloomfield, New Jersey and New York City.
In 1907, Ives suffered the first of several "heart attacks" (as he and his family called them) that he had throughout his lifetime. These attacks may have been psychological in origin rather than physical. Following his recovery from the 1907 attack, Ives entered into one of the most creative periods of his life as a composer.
After marrying Harmony Twitchell in 1908, they moved into their own apartment in New York. He had a remarkably successful career in insurance, and continued to be a prolific composer until he suffered another of several heart attacks in 1918, after which he composed very little, writing his very last piece, the song "Sunrise", in August 1926. In 1922, Ives published his "114 Songs", which represents the breadth of his work as a composer—it includes art songs, songs he wrote as a teenager and young man, and highly dissonant songs such as "The Majority."
According to his wife, one day in early 1927 Ives came downstairs with tears in his eyes. He could compose no more, he said, "nothing sounds right." There have been numerous theories advanced to explain the silence of his late years, which seems as mysterious as the last several decades of the life of Jean Sibelius, who also stopped composing at almost the same time. While he had stopped composing, and was increasingly plagued by health problems, he did continue to revise and refine his earlier work, as well as oversee premieres of his music. After continuing health problems, including diabetes, in 1930 he retired from his insurance business, which gave him more time to devote to his musical work, but he was unable to write any new music. During the 1940s he revised his "Concord Sonata", publishing it in 1947 (an earlier version of the sonata and the accompanying prose volume, "Essays Before a Sonata" were privately printed in 1920).
Ives died of a stroke in 1954 in New York City. His widow, who died in 1969 at age 92, bequeathed the royalties from his music to the American Academy of Arts and Letters for the Charles Ives Prize.
Ives' career and dedication towards music was from the time when he started playing drums in his father’s band at a young age. Ives published a large collection of songs, many of which had piano parts. He composed two string quartets and other works of chamber music, though he is now best known for his orchestral music. His work as an organist led him to write "Variations on "America"" in 1891, which he premiered at a recital celebrating the Fourth of July.
In 1906, Ives composed the first radical musical work of the twentieth century, "Central Park in the Dark". He composed two symphonies — "The Unanswered Question" (1908), written for the unusual combination of trumpet, four flutes, and string quartet. "The Unanswered Question" was influenced by the New England writers Ralph Waldo Emerson and Henry David Thoreau.
Around 1910, Ives began composing his most accomplished works including the "Holiday Symphony" and "Three Places in New England". "The Piano Sonata No. 2, Concord, Mass.", known as the "Concord Sonata" was one of his most remarkable pieces. He started work on this in 1911 and completed most of it in 1915. However, it wasn’t until 1920 that the piece was published and the revised version appeared only in 1947. This piece contains one of the most striking examples of his experimentalism. In the second movement, he instructed the pianist to use a 14¾ in (37.5 cm) piece of wood to create a massive cluster chord.
Another remarkable piece of orchestral music Ives completed was his "Fourth Symphony". He worked on this from 1910 to 1916. This symphony is notable for its complexity and over sized orchestra. It has four movements and a complete performance of it was not given until 1965, i.e. half a century after it was completed.
Ives left behind material for an unfinished "Universe Symphony", which he was unable to assemble in his lifetime despite two decades of work. This was due to his health problems as well as his shifting idea of the work.
Reception.
Ives' music was largely ignored during his lifetime, particularly during the years in which he actively composed. Many of his published works went unperformed even many years after his death in 1954. However, his reputation in more recent years has greatly increased. Juilliard commemorated the 50th anniversary of his death by performing his music over six days in 2004. His musical experiments, including his increasing use of dissonance, were not well received by his contemporaries. Furthermore, the difficulties in performing the rhythmic complexities in his major orchestral works made them daunting challenges even decades after they were composed.
Early supporters of Ives' music included Henry Cowell, Elliott Carter and Aaron Copland. Cowell's periodical "New Music" published a substantial number of Ives' scores (with his approval), but for almost 40 years he had few performances that he did not arrange or back, generally with Nicolas Slonimsky as the conductor. After seeing a copy of his self-published "114 Songs" during the 1930s, Copland published a newspaper article praising the collection.
Ives began to acquire some public recognition during the 1930s, with performances of a chamber orchestra version of his "Three Places in New England" both in the U.S. and on tour in Europe by conductor Nicolas Slonimsky and the New York Town Hall premiere of his "Concord Sonata" by pianist John Kirkpatrick in 1939, which led to favorable commentary in the major New York newspapers. Later, around the time of Ives' death in 1954, Kirkpatrick teamed with soprano Helen Boatwright for the first extended recorded recital of Ives' songs for the obscure Overtone label (Overtone Records catalog number 7). They recorded a new selection of songs for the Ives Centennial Collection that Columbia Records published in 1974.
Ives' obscurity lifted a bit in the 1940s, when he met Lou Harrison, a fan of his music who began to edit and promote it. Most notably, Harrison conducted the premiere of the Symphony No. 3, "The Camp Meeting" (1904) in 1946. The next year, it won him the Pulitzer Prize for Music. He gave the prize money away (half of it to Harrison), saying "prizes are for boys, and I'm all grown up". He himself was a great financial supporter of twentieth century music, often supporting works that were written by other composers. This he did in secret, telling his beneficiaries it was really his wife who wanted him to do so. Nicolas Slonimsky said in 1971, "He financed my entire career."
At this time, Ives was also promoted by Bernard Herrmann, who worked as a conductor at CBS and in 1940 became principal conductor of the CBS Symphony Orchestra. While there, he championed his music. When they met, Herrmann confessed that he had tried his hand at performing the "Concord Sonata". Remarkably, Ives, who actually avoided the radio and the phonograph, agreed to make a series of piano recordings from 1933 to 1943 that were later issued by Columbia Records on a special LP set issued for his centenary in 1974. New World Records issued 42 tracks of his recordings on CD on April 1, 2006. One of the more unusual recordings, made in New York City in 1943, features him playing the piano and singing the words to his popular World War I song "They Are There!", which he composed in 1917, then revised in 1942–43 for World War II.
Also in Canada, the expatriate English pianist Lloyd Powell played a series of concerts including all of Ives' piano works, at the University of British Columbia in the 1950s.
Recognition of Ives' music steadily increased. He received praise from Arnold Schoenberg, who regarded him as a monument to artistic integrity, and from the New York School of William Schuman. Shortly after Schoenberg's death (three years before Ives died), his widow found a note written by her husband. The note had originally been written in 1944 when Schoenberg was living in Los Angeles and teaching at UCLA. It stated:
There is a report that Ives also won the admiration of Gustav Mahler, who said that he was a true musical revolutionary. Reportedly, Mahler talked of premiering Ives' Third Symphony with the New York Philharmonic, but he soon died (in 1911) thus preventing the premiere. However, the source of this story is Ives himself; since Mahler died, there was no way to verify whether he had seen the score of the symphony or decided to perform it in the 1911–12 season. Nonetheless, it is known that Ives regularly attended New York Philharmonic concerts and probably heard Mahler conduct the Philharmonic at Carnegie Hall.
In 1951, Leonard Bernstein conducted the world premiere of Ives' Second Symphony in a broadcast concert by the New York Philharmonic. The Iveses heard the performance on their cook's radio and were amazed at the audience's warm reception to the music. Bernstein continued to conduct Ives' music and made a number of recordings with the Philharmonic for Columbia Records. He even honored him on one of his televised youth concerts and in a special disc included with the reissue of the 1960 recording of the Second Symphony and the "Fourth of July" movement from Ives' .
Another pioneering Ives recording, undertaken during the 1950s, was the first complete set of the four violin sonatas, performed by Cleveland Orchestra concertmaster Rafael Druian and John Simms. Leopold Stokowski took on Symphony No. 4 in 1965, regarding the work as "the heart of the Ives problem". The Carnegie Hall world premiere by the American Symphony Orchestra led to the first recording of the music. Another promoter of his was choral conductor Gregg Smith, who made a series of recordings of his shorter works during the 1960s, including first stereo recordings of the psalm settings and arrangements of many short pieces for theater orchestra. The Juilliard String Quartet recorded the two string quartets during the 1960s.
Today, conductor Michael Tilson Thomas is an enthusiastic exponent of Ives' symphonies as is composer and biographer Jan Swafford. Ives' work is regularly programmed in Europe. He has also inspired pictorial artists, most notably Eduardo Paolozzi, who entitled one of his 1970s sets of prints "Calcium Light Night", each print being named for an Ives piece (including "Central Park in the Dark"). In 1991, Connecticut's legislature designated him as that state's official composer.
The Scottish baritone Henry Herford began a survey of Ives' songs in 1990, but this remains incomplete because the record company involved (Unicorn-Kanchana) collapsed. Pianist-composer and Wesleyan University professor Neely Bruce has made a life's study of Ives. To date, he has staged seven parts of a concert series devoted to the complete songs of Ives. Musicologist David Gray Porter reconstructed a piano concerto, the "Emerson" Concerto, from Ives' sketches. A recording of the work was released by Naxos Records.
Ives continues to be very influential on contemporary composers and arrangers as shown by the most recent Planet Arts Records release "."
Compositions.
"Note: Because Ives often made several different versions of the same piece, and because his work was generally ignored during his lifetime, it is often difficult to put exact dates on his compositions. The dates given here are sometimes best guesses. There have also been controversial speculations that he purposely misdated his own pieces earlier or later than actually written.
Politics.
Ives proposed in 1920 that there be a 20th Amendment to the U.S. Constitution which would authorize citizens to submit legislative proposals to Congress. Members of Congress would then cull the proposals, selecting 10 each year as referendums for popular vote by the nation's electorate. He even had printed at his own expense several thousand copies of a pamphlet on behalf of his proposed amendment. The pamphlet proclaimed the need to curtail "THE EFFECTS OF TOO MUCH POLITICS IN OUR representative DEMOCRACY." His proposal joined his music in being ignored during his lifetime.
It is claimed in the biographical film "A Good Dissonance Like a Man" that the first of Ives' crippling heart attacks occurred as a result of a World War I era argument with a young Franklin D. Roosevelt over his idea of issuing of war bonds in amounts as low as $50 each. Roosevelt was chairman of a war bonds committee on which Ives served, and he "scorned the idea of anything so useless as a $50 bond." Roosevelt changed his mind about small contributions as seen many years later when he endorsed the March of Dimes to combat poliomyelitis.
Notes.
Citations:
General references:

</doc>
<doc id="66284" url="https://en.wikipedia.org/wiki?curid=66284" title="Formic acid">
Formic acid

Formic acid (also called methanoic acid) is the simplest carboxylic acid. The chemical formula is HCOOH or HCO2H. It is an important intermediate in chemical synthesis and occurs naturally, most notably in some ants. The word "formic" comes from the Latin word for ant, "formica", referring to its early isolation by the distillation of ant bodies. Esters, salts, and the anions derived from formic acid are called formates.
Properties.
Formic acid is a colorless liquid having a highly pungent, penetrating odor at room temperature. It is miscible with water and most polar organic solvents, and is somewhat soluble in hydrocarbons. In hydrocarbons and in the vapor phase, it consists of hydrogen-bonded dimers rather than individual molecules. Owing to its tendency to hydrogen-bond, gaseous formic acid does not obey the ideal gas law. Solid formic acid (two polymorphs) consists of an effectively endless network of hydrogen-bonded formic acid molecules. This relatively complicated compound also forms a low-boiling azeotrope with water (22.4%) and liquid formic acid also tends to supercool.
Natural occurrence.
In nature, it is found in certain ants and in the trichomes of stinging nettle ("Urtica dioica"). Formic acid is a naturally occurring component of the atmosphere due primarily to forest emissions.
Production.
In 2009, the worldwide capacity for producing formic acid was 720,000 tonnes/annum, roughly equally divided between Europe (350,000, mainly in Germany) and Asia (370,000, mainly in China) while production was below 1000 tonnes/annum in all other continents. It is commercially available in solutions of various concentrations between 85 and 99 w/w %. , the largest producers are BASF, Eastman Chemical Company, LC Industrial, and Feicheng Acid Chemicals, with the largest production facilities in Ludwigshafen (200,000 tonnes/annum, BASF, Germany), Oulu (105,000, Eastman, Finland), Nakhon Pathom (n/a, LC Industrial) and Feicheng (100,000, Feicheng, China). 2010 prices ranged from around €650/tonne (equivalent to around $800/tonne) in Western Europe to $1250/tonne in the United States.
From methyl formate and formamide.
When methanol and carbon monoxide are combined in the presence of a strong base, the result is methyl formate, according to the chemical equation:
In industry, this reaction is performed in the liquid phase at elevated pressure. Typical reaction conditions are 80 °C and 40 atm. The most widely used base is sodium methoxide. Hydrolysis of the methyl formate produces formic acid:
Efficient hydrolysis of methyl formate requires a large excess of water. Some routes proceed indirectly by first treating the methyl formate with ammonia to give formamide, which is then hydrolyzed with sulfuric acid:
A disadvantage of this approach is the need to dispose of the ammonium sulfate byproduct. This problem has led some manufacturers to develop energy-efficient methods of separating formic acid from the excess water used in direct hydrolysis. In one of these processes (used by BASF) the formic acid is removed from the water by liquid-liquid extraction with an organic base.
Niche chemical routes.
By-product of acetic acid production.
A significant amount of formic acid is produced as a byproduct in the manufacture of other chemicals. At one time, acetic acid was produced on a large scale by oxidation of alkanes, by a process that cogenerates significant formic acid. This oxidative route to acetic acid is declining in importance, so that the aforementioned dedicated routes to formic acid have become more important.
Hydrogenation of carbon dioxide.
The catalytic hydrogenation of CO2 to formic acid has long been studied. This reaction can be conducted homogeneously.
Oxidation of biomass.
Formic acid can also be obtained by aqueous catalytic partial oxidation of wet biomass (OxFA process). A Keggin-type polyoxometalate (H5PV2Mo10O40) is used as the homogeneous catalyst to convert sugars, wood, waste paper or cyanobacteria to formic acid and CO2 as the sole byproduct. Yields of up to 53% formic acid can be achieved.
Laboratory methods.
In the laboratory, formic acid can be obtained by heating oxalic acid in glycerol and extraction by steam distillation. Glycerol acts as a catalyst, as the reaction proceeds through a glyceryl oxalate intermediary. If the reaction mixture is heated to higher temperatures, allyl alcohol results. The net reaction is thus:
The isonitrile can be obtained by reacting ethyl amine with chloroform (note that the fume hood is required because of the overpoweringly objectionable odor of the isonitrile).-->
Another illustrative method involves the reaction between lead formate and hydrogen sulfide, driven by the formation of lead sulfide.
Biosynthesis.
Formic acid occurs widely in nature as its conjugate base formate. This anion is produced by reduction of carbon dioxide, catalyzed by the enzyme formate dehydrogenase. An assay for formic acid in body fluids, designed for determination of formate after methanol poisoning, is based on the reaction of formate with bacterial formate dehydrogenase.
Uses.
A major use of formic acid is as a preservative and antibacterial agent in livestock feed. In Europe, it is applied on silage (including fresh hay) to promote the fermentation of lactic acid and to suppress the formation of butyric acid; it also allows fermentation to occur quickly, and at a lower temperature, reducing the loss of nutritional value. Formic acid arrests certain decay processes and causes the feed to retain its nutritive value longer, and so it is widely used to preserve winter feed for cattle. In the poultry industry, it is sometimes added to feed to kill "E. coli" bacteria. Use as preservative for silage and (other) animal feed constituted 30% of the global consumption in 2009.
Formic acid is also significantly used in the production of leather, including tanning (23% of the global consumption in 2009), and in dyeing and finishing textiles (9% of the global consumption in 2009) because of its acidic nature. Use as a coagulant in the production of rubber consumed 6% of the global production in 2009.
Formic acid is also used in place of mineral acids for various cleaning products, such as limescale remover and toilet bowl cleaner. Some formate esters are artificial flavorings or perfumes. Beekeepers use formic acid as a miticide against the tracheal mite ("Acarapis woodi") and the "Varroa" mite. Formic acid is being investigated for use in fuel cells.
In 1889 Henry Morton Stanley reported to the Royal Geographical Society of London that natives of the Congo used poisoned arrows very effectively against members of his party. The poison was prepared from powdered red ants, cooked in palm oil, and its efficacy was attributed to formic acid from the ants.
Laboratory use.
Formic acid is a source for a formyl group for example in the formylation of methylaniline to N-methylformanilide in toluene. In synthetic organic chemistry, formic acid is often used as a source of hydride ion. The Eschweiler-Clarke reaction and the Leuckart-Wallach reaction are examples of this application. It, or more commonly its azeotrope with triethylamine, is also used as a source of hydrogen in transfer hydrogenation.
Like acetic acid and trifluoroacetic acid, formic acid is commonly used as a volatile pH modifier in HPLC and capillary electrophoresis.
As mentioned below, formic acid readily decomposes with concentrated sulfuric acid to form carbon monoxide.
Medical use.
Formic acid is an effective treatment for warts.
Reactions.
Formic acid shares most of the chemical properties of other carboxylic acids. Because of its high acidity, solutions in alcohols form esters spontaneously. Formic acid shares some of the reducing properties of aldehydes, reducing solutions of gold, silver, and platinum to the metals.
Decomposition.
Heat and especially acids cause formic acid to decompose to carbon monoxide (CO) and water (dehydration). Treatment of formic acid with sulfuric acid is a convenient laboratory source of CO.
In the presence of platinum, it decomposes with a release of hydrogen and carbon dioxide. 
Soluble ruthenium catalysts are also effective. Carbon monoxide free hydrogen has been generated in a very wide pressure range (1–600 bar). Formic acid has been considered as a means of hydrogen storage. The co-product of this decomposition, carbon dioxide, can be rehydrogenated back to formic acid in a second step. Formic acid contains 53 g L−1 hydrogen at room temperature and atmospheric pressure, which is three and a half times as much as compressed hydrogen gas can attain at 350 bar pressure (14.7 g L−1). Pure formic acid is a liquid with a flash point of +69 °C, much higher than that of gasoline (–40 °C) or ethanol (+13 °C).
Addition to alkenes.
Formic acid is unique among the carboxylic acids in its ability to participate in addition reactions with alkenes. Formic acids and alkenes readily react to form formate esters. In the presence of certain acids, including sulfuric and hydrofluoric acids, however, a variant of the Koch reaction occurs instead, and formic acid adds to the alkene to produce a larger carboxylic acid.
Formic acid anhydride.
An unstable formic anhydride, H(C=O)−O−(C=O)H, can be obtained by dehydration of formic acid with N,N'-Dicyclohexylcarbodiimide in ether at low temperature.
History.
Some alchemists and naturalists were aware that ant hills give off an acidic vapor as early as the 15th century. The first person to describe the isolation of this substance (by the distillation of large numbers of ants) was the English naturalist John Ray, in 1671. Ants secrete the formic acid for attack and defense purposes. Formic acid was first synthesized from hydrocyanic acid by the French chemist Joseph Gay-Lussac. In 1855, another French chemist, Marcellin Berthelot, developed a synthesis from carbon monoxide similar to the process used today.
Formic acid was long considered a chemical compound of only minor interest in the chemical industry. In the late 1960s, however, significant quantities became available as a byproduct of acetic acid production. It now finds increasing use as a preservative and antibacterial in livestock feed.
Safety.
Formic acid has low toxicity (hence its use as a food additive), with an LD50 of 1.8 g/kg (oral, mice). The concentrated acid is corrosive to the skin.
Formic acid is readily metabolized and eliminated by the body. Nonetheless, it has specific toxic effects; the formic acid and formaldehyde produced as metabolites of methanol are responsible for the optic nerve damage, causing blindness seen in methanol poisoning. Some chronic effects of formic acid exposure have been documented. Some experiments on bacterial species have demonstrated it to be a mutagen. Chronic exposure in humans may cause kidney damage. Another possible effect of chronic exposure is development of a skin allergy that manifests upon re-exposure to the chemical.
Concentrated formic acid slowly decomposes to carbon monoxide and water, leading to pressure buildup in the containing vessel. For this reason, 98% formic acid is shipped in plastic bottles with self-venting caps.
The hazards of solutions of formic acid depend on the concentration. The following table lists the EU classification of formic acid solutions:
Formic acid in 85% concentration is not flammable, and diluted formic acid is on the U.S. Food and Drug Administration list of food additives. The principal danger from formic acid is from skin or eye contact with the concentrated liquid or vapors. The U.S. OSHA Permissible Exposure Level (PEL) of formic acid vapor in the work environment is 5 parts per million parts of air (ppm).

</doc>
<doc id="66286" url="https://en.wikipedia.org/wiki?curid=66286" title="Organic acid">
Organic acid

An organic acid is an organic compound with acidic properties. The most common organic acids are the carboxylic acids, whose acidity is associated with their carboxyl group –COOH. Sulfonic acids, containing the group –SO2OH, are relatively stronger acids. Alcohols, with –OH, can act as acids but they are usually very weak. The relative stability of the conjugate base of the acid determines its acidity. Other groups can also confer acidity, usually weakly: the thiol group –SH, the enol group, and the phenol group. In biological systems, organic compounds containing these groups are generally referred to as organic acids.
Characteristics.
In general, organic acids are weak acids and do not dissociate completely in water, whereas the strong mineral acids do. Lower molecular mass organic acids such as formic and lactic acids are miscible in water, but higher molecular mass organic acids, such as benzoic acid, are insoluble in molecular (neutral) form.
On the other hand, most organic acids are very soluble in organic solvents. "p"-Toluenesulfonic acid is a comparatively strong acid used in organic chemistry often because it is able to dissolve in the organic reaction solvent.
Exceptions to these solubility characteristics exist in the presence of other substituents that affect the polarity of the compound.
Examples.
The pKa a logarithmic measure of the acid dissociation constant, categorizes the strength of an acid; the lower or more negative 
the number, the stronger and more dissociable the acid. It should not be confused with pH, the logarithmic measure of actual hydrogen ion concentration, and thus strength of a particular acid solution.
A few examples include: (COOH is the carboxyl group)
Carbonic acid, a weak acid formed by CO2 gas dissolving in water, is considered inorganic for historical reasons as its dissociation products are carbonate and bicarbonate ions.
The 19th century name "carbolic acid" came from the original German name "karbolsäure", or "coal-oil-acid". It is a weak, non-carboxylic acid, but is corrosive and causes chemical burns on the skin due to a protein-degenerating effect.
Uric acid is a heterocyclic purine derivative which is a diprotic acid but not a carboxylic one; it loses a hydrogen ion at the location of a nitrogen atom.
One of the few natural sulfonic acids, discovered in bile.
A strong acid that, unlike some strong mineral acids, is non-oxidizing.
One of the strongest acids in existence, organic or inorganic, about a thousand times stronger than sulfuric acid; a superacid. First synthesized in 1954, it also resists oxidation/reduction reactions and does not sulfonate substrates like sulfuric or chlorosulfonic acid.
A strong phosphonic acid
Applications.
Simple organic acids like formic or acetic acids are used for oil and gas well stimulation treatments. These organic acids are much less reactive with metals than are strong mineral acids like hydrochloric acid (HCl) or mixtures of HCl and hydrofluoric acid (HF). For this reason, organic acids are used at high temperatures or when long contact times between acid and pipe are needed.
The conjugate bases of organic acids such as citrate and lactate are often used in biologically-compatible buffer solutions.
Citric and oxalic acids are used as rust removal. As acids, they can dissolve the iron oxides, but without damaging the base metal as do stronger mineral acids. In the dissociated form, they may be able to chelate the metal ions, helping to speed removal.
Biological systems create many and more complex organic acids such as L-lactic, citric, and D-glucuronic acids that contain hydroxyl or carboxyl groups. Human blood and urine contain these plus organic acid degradation products of amino acids, neurotransmitters, and intestinal bacterial action on food components. Examples of these categories are alpha-ketoisocaproic, vanilmandelic, and D-lactic acids, derived from catabolism of L-leucine and epinephrine (adrenaline) by human tissues and catabolism of dietary carbohydrate by intestinal bacteria, respectively.
Application in food.
Organic acids are used in food preservation because of their effects on bacteria. The key basic principle on the mode of action of organic acids on bacteria is that non-dissociated (non-ionized) organic acids can penetrate the bacteria cell wall and disrupt the normal physiology of certain types of bacteria that we call "pH-sensitive", meaning that they cannot tolerate a wide internal and external pH gradient. Among those bacteria are "Escherichia coli", "Salmonella" spp., "C. perfringens", "Listeria monocytogenes", and "Campylobacter" species.
Upon passive diffusion of organic acids into the bacteria, where the pH is near or above neutrality, the acids will dissociate and lower the bacteria internal pH, leading to situations that will impair or stop the growth of bacteria. Thereafter, the anionic part of the organic acids, which cannot escape the bacteria in its dissociated form, will accumulate within the bacteria and disrupt many metabolic functions, leading to osmotic pressure increase, incompatible with the survival of the bacteria.
It has been well demonstrated that the state of the organic acids (undissociated or dissociated) is extremely important in determining their capacity to inhibit the growth of bacteria.
Lactic acid and its salts sodium lactate and potassium lactate are widely used as antimicrobials in food products, in particular, meat and poultry such as ham and sausages.
Application in nutrition and animal feeds.
Organic acids have been used successfully in pig production for more than 25 years. Although less research has been done in poultry, organic acids have also been found to be effective in poultry production.
Organic acids (C1–C7) are widely distributed in nature as normal constituents of plants or animal tissues. They are also formed through microbial fermentation of carbohydrates mainly in the large intestine. They are sometimes found in their sodium, potassium, or calcium salts, or even stronger double salts.
Organic acids added to feeds should be protected to avoid their dissociation in the crop and in the intestine (high pH segments) and reach far into the gastrointestinal tract, where the bulk of the bacteria population is located.
From the use of organic acids in poultry and pigs, one can expect an improvement in performance similar to or better than that of antibiotic growth promoters, without the public health concern, a preventive effect on the intestinal problems like necrotic enteritis in chickens and "Escherichia coli" infection in young pigs. Also one can expect a reduction of the carrier state for "Salmonella" species and "Campylobacter" species.

</doc>
<doc id="66291" url="https://en.wikipedia.org/wiki?curid=66291" title="Wadden Sea">
Wadden Sea

The Wadden Sea (, , Low German: "Wattensee" or "Waddenzee", , West Frisian: "Waadsee", North Frisian: "di Heef") is an intertidal zone in the southeastern part of the North Sea. It lies between the coast of northwestern continental Europe and the range of Frisian Islands, forming a shallow body of water with tidal flats and wetlands. It is rich in biological diversity. In 2009, the Dutch and German parts of the Wadden Sea were inscribed on UNESCO's World Heritage List and the Danish part was added in June 2014.
The Wadden Sea is one of the world's seas whose coastline has been most modified by humans, via systems of dikes and causeways on the mainland and low-lying coastal islands. The Wadden Sea stretches from Den Helder in the Netherlands in the northwest, past the great river estuaries of Germany to its northern boundary at Skallingen north of Esbjerg in Denmark along a total length of some 500 km and a total area of about 10,000 km2. Within the Netherlands it is bounded from the IJsselmeer by the Afsluitdijk.
The islands in the Wadden Sea are called the Wadden Sea Islands or Frisian Islands, named after the Frisians. These are remnants of the once expansive and now submerged Doggerland. However, on the westernmost Dutch island, Texel, the Frisian language has not been spoken for centuries. The Danish Wadden Sea Islands have never been inhabited by Frisians. The outlying German island of Helgoland, although ethnically one of the Frisian Islands, is not situated in the Wadden Sea.
The German part of the Wadden Sea was the setting for the 1903 Erskine Childers novel "The Riddle of the Sands".
Nature.
Environment.
The word "wad" is Dutch for "mud flat" (Low German and , ). The area is typified by extensive tidal mud flats, deeper tidal trenches (tidal creeks) and the islands that are contained within this, a region continually contested by land and sea. The landscape has been formed for a great part by storm tides in the 10th to 14th centuries, overflowing and carrying away former peat land behind the coastal dunes. The present islands are a remnant of the former coastal dunes.
The islands are marked by dunes and wide, sandy beaches towards the North Sea and a low, tidal coast towards the Wadden Sea. The impact of waves and currents, carrying away sediments, is slowly changing the layout of the islands. For example, the islands of Vlieland and Ameland have moved eastwards through the centuries, having lost land on one side and grown on the other.
Fauna.
The Wadden Sea is famous for its rich flora and fauna, especially birds. Hundreds of thousands of waders (shorebirds), ducks, and geese use the area as a migration stopover or wintering site, and it is also a rich habitat for gulls and terns. However, the biodiversity of Wadden Sea today is only a fraction of what was seen before exploitation by humans; for birds, larger species such as geese, eagles, flamingos, pelicans, and herons used to be common as well. Some species that are regionally extinct are still available here.
According to J. B. MacKinnon, larger fish including sturgeons, rays, Atlantic salmon, brown trout, and others like lacuna snails and oyster beds that were once found elsewhere in the region have disappeared as well, as the actual size of the Wadden Sea was reduced to about 50% of the original sea, and nutrients from the river of Rhine no longer flow into the sea, resulting in about 90% of all the species which historically inhabited the Wadden Sea being at risk.
Wadden Sea is an important habitat for two species of seals, harbor and grey seals. Harbour porpoises and Atlantic white-beaked dolphins (seasonally) which once were locally extinct but have re-colonized into the area, and these two are the only resident cetaceans in present days while many other species have either disappeared or only visit seasonally or occasionally.
Nowadays, only 4 species of marine mammals above-mentioned could be counted as regular inhabitants of Wadden sea while many other species that experienced severe declines, habitat loss, and local or functional extinctions. North Atlantic right whales and gray whales were once seen in the region, using the shallow, calm waters for either feeding and breeding before they were completely wiped out by shore-based whaling. These two species are now thought to be either extinct or remnant populations of which low-tens at best survive. One whale, possibly a right whale, was observed close to beaches on Texel in the West Frisian Islands and off Steenbanken, Schouwen-Duiveland in July 2005. Recent increases in number of North Atlantic humpback whales and minke whales might have resulted in more visits and possible re-colonization by the species to the areas especially around Marsdiep. Future recovery of once-extinct local bottlenose dolphins is also expected.
Threats to the ecosystem.
There are number of invasive species including algae, plants, and smaller organisms causing negative effects on native species introduced by human activities into North Sea coasts.
Conservation.
Each of three countries has designated Ramsar sites in the region (see Wadden Sea National Parks).
Although the Wadden Sea is not yet listed as a transboundary Ramsar site, a great part of the Wadden Sea is protected in cooperation of all three countries. 
The governments of the Netherlands, Denmark and Germany have been working together since 1978 on the protection and conservation of the Wadden Sea. Co-operation covers management, monitoring and research, as well as political matters. Furthermore, in 1982, a Joint Declaration on the Protection of the Wadden Sea was agreed upon to co-ordinate activities and measures for the protection of the Wadden Sea. In 1997, a Trilateral Wadden Sea Plan was adopted.
In June 2009, the Wadden Sea (comprising the Dutch Wadden Sea Conservation Area and the German Wadden Sea National Parks of Lower Saxony and Schleswig-Holstein) was placed on the World Heritage list by UNESCO. The Danish part was added to the site in 2014.
Recreation.
Many of the islands have been popular seaside resorts since the 19th century.
Mudflat hiking (Dutch: "Wadlopen", German: "Wattwandern," Danish: "Vadehavsvandring"), i.e., walking on the sandy flats at low tide, has become popular in the Wadden Sea.
It is also a popular region for pleasure boating.

</doc>
<doc id="66294" url="https://en.wikipedia.org/wiki?curid=66294" title="Reinforcement learning">
Reinforcement learning

Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take "actions" in an "environment" so as to maximize some notion of cumulative "reward". The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called "approximate dynamic programming". The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
In machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.
Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.
Introduction.
The basic reinforcement learning model consists of:
The rules are often stochastic. The observation typically involves the scalar immediate reward associated with the last transition.
In many works, the agent is also assumed to observe the current environmental state, in which case we talk about "full observability", whereas in the opposing case we talk about "partial observability". Sometimes the set of actions available to the agent is restricted (e.g., you cannot spend more money than what you possess).
A reinforcement learning agent interacts with its environment in discrete time steps.
At each time formula_3, the agent receives an observation formula_4, which typically includes the reward formula_5. 
It then chooses an action formula_6 from the set of actions available, which is subsequently sent to the environment.
The environment moves to a new state formula_7 and the reward formula_8 associated with the "transition" formula_9 is determined.
The goal of a reinforcement learning agent is to collect as much reward as possible. The agent can choose any action as a function of the history and it can even randomize its action selection.
When the agent's performance is compared to that of an agent which acts optimally from the beginning, the difference in performance gives rise to the notion of "regret". 
Note that in order to act near optimally, the agent must reason about the long term consequences of its actions: In order to maximize my future income I had better go to school now, although the immediate monetary reward associated with this might be negative.
Thus, reinforcement learning is particularly well suited to problems which include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers (Sutton and Barto 1998, Chapter 11) and go (AlphaGo).
Two components make reinforcement learning powerful:
The use of samples to optimize performance and the use of function approximation to deal with large environments.
Thanks to these two key components, reinforcement learning can be used in large environments in any of the following situations:
The first two of these problems could be considered planning problems (since some form of the model is available), while the last one could be considered as a genuine learning problem. However, under a reinforcement learning methodology both planning problems would be converted to machine learning problems.
Exploration.
The reinforcement learning problem as described requires clever exploration mechanisms. Randomly selecting actions, without reference to an estimated probability distribution, is known to give rise to very poor performance. The case of (small) finite MDPs is relatively well understood by now.
However, due to the lack of algorithms that would provably scale well with the number of states (or scale to problems with infinite state spaces), in practice people resort to simple exploration methods. One such method is formula_10-greedy, when the agent chooses the action that it believes has the best long-term effect with probability formula_11, and it chooses an action uniformly at random, otherwise. Here, formula_12 is a tuning parameter, which is sometimes changed, either according to a fixed schedule (making the agent explore less as time goes by), or adaptively based on some heuristics (Tokic & Palm, 2011).
Algorithms for control learning.
Even if the issue of exploration is disregarded and even if the state was observable (which we assume from now on), the problem remains to find out which actions are good based on past experience.
Criterion of optimality.
For simplicity, assume for a moment that the problem studied is "episodic", an episode ending when some "terminal state" is reached. Assume further that no matter what course of actions the agent takes, termination is inevitable. Under some additional mild regularity conditions the expectation of the total reward is then well-defined, for "any" policy and any initial distribution over the states. Here, a policy refers to a mapping that assigns some probability distribution over the actions to all possible histories.
Given a fixed initial distribution formula_13, we can thus assign the expected return formula_14 to policy formula_15:
where the random variable formula_17 denotes the "return" and is defined by
where formula_8 is the reward received after the formula_3-th transition, the initial state is sampled at random from formula_13 and actions are selected by policy formula_15. Here, formula_23 denotes the (random) time when a terminal state is reached, i.e., the time when the episode terminates.
In the case of non-episodic problems the return is often "discounted",
giving rise to the total expected discounted reward criterion. Here formula_25 is the so-called "discount-factor". Since the undiscounted return is a special case of the discounted return, from now on we will assume discounting. Although this looks innocent enough, discounting is in fact problematic if one cares about online performance. This is because discounting makes the initial time steps more important. Since a learning agent is likely to make mistakes during the first few steps after its "life" starts, no uninformed learning algorithm can achieve near-optimal performance under discounting even if the class of environments is restricted to that of finite MDPs. (This does not mean though that, given enough time, a learning agent cannot figure how to act near-optimally, if time was restarted.)
The problem then is to specify an algorithm that can be used to find a policy with maximum expected return. 
From the theory of MDPs it is known that, without loss of generality, the search can be restricted to the set of the so-called "stationary" policies. A policy is called stationary if the action-distribution returned by it depends only on the last state visited (which is part of the observation history of the agent, by our simplifying assumption). In fact, the search can be further restricted to "deterministic" stationary policies. A deterministic stationary policy is one which deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.
Brute force.
The brute force approach entails the following two steps:
One problem with this is that the number of policies can be extremely large, or even infinite. Another is that variance of the returns might be large, in which case a large number of samples will be required to accurately estimate the return of each policy.
These problems can be ameliorated if we assume some structure and perhaps allow samples generated from one policy to influence the estimates made for another. The two main approaches for achieving this are value function estimation and direct policy search.
Value function approaches.
Value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy (usually either the "current" or the optimal one).
These methods rely on the theory of MDPs, where optimality is defined in a sense which is stronger than the above one: A policy is called optimal if it achieves the best expected return from "any" initial state (i.e., initial distributions play no role in this definition). Again, one can always find an optimal policy amongst stationary policies.
To define optimality in a formal manner, define the value of a policy formula_15 by
where formula_17 stands for the random return associated with following formula_15 from the initial state formula_30.
Define formula_31 as the maximum possible value of formula_32, where formula_15 is allowed to change:
A policy which achieves these "optimal values" in "each" state is called "optimal". Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return formula_14, since formula_36, where formula_1 is a state randomly sampled from the distribution formula_13.
Although state-values suffice to define optimality, it will prove to be useful to define action-values. Given a state formula_30, an action formula_40 and a policy formula_15, the action-value of the pair formula_42 under formula_15 is defined by
where, now, formula_17 stands for the random return associated with first taking action formula_40 in state formula_30 and following formula_15, thereafter.
It is well-known from the theory of MDPs that if someone gives us formula_49 for an optimal policy, we can always choose optimal actions (and thus act optimally) by simply choosing the action with the highest value at each state. 
The "action-value function" of such an optimal policy is called the "optimal action-value function" and is denoted by formula_50. 
In summary, the knowledge of the optimal action-value function "alone" suffices to know how to act optimally.
Assuming full knowledge of the MDP, there are two basic approaches to compute the optimal action-value function, value iteration and policy iteration. 
Both algorithms compute a sequence of functions formula_51 (formula_52) which converge to formula_50.
Computing these functions involves computing expectations over the whole state-space, which is impractical for all, but the smallest (finite) MDPs, never mind the case when the MDP is unknown.
In reinforcement learning methods the expectations are approximated by averaging over samples and one uses function approximation techniques to cope with the need to represent value functions over large state-action spaces.
Monte Carlo methods.
The simplest Monte Carlo methods can be used in an algorithm that mimics policy iteration.
Policy iteration consists of two steps: "policy evaluation" and "policy improvement".
The Monte Carlo methods are used in the policy evaluation step.
In this step, given a stationary, deterministic policy formula_15, the goal is to compute the function values formula_55 (or a good approximation to them) for all state-action pairs formula_42. 
Assume (for simplicity) that the MDP is finite and in fact a table representing the action-values fits into the memory.
Further, assume that the problem is episodic and after each episode a new one starts from some random initial state.
Then, the estimate of the value of a given state-action pair formula_42can be computed by simply averaging the sampled returns which originated from formula_42 over time. 
Given enough time, this procedure can thus construct a precise estimate formula_49 of the action-value function formula_60.
This finishes the description of the policy evaluation step.
In the policy improvement step, as it is done in the standard policy iteration algorithm, the next policy is obtained by computing a "greedy" policy with respect to formula_49: Given a state formula_30, this new policy returns an action that maximizes formula_63. In practice one often avoids computing and storing the new policy, but uses lazy evaluation to defer the computation of the maximizing actions to when they are actually needed.
A few problems with this procedure are as follows:
Temporal difference methods.
The first issue is easily corrected by allowing the procedure to change the policy (at all, or at some states) before the values settle. However good this sounds, this may be problematic as this might prevent convergence. Still, most current algorithms implement this idea, giving rise to the class of "generalized policy iteration" algorithm. We note in passing that actor critic methods belong to this category.
The second issue can be corrected within the algorithm by allowing trajectories to contribute to any state-action pair in them.
This may also help to some extent with the third problem, although a better solution when returns have high variance is to use Sutton's temporal difference (TD) methods which are based on the recursive Bellman equation. Note that the computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are collected and then the estimates are computed once based on a large number of transitions). Batch methods, a prime example of which is the least-squares temporal difference method due to Bradtke and Barto (1996), may use the information in the samples better, whereas incremental methods are the only choice when batch methods become infeasible due to their high computational or memory complexity. In addition, there exist methods that try to unify the advantages of the two approaches. Methods based on temporal differences also overcome the second but last issue.
In order to address the last issue mentioned in the previous section, "function approximation methods" are used.
In "linear function approximation" one starts with a mapping formula_64 that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair formula_42 are obtained by linearly combining the components of formula_66 with some "weights" formula_67:
The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs.
However, linear function approximation is not the only choice. 
More recently, methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.
So far, the discussion was restricted to how policy iteration can be used as a basis of the designing reinforcement learning algorithms. Equally importantly, value iteration can also be used as a starting point, giving rise to the Q-Learning algorithm (Watkins 1989) and its many variants.
The problem with methods that use action-values is that they may need highly precise estimates of the competing action values, which can be hard to obtain when the returns are noisy. Though this problem is mitigated to some extent by temporal difference methods and if one uses the so-called compatible function approximation method, more work remains to be done to increase generality and efficiency. Another problem specific to temporal difference methods comes from their reliance on the recursive Bellman equation. Most temporal difference methods have a so-called formula_69 parameter formula_70 that allows one to continuously interpolate between Monte-Carlo methods (which do not rely on the Bellman equations) and the basic temporal difference methods (which rely entirely on the Bellman equations), which can thus be effective in palliating this issue.
Direct policy search.
An alternative method to find a good policy is to search directly in (some subset of) the policy space, in which case the problem becomes an instance of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.
Gradient-based methods (giving rise to the so-called "policy gradient methods") start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector formula_67, let formula_72 denote the policy associated to formula_67.
Define the performance function by
Under mild conditions this function will be differentiable as a function of the parameter vector formula_67.
If the gradient of formula_76 was known, one could use gradient ascent.
Since an analytic expression for the gradient is not available, one must rely on a noisy estimate.
Such an estimate can be constructed in many ways, giving rise to algorithms like Williams' REINFORCE method (which is also known as the likelihood ratio method in the simulation-based optimization literature).
Policy gradient methods have received a lot of attention in the last couple of years (e.g., Peters et al. (2003)), but they remain an active field. An overview of policy search methods in the context of robotics has been given by Deisenroth, Neumann and Peters.
The issue with many of these methods is that they may get stuck in local optima (as they are based on local search).
A large class of methods avoids relying on gradient information.
These include simulated annealing, cross-entropy search or methods of evolutionary computation. 
Many gradient-free methods can achieve (in theory and in the limit) a global optimum.
In a number of cases they have indeed demonstrated remarkable performance.
The issue with policy search methods is that they may converge slowly if the information based on which they act is noisy.
For example, this happens when in episodic problems the trajectories are long and the variance of the returns is large. As argued beforehand, value-function based methods that rely on temporal differences might help in this case. In recent years, several actor-critic algorithms have been proposed following this idea and were demonstrated to perform well in various problems.
Theory.
The theory for small, finite MDPs is quite mature. 
Both the asymptotic and finite-sample behavior of most algorithms is well-understood.
As mentioned beforehand, algorithms with provably good online performance (addressing the exploration issue) are known.
The theory of large MDPs needs more work. Efficient exploration is largely untouched (except for the case of bandit problems).
Although finite-time performance bounds appeared for many algorithms in the recent years, these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages, as well as the limitations of these algorithms.
For incremental algorithm asymptotic convergence issues have been settled. Recently, new incremental, temporal-difference-based algorithms have appeared which converge under a much wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).
Current research.
Current research topics include: 
adaptive methods which work with fewer (or no) parameters under a large number of conditions, 
addressing the exploration problem in large MDPs,
large-scale empirical evaluations,
learning and acting under partial information (e.g., using Predictive State Representation), 
modular and hierarchical reinforcement learning, 
improving existing value-function and policy search methods,
algorithms that work well with large (or continuous) action spaces,
transfer learning, 
lifelong learning,
efficient sample-based planning (e.g., based on Monte-Carlo tree search).
Multiagent or Distributed Reinforcement Learning is also a topic of interest in current research. 
There is also a growing interest in real life applications of reinforcement learning.
Successes of reinforcement learning are collected on 
here and 
here.
Reinforcement learning algorithms such as TD learning are also being investigated as a model for Dopamine-based learning in the brain. In this model, the dopaminergic projections from the substantia nigra to the basal ganglia function as the prediction error. Reinforcement learning has also been used as a part of the model for human skill learning, especially in relation to the interaction between implicit and explicit learning in skill acquisition (the first publication on this application was in 1995-1996, and there have been many follow-up studies). See http://webdocs.cs.ualberta.ca/~sutton/RL-FAQ.html#behaviorism for further details of these research areas above.
Inverse reinforcement learning.
In inverse reinforcement learning (IRL), no reward function is given. Instead, one tries to extract a policy given an observed behavior, in order to mimic the observed behavior which is often optimal or close to optimal. Since an agent learning by inverse reinforcement learning, once it has deviated from the track followed by the observed behavior, often needs some way to get back on the track in order for its own behavior to be stable, it is sometimes necessary to demonstrate the behavior multiple times with small perturbations each time.
In apprenticeship learning, one assumes that an expert demonstrating a behavior is trying to maximize a reward function, and tries to recover the unknown reward function of the expert.
Literature.
Conferences, journals.
Most reinforcement learning papers are published at the major machine learning and AI conferences (ICML, NIPS, AAAI, IJCAI, UAI, AI and Statistics) and journals (JAIR, JMLR, Machine learning journal, IEEE T-CIAIG). Some theory papers are published at COLT and ALT. However, many papers appear in robotics conferences (IROS, ICRA) and the "agent" conference AAMAS. Operations researchers publish their papers at the INFORMS conference and, for example, in the Operation Research, and the Mathematics of Operations Research journals. Control researchers publish their papers at the CDC and ACC conferences, or, e.g., in the journals IEEE Transactions on Automatic Control, or Automatica, although applied works tend to be published in more specialized journals. The Winter Simulation Conference also publishes many relevant papers. Other than this, papers also published in the major conferences of the neural networks, fuzzy, and evolutionary computation communities. The annual IEEE symposium titled Approximate Dynamic Programming and Reinforcement Learning (ADPRL) and the biannual European Workshop on Reinforcement Learning (EWRL) are two regularly held meetings where RL researchers meet.

</doc>
<doc id="66297" url="https://en.wikipedia.org/wiki?curid=66297" title="Chinese art">
Chinese art

Chinese art is visual art that, whether ancient or modern, originated in or is practiced in China or by Chinese artists. The Chinese art in the Republic of China (Taiwan) and that of overseas Chinese can also be considered part of Chinese art where it is based in or draws on Chinese heritage and Chinese culture. Early "stone age art" dates back to 10,000 BC, mostly consisting of simple pottery and sculptures. After this early period Chinese art, like Chinese history, is typically classified by the succession of ruling dynasties of Chinese emperors, most of which lasted several hundred years.
Chinese art has arguably the oldest continuous tradition in the world, and is marked by an unusual degree of continuity within, and consciousness of, that tradition, lacking an equivalent to the Western collapse and gradual recovery of classical styles. The media that have usually been classified in the West since the Renaissance as the decorative arts are extremely important in Chinese art, and much of the finest work was produced in large workshops or factories by essentially unknown artists, especially in the field of Chinese porcelain. Much of the best work in ceramics, textiles and other techniques was produced over a long period by the various Imperial factories or workshops, which as well as being used by the court was distributed internally and abroad on a huge scale to demonstrate the wealth and power of the Emperors. In contrast, the tradition of ink wash painting, practiced mainly by scholar-officials and court painters especially of landscapes, flowers, and birds, developed aesthetic values depending on the individual imagination of and objective observation by the artist that are similar to those of the West, but long pre-dated their development there. After contacts with Western art became increasingly important from the 19th century onwards, in recent decades China has participated with increasing success in worldwide contemporary art.
Painting.
Traditional Chinese painting involves essentially the same techniques as Chinese calligraphy and is done with a brush dipped in black or colored ink; oils are not used. As with calligraphy, the most popular materials on which paintings are made of paper and silk. The finished work can be mounted on scrolls, such as hanging scrolls or handscrolls. Traditional painting can also be done on album sheets, walls, lacquerware, folding screens, and other media.
The two main techniques in Chinese painting are:
Artists from the Han (202 BC) to the Tang (618–906) dynasties mainly painted the human figure. Much of what is known of early Chinese figure painting comes from burial sites, where paintings were preserved on silk banners, lacquered objects, and tomb walls. Many early tomb paintings were meant to protect the dead or help their souls get to paradise. Others illustrated the teachings of the Chinese philosopher Confucius, or showed scenes of daily life. Most Chinese portraits showed a formal full-length frontal view, and were used in the family in ancestor veneration. Imperial portraits were more flexible, but were generally not seen outside the court, and portraiture formed no part of Imperial propaganda, as in other cultures.
Many critics consider landscape to be the highest form of Chinese painting. The time from the Five Dynasties period to the Northern Song period (907–1127) is known as the "Great age of Chinese landscape". In the north, artists such as Jing Hao, Li Cheng, Fan Kuan, and Guo Xi painted pictures of towering mountains, using strong black lines, ink wash, and sharp, dotted brushstrokes to suggest rough rocks. In the south, Dong Yuan, Juran, and other artists painted the rolling hills and rivers of their native countryside in peaceful scenes done with softer, rubbed brushwork. These two kinds of scenes and techniques became the classical styles of Chinese landscape painting.
Sculpture.
Chinese ritual bronzes from the Shang and Western Zhou Dynasties come from a period of over a thousand years from c. 1500, and have exerted a continuing influence over Chinese art. They are cast with complex patterned and zoomorphic decoration, but avoid the human figure, unlike the huge figures only recently discovered at Sanxingdui. The spectacular Terracotta Army was assembled for the tomb of Qin Shi Huang, the first emperor of a unified China from 221–210 BC, as a grand imperial version of the figures long placed in tombs to enable the deceased to enjoy the same lifestyle in the afterlife as when alive, replacing actual sacrifices of very early periods. Smaller figures in pottery or wood were placed in tombs for many centuries afterwards, reaching a peak of quality in the Tang Dynasty.
Native Chinese religions do not usually use cult images of deities, or even represent them, and large religious sculpture is nearly all Buddhist, dating mostly from the 4th to the 14th century, and initially using Greco-Buddhist models arriving via the Silk Road. Buddhism is also the context of all large portrait sculpture; in total contrast to some other areas in medieval China even painted images of the emperor were regarded as private. Imperial tombs have spectacular avenues of approach lined with real and mythological animals on a scale matching Egypt, and smaller versions decorate temples and palaces. Small Buddhist figures and groups were produced to a very high quality in a range of media, as was relief decoration of all sorts of objects, especially in metalwork and jade. Sculptors of all sorts were regarded as artisans and very few names are recorded.
Pottery.
Chinese ceramic ware shows a continuous development since the pre-dynastic periods, and is one of the most significant forms of Chinese art. China is richly endowed with the raw materials needed for making ceramics. The first types of ceramics were made during the Palaeolithic era, and in later periods range from construction materials such as bricks and tiles, to hand-built pottery vessels fired in bonfires or kilns, to the sophisticated Chinese porcelain wares made for the imperial court. Most later Chinese ceramics, even of the finest quality, were made on an industrial scale, thus very few individual potters or painters are known. Many of the most renowned workshops were owned by or reserved for the Emperor, and large quantities of ceramics were exported as diplomatic gifts or for trade from an early date.
Decorative arts.
As well as porcelain, a wide range of materials that were more valuable were worked and decorated with great skill for a range of uses or just for display. Chinese jade was attributed with magical powers, and was used in the Stone and Bronze Ages for large and impractical versions of everyday weapons and tools, as well as the "bi" disks and "cong" vessels. Later a range of objects and small sculptures were carved in jade, a difficult and time-consuming technique. Bronze, gold and silver, rhinoceros horn, Chinese silk, ivory, lacquer, cloisonne enamel and many other materials had specialist artists working in them.
Folding screen() is also a form of decorative art in China. And the folding screen itself was often decorated with beautiful art, major themes included mythology, scenes of palace life, and nature.There are different kinds of materials in making folding screens, such as wood panel, paper and silk. Folding screens were considered ideal ornaments for many painters to display their paintings and calligraphy on. Many artists painted on paper or silk and applied it onto the folding screen. There were two distinct artistic folding screens mentioned in historical literature of the era.
Historical development to 221 BC.
Neolithic pottery.
Early forms of art in China are found in the Neolithic Yangshao culture, which dates back to the 6th millennium BC. Archeological findings such as those at the Banpo have revealed that the Yangshao made pottery; early ceramics were unpainted and most often cord-marked. The first decorations were fish and human faces, but these eventually evolved into symmetrical-geometric abstract designs, some painted.
The most distinctive feature of Yangshao culture was the extensive use of painted pottery, especially human facial, animal, and geometric designs. Unlike the later Longshan culture, the Yangshao culture did not use pottery wheels in pottery making. Excavations have found that children were buried in painted pottery jars.
Jade culture.
The Liangzhu culture was the last Neolithic Jade culture in the Yangtze River Delta and was spaced over a period of about 1,300 years. The Jade from this culture is characterized by finely worked, large ritual jades such as Cong cylinders, Bi discs, Yue axes and also pendants and decorations in the form of chiseled open-work plaques, plates and representations of small birds, turtles and fish. The Liangzhu Jade has a white, milky bone-like aspect due to its Tremolite rock origin and influence of water-based fluids at the burial sites.
Bronze casting.
The Bronze Age in China began with the Xia Dynasty. Examples from this period have been recovered from ruins of the Erlitou culture, in Shanxi, and include complex but unadorned utilitarian objects. In the following Shang Dynasty more elaborate objects, including many ritual vessels, were crafted. The Shang are remembered for their bronze casting, noted for its clarity of detail. Shang bronzesmiths usually worked in foundries outside the cities to make ritual vessels, and sometimes weapons and chariot fittings as well. The bronze vessels were receptacles for storing or serving various solids and liquids used in the performance of sacred ceremonies. Some forms such as the "ku" and "jue" can be very graceful, but the most powerful pieces are the "ding", sometimes described as having an "air of ferocious majesty."
It is typical of the developed Shang style that all available space is decorated, most often with stylized forms of real and imaginary animals. The most common motif is the "taotie", which shows a mythological being presented frontally as though squashed onto a horizontal plane to form a symmetrical design. The early significance of "taotie" is not clear, but myths about it existed around the late Zhou Dynasty. It was considered to be variously a covetous man banished to guard a corner of heaven against evil monsters; or a monster equipped with only a head which tries to devour men but hurts only itself.
The function and appearance of bronzes changed gradually from the Shang to the Zhou. They shifted from been used in religious rites to more practical purposes. By the Warring States period, bronze vessels had become objects of aesthetic enjoyment. Some were decorated with social scenes, such as from a banquet or hunt; whilst others displayed abstract patterns inlaid with gold, silver, or precious and semiprecious stones.
Shang bronzes became appreciated as works of art from the Song Dynasty, when they were collected and prized not only for their shape and design but also for the various green, blue green, and even reddish patinas created by chemical action as they lay buried in the ground. The study of early Chinese bronze casting is a specialized field of art history.
Chu and Southern culture.
A rich source of art in early China was the state of Chu, which developed in the Yangtze River valley. Excavations of Chu tombs have found painted wooden sculptures, jade disks, glass beads, musical instruments, and an assortment of lacquerware. Many of the lacquer objects are finely painted, red on black or black on red. A site in Changsha, Hunan province, has revealed some of the oldest paintings on silk discovered to date.
Early Imperial China (221 BC–AD 220).
Qin sculpture.
The Terracotta Army, inside the Mausoleum of the First Qin Emperor, consists of more than 7,000 life-size tomb terra-cotta figures of warriors and horses buried with the self-proclaimed first Emperor of Qin (Qin Shi Huang) in 210–209 BC. The figures were painted before being placed into the vault. The original colors were visible when the pieces were first unearthed. However, exposure to air caused the pigments to fade, so today the unearthed figures appear terracotta in color. The figures are in several poses including standing infantry and kneeling archers, as well as charioteers with horses. Each figure's head appears to be unique, showing a variety of facial features and expressions as well as hair styles.
Pottery.
Porcelain is made from a hard paste made of the clay kaolin and a feldspar called petuntse, which cements the vessel and seals any . "China" has become synonymous with high-quality porcelain. Most china pots comes from the city of Jingdezhen in China's Jiangxi province. Jingdezhen, under a variety of names, has been central to porcelain production in China since at least the early Han Dynasty.
The most noticeable difference between porcelain and the other pottery clays is that it "wets" very quickly (that is, added water has a noticeably greater effect on the plasticity for porcelain than other clays), and that it tends to continue to "move" longer than other clays, requiring experience in handling to attain optimum results. During medieval times in Europe, porcelain was very expensive and in high demand for its beauty. TLV mirrors also date from the Han dynasty.
Han art.
The Han Dynasty was known for jade burial suits. One of the earliest known depictions of a landscape in Chinese art comes from a pair of hollow-tile door panels from a Western Han Dynasty tomb near Zhengzhou, dated 60 BC. A scene of continuous depth recession is conveyed by the zigzag of lines representing roads and garden walls, giving the impression that one is looking down from the top of a hill. This artistic landscape scene was made by the repeated impression of standard stamps on the clay while it was still soft and not yet fired. However, the oldest known landscape art scene tradition in the classical sense of painting is a work by Zhan Ziqian of the Sui Dynasty (581–618).
Period of division (220–581).
Influence of Buddhism.
Buddhism arrived in China around the 1st century AD (although there are some traditions about a monk visiting China during Asoka's reign), and through to the 8th century it became very active and creative in the development of Buddhist art, particularly in the area of statuary. Receiving this distant religion, China soon incorporated strong Chinese traits in its artistic expression.
In the fifth to sixth century the Northern Dynasties, rather removed from the original sources of inspiration, tended to develop rather symbolic and abstract modes of representation, with schematic lines. Their style is also said to be solemn and majestic. The lack of corporeality of this art, and its distance from the original Buddhist objective of expressing the pure ideal of enlightenment in an accessible, realistic manner, progressively led to a research towards more naturalism and realism, leading to the expression of Tang Buddhist art.
Calligraphy.
In ancient China, painting and calligraphy were the most highly appreciated arts in court circles and were produced almost exclusively by amateurs, aristocrats and scholar-officials who alone had the leisure to perfect the technique and sensibility necessary for great brushwork. Calligraphy was thought to be the highest and purest form of painting. The implements were the brush pen, made of animal hair, and black inks, made from pine soot and animal glue. Writing as well as painting was done on silk. But after the invention of paper in the 1st century, silk was gradually replaced by the new and cheaper material. Original writings by famous calligraphers have been greatly valued throughout China's history and are mounted on scrolls and hung on walls in the same way that paintings are.
Wang Xizhi was a famous Chinese calligrapher who lived in the 4th century AD. His most famous work is the Lanting Xu, the preface of a collection of poems written by a number of poets when gathering at Lan Ting near the town of Shaoxing in Zhejiang province and engaging in a game called "qu shui liu shang".
Wei Shuo was a well-known calligrapher of Eastern Jin Dynasty who established consequential rules about the Regular Script. Her well-known works include "Famous Concubine Inscription" (名姬帖 Ming Ji Tie) and "The Inscription of Wei-shi He'nan" (衛氏和南帖 Wei-shi He'nan Tie).
Painting.
Gu Kaizhi is a celebrated painter of ancient China born in Wuxi. He wrote three books about painting theory: "On Painting" (畫論), "Introduction of Famous Paintings of Wei and Jin Dynasties" (魏晉勝流畫讚) and "Painting Yuntai Mountain" (畫雲臺山記). He wrote, "In figure paintings the clothes and the appearances were not very important. The eyes were the spirit and the decisive factor."
Three of Gu's paintings still survive today. They are "Admonitions of the Instructress to the Court Ladies", "Nymph of the Luo River" (洛神賦), and "Wise and Benevolent Women".
There are other examples of Jin Dynasty painting from tombs. This includes the "Seven Sages of the Bamboo Grove," painted on a brick wall of a tomb located near modern Nanjing and now found in the Shaanxi Provincial Museum. Each of the figures are labeled and shown either drinking, writing, or playing a musical instrument. Other tomb paintings also depict scenes of daily life, such as men plowing fields with teams of oxen.
The Sui and Tang dynasties (581–960).
Buddhist architecture and sculpture.
Following a transition under the Sui Dynasty, Buddhist sculpture of the Tang evolved towards a markedly lifelike expression. As a consequence of the Dynasty's openness to foreign trade and influences through the Silk Road, Tang dynasty Buddhist sculpture assumed a rather classical form, inspired by the Greco-Buddhist art of Central Asia.
However, foreign influences came to be negatively perceived towards the end of the Tang dynasty. In the year 845, the Tang emperor Wu-Tsung outlawed all "foreign" religions (including Christian Nestorianism, Zoroastrianism and Buddhism) in order to support the indigenous Taoism. He confiscated Buddhist possessions and forced the faith to go underground, therefore affecting the ulterior development of the religion and its arts in China.
Most wooden Tang sculptures have not survived, though representations of the Tang international style can still be seen in Nara, Japan. The longevity of stone sculpture has proved much greater. Some of the finest examples can be seen at Longmen, near Luoyang, Yungang near Datong, and Bingling Temple, in Gansu.
One of the most famous Buddhist Chinese pagodas is the Giant Wild Goose Pagoda, built in 652 AD.
Painting.
Beginning in the Tang dynasty (618–907), the primary subject matter of painting was the landscape, known as shanshui (mountain water) painting. In these landscapes, usually monochromatic and sparse, the purpose was not to reproduce exactly the appearance of nature but rather to grasp an emotion or atmosphere so as to catch the "rhythm" of nature.
Painting in the traditional style involved essentially the same techniques as calligraphy and was done with a brush dipped in black or colored ink; oils were not used. As with calligraphy, the most popular materials on which paintings were made were paper and silk. The finished works were then mounted on scrolls, which could be hung or rolled up. Traditional painting was also done in albums, on walls, lacquer work, and in other media.
Dong Yuan was an active painter in the Southern Tang Kingdom. He was known for both figure and landscape paintings, and exemplified the elegant style which would become the standard for brush painting in China over the next 900 years. As with many artists in China, his profession was as an official where he studied the existing styles of Li Sixun and Wang Wei. However, he added to the number of techniques, including more sophisticated perspective, use of pointillism and crosshatching to build up vivid effect.
Zhan Ziqian was a painter during the Sui Dynasty. His only painting in existence is "Strolling About In Spring" arranged mountains perspectively. Because the first pure scenery paintings of Europe emerged after the 17th century, "Strolling About In Spring" may well be the first scenery painting of the world.
The Song and Yuan dynasties (960–1368).
Song painting.
During the Song dynasty (960–1279), landscapes of more subtle expression appeared; immeasurable distances were conveyed through the use of blurred outlines, mountain contours disappearing into the mist, and impressionistic treatment of natural phenomena. Emphasis was placed on the spiritual qualities of the painting and on the ability of the artist to reveal the inner harmony of man and nature, as perceived according to Taoist and Buddhist concepts.
Liang Kai was a Chinese painter who lived in the 13th century (Song Dynasty). He called himself "Madman Liang," and he spent his life drinking and painting. Eventually, he retired and became a Zen monk. Liang is credited with inventing the Zen school of Chinese art. Wen Tong was a painter who lived in the 11th century. He was famous for ink paintings of bamboo. He could hold two brushes in one hand and paint two different distanced bamboos simultaneously. He did not need to see the bamboo while he painted them because he had seen a lot of them.
Zhang Zeduan was a notable painter for his horizontal "Along the River During Qingming Festival" landscape and cityscape painting. It has been quoted as "China's Mona Lisa" and has had many well-known remakes throughout Chinese history. Other famous paintings include "The Night Revels of Han Xizai", originally painted by the Southern Tang artist Gu Hongzhong in the 10th century, while the well-known version of his painting is a 12th-century remake of the Song Dynasty. This is a large horizontal handscroll of a domestic scene showing men of the gentry class being entertained by musicians and dancers while enjoying food, beverage, and wash basins provided by maidservants. In 2000, the modern artist Wang Qingsong created a parody of this painting with a long, horizontal photograph of people in modern clothing making similar facial expressions, poses, and hand gestures as the original painting.
Yuan painting.
With the fall of the Song dynasty in 1279, and the subsequent dislocation caused by the establishment of the Yuan dynasty by the Mongol conquerors, many court and literary artists retreated from social life, and returned to nature, through landscape paintings, and by renewing the "blue and green" style of the Tang era.
Wang Meng was one such painter, and one of his most famous works is the "Forest Grotto". Zhao Mengfu was a Chinese scholar, painter and calligrapher during the Yuan Dynasty. His rejection of the refined, gentle brushwork of his era in favor of the cruder style of the 8th century is considered to have brought about a revolution that created the modern Chinese landscape painting. There was also the vivid and detailed works of art by Qian Xuan (1235–1305), who had served the Song court, and out of patriotism refused to serve the Mongols, instead turning to painting. He was also famous for reviving and reproducing a more Tang Dynasty style of painting.
The later Yuan dynasty is characterized by the work of the so-called "Four Great Masters". The most notable of these was Huang Gongwang (1269–1354) whose cool and restrained landscapes were admired by contemporaries, and by the Chinese literati painters of later centuries. Another of great influence was Ni Zan (1301–1374), who frequently arranged his compositions with a strong and distinct foreground and background, but left the middle-ground as an empty expanse. This scheme was frequently to be adopted by later Ming and Qing dynasty painters.
Late imperial China (1368–1911).
Ming painting.
Under the Ming dynasty, Chinese culture bloomed. Narrative painting, with a wider color range and a much busier composition than the Song paintings, was immensely popular during the time.
Wen Zhengming (1470–1559) developed the style of the Wu school in Suzhou, which dominated Chinese painting during the 16th century.
European culture began to make an impact on Chinese art during this period. The Jesuit priest Matteo Ricci visited Nanjing with many Western artworks, which were influential in showing different techniques of perspective and shading.
Early Qing painting.
The early Qing dynasty developed in two main strands: the Orthodox school, and the Individualist painters, both of which followed the theories of Dong Qichang, but emphasizing very different aspects.
The "Four Wangs", including Wang Jian (1598–1677) and Wang Shimin (1592–1680), were particularly renowned in the Orthodox school, and sought inspiration in recreating the past styles, especially the technical skills in brushstrokes and calligraphy of ancient masters. The younger Wang Yuanqi (1642–1715) ritualized the approach of engaging with and drawing inspiration from a work of an ancient master. His own works were often annotated with his theories of how his painting relates to the master's model.
The Individualist painters included 
Bada Shanren (1626–1705) and
Shitao (1641–1707). They drew more from the revolutionary ideas of transcending the tradition to achieve an original individualistic styles; in this way they were more faithfully following the way of Dong Qichang than the Orthodox school (who were his official direct followers.)
As the techniques of color printing were perfected, illustrated manuals on the art of painting began to be published. Jieziyuan Huazhuan (Manual of the Mustard Seed Garden), a five-volume work first published in 1679, has been in use as a technical textbook for artists and students ever since.
Late Qing Art.
Nianhua were a form of colored woodblock prints in China, depicting images for decoration during the Chinese New Year. In the 19th century Nianhua were used as news mediums.
Shanghai School.
The Shanghai School is a very important Chinese school of traditional arts during the Qing Dynasty and the 20th century. Under efforts of masters from this school, traditional Chinese art reached another climax and continued to the present in forms of "Chinese painting" (中國畫), or "guohua" (國畫) for short. The Shanghai School challenged and broke the literati tradition of Chinese art, while also paying technical homage to the ancient masters and improving on existing traditional techniques. Members of this school were themselves educated literati who had come to question their very status and the purpose of art, and had anticipated the impending modernization of Chinese society. In an era of rapid social change, works from the Shanghai School were widely innovative and diverse, and often contained thoughtful yet subtle social commentary. The best known figures from this school are Ren Xiong, Ren Bonian, Zhao Zhiqian, Wu Changshuo, Sha Menghai, Pan Tianshou, Fu Baoshi, He Tianjian, and Xie Zhiliu. Other well-known painters include Wang Zhen, XuGu, Zhang Xiong, Hu Yuan, and Yang Borun.
New China art (1912–1949).
Transformation.
With the end of the last dynasty in China, the New Culture Movement began and defied all facets of traditionalism. A new breed of 20th century cultural philosophers like Xiao Youmei, Cai Yuanpei, Feng Zikai and Wang Guangqi wanted Chinese culture to modernise and reflect the New China. The Chinese Civil War would cause a drastic split between the Kuomintang and the Communist Party of China. Following was the Second Sino-Japanese War in particular. The Battle of Shanghai would leave the major cultural art center borderline to a humanitarian crisis.
Painting.
Ong Schan Tchow () (1900–1945), artist and friend of Cai Yuanpei accomplished the subtle integration of Western art techniques and perspectives into traditional Chinese painting. Ong was one of the first few batches of Chinese scholars and artists who studied in France in the early 20th Century.
Western style oil painting was introduced to China by painters such as Xiao Tao Sheng. Another important influential artist in the 1940s was Tai Ping Meijing who incorporated nature in all his art and mixed traditional Asian art with realism.
Communist and socialist art (1950-1980s).
Selective art decline.
The Communist Party of China would have full control of the government with Mao Zedong heading the People's Republic of China. If the art was presented in a manner that favored the government, the artists were heavily promoted. Vice versa, any clash with communist party beliefs would force the artists to become farmers via "re-education" processes under the regime. The peak era of governmental control came under the Cultural Revolution. The most notable event was the Destruction of the Four Olds, which had major consequences for pottery, paintings, literary art, architecture and countless others.
Painting.
Artists were encouraged to employ socialist realism. Some Soviet Union socialist realism was imported without modification, and painters were assigned subjects and expected to mass-produce paintings. This regimen was considerably relaxed in 1953, and after the Hundred Flowers Campaign of 1956–57, traditional Chinese painting experienced a significant revival. Along with these developments in professional art circles, there was a proliferation of peasant art depicting everyday life in the rural areas on wall murals and in open-air painting exhibitions. Notable modern Chinese painters include Huang Binhong, Qi Baishi, Xu Beihong, Chang Ta Chien, Pan Tianshou, Wu Changshi, Fu Baoshi, Wang Kangle and Zhang Chongren.
Redevelopment (Mid-1980s – 1990s).
Contemporary Art.
Contemporary Chinese art (中國當代藝術, Zhongguo Dangdai Yishu) often referred to as Chinese avant-garde art, continued to develop since the 1980s as an outgrowth of modern art developments post-Cultural Revolution. 
Contemporary Chinese art fully incorporates painting, film, video, photography, and performance. Until recently, art exhibitions deemed controversial have been routinely shut down by police, and performance artists in particular faced the threat of arrest in the early 1990s. More recently there has been greater tolerance by the Chinese government, though many internationally acclaimed artists are still restricted from media exposure at home or have exhibitions ordered closed. Leading contemporary visual artists include Ai Weiwei, Cai Guoqiang, Chan Shengyao, Fang Lijun, Fu Wenjun, He Xiangyu, Huang Yan, Huang Yong Ping, Han Yajuan, Kong Bai Ji, Li Hongbo, Li Hui, Liu Bolin, Lu Shengzhong, Ma Liuming, Qiu Shihua, Shen Shaomin, Shi Jinsong, Song Dong, Li Wei, Christine Wang, Wang Guangyi, Wenda Gu, Xu Bing, Yang Zhichao, Zhan Wang, Zheng Lianjie, Zhang Dali, Zhang Xiaogang, Zhang Huan, Zhou Chunya, Zhu Yu, Ma Kelu, Ding Fang, Shang Yang and Guo Jian.
Visual art.
Beginning in the late 1980s there was unprecedented exposure for younger Chinese visual artists in the west to some degree through the agency of curators based outside the country such as Hou Hanru. Local curators within the country such as Gao Minglu and critics such as Li Xianting (栗憲庭) reinforced this promotion of particular brands of painting that had recently emerged, while also spreading the idea of art as a strong social force within Chinese culture. There was some controversy as critics identified these imprecise representations of contemporary Chinese art as having been constructed out of personal preferences, a kind of programmatized artist-curator relationship that only further alienated the majority of the avant-garde from Chinese officialdom and western art market patronage.
Art market.
Today, the market for Chinese art, both antique and contemporary, is widely reported to be among the hottest and fastest-growing in the world, attracting buyers all over the world. The "Voice of America" reported in 2006 that modern Chinese art is raking in record prices both internationally and in domestic markets, some experts even fearing the market might be overheating. "The Economist" reported that Chinese art has become the latest darling in the world market according to the record sales from Sotheby's and Christie's, the biggest fine-art auction houses. The "International Herald Tribune" reported that Chinese porcelains were fought over in the art market as "if there was no tomorrow". Contemporary Chinese art also saw record sales throughout the 2000s. In 2007, it was estimated that 5 of the world's 10 best selling living artists at art auction were from China, with artists such as Zhang Xiaogang whose works were sold for a total of $56.8 million at auction in 2007. In terms of buying-market, China overtook France in the late 2000s as the world's third-largest art market, after the United States and the United Kingdom, due to the growing middle-class in the country. Sotheby's noted that contemporary Chinese art has rapidly changed the contemporary Asian art world into one of the most dynamic sectors on the international art market. During the global economic crisis, the contemporary Asian art market and the contemporary Chinese art market experienced a slow down in late 2008. The market for Contemporary Chinese and Asian art saw a major revival in late 2009 with record level sales at Christie's. For centuries largely made-up of European and American buyers, the international buying market for Chinese art has also begun to be dominated by Chinese dealers and collectors in recent years. It was reported in 2011, China has become the world's second biggest market for art and antiques, accounting for 23 percent of the world's total art market, behind the United States (which accounts for 34 percent of the world's art market). Another transformation driving the growth of the Chinese art market is the rise of a clientele no longer mostly European or American. New fortunes from countries once thought of as poor often prefer non-Western art; a large gallerist in the field has offices in both New York and Beijing, but clients mainly hailing from Latin America, Asia and the Middle East.
One of the areas that has revived art concentration and also commercialized the industry is the 798 Art District in Dashanzi of Beijing. The artist Zhang Xiaogang sold a 1993 painting for US$2.3 million in 2006, which included blank faced Chinese families from the Cultural Revolution era, while Yue Minjun's work "Execution" in 2007 was sold for a then record of nearly $6 million at Sotheby's. Collectors including Stanley Ho, the owner of the Macau Casinos, investment manager Christopher Tsai, and casino developer Stephen Wynn, would capitalize on the art trends. Items such as Ming Dynasty vases and assorted Imperial pieces were auctioned off.
Other art works were sold in places such as Christie's including a Chinese porcelain piece with the mark of the Qianlong Emperor sold for HKD $ $151.3 million. A 1964 painting ""All the Mountains Blanketed in Red"" was sold for HKD $35 million. Auctions were also held at Sotheby's where Xu Beihong's 1939 masterpiece ""Put Down Your Whip"" sold for HKD $72 million. The industry is not limited to fine arts, as many other types of contemporary pieces were also sold. In 2000, a number of Chinese artists were included in Documenta and the Venice Biennale of 2003. China now has its own major contemporary art showcase with the Venice Biennale. Fuck Off was a notorious art exhibition which ran alongside the Shanghai Biennial Festival in 2000 and was curated by independent curator Feng Boyi and contemporary artist Ai Weiwei.

</doc>
<doc id="66299" url="https://en.wikipedia.org/wiki?curid=66299" title="Lockheed Martin F-22 Raptor">
Lockheed Martin F-22 Raptor

The Lockheed Martin F-22 Raptor is a fifth-generation, single-seat, twin-engine, all-weather stealth tactical fighter aircraft developed for the United States Air Force (USAF). The result of the USAF's Advanced Tactical Fighter program, the aircraft was designed primarily as an air superiority fighter, but also has ground attack, electronic warfare, and signals intelligence capabilities. The prime contractor, Lockheed Martin, built most of the F-22's airframe and weapons systems and did its final assembly, while Boeing provided the wings, aft fuselage, avionics integration, and training systems.
The aircraft was variously designated F-22 and F/A-22 before it formally entered service in December 2005 as the F-22A. After a protracted development and despite operational issues, the USAF considers the F-22 critical to its tactical air power, and says that the aircraft is unmatched by any known or projected fighter. The Raptor's combination of stealth, aerodynamic performance, and situational awareness gives the aircraft unprecedented air combat capabilities.
The high cost of the aircraft, a lack of clear air-to-air missions due to delays in Russian and Chinese fighter programs, a ban on exports, and development of the more versatile and comparatively lower cost F-35 led to the end of F-22 production. A final procurement tally of 187 operational production aircraft was established in 2009 and the last F-22 was delivered to the USAF in 2012.
Development.
Origins.
In 1981 the U.S. Air Force developed a requirement for an Advanced Tactical Fighter (ATF) as a new air superiority fighter to replace the F-15 Eagle and F-16 Fighting Falcon. Code named ""Senior Sky"", this program was influenced by the emerging worldwide threats, including development and proliferation of Soviet Su-27 "Flanker"- and MiG-29 "Fulcrum"-class fighter aircraft. It would take advantage of the new technologies in fighter design on the horizon, including composite materials, lightweight alloys, advanced flight control systems, more powerful propulsion systems, and stealth technology. The request for proposals (RFP) was issued in July 1986 and two contractor teams, Lockheed/Boeing/General Dynamics and Northrop/McDonnell Douglas, were selected on 31 October 1986 to undertake a 50-month demonstration phase, culminating in the flight test of two technology demonstrator prototypes, the YF-22 and the YF-23.
Each design team produced two prototype air vehicles, one for each of the two engine options. The Lockheed-led team employed thrust vectoring nozzles on YF-22 for enhanced maneuverability in dogfights. The ATF's increasing weight and cost drove out certain requirements during development. Side-looking radars were deleted, and the dedicated infra-red search and track (IRST) system was downgraded from multi-color to single color and then deleted as well. However, space and cooling provisions were retained to allow for future addition of these components. The ejection seat requirement was downgraded from a fresh design to the existing McDonnell Douglas ACES II.
After the flight test demonstration and validation of the prototypes, on 23 April 1991, Secretary of the USAF Donald Rice announced the YF-22 as the winner of the ATF competition. The YF-23 design was considered stealthier and faster while the YF-22 was more maneuverable. The aviation press speculated that the YF-22 was also more adaptable to the U.S. Navy's Navalized Advanced Tactical Fighter (NATF), but by 1992, the Navy had abandoned NATF.
Production and procurement.
Prime contractor Lockheed Martin Aeronautics manufactured the majority of the airframe and performed final assembly at Dobbins Air Reserve Base in Marietta, Georgia; program partner Boeing Defense, Space & Security provided additional airframe components as well as avionics integration and training systems. F-22 production was split up over many subcontractors across 46 states to increase Congressional support, though this production split may have contributed to increased costs and delays. Many capabilities were deferred to post-service upgrades, reducing the initial cost but increasing total program cost. Production supported over 1,000 subcontractors and suppliers and up to 95,000 jobs.
The F-22 had several design changes from the YF-22. The swept-back angle of the leading edge was decreased from 48° to 42°, while the vertical stabilizers were shifted rearward and decreased in area by 20%. To improve pilot visibility, the canopy was moved forward , and the engine intakes moved rearward . The shapes of the wing and stabilator trailing edges were refined to improve aerodynamics, strength, and stealth characteristics. Increasing weight during development caused slight reductions in range and aerodynamic performance.
The first F-22, an engineering and manufacturing development (EMD) aircraft named Raptor 4001, was unveiled at Marietta, Georgia, on 9 April 1997, and first flew on 7 September 1997. In 2006, the Raptor's development team, composed of over 1,000 contractors and the USAF, won the Collier Trophy, American aviation's most prestigious award. The F-22 was in production for 15 years, at a rate of roughly two per month during peak production.
The USAF originally envisioned ordering 750 ATFs at a cost of $26.2 billion, with production beginning in 1994. The 1990 Major Aircraft Review led by Secretary of Defense Dick Cheney reduced this to 648 aircraft beginning in 1996. By 1997, funding instability had further cut the total to 339, which was again reduced to 277 F-22s by 2003. In 2004, the Department of Defense (DoD) further reduced this to 183 operational aircraft, despite the USAF's preference for 381. In 2006, a multi-year procurement plan was implemented to save $15 billion but raise each aircraft's cost. That year the program's total cost was projected to be $62 billion for 183 F-22s distributed to seven combat squadrons. In 2007, Lockheed Martin received a $7.3 billion contract to increase the order to 183 production F-22s and extend manufacturing through 2011.
In April 2006, the Government Accountability Office (GAO) assessed the F-22's cost to be $361 million per aircraft, with $28 billion invested in development and testing; the Unit Procurement Cost was estimated at $178 million in 2006, based on a production run of 181 aircraft. It was estimated by the end of production, $34 billion will have been spent on procurement, resulting in a total program cost of $62 billion, around $339 million per aircraft. The incremental cost for an additional F-22 was estimated at about $138 million in 2009. The GAO stated the estimated cost was $412 million per aircraft in 2012.
Ban on exports.
The F-22 cannot be exported under American federal law to protect its stealth technology and other high-tech features. Customers for U.S. fighters are acquiring earlier designs such as the F-15 Eagle and F-16 Fighting Falcon or the newer F-35 Lightning II Joint Strike Fighter, which contains technology from the F-22 but was designed to be cheaper, more flexible, and available for export. In September 2006, Congress upheld the ban on foreign F-22 sales. Despite the ban, the 2010 defense authorization bill included provisions requiring the DoD to prepare a report on the costs and feasibility for an F-22 export variant, and another report on the effect of F-22 export sales on U.S. aerospace industry.
Some Australian politicians and defense commentators proposed that Australia should attempt to purchase F-22s instead of the planned F-35s, citing the F-22's known capabilities and F-35's delays and developmental uncertainties. However, the Royal Australian Air Force (RAAF) determined that the F-22 was unable to perform the F-35's strike and close air support roles. The Japanese government also showed interest in the F-22 for its Replacement-Fighter program. The Japan Air Self-Defense Force (JASDF) would reportedly require fewer fighters for its mission if it obtained the F-22, thus reducing engineering and staffing costs. However, in 2009 it was reported that acquiring the F-22 would require increases to the defense budget beyond the historical 1 percent of GDP. With the end of F-22 production, Japan chose the F-35 in December 2011. Israel also expressed interest, but eventually chose the F-35 because of the F-22's price and unavailability.
Production termination.
Throughout the 2000s, the need for F-22s was debated due to rising costs and the lack of relevant adversaries. In 2006, Comptroller General of the United States David Walker found that "the DoD has not demonstrated the need" for more investment in the F-22, and further opposition to the program was expressed by Secretary of Defense Donald Rumsfeld, Deputy Secretary of Defense Gordon R. England, Senator John McCain, and Chairman of U.S. Senate Committee on Armed Services Senator John Warner. The F-22 program lost influential supporters in 2008 after the forced resignations of Secretary of the Air Force Michael Wynne and the Chief of Staff of the Air Force General T. Michael Moseley. Nevertheless, in 2008, Congress passed a defense spending bill funding the F-22's continued production and the Pentagon released $50 million of the $140 million for four additional aircraft, raising the total orders for production aircraft to 187 and leaving the program in the hands of the next administration.
In November 2008, Secretary of Defense Robert Gates stated that the Raptor was not relevant in post-Cold War conflicts such as in Iraq and Afghanistan, and in April 2009, under the new Obama Administration, he called for ending F-22 production in fiscal year (FY) 2011, leaving the USAF with 187 production aircraft. In July, General James Cartwright, Vice Chairman of the Joint Chiefs of Staff, stated to the Senate Committee on Armed Services his reasons for supporting termination of F-22 production. They included shifting resources to the multirole F-35 to allow proliferation of fifth-generation fighters for three service branches and preserving the F/A-18 production line to maintain the military's electronic warfare (EW) capabilities in the Boeing EA-18G Growler. Issues with the F-22's reliability and availability also raised concerns. After President Obama threatened to veto further production, the Senate voted in July 2009 in favor of ending production and the House subsequently agreed to abide by the 187 production aircraft cap. Gates stated that the decision was taken in light of the F-35's capabilities, and in 2010, he set the F-22 requirement to 187 aircraft by lowering the number of major regional conflict preparations from two to one.
In 2010, USAF initiated a study to determine the costs of retaining F-22 tooling for a future Service Life Extension Program (SLEP). A RAND Corporation paper from this study estimated that restarting production and building an additional 75 F-22s would cost $17 billion, resulting in $227 million per aircraft or 54 million higher than the flyaway cost. Lockheed Martin stated that restarting the production line itself would cost about $200 million. Production tooling will be documented in illustrated electronic manuals stored at the Sierra Army Depot. Retained tooling will produce additional components; due to the limited production run there are no reserve aircraft, leading to considerable care during maintenance. Later attempts to retrieve this tooling found that the containers were empty.
Russian and Chinese fighter developments have fueled concern, and in 2009, General John Corley, head of Air Combat Command, stated that a fleet of 187 F-22s would be inadequate, but Secretary Gates dismissed this concern. In 2011, Gates explained that Chinese fifth-generation fighter developments had been accounted for when the number of F-22s was set, and that the U.S. would have a considerable advantage in stealth aircraft in 2025, even with F-35 delays. In December 2011, the 195th and final F-22 was completed out of 8 test and 187 operational aircraft produced, the aircraft was delivered to the USAF on 2 May 2012.
In April 2016, the HASC Tactical Air and Land Forces Subcommittee proposed legislation that would direct the Air Force "to conduct a comprehensive assessment and study of the costs associated with resuming production of F-22 aircraft." Defense Secretary Robert Gates had production halted at 187 F-22s (at a cost of $67 billion) to direct funds for ongoing irregular warfare operations in Iraq and Afghanistan. Since then, lawmakers and the Pentagon have noted that air warfare systems of Russia and China are catching up to U.S. air superiority capabilities. The bill notes that Air Combat Command has a stated requirement for 381 F-22s from initial program objectives of 749 aircraft, and would require reviewing of anticipated future air superiority capacity and capability requirements, estimated costs to restart F-22 production, and other measures. In addition to identifying the cost of building another 194 aircraft, the report must also consider the possibility of the 1998 prohibition on the export of the F-22 being repealed. Previous estimates of restarting production placed figures at around $2 billion, including $300–500 million in non-recurring start-up costs, with an estimated unit cost of $233 million for a production run of 75 aircraft over five years. Lockheed has proposed upgrading the 36 early training-model Block 20 Raptors into combat-coded Block 30/35 versions as a way to increase numbers available for deployment.
Upgrades.
The first combat-capable Block 3.0 aircraft first flew in 2001. Increment 2, the first F-22 upgrade program, was implemented in 2005 and enables the aircraft to employ Joint Direct Attack Munitions (JDAM). Increment 3.1 provides improved ground-attack capability through synthetic aperture radar mapping and radio emitter direction finding, electronic attack and the GBU-39 Small Diameter Bomb (SDB); testing began in 2009 and the first upgraded aircraft was delivered in 2012. Increment 3.2 is a two-part upgrade process; 3.2A focuses on electronic warfare, communications and identification, while 3.2B will allow the F-22 to fully exploit the AIM-9X and AIM-120D missiles. The subsequent Increment 3.3 may include the adoption of an open avionics platform and air traffic control updates. Upgrades due in 2015 will allow the F-22 to employ the AIM-9X and have full Link 16 reception and transmission capability, and an upgrade scheduled in 2018 will integrate the AIM-120D into the weapons suite. The F-22 fleet is planned to have 36 Block 20 training and 149 Block 30/35 combat aircraft by 2016.
To enable two-way communication with other platforms, the F-22 can use the Battlefield Airborne Communications Node (BACN) as a gateway. The originally planned MADL integration was cut due to the lack of system maturity. In 2014 Lockheed Martin and Northrop Grumman were competing to connect the F-22 with other platforms while maintaining stealth. Other upgrades being developed include infra-red search and track functionality for the AN/AAR-56 Missile Launch Detector (MLD) and integration of a helmet-mounted cuing system (HMCS) to enable off-boresight missile launches by 2020. Until the F-22 gains a helmet mounted system it will use the AIM-9X's helmetless high off-boresight (HHOBS) capabilities. In March 2010, the USAF accelerated software portions of 3.2 to be completed in FY 2013.
In January 2011, the USAF opened the Raptor enhancement, development and integration (REDI) contract to bidders, with a $16 billion budget. In November 2011, Lockheed Martin's upgrade contract ceiling was raised to $7.4 billion. Nearly $2 billion was allocated for structural repairs and to achieve fleet availability rate of 70.6% by 2015. However, only 63% was achieved. Some F-35 technology, such as more durable stealth coatings, have been applied to the F-22. By 2012, the update schedule had slipped seven years due to instability in requirements and funding. In 2014 the USAF moved to cut upgrade funding.
In 2012 the F-22 was upgraded with a backup oxygen system, software upgrades and oxygen sensors to address the frequent oxygen deprivation issues and normalize operations. In 2013, the faulty flight vest valves were replaced and altitude restrictions lifted; distance restrictions will be lifted once a backup oxygen system is installed. In April 2014 the USAF stated in Congressional testimony that installation of automatic backup oxygen systems on the F-22 fleet would be completed within twelve months.
The F-22 was designed for a lifespan of 30 years and 8,000 flight hours, with a $100 million "structures retrofit program". Investigations are being made for upgrades to extend their useful lives further. In the long term, the F-22 is expected to be superseded by a sixth-generation jet fighter to be fielded in the 2030s.
Design.
Overview.
The F-22 Raptor is a fifth-generation fighter that is considered fourth generation in stealth aircraft technology by the USAF. It is the first operational aircraft to combine supercruise, supermaneuverability, stealth, and sensor fusion in a single weapons platform. The Raptor has clipped delta wings with a reverse sweep on the rear, four empennage surfaces, and a retractable tricycle landing gear. Flight control surfaces include leading and trailing-edge flaps, ailerons, rudders on the canted vertical stabilizers, and all-moving horizontal tails; these surfaces also serve as speed brakes.
The aircraft's dual Pratt & Whitney F119-PW-100 afterburning turbofan engines are closely spaced and incorporate pitch-axis thrust vectoring nozzles with a range of ±20 degrees; each engine has maximum thrust in the 35,000 lbf (156 kN) class. The F-22's thrust to weight ratio in typical combat configuration is nearly at unity in maximum military power and 1.25 in full afterburner. Maximum speed without external stores is estimated to be Mach 1.82 during supercruise and greater than Mach 2 with afterburners.
The F-22 is among only a few aircraft that can supercruise, or sustain supersonic flight without using fuel-inefficient afterburners; it can intercept targets which subsonic aircraft would lack the speed to pursue and an afterburner-dependent aircraft would lack the fuel to reach. The Raptor's high operating altitude is also a significant tactical advantage over prior fighters. The use of internal weapons bays permits the aircraft to maintain comparatively higher performance over most other combat-configured fighters due to a lack of aerodynamic drag from external stores. The F-22's structure contains a significant amount of high-strength materials to withstand stress and heat of sustained supersonic flight. Respectively, titanium alloys and composites comprise 39% and 24% of the aircraft's structural weight.
The F-22 is highly maneuverable at both supersonic and subsonic speeds. Computerized flight control system and full authority digital engine control (FADEC) make the aircraft highly departure resistant and controllable. The Raptor's relaxed stability and powerful thrust-vectoring engines enable the aircraft to turn tightly and perform very high alpha (angle of attack) maneuvers such as the Herbst maneuver (J-turn) and Pugachev's Cobra. The aircraft is also capable of maintaining over 60° alpha while having some roll control.
The Raptor's aerodynamic performance, sensor fusion, and stealth work together for increased effectiveness. Altitude, speed, and advanced active and passive sensors allow the aircraft to spot targets at considerable ranges and increase weapons range; altitude and speed also complement stealth's ability to increase the aircraft's survivability against ground defenses such as surface-to-air missiles.
Avionics.
Key avionics include BAE Systems EI&S AN/ALR-94 radar warning receiver (RWR), Lockheed Martin AN/AAR-56 infrared and ultraviolet Missile Launch Detector (MLD) and Northrop Grumman AN/APG-77 active electronically scanned array (AESA) radar. The MLD features six sensors to provide full spherical infrared coverage. The RWR is a passive radar detector with more than 30 antennas blended into the wings and fuselage for all-round coverage. Tom Burbage, former F-22 program head at Lockheed Martin, described it as "the most technically complex piece of equipment on the aircraft." The range of the RWR (250+ nmi) exceeds the radar's, and can cue radar emissions to be confined to a narrow beam (down to 2° by 2° in azimuth and elevation) to increase stealth. Depending on the detected threat, the defensive systems can prompt the pilot to release countermeasures such as flares or chaff. According to Bill Sweetman, experts had said the ALR-94 can be used as a passive detection system capable of searching targets and providing enough information for a radar lock on.
The AN/APG-77 radar features a low-observable, active-aperture, electronically scanned array that can track multiple targets under any weather conditions. Radar emissions can also be focused to overload enemy sensors as an electronic-attack capability. The radar changes frequencies more than 1,000 times per second to lower interception probability and has an estimated range of 125–150 miles, though planned upgrades will allow a range of or more in narrow beams. Radar information is processed by two Raytheon Common Integrated Processor (CIP)s, each capable of processing up to 10.5 billion instructions per second. In a process known as sensor fusion, data from the radar, other sensors, and external systems is filtered and combined by the CIP into a common view, reducing pilot workload. However, upgrading the aircraft's avionics was reportedly very challenging due to their highly integrated nature.
The F-22's ability to operate close to the battlefield gives the aircraft threat detection and identification capability comparative with the RC-135 Rivet Joint, and the ability to function as a "mini-AWACS", though the radar is less powerful than those of dedicated platforms. The F-22 can designate targets for allies, and determine whether two friendly aircraft are targeting the same aircraft. This radar system can sometimes identify targets "many times quicker than the AWACS". The IEEE 1394B bus developed for the F-22 was derived from the commercial IEEE 1394 "FireWire" bus system. In 2007, the F-22's radar was tested as a wireless data transceiver, transmitting data at 548 megabits per second and receiving at gigabit speed, far faster than the Link 16 system.
The F-22's software has some 1.7 million lines of code, the majority involving processing radar data. Former Secretary of the USAF Michael Wynne blamed the use of the DoD's Ada for cost overruns and delays on many military projects, including the F-22. Cyberattacks on subcontractors have reportedly raised doubts about the security of the F-22's systems and combat-effectiveness. In 2009, former Navy Secretary John Lehman considered the F-22 to be safe from cyberattack, citing the age of its IBM software.
Cockpit.
The F-22 has a glass cockpit with all-digital flight instruments. The monochrome head-up display offers a wide field of view and serves as a primary flight instrument; information is also displayed upon six color liquid-crystal display (LCD) panels. The primary flight controls are a force-sensitive side-stick controller and a pair of throttles. The USAF initially wanted to implement direct voice input (DVI) controls, but this was judged to be too technically risky and was abandoned. The canopy's dimensions are approximately 140 inches long, 45 inches wide, and 27 inches tall (355 cm x 115 cm x 69 cm) and weighs 360 pounds.
The F-22 has integrated radio functionality, the signal processing systems are virtualized rather than as a separate hardware module. There have been several reports on the F-22's inability to communicate with other aircraft, and funding cuts have affected the integration of the Multifunction Advanced Data Link (MADL). Voice communication is possible, but not data transfer.
The integrated control panel (ICP) is a keypad system for entering communications, navigation, and autopilot data. Two up-front displays located around the ICP are used to display integrated caution advisory/warning data, communications, navigation and identification (CNI) data and also serve as the stand-by flight instrumentation group and fuel quantity indicator. The stand-by flight group displays an artificial horizon, for basic instrument meteorological conditions. The primary multi-function display (PMFD) is located under the ICP, and is used for navigation and situation assessment. Three secondary multi-function displays are located around the PMFD for tactical information and stores management.
The ejection seat is a version of the ACES II (Advanced Concept Ejection Seat) commonly used in USAF aircraft, with a center-mounted ejection control. The F-22 has a complex life support system, which includes the on-board oxygen generation system (OBOGS), protective pilot garments, and a breathing regulator/anti-g (BRAG) valve controlling flow and pressure to the pilot's mask and garments. The pilot garments were developed under the Advanced Technology Anti-G Suit (ATAGS) project and are to protect against chemical/biological hazards and cold-water immersion, counter g-forces and low pressure at high altitudes, and provide thermal relief. Suspicions regarding the performance of the OBOGS and life support equipment have been raised by several mishaps, including a fatal crash.
Armament.
The Raptor has three internal weapons bays: a large bay on the bottom of the fuselage, and two smaller bays on the sides of the fuselage, aft of the engine intakes. The main bay can accommodate six LAU-142/A launchers for beyond-visual-range missiles and each side bay has an LAU-141/A launcher for short-range missiles. Four of the launchers in the main bay can be replaced with two bomb racks that can each carry one 1,000 lb (450 kg) or four 250 lb (110 kg) bombs. Carrying armaments internally maintains the aircraft's stealth and minimizes additional drag. Missile launches require the bay doors to be open for less than a second, during which hydraulic arms push missiles clear of the aircraft; this is to reduce vulnerability to detection and to deploy missiles during high speed flight.
The F-22 can also carry air-to-surface weapons such as bombs with Joint Direct Attack Munition (JDAM) guidance and the Small-Diameter Bomb, but cannot self-designate for laser-guided weapons. Internal air-to-surface ordnance is limited to 2,000 lb. An internally mounted M61A2 Vulcan 20 mm cannon is embedded in the right wing root with the muzzle covered by a retractable door to maintain stealth. The radar projection of the cannon fire's path is displayed on the pilot's head-up display.
The F-22's high cruise speed and altitude increase the effective ranges of its munitions, it has 50% greater employment range for the AIM-120 AMRAAM than prior platforms, and range will be further extended with the introduction of the AIM-120D. While specifics are classified, it is expected that JDAMs employed by F-22s will have twice or more the effective range of legacy platforms. In testing, an F-22 dropped a GBU-32 JDAM from 50,000 feet (15,000 m) while cruising at Mach 1.5, striking a moving target away.
While the F-22 typically carries weapons internally, the wings include four hardpoints, each rated to handle . Each hardpoint can accommodate a pylon that can carry a detachable 600 gallon external fuel tank or a launcher holding two air-to-air missiles; the two inboard hardpoints are "plumbed" for external fuel tanks. The use of external stores degrades the aircraft's stealth and kinematic performance; after releasing stores the external attachments can be jettisoned to restore those characteristics. A stealthy ordnance pod and pylon was being developed to carry additional weapons in the mid-2000s.
Stealth.
The F-22 was designed to be highly difficult to detect and track by radar. Measures to reduce radar cross-section include airframe shaping such as alignment of edges, fixed-geometry serpentine inlets that prevent line-of-sight of the engine faces from any exterior view, use of radar-absorbent material (RAM), and attention to detail such as hinges and pilot helmets that could provide a radar return. The F-22 was also designed to have decreased radio emissions, infrared signature and acoustic signature as well as reduced visibility to the naked eye. The aircraft's flat thrust vectoring nozzle reduces infrared emissions to mitigate the threat of infrared homing ("heat seeking") surface-to-air or air-to-air missiles. Additional measures to reduce the infrared signature include special paint and active cooling of leading edges to manage the heat buildup from supersonic flight.
Compared to previous stealth designs like the F-117, the F-22 is less reliant on RAM, which are maintenance-intensive and susceptible to adverse weather conditions. Unlike the B-2, which requires climate-controlled hangars, the F-22 can undergo repairs on the flight line or in a normal hangar. The F-22 features a "Signature Assessment System" which delivers warnings when the radar signature is degraded and necessitates repair. The F-22's exact radar cross-section (RCS) is classified; however, in 2009 Lockheed Martin released information indicating it has an RCS (from certain angles) of −40 dBsm – equivalent to the radar reflection of a "steel marble". Effectively maintaining the stealth features can decrease the F-22's mission capable rate to 62–70%.
The effectiveness of the stealth characteristics is difficult to gauge. The RCS value is a restrictive measurement of the aircraft's frontal or side area from the perspective of a static radar. When an aircraft maneuvers it exposes a completely different set of angles and surface area, potentially increasing radar observability. Furthermore, the F-22's stealth contouring and radar absorbent materials are chiefly effective against high-frequency radars, usually found on other aircraft. The effects of Rayleigh scattering and resonance mean that low-frequency radars such as weather radars and early-warning radars are more likely to detect the F-22 due to its physical size. However, such radars are also conspicuous, susceptible to clutter, and have low precision. Additionally, while faint or fleeting radar contacts make defenders aware that a stealth aircraft is present, reliably vectoring interception to attack the aircraft is much more challenging. According to the USAF an F-22 surprised an Iranian F-4 Phantom II that was attempting to intercept an American UAV, despite Iran's claim of having military VHF radar coverage over the Persian Gulf.
Operational history.
Designation and testing.
The YF-22 was originally given the unofficial name "Lightning II", after the World War II fighter P-38, by Lockheed, which persisted until the mid-1990s when the USAF officially named the aircraft "Raptor". The name "Lightning II" was later given to the F-35. The aircraft was also briefly dubbed "SuperStar" and "Rapier". In September 2002, USAF changed the Raptor's designation to F/A-22, mimicking the Navy's McDonnell Douglas F/A-18 Hornet and intended to highlight a planned ground-attack capability amid debate over the aircraft's role and relevance. The F-22 designation was reinstated in December 2005, when the aircraft entered service.
Flight testing of the F-22 began in 1997 with Raptor 4001, the first EMD jet, and eight more F-22s would participate in the EMD and flight test program. Raptor 4001 was retired from flight testing in 2000 and subsequently sent to Wright-Patterson Air Force Base (AFB) for survivability testing, including live fire testing and battle damage repair training. EMD F-22s have been used for testing upgrades, and also as maintenance trainers. The first production F-22 was delivered to Nellis AFB, Nevada, in January 2003.
In May 2006, a released report documented a problem with the F-22's forward titanium boom, caused by defective heat-treating. This made the boom on roughly the first 80 F-22s less ductile than specified and potentially shortened the part's life. Modifications and inspections were implemented to the booms to restore life expectancy.
In August 2008, an unmodified F-22 of the 411th Flight Test Squadron performed in the first ever air-to-air refueling of an aircraft using synthetic jet fuel as part of a wider USAF effort to qualify aircraft to use the fuel, a 50/50 mix of JP-8 and a Fischer–Tropsch process-produced, natural gas-based fuel. In 2011, an F-22 flew supersonic on a 50% mixture of biofuel derived from camelina.
Introduction into service.
In December 2005, the USAF announced that the F-22 had achieved Initial Operational Capability (IOC). During Exercise Northern Edge in Alaska in June 2006, in simulated combat exercises 12 F-22s of the 94th FS downed 108 adversaries with no losses. In the exercises, the Raptor-led Blue Force amassed 241 kills against two losses in air-to-air combat, with neither loss being an F-22. During Red Flag 07-1 in February 2007, 14 F-22s of the 94th FS supported Blue Force strikes and undertook close air support sorties. Against superior numbers of Red Force Aggressor F-15s and F-16s, 6–8 F-22s maintained air dominance throughout. No sorties were missed because of maintenance or other failures; a single F-22 was judged lost against the defeated opposing force. F-22s also provided airborne electronic surveillance.
The Raptor achieved Full Operational Capability (FOC) in December 2007, when General John Corley of Air Combat Command (ACC) officially declared the F-22s of the integrated active duty 1st Fighter Wing and Virginia Air National Guard 192d Fighter Wing fully operational. This was followed by an Operational Readiness Inspection (ORI) of the integrated wing in April 2008, in which it was rated "excellent" in all categories, with a simulated kill-ratio of 221–0.
Deployments.
F-22 fighter units have been frequently deployed to Kadena Air Base in Okinawa, Japan. In February 2007, on the aircraft's first overseas deployment to Kadena Air Base, six F-22s of 27th Fighter Squadron flying from Hickam AFB, Hawaii, experienced multiple software-related system failures while crossing the International Date Line (180th meridian of longitude). The aircraft returned to Hawaii by following tanker aircraft. Within 48 hours, the error was resolved and the journey resumed. In early 2013, F-22s were involved in U.S.-South Korean military drills.
In November 2007, F-22s of 90th Fighter Squadron at Elmendorf AFB, Alaska, performed their first NORAD interception of two Russian Tu-95MS "Bear-H" bombers. Since then, F-22s have also escorted probing Tu-160 "Blackjack" bombers. The first pair of F-22s assigned to the 49th Fighter Wing became operational at Holloman AFB, New Mexico, in June 2008. In 2014, Holloman F-22s and their support personnel were reassigned to the reactivated 95th Fighter Squadron at Tyndall AFB.
Secretary of Defense Gates initially refused to deploy F-22s to the Middle East in 2007. The type made its first deployment in the region at Al Dhafra Air Base in the UAE in 2009. In April 2012, F-22s have been rotating into Al Dhafra Air Base, less than 200 miles from Iran; the Iranian defense minister referred to the deployment as a security threat. In March 2013 the USAF announced that an F-22 had intercepted an Iranian F-4 Phantom II that approached within 16 miles of an MQ-1 Predator flying off the Iranian coastline.
In June 2014, F-22s from the 199th Fighter Squadron of the Hawaii Air National Guard were deployed to Malaysia to participate in the Cope Taufan 2014 exercise conducted by the USAF Pacific Air Forces and Royal Malaysian Air Force.
On 22 September 2014, F-22s performed the type's first combat sorties during the American-led intervention in Syria; a number of aircraft dropped 1,000-pound GPS-guided bombs on Islamic State targets in the vicinity of Tishrin Dam. Combat operations by F-22s are planned to continue into the foreseeable future. While some missions involve striking targets, the F-22's main role is intelligence, surveillance and reconnaissance (ISR) gathering. By January 2015, the F-22 accounted for three percent of Air Force sorties during Operation Inherent Resolve. General Mike Hostage of ACC said that it performed "flawlessly" during this deployment. Between September 2014 and July 2015, F-22s flew 204 sorties over Syria, dropping 270 bombs at some 60 locations. On 23 June 2015, a pair of F-22s performed the aircraft's first close air support (CAS) mission after receiving a short-notice request for airstrikes in close proximity to friendly forces.
In late 2014, the USAF was testing a rapid deployment concept involving four F-22s and one C-17 for support, first proposed in 2008 by two F-22 pilots. The goal was for the type to be able to set up and engage in combat within 24 hours. Four F-22s were deployed to Spangdahlem Air Base in Germany in August and Lask Air Base in Poland and Amari Air Base in Estonia in September 2015 to train with NATO allies.
Maintenance and training.
F-22 aircraft were available for missions 62% of the time on average in 2004 and 70% in 2009. The rate was at 63% in 2015. Early on, the F-22 required more than 30 hours of maintenance per flight hour and a total cost per flight hour of $44,000; by 2008 it was reduced to 18.1, and 10.5 by 2009; lower than the Pentagon's requirement of 12 maintenance hours per flight hour. When introduced, the F-22 had a Mean Time Between Maintenance (MTBM) of 1.7 hours, short of the required 3.0; in 2012 this rose to 3.2 hours. By 2013, the cost per flight hour was $68,362, over three times as much as the F-16. In 2014, the F-22 fleet required 43 maintenance man-hours per flight hour.
Each aircraft requires a month-long packaged maintenance plan (PMP) every 300 flight hours. The stealth system, including its radar absorbing metallic skin, account for almost one third of maintenance. The canopy was redesigned after the original design lasted an average of 331 hours instead of the required 800 hours. F-22 depot maintenance is performed at Ogden Air Logistics Complex at Hill AFB, Utah.
In January 2007, the F-22 reportedly maintained 97% sortie rate, flying 102 out of 105 tasked sorties while amassing a 144-to-zero kill ratio during "Northern Edge" air-to-air exercises in Alaska. According to Lieutenant Colonel Wade Tolliver, squadron commander of the 27th Fighter Squadron, the stealth coatings of the F-22 are more robust than those used in earlier stealth aircraft, being less sensitive to weather and wear and tear. However, rain caused "shorts and failures in sophisticated electrical components" when F-22s were posted to Guam.
To reduce operating costs and lengthen the F-22's service life, some pilot training sorties are performed using flight simulators, while the T-38 Talon is used for adversary training. DoD budget cuts led to F-22 demonstration flights being halted in 2013 before resuming in 2014. In 2012, it was reported that the F-22's maintenance demands have increased as the fleet aged, the stealth coatings being particularly demanding.
Operational problems.
Operational problems have been experienced and some have caused fleet-wide groundings. Critically, pilots have experienced a decreased mental status, including losing consciousness. There were reports of instances of pilots found to have a decreased level of alertness or memory loss after landing. F-22 pilots have experienced lingering respiratory problems and a chronic cough; other symptoms include irritability, emotional lability and neurological changes. A number of possible causes were investigated, including possible exposure to noxious chemical agents from the respiratory tubing, pressure suit malfunction, side effects from oxygen delivery at greater-than-atmospheric concentrations, and oxygen supply disruptions. Other problems include minor mechanical problems and navigational software failures. The fleet was grounded for four months in 2011 before resuming flight, but reports of oxygen issues persisted.
In 2005, the Raptor Aeromedical Working Group, a USAF expert panel, recommended several changes to deal with the oxygen supply issues. In October 2011, Lockheed Martin was awarded a $24M contract to investigate the breathing difficulties. In July 2012, the Pentagon concluded that a pressure valve on flight vests worn during high-altitude flights and a carbon air filter were likely sources of at least some hypoxia-like symptoms. Long-distance flights were resumed, but were limited to lower altitudes until corrections had been made. The carbon filters were changed to a different model to reduce lung exposure to carbon particulates. The breathing regulator/anti-g (BRAG) valve, used to inflate the pilot's vest during high G maneuvers, was found to be defective, inflating the vest at unintended intervals and restricting the pilot's breathing. The on-board oxygen generating system (OBOGS) also unexpectedly reduced oxygen levels during high-G maneuvers. In late 2012, Lockheed Martin was awarded contracts to install a supplemental automatic oxygen backup system, in addition to the primary and manual backup. Changes recommended by the Raptor Aeromedical Working Group in 2005 received further consideration in 2012; the USAF reportedly considered installing EEG brain wave monitors on the pilot's helmets for inflight monitoring.
New backup oxygen generators and filters have been installed on the aircraft. The coughing symptoms have been attributed to acceleration atelectasis, which may be exacerbated by the F-22's high performance; there is no present solution to the condition. The presence of toxins and particles in some ground crew was deemed to be unrelated. On 4 April 2013, the distance and altitude flight restrictions were lifted after the F-22 Combined Test Force and 412th Aerospace Medicine Squadron determined that breathing restrictions on the pilot were responsible as opposed to an issue with the oxygen provided.
Variants.
Derivatives.
The FB-22 was a proposed medium-range bomber for the USAF. The FB-22 was projected to carry up to 30 Small Diameter Bombs to about twice the range of the F-22A, while maintaining the F-22's stealth and supersonic speed. However, the FB-22 in its planned form appears to have been canceled with the 2006 Quadrennial Defense Review and subsequent developments, in lieu of a larger subsonic bomber with a much greater range.
The X-44 MANTA, or "multi-axis, no-tail aircraft", was a planned experimental aircraft based on the F-22 with enhanced thrust vectoring controls and no aerodynamic surface backup. The aircraft was to be solely controlled by thrust vectoring, without featuring any rudders, ailerons, or elevators. Funding for this program was halted in 2000.
Operators.
Air Combat Command
Air Force Materiel Command
Pacific Air Forces
Air National Guard
Air Force Reserve Command
Accidents.
In April 1992, the second YF-22 crashed while landing at Edwards AFB. The test pilot, Tom Morgenfeld, escaped without injury. The cause of the crash was found to be a flight control software error that failed to prevent a pilot-induced oscillation.
The first F-22 crash occurred during takeoff at Nellis AFB on 20 December 2004, in which the pilot ejected safely before impact. The investigation revealed that a brief interruption in power during an engine shutdown prior to flight caused a flight-control system malfunction; consequently the aircraft design was corrected to avoid the problem. Following a brief grounding, F-22 operations resumed after a review.
On 25 March 2009, an EMD F-22 crashed northeast of Edwards AFB during a test flight, resulting in the death of Lockheed Martin test pilot David P. Cooley. An Air Force Materiel Command investigation found that Cooley momentarily lost consciousness during a high-G maneuver, then ejected when he found himself too low to recover. Cooley was killed during ejection by blunt-force trauma from windblast due to the aircraft's speed. The investigation found no design issues.
On 16 November 2010, an F-22 from Elmendorf AFB crashed, killing the pilot, Captain Jeffrey Haney. F-22s were restricted to flying below 25,000 feet, then grounded during the investigation. The crash was attributed to a bleed air system malfunction after an engine overheat condition was detected, shutting down the Environmental Control System (ECS) and OBOGS. The accident review board ruled Haney was to blame, as he did not react properly and did not engage the emergency oxygen system. Haney's widow sued Lockheed Martin, claiming equipment defects. She later reached a settlement. After the ruling, the engagement handle of the emergency oxygen system was redesigned; the system should engage automatically if OBOGS shuts down due to engine failure. On 11 February 2013, the DoD's Inspector General released a report stating that the USAF had erred in blaming Haney, and that facts did not sufficiently support conclusions; the USAF stated that it stood by the ruling.
During a training mission, an F-22 crashed to the east of Tyndall AFB, on 15 November 2012. The pilot ejected safely and no injuries were reported on the ground. The investigation determined that a "chafed" electrical wire ignited the fluid in a hydraulic line, causing a fire that damaged the flight controls.
Aircraft on display.
EMD F-22A 91-4003 is on display at the National Museum of the United States Air Force.

</doc>
<doc id="66304" url="https://en.wikipedia.org/wiki?curid=66304" title="Yoni">
Yoni

Yoni ( , literally "vagina" or "womb") is the symbol of the Goddess (Shakti or Devi), the Hindu Divine Mother. Within Shaivism, the sect dedicated to the god Shiva, the yoni symbolizes his consort. The male counterpart of the yoni is the lingam. Their union represents the eternal process of creation and regeneration. Since the late 19th century, some have interpreted the yoni and the lingam as aniconic representations of the vulva and a phallus respectively.
In Indian religions.
In Hindu philosophy, according to Tantra, yoni is the origin of life.
In Indian religions according to Vedas and Bhagavad Gita, Yoni is a form of life or a species. There are 8.4 million yonis total with Manushya Yoni (Human form/human species) as one of them. A human (manushya yoni) is obtained on the basis of good karma (deeds) before which a human goes through various forms of yonis (for example, insect, fish, deer, monkey, etc.). Bad karmas will lead one to be born in rakshasa yoni (evil form). The births and rebirths (the cycle of life) of a human happen in various yonis. A human who achieves the enlightenment (Mokshya) breaks the cycle of reincarnation and adjoins Brahma.
Lingam-yoni.
In Sanskrit, Yoni means place of birth, source, origin. Lingam means symbol of Shiva. As Shiva is represented as an endless fire, Lingam-yoni denotes origin of an endless fire which created the universe.
The yoni is the creative power of nature and represents the goddess Shakti. The lingam stone represents Shiva, and is usually placed on the yoni. The lingam is the transcendental source of all that exists. The lingam united with the yoni represents the nonduality of immanent reality and transcendental potentiality.
Worship.
In Shaktism the yoni is celebrated and worshipped during the Ambubachi Mela, an annual fertility festival held in June, in Assam, India, which celebrates the Earth's menstruation. During Ambubachi, the annual menstruation course of the goddess Kamakhya is worshipped in the Kamakhya Temple. The temple stays closed for three days and then reopens to receive pilgrims and worshippers. It is one of the most important pilgrimage sites in the world, attracting millions of visitors each year, particularly for Ambubachi Mela which draws upwards of 100,000 pilgrims per day during the 4-day festival. Darshan at this temple is performed not by sight as in most temples, but by touch. There is a large cleft, a yoni in the bedrock moistened by water flowing upward from an underground spring, generally covered by cloths and ornate chunris, flowers, and red sindoor powder. Devotees and pilgrims offer items for worship directly to the goddess, then touch her and drink water from the spring. They then receive a tilak and prasad by the attending priest. After completing darshan, devotees light lamps and incense outside the temple. Like other temples, worship is not considered complete until the temple is circumambulated clockwise. 
In archeology.
Lingam-yonis have been recovered from the archeological sites at Harappa and Mohenjo-daro, part of the Indus Valley Civilization. There is strong evidence to support cultural continuation from the Indus Valley Civilization (Harappan; Indus-Sarasvati) to Vedic and modern Hindu practices.

</doc>
<doc id="66305" url="https://en.wikipedia.org/wiki?curid=66305" title="Seven Days (TV series)">
Seven Days (TV series)

Seven Days (also written as 7 Days) is a science fiction television series based on the premise of time travel. It was produced by UPN from 1998 to 2001. The television channel Spike syndicated all of the episodes from 2003 to 2005.
"Seven Days" was nominated for six awards, winning one. Actress Justina Vail won a Saturn Award in 2000 for her performance on the show.
Production.
Three seasons of "Seven Days" were produced. All three seasons have been shown in North America, and by the BBC in the United Kingdom. There are currently no plans to release the entire series on DVD and/or Blu-ray from Paramount Home Entertainment and CBS Home Entertainment.
Synopsis.
The plot follows a secret branch of the United States' National Security Agency which has developed a time travelling device based upon alien technology found at Roswell. As the opening of the show says, the Chronosphere, or Backstep Sphere, sends "one human being back in time seven days" to avert disasters. The show's name refers to the fact that the Backstep Project can only "backstep" seven days because of limitations imposed by the fuel source and its reactor. As the fuel source is limited, there is a strict mandate that they only Backstep for events relating to "National Security". The backstep team and the equipment are stationed in a base called "Never Never Land", which is in a secret location somewhere in the desert of Nevada.
Project Backstep.
Project Backstep was initiated by the National Security Agency (NSA) after the Roswell incident of 1947. The crashed alien saucer was taken to a secret base called "Never Never Land" (inspired by Groom Lake in Area 51, nicknamed in real-life as "Dreamland") in the Nevada Desert where it was reverse-engineered.
By reverse-engineering the alien technology, they were able to create a time machine. The "time machine" consists of the Chronosphere, including the Chronosphere's hangar and its supporting equipment.
The time machine uses Element-115 salvaged from the Roswell crash. This transuranic element allowed them to generate a time distortion field around the Chronosphere. The sphere is teleported away from the hangar and into space, after which the chrononaut, Frank Parker, steers the sphere towards the required space and time coordinates (referred to as "flying the needles").
The Time Machine.
Descriptions of how the Chronosphere and the time travel mechanisms work were revealed in the early episodes, and in episodes where Russian time travelers or spies attempt to steal Element-115 or the secrets of the Backstep Project (for example, in Season 3, Episode 21 – "Born in the USSR" and in Season 1, Episode 9 – "As Time Goes By").
While it may appear that the time machine is the Chronosphere itself, this is not the case. The Chronosphere has its own power source for navigation and avionics, but it doesn't produce the time displacement field for time travel (explained in Season 1, Episode 9 – "As Time Goes By").
Instead, the Element-115 fuel source, the reactor and the gravitational field generators are located outside the Chronosphere. A waveguide conduit connects the reactor to the Chronosphere, whereby the gravity wave generated by the Element-115 fuel source is "pumped" towards the sphere. The sphere will then latch on to this gravity wave whereby it is converted into a time displacement field, which is a localized region of spacetime distortion (see Season 1, Episode 9 – "As Time Goes By"). Due to the limited amount of Element-115 fuel, it has to be used sparingly. Furthermore, due to the limitations of the reactor size and output power, the time displacement field has sufficient energy to send the Chronosphere back in time for only seven days (see Season 1, Episodes 1 and 2).
It was also mentioned that they can only go back in time seven days due to the limited quantity of the fuel source. The Element-115 fuel source can "regenerate". It will be depleted after each Backstep, and it will take seven days for the fuel to regenerate to an amount necessary to charge the reactor up to 100 percent (see Season 3, Episode 8 – "Tracker").
However, in another episode – Season 3, Episode 21, "Born in the USSR" – the Soviets mentioned that they encountered a mathematical limitation of the time travel mechanism that they were working on, in which it would appear that time travel is limited to only seven days to the past, and nothing more than that. They mentioned that they were unable to overcome this "barrier".
It was implied that Project Backstep still existed many decades into the future. In Season 2, Episode 23 – "The Cure", an NSA agent from the future traveled far back in time to stop a doctor from developing a cancer cure. He revealed that this cure would mutate and cause an epidemic that brought mankind to the edge of extinction. The NSA of the future salvaged more Element-115 from another saucer crash, and used all the Element-115 fuel that they had at one go, thus enabling the Chronosphere to be transported decades into the past.
The Chronosphere gets damaged by most Backsteps (mostly due to the crash-landings). The NSA will send retrieval teams to recover the sphere and deliver it back to NNL. Welders are constantly seen working on the exterior of the sphere in the NNL hangar.
Significantly, the natural laws of time travel within the series operate to prevent cohabitation of a timeline by more than one version of a person or object: anything arriving from the future replaces its past self. Paradoxically, the effects of the future upon the new arrivals remain in place, creating a paradox that allows information to be sent to the past. For this reason, the on-duty chrononaut is typically confined to base as the absence of the chronosphere and/or its pilot is used as a means of determining a Backstep has taken place (and to prevent civilians from seeing Frank vanish into non-existence when replaced by his future self). This concept however is contradicted in a number of episodes, where Backstep personnel appear to become aware of the Backstep only when Frank calls in as 'Conundrum', his backstepping call sign.
Roswell technology.
In the final episode of the first season (Episode 21, "Lifeboat"), it was revealed that the Roswell crash was actually an alien craft transporting convicts to a penal colony in another solar system when it developed a malfunction and crash-landed on Earth. The aliens were preserved and kept in a secret holding facility deep within Never Never Land, but one of them (nicknamed "Adam") came back to life and attempted to cause a nuclear power plant meltdown.
These aliens are referred to as "Greys" and were said to originate from the Zeta Reticuli system. Donovan explained to Parker in the pilot episode that in order to cross the vast distances between stars, the aliens have developed a propulsion technology that bent space and time (similar to a warp drive).
The NSA have been reverse-engineering the technology left behind from the crash, but was still unsuccessful at replicating a warp drive for faster-than-light travel. They have, however, harnessed the alien fuel source, Element-115, to bend spacetime and the end result was the ability to time travel (explained by Donovan in Season 1, Episode 1).
It would appear that the aliens, who were used to faster-than-light travel, had developed cognitive functions that made them aware of multiple timelines and realities. This was revealed in the first season finale in which the alien Adam was aware of the timelines before and after Parker's Backstep (Season 1, Episode 21 – "Lifeboat").
The aliens also had advanced surgical procedures whereby an implant was able to bridge the signals in a damaged spinal cord, allowing paralyzed aliens to walk again (Season 2, Episode 6 – "Walk Away"). Ballard tried out this implant and it enabled him to walk again, but the neural profile of Adam (from which the implant was taken) was also imprinted in the implant, and it gradually 'leaked' into Ballard's consciousness, effectively causing the residual 'mind' of Adam to possess Ballard.
Time Gremlin.
Other than the Zeta Reticulians, another extraterrestrial entity that was shown on the series was the "Time Gremlin" (Season 2, Episode 11 – "Time Gremlin").
The Chronosphere passed through a wormhole rift just outside the Earth's orbit and was drawn towards it due to its gravitational effects. While lingering around the wormhole's event horizon, an alien creature caught on to the Chronosphere and followed it back to Earth. In transit, the gremlin damaged the sphere, causing Frank Parker to "lose the needles" for the very first time, leading to his first failed Backstep.
It was later revealed that the creature originated from the Hydra system. Ballard somehow traced the origin of the wormhole and suggested to the Backstep team that they should launch the sphere into space again with the hatch open, and let the gremlin return to where it came from.
Soviet time travel projects.
In two episodes (Season 1, Episode 9 – "As Time Goes By" and Season 3, Episode 21 – "Born in the USSR"), it was shown that the Russians had their own time travel projects.
The Soviet time travel project also had possession of Element-115 from a similar alien craft crash in Siberia (Season 3, Episode 21 – "Born in the USSR"). However, Soviet physicists were unable to refine the physics needed to harness the ability of Element-115 for spacetime distortion.
In the episode "As Time Goes By", a Russian time machine from the future returned to the past to visit Project Backstep. The Russian chrononaut (Olga's husband, believed to have died seven years earlier in a failed experiment) tried to steal the Element-115 fuel source and damage the Chronosphere. The Russian time machine was shown to be powered by a "Photon Reactor" that has a similar output to a hydrogen bomb (according to Ballard), something that Ballard is also working on. This allowed the time machine to generate sufficient power to create its own localized time displacement field without relying on Element-115. The Russian chrononaut also claimed that his time machine is able to travel forward in time, not just backward.
Awards.
"Seven Days" was nominated for six awards, winning one. Actress Justina Vail won a Saturn Award in 2000 for her performance on the show.

</doc>
<doc id="66306" url="https://en.wikipedia.org/wiki?curid=66306" title="Osmium tetroxide">
Osmium tetroxide

Osmium tetroxide (also osmium(VIII) oxide) is the chemical compound with the formula OsO4. The compound is noteworthy for its many uses, despite the rarity of osmium. It also has a number of interesting properties, one being that the solid is volatile. The compound is colourless, but most samples appear yellow. This is most likely due to the presence of the impurity OsO2, which is yellow-brown in colour.
Physical properties.
Osmium(VIII) oxide forms monoclinic crystals. It has a characteristic acrid chlorine-like odor. The element name osmium is derived from "osme", Greek for "odor". OsO4 is volatile: it sublimes at room temperature. It is soluble in a wide range of organic solvents. It is also moderately soluble in water, with which it reacts reversibly to form osmic acid (see below). "Pure" osmium(VIII) oxide is probably colourless and it has been suggested that its yellow hue is due to osmium dioxide (OsO2) impurities. The osmium tetroxide molecule is tetrahedral and therefore non-polar. This nonpolarity helps OsO4 penetrate charged cell membranes. OsO4 is 518 times more soluble in carbon tetrachloride than in water.
Structure and electron configuration.
The osmium of OsO4 has an oxidation number of VIII, however the metal does not possess a corresponding 8+ charge as the bonding in the compound is largely covalent in character (the ionization energy required to produce a formal 8+ charge also far exceeds the energies available in normal chemical reactions). The osmium atom has eight valence electrons (6s2, 5d6) with double bonds to the four oxide ligands resulting in a 16 electron complex. This is isoelectronic with permanganate and chromate ions.
Synthesis.
OsO4 is formed slowly when osmium powder reacts with O2 at ambient temperature. Reaction of bulk solid requires heating to 400 °C.
formula_1
Reactions.
Oxidation of alkenes.
Alkenes add to OsO4 to give diolate species that hydrolyze to "cis"-diols. The net process is called dihydroxylation. This proceeds via a + 2 cycloaddition reaction between the OsO4 and alkene to form an intermediate osmate ester which rapidly hydrolyses to yield the vicinal diol. As the oxygen atoms are added in a concerted step the resulting stereochemistry is "cis".
OsO4 is expensive and highly toxic, making it an unappealing reagent to use in stoichiometric amounts. However its reactions are made catalytic by adding reagents to reoxidise the Os(VI) by-product back to Os(VIII). Typical reagents include H2O2 (Milas hydroxylation), N-methylmorpholine N-oxide (Upjohn dihydroxylation) and K3Fe(CN)6, as these will not react with the alkenes on their own. Other osmium compounds can be used as catalysts, including osmate(VI) salts ([OsO2(OH)4)]2−, and osmium trichloride hydrate (OsCl3·"x"H2O). These species oxidise to osmium(VIII) in the presence of such oxidants.
Lewis bases such as tertiary amines and pyridines increase the rate of dihydroxylation. This "ligand-acceleration" arises via the formation of adduct OsO4L, which adds more rapidly to the alkene. If the amine is chiral, then the dihydroxylation can proceed with enantioselectivity (see Sharpless asymmetric dihydroxylation). OsO4 does not react with most carbohydrates.
The process can be extended to give to aldehydes in the Lemieux–Johnson oxidation, which uses periodate to achieve diol cleavage and to regenerate the catalytic loading of OsO4. This process is equivalent to that of ozonolysis.
Coordination chemistry.
OsO4 is a Lewis acid and a mild oxidant. Most of its reactions reflect this pattern. It reacts with alkaline aqueous solution to give the perosmate anion OsO4(OH)22−. This species is easily reduced to osmate anion, OsO2(OH)44−.
When the Lewis base is an amine, adducts are also formed. With tert-BuNH2 the imido derivative is produced:
Similarly, with NH3 one obtains the nitrido complex:
The [Os(N)O3]− anion is isoelectronic and isostructural with OsO4.
OsO4 is very soluble in tert-butyl alcohol and in solution is readily reduced by molecular hydrogen to osmium metal. The suspended osmium metal can be used to catalyze hydrogenation of a wide variety of organic chemicals containing double or triple bonds.
OsO4 undergoes "reductive carbonylation" with carbon monoxide in methanol at 400 K and 200 bar of pressure to produce the triangular cluster Os3(CO)12:
Oxofluorides.
Osmium forms several oxofluorides, all of which are very sensitive to moisture.
Purple "cis"-OsO2F4 forms at 77 K in an anhydrous HF solution:
OsO4 also reacts with F2 to form yellow OsO3F2:
OsO4 reacts with one equivalent of [Me4N]F at 298 K and 2 equivalents at 253 K:
Uses.
Organic synthesis.
In organic synthesis OsO4 is widely used to oxidise alkenes to the vicinal diols, adding two hydroxyl groups at the same side (syn addition). See reaction and mechanism above. This reaction has been made both catalytic (Upjohn dihydroxylation) and asymmetric (Sharpless asymmetric dihydroxylation).
Osmium(VIII) oxide is also used in catalytic amounts in the Sharpless oxyamination to give vicinal amino-alcohols.
In combination with sodium periodate, OsO4 is used for the oxidative cleavage of alkenes (Lemieux-Johnson oxidation) when the periodate serves both to cleave the diol formed by dihydroxylation, and to reoxidize the OsO3 back to OsO4. The net transformation is identical to that produced by ozonolysis. Below an example from the total synthesis of Isosteviol.
Biological staining.
OsO4 is a widely used staining agent used in transmission electron microscopy (TEM) to provide contrast to the image. As a lipid stain, it is also useful in scanning electron microscopy (SEM) as an alternative to sputter coating. It embeds a heavy metal directly into cell membranes, creating a high electron scattering rate without the need for coating the membrane with a layer of metal, which can obscure details of the cell membrane. In the staining of the plasma membrane, osmium(VIII) oxide binds phospholipid head regions, thus creating contrast with the neighbouring protoplasm (cytoplasm). Additionally, osmium(VIII) oxide is also used for fixing biological samples in conjunction with HgCl2. Its rapid killing abilities are used to quickly kill live specimens such as protozoa. OsO4 stabilizes many proteins by transforming them into gels without destroying structural features. Tissue proteins that are stabilized by OsO4 are not coagulated by alcohols during dehydration. Osmium(VIII) oxide is also used as a stain for lipids in optical microscopy. OsO4 also stains the human cornea (see safety considerations).
Polymer staining.
It is also used to stain copolymers preferentially, the best known example being block copolymers where one phase can be stained so as to show the microstructure of the material. For example, styrene-butadiene block copolymers have a central polybutadiene chain with polystyrene end caps. When treated with OsO4, the butadiene matrix reacts preferentially and so absorbs the oxide. The presence of a heavy metal is sufficient to block the electron beam, so the polystyrene domains are seen clearly in thin films in TEM.
Osmeth.
OsO4 can be stored in the form of osmeth, a golden crystalline solid which is OsO4 complexed with hexamine; in this form, it does not emit toxic fumes. Osmeth can be dissolved in tetrahydrofuran (THF) and diluted in an aqueous buffer solution to make a dilute (0.25%) working solution of OsO4.
Osmium ore refining.
OsO4 is an intermediate in osmium ore refining. Osmium residues are reacted with Na2O2 forming anions, which, when reacted with chlorine (Cl2) gas and heated, form OsO4. The oxide is dissolved in alcoholic NaOH forming [OsO2(OH)42− anions, which, when reacted with NH4Cl, forms OsO2Cl2(NH4)4. This is ignited under hydrogen (H2) gas leaving behind pure osmium (Os).
Buckminsterfullerene adduct.
OsO4 allowed for the confirmation of the soccer ball model of buckminsterfullerene, a 60 atom carbon allotrope. The adduct, formed from a derivative of OsO4, was C60(OsO4)(4-"tert"-butylpyridine)2. The adduct broke the fullerene's symmetry allowing for crystallization and confirmation of the structure of C60 by X-ray crystallography.
Safety considerations.
4 is highly poisonous, even at low exposure levels, and must be handled with appropriate precautions. In particular, inhalation at concentrations well below those at which a smell can be perceived can lead to pulmonary edema and subsequent death. Noticeable symptoms can take hours to appear after exposure.
OsO4 also stains the human cornea, which can lead to blindness if proper safety precautions are not observed. The permissible exposure limit for osmium(VIII) oxide (8 hour time-weighted average) is 200 µg/m3. Osmium(VIII) oxide can penetrate plastics and therefore is stored in glass under refrigeration.
On April 6, 2004 British intelligence sources believed they had foiled a plot to detonate a bomb involving OsO4. Experts interviewed by "New Scientist" affirmed osmium(VIII) oxide's toxicity, though some highlighted the difficulties of using it in a weapon: osmium(VIII) oxide is very expensive. The osmium(VIII) oxide may be destroyed by the blast; remaining toxic fumes may also be dispersed by the blast.

</doc>
<doc id="66313" url="https://en.wikipedia.org/wiki?curid=66313" title="Redox">
Redox

Redox is a contraction of the name for a chemical reduction–oxidation reaction. A reduction reaction always occurs with an oxidation reaction. Redox reactions include all chemical reactions in which atoms have their oxidation state changed; in general, redox reactions involve the transfer of electrons between chemical species. The chemical species from which the electron is stripped is said to have been oxidized, while the chemical species to which the electron is added is said to have been reduced. Oxygen is not necessarily included in such reactions as other chemical species can serve the same function.
The term "redox" comes from two concepts involved with electron transfer: reduction and oxidation. It can be explained in simple terms:
Although oxidation reactions are commonly associated with the formation of oxides from oxygen molecules, these are only specific examples of a more general concept of reactions involving electron transfer.
Redox reactions, or oxidation-reduction reactions, have a number of similarities to acid–base reactions. Like acid–base reactions, redox reactions are a matched set, that is, there cannot be an oxidation reaction without a reduction reaction happening simultaneously. The oxidation alone and the reduction alone are each called a "half-reaction", because two half-reactions always occur together to form a whole reaction. When writing half-reactions, the gained or lost electrons are typically included explicitly in order that the half-reaction be balanced with respect to electric charge.
Though sufficient for many purposes, these descriptions are not precisely correct. Although oxidation and reduction properly refer to "a change in oxidation state" — the actual transfer of electrons may never occur. The oxidation state of an atom is the fictitious charge that an atom would have if all bonds between atoms of different elements were 100% ionic. Thus, oxidation is best defined as an "increase in oxidation state", and reduction as a "decrease in oxidation state". In practice, the transfer of electrons will always cause a change in oxidation state, but there are many reactions that are classed as "redox" even though no electron transfer occurs (such as those involving covalent bonds).
There are simple redox processes, such as the oxidation of carbon to yield carbon dioxide (CO2) or the reduction of carbon by hydrogen to yield methane (CH4), and more complex processes such as the oxidation of glucose (C6H12O6) in the human body through a series of complex electron transfer processes.
Etymology.
"Redox" is a combination of "reduction" and "oxidation".
The word "oxidation" originally implied reaction with oxygen to form an oxide, since dioxygen (O2 (g)) was historically the first recognized oxidizing agent. Later, the term was expanded to encompass oxygen-like substances that accomplished parallel chemical reactions. Ultimately, the meaning was generalized to include all processes involving loss of electrons.
The word "reduction" originally referred to the loss in weight upon heating a metallic ore such as a metal oxide to extract the metal. In other words, ore was "reduced" to metal. Antoine Lavoisier (1743–1794) showed that this loss of weight was due to the loss of oxygen as a gas. Later, scientists realized that the metal atom gains electrons in this process. The meaning of "reduction" then became generalized to include all processes involving gain of electrons. Even though "reduction" seems counter-intuitive when speaking of the gain of electrons, it might help to think of reduction as the loss of oxygen, which was its historical meaning.
The electrochemist John Bockris has used the words "electronation" and "deelectronation" to describe reduction and oxidation processes respectively when they occur at electrodes. These words are analogous to protonation and deprotonation, but they have not been widely adopted by chemists.
The term "hydrogenation" could be used instead of reduction, since hydrogen is the reducing agent in a large number of reactions, especially in organic chemistry and biochemistry. But, unlike oxidation, which has been generalized beyond its root element, hydrogenation has maintained its specific connection to reactions that "add" hydrogen to another substance (e.g., the hydrogenation of unsaturated fats into saturated fats, R−CH=CH−R + H2 → R−CH2−CH2−R). The word "redox" was first used in 1928.
Oxidizing and reducing agents.
In redox processes, the reductant transfers electrons to the oxidant. Thus, in the reaction, the reductant or "reducing agent" loses electrons and is oxidized, and the oxidant or "oxidizing agent" gains electrons and is reduced. The pair of an oxidizing and reducing agent that are involved in a particular reaction is called a redox pair. A redox couple is a reducing species and its corresponding oxidized form, e.g., Fe2+/Fe3+.
Oxidizers.
Substances that have the ability to oxidize other substances (cause them to lose electrons) are said to be oxidative or oxidizing and are known as oxidizing agents, oxidants, or oxidizers. That is, the oxidant (oxidizing agent) removes electrons from another substance, and is thus itself reduced. And, because it "accepts" electrons, the oxidizing agent is also called an electron acceptor. Oxygen is the quintessential oxidizer.
Oxidants are usually chemical substances with elements in high oxidation states (e.g., , , , , ), or else highly electronegative elements (O2, F2, Cl2, Br2) that can gain extra electrons by oxidizing another substance.
Reducers.
Substances that have the ability to reduce other substances (cause them to gain electrons) are said to be reductive or reducing and are known as reducing agents, reductants, or reducers. The reductant (reducing agent) transfers electrons to another substance, and is thus itself oxidized. And, because it "donates" electrons, the reducing agent is also called an electron donor. Electron donors can also form charge transfer complexes with electron acceptors.
Reductants in chemistry are very diverse. Electropositive elemental metals, such as lithium, sodium, magnesium, iron, zinc, and aluminium, are good reducing agents. These metals donate or "give away" electrons readily. "Hydride transfer reagents", such as NaBH4 and LiAlH4, are widely used in organic chemistry, primarily in the reduction of carbonyl compounds to alcohols. Another method of reduction involves the use of hydrogen gas (H2) with a palladium, platinum, or nickel catalyst. These "catalytic reductions" are used primarily in the reduction of carbon-carbon double or triple bonds.
Standard electrode potentials (reduction potentials).
Each half-reaction has a "standard electrode potential" ("E"), which is equal to the potential difference or voltage at equilibrium under standard conditions of an electrochemical cell in which the cathode reaction is the half-reaction considered, and the anode is a standard hydrogen electrode where hydrogen is oxidized:
The electrode potential of each half-reaction is also known as its "reduction potential" "E", or potential when the half-reaction takes place at a cathode. The reduction potential is a measure of the tendency of the oxidizing agent to be reduced. Its value is zero for H+ + e− →  H2 by definition, positive for oxidizing agents stronger than H+ (e.g., +2.866 V for F2) and negative for oxidizing agents that are weaker than H+ (e.g., −0.763 V for Zn2+).
For a redox reaction that takes place in a cell, the potential difference is:
However, the potential of the reaction at the anode was sometimes expressed as an "oxidation potential":
The oxidation potential is a measure of the tendency of the reducing agent to be oxidized, but does not represent the physical potential at an electrode. With this notation, the cell voltage equation is written with a plus sign
Examples of redox reactions.
A good example is the reaction between hydrogen and fluorine in which hydrogen is being oxidized and fluorine is being reduced:
We can write this overall reaction as two half-reactions:
the oxidation reaction:
and the reduction reaction:
Analyzing each half-reaction in isolation can often make the overall chemical process clearer. Because there is no net change in charge during a redox reaction, the number of electrons in excess in the oxidation reaction must equal the number consumed by the reduction reaction (as shown above).
Elements, even in molecular form, always have an oxidation state of zero. In the first half-reaction, hydrogen is oxidized from an oxidation state of zero to an oxidation state of +1. In the second half-reaction, fluorine is reduced from an oxidation state of zero to an oxidation state of −1.
When adding the reactions together the electrons are canceled:
And the ions combine to form hydrogen fluoride:
The overall reaction is:
Metal displacement.
In this type of reaction, a metal atom in a compound (or in a solution) is replaced by an atom of another metal. For example, copper is deposited when zinc metal is placed in a copper(II) sulfate solution:
Zn(s)+ CuSO4(aq) → ZnSO4(aq) + Cu(s)
In the above reaction, zinc metal displaces the copper(II) ion from copper sulfate solution and thus liberates free copper metal.
The ionic equation for this reaction is:
As two half-reactions, it is seen that the zinc is oxidized:
And the copper is reduced:
Redox reactions in industry.
Cathodic protection is a technique used to control the corrosion of a metal surface by making it the cathode of an electrochemical cell. A simple method of protection connects protected metal to a more easily corroded "sacrificial anode" to act as the anode. The sacrificial metal instead of the protected metal, then, corrodes. A common application of cathodic protection is in galvanized steel, in which a sacrificial coating of zinc on steel parts protects them from rust.
The primary process of reducing ore at high temperature to produce metals is known as smelting.
Oxidation is used in a wide variety of industries such as in the production of cleaning products and oxidizing ammonia to produce nitric acid, which is used in most fertilizers.
Redox reactions are the foundation of electrochemical cells.
The process of electroplating uses redox reactions to coat objects with a thin layer of a material, as in chrome-plated automotive parts, silver plating cutlery, and gold-plated jewelry.
The production of compact discs depends on a redox reaction, which coats the disc with a thin layer of metal film.
Redox reactions in biology.
Top: ascorbic acid (reduced form of Vitamin C)Bottom: dehydroascorbic acid (oxidized form of Vitamin C)
Many important biological processes involve redox reactions.
Cellular respiration, for instance, is the oxidation of glucose (C6H12O6) to CO2 and the reduction of oxygen to water. The summary equation for cell respiration is:
The process of cell respiration also depends heavily on the reduction of NAD+ to NADH and the reverse reaction (the oxidation of NADH to NAD+). Photosynthesis and cellular respiration are complementary, but photosynthesis is not the reverse of the redox reaction in cell respiration:
Biological energy is frequently stored and released by means of redox reactions. Photosynthesis involves the reduction of carbon dioxide into sugars and the oxidation of water into molecular oxygen. The reverse reaction, respiration, oxidizes sugars to produce carbon dioxide and water. As intermediate steps, the reduced carbon compounds are used to reduce nicotinamide adenine dinucleotide (NAD+), which then contributes to the creation of a proton gradient, which drives the synthesis of adenosine triphosphate (ATP) and is maintained by the reduction of oxygen.
In animal cells, mitochondria perform similar functions. See the "Membrane potential" article.
Free radical reactions are redox reactions that occur as a part of homeostasis and killing microorganisms, where an electron detaches from a molecule and then reattaches almost instantaneously. Free radicals are a part of redox molecules and can become harmful to the human body if they do not reattach to the redox molecule or an antioxidant. Unsatisfied free radicals can spur the mutation of cells they encounter and are, thus, causes of cancer.
The term redox state is often used to describe the balance of GSH/GSSG, NAD+/NADH and NADP+/NADPH in a biological system such as a cell or organ. The redox state is reflected in the balance of several sets of metabolites (e.g., lactate and pyruvate, beta-hydroxybutyrate, and acetoacetate), whose interconversion is dependent on these ratios. An abnormal redox state can develop in a variety of deleterious situations, such as hypoxia, shock, and sepsis. Redox mechanism also control some cellular processes. Redox proteins and their genes must be co-located for redox regulation according to the CoRR hypothesis for the function of DNA in mitochondria and chloroplasts.
Redox cycling.
A wide variety of aromatic compounds are enzymatically reduced to form free radicals that contain one more electron than their parent compounds. In general, the electron donor is any of a wide variety of flavoenzymes and their coenzymes. Once formed, these anion free radicals reduce molecular oxygen to superoxide, and regenerate the unchanged parent compound. The net reaction is the oxidation of the flavoenzyme's coenzymes and the reduction of molecular oxygen to form superoxide. This catalytic behavior has been described as futile cycle or redox cycling.
Examples of redox cycling-inducing molecules are the herbicide paraquat and other viologens and quinones such as menadione.
Redox reactions in geology.
In geology, redox is important to both the formation of minerals and the mobilization of minerals, and is also important in some depositional environments. In general, the redox state of most rocks can be seen in the color of the rock. The rock forms in oxidizing conditions, giving it a red color. It is then "bleached" to a green—or sometimes white—form when a reducing fluid passes through the rock. The reduced fluid can also carry uranium-bearing minerals. Famous examples of redox conditions affecting geological processes include uranium deposits and Moqui marbles.
Balancing redox reactions.
Describing the overall electrochemical reaction for a redox process requires a "balancing" of the component half-reactions for oxidation and reduction. In general, for reactions in aqueous solution, this involves adding H+, OH−, H2O, and electrons to compensate for the oxidation changes.
Acidic media.
In acidic media, H+ ions and water are added to half-reactions to balance the overall reaction.
For instance, when manganese(II) reacts with sodium bismuthate:
The reaction is balanced by scaling the two half-cell reactions to involve the same number of electrons (multiplying the oxidation reaction by the number of electrons in the reduction step and vice versa):
Adding these two reactions eliminates the electrons terms and yields the balanced reaction:
Basic media.
In basic media, OH− ions and water are added to half reactions to balance the overall reaction.
For example, in the reaction between potassium permanganate and sodium sulfite:
Balancing the number of electrons in the two half-cell reactions gives:
Adding these two half-cell reactions together gives the balanced equation:
Memory aids.
The key terms involved in redox are often confusing to students. For example, an element that is oxidized loses electrons; however, that element is referred to as the reducing agent. Likewise, an element that is reduced gains electrons and is referred to as the oxidizing agent. Acronyms or mnemonics are commonly used to help remember the terminology:
References.
Notes
Bibliography

</doc>
<doc id="66315" url="https://en.wikipedia.org/wiki?curid=66315" title="Solid-state chemistry">
Solid-state chemistry

Solid-state chemistry, also sometimes referred to as materials chemistry, is the study of the synthesis, structure, and properties of solid phase materials, particularly, but not necessarily exclusively of, non-molecular solids. It therefore has a strong overlap with solid-state physics, mineralogy, crystallography, ceramics, metallurgy, thermodynamics, materials science and electronics with a focus on the synthesis of novel materials and their characterization.
History.
Because of its direct relevance to products of commerce, solid state inorganic chemistry has been strongly driven by technology. Progress in the field has often been fueled by the demands of industry, well ahead of purely academic curiosity. Applications discovered in the 20th century include zeolite and platinum-based catalysts for petroleum processing in the 1950s, high-purity silicon as a core component of microelectronic devices in the 1960s, and “high temperature” superconductivity in the 1980s. The invention of X-ray crystallography in the early 1900s by William Lawrence Bragg enabled further innovation. Our understanding of how reactions proceed at the atomic level in the solid state was advanced considerably by Carl Wagner's work on oxidation rate theory, counter diffusion of ions, and defect chemistry. Because of this, he has sometimes been referred to as the "father of solid state chemistry".
Synthetic methods.
Given the diversity of solid state compounds, an equally diverse array of methods are used for their preparation. For organic materials, such as charge transfer salts, the methods operate near room temperature and are often similar to the techniques of organic synthesis. Redox reactions are sometimes conducted by electrocrystallisation, as illustrated by the preparation of the Bechgaard salts from tetrathiafulvalene.
Oven techniques.
For thermally robust materials, high temperature methods are often employed. For example, bulk solids are prepared using tube furnaces, which allow reactions to be conducted up to ca. 1100 °C. Special equipment e.g. ovens consisting of a tantalum tube through which an electric current is passed can be used for even higher temperatures up to 2000 °C. Such high temperatures are at times required to induce diffusion of the reactants, but this depends strongly on the system studied. Some solid state reactions already proceed at temperatures as low as 100 °C.
Melt methods.
One method often employed is to melt the reactants together and then later anneal the solidified melt. If volatile reactants are involved the reactants are often put in an ampoule that is evacuated -often while keeping the reactant mixture cold e.g. by keeping the bottom of the ampoule in liquid nitrogen- and then sealed. The sealed ampoule is then put in an oven and given a certain heat treatment.
Solution methods.
It is possible to use solvents to prepare solids by precipitation or by evaporation. At times the solvent is used hydrothermally, i.e. under pressure at temperatures higher than the normal boiling point. A variation on this theme is the use of flux methods, where a salt of relatively low melting point is added to the mixture to act as a high temperature solvent in which the desired reaction can take place.
Gas reactions.
Many solids react vigorously with reactive gas species like chlorine, iodine, oxygen etc. Others form adducts with other gases, e.g. CO or ethylene. Such reactions are often carried out in a tube that is open ended on both sides and through which the gas is passed. A variation of this is to let the reaction take place inside a measuring device such as a TGA. In that case stoichiometric information can be obtained during the reaction, which helps identify the products.
A special case of a gas reaction is a chemical transport reaction. These are often carried out in a sealed ampoule to which a small amount of a transport agent, e.g. iodine is added. The ampoule is then placed in a zone oven. This is essentially two tube ovens attached to each other which allows a temperature gradient to be imposed. Such a method can be used to obtain the product in the form of single crystals suitable for structure determination by X-ray diffraction.
Chemical vapour deposition is a high temperature method that is widely employed for the preparation of coatings and semiconductors from molecular precursors.
Air and moisture sensitive materials.
Many solids are hygroscopic and/or oxygen sensitive. Many halides e.g. are very 'thirsty' and can only be studied in their anhydrous form if they are handled in a glove box filled with dry (and/or oxygen-free) gas, usually nitrogen.
Characterization.
New phases, phase diagrams, structures.
The synthetic methodology and the characterization of the product often go hand in hand in the sense that not one but a series of reaction mixtures are prepared and subjected to heat treatment. The stoichiometry is typically "varied" in a systematic way to find which stoichiometries will lead to new solid compounds or to solid solutions between known ones. A prime method to characterize the reaction products is powder diffraction, because many solid state reactions will produce polycristalline ingots or powders. Powder diffraction will facilitate the identification of known phases in the mixture. If a pattern is found that is not known in the diffraction data libraries an attempt can be made to index the pattern, i.e. to identify the symmetry and the size of the unit cell. (If the product is not crystalline the characterization is typically much more difficult.)
Once the unit cell of a new phase is known, the next step is to establish the stoichiometry of the phase. This can be done in a number of ways. Sometimes the composition of the original mixture will give a clue, if one finds only one product -a single powder pattern- or if one was trying to make a phase of a certain composition by analogy to known materials but this is rare.
Often considerable effort in refining the synthetic methodology is required to obtain a pure sample of the new material.
If it is possible to separate the product from the rest of the reaction mixture elemental analysis can be used. Another way involves SEM and the generation of characteristic X-rays in the electron beam. The easiest way to solve the structure is by using single crystal X-ray diffraction.
The latter often requires "revisiting" and refining the preparative procedures and that is linked to the question which phases are stable at what composition and what stoichiometry. In other words, what does the phase diagram looks like. An important tool in establishing this is thermal analysis techniques like DSC or DTA and increasingly also, thanks to the advent of synchrotrons temperature-dependent powder diffraction. Increased knowledge of the phase relations often leads to further refinement in synthetic procedures in an iterative way. New phases are thus characterized by their melting points and their stoichiometric domains. The latter is important for the many solids that are non-stoichiometric compounds. The cell parameters obtained from XRD are particularly helpful to characterize the homogeneity ranges of the latter.
Further characterization.
In many -but certainly not all- cases new solid compounds are further characterized by a variety of techniques that straddle the fine line that (hardly) separates solid-state chemistry from solid-state physics.
Optical properties.
For non-metallic materials it is often possible to obtain UV/VIS spectra. In the case of semiconductors that will give an idea of the band gap.

</doc>
<doc id="66316" url="https://en.wikipedia.org/wiki?curid=66316" title="Burette">
Burette

A burette (also buret) is a device used in analytical chemistry for the dispensing of variable, measured amounts of a chemical solution. A volumetric burette delivers measured volumes of liquid. Piston burettes are similar to syringes, but with precision bore and plunger. Piston burettes may be manually operated or may be motorized. A weight burette delivers measured weights of liquid.
Overview.
A burette is distinguished from a pipette by the fact that the quantity delivered is variable. Thus in a titration, one solution is dispensed with a pipette, and another solution is added to it from a burette in aliquots of varying size.
Burettes may be designated for use at a particular temperature. If used at another temperature they should be subject to calibration.
Volumetric burettes.
Analogue.
A traditional burette consists of glass tube of constant bore with a graduation scale etched on it and a stopcock at the bottom. The barrel of the stopcock may be made of glass or the plastic PTFE. Stopcocks with glass barrels need to be lubricated with vaseline or a specialized grease. Burettes are manufactured to specified tolerances, designated as class A or B and this also is etched on the glass.
Digital.
Digital burettes are based on a syringe design. The barrel and plunger may be made of glass. With liquids that corrode glass, including solutions of alkali, the barrel and plunger may be made of polyethylene or another resistant plastic material. The barrel is held in a fixed position and the plunger is moved incrementally either by turning a ratcheted wheel by hand, or by means of a step-motor. The volume is shown on a digital display. A high-precision syringe may be used to deliver very precise aliquots. Motorized digital burettes may be controlled by computer; for example, a titration may be recorded digitally and then subject to numerical processing to find the titre at an end-point.

</doc>
<doc id="66317" url="https://en.wikipedia.org/wiki?curid=66317" title="Valence">
Valence

Valence or valency may refer to:
Science.
In several fields the valence of an element refers to the number of elements to which it can connect: 

</doc>
<doc id="66320" url="https://en.wikipedia.org/wiki?curid=66320" title="Romanian">
Romanian

Romanian may refer to:

</doc>
<doc id="66321" url="https://en.wikipedia.org/wiki?curid=66321" title="Group 6 element">
Group 6 element

Group 6, numbered by IUPAC style, is a group of elements in the periodic table. Its members are chromium (Cr), molybdenum (Mo), tungsten (W), and seaborgium (Sg). These are all transition metals and chromium, molybdenum and tungsten are refractory metals. The period 8 elements of group 6 are likely to be either unpenthexium (Uph) or unpentoctium (Upo). This may not be possible; drip instability may imply that the periodic table ends at unbihexium. Neither unpenthexium nor unpentoctium have been synthesized, and it is unlikely that this will happen in the near future.
Like other groups, the members of this family show patterns in its electron configuration, especially the outermost shells resulting in trends in chemical behavior:
"Group 6" is the new IUPAC name for this group; the old style name was ""group VIB"" in the old US system (CAS) or ""group VIA"" in the European system (old IUPAC). Group 6 must not be confused with the group with the old-style group crossed names of either "VIA" (US system, CAS) or "VIB" (European system, old IUPAC). "That" group is now called group 16.
History.
Discoveries.
Chromium was first reported on July 26, 1761, when Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains of Russia, which he named "Siberian red lead," which was found out in less than 10 years to be a bright yellow pigment. Though misidentified as a lead compound with selenium and iron components, the mineral was crocoite with a formula of PbCrO4. Studying the mineral in 1797, Louis Nicolas Vauquelin produced chromium trioxide by mixing crocoite with hydrochloric acid metallic chromium by heating the oxide in a charcoal oven a year later. He was also able to detect traces of chromium in precious gemstones, such as ruby or emerald.
Molybdenite—the principal ore from which molybdenum is now extracted—was previously known as molybdena, which was confused with and often implemented as though it were graphite. Like graphite, molybdenite can be used to blacken a surface or as a solid lubricant. Even when molybdena was distinguishable from graphite, it was still confused with a galena (a common lead ore), which took its name from Ancient Greek "", meaning "lead". It was not until 1778 that Swedish chemist Carl Wilhelm Scheele realized that molybdena was neither graphite nor lead. He and other chemists then correctly assumed that it was the ore of a distinct new element, named "molybdenum" for the mineral in which it was discovered. Peter Jacob Hjelm successfully isolated molybdenum by using carbon and linseed oil in 1781.
Regarding tungsten, in 1781 Carl Wilhelm Scheele discovered that a new acid, tungstic acid, could be made from scheelite (at the time named tungsten). Scheele and Torbern Bergman suggested that it might be possible to obtain a new metal by reducing this acid. In 1783, José and Fausto Elhuyar found an acid made from wolframite that was identical to tungstic acid. Later that year, in Spain, the brothers succeeded in isolating tungsten by reduction of this acid with charcoal, and they are credited with the discovery of the element.
Historical development and uses.
During the 1800s, chromium was primarily used as a component of paints and in tanning salts. At first, crocoite from Russia was the main source, but in 1827, a larger chromite deposit was discovered near Baltimore, United States. This made the United States the largest producer of chromium products until 1848 when large deposits of chromite where found near Bursa, Turkey. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.
For about a century after its isolation, molybdenum had no industrial use, owing to its relative scarcity, difficulty extracting the pure metal, and the immaturity of the metallurgical subfield. Early molybdenum steel alloys showed great promise in their increased hardness, but efforts were hampered by inconsistent results and a tendency toward brittleness and recrystallization. In 1906, William D. Coolidge filed a patent for rendering molybdenum ductile, leading to its use as a heating element for high-temperature furnaces and as a support for tungsten-filament light bulbs; oxide formation and degradation require that moly be physically sealed or held in an inert gas. In 1913, Frank E. Elmore developed a flotation process to recover molybdenite from ores; flotation remains the primary isolation process. During the first World War, demand for molybdenum spiked; it was used both in armor plating and as a substitute for tungsten in high speed steels. Some British tanks were protected by 75 mm (3 in) manganese steel plating, but this proved to be ineffective. The manganese steel plates were replaced with 25 mm (1 in) molybdenum-steel plating allowing for higher speed, greater maneuverability, and better protection. After the war, demand plummeted until metallurgical advances allowed extensive development of peacetime applications. In World War II, molybdenum again saw strategic importance as a substitute for tungsten in steel alloys.
In World War II, tungsten played a significant role in background political dealings. Portugal, as the main European source of the element, was put under pressure from both sides, because of its deposits of wolframite ore at Panasqueira. Tungsten's resistance to high temperatures and its strengthening of alloys made it an important raw material for the arms industry.
Chemistry.
Unlike other groups, the members of this family do not show patterns in its electron configuration, as two lighter members of the group are exceptions from the Aufbau principle:
Most of the chemistry has been observed only for the first three members of the group, the chemistry of seaborgium is not very established and therefore the rest of the section deals only with its upper neighbors in the periodic table. The elements in the group, like those of groups 7—11, have high melting points, and form volatile compounds in higher oxidation states. All the elements of the group are relatively nonreactive metals with a high melting points (1907 °C, 2477 °C, 3422 °C); that of tungsten is the highest of all metals. The metals form compounds in different oxidation states: chromium forms compounds in all states from −2 to +6: disodium pentacarbonylchromate, disodium decacarbonyldichromate, bis(benzene)chromium, tripotassium pentanitrocyanochromate, chromium(II) chloride, chromium(III) oxide, chromium(IV) chloride, potassium tetraperoxochromate(V), and chromium(VI) dichloride dioxide; the same is also true for molybdenum and tungsten, but the stability of the +6 state grows down the group. Depending on oxidation states, the compounds are basic, amphoteric, or acidic; the acidity grows with the oxidation state of the metal.
Precautions.
Tungsten has no known biological role in the human body. Seaborgium's high radioactivity would make it a toxic element, due to radiation poisoning.
Biological occurrences.
Group 6 is notable in that it contains some of the only elements in periods 5 and 6 with a known role in the biological chemistry of living organisms: molybdenum is common in enzymes of many organisms, and tungsten has been identified in an analogous role in enzymes from some archaea, such as "Pyrococcus furiosus". In contrast, and unusually for a first-row d-block transition metal, chromium appears to have few biological roles, although it is thought to form part of the glucose metabolism enzyme in some mammals.

</doc>
<doc id="66322" url="https://en.wikipedia.org/wiki?curid=66322" title="M (1931 film)">
M (1931 film)

M ( — "M - A city looks for a murderer") is a 1931 German drama-thriller film directed by Fritz Lang and starring Peter Lorre. It was written by Lang and his wife Thea von Harbou and was the director's first sound film.
Now considered a classic, the film was deemed by Fritz Lang as the finest work he'd ever done.
Plot.
A group of children are playing an elimination game in the courtyard of an apartment building in Berlin using a chant about a murderer of children. A woman sets the table for dinner, waiting for her daughter to come home from school. A wanted poster warns of a serial killer preying on children, as anxious parents wait outside a school.
Little Elsie Beckmann leaves school, bouncing a ball on her way home. She is approached by Hans Beckert, who is whistling "In the Hall of the Mountain King" by Edvard Grieg. He offers to buy her a balloon from a blind street-vendor. He walks and talks with her. Elsie's place at the dinner table remains empty, her ball is shown rolling away across a patch of grass, and her balloon is lost in the telephone lines overhead.
In the wake of Elsie's death, Beckert sends an angry letter about his crimes to the newspapers, from which the police extract clues using the new techniques of fingerprinting and handwriting analysis. Under mounting pressure from city leaders, the police work around the clock. Inspector Karl Lohmann instructs his men to intensify their search and to check the records of recently released psychiatric patients to look for those with a history of violence against children. They stage frequent raids to question known criminals, disrupting underworld business so badly that "" ("The Safecracker") calls a meeting of the city's criminal bosses. They decide to organize their own manhunt, using beggars to watch and guard the children.
The police discover two clues corresponding to the killer's letter in Beckert's rented rooms. They wait there to arrest him.
Beckert sees a young girl in the reflection of a shop window. Following her, he is thwarted when the girl meets her mother. When he encounters another young girl, he succeeds in befriending her, but the blind beggar recognizes his whistling. The blind man tells one of his friends, who tails the killer with assistance from other beggars he alerts along the way. Afraid of losing him, one young man chalks a large M (for "", meaning "murderer" in German) on his hand, pretends to trip and bumps into Beckert, marking the back of his clothing.
The beggars close in. When Beckert finally realizes he is being followed, he hides inside a large office building just before the workers leave for the evening. The beggars call "", and a team of criminals arrives. They tie up and torture a guard for information. After capturing the remaining watchmen, they systematically search the building from coal cellar to attic, finally catching Beckert. When a watchman manages to trip the silent alarm, the crooks narrowly escape with their prisoner before the police arrive. One, however, is captured and eventually tricked into revealing the purpose of the break-in (nothing was stolen) and where Beckert would be taken.
The criminals drag Beckert to an abandoned distillery to face a kangaroo court. He finds a large, silent crowd awaiting him. Beckert is given a "lawyer", who gamely argues in his defense but fails to win any sympathy from the "jury". Beckert delivers an impassioned monologue, saying that his urges compel him to commit murders that he later regrets, while the other criminals present break the law by choice. His "lawyer" points out that the presiding "judge" is himself wanted on three counts of "" (a form of homicide under German law). Beckert pleads to be handed over to the police, asking, "Who knows what it's like to be me?" Just as the enraged mob is about to kill him, the police arrive to arrest both Beckert and the criminals.
As the real trial passes, five judges prepare to pass judgment on Beckert. Before the sentence is announced, the shot cuts to three of the victims' mothers crying. Elsie's mother says no sentence would bring back the dead children, and "One has to keep closer watch over the children". The screen goes black as she adds, "All of you".
Production.
Lang placed an advert in a newspaper in 1930 stating that his next film would be "" ("Murderer Among Us") and that it was about a child murderer. He immediately began receiving threatening letters in the mail, and was also denied a studio space to shoot the film at the Staaken Studios. When Lang confronted the head of Staaken studio to find out why he was being denied access, the studio head informed Lang that he was a member of the Nazi party and that the party suspected that the film was meant to depict the Nazis. This assumption was based entirely on the film's original title and the Nazi party relented when told the film's plot.
"M" was eventually shot in six weeks at a Staaken Zeppelinhalle studio, just outside Berlin. Lang made the film for Nero-Film, rather than with UFA or his own production company. It was produced by Nero studio head Seymour Nebenzal who would later produce Lang's "The Testament of Doctor Mabuse". Other titles were given to the film before being given the title ""M"": ' ("A City searches for a Murderer"); and ' ("Your Killer Looks At You"). While researching for the film Lang spent eight days inside a mental institution in Germany and met several real child murderers, including Peter Kürten. He used several real criminals as extras in the film and eventually 25 cast members were arrested during the film's shooting. Peter Lorre was cast in the lead role of Hans Beckert, acting for the film during the day and appearing onstage in Valentine Katayev's "Squaring the Circle" at night.
Lang did not show any acts of violence or deaths of children on screen, and later said that by only suggesting violence he forced "each individual member of the audience to create the gruesome details of the murder according to his personal imagination."
"M" has been said, by various critics and reviewers, to be based on serial killer Peter Kürten—the "Vampire of Düsseldorf"—whose crimes took place in the 1920s. Lang denied that he drew from this case in an interview in 1963 with film historian Gero Gandert; "At the time I decided to use the subject matter of "M" there were many serial killers terrorizing Germany—Haarmann, Grossmann, Kürten, Denke, [...]".
Leitmotif.
"M" was Lang's first sound film and Lang experimented with the new technology. It has a dense and complex soundtrack, as opposed to the more theatrical "talkies" being released at the same time. The soundtrack includes a narrator, sounds occurring off-camera, sounds motivating action, and suspenseful moments of silence before sudden noise. Lang was also able to make fewer cuts in the film's editing, since sound effects could now be used to inform the narrative.
The film was one of the first to use a "leitmotif", a technique borrowed from opera, associating a tune with Lorre's character, who whistles the tune "In the Hall of the Mountain King" from Edvard Grieg's "Peer Gynt Suite No. 1". Later in the film, the mere sound of the song lets the audience know that he is nearby, off-screen. This association of a musical theme with a particular character or situation is now a film staple. Peter Lorre could not whistle, however – it is actually Lang's wife and co-writer Thea von Harbou who is heard in the film.
Release and reception.
"M" premiered in Berlin on 11 May 1931 at the UFA-Palast am Zoo in a version lasting 117 minutes. The original negative is preserved at the Federal Film Archive in a 96-minute version. In 1960, an edited 98-minute version was released. The film was restored in 2000 by the Netherlands Film Museum in collaboration with the Federal Film Archive, the Cinemateque Suisse, Kirsch Media and ZDF/ARTE., with Janus Films releasing the 109-minute version as part of its Criterion Collection using prints from the same period from the Cinemateque Suisse and the Netherlands Film Museum. A complete print of the English version and selected scenes from the French version were included in the 2010 Criterion Collection releases of the film.
"M" was later released in the U.S. in 1933 by Foremco Pictures. After playing in German with English subtitles for two weeks, it was pulled from theaters and replaced by an English version. The re-dubbing was directed by Eric Hakim and Lorre was one of the few cast members to reprise his role in the film. As with many other early talkies from the years 1930–1931, "M" was partially reshot with actors (including Lorre) performing dialogue in other languages for foreign markets after the German original was completed, apparently without Lang's involvement. An English-language version was filmed and released in 1932 from an edited script with Lorre speaking his own words, his first English part. An edited French version was also released but despite the fact that Lorre spoke French his speaking parts were dubbed.
A "Variety" review said that the film was "a little too long. Without spoiling the effect—even bettering it—cutting could be done. There are a few repetitions and a few slow scenes." Graham Greene compared the film to "looking through the eye-piece of a microscope, through which the tangled mind is exposed, laid flat on the slide: love and lust; nobility and perversity, hatred of itself and despair jumping at you from the jelly".
In 2013 a DCP version was released by Kino Lorber and played theatrically in North America in the original aspect ratio of 1.19:1. Critic Kenneth Turan of the "Los Angeles Times" called this the "most-complete-ever version" at 111 minutes. The film was restored by TLEFilms Film Restoration & Preservation Services (Berlin) in association with Archives françaises du film - CNC (Paris) and PostFactory GmbH (Berlin).
Legacy.
Lang considered "M" to be his favorite of his own films because of the social criticism in the film. In 1937, he told a reporter that he made the film "to warn mothers about neglecting children".
A Hollywood remake of the same name was released in 1951, shifting the action from Berlin to Los Angeles. Nero Films head Seymour Nebenzal and his son Harold produced the film for Columbia Pictures. Lang had once told a reporter "People ask me why I do not remake "M" in English. I have no reason to do that. I said all I had to say about that subject in the picture. Now I have other things to say." The remake was directed by Joseph Losey and starred David Wayne in Lorre's role. Losey stated that he had seen "M" in the early 1930s and watched it again shortly before shooting the remake, but that he "never referred to it. I only consciously repeated one shot. There may have been unconscious repetitions in terms of the atmosphere, of certain sequences." Lang later said that when the remake was released he "had the best reviews of my life".
The original 1931 "M" was ranked at number thirty-three in "Empire" magazines' "The 100 Best Films Of World Cinema" in 2010.

</doc>
<doc id="66323" url="https://en.wikipedia.org/wiki?curid=66323" title="Crime Traveller">
Crime Traveller

Crime Traveller is a 1997 science fiction detective television series produced by Carnival Films for the BBC based on the premise of using time travel for the purpose of solving crimes.
Anthony Horowitz created the series and wrote every episode. He got the idea while writing an episode of "Poirot". Despite having over eight million viewers on a regular basis, "Crime Traveller" was not renewed after its first series, because according to Horowitz, "The show wasn't exactly cut. There was a chasm at the BBC, created by the arrival of a new Head of Drama and our run ended at that time. There was no-one around to commission a new series...and so it just didn't happen." The final episode of the series was followed the next week by the first episode of Jonathan Creek, which became a popular long-running crime series.
Plot.
Jeff Slade is a detective with the CID department of the local police force led by Kate Grisham, although unusually for such a position he is an armed officer; carrying a handgun as routine. Slade is a good detective who gets results although his approach is somewhat maverick and his methods do leave a lot to be desired and have more than once landed him in trouble. Amongst Slade's colleagues at the department is science officer Holly Turner who has a secret that Slade manages to uncover. Holly owns a working Time Machine that was built by her late father. The machine is able to take Slade and Holly back far enough in time to witness a crime as it happens and discover who committed it. As a result Slade's track record with crime solving goes through the roof with case after case being solved in record time.
Rules of time travel.
The rules of time travel in the series are as follows:
Characters.
Jeff Slade.
Slade is not what could be described as a conventional detective. His style is maverick to say the least and he is more prone to go on instinct rather than cold hard facts. This approach invariably gets him in trouble with his superiors but nine times out of ten they will result in an arrest. Slade clearly enjoys his job and is very dedicated to it even though his attitude sometimes suggests otherwise. He was inspired to join the police because of his father Jack who was a highly respected detective himself until his false arrest and imprisonment for theft in 1992. Jeff took this very hard as he had always admired his father and refused to believe he was capable of such a thing following such a distinguished 30-year career. Slade was once married but very little is known about how long he was married and indeed how the marriage ended but it is thought that his wife died as he keeps a picture in his flat of a girl he tells Holly is now dead. Outside work Slade tends to keep himself to himself and rarely if ever socialises with colleagues such as Morris and Nicky. His only real friend at work is science officer Holly Turner. They have clearly always been on friendly terms but it is suggested that before her telling him about the machine they didn't socialise outside of work. Since his finding out about the machine the two have become a lot closer and spend a lot of time together outside work. It is hinted throughout the series that Slade is attracted to Holly but for whatever reason he has never seen fit to tell her how he feels. As well as making them closer personally their use of the machine has also shown that professionally they are a very good team and together they have solved several crimes including who set up Slade's father five years before and getting him released.
Holly Turner.
Played by Chloë Annett, Holly is the daughter of Professor Frederick Turner, a prominent physicist whose specialist field was that of time travel. After years of research and work, Turner was able to develop a working time machine. Turner used the machine to carry out experiments on time and the laws and rules by which it abides, such as 'you can't change the past' and 'time won't allow paradoxes'. The only other person Turner ever told about the machine was his daughter Holly. When Turner failed to return to the machine in time on his last trip and was trapped in a loop of infinity, Holly took over work on the machine and continued to maintain and perfect it to continue his experiments. To do so, Holly needed money but she didn't dare apply for a grant for fear of someone discovering the machine. So, she went to work for the CID as a science officer and it was there that she met detective Jeff Slade. The two were friendly from the start but when Holly used the machine to help Slade out after a case had gone badly wrong and got Slade in trouble, Slade found out about the machine. This resulted in their relationship becoming closer and they began spending time together outside work. Holly is clearly attracted to Slade as he is to her but like him, she chooses not to reveal this to him, probably because she thinks that if the machine were not around then Slade wouldn't give her a second glance. Like Slade, Holly tends to keep herself to herself, possibly more so than he does because she is so terrified that someone may find out about the machine. Whilst she feels that Slade relies too much on the machine to solve his cases, she does seem to like having someone to talk to about the machine and she no longer has to keep all the financial worries that its upkeep brings completely to herself.
The Machine.
The Time Machine was invented by Professor Frederick Turner. The machine has been cobbled together from various different pieces of electronic equipment over the years and has a distinctly home-made look about it. Turner built the machine in the living room of his flat in Sundown Court where he lived with his daughter Holly and presumably at some stage his wife. Holly is the only other person he ever told about the machine. The most vital component for the machine is the electro-magnetic crystal which is in the heart of the machine. Unfortunately it is also the single most expensive part of the machine. Turner had to sell his house to buy his. The machine can only travel backwards in time as it is not possible to travel into a future which does not yet exist and it can only go back a few hours into the past, although it could in theory go back a week. Attached to the machine is a time piece which has a small analogue clock and a digital countdown display. This indicates how far back the machine has taken you and how much time remains until you are back at the time you left. This time piece can be removed from the machine and placed in a watch which the time traveller can carry round to remind of how much time is left. The time piece must be replaced in the machine before the countdown reaches zero otherwise the traveller or travellers will be caught in a loop of infinity constantly living out the same few hours that they travelled back in time. This is what happened to Frederick Turner. The machine is now maintained by Holly and she was the only person to know about it until she told her friend and colleague Jeff Slade.

</doc>
<doc id="66326" url="https://en.wikipedia.org/wiki?curid=66326" title="Brașov">
Brașov

Brașov (; also known by other alternative names) is a city in Romania and the administrative centre of Brașov County.
According to the last Romanian census, from 2011, there were 253,200 people living within the city of Brașov, making it the 7th most populous city in Romania, and the metropolitan area is home to 369,896 residents.
Brașov is located in the central part of the country, about north of Bucharest and from the Black Sea. It is surrounded by the Southern Carpathians and is part of the Transylvania region.
The city is notable for being the birthplace of the national anthem of Romania and for hosting the Golden Stag International Music Festival.
Etymology.
The city was described in 1235 AD under the name Corona, a Latin word meaning "crown", a name given by the German colonists. According to Binder, the current Romanian and the Hungarian name () are derived from the Turkic word "barasu", meaning "white water" with a Slavic suffix "-ov". Other linguists proposed various etymologies including an Old Slavic anthroponym Brasa.
The first attested mention of Brașov is "Terra Saxonum de Barasu" ("Saxon Land of Baras") in a 1252 document. The German name "Kronstadt" means "Crown City" and is reflected in the city's coat of arms as well as in its Medieval Latin name, "Corona". The two names of the city, "Kronstadt" and "Corona", were used simultaneously in the Middle Ages, along with the Medieval Latin "Brassovia" .
From 1950 to 1960, during part of the Communist period in Romania, the city was called "Orașul Stalin" (Stalin City), after the Soviet leader Joseph Stalin.
History.
The oldest traces of human activity and settlements in Brașov date back to the Neolithic age (about 9500 BCE). Archaeologists working from the last half of the 19th century discovered continuous traces of human settlements in areas situated in Brașov: Valea Cetăţii, Pietrele lui Solomon, Șprenghi, Tâmpa, Dealul Melcilor, and Noua. The first three locations show traces of Dacian citadels; Șprenghi Hill housed a Roman-style construction. The last two locations had their names applied to Bronze Age cultures—"Schneckenberg" ‘Hill of the Snails’ (Early Bronze Age) and "Noua" 'The New’ (Late Bronze Age).
German colonists known as the Transylvanian Saxons played a decisive role in Brașov's development. These Germans were invited by Hungarian kings to develop towns, build mines, and cultivate the land of Transylvania at different stages between 1141 and 1300. The settlers came primarily from the Rhineland, Flanders, and the Moselle region, with others from Thuringia, Bavaria, Wallonia, and even France.
In 1211, by order of King Andrew II of Hungary, the Teutonic Knights fortified the Burzenland to defend the border of the Kingdom of Hungary. On the site of the village of Brașov, the Teutonic Knights built Kronstadt – the city of the crown. Although the crusaders were evicted by 1225, the colonists they brought in remained, along with local population, as did three distinct settlements they founded on the site of Brașov:
Germans living in Brașov were mainly involved in trade and crafts. The location of the city at the intersection of trade routes linking the Ottoman Empire and Western Europe, together with certain tax exemptions, allowed Saxon merchants to obtain considerable wealth and exert a strong political influence. They contributed a great deal to the architectural flavor of the city. Fortifications around the city were erected and continually expanded, with several towers maintained by different craftsmen's guilds, according to medieval custom. Part of the fortification ensemble was recently restored using UNESCO funds, and other projects are ongoing. At least two entrances to the city, "Poarta Ecaterinei" (or "Katharinentor") and "Poarta Șchei" (or "Waisenhausgässertor"), are still in existence. The city center is marked by the mayor's former office building (Casa Sfatului) and the surrounding square ("piaţa"), which includes one of the oldest buildings in Brașov, the Hirscher Haus. Nearby is the "Black Church" ("Biserica Neagră"), which some claim to be the largest Gothic style church in Southeastern Europe.
The cultural and religious importance of the Romanian church and school in Șchei is underlined by the generous donations received from more than thirty hospodars of Moldavia and Wallachia, as well as that from Elizabeth of Russia. In the 17th and 19th centuries, the Romanians in Șchei campaigned for national, political, and cultural rights, and were supported in their efforts by Romanians from all other provinces, as well as by the local Greek merchant community. In 1838 they established the first Romanian language newspaper "Gazeta Transilvaniei" and the first Romanian institutions of higher education ("Școlile Centrale Greco-Ortodoxe" - "The Greek-Orthodox Central Schools", today named after Andrei Șaguna). The Holy Roman Emperor and sovereign of Transylvania Joseph II awarded Romanians citizenship rights for a brief period during the latter decades of the 18th century.
In 1850 the town had 21,782 inhabitants: 8,874 (40.7%) Germans, 8,727 (40%) Romanians, 2,939 (13.4%) Hungarians. In 1910 the town had 41,056 inhabitants: 10,841 (26.4%) Germans, 11,786 (28.7%) Romanians, 17,831 (43.4%) Hungarians.
In 1918, after the Proclamation of union of Alba Iulia (adopted by the Deputies of the Romanians from Transylvania), Deputies of the Saxons from Transylavania supported it, with their vote to be part of Romania, and declared their allegiance to the new Romanian state. The inter-war period was a time of flourishing economic and cultural life in general, which included the Saxons in Brașov as well. However, at the end of World War II many ethnic Germans were forcibly deported to the Soviet Union, and many more emigrated to West Germany after Romania became a communist country.
Jews have lived in Brașov since 1807, when Aron Ben Jehuda was given permission to live in the city, a privilege until then granted only to Saxons. The Jewish community of Brașov was officially founded 19 years later, followed by the first Jewish school in 1864, and the building of the synagogue in 1901. The Jewish population of Brașov was 67 in 1850, but it expanded rapidly to 1,280 people in 1910 and 4,000 by 1940. Today the community has about 230 members, after many families left for Israel between World War II and 1989.
Like many other cities in Transylvania, Brașov is also home to a significant ethnic Hungarian minority.
During the communist period, industrial development was vastly accelerated. Under Nicolae Ceaușescu's rule, the city was the site of the 1987 Brașov strike. This was repressed by the authorities and resulted in numerous workers being imprisoned.
Economy.
Industrial development in Brașov started in the inter-war period, with one of the largest factories being the airplane manufacturing plant (IAR Brașov), which produced the first Romanian fighter planes, which were used in World War II against the Soviets. After Communist rule was imposed, the plant was converted to manufacture of agricultural equipment, being renamed "Uzina Tractorul Brașov" (internationally known as Universal Tractor Brașov).
Industrialization was accelerated in the Communist era, with special emphasis being placed on heavy industry, attracting many workers from other parts of the country. Heavy industry is still abundant, including Roman, which manufactures MAN AG trucks as well as native-designed trucks and coaches. Although the industrial base has been in decline in recent years, Brașov is still a site for manufacturing agricultural tractors and machinery, hydraulic transmissions, auto parts, ball-bearings, helicopters (at the nearby IAR site in Ghimbav), building materials, tools, furniture, textiles, shoes and cosmetics. There are also chocolate factories and a large brewery. In particular, the pharmaceutical industry has undergone further development lately, with GlaxoSmithKline establishing a production site in Brașov.
The large longwave Bod Transmitter, a broadcasting facility, is located near Brașov, at Bod.
Demographics.
Brașov has a total population of 253,200 (2011 census).
Its ethnic composition includes:
In 2005, the Brașov metropolitan area was created. With its surrounding localities, Brașov had 369,896 inhabitants .
Education.
Primary Schools
High Schools
Universities:
Transportation.
The Brașov local transport network is well-developed, with around 50 bus and trolleybus lines. There is also a regular bus line serving Poiana Brașov, a nearby winter resort. All are operated by RAT Brașov. Because of its central location, the Brașov railway station is one of the busiest stations in Romania with trains to/from most destinations in the country served by rail.
The construction of Braşov Airport was initiated by Intelcan Canada on April 15, 2008. Although construction was planned to be finalized in 24 to 30 months, works have lagged and there is no term by which it will be operational. The project consists of a terminal capable of handling 1 million passengers per year and a runway.
The A3 highway is also planned to pass the city. However, there is no foreseeable date for starting construction.
Tourism.
With its central location, Brașov is a suitable location from which to explore Romania, and the distances to several tourist destinations (including the Black Sea resorts, the monasteries in northern Moldavia, and the wooden churches of Maramureș) are similar. It is also the largest city in a mountain resorts area. The old city is very well preserved and is best seen by taking the cable-car to the top of Tâmpa Mountain.
Temperatures from May to September fluctuate around . Brașov benefits from a winter tourism season centered on winter sports and other activities. Poiana Brașov is the most popular Romanian ski resort and an important tourist center preferred by many tourists from other European states.
The city also has several restaurants that serve local as well as international cuisine (e.g. Hungarian and Chinese). Some of these are situated in the city center.
International relations.
Twin towns – Sister cities.
Brașov is twinned with:
Sport.
The city has a long tradition in sports, the first sport associations being established at the end of the 19th century (Target shooting Association, Gymnastics School). The Transylvanian Sports Museum is among the oldest in the country and presents the evolution of consecrated sports in the city. During the communist period, universiades and daciades (derived from "dacian") were held, where local sportsmen were obliged to participate. Nowadays, the infrastructure of the city allows other sports to be practiced, such as football, rugby, tennis, cycling, handball, gliding, skiing, skating, mountain climbing, paintball, bowling, swimming, target shooting, basketball, martial arts, equestrian, volleyball or gymnastics. Annually, at "Olimpia" sports ground, the "Brașov Challenge Cup" tennis competition is held.
The only football champion team based in the city was Colţea Brașov, winning the championship in 1928 and managing second place in 1927, in only 10 years of existence (1921–1931). It was succeeded by Brașovia Brașov.
Brașov hosted the Group A fixtures of the 2008 IIHF World Championship Division II ice hockey tournament. There were 15 games held between April 3 and April 13.
Brașov hosted the 2013 European Youth Winter Olympic Festival.
, Brasov is hosting two trail semi-marathons: Semimaraton "Intersport Brasov" and Brasov Marathon.
In November 2013, Brasov submitted their bid for the 2020 Winter Youth Olympics. They are up against Lausanne, Switzerland to be awarded the event. In December that year, the city was signed the Youth Olympic Game Candidature Procedure. The host city will be announced in July 2015
Media.
The city of Braşov is home to several local media publications such as Transilvania Expres, Monitorul Expres, Bună Ziua Braşov or Braşovul Tău. Also, several local television stations exist, such as TVS, RTT, MIX TV and Nova TV.

</doc>
<doc id="66331" url="https://en.wikipedia.org/wiki?curid=66331" title="Battle of Dien Bien Phu">
Battle of Dien Bien Phu

The Battle of Dien Bien Phu (; , "Campaign of Dien Bien Phu") was the climactic confrontation of the First Indochina War between the French Union's French Far East Expeditionary Corps and Viet Minh communist-nationalist revolutionaries. It was, from the French view before the event, a set piece battle to draw out the Vietnamese and destroy them with superior firepower. The battle occurred between March and May 1954 and culminated in a comprehensive French defeat that influenced negotiations over the future of Indochina at Geneva.
As a result of blunders in French decision-making, the French began an operation to insert then support the soldiers at Điện Biên Phủ, deep in the hills of northwestern Vietnam. Its purpose was to cut off Viet Minh supply lines into the neighboring Kingdom of Laos, a French ally, and tactically draw the Viet Minh into a major confrontation that would cripple them. The Viet Minh, however, under General Võ Nguyên Giáp, surrounded and besieged the French, who knew of the weapons but were unaware of the vast amounts of the Viet Minh's heavy artillery being brought in (including anti-aircraft guns) and their ability to move these weapons through difficult terrain up the rear slopes of the mountains surrounding the French positions, dig tunnels through the mountain, and place the artillery pieces overlooking the French encampment. This positioning of the artillery made it nearly impervious to counter-battery fire.
The Viet Minh opened fire with a massive bombardment from the artillery in March. After several days the French artillery commander, Charles Piroth, unable to structure any sort of counter-battery fire, committed suicide. The Viet Minh proceeded to occupy the highlands around Điện Biên Phủ and bombard the French positions. Tenacious fighting on the ground ensued, reminiscent of the trench warfare of World War I. The French repeatedly repulsed Viet Minh assaults on their positions. Supplies and reinforcements were delivered by air, though as the key French positions were overrun the French perimeter contracted and air resupply on which the French had placed their hopes became impossible, and as the anti-aircraft fire took its toll, fewer and fewer of those supplies reached them. The garrison was overrun after a two-month siege and most French forces surrendered. A few escaped to Laos. The French government resigned and the new Prime Minister, the left-of-centre Pierre Mendès France, supported French withdrawal from Indochina.
The war ended shortly after the Battle of Dien Bien Phu and the signing of the 1954 Geneva Accords. France agreed to withdraw its forces from all its colonies in French Indochina, while stipulating that Vietnam would be temporarily divided at the 17th parallel, with control of the north given to the Viet Minh as the Democratic Republic of Vietnam under Ho Chi Minh, and the south becoming the State of Vietnam nominally under Emperor Bảo Đại, preventing Ho Chi Minh from gaining control of the entire country. The refusal of Ngô Đình Diệm to allow elections in 1956, as had been stipulated by the Geneva Conference, eventually led to the first phase of the "Second Indochina War", better known as the Vietnam War (see War in Vietnam (1959–63)).
Background.
Military situation.
By 1953, the First Indochina War was not going well for France. A succession of commanders — Philippe Leclerc de Hauteclocque, Jean Étienne Valluy, Roger Blaizot, Marcel Carpentier, Jean de Lattre de Tassigny, and Raoul Salan — had proven incapable of suppressing the Viet Minh insurrection. During their 1952–53 campaign, the Viet Minh had overrun vast swathes of Laos, a French ally and Vietnam's western neighbor, advancing as far as Luang Prabang and the Plain of Jars. The French were unable to slow the Viet Minh advance, and the Viet Minh fell back only after outrunning their always-tenuous supply lines. In 1953, the French had begun to strengthen their defenses in the Hanoi delta region to prepare for a series of offensives against Viet Minh staging areas in northwest Vietnam. They had set up fortified towns and outposts in the area, including Lai Châu near the Chinese border to the north, Nà Sản to the west of Hanoi, and the Plain of Jars in northern Laos.
In May 1953, French Premier René Mayer appointed Henri Navarre, a trusted colleague, to take command of French Union Forces in Indochina. Mayer had given Navarre a single order—to create military conditions that would lead to an "honorable political solution". According to military scholar Phillip Davidson, On arrival, Navarre was shocked by what he found. There had been no long-range plan since de Lattre's departure. Everything was conducted on a day-to-day, reactive basis. Combat operations were undertaken only in response to enemy moves or threats. There was no comprehensive plan to develop the organization and build up the equipment of the Expeditionary force. Finally, Navarre, the intellectual, the cold and professional soldier, was shocked by the "school's out" attitude of Salan and his senior commanders and staff officers. They were going home, not as victors or heroes, but then, not as clear losers either. To them the important thing was that they were getting out of Indochina with their reputations frayed, but intact. They gave little thought to, or concern for, the problems of their successors.
Nà Sản and the hedgehog concept.
Simultaneously, Navarre had been searching for a way to stop the Viet Minh threat to Laos. Colonel Louis Berteil, commander of Mobile Group 7 and Navarre's main planner, formulated the "hérisson" ("hedgehog") concept. The French army would establish a fortified airhead by air-lifting soldiers adjacent to a key Viet Minh supply line to Laos. This would effectively cut off Viet Minh soldiers fighting in Laos and force them to withdraw. "It was an attempt to interdict the enemy's rear area, to stop the flow of supplies and reinforcements, to establish a redoubt in the enemy's rear and disrupt his lines".
The hedgehog concept was based on French experiences at the Battle of Nà Sản. In late November and early December 1952, Giáp attacked the French outpost at Nà Sản, which was essentially an "air-land base", a fortified camp supplied only by air. Giáp's forces were beaten back repeatedly with very heavy losses. The French hoped that by repeating the strategy on a much larger scale, they would be able to lure Giáp into committing the bulk of his forces in a massed assault. This would enable superior French artillery, armor, and air support to decimate the exposed Viet Minh forces. The experience at Nà Sản convinced Navarre of the viability of the fortified airhead concept.
French staff officers disastrously failed to treat seriously several crucial differences between Điện Biên Phủ and Nà Sản:
Firstly, at Nà Sản, the French commanded most of the high ground with overwhelming artillery support. At Điện Biên Phủ, however, the Viet Minh controlled much of the high ground around the valley, their artillery far exceeded French expectations and they outnumbered the French four-to-one. Giáp compared Điện Biên Phủ to a "rice bowl", where his troops occupied the edge and the French the bottom.
Secondly, Giáp made a mistake in Nà Sản by committing his forces to reckless frontal attacks before being fully prepared. At Điện Biên Phủ, Giáp spent months meticulously stockpiling ammunition and emplacing heavy artillery and anti-aircraft guns before making his move. Teams of Viet Minh volunteers were sent into the French camp to scout the disposition of the French artillery. Wooden artillery pieces were built as decoys and the real guns were rotated every few salvos to confuse French counterbattery fire. As a result, when the battle finally began, the Viet Minh knew exactly where the French artillery pieces were, while the French were not even aware of how many guns Giáp possessed.
Thirdly, the aerial resupply lines at Nà Sản were never severed, despite Viet Minh anti-aircraft fire. At Điện Biên Phủ, Giáp amassed anti-aircraft batteries that quickly shut down the runway and made it extremely difficult and costly for the French to bring in reinforcements.
Prelude.
Lead up to Castor.
In June, Major General René Cogny, commander of the Tonkin Delta, proposed Điện Biên Phủ, which had an old airstrip built by the Japanese during World War II, as a "mooring point". In another misunderstanding, Cogny had envisioned a lightly defended point from which to launch raids; however, to Navarre, this meant a heavily fortified base capable of withstanding a siege. Navarre selected Điện Biên Phủ for the location of Berteil's "hedgehog" operation. When presented with the plan, every major subordinate officer protested: Colonel Jean-Louis Nicot (commander of the French Air transport fleet), Cogny, and generals Jean Gilles and Jean Dechaux (the ground and air commanders for "Operation Castor", the initial airborne assault on Điện Biên Phủ). Cogny pointed out, presciently, that "we are running the risk of a new Nà Sản under worse conditions". Navarre rejected the criticisms of his proposal and concluded a November 17 conference by declaring that the operation would commence three days later, on 20 November 1953.
Navarre decided to go ahead with the operation, despite operational difficulties, which later became painfully obvious (but at the time may have been less apparent) because he had been repeatedly assured by his intelligence officers that the operation had very little risk of involvement by a strong enemy force. Navarre had previously considered three other ways to defend Laos: mobile warfare, which was impossible given the terrain in Vietnam; a static defense line stretching to Laos, which was not feasible given the number of troops at Navarre's disposal; or placing troops in the Laotian provincial capitals and supplying them by air, which was unworkable due to the distance from Hanoi to Luang Prabang and Vientiane. Thus, the only option left to Navarre was the hedgehog, which he characterized as "a mediocre solution". In a twist of fate, the French National Defense Committee ultimately agreed that Navarre's responsibility did not include defending Laos. However, their decision (which was drawn up on 13 November) was not delivered to him until 4 December, two weeks after the Điện Biên Phủ operation began.
Establishment of Air Operations.
Operations at Điện Biên Phủ began at 10:35 on the morning of 20 November 1953. In "Operation Castor", the French dropped or flew 9,000 troops into the area over three days, including a bulldozer to prepare the airstrip. They were landed at three drop zones: "Natasha" (northwest), "Octavie" (southwest), and "Simone" (southeast) of Điện Biên Phủ. The Viet Minh elite 148th Independent Infantry Regiment, headquartered at Điện Biên Phủ, reacted "instantly and effectively"; three of their four battalions, however, were absent that day. Initial operations proceeded well for the French. By the end of November, six parachute battalions had been landed and the French were consolidating their positions.
It was at this time that Giáp began his counter-moves. He had expected an attack, but could not foresee when or where it would occur. Giáp realized that, if pressed, the French would abandon Lai Châu Province and fight a pitched battle at Điện Biên Phủ. On 24 November, Giáp ordered the 148th Infantry Regiment and the 316th Division to attack Lai Chau, while the 308th, 312th, and 351st divisions assault Điện Biên Phủ from "Việt Bắc".
Starting in December, the French, under the command of Colonel Christian de Castries, began transforming their anchoring point into a fortress by setting up seven satellite positions, each allegedly named after a former mistress of de Castries, although the allegation is probably unfounded, as the eight names begin with letters from the first nine of the alphabet (all but F). The fortified headquarters was centrally located, with positions "Huguette" to the west, "Claudine" to the south, and "Dominique" to the northeast.
Other positions were "Anne-Marie" to the northwest, "Beatrice" to the northeast, "Gabrielle" to the north and "Isabelle" to the south, covering the reserve airstrip. The choice of de Castries as the on-scene commander at Điện Biên Phủ was, in retrospect, a bad one. Navarre had picked de Castries, a cavalryman in the 18th century tradition, because Navarre envisioned Điện Biên Phủ as a mobile battle. In reality, Điện Biên Phủ required someone adept at World War I-style trench warfare, something for which de Castries was not suited.
The arrival of the 316th Viet Minh Division prompted Cogny to order the evacuation of the Lai Chau garrison to Điện Biên Phủ, exactly as Giáp had anticipated. En route, they were virtually annihilated by the Viet Minh. "Of the 2,100 men who left Lai Chau on 9 December, only 185 made it to Điện Biên Phủ on 22 December. The rest had been killed, captured or deserted". The Viet Minh troops now converged on Điện Biên Phủ.
The French had committed 10,800 troops, with more reinforcements totaling nearly 16,000 men, to the defense of a monsoon-affected valley surrounded by heavily wooded hills that had not been secured. Artillery as well as ten M24 Chaffee light tanks and numerous aircraft were committed to the garrison. The garrison comprised French regular troops (notably elite paratroop units plus artillery), Foreign Legionnaires, Algerian and Moroccan tirailleurs, and locally recruited Indochinese infantry. All told, the Viet Minh had moved 50,000 regular troops into the hills surrounding the valley, totaling five divisions including the 351st Heavy Division, which was made up entirely of heavy artillery. Artillery and AA (anti-aircraft) guns, which outnumbered the French artillery by about four to one, were moved into positions overlooking the valley. The French came under direct and sporadic Viet Minh artillery fire for the first time on 31 January 1954, and patrols encountered the Viet Minh in all directions. The French were now surrounded.
Battle.
Beatrice.
The Viet Minh assault began in earnest on 13 March 1954 with an attack on outpost "Beatrice". Viet Minh artillery opened a fierce bombardment of the fortification and French command was disrupted at 6:15 pm when a shell hit the French command post, killing Legionnaire commander Major Paul Pegot and his entire staff. A few minutes later, Colonel Jules Gaucher, commander of the entire northern sector, was killed by Viet Minh artillery. The Viet Minh 312th Division then launched a massive infantry assault, using sappers to defeat French obstacles. French resistance at Beatrice collapsed shortly after midnight following a fierce battle. Roughly 500 French legionnaires were killed. The French estimated that Viet Minh losses totalled 600 dead and 1,200 wounded. The French launched a counter-attack against "Beatrice" the following morning, but it was quickly beaten back by Viet Minh artillery. The victory at "Beatrice" 'galvanized the morale' of the Viet Minh troops.
Much to French disbelief, the Viet Minh had employed direct artillery fire, in which each gun crew does its own artillery spotting (as opposed to indirect fire, in which guns are massed farther away from the target, out of direct line of sight, and rely on a forward artillery spotter). Indirect artillery, generally held as being far superior to direct fire, requires experienced, well-trained crews and good communications, which the Viet Minh lacked. Navarre wrote that ""Under the influence of Chinese advisers, the Viet Minh commanders had used processes quite different from the classic methods. The artillery had been dug in by single pieces ... They were installed in shell-proof dugouts, and fire point-blank from portholes ... This way of using artillery and AA guns was possible only with the expansive ant holes at the disposal of the Vietminh and was to make shambles of all the estimates of our own artillerymen."" Two days later, the French artillery commander, Colonel Charles Piroth, distraught at his inability to bring counterfire on the well-camouflaged Viet Minh batteries, went into his dugout and committed suicide with a hand grenade. He was buried there in secret to prevent loss of morale among the French troops.
Gabrielle.
Following a five-hour cease fire on the morning of 14 March, Viet Minh artillery resumed pounding French positions. The air strip, already closed since 16:00 the day before due to a light bombardment, was now put permanently out of commission. Any further French supplies would have to be delivered by parachute. That night, the Viet Minh launched an attack on "Gabrielle", held by an elite Algerian battalion. The attack began with a concentrated artillery barrage at 5:00 pm; this was very effective and stunned the defenders. Two regiments from the crack 308th Division attacked starting at 8:00 pm. At 4:00 am the following morning, an artillery shell hit the battalion headquarters, severely wounding the battalion commander and most of his staff.
De Castries ordered a counterattack to relieve "Gabrielle". However, Colonel Pierre Langlais, in forming the counterattack, chose to rely on the 5th Vietnamese Parachute Battalion, which had jumped in the day before and was exhausted. Although some elements of the counterattack reached "Gabrielle", most were paralyzed by Viet Minh artillery and took heavy losses. At 08:00 the next day, the Algerian battalion fell back, abandoning "Gabrielle" to the Viet Minh. The French lost around 1,000 men defending Gabrielle, and the Viet Minh between 1,000 and 2,000 attacking the strongpoint. The loss of the outpost "Beatrice" and now "Gabrielle", allowed almost pin point artillery to be rained down for the rest of the battle and cut off any air resupply using the airstrip, and this dictated the resulting events.
Anne-Marie.
"Anne-Marie" was defended by Tai troops, members of a Vietnamese ethnic minority loyal to the French. For weeks, Giáp had distributed subversive propaganda leaflets, telling the Tais that this was not their fight. The fall of "Beatrice" and "Gabrielle" had severely demoralized them. On the morning of 17 March, under the cover of fog, the bulk of the Tais left or defected. The French and the few remaining Tais on "Anne-Marie" were then forced to withdraw.
Lull.
17 March through 30 March saw a lull in fighting. The Viet Minh further tightened the noose around the French central area (formed by the strongpoints "Huguette", "Dominique", "Claudine", and "Eliane"), effectively cutting off Isabelle and its 1,809 personnel. During this lull, the French suffered from a serious crisis of command. "It had become painfully evident to the senior officers within the encircled garrison—and even to Cogny at Hanoi—that de Castries was incompetent to conduct the defense of Dien Bien Phu. Even more critical, after the fall of the northern outposts, he isolated himself in his bunker so that he had, in effect, relinquished his command authority". On 17 March, Cogny attempted to fly into Điện Biên Phủ to take command, but his plane was driven off by anti-aircraft fire. Cogny considered parachuting into the encircled garrison, but his staff talked him out of it.
De Castries' seclusion in his bunker, combined with his superiors' inability to replace him, created a leadership vacuum within the French command. On 24 March, an event took place which later became a matter of historical debate. Historian Bernard Fall records, based on Langlais' memoirs, that Colonel Langlais and his fellow paratroop commanders, all fully armed, confronted de Castries in his bunker on 24 March. They told him he would retain the appearance of command, but that Langlais would exercise it. De Castries is said by Fall to have accepted the arrangement without protest, although he did exercise some command functions thereafter. Phillip Davidson stated that the "truth would seem to be that Langlais did take over effective command of Dien Bien Phu, and that Castries became 'commander emeritus' who transmitted messages to Hanoi and offered advice about matters in Dien Bien Phu". Jules Roy, however, makes no mention of this event, and Martin Windrow argues that the "paratrooper putsch" is unlikely to have happened. Both historians record that Langlais and Marcel Bigeard were known to be on good terms with their commanding officer.
The French aerial resupply took heavy losses from Viet Minh machine guns near the landing strip. On 27 March, Hanoi air transport commander Nicot ordered that all supply deliveries be made from or higher; losses were expected to remain heavy. De Castries ordered an attack against the Viet Minh machine guns west of Điện Biên Phủ. Remarkably, the attack was a complete success, with 350 Viet Minh soldiers killed and seventeen AA machine guns destroyed (French est), while the French lost 20 killed and 97 wounded.
30 March – 5 April assaults.
The next phase of the battle saw more massed Viet Minh assaults against French positions in the central Điện Biên Phủ—at "Eliane" and "Dominique" in particular. Those two areas were held by five understrength battalions, composed of Frenchmen, Legionnaires, Vietnamese, North Africans, and Tais. Giáp planned to use the tactics from the "Beatrice" and "Gabrielle" skirmishes.
At 19:00 on 30 March, the Viet Minh 312th Division captured "Dominique 1 and 2", making "Dominique 3" the final outpost between the Viet Minh and the French general headquarters, as well as outflanking all positions east of the river. At this point, the French 4th Colonial Artillery Regiment entered the fight, setting its 105 mm howitzers to zero elevation and firing directly on the Viet Minh attackers, blasting huge holes in their ranks. Another group of French, near the airfield, opened fire on the Viet Minh with anti-aircraft machine guns, forcing the Viet Minh to retreat.
The Viet Minh were more successful in their simultaneous attacks elsewhere. The 316th Division captured "Eliane 1" from its Moroccan defenders, and half of "Eliane 2" by midnight. On the other side of Điện Biên Phủ, the 308th attacked "Huguette 7", and nearly succeeded in breaking through, but a French sergeant took charge of the defenders and sealed the breach.
Just after midnight on 31 March, the French launched a fierce counterattack against "Eliane 2", and recaptured half of it. Langlais ordered another counterattack the following afternoon against "Dominique 2" and "Eliane 1", using virtually "everybody left in the garrison who could be trusted to fight". The counterattacks allowed the French to retake "Dominique 2" and Eliane 1, but the Viet Minh launched their own renewed assault. The French, who were exhausted and without reserves, fell back from both positions late in the afternoon. Reinforcements were sent north from "Isabelle", but were attacked en route and fell back to "Isabelle".
Shortly after dark on 31 March, Langlais told Major Marcel Bigeard, who was leading the defense at "Eliane", to fall back across the river. Bigeard refused, saying "As long as I have one man alive I won't let go of 'Eliane 4'. Otherwise, Dien Bien Phu is done for." The night of the 31st, the 316th Division attacked "Eliane 2". Just as it appeared the French were about to be overrun, a few French tanks arrived, and helped push the Viet Minh back. Smaller attacks on "Eliane 4" were also pushed back. The Viet Minh briefly captured "Huguette 7", only to be pushed back by a French counterattack at dawn on 1 April.
Fighting continued in this manner over the next several nights. The Viet Minh repeatedly attacked "Eliane 2", only to be beaten back. Repeated attempts to reinforce the French garrison by parachute drops were made, but had to be carried out by lone planes at irregular times to avoid excessive casualties from Viet Minh anti-aircraft fire. Some reinforcements did arrive, but not enough to replace French casualties.
Trench warfare.
On 5 April, after a long night of battle, French fighter-bombers and artillery inflicted particularly devastating losses on one Viet Minh regiment, which was caught on open ground. At that point, Giáp decided to change tactics. Although Giáp still had the same objective—to overrun French defenses east of the river—he decided to employ entrenchment and sapping to try to achieve it.
On 10 April, the French attempted to retake "Eliane 1", which had been lost eleven days earlier. The loss posed a significant threat to "Eliane 4", and the French wanted to eliminate that threat. The dawn attack, which Bigeard devised, was preceded by a short, massive artillery barrage, followed by small unit infiltration attacks, followed by mopping-up operations. "Eliane 1" changed hands several times that day, but by the next morning the French had control of the strongpoint. The Viet Minh attempted to retake it on the evening of 12 April, but were pushed back.
At this point, the morale of the Viet Minh soldiers was greatly lowered due to the massive casualties they had received. During a period of stalemate from 15 April to 1 May, the French intercepted enemy radio messages which told of whole units refusing orders to attack, and Communist prisoners said that they were told to advance or be shot by the officers and noncommissioned officers behind them. Worse still, the Viet Minh lacked advanced medical care, with one stating that "Nothing strikes at combat morale like the knowledge that if wounded, the soldier will go uncared for". To avert the crisis of mutiny, Giáp called in fresh reinforcements from Laos.
During the fighting at "Eliane 1", on the other side of camp, the Viet Minh entrenchments had almost entirely surrounded "Huguette 1 and 6". On 11 April, the garrison of "Huguette 1" attacked, and was joined by artillery from the garrison of "Claudine". The goal was to resupply "Huguette 6" with water and ammunition. The attacks were repeated on the nights of the 14/15 and 16/17 of April. While they did succeed in getting some supplies through, the French suffered heavy casualties, which convinced Langlais to abandon "Huguette 6". Following a failed attempt to link up, on 18 April, the defenders at "Huguette 6" made a daring break out, but only a few managed to make it to French lines. The Viet Minh repeated the isolation and probing attacks against Huguette 1, and overran the fort on the morning of 22 April. With the fall of "Huguette 1", the Viet Minh took control of more than 90% of the airfield, making accurate parachute drops impossible. This caused the landing zone to become perilously small, and effectively choked off much needed supplies. A French attack against "Huguette 1" later that day was repulsed.
Isabelle.
"Isabelle" saw only light action until 30 March, when the Viet Minh succeeded in isolating it and beating back the attempt to send reinforcements north. Following a massive artillery barrage on 30 March, the Viet Minh began employing the same trench warfare tactics that they were using against the central camp. By the end of April, "Isabelle" had exhausted its water supply and was nearly out of ammunition.
Final attacks.
The Viet Minh launched a massed assault against the exhausted defenders on the night of 1 May, overrunning "Eliane 1", "Dominique 3", and "Huguette 5", although the French managed to beat back attacks on "Eliane 2". On 6 May, the Viet Minh launched another massed attack against "Eliane 2". The attack included, for the first time, Katyusha rockets. The French artillery fired a "TOT" (Time On Target) attack, so artillery rounds fired from different positions would strike on target at the same time. This barrage defeated the first assault wave. A few hours later that night, the Viet Minh detonated a mine shaft, blowing "Eliane 2" up. The Viet Minh attacked again, and within a few hours had overrun the defenders.
On 7 May, Giáp ordered an all-out attack against the remaining French units with over 25,000 Viet Minh against fewer than 3,000 garrison troops. At 17:00, de Castries radioed French headquarters in Hanoi and talked with Cogny. 
By nightfall, all French central positions had been captured. The last radio transmission from the French headquarters reported that enemy troops were directly outside the headquarters bunker and that all the positions had been overrun. The radio operator in his last words stated: "The enemy has overrun us. We are blowing up everything. "Vive la France"!" That night the garrison made a breakout attempt, in the Camarón tradition. While some of the main body managed to break out, none succeeded in escaping the valley. However at "Isabelle", a similar attempt later the same night saw about 70 troops, out of 1,700 men in the garrison, escape to Laos.
Aftermath.
Prisoners.
On 8 May, the Viet Minh counted 11,721 prisoners, of whom 4,436 were wounded. This was the greatest number the Viet Minh had ever captured: one-third of the total captured during the entire war. The prisoners were divided into groups. Able-bodied soldiers were force-marched over to prison camps to the north and east, where they were intermingled with Viet Minh soldiers to discourage French bombing runs. Hundreds died of disease along the way. The wounded were given basic first aid until the Red Cross arrived, removed 858, and provided better aid to the remainder. Those wounded who were not evacuated by the Red Cross were sent into detention.
One of their gaolers was Georges Boudarel, a French academic and Communist militant. He was accused of torturing French prisoners for the Viet Minh during the First Indochina War.
Of 10,863 survivors held as prisoners, only 3,290 were officially repatriated four months later; however, the losses figure may include the 3,013 prisoners of Vietnamese origin whose eventual fate is unknown.
Political ramifications.
The garrison constituted roughly a tenth of the total French Union manpower in Indochina. The defeat seriously weakened the position and prestige of the French as previously planned negotiations over the future of Indochina began.
The Geneva Conference opened on 8 May 1954, the day after the surrender of the garrison. Ho Chi Minh entered the conference on the opening day with the news of his troops' victory in the headlines. The resulting agreement temporarily partitioned Vietnam into two zones: the North was administered by the communist Democratic Republic of Vietnam while the South was administered by the French-supported State of Vietnam. The last units of the French Union forces withdrew from Indochina in 1956. This partition was supposed to be temporary, and the two zones were meant to be reunited through national elections in 1956. After the French withdrawal, the United States supported the southern government, under Emperor Bao Dai and Prime Minister Ngo Dinh Diem, which opposed the Geneva agreement, and which claimed that Ho Chi Minh's forces from the North had been killing Northern loyalists and terrorizing people both north and south. The North was supported by both the People's Republic of China (PRC) and the Soviet Union (USSR). This arrangement proved tenuous and escalated into the Vietnam War (Second Indochina War), eventually bringing 500,000 American troops into South Vietnam.
France's defeat in Indochina, coupled with the German destruction of her armies just 14 years earlier, seriously damaged its prestige elsewhere in its colonial empire, as well as with its NATO allies, most importantly, the United States. Within her empire, the defeat in Indochina served to spur independence movements in other colonies, notably the North African territories from which many of the troops who fought at Điện Biên Phủ had been recruited. In 1954, six months after the battle at Điện Biên Phủ ended, the Algerian War started, and by 1956 both the Moroccan and Tunisian protectorates had gained independence. A French board of inquiry, the Catroux Commission, later investigated the defeat.
American participation.
According to the Mutual Defense Assistance Act, the United States provided the French with material aid during the battle – aircraft (supplied by the ), weapons, mechanics, 24 CIA/CAT pilots, and U.S. Air Force maintenance crews. The United States, however, intentionally avoided overt direct intervention. In February 1954, following the French occupation of Điện Biên Phủ but prior to the battle, Democratic senator Michael Mansfield asked United States Defense Secretary Charles Erwin Wilson whether the United States would send naval or air units if the French were subjected to greater pressure there, but Wilson replied that "for the moment there is no justification for raising United States aid above its present level". President Dwight D. Eisenhower also stated, "Nobody is more opposed to intervention than I am". On 31 March, following the fall of "Beatrice", "Gabrielle", and "Anne-Marie", a panel of U.S. Senators and House Representatives questioned the American Chairman of the Joint Chiefs of Staff, Admiral Arthur W. Radford, about the possibility of American involvement. Radford concluded it was too late for the U.S. Air Force to save the French garrison. A proposal for direct intervention was unanimously voted down by the panel, which "concluded that intervention was a positive act of war".
The United States did covertly participate in the battle. Following a request for help from Henri Navarre, Radford provided two squadrons of B-26 Invader bomber aircraft to support the French. Subsequently, 37 American transport pilots flew 682 sorties over the course of the battle. Earlier, in order to succeed the pre-Điện Biên Phủ Operation Castor of November 1953, General Chester McCarty made available 12 additional C-119 Flying Boxcars flown by French crews.
Two of the American pilots, James McGovern, Jr. and Wallace Buford, were killed in action during the siege of Điện Biên Phủ. On 25 February 2005, the seven still living American pilots were awarded the French Legion of Honor by Jean-David Levitte, the French ambassador to the United States. The role that the American pilots played in this battle had remained little known until 2004. The American historian Erik Kirsinger researched the case for more than a year to establish the facts. French author Jules Roy suggests that Admiral Radford discussed with the French the possibility of using nuclear weapons in support of the French garrison. Moreover, John Foster Dulles reportedly mentioned the possibility of lending atomic bombs to the French for use at Điện Biên Phủ, and a similar source claims that the British Foreign Secretary Sir Anthony Eden was aware of the possibility of the use of nuclear weapons in that region.
Khe Sanh.
In January 1968, during the Vietnam War, the North Vietnamese Army (still under Giáp's command) initiated a siege and artillery bombardment on the U.S. Marine Corps infantry and artillery base at Khe Sanh, South Vietnam. Historians are divided on whether this was a genuine attempt to repeat their success at Điện Biên Phủ by forcing the surrender of the Marine base, or else a diversion from the rest of the Tết Offensive, or an example of the North Vietnamese Army keeping its options open. At Khe Sanh, a number of factors were significantly different from Điện Biên Phủ. Khe Sanh was much closer to an American supply base () compared to a French one at Điện Biên Phủ ().
At Khe Sanh, the U.S. Marines held the high ground, and their artillery forced the North Vietnamese to use their own artillery from a much greater distance. By contrast, at Điện Biên Phủ, the French artillery (six 105 mm batteries and one battery of four 155 mm howitzers and mortars) were only sporadically effective; Khe Sanh received 18,000 tons of aerial resupplies during the 77-day battle, whereas during the 167 days that the French forces at Điện Biên Phủ held out, they received only 4,000 tons. And lastly, the US Air Force dropped 114,810 tons of bombs on the Vietnamese at Khe Sanh – roughly as much as on Japan in 1945 during World War II.
Women at Điện Biên Phủ.
Many of the flights operated by the French Air force to evacuate casualties had female flight nurses on board. A total of 15 women served on flights to Điện Biên Phủ. One, Geneviève de Galard, was stranded there when her plane was destroyed by shellfire while being repaired on the airfield. She remained on the ground providing medical services in the field hospital until the surrender. She was later referred to as the "Angel of Điện Biên Phủ". However, historians disagree regarding this moniker, with Martin Windrow maintaining that de Galard was referred to by this name by the garrison itself, but Michael Kenney and Bernard Fall maintaining it was added by outside press agencies.
The French forces came to Điện Biên Phủ accompanied by two "bordels mobiles de campagne", ("mobile field brothels"), served by Algerian and Vietnamese women. When the siege ended, the Viet Minh sent the surviving Vietnamese women for "re-education".
In popular culture.
The battle was depicted in two films.
Dien Bien Phu Falls is mentioned in 1989 song "We Didn't Start the Fire" by Billy Joel.

</doc>
<doc id="66338" url="https://en.wikipedia.org/wiki?curid=66338" title="Holography">
Holography

Holography is the science and practice of making holograms. Typically, a hologram is a photographic recording of a light field, rather than of an image formed by a lens, and it is used to display a fully three-dimensional image of the holographed subject, which is seen without the aid of special glasses or other intermediate optics. The hologram itself is not an image and it is usually unintelligible when viewed under diffuse ambient light. It is an encoding of the light field as an interference pattern of seemingly random variations in the opacity, density, or surface profile of the photographic medium. When suitably lit, the interference pattern diffracts the light into a reproduction of the original light field and the objects that were in it appear to still be there, exhibiting visual depth cues such as parallax and perspective that change realistically with any change in the relative position of the observer.
In its pure form, holography requires the use of laser light for illuminating the subject and for viewing the finished hologram. In a side-by-side comparison under optimal conditions, a holographic image is visually indistinguishable from the actual subject, if the hologram and the subject are lit just as they were at the time of recording. A microscopic level of detail throughout the recorded volume of space can be reproduced. In common practice, however, major image quality compromises are made to eliminate the need for laser illumination when viewing the hologram, and sometimes, to the extent possible, also when making it. Holographic portraiture often resorts to a non-holographic intermediate imaging procedure, to avoid the hazardous high-powered pulsed lasers otherwise needed to optically "freeze" living subjects as perfectly as the extremely motion-intolerant holographic recording process requires. Holograms can now also be entirely computer-generated and show objects or scenes that never existed.
Holography should not be confused with lenticular and other earlier autostereoscopic 3D display technologies, which can produce superficially similar results but are based on conventional lens imaging. Stage illusions such as Pepper's Ghost and other unusual, baffling, or seemingly magical images are also often incorrectly called holograms.
Overview and history.
The Hungarian-British physicist Dennis Gabor (in Hungarian: "Gábor Dénes"), was awarded the Nobel Prize in Physics in 1971 "for his invention and development of the holographic method".
His work, done in the late 1940s, was built on pioneering work in the field of X-ray microscopy by other scientists including Mieczysław Wolfke in 1920 and William Lawrence Bragg in 1939. The discovery was an unexpected result of research into improving electron microscopes at the British Thomson-Houston (BTH) Company in Rugby, England, and the company filed a patent in December 1947 (patent GB685286). The technique as originally invented is still used in electron microscopy, where it is known as electron holography, but optical holography did not really advance until the development of the laser in 1960. The word "holography" comes from the Greek words ("holos"; "whole") and ("graphē"; "writing" or "drawing").
The development of the laser enabled the first practical optical holograms that recorded 3D objects to be made in 1962 by Yuri Denisyuk in the Soviet Union and by Emmett Leith and Juris Upatnieks at the University of Michigan, USA. Early holograms used silver halide photographic emulsions as the recording medium. They were not very efficient as the produced grating absorbed much of the incident light. Various methods of converting the variation in transmission to a variation in refractive index (known as "bleaching") were developed which enabled much more efficient holograms to be produced.
Several types of holograms can be made. Transmission holograms, such as those produced by Leith and Upatnieks, are viewed by shining laser light through them and looking at the reconstructed image from the side of the hologram opposite the source. A later refinement, the "rainbow transmission" hologram, allows more convenient illumination by white light rather than by lasers. Rainbow holograms are commonly used for security and authentication, for example, on credit cards and product packaging.
Another kind of common hologram, the reflection or Denisyuk hologram, can also be viewed using a white-light illumination source on the same side of the hologram as the viewer and is the type of hologram normally seen in holographic displays. They are also capable of multicolour-image reproduction.
Specular holography is a related technique for making three-dimensional images by controlling the motion of specularities on a two-dimensional surface. It works by reflectively or refractively manipulating bundles of light rays, whereas Gabor-style holography works by diffractively reconstructing wavefronts.
Most holograms produced are of static objects but systems for displaying changing scenes on a holographic volumetric display are now being developed.
Holograms can also be used to store, retrieve, and process information optically.
In its early days, holography required high-power expensive lasers, but nowadays, mass-produced low-cost semi-conductor or diode lasers, such as those found in millions of DVD recorders and used in other common applications, can be used to make holograms and have made holography much more accessible to low-budget researchers, artists and dedicated hobbyists.
It was thought that it would be possible to use X-rays to make holograms of very small objects and view them using visible light. Today, holograms with x-rays are generated by using synchrotrons or x-ray free-electron lasers as radiation sources and pixelated detectors such as CCDs as recording medium. The reconstruction is then retrieved via computation. Due to the shorter wavelength of x-rays compared to visible light, this approach allows to image objects with higher spatial resolution. As free-electron lasers can provide ultrashort and x-ray pulses in the range of femtoseconds which are intense and coherent, x-ray holography has been used to capture ultrafast dynamic processes.
How holography works.
Holography is a technique that enables a light field, which is generally the product of a light source scattered off objects, to be recorded and later reconstructed when the original light field is no longer present, due to the absence of the original objects. Holography can be thought of as somewhat similar to sound recording, whereby a sound field created by vibrating matter like musical instruments or vocal cords, is encoded in such a way that it can be reproduced later, without the presence of the original vibrating matter.
Laser.
In laser holography, the hologram is recorded using a flash of laser light that illuminates a scene and then imprints on a recording medium, much in the way a photograph is recorded. In addition, however, part of the light beam must be shone directly onto the recording medium - this second light beam is known as the reference beam. A hologram requires a laser as the sole light source. Lasers can be precisely controlled and have a fixed wavelength, unlike sunlight or light from conventional sources, which contain many different wavelengths. To prevent external light from interfering, holograms are usually taken in darkness, or in low level light of a different color from the laser light used in making the hologram. Holography requires a specific exposure time (just like photography), which can be controlled using a shutter, or by electronically timing the laser.
Apparatus.
A hologram can be made by shining part of the light beam directly into the recording medium, 
and the other part onto the object in such a way that some of the scattered light falls onto the recording medium.
A more flexible arrangement for recording a hologram requires the laser beam to be aimed through a series of elements that change it in different ways. The first element is a beam splitter that divides the beam into two identical beams, each aimed in different directions:
Several different materials can be used as the recording medium. One of the most common is a film very similar to photographic film (silver halide photographic emulsion), but with a much higher concentration of light-reactive grains, making it capable of the much higher resolution that holograms require. A layer of this recording medium (e.g., silver halide) is attached to a transparent substrate, which is commonly glass, but may also be plastic.
Process.
When the two laser beams reach the recording medium, their light waves intersect and interfere with each other. It is this interference pattern that is imprinted on the recording medium. The pattern itself is seemingly random, as it represents the way in which the scene's light "interfered" with the original light source — but not the original light source itself. The interference pattern can be considered an encoded version of the scene, requiring a particular key — the original light source — in order to view its contents.
This missing key is provided later by shining a laser, identical to the one used to record the hologram, onto the developed film. When this beam illuminates the hologram, it is diffracted by the hologram's surface pattern. This produces a light field identical to the one originally produced by the scene and scattered onto the hologram.
Holography vs. photography.
Holography may be better understood via an examination of its differences from ordinary photography:
Physics of holography.
For a better understanding of the process, it is necessary to understand interference and diffraction. Interference occurs when one or more wavefronts are superimposed. Diffraction occurs whenever a wavefront encounters an object. The process of producing a holographic reconstruction is explained below purely in terms of interference and diffraction. It is somewhat simplified but is accurate enough to provide an understanding of how the holographic process works.
For those unfamiliar with these concepts, it is worthwhile to read the respective articles before reading further in this article.
Plane wavefronts.
A diffraction grating is a structure with a repeating pattern. A simple example is a metal plate with slits cut at regular intervals. A light wave incident on a grating is split into several waves; the direction of these diffracted waves is determined by the grating spacing and the wavelength of the light.
A simple hologram can be made by superimposing two plane waves from the same light source on a holographic recording medium. The two waves interfere giving a straight line fringe pattern whose intensity varies sinusoidally across the medium. The spacing of the fringe pattern is determined by the angle between the two waves, and on the wavelength of the light.
The recorded light pattern is a diffraction grating. When it is illuminated by only one of the waves used to create it, it can be shown that one of the diffracted waves emerges at the same angle as that at which the second wave was originally incident so that the second wave has been 'reconstructed'. Thus, the recorded light pattern is a holographic recording as defined above.
Point sources.
If the recording medium is illuminated with a point source and a normally incident plane wave, the resulting pattern is a sinusoidal zone plate which acts as a negative Fresnel lens whose focal length is equal to the separation of the point source and the recording plane.
When a plane wavefront illuminates a negative lens, it is expanded into a wave which appears to diverge from the focal point of the lens. Thus, when the recorded pattern is illuminated with the original plane wave, some of the light is diffracted into a diverging beam equivalent to the original plane wave; a holographic recording of the point source has been created.
When the plane wave is incident at a non-normal angle, the pattern formed is more complex but still acts as a negative lens provided it is illuminated at the original angle.
Complex objects.
To record a hologram of a complex object, a laser beam is first split into two separate beams of light. One beam illuminates the object, which then scatters light onto the recording medium. According to diffraction theory, each point in the object acts as a point source of light so the recording medium can be considered to be illuminated by a set of point sources located at varying distances from the medium.
The second (reference) beam illuminates the recording medium directly. Each point source wave interferes with the reference beam, giving rise to its own sinusoidal zone plate in the recording medium. The resulting pattern is the sum of all these 'zone plates' which combine to produce a random (speckle) pattern as in the photograph above.
When the hologram is illuminated by the original reference beam, each of the individual zone plates reconstructs the object wave which produced it, and these individual wavefronts add together to reconstruct the whole of the object beam. The viewer perceives a wavefront that is identical to the wavefront scattered from the object onto the recording medium, so that it appears to him or her that the object is still in place even if it has been removed.
Mathematical model.
A single-frequency light wave can be modeled by a complex number U, which represents the electric or magnetic field of the light wave. The amplitude and phase of the light are represented by the absolute value and angle of the complex number. The object and reference waves at any point in the holographic system are given by UO and UR. The combined beam is given by UO + UR. The energy of the combined beams is proportional to the square of magnitude of the combined waves as:
If a photographic plate is exposed to the two beams and then developed, its transmittance, T, is proportional to the light energy that was incident on the plate and is given by
where "k" is a constant.
When the developed plate is illuminated by the reference beam, the light transmitted through the plate, UH is equal to the transmittance T multiplied by the reference beam amplitude UR, giving
It can be seen that UH has four terms, each representing a light beam emerging from the hologram. The first of these is proportional to UO. This is the reconstructed object beam which enables a viewer to 'see' the original object even when it is no longer present in the field of view.
The second and third beams are modified versions of the reference beam. The fourth term is known as the "conjugate object beam". It has the reverse curvature to the object beam itself and forms a real image of the object in the space beyond the holographic plate.
When the reference and object beams are incident on the holographic recording medium at significantly different angles, the virtual, real and reference wavefronts all emerge at different angles, enabling the reconstructed object to be seen clearly.
Recording a hologram.
Items required.
To make a hologram, the following are required:
These requirements are inter-related, and it is essential to understand the nature of optical interference to see this. Interference is the variation in intensity which can occur when two light waves are superimposed. The intensity of the maxima exceeds the sum of the individual intensities of the two beams, and the intensity at the minima is less than this and may be zero. The interference pattern maps the relative phase between the two waves, and any change in the relative phases causes the interference pattern to move across the field of view. If the relative phase of the two waves changes by one cycle, then the pattern drifts by one whole fringe. One phase cycle corresponds to a change in the relative distances travelled by the two beams of one wavelength. Since the wavelength of light is of the order of 0.5 μm, it can be seen that very small changes in the optical paths travelled by either of the beams in the holographic recording system lead to movement of the interference pattern which is the holographic recording. Such changes can be caused by relative movements of any of the optical components or the object itself, and also by local changes in air-temperature. It is essential that any such changes are significantly less than the wavelength of light if a clear well-defined recording of the interference is to be created.
The exposure time required to record the hologram depends on the laser power available, on the particular medium used and on the size and nature of the object(s) to be recorded, just as in conventional photography. This determines the stability requirements. Exposure times of several minutes are typical when using quite powerful gas lasers and silver halide emulsions. All the elements within the optical system have to be stable to fractions of a μm over that period. It is possible to make holograms of much less stable objects by using a pulsed laser which produces a large amount of energy in a very short time (μs or less). These systems have been used to produce holograms of live people. A holographic portrait of Dennis Gabor was produced in 1971 using a pulsed ruby laser.
Thus, the laser power, recording medium sensitivity, recording time and mechanical and thermal stability requirements are all interlinked. Generally, the smaller the object, the more compact the optical layout, so that the stability requirements are significantly less than when making holograms of large objects.
Another very important laser parameter is its coherence. This can be envisaged by considering a laser producing a sine wave whose frequency drifts over time; the coherence length can then be considered to be the distance over which it maintains a single frequency. This is important because two waves of different frequencies do not produce a stable interference pattern. The coherence length of the laser determines the depth of field which can be recorded in the scene. A good holography laser will typically have a coherence length of several meters, ample for a deep hologram.
The objects that form the scene must, in general, have optically rough surfaces so that they scatter light over a wide range of angles. A specularly reflecting (or shiny) surface reflects the light in only one direction at each point on its surface, so in general, most of the light will not be incident on the recording medium. A hologram of a shiny object can be made by locating it very close to the recording plate.
Hologram classifications.
There are three important properties of a hologram which are defined in this section. A given hologram will have one or other of each of these three properties, e.g. an amplitude modulated thin transmission hologram, or a phase modulated, volume reflection hologram.
Amplitude and phase modulation holograms.
An amplitude modulation hologram is one where the amplitude of light diffracted by the hologram is proportional to the intensity of the recorded light. A straightforward example of this is photographic emulsion on a transparent substrate. The emulsion is exposed to the interference pattern, and is subsequently developed giving a transmittance which varies with the intensity of the pattern - the more light that fell on the plate at a given point, the darker the developed plate at that point.
A phase hologram is made by changing either the thickness or the refractive index of the material in proportion to the intensity of the holographic interference pattern. This is a phase grating and it can be shown that when such a plate is illuminated by the original reference beam, it reconstructs the original object wavefront. The efficiency (i.e., the fraction of the illuminated beam which is converted to reconstructed object beam) is greater for phase than for amplitude modulated holograms.
Thin holograms and thick (volume) holograms.
A thin hologram is one where the thickness of the recording medium is much less than the spacing of the interference fringes which make up the holographic recording.
A thick or volume hologram is one where the thickness of the recording medium is greater than the spacing of the interference pattern. The recorded hologram is now a three dimensional structure, and it can be shown that incident light is diffracted by the grating only at a particular angle, known as the Bragg angle. If the hologram is illuminated with a light source incident at the original reference beam angle but a broad spectrum of wavelengths; reconstruction occurs only at the wavelength of the original laser used. If the angle of illumination is changed, reconstruction will occur at a different wavelength and the colour of the re-constructed scene changes. A volume hologram effectively acts as a colour filter.
Transmission and reflection holograms.
A transmission hologram is one where the object and reference beams are incident on the recording medium from the same side. In practice, several more mirrors may be used to direct the beams in the required directions.
Normally, transmission holograms can only be reconstructed using a laser or a quasi-monochromatic source, but a particular type of transmission hologram, known as a rainbow hologram, can be viewed with white light.
In a reflection hologram, the object and reference beams are incident on the plate from opposite sides of the plate. The reconstructed object is then viewed from the same side of the plate as that at which the re-constructing beam is incident.
Only volume holograms can be used to make reflection holograms, as only a very low intensity diffracted beam would be reflected by a thin hologram.
Holographic recording media.
The recording medium has to convert the original interference pattern into an optical element that modifies either the amplitude or the phase of an incident light beam in proportion to the intensity of the original light field.
The recording medium should be able to resolve fully all the fringes arising from interference between object and reference beam. These fringe spacings can range from tens of micrometers to less than one micrometer, i.e. spatial frequencies ranging from a few hundred to several thousand cycles/mm, and ideally, the recording medium should have a response which is flat over this range. If the response of the medium to these spatial frequencies is low, the diffraction efficiency of the hologram will be poor, and a dim image will be obtained. Standard photographic film has a very low or even zero response at the frequencies involved and cannot be used to make a hologram - see, for example, Kodak's professional black and white film whose resolution starts falling off at 20 lines/mm — it is unlikely that any reconstructed beam could be obtained using this film.
If the response is not flat over the range of spatial frequencies in the interference pattern, then the resolution of the reconstructed image may also be degraded.
The table below shows the principal materials used for holographic recording. Note that these do not include the materials used in the mass replication of an existing hologram, which are discussed in the next section. The resolution limit given in the table indicates the maximal number of interference lines/mm of the gratings. The required exposure, expressed as millijoules (mJ) of photon energy impacting the surface area, is for a long exposure time. Short exposure times (less than of a second, such as with a pulsed laser) require much higher exposure energies, due to reciprocity failure.
Copying and mass production.
An existing hologram can be copied by embossing or optically.
Most holographic recordings (e.g. bleached silver halide, photoresist, and photopolymers) have surface relief patterns which conform with the original illumination intensity. Embossing, which is similar to the method used to stamp out plastic discs from a master in audio recording, involves copying this surface relief pattern by impressing it onto another material.
The first step in the embossing process is to make a stamper by electrodeposition of nickel on the relief image recorded on the photoresist or photothermoplastic. When the nickel layer is thick enough, it is separated from the master hologram and mounted on a metal backing plate. The material used to make embossed copies consists of a polyester base film, a resin separation layer and a thermoplastic film constituting the holographic layer.
The embossing process can be carried out with a simple heated press. The bottom layer of the duplicating film (the thermoplastic layer) is heated above its softening point and pressed against the stamper, so that it takes up its shape. This shape is retained when the film is cooled and removed from the press. In order to permit the viewing of embossed holograms in reflection, an additional reflecting layer of aluminum is usually added on the hologram recording layer. This method is particularly suited to mass production.
The first book to feature a hologram on the front cover was "The Skook" (Warner Books, 1984) by JP Miller, featuring an illustration by Miller. That same year, "Telstar" by Ad Infinitum became the first record with a hologram cover and "National Geographic" published the first magazine with a hologram cover. Embossed holograms are used widely on credit cards, banknotes, and high value products for authentication purposes.
It is possible to print holograms directly into steel using a sheet explosive charge to create the required surface relief. The Royal Canadian Mint produces holographic gold and silver coinage through a complex stamping process.
A hologram can be copied optically by illuminating it with a laser beam, and locating a second hologram plate so that it is illuminated both by the reconstructed object beam, and the illuminating beam. Stability and coherence requirements are significantly reduced if the two plates are located very close together. An index matching fluid is often used between the plates to minimize spurious interference between the plates. Uniform illumination can be obtained by scanning point-by-point or with a beam shaped into a thin line.
Reconstructing and viewing the holographic image.
When the hologram plate is illuminated by a laser beam identical to the reference beam which was used to record the hologram, an exact reconstruction of the original object wavefront is obtained. An imaging system (an eye or a camera) located in the reconstructed beam 'sees' exactly the same scene as it would have done when viewing the original. When the lens is moved, the image changes in the same way as it would have done when the object was in place. If several objects were present when the hologram was recorded, the reconstructed objects move relative to one another, i.e. exhibit parallax, in the same way as the original objects would have done. It was very common in the early days of holography to use a chess board as the object and then take photographs at several different angles using the reconstructed light to show how the relative positions of the chess pieces appeared to change.
A holographic image can also be obtained using a different laser beam configuration to the original recording object beam, but the reconstructed image will not match the original exactly. When a laser is used to reconstruct the hologram, the image is speckled just as the original image will have been. This can be a major drawback in viewing a hologram.
White light consists of light of a wide range of wavelengths. Normally, if a hologram is illuminated by a white light source, each wavelength can be considered to generate its own holographic reconstruction, and these will vary in size, angle, and distance. These will be superimposed, and the summed image will wipe out any information about the original scene, as if superimposing a set of photographs of the same object of different sizes and orientations. However, a holographic image can be obtained using white light in specific circumstances, e.g. with volume holograms and rainbow holograms. The white light source used to view these holograms should always approximate to a point source, i.e. a spot light or the sun. An extended source (e.g. a fluorescent lamp) will not reconstruct a hologram since its light is incident at each point at a wide range of angles, giving multiple reconstructions which will "wipe" one another out.
White light reconstructions do not contain speckles.
Volume holograms.
A volume hologram can give a reconstructed beam using white light, as the hologram structure effectively filters out colours other than those equal to or very close to the colour of the laser used to make the hologram so that the reconstructed image will appear to be approximately the same colour as the laser light used to create the holographic recording.
Rainbow holograms.
In this method, parallax in the vertical plane is sacrificed to allow a bright well-defined single color re-constructed image to be obtained using white light. The rainbow holography recording process uses a horizontal slit to eliminate vertical parallax in the output image. The viewer is then effectively viewing the holographic image through a narrow horizontal slit. Horizontal parallax information is preserved but movement in the vertical direction produces color rather than different vertical perspectives. Stereopsis and horizontal motion parallax, two relatively powerful cues to depth, are preserved.
The holograms found on credit cards are examples of rainbow holograms. These are technically transmission holograms mounted onto a reflective surface like a metalized polyethylene terephthalate substrate commonly known as PET.
Fidelity of the reconstructed beam.
To replicate the original object beam exactly, the reconstructing reference beam must be identical to the original reference beam and the recording medium must be able to fully resolve the interference pattern formed between the object and reference beams. Exact reconstruction is required in holographic interferometry, where the holographically reconstructed wavefront interferes with the wavefront coming from the actual object, giving a null fringe if there has been no movement of the object and mapping out the displacement if the object has moved. This requires very precise relocation of the developed holographic plate.
Any change in the shape, orientation or wavelength of the reference beam gives rise to aberrations in the reconstructed image. For instance, the reconstructed image is magnified if the laser used to reconstruct the hologram has a shorter wavelength than the original laser. Nonetheless, good reconstruction is obtained using a laser of a different wavelength, quasi-monochromatic light or white light, in the right circumstances.
Since each point in the object illuminates all of the hologram, the whole object can be reconstructed from a small part of the hologram. Thus, a hologram can be broken up into small pieces and each one will enable the whole of the original object to be imaged. One does, however, lose information and the spatial resolution gets worse as the size of the hologram is decreased — the image becomes "fuzzier". The field of view is also reduced, and the viewer will have to change position to see different parts of the scene.
Applications.
Art.
Early on, artists saw the potential of holography as a medium and gained access to science laboratories to create their work. Holographic art is often the result of collaborations between scientists and artists, although some holographers would regard themselves as both an artist and a scientist.
Salvador Dalí claimed to have been the first to employ holography artistically. He was certainly the first and best-known surrealist to do so, but the 1972 New York exhibit of Dalí holograms had been preceded by the holographic art exhibition that was held at the Cranbrook Academy of Art in Michigan in 1968 and by the one at the Finch College gallery in New York in 1970, which attracted national media attention. In Great Britain, Margaret Benyon began using holography as an artistic medium in the late 1960s and had a solo exhibition at the University of Nottingham art gallery in 1969. This was followed in 1970 by a solo show at the Lisson Gallery in London, which was billed as the "first London expo of holograms and stereoscopic paintings".
During the 1970s, a number of art studios and schools were established, each with their particular approach to holography. Notably, there was the San Francisco School of Holography established by Lloyd Cross, The Museum of Holography in New York founded by Rosemary (Posy) H. Jackson, the Royal College of Art in London and the Lake Forest College Symposiums organised by Tung Jeong. None of these studios still exist; however, there is the Center for the Holographic Arts in New York and the HOLOcenter in Seoul, which offers artists a place to create and exhibit work.
During the 1980s, many artists who worked with holography helped the diffusion of this so-called "new medium" in the art world, such as Harriet Casdin-Silver of the USA, Dieter Jung of Germany, and Moysés Baumstein of Brazil, each one searching for a proper "language" to use with the three-dimensional work, avoiding the simple holographic reproduction of a sculpture or object. For instance, in Brazil, many concrete poets (Augusto de Campos, Décio Pignatari, Julio Plaza and José Wagner Garcia, associated with Moysés Baumstein) found in holography a way to express themselves and to renew Concrete Poetry.
A small but active group of artists still integrate holographic elements into their work. Some are associated with novel holographic techniques; for example, artist Matt Brand employed computational mirror design to eliminate image distortion from specular holography.
The MIT Museum and Jonathan Ross both have extensive collections of holography and on-line catalogues of art holograms.
Data storage.
Holography can be put to a variety of uses other than recording images. Holographic data storage is a technique that can store information at high density inside crystals or photopolymers. The ability to store large amounts of information in some kind of medium is of great importance, as many electronic products incorporate storage devices. As current storage techniques such as Blu-ray Disc reach the limit of possible data density (due to the diffraction-limited size of the writing beams), holographic storage has the potential to become the next generation of popular storage media. The advantage of this type of data storage is that the volume of the recording media is used instead of just the surface.
Currently available SLMs can produce about 1000 different images a second at 1024×1024-bit resolution. With the right type of medium (probably polymers rather than something like LiNbO3), this would result in about one-gigabit-per-second writing speed. Read speeds can surpass this, and experts believe one-terabit-per-second readout is possible.
In 2005, companies such as Optware and Maxell produced a 120 mm disc that uses a holographic layer to store data to a potential 3.9 TB, a format called Holographic Versatile Disc. As of September 2014, no commercial product has been released.
Another company, InPhase Technologies, was developing a competing format, but went bankrupt in 2011 and all its assets were sold to Akonia Holographics, LLC.
While many holographic data storage models have used "page-based" storage, where each recorded hologram holds a large amount of data, more recent research into using submicrometre-sized "microholograms" has resulted in several potential 3D optical data storage solutions. While this approach to data storage can not attain the high data rates of page-based storage, the tolerances, technological hurdles, and cost of producing a commercial product are significantly lower.
Dynamic holography.
In static holography, recording, developing and reconstructing occur sequentially, and a permanent hologram is produced.
There also exist holographic materials that do not need the developing process and can record a hologram in a very short time. This allows one to use holography to perform some simple operations in an all-optical way. Examples of applications of such real-time holograms include phase-conjugate mirrors ("time-reversal" of light), optical cache memories, image processing (pattern recognition of time-varying images), and optical computing.
The amount of processed information can be very high (terabits/s), since the operation is performed in parallel on a whole image. This compensates for the fact that the recording time, which is in the order of a microsecond, is still very long compared to the processing time of an electronic computer. The optical processing performed by a dynamic hologram is also much less flexible than electronic processing. On one side, one has to perform the operation always on the whole image, and on the other side, the operation a hologram can perform is basically either a multiplication or a phase conjugation. In optics, addition and Fourier transform are already easily performed in linear materials, the latter simply by a lens. This enables some applications, such as a device that compares images in an optical way.
The search for novel nonlinear optical materials for dynamic holography is an active area of research. The most common materials are photorefractive crystals, but in semiconductors or semiconductor heterostructures (such as quantum wells), atomic vapors and gases, plasmas and even liquids, it was possible to generate holograms.
A particularly promising application is optical phase conjugation. It allows the removal of the wavefront distortions a light beam receives when passing through an aberrating medium, by sending it back through the same aberrating medium with a conjugated phase. This is useful, for example, in free-space optical communications to compensate for atmospheric turbulence (the phenomenon that gives rise to the twinkling of starlight).
Hobbyist use.
Since the beginning of holography, experimenters have explored its uses. Starting in 1971, Lloyd Cross started the San Francisco School of Holography and started to teach amateurs the methods of making holograms with inexpensive equipment. This method relied on the use of a large table of deep sand to hold the optics rigid and damp vibrations that would destroy the image.
Many of these holographers would go on to produce art holograms. In 1983, Fred Unterseher published the "Holography Handbook", a remarkably easy-to-read description of making holograms at home. This brought in a new wave of holographers and gave simple methods to use the then-available AGFA silver halide recording materials.
In 2000, Frank DeFreitas published the "Shoebox Holography Book" and introduced the use of inexpensive laser pointers to countless hobbyists. This was a very important development for amateurs, as the cost for a 5 mW laser dropped from $1200 to $5 as semiconductor laser diodes reached mass market. Now, there are hundreds to thousands of amateur holographers worldwide.
By late 2000, holography kits with the inexpensive laser pointer diodes entered the mainstream consumer market. These kits enabled students, teachers, and hobbyists to make many kinds of holograms without specialized equipment, and became popular gift items by 2005. The introduction of holography kits with self-developing film plates in 2003 made it even possible for hobbyists to make holograms without using chemical developers.
In 2006, a large number of surplus holography quality green lasers (Coherent C315) became available and put dichromated gelatin (DCG) within the reach of the amateur holographer. The holography community was surprised at the amazing sensitivity of DCG to green light. It had been assumed that the sensitivity would be non-existent. Jeff Blyth responded with the G307 formulation of DCG to increase the speed and sensitivity to these new lasers.
Many film suppliers have come and gone from the silver-halide market. While more film manufactures have filled in the voids, many amateurs are now making their own film. The favorite formulations are dichromated gelatin, Methylene blue sensitised dichromated gelatin and diffusion method silver halide preparations. Jeff Blyth has published very accurate methods for making film in a small lab or garage.
A small group of amateurs are even constructing their own pulsed lasers to make holograms of moving objects.
Holographic interferometry.
Holographic interferometry (HI) is a technique that enables static and dynamic displacements of objects with optically rough surfaces to be measured to optical interferometric precision (i.e. to fractions of a wavelength of light). It can also be used to detect optical-path-length variations in transparent media, which enables, for example, fluid flow to be visualized and analyzed. It can also be used to generate contours representing the form of the surface.
It has been widely used to measure stress, strain, and vibration in engineering structures.
Interferometric microscopy.
The hologram keeps the information on the amplitude and phase of the field. Several holograms may keep information about the same distribution of light, emitted to various directions. The numerical analysis of such holograms allows one to emulate large numerical aperture, which, in turn, enables enhancement of the resolution of optical microscopy. The corresponding technique is called interferometric microscopy. Recent achievements of interferometric microscopy allow one to approach the quarter-wavelength limit of resolution.
Sensors or biosensors.
The hologram is made with a modified material that interacts with certain molecules generating a change in the fringe periodicity or refractive index, therefore, the color of the holographic reflection.
Security.
Security holograms are very difficult to forge, because they are replicated from a master hologram that requires expensive, specialized and technologically advanced equipment. They are used widely in many currencies, such as the Brazilian 20, 50, and 100-reais notes; British 5, 10, and 20-pound notes; South Korean 5000, 10,000, and 50,000-won notes; Japanese 5000 and 10,000 yen notes, India 50,100,500, and 1000 rupee notes; and all the currently-circulating banknotes of the Canadian dollar, Danish krone, and Euro. They can also be found in credit and bank cards as well as passports, ID cards, books, DVDs, and sports equipment.
Covertly storing information within a full colour image hologram was achieved in Canada, in 2008, at the UHR lab. The method used a fourth wavelength, aside from the RGB components of the object and reference beams, to record additional data, which could be retrieved only with the correct key combination of wavelength and angle. This technique remained in the prototype stage and was never developed for commercial applications.
Other applications.
Holographic scanners are in use in post offices, larger shipping firms, and automated conveyor systems to determine the three-dimensional size of a package. They are often used in tandem with checkweighers to allow automated pre-packing of given volumes, such as a truck or pallet for bulk shipment of goods.
Holograms produced in elastomers can be used as stress-strain reporters due to its elasticity and compressibility, the pressure and force applied are correlated to the reflected wavelength, therefore its color.
FMCG industry
These are the hologram adhesive strips that provide protection against counterfeiting and duplication of products. These protective strips can be used on FMCG products like cards, medicines, food, audio-visual products etc. Hologram protection strips can be directly laminated on the product covering.
Electrical and electronic products.
Hologram tags have an excellent ability to inspect an identical product. These kind of tags are more often used for protecting duplication of electrical and electronic products. These tags are available in a variety colors, sizes and shapes.
Hologram dockets for vehicle number plate.
Some vehicle number plates on bikes or cars have registered hologram stickers which indicate authenticity. For the purpose of identification they have unique ID numbers.
High security holograms for credit cards.
These are holograms with high security features like micro texts, nano texts, complex images, logos and a multitude of other features. Holograms once affixed on Debit cards/passports cannot be removed easily. They offer an individual identity to a brand along with its protection.
Non-optical holography.
In principle, it is possible to make a hologram for any wave.
Electron holography is the application of holography techniques to electron waves rather than light waves. Electron holography was invented by Dennis Gabor to improve the resolution and avoid the aberrations of the transmission electron microscope. Today it is commonly used to study electric and magnetic fields in thin films, as magnetic and electric fields can shift the phase of the interfering wave passing through the sample. The principle of electron holography can also be applied to interference lithography.
Acoustic holography is a method used to estimate the sound field near a source by measuring acoustic parameters away from the source via an array of pressure and/or particle velocity transducers. Measuring techniques included within acoustic holography are becoming increasingly popular in various fields, most notably those of transportation, vehicle and aircraft design, and NVH. The general idea of acoustic holography has led to different versions such as near-field acoustic holography (NAH) and statistically optimal near-field acoustic holography (SONAH). For audio rendition, the wave field synthesis is the most related procedure.
"Atomic holography" has evolved out of the development of the basic elements of atom optics. With the Fresnel diffraction lens and atomic mirrors atomic holography follows a natural step in the development of the physics (and applications) of atomic beams. Recent developments including atomic mirrors and especially ridged mirrors have provided the tools necessary for the creation of atomic holograms, although such holograms have not yet been commercialized.
Fake holograms.
Effects produced by lenticular printing, the Pepper's Ghost illusion (or modern variants such as the Musion Eyeliner), tomography and volumetric displays are often confused with holograms. Such illusions are termed "fauxlography".
The Pepper's ghost technique, being the easiest to implement of these methods, is most prevalent in 3D displays that claim to be (or are referred to as) "holographic". While the original illusion, used in theater, involved actual physical objects and persons, located offstage, modern variants replace the source object with a digital screen, which displays imagery generated with 3D computer graphics to provide the necessary depth cues. The reflection, which seems to float mid-air, is still flat, however, thus less realistic than if an actual 3D object was being reflected.
Examples of this digital version of Pepper's ghost illusion include the Gorillaz performances in the 2005 MTV Europe Music Awards and the 48th Grammy Awards; and Tupac Shakur's virtual performance at Coachella Valley Music and Arts Festival in 2012, rapping alongside Snoop Dogg during his set with Dr. Dre.
An even simpler illusion can be created by rear-projecting realistic images into semi-transparent screens. The rear projection is necessary because otherwise the semi-transparency of the screen would allow the background to be illuminated by the projection, which would break the illusion.
Crypton Future Media, a music software company that produced Hatsune Miku, one of many Vocaloid singing synthesizer applications, has produced concerts that have Miku, along with other Crypton Vocaloids, performing on stage as "holographic" characters. These concerts use rear projection onto a semi-transparent DILAD screen to achieve its "holographic" effect.
In 2011, in Beijing, apparel company Burberry produced the "Burberry Prorsum Autumn/Winter 2011 Hologram Runway Show", which included life size 2-D projections of models. The company's own video shows several centered and off-center shots of the main 2-dimensional projection screen, the latter revealing the flatness of the virtual models. The claim that holography was used was reported as fact in the trade media.
In Madrid, on 10 April 2015, a public visual presentation called "Hologramas por la Libertad" (Holograms for Liberty), featuring a ghostly virtual crowd of demonstrators, was used to protest a new Spanish law that prohibits citizens from demonstrating in public places. Although widely called a "hologram protest" in news reports, No actual holography was involved, it was just another technologically updated variant of the Pepper's Ghost illusion.
Microsoft Hololens has presented several examples of augmented reality and claimed to be creating a "holographic computer". This, however, has nothing to do with actual holography.
Holography in fiction.
Holography has been widely referred to in novels, TV and movies.

</doc>
<doc id="66340" url="https://en.wikipedia.org/wiki?curid=66340" title="Goldberg Variations">
Goldberg Variations

The Goldberg Variations, BWV 988, is a work written for harpsichord by Johann Sebastian Bach, consisting of an aria and a set of 30 variations. First published in 1741, the work is considered to be one of the most important examples of variation form. The "Variations" are named after Johann Gottlieb Goldberg, who may have been the first performer.
Composition.
The tale of how the variations came to be composed comes from an early biography of Bach by Johann Nikolaus Forkel:
Forkel wrote his biography in 1802, more than 60 years after the events related, and its accuracy has been questioned. The lack of dedication on the title page also makes the tale of the commission unlikely. Goldberg's age at the time of publication (14 years) has also been cited as grounds for doubting Forkel's tale, although it must be said that he was known to be an accomplished keyboardist and sight-reader. Williams (2001) contends that the Forkel story is entirely spurious.
Arnold Schering has suggested that the aria on which the variations are based was not written by Bach. More recent scholarly literature (such as the edition by Christoph Wolff) suggests that there is no basis for such doubts.
Publication.
Rather unusually for Bach's works, the "Goldberg Variations" were published in his own lifetime, in 1741. The publisher was Bach's friend Balthasar Schmid of Nuremberg. Schmid printed the work by making engraved copper plates (rather than using movable type); thus the notes of the first edition are in Schmid's own handwriting. The edition contains various printing errors.
The title page, shown in the figure above, reads in German:
"Clavier Ubung / bestehend / in einer ARIA / mit verschiedenen Verænderungen / vors Clavicimbal / mit 2 Manualen. / Denen Liebhabern zur Gemüths- / Ergetzung verfertiget von / Johann Sebastian Bach / Königl. Pohl. u. Churfl. Sæchs. Hoff- / Compositeur, Capellmeister, u. Directore / Chori Musici in Leipzig. / Nürnberg in Verlegung / Balthasar Schmids"
Keyboard exercise, consisting of an ARIA with diverse variations for harpsichord with two manuals. Composed for connoisseurs, for the refreshment of their spirits, by Johann Sebastian Bach, composer for the royal court of Poland and the Electoral court of Saxony, Kapellmeister and Director of Choral Music in Leipzig. Nuremberg, Balthasar Schmid, publisher.
The term "Clavier Ubung" (nowadays spelled "Klavierübung") had been assigned by Bach to some of his previous keyboard works. Klavierübung part 1 was the six partitas, part 2 the "Italian Concerto" and "French Overture", and part 3 a series of chorale preludes for organ framed by a prelude and fugue in E major. Although Bach also called his variations "Klavierübung", he did not specifically designate them as the fourth in this series.
Nineteen copies of the first edition survive today. Of these, the most valuable is the "handexemplar", discovered in 1974 in Strasbourg by the French musicologist Olivier Alain and now kept in the Bibliothèque nationale de France, Paris. This copy includes printing corrections made by the composer, and additional music in the form of fourteen canons on the Goldberg ground (see below). The nineteen printed copies provide virtually the only information available to modern editors trying to reconstruct Bach's intent, as the autograph (hand-written) score has not survived. A handwritten copy of just the aria is found in the 1725 Notebook for Anna Magdalena Bach. Christoph Wolff suggests on the basis of handwriting evidence that Anna Magdalena copied the aria from the autograph score around 1740; it appears on two pages previously left blank.
Instrumentation.
On the title page (see above), Bach specified that the work was intended for harpsichord. It is widely performed on this instrument today, though there are also a great number of performances on the piano (see Discography below). The piano was rare in Bach's day and there is no indication that Bach would either have approved or disapproved of performing the variations on this instrument.
Bach's specification is, more precisely, a two-manual harpsichord (see musical keyboard), and he indicated in the score which variations ought to be played using one hand on each manual: Variations 8, 11, 13, 14, 17, 20, 23, 25, 26, 27 and 28 are specified for two manuals, while variations 5, 7 and 29 are specified as playable with either one or two. With greater difficulty, the work can nevertheless be played on a single-manual harpsichord or piano.
Form.
After a statement of the aria at the beginning of the piece, there are thirty variations. The variations do not follow the melody of the aria, but rather use its bass line () and chord progression (). The bass line is notated by Ralph Kirkpatrick in his performing edition as follows.
The digits above the notes indicate the specified chord in the system of figured bass; where digits are separated by comma, they indicate different options taken in different variations.
Every third variation in the series of 30 is a canon, following an ascending pattern. Thus, variation 3 is a canon at the unison, variation 6 is a canon at the second (the second entry begins the interval of a second above the first), variation 9 is a canon at the third, and so on until variation 27, which is a canon at the ninth. The final variation, instead of being the expected canon in the tenth, is a quodlibet, discussed below.
As Ralph Kirkpatrick has pointed out, the variations that intervene between the canons are also arranged in a pattern. If we leave aside the initial and final material of the work (specifically, the Aria, the first two variations, the Quodlibet, and the aria da capo), the remaining material is arranged as follows. The variations found just "after" each canon are genre pieces of various types, among them three Baroque dances (4, 7, 19); a fughetta (10); a French overture (16); and two ornate arias for the right hand (13, 25). The variations located "two" after each canon (5, 8, 11, 14, 17, 20, 23, 26, and 29) are what Kirkpatrick calls "arabesques"; they are variations in lively tempo with a great deal of hand-crossing. This ternary pattern—"canon", "genre piece", "arabesque"—is repeated a total of nine times, until the Quodlibet breaks the cycle.
All the variations are in G major, apart from variations 15, 21, and 25, which are in G minor.
At the end of the thirty variations, Bach writes "Aria da Capo e fine", meaning that the performer is to return to the beginning ("da capo") and play the aria again before concluding.
Aria.
The aria is a sarabande in 3/4 time, and features a heavily ornamented melody:
The French style of ornamentation suggests that the ornaments are supposed to be parts of the melody; however, some performers (for example Wilhelm Kempff on piano) omit some or all ornaments and present the aria unadorned.
Peter Williams opines in "Bach: The Goldberg Variations" that this is not the theme at all, but actually the first variation (a view emphasising the idea of the work as a chaconne rather than a piece in true variation form).
Variatio 1. a 1 Clav..
This sprightly variation contrasts markedly with the slow, contemplative mood of the theme. The rhythm in the right hand forces the emphasis on the second beat, giving rise to syncopation from bars 1 to 7. Hands cross at bar 13 from the upper register to the lower, bringing back this syncopation for another two bars. In the first two bars of the B part, the rhythm mirrors that of the beginning of the A part, but after this a different idea is introduced.
Williams sees this as a sort of polonaise. The characteristic rhythm (1&uh-2&-3&) in the left hand is also found in Bach's Partita No. 3 for solo violin, in the A major prelude from the first book of "The Well-Tempered Clavier", and in the D minor prelude of the second book. Heinz Niemüller also mentions the polonaise character of this variation.
Variatio 2. a 1 Clav..
This is a simple three-part contrapuntal piece in 2/4 time, two voices engage in constant motivic interplay over an incessant bass line. Each section has an alternate ending to be played on the first and second repeat.
Variatio 3. Canone all’Unisono. a 1 Clav..
The first of the regular canons, this is a canon at the unison: the follower begins on the same note as the leader, a bar later. As with all canons of the "Goldberg Variations" (except the 27th variation, canon at the ninth), there is a supporting bass line here. The time signature of 12/8 and the many sets of triplets suggest a kind of a simple dance.
Variatio 4. a 1 Clav..
Like the passepied, a Baroque dance movement, this variation is in 3/8 time with a preponderance of quaver rhythms. Bach uses close but not exact imitation: the musical pattern in one part reappears a bar later in another (sometimes inverted).
Each repeated section has alternate endings for the first or second time.
Variatio 5. a 1 ô vero 2 Clav..
This is the first of the hand-crossing, two-part variations. It is in 3/4 time. A rapid melodic line written predominantly in sixteenth notes is accompanied by another melody with longer note values, which features very wide leaps:
The Italian type of hand-crossing such as is frequently found in the sonatas of Scarlatti is employed here, with one hand constantly moving back and forth between high and low registers while the other hand stays in the middle of the keyboard, playing the fast passages.
Variatio 6. Canone alla Seconda. a 1 Clav..
The sixth variation is a canon at the second: the follower starts a major second higher than the leader. The piece is based on a descending scale and is in 3/8 time. The harpsichordist Ralph Kirkpatrick describes this piece as having "an almost nostalgic tenderness". Each section has an alternate ending to be played on the first and second repeat.
Variatio 7. a 1 ô vero 2 Clav. al tempo di Giga.
The variation is in 6/8 meter, suggesting several possible Baroque dances. In 1974, when scholars discovered Bach's own copy of the first printing of the "Goldberg Variations", they noted that over this variation Bach had added the heading "al tempo di Giga". But the implications of this discovery for modern performance have turned out to be less clear than was at first assumed. In his book "The Keyboard Music of J. S. Bach" the scholar and keyboardist David Schulenberg notes that the discovery "surprised twentieth-century commentators who supposed gigues were always fast and fleeting." However, "despite the Italian terminology ["giga"], this is a fleet French gigue." Indeed, he notes, the dotted rhythmic pattern of this variation (pictured) is very similar to that of the gigue from Bach's second French suite and the gigue of the "French Overture". This kind of gigue is known as a "Canary", based on the rhythm of a dance which originated from the Canary islands. 
The pianist Angela Hewitt, in the liner notes to her 1999 Hyperion recording, argues that by adding the "al tempo di giga" notation, Bach was trying to caution against taking too slow a tempo, and thus turning the dance into a forlane or siciliano. She does however argue, like Schulenberg, that it is a French "gigue", not an Italian "giga" and does play it at an unhurried tempo.
Variatio 8. a 2 Clav..
This is another two-part hand-crossing variation, in 3/4 time. The French style of hand-crossing such as is found in the clavier works of Francois Couperin is employed, with both hands playing at the same part of the keyboard, one above the other. This is relatively easy to perform on a two-manual harpsichord, but quite difficult to do on a piano.
Most bars feature either a distinctive pattern of eleven sixteenth notes and a sixteenth rest, or ten sixteenth notes and a single eighth note. Large leaps in the melody occur. Both sections end with descending passages in thirty-second notes.
Variatio 9. Canone alla Terza. a 1 Clav..
This is a canon at the third, in 4/4 time. The supporting bass line is slightly more active than in the previous canons. This short variation (16 bars) is usually played at a slow tempo.
Variatio 10. Fughetta. a 1 Clav..
Variation 10 is a four-voice fughetta, with a four-bar subject heavily decorated with ornaments and somewhat reminiscent of the opening aria's melody.
The exposition takes up the whole first section of this variation (pictured). First the subject is stated in the bass, starting on the G below middle C. The answer (in the tenor) enters in bar 5, but it's a tonal answer, so some of the intervals are altered. The soprano voice enters in bar 9, but only keeps the first two bars of the subject intact, changing the rest. The final entry occurs in the alto in bar 13. There is no regular counter-subject in this fugue.
The second section develops using the same thematic material with slight changes. It resembles a counter-exposition: the voices enter one by one, all begin by stating the subject (sometimes a bit altered, like in the first section). The section begins with the subject heard once again, in the soprano voice, accompanied by an active bass line, making the bass part the only exception since it doesn't pronounce the subject until bar 25.
Variatio 11. a 2 Clav..
This is a virtuosic two-part toccata in 12/16 time. Specified for two manuals, it is largely made up of various scale passages, arpeggios and trills, and features much hand-crossing of different kinds.
Variatio 12. a 1 Clav. Canone alla Quarta in moto contrario.
This is a canon at the fourth in 3/4 time, of the inverted variety: the follower enters in the second bar in contrary motion to the leader. The follower appears inverted in the second bar.
In the first section, the left hand accompanies with a bass line written out in repeated quarter notes, in bars 1, 2, 3, 5, 6, and 7. This repeated note motif also appears in the first bar of the second section (bar 17, two Ds and a C), and, slightly altered, in bars 22 and 23. In the second section, Bach changes the mood slightly by introducing a few appoggiaturas (bars 19 and 20) and trills (bars 29–30).
Variatio 13. a 2 Clav..
This variation is a slow, gentle and richly decorated sarabande in 3/4 time. Most of the melody is written out using thirty-second notes, and ornamented with a few appoggiaturas (more frequent in the second section) and a few mordents. Throughout the piece, the melody is in one voice, and in bars 16 and 24 an interesting effect is produced by the use of an additional voice. Here are bars 15 and 16, the ending of the first section (bar 24 exhibits a similar pattern):
Variatio 14. a 2 Clav..
This is a rapid two-part hand-crossing toccata in 3/4 time, with many trills and other ornamentation. It is specified for two manuals and features large jumps between registers. Both features (ornaments and leaps in the melody) are apparent from the first bar: the piece begins with a transition from the G two octaves below middle C, with a lower mordent, to the G two octaves above it with a trill with initial turn.
Bach uses a loose inversion motif between the first half and the second half of this variation, "recycling" rhythmic and melodic material, passing material that was in the right hand to the left hand, and loosely (selectively) inverting it.
Contrasting it with Variation 15, Glenn Gould described this variation as "certainly one of the giddiest bits of neo-Scarlatti-ism imaginable."
Variatio 15. Canone alla Quinta. a 1 Clav.: Andante.
This is a canon at the fifth in 2/4 time. Like Variation 12, it is in contrary motion with the leader appearing inverted in the second bar. This is the first of the three variations in G minor, and its melancholic mood contrasts sharply with the playfulness of the previous variation. Pianist Angela Hewitt notes that there is "a wonderful effect at the very end this variation: the hands move away from each other, with the right suspended in mid-air on an open fifth. This gradual fade, leaving us in awe but ready for more, is a fitting end to the first half of the piece."
Glenn Gould said of this variation, "It’s the most severe and rigorous and beautiful canon … the most severe and beautiful that I know, the canon in inversion at the fifth. It’s a piece so moving, so anguished—and so uplifting at the same time—that it would not be in any way out of place in the St. Matthew’s Passion; matter of fact, I’ve always thought of Variation 15 as the perfect Good Friday spell."
Variatio 16. Ouverture. a 1 Clav..
The set of variations can be seen as being divided into two halves, clearly marked by this grand French overture, commencing with a particularly emphatic opening and closing chords. It consists of a slow prelude with dotted rhythms with a following fugue-like contrapuntal section.
Variatio 17. a 2 Clav..
This variation is another two-part virtuosic toccata. Peter Williams sees echoes of Antonio Vivaldi and Domenico Scarlatti here. Specified for two manuals, the piece features hand-crossing. It is in 3/4 time and usually played at a moderately fast tempo. Rosalyn Tureck is one of the very few performers who recorded slow interpretations of the piece. In making his 1981 re-recording of the "Goldberg Variations", Glenn Gould considered playing this variation at a slower tempo, in keeping with the tempo of the preceding variation (Variation 16), but ultimately decided not to because "Variation 17 is one of those rather skittish, slightly empty-headed collections of scales and arpeggios which Bach indulged when he wasn’t writing sober and proper things like fugues and canons, and it just seemed to me that there wasn't enough substance to it to warrant such a methodical, deliberate, Germanic tempo."
Variatio 18. Canone alla Sesta. a 1 Clav..
This is a canon at the sixth in 2/2 time. The canonic interplay in the upper voices features many suspensions. Commenting on the structure of the canons of the "Goldberg Variations", Glenn Gould cited this variation as the extreme example of "deliberate duality of motivic emphasis […] the canonic voices are called upon to sustain the passacaille role which is capriciously abandoned by the bass." Nicholas Kenyon calls Variation 18 "an imperious, totally confident movement which must be among the most supremely logical pieces of music ever written, with the strict imitation to the half-bar providing ideal impetus and a sense of climax."
Variatio 19. a 1 Clav..
This is a dance-like three-part variation in 3/8 time. The same sixteenth note figuration is continuously employed and variously exchanged between each of the three voices. This variation incorporates the rhythmic model of variation 13 (complementary exchange of quarter and sixteenth notes) with variations 1 and 2 (syncopations).
Variatio 20. a 2 Clav..
This variation is a virtuosic two-part toccata in 3/4 time. Specified for two manuals, it involves rapid hand-crossing. The piece consists mostly of variations on the texture introduced during its first eight bars, where one hand plays a string of eighth notes and the other accompanies by plucking sixteenth notes after each eighth note. To demonstrate this, here are the first two bars of the first section:
Variatio 21. Canone alla Settima.
The second of the three minor key variations, variation 21 has a tone that is somber or even tragic, which contrasts starkly with variation 20. The bass line here is one of the most eloquent found in the variations, to which Bach adds chromatic intervals that provide tonal shadings. This variation is a canon at the seventh in 4/4 time; Kenneth Gilbert sees it as an allemande despite the lack of anacrusis. The bass line begins the piece with a low note, proceeds to a slow lament bass and only picks up the pace of the canonic voices in bar 3:
A similar pattern, only a bit more lively, occurs in the bass line in the beginning of the second section, which begins with the opening motif inverted.
Variatio 22. a 1 Clav. alla breve.
This variation features four-part writing with many imitative passages and its development in all voices but the bass is much like that of a fugue. The only specified ornament is a trill which is performed on a whole note and which lasts for two bars (11 and 12).
The ground bass on which the entire set of variations is built is heard perhaps most explicitly in this variation (as well as in the Quodlibet) due to the simplicity of the bass voice.
Variatio 23. a 2 Clav..
Another lively two-part virtuosic variation for two manuals, in 3/4 time. It begins with the hands chasing one another, as it were: the melodic line, initiated in the left hand with a sharp striking of the G above middle C, and then sliding down from the B one octave above to the F, is offset by the right hand, imitating the left at the same pitch, but a quaver late, for the first three bars, ending with a small flourish in the fourth:
This pattern is repeated during bars 5–8, only with the left hand imitating the right one, and the scales are ascending, not descending. We then alternate between hands in short bursts written out in short note values until the last three bars of the first section. The second section starts with this similar alternation in short bursts again, then leads to a dramatic section of alternating thirds between hands. Peter Williams, marvelling at the emotional range of the work, asks: "Can this really be a variation of the same theme that lies behind the adagio no 25?"
Variatio 24. Canone all'Ottava. a 1 Clav..
This variation is a canon at the octave, in 9/8 time. The leader is answered both an octave below and an octave above; it is the only canon of the variations in which the leader alternates between voices in the middle of a section. 
Variatio 25. a 2 Clav.: Adagio.
Variation 25 is the third and last variation in G minor; a three-part piece, it is marked adagio in Bach's own copy and is in 3/4 time. The melody is written out predominantly in 16th and 32nd notes, with many chromaticisms. This variation generally lasts longer than any other piece of the set.
Wanda Landowska famously described this variation as "the black pearl" of the "Goldberg Variations". Peter Williams writes that "the beauty and dark passion of this variation make it unquestionably the emotional high point of the work", and Glenn Gould said that "the appearance of this wistful, weary cantilena is a master-stroke of psychology." In an interview with Gould, Tim Page described this variation as having an "extraordinary chromatic texture"; Gould agreed: "I don't think there's been a richer lode of enharmonic relationships any place between Gesualdo and Wagner."
Variatio 26. a 2 Clav..
In sharp contrast with the introspective and passionate nature of the previous variation, this piece is another virtuosic two-part toccata, joyous and fast-paced. Underneath the rapid arabesques, this variation is basically a sarabande. Two time signatures are used, 18/16 for the incessant melody written in 16th notes and 3/4 for the accompaniment in quarter and eighth notes; during the last five bars, both hands play in 18/16.
Variatio 27. Canone alla Nona. a 2 Clav..
Variation 27 is the last canon of the piece, at the ninth and in 6/8 time. This is the only canon where two manuals are specified not due to hand-crossing difficulties, and the only pure canon of the work, because it does not have a bass line.
Variatio 28. a 2 Clav..
This variation is a two-part toccata in 3/4 time that employs a great deal of hand crossing. Trills are written out using 32nd notes and are present in most of the bars. The piece begins with a pattern in which each hand successively picks out a melodic line while also playing trills. Following this is a section with both hands playing in contrary motion in a melodic contour marked by 16th notes (bars 9–12). The end of the first section features trills again, in both hands now and mirroring one another:
The second section starts and closes with the contrary motion idea seen in bars 9–12. Most of the closing bars feature trills in one or both hands.
Variatio 29. a 1 ô vero 2 Clav..
This variation consists mostly of heavy chords alternating with sections of brilliant arpeggios shared between the hands. It is in 3/4 time. A rather grand variation, it adds an air of resolution after the lofty brilliance of the previous variation. Glenn Gould states that variations 28 and 29 present the only case of "motivic collaboration or extension between successive variations."
Variatio 30. a 1 Clav. Quodlibet.
This quodlibet is based on multiple German folk songs, two of which are "Ich bin solang nicht bei dir g'west, ruck her, ruck her" ("I have so long been away from you, come closer, come closer") and "Kraut und Rüben haben mich vertrieben, hätt mein' Mutter Fleisch gekocht, wär ich länger blieben" ("Cabbage and turnips have driven me away, had my mother cooked meat, I'd have opted to stay"). The others have been forgotten. The "Kraut und Rüben" theme, under the title of "La Capricciosa", had previously been used by Dieterich Buxtehude for his thirty-two partite in G major, BuxWV 250.
Bach's biographer Forkel explains the Quodlibet by invoking a custom observed at Bach family reunions (Bach's relatives were almost all musicians):
As soon as they were assembled a chorale was first struck up. From this devout beginning they proceeded to jokes which were frequently in strong contrast. That is, they then sang popular songs partly of comic and also partly of indecent content, all mixed together on the spur of the moment. … This kind of improvised harmonizing they called a Quodlibet, and not only could laugh over it quite whole-heartedly themselves, but also aroused just as hearty and irresistible laughter in all who heard them.
Forkel's anecdote (which is likely to be true, given that he was able to interview Bach's sons), suggests fairly clearly that Bach meant the Quodlibet to be a joke.
Aria da Capo.
A note for note repeat of the aria at the beginning. Williams writes that the work's "elusive beauty … is reinforced by this return to the Aria. … no such return can have a neutral "Affekt". Its melody is made to stand out by what has gone on in the last five variations, and it is likely to appear wistful or nostalgic or subdued or resigned or sad, heard on its repeat as something coming to an end, the same notes but now final."
Canons on the Goldberg ground, BWV 1087.
When Bach's personal copy of the printed edition of the "Goldberg Variations" (see above) was discovered in 1974, it was found to include an appendix in the form of fourteen canons built on the first eight bass notes from the aria. The number 14 is a clear reference to the ordinal values of the letters in the composer's name: B(2) + A(1) + C(3) + H(8) = 14. Among those canons, the eleventh and the thirteenth are first versions of BWV 1077 and BWV 1076; the latter is included in the famous portrait of Bach painted by Elias Gottlob Haussmann in 1746.
Transcribed and popularized versions.
The "Goldberg Variations" have been reworked freely by many performers, changing either the instrumentation, the notes, or both. The Italian composer Ferruccio Busoni prepared a greatly altered transcription for piano. According to the art critic Michael Kimmelman, "Busoni shuffled the variations, skipping some, then added his own rather voluptuous coda to create a three-movement structure; each movement has a distinct, arcing shape, and the whole becomes a more tightly organized drama than the original." Other arrangements include:

</doc>
<doc id="66341" url="https://en.wikipedia.org/wiki?curid=66341" title="Kings">
Kings

Kings or King's may refer to:

</doc>
<doc id="66345" url="https://en.wikipedia.org/wiki?curid=66345" title="Robin Givens">
Robin Givens

Robin Simone Givens (born November 27, 1964) is an American stage, television, and film actress.
Givens began her acting career after graduating from Sarah Lawrence College in 1984. In 1986, she won the role of Darlene Merriman in the ABC sitcom "Head of the Class". She remained with the series for its entire five-year run. After the series ended in 1991, she continued her career with film roles and guest starring roles on television.
In 1996, Givens co-starred on the sitcom "Sparks", which aired for two seasons on UPN. In January 2000, she took over hosting duties on the syndicated talk show "Forgive or Forget". The show was canceled four months later. Givens has since had recurring roles on "The Game", "Tyler Perry's House of Payne", and "Chuck". In 2007, Givens released her autobiography, "Grace Will Lead Me Home".
Givens has been married and divorced twice. Her first marriage, to boxer Mike Tyson in 1988, drew considerable media attention as did their divorce the following year. Her second marriage, to tennis instructor Svetozar Marinković in 1997, also lasted a year. In 1993, Givens adopted a son. In 2000, she gave birth to a second son fathered by her ex-boyfriend, tennis player Murphy Jensen.
Early life.
Givens was born in New York City, to Ruth Roper (née Newby) and Reuben Givens, who divorced when she was two. Her mother raised Givens and her sister Stephanie in Mount Vernon and New Rochelle, New York. Givens graduated from New Rochelle Academy (a N–12 private school which closed in June 1987), and enrolled at Sarah Lawrence College at age 15. She graduated in 1984.
Career.
1980s–1990s.
Givens first began acting in 1985 with an appearance on "The Cosby Show", followed by roles in "Diff'rent Strokes" and the 1986 television film "Beverly Hills Madam", opposite Faye Dunaway. That same year, she landed her breakthrough role as rich girl Darlene Merriman on the ABC sitcom "Head of the Class". The series lasted five seasons, ending in 1991. In 1989, while starring in "Head of the Class", she appeared in "The Women of Brewster Place" with Oprah Winfrey. She later appeared in "Boomerang" (1992). In 1994, she posed nude for "Playboy" magazine and was ranked #88 on "Empire Magazine'"s "100 Sexiest Stars in Film History" list in May 1995.
In 1996, Givens portrayed Claudia in the television movie "The Face" (also known as "A Face to Die For") with Yasmine Bleeth. Later that year, she co-starred in the UPN sitcom "Sparks", which series ended its run in 1998. She also played Denise in "The Fresh Prince of Bel-Air".
2000s–2010s.
In January 2000, Givens appeared in a cameo in Toni Braxton's music video "He Wasn't Man Enough", as the wife of a cheating husband. She returned to the entertainment industry later that year as the host of the talk show "Forgive or Forget", replacing television personality Mother Love halfway through the show's second. Ratings initially increased after Givens took over hosting duties, but soon fell. The series was canceled after this season.
In 2006, Givens attempted a return to television on MyNetworkTV's telenovela "Saints and Sinners", but the show garnered low ratings and was soon canceled. Givens continued acting in made-for-television films while also making appearances on Trinity Broadcasting Network's "Praise the Lord" program (July 12, 2007), and "Larry King Live". In June 2007, she released her autobiography "Grace Will Lead Me Home".
Givens returned to feature films in Tyler Perry's Southern drama "The Family That Preys" (2008). She also had a recurring role portraying a fictionalized version of herself on the CW comedy-drama "The Game". Additionally, she has had a recurring role on the TBS show "Tyler Perry's House of Payne", and a guest role on USA Network's "Burn Notice".
In addition to television and film roles, Givens has performed onstage. In 2001, she appeared in an Off Broadway production of "The Vagina Monologues". From February to April 16, 2006, she played the role of Roxie Hart in the Broadway play "Chicago". In 2007, she toured the country playing a part in the I'm Ready Productions play "Men, Money & Golddiggers".
Givens starred in the 2009 stage play "A Mother's Prayer", which also starred Johnny Gill, Shirley Murdock, and Jermaine Crawford.
In 2007, Givens published a memoir entitled "Grace Will Lead Me Home". In it, she reflects on the life of her praying grandmother, Grace, her experiences of domestic violence, her strong will to survive, feeling abandoned by her father, and her faith in God.
In 2011, she guest-starred in three episodes of NBC's spy-comedy "Chuck": "Chuck Versus the Masquerade", "Chuck Versus the A-Team", and "Chuck Versus the Muuurder", as Jane Bentley. 2015 starred along Clifton Powell, Mishon Ratliff and Malachi Malik in the segment "Mama's Boy" of TV One's anthology romance horror film "Fear Files".
Personal life.
After meeting in March 1986, Givens married boxer Mike Tyson on February 7, 1988. Tyson was then estimated to have US$50 million, and he and Givens did not make a prenuptial agreement. During their marriage, Givens and her mother bought a $4.6 million mansion in the affluent suburb of Bernardsville, New Jersey.
After a miscarriage in May 1988, the marriage began to fall apart. Givens alleged spousal abuse, while Tyson alleged alienation and interest in his money, not in him.
The marriage ended on Valentine's Day, just a year later. Newspapers reported that Givens received a divorce settlement of over $10 million from her marriage to Tyson. She later denied the report, stating: "I didn’t receive one dime". She received negative press following her split from Tyson, particularly within the African American community. One article, in particular, described her as "the most hated woman in America".
In 1993, Givens adopted her first son, Michael.
In 1997, she married her tennis instructor, Svetozar Marinković; Givens filed for divorce months later.
In 1999, she had a second son, William, with ex-boyfriend, Murphy Jensen.
Legal issues.
In January 2004, Givens critically injured an 89-year-old pedestrian, Maria Antonia Alcover, while driving an SUV through a Miami, Florida intersection. Givens was ticketed for failing to use due care with a pedestrian in a crosswalk, but the charges were later dismissed. In June 2004, Alcover filed a civil lawsuit against Givens and her sister (the owner of the SUV Givens was driving) for an unknown amount.
A May 7, 2009 article in "Forbes" magazine reported that the Internal Revenue Service was suing Givens for unpaid federal income taxes totaling $292,000, which includes interest and penalties. The government had asked a federal court in Florida for a judgment against her on 39 assessments covering a span of eight years.

</doc>
<doc id="66346" url="https://en.wikipedia.org/wiki?curid=66346" title="The Stardroppers">
The Stardroppers

The Stardroppers is a science fiction novel by John Brunner. It was originally published at novella-length in 1962 as Listen, the Stars, in "Analog" and then as part of an Ace Double in 1963; in 1972 the revised, novel-length form was published by DAW Books.
Plot introduction.
"The Stardroppers" is about an undercover United Nations agent investigating a new fad, "stardropping", whereby physics-violating equipment is used to listen to sounds believed to be alien or paranormal signals. Superficially a harmless but expensive hobby, stardropping reins in a fanaticism resembling addiction, where some users assemble in semi-social communes and spend all of their money on increasingly improved equipment. The fad gains an additional aspect of risk when users begin disappearing into thin air, in cases of increasing profile and witnessing.
Reception.
Lester del Rey characterized the novel as "a good adventure story not of one Brunner's stronger literary efforts."

</doc>
<doc id="66348" url="https://en.wikipedia.org/wiki?curid=66348" title="Paul Pimsleur">
Paul Pimsleur

Paul Pimsleur (October 17, 1927 – June 22, 1976) was a scholar in the field of applied linguistics. He developed the Pimsleur language learning system, which, along with his many publications, had a significant effect upon theories of language learning and teaching .
Early life and education.
Paul M. Pimsleur was born in New York City and grew up in The Bronx. His father, Solomon Pimsleur, was an immigrant from France and a composer of music; his American-born mother was a librarian at Columbia University. Pimsleur earned a bachelor's degree at the City College of New York, and from Columbia University he earned a master's degree in psychological statistics and a Ph.D. in French.
Career.
His first position involved teaching French phonetics and phonemics at the University of California, Los Angeles. After leaving UCLA, Pimsleur went on to faculty positions at the Ohio State University, where he taught French and foreign language education. At the time, the foreign language education program at OSU was the major doctoral program in that field in the U.S. While at Ohio State he created and directed the Listening Center, one of the largest language laboratories in the United States. The center was developed in conjunction with Ohio Bell Telephone and allowed self-paced language study using a series of automated tapes and prompts that were delivered over the telephone.
Later, Pimsleur was a professor of education and romance languages at The State University of New York at Albany, where he held dual professorships in education and French. He was a Fulbright lecturer at the Ruprecht Karls University of Heidelberg in 1968 and 1969 and a founding member of the "American Council on the Teaching of Foreign Languages" (ACTFL). He did research on the psychology of language learning and in 1969 was section head of psychology of second languages learning at the International Congress of Applied Linguistics.
His research focused on understanding the language acquisition process, especially the learning process of children, who speak a language without knowing its formal structure. The term "organic learning" was applied to that phenomenon. For this, he studied the learning process of groups made of children, adults, and multilingual adults. The result of this research was the Pimsleur language learning system. His many books and articles had an impact on theories of language learning and teaching.
In the period from 1958 to 1966, Pimsleur reviewed previously published studies regarding linguistic and psychological factors involved in language learning. He also conducted several studies independently. This led to the publication in 1963 of a coauthored monograph, "Underachievement in Foreign Language Learning", which was published by the Modern Language Association of America.
Through this research, he identified three factors that could be measured to calculate language learning aptitude: verbal intelligence, auditory ability, and motivation. Pimsleur and his associates developed the Pimsleur Language Aptitude Battery (PLAB) based on these three factors to assess language aptitude. He was one of the first foreign language educators to show an interest in students who have difficulty in learning a foreign language while doing well in other subjects. Today, the PLAB is used to determine the language-learning aptitude, or even a language-learning disability, among secondary-school students.
Death.
Pimsleur died unexpectedly of a heart attack during a visit to France in 1976.
Legacy.
Since its creation in 1977, "The ACTFL-MLJ Paul Pimsleur Award for Research in Foreign Language Education", which is awarded annually, bears his name.
Paul's business partner, Charles Heinle, continued to develop the Pimsleur courses until he sold the company to Simon & Schuster Audio in 1997.
In 2006, Pimsleur's daughter, Julia Pimsleur, created the Entertainment Immersion Method® inspired by the Pimsleur Method, which is the foundation of the Little Pim language teaching program for young children, sold in the U.S. and 22 countries. 
In 2013, Simon & Schuster reissued Dr. Paul Pimsleur's out-of-print book "How to Learn a Foreign Language" in hardcover and eBook format to celebrate the 50th anniversary of Paul Pimsleur's first course.

</doc>
<doc id="66350" url="https://en.wikipedia.org/wiki?curid=66350" title="Ami Dolenz">
Ami Dolenz

Ami Dolenz (born January 8, 1969) is an American television and film actress and producer.
Early life.
Born in Burbank, California, into a show business family, Dolenz is the daughter of Micky Dolenz of the 1960s group the Monkees, and British television presenter Samantha Juste. Her paternal grandparents were the film actors George Dolenz and Janelle Johnson.
Career.
At age 15, Dolenz won a junior talent contest and decided to become an actress. She dropped out of high school and began appearing in roles on various television series. One of her first acting roles was in the television movie "The Children of Times Square", followed by a two-episode stint on "Growing Pains". In 1987 she had a small role in the comedy "Can't Buy Me Love"; later that year she landed the role of Melissa McKee in the long-running soap opera "General Hospital". The role garnered Dolenz attention, earning her two nominations (in 1988 and 1989) for a Young Artist Award.
After leaving "General Hospital" in 1989, Dolenz landed a co-starring role opposite Tony Danza in "She's Out of Control". The following year she portrayed Sloan Peterson in the television series of "Ferris Bueller", which lasted only 13 episodes and was cancelled in 1991. After its demise, Dolenz starred in "Children of the Night" then had the lead role in 1992's "Miracle Beach".
Throughout the 1990s, Dolenz continued to appear in films and television including ', ', "Murder, She Wrote", "Wake, Rattle and Roll", "", "Demolition University", "Pacific Blue", and "Teen Angel". In 1998, she voiced a character for the children's show "The Secret Files of the Spy Dogs". After a four-year hiatus from acting, Dolenz returned in the independent film "Mr. Id". In 2007 she appeared in the film "Even If", which she also produced.
Personal life.
On August 10, 2002, Dolenz married actor and martial artist Jerry Trimble.
In addition to acting, Dolenz manages KidPix Productions with her husband. The company stages movie shoots as birthday parties for children. She also performs with the Write Act Repertory Theatre, and co-owns Bluebell Boutique, an Internet custom jewelry shop, with her mother.

</doc>
<doc id="66359" url="https://en.wikipedia.org/wiki?curid=66359" title="Coatimundi">
Coatimundi

Coatimundi may refer to:

</doc>
<doc id="66365" url="https://en.wikipedia.org/wiki?curid=66365" title="Kitten">
Kitten

A kitten (also known as a kitty or kitty cat) is a juvenile cat. After being born, kittens are totally dependent on their mother for survival and do not normally open their eyes until after seven to ten days. After about two weeks, kittens quickly develop and begin to explore the world outside the nest. After a further three to four weeks, they begin to eat solid food and grow adult teeth. Domestic kittens are highly social animals and enjoy human companionship.
Etymology and development.
The word "kitten" derives from the Middle English word "kitoun", which in turn came from the Old French "chitoun" or "cheton". The big cats are called "cubs" rather than kittens; either term may be used for the young of smaller wild felids, such as ocelots, caracals and lynx, but "kitten" is usually more common for these species.
A feline litter usually consists of two to five kittens. The kits are born after a gestation that lasts between 64 and 67 days, with an average length of 66 days. Kittens emerge in a sac called the amnion, which is bitten off and eaten by the mother cat.
For the first several weeks, kittens are unable to urinate or defecate without being stimulated by their mother. They are also unable to regulate their body temperature for the first three weeks, so kittens born in temperatures less than can die from hypothermia if their mother does not keep them warm. The mother's milk is very important for the kittens' nutrition and proper growth. This milk transfers antibodies to the kittens, which helps protect them against infectious disease. Newborn kittens are unable to produce concentrated urine, and so have a very high requirement for fluids. Kittens open their eyes about seven to ten days after birth. At first, the retina is poorly developed and vision is poor. Kittens are not able to see as well as adult cats until about ten weeks after birth.
Kittens develop very quickly from about two weeks of age until their seventh week. Their coordination and strength improve. They play-fight with their litter-mates and begin to explore the world outside the nest or den. They learn to wash themselves and others as well as play hunting and stalking games, showing their inborn ability as predators. These innate skills are developed by the kittens' mother or other adult cats, who bring live prey to the nest. Later, the adult cats demonstrate hunting techniques for the kittens to emulate. As they reach three to four weeks old, the kittens are gradually weaned and begin to eat solid food, with weaning usually complete by six to eight weeks. Kittens generally begin to lose their baby teeth around three months of age, and have a complete set of adult teeth by nine months. Kittens live primarily on solid food after weaning, but usually continue to suckle from time to time until separated from their mothers. Some mother cats will scatter their kittens as early as three months of age, while others continue to look after them until they approach sexual maturity.
The sex of kittens is usually easy to determine at birth. By six to eight weeks they are harder to sex because of the growth of fur in the genital region. The male's urethral opening is round, whereas the female's urethral opening is a slit. Another marked difference is the distance between anus and urethral opening, which is greater in males than in females.
Kittens are highly social animals and spend most of their waking hours interacting with available animals and playing on their own. Play with other kittens peaks in the third or fourth month after birth, with more solitary hunting and stalking play peaking later, at about five months. Kittens are vulnerable because they like to find dark places to hide, sometimes with fatal results if they are not watched carefully. Although domestic kittens are commonly sent to new homes at six to eight weeks of age, it has been suggested that being with its mother and litter mates from six to twelve weeks is important for a kitten's social and behavioural development. Usually, breeders will not sell a kitten that is younger than twelve weeks, and in many jurisdictions, it is illegal to give away kittens younger than eight weeks old. Kittens generally reach sexual maturity around seven months of age. A cat reaches full "adulthood" around one year of age.
Health.
Domestic kittens in developed societies are usually vaccinated against common illnesses from two to three months of age. The usual combination vaccination protects against feline viral rhinotracheitis (FVR), feline calicivirus (C), and feline panleukopenia (P). This FVRCP inoculation is usually given at eight, twelve, and sixteen weeks, and an inoculation against rabies may be given at sixteen weeks. Kittens are usually spayed or neutered at seven months of age, but kittens may be neutered as young as seven weeks (if large enough), especially in animal shelters. Such early neutering does not appear to have any long-term health risks to cats, and may even be beneficial in male cats. Kittens are commonly wormed against roundworms from about four weeks.
Orphaned kittens.
Kittens require a high-calorie diet that contains more protein than the diet of adult cats. Young orphaned kittens require cat milk every two to four hours, and they need physical stimulation to defecate and urinate. Cat milk replacement is manufactured to feed to young kittens, because cow's milk does not provide all the necessary nutrients. Human-reared kittens tend to be very affectionate with humans as adults and sometimes more dependent on them than kittens reared by their mothers, but they can also show volatile mood swings and aggression. Depending on the age at which they were orphaned and how long they were without their mothers, these kittens may be severely underweight and can have health problems later in life, such as heart conditions. The compromised immune system of orphaned kittens (from lack of antibodies found naturally in the mother's milk) can make them especially susceptible to infections, making antibiotics a necessity.

</doc>
<doc id="66366" url="https://en.wikipedia.org/wiki?curid=66366" title="Taiwan Relations Act">
Taiwan Relations Act

The Taiwan Relations Act (TRA; ; ) is an act of the United States Congress. Since the recognition of the People's Republic of China, the Act has defined the non-diplomatic relations between the people of the United States and the people on Taiwan.
Background.
At The Third Plenum in 1978, Deng Xiaoping became the paramount leader of the People's Republic of China (PRC), definitively ending Maoist rule and beginning the reform era of Chinese history. During his speech at the plenum, he outlined a new Chinese foreign policy, whereby the Soviet Union—not the United States, as in the past—was identified as the main national security threat to China. During this time, China regarded itself as in a "united front" with the U.S., Japan, and western Europe against the Soviets. Accordingly, China established relations with the United States, supported American operations in Communist Afghanistan, and leveled a punitive expedition against Vietnam, America's main antagonist in Southeast Asia. In exchange, the United States abrogated its mutual defense treaty with the Republic of China (ROC). So as to maintain good relations with the United States, the PRC offered new, more generous proposals to the Taipei government for Chinese reunification, introducing the one country, two systems concept which would allow Taiwan near-complete autonomy. However, the ROC government hardened its position with the Three Noes Policy and mobilized its ethnic lobby in the United States to agitate Congress for the swift passage of an American security guarantee for the island.
The Act was passed by the United States Congress and signed by President Jimmy Carter in 1979 after the establishment of diplomatic relations with the PRC and the breaking of relations between the United States and Taiwan. Congress rejected the State Department's proposed draft and replaced it with language that has remained in effect since 1979. The Carter Administration signed the Taiwan Relations Act to maintain commercial, cultural, and other relations through the unofficial relations in the form by a nonprofit corporation incorporated under the laws of the District of Columbia, the American Institute in Taiwan (AIT), without official government representation and without formal diplomatic relations.
Provisions.
Definition of Taiwan.
The act does not recognize the terminology of 'Republic of China' after 1 January 1979, but uses the terminology of "governing authorities on Taiwan". Geographically speaking and following the similar content in the earlier defense treaty from 1955, it defines the term "Taiwan" to include, as the context may require, the islands of Taiwan (the main Island) and Penghu. 
Of the other islands or archipelagos under the control of Taiwan's governing authorities, Jinmen, the Matsus, the Wuqiu Islands, the Pratas and Taiping Island are left outside the definition of Taiwan. Also any area claimed by the authorities, but not under their control, is without mention.
"De facto" diplomatic relations.
The act authorizes "de facto" diplomatic relations with the governing authorities by giving special powers to the AIT to the level that it is the "de facto" embassy, and states that any international agreements made between the ROC and U.S. before 1979 are still valid unless otherwise terminated. One agreement that was unilaterally terminated by President Jimmy Carter upon the establishment of relations with the PRC was the Sino-American Mutual Defense Treaty; that termination was the subject of the Supreme Court case "Goldwater v. Carter".
The act provides for Taiwan to be treated under U.S. laws the same as "foreign countries, nations, states, governments, or similar entities". The act provides that for most practical purposes of the U.S. government, the absence of diplomatic relations and recognition will have no effect.
Military provisions.
The Taiwan Relations Act potentially requires the U.S. to intervene militarily if the PRC attacks or invades Taiwan. The act states that "the United States will make available to Taiwan such defense articles and defense services in such quantity as may be necessary to enable Taiwan to maintain a sufficient self-defense capabilities". However, the decision about the nature and quantity of defense services that America will provide to Taiwan is to be determined by the President and Congress. America's policy has been called "strategic ambiguity" and it is designed to dissuade Taiwan from a unilateral declaration of independence, and to dissuade the PRC from unilaterally unifying Taiwan with the PRC.
The act further stipulates that the United States will "consider any effort to determine the future of Taiwan by other than peaceful means, including by boycotts or embargoes, a threat to the peace and security of the Western Pacific area and of grave concern to the United States".
This act also requires the United States "to provide Taiwan with arms of a defensive character", and "to maintain the capacity of the United States to resist any resort to force or other forms of coercion that would jeopardize the security, or the social or economic system, of the people on Taiwan." Successive U.S. administrations have sold arms to Taiwan in compliance with the Taiwan Relations Act despite demands from the PRC that the U.S. follow the legally non-binding Three Joint Communiques and the U.S. government's proclaimed One-China policy (which differs from the PRC's One-China Principle).
Reactions.
The passage of the Act severely damaged Sino-American relations in the eyes of the Chinese leadership, with Deng citing the Act in 1984 as the single most important reason why China discontinued its nascent pro-American foreign policy. From that time on, the PRC aligned itself with the Third World countries rather than with the United States or the Soviet Union, engaging itself in various movements such as nuclear non-proliferation that would allow it to critique the superpowers. In the August 17th communique of 1982, the United States agreed to reduce arms sales to Taiwan. However, the United States declared that the United States would not formally recognize PRC's sovereignty over Taiwan as part of the Six Assurances offered to Taipei in 1982. In the late 1990s, the United States Congress passed a non-binding resolution stating that relations between Taiwan and the United States will be honored through the TRA first. This resolution, which puts greater weight on the TRA's value over that of the three communiques, was signed by President Bill Clinton as well. A July 2007 Congressional Research Service Report confirmed that U.S. policy has not recognized the PRC's sovereignty over Taiwan. The PRC continues to view the Taiwan Relations Act as "an unwarranted intrusion by the United States into the internal affairs of China". As of 2015 the United States continued supplying Taiwan with armaments and China continued to protest.

</doc>
<doc id="66373" url="https://en.wikipedia.org/wiki?curid=66373" title="Taiwanization">
Taiwanization

Taiwanization or Taiwanisation (; Pe̍h-oē-jī: Tâi-oân pún-thó͘-hòa ūn-tōng), also known as the Taiwanese localization movement, is a conceptual term used in Taiwan to emphasize the importance of a (separate) Taiwanese culture, society, economy, and nationality, rather than to regard Taiwan as solely an appendage of China. This involves the teaching of the history of Taiwan, geography, and culture from a Taiwan-centric perspective, as well as promoting languages locally established in Taiwan, including Taiwanese Hokkien (Taiwanese), Hakka, and aboriginal languages.
Originally part of the Taiwan independence movement and related to the Taiwan Name Rectification Campaign, some of Taiwanization's aims are now endorsed by some supporters of Chinese Unification on Taiwan.
The localization movement has been expressed in forms such as the use of language or dialect in the broadcast media and entire channels devoted to aboriginal and Hakka affairs. Textbooks have been rewritten by scholars to more prominently emphasize Taiwan. The political compromise that has been reached is to teach both the history of Taiwan and the history of mainland China.
Some Taiwanese-owned companies or organizations established in earlier times have names containing the words "China" or "Chinese". They have been encouraged in recent years to change the word "China" in their names to "Taiwan" as an act of Taiwanization. This campaign for changing the names is known as the "Name Rectification Campaign" () or "Taiwan Name Rectification". Many Taiwan-based companies in international sectors already identify themselves as "Taiwan"-based for clarity's sake. This keeps international customers from confusing them with an enterprise based in the People's Republic of China. Other Taiwan-based companies decline to change to a "Taiwanese" name because of expense or the political views held by important clients and company leaders.
History and development.
No one can make sure when the concept of Taiwanization has started. Some people will say when the first wave of large Han People immigration decided to leave from mainland China to Taiwan in the mid-16th-century, then they must have had some concept to maintain some independence from the control of ruling class in their original hometown. And some other people will say not until the Kingdom of Tungning, with its capital at Tainan, had been built by Zheng Family in 1662, this kind of concept hasn't appeared.
And for most Chinese contemporary scholar of Mainland China, they believe the roots of the localization movement began during the Japanese rule of Taiwan from 1895 to 1945, when groups organized to lobby the Imperial Japanese government for greater Taiwanese autonomy and home rule. After the Kuomintang (KMT) took over Taiwan, the Taiwan home-rule groups were decimated in the wake of the February 28 Incident of 1947. The Kuomintang viewed Taiwan primarily as a base to retake mainland China and quickly tried to subdue potential political opposition on the island. The Kuomintang did little to promote a unique Taiwanese identity; often newly immigrated Chinese or "mainlanders" as they were called, working in administrative positions lived in neighborhoods where they were segregated from the Taiwanese. Others, especially poorer refugees, were shunned by the Hoklo Taiwanese and lived among aborigines instead. The mainlanders often learned Hokkien. However, since Mandarin was enforced as the official language of the Republic of China and Taiwanese was not allowed to be spoken in schools, the mainlanders who learned Taiwanese found their new language skills to diminish. As Taiwanese, or any language other than Mandarin, was forbidden in the military posts, many mainlanders whose family lived in martial villages only spoke Mandarin and perhaps their home language (e.g. Cantonese, Shanghainese, etc.). The promotion of Chinese nationalism within Taiwan and the fact that the ruling group on Taiwan were considered outsiders by some were the reasons cited for both the Taiwan independence movement and Taiwanization.
In the 1970s and 1980s there was a shift in power away from the Kuomintang to people native to Taiwan. This, combined with cultural liberalization and the increasing remoteness of the possibility of retaking mainland China, led to a cultural and political movement which emphasized a Taiwan-centered view of history and culture rather than one which was China-centered or even, as before 1946, Japan-centered. Taiwanization was strongly supported by President Lee Teng-hui.
The "Bentuhua" or "localization/indigenization movement" was sparked in the mid-1970s with the growing expression of ethnic discontent due to unequal distribution of political and cultural power between mainlanders and Taiwanese people. Beginning in the 1960s, Taiwan was enveloped by the problems of rapid industrial development, rural abandonment, labor disputes and the uneven distribution of access to wealth and social power. These changes, combined with the loss of several key allies, forced the KMT regime to institute limited reforms. The reforms permitted under Chiang Ching-kuo allowed indigenization to increase as leading dissidents generated a response to the government’s failures. The dissident groups, united under the "Tangwai", or “outside the party” banner, called for the government to accept the reality that it was only the government of Taiwan and not China. The key demands of the "Tangwai" involved instituting democracy and seeking international recognition as a sovereign state. Taiwanese demanded full civil rights as guaranteed under the ROC constitution and equal political rights as those experienced by the Mainlander elite.
The Taiwanese cultural elite fully promoted the development of "Xiang tu" literature and cultural activities, including rediscovering Taiwanese nativist literature written under Japanese colonial rule. The tangwai movement revived symbols of Taiwanese resistance to Japanese rule in the effort to mobilize ethnic Taiwanese. The opposition to the KMT’s China-centered cultural policies resulted in dissidents crafting new national-historical narratives that placed the island of Taiwan itself at the center of the island's history. The Taiwanese emerged as a frequently colonized and often oppressed people. The concept of "bentuhua" was finally expressed in the cultural domain in the premise of Taiwan as a place with a unique society, culture and history. This principle has been largely adopted for understanding Taiwan’s cultural representation and expressed in a variety of cultural activities, including music, film and the literary and performing arts.
The pressures of indigenization and the growing acceptance of a unique Taiwanese cultural identity have met opposition from more conservative elements of Taiwan society. Critics argue that the new perspective creates a “false” identity rooted in ethnic nationalism as opposed to an “authentic” Chinese identity, which is primordial and inherent. Many mainlanders living on Taiwan complain that their own culture is marginalized by "bentuhua", and initially expressed fear of facing growing alienation. In the past decade these complaints have subsided somewhat as Taiwan increasingly views itself as a pluralistic society that embraces many cultures and recognizes the rights of all citizens.
In the mid-to-late 1990s the gestures of Taiwanization were increasingly adopted by pro-unification figures who, while supporting the Chinese nationalism of Chiang Kai-shek, saw it as appropriate, or at least advisable, to display more appreciation for cultures of Taiwan. Pro-unification politicians such as James Soong, the former head of the Government Information Office who once oversaw the limitation of Taiwanese dialects, began speaking in Hoklo on semi-formal occasions.
Name Rectification Campaign.
The "Name Rectification Campaign" includes efforts by the Republic of China (Taiwan) government beginning in 2000 to distance itself from China and rollback earlier sinicization efforts by taking actions such as removing Chinese influence from items within Taiwan control. While the Taiwanese localization movement may view such efforts as emphasizing the importance of Taiwan's culture, this section addresses the perspective of those who likely support the Chinese reunification of all of Greater China under a single political entity.
Origin.
At the end of World War II, Chinese Kuomintang forces took over Taiwan and soon began an effort to sinicize the population. Taiwanese urban elites were wiped out in the February 28 Incident. Mandarin Chinese became the only language allowed in media and school to the exclusion of Taiwanese languages. Public institutions and corporations were given names that included the words "China" or "Chinese". School history and geography lessons focused on China with little attention paid to Taiwan. Street names in Taipei were changed from their original names to Chinese names that reflected the geography of China and Kuomintang ideals.
With the end of martial law in 1987 and the introduction of democracy in the 1990s after the Wild Lily student movement, an effort began to re-assert Taiwanese identity and culture while trying to get rid of many Chinese influences imposed by the Kuomintang.
Education and language campaign.
In 2000, then-ROC president Lee Teng-hui began making statements such as "Taiwan culture is not a branch of Chinese culture" and "Taiwan's southern Fujian dialect is not a branch of Fujian's southern Fujian dialect, but rather a 'Taiwan dialect'.". Taiwan radio and TV increased their Taiwan dialect (southern Fujian dialect) programming. These efforts were perceived in China as initial efforts towards breaking the ties between Taiwan culture and Chinese culture by downplaying the long-term Chinese cultural and historic identification in that region.
In April 2003, the Committee for Promoting Mandarin, which was part of Taiwan's Ministry of Education, released a legislation proposal entitled "Language Equality Law." The proposed legislation sought to designate fourteen languages as the national languages of Taiwan. In mainland China, this was seen as an effort to diminish the use of standard Mandarin and its cultural influences in favor of revising the cultural and psychological foundations on the island of Taiwan by using other languages. The draft was not adopted.
The textbook issue was raised in November 2004, when a group of lawmakers, legislative candidates and supporters of the pro-independence Taiwan Solidarity Union (TSU) urged the ROC Ministry of Education to publish Taiwan-centric history and geography textbooks for school children as part of the Taiwanization campaign. Although the resulting draft outline of history course for regular senior middle schools was criticized by a variety of groups, President Chen Shui-bian responded that "to seek the truth of Taiwan's history" is not equal to desinicization nor an act of independence and indicated that he would not interfere with the history editing and compilation efforts.
The proposals to revise Taiwan's history textbooks were condemned in February 2007 by the People's Republic of China's Taiwan Affairs Office of the State Council as being part of the desinicization campaign. In July 2007, the Taiwan Ministry of Education released a study that found 5,000 textbook terms, some relating to Chinese culture, as being "unsuitable". The Kuomintang saw this as part of a textbook censorship desinicization campaign. The proposals have not been adopted.
Name change campaign.
Between 2002 and 2007, the ROC government under Chen Shui-bian took steps to revise the terms "China," "Republic of China," "Taipei", and others that impart an association with the Chinese culture.
In 2002, the "Name Rectification Campaign" made significant advances in replacing the terms "China," "Republic of China," or "Taipei" with the term "Taiwan" on official documents, in the names of Taiwan-registered organizations, companies, and public enterprises on the island, and in the names of businesses stationed abroad. In 2003, the ROC Foreign Ministry issued a new passport with the word "Taiwan" printed in English on its cover. Moreover, in January 2005, Taiwan adopted a Westernized writing format for government documents, denied that it was an attempt at desinicization, and promoted the actions as "a concerted effort at globalizing Taiwan's ossified bureaucracies and upgrading the nation's competitive edge."
Campaigning in this area continued in March 2006, where the Democratic Progressive Party sought to change the Republic of China year designation used in Taiwan to the Gregorian calendar. Instead of the year 2006 being referred to as the "95th year of the ROC"—with the 1912 founding of the Republic of China being referred to as "the first year of the ROC"—the year 2006 would be identified as 2006 in official usage such as on banknotes, IDs, national health insurance cards, driver's licenses, diplomas and wedding certificates. This was viewed as the government trying another angle for desinicization by removing any trace of China from Taiwan.
In February 2007, the term "China" was replaced by the term "Taiwan" on Taiwan postage stamps to coincide with the 60th anniversary of the February 28 Incident that began on February 28, 1947 that was violently suppressed by the Kuomintang (KMT). In that same month, the name of the official postal service of Taiwan was changed from the Chunghwa Post Co. to The Taiwan Post Co. The company's name was changed back on 1 August 2008, and the names on the postal stamps were reversed in late 2008, soon after the Kuomintang (KMT) candidate Ma Ying-Jeou won back presidency and ended 8 years of the Democratic Progressive Party (DPP) rule.
In March 2007, the name plate of the ROC Embassy in Panama was revised both to include the word "Taiwan" in parentheses between the words "the Republic of China" and "Embassy" in both of its Chinese and Spanish titles, and to omit the ROC national emblem.
Supporters of the name-change movement argue that the Republic of China no longer exists, as it did not include Taiwan when it was founded in 1912 and mainland China is now controlled by the Chinese Communist Party as the People's Republic of China. Furthermore, the ambiguity surrounding the legal status of Taiwan as a result of the Treaty of peace with Japan and Treaty of San Francisco after World War II, means that the Republic of China was merely a military occupier of Taiwan. As Japan relinquished its sovereignty over Taiwan without passing it to a specific country, it is argued that Taiwan ought to be deemed a land belonging to no country, whose international status has yet to be defined.
Constitutional and political campaign.
In October 2003, President Chen Shui-bian announced that Taiwan would seek a new constitution suitable for the Taiwan people that would turn Taiwan into a "normal country." In explaining what a normal country was in the context of desinicization and the 1992 One-China policy, Chen Shui-bian stated,
In response, the Pan-Blue Coalition within Taiwan sought to portray President Chen Shui-bian and his Democratic Progressive Party as radicals intent on implementing revolutionary desinicization that would disenfranchise various ethnic groups within Taiwan who have an affinity for China and the Chinese culture.
In February 2007, the Democratic Progressive Party (DPP) adopted a resolution to identify those responsible for the 1947 February 28 Incident massacre of Taiwanese people in order to charge them with war crimes and crimes against humanity. The effort also sought to remove the "remnants of dictatorship" traced to that sixty-year-old incident. This was seen in mainland China as being in line with a series of desinicization actions by both the Taiwan government and the DPP to rid both Chiang and China from the Taiwan public scene. Some applauded this as a courageous act of seeking justice. Others criticized the request, seeing it as "rubbing salt into wounds" by playing up the historical issues for political gain.
Other campaign.
In March 2007, it was noted that the destruction of the Western Line railway base found below the floor of the Taipei Main Station and built in 1893 by Qing Empire-appointed Governor of Taiwan Province Liu Mingchuan was part of the government's call for desinicization through removal of the Chinese site.
In July 2007, President Chen Shui-bian announced that he would allow mainland Chinese diplomas or students into Taiwan during the rest of his presidential term. This, however, was not achieved.
Impact.
One phenomenon that has resulted from the Taiwanization movement is the advent of Taike subculture, in which young people consciously adopt the wardrobe, language and cuisine to emphasize the uniqueness of popular, grassroots Taiwanese culture, which in previous times had often been seen as provincial and backwards by the mainstream.
In April 2002, the Communist Party of China (CPC) noted both active efforts on the part of Taiwan to push ahead its Taiwanization policy and intensified United States-Taiwan military cooperation. In response, the CPC publicly reminded its military to be prepared to achieve its goal of "Chinese reunification" (intended to mean making Taiwan a part of the People's Republic of China) through military means. In addition, the CPC sought assistance from the United States to address the matter with Taiwan. As part of making the upcoming U.S. visit by then vice-president Hu Jintao go smoothly, the United States cautioned the Chen Shui-bian administration not to "go too far" in cross-Strait relations.
In April 2005, the Communist Party of China (CPC) general secretary Hu Jintao and the former ROC Vice President and then chairman of the Kuomintang party (KMT) Lien Chan shook hands. Billed as a historic moment, this was the first handshake by the top leaders of the KMT and the CPC in 60 years. In remarking on the handshake, chairman Chan noted that it was a turning point where the KMT and the CPC would work together to bring about peaceful cross-strait relations and specifically distanced the KMT from Taiwan independence and desinicization efforts.
Support and opposition.
Significant outcries surfaced both within Taiwan and abroad opposing the concept of Taiwan localization in the early years after President Chiang Ching-kuo's death, denouncing it as the "independent Taiwan movement" (Chinese: 台獨運動). Vocal opponents are primarily the 1949-generation Mainlanders, or older generations of Mainlanders living in Taiwan that had spent their formative years and adulthood on the pre-1949 mainland Republic of China, and native Taiwanese who identify with a pan-Han Chinese cultural identity. They included people ranging from academics like Chien Mu, reputed to be the last prominent Chinese intellectual opposing the conventional wisdom take on the May Fourth Movement, politicians like Lien Chan, from a family with a long history of active pan-Chinese patriotism despite being native Taiwanese, to gang mobsters like Chang An-le, a leader of the notorious United Bamboo Gang.
The opposing voices were subsequently confined to the fringe in the mid 2000s Taiwan itself. Issues persist, particularly supporters of the Pan-Blue coalition, which advocates retaining a strong link to mainland China, dispute over such issues as what histories to teach. Nonetheless both of the two major political forces in Taiwan reached a consensus, and the movement has overwhelming support among the population. This is in part due to the 1949-generation Mainlanders have gradually passed on from the scene, and politicians supporting and opposing the Taiwanese independence movement both realize a majority of Taiwan's current residents, either because they are born in Taiwan to Mainlander parents with no collective memories of their ancestral homes, or they are native Taiwanese, thus feeling no historical connotations with the entire pre-1949 Republic of China on mainland China, support the movement as such.
In mainland China, the PRC government has on the surface adopted a neutral policy on Taiwanization and its highest-level leaders publicly proclaim it does not consider the Taiwanization movement to be either a violation of its One China Policy or equivalent to the independence movement. Nonetheless, the state-owned media and academics employed by organizations such as universities' Institutes of Taiwan Studies or the Chinese Academy of Social Sciences (CASS) periodically release study results, academic journal articles, or editorials strongly criticizing the movement as "the cultural arm of Taiwanese independence movement" (Chinese: 文化台獨) with the government's tacit approval, showing the PRC government's opposition towards Taiwanization.
Nowadays, another source of significant opposition to the Taiwanization movement remains in the overseas Chinese communities in Southeast Asia and the Western world, who identify more with the historic pre-1949 mainland Republic of China or pre-Taiwanization movement ROC on Taiwan that oriented itself as the rump legitimate government of China. A great many are themselves refugees and dissidents who fled mainland China, either directly or through Hong Kong or Taiwan, during the founding of the People's Republic of China and the subsequent periods of destructive policies (such as the Land Reform, the Anti-Rightist Movement, Great Leap Forward, or the Cultural Revolution), Hong Kong anti-Communist immigrants who fled Hong Kong in light of the Handover to the PRC in 1997, or Mainlanders living in Taiwan who moved to the West in response to the Taiwanization movement. Conversely, the current population of Taiwan regard these overseas Chinese as foreigners akin to Singaporean Chinese, as opposed to the pre-Taiwanization era when they were labeled as fellow Chinese compatriots. The PRC has capitalized on this window of opportunity in making overtures to the traditionally anti-Communist overseas Chinese communities, including gestures in supporting traditional Chinese culture and dumping explicitly Communist tones in overseas communications. This results in a decline of active political opposition to the PRC from overseas Chinese when compared with the times before the Taiwanization movement in Taiwan.
In Hong Kong, Taiwanization movements have pushed localization or pro-Chinese Communist tilts among the traditionally pro-Republic of China individuals and organizations. A prominent example is Chu Hai College, whose academic degree programs were recognized officially by the Hong Kong SAR government in May 2004, and registered as an "Approved Post-secondary College" with the Hong Kong SAR government since July of the same year. It has since been renamed the Chu Hai College of Higher Education (珠海學院) and no longer registered with the Republic of China's Ministry of Education. New students from 2004 have been awarded degrees in the right of Hong Kong rather than Taiwan.
Role in domestic politics.
Even though it is a broad consensus currently regarding the overall ideology of Taiwanization, there are still deep disputes over practical policies between the three main political groups of Taiwan independence, Chinese reunification, and supporters of Chinese culture. Pro-independence supporters argue that Taiwan is and should be enhancing an identity which is separate from the Chinese one, and in more extreme cases advocates the removal of Chinese "imprints". Meanwhile, some would argue that Taiwan should create a distinctive identity that either includes certain Chinese aspect or exists within a broader Chinese one. Those who support Chinese reunification call for a policy of enhancing the Chinese identity. Groups that support Chinese reunification and Chinese nationalism have emphasized the distinction between Taiwanization and what some perceive as desinicization and argued that they do not oppose the promotion of a Taiwanese identity, but rather oppose the use of that identity to separate itself from a broader Chinese one. On the other hand, a few apolitical groups have claimed that most of the political factions merely use these points to win support for elections.

</doc>
<doc id="66380" url="https://en.wikipedia.org/wiki?curid=66380" title="TOPS-20">
TOPS-20

The TOPS-20 operating system by Digital Equipment Corporation (DEC) was a proprietary OS for the PDP-10 mainframe computer.
TOPS-20 began in 1969 as the TENEX operating system of Bolt, Beranek and Newman (BBN) and shipped as a product by DEC starting in 1976. TOPS-20 is almost entirely unrelated to the similarly named TOPS-10, but it was shipped with the PA1050 TOPS-10 Monitor Calls emulation facility which allowed most, but not all, TOPS-10 executables to run unchanged. As a matter of policy, DEC did not update PA1050 to support later TOPS-10 additions except where required by DEC software.
TOPS-20 competed with TOPS-10, ITS and WAITS—all available for the PDP-10 during this timeframe.
TENEX.
In the 1960s, BBN was involved in a number of LISP-based artificial intelligence projects for DARPA, many of which had very large (for the era) memory requirements. One solution to this problem was to add "paging" software to the LISP language, allowing it to write out unused portions of memory to disk for later recall if needed. One such system had been developed for the PDP-1 at MIT by Daniel Murphy before he joined BBN. Early DEC machines were based on an 18-bit word, allowing addresses to encode for a 262-kword memory. The machines were based on expensive core memory and included nowhere near the required amount. The pager used the most significant bits of the address to index a table of blocks on a magnetic drum that acted as the pager's "backing store", and the software would fetch the pages if needed and then re-write the address to point to the proper area of RAM.
In 1964 DEC announced the PDP-6. DEC was still heavily involved with MIT's AI Lab, and many feature requests from the LISP hackers were moved into this machine. 36-bit computing was especially useful for LISP programming because with an 18-bit address space, a word of storage on these systems contained two addresses, a perfect match for the common LISP CAR and CDR operations. BBN became interested in buying one for their AI work when they became available, but wanted DEC to add a hardware version of Murphy's pager directly into the system. With such an addition, every program on the system would have paging support invisibly, making it much easier to do any sort of programming on the machine. DEC was initially interested, but soon (1966) announced they were in fact dropping the PDP-6 and concentrating solely on their smaller 18-bit and new 16-bit lines. The PDP-6 was expensive and complex, and had not sold well for these reasons.
It was not long until it became clear that DEC was once again entering the 36-bit business with what would become the PDP-10. BBN started talks with DEC to get a paging subsystem in the new machine, then known by its CPU name, the KA-10. DEC was not terribly interested. However, one development of these talks was support for a second virtual memory segment, allowing part of the user address space to be mapped to a separate (potentially read-only) region of physical memory. Additionally, DEC was firm on keeping the cost of the machine as low as possible, such as supporting bare-bones systems with a minimum of 16K words of core, and omitting the fast semiconductor register option (substituting core), at the cost of a considerable performance decrease.
BBN nevertheless went ahead with its purchase of several PDP-10s, and decided to build their own hardware pager. During this period a debate began on what operating system to run on the new machines. Strong arguments were made for the continued use of TOPS-10, in order to keep their existing software running with minimum effort. This would require a re-write of TOPS to support the paging system, and this seemed like a major problem. At the same time, TOPS did not support a number of features the developers wanted. In the end they decided to make a new system, but include an emulation library that would allow it to run existing TOPS-10 software with minor effort.
The developer team—amongst them Daniel Murphy and Daniel G. Bobrow—chose the name TENEX (TEN-EXtended) for the new system. It included a full virtual memory system—that is, not only could programs access a full 262kwords of memory, "every" program could do so at the same time. The pager system would handle mapping as it would always, copying data to and from the backing store as needed. The only change needed was for the pager to be able to hold several sets of mappings between RAM and store, one for each program using the system. The pager also held access time information in order to tune performance. The resulting pager was fairly complex, filling a full-height 19" rackmount chassis.
One notable feature of TENEX was its user-oriented command line interpreter. Unlike typical systems of the era, TENEX deliberately used long command names and even included non-significant "noise words" to further expand the commands for clarity. For instance, Unix uses codice_1 to print a list of files in a directory, whereas TENEX used codice_2. "codice_3" was the command word, "codice_4" was noise added to make the purpose of the command clearer. To relieve users of the need to type these long commands, TENEX used a "command completion" system that understood unambiguously abbreviated command words, and expanded partial command words into complete words or phrases. For instance, the user could type codice_5 and the escape key, at which point TENEX would replace codice_5 with the full command. The completion feature also worked with file names, which took some effort on the part of the interpreter, and the system allowed for long file names with human-readable descriptions. TENEX also included a "command recognition" help system: typing a question mark (codice_7), printed out a list of possible matching commands and then return the user to the command line with the question mark removed. The command line completion and help live on in current CLIs like tcsh.
TENEX became fairly popular in the small PDP-10 market, and the external pager hardware developed into a small business of its own. In early 1970 DEC started work on an upgrade to the PDP-10 processor, the KI-10. BBN once again attempted to get DEC to support a complex pager with indirect page tables, but instead DEC decided on a much simpler single-level page mapping system. This compromise impacted system sales; by this point TENEX was the most popular customer-written PDP-10 operating systems, but it would not run on the new, faster KI-10s.
To correct this problem, the DEC PDP-10 sales manager purchased the rights to TENEX from BBN and set up a project to port it to the new machine. At around this time Murphy moved from BBN to DEC as well, helping on the porting project. Most of the work centered on emulating the BBN pager hardware in a combination of software and the KI-10's simpler hardware. The speed of the KI-10 compared to the PDP-6 made this possible. Additionally the porting effort required a number of new device drivers to support the newer backing store devices being used.
Just as the new TENEX was shipping, DEC started work on the KL-10, intended to be a low-cost version of the KI-10. While this was going on, Stanford University AI programmers, many of them MIT alumni, were working on their own project to build a PDP-10 that was ten times faster than the original KA-10. The project evolved into the Foonly line of computers. DEC visited them and many of their ideas were then folded into the KL-10 project. The same year IBM also announced their own machine with virtual memory, making it a standard requirement for any computer. In the end the KL integrated a number of major changes to the system, but did not end up being any lower in cost. From the start, the new DECSYSTEM-20 would run a version of TENEX as its default operating system.
Functional upgrades for the KL-10 processor architecture were limited. The most significant new feature (called "extended addressing") was modified pager microcode running on a "Model B" hardware revision to enlarge the user virtual address space. Some effective address calculations by instructions located beyond the original 18-bit address space were performed to 30 significant bits, although only a 23-bit virtual address space was supported. Program code located in the original 18-bit address space had unchanged semantics, for backward compatibility.
The first in-house code name for the operating system was VIROS (VIRtual memory Operating System); when customers started asking questions, the name was changed to SNARK so that DEC could truthfully deny that there was any project called VIROS. When the name SNARK became known, the name was briefly reversed to become KRANS; this was quickly abandoned when someone objected that "krans" meant "funeral wreath" in Swedish (though it simply means "wreath"; this part of the story may be apocryphal).
Ultimately DEC picked TOPS-20 as the name of the operating system, and it was as TOPS-20 that it was marketed. The hacker community, mindful of its origins, quickly dubbed it TWENEX (a contraction of "twenty TENEX"), even though by this point very little of the original TENEX code remained (analogously to the differences between AT&T V7 Unix and BSD). DEC people cringed when they heard "TWENEX", but the term caught on nevertheless (the written abbreviation "20x" was also used).
TWENEX was successful and very popular; in fact, there was a period in the early 1980s when it commanded as fervent a culture of partisans as Unix or ITS—but DEC's decision to scrap all the internal rivals to the VAX architecture and its VMS OS killed the DEC-20 and put an end to TWENEX's brief period of popularity. DEC attempted to convince TOPS-20 users to convert to VMS, but instead, by the late 1980s, most of the TOPS-20 users had migrated to Unix. A loyal group of TOPS-20 enthusiasts kept working on various projects to preserve and extend TOPS-20, notably Mark Crispin and the Panda TOPS-20 distribution.

</doc>
<doc id="66382" url="https://en.wikipedia.org/wiki?curid=66382" title="Beta Carinae">
Beta Carinae

Beta Carinae (β Car, β Carinae) is the second brightest star in the constellation Carina and one of the brightest stars in the night sky, with apparent magnitude 1.68. It is the brightest star in the south polar asterism known as the Diamond Cross, marking the southwestern end of the asterism. Beta Carinae also has the traditional name Miaplacidus, meaning ""placid waters"". It lies near the planetary nebula IC 2448. Parallax measurements place it at a distance of from Earth.
Beta Carinae's traditional name Miaplacidus made its debut on star maps in 1856 when the star atlas "Geography of the Heavens", composed by Elijah Hinsdale Burritt, was published. The meaning and linguistic origin of the name remained an enigma for many decades, until William Higgins, a great scholar and expert on star names, surmised that the name Miaplacidus is apparently a bilingual combination of Arabic مياه "miyāh" for "waters" and Latin "placidus" for "placid".
In Chinese, (), meaning "Southern Boat", refers to an asterism consisting of β Carinae, V337 Carinae, PP Carinae, θ Carinae and ω Carinae . Consequently, β Carinae itself is known as (, .)
The stellar classification of A1 III– suggests this is an evolved giant star, although Malagnini and Morossi (1990) rated it as an A2 IV subgiant star. It has an estimated age of 260 million years. This star does not show an excess emission of infrared radiation that might otherwise suggest the presence of a debris disk. It has about 3.5 times the Sun's mass and has expanded to almost seven times the radius of the Sun. Presently is it radiating 288 times as much luminosity as the Sun from its outer envelope at an effective temperature of 8,866 K. Despite its enlarged girth, this star still shows a rapid rotation rate, with a projected rotational velocity of .

</doc>
<doc id="66387" url="https://en.wikipedia.org/wiki?curid=66387" title="Hypertension (disambiguation)">
Hypertension (disambiguation)

Hypertension may refer to the following:

</doc>
<doc id="66388" url="https://en.wikipedia.org/wiki?curid=66388" title="Bronchodilator">
Bronchodilator

A bronchodilator is a substance that the bronchi and bronchioles, decreasing resistance in the respiratory airway and increasing airflow to the lungs. Bronchodilators may be endogenous (originating naturally within the body), or they may be medications administered for the treatment of breathing difficulties. They are most useful in obstructive lung diseases, of which asthma and chronic obstructive pulmonary disease are the most common conditions. Although this remains somewhat controversial, they might be useful in bronchiolitis and Bronchiectasis.They are often prescribed but of unproven significance in restrictive lung diseases.
Bronchodilators are either short-acting or long-acting. Short-acting medications provide quick or "rescue" relief from acute bronchoconstriction. Long-acting bronchodilators help to control and prevent symptoms. The three types of prescription bronchodilating drugs are β2-agonists (short- and long-acting), anticholinergics (short-acting), and theophylline (long-acting).
Short-acting β2-agonists.
These are quick-relief or "rescue" medications that provide quick, temporary relief from asthma symptoms or flare-ups. These medications usually take effect within 20 minutes or less, and can last from four to six hours. These inhaled medications are best for treating sudden and severe or new asthma symptoms. Taken 15 to 20 minutes ahead of time, these medications can also prevent asthma symptoms triggered by exercise or exposure to cold air. Some short-acting β-agonists (for example salbutamol) are specific to the lungs; they are called β2-agonists and can relieve bronchospasms without unwanted cardiac (β1) side effects of nonspecific β-agonists (for example, ephedrine or epinephrine). Patients who regularly or frequently need to take short-acting β-agonists should consult their doctor, as such usage indicates uncontrolled asthma, and their routine medications may need adjustment.
Long-acting β2-agonists.
These are long-term medications taken routinely in order to control and prevent bronchoconstriction. They are not intended for fast relief. These medications may take longer to begin working, but relieve airway constriction for up to 12 hours.
Commonly taken twice a day with an anti-inflammatory medication, they maintain open airways and prevent asthma symptoms, particularly at night.
Salmeterol and Formoterol are examples of these.
Anticholinergics.
Some examples of anticholinergics are tiotropium (Spiriva) and ipratropium bromide.
Tiotropium is a long-acting, 24-hour, anticholinergic bronchodilator used in the management of chronic obstructive pulmonary disease (COPD).
Only available as an inhalant, ipratropium bromide is used in the treatment of asthma and COPD. It relieves acute or new asthma symptoms. It will not stop an asthma attack already in progress. Because it has no effect on asthma symptoms when used alone, it is most often paired with a short-acting β2-agonist. While it is considered a relief or rescue medication, it can take a full hour to begin working. For this reason, it plays a minor role in asthma treatment. Dry throat is the most common side effect. If the medication gets in contact with the eyes, it may cause blurred vision for a brief time.
Other.
Available in oral and injectable form, theophylline is a long-acting bronchodilator that prevents asthma episodes. It belongs to the chemical class "methyl xanthines" (along with caffeine). It is prescribed in severe cases of asthma or those that are difficult to control. It must be taken 1–4 times daily, and doses cannot be missed. Blood tests are required to monitor therapy and to indicate when dosage adjustment is necessary. Side effects can include nausea, vomiting, diarrhea, stomach or headache, rapid or irregular heart beat, muscle cramps, nervous or jittery feelings, and hyperactivity. These symptoms may signal the need for an adjustment in medication. It may promote acid reflux, also known as GERD, by relaxing the lower esophageal sphincter muscle. Some medications, such as seizure and ulcer medications and antibiotics containing erythromycin, can interfere with the way theophylline works. Coffee, tea, colas, cigarette-smoking, and viral illnesses can all affect the action of theophylline and change its effectiveness. A physician should monitor dosage levels to meet each patient's profile and needs.
Additionally some psychostimulant drugs that have an amphetamine like mode of action, such as amphetamine, methamphetamine, and cocaine, have bronchodilating effects and were used often for asthma due to the lack of effective β2-agonists for use as bronchodilator, but are now rarely, if ever, used medically for their bronchodilation effect.
Common bronchodilators.
The bronchodilators are divided in short- and long-acting groups. Short-acting bronchodilators are used for relief of bronchoconstriction, while long-acting bronchodilators are predominantly used as preventers.
Short-acting bronchodilators include:
Long-acting bronchodilators include

</doc>
<doc id="66390" url="https://en.wikipedia.org/wiki?curid=66390" title="Sarin">
Sarin

Sarin, or GB, is a colorless, odorless liquid, used as a chemical weapon owing to its extreme potency as a nerve agent. It is generally considered as a weapon of mass destruction. Production and stockpiling of sarin was outlawed as of April 1997 by the Chemical Weapons Convention of 1993, and it is classified as a Schedule 1 substance. In June 1994, the UN Special Commission on Iraqi disarmament destroyed the nerve agent sarin under Security Council resolution 687 (1991) concerning the disposal of Iraq's weapons of mass destruction.
Sarin is an organophosphorus compound with the formula [(CH3)2CHO]CH3P(O)F. It can be lethal even at very low concentrations, where death can occur within one to ten minutes after direct inhalation of a lethal dose, due to suffocation from lung muscle paralysis, unless some antidotes, typically atropine and an oxime, such as pralidoxime, are quickly administered. People who absorb a non-lethal dose, but do not receive immediate medical treatment, may suffer permanent neurological damage.
Production and structure.
Sarin is a chiral molecule because it has four chemically distinct substituents attached to the tetrahedral phosphorus center. The SP form (the (–) optical isomer) is the more active enantiomer due to its greater binding affinity to acetylcholinesterase. The P-F bond is easily broken by nucleophilic agents, such as water and hydroxide. At high pH, sarin decomposes rapidly to nontoxic phosphonic acid derivatives.
It is usually manufactured and weaponized as a racemic mixture—an equal mixture of both enantiomeric forms, as this is a simpler process and provides an adequate weapon.
There are a number of production pathways that can be used to create sarin. The final reaction typically involves attachment of the isopropoxy group to the phosphorus with an alcoholysis with isopropyl alcohol. Two variants of this process are common. One is the reaction of methylphosphonyl difluoride with isopropyl alcohol, which produces hydrofluoric acid as a byproduct:
The second process, uses an equal quantities of methylphosphonyl difluoride and methylphosphonic dichloride, a mixture "Di-Di" in this process, rather than just the difluoride. This reaction also gives sarin, but hydrochloric acid as a byproduct instead. The Di-Di process was used by the United States for the production of its unitary sarin stockpile.
As both reactions leave considerable acid in the product, bulk sarin produced without further treatment has a very poor shelf life and would be rather destructive to containers or weapon systems. Various methods have been tried to resolve these problems. In addition to industrial refining techniques to purify the chemical itself, various additives have been tried to combat the effects of the acid, such as:
Another byproduct of these two chemical processes is diisopropyl methylphosphonate, formed when a second isopropyl alcohol reacts with the sarin itself. This chemical degrades into isopropyl methylphosphonic acid.
Biological effects.
Like all other nerve agents, sarin attacks the nervous system by interfering with the degradation of the neurotransmitter acetylcholine at neuromuscular junctions. Death will usually occur as a result of asphyxia due to the inability to control the muscles involved in breathing function.
Specifically, sarin is a potent inhibitor of acetylcholinesterase, an enzyme that degrades the neurotransmitter acetylcholine after it is released into the synaptic cleft. In vertebrates, acetylcholine is the neurotransmitter used at the neuromuscular junction, where signals are transmitted between neurons from the central nervous systems to muscle fibres. Normally, acetylcholine is released from the neuron to stimulate the muscle, after which it is degraded by acetylcholinesterase, allowing the muscle to relax. A build-up of acetylcholine in the synaptic cleft, due to the inhibition of cholinesterase, means the neurotransmitter continues to act on the muscle fibre, so that any nerve impulses are effectively continually transmitted.
Sarin acts on cholinesterase by forming a covalent bond with the particular serine residue at the active site. Fluoride is the leaving group, and the resulting phosphoester is robust and biologically inactive.
Its mechanism of action resembles that of some commonly used insecticides, such as malathion. In terms of biological activity, it resembles carbamate insecticides, such as Sevin, and the medicines pyridostigmine, neostigmine, and physostigmine.
Degradation and shelf life.
The most important chemical reactions of phosphoryl halides is the hydrolysis of the bond between phosphorus and the fluoride. This P-F bond is easily broken by nucleophilic agents, such as water and hydroxide. At high pH, sarin decomposes rapidly to nontoxic phosphonic acid derivatives. The initial breakdown of sarin is into isopropyl methylphosphonic acid (IMPA), a chemical that is not commonly found in nature except as a breakdown product of sarin. IMPA then degrades into methylphosphonic acid (MPA), which can also be produced by other organophosphates.
Sarin without the residual acid removed degrades after a period of several weeks to several months. The shelf life can be shortened by impurities in precursor materials. According to the CIA, some Iraqi sarin had a shelf life of only a few weeks, owing mostly to impure precursors.
Along with nerve agents such as tabun and VX, sarin can have a maximum shelf-life of five years. Sarin's otherwise-short shelf life can be extended by increasing the purity of the precursor and intermediates and incorporating stabilizers such as tributylamine. In some formulations, tributylamine is replaced by diisopropylcarbodiimide (DIC), allowing sarin to be stored in aluminium casings. In binary chemical weapons, the two precursors are stored separately in the same shell and mixed to form the agent immediately before or when the shell is in flight. This approach has the dual benefit of solving the stability issue and increasing the safety of sarin munitions.
Effects and treatment.
Sarin has a high volatility (ease with which a liquid can turn into a gas) relative to similar nerve agents, therefore inhalation can be very dangerous and even vapor concentrations may immediately penetrate the skin. A person’s clothing can release sarin for about 30 minutes after it has come in contact with sarin gas, which can lead to exposure of other people.
Even at very low concentrations, sarin can be fatal. Death may follow in 1 to 10 minutes after direct inhalation of a lethal dose unless antidotes, typically atropine and pralidoxime, are quickly administered. Atropine, an antagonist to muscarinic acetylcholine receptors, is given to treat the physiological symptoms of poisoning. Since muscular response to acetylcholine is mediated through nicotinic acetylcholine receptors, atropine does not counteract the muscular symptoms. Pralidoxime can regenerate cholinesterases if administered within approximately five hours. Biperiden, a synthetic acetylcholine antagonist, has been suggested as an alternative to atropine due to its better blood–brain barrier penetration and higher efficacy.
As a nerve gas, sarin in its purest form is estimated to be 26 times more deadly than cyanide. The LD50 of subcutaneously injected sarin in mice is 172 μg/kg. Treatment measures have been described.
Initial symptoms following exposure to sarin are a runny nose, tightness in the chest and constriction of the pupils. Soon after, the victim has difficulty breathing and experiences nausea and drooling. As the victim continues to lose control of bodily functions, the victim vomits, defecates and urinates. This phase is followed by twitching and jerking. Ultimately, the victim becomes comatose and suffocates in a series of convulsive spasms. Moreover, common mnemonics for the symptomatology of organophosphate poisoning, including sarin gas, are the "killer B's" of bronchorrhea and bronchospasm because they are the leading cause of death, and SLUDGE – Salivation, Lacrimation, Urination, Defecation, Gastrointestinal distress, and Emesis.
Diagnostic tests.
Controlled studies in healthy men have shown that a nontoxic 0.43 mg oral dose administered in several portions over a 3-day interval caused average maximum depressions of 22 and 30%, respectively, in plasma and erythrocyte cholinesterase levels. A single acute 0.5 mg dose caused mild symptoms of intoxication and an average reduction of 38% in both measures of cholinesterase activity. Sarin in blood is rapidly degraded either "in vivo" or "in vitro". Its primary inactive metabolites have "in vivo" serum half-lives of approximately 24 hours. The serum level of unbound isopropylmethylphosphonic acid (IMPA), a sarin hydrolysis product, ranged from 2-135 µg/L in survivors of a terrorist attack during the first 4 hours post-exposure. Sarin or its metabolites may be determined in blood or urine by gas or liquid chromatography, while cholinesterase activity is usually measured by enzymatic methods.
A newer method called "Fluoride Regeneration" or "Fluoride Reactivation" detects the presence of nerve agents for a longer period after exposure than the methods described above. Fluoride reactivation is a technique has been explored since at least the early 2000s. This technique obviates some of the deficiencies of older procedures. Sarin not only reacts with the water in the blood plasma through hydrolysis (forming so-called ‘free metabolites’), but also reacts with various proteins to form ‘protein adducts’. These protein adducts are not so easily removed from the body, and remain for a longer period of time than the free metabolites. One clear advantage of this process is that the period, post-exposure, for determination of Sarin exposure is much longer, possibly 5 to 8 weeks according to at least one study.
Toxicity.
Sarin is highly toxic, whether by respiratory or dermal exposure. The toxicity of sarin in humans is largely based on calculations from studies with animals. The general consensus is that the lethal concentration of sarin in air is approximately 35 mg per cubic meter per minute for a two-minute exposure time by a healthy adult breathing normally (exchanging 15 liters of air per minute). This number represents the estimated lethal concentration for 50% of exposed victims, the LCt50 value. There are many ways to make relative comparisons between toxic substances. The list below compares some current and historic chemical warfare agents with sarin, with a direct comparison to the respiratory Lct50:
History.
Sarin was discovered in 1938 in Wuppertal-Elberfeld in Germany by scientists at IG Farben who were attempting to create stronger pesticides; it is the most toxic of the four G-Series nerve agents made by Germany. The compound, which followed the discovery of the nerve agent tabun, was named in honor of its discoverers: Schrader, Ambros, Gerhard Ritter, and Van der Linde.
Use as a weapon.
In mid-1939, the formula for the agent was passed to the chemical warfare section of the German Army Weapons Office, which ordered that it be brought into mass production for wartime use. Pilot plants were built, and a high-production facility was under construction (but was not finished) by the end of World War II. Estimates for total sarin production by Nazi Germany range from 500 kg to 10 tons. Though sarin, tabun and soman were incorporated into artillery shells, Germany did not use nerve agents against Allied targets.

</doc>
<doc id="66391" url="https://en.wikipedia.org/wiki?curid=66391" title="Stimulant">
Stimulant

Stimulants (also referred to as psychostimulants) are psychoactive drugs that induce temporary improvements in either mental or physical functions or both. Examples of these kinds of effects may include enhanced alertness, wakefulness, and locomotion, among others. Due to their rendering a characteristic "up" feeling, stimulants are also occasionally referred to as "uppers". Depressants or "downers", which decrease mental and/or physical function, are in stark contrast to stimulants and are considered to be the functionally opposite drug class. Stimulants are widely used throughout the world as prescription medicines and without prescription both as legal substances and illicit substances of recreational use or abuse.
Effects.
Stimulants produce a variety of different kinds of effects by enhancing the activity of the central and peripheral nervous systems. Common effects, which vary depending on the substance and dosage in question, may include enhanced alertness, awareness, wakefulness, endurance, productivity, and motivation, increased arousal, locomotion, heart rate, and blood pressure, and the perception of a diminished requirement for food and sleep. Many stimulants are also capable of improving mood and relieving anxiety, and some can even induce feelings of euphoria. However different effects are often dose related, such as amphetamine causing anxiety, dysthymia, hyperactivity and potentially heart failure at high doses, but relieving anxiety, producing euthymia or euphoria, reducing hyperactivity and being generally free of serious side effects at moderate doses used in clinical medicine. Stimulants exert their effects through a number of different pharmacological mechanisms, the most prominent of which include facilitation of norepinephrine (noradrenaline) and/or dopamine activity (e.g., via monoamine transporter inhibition or reversal), adenosine receptor antagonism, and nicotinic acetylcholine receptor agonism.
Medical uses.
Stimulants are used both individually and clinically for therapeutic purposes in the treatment of a number of indications, including the following:
ADHD drugs.
Stimulants are the most effective, most commonly prescribed medications for ADHD. The most common stimulant medications are substituted phenethylamines: amphetamine, methylphenidate (Ritalin, Metadate, Concerta), dexmethylphenidate (Focalin), dextroamphetamine (Dexedrine, Zenzedi), mixed amphetamine salts (Adderall), dextromethamphetamine (Desoxyn) and lisdexamfetamine (Vyvanse). Controlled-release formulations may allow once or twice daily administration of medication. Once daily morning administration is especially helpful for children preferring not to take their medication in the middle of the school day. Several controlled-release methods are used.
Ampakines.
Ampakines are a class of compounds observed to enhance attention span and alertness, and facilitate learning and memory in clinical trials. They take their name from the glutamatergic AMPA receptor with which they strongly interact.
These stimulants tend to increase alertness without the peripheral (body) effects or addiction/tolerance/abuse potential of "traditional" stimulants (such as amphetamine), as they lack direct dopaminergic action. Their effect on sleep structure is not fully established and may reduce quality of sleep. The ampakine CX717, when administered at doses necessary to reduce the effects of sleep deprivation, reduced subsequent stage 4 and slow-wave recovery sleep. Ampakines such as ampalex and CX717 have been developed but are awaiting further research before being commercially released. They have been investigated by DARPA for potential use in increasing military effectiveness.
Eugeroics.
A wakefulness-promoting agent (eugeroic) is a type of psychoactive drug that improves wakefulness and alertness, and reduces tiredness, drowsiness, and the need for sleep. They are used mainly in the treatment of sleeping disorders, excessive daytime sleepiness, and narcolepsy, though they are also used merely to counteract fatigue and lethargy and to enhance motivation and productivity. Wakefulness-promoting agents appear to function primarily by increasing catecholaminergic (adrenergic, dopaminergic) and histaminergic activity in the brain. Unlike many other stimulants, eugeroics are relatively non-addictive and non-dependence-forming.
The prototype drug in this class is modafinil, and other drugs include adrafinil, hydrafinil, and armodafinil. The primary difference between these drugs and amphetamine-like stimulants is that wakefulness-promoting agents trigger activation of neurons in the hypothalamus-based wakefulness circuits, as opposed to producing diffuse neuronal activation.
The functional opposites of wakefulness-promoting agents would be hypnotics/sedatives like antihistamines, opioids, and benzodiazepines.
Chemistry.
Classifying stimulants is difficult, because of the large number of classes the drugs occupy, and the fact that they may belong to multiple classes; for example, ecstasy is a substituted methylenedioxyphenethylamine as well as a substituted amphetamine (and consequently, a substituted phenethylamine as well).
When referring to stimulants, the parent drug (e.g., "amphetamine") will always be expressed in the singular; with the word "substituted" placed before the parent drug ("substituted amphetamines").
Major stimulant classes include phenethylamines and their daughter class substituted amphetamines.
Amphetamines (class).
Substituted amphetamines are a group of phenylethylamine stimulants such as amphetamine and methamphetamine. With the exception of cathinones, many drugs in this class work primarily by activating trace amine-associated receptor 1 (TAAR1); in turn, this causes reuptake inhibition and effluxion, or release, of dopamine, norepinephrine, and serotonin. An additional mechanism of some substituted amphetamines is the release of neurotransmitters from synaptic vesicles into the cytosol, or intracellular fluid of the presynaptic neuron.
Amphetamines-type stimulants are often used for their therapeutic effects. Physicians sometimes prescribe amphetamine to treat major depression, where subjects do not respond well to traditional SSRI medications, but evidence supporting this use is poor/mixed. Notably, two recent large phase III studies of lisdexamfetamine (a prodrug to amphetamine) as an adjunct to an SSRI or SNRI in the treatment of major depressive disorder showed no further benefit relative to placebo in effectiveness. Numerous studies have demonstrated the effectiveness of drugs such as Adderall (a mixture of salts of amphetamine and dextroamphetamine) in controlling symptoms associated with ADD/ADHD. Due to their availability and fast-acting effects, substituted amphetamines are prime candidates for abuse.
Dopamine precursors.
Dopamine is one of the principal neurotransmitters involved with stimulant activity in the brain, (others being norepinephrine and serotonin). Increase in its precursors may result in increased dopamine biosynthesis, especially in malnourished individuals. However levels of the enzyme tyrosine hydroxylase ultimately limit the biosynthesis regardless of increased tyrosine.
L-Tyrosine is the precursor that is 'closest' to being dopamine among those supplements legally available without prescription in most jurisdictions. It is converted by tyrosine hydroxylase into L-Dopa. Some of this L-Dopa is converted into dopamine and norepinephrine. Because tyrosine competes with other amino acids for entry into the brain supplement makers recommend tyrosine be taken on an empty stomach. However tyrosine hydroxylase is the rate limiting factor and even large dosages recommended by most supplement companies may not produce any noticeable effect. L-Phenylalanine is 'one step back' from L-Tyrosine—it must be converted into tyrosine before the tyrosine can be converted into L-Dopa, which in turn becomes dopamine. Dopamine is also the direct precursor of norepinephrine.
Notable stimulants.
Amphetamine (drug).
Amphetamine is a potent central nervous system (CNS) stimulant of the phenethylamine class that is used in the treatment of attention deficit hyperactivity disorder (ADHD) and narcolepsy. Amphetamine was discovered in 1887 and exists as two enantiomers: levoamphetamine and dextroamphetamine. "Amphetamine" refers to equal parts of the enantiomers, i.e., 50% levoamphetamine and 50% dextroamphetamine. Historically, it has been used to treat nasal congestion, depression, and obesity. Amphetamine is also used as a performance and cognitive enhancer, and recreationally as an aphrodisiac and euphoriant. Although it is a prescription medication in many countries, unauthorized possession and distribution of amphetamine is often tightly controlled due to the significant health risks associated with uncontrolled or heavy use. As a consequence, amphetamine is illegally synthesized by clandestine chemists, trafficked, and sold. Based upon drug and drug precursor seizures worldwide, illicit amphetamine production and trafficking is much less prevalent than that of methamphetamine.
The first pharmaceutical amphetamine was Benzedrine, a brand of inhalers used to treat a variety of conditions. Because the dextro isomer has greater stimulant properties, Benzedrine was gradually discontinued in favor of formulations containing all or mostly dextroamphetamine. Presently, it is typically prescribed as Adderall, dextroamphetamine (e.g., Dexedrine), or the inactive prodrug lisdexamfetamine (e.g., Vyvanse). Amphetamine, through activation of a trace amine receptor, increases biogenic amine and excitatory neurotransmitter activity in the brain, with its most pronounced effects targeting the catecholamine neurotransmitters norepinephrine and dopamine. At therapeutic doses, this causes emotional and cognitive effects such as euphoria, change in libido, increased arousal, and improved cognitive control. Likewise, it induces physical effects such as decreased reaction time, fatigue resistance, and increased muscle strength.
In contrast, much larger doses of amphetamine are likely to impair cognitive function and induce rapid muscle breakdown. Substance dependence (i.e., addiction) is a serious risk of amphetamine abuse, but only rarely arises from proper medical use. Very high doses can result in a psychosis (e.g., delusions and paranoia), which very rarely occurs at therapeutic doses even during long-term use. As recreational doses are generally much larger than prescribed therapeutic doses, recreational use carries a far greater risk of serious side effects.
Amphetamine is the parent compound of its own structural class, the (substituted) amphetamines, which includes prominent substances such as bupropion, cathinone, ecstasy, and methamphetamine. It is chemically related to methamphetamine; however, unlike methamphetamine, its salts lack sufficient volatility to be smoked. During long-term treatment in humans, amphetamine has been shown to normalize, or improve, brain function, in particular in the right caudate nucleus; in contrast, methamphetamine induces permanent reductions in brain structure and function. Amphetamine is also chemically related to the naturally occurring trace amines, to be specific phenethylamine and , both of which produced within the human body.
Caffeine.
Caffeine is a stimulant compound belonging to the xanthine class of chemicals naturally found in coffee, tea, and (to a lesser degree) cocoa or chocolate. It is included in many soft drinks, as well as a larger amount in energy drinks. Caffeine is the world's most widely used psychoactive drug and by far the most common stimulant. In North America, 90% of adults consume caffeine daily. A few jurisdictions restrict its sale and use. Caffeine is also included in some medications, usually for the purpose of enhancing the effect of the primary ingredient, or reducing one of its side-effects (especially drowsiness). Tablets containing standardized doses of caffeine are also widely available.
Ephedrine.
Ephedrine is a sympathomimetic amine similar in molecular structure to the well-known drugs phenylpropanolamine and methamphetamine, as well as to the important neurotransmitter epinephrine (adrenaline). Ephedrine is commonly used as a stimulant, appetite suppressant, concentration aid, and decongestant, and to treat hypotension associated with anaesthesia.
In chemical terms, it is an alkaloid with a phenethylamine skeleton found in various plants in the genus "Ephedra" (family Ephedraceae). It works mainly by increasing the activity of norepinephrine (noradrenaline) on adrenergic receptors. It is most usually marketed as the "hydrochloride" or "sulfate" salt.
The herb "má huáng" ("Ephedra sinica"), used in traditional Chinese medicine (TCM), contains ephedrine and pseudoephedrine as its principal active constituents. The same may be true of other herbal products containing extracts from other "Ephedra" species.
MDMA.
3,4-Methylenedioxymethamphetamine (MDMA, ecstasy, or molly), typically comes as tablets, capsules, and in powder/crystal form. Briefly used by some psychotherapists as an adjunct to therapy, the drug became popular recreationally and the DEA listed MDMA as a Schedule I controlled substance, prohibiting most medical studies and applications. MDMA is known for its entactogenic properties. The stimulant effects of MDMA include hypertension, anorexia (appetite loss), euphoria, social disinhibition, insomnia (enhanced wakefulness/inability to sleep), improved energy, increased arousal, and increased perspiration, among others.
MDMA differs from most stimulants in that its primary pharmacological effect is on the neurotransmitter serotonin rather than dopamine, epinephrine, or norepinephrine. Because of this, it is considered to be primarily an entactogen or an empathogen.
MDPV.
Methylenedioxypyrovalerone (MDPV) is a psychoactive drug with stimulant properties that acts as a norepinephrine-dopamine reuptake inhibitor (NDRI). It was first developed in the 1960s by a team at Boehringer Ingelheim. MDPV remained an obscure stimulant until around 2004, when it was reported to be sold as a designer drug. Products labeled as bath salts containing MDPV were previously sold as recreational drugs in gas stations and convenience stores in the United States, similar to the marketing for Spice and K2 as incense.
Incidents of psychological and physical harm have been attributed to MDPV use.
Mephedrone.
Mephedrone is a synthetic stimulant drug of the amphetamine and cathinone classes. Slang names include drone and MCAT. It is reported to be manufactured in China and is chemically similar to the cathinone compounds found in the khat plant of eastern Africa. It comes in the form of tablets or a powder, which users can swallow, snort, or inject, producing similar effects to MDMA, amphetamines, and cocaine.
Mephedrone was first synthesized in 1929, but did not become widely known until it was rediscovered in 2003. By 2007, mephedrone was reported to be available for sale on the Internet; by 2008 law enforcement agencies had become aware of the compound; and, by 2010, it had been reported in most of Europe, becoming particularly prevalent in the United Kingdom. Mephedrone was first made illegal in Israel in 2008, followed by Sweden later that year. In 2010, it was made illegal in many European countries, and, in December 2010, the EU ruled it illegal. In Australia, New Zealand, and the USA, it is considered an analog of other illegal drugs and can be controlled by laws similar to the Federal Analog Act. In September 2011, the USA temporarily classified mephedrone as illegal, in effect from October 2011.
Methamphetamine.
Methamphetamine (contracted from ) is a neurotoxin and potent psychostimulant of the phenethylamine and amphetamine classes that is used to treat attention deficit hyperactivity disorder (ADHD) and obesity. Methamphetamine exists as two enantiomers, dextrorotary and levorotary. Dextromethamphetamine is a stronger CNS stimulant than levomethamphetamine; however, both are addictive and produce the same toxicity symptoms at high doses. Although rarely prescribed due to the potential risks, methamphetamine hydrochloride is approved by the United States Food and Drug Administration (USFDA) under the trade name "Desoxyn". Recreationally, methamphetamine is used to increase sexual desire, lift the mood, and increase energy, allowing some users to engage in sexual activity continuously for several days straight.
Methamphetamine may be sold illicitly, either as pure dextromethamphetamine or in an equal parts mixture of the right- and left-handed molecules (i.e., 50% levomethamphetamine and 50% dextromethamphetamine). Both dextromethamphetamine and racemic methamphetamine are schedule II controlled substances in the United States. Also, the production, distribution, sale, and possession of methamphetamine is restricted or illegal in many other countries due to its placement in schedule II of the United Nations Convention on Psychotropic Substances treaty. In contrast, levomethamphetamine is an over-the-counter drug in the United States.
In low doses, methamphetamine can cause an elevated mood and increase alertness, concentration, and energy in fatigued individuals. At higher doses, it can induce psychosis, rhabdomyolysis, and cerebral hemorrhage. Methamphetamine is known to have a high potential for abuse and addiction. Recreational use of methamphetamine may result in psychosis or lead to post-withdrawal syndrome, a withdrawal syndrome that can persist for months beyond the typical withdrawal period. Unlike amphetamine and cocaine, methamphetamine is neurotoxic to humans, damaging both dopamine and serotonin neurons in the central nervous system (CNS). Entirely opposite to the long-term use of amphetamine, there is evidence that methamphetamine causes brain damage from long-term use in humans; this damage includes adverse changes in brain structure and function, such as reductions in gray matter volume in several brain regions and adverse changes in markers of metabolic integrity.
Nicotine.
Nicotine is the active chemical constituent in tobacco, which is available in many forms, including cigarettes, cigars, chewing tobacco, and smoking cessation aids such as nicotine patches, nicotine gum, and electronic cigarettes. Nicotine is used widely throughout the world for its stimulating and relaxing effects.
Phenylpropanolamine.
Phenylpropanolamine (PPA; Accutrim; β-hydroxyamphetamine), also known as the stereoisomers norephedrine and norpseudoephedrine, is a psychoactive drug of the phenethylamine and amphetamine chemical classes that is used as a stimulant, decongestant, and anorectic agent. It is commonly used in prescription and over-the-counter cough and cold preparations. In veterinary medicine, it is used to control urinary incontinence in dogs under trade names Propalin and Proin.
In the United States, PPA is no longer sold without a prescription due to a proposed increased risk of stroke in younger women. In a few countries in Europe, however, it is still available either by prescription or sometimes over-the-counter. In Canada, it was withdrawn from the market on 31 May 2001. In India, human use of PPA and its formulations were banned on 10 February 2011.
Propylhexedrine.
Propylhexedrine (Hexahydromethamphetamine, Obesin) is a stimulant medication, sold over-the-counter in the United States as the cold medication Benzedrex. The drug has also been used as an appetite suppressant in Europe. Propylhexedrine is not an amphetamine, though it is structurally similar; it is instead a cycloalkylamine, and thus has stimulant effects that are less potent than similarly structured amphetamines, such as methamphetamine.
The abuse potential of propylhexedrine is fairly limited, due its limited routes of administration: in the United States, Benzedrex is only available as an inhalant, mixed with lavender oil and menthol. These ingredients cause unpleasant tastes, and abusers of the drug have reported unpleasant "menthol burps." Injection of the drug has been found to cause transient diplopia and brain stem dysfunction.
Dimethylamylamine.
Dimethylamylamine is a stimulant drug, once sold in over-the-counter workout supplements and study aids in the United States as in the supplement Jack 3D, but it was later discontinued. Dimethylamylamine is not an amphetamine, though it is structurally similar, and thus has stimulant effects that are less potent than similarly structured amphetamines, such as amphetamine.
Pseudoephedrine.
Pseudoephedrine is a sympathomimetic drug of the phenethylamine and amphetamine chemical classes. It may be used as a nasal/sinus decongestant, as a stimulant, or as a wakefulness-promoting agent.
The salts pseudoephedrine hydrochloride and pseudoephedrine sulfate are found in many over-the-counter preparations, either as a single ingredient or (more commonly) in combination with antihistamines, guaifenesin, dextromethorphan, and/or paracetamol (acetaminophen) or another NSAID (such as aspirin or ibuprofen).
"Catha edulis" (Khat).
Khat is a flowering plant native to the Horn of Africa and the Arabian Peninsula.
Khat contains a monoamine alkaloid called cathinone, a "keto-amphetamine", that is said to cause excitement, loss of appetite, and euphoria. In 1980, the World Health Organization (WHO) classified it as a drug of abuse that can produce mild to moderate psychological dependence (less than tobacco or alcohol), although the WHO does not consider khat to be seriously addictive. It is a controlled substance in some countries, such as the United States, Canada, and Germany, while its production, sale, and consumption are legal in other nations, including Djibouti, Ethiopia, Somalia, and Yemen.
Cocaine.
Cocaine is an SNDRI. Cocaine is made from the leaves of the coca shrub, which grows in the mountain regions of South American countries such as Bolivia, Colombia, and Peru. In Europe, North America, and some parts of Asia, the most common form of cocaine is a white crystalline powder. Cocaine is a stimulant but is not normally prescribed therapeutically for its stimulant properties, although it sees clinical use as a local anesthetic, in particular in ophthalmology. Most cocaine use is recreational and its abuse potential is high(albeit higher than amphetamine), and so its sale and possession are strictly controlled in most jurisdictions. Other tropane derivative drugs related to cocaine are also known such as troparil and lometopane but have not been widely sold or used recreationally.
Abuse.
Abuse of central nervous system (CNS) stimulants is common. Addiction to some CNS stimulants can quickly lead to medical, psychiatric, and psychosocial deterioration. Drug tolerance, dependence, and sensitization as well as a withdrawal syndrome can occur.
Stimulants enhance the activity of the central and peripheral nervous systems. Common effects may include increased alertness, awareness, wakefulness, endurance, productivity, and motivation, arousal, locomotion, heart rate, and blood pressure, and a diminished desire for food and sleep.
Use of stimulants may cause the body to reduce significantly its production of natural body chemicals that fulfill similar functions. Until the body reestablishes its normal state, once the effect of the ingested stimulant has worn off the user may feel depressed, lethargic, confused, and miserable. This is referred to as a "crash", and may provoke reuse of the stimulant.
Testing.
The presence of stimulants in the body may be tested by a variety of procedures. Serum and urine are the common sources of testing material although saliva is sometimes used. Commonly used tests include chromatography, immunologic assay, and mass spectrometry. Patients taking ADHD-prescribed, Adderall-type amphetamine compounds are commonly surprised upon being tested as "positive" for "meth", or methamphetamine (Desoxyn—its licit, FDA-licensed, medicinal form) in forensically unsophisticated urinalysis, as methamphetamine is the active ingredient of the drug Desoxyn, and is chemically similar to the active ingredients of other ADHD medications.

</doc>
