<doc id="42720" url="https://en.wikipedia.org/wiki?curid=42720" title="Second Boer War">
Second Boer War

The Second Boer War (, , literally "Second Freedom War") otherwise known as the Second Anglo-Boer War or the South African War, occurred from 11 October 1899 until 31 May 1902. The United Kingdom fought the South African Republic (Transvaal Republic) and the Orange Free State. The British war effort was supported by troops from several regions of the British Empire, including Southern Africa, the Australian colonies, Canada, Newfoundland, British India, and New Zealand. The war ended in victory for the British and the annexation of both republics. Both would eventually be incorporated into the Union of South Africa in 1910.
Name.
The conflict is commonly referred to as simply the Boer War, since the First Boer War (December 1880 to March 1881) is much less well known. "Boer" was the common term for Afrikaans-speaking settlers in southern Africa at the time. It is also known as the South African War outside South Africa and as the (Second) Anglo-Boer War among South Africans. In Afrikaans it may be called the "Anglo-Boereoorlog" ("Anglo-Boer War"), "Tweede Boereoorlog" ("Second Boer War"), "Tweede Vryheidsoorlog" ("Second Freedom War", i.e., a war of liberation) or "Engelse oorlog" ("English War").
Origins.
The complex origins of the war resulted from more than a century of conflict between the Boers and the British Empire, but of particular immediate importance was the question as to which nation would control and benefit most from the very lucrative Witwatersrand gold mines. During the Napoleonic Wars, a British military expedition landed in the Cape Colony and defeated the defending Dutch forces at the Battle of Blaauwberg (1806). After the war, the British formally acquired the colony (1814), and encouraged immigration by British settlers who were largely at odds with the Dutch settlers. Many Boers who were dissatisfied with aspects of British administration, in particular with Britain's abolition of slavery on 1 December 1834, elected to migrate away from British rule in what became known as the Great Trek.
The Trekkers initially followed the eastern coast towards Natal and then, after Britain annexed the Natal in 1843, journeyed northwards towards the interior. There they established two independent Boer republics: the South African Republic (1852; also known as the Transvaal Republic) and the Orange Free State (1854). The British recognised the two Boer republics in 1852 and 1854, but attempted British annexation of the Transvaal in 1877 led to the First Boer War in 1880–81. After the British suffered defeats, particularly at the Battle of Majuba Hill (1881), the independence of the two republics was restored subject to certain conditions; relations, however, remained uneasy.
In 1866 Erasmus Jacobs discovered diamonds at Kimberley, prompting a diamond rush and a massive influx of foreigners to the borders of the Orange Free State. Then in 1886, an Australian discovered gold in the Witwatersrand area of the South African Republic. Gold made the Transvaal the richest and potentially the most powerful nation in southern Africa; however, the country had neither the manpower nor the industrial base to develop the resource on its own. As a result, the Transvaal reluctantly acquiesced to the immigration of "uitlanders" (foreigners), mainly from Britain, who came to the Boer region in search of fortune and employment. This resulted in the number of uitlanders in the Transvaal potentially exceeding the number of Boers, and precipitated confrontations between the earlier-arrived Boer settlers and the newer, non-Boer arrivals.
British expansionist ideas (notably propagated by Cecil Rhodes) as well as disputes over uitlander political and economic rights resulted in the failed Jameson Raid of 1895. Dr. Leander Starr Jameson, who led the raid, intended to encourage an uprising of the uitlanders in Johannesburg. However, the uitlanders did not take up arms in support, and Transvaal government forces surrounded the column and captured Jameson's men before they could reach Johannesburg.
As tensions escalated, political manoeuvrings and negotiations attempted to reach compromise on the issues of the rights of the uitlanders within the South African Republic, control of the gold mining industry, and the British desire to incorporate the Transvaal and the Orange Free State into a federation under British control. Given the British origins of the majority of uitlanders and the ongoing influx of new uitlanders into Johannesburg, the Boers recognised that granting full voting rights to the uitlanders would eventually result in the loss of ethnic Boer control in the South African Republic.
The June 1899 negotiations in Bloemfontein failed, and in September 1899 British Colonial Secretary Joseph Chamberlain demanded full voting rights and representation for the uitlanders residing in the Transvaal. Paul Kruger, the President of the South African Republic, issued an ultimatum on 9 October 1899, giving the British government 48 hours to withdraw all their troops from the borders of both the Transvaal and the Orange Free State, albeit Kruger had ordered Commandos to the Natal border in early September and the British only had troops in garrison towns far from the border, failing which the Transvaal, allied to the Orange Free State, would declare war on the British government. The British government rejected the South African Republic's ultimatum, resulting in the South African Republic and Orange Free State declaring war on Britain.
Phases.
The war had three distinct phases. In the first phase, the Boers mounted pre-emptive strikes into British-held territory in Natal and the Cape Colony, besieging the British garrisons of Ladysmith, Mafeking and Kimberley. The Boers then won a series of tactical victories at Colenso, Magersfontein and Spionkop.
In the second phase, after the introduction of greatly increased British troop numbers under the command of Lord Roberts, the British launched another offensive in 1900 to relieve the sieges, this time achieving success. After Natal and the Cape Colony were secure, the British were able to invade the Transvaal, and the republic's capital, Pretoria, was ultimately captured in June 1900.
In the third and final phase, beginning in March 1900, the Boers launched a protracted hard-fought guerrilla war against the British forces, lasting a further two years, during which the Boers raided targets such as British troop columns, telegraph sites, railways and storage depots. In an effort to cut off supplies to the raiders, the British, now under the leadership of Lord Kitchener, responded with a scorched earth policy of destroying Boer farms and moving civilians into concentration camps.
Some parts of the British press and British government expected the campaign to be over within months, and the protracted war gradually became less popular, especially after revelations about the conditions in the concentration camps (where as many as 26,000 Afrikaner women and children died of disease and malnutrition). The Boer forces finally surrendered on Saturday, 31 May 1902, with 54 of the 60 delegates from the Transvaal and Orange Free State voting to accept the terms of the peace treaty. This was known as the Treaty of Vereeniging, and under its provisions, the two republics were absorbed into the British Empire, with the promise of self-government in the future. This promise was fulfilled with the creation of the Union of South Africa in 1910.
The war had a lasting effect on the region and on British domestic politics. For Britain, the Second Boer War was the longest, the most expensive (£200 million, almost £22 billion as at 2015), and the bloodiest conflict between 1815 and 1914, lasting three months longer and resulting in higher British casualties than the Crimean War (1853–56) (although more soldiers died from disease in the Crimean War).
Background.
The southern part of the African continent was dominated in the 19th century by a set of struggles to create within it a single unified state. While the Berlin Conference of 1884–5 sought to draw boundaries between the European powers' African possessions, it also set the stage for further scrambles. The British attempted to annex first the South African Republic in 1880, and then, in 1899, both the South African Republic and the Orange Free State. In 1868, the British annexed Basutoland in the Drakensberg Mountains following an appeal from Moshesh, the leader of a mixed group of African refugees from the Zulu wars, who sought British protection against the Boers.
In the 1880s, Bechuanaland (modern Botswana, located north of the Orange River) became the object of a dispute between the Germans to the west, the Boers to the east, and the British Cape Colony to the south. Although Bechuanaland had no economic value, the "Missionaries Road" passed through it towards territory farther north. After the Germans annexed Damaraland and Namaqualand (modern Namibia) in 1884, the British annexed Bechuanaland in 1885.
In the First Boer War of 1880–81 the Boers of the Transvaal Republic had proved skilful fighters in resisting the British attempt at annexation, causing a series of British defeats. The British government of William Ewart Gladstone had been unwilling to become mired in a distant war, requiring substantial troop reinforcement and expense, for what was at the time perceived to be a minimal return. An armistice followed, ending the war, and subsequently a peace treaty was signed with the Transvaal President Paul Kruger.
However, when, in 1886, a major gold field was discovered at an outcrop on a large ridge some sixty kilometres south of the Boer capital at Pretoria, it reignited British imperial interests. The ridge, known locally as the "Witwatersrand" (literally "white water ridge"–a watershed) contained the world's largest deposit of gold-bearing ore. Although it was not as rich as gold finds in Canada and Australia, its consistency made it especially well-suited to industrial mining methods. With the 1886 discovery of gold in the Transvaal, the resulting gold rush brought thousands of British and other prospectors and settlers from across the globe and over the border from the Cape Colony (under British control since 1806).
The city of Johannesburg sprang up as a shanty town nearly overnight as the "uitlanders" ("foreigners," meaning non-Boer whites) poured in and settled around the mines. The influx was such that the uitlanders quickly outnumbered the Boers in Johannesburg and along the Rand, although they remained a minority in the Transvaal as a whole. The Boers, nervous and resentful of the uitlanders' growing presence, sought to contain their influence through requiring lengthy residential qualifying periods before voting rights could be obtained, by imposing taxes on the gold industry, and by introducing controls through licensing, tariffs and administrative requirements. Among the issues giving rise to tension between the Transvaal government on the one hand, and the uitlanders and British interests on the other, were:
Certain self-appointed "uitlanders" representatives and British mine owners became increasingly angered and frustrated by their dealings with the Transvaal government. A Reform Committee (Transvaal) was formed to represent the uitlanders.
Jameson Raid.
In 1895, a plan was hatched with the connivance of the Cape Prime Minister Cecil Rhodes and Johannesburg gold magnate Alfred Beit to take Johannesburg, ending the control of the Transvaal government. A column of 600 armed men (mainly made up of his Rhodesian and Bechuanaland policemen) was led by Dr. Leander Starr Jameson (the Administrator in Rhodesia of the British South Africa Company (or "Chartered Company") of which Cecil Rhodes was the chairman) over the border from Bechuanaland towards Johannesburg. The column was equipped with Maxim machine guns, and some artillery pieces.
The plan was to make a three-day dash to Johannesburg before the Boer commandos could mobilise, and once there, trigger an uprising by the primarily British expatriate workers (uitlanders) organised by the Reform Committee. However, the Transvaal authorities had advance warning of the Jameson Raid and tracked it from the moment it crossed the border. Four days later, the weary and dispirited column was surrounded near Krugersdorp within sight of Johannesburg. After a brief skirmish in which the column lost 65 killed and wounded—while the Boers lost but one man—Jameson's men surrendered and were arrested by the Boers.
The botched raid resulted in repercussions throughout southern Africa and in Europe. In Rhodesia, the departure of so many policemen enabled the Matabele and Mashona tribes to rise up against the Chartered Company, and the rebellion, known as the Second Matabele War, was suppressed only at great cost.
A few days after the raid, the German Kaiser sent a telegram ("Kruger telegram") congratulating President Kruger and the government of the South African Republic on their success, and when the text of this telegram was disclosed in the British press, it generated a storm of anti-German feeling. In the baggage of the raiding column, to the great embarrassment of the British, the Boers found telegrams from Cecil Rhodes and the other plotters in Johannesburg. Joseph Chamberlain, the British Colonial Secretary, quickly moved to condemn the raid, despite previously having approved Rhodes' plans to send armed assistance in the case of a Johannesburg uprising. Subsequently, Rhodes was severely censured at the Cape inquiry and the London parliamentary inquiry, and forced to resign as Prime Minister of the Cape and as Chairman of the Chartered Company for having sponsored the failed coup d'état.
The Boer government handed their raid prisoners over to the British for trial. Dr. Jameson was tried in England for leading the raid. However, the British press and London society inflamed by anti-Boer and anti-German feeling and in a frenzy of jingoism, lionised Dr. Jameson and treated him as a hero. Although sentenced to 15 months imprisonment (which he served in Holloway), Jameson was later rewarded by being named Prime Minister of the Cape Colony (1904–08) and ultimately anointed as one of the founders of the Union of South Africa. For conspiring with Jameson, the uitlander members of the Reform Committee (Transvaal) were tried in the Transvaal courts and found guilty of high treason. The four leaders were sentenced to death by hanging, but this sentence was next day commuted to 15 years' imprisonment; and in June 1896, the other members of the Committee were released on payment of £2,000 each in fines, all of which were paid by Cecil Rhodes. One Reform Committee member, Frederick Gray, had committed suicide while in Pretoria gaol, on 16 May, and his death was a factor in softening the Transvaal government's attitude to the remaining prisoners.
Jan C. Smuts wrote in 1906, "The Jameson Raid was the real declaration of war ... And that is so in spite of the four years of truce that followed ... aggressors consolidated their alliance ... the defenders on the other hand silently and grimly prepared for the inevitable."
Escalation and war.
The Jameson Raid alienated many Cape Afrikaners from the British, and united the Transvaal Boers behind President Kruger and his government. It also had the effect of drawing the Transvaal and the Orange Free State (led by President Martinus Theunis Steyn) together in opposition to perceived British imperialism. In 1897, a military pact was concluded between the two republics. President Paul Kruger proceeded to re-equip the Transvaal army, and imported 37,000 of the latest Mauser Model 1895 rifles, and some 40 to 50 million rounds of ammunition. The best modern European artillery was also purchased.
By October 1899 the Transvaal State Artillery had 73 guns, of which 59 were new, including four 155-mm Creusot fortress guns, and 25 37mm Maxim Nordenfeldt guns. The Transvaal army had been transformed; approximately 25,000 men equipped with modern rifles and artillery could mobilise within two weeks. However, President Kruger's victory in the Jameson Raid incident did nothing to resolve the fundamental problem; the impossible dilemma continued, namely how to make concessions to the uitlanders without surrendering the independence of the Transvaal.
The failure to gain improved rights for uitlanders became a pretext for war and a justification for a major military buildup in the Cape Colony. The case for war was developed and espoused as far away as the Australian colonies. Several key British colonial leaders favoured annexation of the independent Boer republics. These figures included Cape Colony Governor Sir Alfred Milner, Cape Prime Minister Cecil Rhodes, British Colonial Secretary Joseph Chamberlain, and mining syndicate owners or Randlords (nicknamed the "gold bugs"), such as Alfred Beit, Barney Barnato, and Lionel Phillips. Confident that the Boers would be quickly defeated, they planned and organised a short war, citing the uitlanders' grievances as the motivation for the conflict.
Their influence with the British government was, however, limited. Lord Salisbury, the Prime Minister, despised jingoism and jingoists. He also distrusted the abilities of the British Army. Yet he led Britain into war for three main reasons: because he believed the British government had an obligation to British South Africans; because he thought that the Transvaal, the Orange Free State, and the Cape Boers aspired to a Dutch South Africa, and that the achievement of such a state would damage Britain's imperial prestige around the world; and because of the Boers' treatment of black South Africans (Salisbury had referred to the London Convention of 1884, after the British defeat, as an agreement 'really in the interest of slavery'). Salisbury was not alone in this concern over the treatment of black South Africans; Roger Casement, already well on the way to becoming an Irish Nationalist, was nevertheless happy to gather intelligence for the British against the Boers because of their treatment of black Africans.
Given this sense of caution among key members of the British cabinet and of the army, it is even harder to understand why the British government went against the advice of its generals (such as Wolseley) to send substantial reinforcements to South Africa before war broke out. One strong argument is that Lansdowne, Secretary of State for War, did not believe the Boers were preparing for war, and also believed that if Britain were to send large numbers of troops, it would strike too aggressive a posture and so prevent a negotiated settlement being reached or even encourage a Boer attack.
President Steyn of the Orange Free State invited Milner and Kruger to attend a conference in Bloemfontein. The conference started on 30 May 1899, but negotiations quickly broke down, despite Kruger's offer of concessions. In September 1899, Chamberlain sent an ultimatum demanding full equality for British citizens resident in Transvaal. Kruger, seeing that war was inevitable, simultaneously issued his own ultimatum prior to receiving Chamberlain's. This gave the British 48 hours to withdraw all their troops from the border of Transvaal; otherwise the Transvaal, allied with the Orange Free State, would declare war.
News of the ultimatum reached London on the day it expired. Outrage and laughter were the main responses. The editor of the "Times" laughed out loud when he read it, saying 'an official document is seldom amusing and useful yet this was both.' "The Times" denounced the ultimatum as an 'extravagant farce.' "The Globe" denounced this 'trumpery little state.' Most editorials were similar to the "Daily Telegraph", which declared: 'of course there can only be one answer to this grotesque challenge. Kruger has asked for war and war he must have!'
Such views were far from those of the British government, and from those in the army. To most sensible observers, army reform had been a matter of pressing concern from the 1870s, constantly put off because the British public did not want the expense of a larger, more professional army, and because a large home army was not politically welcome. Lord Salisbury, the Prime Minister, then had to explain to a surprised Queen Victoria that: 'We have no army capable of meeting even a second-class Continental Power.'
First phase: The Boer offensive (October – December 1899).
War was declared on 11 October 1899 with a Boer offensive into the British-held Natal and Cape Colony areas. The Boers had no problems with mobilisation, since the fiercely independent Boers had no regular army units, apart from the "Staatsartillerie" (Afrikaans for 'States Artillery') of both republics. As with the First Boer War, since the Boers were civilian militia, each man wore what he wished, usually his everyday dark-grey, light-grey, neutral-coloured, or earthtone khaki farming clothes—often a jacket, trousers and slouch hat. Only the members of the "Staatsartillerie" wore light green uniforms.
When danger loomed, all the "burghers" (citizens) in a district would form a military unit called a "commando" and would elect officers. A full-time official titled a "Veldkornet" maintained muster rolls, but had no disciplinary powers. Each man brought his own weapon, usually a hunting rifle, and his own horse. Those who could not afford a gun were given one by the authorities. (See also the arms procurement mentioned above.) The Presidents of the Transvaal and Orange Free State simply signed decrees to concentrate within a week and the Commandos could muster between 30,000–40,000 men.
The average Boer nevertheless was not thirsty for war. Many did not look forward to fighting against fellow Christians and, by and large, fellow Christian Protestants. Many may have had an overly optimistic sense of what the war would involve, imagining that victory could be won as easily as in the First South African War. Many, including many generals, also had a sense that their cause was holy and just, and blessed by God.
It rapidly became clear that the Boer forces presented the British forces with a severe tactical challenge. What the Boers presented was a mobile and innovative approach to warfare, drawing on their experiences from the First Boer War. The average Boers who made up their Commandos were farmers who had spent almost all their working life in the saddle, both as farmers and hunters. They depended on the pot, horse and rifle and were skilled stalkers and marksmen. As hunters they had learned to fire from cover, from a prone position and to make the first shot count, knowing that if they missed, the game would either be long gone or could charge and potentially kill them.
At community gatherings, target shooting was a major sport, and they practised shooting at targets such as hens' eggs perched on posts away. They made expert mounted infantry, using every scrap of cover, from which they could pour in a destructive fire using their modern, smokeless, Mauser rifles. Furthermore, in preparation for hostilities, the Boers had acquired around one hundred of the latest Krupp field guns, all horse-drawn and dispersed among the various Commando groups, and several Le Creusot "Long Tom" siege guns. The Boers' skill in adapting themselves to becoming first-rate artillerymen shows them to have been a versatile adversary. The Transvaal also had an intelligence service that stretched across South Africa, and of whose extent and efficiency the British were unaware.
The Boers struck first on 12 October at Kraaipan, an attack that heralded the invasion of the Cape Colony and Colony of Natal between October 1899 and January 1900. With elements of both speed and surprise the Boer drove quickly towards the major British garrison at Ladysmith and the smaller ones at Mafeking and Kimberley. The quick Boer mobilisation resulted in early military successes against the scattered British forces.
Sir George Stuart White, commanding the British division at Ladysmith, had unwisely allowed Major-General Penn Symons to throw a brigade forward to the coal-mining town of Dundee (also reported as Glencoe), which was surrounded by hills. This became the site of the first engagement of the war, the Battle of Talana Hill. Boer guns began shelling the British camp from the summit of Talana Hill at dawn on 20 October. Penn Symons immediately counter-attacked. His infantry drove the Boers from the hill, but at the cost of 446 British casualties including Penn Symons himself.
Another Boer force occupied Elandslaagte, which lay between Ladysmith and Dundee. The British under Major General John French and Colonel Ian Hamilton attacked to clear the line of communications to Dundee. The resulting Battle of Elandslaagte was a clear-cut British tactical victory, but Sir George White feared that more Boers were about to attack his main position and ordered a chaotic retreat from Elandslaagte, throwing away any advantage gained. The detachment from Dundee was compelled to make an exhausting cross-country retreat to rejoin White's main force.
As Boers surrounded Ladysmith and opened fire on the town with siege guns, White ordered a major sortie against the Boer artillery positions. The result was a disaster, with 140 men killed and over 1,000 captured. The Siege of Ladysmith began, and was to last several months.
Meanwhile, to the north-west at Mafeking, on the border with Transvaal, Colonel Robert Baden-Powell had raised two regiments of local forces amounting to about 1,200 men in order to attack and create diversions if things further south went amiss. Mafeking, being a railway junction, provided good supply facilities and was the obvious place for Baden-Powell to fortify in readiness for such attacks. However, instead of being the aggressor Baden-Powell and Mafeking were forced to defend when 6,000 Boer, commanded by Piet Cronje, attempted a determined assault on the town. But this quickly subsided into a desultory affair with the Boers prepared to starve the stronghold into submission, and so, on 13 October, began the 217-day Siege of Mafeking.
Lastly, over to the south of Mafeking lay the diamond mining city of Kimberley, which was also subjected to a siege. Although not militarily significant, it nonetheless represented an enclave of British imperialism on the borders of the Orange Free State and was hence an important Boer objective. From early November about 7,500 Boer began their siege, again content to starve the town into submission. Despite Boer shelling, the 40,000 inhabitants, of which only 5,000 were armed, were under little threat as the town was well-stocked with provisions. The garrison was commanded by Lieutenant Colonel Robert Kekewich, although Cecil Rhodes was also a prominent figure in the defence.
Siege life took its toll on both the defending soldiers and the civilians in the cities of Mafeking, Ladysmith, and Kimberley as food began to grow scarce after a few weeks. In Mafeking, Sol Plaatje wrote, "I saw horseflesh for the first time being treated as a human foodstuff." The cities under siege also dealt with constant artillery bombardment, making the streets a dangerous place. Near the end of the siege of Kimberley, it was expected that the Boers would intensify their bombardment, so Rhodes displayed a notice encouraging people to go down into shafts of the Kimberley Mine for protection. The townspeople panicked, and people surged into the mine-shafts constantly for a 12-hour period. Although the bombardment never came, this did nothing to diminish the distress of the civilians. The most well-heeled of the townspeople, such as Cecil Rhodes, sheltered in the Sanatorium, site of the present-day McGregor Museum; the poorer residents, notably the black population, did not have any shelter from the shelling.
In retrospect, the Boer decision to commit themselves to sieges (Sitzkrieg) was a mistake, and one of the best illustrations of the Boers' lack of strategic vision. Historically, it had little in its favour. Of the seven sieges in the First Boer War, the Boers had won none. More importantly, it handed the initiative back to the British and allowed them time to recover, which they then did. Generally speaking, throughout the campaign, the Boers were too defensive and passive, wasting the opportunities they had for victory. Yet that passiveness also testified to the fact that they had no desire to conquer British territory, but only to preserve their ability to rule in their own territory.
First British relief attempts.
It was at this point that General Sir Redvers Henry Buller, a much respected commander, arrived in South Africa with major British reinforcements (including an army corps of three divisions). Buller originally intended an offensive straight up the railway line leading from Cape Town through Bloemfontein to Pretoria. Finding on arrival that the British troops already in South Africa were under siege, he split his army corps into several widely spread detachments, to relieve the besieged garrisons. One division, led by Lieutenant General Lord Methuen, was to follow the Western Railway to the north and relieve Kimberley and Mafeking. A smaller force of about 3,000 led by Major General William Gatacre, was to push north toward the railway junction at Stormberg, to secure the Cape Midlands district from Boer raids and local rebellions by Boer inhabitants. Finally, Buller himself would lead the major part of the army corps to relieve Ladysmith to the east.
The initial results of this offensive were mixed, with Methuen winning several bloody skirmishes at Belmont on 23 November, at Graspan on 25 November, and at a larger conflict, Modder River on 28 November resulting in British losses of 71 dead and over 400 wounded. British commanders had trained on the lessons of the Crimean War, and were adept at battalion and regimental set pieces with columns manoeuvring in jungles, deserts and mountainous regions. What they entirely failed to comprehend, however, was both the impact of destructive fire from trench positions and the mobility of cavalry raids, both of which had been developed in the American Civil War. The British troops went to war with what would prove to be antiquated tactics, and in some cases antiquated weapons, against the mobile Boer forces with the destructive fire of their modern Mausers, the latest Krupp field guns, and their innovative tactics.
The middle of December was disastrous for the British Army. In a period known as Black Week (10 – 15 December 1899), the British suffered a series of losses on each of the three major fronts.
On 10 December, General Gatacre tried to recapture Stormberg railway junction about south of the Orange River. Gatacre's attack was marked by administrative and tactical blunders, and the Battle of Stormberg ended in a British defeat, with 135 killed and wounded, and two guns and over 600 troops captured.
At the Battle of Magersfontein on 11 December, Methuen's 14,000 British troops attempted to capture a Boer position in a dawn attack to relieve Kimberley. This too turned into a disaster when the Highland Brigade became pinned down by accurate Boer fire. After suffering from intense heat and thirst for nine hours, they eventually broke in ill-disciplined retreat. The Boer commanders, Koos de la Rey and Piet Cronjé, had ordered trenches to be dug in an unconventional place to fool the British and to give their riflemen a greater firing range. The plan worked and this tactic helped write the doctrine of the supremacy of the defensive position, using modern small arms and trench fortifications. The British lost 120 killed and 690 wounded and were prevented from relieving Kimberley and Mafeking. A British soldier encapsulated the soldiers' view of the defeat:
""Such was the day for our regiment"<br>"Dread the revenge we will take. "<br>"Dearly we paid for the blunder -"<br>"A drawing-room General's mistake. "<br>"Why weren't we told of the trenches?"<br>"Why weren't we told of the wire? "<br>"Why were we marched up in column, "<br>"May Tommy Atkins enquire ..." "
However, the nadir of Black Week was the Battle of Colenso on 15 December where 21,000 British troops commanded by Buller himself, attempted to cross the Tugela River to relieve Ladysmith where 8,000 Transvaal Boers, under the command of Louis Botha, were awaiting them. Through a combination of artillery and accurate rifle fire, and a better use of the ground, the Boers repelled all British attempts to cross the river. After his first attacks failed, Buller broke off the battle and ordered a retreat, abandoning many wounded men, several isolated units and ten field guns to be captured by Botha's men. Buller's forces lost 145 men killed and 1,200 missing or wounded. The Boers suffered 40 casualties, including only 8 killed.
Second phase: The British offensive of January to September 1900.
The British government took these defeats badly and with the sieges still continuing was compelled to send two more divisions plus large numbers of colonial volunteers. By January 1900 this would become the largest force Britain had ever sent overseas, amounting to some 180,000 men with further reinforcements being sought.
While watching for these reinforcements, Buller made another bid to relieve Ladysmith by crossing the Tugela west of Colenso. Buller's subordinate, Major General Charles Warren, successfully crossed the river, but was then faced with a fresh defensive position centred on a prominent hill known as Spion Kop. In the resulting Battle of Spion Kop, British troops captured the summit by surprise during the early hours of 24 January 1900, but as the early morning fog lifted they realised too late that they were overlooked by Boer gun emplacements on the surrounding hills. The rest of the day resulted in a disaster caused by poor communication between Buller and his commanders. Between them they issued contradictory orders, on the one hand ordering men off the hill, while other officers ordered fresh reinforcements to defend it. The result was 350 men killed and nearly 1,000 wounded and a retreat across the Tugela River into British territory. There were nearly 300 Boer casualties.
Buller attacked Louis Botha again on 5 February at Vaal Krantz and was again defeated. Buller withdrew early when it appeared that the British would be isolated in an exposed bridgehead across the Tugela, and was nicknamed "Sir Reverse" by some of his officers.
By taking command in person in Natal, Buller had allowed the overall direction of the war to drift. Because of concerns about his performance and negative reports from the field, he was replaced as Commander in Chief by Field Marshal Lord Roberts. Roberts quickly assembled an entirely new team for headquarters staff and he chose military men from far and wide: Lord Kitchener (Chief of Staff) from the Sudan; Frederick Russell Burnham (Chief of Scouts), the American scout, from the Klondike; David Henderson from the Staff College; Neville Bowles Chamberlain from Afghanistan; and William Nicholson (Military Secretary) from Calcutta Like Buller, Roberts first intended to attack directly along the Cape Town – Pretoria railway but, again like Buller, was forced to relieve the beleaguered garrisons. Leaving Buller in command in Natal, Roberts massed his main force near the Orange River and along the Western Railway behind Methuen's force at the Modder River, and prepared to make a wide outflanking move to relieve Kimberley.
Except in Natal, the war had stagnated. Other than a single attempt to storm Ladysmith, the Boers made no attempt to capture the besieged towns. In the Cape Midlands, the Boers did not exploit the British defeat at Stormberg, and were prevented from capturing the railway junction at Colesberg. In the dry summer, the grazing on the veld became parched, weakening the Boers' horses and draught oxen, and many Boer families joined their menfolk in the siege lines and "laagers" (encampments), fatally encumbering Cronje's army.
Roberts launched his main attack on 10 February 1900 and although hampered by a long supply route, managed to outflank the Boers defending Magersfontein. On 14 February, a cavalry division under Major General John French launched a major attack to relieve Kimberley. Although encountering severe fire, a massed cavalry charge split the Boer defences on 15 February, opening the way for French to enter Kimberley that evening, ending its 124 days' siege.
Meanwhile, Roberts pursued Piet Cronje's 7,000-strong force, which had abandoned Magersfontein to head for Bloemfontein. General French's cavalry was ordered to assist in the pursuit by embarking on an epic drive towards Paardeberg where Cronje was attempting to cross the Modder River. At the Battle of Paardeberg from 18 to 27 February, Roberts then surrounded General Piet Cronje's retreating Boer army. On 17 February, a pincer movement involving both French's cavalry and the main British force attempted to take the entrenched position, but the frontal attacks were uncoordinated and so were easily repulsed by the Boers. Finally, Roberts resorted to bombarding Cronje into submission, but it took a further ten precious days and with the British troops using the polluted Modder River as water supply, resulting in a typhoid epidemic killing many troops. General Cronje was forced to surrender at Surrender Hill with 4000 men.
In Natal, the Battle of the Tugela Heights, which started on 14 February was Buller's fourth attempt to relieve Ladysmith. The losses Buller's troops had sustained convinced Buller to adopt Boer tactics "in the firing line - to advance in small rushes, covered by rifle fire from behind; to use the tactical support of artillery; and above all, to use the ground, making rock and earth work for them as it did for the enemy." Despite reinforcements his progress was painfully slow against stiff opposition. However, on 26 February, after much deliberation, Buller used all his forces in one all-out attack for the first time and at last succeeded in forcing a crossing of the Tugela, and defeated Botha's outnumbered forces north of Colenso. After a siege lasting 118 days, the Relief of Ladysmith was effected, the day after Cronje surrendered, but at a total cost of 7,000 British casualties. Buller's troops marched into Ladysmith on 28 February.
After a succession of defeats, the Boers realised that against such overwhelming numbers of troops, they had little chance of defeating the British and so became demoralised. Roberts then advanced into the Orange Free State from the west, putting the Boers to flight at the Battle of Poplar Grove and capturing Bloemfontein, the capital, unopposed on 13 March with the Boer defenders escaping and scattering. Meanwhile, he detached a small force to relieve Baden-Powell, and the Relief of Mafeking on 18 May 1900 provoked riotous celebrations in Britain.
On 28 May, the Orange Free State was annexed and renamed the Orange River Colony.
After being forced to delay for several weeks at Bloemfontein by a shortage of supplies and an outbreak of enteric (typhoid) fever caused by poor hygiene, drinking bad water at Paardeburg and appalling medical care, Roberts resumed his advance. He was forced to halt again at Kroonstad for 10 days, due once again to the collapse of his medical and supply systems, but finally captured Johannesburg on 31 May and the capital of the Transvaal, Pretoria, on 5 June. The first into Pretoria was Lt. William Watson of the New South Wales Mounted Rifles, who persuaded the Boers to surrender the capital. (Before the war, the Boers had constructed several forts south of Pretoria, but the artillery had been removed from the forts for use in the field, and in the event the Boers abandoned Pretoria without a fight).
This allowed Roberts to declare the war over, having won the principal cities and so, on 3 September 1900, the South African Republic was formally annexed.
British observers believed the war to be all but over after the capture of the two capital cities. However, the Boers had earlier met at the temporary new capital of the Orange Free State, Kroonstad, and planned a guerrilla campaign to hit the British supply and communication lines. The first engagement of this new form of warfare was at Sanna's Post on 31 March where 1,500 Boers under the command of Christiaan De Wet attacked Bloemfontein's waterworks about east of the city, and ambushed a heavily escorted convoy, which caused 155 British casualties and the capture of seven guns, 117 wagons, and 428 British troops.
After the fall of Pretoria, one of the last formal battles was at Diamond Hill on 11 – 12 June, where Roberts attempted to drive the remnants of the Boer field army beyond striking distance of Pretoria. Although Roberts drove the Boers from the hill, the Boer commander, Louis Botha, did not regard it as a defeat, for he inflicted more casualties on the British (totalling 162 men) while suffering around 50 casualties.
The set-piece period of the war now largely gave way to a mobile guerrilla war, but one final operation remained. President Kruger and what remained of the Transvaal government had retreated to eastern Transvaal. Roberts, joined by troops from Natal under Buller, advanced against them, and broke their last defensive position at Bergendal on 26 August. As Roberts and Buller followed up along the railway line to Komatipoort, Kruger sought asylum in Portuguese East Africa (modern Mozambique). Some dispirited Boers did likewise, and the British gathered up much war material. However, the core of the Boer fighters under Botha easily broke back through the Drakensberg Mountains into the Transvaal highveld after riding north through the bushveld. Under the new conditions of the war, heavy equipment was no use to them, and therefore no great loss.
As Roberts's army occupied Pretoria, the Boer fighters in the Orange Free State had been driven into a fertile area known as the Brandwater Basin in the north east of the Republic. This offered only temporary sanctuary, as the mountain passes leading to it could be occupied by the British, trapping the Boers. A force under General Archibald Hunter set out from Bloemfontein to achieve this in July 1900. The hard core of the Free State Boers under Christiaan De Wet, accompanied by President Steyn, left the basin early. Those remaining fell into confusion and most failed to break out before Hunter trapped them. 4,500 Boers surrendered and much equipment was captured but as with Roberts's drive against Kruger at the same time, these losses were of relatively little consequence, as the hardcore of the Boer armies and their most determined and active leaders remained at large.
From the Basin, Christiaan De Wet headed west. Although hounded by British columns, he succeeded in crossing the Vaal into western Transvaal, to allow Steyn to travel to meet the Transvaal leaders.
There was much sympathy for the Boers on mainland Europe and in October, President Kruger and members of the Transvaal government left Portuguese East Africa on the Dutch warship "De Gelderland", sent by the Queen of the Netherlands Wilhelmina, who had simply ignored the British naval blockade of South Africa. Paul Kruger's wife, however, was too ill to travel and remained in South Africa where she died on 20 July 1901 without seeing her husband again. President Kruger first went to Marseille and then on to The Netherlands where he stayed for a while before moving finally to Clarens, Switzerland, where he died in exile on 14 July 1904.
POWs sent overseas.
The first sizeable batch of Boer prisoners of war taken by the British consisted of those captured at the Battle of Elandslaagte on 21 October 1899. At first, many were put on ships, but as numbers grew, the British decided they did not want them kept locally. The capture of 400 POWs in February 1900 was a key event, which made the British realise they could not accommodate all POWs in South Africa. The British feared they could be freed by sympathetic locals. Moreover, they already had trouble supplying their own troops in South Africa, and did not want the added burden of sending supplies for the POWs. Britain therefore chose to send many POWs overseas.
The first overseas (off African mainland) camps were opened in Saint Helena, which ultimately received about 5,000 POWs. About 5,000 POWs were sent to Ceylon. Other POWs were sent to Bermuda and India. No evidence exists of Boer POWs being sent to the Dominions of the British Empire such as Australia, Canada or New Zealand.
In all, about 26,000 POWs were sent overseas.
Third phase: Guerrilla war (September 1900 – May 1902).
By September 1900, the British were nominally in control of both Republics, with the exception of the northern part of Transvaal. However, they soon discovered that they only controlled the territory their columns physically occupied. Despite the loss of their two capital cities and half of their army, the Boer commanders adopted guerrilla warfare tactics, primarily conducting raids against infrastructure, resource and supply targets, all aimed at disrupting the operational capacity of the British Army.
Each Boer commando unit was sent to the district from which its members had been recruited, which meant that they could rely on local support and personal knowledge of the terrain and the towns within the district thereby enabling them to live off the land. Their orders were simply to act against the British whenever possible. Their tactics were to strike fast and hard causing as much damage to the enemy as possible, and then to withdraw and vanish before enemy reinforcements could arrive. The vast distances of the Republics allowed the Boer commandos considerable freedom to move about and made it nearly impossible for the 250,000 British troops to control the territory effectively using columns alone. As soon as a British column left a town or district, British control of that area faded away.
The Boer commandos were especially effective during the initial guerrilla phase of the war because Roberts had assumed that the war would end with the capture of the Boer capitals and the dispersal of the main Boer armies. Many British troops were therefore redeployed out of the area, and had been replaced by lower-quality contingents of Imperial Yeomanry and locally raised irregular corps.
From late May 1900, the first successes of the Boer guerrilla strategy were at Lindley (where 500 Yeomanry surrendered), and at Heilbron (where a large convoy and its escort were captured) and other skirmishes resulting in 1,500 British casualties in less than ten days. In December 1900, De la Rey and Christiaan Beyers attacked and mauled a British brigade at Nooitgedacht. As a result of these and other Boer successes, the British, led by Lord Kitchener, mounted three extensive searches for De Wet, but without success. However, the very nature of the Boer guerrilla war was sporadic, poorly planned, and had little overall long-term objective, with the exception to simply harass the British. This led to a disorganised pattern of scattered engagements throughout the region.
British response.
The British were forced to quickly revise their tactics. They concentrated on restricting the freedom of movement of the Boer commandos and depriving them of local support. The railway lines had provided vital lines of communication and supply, and as the British had advanced across South Africa, they had used armoured trains and had established fortified blockhouses at key points. They now built additional blockhouses (each housing 6–8 soldiers) and fortified these to protect supply routes against Boer raiders. Eventually some 8,000 such blockhouses were built across the two South African republics, radiating from the larger towns along principal routes. Each blockhouse cost between £800 to £1,000 and took about three months to build. However, they proved very effective. Not one bridge where one of these blockhouses was sited and manned was blown.
The blockhouse system required an enormous number of troops to garrison. Well over 50,000 British troops, or 50 battalions, were involved in blockhouse duty, greater than the approximately 30,000 Boers in the field during the guerrilla phase. In addition, up to 16,000 Africans were used both as armed guards and to patrol the line at night. The Army linked the blockhouses with barbed wire fences to parcel up the wide veld into smaller areas. "New Model" drives were mounted under which a continuous line of troops could sweep an area of veld bounded by blockhouse lines, unlike the earlier inefficient scouring of the countryside by scattered columns.
The British also implemented a "scorched earth" policy under which they targeted everything within the controlled areas that could give sustenance to the Boer guerrillas with a view to making it harder for the Boers to survive. As British troops swept the countryside, they systematically destroyed crops, burned homesteads and farms, poisoned wells, and interned Boer and African women, children and workers in concentration camps. Finally, the British also established their own mounted raiding columns in support of the sweeper columns. These were used to rapidly follow and relentlessly harass the Boers with a view to delaying them and cutting off escape, while the sweeper units caught up. Many of the 90 or so mobile columns formed by the British to participate in such drives were a mixture of British and colonial troops, but they also had a large minority of armed Africans. The total number of armed Africans serving with these columns has been estimated at approximately 20,000.
The British Army also made use of Boer auxiliaries who had been persuaded to change sides and enlist as "National Scouts". Serving under the command of General Andries Cronje, the National Scouts were despised as "hensoppers" (collaborators) but came to number a fifth of the fighting Afrikaners by the end of the War.
The British utilised armoured trains throughout the War to deliver rapid reaction forces much more quickly to incidents (such as Boer attacks on blockhouses and columns) or to drop them off ahead of retreating Boer columns.
Orange Free State.
After having conferred with the Transvaal leaders, De Wet returned to the Orange Free State, where he inspired a series of successful attacks and raids from the hitherto quiet western part of the country, though he suffered a rare defeat at Bothaville in November 1900. Many Boers who had earlier returned to their farms, sometimes giving formal parole to the British, took up arms again. In late January 1901, De Wet led a renewed invasion of Cape Colony. This was less successful, because there was no general uprising among the Cape Boers, and De Wet's men were hampered by bad weather and relentlessly pursued by British forces. They narrowly escaped across the Orange River.
From then until the final days of the war, De Wet remained comparatively quiet, partly because the Orange Free State was effectively left desolate by British sweeps. In late 1901, De Wet overran an isolated British detachment at Groenkop, inflicting heavy casualties. This prompted Kitchener to launch the first of the "New Model" drives against him. De Wet escaped the first such drive, but lost 300 of his fighters. This was a severe loss, and a portent of further attrition, although the subsequent attempts to round up De Wet were badly handled, and De Wet's forces avoided capture.
Western Transvaal.
The Boer commandos in the Western Transvaal were very active after September 1901. Several battles of importance were fought here between September 1901 and March 1902. At Moedwil on 30 September 1901 and again at Driefontein on 24 October, General Koos De La Rey's forces attacked the British, but were forced to withdraw after the British offered strong resistance.
A time of relative quiet descended thereafter on the western Transvaal. February 1902 saw the next major battle in that region. On 25 February, Koos De La Rey attacked a British column under Lieutenant-Colonel S. B. von Donop at Ysterspruit near Wolmaransstad. De La Rey succeeded in capturing many men and a large amount of ammunition. The Boer attacks prompted Lord Methuen, the British second-in-command after Lord Kitchener, to move his column from Vryburg to Klerksdorp to deal with De La Rey. On the morning of 7 March 1902, the Boers attacked the rear guard of Methuen's moving column at Tweebosch. Confusion reigned in British ranks and Methuen was wounded and captured by the Boers.
The Boer victories in the west led to stronger action by the British. In the second half of March 1902, large British reinforcements were sent to the Western Transvaal under the direction of Ian Hamilton. The opportunity the British were waiting for arose on 11 April 1902 at Rooiwal, where a commando led by General Jan Kemp and Commandant Potgieter attacked a superior force under Kekewich. The British soldiers were well positioned on the hillside and inflicted severe casualties on the Boers charging on horseback over a large distance, beating them back. This was the end of the war in the Western Transvaal and also the last major battle of the war.
Eastern Transvaal.
Two Boer forces fought in this area, one under Botha in the south east and a second under Ben Viljoen in the north east around Lydenburg. Botha's forces were particularly active, raiding railways and British supply convoys, and even mounting a renewed invasion of Natal in September 1901. After defeating British mounted infantry in the Battle of Blood River Poort near Dundee, Botha was forced to withdraw by heavy rains that made movement difficult and crippled his horses. Back on the Transvaal territory around his home district of Vryheid, Botha attacked a British raiding column at Bakenlaagte, using an effective mounted charge. One of the most active British units was effectively destroyed in this engagement. This made Botha's forces the target of increasingly large and ruthless drives by British forces, in which the British made particular use of native scouts and informers. Eventually, Botha had to abandon the high veld and retreat to a narrow enclave bordering Swaziland.
To the north, Ben Viljoen grew steadily less active. His forces mounted comparatively few attacks and as a result, the Boer enclave around Lydenburg was largely unmolested. Viljoen was eventually captured.
Cape Colony.
In parts of Cape Colony, particularly the Cape Midlands district where Boers formed a majority of the white inhabitants, the British had always feared a general uprising against them. In fact, no such uprising took place, even in the early days of the war when Boer armies had advanced across the Orange. The cautious conduct of some of the elderly Orange Free State generals had been one factor that discouraged the Cape Boers from siding with the Boer republics. Nevertheless, there was widespread pro-Boer sympathy.
After he escaped across the Orange in March 1901, De Wet had left forces under Cape rebels Kritzinger and Scheepers to maintain a guerrilla campaign in the Cape Midlands. The campaign here was one of the least chivalrous of the war, with intimidation by both sides of each other's civilian sympathizers. In one of many skirmishes, Commandant Lotter's small commando was tracked down by a much-superior British column and wiped out at Groenkloof. Several captured rebels, including Lotter and Scheepers, who was captured when he fell ill with appendicitis, were executed by the British for treason or for capital crimes such as the murder of prisoners or of unarmed civilians. Some of the executions took place in public, to deter further disaffection. Since the Cape Colony was Imperial territory, its authorities forbade the British Army to burn farms or to force Boers into concentration camps.
Fresh Boer forces under Jan Christiaan Smuts, joined by the surviving rebels under Kritzinger, made another attack on the Cape in September 1901. They suffered severe hardships and were hard pressed by British columns, but eventually rescued themselves by routing some of their pursuers at the Battle of Elands River and capturing their equipment. From then until the end of the war, Smuts increased his forces from among Cape rebels until they numbered 3,000. However, no general uprising took place, and the situation in the Cape remained stalemated.
In January 1902, Boer leader Manie Maritz was implicated in the Leliefontein massacre in the far Northern Cape.
Surgery and medicine during the war.
More than half of British casualties during the war were caused by illness, especially typhoid fever, rather than enemy action.
Concentration camps (1900–1902).
The term "concentration camp" was used to describe camps operated by the British in South Africa during this conflict, and the term grew in prominence during this period.
The camps had originally been set up by the British Army as "refugee camps" to provide refuge for civilian families who had been forced to abandon their homes for whatever reason related to the war. However, when Kitchener succeeded Roberts as commander-in-chief in South Africa on 29 November 1900, the British Army introduced new tactics in an attempt to break the guerrilla campaign and the influx of civilians grew dramatically as a result. Kitchener initiated plans to
flush out guerrillas in a series of systematic drives, organised like a sporting shoot, with success defined in a weekly 'bag' of killed, captured and wounded, and to sweep the country bare of everything that could give sustenance to the guerrillas, including women and children ... It was the clearance of civilians—uprooting a whole nation—that would come to dominate the last phase of the war.
As Boer farms were destroyed by the British under their "Scorched Earth" policy—including the systematic destruction of crops and slaughtering of livestock, the burning down of homesteads and farms, and the poisoning of wells and salting of fields—to prevent the Boers from resupplying from a home base many tens of thousands of women and children were forcibly moved into the concentration camps. This was not the first appearance of internment camps. The Spanish had used internment in the Ten Years' War that led to the Spanish–American War, and the United States had used them to devastate guerrilla forces during the Philippine–American War. But the Boer War concentration camp system was the first time that a whole nation had been systematically targeted, and the first in which some whole regions had been depopulated.
Eventually, there were a total of 45 tented camps built for Boer internees and 64 for black Africans. Of the 28,000 Boer men captured as prisoners of war, 25,630 were sent overseas. The vast majority of Boers remaining in the local camps were women and children. Over 26,000 women and children were to perish in these concentration camps.
The camps were poorly administered from the outset and became increasingly overcrowded when Kitchener's troops implemented the internment strategy on a vast scale. Conditions were terrible for the health of the internees, mainly due to neglect, poor hygiene and bad sanitation. The supply of all items was unreliable, partly because of the constant disruption of communication lines by the Boers. The food rations were meager and there was a two-tier allocation policy, whereby families of men who were still fighting were routinely given smaller rations than others (Pakenham 1979, p. 505). The inadequate shelter, poor diet, bad hygiene and overcrowding led to malnutrition and endemic contagious diseases such as measles, typhoid and dysentery to which the children were particularly vulnerable. An additional problem was the Boers' use of traditional medicines like a cow-dung poultice for skin diseases and crushed insects for convulsions. Coupled with a shortage of modern medical facilities, many of the internees died.
As the war raged across their farms and their homes were destroyed, many Africans became refugees and they, like the Boers, moved to the towns where the British Army hastily created internment camps. Subsequently, the "Scorched Earth" policy was ruthlessly applied to both Boers and Africans. Although most black Africans were not considered by the British to be hostile, many tens of thousands were also forcibly removed from Boer areas and also placed in concentration camps.
Africans were held separately from Boer internees. Eventually there were a total of 64 tented camps for Africans. Conditions were as bad as in the camps for the Boers, but even though, after the Fawcett Commission report, conditions improved in the Boer camps, "improvements were much slower in coming to the black camps."
Public opinion and political opposition.
Although the 1900 UK general election, also known as the "Khaki election," had resulted in a victory for the Conservative government on the back of recent British victories against the Boers, public support quickly waned as it became apparent that the war would not be easy and further unease developed following reports about the treatment by the British army of the Boer civilians. Public and political opposition to government policies in South Africa regarding Boer civilians was first expressed in Parliament in February 1901 in the form of an attack on the policy, the government, and the army by the radical Liberal MP David Lloyd George.
Emily Hobhouse, a delegate of the South African Women and Children's Distress Fund, visited some of the camps in the Orange Free State from January 1901, and in May 1901 she returned to England on board the ship, the "Saxon". Alfred Milner, High Commissioner in South Africa, also boarded the "Saxon" for holiday in England but, unfortunately for both the camp internees and the British government, he had no time for Miss Hobhouse, regarding her as a Boer sympathiser and "trouble maker." On her return, Emily Hobhouse did much to publicise the distress of the camp inmates. She managed to speak to the Liberal Party leader, Henry Campbell-Bannerman who professed to be suitably outraged but was disinclined to press the matter, as his party was split between the imperialists and the pro-Boer factions.
The more radical Liberals however such as David Lloyd George and John Ellis were prepared to raise the matter in Parliament and to harass the government on the issue, which they duly did. St John Brodrick, the Conservative secretary of state for war, first defended the government's policy by arguing that the camps were purely "voluntary" and that the interned Boers were "contented and comfortable," but was somewhat undermined as he had no firm statistics to back up his argument, so when his "voluntary" argument proved untenable, he resorted to the "military necessity" argument and stated that everything possible was being done to ensure satisfactory conditions in the camps.
Hobhouse published a report in June 1901 that contradicted Brodrick's claim, and Lloyd George then openly accused the government of "a policy of extermination" directed against the Boer population. The same month Liberal opposition party leader Campbell-Bannerman took up the assault and answered the rhetorical question "When is a war not a war?" with his own rhetorical answer "When it is carried on by methods of barbarism in South Africa," referring to those same camps and the policies that created them. The Hobhouse report caused uproar both domestically and in the international community. However, there was very little public sympathy for the highly reactionary Boer president Kruger.
The Fawcett Commission.
Although the government had comfortably won the parliamentary debate by a margin of 252 to 149, it was stung by the criticism and concerned by the escalating public outcry, and called on Kitchener for a detailed report. In response, complete statistical returns from camps were sent in July 1901. By August 1901, it was clear to government and opposition alike that Miss Hobhouse's worst fears were being confirmed – 93,940 Boers and 24,457 black Africans were reported to be in "camps of refuge" and the crisis was becoming a catastrophe as the death rates appeared very high, especially among the children.
The government responded to the growing clamour by appointing a commission. The Fawcett Commission, as it became known was, uniquely for its time, an all-woman affair headed by Millicent Fawcett who despite being the leader of the women's suffrage movement was a Liberal Unionist and thus a government supporter and considered a safe pair of hands. Between August and December 1901, the Fawcett Commission conducted its own tour of the camps in South Africa. While it is probable that the British government expected the Commission to produce a report that could be used to fend off criticism, in the end it confirmed everything that Emily Hobhouse had said. Indeed, if anything the Commission's recommendations went even further. The Commission insisted that rations should be increased and that additional nurses be sent out immediately, and included a long list of other practical measures designed to improve conditions in the camp. Millicent Fawcett was quite blunt in expressing her opinion that much of the catastrophe was owed to a simple failure to observe elementary rules of hygiene.
In November 1901, the Colonial Secretary Joseph Chamberlain ordered Alfred Milner to ensure that "all possible steps are being taken to reduce the rate of mortality." The civil authority took over the running of the camps from Kitchener and the British command and by February 1902 the annual death-rate in the concentration camps for white inmates dropped to 6.9 percent and eventually to 2 percent, which was a lower rate than pertained in many British cities at the time. However, by then the damage had been done. A report after the war concluded that 27,927 Boers (of whom 24,074 percent of the Boer child population were children under 16) had died of starvation, disease and exposure in the concentration camps. In all, about one in four (25 percent) of the Boer inmates, mostly children, died.
"Improvements were much slower in coming to the black camps." It is thought that about 12 percent of black African inmates died (about 14,154) but the precise number of deaths of black Africans in concentration camps is unknown as little attempt was made to keep any records of the 107,000 black Africans who were interned.
The main decisions (or their absence) had been left to the soldiers, to whom the life or death of the 154,000 Boer and African civilians in the camps rated as an abysmally low priority. was only ... ten months after the subject had first been raised in Parliament ... after public outcry and after the Fawcett Commission that remedial action was taken and ... the terrible mortality figures were at last declining. In the interval, at least twenty thousand whites and twelve thousand coloured people had died in the concentration camps, the majority from epidemics of measles and typhoid that could have been avoided.
Somewhat higher figures for total deaths in the concentration camps are given by S.B. Spies.
Sir Arthur Conan Doyle had served as a volunteer doctor in the Langman Field Hospital at Bloemfontein between March and June 1900. In his widely distributed and translated pamphlet 'The War in South Africa: Its Cause and Conduct' he justified both the causes of the war and its conduct. He also pointed out that over 14,000 British soldiers had died of disease during the conflict (as opposed to 8000 killed in combat) and at the height of epidemics he was seeing 50–60 British soldiers dying each day in a single ill-equipped and overwhelmed military hospital.
Kitchener's policy and the post-war debate.
It has been argued that "this was not a deliberately genocidal policy; rather it was the result of disastrous lack of foresight and rank incompetence on [the part of the military." British historian Niall Ferguson also argues that "Kitchener no more desired the deaths of women and children in the camps than of the wounded Dervishes after Omdurman, or of his own soldiers in the typhoid stricken hospitals of Bloemfontein."
However, to Kitchener and the British Command "the life or death of the 154,000 Boer and African civilians in the camps rated as an abysmally low priority" against military objectives. As the Fawcett Commission was delivering its recommendations, Kitchener wrote to St John Brodrick defending his policy of sweeps, and emphasising that no new Boer families were being brought in unless they were in danger of starving. This was disingenuous as the countryside had by then been devastated under the "Scorched Earth" policy (the Fawcett Commission in December 1901 in its recommendations commented that: "to turn 100,000 people now being held in the concentration camps out on the veldt to take care of themselves would be cruelty") and now that the New Model counter insurgency tactics were in full swing, it made cynical military sense to leave the Boer families in desperate conditions in the countryside.
According to writer S.B. Spies, "at Vereeniging negotiations in May 1902 Boer leader Louis Botha stated that he had tried to send families to the British, but they had refused to receive them." Spies quotes a Boer commandant referring to Boer women and children made refugees by Britain's scorched-earth policy as saying, "Our families are in a pitiable condition and the enemy uses those families to force us to surrender." Spies adds, "and there is little doubt that that was indeed the intention of Kitchener when he had issued instructions that no more families were to be brought into the concentration camps." Thomas Pakenham writes of Kitchener's policy U-turn,
No doubt the continued 'hullabaloo' at the death-rate in these concentration camps, and Milner's belated agreement to take over their administration, helped change Kitchener's mind time at the end of 1901. ... By mid-December at any rate, Kitchener was already circulating all column commanders with instructions not to bring in women and children when they cleared the country, but to leave them with the guerrillas. ... Viewed as a gesture to Liberals, on the eve of the new session of Parliament at Westminster, it was a shrewd political move. It also made excellent military sense, as it greatly handicapped the guerrillas, now that the drives were in full swing. ... It was effective precisely because, contrary to the Liberals' convictions, it was less humane than bringing them into camps, though this was of no great concern to Kitchener.
The end of the war.
Towards the end of the war, British tactics of containment, denial, and harassment began to yield results against the guerrillas. The sourcing and coordination of intelligence became increasingly efficient with regular reporting from observers in the blockhouses, from units patrolling the fences and conducting "sweeper" operations, and from native Africans in rural areas who increasingly supplied intelligence, as the Scorched Earth policy took effect and they found themselves competing with the Boers for food supplies. Kitchener's forces at last began to seriously affect the Boers' fighting strength and freedom of manoeuvre, and made it harder for the Boers and their families to survive.
The Boers and the British both feared the consequences of arming Africans. The memories of the Zulu and other tribal conflicts were still fresh, and they recognised that whoever won would have to deal with the consequences of a mass militarisation of the tribes. There was therefore an unwritten agreement that this war would be a "white man's war." At the outset, British officials instructed all white magistrates in the Natal Colony to appeal to Zulu ama-khosi to remain neutral, and President Kruger sent emissaries asking them to stay out of it. However, in some cases there were old scores to be settled, and some Africans, such as the Swazis, were eager to enter the war with the specific aim of reclaiming land confiscated by the Boers. As the war went on there was greater involvement of Africans, and in particular large numbers became embroiled in the conflict on the British side, either voluntarily or involuntarily. By the end of the war, many blacks had been armed and had shown conspicuous gallantry in roles such as scouts, messengers, watchmen in blockhouses, and auxiliaries.
And there were more flash-points outside of the war. On 6 May 1902 at Holkrantz in the southeastern Transvaal, a Zulu faction had their cattle stolen and their people mistreated by the Boers as a punishment for helping the British. The local Boer officer then sent an insulting message to the tribe, challenging them to take back their cattle. The Zulus attacked at night, and in a mutual bloodbath, the Boers lost 56 killed and 3 wounded, while the Africans suffered 52 killed and 48 wounded.
The British offered terms of peace on various occasions, notably in March 1901, but were rejected by Botha. The last of the Boers surrendered in May 1902 and the war ended with the Treaty of Vereeniging signed on 31 May 1902. Although the British had won, this came at a cost; the Boers were given £3,000,000 for reconstruction and were promised eventual limited self-government, which was granted in 1906 and 1907. The treaty ended the existence of the South African Republic and the Orange Free State as independent Boer republics and placed them within the British Empire. The Union of South Africa was established as a member of the Commonwealth in 1910.
Cost of the war.
It is estimated that the total cost of the war to the British government was £211,156,000 (equivalent to £202,000,000,000 in 2014).
Aftermath and analysis.
The Second Boer War cast long shadows over the history of the South African region. The predominantly agrarian society of the former Boer republics was profoundly and fundamentally affected by the scorched earth policy of Roberts and Kitchener. The devastation of both Boer and black African populations in the concentration camps and through war and exile were to have a lasting effect on the demography and quality of life in the region. Many exiles and prisoners were unable to return to their farms at all; others attempted to do so but were forced to abandon the farms as unworkable given the damage caused by farm burning and salting of the fields in the course of the scorched earth policy. Destitute Boers and black Africans swelled the ranks of the unskilled urban poor competing with the "uitlanders" in the mines.
The postwar reconstruction administration was presided over by Lord Milner and his largely Oxford trained Milner's Kindergarten. This small group of civil servants had a profound effect on the region, eventually leading to the Union of South Africa. "In the aftermath of the war, an imperial administration freed from accountability to a domestic electorate set about reconstructing an economy that was by then predicated unambiguously on gold. At the same time, British civil servants, municipal officials, and their cultural adjuncts were hard at work in the heartland of the former Boer Republics helping to forge new identities – first as 'British South Africans' and then, later still, as 'white South Africans'." Some scholars, for good reasons, identify these new identities as partly underpinning the act of union that followed in 1910. Although challenged by a Boer rebellion only four years later, they did much to shape South African politics between the two world wars and right up to the present day."
The counterinsurgency techniques and lessons (the restriction of movement, the containment of space, the ruthless targeting of anything, everything and anyone that could give sustenance to guerrillas, the relentless harassment through sweeper groups coupled with rapid reaction forces, the sourcing and coordination of intelligence, and the nurturing of native allies) learned during the Boer War were used by the British (and other forces) in future guerrilla campaigns including to counter Malayan communist rebels during the Malayan Emergency. In World War II the British also adopted some of the concepts of raiding from the Boer commandos when, after the fall of France, they set up their special raiding forces, and in acknowledgement of their erstwhile enemies, chose the name British Commandos.
Many of the Boers referred to the war as the second of the "Freedom Wars". The most resistant of Boers wanted to continue the fight and were known as ""bittereinders"" (or "irreconcilables") and at the end of the war a number of Boer fighters such as Deneys Reitz chose exile rather than sign an oath, such as the following, to pledge allegiance to Britain: Over the following decade, many returned to South Africa and never signed the pledge. Some, like Reitz, eventually reconciled themselves to the new "status quo", but others could not.
Union of South Africa.
One of the most important events in the decade after the end of the war was the creation of the Union of South Africa (later the Republic of South Africa). It proved a key ally to Britain as a Dominion of the British Empire during the World Wars. At the start of First World War a crisis ensued when the South African government led by Louis Botha and other former Boer fighters, such as Jan Smuts, declared support for Britain and agreed to send troops to take over the German colony of German South-West Africa (Namibia).
Many Boers were opposed to fighting for Britain, especially against Germany, which had been sympathetic to their struggle. A number of bittereinders and their allies took part in a revolt known as the Maritz Rebellion. This was quickly suppressed and in 1916, the leading Boer rebels in the Maritz Rebellion got off lightly (especially compared with the fate of leading Irish rebels of the Easter Rising), with terms of imprisonment of six and seven years and heavy fines. Two years later, they were released from prison, as Louis Botha recognised the value of reconciliation. Thereafter the bittereinders concentrated on political organisation within the constitutional system and built up what later became the National Party, which took power in 1948 and dominated the politics of South Africa from the late 1940s until the early 1990s, under the apartheid system.
Effect of the war on domestic British politics.
Many Irish nationalists sympathised with the Boers, viewing them to be a people oppressed by British imperialism, much like themselves. Irish miners already in the Transvaal at the start of the war formed the nucleus of two Irish commandos. The Second Irish Brigade was headed up by an Australian of Irish parents, Colonel Arthur Lynch. In addition, small groups of Irish volunteers went to South Africa to fight with the Boers – this despite the fact that there were many Irish troops fighting in the British army. In Britain, the "Pro-Boer" campaign expanded, with writers often idealising the Boer society.
The war also highlighted the dangers of Britain's policy of non-alignment and deepened her isolation. The 1900 UK general election, also known as the "Khaki election", was called by the Prime Minister, Lord Salisbury, on the back of recent British victories. There was much enthusiasm for the war at this point, resulting in a victory for the Conservative government.
However, public support quickly waned as it became apparent that the war would not be easy and it dragged on, partially contributing to the Conservatives' spectacular defeat in 1906. There was public outrage at the use of scorched earth tactics – the forced clearance of women and children, the destruction of the countryside, burning of Boer homesteads and poisoning of wells, for example – and the conditions in the concentration camps. It also became apparent that there were serious problems with public health in Britain: up to 40% of recruits in Britain were unfit for military service, suffering from medical problems such as rickets and other poverty-related illnesses. This came at a time of increasing concern for the state of the poor in Britain.
Having taken the country into a prolonged war, the Conservative government was rejected by the electorate at the first general election after the war was over. Balfour, succeeding his uncle Lord Salisbury in 1903 immediately after the war, took over a Conservative party that had won two successive landslide majorities but led it to a landslide defeat in 1906.
Horses.
The number of horses killed in the war was at the time unprecedented in modern warfare. For example, in the Relief of Kimberley, French's cavalry rode 500 horses to their deaths in a single day. The wastage was particularly heavy among British forces for several reasons: overloading of horses with unnecessary equipment and saddlery, failure to rest and acclimatise horses after long sea voyages and, later in the war, poor management by inexperienced mounted troops and distant control by unsympathetic staffs. The average life expectancy of a British horse, from the time of its arrival in Port Elizabeth, was around six weeks.
Horses were on occasion slaughtered for their meat. During the Siege of Kimberley and Siege of Ladysmith, horses were consumed as food once the regular sources of meat were depleted. The besieged British forces in Ladysmith also produced "chevril", a Bovril-like paste, by boiling down the horse meat to a jelly paste and serving it like beef tea.
The Horse Memorial in Port Elizabeth is a tribute to the 300,000 horses that died during the conflict.
British Empire involvement.
The vast majority of troops fighting for the British army came from the United Kingdom. However, a significant number came from other parts of the British Empire. These countries had their own internal disputes over whether they should remain tied to the United Kingdom, or have full independence, which carried over into the debate around the sending of forces to assist the United Kingdom. Though not fully independent on foreign affairs, these countries did have local say over how much support to provide, and the manner it was provided. Ultimately, Australia, Canada, New Zealand and Company-ruled Rhodesia all sent volunteers to aid the United Kingdom. Australia provided the largest number of troops followed by Canada. Troops were also raised to fight with the British from the Cape Colony and the Colony of Natal. Some Boer fighters, such as Jan Smuts and Louis Botha, were technically British subjects as they came from the Cape Colony and Colony of Natal, respectively.
There were also many volunteers from the Empire who were not selected for the official contingents from their countries and travelled privately to South Africa to form private units, such as the Canadian Scouts and Doyle's Australian Scouts. There were also some European volunteer units from British India and British Ceylon, though the British Government refused offers of non-white troops from the Empire. Some Cape Coloureds also volunteered early in the war, but later some of them were effectively conscripted and kept in segregated units. As a community, they received comparatively little reward for their services. In many ways, the war set the pattern for the Empire's later involvement in the two World Wars. Specially raised units, consisting mainly of volunteers, were dispatched overseas to serve with forces from elsewhere in the British Empire.
Technically the United States stayed neutral in the conflict, but some American citizens were asked to participate. Early in the war Lord Roberts cabled the American Frederick Russell Burnham, a veteran of both Matabele wars but at that very moment prospecting in the Klondike, to serve on his personal staff as Chief of Scouts. Burnham went on to receive the highest awards of any American who served in the war, but American mercenaries participated on both sides.
Australia.
From 1899 to 1901 the six separate self-governing colonies in Australia sent their own contingents to serve in the Boer War. Much of the population of the colonies had originated from what was then the United Kingdom of Great Britain and Ireland (England, Scotland, Wales, Ireland) and the desire to support Britain during the conflict appealed to many. After the colonies formed the Commonwealth of Australia in 1901, the new Government of Australia sent "Commonwealth" contingents to the war. The Boer War was thus the first war in which the Commonwealth of Australia fought. A few Australians fought on the Boer side. The most famous and colourful character was Colonel Arthur Alfred Lynch, formerly of Ballarat, Victoria, who raised the Second Irish Brigade.
The Australian climate and geography were far closer to that of South Africa than most other parts of the empire, so Australians adapted quickly to the environment, with troops serving mostly among the army's "mounted rifles." Enlistment in all official Australian contingents totalled 16,463. Another five to seven thousand Australians served in "irregular" regiments raised in South Africa. Perhaps five hundred Australian irregulars were killed. In total, 20,000 or more Australians served and about 1,000 were killed. A total of 267 died from disease, 251 were killed in action or died from wounds sustained in battle. A further 43 men were reported missing.
When the war began some Australians, like some Britons, opposed it. As the war dragged on some Australians became disenchanted, in part because of the sufferings of Boer civilians reported in the press. In an interesting twist (for Australians), when the British missed capturing President Paul Kruger, as he escaped Pretoria during its fall in June 1900, a "Melbourne Punch", 21 June 1900, cartoon depicted how the War could be won, using the Kelly Gang.
The convictions and executions of two Australian lieutenants, Harry Harbord Morant, colloquially known as 'The Breaker' for his skill with horses, and Peter Handcock in 1902, and the imprisonment of a third, George Witton, had little impact on the Australian public at the time despite later legend. The controversial court-martial saw the three convicted of executing Boer prisoners under their authority. After the war, though, Australians joined an empire-wide campaign that saw Witton released from jail. Much later, some Australians came to see the execution of Morant and Handcock as instances of wrongfully executed Australians, as illustrated in the 1980 Australian film "Breaker Morant".
Canada.
Over 7,000 Canadian soldiers and support personnel were involved in the second Boer war from October 1899 to May 1902. With approximately 7,368 soldiers in a combat situation, the conflict became the largest military engagement involving Canadian soldiers from the time of Confederation until the Great War. Eventually, 270 soldiers died in the course of the Boer War. The Canadian public was initially divided on the decision to go to war as some citizens did not want Canada to become Britain's 'tool' for engaging in armed conflicts. Many Anglophone citizens were pro-Empire, and wanted the Prime Minister, Sir Wilfrid Laurier, to support the British in their conflict. On the other hand, many Francophone citizens felt threatened by the continuation of British Imperialism to their national sovereignty.
In the end, in order to appease the citizens who wanted war and avoid angering those who didn't, Laurier sent 1,000 volunteers under the command of Lieutenant Colonel William Otter to aid the confederation in its war to 'liberate' the peoples of the Boer controlled states in South Africa. The volunteers were provided to the British with the stipulation that the British pay costs of the battalion after it arrived in South Africa.
The supporters of the war claimed that it "pitted British Freedom, justice and civilization against Boer backwardness". The French Canadians' opposition to the Canadian involvement in a British 'colonial venture' eventually led to a three-day riot in various areas of Quebec.
Commonwealth involvement in the Boer War can be summarized into three parts. The first part (October 1899 – December 1899) was characterized by questionable decisions and blunders from the Commonwealth leadership which affected its soldiers greatly. The soldiers of the Commonwealth were shocked at the number of Afrikaner soldiers who were willing to oppose the British. The Afrikaner troops were very willing to fight for their country, and were armed with modern weaponry and were highly mobile soldiers. This was one of the best examples of Guerrilla style warfare, which would be employed throughout the twentieth century after set piece fighting was seen as a hindrance by certain groups. The Boer soldiers would evade capture and secure provisions from their enemies therefore they were able to exist as a fighting entity for an indeterminate period of time.
The end of the First part was the period in mid-December which is referred to as the "Black Week". During the week of 10–17 December 1899, the British suffered three major defeats at the hands of the Boers at the battlefields of Stormberg, Magersfontein and Colenso. Afterwards, the British called upon more volunteers to take part in the war from the Commonwealth.
The second part of the war (February–April 1900) was the opposite of the first. After the British reorganized and reinforced under new leadership, they began to experience success against the Boer soldiers. Commonwealth soldiers resorted to using blockhouses, farm burning and concentration camps to 'persuade' the resisting Boers into submission.
The final phase of the war was the guerrilla phase where many Boer soldiers turned to Guerrilla tactics such as raiding infrastructure or communications lines. Many Canadian soldiers did not actually see combat after getting shipped over to South Africa as many arrived around the time of the signing of the Treaty of Vereeniging on 31 May 1902.
New Zealand.
When the Second Boer War seemed imminent, New Zealand offered its support. On 28 September 1899, Prime Minister Richard Seddon asked Parliament to approve the offer to the imperial government of a contingent of mounted rifles, thus becoming the first British Colony to send troops to the Boer War. The British position in the dispute with the Transvaal was "moderate and righteous," he maintained. He stressed the "crimson tie" of Empire that bound New Zealand to the mother-country and the importance of a strong British Empire for the colony's security.
By the time peace was concluded two and a half years later, 10 contingents of volunteers, totalling nearly 6,500 men from New Zealand, with 8,000 horses had fought in the conflict, along with doctors, nurses, veterinary surgeons and a small number of school teachers. Some 70 New Zealanders died from enemy action, with another 158 killed accidentally or by disease.
"The New Zealanders in South Africa 1899–1902"
South Africa.
During the war, the British army also included substantial contingents from South Africa itself. There were large communities of English-speaking immigrants and settlers in Natal and Cape Colony (especially around Cape Town and Grahamstown), which formed volunteer units that took the field, or local "town guards." At one stage of the war, a "Colonial Division," consisting of five light horse and infantry units under Brigadier General Edward Brabant, took part in the invasion of the Orange Free State. Part of it withstood a siege by Christiaan De Wet at Wepener on the borders of Basutoland. Another large source of volunteers was the "uitlander" community, many of whom hastily left Johannesburg in the days immediately preceding the war.
Later during the war, Lord Kitchener attempted to form a Boer Police Force, as part of his efforts to pacify the occupied areas and effect a reconciliation with the Boer community. The members of this force were despised as traitors by the Boers still in the field. Those Boers who attempted to remain neutral after giving their parole to British forces were derided as ""hensoppers"" (hands-uppers) and were often coerced into giving support to the Boer guerrillas. (This was one of the reasons for the British ruthlessly scouring the countryside of people, livestock and anything else the Boer commandos might find useful.)
Like the Canadian and particularly the Australian and New Zealand contingents, many of the volunteer units formed by South Africans were "light horse" or mounted infantry, well suited to the countryside and manner of warfare. Some regular British officers scorned their comparative lack of formal discipline, but the light horse units were hardier and more suited to the demands of campaigning than the overloaded British cavalry, who were still obsessed with the charge with lance or sabre. At their peak, 24,000 South Africans (including volunteers from the Empire) served in the field in various "colonial" units. Notable units (in addition to the Imperial Light Horse) were the South African Light Horse, Rimington's Guides, Kitchener's Horse and the Imperial Light Infantry.
Notable people involved in the Boer War.
Harold Lothrop Borden was the only son of Canada's Canadian Minister of Defence and Militia, Frederick William Borden. Serving in the Royal Canadian Dragoons, he became the most famous Canadian casualty of the Second Boer War. Queen Victoria asked F. W. Borden for a photograph of his son, Prime Minister Wilfrid Laurier praised his services, tributes arrived from across Canada, and in his home town Canning, Nova Scotia, there is a monument (by Hamilton MacCarthy) erected to his memory.
Sam Hughes – Senior Militia officer and later a Federally elected cabinet minister. As a very patriotic individual, Hughes became involved in the Boer war as a member of Brigadier-General Herbert Settle's expedition after Hughes unsuccessfully tried to raise his own brigade of soldiers. Hughes was noted by his colleagues for having a dislike of professional soldiers and he was noted for being an exceptional leader of irregular soldiers, whom he preferred to lead in combat. However, Hughes was dismissed and was sent home in the summer of 1900 for; sending letters back home which were published outlining British command incompetence, his impatience and boastfulness and his providing surrendering enemies favourable conditions. When he arrived back in Canada, Hughes became very active politically, and he would eventually start his political career with the Conservatives. When he became a member of parliament, Hughes would be in the position to become the Canadian Minister of Defence and Militia in 1911, just prior the outbreak of World War I. This was a position that Hughes would be dismissed from in 1916, due once again to his impatience, among other reasons.
John McCrae – Best known as the author of the World War I poem In Flanders Fields, McCrae started his active military service in the Boer War as an artillery officer. After completing several major campaigns, McCrae's artillery unit was sent home to Canada in 1901 with what would be referred to today as an 'honourable discharge'. McCrae ended up becoming a special professor in the University of Vermont for pathology and he would later serve in World War I as a Medical officer until his death from pneumonia while on active duty in 1918 .
Harry "Breaker" Morant – Anglo-Australian poet and soldier who participated in the summary execution of several Boer (Afrikaner) prisoners and the killing of a German missionary, Daniel Heese, who had been a witness to the shootings. Court-martialed and executed for murder.
Winston Churchill – Best known as the prime minister of Britain during the main part of the Second World War worked as a war correspondent for "The Morning Post". At the age of twenty-six, he was captured and held prisoner in a camp in Pretoria from which he escaped and rejoined the British army. He received a commission in the South African Light Horse (still working as a correspondent) and witnessed the capture of Ladysmith and Pretoria.
Mahatma Gandhi – Best known as the preeminent leader of Indian independence movement in British-ruled India he volunteered in 1900 to form a group of ambulance drivers raising eleven hundred Indian volunteer medics. At Spion Kop Gandhi and his bearers had to carry wounded soldiers for miles to a field hospital because the terrain was too rough for the ambulances. General Redvers Buller mentioned the courage of the Indians in his dispatch. Gandhi and thirty-seven other Indians received the War Medal.
Victoria Cross recipients.
Four Canadian soldiers in the Second Boer War received a Victoria Cross, which is the highest military medal available to soldiers of the Commonwealth and former British Territories. It is awarded based on exemplary bravery and valour in the presence of danger.
Sergeant Arthur Herbert Lindsay Richardson – Soldier of Lord Strathcona's Horse, Richardson rode a wounded horse, while wounded himself, back into enemy fire to retrieve a wounded comrade whose horse had been killed at Wolve Spruit on 5 July 1900.
Lieutenant Hampden Zane Churchill Cockburn – Soldier of the Royal Canadian Dragoons, Cockburn received his Victoria Cross on 7 November 1900 when his unit was the rear guard at Leliefontein. Cockburn, along with fellow Victoria Cross recipient Lieutenant R.E.W. Turner, held off an advancing group of Boer soldiers in order to allow two Canadian Field guns to escape along with their crews. Cockburn was wounded and captured by the Boer soldiers.
Lieutenant Richard Ernest William Turner- Soldier of the Royal Canadian Dragoons, Turner received his Victoria Cross during the same portion of the conflict as Cockburn. Turner was wounded in the conflict, however unlike Cockburn, Turner escaped. Turner would later become a high-ranking officer in the Canadian army in World War I.
Sergeant Edward James Gibson Holland – Soldier of the Royal Canadian Dragoons. Holland received his Victoria Cross from the same rear guard conflict at Leliefontein on 7 November 1900 as Cockburn and Turner. However, Holland received his medal for a different reason than the two aforementioned Lieutenants. During the Boer advance, Holland kept the Boer soldiers at bay with his carriage mounted Colt machine gun despite the position becoming increasingly dangerous due to the proximity of the enemy. With his gun jammed and in danger of falling into enemy hands, Holland removed the Colt from its carriage and rode away on his horse with the gun in hand.
Final overview.
The Second Boer War was the harbinger for a new type of combat which would persevere throughout the twentieth century, guerrilla warfare. After the war was over, the entire British army underwent a period of reform which was focused on lessening the emphasis placed on mounted units in combat. It was determined that the idea of Cavalry was antiquated and improperly used on the battlefield in the modern warfare of the Boer War, and that the First World War was the final proof that cavalry had no place in twentieth century combat. Yet some British soldiers held dear to the fact that cavalry was put to better use after the reforms in the theatres of the Middle East and World War I, and that the idea of mounted infantry was useful in the times where the war was more mobile. An example of this was in the First World War during the battle of Mons where the British cavalry held the Belgian town against an initial German assault.
The Canadian units of the Royal Canadian Dragoons and the Royal Canadian Mounted Rifles fought in the first world war in the same role as the Boer war. However, during, and after, the Second World War the regiments swapped their horses for mechanized vehicles. The second Boer War was also the beginning of types of conflict involving machine guns, shrapnel and observation balloons which were all used extensively in the First World War. To the Canadians however, attrition was the leading cause of death in the second Boer war, with disease being the cause of approximately half of the Canadian deaths.
Canadians ended the war with four Victoria Crosses to its soldiers and two more Victoria Crosses were given to Canadian doctors attached to British Medical Corps units, Lieutenant H.E.M. Douglas (1899, Magersfontein) and Lieutenant W.H.S. Nickerson (1900, Wakkerstroom). Not all soldiers saw action since many landed in South Africa after the hostilities ended while others (including the 3rd Special Service Battalion, The Royal Canadian Regiment) performed garrison duty in Halifax, Nova Scotia so that their British counterparts could join at the front lines. Later on, contingents of Canadians served with the paramilitary South Africa Constabulary. The war also had its fair share of controversy, as Commonwealth soldiers used a scorched Earth policy as well as concentration camps to subdue the Boers. A total of 116 000 women, children and Boer soldiers were confined to the Commonwealth concentration camps, of which at least 28 000, mainly women and children, would die.
The British saw their tactics of Scorched Earth and concentration as ways of controlling the Boers by "eliminating the decay and deterioration of the national character" and as a way of reinforcing the values, through subjugation of citizens and the destruction of the means for the Boer soldiers to continue fighting, of British society that the Boers were rejecting by engaging in a war against the Commonwealth. The Boers saw it as a British ploy designed to coerce the Boer soldiers into a surrender. With approximately 10% of their population confined, many of whom were women and children, the Boers suggested that the British were forcing the Afrikaners to return to their homes and protect their families who were in danger of internment.
Commemorations.
The Australian National Boer War Memorial Committee organises events to mark the war on 31 May each year. In Canberra, a commemorative service is usually held at the Saint John the Baptist Anglican Church in Reid. Floral tributes are laid for the dead.

</doc>
<doc id="42721" url="https://en.wikipedia.org/wiki?curid=42721" title="Mary Elizabeth Braddon">
Mary Elizabeth Braddon

Mary Elizabeth Braddon (4 October 1835 – 4 February 1915) was an English popular novelist of the Victorian era. She is best known for her 1862 sensation novel "Lady Audley's Secret".
Life and works.
Born in London, Mary Elizabeth Braddon was privately educated. Her mother Fanny separated from her father Henry in 1840, when Mary was five. When Mary was ten years old, her brother Edward Braddon left for India and later Australia, where he became Premier of Tasmania. Mary worked as an actress for three years in order to support herself and her mother.
In 1860, Mary met John Maxwell (1824–1895), a publisher of periodicals. She started living with him in 1861. However, Maxwell was already married with five children, and his wife was living in an asylum in Ireland. Mary acted as stepmother to his children until 1874, when Maxwell's wife died and they were able to get married. She had six children by him, including the novelist William Babington Maxwell.
Braddon was a prolific writer, producing more than 80 novels with inventive plots. The most famous is "Lady Audley's Secret" (1862), which won her recognition, and a fortune as a bestseller. It has remained in print since its publication and been dramatised and filmed several times. R. D. Blackmore's anonymous sensation novel "Clara Vaughan" (1864) was wrongly attributed to her by some critics.
Braddon wrote several works of supernatural fiction, including the pact with the devil story "Gerald, or the World, the Flesh and the Devil" (1891), and the ghost stories "The Cold Embrace", "Eveline's Visitant" and "At Chrighton Abbey". From the 1930s onwards, these stories were often anthologised in collections such as Montague Summers's "The Supernatural Omnibus" (1931) and 
"Fifty Years of Ghost Stories" (1935). Braddon's legacy is tied to the sensation fiction of the 1860s.
Braddon also founded "Belgravia" magazine (1866), which presented readers with serialised sensation novels, poems, travel narratives and biographies, as well as essays on fashion, history and science. The magazine was accompanied by lavish illustrations and offered readers a source of literature at an affordable cost. She also edited "Temple Bar" magazine.
She died on 4 February 1915 in Richmond, then in Surrey and now in London, and is interred in Richmond Cemetery. Her home had been Lichfield House in the centre of then town, which was replaced by a block of flats in 1936, Lichfield Court, now listed. She has a plaque in Richmond parish church which calls her simply 'Miss Braddon'. A number of streets in the area are named after characters in her novels – her husband was a property developer in the area.
There is a critical essay on Braddon's work in Michael Sadleir's book "Things Past" (1944). In 2014 the Mary Elizabeth Braddon Association was founded to pay tribute to Braddon's life and work.
Dramatisations.
Several of Braddon's works have been dramatised, including:

</doc>
<doc id="42722" url="https://en.wikipedia.org/wiki?curid=42722" title="Guar">
Guar

The Guar or cluster bean, with the botanical name Cyamopsis tetragonoloba, is an annual legume and the source of guar gum. It is also known as Gavar, Guwar or Guvar bean.
The origin of "Cyamopsis tetragonoloba" is unknown, since it has never been found in the wild. It is assumed to have developed from the African species "Cyamopsis senegalesis". It was further domesticated in India and Pakistan, where it has been cultivated for many centuries. 
Guar grows well in semiarid areas, but frequent rainfall is necessary. 
This legume is a very valuable plant within a crop rotation cycle, as it lives in symbiosis with nitrogen-fixing bacteria. 
In fact, agriculturists in semi-arid regions of Rajasthan follow crop-rotation and use guar as a source to replenish the soil with essential fertilizers and nitrogen fixation, before the next crop. Guar as a plant has a multitude of different functions for human and animal nutrition but its gelling-agent-containing seeds (guar gum) are today the most important use. Demand is rising rapidly due to industrial use of guar gum in hydraulic fracturing (oil shale gas). About 80% of world production occurs in India and Pakistan, but due to strong demand, the plant is being introduced into new areas.
Biology.
"Cyamopsis tetragonoloba" grows upright, reaching a maximum height of up to 2–3 m. 
It has a main single stem with either basal branching or fine branching along the stem. 
Thanks to taproots, the guar plant can access soil moisture in low soil depths. Additionally, this legume develops root nodules with nitrogen-fixing soil bacteria rhizobia in the surface part of its rooting system.
Its leaves and stems are mostly hairy, dependent on the cultivar. Its fine leaves have an elongated oval shape (5 to 10 cm length) and of alternate position. Clusters of flowers grow in the plant axil and are of white to blueish color. The developing pods are rather flat and slim containing 5 to 12 small oval seeds of 5 mm length (TGW = 25-40 g). Usually, mature seeds are white or gray, but in case of excess moisture they can turn black and lose germination capacity. The chromosome number of guar seeds is 2n=14.
The seeds of guar beans have a very remarkable characteristic. Its kernel consists of a protein-rich germ (43-46%) and a relatively large endosperm (34-40 %), containing big amounts of the galactomannan. The latter is polysaccharide containing polymers of mannose and galactose in a ratio of 2:1 with many branches. Thanks to the latter, it exhibits a great hydrogen bonding activity having a viscosifying effect in liquids.
Cultivation.
Climate Requirements.
Guar is very drought-tolerant and sun-loving, but it is very susceptible to frost. Even though it can cope with little but regular rainfall, it requires sufficient soil moisture before planting and during maturation of seeds. Frequent drought periods can lead to delayed maturation. On the contrary, too much moisture during early phase of growth and after maturation lead to lower seed quality. Guar is also produced near to coastal areas in the Gandhidham region of Kutch, Gujarat, India.
Soil Requirements.
"Cyamopsis tetragonoloba" (L.) can grow on a wide range of different soil types. Preferably in fertile, medium-textured and sandy loam soils that are well-drained because waterlogging decreases plant performance. In respect of soil acidity, guar grows best in moderate alkaline conditions (pH 7-8) and is tolerant of salinity. Thanks to its taproots which are inoculated with rhizobia nodules, it produces nitrogen-rich biomass and improves soil quality.
Cultivation Areas.
It is grown principally in north-western India and Pakistan with smaller crops grown in the semiarid areas of the high plains of Texas in the US, Australia and Africa. The most important growing area centres on Jodhpur in Rajasthan, India where demand for guar for fractionation produced an agricultural boom as in 2012. 
Currently, India and Pakistan are the main producers of cluster bean, accounting for 80% production of the world's total, while Thar, Punjab Dry Areas in Pakistan and Rajasthan, Gujarat, Kutch region occupies the largest area (82.1%) under guar cultivation in India. In addition to its cultivation in India and Pakistan, the crop is also grown as a cash crop in other parts of the world. Several commercial growers have converted their crops to guar production to support the increasing demand for guar and other organic crops in the United States.
Varieties.
Pusa Naubahar and Pusa Sadabahar. Seeds at the rate of 30 kilograms/hectare (9–11 lb/acre) are planted at a spacing of 45-60 x 20–30 cm (18–24 x 8–12 in) in February–March and June–July. During rainy season, the seeds are sown 2–3 cm (~1 in) deep on ridges and in furrows during summer months. FYM is applied at the rate of 25 tonnes/ha (11.1 tons/acre). N, P2O5 and K2O recommendation for the crop is 20:60:80 kg/ha (18:53:71 lb/acre). Average yield is 5 to 6 tonnes/ha (2.2–2.6 tons/acre). Meager information is available for genetic variability in clusterbean addressing the qualitative traits (Pathak et al. 2011)
Uses.
Guar plant.
Agriculture
Domestic use
Guar gum.
As mentioned in the biology section, the seeds of the guar bean contain a very large endosperm. This endosperm consists of a very large polysaccharide of galactose and mannose. This polymer is water-soluble and exhibits a viscosifying effect in water. Guar gum has a multitude of different applications in food products, industrial products, and extractive industry.
Food.
In several food and beverages guar gum is used as additive in order to change its viscosity or as fiber source.
Partially hydrolyzed guar gum (PHGG) is produced by the partial enzymatic hydrolysis of guaran, the galactomannan of the endosperm of guar seeds (guar gum). It is a neutral polysaccharide consisting of a mannose backbone chain with single galactose side units occurring on almost two out of every three mannose units. The average molecular weight is about 25,000 Daltons. This gives a PHGG that still assays and functions as a soluble dietary fiber. 
PHGG as sold commercially is completely soluble, acid and heat stable, unaffected by ions, and will not gel at high concentrations. Commercial PHGG is approximately 75% dietary fiber and has minimal effect on taste and texture in food and beverage items. PHGG is fully fermentable in the large bowel, with a high rate of volatile fatty acid formation. The pH of the feces is lowered along with an increase in fecal bulk that mainly consists of bacterial cell mass and water. Clinical studies have demonstrated a prebiotic effect of PHGG. Studies have also shown that PHGG can be used to maintain regularity. PHGG is used in foods for particulate suspension, emulsification, antistaling, ice crystal control, and reduced fat baked goods.
Industry.
Derivatives of guar gum that has been further reacted is also used in industrial applications, such as the paper and textile industry, ore flotation, the manufacture of explosives and hydraulic fracturing (fracking) of oil and gas formations. Guar gum is often crosslinked with boron or chromium ions to make it more stable and heat-resistant. The crosslinking of guar with metal ions results in a gel that does not block the formation and helps efficiently in formation cleaning process. 
Guar and its derivatives make gel complexes with ions of Aluminium, Zirconium, Titanium, Chromium and Boron. 
The borate–guar reaction is reversible, and depends on the pH (hydrogen ion concentration) of the solution. Crosslinking of guar with borate occurs at high pH (approximately 9–10) of the solution. Guar gum has also proven a useful substitute for locust bean gum (made from carob seeds).
Feeds.
Guar meal korma and Guar meal Churi are widely used as prime raw material for Producing various kinds of Cattle feeds, Aqua feeds, Fish feeds, Poultry Feeds, Dairy feeds, Swine feeds etc.
Fracking agent.
Through the use of guar gum in the hydraulic fracturing (fracking) extraction of oil and shale gas, the demand has increased substantially. Only 10% of the Indian production stays within the country and the remaining 90% is exported for shale gas and oil industries. Consequently, many former cotton or wheat fields are converted into guar fields as production costs are also lower. But the increase of guar gum prices also has other reasons. But since prices are lower the farmers stop harvesting the Guar and returned to cotton & cumin and sesame crops sowing.

</doc>
<doc id="42725" url="https://en.wikipedia.org/wiki?curid=42725" title="Lady Audley's Secret">
Lady Audley's Secret

"Lady Audley's Secret" is a sensation novel by Mary Elizabeth Braddon published in 1862. It was Braddon's most successful and well-known novel. Critic John Sutherland (1989) described the work as "the most sensationally successful of all the sensation novels". The plot centres on "accidental bigamy" which was in literary fashion in the early 1860s. The plot was summarised by literary critic Elaine Showalter (1982): "Braddon's bigamous heroine deserts her child, pushes husband number one down a well, thinks about poisoning husband number two and sets fire to a hotel in which her other male acquaintances are residing". Elements of the novel mirror themes of the real-life Constance Kent case of June 1860 which gripped the nation for years. A follow-up novel, "Aurora Floyd", appeared in 1863. Braddon set the story in Ingatestone Hall, Essex, inspired by a visit there. There have been three silent film adaptations, one UK television version in 2000, and three minor stage adaptations.
History.
"Lady Audley's Secret" was partly serialised in "Robin Goodfellow" magazine July–September 1861, then entirely serialised in "Sixpenny Magazine" January–December 1862 and once again serialised in "London Journal" March–August 1863. It was published in 1862 in three volumes by William Tinsley.
Braddon initially sold the rights to the Irish publisher John Maxwell, with whom Braddon also lived and had children. Maxwell published it in his ailing magazine "Robin Goodfellow", but Braddon did not labour much, writing the final third in less than two weeks. Not until it was published as a three-volume novel by William Tinsley did it become a success and allow Braddon to be financially independent for the remainder of her life. It also enriched her publisher William Tinsley, who went on to build a villa at Barnes, 'Audley Lodge', with the profits.
Notably for the bigamous nature of the plot, Maxwell himself was married to another woman and thus Braddon was unable to marry him until his wife died in 1874. When it became public that Maxwell and Braddon had been living in an "irregular" arrangement all those years, it caused a minor scandal during which all their servants gave notice.
Plot.
The novel opens with the marriage in June 1857, of Lucy Graham, a beautiful, childlike blonde who enchants almost all who meet her, to Sir Michael Audley, a middle-aged, rich, and kind widower. Lucy was a governess for the local doctor, Mr. Dawson until her marriage. Previous to that Lucy was in service with Mrs. Vincent, but very little is known about her past before this. Around the time of the marriage, Sir Michael's nephew, the barrister Robert Audley, welcomes his old friend George Talboys back to England, after three years of gold prospecting in Australia.
George is anxious to get news of his wife, Helen, whom he left three years ago when their financial situation became desperate, to seek gold in Australia. He reads in the newspaper that she has died, and, after visiting her home to confirm this, he becomes despondent. Robert Audley cares for his friend, and, hoping to distract him, offers to take him to his wealthy uncle's country manor. George had a child, Georgey, who was left under the care of Lieutenant Maldon, George's father-in-law. Robert and George set off to visit Georgey, and George decides to make Robert little Georgey's guardian and caretaker of 20,000 pounds put into the boy's name. After settling the matter of the boy's guardianship, the two set off to visit Sir Michael.
While at the country manor Audley Court, Lady Audley avoids meeting with George. When the two seek an audience with the new Lady Audley, she makes many excuses to avoid their visit, but he and Robert are shown a portrait of her by Alicia Audley, Robert's cousin. George appears greatly struck by the portrait, unbeknownst to Robert (who credits the unfavourable reaction to that evening's storm). Shortly thereafter, George disappears upon a visit to Audley Court, much to Robert's consternation. Unwilling to believe that George has simply left suddenly and without notice, Robert begins to look into the circumstances around the strange disappearance.
While searching for his friend, Robert begins to take notes of the events as they unfold. His notes indicate the involvement of Lady Audley, much to his chagrin, and he slowly begins to collect evidence against her. One night, he reveals the evidence and notes that George was in possession of many letters that his former wife wrote. Lady Audley immediately sets off to London, where the letters were kept, and Robert follows after her. However, by the time he arrives, he discovers that George's possessions have been broken into with the help of a local locksmith and that the letters have vanished. One possession, however, remains – a book with a note written by George's wife that matches Lady Audley's handwriting. This confirms Robert's suspicion that Lady Audley is implicated in George's disappearance; it also leads Robert to conclude that Lady Audley is actually George's supposedly dead wife.
Suspecting the worst of Lady Audley and being afraid for little Georgey's life, Robert travels to Lieutenant Maldon's house and demands possession of the boy. Once Robert has Georgey under his control, he places the boy in a school run by Mr. Marchmont. Afterwards, Robert visits George's father, Mr. Harcourt Talboys, and confronts the Squire with his son's death. Mr. Harcourt listens dispassionately to the story. In the course of his visit to the Talboys' manor, Robert is entranced by George's sister Clara, who looks startlingly like George. Clara's passion for finding her brother spurs Robert on.
In February 1859, Robert continues searching for evidence. He receives a notice that his uncle is ill, and he quickly returns to Audley Court. While there, Robert speaks with Mr. Dawson and receives a brief description of all that is known about Lucy's background. He hears that Lucy was employed by Mrs. Vincent at her school since 1852, and, to verify this claim, Robert tracks down Mrs. Vincent, who is in hiding because of debts. According to Miss Tonks, a teacher at Mrs. Vincent's school, Lucy actually arrived at the school in August 1854 and was secretive about her past. Miss Tonks gives Robert a travel box that used to belong to Lucy, and upon examining stickers on the box, Robert discovers both the name Lucy Graham and the name Helen Talboys.
Robert realises that Helen Talboys faked her death before creating her new identity. When Robert confronts Lucy, she tells him that he has no proof, and he leaves to find more evidence, heading to Castle Inn, which is run by Luke Marks. During the night, Lucy forces Luke's wife Phoebe to let her into the inn and Lucy sets the place on fire, with the intention of killing Robert. However, Robert survives and returns to Audley Court and again confronts Lucy. This time, she says she is insane and confesses her life's story to Robert and Sir Michael, claiming that George abandoned her originally and she had no choice but to abandon her old life and child in order to find another, wealthier husband.
Sir Michael is unhappy and leaves with Alicia to travel through Europe. Robert invites a Dr. Mosgrave to make a more astute judgment regarding Lucy's sanity, and he proclaims that she is indeed victim to latent insanity, which overpowers her in times of stress and makes her very dangerous to any and all. Lucy, under the name of Madame Taylor, enters a mental institution located somewhere in Belgium along the route between Brussels and Paris. While being committed, Lucy confesses to Robert that she killed George by pushing him down a deserted well in the garden of Audley Court.
Robert grieves for his friend George until Luke Marks, who was fatally injured in the fire, manages, before dying, to tell Robert that George survived Lady Audley's attempted murder and that George, with Luke's help, left intending to return to Australia. Robert is overjoyed, and he asks Clara to marry him and go with him to Australia to find George. Clara accepts, but before they set out, George returns and reveals that he actually visited New York instead. The narrative ends with the death of Lucy abroad, and Clara and Robert happily married and living in a country cottage with George and his son. Robert's formerly infatuated cousin Alicia marries her once-spurned suitor, Sir Harry Towers, and Audley Court is left abandoned along with all of its unhappy memories.
Analysis and themes.
"Lady Audley's Secret" plays on Victorian anxieties about the domestic sphere. The home was supposed to be a refuge from the dangers of the outside world, but in the novel, the seemingly perfect domestic lady turns out to be a violent criminal who has not only tried to commit murder but who has also committed bigamy and abandoned her child. This unsettled Victorian readers because it indicated that the concepts of "the perfect lady/mother" and "domestic bliss" were more idealistic than realistic. In addition, anxieties about the increasing urbanisation of Britain abound; the city gives Lady Audley the power to change her identity because it renders its citizens effectively anonymous. The small town of Audley is no longer a refuge where everyone knows the life story of every neighbour; the residents of Audley must accept Lucy Graham's account of herself since they have no other information about her past. Other anxieties about unstable identity appear throughout the novel: Robert's relationship with George has homosexual overtones, especially considered in light of his attraction to Clara, George's sister, who is described as looking identical to George. Additionally, Lady Audley's maid, Phoebe, resembles Lady Audley, thus banishing the idea of physical distinction between the upper and lower classes and therefore of any inherent superiority of the former.
"Lady Audley's Secret" is, furthermore, a story about gender and class, and Lady Audley's objectionable upward mobility suggests a threat to the paradigm of social class. Madness is also a key issue. Lady Audley and others often converse about the meaning of this word, but many readers believe that Lady Audley is not mad. In fact, many critics view Lady Audley's deception as a feminist act in which a woman takes control of the direction of her own life.
The novel mirrors many of the same themes from the real-life Constance Kent case of June 1860 that gripped the nation with headline news for years. The first instalment of "Lady Audley's Secret" came out almost exactly one year after the Kent murder. The novel, like the real-life case, featured a wicked stepmother (and former governess who married a gentleman), a mysterious and brutal murder in a country manor house, a body thrown down a well, and characters fascinated by madness. Constance Kent can be seen in many of the female characters in the novel: the murderess Lady Audley, the tomboyish Alicia Audley, the restrained Phoebe Marks and the lonely Clara Talboys. Jack Whicher, the detective and case investigator, can be seen in the character of Robert Audley.
In popular culture.
"Lady Audley's Secret" is involved in a subplot of "Betsy and Tacy Go Downtown", the fourth book in the Betsy-Tacy series by Maud Hart Lovelace. Betsy has read it and other books in the same genre, and aspires to write similar works.

</doc>
<doc id="42726" url="https://en.wikipedia.org/wiki?curid=42726" title="Cephalopod">
Cephalopod

A cephalopod (pronounced ) is any member of the molluscan class Cephalopoda (Greek plural , "kephalópoda"; "head-feet"). These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot. Fishermen sometimes call them inkfish, referring to their common ability to squirt ink. The study of cephalopods is a branch of malacology known as teuthology.
Cephalopods became dominant during the Ordovician period, represented by primitive nautiloids. The class now contains two, only distantly related, extant subclasses: Coleoidea, which includes octopuses, squid, and cuttlefish; and Nautiloidea, represented by "Nautilus" and "Allonautilus". In the Coleoidea, the molluscan shell has been internalized or is absent, whereas in the Nautiloidea, the external shell remains. About 800 living species of cephalopods have been identified. Two important extinct taxa are the Ammonoidea (ammonites) and Belemnoidea (belemnites).
Distribution.
There are over 800 extant species of cephalopod, although new species continue to be described. An estimated 11,000 extinct taxa have been described, although the soft-bodied nature of cephalopods means they are not easily fossilised.
Cephalopods are found in all the oceans of Earth. None of them can tolerate freshwater, but the brief squid, "Lolliguncula brevis", found in Chesapeake Bay, may be a notable exception in that it tolerates brackish water.
Cephalopods occupy most of the depth of the ocean, from the abyssal plain to the sea surface. Their diversity is greatest near the equator (~40 species retrieved in nets at 11°N by a diversity study) and decreases towards the poles (~5 species captured at 60°N).
Nervous system and behavior.
Cephalopods are widely regarded as the most intelligent of the invertebrates, and have well developed senses and large brains (larger than those of gastropods). The nervous system of cephalopods is the most complex of the invertebrates and their brain-to-body-mass ratio falls between that of endothermic and ectothermic vertebrates. The brain is protected in a cartilaginous cranium. The giant nerve fibers of the cephalopod mantle have been widely used for many years as experimental material in neurophysiology; their large diameter (due to lack of myelination) makes them relatively easy to study compared with other animals. Cephalopods have also been known to climb out of their aquaria, maneuver a distance of the lab floor, enter another aquarium to feed on the crabs, and return to their own aquarium.
Cephalopods are social creatures; when isolated from their own kind, they will sometimes shoal with fish.
Some cephalopods are able to fly through the air for distances of up to 50 m. While cephalopods are not particularly aerodynamic, they achieve these impressive ranges by jet-propulsion; water continues to be expelled from the funnel while the organism is in the air. The animals spread their fins and tentacles to form wings and actively control lift force with body posture. One species, "Todarodes pacificus", has been observed spreading tentacles in a flat fan shape with a mucus film between the individual tentacles while another, "Sepioteuthis sepioidea," has been observed putting the tentacles in a circular arrangement.
Senses.
Cephalopods have advanced vision, can detect gravity with statocysts, and have a variety of chemical sense organs. Octopuses use their arms to explore their environment and can use them for depth perception.
Vision.
Most cephalopods rely on vision to detect predators and prey, and to communicate with one another. Consequently, cephalopod vision is acute: training experiments have shown that the common octopus can distinguish the brightness, size, shape, and horizontal or vertical orientation of objects. The morphological construction gives cephalopod eyes the same performance as sharks'; however, their construction differs, as cephalopods lack a cornea, and have an everted retina. Cephalopods' eyes are also sensitive to the plane of polarization of light. Surprisingly—given their ability to change color—all octopi and most cephalopods are color blind. When camouflaging themselves, they use their chromatophores to change brightness and pattern according to the background they see, but their ability to match the specific color of a background may come from cells such as iridophores and leucophores that reflect light from the environment. They also produce visual pigments throughout their body, and may sense light levels directly from their body. Evidence of color vision has been found in the sparkling enope squid ("Watasenia scintillans"), which achieves color vision by the use of three distinct retinal molecules (A1, sensitive to red; A2, to purple, and A4, to yellow?) which bind to its opsin.
In 2015, molecular evidence was published indicating that cephalopod chromatophores are photosensitive; reverse transcription polymerase chain reactions (RT-PCR) revealed transcripts encoding rhodopsin and retinochrome within the retinas and skin of the longfin inshore squid ("Doryteuthis pealeii"), and the common cuttlefish ("Sepia officinalis") and broadclub cuttlefish ("Sepia latimanus"). The authors claim this is the first evidence that cephalopod dermal tissues may possess the required combination of molecules to respond to light.
Unlike many other cephalopods, nautiluses do not have good vision; their eye structure is highly developed, but lacks a solid lens. They have a simple "pinhole" eye through which water can pass. Instead of vision, the animal is thought to use olfaction as the primary sense for foraging, as well as locating or identifying potential mates.
Hearing.
Some squids have been shown to detect sound using their statocysts.
Use of light.
Most cephalopods possess chromatophores - colored pigment cells that expand and contract in accordance with their counterparts to produce color and pattern - which they can use in a startling array of fashions. As well as providing camouflage with their background, some cephalopods bioluminesce, shining light downwards to disguise their shadows from any predators that may lurk below. The bioluminescence is produced by bacterial symbionts; the host cephalopod is able to detect the light produced by these organisms. Bioluminescence may also be used to entice prey, and some species use colorful displays to impress mates, startle predators, or even communicate with one another. It is not certain whether bioluminescence is actually of epithelial origin or if it is a bacterial production.
Coloration.
Cephalopods can change their colors and patterns in milliseconds, whether for signalling (both within the species and for warning) or active camouflage, as their chromatophores are expanded or contracted. Coloration is typically stronger in near-shore species than those living in the open ocean, whose functions tend to be restricted to disruptive camouflage.
Evidence of original coloration has been detected in cephalopod fossils dating as far back as the Silurian; these orthoconic individuals bore concentric stripes, which are thought to have served as camouflage. Devonian cephalopods bear more complex color patterns, of unknown function.
Ink.
With the exception of the Nautilidae and the species of octopus belonging to the suborder Cirrina, all known cephalopods have an ink sac, which can be used to expel a cloud of dark ink to confuse predators. This sac is a muscular bag which originated as an extension of the hind gut. It lies beneath the gut and opens into the anus, into which its contents – almost pure melanin – can be squirted; its proximity to the base of the funnel means the ink can be distributed by ejected water as the cephalopod uses its jet propulsion. The ejected cloud of melanin is usually mixed, upon expulsion, with mucus, produced elsewhere in the mantle, and therefore forms a thick cloud, resulting in visual (and possibly chemosensory) impairment of the predator, like a smokescreen. However, a more sophisticated behaviour has been observed, in which the cephalopod releases a cloud, with a greater mucus content, that approximately resembles the cephalopod that released it (this decoy is referred to as a Pseudomorph). This strategy often results in the predator attacking the pseudomorph, rather than its rapidly departing prey. For more information, see Inking behaviors.
The inking behaviour of cephalopods has led to a common name of "inkfish", primarily used in fisheries science and the fishing industry, paralleling the terms white fish, oily fish, and shellfish.
Circulatory system.
Cephalopods are the only mollusks with a closed circulatory system. Coleoids have two gill hearts (also known as branchial hearts) that move blood through the capillaries of the gills. A single systemic heart then pumps the oxygenated blood through the rest of the body.
Like most molluscs, cephalopods use hemocyanin, a copper-containing protein, rather than hemoglobin, to transport oxygen. As a result, their blood is colorless when deoxygenated and turns blue when exposed to air.
Respiration.
Cephalopods exchange gases with the seawater by forcing water through their gills, which are attached to the roof of the organism. Water enters the mantle cavity on the outside of the gills, and the entrance of the mantle cavity closes. When the mantle contracts, water is forced through the gills, which lie between the mantle cavity and the funnel. The water's expulsion through the funnel can be used to power jet propulsion. The gills, which are much more efficient than those of other molluscs, are attached to the ventral surface of the mantle cavity.
There is a trade-off with gill size regarding lifestyle. To achieve fast speeds, gills need to be small—water will be passed through them quickly when energy is needed, compensating for their small size. However, organisms which spend most of their time moving slowly along the bottom do not naturally pass much water through their cavity for locomotion; thus they have larger gills, along with complex systems to ensure that water is constantly washing through their gills, even when the organism is stationary. The water flow is controlled by contractions of the radial and circular mantle cavity muscles.
The gills of cephalopods are supported by a skeleton of robust fibrous proteins; the lack of mucopolysaccharides distinguishes this matrix from cartilage. The gills are also thought to be involved in excretion, with NH4+ being swapped with K+ from the seawater.
Locomotion and buoyancy.
While most cephalopods can move by jet propulsion, this is a very energy-consuming way to travel compared to the tail propulsion used by fish. The efficiency of a propellor-driven waterjet (i.e. Froude efficiency) is a more efficient model than rocket efficiency. The relative efficiency of jet propulsion decreases further as animal size increases; paralarvae are far more efficient than juvenile and adult individuals. Since the Paleozoic era, as competition with fish produced an environment where efficient motion was crucial to survival, jet propulsion has taken a back role, with fins and tentacles used to maintain a steady velocity.
Whilst jet propulsion is never the sole mode of locomotion, the stop-start motion provided by the jets continues to be useful for providing bursts of high speed - not least when capturing prey or avoiding predators. Indeed, it makes cephalopods the fastest marine invertebrates,
and they can out-accelerate most fish.
The jet is supplemented with fin motion; in the squid, the fins flap each time that a jet is released, amplifying the thrust; they are then extended between jets (presumably to avoid sinking).
Oxygenated water is taken into the mantle cavity to the gills and through muscular contraction of this cavity, the spent water is expelled through the hyponome, created by a fold in the mantle. The size difference between the posterior and anterior ends of this organ control the speed of the jet the organism can produce. The velocity of the organism can be accurately predicted for a given mass and morphology of animal. Motion of the cephalopods is usually backward as water is forced out anteriorly through the hyponome, but direction can be controlled somewhat by pointing it in different directions. Some cephalopods accompany this expulsion of water with a gunshot-like popping noise, thought to function to frighten away potential predators.
Cephalopods employ a similar method of propulsion despite their increasing size (as they grow) changing the dynamics of the water in which they find themselves. Thus their paralarvae do not extensively use their fins (which are less efficient at low Reynolds numbers) and primarily use their jets to propel themselves upwards, whereas large adult cephalopods tend to swim less efficiently and with more reliance on their fins.
Early cephalopods are thought to have produced jets by drawing their body into their shells, as "Nautilus" does today. "Nautilus" is also capable of creating a jet by undulations of its funnel; this slower flow of water is more suited to the extraction of oxygen from the water. The jet velocity in "Nautilus" is much slower than in coleoids, but less musculature and energy is involved in its production. Jet thrust in cephalopods is controlled primarily by the maximum diameter of the funnel orifice (or, perhaps, the average diameter of the funnel) and the diameter of the mantle cavity. Changes in the size of the orifice are used most at intermediate velocities. The absolute velocity achieved is limited by the cephalopod's requirement to inhale water for expulsion; this intake limits the maximum velocity to eight body-lengths per second, a speed which most cephalopods can attain after two funnel-blows. Water refills the cavity by entering not only through the orifices, but also through the funnel. Squid can expel up to 94% of the fluid within their cavity in a single jet thrust. To accommodate the rapid changes in water intake and expulsion, the orifices are highly flexible and can change their size by a factor of twenty; the funnel radius, conversely, changes only by a factor of around 1.5.
Some octopus species are also able to walk along the sea bed. Squids and cuttlefish can move short distances in any direction by rippling of a flap of muscle around the mantle.
While most cephalopods float (i.e. are neutrally buoyant or nearly so; in fact most cephalopods are about 2-3% denser than seawater), they achieve this in different ways.
Some, such as "Nautilus", allow gas to diffuse into the gap between the mantle and the shell; others allow purer water to ooze from their kidneys, forcing out denser salt water from the body cavity; others, like some fish, accumulate oils in the liver; and some octopuses have a gelatinous body with lighter chlorine ions replacing sulfate in the body chemistry.
Shell.
Nautiluses are the only extant cephalopods with a true external shell. However, all molluscan shells are formed from the ectoderm (outer layer of the embryo); in cuttlefish ("Sepia" spp.), for example, an invagination of the ectoderm forms during the embryonic period, resulting in a shell (cuttlebone) that is internal in the adult. The same is true of the chitinous gladius of squid and octopuses. Cirrate octopods have arch-shaped cartilaginous fin supports, which are sometimes referred to as a "shell vestige" or "gladius". The Incirrina have either a pair of rod-shaped stylets or no vestige of an internal shell, and some squid also lack a gladius. Interestingly, the shelled coleoids do not form a clade or even a paraphyletic group. The "Spirula" shell begins as an organic structure, and is then very rapidly mineralized. Shells that are "lost" may be lost by resorption of the calcium carbonate component.
Females of the octopus genus "Argonauta" secrete a specialised paper-thin eggcase in which they reside, and this is popularly regarded as a "shell", although it is not attached to the body of the animal.
The largest group of shelled cephalopods, the ammonites, are extinct, but their shells are very common as fossils.
The deposition of carbonate, leading to a mineralized shell, appears to be related to the acidity of the organic shell matrix (see Mollusc shell); shell-forming cephalopods have an acidic matrix, whereas the gladius of squid has a basic matrix.
Head appendages.
Cephalopods, as the name implies, have muscular appendages extending from their heads and surrounding their mouths. These are used in feeding, mobility, and even reproduction. In coleoids they number eight or ten. Decapods such as cuttlefish and squid have five pairs. The longer two, termed "tentacles", are actively involved in capturing prey; they can lengthen rapidly (in as little as 15 milliseconds). In giant squid they may reach a length of 8 metres. They may terminate in a broadened, sucker-coated club. The shorter four pairs are termed "arms", and are involved in holding and manipulating the captured organism. They too have suckers, on the side closest to the mouth; these help to hold onto the prey. Octopods only have four pairs of sucker-coated arms, as the name suggests, though developmental abnormalities can modify the number of arms expressed.
The tentacle consists of a thick central nerve cord (which must be thick to allow each sucker to be controlled independently) surrounded by circular and radial muscles. Because the volume of the tentacle remains constant, contracting the circular muscles decreases the radius and permits the rapid increase in length. Typically a 70% lengthening is achieved by decreasing the width by 23%. The shorter arms lack this capability.
The size of the tentacle is related to the size of the buccal cavity; larger, stronger tentacles can hold prey as small bites are taken from it; with more numerous, smaller tentacles, prey is swallowed whole, so the mouth cavity must be larger.
Externally shelled nautilids ("Nautilus" and "Allonautilus") have on the order of 90 finger-like appendages, termed "tentacles", which lack suckers but are sticky instead, and are partly retractable.
Feeding.
All living cephalopods have a two-part beak; most have a radula, although it is reduced in most octopus and absent altogether in "Spirula". They feed by capturing prey with their tentacles, drawing it into their mouth and taking bites from it. They have a mixture of toxic digestive juices, some of which are manufactured by symbiotic algae, which they eject from their salivary glands onto their captured prey held in their mouth. These juices separate the flesh of their prey from the bone or shell. The salivary gland has a small tooth at its end which can be poked into an organism to digest it from within.
The digestive gland itself is rather short. It has four elements, with food passing through the crop, stomach and caecum before entering the intestine. Most digestion, as well as the absorption of nutrients, occurs in the digestive gland, sometimes called the liver. Nutrients and waste materials are exchanged between the gut and the digestive gland through a pair of connections linking the gland to the junction of the stomach and caecum. Cells in the digestive gland directly release pigmented excretory chemicals into the lumen of the gut, which are then bound with mucus passed through the anus as long dark strings, ejected with the aid of exhaled water from the funnel. Cephalopods tend to concentrate ingested heavy metals in their body tissue.
Radula.
The cephalopod radula consists of multiple symmetrical rows of up to nine teeth – thirteen in fossil classes. The organ is reduced or even vestigial in certain octopus species and is absent in "Spirula". The teeth may be homodont (i.e. similar in form across a row), heterodont (otherwise), or ctenodont (comb-like). Their height, width and number of cusps is variable between species. The pattern of teeth repeats, but each row may not be identical to the last; in the octopus, for instance, the sequence repeats every five rows.
Cephalopod radulae are known from fossil deposits dating back to the Ordovician. They are usually preserved within the cephalopod's body chamber, commonly in conjunction with the mandibles; but this need not always be the case; many radulae are preserved in a range of settings in the Mason Creek.
Radulae are usually difficult to detect, even when they are preserved in fossils, as the rock must weather and crack in exactly the right fashion to expose them; for instance, radulae have only been found in nine of the 43 ammonite genera, and they are rarer still in non-ammonoid forms: only three pre-Mesozoic species possess one.
Excretory system.
Most cephalopods possess a single pair of large nephridia. Filtered nitrogenous waste is produced in the pericardial cavity of the branchial hearts, each of which is connected to a nephridium by a narrow canal. The canal delivers the excreta to a bladder-like renal sac, and also resorbs excess water from the filtrate. Several outgrowths of the lateral vena cava project into the renal sac, continuously inflating and deflating as the branchial hearts beat. This action helps to pump the secreted waste into the sacs, to be released into the mantle cavity through a pore.
"Nautilus", unusually, possesses four nephridia, none of which are connected to the pericardial cavities.
The incorporation of ammonia is important for shell formation in terrestrial molluscs and other non-molluscan lineages. Because protein (i.e. flesh) is a major constituent of the cephalopod diet, large amounts of ammonium are produced as waste. The main organs involved with the release of this excess ammonium are the gills. The rate of release is lowest in the shelled cephalopods "Nautilus" and "Sepia" as a result of their using nitrogen to fill their shells with gas to increase buoyancy. Other cephalopods use ammonium in a similar way, storing the ions (as ammonium chloride) to reduce their overall density and increase buoyancy.
Reproduction and life cycle.
Cephalopods are a diverse group of species, but share common life history traits, for example they have a rapid growth rate and short life spans. Stearns (1992) suggested that in order to produce the largest possible number of viable offspring, spawning events depend on ecological environmental factors of the organism. The majority of cephalopods do not provide parental care to their offspring, except for example, octopus, which helps this organism increase the survival rate of their offspring. Marine species life cycles are affected by various environmental conditions. The development of a cephalopod embryo can be greatly affected by temperature, oxygen saturation, pollution, light intensity, and salinity. These factors are important to the rate of embryonic development and the success of hatching of the embryos. Food availability also plays an important role in the reproductive cycle of cephalopods. A limitation of food influences the timing of spawning along with their function and growth. Spawning time and spawning vary among marine species; it’s correlated with temperature, though cephalopods in shallow water spawn in cold months so that the offspring would hatch at warmer temperatures. Breeding can last from several days to a month.
Sexual maturity.
Cephalopods that are sexually mature and of adult size, begin spawning and reproducing. After the transfer of genetic material to the following generation, the adult cephalopods then die. Sexual maturation in male and female cephalopods can be observed internally by the enlargement of gonads and accessory glands. Mating would be a poor indicator of sexual maturation in females; they can receive sperm when not fully reproductively mature and store them until they are ready to fertilize the eggs. Most cephalopod males develop a hectocotylus, an arm tip which is capable of transferring their spermatozoa into the female mantel cavity. Though not all species use a hectocotylus; for example Nautilus releases a spadix. An indication of sexual maturity of females is the development of brachial photophores to attract mates.
Fertilization.
Cephalopods are not broadcast spawners. During the process of fertilization, the females use sperm provided by the male via external fertilization. Internal fertilization is seen only in octopods. The initiation of copulation begins when the male catches a female and wrapping his arm around her; either in a ‘male to female neck’ position or mouth to mouth position, depending on the species. The males then initiate the process of fertilization by contracting their mantle several times to release the spermatozoa. Cephalopods often mate several times, which influences males to mate longer with females that have previously, nearly tripling the amount of contractions of the mantle. To ensure the fertilization of the eggs, female cephalopods release a sperm-attracting peptide through the gelatinous layers of the egg to direct the spermatozoa. Female cephalopods lay eggs in clutches; each egg is composed of a protective coat to ensure the safety of the developing embryo when released into the water column. Reproductive strategies differ between cephalopod species. In giant pacific octopus, large eggs are laid on den, it will often take several days to lay all of them. Once the eggs are released and attached to a sheltered substrate the females then die. In some species of cephalopods, egg clutches are anchored to substrates by a mucilaginous adhesive substance. These eggs are swelled with preivitelline fluid (PVF), a hypertonic fluid that prevents premature hatching. Fertilized egg clusters are neutrally buoyant depending at the depth that they were laid but can also be found in substrates such as sand, matrix of corals, seaweed. Because these species do not provide parental care for their offspring, egg capsules can be injected with ink by the female in order to camouflage the embryos from predators.
Male-male competition.
Aggressive spawning is an activity that most cephalopods engage in; a protein in the male capsule sheath stimulates this behavior. Male- male aggression is seen in cephalopods; smaller males tend to lose these interactions. When a female is near the males charge on another continuously and begin to flair their arms. If neither male backed away, the arms extend to the back exposing the mouth which leads to the biting of arm tips. During mate competition males also participate in a technique called flushing. This technique is used by the second male attempting to mate with a female. Flushing removes spermatophores in the buccal cavity that was placed there by the first mate by forcing water into the cavity. Another behavior that males engage in is sneaker mating or mimicry- smaller males adjust their behavior to that of a female in order to reduce aggression. By using this technique, they are able to fertilize the eggs while the larger male is distracted by different male. During this process, the sneaker males quickly insert drop like sperm into the seminal receptacle.
Mate choice.
Mate choice is seen in cuttlefish species, where females prefer some males over others, though characteristics of the preferred males are unknown. A hypothesis states that females reject males by olfactory cues rather than visual cues. Several cephalopod species are polyandrous- accepting and storing multiple male spermatophores, which has been identified by DNA fingerprinting. Females are no longer receptive to mating attempts when holding their eggs in their arm. Females can store sperm in two places (1) the buccal cavity where recently mated males place their spermatophores, and (2) the internal sperm-storage receptacles where sperm packages from previous males are stored. Spermatophore storage results in sperm competition; which states that the female controls which mate fertilizes the eggs. In order to reduce this sort of competition, males develop agonistic behaviors like mate guarding and flushing. Flushing is used by both the male and female; it is the process of removing spermatophores of other males by continuously pumping strong jets of water into the buccal cavity of the female. This behavior however, reduces the available time to mate with other females.
Sexual dimorphism.
In a variety of marine organisms it is seen that females are larger in size compared to the males in some close related species. In some lineages, such as the blanket octopus, males become structurally smaller and smaller resembling a term, "dwarfism" dwarf males usually occurs at low densities. The blanket octopus male is an example of sexual-evolutionary dwarfism; females grow 10,000 to 40,000 times larger than the males and the sex ratio between males and females can be distinguished right after hatching of the eggs.
Embryology.
Cephalopod eggs span a large range of sizes, from 1 to 30 mm in diameter. The fertilised ovum initially divides to produce a disc of germinal cells at one pole, with the yolk remaining at the opposite pole. The germinal disc grows to envelop and eventually absorb the yolk, forming the embryo. The tentacles and arms first appear at the hind part of the body, where the foot would be in other molluscs, and only later migrate towards the head.
The funnel of cephalopods develops on the top of their head, whereas the mouth develops on the opposite surface. The early embryological stages are reminiscent of ancestral gastropods and extant Monoplacophora.
The shells develop from the ectoderm as an organic framework which is subsequently mineralised. In "Sepia", which has an internal shell, the ectoderm forms an invagination whose pore is sealed off before this organic framework is deposited.
Development.
The length of time before hatching is highly variable; smaller eggs in warmer waters are the fastest to hatch, and newborns can emerge after as little as a few days. Larger eggs in colder waters can develop for over a year before hatching.
The process from spawning to hatching follows a similar trajectory in all species, the main variable being the amount of yolk available to the young and when it is absorbed by the embryo.
Unlike most other molluscs, cephalopods do not have a morphologically distinct larval stage. Instead the juveniles are known as paralarvae. They quickly learn how to hunt, using encounters with prey to refine their strategies.
Growth in juveniles is usually allometric, whilst adult growth is isometric.
Evolution.
The traditional view of cephalopod evolution holds that they evolved in the Late Cambrian from a monoplacophoran-like ancestor with a curved, tapering shell, which was closely related to the gastropods (snails). The similarity of the early shelled cephalopod "Plectronoceras" to some gastropods was used in support of this view. The development of a siphuncle would have allowed the shells of these early forms to become gas-filled (thus buoyant) in order to support them and keep the shells upright while the animal crawled along the floor, and separated the true cephalopods from putative ancestors such as "Knightoconus", which lacked a siphuncle. Neutral or positive buoyancy (i.e. the ability to float) would have come later, followed by swimming in the Plectronocerida and eventually jet propulsion in more derived cephalopods.
However, some morphological evidence is difficult to reconcile with this view, and the redescription of "Nectocaris pteryx", which did not have a shell and appeared to possess jet propulsion in the manner of "derived" cephalopods, complicated the question of the order in which cephalopod features developed – provided "Nectocaris" is a cephalopod at all. Their position within the Mollusca is currently wide open to interpretation - see Mollusca#Phylogeny.
Early cephalopods were likely predators near the top of the food chain. They underwent pulses of diversification during the Ordovician period to become diverse and dominant in the Paleozoic and Mesozoic seas.
In the Early Palaeozoic, their range was far more restricted than today; they were mainly constrained to sublittoral regions of shallow shelves of the low latitudes, and usually occur in association with thrombolites. A more pelagic habit was gradually adopted as the Ordovician progressed. Deep-water cephalopods, whilst rare, have been found in the Lower Ordovician—but only in high-latitude waters.
The mid Ordovician saw the first cephalopods with septa strong enough to cope with the pressures associated with deeper water, and could inhabit depths greater than 100–200 m. The direction of shell coiling would prove to be crucial to the future success of the lineages; endogastric coiling would only permit large size to be attained with a straight shell, whereas exogastric coiling - initially rather rare - permitted the spirals familiar from the fossil record to develop, with their corresponding large size and diversity. (Endogastric mean the shell is curved so as the ventral or lower side is longitudinally concave (belly in); exogastric means the shell is curved so as the ventral side is longitudinally convex (belly out) allowing the funnel to be pointed backwards beneath the shell.)
The ancestors of coleoids (including most modern cephalopods) and the ancestors of the modern nautilus, had diverged by the Floian Age of the Early Ordovician Period, over 470 million years ago. The Bactritida, a Silurian–Triassic group of orthocones, are widely held to be paraphyletic to the coleoids and ammonoids, that is, the latter groups arose from within the Bactritida. An increase in the diversity of the coleoids and ammonoids is observed around the start of the Devonian period, and corresponds with a profound increase in fish diversity. This could represent the origin of the two derived groups.
Unlike most modern cephalopods, most ancient varieties had protective shells. These shells at first were conical but later developed into curved nautiloid shapes seen in modern nautilus species.
Competitive pressure from fish is thought to have forced the shelled forms into deeper water, which provided an evolutionary pressure towards shell loss and gave rise to the modern coleoids, a change which led to greater metabolic costs associated with the loss of buoyancy, but which allowed them to recolonise shallow waters. However, some of the straight-shelled nautiloids evolved into belemnites, out of which some evolved into squid and cuttlefish. The loss of the shell may also have resulted from evolutionary pressure to increase manoeuvrability, resulting in a more fish-like habit.
Phylogeny.
The internal phylogeny of the cephalopods is difficult to constrain; many molecular techniques have been adopted, but the results produced are conflicting. "Nautilus" tends to be considered an outgroup, with "Vampyroteuthis" forming an outgroup to other squid; however in one analysis the nautiloids, octopus and teuthids plot as a polytomy. Some molecular phylogenies do not recover the mineralized coleoids ("Spirula", "Sepia", and "Metasepia") as a clade; however, others do recover this more parsimonious-seeming clade, with "Spirula" as a sister group to "Sepia" and "Metasepia" in a clade that had probably diverged before the end of the Triassic.
Molecular estimates for clade divergence vary. One 'statistically robust' estimate has "Nautilus" diverging from "Octopus" at .
Taxonomy.
The classification presented here, for recent cephalopods, follows largely from Current Classification of Recent Cephalopoda (May 2001), for fossil cephalopods takes from Arkell et al. 1957, Teichert and Moore 1964, Teichert 1988, and others. The three subclasses are traditional, corresponding to the three orders of cephalopods recognized by Bather.
Class Cephalopoda († indicates extinct groups)
Other classifications differ, primarily in how the various decapod orders are related, and whether they should be orders or families.
Suprafamilial classification of the Treatise.
This is the older classification that combines those found in parts K and L of the "Treatise on Invertebrate Paleontology", which forms the basis for and is retained in large part by classifications that have come later.
Nautiloids in general (Teichert and Moore, 1964) sequence as given.
Paleozoic Ammonoidea (Miller, Furnish and Schindewolf, 1957)
Mesozoic Ammonoidea (Arkel et al., 1957)
Subsequent revisions include the establishment of three Upper Cambrian orders, the Plectronocerida, Protactinocerida and Yanhecerida; separation of the pseudorthocerids as the Pseudorthocerida, and elevating orthoceritoids as the Subclass Orthoceratoidea.
Shevyrev classification.
Shevyrev (2005) suggested a division into eight subclasses, mostly comprising the more diverse and numerous fossil forms, although this classification has been criticized as arbitrary.
Class Cephalopoda
Cladistic classification.
Another recent system divides all cephalopods into two clades. One includes nautilus and most fossil nautiloids. The other clade (Neocephalopoda or Angusteradulata) is closer to modern coleoids, and includes belemnoids, ammonoids, and many orthocerid families. There are also stem group cephalopods of the traditional Ellesmerocerida that belong to neither clade.
Monophyly of coeloids.
The coeloids have been thought to possibly represent a polyphyletic group, although this has not been supported by the rising body of molecular data.
Post-mortem decay.
After death, if undisturbed, cephalopods decay relatively quickly. Their muscle softens within a couple of days, and may swell; egg sacs can swell so much that they rip through the mantle. Subsequently, the organs shrink again; at this point the organism may start to break up into fragments. The eyes retain their size while the head shrinks around them. The gills may remain swollen at this point. After around a week, the carcass collapses in on itself and begins to disintegrate. The ink sac solidifies around this point. After a fortnight little is left but a blob with eyes, arms and ink sac visible. After a couple of months, these are only recognisable as flattened dark stains — although in some cases the eye lenses can remain intact for up to a year.
In popular culture.
Cephalopods, typically octopuses and squids, have been depicted commonly in Western pop culture as creatures that enjoy hugging or latching onto objects with their limbs and refusing to release. Some of the most notable uses of cephalopods in popular culture include Cthulhu, Squidward Tentacles, and the cephalopod-like robotic arms of Doctor Octopus.

</doc>
<doc id="42728" url="https://en.wikipedia.org/wiki?curid=42728" title="Signal reflection">
Signal reflection

Signal reflection occurs when a signal is transmitted along a transmission medium, such as a copper cable or an optical fiber. Some of the signal power may be reflected back to its origin rather than being carried all the way along the cable to the far end. This happens because imperfections in the cable cause impedance mismatches and non-linear changes in the cable characteristics. These abrupt changes in characteristics cause some of the transmitted signal to be reflected. In radio frequency (RF) practice this is often measured in a dimensionless ratio known as voltage standing wave ratio (VSWR) with a VSWR bridge. The ratio of energy bounced back depends on the impedance mismatch. Mathematically, it is defined using the reflection coefficient.
Because the principles are the same, this concept is perhaps easiest to understand when considering an optical fiber. Imperfections in the glass create mirrors that reflect the light back along the fiber.
Impedance discontinuities cause attenuation, attenuation distortion, standing waves, ringing and other effects because a portion of a transmitted signal will be reflected back to the transmitting device rather than continuing to the receiver, much like an echo. This effect is compounded if multiple discontinuities cause additional portions of the remaining signal to be reflected back to the transmitter. This is a fundamental problem with the daisy chain method of connecting electronic components. 
When a returning reflection strikes another discontinuity, some of the signal rebounds in the original signal direction, creating multiple echo effects. These forward echoes strike the receiver at different intervals making it difficult for the receiver to accurately detect data values on the signal. The effects can resemble those of jitter.
Because damage to the cable can cause reflections, an instrument called an electrical time-domain reflectometer (ETDR; for electrical cables) or an optical time-domain reflectometer (OTDR; for optical cables) can be used to locate the damaged part of a cable. These instruments work by sending a short pulsed signal into the cable and measuring how long the reflection takes to return. If only reflection magnitudes are desired, however, and exact fault locations are not required, VSWR bridges perform a similar but lesser function for RF cables.
The combination of the effects of signal attenuation and impedance discontinuities on a communications link is called insertion loss. Proper network operation depends on constant characteristic impedance in all cables and connectors, with no impedance discontinuities in the entire cable system. When a sufficient degree of impedance matching is not practical, echo suppressors or echo cancellers, or both, can sometimes reduce the problems.
The Bergeron Diagram method, valid for both linear and non-linear models, evaluates the reflection's effects in an electric line.

</doc>
<doc id="42730" url="https://en.wikipedia.org/wiki?curid=42730" title="Emory University">
Emory University

Emory University is a private doctoral university in metropolitan Atlanta, located in the Druid Hills section of unincorporated DeKalb County, Georgia, United States. The university was founded as Emory College in 1836 in Oxford, Georgia by the Methodist Episcopal Church and was named in honor of Methodist bishop John Emory. In 1915, the college relocated to metropolitan Atlanta and was rechartered as Emory University. The university is the second-oldest private institution of higher education in Georgia and among the fifty oldest private universities in the United States.
Emory University has nine academic divisions: Emory College of Arts and Sciences, Oxford College, Goizueta Business School, Laney Graduate School, School of Law, School of Medicine, Nell Hodgson Woodruff School of Nursing, Rollins School of Public Health, and the Candler School of Theology. Emory University, the Georgia Institute of Technology, and Peking University in Beijing, China jointly administer the Wallace H. Coulter Department of Biomedical Engineering. The university operates the Confucius Institute in Atlanta in partnership with Nanjing University. Emory has a growing faculty research partnership with the Korea Advanced Institute of Science and Technology (KAIST). Emory University students come from all 50 states, 6 territories of the United States, and over 100 foreign countries.
Emory Healthcare is the largest healthcare system in the state of Georgia and comprises seven major hospitals, including the internationally renowned Emory University Hospital and Emory University Hospital Midtown. The university operates the Winship Cancer Institute, Yerkes National Primate Research Center, and many disease and vaccine research centers. Emory university is one of four institutions involved in the NIAID's Tuberculosis Research Units Program and is the leading coordinator of the U.S. Health Department's National Ebola Training and Education Center. The International Association of National Public Health Institutes is headquartered at the university and the Centers for Disease Control and Prevention and the American Cancer Society are national affiliate institutions located adjacent to the campus. The university is partnered with the Carter Center.
Emory University is 18th among the list of colleges and universities in the United States by endowment, 21st among universities in the world by endowment, and 21st in "U.S. News & World Report's" 2016 National Universities Rankings. Emory University has a Carnegie Classification of Institutions of Higher Education status of R1: "highest research activity" and is cited for high scientific performance and citation impact in the CWTS Leiden Ranking. The National Science Foundation ranked the university 36th among academic institutions in the United States for research and development (R&D) expenditures. Emory University research is funded primarily by federal government agencies, namely the National Institutes of Health (NIH). In 1995 Emory University was elected to the Association of American Universities, an association of the 62 leading research universities in the United States & Canada.
History.
Nineteenth century.
Emory College was founded in 1836 in Oxford, Georgia by the Methodist Episcopal Church. The college was named in honor of the departed Methodist bishop John Emory. Ignatius Alphonso Few was the college's first president. In 1854, the Atlanta Medical College, a forerunner of Emory University School of Medicine, was founded. On April 12, 1861, the American Civil War began. Emory College was closed in November 1861 and all of its students enlisted. In late 1863 the war came to Georgia and the college was used as hospital and later a headquarters for the Union Army. Thirty five Emory students lost their lives and much of the campus was destroyed during the war.
Emory College, as with the entire Southeastern United States, struggled to overcome financial devastation during the Reconstruction Era. In 1880, Atticus Greene Haygood, Emory College President, delivered a speech expressing gratitude for the end of slavery in the United States, which captured the attention of George I. Seney, a New York banker. Seney gave Emory College $5,000 to repay its debts, $50,000 for construction, and $75,000 to establish a new endowment. In the 1880s, the technology department was launched by Isaac Stiles Hopkins, a polymath professor at Emory College. Hopkins became the first president of the Georgia Institute of Technology in 1888. Emory University's first international student, Yun Chi-ho, graduated in 1893. Yun became an important political activist in Korea and is the author of "Aegukga", the national anthem of the Republic of Korea.
Twentieth century.
On August 16, 1906, the Wesley Memorial Hospital and Training School for Nurses, later renamed the Nell Hodgson Woodruff School of Nursing, was established. In 1914, the Candler School of Theology was established. In 1915, Emory College relocated to metropolitan Atlanta and was rechartered as Emory University after accepting a land grant from Asa Griggs Candler, founder of the The Coca-Cola Company. The Emory University School of Law was established in 1916. From the 1920s through the 1970s, Emory University established its reputation as a regional institution that offered a solid education in medicine, law, theology, business, and the liberal arts.
First and Second World Wars.
On April 6, 1917 the United States entered the First World War. Emory University organized a medical unit, composed of medical school faculty and medical alumni, that would be known as Emory Unit, Base Hospital 43. The unit served in Loir-et-Cher, France from July 1918 to January 1919. The Emory Unit, Base Hospital 43 was remobilized during the Second World War and served in the North African Campaign and Europe. To recognize Emory’s participation in the war effort, a ship was christened M.S. Emory Victory and served through World War II and in the Korean War.
In the 1940s, Emory University students, alumni, and faculty served in the Asia-Pacific War and European theater of World War II. Bobby Jones (golfer), served during the Battle of Normandy. Dr. Alfred A. Weinstein, a professor of surgery at Emory University School of Medicine, was a prisoner of war of the Empire of Japan between 1942 and 1945. His memoir "Barbed Wire Surgeon" is considered one of the finest accounts concerning allied prisoners under Japanese captivity and highlights the abuses of the war criminal Mutsuhiro Watanabe. Kiyoshi Tanimoto, who graduated from the Candler School of Theology in 1940 and is portrayed in John Hersey's Hiroshima (book), was able to organize the Hiroshima Maidens reconstructive surgery program based on the associations he made while studying in the United States. Tatsumasa Shirakawa, a Japanese student at the Candler School of Theology, was placed under arrest temporarily until Dean Henry Burton Trimble negotiated his release. Emory helped the nation prepare for war by participating in the V-12 Navy College Training Program and Army Specialized Training Program, programs designed to supplement the force of commissioned officers in the United States Navy and United States Army. The Candler School of Theology trained men for military chaplaincy. During the war, university enrollment boasted two military students for every one civilian. Emory University alumni would go on to serve in the Korean War, Second Indochina War (Vietnam War), Persian Gulf War, Yugoslav Wars, and the Global War on Terrorism.
Women's and Civil Rights Movement.
The Women's Movement and Civil Rights Movement during the 1950s and 1960s in the United States profoundly shaped the future of Emory University. Formerly an all-male school, Emory officially became a coeducational institution in 1953. Although it had previously admitted women under limited circumstances, the university had never before had a policy through which they could enroll in large numbers and as resident students. In 1959, sororities first appeared on campus. In 1962, in the midst of the Civil Rights Movement, Emory embraced the initiative to end racial restrictions when it asked the courts to declare portions of the Georgia statutes unconstitutional. Previously, Georgia law denied tax-exempt status to private universities with racially integrated student bodies. The Supreme Court of Georgia ruled in Emory's favor and Emory officially became racially integrated. Marvin S. Arrington, Sr. was Emory University's first, full-time African American student and graduated from Emory University School of Law in 1967.
Emory's diversity and academic reputation flourished under the leadership of the university's fifth president, James T. Laney. In addition to leading universities in the Southeastern United States in the promotion of racial equality, Laney and many of the school's faculty and administrators were outspoken advocates of global human rights and thus were openly opposed to the military dictatorship in South Korea (1961-1987). On March 30, 1983, Laney's friend Kim Dae-jung, while in political exile in the United States, presented a speech on human rights and democracy at Emory University and accepted an honorary Doctor of Laws degree. Kim would go on to play a major role in ending authoritarianism in South Korea, served as the 8th President of South Korea from 1998 to 2003, and was awarded the Nobel Peace Prize in 2000 for his successful implementation of the Sunshine Policy. Laney would later serve as United States Ambassador to South Korea and the Emory graduate school, founded in 1919, was named in his honor in 2009.
In 2005, the university presented the President Medal, an rare award conferred only on individuals whose impact on the world has enhanced the dominion of peace or has enlarged the range of cultural achievement, to Civil Rights Movement activist Rosa Parks. The award is one of the highest honors presented by Emory.
In 2014, at Emory’s 169th Commencement, John Lewis, the only living "Big Six" leader of the Civil Rights Movement, delivered the keynote address and received an honorary doctor of laws degree. In 2015, Emory University School of Law received a $1.5 million donation to help establish a John Lewis Chair in Civil Rights and Social Justice. The gift, given anonymously, funds a professorship which will enable Emory Law to conduct a national search for a scholar with an established academic profile of distinction and a demonstrated desire to promote the rule of law through the study of civil rights. The law school has committed to raise an additional $500,000 to fund the chair fully.
Expansion and modernization.
The course of Emory's history changed dramatically in November 1979 when Robert Winship Woodruff and George Waldo Woodruff presented the institution with a gift of $105 million in Coca-Cola stock. At the time this was the largest single gift to any institution of higher education in American history, and it made a profound impact on Emory's direction in the next two decades, boosting the university to the top ranks of American research universities.
Twenty-first century.
As one of the fastest-growing research universities in the United States in the 21st century, Emory University has established a national reputation on the strength of the scholarly achievements of its faculty and students, its highly ranked professional schools, a long-term commitment to the arts and sciences, and the presence of more than seventy cutting-edge research centers that are addressing major social problems. Emory has extended its ties to the community, creating close links with Atlanta's neighborhoods, clinics, hospitals, nonprofit organizations, and boardrooms. To accommodate its growth, Emory has undergone a physical transformation that has increased classroom and research space. The latest additions to the campus include buildings for cancer research, biomedical research, scientific computation, mathematics and science, vaccine research, and the performing arts.
Academics.
Undergraduate schools.
The Emory College of Arts and Sciences offers the Bachelor of Arts (B.A.) and the Bachelor of Science (B.S) undergraduate academic degrees. Academic Departments include African American Studies, African Studies, American Studies, Ancient Mediterranean Studies, Anthropology, Art History, Biology, Chemistry, Classics, Comparative Literature, East Asian Studies, Economics, English, Environmental Sciences, Film & Media Studies, French and Italian Studies, German Studies, Global Health, Culture, and Society, History, Human Health, Jewish Studies, Latin American and Caribbean Studies, Linguistics, Mathematics and Computer Science, Middle Eastern and South Asian Studies, Music, Neuroscience and Behavioral Biology, Philosophy, Physics, Political Science, Psychology, Quantitative Theory and Methods, Religion, Russian and East Asian Languages and Cultures, Sociology, Spanish and Portuguese, Theater and Dance, and Women's, Gender and Sexuality Studies. The Confucius Institute, a non-profit public institution affiliated with the Ministry of Education of the People's Republic of China, operates in co-operation with the university at the Emory College of Arts and Sciences. The Emory-Tibet Partnership was established in 1998.
Emory University offers a five-year dual degree program in engineering, in collaboration with the Georgia Institute of Technology. Emory University also offers a dual master's degree in social work with the University of Georgia.
Oxford College offers an Associate degree (A.A.) in liberal arts. Students that successfully complete Oxford College advance to Emory College of Arts and Sciences to complete their undergraduate education. Academic Departments include Anthropology, Art, Biology, Chemistry, Economics, English, Geology, History, Languages, Mathematics & Computer Science, Music, Political Science, Philosophy, Psychology, Physics & Astronomy, Quantitative Theory and Methods, Religion, Sociology, Theater, and Women's Studies.
Graduate and professional schools.
The Emory University School of Medicine offers the Doctor of Medicine (MD), Doctor of Physical Therapy, Master of Medical Science in Anesthesiology, Master of Medical Science in Human Genetics & Genetic Counseling, Master of Medical Science in Physician Assistant, and Bachelor of Medical Science in Medical Imaging. Academic Departments include Biochemistry, Biomedical Engineering, Biomedical Informatics, Cell Biology, Human Genetics, Microbiology/Immunology, Pharmacology, and Physiology. Clinical Science Departments include Anesthesiology, Dermatology, Emergency Medicine, Family & Preventive Medicine, Gynecology/Obstetrics, Hematology/Medical Oncology, Neurology, Neurosurgery, Ophthalmology, Orthopaedics, Otolaryngology, Pathology, Pediatrics, Psychiatry & Behavioral Sciences, Radiation Oncology, Radiology, Rehabilitation Medicine, Surgery, and Urology.
The Nell Hodgson Woodruff School of Nursing offers the Bachelor of Science in Nursing (BSN), Masters of Science in Nursing, and Doctor of Nursing Practice (DNP).
The Candler School of Theology offers the Master of Divinity (MDiv), Master of Religious Leadership (MRL), Master of Religion and Public Life (MRPL), Master of Theological Studies (MTS), Master of Theology (ThM), Doctor of Theology in Pastoral Counseling (ThD), and Doctor of Ministry (DMin), an online degree.
The Emory University School of Law offers the Juris Doctor, Juris Master, Master of Laws, and Doctor of Juridical Science.
The Laney Graduate School offers the Master of Arts degree in Bioethics, Clinical Research, Computer Science and Informatics, Development Practice, Educational Studies, Film Studies, Mathematics, and Music. The school offers the Doctor of Philosophy in Anthropology, Art History, Behavioral Sciences and Health Education, Biochemistry, Cell and Developmental Biology (GDBBS), Biomedical Engineering, Biostatistics, Business, Cancer Biology (GDBBS), Chemistry, Clinical Psychology, Cognition and Development (Psychology), Comparative Literature, Computer Science and Informatics, Economics, Educational Studies, English, Environmental Health Sciences, Epidemiology, French, Genetics and Molecular Biology (GDBBS), Health Services Research and Health Policy, History, Immunology and Molecular Pathogenesis (GDBBS), Islamic Civilizations Studies, Mathematics, Microbiology and Molecular Genetics (GDBBS), Molecular and Systems Pharmacology (GDBBS), Neuroscience (GDBBS), Neuroscience and Animal Behavior (Psychology), Nursing, Nutrition and Health Sciences (GDBBS), Philosophy, Physics, Political Science, Population Biology, Ecology and Evolution (GDBBS), Religion, Sociology, Spanish, and Women's, Gender, and Sexuality Studies.
The Goizueta Business School offers the Bachelor of Business Administration, Master of Business Administration, Executive Master of Business Administration, and a Doctor of Philosophy in Business Administration.
The Rollins School of Public Health offers the Master of Public Health (MPH) and Master of Science in Public Health (MSPH). Academic Departments include Behavioral Sciences & Health Education, Biostatistics & Bioinformatics, Environmental Health, Epidemiology, Global Health, and Health Policy & Management.
Library system.
Emory University is a member of the Association of Research Libraries. The Emory University library system includes over 3.9 million print and electronic volumes and 83,000-plus electronic journals. Emory University libraries include the Robert W. Woodruff Library, Woodruff Health Science Center Library, Hugh F. MacMillan Law Library, James S. Guy Chemistry Library, Pitts Theology Library, Goizueta Business Library, Marian K. Heilbrun Music & Media Library, and the Manuscript, Archives, and Rare Book Library (MARBL). MARBL, contains rare materials relating to literature, African American history and culture, and Southern and Georgia history.
Subject specialist librarians provided research assistance for every academic department at the university. The Annual Robert W. Woodruff Library Undergraduate Research Award recognizes undergraduate students who make extensive use of Woodruff Library’s collections and research resources in their original scholarship and show evidence of critical analysis in their research skills.
In 2012, the Princeton Review ranked the Robert W. Woodruff Library among the top 10 "Best College Libraries" in the United States. In 2013, the Pitts Theology Library of the Candler School of Theology was named as one of "Most Beautiful College Libraries in the World."
Reputation and rankings.
Emory University is currently ranked 21st among national universities in the United States by U.S. News and World Report and 90th among global universities in the Times Higher Education World University Rankings. However, in 2012 it was revealed that Emory University had intentionally misreported data to rankings institutions for more than a decade. Both SAT/ACT and class rank were overstated. For example, while Emory reported that 89% of its students graduated in the top 10% of their class, only 75% actually had. The university has been named both a Hidden Ivy and Southern Ivy. Business Insider named Emory among the "50 smartest colleges in America." Emory is considered to have one of the best writing programs in the United States and was ranked 1st among the list of the best colleges and universities for writers by "The Huffington Post" and "USA Today". Emory University's programs consistently rank among the most competitive in their fields by "U.S. News and World Report". In 2015, the Wallace H. Coulter Department of Biomedical Engineering Program was ranked 2nd in the United States for the ninth consecutive year. The Emory University School of Medicine was ranked the 23rd Best Medical Research School in the United States in 2015. Rollins School of Public Health was ranked 7th among public health schools in the United States in 2015. The Emory University School of Medicine Physician Assistant Program was ranked 3rd among physician assistant programs in the United States in 2015. Emory University's Nell Hodgson Woodruff School of Nursing was ranked 10th among Nursing Schools in the United States in 2015. The university is ranked the 16th best college for veterans among national universities in the United States.
Emory University is ranked 13th in Immunology, 22nd in Microbiology, 28th in Psychiatry, 29th in Social Sciences and Public Health, 32nd in Clinical Medicine, 37th in Neuroscience and Behavior, 45th in Pharmacology and Toxicology, 50th in Biochemistry, and 67th in Molecular Biology and Genetics in the world by U.S. News and World Report Emory University is ranked 6th among national universities in the United States in Social Psychology, 11th in Behavioral Neuroscience, 18th in Clinical Psychology, 25th in Political Science, 26th in English, 27th in History, 30th Biological Sciences, 35th in Chemistry, 35th in Sociology, 38th in Psychology, 38th in Statistics, 64th in Economics, 65th in Mathematics, 85th in Physics by U.S. News and World Report. The Emory University School of Law is ranked 19th among Law Schools in the United States by "U.S. News and World Report". The Princeton Review named the Emory University School of Law as one of best 169 law schools in the United States in 2014. Emory University's Goizueta Business School is ranked 20th among Business Schools in the United States by U.S. News and World Report. Bloomberg Businessweek ranked Goizueta Business School's BBA Program 9th in the nation in 2014. The Economist ranked Goizueta Business School's MBA program 13th in the nation in 2014.
Research.
Emory University has a Carnegie Classification of Institutions of Higher Education status of RU/VH: "very high research activity". According to The Chronicle of Higher Education, the university is 5th among universities in the United States with licensing revenue per dollars spent on research. The university is the 4th largest contributor in the nation to the discovery of new drugs and vaccines among public-sector research institutions. The Universities Allied for Essential Medicines, ranked Emory 6th among universities in the United States and Canada for global health contributions and research. In 2015, Emory received $572.4 million from external funding agencies. Emory University leads the nation in the number of students with Kirschstein-National Research Service Award pre-doctoral fellowships from the National Institutes of Health.
Emory University has a strong partnership with the Centers for Disease Control and Prevention (CDC). In 1947, the university donated 15 acres of land to the United States Department of Health and Human Services for the construction of the CDC headquarters. The Emory University Prevention Research Center (EPRC) and Emory Center for Injury Control are funded by the CDC. Emory University's African Center of Excellence for Public Health Security, which seeks to improve preparedness and response to health threats in low-income countries, is a five-year, multimillion-dollar cooperative program with the CDC and International Association of National Public Health Institutes (IANPHI). The Emory University Center for Global Safe Water (CGSW), which conducts applied research, evaluation, and training to promote global health equity through universal access to safe water, sanitation, and hygiene, works in collaboration with the CDC. The Emory University Global Health Institute, funded by the Bill & Melinda Gates Foundation, partners with the CDC to enhance public health infrastructure in low-resource countries. The Emory University Hospital Isolation Unit and Quarantine Station was established by the CDC following the 2003 SARS outbreak. The isolation and treatment facilities at Emory University played a crucial role in ending the 2014 Ebola virus cases in the United States. CDC scientists and administrators hold memberships and frequently speak at Emory University's Vaccine Dinner Club (VDC), an association that holds monthly academic meetings to discuss and advance vaccine research. In 2015, Emory was made a member of the CDC's Prevention Epicenters Program, a research program in which CDC’s Division of Healthcare Quality Promotion (DHQP) collaborates with academic investigators to conduct innovative infection control and prevention research.
In 2015, Emory University, the London School of Hygiene & Tropical Medicine, the Public Health Foundation of India, and the All India Institute of Medical Sciences established the Center for Control of Chronic Conditions in New Delhi, India. The center aims to improve the prevention and care of diabetes, heart disease, cancer, mental health, and injuries in India.
The International Association of National Public Health Institutes is based at the university. The association was chartered in 2006 with a $20 million, five-year grant through Emory University from the Bill and Melinda Gates Foundation. In 2015, the Emory Global Health Institute and Centers for Disease Control and Prevention were made lead partners for the newly created, $75 million Bill and Melinda Gates Foundation funded Child Health and Mortality Prevention Surveillance Network (CHAMPS).
Emory University research is heavily funded by the United States Department of Health and Human Services's National Institutes of Health. The federal agency awarded the university nearly $300 million in the fiscal year of 2015. In 2015, Emory University was one of four institutions selected by the National Institute of Allergy and Infectious Diseases for its seven-year, multimillion-dollar Tuberculosis Research Units (TBRU) program, which aims to drive innovation in tuberculosis research and reduce the global burden of the disease. In 2015, an Emory-led research consortium received a five-year, 15 million dollar grant from the National Institutes of Health (NIH) to research human immune responses to Varicella zoster virus and pneumococcal vaccination. The university also received a $9 million grant over five years from the NIH to support one of three national Centers for Collaborative Research in Fragile X syndrome. The grant is a renewal of Emory’s National Fragile X Research Center, continuously funded by the NIH for more than 10 years. In 2015, the university received a $8.9 million grant over five years from the NIH National Heart, Lung and Blood Institute(NHLBI) to better understand the role of reactive oxygen species and inflammation in blood vessel function and to explore new interventions and preventive approaches for atherosclerosis and aortic aneurysms. In 2015, the university received a $8 million grant over five years from the NIH to develop and validate mathematical models of how prior immunity affects recall immune responses to influenza viruses. The researchers will create and disseminate powerful, user-friendly modeling tools for use by the wider research community in developing more effective vaccines. In 2015, the university received a $3.6 million grant over five years from the NIH to examine the effects of maternal stress on brain function, development, and behavior in African-American infants, including the biochemical connection between the brain and the microbiome. In 2015, the university received a $3.5 million grant over five years from the NIH National Cancer Institute (NCI) for an Informatics Technology for Cancer Research award. Winship Cancer Institute and Emory School of Medicine researchers will develop software tools to help the cancer research community gain new insights from cancer imaging “big data” and develop new open-source cancer research applications. In 2015, the university received a $3.4 million grant from the NIH International Collaborations in Infectious Disease Research Program to support a partnership between the Emory Vaccine Center and the International Centre for Genetic Engineering and Biotechnology (ICGEB) in New Delhi, India to study dengue virus infection in India.
The Emory University Center for AIDS Research (CFAR) and the Emory Vaccine Center are world leaders in AIDS Vaccine Development and HIV Parthenogenesis studies are funded by nine different institutes of the National Institutes of Health and by the Georgia Research Alliance. The centers include one of the largest groups of academic vaccine scientists in the world and are currently attempting to develop an effective HIV vaccine. Emory University Researchers Dr. Dennis C. Liotta, Dr. Raymond F. Schinazi and Dr. Woo-Baeg Choi discovered Emtricitabine, a nucleoside reverse transcriptase inhibitor (NRTI) used in the treatment of HIV. The drug was named as one of the world's most important antiviral drugs by the World Health Organization and is included in their Model List of Essential Medicines.
Emory University is a global leader in Ebola research and treatment. The university was one of three institutions that successfully treated medical evacuees during the 2014 Ebola Outbreak. In 2015, the United States Department of Health and Human Services named Emory University the lead coordinating center for the National Ebola Training and Education Center (NETEC). The university will collaborate with the University of Nebraska Medical Center, the New York City Health and Hospitals Corporation, the Centers for Disease Control and Prevention and the Office of the Assistant Secretary for Preparedness and Response on the program, which will receive $12 million in funding over the next five years.The program will support training of health care providers and facilities to manage Ebola and other emerging infectious diseases. Its objectives are to develop metrics to measure facility and health care worker readiness to care for Ebola patients, conduct assessments of state and regional Ebola Treatment Centers, create education materials related to care of patients with possible Ebola and other special pathogens and offer technical and training assistance to public health departments and health facilities. The university also received a $10.8 million grant over three years from the U.S. Department of Defense's Defense Advanced Research Projects Agency (DARPA) to lead a 10-institution national team developing improved therapeutics and vaccines for multiple strains of Ebola virus. In 2015, Emory received a three-year, $2.2 million grant from the CDC to prevent the spread of infectious diseases, including Ebola, in health-care facilities.
Emory and the Georgia Institute of Technology have a strong research partnership. In 2015, Emory and Georgia Tech were awarded an $8.3 million grant by the National Institutes of Health (NIH) to establish a National Exposure Assessment Laboratory. The Laboratory will research the impact of environmental chemicals on children's health. In 2015, the two universities received a five-year, $2.9 million grant from the National Science Foundation (NSF) to create new bachelor’s, master’s, and doctoral degree programs and concentrations in healthcare robotics, which will be the first program of its kind in the Southeastern United States. In 2015, Emory University, Georgia Tech, and Children’s Healthcare of Atlanta were awarded a four-year, $1.8 million grant by the Cystic Fibrosis Foundation in order to expand the Atlanta Cystic Fibrosis Research and Development Program. As of 2015, Emory jointly manages the second largest cystic fibrosis population in the United States. In 2015, Emory and Georgia Tech received a $1.6 million grant from the Coulter Translational Research Partnership Program to accelerate nine promising technologies developed in research laboratories with commercialization potential.
In 2015, Emory University received a $15 million grant from the Wounded Warrior Project in order to establish the "Warrior Care Network" and develop innovative approaches to treat veterans with post-traumatic stress disorder (PTSD) and traumatic brain injury (TBI).
In 2015, Emory University and the University of South Florida received a $2.5 million grant over five years from the John E. Fogarty International Center (FIC) to study links between infectious disease transmission and agricultural practices.
Campus.
The Emory University main campus, established in the early twentieth century, covers more than 600 acres in Atlanta's historic suburb of Druid Hills. The university campus is heavily forested with pine, maples, oak, and magnolias, and Peavine Creek, a branch of the Peachtree Creek, runs through the campus. The Arbor Day Foundation named Emory a Tree Campus USA school in 2015. Many of the university's buildings are designed with multi-hued granite and Spanish Saltillo tile. The university has one of the largest inventories by square footage of Leadership in Energy and Environmental Design-certified building space among campuses in the United States. In 2015, The Best Colleges, a Seattle-based educational ranking service, named Emory one of the top ten "Most Amazing College Campuses."
The campus is home to the Emory University Hospital, Michael C. Carlos Museum, which has the largest collection of ancient artifacts in the Southeastern United States, the Winship Cancer Institute, Georgia’s first and only cancer center designated by the National Cancer Institute, the Yerkes National Primate Research Center, one of eight National Institutes of Health–funded national primate research centers, and a number of other academic, art, medical, and student facilitates. Undergraduate dormitories include the Undergraduate Residential Center, Clairmont Residential Center, Tower Apartments, Alabama Hall, Complex, Dobbs Hall, Holmes Hall, Longstreet-Means Hall, Raoul Hall and Turman Hall. The Centers for Disease Control and Prevention, American Cancer Society, Children's Healthcare of Atlanta Egleston hospital, and Emory Point are located adjacent to the campus.
In 2015, a $52 million expansion and renovation project of the Sanford S. Atwood Chemistry Center was completed. The new, 270,000 square-foot complex features state of the art laboratories, interactive teaching and study spaces, and a chemistry library. The completion of the complex was accompanied by a $1.2 million grant from the Howard Hughes Medical Institute to advance and modernize the university's chemistry curriculum.
In the Candler Library Annex of Robert W. Woodruff Library, there is a 1920s Pietro Caproni reproduction of Bertel Thorvaldsen's "The Triumph of Alexander" frieze. The frieze depicts Alexander the Great and his army entering Babylon following their victory over the Achaemenid Empire in the Battle of Gaugamela.
During the 1996 Summer Olympics in Atlanta, Georgia, the university hosted the United States Olympic women's gymnastics team on its main campus. The team, known as the Magnificent Seven, won the first ever gold medal for the United States in the women's team all-around competition. The university housed international officials and journalists and served as a training facility for Olympians. The Cox Hall Ballroom was transformed into a news center for the Olympic foreign press.
Emory University's original campus was established in Oxford, Georgia in 1836. The 56-acres campus is home to Oxford College of Emory University and was the site of military headquarters and infirmaries during the American Civil War. Many of the buildings were designed with Neoclassical architecture and Gothic Revival architecture. In 1975, the United States National Register of Historic Places designated the campus as part of the Oxford Historic District.
Student life.
Student Body.
Emory University's total enrollment for the 2015-2016 academic year is 14,724 students, with 7,803 undergraduates and 6,921 graduate and professional students. Students come from all 50 states and more than 65 countries. The student to faculty ratio is 7:1, with an average class size of 25 students. Of the 1,389 students in the Class of 2018, 46% are Caucasian, 31% are Asian, 10% are Black/African American, 9% are Latino/Hispanic, and 3% did not identity; 56% are female and 44% are male. Emory University has awarded 4,568 degrees during the 2014-2105 year. Of those awards 2,366 were undergraduate and 2,202 were graduate. Emory University's average GPA for incoming freshman is a 3.70-3.97.
Arts.
Students may engage in the performing and fine arts as an area of academic study or as extracurricular activities. Undergraduates may pursue a major in the performing arts (dance, theater, or music) or in film studies, art history, visual arts, or creative writing. Graduate programs in art history, film studies, and music are offered. There are more than 50 student organizations dedicated to the arts. Students can explore artistic interests as diverse as architecture, breakdancing, poetry, and improvisational comedy. Emory routinely hosts arts events in the Schwartz Center for Performing Arts that are open to the Emory and Atlanta communities. Recent performances include Bang on a Can All-Stars (a side project of drummer Glenn Kotche from the rock band Wilco), jazz performer Esperanza Spalding, and New York’s Cedar Lake Dance Company. A program called Creativity Conversations brings artistic minds to campus to discuss art and the creative process. Guests have included Philip Glass, Jimmy Carter, Salman Rushdie, Seamus Heaney and Rita Dove. Rita Dove also gave the keynote address at Emory's 2013 Commencement.
Athletics.
Emory ranks among top schools in both the "U.S. News & World Report’s" rankings of the best national universities and the Directors Cup of the National Association of Collegiate Directors of Athletics for best all-around athletics program. Emory's 18 varsity sports teams, known as the Eagles, are members of the NCAA’s Division III University Athletic Association (UAA). However, Emory does not have an intercollegiate football team.
Barkley Forum.
The Barkley Forum Center for Debate Education is an intercollegiate debate organization at Emory University. The center is named in honor of Emory alumnus, Alben Barkley, 35th Vice President of the United States, and is directed by Melissa Wade. Debating was established at the university in 1837 and the intercollegiate debate team was formed by Nolan A. Goodyear in 1914. Emory's Barkley Forum debate team has won 3 National Debate Tournaments and over 25 individual champion speaker awards.
Community service.
The university received the 2008 Presidential Award for General Community Service, which is the highest federal recognition given to higher education institutions for their commitment to community service, service-learning and civic engagement. About 25% of Emory students participate in Volunteer Emory, Emory's umbrella community service group. As one of the most popular groups on campus, Volunteer Emory offers dozens of ways to serve the community, working with varied organizations including the Atlanta Community Food Bank, Trees Atlanta, PAWS Atlanta, and Jones Boys and Girls Club. Emory Cares International Service Day brings together students, alumni and other community members to volunteer at a number of projects organized by Emory and its many partners around the city of Atlanta and in cities worldwide.
Newspaper.
"The Emory Wheel" is the student-run newspaper of Emory University. The "Wheel" is published twice a week, on Tuesday and Friday, during the regular school year, and is updated regularly at its website. Serving the Emory community since 1919, the "Wheel" is editorially and financially independent from the University. The staff is composed entirely of students, with the exception of the general manager, who oversees advertising and whose salary is paid by the newspaper.
Programs abroad.
Through the Centers of International Programs Abroad, Emory University students can study in over 40 countries at the top academic institutions in the world including the National University of Singapore, Kyoto Consortium for Japanese Studies, Nanjing University, Oxford University, Imperial College London, the School of Oriental and African Studies, Yonsei University, Trinity College Dublin, University of St. Andrews, University of Melbourne, University of Amsterdam, University of Cape Town, and Tel Aviv University.
Residential life.
Fraternities have existed on Emory's campus as early as 1840. One early chronicler makes the case that Emory's "temple" of the Mystic Seven may have been the first chapter of a national fraternity established anywhere in the South. Today, the Greek-letter sororities and fraternities play an important part in leavening Emory's campus life. For undergraduates, Greek life comprises approximately 30% of the Emory student population. The Office of Greek Life recognizes and regulates on-campus chapters of fraternities and sororities. Fraternities have on-campus housing located on Eagle Row, and Sorority Village, a series of townhouses, faces the fraternity houses. Greek Life is an important social engagement for students, but it is not totally exclusive—students from different sororities and fraternities regularly socialize, and the college's emphasis on on-campus housing helps students make friends inside and outside the Greek system.The intramural sports program provides an athletic outlet for the entire Emory community. Emory has numerous club sports and a variety of recreational and competitive intramural teams. The Outdoor Emory Organization sponsors weekend trips of outdoor activities such as rafting, rock climbing and hiking.
Student organizations.
Hundreds of student clubs and organizations operate on Emory's campus. These include numerous student government, special interest, and service organizations.The Student Government Association (SGA) charters and provides most of the funding for other student groups, and represents students' interests when dealing with the administration. The SGA oversees divisional councils, each coinciding with the undergraduate, graduate and professional schools of the university. Notable among these are the College Council (CC) which handles students concerns primarily for the undergraduate body of the Emory College of Arts and Sciences and annually sponsors the State of Race event, and the BBA Council which does similar activities for the Goizueta Business School BBA Program. The Student Programming Council (SPC) is the school's primary programming organization, responsible for planning five events every year: Homecoming Week, Fall Band Party, Spring Band Party, Swoopstock and Dooley's Week. Emory also has several secret societies—the Paladin Society, the D.V.S. Senior Honor Society, Ducemus, Speculum, and The Order of Ammon. Emory has a partnership with Coca-Cola in which they pledged 3 million dollars over a 5-year period for "Service for Learning" which projects that Emory student volunteers participate to help preserve nature trails, create urban farms, as well as restore neighborhood parks.
Notable alumni and faculty.
Emory University has over 13,200 faculty and staff members and over 133,000 living alumni. Awards and honors recognizing Emory alumni and faculty include the Nobel Prize, Pulitzer Prize, Presidential Medal of Freedom, Bancroft Prize, Booker Prize, Lenore Marshall Poetry Prize, National Humanities Medal, Peabody Award, Breakthrough Prize in Life Sciences, Guggenheim Fellowship, Fulbright Fellowship, American Mathematical Society Fellowship, MacArthur Fellows Program, Rhodes Scholarship, Marshall Scholarship, and membership in the American Academy of Arts and Sciences, Carnegie Foundation for the Advancement of Teaching, Howard Hughes Medical Institute, American Society for Clinical Investigation, National Academy of Sciences, and National Research Council.
Notable Alumni: Alben Barkley (BA 1900), 35th Vice President of the United States, Isaac Stiles Hopkins (1859C) and Robert Stewart Hyer (BA 1881, MA 1882), founding presidents of Georgia Institute of Technology and Southern Methodist University, respectively, Young John Allen (1858C), American Methodist Missionary in the late Qing Dynasty, China, Thomas Milton Rivers (1909C), Director of the Rockefeller Institute, Ernest Cadman Colwell (1923C, 1927PhD), President of the University of Chicago, Bobby Jones (Law 1929), the only golfer to win a Grand Slam, founder of the Masters Golf Tournament, and regarded as one of the greatest golfers of all time, Ely Callaway Jr. (1940C), Founder of the Callaway Golf Company, Ernie Harwell (1940C), baseball broadcaster for the Detroit Tigers, Arnall Patz (BA 1943, MD 1945), ophthalmology researcher and Presidential Medal of Freedom recipient, Lewis Roger Slaton (1949JD), District Attorney, Fulton County, Georgia, Lee Hong-koo (1959C), 26th Prime Minister of the Republic of Korea, Newt Gingrich (BA 1965), 58th Speaker of the House of Representatives, Sonny Carter, NASA astronaut, Crew member of STS-33 Space Shuttle mission (1969C), Peter Buck guitarist for the band R.E.M., Kenneth Cole (BA 1976), clothing designer and founder of Kenneth Cole Productions, Christopher McCandless (1990C), Alaskan wilderness adventurer. Main subject of Jon Krakauer's Into the Wild, Fala Chen (2005C), Chinese American Actress, Kirsten Haglund (2013C), Miss America 2008, Duncan L. Niederauer, Chief Executive Officer of the New York Stock Exchange (NYSE).
Honorary Alumni: Kim Dae-jung, 8th President of South Korea, Vicente Fox, Former President of Mexico, Mikhail Gorbachev, Former President of the Soviet Union, Nobel Peace Prize recipient, Arnold Schwarzenegger, Austrian-American actor, 38th Governor of California, Tom Brokaw, American journalist, author of "The Greatest Generation" (1998), John Lewis, "Big Six" leader of the Civil Rights Movement, Henry Louis Gates, Jr., Emmy Award-winning filmmaker.
Distinguished faculty: Jimmy Carter, 39th President of the United States, Sir Salman Rushdie, Booker Prize-winning novelist, Tenzin Gyatso, the 14th Dalai Lama, Desmond Tutu, Nobel Peace Prize recipient, William Foege, 10th Centers for Disease Control and Prevention Director, Nathan McCall, New York Times bestselling author, James T. Laney, 17th President of Emory University, United States Ambassador to Korea from 1993 to 1997, Natasha Trethewey, Pulitzer Prize winner and US Poet Laureate and Dr. Sanjay Gupta, CNN chief medical correspondent.

</doc>
<doc id="42731" url="https://en.wikipedia.org/wiki?curid=42731" title="Dodoni">
Dodoni

Dodoni () is a village and a municipality in the Ioannina regional unit, Epirus, Greece. The seat of the municipality is the village Agia Kyriaki. Dodoni is located near the ancient city and site of the ancient oracle of Dodona. "Oedipus the King" was shot here in 1967.
Municipality.
The present municipality Dodoni was formed at the 2011 local government reform by the merger of the following 4 former municipalities, that became municipal units:

</doc>
<doc id="42734" url="https://en.wikipedia.org/wiki?curid=42734" title="Transcendental Meditation">
Transcendental Meditation

Transcendental Meditation (TM) refers to a specific form of mantra meditation called the Transcendental Meditation technique, and less commonly to the organizations that constitute the Transcendental Meditation movement. Maharishi Mahesh Yogi (1918–2008) introduced the TM technique and TM movement in India, in the mid-1950s.
The Maharishi taught thousands of people during a series of world tours from 1958 to 1965, expressing his teachings in spiritual and religious terms. TM became more popular in the 1960s and 1970s, as the Maharishi shifted to a more technical presentation, and his meditation technique was practiced by celebrities. At this time, he began training TM teachers and created specialized organizations to present TM to specific segments of the population such as business people and students. By the late 2000s, TM had been taught to millions of people, and the worldwide TM organization had grown to include educational programs, health products, and related services.
The TM technique involves the use of a sound or mantra, and is practiced for 15–20 minutes twice per day. It is taught by certified teachers through a standard course of instruction, which costs a fee that varies by country. According to the Transcendental Meditation movement, it is a method for relaxation, stress reduction, and self-development. Varying views on whether the technique is religious or non-religious have been expressed including by sociologists, scholars, and a New Jersey court case.
TM is one of the most widely practiced and researched meditation techniques. It is impossible to say whether or not it has any effect on health, as the research to date is of poor quality.
History.
Transcendental Meditation dates its origin back to the Vedic traditions of India. The Transcendental Meditation program and the Transcendental Meditation movement originated with Maharishi Mahesh Yogi, founder of the organization, and continue beyond his death in 2008. In 1955, "the Maharishi began publicly teaching a traditional meditation technique" learned from his master Brahmananda Saraswati that he called Transcendental Deep Meditation and later renamed Transcendental Meditation.
The Maharishi initiated thousands of people, then developed a TM teacher training program as a way to accelerate the rate of bringing the technique to more people. He also inaugurated a series of world tours which promoted Transcendental Meditation. These factors, coupled with endorsements by celebrities who practiced TM and claims that scientific research had validated the technique, helped to popularize TM in the 1960s and 1970s. By the late 2000s, TM had been taught to millions of individuals and the Maharishi was overseeing a large multinational movement. Despite organizational changes and the addition of advanced meditative techniques in the 1970s, the Transcendental Meditation technique has remained relatively unchanged.
Among the first organizations to promote TM were the Spiritual Regeneration Movement and the International Meditation Society. In modern times, the movement has grown to encompass schools and universities that teach the practice, and includes many associated programs based on the Maharishi's interpretation of the Vedic traditions. In the U.S., non-profit organizations included the Students International Meditation Society, AFSCI, World Plan Executive Council, Maharishi Vedic Education Development Corporation, Global Country of World Peace and Maharishi Foundation. The successor to Maharishi Mahesh Yogi, and leader of the Global Country of World Peace, is Tony Nader.
Technique.
The meditation practice involves the use of a mantra and is practiced for 15–20 minutes twice per day while sitting with the eyes closed. It is reported to be one of the most widely practiced, and among the most widely researched, meditation techniques, with hundreds of published research studies. The technique is made available worldwide by certified TM teachers in a seven-step course, and fees vary from country to country. Beginning in 1965, the Transcendental Meditation technique has been incorporated into selected schools, universities, corporations, and prison programs in the US, Latin America, Europe, and India. In 1977 a US district court ruled that a curriculum in TM and the Science of Creative Intelligence (SCI) being taught in some New Jersey schools was religious in nature and in violation of the First Amendment. The technique has since been included in a number of educational and social programs around the world.
The Transcendental Meditation technique has been described as both religious and non-religious, as an aspect of a new religious movement, as rooted in Hinduism, and as a non-religious practice for self-development. The public presentation of the TM technique over its 50-year history has been praised for its high visibility in the mass media and effective global propagation, and criticized for using celebrity and scientific endorsements as a marketing tool. Advanced courses supplement the TM technique and include an advanced meditation program called the TM-Sidhi program.
Movement.
The Transcendental Meditation movement refers to the programs and organizations connected with the Transcendental Meditation technique and founded by Maharishi Mahesh Yogi. Transcendental Meditation was first taught in the 1950s in India and has continued since the Maharishi's death in 2008. The organization was estimated to have 900,000 participants worldwide in 1977, a million by the 1980s, and 5 million in more recent years, including some notable practitioners.
Programs include the Transcendental Meditation technique, an advanced meditation practice called the TM-Sidhi program ("Yogic Flying"), an alternative health care program called Maharishi Ayurveda, and a system of building and architecture called Maharishi Sthapatya Ved. The TM movement's past and present media endeavors include a publishing company (MUM Press), a television station (KSCI), a radio station (KHOE), and a satellite television channel (Maharishi Channel). During its 50-year history, its products and services have been offered through a variety of organizations, which are primarily nonprofit and educational. These include the Spiritual Regeneration Movement, the International Meditation Society, World Plan Executive Council, Maharishi Vedic Education Development Corporation, the Global Country of World Peace, and the David Lynch Foundation.
The TM movement also operates a worldwide network of Transcendental Meditation teaching centers, schools, universities, health centers, herbal supplements, solar panel, and home financing companies, plus several TM-centered communities. The global organization is reported to have an estimated net worth of USD 3.5 billion. The TM movement has been characterized in a variety of ways and has been called a spiritual movement, a new religious movement, a millenarian movement, a world affirming movement, a new social movement, a guru-centered movement, a personal growth movement, a religion, and a cult. Additional sources contend that TM and its movement are not a cult. Some state that participation in TM programs does not require a belief system and is practiced by people from a diverse group of religious affiliations including atheists and agnostics. The organization has also been criticized as well as praised for its public presentation and marketing techniques throughout its 50-year history.
Health effects.
The first studies of the health effects of Transcendental Meditation appeared in the early 1970s. Robert Keith Wallace, the founding president of Maharishi University of Management, published a study in "Science" in 1970 reporting that TM induced distinct physiologic changes and a novel state of consciousness in practitioners. In contrast, a 1976 study by independent researchers found that TM was biochemically similar to sitting with one's eyes closed. A second 1976 study of five subjects found that TM practitioners spent much of their meditation time napping rather than in the unique "wakeful hypometabolic state" described by Wallace. By 2004 the US government had given more than $20 million to Maharishi University of Management to study the effect of meditation on health.
It is currently not possible to say whether meditation has any effect on health, as the research to date has been of poor quality, including a high risk for bias due to the connection of researchers to the TM organization and the selection of subjects with a favorable opinion of TM. Most independent systematic reviews have not found health benefits for TM exceeding those of relaxation and health education. A 2013 statement from the American Heart Association said that TM could be considered as a treatment for hypertension, although other interventions such as exercise and device-guided breathing were more effective and better supported by clinical evidence. A 2014 systematic review and meta-analysis funded by the U.S. Agency for Healthcare Research and Quality found no evidence that mantra meditation programs such as TM were effective in reducing psychological stress or improving well-being. A 2015 systematic review and meta-analysis found that TM may effectively reduce blood pressure compared to a control groups, although the underlying studies may have been biased and further studies with better designs are needed to confirm these results. A 2014 Cochrane review found that it was impossible to draw any conclusions about whether TM is effective in preventing cardiovascular disease, as the scientific literature on TM was limited and at "serious risk of bias".
Maharishi Effect.
In the 1960s, Maharishi Mahesh Yogi described a paranormal effect that he claimed a significant number of individuals (1% of the people in a given area) practicing the Transcendental Meditation technique (TM) could have on the local environment. This hypothetical influence was later termed the Maharishi Effect. With the introduction of the TM-Sidhi program in 1976, Maharishi proposed that the square root of one percent of the population practicing the TM-Sidhi program, together at the same time and in the same place, would increase "life-supporting trends". This was referred to as the "Extended Maharishi Effect". Evidence, which TM practitioners believe supports the existence of the effect, has been said to lack a causal basis. The evidence was said to result from cherry-picked data and the credulity of believers.

</doc>
<doc id="42736" url="https://en.wikipedia.org/wiki?curid=42736" title="VOC">
VOC

VOC may refer to:

</doc>
<doc id="42737" url="https://en.wikipedia.org/wiki?curid=42737" title="Dutch East India Company">
Dutch East India Company

The United East Indian Company (; VOC), referred to by the British as the Dutch East India Company, was originally established as a chartered company in 1602, when the Dutch government granted it a 21-year monopoly on Dutch spice trade. It is often considered to have been the first multinational corporation in the world and it was the first company to issue stock. It was a powerful company, possessing quasi-governmental powers, including the ability to wage war, imprison and execute convicts, negotiate treaties, strike its own coins, and establish colonies.
Statistically, the VOC eclipsed all of its rivals in the Asia trade. Between 1602 and 1796 the VOC sent almost a million Europeans to work in the Asia trade on 4,785 ships, and netted for their efforts more than 2.5 million tons of Asian trade goods. By contrast, the rest of Europe combined sent only 882,412 people from 1500 to 1795, and the fleet of the English (later British) East India Company, the VOC's nearest competitor, was a distant second to its total traffic with 2,690 ships and a mere one-fifth the tonnage of goods carried by the VOC. The VOC enjoyed huge profits from its spice monopoly through most of the 17th century.
Having been set up in 1602, to profit from the Malukan spice trade, in 1619 the VOC established a capital in the port city of Jayakarta and changed the city name into Batavia (now Jakarta). Over the next two centuries the Company acquired additional ports as trading bases and safeguarded their interests by taking over surrounding territory. It remained an important trading concern and paid an 18% annual dividend for almost 200 years.
Weighed down by corruption in the late 18th century, the Company went bankrupt and was formally dissolved in 1800, its possessions and the debt being taken over by the government of the Dutch Batavian Republic. The VOC's territories became the Dutch East Indies and were expanded over the course of the 19th century to include the whole of the Indonesian archipelago, and in the 20th century would form the Republic of Indonesia.
History.
Background.
During the 16th century, the spice trade was dominated by the Portuguese who used Lisbon as a staple port. Before the Dutch Revolt, Antwerp had played an important role as a distribution centre in northern Europe. However, after 1591 the Portuguese used an international syndicate of the German Fuggers and Welsers, and Spanish and Italian firms, that used Hamburg as its northern staple port to distribute their goods, thereby cutting Dutch merchants out of the trade.
At the same time, the Portuguese trade system was unable to increase supply to satisfy growing demand, in particular the demand for pepper. Demand for spices was relatively inelastic, and therefore each lag in the supply of pepper caused a sharp rise in pepper prices.
In addition, as the Portuguese crown had been united in a personal union with the Spanish crown in 1580, with which the Dutch Republic was at war, the Portuguese Empire became an appropriate target for Dutch military incursions. These three factors motivated Dutch merchants to enter the intercontinental spice trade themselves. Further, a number of Dutchmen like Jan Huyghen van Linschoten and Cornelis de Houtman obtained first hand knowledge of the "secret" Portuguese trade routes and practices, thereby providing opportunity.
The stage was thus set for Houtman's 1595 four-ship exploratory expedition to Banten, the main pepper port of West Java, where they clashed with both the Portuguese and indigenous Indonesians. Houtman's expedition then sailed east along the north coast of Java, losing twelve crew to a Javanese attack at Sidayu and killing a local ruler in Madura. Half the crew were lost before the expedition made it back to the Netherlands the following year, but with enough spices to make a considerable profit.
In 1598, an increasing number of fleets were sent out by competing merchant groups from around the Netherlands. Some fleets were lost, but most were successful, with some voyages producing high profits. In March 1599, a fleet of eight ships under Jacob van Neck was the first Dutch fleet to reach the 'Spice Islands' of Maluku, the source of pepper, cutting out the Javanese middlemen. The ships returned to Europe in 1599 and 1600 and the expedition made a 400 percent profit.
In 1600, the Dutch joined forces with the Muslim Hituese on Ambon Island in an anti-Portuguese alliance, in return for which the Dutch were given the sole right to purchase spices from Hitu. Dutch control of Ambon was achieved when the Portuguese surrendered their fort in Ambon to the Dutch-Hituese alliance. In 1613, the Dutch expelled the Portuguese from their Solor fort, but a subsequent Portuguese attack led to a second change of hands; following this second reoccupation, the Dutch once again captured Solor, in 1636.
East of Solor on the island of Timor, Dutch advances were halted by an autonomous and powerful group of Portuguese Eurasians called the Topasses. They remained in control of the Sandalwood trade and their resistance lasted throughout the 17th and 18th century, causing Portuguese Timor to remain under the Portuguese sphere of control.
Formation (1602).
At the time, it was customary for a company to be set up only for the duration of a single voyage, and to be liquidated upon the return of the fleet. Investment in these expeditions was a very high-risk venture, not only because of the usual dangers of piracy, disease and shipwreck, but also because the interplay of inelastic demand and relatively elastic supply of spices could make prices tumble at just the wrong moment, thereby ruining prospects of profitability. To manage such risk the forming of a cartel to control supply would seem logical. The English had been the first to adopt this approach, by bundling their resources into a monopoly enterprise, the English East India Company in 1600, thereby threatening their Dutch competitors with ruin.
In 1602 the Dutch government followed suit, sponsoring the creation of a single "United East Indies Company" that was also granted monopoly over the Asian trade. The charter of the new company empowered it to build forts, maintain armies, and conclude treaties with Asian rulers. It provided for a venture that would continue for 21 years, with a financial accounting only at the end of each decade.
In 1603, the first permanent Dutch trading post in Indonesia was established in Banten, West Java and in 1611, another was established at Jayakarta (later "Batavia" and then "Jakarta"). In 1610, the VOC established the post of Governor General to more firmly control their affairs in Asia. To advise and control the risk of despotic Governors General, a Council of the Indies ("Raad van Indië") was created. The Governor General effectively became the main administrator of the VOC's activities in Asia, although the "Heeren XVII", a body of 17 shareholders representing different chambers, continued to officially have overall control.
VOC headquarters were located in Ambon during the tenures of the first three Governors General (1610–1619), but it was not a satisfactory location. Although it was at the centre of the spice production areas, it was far from the Asian trade routes and other VOC areas of activity ranging from Africa to India to Japan. A location in the west of the archipelago was thus sought; the Straits of Malacca were strategic, but had become dangerous following the Portuguese conquest and the first permanent VOC settlement in Banten was controlled by a powerful local ruler and subject to stiff competition from Chinese and English traders.
In 1604, a second English East India Company voyage commanded by Sir Henry Middleton reached the islands of Ternate, Tidore, Ambon and Banda; in Banda, they encountered severe VOC hostility, which saw the beginning of Anglo-Dutch competition for access to spices. From 1611 to 1617, the English established trading posts at Sukadana (southwest Kalimantan), Makassar, Jayakarta and Jepara in Java, and Aceh, Pariaman and Jambi in Sumatra which threatened Dutch ambitions for a monopoly on East Indies trade.
Diplomatic agreements in Europe in 1620 ushered in a period of co-operation between the Dutch and the English over the spice trade.
This ended with a notorious, but disputed incident, known as the 'Amboyna massacre', where ten Englishmen were arrested, tried and beheaded for conspiracy against the Dutch government. Although this caused outrage in Europe and a diplomatic crisis, the English quietly withdrew from most of their Indonesian activities (except trading in Bantam) and focused on other Asian interests.
Growth.
In 1619, Jan Pieterszoon Coen was appointed Governor-General of the VOC. He saw the possibility of the VOC becoming an Asian power, both political and economic. On 30 May 1619, Coen, backed by a force of nineteen ships, stormed Jayakarta driving out the Banten forces; and from the ashes established Batavia as the VOC headquarters. In the 1620s almost the entire native population of the Banda Islands was driven away, starved to death, or killed in an attempt to replace them with Dutch plantations. These plantations were used to grow cloves and nutmeg for export.
Coen hoped to settle large numbers of Dutch colonists in the East Indies, but implementation of this policy never materialised, mainly because very few Dutch were willing to emigrate to Asia.
Another of Coen's ventures was more successful. A major problem in the European trade with Asia at the time was that the Europeans could offer few goods that Asian consumers wanted, except silver and gold. European traders therefore had to pay for spices with the precious metals, and this was in short supply in Europe, except for Spain and Portugal. The Dutch and English had to obtain it by creating a trade surplus with other European countries. Coen discovered the obvious solution for the problem: to start an intra-Asiatic trade system, whose profits could be used to finance the spice trade with Europe. In the long run this obviated the need for exports of precious metals from Europe, though at first it required the formation of a large trading-capital fund in the Indies. The VOC reinvested a large share of its profits to this end in the period up to 1630.
The VOC traded throughout Asia. Ships coming into Batavia from the Netherlands carried supplies for VOC settlements in Asia. Silver and copper from Japan were used to trade with India and China for silk, cotton, porcelain, and textiles. These products were either traded within Asia for the coveted spices or brought back to Europe. The VOC was also instrumental in introducing European ideas and technology to Asia. The Company supported Christian missionaries and traded modern technology with China and Japan. A more peaceful VOC trade post on Dejima, an artificial island off the coast of Nagasaki, was for more than two hundred years the only place where Europeans were permitted to trade with Japan. When the VOC tried to military force Ming dynasty China to open up to Dutch trade, the Chinese defeated the Dutch in a war over the Penghu islands from 1623-1624 and forced the VOC to abandon Penghu for Taiwan. The Chinese defeated the VOC again at the Battle of Liaoluo Bay in 1633.
The Vietnamese Nguyen Lords defeated the VOC in a 1643 battle during the Trịnh–Nguyễn War, blowing up a Dutch ship. The Cambodians defeated the VOC in a war from 1643-44 on the Mekong River.
In 1640, the VOC obtained the port of Galle, Ceylon, from the Portuguese and broke the latter's monopoly of the cinnamon trade. In 1658, Gerard Pietersz. Hulft laid siege to Colombo, which was captured with the help of King Rajasinghe II of Kandy. By 1659, the Portuguese had been expelled from the coastal regions, which were then occupied by the VOC, securing for it the monopoly over cinnamon.
To prevent the Portuguese or the English from ever recapturing Sri Lanka, the VOC went on to conquer the entire Malabar Coast from the Portuguese, almost entirely driving them from the west coast of India. When news of a peace agreement between Portugal and the Netherlands reached Asia in 1663, Goa was the only remaining Portuguese city on the west coast.
In 1652, Jan van Riebeeck established an outpost at the Cape of Good Hope (the southwestern tip of Africa, currently in Cape Town, South Africa) to re-supply VOC ships on their journey to East Asia. This post later became a full-fledged colony, the Cape Colony, when more Dutch and other Europeans started to settle there.
VOC trading posts were also established in Persia, Bengal, Malacca, Siam, Canton, Formosa (now Taiwan), as well as the Malabar and Coromandel coasts in India. In 1662, however, Koxinga expelled the Dutch from Taiwan ("see" History of Taiwan).
In 1663, the VOC signed "Painan Treaty" with several local lords in the Painan area that were revolting against the Aceh Sultanate. The treaty resulted in VOC to build a trading post in the area and eventually monopolise the trade there, especially in gold trade.
By 1669, the VOC was the richest private company the world had ever seen, with over 150 merchant ships, 40 warships, 50,000 employees, a private army of 10,000 soldiers, and a dividend payment of 40% on the original investment.
Many of the VOC employees inter-mixed with the indigenous peoples and expanded the population of Indos in pre-colonial history 
Reorientation.
Around 1670, two events caused the growth of VOC trade to stall. In the first place, the highly profitable trade with Japan started to decline. The loss of the outpost on Formosa to Koxinga in the 1662 Siege of Fort Zeelandia and related internal turmoil in China (where the Ming dynasty was being replaced with the Qing dynasty) brought an end to the silk trade after 1666. Though the VOC substituted Bengali for Chinese silk other forces affected the supply of Japanese silver and gold. The shogunate enacted a number of measures to limit the export of these precious metals, in the process limiting VOC opportunities for trade, and severely worsening the terms of trade. Therefore, Japan ceased to function as the lynchpin of the intra-Asiatic trade of the VOC by 1685.
Even more importantly, the Third Anglo-Dutch War temporarily interrupted VOC trade with Europe. This caused a spike in the price of pepper, which enticed the English East India Company (EIC) to aggressively enter this market in the years after 1672. Previously, one of the tenets of the VOC pricing policy was to slightly over-supply the pepper market, so as to depress prices below the level where interlopers were encouraged to enter the market (instead of striving for short-term profit maximisation). The wisdom of such a policy was illustrated when a fierce price war with the EIC ensued, as that company flooded the market with new supplies from India. In this struggle for market share, the VOC (which had much larger financial resources) could wait out the EIC. Indeed, by 1683, the latter came close to bankruptcy; its share price plummeted from 600 to 250; and its president Josiah Child was temporarily forced from office.
However, the writing was on the wall. Other companies, like the French East India Company and the Danish East India Company also started to make inroads on the Dutch system. The VOC therefore closed the heretofore flourishing open pepper emporium of Bantam by a treaty of 1684 with the Sultan. Also, on the Coromandel Coast, it moved its chief stronghold from Pulicat to Negapatnam, so as to secure a monopoly on the pepper trade at the detriment of the French and the Danes. However, the importance of these traditional commodities in the Asian-European trade was diminishing rapidly at the time. The military outlays that the VOC needed to make to enhance its monopoly were not justified by the increased profits of this declining trade.
Nevertheless, this lesson was slow to sink in and at first the VOC made the strategic decision to improve its military position on the Malabar Coast (hoping thereby to curtail English influence in the area, and end the drain on its resources from the cost of the Malabar garrisons) by using force to compel the Zamorin of Calicut to submit to Dutch domination. In 1710, the Zamorin was made to sign a treaty with the VOC undertaking to trade exclusively with the VOC and expel other European traders. For a brief time, this appeared to improve the Company's prospects. However, in 1715, with EIC encouragement, the Zamorin renounced the treaty. Though a Dutch army managed to suppress this insurrection temporarily, the Zamorin continued to trade with the English and the French, which led to an appreciable upsurge in English and French traffic. The VOC decided in 1721 that it was no longer worth the trouble to try to dominate the Malabar pepper and spice trade. A strategic decision was taken to scale down the Dutch military presence and in effect yield the area to EIC influence.
The 1741 Battle of Colachel by Nair warriors of Travancore under Raja Marthanda Varma defeated the Dutch. The Dutch commander Captain Eustachius De Lannoy was captured. Marthanda Varma agreed to spare the Dutch captain's life on condition that he joined his army and trained his soldiers on modern lines. This defeat in the Travancore-Dutch War is considered the earliest example of an organised Asian power overcoming European military technology and tactics; and it signalled the decline of Dutch power in India.
The attempt to continue as before as a low volume-high profit business enterprise with its core business in the spice trade had therefore failed. The Company had however already (reluctantly) followed the example of its European competitors in diversifying into other Asian commodities, like tea, coffee, cotton, textiles, and sugar. These commodities provided a lower profit margin and therefore required a larger sales volume to generate the same amount of revenue. This structural change in the commodity composition of the VOC's trade started in the early 1680s, after the temporary collapse of the EIC around 1683 offered an excellent opportunity to enter these markets. The actual cause for the change lies, however, in two structural features of this new era.
In the first place, there was a revolutionary change in the tastes affecting European demand for Asian textiles, and coffee and tea, around the turn of the 18th century. Secondly, a new era of an abundant supply of capital at low interest rates suddenly opened around this time. The second factor enabled the Company to easily finance its expansion in the new areas of commerce. Between the 1680s and 1720s, the VOC was therefore able to equip and man an appreciable expansion of its fleet, and acquire a large amount of precious metals to finance the purchase of large amounts of Asian commodities, for shipment to Europe. The overall effect was to approximately double the size of the company.
The tonnage of the returning ships rose by 125 percent in this period. However, the Company's revenues from the sale of goods landed in Europe rose by only 78 percent. This reflects the basic change in the VOC's circumstances that had occurred: it now operated in new markets for goods with an elastic demand, in which it had to compete on an equal footing with other suppliers. This made for low profit margins. Unfortunately, the business information systems of the time made this difficult to discern for the managers of the company, which may partly explain the mistakes they made from hindsight. This lack of information might have been counteracted (as in earlier times in the VOC's history) by the business acumen of the directors. Unfortunately by this time these were almost exclusively recruited from the political "regent" class, which had long since lost its close relationship with merchant circles.
Low profit margins in themselves do not explain the deterioration of revenues. To a large extent the costs of the operation of the VOC had a "fixed" character (military establishments; maintenance of the fleet and such). Profit levels might therefore have been maintained if the increase in the scale of trading operations that in fact took place, had resulted in economies of scale. However, though larger ships transported the growing volume of goods, labour productivity did not go up sufficiently to realise these. In general the Company's overhead rose in step with the growth in trade volume; declining gross margins translated directly into a decline in profitability of the invested capital. The era of expansion was one of "profitless growth".
Concretely: "he long-term average annual profit in the VOC's 1630–70 'Golden Age' was 2.1 million guilders, of which just under half was distributed as dividends and the remainder reinvested. The long-term average annual profit in the 'Expansion Age' (1680–1730) was 2.0 million guilders, of which three-quarters was distributed as dividend and one-quarter reinvested. In the earlier period, profits averaged 18 percent of total revenues; in the latter period, 10 percent. The annual return of invested capital in the earlier period stood at approximately 6 percent; in the latter period, 3.4 percent."
Nevertheless, in the eyes of investors the VOC did not do too badly. The share price hovered consistently around the 400 mark from the mid-1680s (excepting a hiccup around the Glorious Revolution in 1688), and they reached an all-time high of around 642 in the 1720s. VOC shares then yielded a return of 3.5 percent, only slightly less than the yield on Dutch government bonds.
Decline.
However, from there on the fortunes of the VOC started to decline. Five major problems, not all of equal weight, can be used to explain its decline in the next fifty years to 1780.
Despite of all this, the VOC in 1780 remained an enormous operation. Its capital in the Republic, consisting of ships and goods in inventory, totalled 28 million guilders; its capital in Asia, consisting of the liquid trading fund and goods en route to Europe, totalled 46 million guilders. Total capital, net of outstanding debt, stood at 62 million guilders. The prospects of the company at this time therefore need not have been hopeless, had one of the many plans to reform it been taken successfully in hand. However, then the Fourth Anglo-Dutch War intervened. British attacks in Europe and Asia reduced the VOC fleet by half; removed valuable cargo from its control; and devastated its remaining power in Asia. The direct losses of the VOC can be calculated at 43 million guilders. Loans to keep the company operating reduced its net assets to zero.
From 1720 on, the market for sugar from Indonesia declined as the competition from cheap sugar from Brazil increased. European markets became saturated. Dozens of Chinese sugar traders went bankrupt which led to massive unemployment, which in turn led to gangs of unemployed coolies. The Dutch government in Batavia did not adequately respond to these problems. In 1740, rumours of deportation of the gangs from the Batavia area led to widespread rioting. The Dutch military searched houses of Chinese in Batavia for weapons. When a house accidentally burnt down, military and impoverished citizens started slaughtering and pillaging the Chinese community. This massacre of the Chinese was deemed sufficiently serious for the board of the VOC to start an official investigation into the Government of the Dutch East Indies for the first time in its history.
After the Fourth Anglo-Dutch War, the VOC was a financial wreck, and after vain attempts by the provincial States of Holland and Zeeland to reorganise it, was nationalised on 1 March 1796 by the new Batavian Republic. Its charter was renewed several times, but allowed to expire on 31 December 1799. Most of the possessions of the former VOC were subsequently occupied by Great Britain during the Napoleonic wars, but after the new United Kingdom of the Netherlands was created by the Congress of Vienna, some of these were restored to this successor state of the old Dutch Republic by the Anglo-Dutch Treaty of 1814.
European discovery of Australia.
In terms of world history of geography and exploration, the VOC can be credited with putting most of Australia's coast (then Hollandia Nova and other names) on the world map, between 1606 and 1756. An Australian vintner has used the VOC logo since the late 20th century, having re-registered the company's name for the purpose.
Organization.
The VOC had two types of shareholders: the "participanten", who could be seen as non-managing members, and the 76 "bewindhebbers" (later reduced to 60) who acted as managing directors. This was the usual set-up for Dutch joint-stock companies at the time. The innovation in the case of the VOC was, that the liability of not just the "participanten", but also of the "bewindhebbers" was limited to the paid-in capital (usually, "bewindhebbers" had unlimited liability). The VOC therefore was a limited liability company. Also, the capital would be "permanent" during the lifetime of the company. As a consequence, investors that wished to liquidate their interest in the interim could only do this by selling their share to others on the Amsterdam Stock Exchange.
"Confusion of confusions", a 1688 dialogue by the Sephardi Jew Joseph de la Vega analysed the workings of this one-stock exchange.
The VOC consisted of six Chambers ("Kamers") in port cities: Amsterdam, Delft, Rotterdam, Enkhuizen, Middelburg and Hoorn. Delegates of these chambers convened as the Heeren XVII (the Lords Seventeen). They were selected from the "bewindhebber"-class of shareholders.
Of the "Heeren XVII", eight delegates were from the Chamber of Amsterdam (one short of a majority on its own), four from the Chamber of Zeeland, and one from each of the smaller Chambers, while the seventeenth seat was alternatively from the Chamber of Middelburg-Zeeland or rotated among the five small Chambers. Amsterdam had thereby the decisive voice. The Zeelanders in particular had misgivings about this arrangement at the beginning. The fear was not unfounded, because in practice it meant Amsterdam stipulated what happened.
The six chambers raised the start-up capital of the Dutch East India Company:
The raising of capital in Rotterdam did not go so smoothly. A considerable part originated from inhabitants of Dordrecht. Although it did not raise as much capital as Amsterdam or Middelburg-Zeeland, Enkhuizen had the largest input in the share capital of the VOC. Under the first 358 shareholders, there were many small entrepreneurs, who dared to take the risk. The minimum investment in the VOC was 3,000 guilders, which priced the Company's stock within the means of many merchants.
Among the early shareholders of the VOC, immigrants played an important role. Under the 1,143 tenderers were 39 Germans and no fewer than 301 from the Southern Netherlands (roughly present Belgium and Luxembourg, then under Habsburg rule), of whom Isaac le Maire was the largest subscriber with ƒ85,000. VOC's total capitalisation was ten times that of its British rival.
The logo of the VOC consisted of a large capital 'V' with an O on the left and a C on the right leg. It appeared on various corporate items, such as cannons and the coin illustrated above. The first letter of the hometown of the chamber conducting the operation was placed on top (see figure for example of the Amsterdam chamber logo). The flag of the company was orange, white, blue (see Dutch flag) with the company logo embroidered on it.
The "Heeren XVII" (Lords Seventeen) met alternately 6 years in Amsterdam and 2 years in Middelburg-Zeeland. They defined the VOC's general policy and divided the tasks among the Chambers. The Chambers carried out all the necessary work, built their own ships and warehouses and traded the merchandise. The "Heeren XVII" sent the ships' masters off with extensive instructions on the route to be navigated, prevailing winds, currents, shoals and landmarks. The VOC also produced its own charts.
In the context of the Dutch-Portuguese War the company established its headquarters in Batavia, Java (now Jakarta, Indonesia). Other colonial outposts were also established in the East Indies, such as on the Maluku Islands, which include the Banda Islands, where the VOC forcibly maintained a monopoly over nutmeg and mace. Methods used to maintain the monopoly involved extortion and the violent suppression of the native population, including mass murder. In addition, VOC representatives sometimes used the tactic of burning spice trees to force indigenous populations to grow other crops, thus artificially cutting the supply of spices like nutmeg and cloves.
VOC outposts.
Organization and leadership structures were varied as necessary in the various VOC outposts:
"Opperhoofd" is a Dutch word (pl. "Opperhoofden") which literally means 'supreme chief'. In this VOC context, the word is a gubernatorial title, comparable to the English Chief factor, for the chief executive officer of a Dutch "factory" in the sense of trading post, as led by a factor, i.e. agent.
Council of Justice in Batavia.
The Council of Justice in Batavia was the appellate court for all the other VOC Company posts in the VOC empire.
Use of slaves.
By the time the settlement was established at the Cape in 1652, the VOC already had a long experience of practising slavery in the East Indies. Jan van Riebeeck concluded within two months of the establishment of the Cape settlement that slave labor would be needed for the hardest and dirtiest work. Initially, the Dutch East India Trading Company considered enslaving men from the indigenous Khoikhoi population, but the idea was rejected on the grounds that such a policy would be both costly and dangerous. Most Khoikhoi had chosen not to labor for the Dutch because of low wages and harsh conditions. In the beginning, the settlers traded with the Khoikhoi but the harsh working conditions and low wages imposed by the Dutch led to a series of wars. The European population remained under 200 during the settlement's first five years, and war against neighbors numbering more than 20,000 would have been foolhardy. Moreover, the Dutch feared that Khoikhoi people, if enslaved, could always escape into the local community, whereas foreigners would find it much more difficult to elude their "masters."
Between 1652 and 1657, a number of unsuccessful attempts were made to obtain men from the Dutch East Indies and from Mauritius. In 1658, however, the VOC landed two shiploads of slaves at the Cape, one containing more than 200 people brought from Dahomey (later Benin), the second with almost 200 people, most of them children, captured from a Portuguese slaver off the coast of Angola. Except for a few individuals, these were to be the only slaves ever brought to the Cape from West Africa.
From 1658 to the end of the Company’s rule, many more slaves were brought regularly to the Cape in various ways, chiefly by Company-sponsored slaving voyages and slaves brought to the Cape by its return fleets. From these sources and by natural growth, the slave population increased from zero in 1652 to about 1,000 by 1700. During the 18th century, the slave population increased dramatically to 16,839 by 1795.
After the slave trade was initiated, all of the slaves imported into the Cape until the British stopped the trade in 1807 were from East Africa, Mozambique, Madagascar, and South and Southeast Asia. Large numbers were brought from India, Ceylon, and the Indonesian archipelago. Prisoners from other countries in the VOC's empire were also enslaved. The slave population, which exceeded that of the European settlers until the first quarter of the nineteenth century, was overwhelmingly male and was thus dependent on constant imports of new slaves to maintain and to augment its size.
By the 1660s the Cape settlement was importing slaves from India, Malaya (Malaysia), and Madagascar to work on the farms.
Conflict between Dutch farmers and Khoikhoi broke out once it became clear to the latter that the Dutch were there to stay and that they intended to encroach on the lands of the pastoralists. In 1659 Doman, a Khoikhoi who had worked as a translator for the Dutch and had even traveled to Java, led an armed attempt to expel the Dutch from the Cape peninsula. The attempt was a failure, although warfare dragged on until an inconclusive peace was established a year later. During the following decade, pressure on the Khoikhoi grew as more of the Dutch became free burghers, expanded their landholdings, and sought pastureland for their growing herds. War broke out again in 1673 and continued until 1677, when Khoikhoi resistance was destroyed by a combination of superior European weapons and Dutch manipulation of divisions among the local people. Thereafter, Khoikhoi society in the western Cape disintegrated. Some people found jobs as shepherds on European farms; others rejected foreign rule and moved away from the Cape. The final blow for most came in 1713 when a Dutch ship brought smallpox to the Cape. Hitherto unknown locally, the disease ravaged the remaining Khoikhoi, killing 90 percent of the population.
Throughout the eighteenth century, the settlement continued to expand through internal growth of the European population and the continued importation of slaves. The approximately 3,000 Europeans and slaves at the Cape in 1700 had increased by the end of the century to nearly 20,000 Europeans, and approximately 25,000 slaves.
See also.
Other trade companies of the age of the sail
Governors General of the Dutch East India Company

</doc>
<doc id="42739" url="https://en.wikipedia.org/wiki?curid=42739" title="Bubble fusion">
Bubble fusion

Bubble fusion, also known as sonofusion, is the non-technical name for a nuclear fusion reaction hypothesized to occur inside extraordinarily large collapsing gas bubbles created in a liquid during acoustic cavitation. Rusi Taleyarkhan and collaborators claimed to have observed evidence of sonofusion in 2002. The claim was quickly surrounded by controversy, including allegations ranging from experimental error to academic fraud. Subsequent publications claiming independent verification of sonofusion were also highly controversial. Eventually, an investigation by Purdue University found that Taleyarkhan had engaged in falsification of independent verification, and had included a student as an author on a paper when he had not participated in the research. He was subsequently stripped of his professorship. One of his funders, the Office of Naval Research reviewed the report by Purdue and barred him from federal funding for 28 months.
Original experiments.
In the March 8, 2002 issue of the peer-reviewed journal "Science", Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone () showed measurements of tritium and neutron output consistent with the occurrence of fusion. The neutron emission was also reported to be coincident with the sonoluminescence pulse, a key indicator that its source was fusion caused by the heat and pressure inside the collapsing bubbles.
Oak Ridge failed replication.
The results were so startling, that the Oak Ridge National Laboratory asked two independent researchers, D. Shapira and M. J. Saltmarsh, to repeat the experiment using more sophisticated neutron detection equipment. They reported that the neutron release was consistent with random coincidence. A rebuttal by Taleyarkhan and the other authors of the original report argued that the Shapira and Saltmarsh report failed to account for significant differences in experimental setup, including over an inch of shielding between the neutron detector and the sonoluminescing acetone. According to Taleyarkhan "et al.", when properly considering those differences, the results were consistent with fusion.
As early as 2002, while experimental work was still in progress, Aaron Galonsky of Michigan State University, in a letter to the journal "Science"
expressed doubts about the claim made by the Taleyarkhan team. In Galonsky's opinion, the observed neutrons were too high in energy to be from a deuterium-deuterium (d-d) fusion reaction. In their response (published on the same page), the Taleyarkhan team provided detailed counter-arguments and concluded that the energy was "reasonably close" to that which was expected from a fusion reaction.
In February 2005 the documentary series "Horizon" commissioned two leading sonoluminescence researchers, Seth Putterman and Kenneth S. Suslick, to reproduce Taleyarkhan's work. Using similar acoustic parameters, deuterated acetone, similar bubble nucleation, and a much more sophisticated neutron detection device, the researchers could find no evidence of a fusion reaction.
Subsequent reports of replication.
In 2004, new reports of bubble fusion were published by the Taleyarkhan group, claiming that the results of previous experiments had been replicated under more stringent experimental conditions. These results differed from the original results in that fusion was claimed to occur over longer times than previously reported. The original report only claimed neutron emission from the initial bubble collapse following bubble nucleation, whereas this report claimed neutron emission many acoustic cycles later.
In July 2005, two of Taleyarkhan's students at Purdue University published evidence confirming the previous result. They used the same acoustic chamber, the same deuterated acetone fluid and a similar bubble nucleation system. In this report, no neutron-sonoluminescence coincidence was attempted. An article in "Nature" raised issues about the validity of the research and complaints from his Purdue colleagues (see full analysis elsewhere in this page). Charges of misconduct were raised, and Purdue University opened an investigation. It concluded in 2008 that Taleyarkhan's name should have appeared in the author list because of his deep involvement in many steps of the research, that he added one author that had not really participated in the paper just to overcome the criticism of one reviewer, and that this was part of an attempt of "an effort to falsify the scientific record by assertion of independent confirmation". The investigation did not address the validity of the experimental results.
In January 2006, a paper published in the journal "Physical Review Letters" by Taleyarkhan in collaboration with researchers from Rensselaer Polytechnic Institute reported statistically significant evidence of fusion.
In November 2006, in the midst of accusations concerning Taleyarkhan's research standards, two different scientists visited the meta-stable fluids research lab at Purdue University to measure neutrons, using Taleyarkhan's equipment. Dr. Edward R. Forringer and undergraduates David Robbins and Jonathan Martin of LeTourneau University presented two papers at the American Nuclear Society Winter Meeting that reported replication of neutron emission. Their experimental setup was similar to previous experiments in that it used a mixture of deuterated acetone, deuterated benzene, tetrachloroethylene and uranyl nitrate. Notably, however, it operated without an external neutron source and used two types of neutron detectors. They claimed a liquid scintillation detector measured neutron levels at 8 standard deviations above the background level, while plastic detectors measured levels at 3.8 standard deviations above the background. When the same experiment was performed with non-deuterated control liquid, the measurements were within one standard deviation of background, indicating that the neutron production had only occurred during cavitation of the deuterated liquid. William M. Bugg, emeritus physics professor at the University of Tennessee also traveled to Taleyarkhan's lab to repeat the experiment with his equipment. He also reported neutron emission, using plastic neutron detectors. Taleyarkhan claimed these visits counted as independent replications by experts, but Forringer later recognized that he was not an expert, and Bugg later said that Taleyarkhan performed the experiments and he had only watched.
"Nature" report.
Reports as spectacular as the above raise a lot of doubt. In March 2006, "Nature" published a special report that called into question the validity of the results of the Purdue experiments. The report quotes Brian Naranjo of the University of California, Los Angeles to the effect that neutron energy spectrum reported in the 2006 paper by Taleyarkhan, et al. was statistically inconsistent with neutrons produced by the proposed fusion reaction and instead highly consistent with neutrons produced by the radioactive decay of Californium 252, an isotope commonly used as a laboratory neutron source .
The response of Taleyarkhan "et al.", published in "Physical Review Letters", attempts to refute Naranjo's hypothesis as to the cause of the neutrons detected.
Tsoukalas, head of the School of Nuclear Engineering at Purdue, and several of his colleagues at Purdue, had convinced Taleyarkhan to move to Purdue and attempt a joint replication. In the 2006 "Nature" report they detail several troubling issues when trying to collaborate with Taleyarkhan. He reported positive results from certain set of raw data, but his colleagues had also examined that set and it only contained negative results. He never showed his colleagues the raw data corresponding to the positive results, despite several requests. He moved the equipment from a shared laboratory to his own laboratory, thus impeding review by his colleagues, and he didn't give any advance warning or explanation for the move. Taleyarkhan convinced his colleagues that they shouldn't publish a paper with their negative results. Taleyarkhan then insisted that the university's press release presented his experiment as "peer-reviewed" and "independent", when the co-authors were working in his laboratory under his supervision, and his peers in the faculty were not allowed to review the data. In summary, Taleyarkhan's colleagues at Purdue said he placed obstacles to peer review of his experiments, and they had serious doubts about the validity of the research.
"Nature" also revealed that the process of anonymous peer-review had not been followed, and that the journal "Nuclear Engineering and Design" was not independent from the authors. Taleyarkhan was co-editor of the journal, and the paper was only peer-reviewed by his co-editor, with Taleyarkhan's knowledge.
In 2002 Taleyarkhan filed a patent application on behalf of the United States Department of Energy, while working in Oak Ridge. "Nature" reported that the patent had been rejected in 2005 by the US Patent Office. The examiner called the experiment a variation of discredited cold fusion, found that there was "no reputable evidence of record to support any allegations or claims that the invention is capable of operating as indicated", and found that there was not enough detail for others to replicate the invention. The field of fusion suffered from many flawed claims, thus the examiner asked for additional proof that the radiation was generated from fusion and not from other sources. An appeal was not filed because the Department of Energy had dropped the claim in December 2005.
Doubts prompt investigation.
Doubts among Purdue University's Nuclear Engineering faculty as to whether the positive results reported from sonofusion experiments conducted there were truthful prompted the university to initiate a review of the research, conducted by Purdue's Office of the Vice President for Research. In a March 9, 2006 article entitled "Evidence for bubble fusion called into question", "Nature" interviewed several of Taleyarkhan's colleagues who suspected something was amiss.
On February 7, 2007, the Purdue University administration determined that "the evidence does not support the allegations of research misconduct and that no further investigation of the allegations is warranted". Their report also stated that "vigorous, open debate of the scientific merits of this new technology is the most appropriate focus going forward." In order to verify that the investigation was properly conducted, House Representative Brad Miller requested full copies of its documents and reports by March 30, 2007. His congressional report concluded that "Purdue deviated from its own procedures in investigating this case and did not conduct a thorough investigation"; in response, Purdue announced that it would re-open its investigation.
In June 2008, a multi-institutional team including Taleyarkhan published a paper in Nuclear Engineering and Design to "clear up misconceptions generated by a webposting of UCLA which served as the basis for the "Nature" article of March 2006", according to a press release.
On July 18, 2008, Purdue University announced that a committee with members from five institutions had investigated 12 allegations of research misconduct against Rusi Taleyarkhan. It concluded that two allegations were founded—that Taleyarkhan had claimed independent confirmation of his work when in reality the apparent confirmations were done by Taleyarkhan's former students and was not as "independent" as Taleyarkhan implied, and that Taleyarkhan had included a colleague's name on one of his papers who had not actually been involved in the research ("the sole apparent motivation for the addition of Mr. Butt was a desire to overcome a reviewer's criticism", the report concluded).
Taleyarkhan's appeal of the report's conclusions was rejected. He said the two allegations of misconduct were trivial administrative issues and had nothing to do with the discovery of bubble nuclear fusion or the underlying science, and that "all allegations of fraud and fabrication have been dismissed as invalid and without merit — thereby supporting the underlying science and experimental data as being on solid ground". A researcher questioned by the LA Times said that the report had not clarified whether bubble fusion was real or not, but that the low quality of the papers and the doubts cast by the report had destroyed Taleyarkhan's credibility with the scientific community.
On August 27, 2008 he was stripped of his named Arden Bement Jr. Professorship, and forbidden to be a thesis advisor for graduate students for at least the next 3 years.
Despite the findings against him, Taleyarkhan received a $185,000 grant from the National Science Foundation between September 2008 and August 2009 to investigate bubble fusion. In 2009 the Office of Naval Research debarred him for 28 months, until September 2011, from receiving U.S. Federal Funding. During that period his name was listed in the 'Excluded Parties List' to prevent him from receiving further grants from any government agency.

</doc>
<doc id="42741" url="https://en.wikipedia.org/wiki?curid=42741" title="Municipal Art Society">
Municipal Art Society

The Municipal Art Society of New York, founded in 1893, is a non-profit membership organization that fights for intelligent urban planning, design and preservation through education, dialogue and advocacy in New York City.
On January 20, 2010, MAS relocated from its longtime home in the historic Villard Houses on 457 Madison Avenue to the equally famed Steinway Hall
on West 57th Street (across the street and east of Carnegie Hall). The MAS website currently lists their address as 488 Madison Avenue, Suite 1900.
History.
MAS’s advocacy efforts have shaped the city a great deal since its inception in 1893. Some of their early accomplishments include passing the city's first zoning laws, contributing input to the planning of the city’s subway line, and commissioning public art throughout the city.
By the 1950s, scores of notable Manhattan buildings were lost to redevelopment around the city, and the mission of MAS broadened to include historical preservation. In 1956, the Society successfully lobbied for the passage of the Bard Law, which for the first time allowed cities to take aesthetics, history, and cultural associations into account for zoning laws. The law, named after longtime MAS board member and chief advocate, Albert S. Bard, provided a legal foundation for the New York City Landmarks Law, enacted in 1965.
In 1965, public outrage over the destruction of Pennsylvania Station and the Brokaw Mansion helped fuel the Society's mission towards preservation. With like-minded groups, they finally succeeded in establishing New York's Landmarks Preservation Commission, and New York's Landmarks Law.
In 2001, after the demise of Trans World Airlines, the original Trans World Flight Center, completed in 1962 and designed by Eero Saarinen, fell into disuse. During this period, the Municipal Art Society succeeded in 2004 in nominating the facility to the National Trust for Historic Preservation’s list of the 11 Most Endangered Places.
In June 2007, MAS released with the Metropolitan Waterfront Alliance a new documentary about the future of the New York waterfront titled City of Water. In September 2007, the Society opened a major exhibition about Jane Jacobs sponsored by the Rockefeller Foundation.
In May 2008, MAS released a series of renderings showing what the unfinished Atlantic Yards project might look like titled "Atlantic Lots".
In October 2008, MAS launched a new initiative to develop bold new ideas for Coney Island titled ImagineConey.
Recent projects.
__NOTOC__

</doc>
<doc id="42742" url="https://en.wikipedia.org/wiki?curid=42742" title="New York City arts organizations">
New York City arts organizations

The City of New York is home to many arts organizations. They include:

</doc>
<doc id="42745" url="https://en.wikipedia.org/wiki?curid=42745" title="Los Angeles Pierce College">
Los Angeles Pierce College

Los Angeles Pierce College, also known as Pierce College and just Pierce, is a community college that serves more than 23,000 students in the northern Chalk Hills of Woodland Hills, a community within the San Fernando Valley region of the city of Los Angeles, California.
The college began with 70 students and 18 faculty members on September 15, 1947. Originally known as the "Clarence W. Pierce School of Agriculture", the institution’s initial focus was crop cultivation and animal husbandry. Nine years later, in 1956, the school was renamed to "Los Angeles Pierce College", retaining the name of its founder, Dr. Pierce, as well as his commitment to agricultural and veterinary study. (Pierce still maintains a working farm for hands-on training.)
Campus overview.
Pierce College offers courses on more than 100 subjects in 92 academic disciplines, and has transfer alliances with most of the universities in the state. Students at the school successfully transfer to UC and CSU schools.
Students can pursue any of the 44 associate’s degrees or 78 Certificates of Achievement the school offers directly.
Pierce College comprises amidst a dense metropolis, an area larger than many university campuses, including that of UCLA. The grounds are landscaped with more than 2,200 trees, thousands of roses and a botanical garden. The Pierce College farm houses small herds of cattle, sheep, goats, a small poultry flock, as well as a llama and an alpaca for its students to learn from.
Student government.
The Associated Students Organization (ASO) is the official Student Government of the 23,000 students at Los Angeles Pierce College. It has been in existence since the mid-50's and has 2 branches; Senate and Club Council. Club Council is the official organization that addresses and improves student life through the actions of the numerous clubs on campus. The Senate is the abiding student government that conducts their meetings in accordance to the Brown Act. ASO is governed under the LACCD Academic Senate policy. Senators are voting members, which have the ability to vote on certain actions taken at Pierce to improve life for the student. Under the actions of shared governance, ASO Senators sit on numerous committees in school including the Diversity and the Equity Committee.
John Shepard Stadium.
Besides hosting the Brahmas' football and women's soccer teams, John Shepard Stadium (current capacity 5,500) also has hosted many outdoor professional sporting events in San Fernando Valley history.
From 1976 to 1979, the San Fernando Valley's first professional sports team, the Los Angeles Skyhawks of the American Soccer League, played their home games at the Pierce College stadium.
The Los Angeles Express of the USFL played their last home game here on June 15, 1985. The stadium was expanded to 16,000-person capacity for the game.
Shepard Stadium hosts Nuts for Mutts, an annual dog show and pet fair that raises funds for the New Leash on Life Animal Rescue.
The stadium is also the former home stadium of the San Fernando Valley Quakes men's soccer team, which competed in the USL Premier Development League.
Bond construction.
Since the approval of Los Angeles County Propositions A and AA, Pierce College has been undergoing large-scale renovation.
By 2010 two new “green” complexes now under construction—a . Center for the Sciences and a . Student Services Building—will add increased capacity and classrooms and laboratories with enhanced technologies. Other key improvements are increased parking, new infrastructure and roadways, and renovations to existing facilities.
The 2008 passage of Measure J for Jobs will bring further enhancements to the College, with a new Library/Learning Center/Instructional Media Center, and classroom facilities that offer programs focusing on emerging green technologies and new media in the planning stages.
The S. Mark Taper Foundation Life Science Botanic Garden, completed in 2007, is a “living classroom” in the middle of campus. It features one of the finest collections of drought-resistant plants in the region.
Pierce College prides itself as an environmentally-forward institution, with a 191-kilowatt solar generation system that has 1,274 photovoltaic panels and a 360-kilowatt, natural gas co-generation system. This project is the largest of its kind to be undertaken by a U.S. community college, yielding around 4.4 million kilowatt-hours of electricity a year and reducing Carbon dioxide emissions by more than 1,500 tons over its operating lifetime. The college also has a water retention pond beneath its soccer field, collecting run-off from the adjacent parking lot. The Los Angeles River is nearby to the north. Under propositions A and AA, a new water reclamation facility is also being planned, and the new facilities will meet rigorous Silver-level guidelines set by the U.S. Green Building Council for Leadership in Energy and Environmental Design.
The campus is home to "Old Trapper's Lodge," California Historical Landmark No. 939, an outsider art environment that pays homage to the pioneer upbringing of its creator, John Ehn. It represents the life work of John Ehn (1897–1981), a self-taught artist who wished to pass on a sense of the Old West, derived from personal experiences, myths, and tall tales. From 1951 to 1981, using his family as models, and incorporating memorabilia, the 'Old Trapper' followed his dreams and visions to create the Lodge and its 'Boot Hill.' The artwork was moved from the original site in Sun Valley, CA, and relocated to the college.
Pierce College is one of the nine colleges of the Los Angeles Community College District, and is accredited through the Western Association of Schools and Colleges, a nationally recognized accrediting agency. It is located at 6201 Winnetka Ave. near Victory Boulevard. The campus is accessible from the LACMTA Orange Line station of the same name.
In April 2010, after securing a $100,000 grant, Pierce College launched a 24-hour student-run online radio station, KPCRadio.com, airing a mix of music, sports and locally produced news and features. The station has a faculty adviser and a staff of 20.
Pierce College Farm and Farm Center.
The Pierce College Farm covers 226 acres of the college with several units for their animals. The farm has a $13 million equestrian center used for agricultural students' education that offers UC transferable courses for important animal and veterinary science programs.
In April of every year, the Foundation for Pierce College hosts Farmwalk, an outdoor festival including animals, activities, displays, games and music. The Farmwalk also includes face-painting, a petting-zoo and hayrides for children, all to benefit the Pierce College farm.
The Farm Center on the corner of Victory and De Soto is a 32-acre parcel that was partnered between the Foundation for Pierce College and the McBroom family. The McBroom family have invested nearly $3.5 million to operate the Farm Center which covered utility, labor, insurance, and other operational costs. In October the Foundation sponsored an annual Harvest Festival, featuring pumpkins grown on the Pierce farm, a five-mile (8 km) corn maze, rock climbing, games and rides for the children, a petting zoo, live music and Halloween frights for the whole family. In late December 2014, the Farm Center was evicted from Pierce College, and closed to the public.
The College also serves as a Los Angeles County large animal emergency evacuation center. During a slew of fires in Southern California in 2007, Pierce College sheltered and fed more than 150 horses under the direction of the L.A. County Volunteer Equine Response team. The horses were taken in for free at Pierce, and a veterinarian was onsite. Trained volunteers from Pierce's equestrian program assisted the county rescue effort.
Weather Station.
The Pierce College weather station was one of the first to cooperate with the government to provide archived data online as well as being one of the oldest operational cooperative weather stations in the country. It was founded under the direction of Professor A. Lee Haines on July 1, 1949, two years after the college was founded. In 2009, the Pierce College Weather Station was awarded $85,000 used to provide the station with new sensors that are rare for co-op stations in the U.S. The Weather Station organizes tours showing their equipment and their functions upon request.
Athletics.
Pierce College currently fields 11 athletic teams, which compete in the Western State Conference.
Many athletes receive scholarships to four-year universities after playing at Pierce—and Pierce has some of the top sports facilities in the San Fernando Valley.
In 2009 the Pierce Brahmas won the American Pacific Conference, losing in the first round of bowl playoffs to the National Champs Mt. San Antonio College

</doc>
<doc id="42746" url="https://en.wikipedia.org/wiki?curid=42746" title="Los Angeles Community College District">
Los Angeles Community College District

The Los Angeles Community College District (LACCD) is the community college district serving Los Angeles, California, USA and some of its neighboring cities. Its headquarters are in Downtown Los Angeles. Over the past seventy-seven years LACCD has served as educator to more than three million students. In addition to typical college aged students, the LACCD also serves adults of all ages. Indeed, over half of all LACCD students are older than 25 years of age, and more than a quarter are 35 or older. LACCD educates almost three times as many Latino students and nearly four times as many African-American students as all of the University of California campuses combined. Eighty percent of LACCD students are from underserved populations. The Los Angeles Community College District is the largest community college district in the United States and is one of the largest in the world, the nine colleges within the district offer educational opportunities to students in Los Angeles. The district covers the Los Angeles city limits, San Fernando, Calabasas, Agoura Hills, Hidden Hills, Burbank, West Hollywood, Beverly Hills, Culver City, Inglewood, Alhambra, Monterey Park, San Gabriel, Rosemead (southern portion), Montebello, Commerce, East Los Angeles, Vernon, Huntington Park, Bell, Cudahy, Bell Gardens, South Gate, Gardena, Carson, Lomita, Palos Verdes Estates, Rolling Hills, Rancho Palos Verdes, and the unincorporated south Los Angeles neighborhoods of Florence/Firestone, Athens, and Walnut Park. The LACCD consists of nine colleges and covers an area of more than .
Board of trustees.
Board members are elected by voters in the district for terms of four years. Elections are held every two years, with three members being chosen at one election and four members at the other. The President and Vice President of the Board of Trustees are elected by the Board for one-year terms at the annual organizational meeting. A Second Vice president is elected at the discretion of the Board. A student member is elected annually—the term is June 1 through May 31 of each year.
The Board generally meets twice a month on Wednesday with the public session commencing at 2 p.m. with closed session to follow. On certain occasions, special meetings of the Board are sometimes called to handle business that cannot be handled with completely at regular meetings.
Projects.
The District is modernizing all of its facilities, including all nine of its colleges, through a $6 billion Building Program. The program is funded primarily through bond measures approved by voters in 2001, 2003, and 2008, plus additional funding from the State of California. As of its most recent report, approximately $3.1 billion of the $6 billion has been spent or committed.

</doc>
<doc id="42750" url="https://en.wikipedia.org/wiki?curid=42750" title="Enterprise JavaBeans">
Enterprise JavaBeans

Enterprise JavaBeans (EJB) is a managed, server software for modular construction of enterprise software, and one of several Java APIs. EJB is a server-side software component that encapsulates the business logic of an application. The EJB specification is a subset of the Java EE specification. An EJB web container provides a runtime environment for web related software components, including computer security, Java servlet lifecycle management, transaction processing, and other web services.
Specification.
The EJB specification was originally developed in 1997 by IBM and later adopted by Sun Microsystems (EJB 1.0 and 1.1) in 1999 and enhanced under the Java Community Process as JSR 19 (EJB 2.0), JSR 153 (EJB 2.1), JSR 220 (EJB 3.0), JSR 318 (EJB 3.1) and JSR 345 (EJB 3.2).
The EJB specification intends to provide a standard way to implement the server-side (also called "back-end") 'business' software typically found in enterprise applications (as opposed to 'front-end' user interface software). Such machine code addresses the same types of problems, and solutions to these problems are often repeatedly re-implemented by programmers. Enterprise JavaBeans is intended to handle such common concerns as persistence, transactional integrity, and security in a standard way, leaving programmers free to concentrate on the particular parts of the enterprise software at hand.
General responsibilities.
The EJB specification details how an application server provides the following responsibilities:
Additionally, the Enterprise JavaBean specification defines the roles played by the EJB container and the EJBs as well as how to deploy the EJBs in a container. Note that the current EJB 3.2 specification does not detail how an application server provides persistence (a task delegated to the JPA specification), but instead details how business logic can easily integrate with the persistence services offered by the application server.
Rapid adoption followed by criticism.
The vision was persuasively presented by EJB advocates such as IBM and Sun Microsystems, and Enterprise JavaBeans were quickly adopted by large companies. Problems were quick to appear with initial versions. Some developers felt that the APIs of the EJB standard were far more complex than those developers were used to. An abundance of checked exceptions, required interfaces, and the implementation of the bean class as an abstract class were unusual and counter-intuitive for programmers. The problems that the EJB standard was attempting to address, such as object-relational mapping and transactional integrity, were complex; however many programmers found the APIs to be just as difficult, leading to a perception that EJBs introduced complexity without delivering real benefits.
In addition, businesses found that using EJBs to encapsulate business logic brought a performance penalty. This is because the original specification only allowed for remote method invocation through CORBA (and optionally other protocols), even though the large majority of business applications actually do not require this distributed computing functionality. The EJB 2.0 specification addressed this concern by adding the concept of local interfaces which could be called directly without performance penalties by applications that were not distributed over multiple servers.
The complexity issue continued to hinder EJB's acceptance. Although developer tools made it easy to create and use EJBs by automating most of the repetitive tasks, these tools did not make it any easier to learn how to use the technology. Moreover, a counter-movement had grown up on the grass-roots level among programmers. The main products of this movement were the so-called 'lightweight' (i.e. in comparison to EJB) technologies of Hibernate (for persistence and object-relational mapping) and the Spring Framework (which provided an alternate and far less verbose way to encode business logic). Despite lacking the support of big businesses, these technologies grew in popularity and were adopted by businesses.
Reinventing EJBs.
Gradually an industry consensus emerged that the original EJB specification's primary virtue — enabling transactional integrity over distributed applications — was of limited use to most enterprise applications, and the functionality delivered by simpler frameworks like Spring and Hibernate was more useful.
Accordingly, the EJB 3.0 specification (JSR 220) was a radical departure from its predecessors, following this new paradigm. It shows a clear influence from Spring in its use of plain Java objects, and its support for dependency injection to simplify configuration and integration of heterogeneous systems. Gavin King, the creator of Hibernate, participated in the EJB 3.0 process and is an outspoken advocate of the technology. Many features originally in Hibernate were incorporated in the Java Persistence API, the replacement for entity beans in EJB 3.0. The EJB 3.0 specification relies heavily on the use of annotations (a feature added to the Java language with its 5.0 release) and convention over configuration to enable a much less verbose coding style.
Accordingly, in practical terms EJB 3.0 is much more lightweight and nearly a completely new API, bearing little resemblance to the previous EJB specifications.
Example.
The following shows a basic example of what an EJB looks like in code:
The above defines a service class for persisting a Customer object (via O/R mapping). The EJB takes care of managing the persistence context and the addCustomer() method is transactional and thread-safe by default. As demonstrated, the EJB focuses only on business logic and persistence and knows nothing about any particular presentation.
Such an EJB can be used by a class in e.g. the web layer as follows:
The above defines a JavaServer Faces (JSF) backing bean in which the EJB is injected by means of the @EJB annotation. Its addCustomer method is typically bound to some UI component, like a button. Contrary to the EJB, the backing bean does not contain any business logic or persistence code, but delegates such concerns to the EJB. The backing bean does know about a particular presentation, of which the EJB had no knowledge.
Types of Enterprise Beans.
An EJB container holds two major types of beans:
Session beans.
Stateful Session Beans.
Stateful Session Beans are business objects having state: that is, they keep track of which calling client they are dealing with throughout a session and thus access to the bean instance is strictly limited to only one client at a time. If concurrent access to a single bean is attempted anyway the container serializes those requests, but via the @AccessTimeout annotation the container can instead throw an exception. Stateful session beans' state may be persisted (passivated) automatically by the container to free up memory after the client hasn't accessed the bean for some time. The JPA extended persistence context is explicitly supported by Stateful Session Beans.
Stateless Session Beans.
Stateless Session Beans are business objects that do not have state associated with them. However, access to a single bean instance is still limited to only one client at a time, concurrent access to the bean is prohibited. If concurrent access to a single bean is attempted, the container simply routes each request to a different instance. This makes a stateless session bean automatically thread-safe. Instance variables can be used during a single method call from a client to the bean, but the contents of those instance variables are not guaranteed to be preserved across different client method calls. Instances of Stateless Session beans are typically pooled. If a second client accesses a specific bean right after a method call on it made by a first client has finished, it might get the same instance. The lack of overhead to maintain a conversation with the calling client makes them less resource-intensive than stateful beans. 
Singleton Session Beans.
Singleton Session Beans are business objects having a global shared state within a JVM. Concurrent access to the one and only bean instance can be controlled by the container (Container-managed concurrency, CMC) or by the bean itself (Bean-managed concurrency, BMC). CMC can be tuned using the @Lock annotation, that designates whether a read lock or a write lock will be used for a method call. Additionally, Singleton Session Beans can explicitly request to be instantiated when the EJB container starts up, using the @Startup annotation. 
Message driven beans.
Message Driven Beans are business objects whose execution is triggered by messages instead of by method calls. The Message Driven Bean is used among others to provide a high level ease-of-use abstraction for the lower level JMS (Java Message Service) specification. It may subscribe to JMS message queues or message topics, which typically happens via the activationConfig attribute of the @MessageDriven annotation. They were added in EJB to allow event-driven processing. Unlike session beans, an MDB does not have a client view (Local/Remote/No-interface), i. e. clients cannot look-up an MDB instance. An MDB just listens for any incoming message on, for example, a JMS queue or topic and processes them automatically. Only JMS support is required by the Java EE spec, but Message Driven Beans can support other messaging protocols. Such protocols may be asynchronous but can also be synchronous. Since session beans can also be synchronous or asynchronous, the prime difference between session- and message driven beans is not the synchronicity, but the difference between (object oriented) method calling and messaging. 
Execution.
EJBs are deployed in an EJB container, typically within an application server. The specification describes how an EJB interacts with its container and how client code interacts with the container/EJB combination. The EJB classes used by applications are included in the package. (The package is a service provider interface used only by EJB container implementations.)
Clients of EJBs do not instantiate those beans directly via Java's new operator, but instead have to obtain a reference via the EJB container. This reference is usually not a reference to the implementation bean itself, but to a proxy, which either dynamically implements the local or remote business interface that the client requested or dynamically implements a sub-type of the actual bean. The proxy can then be directly cast to the interface or bean. A client is said to have a 'view' on the EJB, and the local interface, remote interface and bean type itself respectively correspond with the local view, remote view and no-interface view.
This proxy is needed in order to give the EJB container the opportunity to transparently provide cross-cutting (AOP-like) services to a bean like transactions, security, interceptions, injections, remoting, etc.
E.g. a client invokes a method on a proxy, which will then first start a transaction with the help of the EJB container and then call the actual bean method. When the actual bean method returns, the proxy ends the transaction (i.e. by committing it or doing a rollback) and transfers control back to the client.
Transactions.
EJB containers must support both container managed ACID transactions and bean managed transactions.
Container-managed transactions (CMT) are by default active for calls to session beans. That is, no explicit configuration is needed. This behavior may be declaratively tuned by the bean via annotations and if needed such configuration can later be overridden in the deployment descriptor. Tuning includes switching off transactions for the whole bean or specific methods, or requesting alternative strategies for transaction propagation and starting or joining a transaction. Such strategies mainly deal with what should happen if a transaction is or isn't already in progress at the time the bean is called. The following variations are supported:
Alternatively, the bean can also declare via an annotation that it wants to handle transactions programmatically via the JTA API. This mode of operation is called Bean Managed Transactions (BMT), since the bean itself handles the transaction instead of the container.
Events.
JMS (Java Message Service) is used to send messages from beans to clients, to let clients receive asynchronous messages from these beans. 
MDBs can be used to receive messages from clients asynchronously using either a JMS 
Queue or a Topic.
Naming and directory services.
As an alternative to injection, clients of an EJB can obtain a reference to the session bean's proxy object (the EJB stub) using Java Naming and Directory Interface (JNDI). This alternative can be used in cases where injection is not available, such as in non-managed code or standalone remote Java SE clients, or when it's necessary to programmatically determine which bean to obtain.
JNDI names for EJB session beans are assigned by the EJB container via the following scheme:
"(entries in square brackets denote optional parts)"
A single bean can be obtained by any name matching the above patterns, depending on the 'location' of the client. Clients in the same module as the required bean can use the module scope and larger scopes, clients in the same application as the required bean can use the app scope and higher, etc.
E.g. code running in the same module as the CustomerService bean "(as given by the example shown earlier in this article)" could use the following code to obtain a (local) reference to it:
Remoting/distributed execution.
EJB session beans have elaborate support for remoting.
For communication with a client that's also written in the Java programming language a session bean can expose a remote-view via an @Remote annotated interface. This allows those beans to be called from clients in other JVMs which themselves may be located on other (remote) systems. From the point of view of the EJB container, any code in another JVM is remote.
Stateless- and Singleton session beans may also expose a "web service client view" for remote communication via WSDL and SOAP or plain XML. This follows the JAX-RPC and JAX-WS specifications. JAX-RPC support however is proposed for future removal. To support JAX-WS, the session bean is annotated with the @WebService annotation, and methods that are to be exposed remotely with the @WebMethod annotation..
Although the EJB specification does not mention exposure as RESTful web services in any way and has no explicit support for this form of communication, the JAX-RS specification does explicitly support EJB. Following the JAX-RS spec, Stateless- and Singleton session beans can be root resources via the @Path annotation and EJB business methods can be mapped to resource methods via the @GET, @PUT, @POST and @DELETE annotations. This however does not count as a "web service client view", which is used exclusively for JAX-WS and JAX-RPC.
Communication via web services is typical for clients not written in the Java programming language, but is also convenient for Java clients who have trouble reaching the EJB server via a firewall. Additionally, web service based communication can be used by Java clients to circumvent the arcane and ill-defined requirements for the so-called "client-libraries"; a set of jar files that a Java client must have on its class-path in order to communicate with the remote EJB server. These client-libraries potentially conflict with libraries the client may already have (for instance, if the client itself is also a full Java EE server) and such a conflict is deemed to be a very hard or impossible to resolve.
Message Driven Beans have no specific support for remoting, but being listeners to end-points (e.g. JMS queues) they are implicitly remote components by virtue of the properties of whatever type of end-point they are listening to.
Security.
The EJB Container is responsible for ensuring the client code has sufficient access rights to an EJB. Security aspects can be declaratively applied to an EJB via annotations.
Legacy.
Home interfaces and required business interface.
With EJB 2.1 and earlier, each EJB had to provide a Java implementation class and two Java interfaces. The EJB container created instances of the Java implementation class to provide the EJB implementation. The Java interfaces were used by client code of the EJB.
The two interfaces, referred to as the "Home" and the "Remote" interface, specified the signatures of the EJB's remote methods. The methods were split into two groups:
Required deployment descriptor.
With EJB 2.1 and earlier, the EJB specification required a deployment descriptor to be present. This was needed to implement a mechanism that allowed EJBs to be deployed in a consistent manner regardless of the specific EJB platform that was chosen. Information about how the bean should be deployed (such as the name of the home or remote interfaces, whether and how to store the bean in a database, etc.) had to be specified in the deployment descriptor.
The deployment descriptor is an XML document having an entry for each EJB to be deployed. This XML document specifies the following information for each EJB:
Old EJB containers from many vendors required more deployment information than that in the EJB specification. They would require the additional information as separate XML files, or some other configuration file format. An EJB platform vendor generally provided their own tools that would read this deployment descriptor, and possibly generated a set of classes that would implement the now deprecated Home and Remote interfaces.
Since EJB 3.0 (JSR 220), the XML descriptor is replaced by Java annotations set in the Enterprise Bean implementation (at source level), although it is still possible to use an XML descriptor instead of (or in addition to) the annotations. If an XML descriptor and annotations are both applied to the same attribute within an Enterprise Bean, the XML definition overrides the corresponding source-level annotation, although some XML elements can also be additive (e.g., an activation-config-property in XML with a different name than already defined via an @ActivationConfigProperty annotation will be added instead of replacing all existing properties).
Container variations.
Starting with EJB 3.1, the EJB specification defines two variants of the EJB container; a full version and a limited version. The limited version adheres to a proper subset of the specification called EJB 3.1 Lite and is part of Java EE 6's web profile (which is itself a subset of the full Java EE 6 specification).
EJB 3.1 Lite excludes support for the following features:
EJB 3.2 Lite excludes less features. Particularly it no longer excludes @Asynchronous and @Schedule/@Timeout, but for @Schedule it does not support the "persistent" attribute that full EJB 3.2 does support. The complete excluded list for EJB 3.2 Lite is:
Version history.
EJB 3.2, final release (2013-05-28).
JSR 345. Enterprise JavaBeans 3.2 was a relatively minor release that mainly contained specification clarifications and lifted some restrictions that were imposed by the spec but over time appeared to serve no real purpose. A few existing full EJB features were also demanded to be in EJB 3 lite and functionality that was proposed to be pruned in EJB 3.1 was indeed pruned (made optional).
The following features were added:
EJB 3.1, final release (2009-12-10).
JSR 318. The purpose of the Enterprise JavaBeans 3.1 specification is to further simplify the EJB architecture by reducing its complexity from the developer's point of view, while also adding new functionality in response to the needs of the community:
EJB 3.0, final release (2006-05-11).
JSR 220 - "Major changes":
This release made it much easier to write EJBs, using 'annotations' rather than the complex 'deployment descriptors' used in version 2.x. The use of home and remote interfaces and the ejb-jar.xml file were also no longer required in this release, having been replaced with a business interface and a bean that implements the interface.
EJB 2.1, final release (2003-11-24).
JSR 153 - "Major changes":
EJB 2.0, final release (2001-08-22).
JSR 19 - "Major changes":
"Overall goals":
EJB 1.1, final release (1999-12-17).
"Major changes":
"Goals" for Release 1.1:
EJB 1.0 (1998-03-24).
Announced at JavaOne 1998, Sun's third Java developers conference (March 24 through 27)
"Goals" for Release 1.0:

</doc>
<doc id="42751" url="https://en.wikipedia.org/wiki?curid=42751" title="J. Michael Straczynski">
J. Michael Straczynski

Joseph Michael Straczynski (; born July 17, 1954), known professionally as J. Michael Straczynski and informally as Joe Straczynski or jms, is an American writer and producer. He works in films, television series, novels, short stories, comic books, radio dramas and other media. Straczynski is a playwright, former journalist, and author of "The Complete Book of Scriptwriting". He was the creator and showrunner for the science fiction television series "Babylon 5", its spin-off "Crusade", as well as "Jeremiah", a series loosely based on Hermann Huppen's comics. Straczynski wrote 92 out of the 110 "Babylon 5" episodes, notably including an unbroken 59-episode run through the third and fourth seasons, and all but one episode of the fifth season. He also wrote the four "Babylon 5" TV movies produced alongside the series. From 2001 to 2007, he was the writer for the long-running Marvel comic book series "The Amazing Spider-Man". He also famously wrote for "Thor", "Superman", the "" original graphic novels, "Before Watchmen" and "Wonder Woman".
In 2009, Straczynski was nominated for the BAFTA Award for his screenplay for "Changeling". His new series, "Sense8", premiered in 2015.
Straczynski is a long-time participant in Usenet and other early computer networks, interacting with fans through various online forums (including GEnie, CompuServe, and America Online) since 1984. He is credited as being the first TV producer ("showrunner" in Hollywood parlance) to directly engage with fans on the Internet, and allow their viewpoints to influence the look and feel of his show. (See "Babylon 5' "s use of the Internet.) Two prominent areas where he had a presence were GEnie and the newsgroup rec.arts.sf.tv.babylon5.moderated.
Straczynski is a graduate of San Diego State University (SDSU), having earned a BA with a double major in psychology and sociology (with minors in philosophy and literature). While at SDSU, he wrote for the student newspaper, "The Daily Aztec," at times penning so many articles that the paper was jokingly referred to as the "Daily Joe". Straczynski resides in the Los Angeles area.
Early years.
Straczynski was born in Paterson, New Jersey, and is the son of Charles Straczynski, a manual laborer, and Evelyn Straczynski (née Pate). He was raised in Newark, New Jersey; Kankakee, Illinois; Dallas, Texas; Chula Vista, California, where he graduated from high school; and San Diego, California. Straczynski's family religion was Catholic, and he has Belarusian ancestry. His grandparents lived in the area which today belongs to Belarus, and fled to America from the Russian Revolution; his father was born in the US, but lived in Poland, Germany and Russia.
Straczynski cut his teeth writing plays, having several produced at Southwestern College and San Diego State University before finally publishing his adaptation of "Snow White" with Performance Publishing. Several other plays were produced around San Diego, including "The Apprenticeship" for the Marquis Public Theater. During the late 1970s, Straczynski also became the on-air entertainment reviewer for KSDO-FM and wrote several radio plays before being hired as a scriptwriter for the radio drama "Alien Worlds". He also produced his first television project in San Diego, "Marty Sprinkle" for KPBS-TV as well as worked on the XETV-TV project "Disasterpiece Theatre". While in San Diego he became a journalist for the "Los Angeles Times" as a special San Diego correspondent and also worked for "San Diego Magazine" and "The San Diego Reader". In 1981 he landed a contract with "Writer's Digest" to write a book about scriptwriting.
He and Kathryn M. Drennan, whom he met at San Diego State, moved to Los Angeles on April 1, 1981. They would marry in 1983, and separate in 2002. He worked on his book while planning a transition to television. The book's first edition was published in 1982. In Los Angeles he worked for the "Los Angeles Herald-Examiner", the "Los Angeles Times", the "Los Angeles Reader", "TV-Cable Week", and "People" magazine. He quit journalism after working for "People", and in 1983, he wrote a spec script for the show "He-Man and the Masters of the Universe" and the producers of He-Man bought it as well as other scripts and then hired Straczynski as a staff writer.
According to the jacket bio for the first edition of his scriptwriting text (see Print below), Straczynski had a play produced when he was 17, a sitcom produced when he was 21, and sold his first movie script when he was 24. It should be noted, however, that these first two credits were for volunteer public radio, and not professional script sales. By 28, his credits included television and film scripts, radio scripts for "Alien Worlds" and the Mutual Broadcasting System, a dozen plays, and more than 150 newspaper and magazine articles. He taught his craft for years at lectures and seminars in California and elsewhere.
He spent five years from 1987 to 1992 co-hosting the "Hour 25" radio talk show on KPFK-FM Los Angeles with Larry DiTillio.
Radio.
Straczynski has written for radio drama, including the series "Alien Worlds" and scripts for "Mutual Radio Theater". He wrote a script called "Where No Shadows Fall" for the company Airstage which was produced through KPBS in San Diego and aired on its radio series for the blind. The program was later aired on KPFK-FM in Los Angeles in 1982.
Straczynski has also been an on-air personality. He began by doing a weekly entertainment segment on KSDO News Radio in San Diego from 1978–1980. In Los Angeles, he put in five years as on-air host of the science fiction talk show Hour 25, which aired on KPFK 90.7 FM from 10 p.m. until midnight. During his tenure, he interviewed such luminaries as John Carpenter, Neil Gaiman, Ray Bradbury, Harlan Ellison and other writers, producers, actors and directors.
In 2000, Straczynski returned to radio drama with "The City of Dreams" for scifi.com and an original 20-part radio drama series entitled "The Adventures of Apocalypse Al" for the Canadian Broadcasting Corporation that was to debut in 2007 but has not yet aired.
Television.
Animation.
Straczynski was a fan of the cartoon "He-Man and the Masters of the Universe". He wrote a spec script in 1984 and sent it directly to Filmation. They purchased his script, bought several others, and hired him on staff. During this time he became friends with Larry DiTillio, and when Filmation produced the He-Man spinoff "", they both worked as story editors on the show. However, Filmation refused to give them credit on-screen and Straczynski and DiTillio both left and found work with DIC on "Jayce and the Wheeled Warriors".
Straczynski and DiTillio also worked to create an animated version of "Elfquest" but that project soon fell through when CBS attempted to retool the show to appeal to younger audiences.
While working on "Jayce", Straczynski was hired to come aboard the Len Janson and Chuck Menville project to adapt the movie "Ghostbusters" to an animated version called "The Real Ghostbusters". When Janson and Menville learned that there was not only a 13-episode order but a 65-episode syndication order as well, they decided that the workload was too much and that they would only work on their own scripts. DIC head Jean Chalopin asked Straczynski to take on the task of story editing the entire 78-episode block as well as writing his own scripts. After the show's successful first season, consultants were brought in to make suggestions for the show, including changing Janine to a more maternal character, giving every character a particular "job" (Peter is the funny one, Egon is the smart one, and Winston, the only black character, was to be the driver), and to add kids into the show. Straczynski left at this point and Janson and Menville took on the story editing job for the second network season. Straczynski then developed a show called "Spiral Zone" but left after only one script when his concept for the show was drastically altered and took his name off the series, substituting the pseudonym "Fettes Grey" (derived from the names of the grave robbers in "The Body Snatcher").
Straczynski also wrote for "CBS Storybreak", writing an adaptation of Evelyn Sibley Lampman's "The Shy Stegosaurus of Cricket Creek").
Live action and the 1988 Writer's Strike.
After leaving animation, Straczynski freelanced for "The Twilight Zone" writing an episode entitled ("What Are Friends For"), and for Shelley Duvall's "Nightmare Classics", adaptating "The Strange Case of Dr. Jekyll and Mr. Hyde", which was nominated for a Writer's Guild Award).
Straczynski was then offered the position of story editor on the syndicated live-action science fiction series "Captain Power and the Soldiers of the Future". Straczynski constructed a season long arc with lasting character changes and wrote a third of the scripts himself. After one season, the toy company Mattel demanded more input into the show, causing Straczynski to quit. He recommended DiTillio to take over the job as story editor for a second season, but the toy company financing fell through and that season was never produced.
Soon after, the 1988 Writers Guild of America strike began. Straczynski met Harlan Ellison during this time and would later become friends with him.
After the strike ended, the producers of the new "Twilight Zone" needed to create more episodes to be able to sell the series into syndication with a complete 65-episode package. They hired Straczynski as executive story editor to fill in the remaining number of needed episodes. Straczynski wrote many of the scripts himself. In addition, one episode, "Crazy as a Soup Sandwich", was written by Ellison.
Network shows and Babylon 5.
After leaving "Twilight Zone", his agent of the time asked him to pitch for the show "Jake and the Fatman". Initially wary, Straczynski finally did and was hired on as story editor under Jeri Taylor and David Moessinger. When Taylor and Moessinger left the show, Straczynski left too as an act of solidarity.
When Moessinger was hired as executive producer for "Murder, She Wrote", he offered Straczynski a job as co-producer. Straczynski joined "Murder" for two seasons and wrote 7 produced episodes. Moessinger and Straczynski moved the protagonist, Jessica Fletcher, from the sleepy Maine town of Cabot Cove to New York City to revitalize the show. The move effectively brought the show back into the top ten from the mid-thirties where it had fallen. Straczynski made Jessica an instructor in writing and criminology, and he emphasized her role as a working writer, with all the deadlines and problems involved in that profession.
Straczynski also wrote one episode of "Walker, Texas Ranger" for Moessinger between Babylon 5's pilot episode and the start of the first season.
In late 1991, Warner Bros. contracted with Straczynski and Doug Netter as partners to produce "Babylon 5" as the flagship program for the new Prime Time Entertainment Network.
Straczynski and Netter hired many of the people from "Captain Power", as well as hiring Ellison as a consultant and DiTillio as a story editor. Babylon 5 won two Emmy Awards, back-to-back Hugo Awards, and dozens of other awards. Straczynski wrote 92 of the 110 episodes, as well as the pilot and five television movies. The show is a character-driven space opera and features an intentional emphasis on realism in its portrayal of space operations. It also pioneered extensive use of CGI for its special effects. Babylon 5 was produced and broadcast for 5 seasons completing Stracynski's planned story arc. Its sequel, "Crusade", was produced for the TNT Network, however it ended with only 13 episodes. Production was halted before the first episode aired. Fans feel that TNT unfairly interfered with the creative process before pulling the financial plug. Fans also feel that the show should have been delivered to the public and ratings captured prior to the decision to end.
In 2005, Straczynski began publishing his "Babylon 5" scripts. This process ended in June 2008, with the scripts no longer being available from the end of July of that year. His scripts for the television movies were published for a limited time in January 2009.
Recent television projects.
Straczynski also ran "Jeremiah", loosely based on the Belgian post-apocalyptic comic of the same name, from 2002-2004. Straczynski ran the series for two seasons but was frustrated with the conflicting directions that MGM and Showtime wanted from the show, and even used the pseudonym "Fettes Grey" for the first time since "Spiral Zone" on one of the scripts. In the second season, Straczynski decided to leave the show if things did not improve, and the show ended after 2 seasons.
In 2004, Straczynski was approached by Paramount Studios to become a producer of the "" series. He declined, believing that he would not be allowed to take the show in the direction he felt it should go. He did write a treatment for a new "Star Trek" series with colleague Bryce Zabel.
Straczynski also wrote and produced the pilot ', a pilot for the SciFi Network, and wrote, directed and produced ' as a two-hour direct-to-DVD movie.
"Sense8", a new science fiction television series created by Straczynski and the Wachowskis was ordered straight-to-series by Netflix on March 2013. "Sense8" first season debuted on June 2015 on Netflix, from Studio JMS and Georgeville Television. Straczynski executive produced and co-wrote all 12 episodes of the first season with fellow creators, executive producers, and directors Lilly and Lana Wachowski. On August 2015, Netflix renewed "Sense8" for a second season.
Straczynski was also hired to adapt "Red Mars" for Spike TV, based on the Kim Stanley Robinson novels, with Vince Gerardis as producer. On December 2015, Spike TV gave a 10-episode straight-to-series order to "Red Mars", set to premiere on January 2017, with Straczynski serving as writer, executive producer, and showrunner through Studio JMS , and production set to begin on Summer 2016.
Film.
Straczynski worked on feature film and television movies. He wrote an adaptation of Robert Louis Stevenson's "The Strange Case of Dr. Jekyll and Mr. Hyde" for the Showtime network, which was nominated for a Writer's Guild of America award, and a "Murder, She Wrote" movie, "," which he produced.
In 2006, Straczynski was hired to write a feature film based on the story of King David for Universal by producers Erwin Stoff and Akiva Goldsman.
Straczynski announced on February 23, 2007 that he had been hired to write the feature film adaptation of Max Brooks's "New York Times"-bestselling novel "World War Z" for Paramount Pictures and Brad Pitt's production company, Plan B, taking screen story credit on the finished film.
In June 2007, it was announced that Straczynski had written a feature screenplay for the "Silver Surfer" movie for Fox, the production of which would depend on the success of the "". Additionally, he has written a script for Tom Hanks' Playtone Productions and Universal Pictures called "They Marched into Sunlight" based upon the Pulitzer nominated novel of the same name and an outline by Paul Greengrass, for Greengrass to direct, should it get a greenlight.
In June 2008, "Daily Variety" named Straczynski one of the top Ten Screenwriters to Watch. They announced Straczynski was writing "Lensman" for Ron Howard (to whom he had sold a screenplay entitled "The Flickering Light"), that he was selling another spec, "Proving Ground", to Tom Cruise and United Artists.
In 2008, Straczynski wrote a draft of "Ninja Assassin" for Joel Silver, which he completed in just 53 hours. The film was produced by the Wachowskis and released on November 25, 2009.
In 2008, Universal Pictures and Imagine Entertainment premiered Straczynski's feature thriller "Changeling", starring Angelina Jolie. The film was directed by Clint Eastwood, since originally slated director Ron Howard declined due to scheduling conflicts.
"Changeling" was one of 20 films placed in competition at the 2008 Cannes Film Festival, and subsequently received eight nominations for the BAFTA Award, including a nomination for Best Original Screenplay.
In October 2008, it was announced that Straczynski was engaged to pen a remake of the science fiction classic "Forbidden Planet".
In the fall of 2009, it was reported that Straczynski was writing a movie titled "Shattered Union" for Jerry Bruckheimer and Disney. The screenplay, based on the video game of that name, concerns itself with a present-day American civil war.
Straczynski is credited as "story writer" along with Mark Protosevich for the 2011 film, "Thor". He also makes a cameo appearance in the film, his first appearance in a movie and his second appearance as an actor (the first being "Sleeping In Light," the final episode of "Babylon 5").
On October 2012, Valiant Entertainment announced a live-action feature film adaptation on its comic book series "Shadowman", written and executive produced by Straczynski.
"The Flickering Light", Straczynski's directorial debut, was announced on February 2013, with the WWII drama set to be written and produced by Straczynski through his Studio JMS.
Straczynski and Studio JMS optioned Harlan Ellison's short story "'Repent, Harlequin!' Said the Ticktockman", who granted the option only after reading a finished screenplay written by Straczynski.
On San Diego Comic-Con 2014, it was announced that Straczynski and Graphic India would team up with Chernin Entertainment to produce a feature film adaptation of his upcoming graphic novel "Titans", to be written and produced by Straczynski, through Studio JMS.
Print.
Novels, short stories and nonfiction.
Straczynski is the author of three horror novels — "Demon Night", "Othersyde", and "Tribulations" — and nearly twenty short stories, many of which are collected in two compilations — "Tales from the New Twilight Zone" and "Straczynski Unplugged". He wrote the outlines for nine of the canonical "Babylon 5" novels, supervised the three produced "B5" telefilm novelizations ("In the Beginning", "Thirdspace", and "A Call to Arms"), and is the author of four "Babylon 5" short stories published in magazines, not yet reprinted ().
Straczynski has been a journalist, reviewer, and investigative reporter, publishing over 500 articles in such publications as the "Los Angeles Times", the "Los Angeles Herald-Examiner", "Writer's Digest", "Penthouse", "San Diego Magazine", "Twilight Zone Magazine", the "San Diego Reader", the "Los Angeles Reader" and "Time".
Straczynski wrote "The Complete Book of Scriptwriting" (ISBN 1-85286-882-1), often used as a text in introductory screenwriting courses, and is now in its third edition.
Comic books.
Straczynski has long been a comic fan, and began writing comics in the late 1980s. His work in comics includes the adaptations of "Captain Power and the Soldiers of the Future", "The Twilight Zone", "Star Trek" and "Babylon 5". In 1999 he created "Rising Stars" for Top Cow/Image Comics. Eventually he worked mostly under his own imprint – Joe's Comics – for which he wrote the "Midnight Nation" miniseries, and the illustrated fantasy parable "Delicate Creatures". Marvel Comics then signed him to an exclusive contract, beginning with a run on "The Amazing Spider-Man", from 2001–2007. He took over the series with issue #30 (cover dated June 2001). Straczynski and artist John Romita Jr. crafted an acclaimed story for "The Amazing Spider-Man" #36 (Dec. 2001) in response to the September 11 attacks. He wrote or co-wrote several major Spider-Man story arcs including "", "", and "". He later wrote several other Marvel titles including "Supreme Power", "Strange", "Fantastic Four", "Thor", and mini-series featuring the Silver Surfer and a "What If" scenario, "Bullet Points".
When his exclusive contract with Marvel ended, he was announced as the writer for a run on "The Brave and the Bold" for DC Comics. He collaborated with artist Shane Davis on an out-of-continuity original graphic novel starring Superman titled "". The story features a young Superman and focus on his decision about the role he want to assume in life. On March 8, 2010 it was announced he would be taking over writing duties for the monthly "Superman" title with a story arc entitled "", and the "Wonder Woman" title, beginning with issues 701 and 601 respectively. Less than a year later he was asked by DC to step away from both titles in order to concentrate on the second volume of "Superman: Earth One" and handed them over to Chris Roberson and Phil Hester to finish his Superman and Wonder Woman stories respectively. In 2012, Straczynski wrote "Before Watchmen: Dr. Manhattan" drawn by Adam Hughes and "Before Watchmen: Nite Owl" drawn by Andy Kubert and Joe Kubert. A second volume of "Superman: Earth One" was released later that same year.
The Joe's Comics line was revived at Image Comics in 2013 with the launch of "Ten Grand" drawn by Ben Templesmith and "Sidekick" drawn by Tom Mandrake.
Dynamite Entertainment announced on July 2013 a new 12 issue "Twilight Zone" comic book series penned by Straczynski. The series ran for its projected 12 issues, from December 2013 to February 2015, with art by Guiu Vilanova. Straczynski was announced as the writer of "Terminator Salvation: The Final Battle", a 12 issue comic book series from Dark Horse Comics, along with artist Pete Woods.
DC Comics announced in San Diego Comic-Con 2015 "The Flash: Earth One", a new graphic novel of its Earth One line featuring The Flash and written by Straczynski, set to be published in 2016.
Bibliography.
Comics.
Joe's Comics.
Joe's Comics was revived at Image Comics in 2013:
Awards and recognition.
His personal awards include the 1996 Hugo Award for Best Dramatic Presentation (shared with director Janet Greek) for the "Babylon 5" episode, "The Coming of Shadows" and the 1997 Hugo Award for Dramatic Presentation (shared with director David Eagle) for the "Babylon 5" episode, "Severed Dreams". Along with the "Babylon 5" cast and crew he received the 1994 Visions Of The Future Award from the Space Frontier Foundation, and in 1998 he received the Ray Bradbury Award for Outstanding Dramatic Presentation for the television series "Babylon 5".
Along with John Romita Jr. and Scott Hanna he was the 2002 Eisner Award winner for Best Serialized Story for his work on the "Coming Home" storyline in "The Amazing Spider-Man". In 2005, he was voted Favorite Comics Writer by UK readers and received that year's Eagle Award. He was also among the recipients of the 1994 Inkpot Award. In 2008, as screenwriter, he was among the recipients of the Christopher Award issued to the movie "Changeling". In 2013 he received the prestigious International Icon Award from the San Diego Comic-Con International, only the eighth time this award has been given with past recipients including George Lucas, Neil Gaiman, Ray Bradbury, Stan Lee, and Matt Groening.
Award nominations include the 2009 BAFTA Award, for his screenplay for "Changeling". Three separate 2009 Eisner Award nominations – for Best Limited Edition ("The Twelve") along with Chris Weston, Best Continuing Series ("Thor") along with Olivier Coipel and Mark Morales, and Best Writer ("Thor"). In 1988, his novel, "Demon Night", was presented for consideration of that year's Bram Stoker award, under the category of Best First Novel. He was also nominated for a Writers Guild Award and a Cable Ace Award for his adaptation of Robert Louis Stevenson's "The Strange Case of Dr. Jekyll and Mr. Hyde", produced for Showtime Network. An asteroid, discovered in 1992 at the Kitt Peak National Observatory, was honorarily named 8379 Straczynski.
Studio JMS.
In July 2012, J. Michael Straczynski announced the launch of Studio JMS to produce TV series, movies, comics and, down the road, games and web series. On March 27, 2013 Netflix announced they would produce the show "Sense8" with Studio JMS and the Wachowskis, which aired on June 5, 2015, and earned a season 2 announcement by August 10, 2015.

</doc>
<doc id="42752" url="https://en.wikipedia.org/wiki?curid=42752" title="Sonoluminescence">
Sonoluminescence

Sonoluminescence is the emission of short bursts of light from imploding bubbles in a liquid when excited by sound.
History.
The sonoluminescence effect was first discovered at the University of Cologne in 1934 as a result of work on sonar. H. Frenzel and H. Schultes put an ultrasound transducer in a tank of photographic developer fluid. They hoped to speed up the development process. Instead, they noticed tiny dots on the film after developing and realized that the bubbles in the fluid were emitting light with the ultrasound turned on. It was too difficult to analyze the effect in early experiments because of the complex environment of a large number of short-lived bubbles. (This experiment is also ascribed to N. Marinesco and J. J. Trillat in 1933, which also credits them with independent discovery). This phenomenon is now referred to as multi-bubble sonoluminescence (MBSL).
In 1960 Dr. Peter Jarman from Imperial College of London proposed the most reliable theory of SL phenomenon. The collapsing bubble generates an imploding shock wave that compresses and heats the gas at the center of the bubble to extremely high temperature.
In 1989 an experimental advance was introduced by D. Felipe Gaitan and Lawrence Crum, who produced stable single-bubble sonoluminescence (SBSL). In SBSL, a single bubble trapped in an acoustic standing wave emits a pulse of light with each compression of the bubble within the standing wave. This technique allowed a more systematic study of the phenomenon, because it isolated the complex effects into one stable, predictable bubble. It was realized that the temperature inside the bubble was hot enough to melt steel. Interest in sonoluminescence was renewed when an inner temperature of such a bubble well above one million kelvins was postulated. This temperature is thus far not conclusively proven; rather, recent experiments conducted by the University of Illinois at Urbana–Champaign indicate temperatures around .
Properties.
Sonoluminescence can occur when a sound wave of sufficient intensity induces a gaseous cavity within a liquid to collapse quickly. This cavity may take the form of a pre-existing bubble, or may be generated through a process known as cavitation. Sonoluminescence in the laboratory can be made to be stable, so that a single bubble will expand and collapse over and over again in a periodic fashion, emitting a burst of light each time it collapses. For this to occur, a standing acoustic wave is set up within a liquid, and the bubble will sit at a pressure anti-node of the standing wave. The frequencies of resonance depend on the shape and size of the container in which the bubble is contained.
Some facts about sonoluminescence: 
Spectral measurements have given bubble temperatures in the range from to , the exact temperatures depending on experimental conditions including the composition of the liquid and gas. Detection of very high bubble temperatures by spectral methods is limited due to the opacity of liquids to short wavelength light characteristic of very high temperatures.
Writing in "Nature", chemists David J. Flannigan and Kenneth S. Suslick describe a method of determining temperatures based on the formation of plasmas. Using argon bubbles in sulfuric acid, their data show the presence of ionized molecular oxygen O2+, sulfur monoxide, and atomic argon populating high-energy excited states, which confirms a hypothesis that the bubbles have a hot plasma core. The ionization and excitation energy of dioxygenyl cations, which they observed, is 18 electronvolts. From this they conclude the core temperatures reach at least 20,000 Kelvin.
Rayleigh–Plesset equation.
The dynamics of the motion of the bubble is characterized to a first approximation by the Rayleigh–Plesset equation (named after Lord Rayleigh and Milton Plesset):
This is an approximate equation that is derived from the incompressible Navier–Stokes equations (written in spherical coordinate system) and describes the motion of the radius of the bubble "R" as a function of time "t". Here, "μ" is the viscosity, "p" the pressure, and "γ" the surface tension. The over-dots represent time derivatives. This equation, though approximate, has been shown to give good estimates on the motion of the bubble under the acoustically driven field except during the final stages of collapse. Both simulation and experimental measurement show that during the critical final stages of collapse, the bubble wall velocity exceeds the speed of sound of the gas inside the bubble. Thus a more detailed analysis of the bubble's motion is needed beyond Rayleigh–Plesset to explore the additional energy focusing that an internally formed shock wave might produce.
Mechanism of phenomenon.
The mechanism of the phenomenon of sonoluminescence is unknown. Hypotheses include: hotspot, bremsstrahlung radiation, collision-induced radiation and corona discharges, nonclassical light, proton tunneling, electrodynamic jets and fractoluminescent jets (now largely discredited due to contrary experimental evidence).
In 2002, M. Brenner, S. Hilgenfeldt, and D. Lohse published a 60-page review bubble sonoluminescence" ("Reviews of Modern Physics" 74, 425) that contains a detailed explanation of the mechanism. An important factor is that the bubble contains mainly inert noble gas such as argon or xenon (air contains about 1% argon, and the amount dissolved in water is too great; for sonoluminescence to occur, the concentration must be reduced to 20–40% of its equilibrium value) and varying amounts of water vapor. Chemical reactions cause nitrogen and oxygen to be removed from the bubble after about one hundred expansion-collapse cycles. The bubble will then begin to emit light for Gas Exchange in Single-Bubble Sonoluminescence", Matula and Crum, "Phys. Rev. Lett." 80 (1998), 865-868). The light emission of highly compressed noble gas is exploited technologically in the argon flash devices.
During bubble collapse, the inertia of the surrounding water causes high pressure and high temperature, reaching around 10,000 Kelvin in the interior of the bubble, causing the ionization of a small fraction of the noble gas present. The amount ionized is small enough for the bubble to remain transparent, allowing volume emission; surface emission would produce more intense light of longer duration, dependent on wavelength, contradicting experimental results. Electrons from ionized atoms interact mainly with neutral atoms, causing thermal bremsstrahlung radiation. As the wave hits a low energy trough, the pressure drops, allowing electrons to recombine with atoms and light emission to cease due to this lack of free electrons. This makes for a 160-picosecond light pulse for argon (even a small drop in temperature causes a large drop in ionization, due to the large ionization energy relative to photon energy). This description is simplified from the literature above, which details various steps of differing duration from 15 microseconds (expansion) to 100 picoseconds (emission).
Computations based on the theory presented in the review produce radiation parameters (intensity and duration time versus wavelength) that match experimental results with errors no larger than expected due to some simplifications (e.g., assuming a uniform temperature in the entire bubble), so it seems the phenomenon of sonoluminescence is at least roughly explained, although some details of the process remain obscure.
Any discussion of sonoluminescence must include a detailed analysis of metastability. Sonoluminescence in this respect is what is physically termed a bounded phenomenon meaning that the sonoluminescence exists in a bounded region of parameter space for the bubble; a coupled magnetic field being one such parameter. The magnetic aspects of sonoluminescence are very well documented.
Other proposals.
Quantum explanations.
An unusually exotic hypothesis of sonoluminescence, which has received much popular attention, is the Casimir energy hypothesis suggested by noted physicist Julian Schwinger and more thoroughly considered in a paper by Claudia Eberlein of the University of Sussex. Eberlein's paper suggests that the light in sonoluminescence is generated by the vacuum within the bubble in a process similar to Hawking radiation, the radiation generated at the event horizon of black holes. According to this vacuum energy explanation, since quantum theory holds that vacuum contains virtual particles, the rapidly moving interface between water and gas converts virtual photons into real photons. This is related to the Unruh effect or the Casimir effect. If true, sonoluminescence may be the first observable example of quantum vacuum radiation. The argument has been made that sonoluminescence releases too large an amount of energy and releases the energy on too short a time scale to be consistent with the vacuum energy explanation, although other credible sources argue the vacuum energy explanation might yet prove to be correct.
Nuclear reactions.
Some have argued that the Rayleigh–Plesset equation described above is unreliable for predicting bubble temperatures and that actual temperatures in sonoluminescing systems can be far higher than 20,000 kelvins. Some research claims to have measured temperatures as high as 100,000 kelvins, and speculates temperatures could reach into the millions of kelvins. Temperatures this high could cause thermonuclear fusion. This possibility is sometimes referred to as bubble fusion and is likened to the implosion design used in the fusion component of thermonuclear weapons.
On January 27, 2006, researchers at Rensselaer Polytechnic Institute claimed to have produced fusion in sonoluminescence experiments.
Experiments in 2002 and 2005 by R. P. Taleyarkhan using deuterated acetone showed measurements of tritium and neutron output consistent with fusion. However, the papers were considered low quality and there were doubts cast by a report about the author's scientific misconduct. This made the report lose credibility among the scientific community.
Biological sonoluminescence.
Pistol shrimp (also called "snapping shrimp") produce a type of sonoluminescence from a collapsing bubble caused by quickly snapping its claw. The animal snaps a specialized claw shut to create a cavitation bubble that generates acoustic pressures of up to 80 kPa at a distance of 4 cm from the claw. As it extends out from the claw, the bubble reaches speeds of 60 miles per hour (97 km/h) and releases a sound reaching 218 decibels. The pressure is strong enough to kill small fish. The light produced is of lower intensity than the light produced by typical sonoluminescence and is not visible to the naked eye. The light and heat produced may have no direct significance, as it is the shockwave produced by the rapidly collapsing bubble which these shrimp use to stun or kill prey. However, it is the first known instance of an animal producing light by this effect and was whimsically dubbed "shrimpoluminescence" upon its discovery in 2001. It has subsequently been discovered that another group of crustaceans, the mantis shrimp, contains species whose club-like forelimbs can strike so quickly and with such force as to induce sonoluminescent cavitation bubbles upon impact.
External links.
Newer research papers largely ruling out the vacuum energy explanation

</doc>
<doc id="42754" url="https://en.wikipedia.org/wiki?curid=42754" title="University of Cologne">
University of Cologne

The University of Cologne () was the sixth university to be established in Central Europe and, although it closed in 1789 before being re-established in 1919, it is now one of the largest universities in Germany with more than 48,000 students. The university has been part of the German Universities Excellence Initiative since 2012, and as of 2015 it ranks 156th globally according to "Times Higher Education", 305th according to "QS World University Rankings" and between 151 and 200 according to the "Academic Ranking of World Universities".
History.
1388–1798.
The University of Cologne was established in 1388 as the fourth university in the Holy Roman Empire, after the Charles University of Prague (1348), the University of Vienna (1365) and the Ruprecht Karl University of Heidelberg (1386). The charter was signed by Pope Urban VI. The university began teaching on January 6, 1389.
In 1798, the university was abolished by the French, who had invaded Cologne in 1794, because under the new French constitution, universities were abolished all over France.The last rector Ferdinand Franz Wallraf was able to preserve the university’s Great Seal, now once more in use.
1919–today.
In 1919, the Prussian government endorsed a decision by the Cologne City Council to re-establish the university. On May 19, 1919, the Cologne Mayor Konrad Adenauer signed the charter of the modern university.
At that point, the new university was located in Neustadt-Süd, but relocated to its current campus in Lindenthal on 2 November 1934. The old premises are now being used for the Cologne University of Applied Sciences.
Initially, the university was composed of the Faculty of Commerce, Economics and Social Sciences (successor to the Institutes of Commerce and of Communal and Social Administration) and the Faculty of Medicine (successor to the Academy of Medicine). In 1920, the Faculty of Law and the Faculty of Arts were added, from which latter the School of Mathematics and Natural Sciences was split off in 1955 to form a separate Faculty. In 1980, the two Cologne departments of the Rhineland School of Education were attached to the university as the Faculties of Education and of Special Education. In 1988, the university became a founding member of the Community of European Management Schools and International Companies (CEMS), today's Global Alliance in Management Education.
The University is a leader in the area of economics and is regularly placed in top positions for law and commerce, both for national and international rankings.
Organization.
The University of Cologne is a statutory corporation (Körperschaft des öffentlichen Rechts), operated by the Federal State of North Rhine-Westphalia.
Faculties.
The university is divided into six faculties, which together offer 200 fields of study. The faculties are those of Management, Economics and Social Sciences, Law, Medicine (with the affiliated University clinic), Arts, Mathematics and Natural Sciences and Human Sciences.
Rectors.
On November 24, 2004, Axel Freimuth was elected as the Rector of the University. His four-year term began on April 1, 2005. He succeeded Tassilo Küpper and is the 49th Rector since 1919. He was previously Dean of Mathematics and Natural Sciences.
Students and faculty.
In 2005, the University enrolled 47,203 students, including 3,718 graduate students. In 2003, the number of post-doctoral students was 670.
The number of international students was 6,157 in the Summer Semester of 2005. This amounts to approximately 13% of the total students. Those from developing countries made up about 60%, representing a total of 123 nations. The largest contingents came from Bulgaria (10.5%), Russia (8.8%), Poland (7.4%), China (6.2%) and Ukraine (5.7%).
There are 508 professors at the university, including 70 women. In addition, the university employs 1,549 research assistants, with an additional 765 at the clinic, and 1,462 other assistants (3,736 at the clinic).
Partner universities.
The University of Cologne maintains twenty official partnerships with universities from ten countries. Of these, the partnerships with Clermont-Ferrand I and Pennsylvania State are the oldest partnerships. In addition, Cologne has further cooperations with more than 260 other universities.
Notable alumni and professors.
Over the centuries, scholars from Cologne have been among the most prominent in their fields, beginning with Albertus Magnus and his pupil Thomas Aquinas (both 13th century). Notable alumni of the 20th century include among others Kurt Alder (Nobel Prize in Chemistry 1950), Peter Grünberg (Nobel Prize in Physics 2007), Heinrich Böll (Nobel Prize for Literature), Karl Carstens (president of the Federal Republic of Germany 1979–1984), Gustav Heinemann (president of the Federal Republic of Germany 1969 to 1974), Karolos Papoulias (former president of the Hellenic Republic), and Erich Gutenberg (founder of modern German business studies).

</doc>
<doc id="42758" url="https://en.wikipedia.org/wiki?curid=42758" title="Murasaki Shikibu">
Murasaki Shikibu

Heian women were traditionally excluded from learning Chinese, the written language of government, but Murasaki, raised in her erudite father's household, showed a precocious aptitude for the Chinese classics and managed to acquire fluency. She married in her mid-to late twenties and gave birth to a daughter before her husband died, two years after they were married. It is uncertain when she began to write "The Tale of Genji", but it was probably while she was married or shortly after she was widowed. In about 1005, Murasaki was invited to serve as a lady-in-waiting to Empress Shōshi at the Imperial court, probably because of her reputation as a writer. She continued to write during her service, adding scenes from court life to her work. After five or six years, she left court and retired with Shōshi to the Lake Biwa region. Scholars differ on the year of her death; although most agree on 1014, others have suggested she was alive in 1031.
Murasaki wrote "The Diary of Lady Murasaki", a volume of poetry, and "The Tale of Genji". Within a decade of its completion, "Genji" was distributed throughout the provinces; within a century it was recognized as a classic of Japanese literature and had become a subject of scholarly criticism. Early in the 20th century her work was translated; a six-volume English translation was completed in 1933. Scholars continue to recognize the importance of her work, which reflects Heian court society at its peak. Since the 13th century her works have been illustrated by Japanese artists and well-known ukiyo-e woodblock masters.
Early life.
Murasaki Shikibu was born c. 973 in Heian-kyō, Japan, into the northern Fujiwara clan descending from Fujiwara no Yoshifusa, the first 9th-century Fujiwara regent. The Fujiwara clan dominated court politics until the end of the 11th century through strategic marriages of Fujiwara daughters into the imperial family and the use of regencies. In the late 10th century and early 11th century, Fujiwara no Michinaga arranged his four daughters into marriages with emperors, giving him unprecedented power. Murasaki's great-grandfather, Fujiwara no Kanesuke, had been in the top tier of the aristocracy, but her branch of the family gradually lost power and by the time of Murasaki's birth was at the middle to lower ranks of the Heian aristocracy—the level of provincial governors. The lower ranks of the nobility were typically posted away from court to undesirable positions in the provinces, exiled from the centralized power and court in Kyoto.
Despite the loss of status, the family had a reputation among the literati through Murasaki's paternal great-grandfather and grandfather, both of whom were well-known poets. Her great-grandfather, Fujiwara no Kanesuke, had fifty-six poems included in thirteen of the Twenty-one Imperial Anthologies, the "Collections of Thirty-six Poets" and the "Yamato Monogatari" ("Tales of Yamato"). Her great-grandfather and grandfather both had been friendly with Ki no Tsurayuki, who became notable for popularizing verse written in Japanese. Her father, Fujiwara no Tametoki, attended the State Academy (Daigaku-ryō) and became a well-respected scholar of Chinese classics and poetry; his own verse was anthologized. He entered public service around 968 as a minor official and was given a governorship in 996. He stayed in service until about 1018. Murasaki's mother was descended from the same branch of northern Fujiwara as Tametoki. The couple had three children, a son and two daughters.
The names of women were not recorded in the Heian era. Murasaki's real name is not known; as was customary for women of the period, she went by a nickname, Murasaki Shikibu. Women took nicknames associated with a male relative: "Shikibu" refers to "Shikibu-shō", the Ministry of Ceremonials where her father was a functionary; "Murasaki" may be derived from the color violet associated with wisteria, the meaning of the word "fuji", although it is more likely that "Murasaki" was a court nickname. Michinaga mentions the names of a few ladies-in-waiting in a 1007 diary entry; one, Fujiwara Takako (Kyōshi), may be Murasaki's real name.
In Heian-era Japan, husbands and wives kept separate households; children were raised with their mothers, although the patrilineal system was still followed. Murasaki was unconventional because she lived in her father's household, most likely on Teramachi Street in Kyoto, with her younger brother Nobunori. Their mother died, perhaps in childbirth, when the children were quite young. Murasaki had at least three half-siblings raised with their mothers; she was very close to one sister who died in her twenties.
Murasaki was born at a period when Japan was becoming more isolated, after missions to China had ended and a stronger national culture was emerging. In the 9th and 10th centuries, Japanese gradually became a written language through the development of kana, a syllabary based on abbreviations of Chinese characters. In Murasaki's lifetime men continued to write in Chinese, the language of government, but kana became the written language of noblewomen, setting the foundation for unique forms of Japanese literature.
Chinese was taught to Murasaki's brother as preparation for a career in government, and during her childhood, living in her father's household, she learned and became proficient in classical Chinese. In her diary she wrote, "When my brother ... was a young boy learning the Chinese classics, I was in the habit of listening to him and I became unusually proficient at understanding those passages that he found too difficult to understand and memorize. Father, a most learned man, was always regretting the fact: 'Just my luck,' he would say, 'What a pity she was not born a man! With her brother she studied Chinese literature, and she probably also received instruction in more traditional subjects such as music, calligraphy and Japanese poetry. Murasaki's education was unorthodox. Louis Perez explains in "The History of Japan" that "Women ... were thought to be incapable of real intelligence and therefore were not educated in Chinese." Murasaki was aware that others saw her as "pretentious, awkward, difficult to approach, prickly, too fond of her tales, haughty, prone to versifying, disdainful, cantankerous and scornful". Asian literature scholar Thomas Inge believes she had "a forceful personality that seldom won her friends."
Marriage.
Aristocratic Heian women lived restricted and secluded lives, allowed to speak to men only when they were close relatives or household members. Murasaki's autobiographical poetry shows that she socialized with women but had limited contact with men other than her father and brother; she often exchanged poetry with women but never with men. Unlike most noblewomen of her status, she did not marry on reaching puberty; instead she stayed in her father's household until her mid-twenties or perhaps even to her early thirties.
In 996 when her father was posted to a four-year governorship in Echizen Province, Murasaki went with him, although it was uncommon for a noblewoman of the period to travel such a distance on a trip that could take as long as five days. She returned to Kyoto, probably in 998, to marry her father's friend Fujiwara no Nobutaka (c. 950 – c. 1001), a much older second cousin. Descended from the same branch of the Fujiwara clan, he was a court functionary and bureaucrat at the Ministry of Ceremonials, with a reputation for dressing extravagantly and as a talented dancer. In his late forties at the time of their marriage, he had multiple households with an unknown number of wives and offspring. Gregarious and well known at court, he was involved in numerous romantic relationships that may have continued after his marriage to Murasaki. As was customary, she would have remained in her father's household where her husband would have visited her. Nobutaka had been granted more than one governorship, and by the time of his marriage to Murasaki he was probably quite wealthy. Accounts of their marriage vary: Richard Bowring writes that the marriage was happy, but Japanese literature scholar Haruo Shirane sees indications in her poems that she resented her husband.
The couple's daughter, Kenshi (Kataiko), was born in 999. Two years later Nobutaka died during a cholera epidemic. As a married woman Murasaki would have had servants to run the household and care for her daughter, giving her ample leisure time. She enjoyed reading and had access to romances (monogatari) such as "The Tale of the Bamboo Cutter" and "The Tales of Ise." Scholars believe she may have started writing "The Tale of Genji" before her husband's death; it is known she was writing after she was widowed, perhaps in a state of grief. In her diary she describes her feelings after her husband's death: "I felt depressed and confused. For some years I had existed from day to day in listless fashion ... doing little more than registering the passage of time ... The thought of my continuing loneliness was quite unbearable".
According to legend, Murasaki retreated to Ishiyama-dera at Lake Biwa, where she was inspired to write "The Tale of Genji" on an August night while looking at the moon. Although scholars dismiss the factual basis of the story of her retreat, Japanese artists often depicted her at Ishiyama Temple staring at the moon for inspiration. She may have been commissioned to write the story and may have known an exiled courtier in a similar position to her hero Prince Genji. Murasaki would have distributed newly written chapters of "Genji" to friends who in turn would have re-copied them and passed them on. By this practice the story became known and she gained a reputation as an author.
In her early to mid-thirties, she became a lady-in-waiting ("nyōbō") at court, most likely because of her reputation as an author. Chieko Mulhern writes in "Japanese Women Writers, a Biocritical Sourcebook" that scholars have wondered why Murasaki made such a move at a comparatively late period in her life. Her diary evidences that she exchanged poetry with Michinaga after her husband's death, leading to speculation that the two may have been lovers. Bowring sees no evidence that she was brought to court as Michinaga's concubine, although he did bring her to court without following official channels. Mulhern thinks Michinaga wanted to have Murasaki at court to educate his daughter Shōshi.
Court life.
Heian culture and court life reached a peak early in the 11th century. The population of Kyoto grew to around 100,000 as the nobility became increasingly isolated at the Heian Palace in government posts and court service. Courtiers became overly refined with little to do, insulated from reality, preoccupied with the minutiae of court life, turning to artistic endeavors. Emotions were commonly expressed through the artistic use of textiles, fragrances, calligraphy, colored paper, poetry, and layering of clothing in pleasing color combinations—according to mood and season. Those who showed an inability to follow conventional aesthetics quickly lost popularity, particularly at court. Popular pastimes for Heian noblewomen—who adhered to rigid fashions of floor-length hair, whitened skin and blackened teeth—included having love affairs, writing poetry and keeping diaries. The literature that Heian court women wrote is recognized as some of the earliest and among the best literature written in the Japanese canon.
Rival courts and women poets.
When in 995 Michinaga's two brothers Fujiwara no Michitaka and Fujiwara no Michikane died leaving the regency vacant, Michinaga quickly won a power struggle against his nephew Fujiwara no Korechika (brother to Teishi, Emperor Ichijō's wife), and, aided by his sister Senshi, he assumed power. Teishi had supported her brother Korechika, who was later discredited and banished from court, causing her to lose power. Four years later Michinaga sent Shōshi, his eldest daughter, to Emperor Ichijō's harem when she was about 12. A year after placing Shōshi in the imperial harem, in an effort to undermine Teishi's influence and increase Shōshi's standing, Michinaga had her named Empress although Teishi already held the title. As historian Donald Shively explains, "Michinaga shocked even his admirers by arranging for the unprecedented appointment of Teishi (or Sadako) and Shōshi as concurrent empresses of the same emperor, Teishi holding the usual title of "Lustrous Heir-bearer" "kōgō" and Shōshi that of "Inner Palatine" ("chūgū"), a toponymically derived equivalent coined for the occasion". About five years later, Michinaga brought Murasaki to Shōshi's court, in a position that Bowring describes as a companion-tutor.
Heian Imperial court life was immensely fashionable, but also dissolute. Court women lived in seclusion, were known by nicknames and, through strategic marriages, were used to gain political power. Despite their seclusion, some women wielded considerable influence, often achieved through competitive salons, dependent on the quality of the attendants. Ichijō's mother and Michinaga's sister, Senshi, had an influential salon, and Michinaga probably wanted Shōshi to surround herself with skilled women such as Murasaki to build a rival salon.
Shōshi was 16 to 19 when Murasaki joined her court. According to Arthur Waley, Shōshi was a serious-minded young lady, whose living arrangements were divided between her father's household and her court at the Imperial Palace. She gathered around her talented women writers such as Izumi Shikibu and Akazome Emon—the author of an early vernacular history, "The Tale of Flowering Fortunes". The rivalry that existed among the women is evident in Murasaki's diary, where she wrote disparagingly of Izumi: "Izumi Shikibu is an amusing letter-writer; but there is something not very satisfactory about her. She has a gift for dashing off informal compositions in a careless running-hand; but in poetry she needs either an interesting subject or some classic model to imitate. Indeed it does not seem to me that in herself she is really a poet at all."
Sei Shōnagon, author of the "The Pillow Book", had been in service as lady-in-waiting to Teishi when Shōshi came to court; it is possible that Murasaki was invited to Shōshi's court as a rival to Shōnagon. Teishi died in 1001, before Murasaki entered service with Shōshi, so the two writers were not there concurrently, but Murasaki, who wrote about Shōnagon in her diary, certainly knew of her, and to an extent was influenced by her. Shōnagon's "The Pillow Book" may have been commissioned as a type of propaganda to highlight Teishi's court, known for its educated ladies-in-waiting. Japanese literature scholar Joshua Mostow believes Michinaga provided Murasaki to Shōshi as an equally or better educated woman, so as to showcase Shōshi's court in a similar manner.
The two writers had different temperaments: Shōnagon was witty, clever, and outspoken; Murasaki was withdrawn and sensitive. Entries in Murasaki's diary show that the two may not have been on good terms. Murasaki wrote, "Sei Shōnagon ... was dreadfully conceited. She thought herself so clever, littered her writing with Chinese characters, left a great deal to be desired." Keene thinks that Murasaki's impression of Shōnagon could have been influenced by Shōshi and the women at her court because Shōnagon served Shōshi's rival empress. Furthermore, he believes Murasaki was brought to court to write "Genji" in response to Shōnagon's popular "Pillow Book". Murasaki contrasted herself to Shōnagon in a variety of ways. She denigrated the pillow book genre and, unlike Shōnagon who flaunted her knowledge of Chinese, Murasaki pretended to not know the language.
"Our Lady of the Chronicles".
Although the popularity of the Chinese language diminished in the late Heian era, Chinese ballads continued to be popular, including those written by Bai Juyi. Murasaki taught Chinese to Shōshi who was interested in Chinese art and Juyi's ballads. Upon becoming Empress, Shōshi installed screens decorated with Chinese script, causing outrage because written Chinese was considered the language of men, far removed from the women's quarters. The study of Chinese was thought to be unladylike and went against the notion that only men should have access to the literature. Women were supposed to read and write only in Japanese, which separated them through language from government and the power structure. Murasaki, with her unconventional classical Chinese education, was one of the few women available to teach Shōshi classical Chinese. Bowring writes it was "almost subversive" that Murasaki knew Chinese and taught the language to Shōshi. Murasaki, who was reticent about her Chinese education, held the lessons between the two women in secret, writing in her diary, "Since last summer ... very secretly, in odd moments when there happened to be no one about, I have been reading with Her Majesty ... There has of course been no question of formal lessons ... I have thought it best to say nothing about the matter to anybody."
Murasaki most likely earned her second nickname, "Our Lady of the Chronicles" ("Nihongi no tsubone)", for teaching Shōshi Chinese literature. A lady-in-waiting who disliked Murasaki accused her of flaunting her knowledge of Chinese and began calling her "Our Lady of the Chronicles"—an allusion to the "Chronicles of Japan"—after an incident in which chapters from "Genji" were read aloud to the Emperor and his courtiers, one of whom remarked that the author showed a high level of education. Murasaki wrote in her diary, "How utterly ridiculous! Would I, who hesitate to reveal my learning to my women at home, ever think of doing so at court?" Although meant to be insulting, Mulhern believes Murasaki was probably flattered by the nickname.
The attitude toward the Chinese language was contradictory. In Teishi's court, Chinese had been flaunted and considered a symbol of imperial rule and superiority. Yet, in Shōshi's salon there was a great deal of hostility towards the language—perhaps owing to political expedience during a period when Chinese began to be rejected in favor of Japanese—even though Shōshi herself was a student of the language. The hostility may have affected Murasaki and her opinion of the court, and forced her to hide her knowledge of Chinese. Unlike Shōnagon, who was both ostentatious and flirtatious, as well as outspoken about her knowledge of Chinese, Murasaki seems to have been humble, an attitude which possibly impressed Michinaga. Although Murasaki used Chinese and incorporated it in her writing, she publicly rejected the language, a commendable attitude during a period of burgeoning Japanese culture.
Murasaki seems to have been unhappy with court life and was withdrawn and somber. No surviving records show that she entered poetry competitions; she appears to have exchanged few poems or letters with other women during her service. In general, unlike Sei Shōnagon, Murasaki gives the impression in her diary that she disliked court life, the other ladies-in-waiting, and the drunken revelry. She did, however, become close friends with a lady-in-waiting named Lady Saishō, and she wrote of the winters that she enjoyed, "I love to see the snow here".
According to Waley, Murasaki may not have been unhappy with court life in general but bored in Shōshi's court. He speculates she would have preferred to serve with the Lady Senshi, whose household seems to have been less strict and more light-hearted. In her diary, Murasaki wrote about Shōshi's court, " has gathered round her a number of very worthy young ladies ... Her Majesty is beginning to acquire more experience of life, and no longer judges others by the same rigid standards as before; but meanwhile her Court has gained a reputation for extreme dullness".
Murasaki disliked the men at court whom she thought to be drunken and stupid. However, some scholars, such as Waley, are certain she was involved romantically with Michinaga. At the least, Michinaga pursued her and pressured her strongly, and her flirtation with him is recorded in her diary as late as 1010. Yet, she wrote to him in a poem, "You have neither read my book, nor won my love." In her diary she records having to avoid advances from Michinaga—one night he snuck into her room, stealing a newly written chapter of "Genji." However, Michinaga's patronage was essential if she was to continue writing. Murasaki described his daughter's court activities: the lavish ceremonies, the complicated courtships, the "complexities of the marriage system", and in elaborate detail, the birth of Shōshi's two sons.
It is likely that Murasaki enjoyed writing in solitude. She believed she did not fit well with the general atmosphere of the court, writing of herself: "I am wrapped up in the study of ancient stories ... living all the time in a poetical world of my own scarcely realizing the existence of other people ... But when they get to know me, they find to their extreme surprise that I am kind and gentle". Inge says that she was too outspoken to make friends at court, and Mulhern thinks Murasaki's court life was comparatively quiet compared to other court poets. Mulhern speculates that her remarks about Izumi were not so much directed at Izumi's poetry but at her behavior, lack of morality and her court liaisons, of which Murasaki disapproved.
Rank was important in Heian court society and Murasaki would not have felt herself to have much, if anything, in common with the higher ranked and more powerful Fujiwaras. In her diary, she wrote of her life at court: "I realized that my branch of the family was a very humble one; but the thought seldom troubled me, and I was in those days far indeed from the painful consciousness of inferiority which makes life at Court a continual torment to me." A court position would have increased her social standing, but more importantly she gained a greater experience to write about. Court life, as she experienced it, is well reflected in the chapters of "Genji" written after she joined Shōshi. Her nickname, Murasaki, was most probably given at a court dinner in an incident she recorded in her diary: in c. 1008 the well-known court poet Fujiwara no Kintō inquired after the "Young Murasaki"—an allusion to the character named Murasaki in "Genji"—which would have been considered a compliment from a male court poet to a female author.
Later life and death.
When Emperor Ichijō died in 1011, Shōshi retired from the Imperial Palace to live in a Fujiwara mansion in Biwa, most likely accompanied by Murasaki, who is recorded as being there with Shōshi in 1013. George Aston explains that when Murasaki retired from court she was again associated with Ishiyama-dera: "To this beautiful spot, it is said, Murasaki no Shikibu retired from court life to devote the remainder of her days to literature and religion. There are sceptics, however, Motoöri being one, who refuse to believe this story, pointing out ... that it is irreconcilable with known facts. On the other hand, the very chamber in the temple where the "Genji" was written is shown—with the ink-slab which the author used, and a Buddhist Sutra in her handwriting, which, if they do not satisfy the critic, still are sufficient to carry conviction to the minds of ordinary visitors to the temple."
Murasaki may have died in 1014. Her father made a hasty return to Kyoto from his post at Echigo Province that year, possibly because of her death. Writing in "A Bridge of Dreams: A Poetics of "The Tale of Genji"", Shirane mentions that 1014 is generally accepted as the date of Murasaki Shikibu's death and 973 as the date of her birth, making her 41 when she died. Bowring considers 1014 to be speculative, and believes she may have lived with Shōshi until as late as 1025. Waley agrees given that Murasaki may have attended ceremonies with Shōshi held for her son, Emperor Go-Ichijō around 1025.
Murasaki's brother Nubonori died in around 1011, which, combined with the death of his daughter, may have prompted her father to resign his post and take vows at Miidera temple where he died in 1029. Murasaki's daughter entered court service in 1025 as a wet nurse to the future Emperor Go-Reizei (1025–68). She went on to become a well-known poet as Daini no Sanmi.
Works.
Three works are attributed to Murasaki: "The Tale of Genji", "The Diary of Lady Murasaki" and "Poetic Memoirs", a collection of 128 poems. Her work is considered important because her writing reflects the creation and development of Japanese writing during a period when Japanese shifted from an unwritten vernacular to a written language. Until the 9th century, Japanese language texts were written in Chinese characters using the man'yōgana writing system. A revolutionary achievement was the development of kana, a true Japanese script, in the mid-to late 9th century. Japanese authors began to write prose in their own language, which led to genres such as tales (monogatari) and poetic journals (Nikki Bungaku). Historian Edwin Reischauer writes that genres such as the monogatari were distinctly Japanese and that "Genji", written in kana, "was the outstanding work of the period".
Diary and poetry.
Murasaki began her diary after she entered service at Shōshi's court. Much of what we know about her and her experiences at court comes from the diary, which covers the period from about 1008 to 1010. The long descriptive passages, some of which may have originated as letters, cover her relationships with the other ladies-in-waiting, Michinaga's temperament, the birth of Shōshi's sons—at Michinaga's mansion rather than at the Imperial Palace—and the process of writing "Genji", including descriptions of passing newly written chapters to calligraphers for transcriptions. Typical of contemporary court diaries written to honor patrons, Murasaki devotes half to the birth of Shōshi's son Emperor Go-Ichijō, an event of enormous importance to Michinaga: he had planned for it with his daughter's marriage which made him grandfather and "de facto" regent to an emperor.
"Poetic Memoirs" is a collection of 128 poems Mulhern describes as "arranged in a biographical sequence". The original set has been lost. According to custom, the verses would have been passed from person to person and often copied. Some appear written for a lover—possibly her husband before he died—but she may have merely followed tradition and written simple love poems. They contain biographical details: she mentions a sister who died, the visit to Echizen province with her father and that she wrote poetry for Shōshi. Murasaki's poems were published in 1206 by Fujiwara no Teika, in what Mulhern believes to be the collection that is closest to the original form; at around the same time Teika included a selection of Murasaki's works in an imperial anthology, "New Collections of Ancient and Modern Times".
"The Tale of Genji".
Murasaki is best known for her "The Tale of Genji", a three-part novel spanning 1100 pages and 54 chapters, which is thought to have taken a decade to complete. The earliest chapters were possibly written for a private patron either during her marriage or shortly after her husband's death. She continued writing while at court and probably finished while still in service to Shōshi. She would have needed patronage to produce a work of such length. Michinaga provided her with costly paper and ink, and with calligraphers. The first handwritten volumes were probably assembled and bound by ladies-in-waiting.
In his "The Pleasures of Japanese Literature", Keene claims Murasaki wrote the "supreme work of Japanese fiction" by drawing on traditions of waka court diaries, and earlier monogatari—written in a mixture of Chinese script and Japanese script—such as "The Tale of the Bamboo Cutter" or "The Tales of Ise". She drew on and blended styles from Chinese histories, narrative poetry and contemporary Japanese prose. Adolphson writes that the juxtaposition of formal Chinese style with mundane subjects resulted in a sense of parody or satire, giving her a distinctive voice. "Genji" follows the traditional format of monogatari—telling a tale—particularly evident in its use of a narrator, but Keene claims Murasaki developed the genre far beyond its bounds, and by doing so created a form that is utterly modern. The story of the "shining prince" Genji is set in the late 9th to early 10th centuries, and Murasaki eliminated from it the elements of fairy tales and fantasy frequently found in earlier monogatari.
The themes in "Genji" are common to the period, and are defined by Shively as encapsulating "the tyranny of time and the inescapable sorrow of romantic love". The main theme is that of the fragility of life, "the sorrow of human existence", "mono no aware"—she used the term over a thousand times in "Genji". Keene speculates that in her tale of the "shining prince", Murasaki may have created for herself an idealistic escape from court life, which she found less than savory. In Prince Genji she formed a gifted, comely, refined, yet human and sympathetic protagonist. Keene writes that "Genji" gives a view into the Heian period; for example love affairs flourished, although women typically remained unseen behind screens, curtains or fusuma.
Helen McCullough describes Murasaki's writing as of universal appeal and believes "The Tale of Genji" "transcends both its genre and age. Its basic subject matter and setting—love at the Heian court—are those of the romance, and its cultural assumptions are those of the mid-Heian period, but Murasaki Shikibu's unique genius has made the work for many a powerful statement of human relationships, the impossibility of permanent happiness in love ... and the vital importance, in a world of sorrows, of sensitivity to the feelings of others." Prince Genji recognizes in each of his lovers the inner beauty of the woman and the fragility of life, which according to Keene, makes him heroic. The story was popular: Emperor Ichijō had it read to him, even though it was written in Japanese. By 1021 all the chapters were known to be complete and the work was sought after in the provinces where it was scarce.
Legacy.
Murasaki's reputation and influence have not diminished since her lifetime when she, with other Heian women writers, was instrumental in developing Japanese into a written language. Her writing was required reading for court poets as early as the 12th century as her work began to be studied by scholars who generated authoritative versions and criticism. Within a century of her death she was highly regarded as a classical writer. In the 17th century, Murasaki's work became emblematic of Confucian philosophy and women were encouraged to read her books. In 1673 Kumazawa Banzan argued that her writing was valuable for its sensitivity and depiction of emotions. He wrote in his "Discursive Commentary on Genji" that when "human feelings are not understood the harmony of the Five Human Relationships is lost."
"The Tale of Genji" was copied and illustrated in various forms as early as a century after Murasaki's death. "The Genji Monogatari Emaki", is a late Heian era 12th-century handscroll, consisting of four scrolls, 19 paintings, and 20 sheets of calligraphy. The illustrations, definitively dated to between 1110 and 1120, have been tentatively attributed to Fujiwara no Takachika and the calligraphy to various well-known contemporary calligraphers. The scroll is housed at the Gotoh Museum and the Tokugawa Art Museum.
Female virtue was tied to literary knowledge in the 17th century, leading to a demand for Murasaki or "Genji" inspired artifacts, known as genji-e. Dowry sets decorated with scenes from "Genji" or illustrations of Murasaki became particularly popular for noblewomen: in the 17th century genji-e symbolically imbued a bride with an increased level of cultural status; by the 18th century they had come to symbolize marital success. In 1628, Tokugawa Iemitsu's daughter had a set of lacquer boxes made for her wedding; Prince Toshitada received a pair of silk genji-e screens, painted by Kanō Tan'yū as a wedding gift in 1649.
Murasaki became a popular subject of paintings and illustrations highlighting her as a virtuous woman and poet. She is often shown at her desk in Ishimyama Temple, staring at the moon for inspiration. Tosa Mitsuoki made her the subject of hanging scrolls in the 17th century. "The Tale of Genji" became a favorite subject of Japanese ukiyo-e artists for centuries with artists such as Hiroshige, Kiyonaga, and Utamaro illustrating various editions of the novel. While early Genji art was considered symbolic of court culture, by the middle of the Edo period the mass-produced ukiyo-e prints made the illustrations accessible for the samurai classes and commoners.
In "Envisioning the "Tale of Genji"" Shirane observes that ""The Tale of Genji" has become many things to many different audiences through many different media over a thousand years ... unmatched by any other Japanese text or artifact." The work and its author were popularized through its illustrations in various media: emaki (illustrated handscrolls); byōbu-e (screen paintings), ukiyo-e (woodblock prints); films, comics, and in the modern period, manga. In her fictionalized account of Murasaki's life, "The Tale of Murasaki: A Novel", Liza Dalby has Murasaki involved in a romance during her travels with her father to Echizen Province.
"The Tale of the Genji" is recognized as an enduring classic. McCullough writes that Murasaki "is both the quintessential representative of a unique society and a writer who speaks to universal human concerns with a timeless voice. Japan has not seen another such genius." Keene writes that "The Tale of Genji" continues to captivate, because, in the story, her characters and their concerns are universal. In the 1920s, when Waley's translation was published, reviewers compared "Genji" to Austen, Proust, and Shakespeare. Mulhern says of Murasaki that she is similar to Shakespeare, who represented his Elizabethan England, in that she captured the essence of the Heian court and as a novelist "succeeded perhaps even beyond her own expectations." Like Shakespeare, her work has been the subject of reams of criticism and many books.
Kyoto held a year-long celebration commemorating the 1000th anniversary of "Genji" in 2008, with poetry competitions, visits to the Tale of Genji Museum in Uji and Ishiyama-dera (where a life size rendition of Murasaki at her desk was displayed), and women dressing in traditional 12-layered Heian court Jūnihitoe and ankle-length hair wigs. The author and her work inspired museum exhibits and Genji manga spin-offs. The design on the reverse of the first 2000 yen note commemorated her and "The Tale of Genji". A plant bearing purple berries has been named after her.
A "Genji Album", only in the 1970s dated to 1510, is housed at Harvard University. The album is considered the earliest of its kind and consists of 54 paintings by Tosa Mitsunobu and 54 sheets of calligraphy on "shikishi" paper in five colors, written by master calligraphers. The leaves are housed in a case dated to the Edo period, with a silk frontispiece painted by Tosa Mitsuoki, dated to around 1690. The album contains Mitsuoki's authentication slips for his ancestor's 16th-century paintings.

</doc>
<doc id="42759" url="https://en.wikipedia.org/wiki?curid=42759" title="Java Transaction API">
Java Transaction API

The Java Transaction API (JTA), one of the Java Enterprise Edition (Java EE) APIs, enables distributed transactions to be done across multiple X/Open XA resources in a Java environment. JTA is a specification developed under the Java Community Process as JSR 907. JTA provides for:
X/Open XA architecture.
In the X/Open XA architecture, a transaction manager or transaction processing monitor (TP monitor) coordinates the transactions across multiple resources such as databases and message queues. Each resource has its own resource manager. The resource manager typically has its own API for manipulating the resource, for example the JDBC API to work with relational databases. In addition, the resource manager allows a TP monitor to coordinate a distributed transaction between its own and other resource managers. Finally, there is the application which communicates with the TP monitor to begin, commit or rollback the transactions. The application also communicates with the individual resources using their own API to modify the resource.
JTA implementation of the X/Open XA architecture.
The JTA API consists of classes in two Java packages:
The JTA is modelled on the X/Open XA architecture, but it defines two different APIs for demarcating transaction boundaries. It distinguishes between an application server such as an EJB server and an application component. It provides an interface, , that is used by the application server itself to begin, commit and rollback the transactions. It provides a different interface, the , that is used by general client code such as a servlet or an EJB to manage the transactions.
The JTA architecture requires that each resource manager must implement the interface in order to be managed by the TP monitor. As stated previously, each resource will have its own specific API, for instance:
Java Transaction API.
The Java Transaction API consists of three elements: a high-level application transaction demarcation interface, a high-level transaction manager interface intended for an application server, and a standard Java mapping of the X/Open XA protocol intended for a transactional resource manager.
UserTransaction interface.
The interface provides the application the
ability to control transaction boundaries programmatically. This interface may be used
by Java client programs or EJB beans.
The method starts a global transaction and associates the
transaction with the calling thread. The transaction-to-thread association is managed
transparently by the Transaction Manager.
Support for nested transactions is not required. The UserTransaction.begin method
throws the NotSupportedException when the calling thread is already associated
with a transaction and the transaction manager implementation does not support nested
transactions.
Transaction context propagation between application programs is provided by the
underlying transaction manager implementations on the client and server machines.
The transaction context format used for propagation is protocol dependent and must be
negotiated between the client and server hosts. For example, if the transaction manager
is an implementation of the JTS specification, it will use the transaction context
propagation format as specified in the CORBA OTS 1.1 specification. Transaction
propagation is transparent to application programs.
@Transactional annotation.
The annotation provides the application the
ability to control transaction boundaries declaratively. This annotation can be applied to any class that the Java EE specification
defines as a managed bean (which includes CDI managed beans).
The code sample below illustrates the usage of @Transactional in a request scoped CDI managed bean:
Transactional behavior can be configured via an attribute on the annotation. The available options closely mirror those of the EJB specification.
@TransactionScoped annotation.
The annotation provides the application the
ability to declare that the scope during which a bean lives is tied to the time a given transaction is active.
The code sample below illustrates the usage of @TransactionScoped in a request scoped CDI managed bean:
If method "foo()" is first called on a managed instance of ExampleBean and then subsequently method "bar()" is called, the number printed will be 0 and not 1. This is because each method had its own transaction and therefore its own instance of TxScopedBean. The number 1 that was set during the call to "foo()" will therefore not be seen during the call to "bar()".
UserTransaction support in EJB server.
EJB servers are required to support the UserTransaction interface for use by EJB
beans with the BEAN value in the annotation (this is called bean-managed transactions or BMT). The UserTransaction
interface is exposed to EJB components through either the EJBContext interface using the
getUserTransaction method, or directly via injection using the general codice_1 annotation. Thus, an EJB application does not interface with the
Transaction Manager directly for transaction demarcation; instead, the EJB bean relies
on the EJB server to provide support for all of its transaction work as defined in the
Enterprise JavaBeans Specification. (The underlying interaction between the EJB
Server and the TM is transparent to the application; the burden of implementing transaction management is on the EJB container and server provider.)
The code sample below illustrates the usage of UserTransaction via bean-managed transactions in an EJB session bean:
Alternatively, the UserTransaction can be obtained from the SessionContext:
Note though that in the example above if the codice_2 annotation is omitted, a JTA transaction is automatically started whenever codice_3 is called and is automatically committed or rolled back when codice_3 is exited. Making use of a UserTransaction is thus not necessary in EJB programming, but might be needed for very specialized code.
UserTransaction support in JNDI.
The UserTransaction should be available under codice_5 (if a JTA implementation is installed in the environment).
Open source JTA implementations.
There exist a number of active (as of September 2010) open source JTA implementations.
Narayana.
Narayana, formerly known as JBossTS and Arjuna Transaction Service, comes with a very robust implementation, which supports both the JTA and JTS APIs. Narayana comes with a recovery service, which could be run as a separate process from your application processes. Narayana is the default transaction manager for Java EE WildFly and JBoss EAP application servers. It supports for three Extended Transaction models: Nested Top Level Transactions, Nested Transactions and a compensation-based model based on "Sagas". Webservice and RESTful Transactions are supported as well. It does not support out-of-the box integration with the Spring framework, but it is easy to integrate.
Atomikos TransactionsEssentials.
Atomikos TransactionsEssentials's documentation and literature on the internet show that it is a production quality implementation, which also supports recovery and some exotic features beyond the JTA API. Atomikos provides out-of-the-box Spring integration along with some nice examples. It also provides support for pooled connections for both database and JMS resources.
Bitronix JTA.
Bitronix claims to support transaction recovery as well as or even better than some of the commercial products. Bitronix also provides connection pooling and session pooling out of the box.

</doc>
<doc id="42760" url="https://en.wikipedia.org/wiki?curid=42760" title="JTA">
JTA

JTA may refer to:

</doc>
<doc id="42761" url="https://en.wikipedia.org/wiki?curid=42761" title="History of Belarus">
History of Belarus

This article describes the history of Belarus. The Belarusian ethnos is traced at least as far in time as other East Slavs.
After an initial period of independent feudal consolidation, Belarusian lands were incorporated into the Kingdom of Lithuania, Grand Duchy of Lithuania, and later in the Polish–Lithuanian Commonwealth, and the Russian Empire and eventually the Soviet Union. Belarus became an independent country in 1991 after declaring itself free from the Soviet Union.
Early history.
The history of Belarus, or more precisely of the Belarusian ethnicity, begins with the migration and expansion of the Slavic peoples throughout Eastern Europe between the 6th and 8th centuries. East Slavs settled on the territory of present-day Belarus, Russia and Ukraine, assimilating local Baltic — (Yotvingians, Dniepr Balts), Ugro-Finnic (in Russia) and steppe nomads (in Ukraine) already living there, their early ethnic integrations contributed to the gradual differentiation of the three East Slavic nations. These East Slavs, a pagan, animistic, agrarian people, had an economy which included trade in agricultural produce, game, furs, honey, beeswax and amber.
The modern Belarusian ethnos was probably formed on the basis of the three Slavic tribes — Kryvians, Drehovians, Radzimians as well as several Baltic tribes.
During the 9th and 10th centuries, Scandinavian Vikings established trade posts on the way from Scandinavia to the Byzantine Empire. The network of lakes and rivers crossing East Slav territory provided a lucrative trade route between the two civilizations. In the course of trade, they gradually took sovereignty over the tribes of East Slavs, at least to the point required by improvements in trade.
The Rus' rulers invaded the Byzantine Empire on few occasions, but eventually they allied against the Bulgars. The condition underlying this alliance was to open the country for Christianization and acculturation from the Byzantine Empire.
The common cultural bond of Eastern Orthodox Christianity and written Church Slavonic (a literary and liturgical Slavic language developed by 8th century missionaries Saints Cyril and Methodius) fostered the emergence of a new geopolitical entity, Kievan Rus' — a loose-knit network of principalities, established along preexisting trade routes, with major centers in Novgorod (currently Russia), Polatsk (in Belarus) and Kiev (currently in Ukraine) — which claimed a sometimes precarious preeminence among them.
First Belarusian states.
Between the 9th and 12th centuries, the Principality of Polotsk (northern Belarus) emerged as the dominant center of power on Belarusian territory, with a lesser role played by the Principality of Turaŭ in the south.
It repeatedly asserted its sovereignty in relation to other centers of Rus', becoming a political capital, the episcopal see of a bishopric and the controller of vassal territories among Balts in the west. The city's Cathedral of the Holy Wisdom (1044–66), though completely rebuilt over the years, remains a symbol of this independent-mindedness, rivaling churches of the same name in Novgorod and Kiev, referring to the original Hagia Sophia in Constantinople (and hence to claims of imperial prestige, authority and sovereignty). Cultural achievements of the Polatsk period include the work of the nun Euphrosyne of Polatsk (1120–73), who built monasteries, transcribed books, promoted literacy and sponsored art (including local artisan Lazarus Bohsha's famous "Cross of Euphrosyne", a national symbol and treasure stolen during World War II), and the prolific, original Church Slavonic sermons and writings of Bishop Cyril of Turau (1130–82).
Grand Duchy of Lithuania.
In the 13th century, the fragile unity of Kievan Rus' disintegrated due to nomadic incursions from Asia, which climaxed with the Mongol sacking of Kiev (1240), leaving a geopolitical vacuum in the region. The East Slavs splintered into a number of independent and competing principalities. Due to military conquest and dynastic marriages the West Ruthenian (Belarusian) principalities were acquired by the expanding Lithuania, beginning with the rule of Lithuanian King Mindaugas (1240–63). From the 13th to 15th century, Baltic and Ukrainian lands were consolidated into the Grand Duchy of Lithuania, with its initial capital unknown, but which presumably could have been either Navahrudak, Voruta, Trakai, Kernavė or Vilnius. Since the 14th century, Vilnius had been the only official capital of the state.
The Lithuanians' smaller numbers in this medieval state gave the Ruthenians (present-day Belarusians and Ukrainians) an important role in the everyday cultural life of the state. Owing to the prevalence of East Slavs and the Eastern Orthodox faith among the population in eastern and southern regions of the state, the Ruthenian language was a widely used colloquial language.
An East Slavic variety ("rus'ka mova", "Old Belarusian" or "West Russian Chancellery language"), gradually influenced by Polish, was the language of administration in the Grand Duchy of Lithuania at least since Vytautas reign until the late 17th century when it was eventually replaced by Polish language.
This period of political breakdown and reorganization also saw the rise of written local vernaculars in place of the literary and liturgical Church Slavonic language, a further stage in the evolving differentiation between the Belarusian, Russian and Ukrainian languages.
Several Lithuanian monarchs — the last being Švitrigaila in 1432–36 — relied on the Eastern Orthodox Ruthenian majority, while most monarchs and magnates increasingly came to reflect the opinions of the Roman Catholics.
Construction of Orthodox churches in some parts of present-day Belarus had been initially prohibited, as was the case of Vitebsk in 1480. On the other hand, further unification of the, mostly Orthodox, Grand Duchy with mostly Catholic Poland led to liberalization and partial solving of the religious problem. In 1511, King and Grand Duke Sigismund I the Old granted the Orthodox clergy an autonomy enjoyed previously only by Catholic clergy. The privilege was enhanced in 1531, when the Orthodox church was no longer responsible to the Catholic bishop and instead the Metropolite was responsible only to the sobor of eight Orthodox bishops, the Grand Duke and the Patriarch of Constantinople. The privilege also extended the jurisdiction of the Orthodox hierarchy over all Orthodox people.
In such circumstances, a vibrant Ruthenian culture flourished, mostly in major present-day Belarusian cities.
Despite the legal usage of the Old Ruthenian language (the predecessor of both modern Belarusian and Ukrainian languages) which was used as a chancellery language in the territory of the Grand Duchy of Lithuania, the literature was mostly non-existent, outside of several chronicles. The first Belarusian book printed with the first printing press in the Cyrillic alphabet was published in Prague in 1517, by Francysk Skaryna, a leading representative of the renaissance Belarusian culture. Soon afterwards he founded a similar printing press in Polatsk and started an extensive undertaking of publishing the Bible and other religious works there. Apart from the Bible itself, before his death in 1551 he published 22 other books, thus laying the foundations for the evolution of the Ruthenian language into the modern Belarusian language.
Polish–Lithuanian Commonwealth.
The Lublin Union of 1569 constituted the Polish–Lithuanian Commonwealth as an influential player in European politics and the largest multinational state in Europe. While Ukraine and Podlaskie became subject to the Polish Crown, present-day Belarus territory was still regarded as part of the Grand Duchy of Lithuania. The new polity was dominated by much more densely populated Poland, which had 134 representatives in the Sejm as compared to 46 representatives of the Grand Duchy of Lithuania. However the Grand Duchy of Lithuania retained much autonomy, and was governed by a separate code of laws called the Lithuanian Statutes, which codified both civil and property rights. Mogilyov was the largest urban centre of the territory of present-day Belarus, followed by Vitebsk, Polotsk, Pinsk, Slutsk, and Brest, whose population exceeded 10,000. In addition, Vilna (Vilnius), the capital of the Grand Duchy of Lithuania, also had a significant Ruthenian population.
With time, the ethnic pattern did not evolve much. Throughout their existence as a separate culture, Ruthenians formed in most cases rural population, with the power held by local szlachta and boyars, often of Lithuanian, Polish or Russian descent. As in the rest of Central and Eastern Europe, the trade and commerce was mostly monopolized by Jews, who formed a significant part of the urban population. Since the Union of Horodlo of 1413, local nobility was assimilated into the traditional clan system by means of the formal procedure of adoption by the "szlachta" (Polish gentry). Eventually it formed a significant part of the szlachta. Initially mostly Ruthenian and Orthodox, with time most of them became polonized. This was especially true for major magnate families (Sapieha and Radziwiłł clans being the most notable), whose personal fortunes and properties often surpassed those of the royal families and were huge enough to be called a state within a state. Many of them founded their own cities and settled them with settlers from other parts of Europe. Indeed, there were Scots, Germans and Dutch people inhabiting major towns of the area, as well as several Italian artists who had been "imported" to the lands of modern Belarus by the magnates. Contrary to Poland, in the lands of the Grand Duchy, the peasants had little personal freedom in the Middle Ages. However, with time, the magnates and the gentry gradually limited the few liberties of the serfs, at the same time increasing their taxation, often in labour for the local gentry. This made many Ruthenians flee to the scarcely populated lands, "Dzikie Pola" (Wild Fields), the Polish name of the Zaporizhian Sich, where they formed a large part of the Cossacks. Others sought refuge in the lands of other magnates or in Russia.
Also, with time the religious conflicts started to arise. The gentry with time started to adopt Catholicism while the common people by large remained faithful to Eastern Orthodoxy. Initially the Warsaw Compact of 1573 codified the preexisting freedom of worship. However, the rule of an ultra-Catholic King Sigismund III Vasa was marked by numerous attempts to spread the Catholicism, mostly through his support for counterreformation and the Jesuits. Possibly to avoid such conflicts, in 1595 the Orthodox hierarchs of Kiev signed the Union of Brest, breaking their links with the Patriarch of Constantinople and placing themselves under the Pope. Although the union was generally supported by most local Orthodox bishops and the king himself, it was opposed by some prominent nobles and, more importantly, by the nascent Cossack movement. This led to a series of conflicts and rebellions against the local authorities. The first of such happened in 1595, when the Cossack insurgents under Severyn Nalivaiko took the towns of Slutsk and Mogilyov and executed Polish magistrates there. Other such clashes took place in Mogilyov (1606–10), Vitebsk (1623), and Polotsk (1623, 1633). This left the population of the Grand Duchy divided between Greek Catholic and Greek Orthodox parts. At the same time, after the schism in the Orthodox Church (Raskol), some Old Believers migrated west, seeking refuge in the Rzeczpospolita, which allowed them to freely practice their faith.
From 1569, the Polish–Lithuanian Commonwealth suffered a series of Tatar invasions, the goal of which was to loot, pillage and capture slaves into jasyr. The borderland area to the south-east was in a state of semi-permanent warfare until the 18th century. Some researchers estimate that altogether more than 3 million people, predominantly Ukrainians but also Russians, Belarusians and Poles, were captured and enslaved during the time of the Crimean Khanate.
Despite the abovementioned conflicts, the literary tradition of Belarus evolved. Until the 17th century, the Ruthenian language, the predecessor of modern Belarusian, was used in Grand Duchy as a "chancery language", that is the language used for official documents. Afterwards, it was replaced with the Polish language, commonly spoken by the upper classes of Belarusian society. Both Polish and Ruthenian cultures gained a major cultural centre with the foundation of the Academy of Vilna. At the same time the Belarusian lands entered a path of economic growth, with the formation of numerous towns that served as centres of trade on the east-west routes.
However, both economic and cultural growth came to an end in mid-17th century with a series of violent wars against Tsardom of Russia, Sweden, Brandenburg and Transylvania, as well as internal conflicts, known collectively as The Deluge. The misfortunes were started in 1648 by Bohdan Chmielnicki, who started a large-scale Cossack uprising in Ukraine. Although the Cossacks were defeated in 1651 in the battle of Beresteczko, Khmelnytsky sought help from Russian tsar, and by the Treaty of Pereyaslav Russia dominated and partially occupied the eastern lands of the Commonwealth since 1655. The Swedes invaded and occupied the rest in the same year. The wars had shown internal problems of the state, with some people of the Grand Duchy supporting Russia while others (most notably Janusz Radziwiłł) supporting the Swedes. Although the Swedes were finally driven back in 1657 and the Russians were defeated in 1662, most of the country was ruined. It is estimated that the Commonwealth lost a third of its population, with some regions of Belarus losing as much as 50%. This broke the power of the once-powerful Commonwealth and the country gradually became vulnerable to foreign influence.
Subsequent wars in the area (Great Northern War and the War of Polish succession) damaged its economy even further. In addition, Russian armies raided the Commonwealth under the pretext of the returning of fugitive peasants. By mid-18th century their presence in the lands of modern Belarus became almost permanent.
The last attempt to save the Commonwealth's independence was a Polish–Belarusian–Lithuanian national uprising of 1794 led by Tadeusz Kościuszko, however it was eventually quenched.
Eventually by 1795 Poland was partitioned by its neighbors. Thus a new period in Belarusian history started, with all its lands annexed by the Russian Empire, in a continuing endeavor of Russian tsars of "gathering the Rus lands" started after the liberation from the Tatar yoke by Grand Duke Ivan III of Russia.
Russian Empire.
Under Russian administration, the territory of Belarus was divided into the "guberniyas" of Minsk, Vitebsk, Mogilyov, and Hrodno. Belarusians were active in the guerrilla movement against Napoleon's occupation.. With Napoleon's defeat, Belarus again became a part of Imperial Russia and its "guberniyas" constituted part of the Northwestern Krai. The anti-Russian uprisings of the gentry in 1830 and 1863 were subdued by government forces.
Although under Nicholas I and Alexander III the national cultures were repressed due to the policies of de-Polonization and Russification, which included the return to Orthodoxy, the 19th century was signified by the rise of the modern Belarusian nation and self-confidence. A number of authors started publishing in the Belarusian language, including Jan Czeczot, Władysław Syrokomla and Konstanty Kalinowski.
In a Russification drive in the 1840s, Nicholas I forbade the use of the term "Belarusia" and renamed the region the "North-Western Territory". He also prohibited the use of Belarusian language in public schools, campaigned against Belarusian publications and tried to pressure those who had converted to Catholicism under the Poles to reconvert to the Orthodox faith. In 1863, economic and cultural pressure exploded into a revolt, led by Kalinowski. After the failed revolt, the Russian government reintroduced the use of Cyrillic to Belarusian in 1864 and banned the use of the Latin alphabet.
In the second half of the 19th century, the Belarusian economy, like that of the entire Europe, was experiencing significant growth due to the spread of the Industrial Revolution to Eastern Europe, particularly after the emancipation of the serfs in 1861. Peasants sought a better lot in foreign industrial centres, with some 1.5 million people leaving Belarus in the half-century preceding the Russian Revolution of 1917.
20th century.
BNR and LBSSR.
Minsk was captured by German troops on 21 February 1918. World War I was the short period when Belarusian culture started to flourish. German administration allowed schools with Belarusian language, previously banned in Russia; a number of Belarusian schools were created until 1919 when they were banned again by the Polish military administration. At the end of World War I, when Belarus was still occupied by Germans, according to the Treaty of Brest-Litovsk, the short-lived Belarus National Republic was pronounced on 25 March 1918, as part of the German Mitteleuropa plan.
In December 1918, Mitteleuropa was obsolete as the Germans withdrew from the Ober-Ost territory, and for the next few years in the newly created political vacuum the territories of Belarus would witness the struggle of various national and foreign factions. On 3 December 1918 the Germans withdrew from Minsk. On 10 December 1918 Soviet troops occupied Minsk. The Rada (Council) of the Belarus National Republic went into exile, first to Kaunas, then to Berlin and finally to Prague. On 2 January 1919, the Soviet Socialist Republic of Byelorussia was declared. On 17 February 1919 it was disbanded. Part of it was included into RSFSR, and part was joined to the Lithuanian SSR to form the LBSSR, Lithuanian–Byelorussian Soviet Socialist Republic, informally known as "Litbel", whose capital was Vilnius. While Belarus National Republic faced off with Litbel, foreign powers were preparing to reclaim what they saw as their territories: Polish forces were moving from the West, and Russians from the East. When Vilnius was captured by Polish forces on 17 April 1919, the capital of the Soviet puppet state Litbel was moved to Minsk. On 17 July 1919 Lenin dissolved Litbel because of the pressure of Polish forces advancing from the West. Polish troops captured Minsk on 8 August 1919.
Belarusian Soviet Republic and West Belarus.
Some time in 1918 or 1919, Sergiusz Piasecki returned to Belarus, joining Belarusian anti-Soviet units, the "Green Oak" (in Polish, "Zielony Dąb"), led by Ataman Wiaczesław Adamowicz (pseudonym: J. Dziergacz). When on 8 August 1919, the Polish Army captured Minsk, Adamowicz decided to work with them. Thus Belarusian units were created, and Piasecki was transferred to a Warsaw school of infantry cadets. In the summer of 1920, during the Polish–Soviet War, Piasecki fought in the Battle of Radzymin.
The frontiers between Poland, which had established an independent government after World War I, and the former Russian Empire were not recognized by the League of Nations. Poland's Józef Piłsudski, who envisioned the formation of an Intermarum Federation as a Central and East European bloc that would be a bulwark against Germany to the west and Russia to the east, carried out a Kiev Offensive into Ukraine in 1920. This met with a Red Army counter-offensive that drove into Polish territory almost to Warsaw, Minsk itself was re-captured by the Soviet Red Army on 11 July 1920 and a new Byelorussian Soviet Socialist Republic was declared on 31 July 1920. Piłsudski, however, halted the Soviet advance at the Battle of Warsaw and resumed his eastward offensive. Finally the Treaty of Riga, ending the Polish–Soviet War, divided Belarus between Poland and Soviet Russia. Over the next two years, the Belarus National Republic prepared a national uprising, ceasing the preparations only when the League of Nations recognized the Soviet Union's western borders on 15 March 1923. The Soviets terrorised Western Belarus, the most radical case being Soviet raid on Stołpce. Poland created Border Protection Corps in 1924.
The Polish part of Belarus was subject to Polonization policies (especially in the 1930s), while the Soviet Belarus was one of the original republics which formed the USSR. For several years, the national culture and language enjoyed a significant boost of revival in the Soviet Belarus. A Polish Autonomous District was also formed. This was however soon ended during the Great Purge, when almost all prominent Belarusian national intelligentsia were executed, many of them buried in Kurapaty. Thousands were deported to Asia. As the result of Polish operation of the NKVD tens of thousands people of many nationalities were killed. Belarusian orthography was Russified in 1933 and use of Belarusian language was discouraged as exhibiting anti-soviet attitude.
In West Belarus, up to 30 000 families of Polish veterans ("osadniks") were settled in the lands formerly belonging to the Russian tsar family and Russian aristocracy. Belarusian representation in Polish parliament was reduced as a result of the 1930 elections. Since the early 1930s, the Polish government introduced a set of policies designed to Polonize all minorities (Belarusians, Ukrainians, Jews, etc.). The usage of Belarusian language was discouraged and the Belarusian schools were facing severe financial problems. In spring of 1939, there already was neither single Belarusian official organisation in Poland nor a single exclusively Belarusian school (with only 44 schools teaching Belarusian language left).
Belarus in World War II.
When the Soviet Union invaded Poland on 17 September 1939, following the terms of the Molotov–Ribbentrop Pact's secret protocol, much of what had been eastern Poland was annexed to the BSSR. Similarly to the times of German occupation during World War I, Belarusian language and Soviet culture enjoyed relative prosperity in this short period. Already in October 1940, over 75% of schools used the Belarusian language, also in the regions where no Belarus people lived, e.g. around Łomża, what was Ruthenization. Western Belarus was sovietised, tens of thousands were imprisoned, deported, murdered. The victims were mostly Polish and Jewish.
After twenty months of Soviet rule, Germany and its Axis allies invaded the Soviet Union on 22 June 1941. Soviet authorities immediately evacuated about 20% of the population of Belarus, killed thousands of prisoners and destroyed all the food supplies. The country suffered particularly heavily during the fighting and the German occupation. Minsk was captured by the Germans on 28 June 1941. Following bloody encirclement battles, all of the present-day Belarus territory was occupied by the Germans by the end of August 1941.
During World War II, the Nazis attempted to establish a puppet Belarusian government, Belarusian Central Rada, with the symbolics similar to BNR. In reality, however, the Germans imposed a brutal racist regime, burning down some 9 000 Belarusian villages, deporting some 380,000 people for slave labour, and killing hundreds of thousands of civilians more. Local police took part in many of those crimes. Almost the whole, previously very numerous, Jewish populations of Belarus that did not evacuate were killed. One of the first uprisings of a Jewish ghetto against the Nazis occurred in 1942 in Belarus, in the small town of Lakhva.
Since the early days of the occupation, a powerful and increasingly well-coordinated Belarusian resistance movement emerged. Hiding in the woods and swamps, the partisans inflicted heavy damage to German supply lines and communications, disrupting railway tracks, bridges, telegraph wires, attacking supply depots, fuel dumps and transports and ambushing German soldiers. Not all anti-German partisans were pro-Soviet. In the largest partisan sabotage action of the entire Second World War, the so-called Asipovichy diversion of 30 July 1943 four German trains with supplies and Tiger tanks were destroyed. To fight partisan activity, the Germans had to withdraw considerable forces behind their front line. On 22 June 1944 the huge Soviet offensive Operation Bagration was launched, Minsk was re-captured on 3 July 1944, and all of Belarus was regained by the end of August. Hundred thousand of Poles were expelled after 1944. As part of the Nazis' effort to combat the enormous Belarusian resistance during World War II, special units of local collaborationists were trained by the SS's Otto Skorzeny to infiltrate the Soviet rear. In 1944 thirty Belarusians (known as Čorny Kot ("Black Cat") and personally led by Michał Vituška) were airdropped by the Luftwaffe behind the lines of the Red Army, which had already liberated Belarus during Operation Bagration. They experienced some initial success due to disorganization in the rear of the Red Army, and some other German-trained Belarusian nationalist units also slipped through the Białowieża Forest in 1945. The NKVD, however, had already infiltrated these units. Vituška managed to escape to the West following the war, along with several other Belarusian Central Rada leaders.
In total, Belarus lost a quarter of its pre-war population in World War II including practically all its intellectual elite. About 9 200 villages and 1.2 million houses were destroyed. The major towns of Minsk and Vitsebsk lost over 80% of their buildings and city infrastructure. For the defence against the Germans, and the tenacity during the German occupation, the capital Minsk was awarded the title "Hero City" after the war. The fortress of Brest was awarded the title "Hero-Fortress".
BSSR from 1945 to 1990.
After the end of War in 1945, Belarus became one of the founding members of the United Nations Organisation. Joining Belarus was the Soviet Union itself and another republic Ukraine. In exchange for Belarus and Ukraine joining the UN, the United States had the right to seek two more votes, a right that has never been exercised.
More than 200 000 Poles run away or were expelled to Poland, some killed by the NKVD or deported to Siberia. Armia Krajowa and post-AK resistance was the strongest in the Hrodna, Vaŭkavysk, Lida and Ščučyn regions.
The Belarusian economy was completely devastated by the events of the war. Most of the industry, including whole production plants were removed either to Russia or Germany. Industrial production of Belarus in 1945 amounted for less than 20% of its pre-war size. Most of the factories evacuated to Russia, with several spectacular exceptions, were not returned to Belarus after 1945. During the immediate postwar period, the Soviet Union first rebuilt and then expanded the BSSR's economy, with control always exerted exclusively from Moscow. During this time, Belarus became a major center of manufacturing in the western region of the USSR. Huge industrial objects like the BelAZ, MAZ, and the Minsk Tractor Plant were built in the country. The increase in jobs resulted in a huge immigrant population of Russians in Belarus. Russian became the official language of administration and the peasant class, which traditionally was the base for Belarusian nation, ceased to exist.
On 26 April 1986, the Chernobyl disaster occurred at the Chernobyl nuclear power plant in Ukraine situated close to the border with Belarus. It is regarded as the worst nuclear accident in the history of nuclear power. It produced a plume of radioactive debris that drifted over parts of the western Soviet Union, Eastern Europe, and Scandinavia. Large areas of Belarus, Ukraine and Russia were contaminated, resulting in the evacuation and resettlement of roughly 200,000 people. About 60% of the radioactive fallout landed in Belarus. The effects of the Chernobyl accident in Belarus were dramatic: about 50,000 km² (or about a quarter of the territory of Belarus) formerly populated by 2.2 million people (or a fifth of the Belarusian population) now require permanent radioactive monitoring (after receiving doses over 37 kBq/m² of caesium-137). 135,000 persons were permanently resettled and many more were resettled temporarily. After 10 years since the accident, the occurrences of thyroid cancer among children increased fifteenfold (the sharp rise started in about four years after the accident).
Republic of Belarus.
On 27 July 1990, Belarus declared its national sovereignty, a key step toward independence from the Soviet Union. The BSSR was formally renamed the Republic of Belarus on 25 August 1991. Around that time, Stanislav Shushkevich became the chairman of the Supreme Soviet of Belarus, the top leadership position in Belarus. On 8 December 1991, Shushkevich met with Boris Yeltsin of Russia and Leonid Kravchuk of Ukraine, in Belavezhskaya Pushcha, to formally declare the dissolution of the Soviet Union and the formation of the Commonwealth of Independent States.
In 1994, the first presidential elections were held and Alexander Lukashenko was elected president of Belarus. The 1996 referendum resulted in the amendment of the constitution that took key powers off the parliament. In 2001, he was re-elected as president in elections described as undemocratic by Western observers. At the same time the west began criticising him of authoritarianism. In 2006, Lukashenko was once again re-elected in presidential elections which were again criticised as flawed by most European Union countries. In 2010, Lukashenko was re-elected once again in presidential elections, which were described as flawed by most EU countries and institutions. A peaceful protest against the electoral fraud was attacked by riot police and by armed men dressed in black. After that, up to 700 opposition activists, including 7 presidential candidates, were arrested by KGB.

</doc>
<doc id="42762" url="https://en.wikipedia.org/wiki?curid=42762" title="Java Naming and Directory Interface">
Java Naming and Directory Interface

The Java Naming and Directory Interface (JNDI) is a Java API for a directory service that allows Java software clients to discover and look up data and objects via a name. Like all Java APIs that interface with host systems, JNDI is independent of the underlying implementation. Additionally, it specifies a service provider interface (SPI) that allows directory service implementations to be plugged into the framework. It may make use of a server, a flat file, or a database; the choice is up to the vendor.
Typical uses of JNDI include:
Background.
The Java RMI and Java EE APIs use the JNDI API to look up objects in a network.
The API provides:
The SPI portion allows support for practically any kind of naming or directory service, including:
Sun Microsystems first released the JNDI specification on March 10, 1997. , the current version is JNDI 1.2.
Basic lookup.
JNDI (Java Naming and Directory Interface) organizes its names into a hierarchy. A name can be any string such as "com.mydomain.ejb.MyBean". A name can also be an object that implements the codice_1 interface; however a string is the most common way to name an object. A name is bound to an object in the directory by storing either the object or a reference to the object in the directory service identified by the name.
The JNDI API defines a context that specifies where to look for an object. The initial context is typically used as a starting point.
In the simplest case, an initial context must be created using the specific implementation and extra parameters required by the implementation. The initial context will be used to look up a name. The initial context is analogous to the root or top of a directory tree for a file system. Below is an example of creating an initial context:
A context is then used to look up previously bound names in that context. For example:
Alternative to above code is as below:
The Context object can also be configured by adding jndi.properties file in classpath containing initial context factory class name and provider URL. The above code will be reduced as shown below:
A context is then used to look up previously bound names in that context. For example:
Searching.
Attributes may be attached to special entries called directories. Directories enable searching for objects by their associated attributes. Directories are a type of context; they restrict the name space much like a directory structure on a file system does.

</doc>
<doc id="42764" url="https://en.wikipedia.org/wiki?curid=42764" title="Hagia Sophia">
Hagia Sophia

Hagia Sophia (from the , "Holy Wisdom"; or "Sancta Sapientia"; ) is a former Greek Orthodox Christian patriarchal basilica (church), later an imperial mosque, and now a museum (Ayasofya Müzesi) in Istanbul, Turkey. From the date of its construction in 537 until 1453, it served as an Orthodox cathedral and seat of the Patriarch of Constantinople, except between 1204 and 1261, when it was converted to a Roman Catholic cathedral under the Latin Empire. The building was a mosque from 29 May 1453 until 1931. It was then secularized and opened as a museum on 1 February 1935.
Famous in particular for its massive dome, it is considered the epitome of Byzantine architecture and is said to have "changed the history of architecture". It remained the world's largest cathedral for nearly a thousand years, until Seville Cathedral was completed in 1520.
The current building was originally constructed as a church between 532 and 537 on the orders of the Byzantine Emperor Justinian I and was the third Church of the Holy Wisdom to occupy the site, the previous two having both been destroyed by rioters. It was designed by the Greek geometers Isidore of Miletus and Anthemius of Tralles.
The church was dedicated to the "Wisdom of God", the Logos, the second person of the Holy Trinity, its patronal feast taking place on 25 December, the commemoration of the birth of the incarnation of the Logos in Christ. Although sometimes referred to as Sancta Sophia (as though it were named after Saint Sophia), "sophia" being the phonetic spelling in Latin of the Greek word for wisdom, its full name in Greek is , "Shrine of the Holy Wisdom of God".
The church contained a large collection of holy relics and featured, among other things, a silver iconostasis. The focal point of the Eastern Orthodox Church for nearly one thousand years, the building witnessed the excommunication of Patriarch Michael I Cerularius on the part of Humbert of Silva Candida, the papal envoy of Pope Leo IX in 1054, an act which is commonly considered the start of the Great Schism.
In 1453, Constantinople was conquered by the Ottoman Turks under Sultan Mehmed II, who ordered this main church of Orthodox Christianity converted into a mosque. By that point, the church had fallen into a state of disrepair. Nevertheless, the Christian cathedral made a strong impression on the new Ottoman rulers and they decided to convert it into a mosque. The bells, altar, iconostasis, and sacrificial vessels and other relics were removed and the mosaics depicting Jesus, his Mother Mary, Christian saints and angels were also removed or plastered over. Islamic features—such as the mihrab, minbar, and four minarets—were added. It remained a mosque until 1931, when it was closed to the public for four years. It was re-opened in 1935 as a museum by the Republic of Turkey. Hagia Sophia is currently (2014) the second-most visited museum in Turkey, attracting almost 3.3 million visitors annually.
From its initial conversion until the construction of the nearby Sultan Ahmed Mosque (Blue Mosque of Istanbul) in 1616, it was the principal mosque of Istanbul. The Byzantine architecture of the Hagia Sophia served as inspiration for many other Ottoman mosques, such as the Blue Mosque, the Şehzade Mosque, the Süleymaniye Mosque, the Rüstem Pasha Mosque and the Kılıç Ali Paşa Mosque.
History.
First church.
The first church on the site was known as the ("Megálē Ekklēsíā", "Great Church"), or in Latin "Magna Ecclesia", because of its larger dimensions in comparison to the contemporary churches in the City. Inaugurated on 15 February 360 (during the reign of Constantius II) by the Arian bishop Eudoxius of Antioch, it was built next to the area where the imperial palace was being developed. The nearby Hagia Eirene ("Holy Peace") church was completed earlier and served as cathedral until the Great Church was completed. Both churches acted together as the principal churches of the Byzantine Empire.
Writing in 440, Socrates of Constantinople claimed that the church was built by Constantius II, who was working on it in 346. A tradition which is not older than the 7th – 8th century, reports that the edifice was built by Constantine the Great. Zonaras reconciles the two opinions, writing that Constantius had repaired the edifice consecrated by Eusebius of Nicomedia, after it had collapsed. Since Eusebius was bishop of Constantinople from 339 to 341, and Constantine died in 337, it seems possible that the first church was erected by the latter. The edifice was built as a traditional Latin colonnaded basilica with galleries and a wooden roof. It was preceded by an atrium. It was claimed to be one of the world's most outstanding monuments at the time.
The Patriarch of Constantinople John Chrysostom came into a conflict with Empress Aelia Eudoxia, wife of the emperor Arcadius, and was sent into exile on 20 June 404. During the subsequent riots, this first church was largely burned down. Nothing remains of the first church today.
Second church.
A second church was ordered by Theodosius II, who inaugurated it on 10 October 415. The basilica with a wooden roof was built by architect Rufinus. A fire started during the tumult of the Nika Revolt and burned the second Hagia Sophia to the ground on 13–14 January 532.
Several marble blocks from the second church survive to the present; among them are reliefs depicting 12 lambs representing the 12 apostles. Originally part of a monumental front entrance, they now reside in an excavation pit adjacent to the museum's entrance after they were discovered in 1935 beneath the western courtyard by A. M. Schneider. Further digging was forsaken for fear of impinging on the integrity of the building.
Third church (current structure).
On 23 February 532, only a few weeks after the destruction of the second basilica, Emperor Justinian I decided to build a third and entirely different basilica, larger and more majestic than its predecessors.
Justinian chose physicist Isidore of Miletus and mathematician Anthemius of Tralles as architects; Anthemius, however, died within the first year of the endeavor. The construction is described in the Byzantine historian Procopius' "On Buildings" ("Peri ktismatōn", Latin: "De aedificiis"). Columns and other marbles were brought from all over the empire, throughout the Mediterranean. The idea of these columns being spoils from cities such as Rome and Ephesus is a later invention. Even though they were made specifically for Hagia Sophia, the columns show variations in size. More than ten thousand people were employed. This new church was contemporaneously recognized as a major work of architecture. The theories of Heron of Alexandria may have been utilized to address the challenges presented by building such an expansive dome over so large a space. The emperor, together with the Patriarch Menas, inaugurated the new basilica on 27 December 537 – 5 years and 10 months after construction start – with much pomp. The mosaics inside the church were, however, only completed under the reign of Emperor Justin II (565–578).
Hagia Sophia was the seat of the Orthodox patriarch of Constantinople and a principal setting for Byzantine imperial ceremonies, such as coronations. Like other churches throughout Christendom, the basilica offered sanctuary from persecution to outlaws.
Earthquakes in August 553 and on 14 December 557 caused cracks in the main dome and eastern half-dome. The main dome collapsed completely during a subsequent earthquake on 7 May 558, destroying the ambon, altar, and ciborium. The crash was due mainly to the too high bearing load and to the enormous shearing load of the dome, which was too flat. These caused the deformation of the piers which sustained the dome. The emperor ordered an immediate restoration. He entrusted it to Isidorus the Younger, nephew of Isidore of Miletus, who used lighter materials and elevated the dome by "30 feet" (about ) – giving the building its current interior height of . Moreover, Isidorus changed the dome type, erecting a ribbed dome with pendentives, whose diameter lay between 32.7 and 33.5 m. Under Justinian's orders, eight Corinthian columns were disassembled from Baalbek, Lebanon, and shipped to Constantinople around 560. This reconstruction, giving the church its present 6th-century form, was completed in 562. The Byzantine poet Paul the Silentiary composed a long epic poem (still extant), known as "Ekphrasis", for the rededication of the basilica presided over by Patriarch Eutychius on 23 December 562.
In 726, the emperor Leo the Isaurian issued a series of edicts against the veneration of images, ordering the army to destroy all icons – ushering in the period of Byzantine iconoclasm. At that time, all religious pictures and statues were removed from the Hagia Sophia. After a brief reprieve under Empress Irene (797–802), the iconoclasts made a comeback. Emperor Theophilus (829–842) was strongly influenced by Islamic art, which forbids the representation of living beings. He had a two-winged bronze doors with his monograms installed at the southern entrance of the church.
The basilica suffered damage, first in a great fire in 859, and again in an earthquake on 8 January 869, that made a half-dome collapse. Emperor Basil I ordered the church repaired.
After the great earthquake of 25 October 989, which collapsed the Western dome arch, Emperor Basil II asked for the Armenian architect Trdat, creator of the great churches of Ani and Argina, to direct the repairs. He erected again and reinforced the fallen dome arch, and rebuilt the west side of the dome with 15 dome ribs. The extent of the damage required six years of repair and reconstruction; the church was re-opened on 13 May 994. At the end of the reconstruction, the church's decorations were renovated, including the additions of paintings of four immense cherubs, a new depiction of Christ on the dome, burial cloth of Christ shown on Fridays, and on the apse a new depiction of the Virgin Mary holding Jesus between the apostles Peter and Paul. On the great side arches were painted the prophets and the teachers of the church.
In his book "De caerimoniis aulae Byzantinae" ("Book of Ceremonies"), Emperor Constantine VII (913–919) wrote a detailed account of the ceremonies held in the Hagia Sophia by the emperor and the patriarch.
Upon the capture of Constantinople during the Fourth Crusade, the church was ransacked and desecrated by the Latin Christians, as described by the Byzantine historian Niketas Choniates. During the Latin occupation of Constantinople (1204–1261) the church became a Roman Catholic cathedral. Baldwin I of Constantinople was crowned emperor on 16 May 1204 in Hagia Sophia, at a ceremony which closely followed Byzantine practices. Enrico Dandolo, the Doge of Venice who commanded the sack and invasion of the city by the Latin Crusaders in 1204, is buried inside the church. The tomb inscription carrying his name, which has become a part of the floor decoration, was spat upon by many of the angry Byzantines who recaptured Constantinople in 1261. However, restoration led by the brothers Gaspare and Giuseppe Fossati during the period 1847–1849 cast doubt upon the authenticity of the doge's grave; it is more likely a symbolic memorial rather than burial site.
After the recapture in 1261 by the Byzantines, the church was in a dilapidated state. In 1317, emperor Andronicus II ordered four new buttresses (Πυραμὶδας, Greek:"Piramídas") to be built in the eastern and northern parts of the church, financing them with the inheritance of his deceased wife, Irene. New cracks developed in the dome after the earthquake of October 1344, and several parts of the building collapsed on 19 May 1346; consequently, the church was closed until 1354, when repairs were undertaken by architects Astras and Peralta.
Mosque (1453–1935).
Constantinople was taken by the Ottomans on 29 May 1453. In accordance with the custom at the time Sultan Mehmet II allowed his troops three days of unbridled pillage once the city fell, after which he would claim its contents for himself.
Hagia Sophia was not exempted from the pillage, becoming its focal point as the invaders believed it to contain the greatest treasures of the city.
Shortly after the city's defenses collapsed, pillagers made their way to the Hagia Sophia and battered down its doors. Throughout the siege worshipers participated in the Holy Liturgy and Prayer of the Hours at the Hagia Sophia, and the church formed a refuge for many of those who were unable to contribute to the city's defense, such as women, children and elderly. Trapped in the church, congregants and refugees became spoils to be divided amongst the Ottoman invaders. The building was desecrated and looted, and occupants enslaved, violated or slaughtered; while elderly and infirm were killed, women and girls were raped and the remainder chained and sold into slavery. Priests continued to perform Christian rites until stopped by the invaders. When the Sultan and his cohort entered the church, he insisted it should be at once transformed into a mosque. One of the Ulama then climbed the pulpit and recited the Shahada.
As described by several Western visitors (such as the Córdoban nobleman Pero Tafur and the Florentine Cristoforo Buondelmonti), the church was in a dilapidated state, with several of its doors fallen from their hinges; Mehmed II ordered a renovation as well as the conversion. Mehmet attended the first Friday prayer in the mosque on 1 June 1453. Aya Sofya became the first imperial mosque of Istanbul. To the corresponding Waqf were endowed most of the existing houses in the city and the area of the future Topkapı Palace. From 1478, 2,360 shops, 1,300 houses, 4 caravanserais, 30 "boza" shops, and 23 shops of sheep heads and trotters gave their income to the foundation. Through the imperial charters of 1520 (AH 926) and 1547 (AH 954) shops and parts of the Grand Bazaar and other markets were added to the foundation.
Before 1481 a small minaret was erected on the southwest corner of the building, above the stair tower. Later, the subsequent sultan, Bayezid II (1481–1512), built another minaret at the northeast corner. One of these collapsed after the earthquake of 1509, and around the middle of the 16th century they were both replaced by two diagonally opposite minarets built at the east and west corners of the edifice.
In the 16th century the sultan Suleiman the Magnificent (1520–1566) brought back two colossal candlesticks from his conquest of Hungary. They were placed on either side of the mihrab. During the reign of Selim II (1566–1574), the building started showing signs of fatigue and was extensively strengthened with the addition of structural supports to its exterior by the great Ottoman architect Mimar Sinan, who is also considered one of the world's first earthquake engineers. In addition to strengthening the historic Byzantine structure, Sinan built the two additional large minarets at the western end of the building, the original sultan's lodge, and the Türbe (mausoleum) of Selim II to the southeast of the building in 1576-7 / AH 984. In order to do that, parts of the Patriarchate at the south corner of the building were pulled down the previous year. Moreover, the golden crescent was mounted on the top of the dome, while a respect zone 35 "arşin" (about 24 m) wide was imposed around the building, pulling down all the houses which in the meantime had nested around it. Later his türbe hosted also 43 tombs of Ottoman princes. In 1594 / AH 1004 "Mimar" (court architect) Davud Ağa built the türbe of Murad III (1574–1595), where the Sultan and his Valide, Safiye Sultan were later buried. The octagonal mausoleum of their son Mehmed III (1595–1603) and his Valide was built next to it in 1608 / 1017 H by royal architect Dalgiç Mehmet Aĝa. His son Mustafa I (1617–1618; 1622–1623) converted the baptistery into his türbe.
Murad III had also two large alabaster Hellenistic urns transported from Pergamon and placed on two sides of the nave.
In 1717, under Sultan Ahmed III (1703–1730), the crumbling plaster of the interior was renovated, contributing indirectly to the preservation of many mosaics, which otherwise would have been destroyed by mosque workers. In fact, it was usual for them to sell mosaics stones – believed to be talismans – to the visitors. Sultan Mahmud I ordered the restoration of the building in 1739 and added a "medrese" (a Koranic school, now the library of the museum), an "Imaret" (soup kitchen for distribution to the poor) and a library, and in 1740 a "Şadirvan" (fountain for ritual ablutions), thus transforming it into a "külliye", i.e. a social complex. At the same time a new sultan's lodge and a new mihrab were built inside.
Renovation of 1847.
The most famous restoration of the Hagia Sophia was ordered by Sultan Abdülmecid and completed by eight hundred workers between 1847 and 1849, under the supervision of the Swiss-Italian architect brothers Gaspare and Giuseppe Fossati. The brothers consolidated the dome and vaults, straightened the columns, and revised the decoration of the exterior and the interior of the building. The mosaics in the upper gallery were uncovered and cleaned, although many were re-covered "for protection against further damage". The old chandeliers were replaced by new pendant ones. New gigantic circular-framed disks or medallions were hung on columns. These were inscribed with the names of Allah, Muhammad, the first four caliphs Abu Bakr, Umar, Uthman and Ali, and the two grandchildren of Muhammad: Hassan and Hussain, by the calligrapher Kazasker Mustafa İzzed Effendi (1801–1877). In 1850 the architect Fossati built a new sultan's lodge or loge in a Neo-Byzantine style connected to the royal pavilion behind the mosque. They also renovated the minbar and mihrab. Outside the main building, the minarets were repaired and altered so that they were of equal height. A timekeeper's building and a new madrasah were built. When the restoration was finished, the mosque was re-opened with ceremonial pomp on 13 July 1849.
Museum (1935–present).
In 1935, the first Turkish President and founder of the Republic of Turkey, Mustafa Kemal Atatürk, transformed the building into a museum. The carpets were removed and the marble floor decorations such as the Omphalion appeared for the first time in centuries, while the white plaster covering many of the mosaics was removed. Nevertheless, the condition of the structure deteriorated, and the World Monuments Fund placed Hagia Sophia on 1996 World Monuments Watch, and again in 1998. The building's copper roof had cracked, causing water to leak down over the fragile frescoes and mosaics. Moisture entered from below as well. Rising ground water had raised the level of humidity within the monument, creating an unstable environment for stone and paint. With the help of financial services company American Express, WMF secured a series of grants from 1997 to 2002 for the restoration of the dome. The first stage of work involved the structural stabilization and repair of the cracked roof, which was undertaken with the participation of the Turkish Ministry of Culture. The second phase, the preservation of the dome's interior, afforded the opportunity to employ and train young Turkish conservators in the care of mosaics. By 2006, the WMF project was complete, though many other areas of Hagia Sophia continue to require significant stability improvement, restoration and conservation. Haghia Sophia is currently (2014) the second most visited museum in Turkey, attracting almost 3.3 million visitors annually.
Although use of the complex as a place of worship (mosque or church) was strictly prohibited, in 2006 the Turkish government allowed the allocation of a small room in the museum complex to be used as a prayer room for Christian and Muslim museum staff, and since 2013 from the minarets of the museum the muezzin sings the call to prayer twice per day, in the afternoon.
In 2007, Greek American politician Chris Spirou launched an international organization "Free Agia Sophia Council" championing the cause of restoring the building to its original function as a Christian church. Since the early 2010s, several campaigns and government high officials, notably Turkey's deputy prime minister Bülent Arınç in November 2013, have been demanding that Hagia Sophia be converted into a mosque again. In 2015, in retaliation for the acknowledgment by Pope Francis of the Armenian Genocide, the Mufti of Ankara, Mefail Hızlı, stated that he believes the conversion of Haghia Sophia into a mosque will be accelerated.
Architecture.
Hagia Sophia is one of the greatest surviving examples of Byzantine architecture. Its interior is decorated with mosaics and marble pillars and coverings of great artistic value. The temple itself was so richly and artistically decorated that Justinian proclaimed, "Solomon, I have outdone thee!" (Νενίκηκά σε Σολομών). Justinian himself had overseen the completion of the greatest cathedral ever built up to that time, and it was to remain the largest cathedral for 1,000 years up until the completion of the cathedral in Seville in Spain.
Justinian's basilica was at once the culminating architectural achievement of late antiquity and the first masterpiece of Byzantine architecture. Its influence, both architecturally and liturgically, was widespread and enduring in the Eastern Orthodox, Roman Catholic, and Muslim worlds alike.
The vast interior has a complex structure. The nave is covered by a central dome which at its maximum is from floor level and rests on an arcade of 40 arched windows. Repairs to its structure have left the dome somewhat elliptical, with the diameter varying between .
At the western entrance side and eastern liturgical side, there are arched openings extended by half domes of identical diameter to the central dome, carried on smaller semi-domed exedras; a hierarchy of dome-headed elements built up to create a vast oblong interior crowned by the central dome, with a clear span of .
Interior surfaces are sheathed with polychrome marbles, green and white with purple porphyry, and gold mosaics. The exterior, clad in stucco, was tinted yellow and red during restorations in the 19th century at the direction of the Fossati architects.
Narthex and portals.
The Imperial Gate was the main entrance between the exo- and esonarthex. It was reserved only for the emperor. The Byzantine mosaic above the portal depicts Christ and an unnamed Emperor. A long ramp from the northern part of the outer narthex leads up to the upper gallery.
Upper Gallery.
The upper gallery is laid out in a horseshoe shape that encloses the nave until the apse. Several mosaics are preserved in the upper gallery, an area traditionally reserved for the empress and her court. The best-preserved mosaics are located in the southern part of the gallery.
The upper gallery contains runic graffiti presumed to be from the Varangian Guard.
Dome.
The dome of Hagia Sophia has spurred particular interest for many art historians, architects and engineers because of the innovative way the original architects envisioned it. The cupola is carried on four spherical triangular pendentives, an element which was first fully realized in this building. The pendentives implement the transition from the circular base of the dome to the rectangular base below, restraining the lateral forces of the dome and allow its weight to flow downwards. They were reinforced with buttresses during Byzantine and later during Ottoman times, under the guidance of the architect Sinan. 
The weight of the dome remained a problem for most of the building's existence. The original cupola collapsed entirely after the quake of 558; in 563 a new dome was built by Isidore the younger, a nephew of Isidore of Miletus. Unlike the original, this included 40 ribs and was slightly taller, in order to lower the lateral forces on the church walls. A larger section of the second dome collapsed as well, in two episodes, so that today only two sections of the present dome, in the north and south side, still date from the 562 reconstruction. Of the whole dome's 40 ribs, the surviving north section contains 8 ribs, while the south section includes 6 ribs.
Although this design stabilizes the dome and the surrounding walls and arches, the actual construction of the walls of Hagia Sophia weakened the overall structure. The bricklayers used more mortar than brick, weakening the walls. The structure would have been more stable if the builders at least let the mortar cure before they began the next layer; however, they did not do this. When the dome was erected, its weight caused the walls to lean outward because of the wet mortar underneath. When Isidore the Younger rebuilt the fallen cupola, he had to first build up the interior of the walls to make them vertical again. Additionally, the architect raised the height of the rebuilt dome by approximately six metres so that the lateral forces would not be as strong and its weight would flow more easily down into the walls. Moreover, he shaped the new cupola like a scalloped shell or the inside of an umbrella, with ribs that extend from the top down to the base. These ribs allow the weight of the dome to flow between the windows, down the pendentives, and ultimately to the foundation.
Hagia Sophia is famous for the light that reflects everywhere in the interior of the nave, giving the dome the appearance of hovering above. This effect was achieved by inserting forty windows around the base of the original structure. Moreover, the insertion of the windows in the dome structure lowers its weight.
Minarets.
One of the minarets (at southwest) was built from red brick while the other three were built from white limestone and sandstone, of which the slender northeast column was erected by Sultan Bayezid II while the two larger minarets to the west were erected by Sultan Selim II and designed by the famous Ottoman architect Mimar Sinan.
Notable elements and decorations.
Originally, under Justinian's reign, the interior decorations consisted of abstract designs on marble slabs on the walls and floors, as well as mosaics on the curving vaults. Of these mosaics, one can still see the two archangels Gabriel and Michael in the spandrels of the bema. There were already a few figurative decorations, as attested by the eulogy of Paul the Silentiary. The spandrels of the gallery are revetted in "opus sectile", showing patterns and figures of flowers and birds in precisely cut pieces of white marble set against a background of black marble. In later stages figurative mosaics were added, which were destroyed during the iconoclastic controversy (726–843). Present mosaics are from the post-iconoclastic period. The number of treasures, relics and miracle-working, painted icons of the Hagia Sophia grew progressively richer into an amazing collection. 
Apart from the mosaics, a large number of figurative decorations were added during the second half of the 9th century: an image of Christ in the central dome; Orthodox saints, prophets and Church Fathers in the tympana below; historical figures connected with this church, such as Patriarch Ignatius; some scenes from the gospel in the galleries.
Basil II let paint on each of the four pendentives a giant six-winged Cherub. The Ottomans covered their face with a golden halo, but in 2009 one of them was restored to the original state.
Loge of the Empress.
The Loge of the Empress is located in the centre of the upper enclosure, or gallery, of the Hagia Sophia. From there the empress and the court-ladies would watch the proceedings down below. A round, green stone marks the spot where the throne of the empress stood.
Lustration urns.
Two huge marble lustration (ritual purification) urns were brought from Pergamon during the reign of Sultan Murad III. Stemming from the Hellenistic period, they are carved from single blocks of marble.
Marble Door.
The Marble Door inside the Hagia Sophia is located in the southern upper enclosure, or gallery. It was used by the participants in synods, they entered and left the meeting chamber through this door.
Wishing column.
At the northwest of the building there is a column with a hole in the middle covered by bronze plates. This column goes by different names; the perspiring column, the wishing column, the sweating column or the crying column. The column is said to be damp when touched and have supernatural powers. The legend states that since St. Gregory the Miracle Worker appeared at the column in year 1200, the column is moist. It is believed that touching the moisture cures many illnesses.
Mosaics.
The church was richly decorated with mosaics throughout the centuries. They either depicted the Virgin Mother, Jesus, saints, or emperors and empresses. Other parts were decorated in a purely decorative style with geometric patterns.
The mosaics however for their most part date to after the end of the Byzantine Iconoclasm of 800 AD.
During the Sack of Constantinople in 1204, the Latin Crusaders vandalized valuable items in every important Byzantine structure of the city, including the golden mosaics of the Hagia Sophia. Many of these items were shipped to Venice, whose Doge, Enrico Dandolo, had organized the invasion and sack of Constantinople.
19th-century restoration.
Following the building's conversion into a mosque in 1453, many of its mosaics were covered with plaster, due to Islam's ban on representational imagery. This process was not completed at once, and reports exist from the 17th century in which travellers note that they could still see Christian images in the former church. In 1847–49, the building was restored by two Swiss Italian Fossati brothers, Gaspare and Giuseppe, and Sultan Abdülmecid allowed them to also document any mosaics they might discover during this process. This work did not include repairing the mosaics and after recording the details about an image, the Fossatis painted it over again. The Fossatis restored the mosaics of the two "hexapteryga" (singular , pr. hexapterygon, six-winged angel); it is uncertain whether they are seraphim or cherubim) located on the two east pendentives, covering their faces again before the end of the restoration. The other two placed on the west pendentives are copies in paint created by the Fossatis, since they could find no surviving remains of them. As in this case, the architects reproduced in paint damaged decorative mosaic patterns, sometimes redesigning them in the process. The Fossati records are the primary sources about a number of mosaic images now believed to have been completely or partially destroyed in the 1894 Istanbul earthquake. These include a mosaic over a now-unidentified "Door of the Poor", a large image of a jewel-encrusted cross, and a large number of images of angels, saints, patriarchs, and church fathers. Most of the missing images were located in the building's two tympana.
One mosaic they documented is Christ Pantocrator in a circle, which would indicate it to be a ceiling mosaic, possibly even of the main dome which was later covered and painted over with Islamic calligraphy that expounds God as the light of the universe. The drawings of the Hagia Sophia mosaics are today kept in the Cantonal Archive of Ticino.
20th-century restoration.
A large number of mosaics were uncovered in the 1930s by a team from the Byzantine Institute of America led by Thomas Whittemore. The team chose to let a number of simple cross images remain covered by plaster, but uncovered all major mosaics found.
Because of its long history as both a church and a mosque, a particular challenge arises in the restoration process. Christian iconographic mosaics can be uncovered, but often at the expense of important and historic Islamic art. Restorers have attempted to maintain a balance between both Christian and Islamic cultures. In particular, much controversy rests upon whether the Islamic calligraphy on the dome of the cathedral should be removed, in order to permit the underlying Pantocrator mosaic of Christ as Master of the World, to be exhibited (assuming the mosaic still exists).
Imperial Gate mosaic.
The Imperial Gate mosaic is located in the tympanum above that gate, which was used only by the emperors when entering the church. Based on style analysis, it has been dated to the late 9th or early 10th century. The emperor with a nimbus or halo could possibly represent emperor Leo VI the Wise or his son Constantine VII Porphyrogenitus bowing down before Christ Pantocrator, seated on a jeweled throne, giving His blessing and holding in His left hand an open book. The text on the book reads as follows: "Peace be with you. I am the light of the world". (John 20:19; 20:26; 8:12) On each side of Christ's shoulders is a circular medallion: on His left the Archangel Gabriel, holding a staff, on His right His Mother Mary.
Southwestern entrance mosaic.
The southwestern entrance mosaic, situated in the tympanum of the southwestern entrance, dates from the reign of Basil II. It was rediscovered during the restorations of 1849 by Fossati. The Virgin sits on a throne without a back, her feet resting on a pedestal, embellished with precious stones. The Child Christ sits on her lap, giving His blessing and holding a scroll in His left hand. On her left side stands emperor Constantine in ceremonial attire, presenting a model of the city to Mary. The inscription next to him says: "Great emperor Constantine of the Saints". On her right side stands emperor Justinian I, offering a model of the Hagia Sophia. The medallions on both sides of the Virgin's head carry the monograms MP and ΘY, an abbreviation of ""Mētēr"" and ""Theou"", meaning "Mother of God".
Apse mosaics.
The Virgin and Child mosaic was the first of the post-iconoclastic mosaics. It was inaugurated on 29 March 867 by Patriarch Photius and the emperors Michael III and Basil I. This mosaic is situated in a high location on the half dome of the apse. Mary is sitting on a throne without a back, holding the Child Jesus on her lap. Her feet rest on a pedestal. Both the pedestal and the throne are adorned with precious stones. The portraits of the archangels Gabriel and Michael (largely destroyed) in the bema of the arch also date from the 9th century. The mosaics are set against the original golden background of the 6th century. These mosaics were believed to be a reconstruction of the mosaics of the 6th century that were previously destroyed during the iconoclastic era by the Byzantines of that time, as represented in the inaugural sermon by the patriarch Photios. However, no record of figural decoration of Hagia Sophia exists before this time.
Emperor Alexander mosaic.
The Emperor Alexander mosaic is not easy to find for the first-time visitor, located in the second floor in a dark corner of the ceiling. It depicts Emperor Alexander in full regalia, holding a scroll in his right hand and a globus cruciger in his left. A drawing by Fossati showed that the mosaic survived until 1849, and that Thomas Whittemore, founder of the Byzantine Institute of America who was granted permission to preserve the mosaics, assumed that it had been destroyed in the earthquake of 1894. Eight years after his death, the mosaic was discovered in 1958 largely through the researches of Robert Van Nice. Unlike most of the other mosaics in Hagia Sophia, which had been covered over by ordinary plaster, the Alexander mosaic was simply painted over and reflected the surrounding mosaic patterns and thus was well hidden. It was duly cleaned by the Byzantine Institute's successor to Whittemore, Paul A. Underwood.
Empress Zoe mosaic.
The Empress Zoe mosaic on the eastern wall of the southern gallery date from the 11th century. Christ Pantocrator, clad in the dark blue robe (as is the custom in Byzantine art), is seated in the middle against a golden background, giving His blessing with the right hand and holding the Bible in His left hand. On either side of His head are the monograms "IC" and "XC", meaning "Iēsous Khristos". He is flanked by Constantine IX Monomachus and Empress Zoe, both in ceremonial costumes. He is offering a purse, as symbol of the donation he made to the church, while she is holding a scroll, symbol of the donations she made. The inscription over the head of the emperor says: "Constantine, pious emperor in Christ the God, king of the Romans, Monomachus". The inscription over the head of the empress reads as follows: "Zoë, the very pious Augusta". The previous heads have been scraped off and replaced by the three present ones. Perhaps the earlier mosaic showed her first husband Romanus III Argyrus or her second husband Michael IV. Another theory is that this mosaic were made for an earlier emperor and empress, with their heads changed into the present ones.
Comnenus mosaic.
The Comnenus mosaic, also located on the eastern wall of the southern gallery, dates from 1122. The Virgin Mary is standing in the middle, depicted, as usual in Byzantine art, in a dark blue gown. She holds the Child Christ on her lap. He gives His blessing with His right hand while holding a scroll in His left hand. On her right side stands emperor John II Comnenus, represented in a garb embellished with precious stones. He holds a purse, symbol of an imperial donation to the church. Empress Irene stands on the left side of the Virgin, wearing ceremonial garments and offering a document. Their eldest son Alexius Comnenus is represented on an adjacent pilaster. He is shown as a beardless youth, probably representing his appearance at his coronation aged seventeen. In this panel one can already see a difference with the Empress Zoe mosaic that is one century older. There is a more realistic expression in the portraits instead of an idealized representation. The empress is shown with plaited blond hair, rosy cheeks and grey eyes, revealing her Hungarian descent. The emperor is depicted in a dignified manner.
Deësis mosaic.
The Deësis mosaic (, "Entreaty") probably dates from 1261. It was commissioned to mark the end of 57 years of Roman Catholic use and the return to the Orthodox faith. It is the third panel situated in the imperial enclosure of the upper galleries. It is widely considered the finest in Hagia Sophia, because of the softness of the features, the humane expressions and the tones of the mosaic. The style is close to that of the Italian painters of the late 13th or early 14th century, such as Duccio. In this panel the Virgin Mary and John the Baptist ("Ioannes Prodromos"), both shown in three-quarters profile, are imploring the intercession of Christ Pantocrator for humanity on Judgment Day. The bottom part of this mosaic is badly deteriorated. This mosaic is considered as the beginning of the Renaissance in Byzantine pictorial art.
Northern tympanum mosaics.
The northern tympanum mosaics feature various saints. They have been able to survive due to the very high and unreachable location. They depict Saints John Chrysostom and Ignatius the Younger standing, clothed in white robes with crosses, and holding richly jeweled Holy Bibles. The names of each saint is given around the statues in Greek, in order to enable an identification for the visitor. The other mosaics in the other tympana have not survived probably due to the frequent earthquakes as opposed to any deliberate destruction by the Ottoman conquerors.
See also.
Related buildings:

</doc>
<doc id="42765" url="https://en.wikipedia.org/wiki?curid=42765" title="Chaz Bono">
Chaz Bono

Chaz Salvatore Bono (born Chastity Sun Bono, March 4, 1969) is an American advocate, writer, musician and actor. He is the only child of American entertainers Sonny and Cher. 
Bono is a transgender man. In 1995, several years after being outed as lesbian by the tabloid press, he publicly self-identified as such in a cover story in a leading American gay monthly magazine, "The Advocate", eventually going on to discuss the process of coming out to oneself and to others in two books. "Family Outing: A Guide to the Coming Out Process for Gays, Lesbians, and Their Families" (1998) includes his coming-out account. The memoir "The End of Innocence" (2003) discusses his outing, music career, and partner Joan's death from non-Hodgkin's lymphoma.
Between 2008 and 2010, Bono underwent female-to-male gender transition. A two-part "Entertainment Tonight" feature in June 2009 explained that his transition had started a year before. In May 2010, he legally changed his gender and name. A documentary on Bono's experience, "Becoming Chaz", was screened at the 2011 Sundance Film Festival and later made its television debut on OWN: Oprah Winfrey Network.
Early life.
Bono was born in Los Angeles, California, the only child of Cher and Sonny Bono of the pop duo Sonny & Cher, stars of a TV variety show on which the young child often appeared. Bono was named "Chastity Sun Bono" after the film "Chastity", which was produced by Sonny and in which Cher (in her first solo role in a feature film) played a bisexual woman.
Bono came out to both parents as lesbian at age 18. In "Family Outing", Bono wrote that, "as a child, I always felt there was something different about me. I'd look at other girls my age and feel perplexed by their obvious interest in the latest fashion, which boy in class was the cutest, and who looked the most like cover girl Christie Brinkley. When I was 13, I finally found a name for exactly how I was different. I realized I was gay."
Ceremony.
Bono began a short music career in 1988 with the band Ceremony, which released one album, "Hang Out Your Poetry", in 1993. The band featured Bono on vocals, acoustic guitar, and percussion. Other members were Steve March Tormé (backup vocals), Heidi Shink a.k.a. Chance, Pete McRae, Steve Bauman, Louis Ruiz, and Bryn Mathieu. All but one of the band's songs were written or co-written by Bono, Shink, and Mark Hudson. They used no synthesizers or digital effects on the album; Shink noted, "We turned our back on technology. [ ... ] It's reminiscent of the 60s, but more a tip of the hat than emulating it. We took the music we love and rejuvenated it, made it 90s." Critical reception of the album was lukewarm, with Roch Parisien of Allmusic describing "Hang Out Your Poetry" as a mildly psychedelic take on early 1990s pop, "pleasant, accessible, well-produced ear-candy that's ultimately toothless".
The songs "Could've Been Love" and "Ready for Love" were released as singles from the album. Sonny and Cher also recorded backing vocals for the track "Livin' It Up" on the album.
LGBT activism.
In April 1995, Bono came out as a lesbian in an interview with "The Advocate", a national gay and lesbian magazine. The 1998 book "Family Outing" detailed how Bono's coming out "catapulted me into a political role that has transformed my life, providing me with affirmation as a lesbian, as a woman, and as an individual." In the same book, Bono reported that Cher, who was both a gay icon and an ally of LGBT communities, was quite uncomfortable with the news at first and "went ballistic" before coming to terms with it: "By August 1996, one year after I came out publicly, my mother had progressed so far that she agreed to 'come out' herself on the cover of "The Advocate" as the proud mother of a lesbian daughter." Cher has since become an outspoken LGBT rights activist.
Bono's paternal relationship became strained after Sonny became a Republican Congressman from California. The differences in their political views separated them, and the two had not spoken for more than a year at the time of Sonny's fatal skiing accident in January 1998.
Bono worked as a writer at large for "The Advocate". As a social activist, Bono became a spokesperson for the Human Rights Campaign, promoted National Coming Out Day, campaigned for the reelection of Bill Clinton for US President, campaigned against the Defense of Marriage Act, and served as Entertainment Media Director for the Gay and Lesbian Alliance Against Defamation (GLAAD). Bono was a team captain for "Celebrity Fit Club 3" (2006) and was supported by girlfriend Jennifer Elia, who orchestrated exercise and training sessions.
Transition.
In mid-2008, Bono began undergoing a physical and social transition from female to male. This was confirmed in June 2009 by his publicist, who identified Bono's preferred name as "Chaz Bono" and said, "It is Chaz's hope that his choice to transition will open the hearts and minds of the public regarding this issue, just as his coming out did." GLAAD and the Empowering Spirits Foundation were quick to offer praise and support for the announcement. Bono's legal transition was completed on May 8, 2010, when a California court granted his request for a gender and name change. He chose the name "Chaz Salvatore Bono" in honor of his parents. Bono made "Becoming Chaz", a documentary film about his transition that premiered at the 2011 Sundance Film Festival. OWN: Oprah Winfrey Network acquired the rights to the documentary and debuted it on May 10, 2011.
In September 2011, he became a competitor on the 13th season of the US version of "Dancing with the Stars", paired with professional ballroom dancer Lacey Schwimmer. The duo was eliminated October 25, 2011. This was the first time an openly transgender man starred on a major network television show for something unrelated to being transgender.

</doc>
<doc id="42766" url="https://en.wikipedia.org/wiki?curid=42766" title="Climbing wall">
Climbing wall

A climbing wall is an artificially constructed wall with grips for hands and feet, usually used for indoor climbing, but sometimes located outdoors. Some are brick or wooden constructions, but on most modern walls, the material most often used is a thick multiplex board with holes drilled into it. Recently, manufactured steel and aluminum have also been used. The wall may have places to attach belay ropes, but may also be used to practise lead climbing or bouldering.
Each hole contains a specially formed t-nut to allow modular climbing holds to be screwed onto the wall. With manufactured steel or aluminum walls, an engineered industrial fastener is used to secure climbing holds. The face of the multiplex board climbing surface is covered with textured products including concrete and paint or polyurethane loaded with sand. In addition to the textured surface and hand holds, the wall may contain surface structures such as indentions (incuts) and protrusions (bulges), or take the form of an overhang, underhang or crack.
Some grips are formed to mimic the conditions of outdoor rock, including some that are oversized and can have other grips bolted onto them.
History.
The earliest artificial climbing walls were typically small concrete faces with protrusions made of medium-sized rocks for hand holds. Schurman Rock in Seattle, WA is believed to be the first artificial climbing structure in the United States, constructed in 1939. 
The modern artificial climbing wall began in the UK. The first wall was created in 1964 by Don Robinson, a lecturer in Physical Education by inserting pieces of rock into a corridor wall. The first commercial wall was built in Sheffield, traditionally England's centre for climbing due to its proximity to the Peak District. The first indoor climbing gym in the U.S. was established by Vertical World in Seattle, WA in 1987.
Wall types.
The simplest type of wall is of plywood construction, known colloquially in the climbing community as a 'woody', with a combination of either bolt-on holds or screw on holds. Bolt-on holds are fixed to a wall with iron bolts which are inserted through the hold, which will have specific bolt points, and then fixed into pre-allocated screw-threaded holes in the wall. Screw-on holds are, by contrast, usually much smaller, owing to the nature of their fixing. These holds are connected to the wall by screws which may be fastened anywhere on the wall's surface.
Some other types of walls include slabs of granite, concrete sprayed onto a wire mesh, pre-made fiberglass panels, large trees, manufactured steel and aluminum panels, textured fiberglass walls and inflatables.
Routes and grading.
Holds come in different colours, those of the same colour often being used to denote a route, allowing routes of different difficulty levels to be overlaid on one another. Coloured tape placed under climbing holds is another way that is often used to mark different climbing routes. In attempting a given route, a climber is only allowed to use grips of the designated colour as handholds but is usually allowed to use both handholds and footholds of the designated colour and surface structures and textures of the "rockface" as footholds.
The grade (difficulty) of the route is usually a consensus decision between the setter of the route and the first few people who climb the route.
Many indoor climbing walls have people who are assigned to set these different climbing routes. These people are called route setters or course setters.
As indoor climbing walls are often used to check the development of climber's ability, climbs are color-coded.

</doc>
<doc id="42768" url="https://en.wikipedia.org/wiki?curid=42768" title="Old Church Slavonic">
Old Church Slavonic

Old Church Slavonic (, ), also known as Old Church Slavic (; often abbreviated to OCS; self-name , "slověnĭskŭ językŭ"), was the first Slavic literary language. The 9th-century Byzantine missionaries Saints Cyril and Methodius of Slavic, Greek descent, or both, are credited with standardizing the language and using it in translating the Bible and other Ancient Greek ecclesiastical texts as part of the Christianization of the Slavs. It is thought to have been based primarily on the dialect of the 9th century Byzantine Slavs living in the Province of Thessalonica (now in Greece). It played an important role in the history of the Slavic languages and served as a basis and model for later Church Slavonic traditions, and some Eastern Orthodox and Eastern Catholic churches use this later Church Slavonic as a liturgical language to this day. As the oldest attested Slavic language, OCS provides important evidence for the features of Proto-Slavic, the reconstructed common ancestor of all Slavic languages.
History.
The language was standardized for the mission of the two apostles to Great Moravia in 863 (see Glagolitic alphabet for details). For that purpose, Cyril and his brother Methodius started to translate religious literature to Old Church Slavonic, allegedly based on the Slavic dialects spoken in the hinterland of their hometown, Thessaloniki, in the today's Greece.
As part of the preparation for the mission, in 862/863, the Glagolitic alphabet was created and the most important prayers and liturgical books, including the Aprakos Evangeliar (a Gospel Book lectionary containing only feast-day and Sunday readings), the Psalter, and Acts of the Apostles, were translated. (The Gospels were also translated early, but it is unclear whether Sts. Cyril or Methodius had a hand in this). The language and the alphabet were taught at the Great Moravian Academy () and were used for government and religious documents and books between 863 and 885. The texts written during this phase contain characteristics of the Slavic vernaculars in Great Moravia.
In 885, the use of Old Church Slavonic in Great Moravia was prohibited by Pope Stephen V in favour of Latin. Students of the two apostles, who were expelled from Great Moravia in 886, brought the Glagolitic alphabet to First Bulgarian Empire. There it was taught at two literary schools: the Preslav Literary School and the Ohrid Literary School. The Glagolitic alphabet was originally used at both schools, though the Cyrillic script was developed early on at the Preslav Literary School where it superseded Glagolitic. The texts written during this era exhibit certain linguistic features of the vernaculars of the First Bulgarian Empire. Old Church Slavonic spread to other South-Eastern and Eastern European Slavic territories, most notably to Bosnia and Herzegovina, Croatia, Serbia, Bohemia, Lesser Poland, and principalities of the Kievan Rus' while retaining characteristically South Slavic linguistic features. Later texts written in each of those territories then began to take on characteristics of the local Slavic vernaculars and, by the mid-11th century, Old Church Slavonic had diversified into a number of regional varieties. These local varieties are collectively known as the Church Slavonic language.
Apart from the Slavic countries, Old Church Slavonic has been used as a liturgical language by the Romanian Orthodox Church, as well as a literary and official language of the princedoms of Wallachia and Moldavia (see Old Church Slavonic in Romania), before gradually being replaced by Romanian during the 18th to 19th centuries. Church Slavonic maintained a prestigious status, particularly in Russia, for many centuriesamong Slavs in the East it had a status analogous to that of Latin in Western Europe, but had the advantage of being substantially less divergent from the vernacular tongues of average parishioners. Some Orthodox churches, such as the Bulgarian Orthodox Church, Russian Orthodox Church, Serbian Orthodox Church, Ukrainian Orthodox Church and Macedonian Orthodox Church – Ohrid Archbishopric, as well as several Eastern Catholic Churches, still use Church Slavonic in their services and chants today.
Script.
Initially Old Church Slavonic was written with the Glagolitic alphabet, but later Glagolitic was replaced by Cyrillic, which was developed in the First Bulgarian Empire by a decree of Boris I of Bulgaria in the 9th century. In Bosnia was preserved the local Bosnian Cyrillic alphabet, while in Croatia a variant of the Glagolitic alphabet was preserved. See Early Cyrillic alphabet for a detailed description of the script and information about the sounds it originally expressed.
Phonology.
For Old Church Slavonic, the following segments are reconstructible. The sounds are given in Slavic transliterated form rather than in IPA, as the exact realisation is uncertain and often differs depending on the area that a text originated from.
Phonotactics.
Several notable constraints on the distribution of the phonemes can be identified, mostly resulting from the tendencies occurring within the Common Slavic period, such as "intrasyllabic synharmony" and the "law of open syllables". For consonant and vowel clusters and sequences of a consonant and a vowel, the following constraints can be ascertained:
Morphophonemic alternations.
As a result of the first and the second Slavic palatalizations, velars alternate with dentals and palatals. In addition, as a result of a process usually termed "iotation" (or "iodization"), velars and dentals alternate with palatals in various inflected forms and in word formation.
In some forms the alternations of /c/ with /č/ and of /dz/ with /ž/ occur, in which the corresponding velar is missing. The dental alternants of velars occur regularly before /ě/ and /i/ in the declension and in the imperative, and somewhat less regularly in various forms after /i/, /ę/, /ь/ and /rь/. The palatal alternants of velars occur before front vowels in all other environments, where dental alternants do not occur, as well as in various places in inflection and word formation described below.
As a result of earlier alternations between short and long vowels in roots in Proto-Indo-European, Proto-Balto-Slavic and Proto-Slavic times, and of the fronting of vowels after palatalized consonants, the following vowel alternations are attested in OCS: /ь/ : /i/;   /ъ/ : /y/ : /u/;   /e/ : /ě/ : /i/;   /o/ : /a/;   /o/ : /e/;   /ě/ : /a/;   /ъ/ : /ь/;   /y/ : /i/;   /ě/ : /i/;   /y/ : /ę/.
Vowel:∅ alternations sometimes occurred as a result of sporadic loss of weak yer, which later occurred in almost all Slavic dialects. The phonetic value of the corresponding vocalized strong jer is dialect-specific.
Grammar.
As an ancient Indo-European language, OCS has a highly inflective morphology. Inflected forms are divided in two groups, nominals and verbs. Nominals are further divided into nouns, adjectives and pronouns. Numerals inflect either as nouns or pronouns, with 1-4 showing gender agreement as well.
Nominals can be declined in three grammatical genders (masculine, feminine, neuter), three numbers (singular, plural, dual) and seven cases: nominative, vocative, accusative, instrumental, dative, genitive, and locative. There are five basic inflectional classes for nouns: "o/jo"-stems, "a/ja"-stems, "i"-stems, "u"-stems and consonant stems. Forms throughout the inflectional paradigm usually exhibit morphophonemic alternations.
Fronting of vowels after palatals and "j" yielded dual inflectional class "o" : "jo" and "a" : "ja", whereas palatalizations affected stem as a synchronic process (N sg. "vlьkъ", V sg. "vlьče"; L sg. "vlьcě"). Productive classes are "o/jo-", "a/ja-" and "i"-stems. Sample paradigms are given in the table below:
Adjectives are inflected as "o/jo"-stems (masculine and neuter) and "a/ja"-stems (feminine), in three genders. They could have short (indefinite) or long (definite) variants, the latter being formed by suffixing to the indefinite form the anaphoric third-person pronoun "jь".
Synthetic verbal conjugation is expressed in present, aorist and imperfect tenses while perfect, pluperfect, future and conditional tenses/moods are made by combining auxiliary verbs with participles or synthetic tense forms. Sample conjugation for the verb "vesti" "to lead" (underlyingly "ved-ti") is given in the table below.
Basis and local influences.
Written evidence of Old Church Slavonic survives in a relatively small body of manuscripts, most of them written in First Bulgarian Empire during the late 10th and the early 11th centuries. The language has a Southern Slavic basis with an admixture of Western Slavic features inherited during the mission of Saints Cyril and Methodius to Great Moravia (863–885).
The only well-preserved manuscript of the Moravian recension, the Kiev Folia, is characterised by the replacement of some Southern Slavic phonetic and lexical features with Western Slavic ones. Manuscripts written in the Second Bulgarian Empire (1185-1396) have, on the other hand, few Western Slavic features.
Old Church Slavonic is valuable to historical linguists since it preserves archaic features believed to have once been common to all Slavic languages such as these:
Old Church Slavonic is also likely to have preserved an extremely archaic type of accentuation (probably close to the Chakavian dialect of modern Serbo-Croatian), but unfortunately, no accent marks appear in the written manuscripts.
The Southern Slavic nature of the language is evident from the following variations:
Old Church Slavonic has some extra features in common with Bulgarian:
Great Moravia.
The language was standardized for the first time by the mission of the two apostles to Great Moravia from 863. The manuscripts of the Moravian recension are therefore the earliest dated of the OCS recensions. The recension takes its name from the Slavic state of Great Moravia which existed in Central Europe during the 9th century on the territory of today's western Slovakia and Czech Republic.
Moravian recension.
In the "Prague fragments" the only Moravian influence is replacing with and with . This recension is exemplified by the Kiev Folia. Certain other linguistic characteristics include:
First Bulgarian Empire.
Old Church Slavonic developed in the First Bulgarian Empire and was taught in Preslav (Bulgarian capital between 893 and 972), and in Ohrid (Bulgarian capital between 991/997 and 1015). It didn't represent one regional dialect but a generalized form of early eastern South Slavic, which cannot be localized. The existence of two major literary centres in the Empire led in the period from the 9th to the 11th centuries to the emergence of two recensions (otherwise called "redactions"), termed "Bulgarian" and "Macedonian" respectively. Some researchers do not differentiate between manuscripts of the two recensions, preferring to group them together in a "Macedo-Bulgarian" or simply "Bulgarian" recension. Others, as Horace Lunt, have changed their opinion with time. Initially Lunt (1974:5-6) stated that the differences in the initial OCS were neither great, nor consistent to oppose the Macedonian from the Bulgarian recension. However, a decade later Lunt (1985:202) seems to conceive OCS and its "adjustments" in somewhat different terms, that a Macedonian and a Bulgarian variety of OCS existed, illustrating his point with paleographic, phonological and other differences. The development of Old Church Slavonic literacy had the effect of preventing the assimilation of the South Slavs into neighboring cultures, which promoted the formation of a distinct Bulgarian identity.
Bulgarian recension.
The manuscripts of the Bulgarian recension or "Eastern" variant are among the oldest of the Old Church Slavonic language. This recension was centred around the Preslav Literary School. Since the earliest datable Cyrillic inscriptions were found in the area of Preslav, it is this school which is credited with the development of the Cyrillic alphabet which gradually replaced the Glagolic one. A number of prominent Bulgarian writers and scholars worked at the Preslav Literary School, including Naum of Preslav (until 893), Constantine of Preslav, John Exarch, Chernorizets Hrabar, etc. The main linguistic features of this recension are the following:
Macedonian recension.
The manuscripts of the Macedonian recension or "Western" variant are among the oldest of the Old Church Slavonic language. The recension is sometimes named Macedonian because its literary centre, Ohrid, lies in the historical region of Macedonia. At that period, administratively Ohrid formed part of the province of Kutmichevitsa in the First Bulgarian Empire until the Byzantine conquest. The main literary centre of this dialect was the Ohrid Literary School, whose most prominent member and most likely founder, was Saint Clement of Ohrid who was commissioned by Boris I of Bulgaria to teach and instruct the future clergy of the state in the Slavonic language. The language variety that was used in the area started shaping the modern Macedonian dialects. This recension is represented by the Codex Zographensis and Marianus, among others. The main linguistic features of this recension include:
Later recensions.
Later use of the language in a number of medieval Slavic polities resulted in the adjustment of Old Church Slavonic to the local vernacular, though a number of Southern Slavic, Moravian or Bulgarian features also survived. Significant later recensions of Old Church Slavonic (referred to as Church Slavonic) in the present time include: Slovene, Croatian, Serbian and Russian. In all cases, denasalization of the yuses occurred; so that only Old Church Slavonic, modern Polish and some isolated Bulgarian dialects retained the old Slavonic nasal vowels.
Serbian recension.
The Serbian recension was written mostly in Cyrillic, but also in the Glagolitic alphabet (depending on region); by the 12th century the Serbs used exclusively the Cyrillic alphabet (and Latin script in coastal areas). The 1186 Miroslav Gospels belong to the Serbian recension. They feature the following linguistic characteristics:
Due to the annexation of Bulgaria in 1396 and to the Ottoman conquest of Serbia in 1459, Serbia saw an influx of educated refugee-scribes who re-introduced a more classical form - as in manuscripts of the Bulgarian recension.
Russian recension.
The Russian recension emerged after the 10th century on the basis of the earlier Bulgarian recension, from which it differed slightly. Its main features are:
Middle Bulgarian.
The line between OCS and post-OCS manuscripts is arbitrary, and terminology varies. The common term "Middle Bulgarian" is usually contrasted to "Old Bulgarian" (an alternative name for Old Church Slavonic), and loosely used for manuscripts whose language demonstrates a broad spectrum of regional and temporal dialect features after the 11th century.
Bosnian recension.
The Bosnian recension used the Bosnian Cyrillic alphabet (better known as "Bosančica") and the Glagolitic alphabet.
Croatian recension.
The Croatian recension of Old Church Slavonic used only the Glagolitic alphabet of angular Croatian type. It shows the development of the following characteristics:
Canon.
The core corpus of Old Church Slavonic manuscripts is usually referred to as "canon". Manuscripts must satisfy certain linguistic, chronological and cultural criteria to be incorporated into the canon: they must not significantly depart from the language and tradition of Constantine and Methodius, usually known as the "Cyrillo-Methodian tradition".
For example, the Freising Fragments, dating from the 10th century, show some linguistic and cultural traits of Old Church Slavonic, but they are usually not included in the canon, as some of the phonological features of the writings appear to belong to certain Pannonian Slavic dialect of the period. Similarly, the Ostromir Gospels exhibits dialectal features that classify it as East Slavic, rather than South Slavic so it is not included in the canon either. On the other hand, the Kiev Missal is included in the canon even though it manifests some West Slavic features and contains Western liturgy because of the Bulgarian linguistic layer and connection to the Moravian mission.
Manuscripts are usually classified in two groups, depending on the alphabet used, Cyrillic or Glagolitic. With the exception of the Kiev Missal and Glagolita Clozianus, which exhibit West Slavic and Croatian features respectively, all Glagolitic texts are assumed to be of the Macedonian recension:
All Cyrillic manuscripts are of the Bulgarian recension and date from the 11th century except for the "Zographos, which is of the Macedonian recension:
Sample text.
Here is the Lord's Prayer in Old Church Slavonic:
Authors.
The history of Old Church Slavonic writing includes a northern tradition begun by the mission to Great Moravia, including a short mission in the Balaton principality, and a Bulgarian tradition begun by some of the missionaries who relocated to Bulgaria after the expulsion from Great Moravia.
Old Church Slavonic's first writings, translations of Christian liturgical and Biblical texts, were produced by Byzantine missionaries Saint Cyril and Saint Methodius, mostly during their mission to Great Moravia.
The most important authors in Old Church Slavonic after the death of Methodius and the dissolution of the Great Moravian academy were Clement of Ohrid (active also in Great Moravia), Constantine of Preslav, Chernorizetz Hrabar and John Exarch, all of whom worked in medieval Bulgaria at the end of the 9th and the beginning of the 10th century. The Second Book of Enoch was only preserved in Old Church Slavonic, although the original most certainly had been Greek or even Hebrew or Aramaic.
Nomenclature.
The name of the language in Old Church Slavonic texts was simply "Slavic" (словѣ́ньскъ ѩꙁꙑ́къ, "slověnĭskŭ językŭ"), derived from the word for "Slavs" (словѣ́нє, "slověne"), the self-designation of the compilers of the texts. This name is preserved in the modern names of the Slovak and Slovene languages. The language is sometimes called "Old Slavic", which may be confused with the distinct Proto-Slavic language. The commonly accepted terms in modern English-language Slavic studies are "Old Church Slavonic" and "Old Church Slavic".
The term "Old Bulgarian" () is the only designation used by Bulgarian-language writers. It was used in numerous 19th-century sources, e.g. by August Schleicher, Martin Hattala, Leopold Geitler and August Leskien, who noted similarities between the first literary Slavic works and the modern Bulgarian language. For similar reasons, Russian linguist Aleksandr Vostokov used the term "Slav-Bulgarian". The term is still used by some writers but nowadays normally avoided in favor of "Old Church Slavonic".
The term "Old Macedonian" is occasionally used by Western scholars in a regional context.
The obsolete term "Old Slovenian" was used by early 19th century scholars who conjectured that the language was based on the dialect of Pannonia.
Modern Slavic nomenclature.
Here are some of the names used by speakers of modern Slavic languages:

</doc>
<doc id="42777" url="https://en.wikipedia.org/wiki?curid=42777" title="299">
299

__NOTOC__
Year 299 (CCXCIX) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Valerius (or, less frequently, year 1052 "Ab urbe condita"). The denomination 299 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42778" url="https://en.wikipedia.org/wiki?curid=42778" title="298">
298

__NOTOC__
Year 298 (CCXCVIII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Faustus and Gallus (or, less frequently, year 1051 "Ab urbe condita"). The denomination 298 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42779" url="https://en.wikipedia.org/wiki?curid=42779" title="297">
297

__NOTOC__
Year 297 (CCXCVII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Valerius (or, less frequently, year 1050 "Ab urbe condita"). The denomination 297 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Persia.
</onlyinclude>

</doc>
<doc id="42780" url="https://en.wikipedia.org/wiki?curid=42780" title="Nordic Council">
Nordic Council

The Nordic Council is a geo-political inter-parliamentary forum for co-operation between the Nordic countries. It was formed after the Second World War in 1952 to promote co-operation between the five Nordic countries. Its first concrete result was the introduction in 1952 of a common labour market and free movement across borders without passports for the countries' citizens. The Council has 87 elected members from Denmark, Finland, Iceland, Norway and Sweden as well as from the Faroe Islands, Greenland and the Åland Islands. 
In 1971, the Nordic Council of Ministers, an intergovernmental forum, was established to complement the Council.
History.
During World War II, Denmark and Norway were occupied by Germany; Finland fought a costly war with the Soviet Union; while Sweden, though neutral, still felt the war's effects. Following the war, the Nordic countries pursued the idea of a Scandinavian defence union to ensure their mutual defence. However, Finland, due to its Paasikivi-Kekkonen policy of neutrality and FCMA treaty with the USSR, could not participate.
It was proposed that the Nordic countries would unify their foreign policy and defence, remain neutral in the event of a conflict and not ally with NATO, which some were planning at the time. The United States, keen on getting access to bases in Scandinavia and believing the Nordic countries incapable of defending themselves, stated it would not ensure military support for Scandinavia if they did not join NATO. As Denmark and Norway sought US aid for their post-war reconstruction, the project collapsed, with Denmark, Norway and Iceland joining NATO.
Further Nordic co-operation, such as an economic customs union, also failed. This led Danish Prime Minister Hans Hedtoft to propose, in 1951, a consultative inter-parliamentary body. This proposal was agreed by Denmark, Iceland, Norway and Sweden in 1952. The Council's first session was held in the Danish Parliament on 13 February 1953 and it elected Hans Hedtoft as its president. When Finnish-Soviet relations thawed following the death of Joseph Stalin, Finland joined the council in 1955.
On 2 July 1954, the Nordic labour market was created and in 1958, building upon a 1952 passport-free travel area, the Nordic Passport Union was created. These two measures helped ensure Nordic citizens' free movement around Scandinavia. A Nordic Convention on Social Security was implemented in 1955. There were also plans for a single market but they were abandoned in 1959 shortly before Denmark, Norway and Sweden joined the European Free Trade Area (EFTA). Finland became an associated member of EFTA in 1961 and Denmark and Norway applied to join the European Economic Community (EEC).
This move towards the EEC led to desire for a formal Nordic treaty; the Helsinki Treaty outlined the workings of the Council and came into force on 24 March 1962. Further advancements on Nordic cooperation were made in the following years: a Nordic School of Public Health, a Nordic Cultural Fund and Nordic House in Reykjavík. Danish Prime Minister Hilmar Baunsgaard proposed full economic cooperation ("Nordek") in 1968. Nordek was agreed in 1970, but Finland then backtracked, stating that its ties with the Soviet Union meant it could not form close economic ties with potential members of the EEC (Denmark and Norway). Nordek was then abandoned.
As a consequence, Denmark and Norway applied to join the EEC and the Nordic Council of Ministers was set up in 1971 to ensure continued Nordic cooperation. In 1970 representatives of the Faroe Islands and Åland were allowed to take part in the Nordic Council as part of the Danish and Finnish delegations. Norway turned down EEC membership in 1972 while Denmark acted as a bridge builder between the EEC and the Nordics. Also in 1973, although Finland did not opt for full membership of the EEC, Finland negotiated a free trade treaty with the EEC that in practice removed customs duties from 1977 on, although there were transition periods up to 1985 for some products. Sweden did not apply due to its non-alliance policy, which was aimed at preserving neutrality. Greenland subsequently left the EEC and has since sought a more active role in circumpolar affairs.
In the 1970s, the Nordic Council founded the Nordic Industrial Fund, Nordtest and the Nordic Investment Bank. The Council's remit was also expanded to include environmental protection and, in order to clean up the pollution in the Baltic Sea and North Atlantic, a joint energy network was established. The Nordic Science Policy Council was set up in 1983 and, in 1984, representatives from Greenland were allowed to join the Danish delegation.
Following the collapse of the Soviet Union in 1991, the Nordic Council began to cooperate more with the Baltic states and new Baltic Sea organisations. Sweden and Finland joined the European Union (EU), the EEC's successor, in 1995. Norway had also applied, but once again voted against membership. However, Norway and Iceland did join the European Economic Area (EEA) which integrated them economically with the EU. The Nordic Passport Union was also subsumed into the EU's Schengen Area in 1996.
The Nordic Council became more outward-looking, to the Arctic, Baltic, Europe and Canada. The Øresund Bridge linking Sweden and Denmark led to a large amount of cross-border travel, which in turn led to further efforts to reduce barriers. However, the initially envisioned tasks and functions of the Nordic Council have become partially dormant due to the significant overlap with the EU and EEA. Since 2008, Iceland has also sought EU membership.
Structure.
Council.
The Nordic Council consists of 87 representatives, elected from its members' parliaments and reflecting the relative representation of the political parties in those parliaments. It holds its main session in the autumn, while a so-called "theme session" is arranged in the spring. Each of the national delegations has its own secretariat in the national parliament. The autonomous territoriesGreenland, the Faroe Islands and Ålandalso have Nordic secretariats, and the Baltic states of Estonia, Latvia and Lithuania sit as observer states.
The Nordic Council uses the three Continental-Scandinavian languages (Danish, Norwegian and Swedish) as its official working languages, but also publishes material in Finnish, Icelandic and English. Since 1987, under the Nordic Language Convention, citizens of the Nordic countries have the opportunity to use their native language when interacting with official bodies in other Nordic countries without being liable to any interpretation or translation costs. The Convention covers visits to hospitals, job centres, the police and social security offices. The languages included are Swedish, Danish, Norwegian, Finnish and Icelandic.
The Council does not have any formal power on its own, but each government has to implement any decisions through its country's legislative assembly (parliament). With Denmark, Norway and Iceland being members of NATO and Finland and Sweden being neutral, the Nordic Council has not been involved in any military cooperation.
Council of Ministers.
The original Nordic Council concentrates on inter-parliamentary cooperation. The "Nordic Council of Ministers", founded in 1971, is responsible for inter-governmental cooperation. Prime Ministers have ultimate responsibility but this is usually delegated to the Minister for Nordic Cooperation and the Nordic Committee for Co-operation, which co-ordinates the day-to-day work. The autonomous territories have the same representation as states.
Location.
The Nordic Council and the Council of Ministers have their headquarters in Copenhagen and various installations in each separate country, as well as many offices in neighbouring countries. The headquarters are located at Ved Stranden No. 18, close to Slotsholmen.
Members.
Members of the Council:
The Sámi political structures (Sami Parliament) have long desired formal representation in the Nordic Council's structures, and are increasingly de facto included in activities touching upon their interests. In addition, the Faroe Islands have expressed their wishes for full membership in the Nordic Council instead of the current associate membership.
Future.
Some desire the Nordic Council's promotion of Nordic cooperation to go much further than at present. If the states of Iceland, Sweden, Norway, Denmark and Finland were to merge in such an integration as some desire, it would command a gross domestic product of US$1.60 trillion, making it the twelfth largest economy in the world, larger than that of Australia, Spain, Mexico or South Korea. Gunnar Wetterberg, a Swedish historian and economist, wrote a book entered into the Nordic Council's year book that proposes the creation of a Nordic Federation from the Council in a few decades.
Expansion.
By 1999 Estonia's Minister of Foreign Affairs Toomas Hendrik Ilves was encouraging the Baltic states to become full members. In his famous "Estonia as a Nordic Country" speech, he outlined Estonia's "special case" for membership. Ilves became President of Estonia in 2006.

</doc>
<doc id="42781" url="https://en.wikipedia.org/wiki?curid=42781" title="388">
388

__NOTOC__
Year 388 (CCCLXXXVIII) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus without colleague (or, less frequently, year 1141 "Ab urbe condita"). The denomination 388 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42782" url="https://en.wikipedia.org/wiki?curid=42782" title="387">
387

__NOTOC__
Year 387 (CCCLXXXVII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus and Eutropius (or, less frequently, year 1140 "Ab urbe condita"). The denomination 387 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42783" url="https://en.wikipedia.org/wiki?curid=42783" title="386">
386

__NOTOC__
Year 386 (CCCLXXXVI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Honorius and Euodius (or, less frequently, year 1139 "Ab urbe condita"). The denomination 386 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42784" url="https://en.wikipedia.org/wiki?curid=42784" title="384">
384

__NOTOC__
Year 384 (CCCLXXXIV) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Ricomer and Clearchus (or, less frequently, year 1137 "Ab urbe condita"). The denomination 384 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42785" url="https://en.wikipedia.org/wiki?curid=42785" title="383">
383

__NOTOC__
Year 383 (CCCLXXXIII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Merobaudes and Saturninus (or, less frequently, year 1136 "Ab urbe condita"). The denomination 383 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42786" url="https://en.wikipedia.org/wiki?curid=42786" title="382">
382

__NOTOC__
Year 382 (CCCLXXXII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Antonius and Syagrius (or, less frequently, year 1135 "Ab urbe condita"). The denomination 382 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
