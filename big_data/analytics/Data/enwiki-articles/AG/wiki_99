<doc id="47627" url="https://en.wikipedia.org/wiki?curid=47627" title="Llywelyn ap Gruffudd">
Llywelyn ap Gruffudd

Llywelyn ap Gruffudd (c. 1223 – 11 December 1282), sometimes written as Llywelyn ap Gruffydd, also known as Llywelyn the Last, or, in Welsh, Llywelyn Ein Llyw Olaf (), was King of Wales from 1258 until his death at Cilmeri in 1282. The son of Gruffudd ap Llywelyn Fawr and grandson of Llywelyn the Great, he was the last sovereign prince and king of Wales before its conquest by Edward I of England.
Genealogy and early life.
Llywelyn was the last of the four sons of Gruffudd, the eldest son of Llywelyn the Great, and Senana ferch Caradog, the daughter of Caradoc ap Thomas ap Rhodri, Lord of Anglesey. The eldest was Owain Goch ap Gruffudd and Llywelyn had two younger brothers, Dafydd ap Gruffudd and Rhodri ap Gruffudd. Llywelyn is thought to have been born around 1222 or 1223. He is first heard of holding lands in the Vale of Clwyd around 1244.
Following his grandfather's death in 1240, Llywelyn's uncle, Dafydd ap Llywelyn (who was Llywelyn the Great's eldest legitimate son), succeeded him as ruler of Gwynedd. Llywelyn's father, Gruffudd (who was Llywelyn's eldest son but illegitimate), and his brother, Owain, were initially kept prisoner by Dafydd, then transferred into the custody of King Henry III of England. Gruffudd died in 1244, from a fall while trying to escape from his cell at the top of the Tower of London. The window from which he attempted to escape the Tower was bricked up and can still be seen to this day.
This freed Dafydd ap Llywelyn's hand as King Henry could no longer use Gruffudd against him, and war broke out between him and King Henry in 1245. Llywelyn supported his uncle in the savage fighting that followed. Owain, meanwhile, was freed by Henry after his father's death in the hope that he would start a civil war in Gwynedd, but stayed in Chester, so when Dafydd died in February 1246 without leaving an heir, Llywelyn had the advantage of being on the spot.
Early reign.
Llywelyn and Owain came to terms with King Henry and in 1247, signed the Treaty of Woodstock at Woodstock Palace. The terms they were forced to accept restricted them to Gwynedd Uwch Conwy, the part of Gwynedd west of the River Conwy, which was divided between them. Gwynedd Is Conwy, east of the river, was taken over by King Henry.
When Dafydd ap Gruffudd came of age, King Henry accepted his homage and announced his intention to give him part of the already reduced Gwynedd. Llywelyn refused to accept this, and Owain and Dafydd formed an alliance against him. This led to the Battle of Bryn Derwin in June 1255. Llywelyn defeated Owain and Dafydd and captured them, thereby becoming sole ruler of Gwynedd Uwch Conwy. Llywelyn now looked to expand his area of control. The population of Gwynedd Is Conwy resented English rule. This area, also known as "Perfeddwlad"(meaning 'middle land') had been given by King Henry to his son Edward and during the summer of 1256, he visited the area, but failed to deal with grievances against the rule of his officers. An appeal was made to Llywelyn, who, that November, crossed the River Conwy with an army, accompanied by his brother, Dafydd, whom he had released from prison. By early December, Llywelyn controlled all of Gwynedd Is Conwy apart from the royal castles at Dyserth and Dnoredudd as a reward for his support and dispossessing his brother-in-law, Rhys Fychan, who supported the king. An English army led by Stephen Bauzan invaded to try to restore Rhys Fychan but was decisively defeated by Welsh forces at the Battle of Cadfan in June 1257, with Rhys having previously slipped away to make his peace with Llywelyn.
Rhys Fychan now accepted Llywelyn as overlord, but this caused problems for Llywelyn, as Rhys's lands had already been given to Maredudd. Llywelyn restored his lands to Rhys, but the king's envoys approached Maredudd and offered him Rhys's lands if he would change sides. Maredudd paid homage to Henry in late 1257. By early 1258, Llywelyn was using the title Prince of Wales, first used in an agreement between Llywelyn and his supporters and the Scottish nobility associated with the Comyn family. The English Crown refused to recognise this title however, and in 1263, Llywelyn's brother, Dafydd, went over to King Henry.
On 12 December 1263 in the commote of Ystumanner, Gruffydd ap Gwenwynwyn did homage and swore fealty to Llywelyn. In return he was made a vassal lord and the lands taken from him by Llywelyn about six years earlier were restored to him.
In England, Simon de Montfort (the Younger) defeated the king's supporters at the Battle of Lewes in 1264, capturing the king and Prince Edward. Llywelyn began negotiations with de Montfort, and in 1265, offered him 30,000 marks in exchange for a permanent peace, in which Llywelyn's right to rule Wales would be acknowledged. The Treaty of Pipton, 22 June 1265, established an alliance between Llywelyn and de Montfort, but the very favourable terms given to Llywelyn in this treaty were an indication of de Montfort's weakening position. De Montfort was to die at the Battle of Evesham in 1265, a battle in which Llywelyn took no part.
Supremacy in Wales.
After Simon de Montfort's death, Llywelyn launched a campaign in order to rapidly gain a bargaining position before King Henry had fully recovered. In 1265, Llywelyn captured Hawarden Castle and routed the combined armies of Hamo Lestrange and Maurice fitz Gerald in north Wales. Llywelyn then moved on to Brycheiniog, and in 1266, he routed Roger Mortimer's army. With these victories and the backing of the papal legate, Ottobuono, Llywelyn opened negotiations with the king, and was eventually recognised as Prince of Wales by King Henry in the Treaty of Montgomery in 1267. In return for the title, the retention of the lands he had conquered and the homage of almost all the native rulers of Wales, he was to pay a tribute of 25,000 marks in yearly instalments of 3,000 marks, and could if he wished, purchase the homage of the one outstanding native prince - Maredudd ap Rhys of Deheubarth - for another 5,000 marks. However, Llywelyn's territorial ambitions gradually made him unpopular with some minor Welsh leaders, particularly the princes of south Wales.
The Treaty of Montgomery marked the high point of Llywelyn's power. Problems began arising soon afterwards, initially a dispute with Gilbert de Clare concerning the allegiance of a Welsh nobleman holding lands in Glamorgan. Gilbert built Caerphilly Castle in response to this. King Henry sent a bishop to take possession of the castle while the dispute was resolved but when Gilbert regained the castle by trickery, the king was unable to do anything about it.
Following the death of King Henry in late 1272, with the new King Edward I of England away from the kingdom, the rule fell to three men. One of them, Roger Mortimer was one of Llywelyn's rivals in the marches. When Humphrey de Bohun tried to take back Brycheiniog, which was granted to Llywelyn by the Treaty of Montgomery, Mortimer supported de Bohun. Llywelyn was also finding it difficult to raise the annual sums required under the terms of this treaty, and ceased making payments.
In early 1274, there was a plot by Llywelyn's brother, Dafydd, and Gruffudd ap Gwenwynwyn of Powys Wenwynwyn and his son, Owain, to kill Llywelyn. Dafydd was with Llywelyn at the time, and it was arranged that Owain would come with armed men on 2 February to carry out the assassination; however, he was prevented by a snowstorm. Llywelyn did not discover the full details of the plot until Owain confessed to the Bishop of Bangor. He said that the intention had been to make Dafydd prince of Gwynedd, and that Dafydd would reward Gruffudd with lands. Dafydd and Gruffudd fled to England where they were maintained by the king and carried out raids on Llywelyn's lands, increasing Llywelyn's resentment. When Edward called Llywelyn to Chester in 1275 to pay homage, Llywelyn refused to attend.
Llywelyn also made an enemy of King Edward by continuing to ally himself with the family of Simon de Montfort, even though their power was now greatly reduced. Llywelyn sought to marry Eleanor de Montfort, born in 1252, Simon de Montfort's daughter. They were married by proxy in 1275, but King Edward took exception to the marriage, in part because Eleanor was his first cousin: her mother was Eleanor of England, daughter of King John and princess of the House of Plantagenet. When Eleanor sailed from France to meet Llywelyn, Edward hired pirates to seize her ship and she was imprisoned at Windsor Castle until Llywelyn made certain concessions.
In 1276, Edward declared Llywelyn a rebel and in 1277, gathered an enormous army to march against him. Edward's intention was to disinherit Llywelyn completely and take over Gwynedd Is Conwy himself. He was considering two options for Gwynedd Uwch Conwy: either to divide it between Llywelyn's brothers, Dafydd and Owain, or to annex Anglesey and divide only the mainland between the two brothers. Edward was supported by Dafydd ap Gruffudd and Gruffudd ap Gwenwynwyn. Many of the lesser Welsh princes who had supported Llywelyn now hastened to make peace with Edward. By the summer of 1277, Edward's forces had reached the River Conwy and encamped at Deganwy, while another force had captured Anglesey and took possession of the harvest there. This deprived Llywelyn and his men of food, forcing them to seek terms.
Treaty of Aberconwy.
What resulted was the Treaty of Aberconwy, which guaranteed peace in Gwynedd in return for several difficult concessions from Llywelyn, including confining his authority to Gwynedd Uwch Conwy once again. Part of Gwynedd Is Conwy was given to Dafydd ap Gruffudd, with a promise that if Llywelyn died without an heir, he would be given a share of Gwynedd Uwch Conwy instead.
Llywelyn was forced to acknowledge the English king as his sovereign; initially he had refused, but after the events of 1276, Llywelyn was stripped of all but a small portion of his lands. He went to meet Edward, and found Eleanor lodged with the royal family at Worcester; after Llywelyn agreed to Edward's demands, Edward gave them permission to be married at Worcester Cathedral. A stained glass window exists to this day depicting the wedding of the Prince of Wales and Lady Eleanor. By all accounts, the marriage was a genuine love match; Llywelyn is not known to have fathered any illegitimate children, which is extremely unusual for the Welsh royalty. (In medieval Wales, illegitimate children were as entitled to their father's property as legitimate children.)
Last campaign and death.
By early 1282, many of the lesser princes who had supported Edward against Llywelyn in 1277 were becoming disillusioned with the exactions of the royal officers. On Palm Sunday that year, Dafydd ap Gruffudd attacked the English at Hawarden Castle and then laid siege to Rhuddlan. The revolt quickly spread to other parts of Wales, with Aberystwyth castle captured and burnt and rebellion in Ystrad Tywi in south Wales, also inspired by Dafydd according to the annals, where Carreg Cennen castle was captured.
Llywelyn, according to a letter he sent to the Archbishop of Canterbury John Peckham, was not involved in the planning of the revolt. He felt obliged, however, to support his brother and a war began for which the Welsh were ill-prepared. Personal tragedy also struck him at this time when, on or about 19 June 1282, his wife Eleanor de Montfort, died shortly after giving birth to their daughter Gwenllian.
Events followed a similar pattern to 1277, with Edward's forces capturing Gwynedd Is Conwy, Anglesey and taking the harvest. The force occupying Anglesey were defeated, however, when trying to cross to the mainland in the Battle of Moel-y-don. The Archbishop of Canterbury tried mediating between Llywelyn and Edward, and Llywelyn was offered a large estate in England if he would surrender Wales to Edward, while Dafydd was to go on crusade and not return without the king's permission. In an emotional reply, which has been compared to the Declaration of Arbroath, Llywelyn said he would not abandon the people whom his ancestors had protected since "the days of Kamber son of Brutus". The offer was refused.
Llywelyn now left Dafydd to lead the defence of Gwynedd and took a force south, trying to rally support in mid and south Wales and open up an important second front. On 11 December at the Battle of Orewin Bridge at Builth Wells, he was killed while separated from his army. The exact circumstances are unclear and there are two conflicting accounts of his death. Both accounts agree that Llywelyn was tricked into leaving the bulk of his army and was then attacked and killed. The first account says that Llywelyn and his chief minister approached the forces of Edmund Mortimer and Hugh Le Strange after crossing a bridge. They then heard the sound of battle as the main body of his army was met in battle by the forces of Roger Despenser and Gruffudd ap Gwenwynwyn. Llywelyn turned to rejoin his forces and was pursued by a lone lancer who struck him down. It was not until some time later that an English knight recognised the body as that of the prince. This version of events was written in the north of England some fifty years later and has suspicious similarities with details about the Battle of Stirling Bridge in Scotland. An alternative version of events written in the east of England by monks in contact with Llywelyn's exiled daughter, Gwenllian ferch Llywelyn, and niece, Gwladys ferch Dafydd, states that Llywelyn, at the front of his army, approached the combined forces of Edmund and Roger Mortimer, Hugo Le Strange and Gruffudd ap Gwenwynwyn on the promise that he would receive their homage. This was a deception. His army was immediately engaged in fierce battle during which a significant section of it was routed, causing Llywelyn and his eighteen retainers to become separated. At around dusk, Llywelyn and a small group of his retainers (which included clergy), were ambushed and chased into a wood at Aberedw. Llywelyn was surrounded and struck down. As he lay dying, he asked for a priest and gave away his identity. He was then killed and his head hewn from his body. His person was searched and various items recovered, including a list of "conspirators", (which may well have been faked), and his privy seal.
"If the king wishes to have the copy "the list" found in the breeches of Llywelyn, he can have it from Edmund Mortimer, who has custody of it and also of Llywelyn’s privy seal and certain other things found in the same place." Archbishop Peckham, in his first letter to Robert Bishop of Bath and Wells, dated 17 December 1282 (Lambeth Palace Archives)
There are legends surrounding the fate of Llywelyn's severed head. It is known that it was sent to Edward at Rhuddlan and after being shown to the English troops based in Anglesey, Edward sent the head on to London. In London, it was set up in the city pillory for a day, and crowned with ivy (i.e. to show he was a "king" of Outlaws and in mockery of the ancient Welsh prophecy, which said that a Welshman would be crowned in London as king of the whole of Britain). Then it was carried by a horseman on the point of his lance to the Tower of London and set up over the gate. It was still on the Tower of London 15 years later.
The last resting place of Llywelyn's body is not known for certain, however it has always been tradition that it was interred at the Cistercian Abbey at Abbeycwmhir. On 28 December 1282 Archbishop Peckham wrote a letter to the Archdeacon of Brecon at Brecon Priory, in order to;
"...inquire and clarify if the body of Llywelyn has been buried in the church of Cwmhir, and he was bound to clarify the latter before the feast of Epiphany, because he had another mandate on this matter, and ought to have certified the lord Archbishop before Christmas, and has not done so."
There is further supporting evidence for this hypothesis in the Chronicle of Florence of Worcester;
"As for the body of the Prince, his mangled trunk, it was interred in the Abbey of Cwm Hir, belonging to the Cistercian Order."
Another theory is that his body was transferred to Llanrumney Hall in Cardiff.
The poet Gruffudd ab yr Ynad Coch wrote in an elegy on Llywelyn:
There is an enigmatic reference in the Welsh annals Brut y Tywysogion, "…and then Llywelyn was betrayed in the belfry at Bangor by his own men". No further explanation is given.
Annexation.
With the loss of Llywelyn, Welsh morale and the will to resist diminished. Dafydd was Llywelyn's named successor. He carried on the struggle for several months, but in June 1283 was captured in the uplands above Abergwyngregyn at Bera Mountain together with his family. He was brought before Edward, then taken to Shrewsbury where a special session of Parliament condemned him to death. He was dragged through the streets, hanged, drawn and quartered.
After the final defeat of 1283, Gwynedd was stripped of all royal insignia, relics, and regalia. Edward took particular delight in appropriating the royal home of the Gwynedd dynasty. In August 1284, he set up his court at Abergwyngregyn, Gwynedd. With equal deliberateness, he removed all the insignia of majesty from Gwynedd; a coronet was solemnly presented to the shrine of St. Edward at Westminster; the matrices of the seals of Llywelyn, of his wife, and his brother Dafydd were melted down to make a chalice which was given by the king to Vale Royal Abbey where it remained until the dissolution of that institution in 1538 (after which it came into the possession of the family of the final abbot) The most precious religious relic in Gwynedd, the fragment of the True Cross known as Cross of Neith, was paraded through London in May 1285 in a solemn procession on foot led by the king, the queen, the archbishop of Canterbury and fourteen bishops, and the magnates of the realm. Edward was thereby appropriating the historical and religious regalia of the house of Gwynedd and placarding to the world the extinction of its dynasty and the annexation of the principality to his Crown. Commenting on this a contemporary chronicler is said to have declared "and then all Wales was cast to the ground."
Most of Llywelyn's relatives ended their lives in captivity — with the notable exceptions of his younger brother Rhodri, who had long since sold his claim to the crown and endeavoured to keep a very low profile, and a distant cousin, Madog ap Llywelyn, who led a future revolt and claimed the title Prince of Wales in 1294. Llywelyn and Eleanor's baby daughter Gwenllian of Wales was captured by Edward's troops in 1283. She was interned at Sempringham Priory in England for the rest of her life, becoming a nun in 1317 and dying without issue in 1337, probably knowing little of her heritage and speaking none of her language.
Dafydd's two surviving sons were captured and incarcerated at Bristol Gaol, where they eventually died many years later. Llywelyn's elder brother Owain Goch disappears from the record in 1282 and the presumption is that he was murdered. Llywelyn's surviving brother Rhodri (who had been exiled from Wales since 1272) survived and held manors in Gloucestershire, Cheshire, Surrey, and Powys and died around 1315. His grandson, Owain Lawgoch, later claimed the title Prince of Wales.
Historical fiction.
The life of Llywelyn the Last is the subject of Edith Pargeter's "Brothers of Gwynedd Quartet": 'Sunrise in the West' (1974); 'The Dragon at Noonday' (1975); 'The Hounds of Sunset' (1976); and 'Afterglow and Nightfall' (1977).
The stories of Llywelyn Fawr, Llywelyn ap Gryffydd and Dafydd ap Gryffydd are depicted in Sharon Penman's "Welsh Trilogy": 'Here be Dragons' (1985); 'Falls the Shadow' (1988); and 'The Reckoning' (1991). Also, "A Memory of Love" by Bertrice Small.
An alternate history/time travel series, "After Cilmeri" by Sarah Woodbury, explores what might have happened if Llywelyn had survived the ambush at Cilmeri, and had a son and assistance from people from the future.
Llywelyn the Last is the subject of the New Riders of the Purple Sage song "Llewellyn". The song focuses on the Conquest of Wales by Edward I, but specifically on the Campaign of 1282-83. In the song, the band claims "In September, Edward [Edward I] moved up to the baird/ His forces stronger every day/ Llewellyn then turned southward bound/ His forces lay upon the ground." It also claims that the message of Llywelyn's death came "soon thereafter."
Llywelyn is a minor character in Jean Plaidy's novel "Edward Longshanks," the seventh novel of the Plantagenet Saga series.
Bertrice Small includes Llywelyn's life in her book, "A Memory of Love", which tells the story of his bastard daughter, Rhonwyn.

</doc>
<doc id="47628" url="https://en.wikipedia.org/wiki?curid=47628" title="Bomb">
Bomb

A bomb is an explosive weapon that uses the exothermic reaction of an explosive material to provide an extremely sudden and violent release of energy. Detonations inflict damage principally through ground- and atmosphere-transmitted mechanical stress, the impact and penetration of pressure-driven projectiles, pressure damage, and explosion-generated effects. Bombs have been in use since the 11th century in Song Dynasty China.
The term bomb is not usually applied to explosive devices used for civilian purposes such as construction or mining, although the people using the devices may sometimes refer to them as a "bomb". The military use of the term "bomb", or more specifically aerial bomb action, typically refers to airdropped, unpowered explosive weapons most commonly used by air forces and naval aviation. Other military explosive weapons not classified as "bombs" include grenades, shells, depth charges (used in water), warheads when in missiles, or land mines. In unconventional warfare, "bomb" can refer to a range of offensive weaponry. For instance, in recent conflicts, "bombs" known as improvised explosive devices (IEDS) have been employed by insurgent fighters to great effectiveness.
The word comes from the Latin "bombus", which in turn comes from the Greek βόμβος ("bombos"), an onomatopoetic term meaning "booming", "buzzing".
History.
Explosive bombs were used in China in 1221, by a Jin dynasty army against a Song Dynasty city. Bombs built using bamboo tubes appear in the 11th century. Bombs made of cast iron shells packed with explosive gunpowder date to 13th century China. The term was coined for this bomb (i.e. "thunder-crash bomb") during a Jin dynasty (1115–1234) naval battle of 1231 against the Mongols. The "History of Jin" 《金史》 (compiled by 1345) states that in 1232, as the Mongol general Subutai (1176–1248) descended on the Jin stronghold of Kaifeng, the defenders had a "thunder-crash bomb" which "consisted of gunpowder put into an iron container ... then when the fuse was lit (and the projectile shot off) there was a great explosion the noise whereof was like thunder, audible for more than a hundred "li", and the vegetation was scorched and blasted by the heat over an area of more than half a "mou". When hit, even iron armour was quite pierced through." The Song Dynasty (960–1279) official Li Zengbo wrote in 1257 that arsenals should have several hundred thousand iron bomb shells available and that when he was in Jingzhou, about one to two thousand were produced each month for dispatch of ten to twenty thousand at a time to Xiangyang and Yingzhou. The Ming Dynasty text "Huolongjing" describes the use of poisonous gunpowder bombs, including the "wind-and-dust" bomb.
During the Mongol invasions of Japan, the Mongols used the explosive "thunder-crash bombs" against the Japanese. Archaeological evidence of the "thunder-crash bombs" has been discovered in an underwater shipwreck off the shore of Japan by the Kyushu Okinawa Society for Underwater Archaeology. X-rays by Japanese scientists of the excavated shells confirmed that they contained gunpowder.
Shock.
Explosive shock waves can cause situations such as body displacement (i.e., people being thrown through the air), dismemberment, internal bleeding and ruptured eardrums.
Shock waves produced by explosive events have two distinct components, the positive and negative wave. The positive wave shoves outward from the point of detonation, followed by the trailing vacuum space "sucking back" towards the point of origin as the shock bubble collapses.
The greatest defense against shock injuries is distance from the source of shock. As a point of reference, the overpressure at the Oklahoma City bombing was estimated in the range of 
Heat.
A thermal wave is created by the sudden release of heat caused by an explosion. Military bomb tests have documented temperatures of up to 2,480 °C (4,500 °F). While capable of inflicting severe to catastrophic burns and causing secondary fires, thermal wave effects are considered very limited in range compared to shock and fragmentation. This rule has been challenged, however, by military development of thermobaric weapons, which employ a combination of negative shock wave effects and extreme temperature to incinerate objects within the blast radius. This would be fatal to humans, as bomb tests have proven.
Fragmentation.
Fragmentation is produced by the acceleration of shattered pieces of bomb casing and adjacent physical objects. The use of fragmentation in bombs dates to the 14th century, and appears in the Ming Dynasty text "Huolongjing". The fragmentation bombs were filled with iron pellets and pieces of broken porcelain. Once the bomb explodes, the resulting shrapnel is capable of piercing the skin and blinding enemy soldiers.
While conventionally viewed as small metal shards moving at super-supersonic and hypersonic speeds, fragmentation can occur in epic proportions and travel for extensive distances. When the S.S. Grandcamp exploded in the Texas City Disaster on April 16, 1947, one fragment of that blast was a two-ton anchor which was hurled nearly two miles inland to embed itself in the parking lot of the Pan American refinery. Fragmentation should not be confused with shrapnel, which relies on the momentum of a shell to cause damage.
Effects on living things.
To people who are close to a blast incident, such as bomb disposal technicians, soldiers wearing body armor, deminers or individuals wearing little to no protection, there are four types of blast effects on the human body: overpressure (shock), fragmentation, impact and heat. Overpressure refers to the sudden and drastic rise in ambient pressure that can damage the internal organs, possibly leading to permanent damage or death. Fragmentation includes the shrapnel described above but can also include sand, debris and vegetation from the area surrounding the blast source. This is very common in anti-personnel mine blasts. The projection of materials poses a potentially lethal threat caused by cuts in soft tissues, as well as infections, and injuries to the internal organs. When the overpressure wave impacts the body it can induce violent levels of blast-induced acceleration. Resulting injuries may range from minor to unsurvivable. Immediately following this initial acceleration, deceleration injuries can occur when a person impacts directly against a rigid surface or obstacle after being set in motion by the force of the blast. Finally, injury and fatality can result from the explosive fireball as well as incendiary agents projected onto the body. Personal protective equipment, such as a bomb suit or demining ensemble, as well as helmets, visors and foot protection, can dramatically reduce the four effects, depending upon the charge, proximity and other variables.
Types.
Experts commonly distinguish between civilian and military bombs. The latter are almost always mass-produced weapons, developed and constructed to a standard design out of standard components and intended to be deployed in a standard explosive device. IEDs are divided into three basic categories by basic size and delivery. Type 76, IEDs are hand-carried parcel or suitcase bombs, type 80, are "suicide vests" worn by a bomber, and type 3 devices are vehicles laden with explosives to act as large-scale stationary or self-propelled bombs, also known as VBIED (vehicle-borne IEDs).
Improvised explosive materials are typically very unstable and subject to spontaneous, unintentional detonation triggered by a wide range of environmental effects ranging from impact and friction to electrostatic shock. Even subtle motion, change in temperature, or the nearby use of cellphones or radios, can trigger an unstable or remote-controlled device. Any interaction with explosive materials or devices by unqualified personnel should be considered a grave and immediate risk of death or dire injury. The safest response to finding an object believed to be an explosive device is to get as far away from it as possible.
Atomic bombs are based on the theory of nuclear fission, that when a large atom splits it releases a massive amount of energy. Hydrogen bombs use the energy from an initial fission explosion to create an even more powerful fusion explosion.
The term dirty bomb refers to a specialized device that relies on a comparatively low explosive yield to scatter harmful material over a wide area. Most commonly associated with radiological or chemical materials, dirty bombs seek to kill or injure and then to deny access to a contaminated area until a thorough clean-up can be accomplished. In the case of urban settings, this clean-up may take extensive time, rendering the contaminated zone virtually uninhabitable in the interim.
The power of large bombs is typically measured in kilotons (kt) or megatons of TNT (Mt). The most powerful bombs ever used in combat were the two atomic bombs dropped by the United States to attack Hiroshima and Nagasaki, and the most powerful ever tested was the Tsar Bomba. The most powerful non-nuclear bomb is Russian "Father of All Bombs" (officially Aviation Thermobaric Bomb of Increased Power (ATBIP)) followed by the United States Air Force's MOAB (officially Massive Ordnance Air Blast, or more commonly known as the "Mother of All Bombs").
Below is a list of five different types of bombs based on the fundamental explosive mechanism they employ.
Compressed gas.
Relatively small explosions can be produced by pressurizing a container until catastrophic failure such as with a dry ice bomb. Technically, devices that create explosions of this type can not be classified as "bombs" by the definition presented at the top of this article. However, the explosions created by these devices can cause property damage, injury, or death. Flammable liquids, gasses and gas mixtures dispersed in these explosions may also ignite if exposed to a spark or flame.
Low explosive.
The simplest and oldest type of bombs store energy in the form of a low explosive. Black powder is an example of a low explosive. Low explosives typically consist of a composition of an oxidizing salt, such as potassium nitrate, and solid fuel, such as charcoal or aluminum powder. These compositions deflagrate upon ignition producing hot gas. Under normal circumstances deflagration occurs too slowly to produce a significant pressure wave. Low explosives must, therefore, be used in large quantities or confined in a container with a high burst pressure to be used as a bomb.
High explosive.
A high explosive bomb is one that employs a process called "detonation" to rapidly go from an initially high energy molecule to a very low energy molecule. Detonation is distinct from deflagration in that the chemical reaction propagates faster than the speed of sound (often many times faster) in an intense shock wave. Therefore, the pressure wave produced by a high explosive is not significantly increased by confinement as detonation occurs so quickly that the resulting plasma does not expand much before all the explosive material has reacted. This has led to the development of plastic explosive. A casing is still employed in some high explosive bombs, but with the purpose of fragmentation. Most high explosive bombs consist of an insensitive secondary explosive that must be detonated with a blasting cap containing a more sensitive primary explosive.
Nuclear fission.
Nuclear fission type atomic bombs utilize the energy present in very heavy atomic nuclei, such as U-235 or Pu-239. In order to release this energy rapidly, a certain amount of the fissile material must be very rapidly consolidated while being exposed to a neutron source. If consolidation occurs slowly, repulsive forces drive the material apart before a significant explosion can occur. Under the right circumstances, rapid consolidation can provoke a chain reaction that can proliferate and intensify by many orders of magnitude within microseconds. The energy released by a nuclear fission bomb may be tens of thousands of times greater than a chemical bomb of the same mass.
Nuclear fusion.
Nuclear fusion type atomic bombs release energy through the fusion of the light atomic nuclei of deuterium and tritium. With this type of bomb, a thermonuclear detonation is triggered by the detonation of a fission type nuclear bomb contained within a material containing high concentrations of deuterium and tritium. Weapon yield is typically increased with a tamper that increases the duration and intensity of the reaction through inertial confinement and neutron reflection. Nuclear fusion bombs can have arbitrarily high yields making them hundreds or thousands of times more powerful than nuclear fission.
Other.
Concrete bomb.
A concrete bomb is an aerial bomb which contains dense, inert material (typically concrete) instead of explosive. The target is destroyed using the kinetic energy of the falling bomb.
Inert bomb.
An inert munition is one whose inner energetic material has been removed or otherwise rendered harmless. Inert munitions are used in military and naval training, and they are also collected and displayed by public museums, or by private parties.
Typically, NATO inert munitions are painted entirely in light blue and/or have the word "INERT" stenciled on them in prominent locations.
Delivery.
The first air-dropped bombs were used by the Austrians in the 1849 siege of Venice. Two hundred unmanned balloons carried small bombs although few bombs actually hit the city.
The first bombing from a fixed-wing aircraft took place in 1911 when the Italians dropped bombs by hand on the Turkish lines in what is now Libya, during the Italo-Turkish War. The first large scale dropping of bombs took place during World War I starting in 1915 with the German Zeppelin airship raids on London, England, and the same war saw the invention of the first heavy bombers. One Zeppelin raid on 8 September 1915 dropped of high explosives and incendiary bombs, including one bomb that weighed .
During World War II bombing became a major military feature, and a number of novel delivery methods were introduced. These included Barnes Wallis's bouncing bomb, designed to bounce across water, avoiding torpedo nets and other underwater defenses, until it reached a dam, ship or other destination where it would sink and explode. By the end of the war, planes such as the allied forces' Avro Lancaster were delivering with 50 yard accuracy from 20000 feet, ten ton earthquake bombs (also invented by Barnes Wallis) named "Grand Slam", which unusually for the time were delivered from high altitude in order to gain high speed, and would upon impact penetrate and explode deep underground ("camouflet"), causing massive caverns or craters, and affecting targets too large or difficult to be affected by other types of bomb.
Modern military bomber aircraft are designed around a large-capacity internal bomb bay while fighter bombers usually carry bombs externally on pylons or bomb racks, or on multiple ejection racks which enable mounting several bombs on a single pylon. Some bombs are equipped with a parachute, such as the World War II "parafrag", which was an 11 kg fragmentation bomb, the Vietnam war-era daisy cutters, and the bomblets of some modern cluster bombs. Parachutes slow the bomb's descent, giving the dropping aircraft time to get to a safe distance from the explosion. This is especially important with airburst nuclear weapons, and in situations where the aircraft releases a bomb at low altitude. A number of modern bombs are also precision-guided munitions, and may be guided after they leave an aircraft by remote control, or by autonomous guidance.
A hand grenade is delivered by being thrown. Grenades can also be projected by other means, such as being launched from the muzzle of a rifle, as in the rifle grenade or using a grenade launcher such as the M203 or by attaching a rocket to the explosive grenade as in a rocket-propelled grenade (RPG).
A bomb may also be positioned in advance and concealed.
A bomb destroying a rail track just before a train arrives causes a train to derail. Apart from the damage to vehicles and people, a bomb exploding in a transport network often also damages, and is sometimes mainly intended to damage that network. This applies for railways, bridges, runways, and ports, and to a lesser extent, depending on circumstances, to roads.
In the case of suicide bombing the bomb is often carried by the attacker on his or her body, or in a vehicle driven to the target.
The Blue Peacock nuclear mines, which were also termed "bombs", were planned to be positioned during wartime and be constructed such that, if they were disturbed, they would explode within ten seconds.
The explosion of a bomb may be triggered by a detonator or a fuse. Detonators are triggered by clocks, remote controls like cell phones or some kind of sensor, such as pressure (altitude), radar, vibration or contact. Detonators vary in ways they work, they can be electrical, fire fuze or blast initiated detonators and others,
Blast seat.
In forensic science, the point of detonation of a bomb is referred to as its blast seat, seat of explosion, blast hole or epicenter. Depending on the type, quantity and placement of explosives, the blast seat may be either spread out or concentrated (i.e., an explosion crater).
Other types of explosions, such as dust or vapor explosions, do not cause craters or even have definitive blast seats.

</doc>
<doc id="47629" url="https://en.wikipedia.org/wiki?curid=47629" title="Atil">
Atil

Atil (; cf. "A-de Shui"), literally meaning "Big River", was the capital of Khazaria from the middle of the 8th century until the end of the 10th century. The word is also a Turkic name for the Volga River.
History.
Atil was located along the Volga delta at the northwestern corner of the Caspian Sea. Following the defeat of the Khazars in the Second Arab-Khazar War, Atil became the capital of Khazaria. The city is referred to as Khamlij in 9th-century Arab sources, and the name Atil appears in the 10th century. At its height, the city was a major center of trade, and consisted of three parts separated by the Volga. The western part contained the administrative center of the city, with a court house and a large military garrison. The eastern part of the city was built later and acted as the commercial center of the Atil, and had many public baths and shops. Between them was an island on which stood the palaces of the Khazar Khagan and Bek. The island was connected to one of the other parts of the city by a pontoon bridge. According to Arab sources, one half of the city was referred to as Atil, while the other was named Khazaran.
Atil was a multi-ethnic and religiously diverse city, inhabited by Jews, Christians, Muslims, Shamanists, and Pagans, many of them traders from foreign countries. All of the religious groups had their own places of worship in the city, and there were 7 judges appointed to settle disputes (two Christian, two Jewish, and two Muslim judges, with a single judge for all of the Shamanists and other Pagans).
Svyatoslav I of Kiev sacked Atil in 968 or 969 CE. Ibn Hawqal and al-Muqaddasi refer to Atil after 969, indicating that it may have been rebuilt. Al-Biruni (mid-11th century) reported that Atil was again in ruins, and did not mention the later city of Saqsin which was built nearby, so it is possible that this new Atil was only destroyed in the middle of the 11th century.
The archaeological remains of Atil have never been positively identified. It has been hypothesized that they were washed away by the rising level of the Caspian Sea. However, beginning in 2003 Dmitri Vasilyev of Astrakhan State University led a series of excavations at the Samosdelskoye site near the village of Samosdelka (Russian: Самосделка) in the Volga Delta. Vasilyev connected artifacts from the site with Khazar, Oghuz and Bulgar culture, leading him to believe that he had discovered the site of Saqsin. The matter is still unresolved. In 2006 Vasilyev announced his belief that the lowest stratum at the Samosdelka site was identical with the site of Atil. In 2008, this team of Russian archaeologists announced that they had discovered the ruins of Itil.

</doc>
<doc id="47630" url="https://en.wikipedia.org/wiki?curid=47630" title="Paella">
Paella

Paella ( or , ; English approximation: or ) is a Valencian rice dish with ancient roots that originated in its modern form in the mid-19th century near Albufera lagoon on the east coast of Spain adjacent to the city of Valencia. Many non-Spaniards view paella as Spain's national dish, but most Spaniards consider it to be a regional Valencian dish. Valencians, in turn, regard paella as one of their identifying symbols.
Types of paella include Valencian paella (), vegetarian/vegan paella (), seafood paella (), and mixed paella (), but there are many others as well. Valencian paella is believed to be the original recipe and consists of white rice, green beans ("bajoqueta" and "tavella"), meat (chicken and rabbit), white beans ("garrofón"), snails, and seasoning such as saffron and rosemary. Another very common but seasonal ingredient is artichoke. Seafood paella replaces meat with seafood and omits beans and green vegetables. Mixed paella is a free-style combination of meat from land animals, seafood, vegetables, and sometimes beans. Most paella chefs use calasparra or bomba rices. All types of paellas use olive oil.
Etymology.
Paella is a Valencian/Catalan word which derives from the Old French word "paelle" for pan, which in turn comes from the Latin word "patella" for pan as well. "Patella" is also akin to the modern French "poêle", the Italian "padella" and the Old Spanish "padilla".
Valencians use the word "paella" for all pans, including the specialized shallow pan used for cooking paellas. However, in most other parts of Spain and throughout Latin America, the term "paellera" is more commonly used for this pan, though both terms are correct, as stated by the Royal Spanish Academy, the body responsible for regulating the Spanish language in Spain. "Paelleras" are traditionally round, shallow and made of polished steel with two handles.
History.
Possible origins.
The Moors of Moorish Spain began rice cultivation around the 10th century. Consequently, residents of the Valencian region often made casseroles of rice, fish and spices for family gatherings and religious feasts, thus establishing the custom of eating rice in Spain. This led to rice becoming a staple by the 15th century. Afterwards, it became customary for cooks to combine rice with vegetables, beans and dry cod, providing an acceptable meal for Lent. Along Spain's eastern coast, rice was predominantly eaten with fish.
Spanish food historian Lourdes March notes that the dish "symbolizes the union and heritage of two important cultures, the Roman, which gives us the utensil and the Arab which brought us the basic food of humanity for centuries."
Valencian paella.
On special occasions, 18th century Valencians used "calderos" to cook rice in the open air of their orchards near lake Albufera. Water vole meat was one of the main ingredients of early paellas, along with eel and butter beans. Novelist Vicente Blasco Ibáñez described the Valencian custom of eating water voles in "Cañas y Barro" (1902), a realistic novel about life among the fishermen and peasants near lake Albufera.
Living standards rose with the sociological changes of the late 19th century in Spain, giving rise to gatherings and outings in the countryside. This led to a change in paella's ingredients as well, using instead rabbit, chicken, duck and sometimes snails. This dish became so popular that in 1840 a local Spanish newspaper first used the word "paella" to refer to the recipe rather than the pan.
The most widely used, complete ingredient list of this era was as follows: short-grain white rice, chicken, rabbit, snails (optional), duck (optional), butter beans, great northern beans, runner beans, artichoke (a substitute for runner beans in the winter), tomatoes, fresh rosemary, sweet paprika, saffron, garlic (optional), salt, olive oil and water. Poorer Valencians, however, sometimes used nothing more than snails for meat. Valencians insist that only these ingredients should go into making modern Valencian paella.
Seafood and mixed paella.
On the Mediterranean coast, Valencians used seafood instead of meat and beans to make paella. Valencians regard this recipe as authentic as well. In this recipe, the seafood is served in the shell. A variant on this is "paella del senyoret" which utilizes seafood without shells. Later, however, Spaniards living outside of Valencia combined seafood with meat from land animals and mixed paella was born. This paella is sometimes called "preparación barroca" (baroque preparation) due to the variety of ingredients and its final presentation.
During the 20th century, paella's popularity spread past Spain's borders. As other cultures set out to make paella, the dish invariably acquired regional influences. Consequently, paella recipes went from being relatively simple to including a wide variety of seafood, meat, sausage, (even chorizo) vegetables and many different seasonings. However, the most globally popular recipe is seafood paella.
Throughout non-Valencian Spain, mixed paella is very popular. Some restaurants in Spain (and many in the United States) that serve this mixed version, refer to it as "Valencian" paella. However, Valencians insist only the original two Valencian recipes are authentic. They generally view all others as inferior, not genuine or even grotesque.
Basic cooking methods.
According to tradition in Valencia, paella is cooked over an open fire, fueled by orange and pine branches along with pine cones. This produces an aromatic smoke which infuses the paella. Also, dinner guests traditionally eat directly out of the "paellera".
Some recipes call for paella to be covered and left to settle for five or ten minutes after cooking.
Valencian paella.
This recipe is standardized because Valencians consider it traditional and very much part of their culture. Rice in Valencian paella is never braised in oil, as pilaf, though the paella made further southwest of Valencia often is.
Seafood paella.
Recipes for this dish vary somewhat, even in Valencia. Below is a recipe by Juanry Segui, a prominent Valencian chef.
Mixed paella.
There are countless mixed paella recipes. The following method is common to most of these. Seasoning depends greatly on individual preferences and regional influences. However, salt, saffron and garlic are almost always included.
For all recipes.
After cooking paella, there is usually a layer of toasted rice at the bottom of the pan, called "socarrat" in Spain. This is considered a delicacy among Spaniards and is essential to a good paella. The toasted rice develops on its own if the paella is cooked over a burner or open fire. If cooked in an oven, however, it will not. To correct this, place the "paellera" over a high flame while listening to the rice toast at the bottom of the pan. Once the aroma of toasted rice wafts upwards, it is removed from the heat. The paella must then sit for about five minutes (most recipes recommend the paella be covered with a tea-towel at this point) to absorb the remaining broth.
Competitions and records.
It has become a custom at mass gatherings in the Valencian Community (festivals, political campaigns, protests, etc.) to prepare enormous paellas, sometimes to win mention in the "Guinness Book of World Records". Chefs use gargantuan "paelleras" for these events.
Valencian restaurateur Juan Galbis claims to have made the world's largest paella with help from a team of workers on 2 October 2001. This paella fed about 110,000 people according to Galbis' former website. Galbis says this paella was even larger than his earlier world-record paella made on 8 March 1992 which fed about 100,000 people. Galbis's record-breaking 1992 paella is listed in "Guinness World Records".
Similar dishes.
Traditional Valencian cuisine offers recipes similar to "paella valenciana" and "paella de marisco" such as "arròs negre", "arròs al forn", "arròs a banda" and "arròs amb fesols i naps". Fideuà is a noodle dish variation of the paella cooked in a similar fashion, though it may be served with allioli sauce.
The following is a list of other similar rice dishes:

</doc>
<doc id="47631" url="https://en.wikipedia.org/wiki?curid=47631" title="Khazaran">
Khazaran

Khazaran was a city in the Khazar kingdom, located on the eastern bank of the lower Volga River. It was connected to Atil by a pontoon bridge.
Khazaran was later inhabited primarily by Muslims and featured numerous mosques, minarets, and madrasas. It was a bustling trade center easily reachable by ship from the Caspian Sea and Volga River, and many of its inhabitants were crafters, fishers, and traders. The leader of the city was a Muslim official known as the Khazz; in Arab sources the title given is sometimes vizier.

</doc>
<doc id="47632" url="https://en.wikipedia.org/wiki?curid=47632" title="Falles">
Falles

The Falles (, sing. "Falla") is a traditional celebration held in commemoration of Saint Joseph in the city of Valencia, Spain. The term "Falles" refers to both the celebration and the monuments burnt during the celebration. A number of towns in the Valencian Community have similar celebrations inspired by the original Falles de Valéncia celebration.
Each neighbourhood of the city has an organised group of people, the "Casal faller", that works all year long holding fundraising parties and dinners, usually featuring the famous dish, paella, a specialty of the region. Each "casal faller" produces a construction known as a "falla" which is eventually burnt. A "casal faller" is also known as a "comissió fallera".
Etymology.
The name of the festival is the plural of the Valencian word "falla". The word's derivation is as follows:
"Falles" and "ninots".
Formerly, much time would be spent by the "casal faller" preparing the "ninots" (Valencian for puppets or dolls). During the four days leading up to 19 March, each group takes its "ninot" out for a grand parade, and then mounts it, each on its own elaborate firecracker-filled cardboard and paper-mâché artistic monument in a street of the given neighbourhood. This whole assembly is a "falla".
The "ninots" and their "falles" are constructed according to an agreed upon theme that has traditionally been a satirical jab at whatever draws the attention of the "fallers" (the registered participants of the "casals"). In modern times, the two-week-long festival has spawned a substantial local industry, to the point that an entire suburban area has been designated the "Ciutat fallera" (Falles City). Here, crews of artists and artisans, sculptors, painters, and other craftsmen, all spend months producing elaborate constructions of paper and wax, wood and polystyrene foam tableaux towering up to five stories, composed of fanciful figures, often caricatures, in provocative poses arranged in a gravity-defying manner. Each of them is produced under the direction of one of the many individual neighbourhood "casals fallers" who vie with each other to attract the best artists, and then to create the most outrageous allegorical monument to their target. There are about 750 of these neighbourhood associations in Valencia, with over 200,000 members, or a quarter of the city's population.
During Falles, many people wear their "casal faller" dress of regional and historical costumes from different eras of Valéncia's history; the "dolçaina" (an oboe-like reed instrument) and "tabalet" (a kind of Valencian drum) are frequently heard, as most of the different casals fallers have their own traditional bands.
Although the "Falles" is a very traditional event and many participants dress in medieval clothing, the "ninots" for 2005 included such modern characters as Shrek and George W. Bush, and the 2012 "Falles" included characters like Barack Obama and Lady Gaga.
Events during "Falles".
The five days and nights of "Falles" might be described as a continuous street party. There are a multitude of processions: historical, religious, and comedic. Crowds in the restaurants spill out into the streets. Explosions can be heard all day long and sporadically through the night. Everyone from small children to elderly people can be seen throwing fireworks and noisemakers in the streets, which are littered with pyrotechnical debris. The timing of the events is fixed, and they fall on the same date every year, though there has been discussion about holding some events on the weekend preceding the Falles, to take greater advantage of the tourist potential of the festival or changing the end date in years where it is due to occur in midweek.
La Despertà.
Each day of Falles begins at 8:00 am with "La Despertà" ("the wake-up call"). Brass bands appear from the casals and begin to march down every street playing lively music. Close behind them are the "fallers", throwing large firecrackers in the street as they go.
La Mascletà.
The "Mascletà", an explosive barrage of coordinated firecracker and fireworks displays, takes place at 2:00 pm every day of the festival; the main event is the municipal Mascletà in the "Plaça de l'Ajuntament" where the pyrotechnicians compete for the honour of providing the final Mascletà of the "festes" (on 19 March). At 2:00 pm the clock chimes and the Fallera Major, dressed in her "fallera" finery, will call from the balcony of City Hall, "Senyor pirotècnic, pot començar la mascletà!" ("Mr. Pyrotechnic, you may commence the Mascletà!"), and the Mascletà begins.
The Mascletà is almost unique to the Valencian Community, and very popular with the Valencian people. Smaller neighbourhoods often hold their own mascletà for saint's days, weddings and other celebrations.
La Plantà.
The day of the 15th all of the "falles" infantils are to be finished being constructed and later that night all of the "falles majors" (big Falles) are to be completed. If not, they face disqualification.
L'Ofrena de flors.
In this event, the flower offering, each of the casals fallers takes an offering of flowers to the Virgin Mary as "Our Lady of the Forsaken." This occurs all day during 17–18 March. A statue of the Virgin Mary and its large pedestal are then covered with all the flowers.
Els Castells and La Nit del Foc.
On the nights of the 15, 16, 17, and 18th there are firework displays in the old riverbed in Valéncia. Each night is progressively grander and the last is called "La Nit del Foc" (the Night of Fire).
Cavalcada del Foc.
On the final evening of Falles, at 7:00 pm on March 19, a parade known in Valencian as the "Cavalcada del Foc" (the Fire Parade) takes place along Colom street and Porta de la Mar square. This spectacular celebration of fire, the symbol of the fiesta’s spirit, is the "grand finale" of Falles and a colourful, noisy event featuring exhibitions of the varied rites and displays from around the world which use fire; it incorporates floats, giant mechanisms, people in costumes, rockets, gunpowder, street performances and music.
La Cremà.
On the final night of Falles, around midnight on March 19, these "falles" are burnt as huge bonfires. This is known as "La Cremà" (the Burning), the climax of the whole event, and the reason why the constructions are called "falles" ("torches"). Traditionally, the falla in the "Plaça de l'Ajuntament" is burned last.
Many neighbourhoods have a "falla infantil" (a children's "falla", smaller and without satirical themes), which is held a few metres away from the main one. This is burnt first, at 10:00 pm. The main neighbourhood "falles" are burnt closer to midnight; the burning of the "falles" in the city centre often starts later. For example, in 2005, the fire brigade delayed the burning of the Egyptian funeral "falla" in "Carrer del Convent de Jerusalem" until 1:30 am, when they were sure all safety concerns were addressed.
Each "falla" is laden with fireworks which are lit first. The construction itself is lit either after or during the explosion of these fireworks. "Falles" burn quite quickly, and the heat given off is felt by all around. The heat from the larger ones often drives the crowd back a couple of metres, even though they are already behind barriers that the fire brigade has set several metres from the construction. In narrower streets, the heat scorches the surrounding buildings, and the firemen douse the fronteres, window blinds, street signs, etc. with their hoses to stop them catching fire or melting, from the beginning of the "cremà" until it cools down.
Away from the "falles", people frolic in the streets, the whole city resembling an open-air dance party, except that instead of music there is the incessant (and occasionally deafening) sound of people throwing fireworks around randomly. There are stalls selling products such as the typical fried snacks "porres", "churros" and "bunyols", as well as roasted chestnuts or trinkets.
While the smaller fallas dotted around the streets are burned at approximately the same time as each other, the last falla to be burned is the main one, which is saved until last so that everybody can watch it. This main falla is found outside the "Ajuntament" - the town hall. People arrive a few hours before the scheduled burning time to get a front row view.
History.
There are different conjectures regarding the origin of the Falles festival. One suggests that the Falles started in the Middle Ages, when artisans disposed of the broken artifacts and pieces of wood they saved during the winter by burning them to celebrate the spring equinox. Valencian carpenters used planks of wood called "parots" to hang their candles on during the winter, as these were needed to provide light to work by. With the coming of the spring, they were no longer necessary, so they were burned. Over time, and with the intervention of the Church, the date of the burning of these "parots" was made to coincide with the celebration of the festival of Saint Joseph, the patron saint of carpenters.
This tradition continued to evolve. The "parot" was dressed with clothing so that it looked like a person; features identifiable with some well-known person from the neighbourhood were often added as well. To collect these materials, children went from house to house asking for "una estoreta velleta" (an old rug) to add to the "parot". This became a popular song that the children sang as they gathered all sorts of old flammable furniture and utensils to burn in the bonfire with the "parot". These "parots" were the first "ninots". With time, people of the neighbourhoods organized the building of the falles and the typically intricate constructions, including their various figures, were born.
Until the beginning of the 20th century, the "falles" were tall boxes with three or four wax dolls dressed in fabric clothing. This changed when the creators began to use cardboard. The fabrication of the "falles" continues to evolve in modern times, when the largest displays are made of polystyrene and soft cork easily molded with hot saws. These techniques have allowed the creation of "falles" over 30 metres high.
The origin of the pagan festival is similar to that of the Bonfires of Saint John celebrated in the Alacant region, in the sense that both came from the Latin habit of lighting fires to welcome spring. But in Valéncia, this ancient tradition led to the burning of accumulated waste at the end of winter, particular wood, on the day of Saint Joseph, as was fitting. Given the reputed humorous character of Valencians, it was natural that they began to burn figurines depicting people and events of the past year. The burning symbolised liberation from servitude to the memory of these events or else represented humorous and often critical commentary on them. The festival thus evolved a more satirical and ironic character, and the wooden castoffs gradually came to be assembled into progressively more elaborate 'monuments' that were designed and painted in advance.
During the early 20th century and especially during the Spanish Civil War, the monuments became more anti-clerical in nature and were often highly critical of the local or national governments, which in fact tried to ban the Falles many times, without success. Under the dictatorship of Franco the celebration lost much of its satirical nature because of government censorship, but the monuments were among the few fervent public expressions allowed then, and they could be made freely in Valéncia. During this period, many religious customs such as the offering of flowers to Mare de Déu dels Desamparat"s" (Our Lady of the Forsaken) were taken up, which today are essential parts of the festival, even though unrelated to the original purpose of the celebration, and somewhat antithetical in spirit.
With the restoration of democracy and the end of government censorship, the critical "falles" reappeared, and obscene satirical ones with them. Despite thirty years of freedom of expression, the world view of the "fallero" can still be socially conservative, is often sexist and may involve some of the amoralism of Valencian politics. This has sometimes led to criticism by certain cultural critics, environmentalists, and progressives. Yet there are celebrants of all ideologies and factions, and they have different interpretations of the spirit of the celebration. Although recent initiatives such as the pilota championships, literary competitions and other events have broadened its cultural expression, the city still embraces such ancient traditions to express its own singular identity.
Secció Especial.
The Secció Especial is a group of the largest and most prestigious "falles" commissions in the city of Valencia. In 2007, the group consisted of 14 commissions. This class of "falles" was first started in 1942 and originally included the "falles" of Barques, Reina-Pau and Plaça del Mercat. Currently, none of these are still in the group. The commission that has most often participated in this group as of 2015 was Na Jordana, with 62 times. Winning the first prize in the Secció Especial is the most prestigious prize any falla can win. All other "falles" fall into different classes that also award prizes with the exception of the one erected by the town hall.

</doc>
<doc id="47634" url="https://en.wikipedia.org/wiki?curid=47634" title="Samandar (city)">
Samandar (city)

Samandar (also Semender) was a city in Khazaria on the western shore of the Caspian Sea, south of the city of Atil, in the North Caucasus. The exact location of the city is unknown, but most likely, it was situated on the Terek river near the present-day city of Kizlyar, which, like Samandar, is noted for its vineyards. According to the Soviet archeologist Mikhail Artamonov, remains of a large town have been found deep in the woods along the lower Terek.
The name of the city may derive from the name of a Hunnish tribe "Zabender". The Greek writer Theophylact Simocatta refers to a migration of Zabender from Asia to Europe in about 598; in addition, an Armenian book on geography attributed to Moses of Chorene mentions a town "M-s-n-d-r" in the land of Huns located to the north of Derbent.
Samandar was inhabited by Jews, Christians, Muslims, and members of other religious faiths, each of which had its houses of worship. The city served as the capital of Khazaria from the 720s to about 750, when the capital was moved northwards to Atil, which was less vulnerable to Arab attacks. Both cities were destroyed by Kievan Rus' prince Sviatoslav in the 960s, leading to a decline and disappearance of Khazaria.
According to al-Istakhri, Samandar was famous for its fertile gardens and vineyards, and large quantities of wine were made there.

</doc>
<doc id="47635" url="https://en.wikipedia.org/wiki?curid=47635" title="Murphy's law">
Murphy's law

Murphy's law is an adage or epigram that is typically stated as: "Anything that can go wrong, will go wrong".
History.
The perceived perversity of the universe has long been a subject of comment, and precursors to the modern version of Murphy's law are not hard to find, such as the German phrase "Die Tücke Der Dinge" ("The perverseness of things", or "The treachery of things"). The concept may be as old as humanity. Recent significant research in this area has been conducted by members of the American Dialect Society. Society member Stephen Goranson has found a version of the law, not yet generalized or bearing that name, in a report by Alfred Holt at an 1877 meeting of an engineering society.
It is found that anything that can go wrong at sea generally does go wrong sooner or later, so it is not to be wondered that owners prefer the safe to the scientific ... Sufficient stress can hardly be laid on the advantages of simplicity. The human factor cannot be safely neglected in planning machinery. If attention is to be obtained, the engine must be such that the engineer will be disposed to attend to it.
Mathematician Augustus De Morgan wrote on June 23, 1866:
"The first experiment already illustrates a truth of the theory, well confirmed by practice, what-ever can happen will happen if we make trials enough." In later publications "whatever can happen will happen" occasionally is termed "Murphy's law," which raises the possibility—if something went wrong—that "Murphy" is "De Morgan" misremembered (an option, among others, raised by Goranson on the American Dialect Society list).
American Dialect Society member Bill Mullins has found a slightly broader version of the aphorism in reference to stage magic. The British stage magician Nevil Maskelyne wrote in 1908:
It is an experience common to all men to find that, on any special occasion, such as the production of a magical effect for the first time in public, everything that "can" go wrong "will" go wrong. Whether we must attribute this to the malignity of matter or to the total depravity of inanimate things, whether the exciting cause is hurry, worry, or what not, the fact remains.
In 1948, humorist Paul Jennings coined the term "resistentialism", a jocular play on "resistance" and "existentialism", to describe "seemingly spiteful behavior manifested by inanimate objects", where objects that cause problems (like lost keys or a runaway bouncy ball) are said to exhibit a high degree of malice toward humans.
The contemporary form of Murphy's law goes back as far as 1952, as an epigraph to a mountaineering book by John Sack, who described it as an "ancient mountaineering adage":
Anything that can possibly go wrong, does.
Fred R. Shapiro, the editor of the "Yale Book of Quotations", has shown that in 1952 the adage was called "Murphy's law" in a book by Anne Roe, quoting an unnamed physicist:
he described as "Murphy's law or the fourth law of thermodynamics" (actually there were only three last I heard) which states: "If anything can go wrong, it will."
In May 1951, Anne Roe gives a transcript of an interview (part of a Thematic Apperception Test, asking impressions on a photograph) with Theoretical Physicist number 3: "...As for himself he realized that this was the inexorable working of the second law of the thermodynamics which stated Murphy's law ‘If anything can go wrong it will’." Anne Roe's papers are in the American Philosophical Society archives in Philadelphia; those records (as noted by Stephen Goranson on the American Dialect Society list 12/31/2008) identify the interviewed physicist as Howard Percy "Bob" Robertson (1903–1961). Robertson's papers are at the Caltech archives; there, in a letter Robertson offers Roe an interview within the first three months of 1949 (as noted by Goranson on American Dialect Society list 5/9/2009). The Robertson interview apparently predated the Muroc scenario said by Nick Spark ("American Aviation Historical Society Journal" 48 (2003) p. 169) to have occurred in or after June, 1949.
The name "Murphy's law" was not immediately secure. A story by Lee Correy in the February 1955 issue of "Astounding Science Fiction" referred to "Reilly's law," which "states that in any scientific or engineering endeavor, anything that can go wrong "will" go wrong". Atomic Energy Commission Chairman Lewis Strauss was quoted in the "Chicago Daily Tribune" on February 12, 1955, saying "I hope it will be known as Strauss' law. It could be stated about like this: If anything bad can happen, it probably will."
Arthur Bloch, in the first volume (1977) of his "Murphy's Law, and Other Reasons Why Things Go WRONG" series, prints a letter that he received from George E. Nichols, a quality assurance manager with the Jet Propulsion Laboratory. Nichols recalled an event that occurred in 1949 at Edwards Air Force Base, Muroc, California that, according to him, is the origination of Murphy's law, and first publicly recounted by USAF Col. John Paul Stapp. An excerpt from the letter reads:
The law's namesake was Capt. Ed Murphy, a development engineer from Wright Field Aircraft Lab. Frustration with a strap transducer which was malfunctioning due to an error in wiring the strain gage bridges caused him to remark – "If there is any way to do it wrong, he will" – referring to the technician who had wired the bridges at the Lab. I assigned Murphy's law to the statement and the associated variations.
In reality.
According to Richard Dawkins of the University of Oxford, so-called laws like Murphy's law and Sod's law are nonsense because they require inanimate objects to have desires of their own, or else to react according to one's own desires. Dawkins points out that a certain class of events may occur all the time, but are only noticed when they become a nuisance. He gives as an example aircraft noise interfering with filming. Aircraft are in the sky all the time, but are only taken note of when they cause a problem. This is a form of confirmation bias whereby the investigator seeks out evidence to confirm his already formed ideas, but does not look for evidence that contradicts them.
Similarly, David Hand, emeritus professor of mathematics and senior research investigator at Imperial College London, points out that the law of truly large numbers should lead one to expect the kind of events predicted by Murphy's law to occur occasionally. Selection bias will ensure that those ones are remembered and the many times Murphy's law was not true are forgotten.
There have been persistent references to Murphy's law associating it with the laws of thermodynamics from early on (see the quotation from Anne Roe's book above). In particular, Murphy's law is often cited as a form of the second law of thermodynamics (the law of entropy) because both are predicting a tendency to a more disorganised state. Atanu Chatterjee investigated this idea by formally stating Murphy's law in mathematical terms. Chatterjee found that Murphy's law so stated could be disproved using the principle of least action.
Association with Murphy.
According to the book "A History of Murphy's Law" by author Nick T. Spark, differing recollections years later by various participants make it impossible to pinpoint who first coined the saying "Murphy's law". The law's name supposedly stems from an attempt to use new measurement devices developed by the eponymous Edward Murphy. The phrase was coined in adverse reaction to something Murphy said when his devices failed to perform and was eventually cast into its present form prior to a press conference some months later — the first ever (of many) given by Dr. John Stapp, a U.S. Air Force colonel and Flight Surgeon in the 1950s. These conflicts (a long running interpersonal feud) were unreported until Spark researched the matter. His book expands upon and documents an original four part article published in 2003 (Annals of Improbable Research (AIR)) on the controversy: "Why Everything You Know About Murphy's Law is Wrong".
From 1948 to 1949, Stapp headed research project MX981 at Muroc Army Air Field (later renamed Edwards Air Force Base) for the purpose of testing the human tolerance for g-forces during rapid deceleration. The tests used a rocket sled mounted on a railroad track with a series of hydraulic brakes at the end. Initial tests used a humanoid crash test dummy strapped to a seat on the sled, but subsequent tests were performed by Stapp, at that time an Air Force captain. During the tests, questions were raised about the accuracy of the instrumentation used to measure the g-forces Captain Stapp was experiencing. Edward Murphy proposed using electronic strain gauges attached to the restraining clamps of Stapp's harness to measure the force exerted on them by his rapid deceleration. Murphy was engaged in supporting similar research using high speed centrifuges to generate g-forces. Murphy's assistant wired the harness, and a trial was run using a chimpanzee.
The sensors provided a zero reading; however, it became apparent that they had been installed incorrectly, with each sensor wired backwards. It was at this point that a disgusted Murphy made his pronouncement, despite being offered the time and chance to calibrate and test the sensor installation prior to the test proper, which he declined somewhat irritably, getting off on the wrong foot with the MX981 team. In an interview conducted by Nick Spark, George Nichols, another engineer who was present, stated that Murphy blamed the failure on his assistant after the failed test, saying, "If that guy has any way of making a mistake, he will." Nichols' account is that "Murphy's law" came about through conversation among the other members of the team; it was condensed to "If it can happen, it will happen," and named for Murphy in mockery of what Nichols perceived as arrogance on Murphy's part. Others, including Edward Murphy's surviving son Robert Murphy, deny Nichols' account (which is supported by Hill, both interviewed by Spark), and claim that the phrase did originate with Edward Murphy. According to Robert Murphy's account, his father's statement was along the lines of "If there's more than one way to do a job, and one of those ways will result in disaster, then he will do it that way."
The phrase first received public attention during a press conference in which Stapp was asked how it was that nobody had been severely injured during the rocket sled tests. Stapp replied that it was because they always took "Murphy's law" under consideration; he then summarized the law and said that in general, it meant that it was important to consider all the possibilities (possible things that could go wrong) before doing a test and act to counter them. Thus Stapp's usage and Murphy's alleged usage are very different in outlook and attitude. One is sour, the other an affirmation of the predictable being surmountable, usually by sufficient planning and redundancy. Hill and Nichols believe Murphy was unwilling to take the responsibility for the device's initial failure (by itself a blip of no large significance) and is to be doubly damned for not allowing the MX981 team time to validate the sensor's operability and for trying to blame an underling when doing so in the embarrassing aftermath.
The association with the 1948 incident is by no means secure. Despite extensive research, no trace of documentation of the saying as "Murphy's law" has been found before 1951 (see above). The next citations are not found until 1955, when the May–June issue of "Aviation Mechanics Bulletin" included the line "Murphy's law: If an aircraft part can be installed incorrectly, someone will install it that way," and Lloyd Mallan's book, "Men, Rockets and Space Rats", referred to: "Colonel Stapp's favorite takeoff on sober scientific laws—Murphy's law, Stapp calls it—'Everything that can possibly go wrong will go wrong'." The Mercury astronauts in 1962 attributed Murphy's law to U.S. Navy training films.
Other variations on Murphy's law.
From its initial public announcement, Murphy's law quickly spread to various technical cultures connected to aerospace engineering. Before long, variants had passed into the popular imagination, changing as they went.
Author Arthur Bloch has compiled a number of books full of corollaries to Murphy's law and variations thereof. The first of these was "Murphy's law and other reasons why things go wrong!",
Yhprum's law, where the name is spelled backwards, is "anything that can go right, will go right" — the optimistic application of Murphy's law in reverse.
Peter Drucker, the management consultant, with a nod to Murphy, formulated "Drucker's Law" in dealing with complexity of management: "If one thing goes wrong, everything else will, and at the same time."

</doc>
<doc id="47636" url="https://en.wikipedia.org/wiki?curid=47636" title="International Sign">
International Sign

International Sign (IS) is a contact variety of sign language used in a variety of different contexts, particularly at international meetings such as the World Federation of the Deaf (WFD) congress, events such as the Deaflympics, in video clips produced by Deaf people and watched by other Deaf people from around the world, and informally when travelling and socialising. It is a sign-language pidgin. It is not as conventionalised or complex as natural sign languages, and has a limited lexicon.
Naming.
While the more commonly used term is International Sign, it is sometimes referred to as Gestuno, or informally as International Sign Language (ISL), International Sign Pidgin and International Gesture (IG). International Sign is a term used by the World Federation of the Deaf and other international organisations.
History.
Deaf people in the Western and Middle Eastern world have gathered together using sign language for 2,000 years. When Deaf people from different sign language backgrounds get together, a contact variety of sign language arises from this contact, whether it is in an informal personal context or in a formal international context. Deaf people have therefore used a kind of auxiliary gestural system for international communication at sporting or cultural events since the early 19th century. The need to standardise an international sign system was discussed at the first World Deaf Congress in 1951, when the WFD was formed. In the following years, a pidgin developed as the delegates from different language backgrounds communicated with each other, and in 1973, a WFD committee ("the Commission of Unification of Signs") published a standardized vocabulary. They selected "naturally spontaneous and easy signs in common use by deaf people of different countries" to make the language easy to learn. A book published by the commission in the early 1970s, "Gestuno: International Sign Language of the Deaf", contains a vocabulary list of about 1500 signs. The name "Gestuno" was chosen, referencing gesture and oneness.
However, when Gestuno was first used, at the WFD congress in Bulgaria in 1976, it was incomprehensible to deaf participants. Subsequently, it was developed informally by deaf and hearing interpreters, and came to include more grammar — especially linguistic features that are thought to be universal among sign languages, such as role shifting and the use of classifiers. Additionally, the vocabulary was gradually replaced by more iconic signs and loan signs from different sign languages.
The first training course in Gestuno was conducted in Copenhagen in 1977 to prepare interpreters for the 5th World Conference on Deafness. Sponsored by the Danish Association of the Deaf and the University of Copenhagen, the course was designed by Robert M. Ingram and taught by Betty L. Ingram, two American interpreters.
The name Gestuno has fallen out of use, and the phrase "International Sign" is now more commonly used in English to identify this sign variety. Indeed, current IS has little in common with the signs published under the name 'Gestuno'.
A parallel development has been occurring in Europe in recent years, where increasing interaction between Europe's deaf communities has led to the emergence of a pan-European pidgin or creole sign. It is referred to by some sign linguists as "Eurosigns". Influence in Euro-Signs can be seen from British Sign Language, French Sign Language and Scandinavian signs.
Vocabulary.
The lexicon of International Sign is limited, and varies between signers. IS interpreter Bill Moody noted in a 1994 paper that the vocabulary used in conference settings is largely derived from the sign languages of the Western world and is less comprehensible to those from African or Asian sign language backgrounds. A 1999 study by Bencie Woll suggested that IS signers often use a large amount of vocabulary from their native language, choosing sign variants that would be more easily understood by a foreigner. In contrast, Rachel Rosenstock notes that the vocabulary exhibited in her study of International Sign was largely made up of highly iconic signs common to many sign languages:
Over 60% of the signs occurred in the same form in more than eight SLs as well as in IS. This suggests that the majority of IS signs are not signs borrowed from a specific SL, as other studies found, but rather are common to many natural SLs. Only 2% of IS signs were found to be unique to IS. The remaining 38% were borrowed (or "loan") signs that could be traced back to one SL or a group of related SLs. 
Grammar.
People communicating in International Sign tend to make heavy use of role play, as well as a feature common to most sign languages researched to date: an extensive formal system of classifiers. Classifiers are used to describe things, and they transfer well across linguistic barriers. It has been noted that signers are generally better at interlingual communication than non-signers, even without a lingua franca. Perhaps, along with deaf people's experience with bridging communication barriers, the use of classifiers is a key reason.
A paper presented in 1994 suggested that IS signers "combine a relatively rich and structured grammar with a severely impoverished lexicon". Supalla and Webb (1995) describe IS as a kind of a pidgin, but conclude that it is "more complex than a typical pidgin and indeed is more like that of a full sign language".
Letters and numbers.
A manual alphabet is used for fingerspelling names, which is based on the one-handed systems used in Europe and America for representing the Roman alphabet. In a two-way conversation, any manual alphabet known may be used; often one speaker will fingerspell using the alphabet of the other party, as it is often easier to spell quickly in an unfamiliar alphabet than to read quickly. ISL also has a standardised system of numbers as these signs vary greatly between sign languages.
Use of indigenous signs.
Each region's own sign is preferred for country and city names. This may be used in conjunction with spelling and classifying for the first instance, and the indigenous sign used alone from then on.

</doc>
<doc id="47638" url="https://en.wikipedia.org/wiki?curid=47638" title="Kerch">
Kerch

Kerch (, , Old East Slavic: Кърчевъ, , , Ancient Greek: "Pantikapaion") is a city of regional significance on the Kerch Peninsula in the east of the Crimea.
Population: 
Founded 2,600 years ago as an ancient Greek colony, Kerch is considered to be one of the most ancient cities in Crimea. The city experienced rapid growth starting in the 1920s and was the site of a major battle during World War II.
Today, it is one of the largest cities in Crimea and is among the republic's most important industrial, transport and tourist centres.
History.
Ancient times.
Archeological digs at Mayak village near the city ascertained that the area had already been inhabited in 17th–15th centuries BC.
Kerch as a city starts its history in 7th century BC, when Greek colonists from Miletus founded a city-state named Panticapaeum on Mount Mithridat near the mouth of the Melek-Chesme river. Panticapaeum subdued nearby cities and by 480 BC became a capital of the Kingdom of Bosporus. Later, during the rule of Mithradates VI Eupator, Panticapaeum for a short period of time became the capital of the much more powerful and extensive Kingdom of Pontus.
The city was located at the intersection of trade routes between the steppe and Europe. This caused it to grow rapidly. The city's main exports were grain and salted fish, wine-making was also common. Panticapaeum minted its own coins. According to extant documents the Melek-Chesme river (small and shallow nowadays) was navigable in Bosporan times, and sea galleys were able to enter the river. A large portion of the city's population was ethnically Scythian, later Sarmatian, as the large royal barrow at Kul-Oba testifies.
In the 1st century AD Panticapaeum and the Kingdom of Bosporus suffered from Ostrogoth raids, then the city was devastated by the Huns in AD 375.
Middle Ages.
From the 6th century the city was under the control of the Byzantine Empire. By order of Emperor Justinian I, a citadel named Bospor was built there. Bospor was the centre of a bishopric, the diocese of Bosporus and developed under the influence of Greek Christianity. In 576, it withstood a siege by the Göktürks under Bokhan, aided by Anagai, the last khan (ruler) of the Uturgurs (tribe of Huns).
In the 7th century, the Turkic Khazars took control of Bospor, and the city was named Karcha from Turkic "karşı" meaning 'opposite, facing.' The main local government official during Khazar times was the tudun. Christianity was a major religion in Kerch during the period of Khazar rule. Kerch's Church of St. John the Baptist was founded in 717, thus, it is the oldest church in Ukraine. The "Church of the Apostles" existed during the late 8th and early 9th centuries, according to the "Life of the Apostle Andrew" by Epiphanius of Salamis.
Following the fall of Khazaria to Kievan Rus' in the late 10th century, Kerch became the centre of a Khazar successor-state. Its ruler, Georgius Tzul, was deposed by a Byzantine-Rus expedition in 1016.
From the 10th century, the city was a Slavic settlement named Korchev, which belonged to the Tmutarakan principality. Kerch was a center of trade between Russia', Crimea, Caucasus and the Orient.
In the 13th century, the Crimea including Korchev was invaded by Mongols. After Mongols, the city became the Genoese colony of Cerco (Cherkio) in 1318 and served as a sea harbour, where townspeople worked at salt-works and fishery.
In 1475, city was passed to the Ottoman Empire. During the Turkish rule Kerch fell into decay and served as a slave-market. It repeatedly suffered from raids of Zaporizhian Cossacks.
18th - 20th centuries.
In response to strengthening of Russian military forces in Azov area, the Turks built a fortress, named Yenikale, near Kerch on the shore of Kerch Strait. The fortress was completed by 1706. In 1771 the Imperial Russian Army invaded Crimea and approached Yenikale. The Turks decided to abandon the fortress, though reinforcements from the Ottoman Empire had arrived a few days earlier. By the Peace Treaty of Kuchuk-Kainarji in 1774, Kerch and Yenikale were ceded to Russia. As a result, the Turkish heritage has been almost completely wiped out.
In 1790 Russian naval forces under the command of admiral Fyodor Ushakov defeated the Turkish fleet at the Battle of Kerch Strait.
Because of its location, from 1821 Kerch developed into an important trade and fishing port. The state museum of ancient times and a number of educational institutions were opened in the city. The ironwork factory was built in 1846 based on a huge iron ore deposit found on Kerch Peninsula.
During the Crimean War the city was devastated by British forces in 1855.
In the late 19th century, mechanical and cement factories were built, and tinned food and tobacco factories were established.
By 1900, Kerch was connected to a railroad system, and the fairway of Kerch Strait was deepened and widened. At this time, the population had reached 33,000.
After suffering a decline during the First World War and the Russian Civil War, the city resumed its growth in the late 1920s, with the expansion of various industries, iron ore and metallurgy in particular, and by 1939 its population had reached 104,500.
Kerch in World War II.
On the Eastern Front of World War II from 1941 to 1945, Kerch was the site of heavy fighting between Soviet Army and Axis forces.
After fierce fighting, the city was taken by the Germans in November 1941. On 31 December 1941 the 302nd Mountain Rifle Division recaptured the city following a naval landing operation at Kamysh Burun, to the south of the city, five days earlier. In 1942 the Germans occupied the city again. The Red Army lost over 160,000 men, either killed or taken POW at the Battle of the Kerch Peninsula. On 31 October 1943 another Soviet naval landing operation was launched. Kerch returned to Soviet control on 11 April 1944.
The German invaders killed about 15,000 citizens and deported another 14,000 during their occupation.
Evidence of German atrocities in Kerch was presented in the Nuremberg trials. After the war, the city was awarded the title Hero City.
The Adzhimushkay catacombs (mines) in the city's suburbs were the site of guerrilla warfare against the occupation. Thousands of soldiers and refugees found shelter inside, and were involved in counterattacks. Many of them died underground, including those who died of numerous alleged poison gas attacks. Later, a memorial was established on the site.
Modern Kerch.
Administrative divisions.
The city municipality stretches over a substantial area and includes several separate neighborhoods that are part of the Kerch city: Eltigen (Heroyevskoe), Kamysh-Burun (Arshyntsevo), Port Krym, Adzhimushkai, and Tuzla Island.
Industry.
Today Kerch is considered as a city of metallurgists, shipbuilders and fishermen.
The largest enterprises in the city are:
Construction-materials, food processing, and light industries play a significant role in the city's economy. Kerch is also a fishing fleet base and an important processing centre for numerous fish products.
Transport.
Kerch has a harbour on the Kerch Strait, which makes it a key to the Sea of Azov, several railroad terminals and a small airport. The Kerch Strait ferry line across the Kerch Strait was established in 1953, connecting Crimea and the Krasnodar Krai (Port Krym - Port Kavkaz line); (as of November 2009) there are also plans for a Kerch-Poti ferry route.
There are several ports in Kerch, including Kerch Maritime Trading Port, Kerch Maritime Fishing Port, Port Krym (ferry crossing), Kamysh-Burun Port.
The railroad terminals include: Kerch, Kerch I, Kerch Factory, Arshyntsevo, and Krym.
Bus network connects Kerch to other cities in Crimea and Krasnodar Krai.
Education.
Kerch hosts (2004):
Archaeology.
Archaeological digs in Kerch were launched under Russian auspices in the middle of the 19th century. Since then the site of ancient Panticapaeum city on Mount Mithridat has been systematically excavated. Located nearby are several ancient burial mounds (kurgans) and excavated cities. Kerch takes part in UNESCO's "Silk Road" programme.
Treasures and historical findings of Kerch adorn the collections of major museums around the world. Such as: the Hermitage, the Louvre, the British Museum, the Berlin Museum, the Moscow State Museum of fine arts and many others.
Currently, excavations at ancient fortresses of Kerch are led by scientists from Russia, Ukraine, and Poland.
Tourism.
Because of its location on shores of Azov and Black seas, Kerch became a popular summer resort among people of former USSR. Also, several mud-cure sources are located near the city. Despite the seaside location, the tourist appeal of Kerch today is limited because of the industrial character of the city and associated pollution.
Despite the lack of beaches in the town's area, there are a lot of them at a distance of 20 minutes' travel by bus, train or taxi.
Kerch has a number of impressive architectural and historical monuments. Ancient historical heritage of the city makes it attractive for scientific tourism. The most notable of Kerch's sights are:
Honours.
A minor planet 2216 Kerch discovered in 1971 by Soviet astronomer Tamara Mikhailovna Smirnova is named after the city.
Climate.
Kerch has a humid subtropical climate (Köppen climate classification "Cfa") with cool to cold winters and warm to hot summers.
Recent events.
Autumn storm of 2007.
On 11 November 2007 there was a great storm that passed through the city, causing much damage and an ecological disaster as a few ships, including an oil tanker, were shipwrecked and blocked the Kerch Strait.
Kerch Strait Bridge.
On 25 April 2010, Ukrainian President Viktor Yanukovych and Russian President Dmitry Medvedev signed an agreement to build a bridge across the Kerch Strait.

</doc>
<doc id="47639" url="https://en.wikipedia.org/wiki?curid=47639" title="Liane Gabora">
Liane Gabora

Liane Gabora is a professor of psychology at the University of British Columbia - Okanagan. She is best known for her theory of the "Origin of the modern mind through conceptual closure." This built on her earlier work on "Autocatalytic closure in a cognitive system: A tentative scenario for the origin of culture."
Career.
Gabora has contributed to the study of cultural evolution and evolution of societies, focusing strongly on the role of personal creativity, as opposed to memetic imitation or instruction, in differentiating modern human from prior hominid or modern ape culture. In particular, she seems to follow feminist economists and green economists in making a very strong, indeed pivotal, distinction between creative "enterprise", invention, art or "individual capital" and imitative "meme", rule, social category or "instructional capital". 
Gabora's views contrasts with that of memetics and of the strongest social capital theorists (e.g. Karl Marx or Paul Adler) in that she seems to see, as do theorists of intellectual capital, social signals or labels as markers of trust already invested in individual and instructional complexes - rather than as first class actors in themselves. She puts special emphasis on quantifiable archaeological data, such as the number of different arrow points styles, than on contemporary observations to minimize cultural bias and notational bias.
Some of her recent work raises extremely controversial themes in philosophy of science and strongly challenges the particle physics foundation ontology (e.g. studying the "violation of Bell inequalities in the macroworld"). She is also known for her contributions to the subtle technology field.
Sources.
Gabora, L. (2010). Revenge of the 'neurds': Characterizing creative thought in terms of the structure and dynamics of human memory. Creativity Research Journal, 22(1), 1-13.

</doc>
<doc id="47640" url="https://en.wikipedia.org/wiki?curid=47640" title="British Sign Language">
British Sign Language

British Sign Language (BSL) is the sign language used in the United Kingdom (UK), and is the first or preferred language of some deaf people in the UK; there are 125,000 deaf adults in the UK who use BSL plus an estimated 20,000 children. In 2011, 15,000 people, living in England and Wales, reported themselves using BSL as their main language. The language makes use of space and involves movement of the hands, body, face and head. Many thousands of people who are not deaf also use BSL, as hearing relatives of deaf people, sign language interpreters or as a result of other contact with the British deaf community.
History.
Records exist of a sign language existing within deaf communities in England as far back as 1570. British Sign Language has evolved, as all languages do, from these origins by modification, invention and importation. Thomas Braidwood, an Edinburgh teacher, founded 'Braidwood's Academy for the Deaf and Dumb' in 1760 which is recognised as the first school for the deaf in Britain. His pupils were the sons of the well-to-do. His early use of a form of sign language, "the combined system", was the first codification of what was to become British Sign Language. Joseph Watson was trained as a teacher of the Deaf under Thomas Braidwood and he eventually left in 1792 to become the headmaster of the first public school for the Deaf in Britain, the London Asylum for the Deaf and Dumb in Bermondsey.
In 1815, an American Protestant minister, Thomas Hopkins Gallaudet, travelled to Europe to research teaching of the deaf. He was rebuffed by both the Braidwood schools who refused to teach him their methods. Gallaudet then travelled to Paris and learned the educational methods of the French Royal Institution for the Deaf, a combination of Old French Sign Language and the signs developed by Abbé de l’Épée. As a consequence American Sign Language today has a 60% similarity to modern French Sign Language and is almost unintelligible to users of British Sign Language.
Until the 1940s sign language skills were passed on unofficially between deaf people often living in residential institutions. Signing was actively discouraged in schools by punishment and the emphasis in education was on forcing deaf children to learn to lip read and finger spell. From the 1970s there has been an increasing tolerance and instruction in BSL in schools. The language continues to evolve as older signs such as "alms" and "pawnbroker" have fallen out of use and new signs such as "internet" and "laser" have been coined. The evolution of the language and its changing level of acceptance means that older users tend to rely on finger spelling while younger ones make use of a wider range of signs.
On March 18, 2003 the UK government formally recognized that BSL is a language in its own right.
Linguistics.
Phonology.
Like many other sign languages, BSL phonology is defined by elements such as hand shape, orientation, location, and motion.
Grammar.
BSL uses a topic–comment structure. Canonical word order outside of topic–comment structure is OSV, and noun phrases are head-initial.
Relationships with other sign languages.
Although the United Kingdom and the United States share English as the predominant oral language, British Sign Language is quite distinct from American Sign Language (ASL) - having only 31% signs identical, or 44% cognate. BSL is also distinct from Irish Sign Language (ISL) (ISG in the ISO system) which is more closely related to French Sign Language (LSF) and ASL.
It is also distinct from Signed English, a manually coded method expressed to represent the English language.
The sign languages used in Australia and New Zealand, Auslan and New Zealand Sign Language, respectively, evolved largely from 19th century BSL, and all retain the same manual alphabet and grammar and possess similar lexicons. These three languages may technically be considered dialects of a single language (BANZSL) due to their use of the same grammar and manual alphabet and the high degree of lexical sharing (overlap of signs). The term BANZSL was coined by Trevor Johnston and Adam Schembri.
In Australia deaf schools were established by educated deaf people from London, Edinburgh and Dublin. This introduced the London and Edinburgh dialects of BSL to Melbourne and Sydney respectively and Irish Sign Language to Sydney in Roman Catholic schools for the deaf. The language contact post secondary education between Australian ISL users and 'Australian BSL' users accounts for some of the dialectal differences we see between modern BSL and Auslan. Tertiary education in the US for some deaf Australian adults also accounts for some ASL borrowings found in modern Auslan.
Auslan, BSL and NZSL have 82% of signs identical (using concepts from a Swadesh list). When considering similar or related signs as well as identical, they are 98% cognate. Further information will be available after the completion of the BSL corpus is completed and allows for comparison with the Auslan corpus and the Sociolinguistic Variation in New Zealand Sign Language project . There continues to be language contact between BSL, Auslan and NZSL through migration (deaf people and interpreters), the media (television programmes such as See Hear, Switch, Rush and SignPost are often recorded and shared informally in all three countries) and conferences (the World Federation of the Deaf Conference – WFD – in Brisbane 1999 saw many British deaf people travelling to Australia).
Makaton, a communication system for people with cognitive impairments or other communication difficulties, was originally developed with signs borrowed from British Sign Language. The sign language used in Sri Lanka is also closely related to BSL despite the oral language not being English, demonstrating the distance between sign languages and spoken ones.
BSL users campaigned to have BSL recognised on a similar level to Welsh, Scottish Gaelic, and Irish. BSL was recognised as a language in its own right by the UK government on 18 March 2003, but it has no legal protection. There is however legislation requiring the provision of interpreters such as the Police and Criminal Evidence Act 1984.
Usage.
BSL has many regional dialects. Signs used in Scotland, for example, may not be used, and may not be understood immediately by those in Southern England, and vice versa. Some signs are even more local, occurring only in certain towns or cities (such as the Manchester system of number signs). Likewise, some may go in or out of fashion, or evolve over time, just as terms in oral languages do.
Many British television channels broadcast programmes with in-vision signing, using BSL, as well as specially made programmes aimed mainly at deaf people such as the BBC's "See Hear" and Channel 4's "VEE-TV".
BBC News broadcasts in-vision signing at 07:00-07:45, 08:00-08:20 and 13:00-13:45 GMT/BST each weekday. BBC One also broadcasts in-vision signed repeats of the channel's primetime programmes between 00:30 and 04:00 each weekday. All BBC channels (excluding BBC Alba and BBC Parliament) provide in-vision signing for some of their programmes.
BSL is used in some educational establishments, but is not always the policy for deaf children in some local authority areas. The Let's Sign BSL and fingerspelling graphics are being developed for use in education by deaf educators and tutors and include many of the regional signs referred to above.
Learning British Sign Language.
British Sign Language can be learnt throughout the UK and four examination systems exist. Courses are provided by community colleges, local centres for deaf people and private organisations. Most tutors are native users of sign language and hold a relevant teaching qualification.
Signature is an awarding body accredited by the Qualifications and Curriculum Authority (QCA) who provide the following qualifications:
The British Deaf Association has formed the British Sign Language Academy to provide an official British Sign Language curriculum and tutor training.
iBSL also award language qualifications: a Level 1 Award and Level 2, 3, 4 and 6 Certificates.
In Scotland, there is a Scottish Qualifications Authority (SQA) system for students learning British Sign Language. Currently there are 5 levels in the SQA system (continuing assessments):
Becoming a BSL / English interpreter.
There are two qualification routes: via post-graduate studies, or via National Vocational Qualifications. Deaf Studies undergraduate courses with specific streams for sign language interpreting exist at several British universities; post-graduate level interpreting diplomas are also on offer from universities and one private company. Course entry requirements vary from no previous knowledge of BSL to NVQ level 6 BSL (or equivalent). The alternative to university studies are either NVQ language and interpreting courses on offer from Signature or IBSL language qualifications followed by an interpreting qualification which is mapped against the CILT National Occupational Standards for Interpreting.
The qualification process allows interpreters to register with the National Registers of Communication Professionals with Deaf and Deafblind People (NRCPD), a voluntary regulator. Registrants are asked to self-certify that they have both cleared a DBS (Disclosure and Barring Service) check and are covered by professional indemnity insurance. Completing a level 3 BSL language assessment and enrolling on an approved interpreting course allows applications to register as a TSLI (Trainee Sign Language Interpreter). After completing an approved interpreting course, trainees can then apply to achieve RSLI (Registered Sign Language Interpreter) status. RSLIs are currently required by NRCPD to log Continuous Professional Development activities. Post-qualification, specialist training is still considered necessary to work in specific critical domains.
Both the Association of Sign Language Interpreters and Visual Language Professionals provide a network of regional groups, professional development opportunities and mentoring. These membership organisations represent the sign language interpreting profession in England, Wales and Northern Ireland and provide interpreters with professional indemnity insurance.
Communication Support Workers.
Communication Support Workers (CSWs) are professionals who support the communication of deaf students in education at all ages, and deaf people in many areas of work, using British Sign Language and other communication methods such as Sign Supported English. The Association of Deaf Education Professionals and Trainees (ADEPT) is a national association, formed from a merger of ACSW and NATED in 2014, that supports and represents the interests and views of CSWs, encourages good practice and aims to improve the training standards and opportunities for current and future CSWs, among other things. The Association provides a professional network, improving information exchange, professional standards and support. The qualifications and experience of CSWs varies: some are fully qualified interpreters, others are not. There is a Level 3 Certificate in Communication Support for Deaf Learners available from Signature; this qualification is modelled on standards for learning support in Further Education only and is not required by all employers.
Let Sign Shine.
"Let Sign Shine" is a campaign started by Norfolk teenager Jade Chapman to raise the awareness of British Sign Language (BSL) and attract signatures for a petition for BSL to be taught in schools. The campaign's petition to the Parliament of the United Kingdom has attracted support from over four thousand people.
Let Sign Shine campaigner Chapman was nominated for the Bernard Matthews Youth Award 2014 for her work and devotion to raising awareness of the importance of sign language. Chapman won the education award category and was presented with an award by Olympic Swimmer Rebecca Adlington.
Let Sign Shine campaigner Chapman was also awarded an Outstanding Achievement Award from the Radio Norwich 99.9 Local Hero Awards on 7 October 2015. The award ceremony featured a performance by Alesha Dixon.
Having been donated £1,000 from the Bernard Matthews Youth Award, Let Sign Shine used this to start a British Sign Language course at Dereham Neatherd High School.

</doc>
<doc id="47641" url="https://en.wikipedia.org/wiki?curid=47641" title="Standard Model">
Standard Model

The Standard Model of particle physics is a theory concerning the electromagnetic, weak, and strong nuclear interactions, as well as classifying all the subatomic particles known. It was developed throughout the latter half of the 20th century, as a collaborative effort of scientists around the world. The current formulation was finalized in the mid-1970s upon experimental confirmation of the existence of quarks. Since then, discoveries of the top quark (1995), the tau neutrino (2000), and more recently the Higgs boson (2012), have given further credence to the Standard Model. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as the "theory of almost everything".
Although the Standard Model is believed to be theoretically self-consistent and has demonstrated huge and continued successes in providing experimental predictions, it does leave some phenomena unexplained and it falls short of being a complete theory of fundamental interactions. It does not incorporate the full theory of gravitation as described by general relativity, or account for the accelerating expansion of the universe (as possibly described by dark energy). The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations (and their non-zero masses).
The development of the Standard Model was driven by theoretical and experimental particle physicists alike. For theorists, the Standard Model is a paradigm of a quantum field theory, which exhibits a wide range of physics including spontaneous symmetry breaking, anomalies and non-perturbative behavior. It is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) in an attempt to explain experimental results at variance with the Standard Model, such as the existence of dark matter and neutrino oscillations.
Historical background.
The first step towards the Standard Model was Sheldon Glashow's discovery in 1961 of a way to combine the electromagnetic and weak interactions. In 1967 Steven Weinberg and Abdus Salam incorporated the Higgs mechanism into Glashow's electroweak interaction, giving it its modern form.
The Higgs mechanism is believed to give rise to the masses of all the elementary particles in the Standard Model. This includes the masses of the W and Z bosons, and the masses of the fermions, i.e. the quarks and leptons.
After the neutral weak currents caused by Z boson exchange were discovered at CERN in 1973, the electroweak theory became widely accepted and Glashow, Salam, and Weinberg shared the 1979 Nobel Prize in Physics for discovering it. The W and Z bosons were discovered experimentally in 1981, and their masses were found to be as the Standard Model predicted.
The theory of the strong interaction, to which many contributed, acquired its modern form around 1973–74, when experiments confirmed that the hadrons were composed of fractionally charged quarks.
Overview.
At present, matter and energy are best understood in terms of the kinematics and interactions of elementary particles. To date, physics has reduced the laws governing the behavior and interaction of all known forms of matter and energy to a small set of fundamental laws and theories. A major goal of physics is to find the "common ground" that would unite all of these theories into one integrated theory of everything, of which all the other known laws would be special cases, and from which the behavior of all matter and energy could be derived (at least in principle).
Particle content.
The Standard Model includes members of several classes of elementary particles (fermions, gauge bosons, and the Higgs boson), which in turn can be distinguished by other characteristics, such as color charge.
Fermions.
The Standard Model includes 12 elementary particles of spin-½ known as fermions. According to the spin-statistics theorem, fermions respect the Pauli exclusion principle. Each fermion has a corresponding antiparticle.
The fermions of the Standard Model are classified according to how they interact (or equivalently, by what charges they carry). There are six quarks (up, down, charm, strange, top, bottom), and six leptons (electron, electron neutrino, muon, muon neutrino, tau, tau neutrino). Pairs from each classification are grouped together to form a generation, with corresponding particles exhibiting similar physical behavior (see table).
The defining property of the quarks is that they carry color charge, and hence, interact via the strong interaction. A phenomenon called color confinement results in quarks being very strongly bound to one another, forming color-neutral composite particles (hadrons) containing either a quark and an antiquark (mesons) or three quarks (baryons). The familiar proton and the neutron are the two baryons having the smallest mass. Quarks also carry electric charge and weak isospin. Hence they interact with other fermions both electromagnetically and via the weak interaction.
The remaining six fermions do not carry colour charge and are called leptons. The three neutrinos do not carry electric charge either, so their motion is directly influenced only by the weak nuclear force, which makes them notoriously difficult to detect. However, by virtue of carrying an electric charge, the electron, muon, and tau all interact electromagnetically.
Each member of a generation has greater mass than the corresponding particles of lower generations. The first generation charged particles do not decay; hence all ordinary (baryonic) matter is made of such particles. Specifically, all atoms consist of electrons orbiting around atomic nuclei, ultimately constituted of up and down quarks. Second and third generation charged particles, on the other hand, decay with very short half lives, and are observed only in very high-energy environments. Neutrinos of all generations also do not decay, and pervade the universe, but rarely interact with baryonic matter.
Gauge bosons.
In the Standard Model, gauge bosons are defined as force carriers that mediate the strong, weak, and electromagnetic fundamental interactions.
Interactions in physics are the ways that particles influence other particles. At a macroscopic level, electromagnetism allows particles to interact with one another via electric and magnetic fields, and gravitation allows particles with mass to attract one another in accordance with Einstein's theory of general relativity. The Standard Model explains such forces as resulting from matter particles exchanging other particles, generally referred to as "force mediating particles". When a force-mediating particle is exchanged, at a macroscopic level the effect is equivalent to a force influencing both of them, and the particle is therefore said to have "mediated" (i.e., been the agent of) that force. The Feynman diagram calculations, which are a graphical representation of the perturbation theory approximation, invoke "force mediating particles", and when applied to analyze high-energy scattering experiments are in reasonable agreement with the data. However, perturbation theory (and with it the concept of a "force-mediating particle") fails in other situations. These include low-energy quantum chromodynamics, bound states, and solitons.
The gauge bosons of the Standard Model all have spin (as do matter particles). The value of the spin is 1, making them bosons. As a result, they do not follow the Pauli exclusion principle that constrains fermions: thus bosons (e.g. photons) do not have a theoretical limit on their spatial density (number per volume). The different types of gauge bosons are described below.
The interactions between all the particles described by the Standard Model are summarized by the diagrams on the right of this section.
Higgs boson.
The Higgs particle is a massive scalar elementary particle theorized by Robert Brout, François Englert, Peter Higgs, Gerald Guralnik, C. R. Hagen, and Tom Kibble in 1964 (see 1964 PRL symmetry breaking papers) and is a key building block in the Standard Model. It has no intrinsic spin, and for that reason is classified as a boson (like the gauge bosons, which have integer spin).
The Higgs boson plays a unique role in the Standard Model, by explaining why the other elementary particles, except the photon and gluon, are massive. In particular, the Higgs boson explains why the photon has no mass, while the W and Z bosons are very heavy. Elementary particle masses, and the differences between electromagnetism (mediated by the photon) and the weak force (mediated by the W and Z bosons), are critical to many aspects of the structure of microscopic (and hence macroscopic) matter. In electroweak theory, the Higgs boson generates the masses of the leptons (electron, muon, and tau) and quarks. As the Higgs boson is massive, it must interact with itself.
Because the Higgs boson is a very massive particle and also decays almost immediately when created, only a very high-energy particle accelerator can observe and record it. Experiments to confirm and determine the nature of the Higgs boson using the Large Hadron Collider (LHC) at CERN began in early 2010, and were performed at Fermilab's Tevatron until its closure in late 2011. Mathematical consistency of the Standard Model requires that any mechanism capable of generating the masses of elementary particles becomes visible at energies above ; therefore, the LHC (designed to collide two 7 to 8 TeV proton beams) was built to answer the question of whether the Higgs boson actually exists.
On 4 July 2012, the two main experiments at the LHC (ATLAS and CMS) both reported independently that they found a new particle with a mass of about (about 133 proton masses, on the order of 10−25 kg), which is "consistent with the Higgs boson." Although it has several properties similar to the predicted "simplest" Higgs, they acknowledged that further work would be needed to conclude that it is indeed the Higgs boson, and exactly which version of the Standard Model Higgs is best supported if confirmed.
On 14 March 2013 the Higgs Boson was tentatively confirmed to exist.
Total particle count.
Counting particles by a rule that distinguishes between particles and their corresponding antiparticles, and among the many color states of quarks and gluons, gives a total of 61 elementary particles. (If neutrinos are their own antiparticles, then by the same counting conventions the total number of elementary particles would be 58.)
Theoretical aspects.
Construction of the Standard Model Lagrangian.
Technically, quantum field theory provides the mathematical framework for the Standard Model, in which a Lagrangian controls the dynamics and kinematics of the theory. Each kind of particle is described in terms of a dynamical field that pervades space-time. The construction of the Standard Model proceeds following the modern method of constructing most field theories: by first postulating a set of symmetries of the system, and then by writing down the most general renormalizable Lagrangian from its particle (field) content that observes these symmetries.
The global Poincaré symmetry is postulated for all relativistic quantum field theories. It consists of the familiar translational symmetry, rotational symmetry and the inertial reference frame invariance central to the theory of special relativity. The local SU(3)×SU(2)×U(1) gauge symmetry is an internal symmetry that essentially defines the Standard Model. Roughly, the three factors of the gauge symmetry give rise to the three fundamental interactions. The fields fall into different representations of the various symmetry groups of the Standard Model (see table). Upon writing the most general Lagrangian, one finds that the dynamics depend on 19 parameters, whose numerical values are established by experiment. The parameters are summarized in the table above (note: with the Higgs mass is at 125 GeV, the Higgs self-coupling strength "λ" ~ 1/8).
Quantum chromodynamics sector.
The quantum chromodynamics (QCD) sector defines the interactions between quarks and gluons, with SU(3) symmetry, generated by Ta. Since leptons do not interact with gluons, they are not affected by this sector. The Dirac Lagrangian of the quarks coupled to the gluon fields is given by
formula_2 is the SU(3) gauge field containing the gluons, formula_3 are the Dirac matrices, D and U are the Dirac spinors associated with up- and down-type quarks, and gs is the strong coupling constant.
Electroweak sector.
The electroweak sector is a Yang–Mills gauge theory with the simple symmetry group U(1)×SU(2)L,
where "B""μ" is the U(1) gauge field; "Y"W is the weak hypercharge—the generator of the U(1) group; formula_5 is the
three-component SU(2) gauge field; formula_6 are the Pauli matrices—infinitesimal generators of the SU(2) group. The subscript L indicates that they only act on left fermions; "g"′ and "g" are coupling constants.
Higgs sector.
In the Standard Model, the Higgs field is a complex scalar of the group SU(2)L:
where the indices + and 0 indicate the electric charge ("Q") of the components. The weak isospin ("Y"W) of both components is 1.
Before symmetry breaking, the Higgs Lagrangian is:
which can also be written as:
Fundamental forces.
The Standard Model classified all four fundamental forces in nature. In the Standard Model, a force is described as an exchange of bosons between the objects affected, such as a photon for the electromagnetic force and a gluon for the strong interaction. Those particles are called force carriers.
Tests and predictions.
The Standard Model (SM) predicted the existence of the W and Z bosons, gluon, and the top and charm quarks before these particles were observed. Their predicted properties were experimentally confirmed with good precision. To give an idea of the success of the SM, the following table compares the measured masses of the W and Z bosons with the masses predicted by the SM:
The SM also makes several predictions about the decay of Z bosons, which have been experimentally confirmed by the Large Electron-Positron Collider at CERN.
In May 2012 BaBar Collaboration reported that their recently analyzed data may suggest possible flaws in the Standard Model of particle physics. These data show that a particular type of particle decay called "B to D-star-tau-nu" happens more often than the Standard Model says it should. In this type of decay, a particle called the B-bar meson decays into a D meson, an antineutrino and a tau-lepton.
While the level of certainty of the excess (3.4 sigma) is not enough to claim a break from the Standard Model, the results are a potential sign of something amiss and are likely to impact existing theories, including those attempting to deduce the properties of Higgs bosons.
On December 13, 2012, physicists reported the constancy, over space and time, of a basic physical constant of nature that supports the "standard model of physics". The scientists, studying methanol molecules in a distant galaxy, found the change (∆μ/μ) in the proton-to-electron mass ratio μ to be equal to "(0.0 ± 1.0) × 10−7 at redshift z = 0.89" and consistent with "a null result".
Challenges.
Self-consistency of the Standard Model (currently formulated as a non-abelian gauge theory quantized through path-integrals) has not been mathematically proven. While regularized versions useful for approximate computations (for example lattice gauge theory) exist, it is not known whether they converge (in the sense of S-matrix elements) in the limit that the regulator is removed. A key question related to the consistency is the Yang–Mills existence and mass gap problem.
Experiments indicate that neutrinos have mass, which the classic Standard Model did not allow. To accommodate this finding, the classic Standard Model can be modified to include neutrino mass.
If one insists on using only Standard Model particles, this can be achieved by adding a non-renormalizable interaction of leptons with the Higgs boson. On a fundamental level, such an interaction emerges in the seesaw mechanism where heavy right-handed neutrinos are added to the theory.
This is natural in the left-right symmetric extension of the Standard Model and in certain grand unified theories. As long as new physics appears below or around 1014 GeV, the neutrino masses can be of the right order of magnitude.
Theoretical and experimental research has attempted to extend the Standard Model into a Unified field theory or a Theory of everything, a complete theory explaining all physical phenomena including constants. Inadequacies of the Standard Model that motivate such research include:
Currently, no proposed Theory of Everything has been widely accepted or verified.

</doc>
<doc id="47642" url="https://en.wikipedia.org/wiki?curid=47642" title="Castling">
Castling

Castling is a move in the game of chess involving a player's king and either of the player's original rooks. It is the only move in chess in which a player moves two pieces in the same move, and it is the only move aside from the knight's move where a piece can be said to "jump over" another.
Castling consists of moving the king two squares towards a rook on the player's first rank, then moving the rook to the square over which the king crossed. Castling may only be done if the king has never moved, the rook involved has never moved, the squares between the king and the rook involved are unoccupied, the king is not in check, and the king does not cross over or end on a square in which it would be in check. Castling is one of the rules of chess and is technically a king move .
The notation for castling, in both the descriptive and the algebraic systems, is 0-0 with the kingside rook and 0-0-0 with the queenside rook; in PGN, O-O and O-O-O are used instead. Castling on the kingside is sometimes called "castling short" and castling on the queenside is called "castling long" – the difference based on whether the rook moves a short distance (two squares) or a long distance (three squares) .
Castling was added to European chess in the 14th or 15th century and did not develop into its present form until the 17th century. The Asian versions of chess do not have such a move.
Requirements.
Castling is permissible if and only if all of the following conditions hold :
Conditions 4 through 6 can be summarized with the more memorable phrase: "One may not castle out of, through, or into check."
It is a common misperception that the requirements for castling are even more stringent than the above. To clarify:
In handicap games where odds of a rook are given, the player giving odds may still castle with the absent rook, moving only the king.
Strategy.
Castling is an important goal in the opening, because it serves two valuable purposes: it moves the king into a safer position away from the center of the board, and it moves the rook to a more active position in the center of the board (it is even possible to checkmate with castling).
The choice as to which side to castle often hinges on an assessment of the trade-off between king safety and activity of the rook. Kingside castling is generally slightly safer, because the king ends up closer to the edge of the board and all the pawns on the castled side are defended by the king. In queenside castling, the king is placed closer to the center and the pawn on the a-file is undefended; the king is thus often moved to the b-file to defend the a-pawn and to move the king away from the center of the board. In addition, queenside castling requires moving the queen; therefore, it may take slightly longer to achieve than kingside castling. On the other hand, queenside castling places the rook more efficiently – on the central d-file. It is often immediately active, whereas with kingside castling a tempo may be required to move the rook to a more efficient square.
It is common for both players to castle kingside, and rare for both players to castle queenside. If one player castles kingside and the other queenside, it is called "opposite" (or "opposite-side") "castling". Castling on opposite sides usually results in a fierce fight as both players' pawns are free to advance to attack the opposing king's castled position without exposing the player's own castled king. An example is the Yugoslav Attack, in the Dragon Variation of the Sicilian Defence.
If the king is forced to move before it has the opportunity to castle, the player may still wish to maneuver the king towards the edge of the board and the corresponding rook towards the center. When a player takes three or four moves to accomplish what castling would have accomplished in one move, it is sometimes called "artificial castling", or "castling by hand".
Tournament rules.
Under the strict touch-move rules enforced in most tournaments, castling is considered a king move. But under current US Chess Federation rules, a player who intends to castle and touches the rook first would suffer no penalty, and would be permitted to perform castling, provided castling is legal in the position. Still, the correct way to castle is to first move the king. As usual, the player's mind may change between all legal destination squares for the king until it is released. When the two-square king move is completed, however, the player has formally chosen to castle (if it is legal), and the rook must be moved accordingly. A player who performs a forbidden castling must return the king and the rook to their original places and then move the king, if there is another legal king move, including castling on the other side. If there is no legal king move, the touch-move rule does "not" apply to the rook .
It is also required by the official rules that the entire move be completed using only a single hand. Neither of these rules is commonly enforced in casual play, nor commonly known by non-competitive players .
The right to castle must be the same in all three positions for a valid draw claim under the threefold repetition rule.
Chess variants and problems.
Some chess variants, for example Chess960, have modified castling rules to handle modified starting positions. Castling can also be adapted to large chess variants, like Capablanca chess, which is played on a 10×8 board.
In chess problems, castling is assumed to be allowed if it appears possible, unless it can be proved by retrograde analysis that either the king or chosen rook has previously moved.
Notable examples.
Averbakh game.
In this game between Yuri Averbakh and Cecil Purdy, Black castled queenside. Averbakh pointed out that the rook passed over a square controlled by White and thought it was illegal. Purdy proved that the castling was legal since this applies only to the king, to which Averbakh replied "Only the king? Not the rook?" , .
Lasker game.
In this game between Edward Lasker and Sir George Thomas (London 1912), Black had just played 17...Kg1. White might have checkmated by 18.0-0-0# but instead played 18.Kd2#. (See Edward Lasker's notable games.)
Prins versus Day.
This game between Lodewijk Prins and Lawrence Day ended in a checkmate by castling: 31...0-0-0#. (See Lawrence Day' notable chess games.)
Feuer versus O'Kelly.
In the game Feuer–O'Kelly, Belgian Championship 1934, Feuer perpetrated what later became known as a famous opening trap against O'Kelly when he castled queenside with check, simultaneously attacking and winning O'Kelly's rook on b2, which had captured Feuer's pawn on that square.
History.
Castling has its roots in the "king's leap". There were two forms of the leap: (1) the king would move once like a knight, and (2) the king would move two squares on his first move. The knight-move might be used early in the game to get the king to safety or later in the game to escape a threat. This second form was played in Europe as early as the 13th Century. In North Africa, the king was moved to a safe square by a two-step procedure: (1) the king moved to the second rank and (2) the rook moved to the king's original square and the king moved to the rook's original square .
Before the bishop and queen acquired their current moves in the 16th century they were weak pieces and the king was relatively safe in the middle of the board. When the bishop and queen got their current moves they became very powerful and the king was no longer safe on its original square, since it can be attacked from a distance and from both sides. Castling was added to allow the king to get to a safer location and to allow rooks to get into the game earlier .
The rule of castling has varied by location and time. In medieval England, Spain, and France, the white king was allowed to jump to c1, c2, d3, e3, f3, or g1, if no capture was made, the king was not in check, and did not move over check. (The black king might move similarly.) In Lombardy, the white king might jump an additional square to b1 or h1 or to a2 (and equivalent squares for the black king). Later in Germany and Italy, the king move was combined with a pawn move.
In Rome from the early 17th century until the late 19th century, the rook might be placed on any square up to and including the king's square, and the king might be moved to any square on the other side of the rook. This was called "free castling".
In the Göttingen manuscript (c. 1500) and a game published by Luis Ramírez de Lucena in 1498, castling consisted of two moves: first the rook and then the king.
The current version of castling was established in France in 1620 and England in 1640 .
In the 1811 edition of his chess treatise, Johann Allgaier introduced the 0-0 notation. He differentiated between "0-0r" (r=right) and "0-0l" (l=left). The 0-0-0 notation for queenside castling was added in 1837 by Aaron Alexandre. The practice was then accepted in the first edition (1843) of the "Handbuch des Schachspiels".
Nomenclature.
Castling is in most European languages other than English known as "roszada", "rochieren", "rochada", "rocada", "enroc", "enroque", "rošáda", "arrocco", "rokada", "рокировка", or some other derivative of the same root (from which also the English word "rook" is derived), while the local adjectives meaning "long" and "short" (or "big" and "small") are used in those countries to refer to queenside and kingside castling, respectively.
References.
Bibliography

</doc>
<doc id="47643" url="https://en.wikipedia.org/wiki?curid=47643" title="Michel Foucault">
Michel Foucault

Michel Foucault (; born Paul-Michel Foucault, 15 October 1926 – 25 June 1984) was a French philosopher, historian of ideas, social theorist, philologist and literary critic. His theories addressed the relationship between power and knowledge, and how they are used as a form of social control through societal institutions. Though often cited as a post-structuralist and postmodernist, Foucault rejected these labels, preferring to present his thought as a critical history of modernity. His thought has been highly influential both for academic and for activist groups, such as within post-anarchism.
Born in Poitiers, France, into an upper-middle-class family, Foucault was educated at the Lycée Henri-IV and then at the École Normale Supérieure, where he developed an interest in philosophy and came under the influence of his tutors Jean Hyppolite and Louis Althusser. After several years as a cultural diplomat abroad, he returned to France and published his first major book, "The History of Madness". After obtaining work between 1960 and 1966 at the University of Clermont-Ferrand, he produced two more significant publications, "The Birth of the Clinic" and "The Order of Things", which displayed his increasing involvement with structuralism, a theoretical movement in social anthropology from which he later distanced himself. These first three histories exemplified a historiographical technique Foucault was developing called "archaeology".
From 1966 to 1968, Foucault lectured at the University of Tunis, Tunisia, before returning to France, where he became head of the philosophy department at the new experimental university of Paris VIII. In 1970 he was admitted to the Collège de France, membership of which he retained until his death. He also became active in a number of left-wing groups involved in anti-racist campaigns, anti-human rights abuses movements, and the struggle for penal reform. He went on to publish "The Archaeology of Knowledge", "Discipline and Punish", and "The History of Sexuality". In these books he developed archaeological and genealogical methods which emphasized the role which power plays in the evolution of discourse in society. Foucault died in Paris of neurological problems compounded by HIV/AIDS; he became the first public figure in France to die from the disease, and his partner Daniel Defert founded the AIDES charity in his memory.
Early life.
Youth: 1926–1946.
Paul-Michel Foucault was born on 15 October 1926 in the city of Poitiers, west-central France, as the second of three children to a prosperous and socially conservative upper-middle-class family.
He had been named after his father, Dr. Paul Foucault, as was the family tradition, but his mother insisted on the addition of the double-barrelled "Michel"; referred to as "Paul" at school, throughout his life he always expressed a preference for "Michel".
His father (1893–1959) was a successful local surgeon, having been born in Fontainebleau before moving to Poitiers, where he set up his own practice and married local woman Anne Malapert. She was the daughter of prosperous surgeon Dr. Prosper Malapert, who owned a private practice and taught anatomy at the University of Poitiers' School of Medicine. Paul Foucault eventually took over his father-in-law's medical practice, while his wife took charge of their large mid-19th-century house, Le Piroir, in the village of Vendeuvre-du-Poitou. Together the couple had three children, a girl named Francine and two boys, Paul-Michel and Denys, all of whom shared the same fair hair and bright blue eyes. The children were raised to be nominal Roman Catholics, attending mass at the Church of Saint-Porchair, and while Michel briefly became an altar boy, none of the family were devout.
In later life, Foucault would reveal very little about his childhood. Describing himself as a "juvenile delinquent", he claimed his father was a "bully" who would sternly punish him. In 1930, Foucault began his schooling two years early at the local Lycée Henry-IV. Here he undertook two years of elementary education before entering the main "lycée", where he stayed until 1936. He then undertook his first four years of secondary education at the same establishment, excelling in French, Greek, Latin and history but doing poorly at arithmetic and mathematics. In 1939, the Second World War broke out and France was occupied by Nazi Germany until 1945; his parents opposed the occupation and the Vichy regime, but did not join the Resistance. In 1940, Foucault's mother enrolled him in the Collège Saint-Stanislas, a strict Roman Catholic institution run by the Jesuits. Lonely, he described his years there as the "ordeal", but excelled academically, particularly in philosophy, history and literature. In 1942, he entered his final year, the "terminale", where he focused on the study of philosophy, earning his "baccalauréat" in 1943.
Returning to the local Lycée Henry-IV, he studied history and philosophy for a year, aided by a personal tutor, the philosopher Louis Girard. Rejecting his father's wishes that he become a surgeon, in 1945 Foucault traveled to Paris, where he enrolled in one of the country's most prestigious secondary schools, which was also known as the Lycée Henri-IV. Here, he studied under the philosopher Jean Hyppolite, an existentialist and expert on the work of 19th-century German philosopher Hegel who had devoted himself to uniting existentialist theories with the dialectical theories of Hegel and Karl Marx. These ideas influenced Foucault, who adopted Hyppolite's conviction that philosophy must be developed through a study of history.
École Normale Supérieure: 1946–1951.
Attaining excellent results, in autumn 1946 Foucault was admitted to the elite École Normale Supérieure (ENS); to gain entry, he undertook exams and an oral interrogation by Georges Canguilhem and Pierre-Maxime Schuhl. Of the hundred students entering the ENS, Foucault was ranked fourth based on his entry results, and encountered the highly competitive nature of the institution. Like most of his classmates, he was housed in the school's communal dormitories on the Parisian Rue d'Ulm.
He remained largely unpopular, spending much time alone, reading voraciously. His fellow students noted his love of violence and the macabre; he decorated his bedroom with images of torture and war drawn during the Napoleonic Wars by Spanish artist Francisco Goya, and on one occasion chased a classmate with a dagger. Prone to self-harm, in 1948 Foucault allegedly undertook a failed suicide attempt, for which his father sent him to see the psychiatrist Jean Delay at the Sainte-Anne Hospital (). Obsessed with the idea of self-mutilation and suicide, Foucault attempted the latter several times in ensuing years, praising suicide in later writings. The ENS's doctor examined Foucault's state of mind, suggesting that his suicidal tendencies emerged from the distress surrounding his homosexuality, because same-sex sexual activity was socially taboo in France. At the time, Foucault engaged in homosexual activity with men whom he encountered in the underground Parisian gay scene, also indulging in drug use; according to biographer James Miller, he enjoyed the thrill and sense of danger that these activities offered him.
Although studying various subjects, Foucault's particular interest was soon drawn to philosophy, reading not only Hegel and Marx but also Immanuel Kant, Edmund Husserl and most significantly, Martin Heidegger. He began reading the publications of philosopher Gaston Bachelard, taking a particular interest in his work exploring the history of science. He graduated from the ENS with a DES ("", roughly equivalent to an MA) in Philosophy in 1949. His DES thesis under the direction of Hyppolite was titled "La Constitution d'un transcendental dans La Phénoménologie de l'esprit de Hegel" ("The Constitution of a Historical Transcendental in Hegel's Phenomenology of Spirit").
In 1948, the philosopher Louis Althusser became a tutor at the ENS. A Marxist, he proved to be an influence both on Foucault and a number of other students, encouraging them to join the French Communist Party ("Parti communiste français", PCF). Foucault did so in 1950, but never became particularly active in its activities, and never adopted an orthodox Marxist viewpoint, refuting core Marxist tenets such as class struggle. He soon became dissatisfied with the bigotry that he experienced within the party's ranks; he personally faced homophobia and was appalled by the anti-semitism exhibited during the Doctors' plot in the Soviet Union. He left the Communist Party in 1953, but remained Althusser's friend and defender for the rest of his life. Although failing at the first attempt in 1950, he passed his "agrégation" in philosophy on the second try, in 1951. Excused from national service on medical grounds, he decided to study for a doctorate at the Fondation Thiers, focusing on the philosophy of psychology.
Early career: 1951–1955.
Over the following few years, Foucault embarked on a variety of research and teaching jobs. From 1951 to 1955, he worked as a psychology instructor at the ENS at Althusser's invitation. In Paris, he shared a flat with his brother, who was training to become a surgeon, but for three days in the week commuted to the northern town of Lille, teaching psychology at the Université de Lille from 1953 to 1954. Many of his students liked his lecturing style. Meanwhile, he continued working on his thesis, visiting the Bibliothèque Nationale every day to read the work of psychologists like Ivan Pavlov, Jean Piaget and Karl Jaspers. Undertaking research at the psychiatric institute of the Sainte-Anne Hospital, he became an unofficial intern, studying the relationship between doctor and patient and aiding experiments in the electroencephalographic laboratory. Foucault adopted many of the theories of the psychoanalyst Sigmund Freud, undertaking psychoanalytical interpretation of his dreams and making friends undergo Rorschach tests.
Embracing the Parisian "avant-garde", Foucault entered into a romantic relationship with the serialist composer Jean Barraqué. Together, they tried to produce their greatest work; heavily used recreational drugs and engaged in sado-masochistic sexual activity. In August 1953, Foucault and Barraqué holidayed in Italy, where the philosopher immersed himself in "Untimely Meditations" (1873–1876), a set of four essays by philosopher Friedrich Nietzsche. Later describing Nietzsche's work as "a revelation", he felt that reading the book deeply affected him, being a watershed moment in his life. Foucault subsequently experienced another groundbreaking self-revelation when watching a Parisian performance of Samuel Beckett's new play, "Waiting for Godot", in 1953.
Interested in literature, Foucault was an avid reader of the philosopher Maurice Blanchot's book reviews published in "Nouvelle Revue Française". Enamoured with Blanchot's literary style and critical theories, in later works he adopted Blanchot's technique of "interviewing" himself. Foucault also came across Hermann Broch's 1945 novel "The Death of Virgil", a work that obsessed both him and Barraqué. While the latter attempted to convert the work into an epic opera, Foucault admired Broch's text for its portrayal of death as an affirmation of life. The couple took a mutual interest in the work of such authors as the Marquis de Sade, Fyodor Dostoyevsky, Franz Kafka and Jean Genet, all of whose works explored the themes of sex and violence.
Interested in the work of Swiss psychologist Ludwig Binswanger, Foucault aided family friend Jacqueline Verdeaux in translating his works into French. Foucault was particularly interested in Binswanger's studies of Ellen West who, like himself, had a deep obsession with suicide, eventually killing herself. In 1954, Foucault authored an introduction to Binswanger's paper "Dream and Existence", in which he argued that dreams constituted "the birth of the world" or "the heart laid bare", expressing the mind's deepest desires. That same year, Foucault published his first book, "Mental Illness and Personality" ("Maladie mentale et personnalité"), in which he exhibited his influence from both Marxist and Heideggerian thought, covering a wide range of subject matter from the reflex psychology of Pavlov to the classic psychoanalysis of Freud. Referencing the work of sociologists and anthropologists such as Émile Durkheim and Margaret Mead, he presented his theory that illness was culturally relative. Biographer James Miller noted that while the book exhibited "erudition and evident intelligence", it lacked the "kind of fire and flair" which Foucault exhibited in subsequent works. It was largely critically ignored, receiving only one review at the time. Foucault grew to despise it, unsuccessfully attempting to prevent its republication and translation into English.
Sweden, Poland, and West Germany: 1955–1960.
Foucault spent the next five years abroad, first in Sweden, working as cultural diplomat at the University of Uppsala, a job obtained through his acquaintance with historian of religion Georges Dumézil. At Uppsala he was appointed a Reader in French language and literature, while simultaneously working as director of the Maison de France, thus opening the possibility of a cultural-diplomatic career. Although finding it difficult to adjust to the "Nordic gloom" and long winters, he developed close friendships with two Frenchmen, biochemist Jean-François Miquel and physicist Jacques Papet-Lépine, and entered into romantic and sexual relationships with various men. In Uppsala, he became known for his heavy alcohol consumption and reckless driving in his new Jaguar car. In spring 1956, Barraqué broke from his relationship with Foucault, announcing that he wanted to leave the "vertigo of madness". In Uppsala, Foucault spent much of his spare time in the university's "Carolina Rediviva" library, making use of their Bibliotheca Walleriana collection of texts on the history of medicine for his ongoing research. Finishing his doctoral thesis, Foucault hoped it would be accepted by Uppsala University, but Sten Lindroth, a positivistic historian of science there, was unimpressed, asserting that it was full of speculative generalisations and was a poor work of history; he refused to allow Foucault to be awarded a doctorate at Uppsala. In part because of this rejection, Foucault left Sweden. Later, Foucault admitted that the work was a first draft with certain lack of quality.
Again at Dumézil's recognition, in October 1958 Foucault arrived in the Polish city of Warsaw, placed in charge of the University of Warsaw's Centre Français. Foucault found life in Poland difficult due to the lack of material goods and services following the destruction of the Second World War. Witnessing the aftermath of the Polish October in which students had protested against the governing communist Polish United Workers' Party, he felt that most Poles despised their government as a puppet regime of the Soviet Union, and thought that the system ran "badly". Considering the university a liberal enclave, he traveled the country giving lectures; proving popular, he adopted the position of "de facto" cultural attaché. As in France and Sweden, homosexual activity was legal but socially frowned upon in Poland, and he undertook relationships with a number of men; one was a Polish security agent who hoped to trap Foucault in an embarrassing situation, which would therefore reflect badly on the French embassy. Wracked in diplomatic scandal, he was ordered to leave Poland for a new destination. Various positions were available in West Germany, and so Foucault relocated to the (where he was director in 1958–60), teaching the same courses he had given in Uppsala and Warsaw. Spending much time in the Reeperbahn red light district, he entered into a relationship with a transvestite.
Growing career.
"Madness and Civilization": 1960.
In West Germany, Foucault completed in 1960 his primary thesis ("thèse principale") for his State doctorate, entitled "Folie et déraison: Histoire de la folie à l'âge classique" ("Madness and Insanity: History of Madness in the Classical Age"), a philosophical work based upon his studies into the history of medicine. The book discussed how West European society had dealt with madness, arguing that it was a social construct distinct from mental illness. Foucault traces the evolution of the concept of madness through three phases: the Renaissance, the later 17th and 18th centuries, and the modern experience. The work alludes to the work of French poet and playwright Antonin Artaud, who exerted a strong influence over Foucault's thought at the time.
"Histoire de la folie" was an expansive work, consisting of 943 pages of text, followed by appendices and a bibliography. Foucault submitted it at the University of Paris, although the university's regulations for awarding a State doctorate required the submission of both his main thesis and a shorter complementary thesis. Obtaining a doctorate in France at the period was a multi-step process. The first step was to obtain a "rapporteur", or "sponsor" for the work: Foucault chose Georges Canguilhem. The second was to find a publisher, and as a result "Folie et déraison" would be published in French in May 1961 by the company Plon, whom Foucault chose over Presses Universitaires de France after being rejected by Gallimard. In 1964, a heavily abridged version was published as a mass market paperback, then translated into English for publication the following year as "Madness and Civilization".
"Folie et déraison" received a mixed reception in France and in foreign journals focusing on French affairs. Although it was critically acclaimed by Maurice Blanchot, Michel Serres, Roland Barthes, Gaston Bachelard, and Fernand Braudel, it was largely ignored by the leftist press, much to Foucault's disappointment. It was notably criticised for advocating metaphysics by young philosopher Jacques Derrida in a March 1963 lecture at the University of Paris. Responding with a vicious retort, Foucault criticised Derrida's interpretation of René Descartes. The two remained bitter rivals until reconciling in 1981. In the English-speaking world, the work became a significant influence on the anti-psychiatry movement during the 1960s; Foucault took a mixed approach to this, associating with a number of anti-psychiatrists but arguing that most of them misunderstood his work.
Foucault's secondary thesis (his "thèse complémentaire" written in Hamburg between 1959 and 1960) was a translation and commentary on German philosopher Immanuel Kant's 1798 work "Anthropology from a Pragmatic Point of View" (the title of his thesis was "Introduction à l'"Anthropologie"", "Introduction to Kant's "Anthropology""). Largely consisting of Foucault's discussion of textual dating—an "archaeology of the Kantian text"—he rounded off the thesis with an evocation of Nietzsche, his biggest philosophical influence. This work's "rapporteur" was his old tutor and then director of the ENS, Hyppolite, who was well acquainted with German philosophy. After both theses were championed and reviewed, he underwent his public defense, the "soutenance de thèse", on 20 May 1961. The academics responsible for reviewing his work were concerned about the unconventional nature of his major thesis; reviewer Henri Gouhier noted that it was not a conventional work of history, making sweeping generalisations without sufficient particular argument, and that Foucault clearly "thinks in allegories". They all agreed however that the overall project was of merit, awarding Foucault his doctorate "despite reservations".
University of Clermont-Ferrand, "The Birth of the Clinic", and "The Order of Things": 1960–1966.
In October 1960, Foucault took a tenured post in philosophy at the University of Clermont-Ferrand, commuting to the city every week from Paris, where he lived in a high-rise block on the rue du Dr Finlay. Responsible for teaching psychology, which was subsumed within the philosophy department, he was considered a "fascinating" but "rather traditional" teacher at Clermont. The department was run by Jules Vuillemin, who soon developed a friendship with Foucault. Foucault then took Vuillemin's job when the latter was elected to the Collège de France in 1962. In this position, Foucault took a dislike to another staff member whom he considered stupid: Roger Garaudy, a senior figure in the Communist Party. Foucault made life at the university difficult for Garaudy, leading the latter to transfer to Poitiers. Foucault also caused controversy by securing a university job for his lover, the philosopher Daniel Defert, with whom he retained a non-monogamous relationship for the rest of his life.
Foucault maintained a keen interest in literature, publishing reviews in amongst others the literary journals "Tel Quel" and "Nouvelle Revue Française", and sitting on the editorial board of "Critique". In May 1963, he published a book devoted to poet, novelist, and playwright Raymond Roussel. It was written in under two months, published by Gallimard, and would be described by biographer David Macey as "a very personal book" that resulted from a "love affair" with Roussel's work. It would be published in English in 1983 as "Death and the Labyrinth: The World of Raymond Roussel". Receiving few reviews, it was largely ignored. That same year he published a sequel to " Folie et déraison", entitled "Naissance de la Clinique", subsequently translated as "Birth of the Clinic: An Archaeology of Medical Perception". Shorter than its predecessor, it focused on the changes that the medical establishment underwent in the late 18th and early 19th centuries. Like his preceding work, "Naissance de la Clinique" was largely critically ignored, but later gained a cult following. Foucault was also selected to be among the "Eighteen Man Commission" that assembled between November 1963 and March 1964 to discuss university reforms that were to be implemented by Christian Fouchet, the Gaullist Minister of National Education. Implemented in 1967, they brought staff strikes and student protests.
In April 1966, Gallimard published Foucault's "Les Mots et les choses" ("Words and Things"), later translated as "The Order of Things: An Archaeology of the Human Sciences". Exploring how man came to be an object of knowledge, it argued that all periods of history have possessed certain underlying conditions of truth that constituted what was acceptable as scientific discourse. Foucault argues that these conditions of discourse have changed over time, from one period's episteme to another. Although designed for a specialist audience, the work gained media attention, becoming a surprise bestseller in France. Appearing at the height of interest in structuralism, Foucault was quickly grouped with scholars Jacques Lacan, Claude Lévi-Strauss, and Roland Barthes, as the latest wave of thinkers set to topple the existentialism popularized by Jean-Paul Sartre. Although initially accepting this description, Foucault soon vehemently rejected it. Foucault and Sartre regularly criticised one another in the press. Both Sartre and Simone de Beauvoir attacked Foucault's ideas as "bourgeois", while Foucault retaliated against their Marxist beliefs by proclaiming that "Marxism exists in nineteenth-century thought as a fish exists in water; that is, it ceases to breathe anywhere else."
University of Tunis and Vincennes: 1966–1970.
In September 1966, Foucault took a position teaching psychology at the University of Tunis in Tunisia. His decision to do so was largely because his lover, Defert, had been posted to the country as part of his national service. Foucault moved a few kilometres from Tunis, to the village of Sidi Bou Saïd, where fellow academic Gérard Deledalle lived with his wife. Soon after his arrival, Foucault announced that Tunisia was "blessed by history", a nation which "deserves to live forever because it was where Hannibal and St. Augustine lived." His lectures at the university proved very popular, and were well attended. Although many young students were enthusiastic about his teaching, they were critical of what they believed to be his right-wing political views, viewing him as a "representative of Gaullist technocracy", even though he considered himself a leftist.
Foucault was in Tunis during the anti-government and pro-Palestinian riots that rocked the city in June 1967, and which continued for a year. Although highly critical of the violent, ultra-nationalistic and anti-semitic nature of many protesters, he used his status to try to prevent some of his militant leftist students from being arrested and tortured for their role in the agitation. He hid their printing press in his garden, and tried to testify on their behalf at their trials, but was prevented when the trials became closed-door events. While in Tunis, Foucault continued to write. Inspired by a correspondence with the surrealist artist René Magritte, Foucault started to write a book about the impressionist artist Eduard Manet, but never completed it.
In 1968, Foucault returned to Paris, moving into an apartment on the Rue de Vaugirard. After the May 1968 student protests, Minister of Education Edgar Faure responded by founding new universities with greater autonomy. Most prominent of these was the Centre Expérimental de Vincennes in Vincennes on the outskirts of Paris. A group of prominent academics were asked to select teachers to run the Centre's departments, and Canguilheim recommended Foucault as head of the Philosophy Department. Becoming a tenured professor of Vincennes, Foucault's desire was to obtain "the best in French philosophy today" for his department, employing Michel Serres, Judith Miller, Alain Badiou, Jacques Rancière, François Regnault, Henri Weber, Étienne Balibar, and François Châtelet; most of them were Marxists or ultra-left activists.
Lectures began at the university in January 1969, and straight away its students and staff, including Foucault, were involved in occupations and clashes with police, resulting in arrests. In February, Foucault gave a speech denouncing police provocation to protesters at the Latin Quarter of the Mutualité. Such actions marked Foucault's embrace of the ultra-left, undoubtedly influenced by Defert, who had gained a job at Vincennes' sociology department and who had become a Maoist. Most of the courses at Foucault's philosophy department were Marxist-Leninist oriented, although Foucault himself gave courses on Nietzsche, "The end of Metaphysics", and "The Discourse of Sexuality", which were highly popular and over-subscribed. While the right-wing press was heavily critical of this new institution, new Minister of Education Olivier Guichard was angered by its ideological bent and the lack of exams, with students being awarded degrees in a haphazard manner. He refused national accreditation of the department's degrees, resulting in a public rebuttal from Foucault.
Later life.
Collège de France and "Discipline and Punish": 1970–1975.
Foucault desired to leave Vincennes and become a fellow of the prestigious Collège de France. He requested to join, taking up a chair in what he called the "history of systems of thought," and his request was championed by members Dumézil, Hyppolite, and Vuillemin. In November 1969, when an opening became available, Foucault was elected to the Collège, though with opposition by a large minority. He gave his inaugural lecture in December 1970, which was subsequently published as "L'Ordre du discours" ("The Discourse of Language"). He was obliged to give 12 weekly lectures a year—and did so for the rest of his life—covering the topics that he was researching at the time; these became "one of the events of Parisian intellectual life" and were repeatedly packed out events. On Mondays, he also gave seminars to a group of students; many of them became a "Foulcauldian tribe" who worked with him on his research. He enjoyed this teamwork and collective research, and together they would publish a number of short books. Working at the Collège allowed him to travel widely, giving lectures in Brazil, Japan, Canada, and the United States over the next 14 years. In 1970 and 1972, Foucault served as a professor in the French Department of the University at Buffalo in Buffalo, New York.
In May 1971, Foucault co-founded the Group d'Information sur les Prisons (GIP) along with historian Pierre Vidal-Naquet and journalist Jean-Marie Domenach. The GIP aimed to investigate and expose poor conditions in prisons and give prisoners and ex-prisoners a voice in French society. It was highly critical of the penal system, believing that it converted petty criminals into hardened delinquents. The GIP gave press conferences and staged protests surrounding the events of the Toul prison riot in December 1971, alongside other prison riots that it sparked off; in doing so it faced police crack down and repeated arrest. The group became active across France, with 2,000 to 3,000, members, but disbanded before 1974. Also campaigning against the death penalty, Foucault co-authored a short book on the case of the executed murderer Pierre Rivière. After his research into the penal system, Foucault published "Surveiller et punir: Naissance de la prison" ("Discipline and Punish") in 1975, offering a history of the system in western Europe. Biographer Didier Eribon described it as "perhaps the finest" of Foucault's works, and it was well received.
Foucault was also active in anti-racist campaigns; in November 1971, he was a leading figure in protests following the perceived racist killing of Arab migrant Dejellali Ben Ali. In this he worked alongside his old rival Sartre, the journalist Claude Mauriac, and one of his literary heroes, Jean Genet. This campaign was formalised as the Committee for the Defence of the Rights of Immigrants, but there was tension at their meetings as Foucault opposed the anti-Israeli sentiment of many Arab workers and Maoist activists. At a December 1972 protest against the police killing of Algerian worker Mohammad Diab, both Foucault and Genet were arrested, resulting in widespread publicity. Foucault was also involved in founding the Agence de Press-Libération (APL), a group of leftist journalists who intended to cover news stories neglected by the mainstream press. In 1973, they established the daily newspaper "Libération", and Foucault suggested that they establish committees across France to collect news and distribute the paper, and advocated a column known as the "Chronicle of the Workers' Memory" to allow workers' to express their opinions. Foucault wanted an active journalistic role in the paper, but this proved untenable, and he soon became disillusioned with "Libération", believing that it distorted the facts; he would not publish in it until 1980.
"The History of Sexuality" and Iranian Revolution: 1976–1979.
In 1976 Gallimard published Foucault's "Histoire de la sexualité: la volonté de savoir" ("The History of Sexuality: The Will to Knowledge"), a short book exploring what Foucault called the "repressive hypothesis". It revolved largely around the concept of power, rejecting Marxist theories of power and rejecting psychoanalysis. Foucault intended it as the first in a seven-volume exploration of the subject. "Histoire de la sexualité" was a best-seller and gained a positive press reception, but lukewarm intellectual interest, something that upset Foucault, who felt that many misunderstood his hypothesis. He soon became dissatisfied with Gallimard after being offended by senior staff member Pierre Nora. Along with Paul Veyne and François Wahl, Foucault launched a new series of academic books, known as "Des travaux" ("Some Works"), through the company Seuil, which he hoped would improve the state of academic research in France. He also produced introductions for the memoirs of Herculine Barbin and "My Secret Life".
Foucault remained active as a political activist, focusing on protesting government abuses of human rights across the world. He was a key player in the 1975 protests against the Spanish government to execute 11 militants sentenced to death without fair trial. It was his idea to travel to Madrid with 6 others to give their press conference there; they were subsequently arrested and deported back to Paris. In 1977, he protested the extradition of Klaus Croissant to West Germany, and his rib was fractured during clashes with riot police. In July that year, he organised an assembly of Eastern Bloc dissidents to mark the visit of Soviet Premier Leonid Brezhnev to Paris. In 1979, he campaigned for Vietnamese political dissidents to be granted asylum in France.
In 1977, Italian newspaper "Corriere della sera" asked Foucault to write a column for them. In doing so, in 1978 he travelled to Tehran in Iran, days after the Black Friday massacre. Documenting the developing Iranian Revolution, he met with opposition leaders such as Mohammad Kazem Shariatmadari and Mehdi Bazargan, and discovered the popular support for Islamism. Returning to France, he was one of the journalists who visited the Ayatollah Khomeini, before he visited Tehran again. His articles expressed awe of Khomeini's Islamist movement, for which he was widely criticised in the French press, including by Iranian liberal dissidents. Foucault's response was that Islamism was to become a major political force in the region, and that the West must treat it with respect rather than hostility. In April 1978, Foucault traveled to Japan, where he studied Zen Buddhism under Omori Sogen at the Seionji temple in Uenohara.
Final years: 1980–1984.
Although remaining critical of power relations, Foucault expressed cautious support for the Socialist Party government of François Mitterrand following its electoral victory in 1981. But his support soon deteriorated when that party refused to condemn the Polish government's crackdown on the 1982 demonstrations in Poland orchestrated by the Solidarity trade union. He and sociologist Pierre Bourdieu authored a document condemning Mitterrand's inaction that was published in "Libération", and they also took part in large public protests on the issue. Foucault continued to support Solidarity, and with his friend Simone Signoret traveled Poland as part of a Médecins du Monde expedition, taking time out to visit the Auschwitz concentration camp. He continued his academic research, and in June 1984 Gallimard published the second and third volumes of "Histoire de la sexualité". Volume two, "L'Usage des plaisirs", dealt with the "techniques of self" prescribed by ancient Greek pagan morality in relation to sexual ethics, while volume three, "Le Souci de soi" explored the same theme in the Greek and Latin texts of the first two centuries CE. A fourth volume, "Les Aveux de la chair", examined it in early Christianity, but it remained unfinished at Foucault's death.
In October 1980, Foucault became a visiting professor at the University of California, Berkeley, giving the Howison Lectures on "Truth and Subjectivity", while in November he lectured at the Humanities Institute at the New York University. His growing popularity in American intellectual circles was noted by "Time" magazine, while Foucault went on to lecture at UCLA in 1981, the University of Vermont in 1982, and Berkeley again in 1983, where his lectures drew huge crowds. When in California, Foucault spent many evenings in the gay scene of the San Francisco Bay Area, frequenting sado-masochistic bathhouses, engaging in sexual intercourse with other patrons. He would praise sado-masochistic activity in interviews with the gay press, describing it as "the real creation of new possibilities of pleasure, which people had no idea about previously." Through this sexual activity, Foucault contracted HIV, which eventually developed into AIDS. Little was known of the virus at the time; the first cases had only been identified in 1980. In summer 1983, he developed a persistent dry cough, which concerned friends in Paris, but Foucault insisted it was just a pulmonary infection. Only when hospitalized was Foucault correctly diagnosed; treated with antibiotics, he delivered a final set of lectures at the Collège de France. Foucault entered Paris' Hôpital de la Salpêtrière – the same institution that he had studied in "Madness and Civilisation" – on 9 June 1984, with neurological symptoms complicated by septicemia. He died in the hospital on 25 June.
On 26 June, "Libération" announced his death, mentioning the rumour that it had been brought on by AIDS. The following day, "Le Monde" issued a medical bulletin cleared by his family which made no reference to HIV/AIDS. On 29 June, Foucault's "la levée du corps" ceremony was held, in which the coffin was carried from the hospital morgue. Hundreds attended, including activist and academic friends, while Gilles Deleuze gave a speech using text from "The History of Sexuality". His body was then buried at Vendeuvre in a small ceremony. Soon after his death, Foucault's partner Daniel Defert founded the first national HIV/AIDS organisation in France, AIDES; a pun on the French language word for "help" ("aide") and the English language acronym for the disease. On the second anniversary of Foucault's death, Defert publicly revealed that Foucault's death was AIDS-related in California-based gay magazine, "The Advocate".
Personal life.
Foucault's first biographer, Didier Eribon, described the philosopher as "a complex, many-sided character", and that "under one mask there is always another". He also noted that he exhibited an "enormous capacity for work". At the ENS, Foucault's classmates unanimously summed him up as a figure who was both "disconcerting and strange" and "a passionate worker". His personality would change as he aged however; Eribon noted that while he was a "tortured adolescent", post-1960, he had become "a radiant man, relaxed and cheerful", even being described by those who worked with him as a dandy. He noted that in 1969, Foucault embodied the idea of "the militant intellectual".
Foucault was a fan of classical music, particularly enjoying the work of Johann Sebastian Bach and Wolfgang Amadeus Mozart. Foucault became known for wearing turtleneck jumpers. After his death, Foucault's friend Georges Dumézil described him as having possessed "a profound kindness and goodness", also exhibiting an "intelligence literally knew no bounds."
Politically, Foucault remained a leftist throughout his life, but his particular stance within the left often changed. In the early 1950s he had been a member of the French Communist Party, although never adopted an orthodox Marxist viewpoint and left the party after three years, disgusted by the prejudice against Jews and homosexuals within its ranks. After spending some time working in Poland, then governed as a socialist state by the Polish United Workers' Party, he became further disillusioned with communist ideology. As a result, in the early 1960s he was considered to be "violently anticommunist" by some of his detractors, even though totally involved in leftist campaigns along with most of his students and colleagues.
Foucault was an atheist.
Thought.
Foucault's colleague Pierre Bourdieu summarised the philosopher's thought as "a long exploration of transgression, of going beyond social limits, always inseparably linked to knowledge and power."
Philosopher Philip Stokes of the University of Reading noted that overall, Foucault's work was "dark and pessimistic", but that it did leave some room for optimism, in that it illustrates how the discipline of philosophy can be used to highlight areas of domination. In doing so, Stokes claimed, we are able to understand how we are being dominated and strive to build social structures that minimise this risk of domination. In all of this development there had to be close attention to detail; it is the detail which eventually individualises people.
Later in his life, Foucault explained that his work was less about analysing power as a phenomenon than about trying to characterise the different ways in which contemporary society has expressed the use of power to "objectivise subjects." These have taken three broad forms: one involving scientific authority to classify and 'order' knowledge about human populations. A second, and related form, has been to categorise and 'normalise' human subjects (by identifying madness, illness, physical features, and so on). The third relates to the manner in which the impulse to fashion sexual identities and train one's own body to engage in routines and practices ends up reproducing certain patterns within a given society.
Literature.
In addition to his philosophical work, Foucault also wrote on literature. "Death and the Labyrinth: The World of Raymond Roussel" was published in 1963, and translated into English in 1986. It is Foucault's only book-length work on literature. Foucault described it as "by far the book I wrote most easily, with the greatest pleasure, and most rapidly." Foucault explores theory, criticism, and psychology with reference to the texts of Raymond Roussel, one of the first notable experimental writers.
Influence.
Foucault's discussions on power and discourse have inspired many critical theorists, who believe that Foucault's analysis of power structures could aid the struggle against inequality. They claim that through discourse analysis, hierarchies may be uncovered and questioned by way of analyzing the corresponding fields of knowledge through which they are legitimated. This is one of the ways that Foucault's work is linked to critical theory.
In 2007, Foucault was listed as the most cited scholar in the humanities by the "ISI Web of Science" among a large quantity of French philosophers, the compilation's author commenting that "What this says of modern scholarship is for the reader to decide – and it is imagined that judgments will vary from admiration to despair, depending on one’s view".
Critiques and engagements.
Crypto-normativity.
A prominent critique of Foucault's thought concerns his refusal to propose positive solutions to the social and political issues that he critiques. Since no human relation is devoid of power, freedom becomes elusive - even as an ideal. This stance which critiques normativity as socially constructed and contingent, but which relies on an implicit norm in order to mount the critique led philosopher Jürgen Habermas to describe Foucault's thinking as "crypto-normativist", covertly reliant on the very Enlightenment principles he attempts to argue against. A similar critique has been advanced by Diana Taylor, and by Nancy Fraser who argues that "Foucault's critique encompasses traditional moral systems, he denies himself recourse to concepts such as "freedom" and "justice", and therefore lacks the ability to generate positive alternatives ".
Genealogy as historical method.
Philosopher Richard Rorty has argued that Foucault's 'archaeology of knowledge' is fundamentally negative, and thus fails to adequately establish any 'new' theory of knowledge "per se". Rather, Foucault simply provides a few valuable maxims regarding the reading of history. Says Rorty:
Foucault has frequently been criticized by historians for what they consider to be a lack of rigor in his analyses. For example, Hans-Ulrich Wehler harshly criticized Foucault in 1998. Wehler regards Foucault as a bad philosopher who wrongfully received a good response by the humanities and by social sciences. According to Wehler, Foucault's works are not only insufficient in their empiric historical aspects, but also often contradictory and lacking in clarity. For example, Foucault's concept of power is "desperatingly undifferentiated", and Foucault's thesis of a "disciplinary society" is, according to Wehler, only possible because Foucault does not properly differentiate between authority, force, power, violence and legitimacy. In addition, his thesis is based on a one-sided choice of sources (prisons and psychiatric institutions) and neglects other types of organizations as e.g. factories. Also, Wehler criticizes Foucault's "francocentrism" because he did not take into consideration major German-speaking theorists of social sciences like Max Weber and Norbert Elias. In all, Wehler concludes that Foucault is "because of the endless series of flaws in his so-called empirical studies ... an intellectually dishonest, empirically absolutely unreliable, crypto-normativist seducer of Postmodernism".
Feminist critiques.
Though American feminists have built on Foucault's critiques of the historical construction of gender roles and sexuality, some feminists have accused him of androcentrism, adopting exclusively male perspectives on subjectivity and ethics.
Queer theory.
Foucault's approach to sexuality, in which he understands sexualities as socially constructed concepts that are ascribed onto bodies has become widely influential for example through the work of queer theorist Judith Butler and Eve Sedgwick. Nonetheless Foucault's resistance to identity politics and the rejection of sexual object choice as fixed foundation for sexual behavior, stands at odds with some formulations of queer or gay identity.
Social constructionism and human nature.
Foucault is sometimes criticized for his prominent formulation of principles of social constructionism, which some see as an affront to the concept of truth. In Foucault's 1971 televised debate with Noam Chomsky, Foucault argued against the possibility of any fixed human nature, as posited by Chomsky's concept of innate human faculties. Chomsky argued that concepts of justice were rooted in human reason, whereas Foucault rejected the universal basis for a concept of justice. Following the debate, Chomsky was stricken with Foucault's total rejection of the possibility of a universal morality, stating "He struck me as completely amoral, I’d never met anyone who was so totally amoral" ... "I mean, I liked him personally, it's just that I couldn't make sense of him. It's as if he was from a different species, or something."
Education and authority.
Peruvian writer Mario Vargas Llosa, while acknowledging that Foucault contributed to give a right of citizenship in cultural life to certain marginal and eccentric experiences (of sexuality, of cultural repression, of madness), asserts that his radical critique of authority was detrimental to education.
External links.
General sites (updated regularly):
Biographies:
Bibliographies:
Journals:

</doc>
<doc id="47645" url="https://en.wikipedia.org/wiki?curid=47645" title="Value">
Value

Value or values may refer to:

</doc>
<doc id="47646" url="https://en.wikipedia.org/wiki?curid=47646" title="Hippie">
Hippie

A hippie (or hippy) is a member of a liberal counterculture, originally a youth movement that started in the United States and United Kingdom during the mid-1960s and spread to other countries around the world. The word "hippie" came from "hipster" and was initially used to describe beatniks who had moved into New York City's Greenwich Village and San Francisco's Haight-Ashbury district. The term "hippie" was first popularized in San Francisco by Herb Caen, who was a journalist for the "San Francisco Chronicle". The origins of the terms "hip" and "hep" are uncertain, although by the 1940s both had become part of African American jive slang and meant "sophisticated; currently fashionable; fully up-to-date". The Beats adopted the term "hip", and early hippies inherited the language and countercultural values of the Beat Generation. Hippies created their own communities, listened to psychedelic music, embraced the sexual revolution, and used drugs such as cannabis, LSD, peyote and psilocybin mushrooms to explore altered states of consciousness.
In January 1967, the Human Be-In in Golden Gate Park in San Francisco popularized hippie culture, leading to the Summer of Love on the West Coast of the United States, and the 1969 Woodstock Festival on the East Coast. Hippies in Mexico, known as "jipitecas", formed "La Onda" and gathered at Avándaro, while in New Zealand, nomadic housetruckers practiced alternative lifestyles and promoted sustainable energy at Nambassa. In the United Kingdom in 1970 many gathered at the gigantic Isle of Wight Festival with a crowd of around 400,000 people. In later years, mobile "peace convoys" of New Age travelers made summer pilgrimages to free music festivals at Stonehenge and elsewhere. In Australia, hippies gathered at Nimbin for the 1973 Aquarius Festival and the annual Cannabis Law Reform Rally or MardiGrass. ""Piedra Roja" Festival", a major hippie event in Chile, was held in 1970.
Hippie fashion and values had a major effect on culture, influencing popular music, television, film, literature, and the arts. Since the 1960s, many aspects of hippie culture have been assimilated by mainstream society. The religious and cultural diversity espoused by the hippies has gained widespread acceptance, and Eastern philosophy and spiritual concepts have reached a larger audience.
Etymology.
Lexicographer Jesse Sheidlower, the principal American editor of the "Oxford English Dictionary", argues that the terms "hipster" and "hippie" derive from the word "hip", whose origins are unknown. The word "hip" in the sense of "aware, in the know" is first attested in a 1902 cartoon by Tad Dorgan, and first appeared in print in a 1904 novel by George Vere Hobart, "Jim Hickey, A Story of the One-Night Stands", where a black American character uses the slang phrase "Are you hip?"
The term "hipster" was coined by Harry Gibson in 1944. By the 1940s, the terms "hip", "hep" and "hepcat" were popular in Harlem jazz slang, although "hep" eventually came to denote an inferior status to "hip".
In Greenwich Village in the early 1960s, New York City, young counterculture advocates were named "hips" because they were considered "in the know" or "cool", as opposed to being "square". In a 1961 essay, Kenneth Rexroth used both the terms "hipster" and "hippies" to refer to young people participating in black American or Beatnik nightlife. According to Malcolm X's 1964 autobiography, the word "hippie" in 1940s Harlem had been used to describe a specific type of white man who "acted more Negro than Negroes". Andrew Loog Oldham refers to "all the Chicago hippies," seemingly in reference to black blues/R&B musicians, in his rear sleeve notes to the 1965 LP "The Rolling Stones, Now!"
The word hippie was also used in reference to Philadelphia in at least two popular songs in 1963: South Street by The Orlons, and You Can't Sit Down by The Dovells. In both songs, the term is applied to residents of Philadelphia's South Street.
Although the word "hippies" made other isolated appearances in print during the early 1960s, the first use of the term on the West Coast appeared on September 5, 1965, in the article, "A New Haven for Beatniks", by San Francisco journalist Michael Fallon. In that article, Fallon wrote about the Blue Unicorn coffeehouse, using the term "hippie" to refer to the new generation of beatniks who had moved from North Beach into the Haight-Ashbury district. "New York Times" editor and usage writer Theodore M. Bernstein said the paper changed the spelling from "hippy" to "hippie" to avoid the ambiguous description of clothing as "hippy fashions".
History.
Origins.
A July 1968 "Time Magazine" study on hippie philosophy credited the foundation of the hippie movement with historical precedent as far back as the "Sadhu" of India, the spiritual seekers who had renounced the world by taking "Sannyas". Even the counterculture of the Ancient Greeks, espoused by philosophers like Diogenes of Sinope and the Cynics were also early forms of hippie culture. It also named as notable influences the religious and spiritual teachings of Henry David Thoreau, Hillel the Elder, Jesus, Buddha, St. Francis of Assisi, Gandhi, and J.R.R. Tolkien.
The first signs of modern "proto-hippies" emerged in fin de siècle Europe. Between 1896 and 1908, a German youth movement arose as a countercultural reaction to the organized social and cultural clubs that centered around German folk music. Known as "Der Wandervogel" ("wandering bird"), the hippie movement opposed the formality of traditional German clubs, instead emphasizing amateur music and singing, creative dress, and communal outings involving hiking and camping. Inspired by the works of Friedrich Nietzsche, Goethe, Hermann Hesse, and Eduard Baltzer, Wandervogel attracted thousands of young Germans who rejected the rapid trend toward urbanization and yearned for the pagan, back-to-nature spiritual life of their ancestors. During the first several decades of the 20th century, Germans settled around the United States, bringing the values of the Wandervogel with them. Some opened the first health food stores, and many moved to southern California where they could practice an alternative lifestyle in a warm climate. Over time, young Americans adopted the beliefs and practices of the new immigrants. One group, called the "Nature Boys", took to the California desert and raised organic food, espousing a back-to-nature lifestyle like the Wandervogel. Songwriter eden ahbez wrote a hit song called "Nature Boy" inspired by Robert Bootzin (Gypsy Boots), who helped popularize health-consciousness, yoga, and organic food in the United States.
Like Wandervogel, the hippie movement in the United States began as a youth movement. Composed mostly of white teenagers and young adults between 15 and 25 years old, hippies inherited a tradition of cultural dissent from bohemians and beatniks of the Beat Generation in the late 1950s. Beats like Allen Ginsberg crossed over from the beat movement and became fixtures of the burgeoning hippie and anti-war movements. By 1965, hippies had become an established social group in the U.S., and the movement eventually expanded to other countries, extending as far as the United Kingdom and Europe, Australia, Canada, New Zealand, Japan, Mexico, and Brazil. The hippie ethos influenced The Beatles and others in the United Kingdom and other parts of Europe, and they in turn influenced their American counterparts. Hippie culture spread worldwide through a fusion of rock music, folk, blues, and psychedelic rock; it also found expression in literature, the dramatic arts, fashion, and the visual arts, including film, posters advertising rock concerts, and album covers. In 1968, self-described hippies represented just under 0.2% of the U.S. population and dwindled away by mid-1970s.
Along with the New Left and the Civil Rights Movement, the hippie movement was one of three dissenting groups of the 1960s counterculture. Hippies rejected established institutions, criticized middle class values, opposed nuclear weapons and the Vietnam War, embraced aspects of Eastern philosophy, championed sexual liberation, were often vegetarian and eco-friendly, promoted the use of psychedelic drugs which they believed expanded one's consciousness, and created intentional communities or communes. They used alternative arts, street theatre, folk music, and psychedelic rock as a part of their lifestyle and as a way of expressing their feelings, their protests and their vision of the world and life. Hippies opposed political and social orthodoxy, choosing a gentle and nondoctrinaire ideology that favored peace, love and personal freedom, expressed for example in The Beatles' song "All You Need is Love". Hippies perceived the dominant culture as a corrupt, monolithic entity that exercised undue power over their lives, calling this culture "The Establishment", "Big Brother", or "The Man". Noting that they were "seekers of meaning and value", scholars like Timothy Miller have described hippies as a new religious movement.
Early hippies (1958–1966).
[[File:Furthur 02.jpg|thumb|250px|Escapin' through the lily fields<br>I came across an empty space<br>It trembled and exploded<br>Left a bus stop in its place<br>The bus came by and I got on<br>That's when it all began<br>There was cowboy Neal<br>At the wheel<br>Of a bus to never-ever land - Grateful Dead, lyrics from "That's It for the Other One"]]
During the late 1950s and early 1960s, novelist Ken Kesey and the Merry Pranksters lived communally in California. Members included Beat Generation hero Neal Cassady, Ken Babbs, Carolyn Adams (aka Mountain Girl/Carolyn Garcia), Stewart Brand, Del Close, Paul Foster, George Walker, Sandy Lehmann-Haupt and others. Their early escapades were documented in Tom Wolfe's book "The Electric Kool-Aid Acid Test". With Cassady at the wheel of a school bus named Further, the Merry Pranksters traveled across the United States to celebrate the publication of Kesey's novel "Sometimes a Great Notion" and to visit the 1964 World's Fair in New York City. The Merry Pranksters were known for using cannabis, amphetamine, and LSD, and during their journey they "turned on" many people to these drugs. The Merry Pranksters filmed and audio taped their bus trips, creating an immersive multimedia experience that would later be presented to the public in the form of festivals and concerts. The Grateful Dead wrote a song about the Merry Pranksters' bus trips called "That's It for the Other One".
During this period Greenwich Village in New York City and Berkeley, California anchored the American folk music circuit. Berkeley's two coffee houses, the Cabale Creamery and the Jabberwock, sponsored performances by folk music artists in a beat setting. In April 1963, Chandler A. Laughlin III, co-founder of the Cabale Creamery, established a kind of tribal, family identity among approximately fifty people who attended a traditional, all-night Native American peyote ceremony in a rural setting. This ceremony combined a psychedelic experience with traditional Native American spiritual values; these people went on to sponsor a unique genre of musical expression and performance at the Red Dog Saloon in the isolated, old-time mining town of Virginia City, Nevada.
During the summer of 1965, Laughlin recruited much of the original talent that led to a unique amalgam of traditional folk music and the developing psychedelic rock scene. He and his cohorts created what became known as "The Red Dog Experience", featuring previously unknown musical acts — Grateful Dead, Jefferson Airplane, Big Brother and the Holding Company, Quicksilver Messenger Service, The Charlatans, and others — who played in the completely refurbished, intimate setting of Virginia City's Red Dog Saloon. There was no clear delineation between "performers" and "audience" in "The Red Dog Experience", during which music, psychedelic experimentation, a unique sense of personal style and Bill Ham's first primitive light shows combined to create a new sense of community. Laughlin and George Hunter of the Charlatans were true "proto-hippies", with their long hair, boots and outrageous clothing of 19th-century American (and Native American) heritage. LSD manufacturer Owsley Stanley lived in Berkeley during 1965 and provided much of the LSD that became a seminal part of the "Red Dog Experience", the early evolution of psychedelic rock and budding hippie culture. At the Red Dog Saloon, The Charlatans were the first psychedelic rock band to play live (albeit unintentionally) loaded on LSD.
When they returned to San Francisco, Red Dog participants Luria Castell, Ellen Harman and Alton Kelley created a collective called "The Family Dog." Modeled on their Red Dog experiences, on October 16, 1965, the Family Dog hosted "A Tribute to Dr. Strange" at Longshoreman's Hall. Attended by approximately 1,000 of the Bay Area's original "hippies", this was San Francisco's first psychedelic rock performance, costumed dance and light show, featuring Jefferson Airplane, The Great Society and The Marbles. Two other events followed before year's end, one at California Hall and one at the Matrix. After the first three Family Dog events, a much larger psychedelic event occurred at San Francisco's Longshoreman's Hall. Called "The Trips Festival", it took place on January 21–January 23, 1966, and was organized by Stewart Brand, Ken Kesey, Owsley Stanley and others. Ten thousand people attended this sold-out event, with a thousand more turned away each night. On Saturday January 22, the Grateful Dead and Big Brother and the Holding Company came on stage, and 6,000 people arrived to imbibe punch spiked with LSD and to witness one of the first fully developed light shows of the era.
By February 1966, the Family Dog became Family Dog Productions under organizer Chet Helms, promoting happenings at the Avalon Ballroom and the Fillmore Auditorium in initial cooperation with Bill Graham. The Avalon Ballroom, the Fillmore Auditorium and other venues provided settings where participants could partake of the full psychedelic music experience. Bill Ham, who had pioneered the original Red Dog light shows, perfected his art of liquid light projection, which combined light shows and film projection and became with the San Francisco ballroom experience. The sense of style and costume that began at the Red Dog Saloon flourished when San Francisco's Fox Theater went out of business and hippies bought up its costume stock, reveling in the freedom to dress up for weekly musical performances at their favorite ballrooms. As "San Francisco Chronicle" music columnist Ralph J. Gleason put it, "They danced all night long, orgiastic, spontaneous and completely free form."
Some of the earliest San Francisco hippies were former students at San Francisco State College who became intrigued by the developing psychedelic hippie music scene. These students joined the bands they loved, living communally in the large, inexpensive Victorian apartments in the Haight-Ashbury. Young Americans around the country began moving to San Francisco, and by June 1966, around 15,000 hippies had moved into the Haight. The Charlatans, Jefferson Airplane, Big Brother and the Holding Company, and the Grateful Dead all moved to San Francisco's Haight-Ashbury neighborhood during this period. Activity centered around the Diggers, a guerrilla street theatre group that combined spontaneous street theatre, anarchistic action, and art happenings in their agenda to create a "free city". By late 1966, the Diggers opened free stores which simply gave away their stock, provided free food, distributed free drugs, gave away money, organized free music concerts, and performed works of political art.
On October 6, 1966, the state of California declared LSD a controlled substance, which made the drug illegal. In response to the criminalization of psychedelics, San Francisco hippies staged a gathering in the Golden Gate Park panhandle, called the Love Pageant Rally, attracting an estimated 700–800 people. As explained by Allan Cohen, co-founder of the "San Francisco Oracle", the purpose of the rally was twofold: to draw attention to the fact that LSD had just been made illegal — and to demonstrate that people who used LSD were not criminals, nor were they mentally ill. The Grateful Dead played, and some sources claim that LSD was consumed at the rally. According to Cohen, those who took LSD "were not guilty of using illegal substances...We were celebrating transcendental consciousness, the beauty of the universe, the beauty of being."
Summer of Love (1967).
On January 14, 1967, the outdoor Human Be-In organized by Michael Bowen helped to popularize hippie culture across the United States, with 20,000 hippies gathering in San Francisco's Golden Gate Park. On March 26, Lou Reed, Edie Sedgwick and 10,000 hippies came together in Manhattan for the Central Park Be-In on Easter Sunday. The Monterey Pop Festival from June 16 to June 18 introduced the rock music of the counterculture to a wide audience and marked the start of the "Summer of Love". Scott McKenzie's rendition of John Phillips' song, "San Francisco", became a hit in the United States and Europe. The lyrics, "If you're going to San Francisco, be sure to wear some flowers in your hair", inspired thousands of young people from all over the world to travel to San Francisco, sometimes wearing flowers in their hair and distributing flowers to passersby, earning them the name, "Flower Children". Bands like the Grateful Dead, Big Brother and the Holding Company (with Janis Joplin), and Jefferson Airplane lived in the Haight.
In June 1967, Herb Caen was approached by "a distinguished magazine" to write about why hippies were attracted to San Francisco. He declined the assignment but interviewed hippies in the Haight for his own newspaper column in the "San Francisco Chronicle". Caen determined that, "Except in their music, they couldn't care less about the approval of the straight world." Caen himself felt that the city of San Francisco was so straight that it provided a visible contrast with hippie culture. On July 7, "Time" magazine featured a cover story entitled, "The Hippies: The Philosophy of a Subculture." The article described the guidelines of the hippie code: "Do your own thing, wherever you have to do it and whenever you want. Drop out. Leave society as you have known it. Leave it utterly. Blow the mind of every straight person you can reach. Turn them on, if not to drugs, then to beauty, love, honesty, fun." It is estimated that around 100,000 people traveled to San Francisco in the summer of 1967. The media was right behind them, casting a spotlight on the Haight-Ashbury district and popularizing the "hippie" label. With this increased attention, hippies found support for their ideals of love and peace but were also criticized for their anti-work, pro-drug, and permissive ethos.
At this point, The Beatles had released their groundbreaking album "Sgt. Pepper's Lonely Hearts Club Band" which was quickly embraced by the hippie movement with its colorful psychedelic sonic imagery.
By the end of the summer, the Haight-Ashbury scene had deteriorated. The incessant media coverage led the Diggers to declare the "death" of the hippie with a parade. According to poet Susan 'Stormi' Chambless, the hippies buried an effigy of a hippie in the Panhandle to demonstrate the end of his/her reign. Haight-Ashbury could not accommodate the influx of crowds (mostly naive youngsters) with no place to live. Many took to living on the street, panhandling and drug-dealing. There were problems with malnourishment, disease, and drug addiction. Crime and violence skyrocketed. None of these trends reflected what the hippies had envisioned. By the end of 1967, many of the hippies and musicians who initiated the Summer of Love had moved on. Beatle George Harrison had once visited Haight-Ashbury and found it to be just a haven for dropouts, inspiring him to give up LSD. Misgivings about the hippie culture, particularly with regard to drug abuse and lenient morality, fueled the moral panics of the late 1960s.
Revolution (1967–1969).
By 1968, hippie-influenced fashions were beginning to take off in the mainstream, especially for youths and younger adults of the populous "Baby Boomer" generation, many of whom may have aspired to emulate the hardcore movements now living in tribalistic communes, but had no overt connections to them. This was noticed not only in terms of clothes and also longer hair for men, but also in music, film, art, and literature, and not just in the US, but around the world. Eugene McCarthy's brief presidential campaign successfully persuaded a significant minority of young adults to "get clean for Gene" by shaving their beards or wearing longer skirts; however the "Clean Genes" had little impact on the popular image in the media spotlight, of the hirsute hippy adorned in beads, feathers, flowers and bells.
People commonly label other cultural movements of that period as Hippie, however it is important to know the difference. For example, Hippies were often not directly engaged in politics, as opposed to their activist counterparts known as “Yippies” (Youth International Party). The Yippies came to national attention during their celebration of the 1968 spring equinox, when some 3,000 of them took over Grand Central Terminal in New York — eventually resulting in 61 arrests. The Yippies, especially their leaders Abbie Hoffman and Jerry Rubin, became notorious for their theatrics, such as trying to levitate the Pentagon at the October 1967 war protest, and such slogans as "Rise up and abandon the creeping meatball!" Their stated intention to protest the 1968 Democratic National Convention in Chicago in August, including nominating their own candidate, "Lyndon Pigasus Pig" (an actual pig), was also widely publicized in the media at this time. In Cambridge, hippies congregated each Sunday for a large "be-in" at Cambridge Park with swarms of drummers and those beginning the Women's Movement. In the US the Hippie movement started to be seen as part of the "New Left" which was associated with anti-war college campus protest movements. The New Left was a term used mainly in the United Kingdom and United States in reference to activists, educators, agitators and others in the 1960s and 1970s who sought to implement a broad range of reforms on issues such as gay rights, abortion, gender roles and drugs in contrast to earlier leftist or Marxist movements that had taken a more vanguardist approach to social justice and focused mostly on labor unionization and questions of social class.</ref>
In April 1969, the building of People's Park in Berkeley, California received international attention. The University of California, Berkeley had demolished all the buildings on a parcel near campus, intending to use the land to build playing fields and a parking lot. After a long delay, during which the site became a dangerous eyesore, thousands of ordinary Berkeley citizens, merchants, students, and hippies took matters into their own hands, planting trees, shrubs, flowers and grass to convert the land into a park. A major confrontation ensued on May 15, 1969, when Governor Ronald Reagan ordered the park destroyed, which led to a two-week occupation of the city of Berkeley by the California National Guard. Flower power came into its own during this occupation as hippies engaged in acts of civil disobedience to plant flowers in empty lots all over Berkeley under the slogan "Let a Thousand Parks Bloom".
In August 1969, the Woodstock Music and Art Fair took place in Bethel, New York, which for many, exemplified the best of hippie counterculture. Over 500,000 people arrived to hear some of the most notable musicians and bands of the era, among them Canned Heat, Richie Havens, Joan Baez, Janis Joplin, The Grateful Dead, Creedence Clearwater Revival, Crosby, Stills, Nash & Young, Carlos Santana, Sly & The Family Stone, The Who, Jefferson Airplane, and Jimi Hendrix. Wavy Gravy's Hog Farm provided security and attended to practical needs, and the hippie ideals of love and human fellowship seemed to have gained real-world expression. Similar rock festivals occurred in other parts of the country, which played a significant role in spreading hippie ideals throughout America.
In December 1969, a rock festival took place in Altamont, California, about 30 miles (45 km) east of San Francisco. Initially billed as "Woodstock West", its official name was The Altamont Free Concert. About 300,000 people gathered to hear The Rolling Stones; Crosby, Stills, Nash and Young; Jefferson Airplane and other bands. The Hells Angels provided security that proved far less benevolent than the security provided at the Woodstock event: 18-year-old Meredith Hunter was stabbed and killed during The Rolling Stones' performance after he brandished a gun and waved it toward the stage.
Aftershocks (1970–present).
By the 1970s, the 1960s zeitgeist that had spawned hippie culture seemed to be on the wane. The events at Altamont Free Concert shocked many Americans, including those who had strongly identified with hippie culture. Another shock came in the form of the Sharon Tate and Leno and Rosemary LaBianca murders committed in August 1969 by Charles Manson and his "family" of followers. Nevertheless, the turbulent political atmosphere that featured the bombing of Cambodia and shootings by National Guardsmen at Jackson State University and Kent State University still brought people together. These shootings inspired the May 1970 song by Quicksilver Messenger Service "What About Me?", where they sang, "You keep adding to my numbers as you shoot my people down", as well as Neil Young's "Ohio", recorded by Crosby, Stills, Nash and Young.
Much of hippie style had been integrated into mainstream American society by the early 1970s. Large rock concerts that originated with the 1967 KFRC Fantasy Fair and Magic Mountain Music Festival and Monterey Pop Festival and the 1968 Isle of Wight Festival became the norm, evolving into stadium rock in the process. The anti-war movement reached its peak at the 1971 May Day Protests as over 12,000 protesters were arrested in Washington DC. President Nixon himself actually ventured out of the White House and chatted with a group of the 'hippie' protesters. The draft was ended soon thereafter, in 1973. During the mid 1970s, with the end of the draft and the Vietnam War, a renewal of patriotic sentiment associated with the approach of the United States Bicentennial and the emergence of punk in London, Manchester, New York and Los Angeles, the mainstream media lost interest in the hippie counterculture. At the same time there was a revival of the Mod subculture, skinheads, teddy boys and the emergence of new youth cultures, like the goths (an arty offshoot of punk) and football casuals. Acid rock gave way to prog rock, heavy metal, disco, and punk rock.
Starting in the late 1960s, hippies began to come under attack by skinheads. Hippies were also vilified and sometimes attacked by punks, revivalist mods, greasers, football casuals, Teddy boys, and members of other youth subcultures of the 1970s and 1980s. The countercultural movement was also under covert assault by J. Edgar Hoover's infamous "Counter Intelligence Program" (COINTELPRO), but in some countries it was other youth groups that were a threat. Hippie ideals had a marked influence on anarcho-punk and some post-punk youth subcultures, especially during the Second Summer of Love.
Hippie communes, where members tried to live the ideals of the hippie movement continued to flourish. On the west coast, Oregon had quite a few. Some faded away. Some are still around.
While many hippies made a long-term commitment to the lifestyle, some people argue that hippies "sold out" during the 1980s and became part of the materialist, consumer culture. Although not as visible as it once was, hippie culture has never died out completely: hippies and neo-hippies can still be found on college campuses, on communes, and at gatherings and festivals. Many embrace the hippie values of peace, love, and community, and hippies may still be found in bohemian enclaves around the world.
Towards the end of the 20th century, a trend of "cyber hippies" emerged, that embraced some of the qualities of the 1960s psychedelic counterculture. The hippie subculture is also linked to the psychedelic trance or psytrance scene, born out of the Goa scene in India.
Ethos and characteristics.
Hippies sought to free themselves from societal restrictions, choose their own way, and find new meaning in life. One expression of hippie independence from societal norms was found in their standard of dress and grooming, which made hippies instantly recognizable to one another, and served as a visual symbol of their respect for individual rights. Through their appearance, hippies declared their willingness to question authority, and distanced themselves from the "straight" and "square" (i.e., conformist) segments of society. Personality traits and values that hippies tend to be associated with are "altruism and mysticism, honesty, joy and nonviolence".
At the same time, many thoughtful hippies distanced themselves from the very idea that the way a person dresses could be a reliable signal of who he or she was — especially after outright criminals such as Charles Manson began to adopt superficial hippie characteristics, and also after plainclothes policemen started to "dress like hippies" to divide and conquer legitimate members of the counterculture. Frank Zappa, known for lampooning hippie ethos, particularly with songs like "Who Needs the Peace Corps?" (1968), admonished his audience that "we all wear a uniform". The San Francisco clown/hippie Wavy Gravy said in 1987 that he could still see fellow-feeling in the eyes of Market Street businessmen who had dressed conventionally to survive.
Art and fashion.
Leading proponents of the 1960s Psychedelic Art movement were San Francisco poster artists such as: Rick Griffin, Victor Moscoso, Bonnie MacLean, Stanley Mouse & Alton Kelley, and Wes Wilson. Their Psychedelic Rock concert posters were inspired by Art Nouveau, Victoriana, Dada, and Pop Art. The "Fillmore Posters" were among the most notable of the time. Richly saturated colors in glaring contrast, elaborately ornate lettering, strongly symmetrical composition, collage elements, rubber-like distortions, and bizarre iconography are all hallmarks of the San Francisco psychedelic poster art style. The style flourished from roughly the years 1966 to 1972. Their work was immediately influential to album cover art, and indeed all of the aforementioned artists also created album covers. Psychedelic light-shows were a new art-form developed for rock concerts. Using oil and dye in an emulsion that was set between large convex lenses upon overhead projectors, the lightshow artists created bubbling liquid visuals that pulsed in rhythm to the music. This was mixed with slideshows and film loops to create an improvisational motion picture art form, and to give visual representation to the improvisational jams of the rock bands and create a completely "trippy" atmosphere for the audience. The Brotherhood of Light were responsible for many of the light-shows in San Francisco psychedelic rock concerts.
Out of the psychedelic counterculture there also arose a new genre of comic books: underground comix. "Zap Comix" was among the original underground comics, and featured the work of Robert Crumb, S. Clay Wilson, Victor Moscoso, Rick Griffin, and Robert Williams among others. Underground Comix were ribald, intensely satirical, and seemed to pursue weirdness for the sake of weirdness. Gilbert Shelton created perhaps the most enduring of underground cartoon characters, "The Fabulous Furry Freak Brothers", whose drugged-out exploits held a hilarious mirror up to the hippy lifestyle of the 1960s.
As in the beat movement preceding them, and the punk movement that followed soon after, hippie symbols and iconography were purposely borrowed from either "low" or "primitive" cultures, with hippie fashion reflecting a disorderly, often vagrant style. As with other adolescent, white middle-class movements, deviant behavior of the hippies involved challenging the prevailing gender differences of their time: both men and women in the hippie movement wore jeans and maintained long hair, and both genders wore sandals, moccasins or went barefoot. Men often wore beards, while women wore little or no makeup, with many going braless. Hippies often chose brightly colored clothing and wore unusual styles, such as bell-bottom pants, vests, tie-dyed garments, dashikis, peasant blouses, and long, full skirts; non-Western inspired clothing with Native American, Asian, Indian, African and Latin American motifs were also popular. Much hippie clothing was self-made in defiance of corporate culture, and hippies often purchased their clothes from flea markets and second-hand shops. Favored accessories for both men and women included Native American jewelry, head scarves, headbands and long beaded necklaces. Hippie homes, vehicles and other possessions were often decorated with psychedelic art. The bold colors, hand-made clothing and loose fitting clothes opposed the tight and uniform clothing of the 1940s and 1950s. It also rejected consumerism in that the hand-production of clothing called for self-efficiency and individuality.
Love and sex.
The common stereotype on the issues of love and sex had it that the hippies were "promiscuous, having wild sex orgies, seducing innocent teenagers and every manner of sexual perversion." The hippie movement appeared concurrently in the midst of a rising sexual revolution, in which many views of the "status quo" on this subject were being challenged.
The clinical study "Human Sexual Response" was published by Masters and Johnson in 1966, and the topic suddenly became more commonplace in America. The 1969 book "Everything You Always Wanted to Know About Sex (But Were Afraid to Ask)" by psychiatrist David Reuben was a more popular attempt at answering the public's curiosity regarding such matters. Then in 1972 appeared "The Joy of Sex" by Alex Comfort, reflecting an even more candid perception of love-making. By this time, the recreational or 'fun' aspects of sexual behavior were being discussed more openly than ever before, and this more 'enlightened' outlook resulted not just from the publication of such new books as these, but from a more pervasive sexual revolution that had already been well underway for some time.
The hippies inherited various countercultural views and practices regarding sex and love from the Beat Generation; "their writings influenced the hippies to open up when it came to sex, and to experiment without guilt or jealousy." One popular hippie slogan that appeared was "If it feels good, do it!" which for many "meant you were free to love whomever you pleased, whenever you pleased, however you pleased. This encouraged spontaneous sexual activity and experimentation. Group sex, public sex... homosexuality under the influence of drugs, all the taboos went out the window. This doesn't mean that straight sex... or monogamy were unknown, quite the contrary. Nevertheless, the open relationship became an accepted part of the hippy lifestyle. This meant that you might have a primary relationship with one person, but if another attracted you, you could explore that relationship without rancor or jealousy."
Hippies embraced the old slogan of free love of the radical social reformers of other eras; it was accordingly observed that "Free love made the whole love, marriage, sex, baby package obsolete. Love was no longer limited to one person, you could love anyone you chose. In fact love was something you shared with everyone, not just your sex partners. Love exists to be shared freely. We also discovered the more you share, the more you get! So why reserve your love for a select few? This profound truth was one of the great hippie revelations." Sexual experimentation alongside psychedelics also occurred, due to the perception of their being uninhibitors. Others explored the spiritual aspects of sex.
Travel.
Hippies tended to travel light, and could pick up and go wherever the action was at any time. Whether at a "love-in" on Mount Tamalpais near San Francisco, a demonstration against the Vietnam War in Berkeley, or one of Ken Kesey's "Acid Tests", if the "vibe" wasn't right and a change of scene was desired, hippies were mobile at a moment's notice. Planning was eschewed, as hippies were happy to put a few clothes in a backpack, stick out their thumbs and hitchhike anywhere. Hippies seldom worried whether they had money, hotel reservations or any of the other standard accoutrements of travel. Hippie households welcomed overnight guests on an "impromptu" basis, and the reciprocal nature of the lifestyle permitted greater freedom of movement. People generally cooperated to meet each other's needs in ways that became less common after the early 1970s. This way of life is still seen among Rainbow Family groups, new age travellers and New Zealand's housetruckers.
A derivative of this free-flow style of travel were the hippie trucks and buses, hand-crafted mobile houses built on a truck or bus chassis to facilitate a nomadic lifestyle, as documented in the 1974 book "Roll Your Own". Some of these mobile gypsy houses were quite elaborate, with beds, toilets, showers and cooking facilities.
On the West Coast, a unique lifestyle developed around the Renaissance Faires that Phyllis and Ron Patterson first organized in 1963. During the summer and fall months, entire families traveled together in their trucks and buses, parked at Renaissance Pleasure Faire sites in Southern and Northern California, worked their crafts during the week, and donned Elizabethan costume for weekend performances, and to attend booths where handmade goods were sold to the public. The sheer number of young people living at the time made for unprecedented travel opportunities to special happenings. The peak experience of this type was the Woodstock Festival near Bethel, New York, from August 15 to 18, 1969, which drew between 400,000 and 500,000 people.
Hippie trail.
One travel experience, undertaken by hundreds of thousands of hippies between 1969 and 1971, was the Hippie trail overland route to India. Carrying little or no luggage, and with small amounts of cash, almost all followed the same route, hitch-hiking across Europe to Athens and on to Istanbul, then by train through central Turkey via Erzurum, continuing by bus into Iran, via Tabriz and Tehran to Mashhad, across the Afghan border into Herat, through southern Afghanistan via Kandahar to Kabul, over the Khyber Pass into Pakistan, via Rawalpindi and Lahore to the Indian frontier. Once in India, hippies went to many different destinations, but gathered in large numbers on the beaches of Goa and Kovalam in Trivandrum (Kerala), or crossed the border into Nepal to spend months in Kathmandu. In Kathmandu, most of the hippies hung out in the tranquil surroundings of a place called Freak Street, (Nepal Bhasa: Jhoo Chhen) which still exists near Kathmandu Durbar Square.
Spirituality and religion.
Many hippies rejected mainstream organized religion in favor of a more personal spiritual experience, often drawing on indigenous and folk beliefs. If they adhered to mainstream faiths, hippies were likely to embrace Buddhism, Unitarian Universalism, Hinduism and the restorationist Christianity of the Jesus Movement. Some hippies embraced neo-paganism, especially Wicca.
In his 1991 book, "Punk and American Values", Timothy Miller described the hippie ethos as essentially a "religious movement" whose goal was to transcend the limitations of mainstream religious institutions. "Like many dissenting religions, the hippies were enormously hostile to the religious institutions of the dominant culture, and they tried to find new and adequate ways to do the tasks the dominant religions failed to perform." In his seminal, contemporaneous work, "The Hippie Trip", author Lewis Yablonsky notes that those who were most respected in hippie settings were the spiritual leaders, the so-called "high priests" who emerged during that era.
One such hippie "high priest" was San Francisco State University Professor Stephen Gaskin. Beginning in 1966, Gaskin's "Monday Night Class" eventually outgrew the lecture hall, and attracted 1,500 hippie followers in an open discussion of spiritual values, drawing from Christian, Buddhist, and Hindu teachings. In 1970 Gaskin founded a Tennessee community called The Farm, and he still lists his religion as "Hippie."
Timothy Leary was an American psychologist and writer, known for his advocacy of psychedelic drugs. On September 19, 1966, Leary founded the League for Spiritual Discovery, a religion declaring LSD as its holy sacrament, in part as an unsuccessful attempt to maintain legal status for the use of LSD and other psychedelics for the religion's adherents based on a "freedom of religion" argument. "The Psychedelic Experience" was the inspiration for John Lennon's song "Tomorrow Never Knows" in The Beatles' album "Revolver". He published a pamphlet in 1967 called "Start Your Own Religion" to encourage just that and was invited to attend the January 14, 1967 Human Be-In a gathering of 30,000 hippies in San Francisco's Golden Gate Park In speaking to the group, he coined the famous phrase "Turn on, tune in, drop out". The English magician Aleister Crowley became an influential icon to the new alternative spiritual movements of the decade as well as for rock musicians. The Beatles included him as one of the many figures on the cover sleeve of their 1967 album "Sgt. Pepper's Lonely Hearts Club Band" while Jimmy Page, the guitarist of The Yardbirds and co-founder of 1970s rock band Led Zeppelin was fascinated by Crowley, and owned some of his clothing, manuscripts and ritual objects, and during the 1970s bought Boleskine House, which also appears in the band's movie "The Song Remains the Same". On the back cover of the Doors "13" album, Jim Morrison and the other members of the Doors are shown posing with a bust of Aleister Crowley. Timothy Leary also openly acknowledged Crowley's inspiration.
After the hippie era, the Dudeist philosophy and lifestyle developed, its inspired by "The Dude", the neo-hippie protagonist of the Coen Brothers' 1998 film "The Big Lebowski". Dudeism's stated primary objective is to promote a modern form of Chinese Taoism, outlined in "Tao Te Ching" by Laozi (6th century BC), blended with concepts by the Ancient Greek philosopher Epicurus (341-270 BC), and presented in a style as personified by the character of Jeffrey "The Dude" Lebowski, a fictional hippie character portrayed by Jeff Bridges in the film. Dudeism has sometimes been regarded as a mock religion, though its founder and many adherents regard it seriously.
Politics.
For the historian of the anarchist movement Ronald Creagh, the hippie movement could be considered as the last spectacular resurgence of utopian socialism. For Creagh, a characteristic of this is the desire for the transformation of society not through political revolution, or through reformist action pushed forward by the state, but through the creation of a counter-society of a socialist character in the midst of the current system, which will be made up of ideal communities of a more or less libertarian social form.
The peace symbol was developed in the UK as a logo for the Campaign for Nuclear Disarmament, and was embraced by U.S. anti-war protesters during the 1960s.
Hippies were often pacifists, and participated in non-violent political demonstrations, such as Civil Rights Movement, the marches on Washington D.C., and anti–Vietnam War demonstrations, including draft-card burnings and the 1968 Democratic National Convention protests. The degree of political involvement varied widely among hippies, from those who were active in peace demonstrations, to the more anti-authority street theater and demonstrations of the Yippies, the most politically active hippie sub-group. Bobby Seale discussed the differences between Yippies and hippies with Jerry Rubin, who told him that Yippies were the political wing of the hippie movement, as hippies have not "necessarily become political yet". Regarding the political activity of hippies, Rubin said, "They mostly prefer to be stoned, but most of them want peace, and they want an end to this stuff."
In addition to non-violent political demonstrations, hippie opposition to the Vietnam War included organizing political action groups to oppose the war, refusal to serve in the military and conducting "teach-ins" on college campuses that covered Vietnamese history and the larger political context of the war.
Scott McKenzie's 1967 rendition of John Phillips' song "San Francisco (Be Sure to Wear Flowers in Your Hair)", which helped to inspire the hippie Summer of Love, became a homecoming song for all Vietnam veterans arriving in San Francisco from 1967 onward. McKenzie has dedicated every American performance of "San Francisco" to Vietnam veterans, and he sang in 2002 at the 20th anniversary of the dedication of the Vietnam Veterans Memorial. Hippie political expression often took the form of "dropping out" of society to implement the changes they sought.
Politically motivated movements aided by hippies include the back to the land movement of the 1960s, cooperative business enterprises, alternative energy, the free press movement, and organic farming. The San Francisco group known as the Diggers articulated an influential radical criticism of contemporary mass consumer society, and so they opened free stores which simply gave away their stock, provided free food, distributed free drugs, gave away money, organized free music concerts, and performed works of political art. The Diggers took their name from the original English Diggers (1649–50) led by Gerrard Winstanley, and they sought to create a mini-society free of money and capitalism.
Such activism was ideally carried through anti-authoritarian and non-violent means; thus it was observed that "The way of the hippie is antithetical to all repressive hierarchical power structures since they are adverse to the hippie goals of peace, love and freedom... Hippies don't impose their beliefs on others. Instead, hippies seek to change the world through reason and by living what they believe."
The political ideals of hippies influenced other movements, such as anarcho-punk, rave culture, green politics, stoner culture and the New Age movement. Penny Rimbaud of the English anarcho-punk band Crass said in interviews, and in an essay called "The Last Of The Hippies", that Crass was formed in memory of his friend, Wally Hope. Crass had its roots in Dial House, which was established in 1967 as a commune. Some punks were often critical of Crass for their involvement in the hippie movement. Like Crass, Jello Biafra was influenced by the hippie movement, and cited the yippies as a key influence on his political activism and thinking, though he also wrote songs critical of hippies.
Drugs.
Following in the footsteps of the Beats, many hippies used cannabis (marijuana), considering it pleasurable and benign. They enlarged their spiritual pharmacopeia to include hallucinogens such as peyote, LSD, psilocybin mushrooms and DMT, while often renouncing the use of alcohol. On the East Coast of the United States, Harvard University professors Timothy Leary, Ralph Metzner and Richard Alpert (Ram Dass) advocated psychotropic drugs for psychotherapy, self-exploration, religious and spiritual use. Regarding LSD, Leary said, "Expand your consciousness and find ecstasy and revelation within."
On the West Coast of the United States, Ken Kesey was an important figure in promoting the recreational use of psychotropic drugs, especially LSD, also known as "acid." By holding what he called "Acid Tests", and touring the country with his band of Merry Pranksters, Kesey became a magnet for media attention that drew many young people to the fledgling movement. The Grateful Dead (originally billed as "The Warlocks") played some of their first shows at the Acid Tests, often as high on LSD as their audiences. Kesey and the Pranksters had a "vision of turning on the world." Harder drugs, such as cocaine, amphetamines and heroin, were also sometimes used in hippie settings; however, these drugs were often disdained, even among those who used them, because they were recognized as harmful and addictive.
In the 1960s the hippie's heyday, it is believed drugs were running rampant and little was done to enforce drug laws. This stereotype is vastly untrue, by 1969 only 4% of Americans had even tried marijuana. In reality very few people consumed drugs.
Legacy.
The legacy of the hippie movement continues to permeate Western society. In general, unmarried couples of all ages feel free to travel and live together without societal disapproval. Frankness regarding sexual matters has become more common, and the rights of homosexual, bisexual and transsexual people, as well as people who choose not to categorize themselves at all, have expanded. Religious and cultural diversity has gained greater acceptance. Co-operative business enterprises and creative community living arrangements are more accepted than before. Some of the little hippie health food stores of the 1960s and 1970s are now large-scale, profitable businesses, due to greater interest in natural foods, herbal remedies, vitamins and other nutritional supplements. Authors Stewart Brand and John Markoff argue that the development and popularization of personal computers and the Internet find one of their primary roots in the anti-authoritarian ethos promoted by hippie culture.
Distinct appearance and clothing was one of the immediate legacies of hippies worldwide. During the 1960s and 1970s, mustaches, beards and long hair became more commonplace and colorful, while multi-ethnic clothing dominated the fashion world. Since that time, a wide range of personal appearance options and clothing styles, including nudity, have become more widely acceptable, all of which was uncommon before the hippie era. Hippies also inspired the decline in popularity of the necktie and other "business" clothing, which had been unavoidable for men during the 1950s and early 1960s. Additionally, hippie fashion itself has been commonplace in the years since the 1960s in clothing and accessories, particularly the peace symbol. Astrology, including everything from serious study to whimsical amusement regarding personal traits, was integral to hippie culture. The generation of the 1970s became influenced by the hippie and the 60s countercultural legacy. As such in New York City musicians and audiences from the female, homosexual, black, and Latino communities adopted several traits from the hippies and psychedelia. They included overwhelming sound, free-form dancing, weird lighting, colorful costumes, and hallucinogens. Psychedelic soul groups like the Chambers Brothers and especially Sly and The Family Stone influenced proto-disco acts such as Isaac Hayes, Willie Hutch and the Philadelphia Sound. In addition, the perceived positivity, lack of irony, and earnestness of the hippies informed proto-disco music like M.F.S.B.'s album "Love Is the Message".
The hippie legacy in literature includes the lasting popularity of books reflecting the hippie experience, such as "The Electric Kool-Aid Acid Test". In music, the folk rock and psychedelic rock popular among hippies evolved into genres such as acid rock, world beat and heavy metal music. Psychedelic trance (also known as psytrance) is a type of electronic music influenced by 1960s psychedelic rock. The tradition of hippie music festivals began in the United States in 1965 with Ken Kesey's Acid Tests, where the Grateful Dead played tripping on LSD and initiated psychedelic jamming. For the next several decades, many hippies and neo-hippies became part of the Deadhead community, attending music and art festivals held around the country. The Grateful Dead toured continuously, with few interruptions between 1965 and 1995. Phish and their fans (called "Phish Heads") operated in the same manner, with the band touring continuously between 1983 and 2004. Many contemporary bands performing at hippie festivals and their derivatives are called jam bands, since they play songs that contain long instrumentals similar to the original hippie bands of the 1960s.
With the demise of Grateful Dead and Phish, nomadic touring hippies attend a growing series of summer festivals, the largest of which is called the Bonnaroo Music & Arts Festival, which premiered in 2002. The Oregon Country Fair is a three-day festival featuring handmade crafts, educational displays and costumed entertainment. The annual Starwood Festival, founded in 1981, is a seven-day event indicative of the spiritual quest of hippies through an exploration of non-mainstream religions and world-views, and has offered performances and classes by a variety of hippie and counter-culture icons.
The Burning Man festival began in 1986 at a San Francisco beach party and is now held in the Black Rock Desert northeast of Reno, Nevada. Although few participants would accept the "hippie" label, Burning Man is a contemporary expression of alternative community in the same spirit as early hippie events. The gathering becomes a temporary city (36,500 occupants in 2005, 50,000+ in 2011), with elaborate encampments, displays, and many art cars. Other events that enjoy a large attendance include the Rainbow Family Gatherings, The Gathering of the Vibes, Community Peace Festivals, and the Woodstock Festivals.
In the UK, there are many new age travellers who are known as hippies to outsiders, but prefer to call themselves the Peace Convoy. They started the Stonehenge Free Festival in 1974, but English Heritage later banned the festival in 1985, resulting in the Battle of the Beanfield. With Stonehenge banned as a festival site, new age travellers gather at the annual Glastonbury Festival. Today, hippies in the UK can be found in parts of South West England, such as Bristol (particularly the neighborhoods of Montpelier, Stokes Croft, St Werburghs, Bishopston, Easton and Totterdown), Glastonbury in Somerset, Totnes in Devon, and Stroud in Gloucestershire, as well as areas of London and Brighton. In the summer, many hippies and those of similar subcultures gather at numerous outdoor festivals in the countryside.
In New Zealand between 1976 and 1981 tens of thousands of hippies gathered from around the world on large farms around Waihi and Waikino for music and alternatives festivals. Named "Nambassa", the festivals focused on peace, love, and a balanced lifestyle. The events featured practical workshops and displays advocating alternative lifestyles, self sufficiency, clean and sustainable energy and sustainable living.
In the UK and Europe, the years 1987 to 1989 were marked by a large-scale revival of many characteristics of the hippie movement. This later movement, composed mostly of people aged 18 to 25, adopted much of the original hippie philosophy of love, peace and freedom. The summer of 1988 became known as the Second Summer of Love. Although the music favored by this movement was modern electronic music, especially house music and acid house, one could often hear songs from the original hippie era in the "chill out rooms" at raves. In the UK, many of the well-known figures of this movement first lived communally in Stroud Green, an area of north London located in Finsbury Park. In 1995, The Sekhmet Hypothesis attempted to link both hippie and rave culture together in relation to transactional analysis, suggesting that rave culture was a social archetype based on the mood of friendly strength, compared to the gentle hippie archetype, based on friendly weakness. The later electronic dance genres known as goa trance and psychedelic trance and its related events and culture have important hippie legacies and neo hippie elements. The popular DJ of the genre Goa Gil, like other hippies from the 1960s, decided to leave the US and Western Europe to travel on the hippie trail and later developing psychedelic parties and music in the Indian island of Goa in which the goa and psytrance genres were born and exported around the world in the 1990s and 2000s.
Popular films depicting the hippie ethos and lifestyle include "Woodstock", "Easy Rider", "Hair", "The Doors", "Across the Universe", "Taking Woodstock", and "Crumb".
In 2002, photojournalist John Bassett McCleary published a 650-page, 6,000-entry unabridged slang dictionary devoted to the language of the hippies titled "The Hippie Dictionary: A Cultural Encyclopedia of the 1960s and 1970s". The book was revised and expanded to 700 pages in 2004. McCleary believes that the hippie counterculture added a significant number of words to the English language by borrowing from the lexicon of the Beat Generation, through the hippies' shortening of beatnik words and then popularizing their usage. 
In 2005, journalist Oliver Benjamin founded The Church of Latter-Day Dude, a website-philosophy and mock religion inspired by the character "the Dude", a former hippie, in the 1998 movie "The Big Lebowski". Dudeism, as it is known, holds many connections to the hippie ethos, from its “take it easy” attitude and rebel shrug, to its come-as-you-are sense of individual freedom and expression. Dudeism is very much influenced by the hippie movement, maintaining that the "revolution is not over", that it actually began a very long time ago, and will continue far into the future. Dudeist literature even claims that Dudeism has provided a contemporary spiritual home for the hippie philosophy.

</doc>
<doc id="47651" url="https://en.wikipedia.org/wiki?curid=47651" title="Reproducibility">
Reproducibility

Reproducibility is the ability of an entire experiment or study to be duplicated, either by the same researcher or by someone else working independently. Reproducing an experiment is called replicating it. Reproducibility is one of the main principles of the scientific method.
The values obtained from distinct experimental trials are said to be "commensurate" if they are obtained according to the same reproducible experimental description and procedure. The basic idea can be seen in Aristotle's dictum that there is no scientific knowledge of the individual, where the word used for "individual" in Greek had the connotation of the "idiosyncratic", or wholly isolated occurrence. Thus all knowledge, all science, necessarily involves the formation of general concepts and the invocation of their corresponding symbols in language (cf. Turner). Aristotle′s conception about the knowledge of the individual being considered unscientific is due to lack of the field of statistics in his time, so he could not appeal to statistical averaging by the individual.
A particular experimentally obtained value is said to be reproducible if there is a high degree of agreement between measurements or observations conducted on replicate specimens in different locations by different people—that is, if the experimental value is found to have a high precision.
History.
The first to stress the importance of reproducibility in science was the Irish chemist Robert Boyle, in England in the 17th century. Boyle's air pump was designed to generate and study vacuum, which at the time was a very controversial concept. Indeed, distinguished philosophers such as René Descartes and Thomas Hobbes denied the very possibility of vacuum existence. Historians of science e.g. bob Steven Shapin and Simon Schaffer, in their 1985 book "Leviathan and the Air-Pump", describe the debate between Boyle and Hobbes, ostensibly over the nature of vacuum, as fundamentally an argument about how useful knowledge should be gained. Boyle, a pioneer of the experimental method, maintained that the foundations of knowledge should be constituted by experimentally produced facts, which can be made believable to a scientific community by their reproducibility. By repeating the same experiment over and over again, Boyle argued, the certainty of fact will emerge.
The air pump, which in the 17th century was a complicated and expensive apparatus to build, also led to one of the first documented disputes over the reproducibility of a particular scientific phenomenon. In the 1660s, the Dutch scientist Christiaan Huygens built his own air pump in Amsterdam, the first one outside the direct management of Boyle and his assistant at the time Robert Hooke. Huygens reported an effect he termed "anomalous suspension", in which water appeared to levitate in a glass jar inside his air pump (in fact suspended over an air bubble), but Boyle and Hooke could not replicate this phenomenon in their own pumps. As Shapin and Schaffer describe, “it became clear that unless the phenomenon could be produced in England with one of the two pumps available, then no one in England would accept the claims Huygens had made, or his competence in working the pump”. Huygens was finally invited to England in 1663, and under his personal guidance Hooke was able to replicate anomalous suspension of water. Following this Huygens was elected a Foreign Member of the Royal Society. However, Shapin and Schaffer also note that “the accomplishment of replication was dependent on contingent acts of judgment. One cannot write down a formula saying when replication was or was not achieved”.
The philosopher of science Karl Popper noted briefly in his famous 1934 book "The Logic of Scientific Discovery" that “non-reproducible single occurrences are of no significance to science”. The Statistician Ronald Fisher wrote in his 1935 book "The Design of Experiments", which set the foundations for the modern scientific practice of hypothesis testing and statistical significance, that “we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us statistically significant results”. Such assertions express a common dogma in modern science that reproducibility is a necessary condition (although not necessarily sufficient) for establishing a scientific fact, and in practice for establishing scientific authority in any field of knowledge. However, as noted above by Shapin and Schaffer, this dogma is not well-formulated quantitatively, such as statistical significance for instance, and therefore it is not explicitly established how many times must a fact be replicated to be considered reproducible.
Reproducible data.
Reproducibility is one component of the precision of a measurement or test method. The other component is repeatability which is the degree of agreement of tests or measurements on replicate specimens by the same observer in the same laboratory. Both repeatability and reproducibility are usually reported as a standard deviation. A reproducibility limit is the value below which the difference between two test results obtained under reproducibility conditions may be expected to occur with a probability of approximately 0.95 (95%).
Reproducibility is determined from controlled interlaboratory test programs or a measurement systems analysis.
Although they are often confused, there is an important distinction between replicates and an independent repetition of an experiment. Replicates are performed within an experiment. They are not and cannot provide independent evidence of reproducibility. Rather they serve as an internal "check" on an experiment and should not be shown as part of the experimental results within a scientific publication. It is the independent repetition of an experiment that serves to underpin its reproducibility 
Reproducible research.
The term "reproducible research" refers to the idea that the ultimate product of academic research is the paper along with the full computational environment used to produce the results in the paper such as the code, data, etc. that can be used to reproduce the results and create new work based on the research.
Psychology has seen a renewal of internal concerns about irreproducible results. Researchers showed in a 2006 study that, of 141 authors of a publication from the American Psychology Association (APA) empirical articles, 103 (73%) did not respond with their data over a 6-month period. In a follow up study published in 2015, it was found that 246 out of 394 contacted authors of papers in APA journals did not share their data upon request (62%). In a 2012 paper, it was suggested that researchers should publish data along with their works, and a dataset was released alongside as a demonstration. In 2015, Psychology became the first discipline to conduct and publish an open, registered empirical study of reproducibility called the Reproducibility Project. 270 researchers from around the world collaborated to replicate 100 empirical studies from three top Psychology journals. Fewer than half of the attempted replications were successful.
There have been initiatives to improve reporting and hence reproducibility in the medical literature for many years, which began with the CONSORT initiative, which is now part of a wider initiative, the EQUATOR network. This group has recently turned its attention to how better reporting might reduce waste in research, especially biomedical research.
Reproducible research is key to new discoveries in pharmacology. A Phase I discovery will be followed by Phase II reproductions as a drug develops towards commercial production. In recent decades Phase II success has fallen from 28% to 18%. A 2011 study found that 65% of medical studies were inconsistent when re-tested, and only 6% were completely reproducible.
In 2012, a study by Begley and Ellis was published in "Nature" that reviewed a decade of research. That study found that 47 out of 53 medical research papers focused on cancer research were irreproducible. The irreproducible studies had a number of features in common, including that studies were not performed by investigators blinded to the experimental versus the control arms, there was a failure to repeat experiments, a lack of positive and negative controls, failure to show all the data, inappropriate use of statistical tests and use of reagents that were not appropriately validated. John P. A. Ioannidis writes, "While currently there is unilateral emphasis on 'first' discoveries, there should be as much emphasis on replication of discoveries." The "Nature" study was itself reproduced in the journal PLOS ONE, which confirmed that a majority of cancer researchers surveyed had been unable to reproduce a result. Attempts to reproduce studies often strained relationships with the laboratories that were first to publish.
Noteworthy irreproducible results.
Hideyo Noguchi became famous for correctly identifying the bacterial agent of syphilis, but also claimed that he could culture this agent in his laboratory. Nobody else has been able to produce this latter result.
In March 1989, University of Utah chemists Stanley Pons and Martin Fleischmann reported the production of excess heat that could only be explained by a nuclear process ("cold fusion"). The report was astounding given the simplicity of the equipment: it was essentially an electrolysis cell containing heavy water and a palladium cathode which rapidly absorbed the deuterium produced during electrolysis. The news media reported on the experiments widely, and it was a front-page item on many newspapers around the world (see science by press conference). Over the next several months others tried to replicate the experiment, but were unsuccessful.
Nikola Tesla claimed as early as 1899 to have used a high frequency current to light gas-filled lamps from over away without using wires. In 1904 he built Wardenclyffe Tower on Long Island to demonstrate means to send and receive power without connecting wires. The facility was never fully operational and was not completed due to economic problems, so no attempt to reproduce his first result was ever carried out.
Other examples which contrary evidence has refuted the original claim:
Hard Determinism and Reproducibility.
If reality is governed by Superdeterminism or other forms of hard determinism, no experiment is Truly Reproducible as the exact global patch of that event in spacetime (i.e.: reality) only occurs once. Locally, the same spatio-temporal patterns appear to occur everywhere, but this is only a local observation. This result follows as, in order to Exactly Reproduce an event, ALL the variables that cause the event have to be the same AND the event has to be the same, Superdeterminism implies that this can only happen if it is predetermined to happen, and not otherwise.

</doc>
<doc id="47653" url="https://en.wikipedia.org/wiki?curid=47653" title="Chess clock">
Chess clock

A chess clock consists of two adjacent clocks with buttons to stop one clock while starting the other, so that the two clocks never run simultaneously. Chess clocks are used in chess and other two-player games where the players move in turn. The purpose is to keep track of the total time each player takes for their own moves, and ensure that neither player overly delays the game.
Chess clocks were first used extensively in tournament chess, and are often called game clocks. The first time that game clocks were used in a chess tournament was in the London 1883 tournament. Their use has since spread to tournament Scrabble, shogi, go, and nearly every competitive two-player board game, as well as other types of games. 
In a tournament, the arbiter typically places all clocks in the same orientation, so that they can easily assess games that need attention at later stages.
The simplest time control is "sudden death", in which players must make a predetermined number of moves in a certain amount of time or forfeit the game immediately.
A particularly popular variant in informal play is blitz chess, in which each player is given a short time (e.g. five minutes) on the clock in which to play the entire game.
The players may take more or less time over any individual move. The opening moves in chess are often played quickly due to their familiarity, which leaves the players more time to consider more complex and unfamiliar positions later. It is not unusual in slow chess games for a player to leave the table, but the clock of the absent player continues to run if it is their turn, or starts to run if their opponent makes a move.
Analog game clocks.
Analog clocks are equipped with a "flag" (a Dutch invention) that falls to indicate the exact moment the player's time has expired. Analog clocks use mechanical buttons. Pressing the button on one player's side physically stops the movement of that player's clock and releases the hold on the opponent's.
The drawbacks of the mechanical clocks include accuracy and matching of the two clocks, and matching of the indicators (flags) of time expiration. Additional time cannot easily be added for more complex time controls, especially those that call for an increment or delay on every move, such as some forms of byoyomi. However, a malfunctioning analog clock is a less serious event than a malfunctioning digital clock.
Early development of digital game clocks.
In 1973, to address the issues with analog clocks, Bruce Cheney, a Cornell University Electrical Engineering student and chess player, created the first digital chess clock as a project for an undergraduate EE course. Typical of most inventions, it was crude compared to the products on the market many years later and was limited by the technology that existed at the time. For example, the display was done with red LEDs. LEDs require significant power, and as a result, the clock had to be plugged into a wall outlet. The high cost of LEDs at the time meant that only one set of digits could be displayed, that of the player whose turn it was to move. This meant that each player's time had to be multiplexed to the display when their time was running. In 1973, LSI chips were not readily or cheaply available, so all the multiplexing and logic were done using chips that consisted of four two-input TTL NAND gates, which resulted in excessive power consumption. Being plugged into the wall is obviously a major drawback, but had one advantage: the timebase for the clock was driven off of a rectified version of 60 cycle AC current. Each player had a separate counter, and, in a parallel to the original mechanical architecture, one player's counter was disabled while the other's was running. The clock only had one mode: time ran forward. It could be reset, but not set. It did not count the number of moves. But it successfully addressed the original goals of the project (accurate and matched timing).
A chess clock that was patented in 1975 was developed by Joseph Meshi and became the first commercially available digital chess clock. The patent numbers are 4,062,180 (filed on July 1975) & 4,247,925 (filed on July 1978). Mr. Meshi published his MBA Thesis on this subject titled-"Demand Analysis for a New Product (The Digital Chess Clock)", at San Diego State University in 1978. Mr. Meshi designed and built two versions of the clock-the Micromate-80 (based on the original patent) and the Micromate-180, which incorporated the latest innovations. The Micromate 180 was truly revolutionary and clearly ahead of its time.
Recent developments of digital clocks and current usage.
Digital clocks and Internet gaming have spurred a wave of experimentation with more varied and complex time controls than the traditional standards. Time control is commonly used in modern chess in many different methodologies. One particularly notable development, which has gained quite wide acceptance in chess, was proposed by former world champion Bobby Fischer, who in 1988 filed for (awarded in 1989) for a new type of digital chess clock. Fischer's digital clock gave each player a fixed period of time at the start of the game and then added a small amount after each move. Joseph Meshi called this "Accumulation" as it was a main feature of his patented Micromate-180 (US Patent 4,247,925 1978). This became the linchpin of Fischer's clock patented 10 years later. In this way, the players would never be desperately short of time, but games could also be completed more quickly, doing away with the need for adjournments (in which a game is left incomplete to be finished at a later date). Although it was slow to catch on, as of 2004 a very large number of top class tournaments use Fischer's system, though usually in combination with the more traditional clocks (at lower levels, more traditional clocks are still employed, as they are cheaper). Other aspects of Fischer's patent, such as a synthesized voice announcing how much time the players have, thus eliminating the need for them to keep looking at the clock, have not been adopted.
On March 10, 1994, a patent application was filed by inventors Frank A. Camaratta, jr. of Huntsville, AL, and William Goichberg of Salisbury Mills, NY for a game timer, especially suitable for playing the game of chess, which employed a "Delay" feature. The game timer provides, among other features, a user definable delay between the time the activation button is pressed and the time that the activated clock actually begins to count down. United States Patent 5,420,830 was issued on May 10, 1995 and subsequently assigned to the United States Chess Federation by the inventors. The benefit of the delay clock is to reduce the likelihood that a player with positional and/or material superiority will lose a match solely because of the expiration of time on that player's time clock. The "Delay" mode is still a popular feature for both Standard and sudden-death time controls in major tournaments throughout the U.S.
Time controls.
There are five main types of "Time Controls": (1) "Fischer" (invented by Bobby Fischer), (2) "Bronstein" (invented by David Bronstein), (3) "Simple Delay", (4) "Overtime Penalty" and (5) "Hour Glass". The first three time controls implement some sort of delay clock, a small amount of time that is added for each move. The reason is that with a sudden-death time limit, all moves must be completed in the specified time, or the player loses. With a small delay added at each move, the player always has at least that much time to make a move. The three types of delay clocks differ in how the delay is implemented. The last two time controls are somewhat different, as they do not rely on time delay, as explained below.

</doc>
<doc id="47656" url="https://en.wikipedia.org/wiki?curid=47656" title="Kamov">
Kamov

Kamov is a Russian rotorcraft manufacturing company, founded by Nikolai Ilyich Kamov, who started building his first rotary-winged aircraft in 1929, together with N. K. Skrzhinskii. Up to the 1940s, they created many autogyros, including the TsAGI A-7-3, the only armed autogyro to see (limited) combat action.
The Kamov Design Bureau (design office prefix Ka) has more recently specialised in compact helicopters with coaxial rotors, suitable for naval service and high-speed operations.
Kamov merged with Mil and Rostvertol to form Oboronprom Corp. in 2006. The Kamov brand name was retained, though the new company dropped overlapping product lines.

</doc>
<doc id="47658" url="https://en.wikipedia.org/wiki?curid=47658" title="Aérospatiale">
Aérospatiale

Aérospatiale (), was a French state-owned aerospace manufacturer that built both civilian and military aircraft, rockets and satellites. It was originally known as Société nationale industrielle aérospatiale (SNIAS). Its head office was in the 16th arrondissement of Paris.
The former assets of Aérospatiale are now part of Airbus Group, except the Satellites activities which merged with Alcatel and became Alcatel Space, in 1999, now Thales Alenia Space.
History.
The company (as SNIAS) was created in 1970 by the merger of the state-owned companies Sud Aviation, Nord Aviation and "Société d'études et de réalisation d'engins balistiques" (SEREB). Starting in 1971 it was directed by Henri Ziegler. Its North American Marketing arm French Aerospace Corporation was renamed to European Aerospace Corporation in 1971.
In 1991 the company helped construct the revolutionary chassis of the Bugatti EB110 Supercar. The chassis was built completely of carbon fibre, and was very lightweight.
In 1992, DaimlerBenz Aerospace AG (DASA) and Aérospatiale combined their helicopter divisions to form the Eurocopter Group.
In 1999, Aérospatiale, except for the satellites activities, merged with Matra Haute Technologie to form Aérospatiale-Matra. In 2001, Aérospatiale-Matra's missile group was merged with Matra BAe Dynamics and the missile division of Alenia Marconi Systems to form MBDA. Lionel Jospin's Plural Left government initiated the privatization of Aérospatiale.
On July 10, 2000, Aérospatiale-Matra merged with Construcciones Aeronáuticas SA (CASA) of Spain and DaimlerChrysler Aerospace AG (DASA) of Germany to form the European Aeronautic Defence and Space Company (EADS).

</doc>
<doc id="47660" url="https://en.wikipedia.org/wiki?curid=47660" title="Espresso">
Espresso

Espresso (, ) is coffee brewed by forcing a small amount of nearly boiling water under pressure through finely ground coffee beans. Espresso is generally thicker than coffee brewed by other methods, has a higher concentration of suspended and dissolved solids, and has "crema" on top (a foam with a creamy consistency). As a result of the pressurized brewing process, the flavors and chemicals in a typical cup of espresso are very concentrated. Espresso is also the base for other drinks such as a caffè latte, cappuccino, caffè macchiato, cafe mocha, or caffè Americano. Espresso has more caffeine per unit volume than most coffee beverages, but because the usual serving size is much smaller, the total caffeine content is less than a mug of standard brewed coffee, contrary to a common belief. Although the actual caffeine content of any coffee drink varies by size, bean origin, roast method and other factors, the caffeine content of "typical" servings of espresso vs. drip brew are 120 to 170 mg vs. 150 to 200 mg.
Brewing process.
Espresso is made by forcing very hot water under high pressure through finely ground, compacted coffee. Tamping down the coffee promotes the water's even penetration of the grounds. This process produces an almost syrupy beverage by extracting both solid and dissolved components. The "crema" is produced by emulsifying the oils in the ground coffee into a colloid, which does not occur in other brewing methods. There is no universal standard defining the process of extracting espresso, but there are several published definitions which attempt to place constraints on the amount and type of ground coffee used, the temperature and pressure of the water, and the rate of extraction. Generally, one uses an espresso machine to make espresso. The act of producing a shot of espresso is often termed "pulling" a shot, originating from lever espresso machines, which require pulling down a handle attached to a spring-loaded piston, forcing hot water through the coffee at high pressure. Today, however, it is more common for the pressure to be generated by an electric pump.
The technical parameters outlined by the Italian Espresso National Institute for making 
a "certified Italian espresso" are:
Espresso roast.
Espresso is both a coffee beverage and a brewing method. It is not a specific bean, bean blend, or roast level. Any bean or roasting level can be used to produce authentic espresso. For example, in southern Italy, a darker roast is generally preferred. Farther north, the trend moves toward slightly lighter roasts, while outside Italy, a wide range is popular.
Popularity.
Espresso has risen in popularity worldwide since the 1980s. In the United States, cafés serve many variations by adding syrup, whipped cream, flavor extracts, soy milk, and spices to their drinks. The American Pacific Northwest has been viewed as the driver behind this trend. The popularity later spread to shops in other regions and into homes as kitchen-friendly machines became available at moderate cost. In other parts of the world, espresso has long been the customary method of coffee preparation in restaurants, bars and coffee shops.
History.
Angelo Moriondo’s Italian patent for a steam-driven "instantaneous" coffee beverage making device, which was registered in Turin in 1884 (No. 33/256), is notable. Author Ian Bersten, whose history of coffee brewers is cited below, claims to have been the first to discover Moriondo’s patent. Bersten describes the device as “… almost certainly the first Italian bar machine that controlled the supply of steam and water separately through the coffee” and Moriondo as “... certainly one of the earliest discoverers of the expresso machine, if not the earliest.” Unlike true espresso machines, it was a bulk brewer, and did not brew coffee “expressly” for the individual customer.
Seventeen years later, in 1901, Luigi Bezzera, from Milan, came up with a number of improvements to the espresso machine. He patented a number of these, the first of which was applied for on the 19th of December 1901. It was titled “Innovations in the machinery to prepare and immediately serve coffee beverage” (Patent No. 153/94, 61707, granted on the 5th of June 1902).
In 1905, the patent was bought by Desiderio Pavoni, who founded the “La Pavoni” company and began to produce the machine industrially (one a day) in a small workshop in Via Parini in Milan.
The popularity of espresso developed in various ways; a detailed discussion of the spread of espresso is given in , which is a source of various statements below.
In Italy, the rise of espresso consumption was associated with urbanization, espresso bars providing a place for socializing. Further, coffee prices were controlled by local authorities, provided the coffee was consumed standing up, encouraging the "stand at a bar" culture.
In the English-speaking world, espresso became popular, particularly in the form of cappuccino, due to the tradition of drinking coffee with milk and the exotic appeal of the foam; in the United States, this was more often in the form of lattes, with or without flavored syrups added. The latte is claimed to have been invented in the 1950s by Italian American Lino Meiorin of Caffe Mediterraneum in Berkeley, California, as a long cappuccino, and was then popularized in Seattle, and then nationally and internationally by Seattle-based Starbucks in the late 1980s and 1990s.
In the United Kingdom, espresso grew in popularity among youth in the 1950s, who felt more welcome in the coffee shops than in public houses (pubs).
Espresso was initially popular, particularly within the Italian diaspora, growing in popularity with tourism to Italy exposing others to espresso, as developed by Eiscafès established by Italians in Germany.
Initially, expatriate Italian espresso bars were downmarket venues, serving the working class Italian diaspora – and thus providing appeal to the alternative subculture / counterculture; this can still be seen in the United States in Italian American neighborhoods, such as Boston's North End, New York's Little Italy, and San Francisco's North Beach. As specialty coffee developed in the 1980s (following earlier developments in the 1970s and even 1960s), an indigenous artisanal coffee culture developed, with espresso instead positioned as an upmarket drink.
Today, coffee culture commentators distinguish large chain, midmarket coffee as "Second Wave Coffee", and upmarket, artisanal coffee as "Third Wave Coffee".
In the Middle East, espresso is growing in popularity, with the opening of Western coffee shop chains.
Café vs. home preparation.
A distinctive feature of espresso, as opposed to brewed coffee, is espresso's association with cafés, due both to the specialized equipment and skill required, thus making the enjoyment of espresso a social experience.
Home espresso machines have increased in popularity with the general rise of interest in espresso. Today, a wide range of home espresso equipment can be found in kitchen and appliance stores, online vendors, and department stores. The first espresso machine for home use was the Gaggia Gilda. Soon afterwards, similar machines such as the Faema Faemina, FE-AR La Peppina and VAM Caravel followed suit in similar form factor and operational principles. These machines still have a small but dedicated share of fans. Until the advent of the first small electrical pump-based espresso machines such as the Gaggia Baby and Quickmill 810, home espresso machines were not widely adopted. In recent years, the increased availability of convenient counter-top fully automatic home espresso makers and pod-based espresso serving systems has increased the quantity of espresso consumed at home.
The popularity of home espresso making parallels the increase of home coffee roasting. Some amateurs pursue both home roasting coffee and making espresso.
Etymology and usage of the term.
Although some Anglo-American dictionaries simply refer to "pressed-out", "espresso," much like the English word "express", conveys the senses of "just for you" and "quickly," which can be related to the method of espresso preparation.
Another source, the "Online Etymology Dictionary", favors the "pressed out" explanation: "coffee made under steam pressure, 1945, from Italian (caffe) espresso, from espresso 'pressed out,' past participle of esprimere, from Latin exprimere 'press out, squeeze out' ... [, i]n reference to the steam pressure."
Modern espresso, using hot water under pressure, as pioneered by Gaggia in the 1940s, was originally called "crema caffè", in English "cream coffee", as can be seen on old Gaggia machines, due to the crema. This term is no longer used, though "crema caffè" and variants ("caffè crema, café crema") find occasional use in branding.
There is a debate over whether the spelling expresso is incorrect or whether it is an acceptable variant. It is called a less common variant in some sources. Italy uses the term "espresso", substituting most "x" letters in Latin root words with "s"; x is not considered part of the standard Italian alphabet. Italian people commonly refer to it simply as "caffè" (coffee), espresso being the ordinary coffee to order; in Spain, while "café expreso" is seen as the more "formal" denomination, "café solo" (alone, without milk) is the usual way to ask for it when at an espresso bar. Some sources state that "expresso" is an incorrect spelling, including "Garner's Modern American Usage". While the 'expresso' spelling is recognized as mainstream usage in some American dictionaries, some cooking websites call the 'x' variant illegitimate. Oxford Dictionaries online states "The spelling "expresso" is not used in the original Italian and is strictly incorrect, although it is common." The "Oxford English Dictionary" and "Merriam-Webster", though, call it a variant spelling. The "Online Etymology Dictionary" calls "expresso" a variant of "espresso."
Shot variables.
The main variables in a shot of espresso are the "size" and "length". This terminology is standardized, but the precise sizes and proportions vary substantially.
Cafés may have a standardized shot (size and length), such as "triple "ristretto"", only varying the number of shots in espresso-based drinks such as lattes, but not changing the extraction – changing between a double and a triple requires changing the filter basket size, while changing between "ristretto", "normale", and "lungo" may require changing the grind, which is less easily accommodated in a busy café, as fine tweaking of the grind is a central aspect to consistent quality espresso-making.
Size.
The size can be a single, double, or triple, using a proportional amount of ground coffee, roughly 7, 14, and 21 grams; correspondingly sized filter baskets are used. The Italian multiplier term "doppio" is often used for a double, with "solo" and "triplo" being more rarely used for singles and triples. The single shot is the traditional shot size, being the maximum that could easily be pulled on a lever machine, while the double is the standard shot today.
Single baskets are sharply tapered or stepped down in diameter to provide comparable depth to the double baskets and, therefore, comparable resistance to water pressure. Most double baskets are gently tapered (the "Faema model"), while others, such as the La Marzocco, have straight sides. Triple baskets are normally straight-sided.
Portafilters will often come with two spouts, usually closely spaced, and a double-size basket – each spout can optionally dispense into a separate cup, yielding two solo-size (but doppio-brewed) shots, or into a single cup (hence the close spacing). True "solo" shots are rare, with a single shot in a café generally being half of a "doppio" shot.
In espresso-based drinks, particularly larger milk-based drinks, a drink with three or four shots of espresso will be called a "triple" or "quad", respectively.
Length.
The length of the shot can be "ristretto" (or "stretto") (reduced), "normale"/standard (normal), or "lungo" (long): these may correspond to a smaller or larger drink with the same amount of ground coffee and same level of extraction or to different length of extraction. Proportions vary and the volume (and low density) of crema make volume-based comparisons difficult (precise measurement uses the mass of the drink). Typically "ristretto" is half the volume of "normale", and "lungo" is double to triple the "normale" volume. For a double shot, (14 grams of dry coffee), a "normale" uses about 60 ml of water. A "double ristretto," a common form associated with artisanal espresso, uses half the amount of water, about 30 ml.
"Ristretto, normale", and "lungo" may not simply be the same shot, stopped at different times – which may result in an underextracted shot (if run too short a time) or an overextracted shot (if run too long a time). Rather, the grind is adjusted (finer for "ristretto", coarser for "lungo") so the target volume is achieved by the time extraction finishes.
A significantly longer shot is the "caffè crema", which is longer than a "lungo", ranging in size from 120–240 ml (4–8 US fl oz), and brewed in the same way, with a coarser grind.
The method of adding hot water produces a milder version of original flavor, while passing more water through the load of ground coffee will add other flavors to the espresso, which might be unpleasant for some people.
Nutrition.
Likely due to its higher amount of suspended solids than typical coffee which is absent of essential nutrients, espresso has significant contents of the dietary mineral magnesium, the B vitamins niacin and riboflavin, and 212 mg of caffeine per 100 grams of grounds (table).
Espresso-based drinks.
In addition to being served alone, espresso is frequently blended, notably with milk - either steamed (without significant foam), wet foamed ("microfoam"), or dry foamed, and with hot water. Notable milk-based espresso drinks, in order of size, include: macchiato, cappuccino, flat white, and latte; other milk and espresso combinations include latte macchiato, cortado and galão, which are made primarily with steamed milk with little or no foam. Espresso and water combinations include Americano and long black. Other combinations include coffee with espresso, sometimes called "red eye" or "shot in the dark".
In order of size, these may be organized as follows:
Some common combinations may be organized graphically as follows:
Methods of preparation differ between drinks and between baristas. For macchiatos, cappuccino, flat white, and smaller lattes and Americanos, the espresso is brewed into the cup, then the milk or water is poured in. For larger drinks, where a tall glass will not fit under the brew head, the espresso is brewed into a small cup, then poured into the larger cup; for this purpose a demitasse or specialized espresso brew pitcher may be used. This "pouring into an existing glass" is a defining characteristic of the latte macchiato and classic renditions of the red eye. Alternatively, a glass with "existing" water may have espresso brewed into it – to preserve the crema – in the long black. Brewing onto milk is not generally done.

</doc>
<doc id="47661" url="https://en.wikipedia.org/wiki?curid=47661" title="Perkele">
Perkele

Perkele () means devil in modern Finnish and is used as a rude profanity. Some researchers consider "Perkele" an original name of Ukko, the chief god of the Finnish pagan pantheon but this view is not shared by all researchers. There are related words in other Balto-Finnic languages: in Estonian, "põrgu" means hell, in Karelian "perkeleh" means an evil spirit.
Origins.
The name is of Indo-European origin. Related gods from other areas are Perkūnas (Lithuania), Pērkons (Latvia), Percunis (Prussia), Piarun (Belarus), Peko or Pekolasõ (Estonia) and Perun or Piorun (Bulgaria, Croatia, Czech Republic, Poland, Russia, Ukraine, Serbia, Slovakia, Slovenia).
Use.
It has a history of being used as a curse: a cry for the god for strength. It still is a common curse word in vernacular Finnish. To a Finn, the word entails seriousness and potency that more lightly used curses lack. Also, when the Research Institute for the Languages of Finland held a popular contest to nominate the "most energizing" word in the Finnish language, one of the suggestions was Perkele because "it is the curse word that gave the most strength for the reconstruction of Finland after the wars."
For comparison, "Parom" a corrupted form of the name "Perun", is used as a mild curse in Slovak language - "Do Paroma!" is roughly equivalent to perkele in Finnish.
Introduction of Christianity.
As Finland was Christianized, the church started demonizing the Finnish gods. This led to the use of "Perkele" as a translation for "devil" in the Finnish translation of the Bible, . Later, in a 1992 translation, the word is switched to "paholainen".
Uses in popular culture.
Many Finnish heavy metal bands like Impaled Nazarene and Norther use the word perkele for emphasis and to reference Finnishness, while another Finnish metal band, Amorphis, have song titled "Perkele (The God of Fire)", which serves as the sixth track from their album "Eclipse".
In a Finnish-speaking Norwegian whaler crew can be heard exclaiming 'Perkele!' after the Klingon Bird of Prey decloaks ahead of the whaling vessel.

</doc>
<doc id="47662" url="https://en.wikipedia.org/wiki?curid=47662" title="Antonov">
Antonov

Antonov State Company (), formerly the Antonov Aeronautical Scientific-Technical Complex (Antonov ASTC) (), and earlier the Antonov Design Bureau, is a Ukrainian aircraft manufacturing and services company. Antonov's particular expertise is in the fields of very large aeroplanes and aeroplanes using unprepared runways. Antonov (model prefix An-) has built a total of approximately 22,000 aircraft, and thousands of planes are currently operating in the former Soviet Union and in developing countries.
Antonov StC is a state-owned commercial company. Its headquarters and main industrial grounds are located in or adjacent to Kiev. On 12 May 2015 it was transferred from the Ministry of Economic Development and Trade to the Ukroboronprom (Ukrainian Defense Industry).
History.
Soviet era.
Foundation and relocation.
The company was established in 1946 at the Novosibirsk Aircraft Production Association as the top-secret Soviet Research and Design Bureau No. 153. It was headed by Oleg Antonov and specialised in turboprop military transport aircraft. The An-2 biplane was a major achievement of this period, with hundreds of these aircraft still operating as of 2013. In 1952, the Bureau was relocated to Kiev, a city with a rich aviation history and an aircraft-manufacturing infrastructure restored after the destruction caused by World War II.
First serial aircraft and expansion.
The 1957 introduction of the An-10/An-12 family of mid-range turboprop aeroplanes began the successful production of thousands of these aircraft. Their use for both heavy combat and civilian purposes around the globe continues to the present; the An-10/An-12 were used most notably in the Vietnam War, the Soviet war in Afghanistan and the Chernobyl disaster relief megaoperation.
In 1959, the bureau began construction of the separate Flight Testing and Improvement Base in suburban Hostomel (now the Antonov Airport).
In 1965, the Antonov An-22 heavy military transport entered serial production to supplement the An-12 in major military and humanitarian airlifts by the Soviet Union. The model became the first Soviet wide-body aircraft, and it remains the world's largest turboprop-powered aircraft. Antonov designed and presented a nuclear-powered version of the An-22. It was never flight tested.
In 1966, after major expansion in the Sviatoshyn neighbourhood of the city, the company was renamed to another disguise name: "Kiev Mechanical Plant". Two independent aircraft production and repair facilities, under engineering-supervision of the Antonov Bureau, also appeared in Kiev during this period.
Prominence and Antonov's retirement.
In the 1970s and early 1980s, the company established itself as USSR's main designer of military transport aircraft with dozens of new modifications in development and production. After Oleg Antonov's death in 1984, the company is officially renamed as the Research and Design Bureau named after O.K. Antonov () while continuing the use of "Kiev Mechanical Plant" alias for some purposes.
Late Soviet-era: superlarge projects and first commercialisation.
In the late 1980s, the Antonov Bureau achieved global prominence after introduction of its extra large aeroplanes. The An-124 "Ruslan" (1982) became Soviet Union's serial-produced strategic airlifter. The Bureau enlarged the "Ruslan" design even more for the Soviet space shuttle programme logistics, creating the An-225 "Mriya" in 1989. "Mriya" has since been the world's largest and heaviest aeroplane.
End of the Cold War and perestroika allowed the Antonov's first step to commercialisation and foreign expansion. In 1989, the Antonov Airlines subsidiary was created for its own aircraft maintenance and cargo projects.
Independent Ukraine.
Antonov Design Bureau remained a state-owned company after Ukraine achieved its independence in 1991 and is since regarded as a strategic national asset.
Expansion to free market.
Since independence, Antonov is busy with certifying and marketing of its models (both Soviet-era and newly developed) to free commercial aeroplanes' markets. New models introduced to serial production and delivered to customers include the Antonov An-140, Antonov An-148 and Antonov An-158 regional airliners.
Among several modernisation projects, Antonov received orders for upgrading "hundreds" of its legendary An-2 utility planes still in operation in Azerbaijan, Cuba and Russia to the An-2-100 upgrade version.
Production facilities' consolidation.
During the Soviet period, not all Antonov-designed aircraft were manufactured by the company itself. This was a result of Soviet industrial strategy that split military production between different regions of the USSR to minimise potential war loss risks. As a result, Antonov aeroplanes are often assembled by the specialist contract manufacturers.
In 2009, the once-independent "Aviant" aeroplane-assembling plant in Kyiv became part of the Antonov State Company, facilitating a full serial manufacturing cycle of the company. However, the old tradition of co-manufacturing with contractors is continued, both with Soviet-time partners and with new licensees like Iran's HESA.
Products and activities.
Fields of commercial activity of Antonov ASTC include:
Chief Designers.
Aircraft.
Antonov's aeroplanes (design office prefix An) range from the rugged An-2 biplane (which itself is comparatively large for a biplane) through the An-28 reconnaissance aircraft to the massive An-124 Ruslan and An-225 Mriya strategic airlifters (the latter being the world's heaviest aircraft with only one currently in service). Whilst less famous, the An-24, An-26, An-30 and An-32 family of twin turboprop, high winged, passenger/cargo/troop transport aircraft are important for domestic/short-haul air services particularly in parts of the world once led by communist governments. The An-72/An-74 series of small jetliners is slowly replacing that fleet, and a larger An-70 freighter is under certification.
The Antonov An-148 is a new regional airliner of twin-turbofan configuration. Over 150 aircraft have been ordered since 2007. A stretched version is in development, the An-158 (from 60–70 to 90–100 passengers).

</doc>
<doc id="47663" url="https://en.wikipedia.org/wiki?curid=47663" title="Breguet">
Breguet

Breguet may refer to: 

</doc>
<doc id="47665" url="https://en.wikipedia.org/wiki?curid=47665" title="British Aerospace">
British Aerospace

British Aerospace plc (BAe) was a British aircraft, munitions and defence-systems manufacturer. Its head office was at Warwick House in the Farnborough Aerospace Centre in Farnborough, Hampshire. In 1999 it purchased Marconi Electronic Systems, the defence electronics and naval shipbuilding subsidiary of the General Electric Company plc, to form BAE Systems.
History.
1977 to 1997.
The company was formed in the United Kingdom as a statutory corporation on 29 April 1977 as a result of the Aircraft and Shipbuilding Industries Act. This called for the nationalisation and merger of the British Aircraft Corporation, Hawker Siddeley Aviation, Hawker Siddeley Dynamics and Scottish Aviation. In 1979 BAe officially joined Airbus, the UK having previously withdrawn support for the consortium in April 1969.
In accordance with the provisions of the British Aerospace Act 1980 the statutory corporation was changed to a public limited company (plc), British Aerospace Public Limited Company, on 1 January 1981. On 4 February 1981 the government sold 51.57% of its shares. The British government sold its remaining shares in 1985, maintaining a £1 golden share which allows it veto foreign control of the board or company.
On 26 September 1985, the UK and Saudi Arabian governments signed the Al Yamamah contract, with BAe as prime contractor. The contracts, extended in the 1990s and never fully detailed, involved the supply of Panavia Tornado strike and air defence aircraft, Hawk trainer jets, Rapier missile systems, infrastructure works and naval vessels. The Al Yamamah deals are valued at anything up to £20 billion and still continue to provide a large percentage of BAE Systems' profits.
In 1986, With Alenia Aeronautica, CASA and DASA, BAe formed Eurofighter GmbH for the development of the Eurofighter Typhoon.
On 22 April 1987 BAe acquired Royal Ordnance, the British armaments manufacturer, for £190 million. Heckler & Koch GmbH was folded into this division when BAe acquired it in 1991.
In 1988 BAe purchased the Rover Group which then was privatised by the British government of Margaret Thatcher.
In 1991 BAe acquired a 30% interest in Hutchison Telecommunications through a stock swap deal, where Hutchison was given a controlling stake of 65% in BAe’s wholly owned subsidiary - Microtel Communications Ltd. In August 1991, BAe formed a naval systems joint venture, BAeSEMA, with the Sema Group. BAe acquired Sema's 50% share in 1998. 1991 also saw BAe begin to experience major difficulties. BAe saw its share price fall below 100p for the first time. On 9 September 1991, the company issued a profits warning and later that week "bungled" the launch of a £432 million rights issue. On 25 September 1991 BAe directors led by CEO Richard Evans ousted the Chairman Professor Sir Roland Smith in a move described by "The Independent" as "one of the most spectacular and brutal boardroom coups witnessed in many years." Evans described the troubles as a confluence of events:
In 1992 BAe formed Avro RJ Regional Jets to produce the Avro RJ series, an evolution of the BAe 146. In mid-1992 BAe wrote off £1 billion of assets, largely as part of redundancies and restructuring of its regional aircraft division. This was largest asset write-off in UK corporate history. The General Electric Company (GEC), later to sell its defence interests to BAe, came close to acquiring BAe at this time. BAe cut 47% of its workforce (60,000 out of 127,000), 40,000 of which were from the regional aircraft division.
Evans decided to sell non-core business activities which included The Rover Group, Arlington Securities, BAe Corporate Jets, BAe Communications and Ballast Nedam. Although the rationale of diversification was sound (to shield the company from cyclical aerospace and defence markets) the struggling company could not afford to continue the position: "We simply could not afford to carry two core businesses, cars and aerospace. At one point Rover was eating up about £2 billion of our banking capacity." BAe Corporate Jets Ltd and Arkansas Aerospace Inc were sold to Raytheon in 1993. In 1994 the Rover Group was sold to BMW and British Aerospace Space Systems was sold to Matra Marconi Space. In 1998 BAe's shareholding of Orange plc was reduced to 5%. The Orange shareholding was a legacy of the 30% stake in Hutchison Telecommunications (UK) Ltd when Hutchison exchanged its own shares for a mobile phone company (Microtel Communications Ltd) from BAe.
BAeSEMA, Siemens Plessey and GEC-Marconi formed UKAMS Ltd in 1994 as part of the Principal Anti-Air Missile System (PAAMS) consortium. UKAMS would become a wholly owned subsidiary of BAe Dynamics in 1998. In 1995 Saab Military Aircraft and BAe signed an agreement for the joint development and marketing of the export version of the JAS 39 Gripen. In 1996 BAe and Matra Defense agreed to merge their missile businesses into a joint venture called Matra BAe Dynamics. In 1997 BAe joined the Lockheed Martin X-35 Joint Strike Fighter team. The company acquired the UK operations of Siemens Plessey Systems (SPS) in 1998 from Siemens AG. DASA purchased SPS' German assets.
Transition to BAE Systems: 1997 to 1999.
Defence consolidation became a major issue in 1998, with numerous reports linking various European defence groups – mainly with each other but also with American defence contractors. It was widely anticipated that BAe would merge with Germany’s DASA to form a pan-European aerospace giant. A merger deal was negotiated between Richard Evans and DASA CEO Jürgen Schrempp. However, when it became clear that GEC was selling its defence electronics business Marconi Electronic Systems, Evans put the DASA merger on hold in favour of purchasing Marconi. Evans stated in 2004 that his fear was that an American defence contractor would acquire Marconi and challenge both BAe and DASA. Schrempp was angered by Evans' actions and chose instead to merge DASA with Aerospatiale to create the European Aeronautic Defence and Space Company (EADS). This group was joined by Spain’s CASA following an agreement in December 1999.
The GEC merger to create a UK company compared to what would have been an Anglo-German firm, made the possibility of further penetration of the United States (US) defence market more likely. The company, initially called "New British Aerospace", was officially formed on 30 November 1999 and known as BAE Systems.
Products.
BAe was the UK's largest exporter, a Competition Commission report gives a ten-year aggregate figure of £45 billion, with defence sales accounting for approximately 80%.
Corruption investigation and criticisms.
There have been allegations that the Al Yamamah contracts were a result of bribes ("douceurs") to members of the Saudi royal family and government officials. Some allegations suggested that the former Prime Minister's son Mark Thatcher may have been involved; he has strongly denied receiving payments or exploiting his mother's connections in his business dealings. The UK National Audit Office investigated the contracts and has so far never released its conclusions – the only NAO report ever to be withheld. The BBC's "Newsnight" observed that it is ironic that the once classified report analysing the construction of MI5's Thames House and MI6's Vauxhall Cross headquarters has been released, but the Al Yamamah report is still deemed too sensitive.
The 2007 documentary film Welcome Aboard Toxic Airlines contained evidence that vital data was withheld from a 1999-2000 Australian Senate Inquiry into the health and flight safety issues relating to oil fumes on the BAe 146. The film also contains an Australian Senator speech about money being paid by BAe for silence on the fumes issue.

</doc>
<doc id="47667" url="https://en.wikipedia.org/wiki?curid=47667" title="Marconi Electronic Systems">
Marconi Electronic Systems

Marconi Electronic Systems (MES), or GEC-Marconi as it was until 1998, was the defence arm of The General Electric Company (GEC). It was demerged from GEC and acquired by British Aerospace (BAe) on 30 November 1999 to form BAE Systems. GEC then renamed itself Marconi plc.
MES exists today as BAE Systems Electronics Limited, a subsidiary of BAE Systems, but the assets were rearranged elsewhere within that company. MES-related businesses include BAE Systems Submarine Solutions, BAE Systems Surface Ships, BAE Systems Insyte and Selex ES (now a part of Finmeccanica).
History.
MES represented the pinnacle of GEC's defence businesses which had a heritage of almost 100 years. Following GEC's acquisition of Marconi as part of English Electric in 1968 the Marconi brand was used for its defence businesses e.g. Marconi Space & Defence Systems (MSDS), Marconi Underwater Systems Ltd (MUSL). GEC's history of military products dates back to World War I with its contribution to the war effort then including radios and bulbs. World War II consolidated this position with the company involved in many important technological advances, most notably radar.
Between 1945 and GEC's demerger of its defence business in 1999, the company became one of the world's most important defence contractors. GEC's major defence related acquisitions included Associated Electrical Industries in 1967, English Electric Company (including its Marconi subsidiary) in 1968, Yarrow Shipbuilders in 1985, parts of Ferranti's defence business in 1990, Vickers Shipbuilding and Engineering in 1995 and Kvaerner Govan in 1999. In June 1998, MES acquired Tracor, a major American defence contractor, for $1.4bn.
Demerger.
The 1997 merger of American corporations Boeing and McDonnell Douglas, which followed the forming of Lockheed Martin, the world's largest defence contractor in 1995, increased the pressure on European defence companies to consolidate. In June 1997 British Aerospace Defence managing director John Weston commented "Europe... is supporting three times the number of contractors on less than half the budget of the U.S.". European governments wished to see the merger of their defence manufacturers into a single entity, a European Aerospace and Defence Company.
As early as 1995 British Aerospace and the German aerospace and defence company DaimlerChrysler Aerospace (DASA) were said to be keen to create a transnational aerospace and defence company. Merger discussions began between British Aerospace and DASA in July 1998. A merger was agreed between British Aerospace chairman Richard Evans and DASA CEO Jürgen Schrempp in December 1998.
GEC was also under pressure to participate in defence industry consolidation. Reporting the appointment of George Simpson as GEC managing director in 1996, "The Independent" had said "some analysts believe that Mr Simpson's inside knowledge of BAe, a long-rumoured GEC bid target, was a key to his appointment. GEC favours forging a national 'champion' defence group with BAe to compete with the giant US organisations." When GEC put MES up for sale on 22 December 1998, BAE abandoned the DASA merger in favour of purchasing its British rival. The merger of British Aerospace and MES was announced on 19 January 1999. Evans stated that in 2004 that his fear was that an American defence contractor would acquire MES and challenge both British Aerospace and DASA. The merger created a vertically integrated company which "The Scotsman" described as "combination of British Aerospace's contracting and platform-building skills with Marconi's coveted electronics systems capability". for example combining the manufacturer of the Eurofighter with the company that provided many of the aircraft's electronic systems; British Aerospace was MES' largest customer. In contrast, DASA's response to the breakdown of the merger discussion was to merge with Aérospatiale to create the European Aeronautic Defence and Space Company (EADS), a horizontal integration. EADS has since considered a merger with Thales to create a "fully rounded" company.
While MES was responsible for the majority of GEC's defence sales other GEC companies achieved defence related sales, principally GEC Alsthom, GEC-Plessey Telecommunications (GPT) and GEC Plessey Semiconductors.
Major projects.
"This is a partial list:"

</doc>
<doc id="47668" url="https://en.wikipedia.org/wiki?curid=47668" title="AviaBellanca Aircraft">
AviaBellanca Aircraft

AviaBellanca Aircraft Corporation is an American aircraft design and manufacturing company. Prior to 1983 it was known as the Bellanca Aircraft Company. The company was founded in 1927 by Giuseppe Mario Bellanca.
History.
After Giuseppe Mario Bellanca, the designer and builder of Italy's first aircraft, came to the United States in 1911, he began to design aircraft for a number of firms, including the Maryland Pressed Steel Company, Wright Aeronautical Corporation and the Columbia Aircraft Corporation. Bellanca founded his own company, Bellanca Aircraft Corporation of America, in 1927, sited first in Richmond Hill, New York and moving in 1928 to New Castle (Wilmington), Delaware. In the 1920s and 1930s, Bellanca's aircraft of his own design were known for their efficiency and low operating cost, gaining fame for world record endurance and distance flights. Lindbergh's first choice for his New York to Paris flight was a Bellanca WB-2. The company's insistence on selecting the crew drove Lindbergh to Ryan.
Bellanca remained President and Chairman of the Board from the corporation's inception on the last day of 1927 until he sold the company to L. Albert and Sons in 1954. From that time on, the Bellanca line was part of a succession of companies that maintained the lineage of the original aircraft produced by Bellanca.
Aircraft.
First Flight - Model / Military number - Name

</doc>
<doc id="47669" url="https://en.wikipedia.org/wiki?curid=47669" title="Church of Domine Quo Vadis">
Church of Domine Quo Vadis

The Church of St Mary in Palmis (, ), better known as Chiesa del Domine Quo Vadis, is a small church southeast of Rome, central Italy. It is located about some 800 m from Porta San Sebastiano, where the Via Ardeatina branches off the Appian Way, on the site where, according to the apocryphal "Acts of Peter", Saint Peter met Jesus while the former was fleeing persecution in Rome. According to the legend, Peter asked Jesus, "Lord, where are you going?" (). Jesus answered, "I am going to Rome to be crucified again" ().
History.
There has been a sanctuary on the spot since the ninth century, but the current church is from 1637. The current façade was added in the 17th century.
It has been supposed that the sanctuary might have been even more ancient, perhaps a Christian adaption of some already existing temple: the church is in fact located just in front of the sacred "campus" dedicated to Rediculus, the Roman "God of the Return". This "campus" hosted a sanctuary for the cult of the deity that received devotion by travellers before their departure, especially by those who were going to face long and dangerous journeys to far places like Egypt, Greece or the East. Those travellers who returned also stopped to thank the god for the happy outcome of their journey.
The presence of the Apostle Peter in this area, where he is supposed to have lived, appears to be confirmed in an epigraph in the Catacombs of Saint Sebastian that reads "Domus Petri" (). An epigram by Pope Damasus I (366–384) in honor of Peter and Paul reads: "You that are looking for the names of Peter and Paul, you must know that the saints have lived here."
The two footprints on a marble slab at the center of the church — nowadays a copy of the original, which is kept in the nearby Basilica of San Sebastiano fuori le mura — are popularly held to be a miraculous sign left by Jesus. It is to these footprints that the official name of the church alludes: "palmis" refers to the soles of Jesus' feet. It is likely that these footprints are actually the draft of an ancient Roman "ex voto", a tribute paid to the gods for the good outcome of a journey.
There was an inscription above the front door on the church's façade which used to say: "Stop your walking, traveller, and enter this sacred temple in which you will find the footprint of our Lord Jesus Christ when He met with St. Peter who escaped from the prison. An alms for the wax and the oil is recommended in order to free some spirits from Purgatory." Pope Gregory XVI found the advertising tone of this inscription so inappropriate that he ordered its removal in 1845.
There is also a modern column with a bust of Henryk Sienkiewicz, the Polish author of the famous historical fiction novel "Quo Vadis: A Narrative of the Time of Nero" (1886). It is said that Sienkiewicz was inspired to write his novel while sitting in this church. 
The church is currently administered by priests of the Congregation of Saint Michael the Archangel.

</doc>
<doc id="47671" url="https://en.wikipedia.org/wiki?curid=47671" title="Toxic heavy metal">
Toxic heavy metal

A toxic heavy metal is any relatively dense metal or metalloid that is noted for its potential toxicity, especially in environmental contexts. The term has particular application to cadmium, mercury, lead and arsenic, all of which appear in the World Health Organisation's list of 10 chemicals of major public concern. Other examples include manganese, chromium, cobalt, nickel, copper, zinc, selenium, silver, antimony and thallium.
Heavy metals are found naturally in the earth. They become concentrated as a result of human caused activities and can enter plant, animal, and human tissues via inhalation, diet, and manual handling. Then, they can bind to and interfere with the functioning of vital cellular components. The toxic effects of arsenic, mercury, and lead were known to the ancients, but methodical studies of the toxicity of some heavy metals appear to date from only 1868. In humans, heavy metal poisoning is generally treated by the administration of chelating agents. Some elements otherwise regarded as toxic heavy metals are essential, in small quantities, for human health.
Contamination sources.
Heavy metals are found naturally in the earth, and become concentrated as a result of human caused activities. Common sources are from mining and industrial wastes; vehicle emissions; lead-acid batteries; fertilisers; paints; treated woods; aging water supply infrastructure; and microplastics floating in the world's oceans. Arsenic, cadmium and lead may be present in children's toys at levels that exceed regulatory standards. Lead can be used in toys as a stabilizer, color enhancer, or anti-corrosive agent. Cadmium is sometimes employed as a stabilizer, or to increase the mass and luster of toy jewelry. Arsenic is thought to be used in connection with coloring dyes. Regular imbibers of illegally distilled alcohol may be exposed to arsenic or lead poisoning the source of which is arsenic-contaminated lead used to solder the distilling apparatus. Rat poison used in grain and mash stores may be another source of the arsenic.
Lead is the most prevalent heavy metal contaminant. As a component of tetraethyl lead, , it was used extensively in gasoline during the 1930s–1970s. Lead levels in the aquatic environments of industrialised societies have been estimated to be two to three times those of pre-industrial levels. Although the use of leaded gasoline was largely phased out in North America by 1996, soils next to roads built before this time retain high lead concentrations. Lead (from lead azide or lead styphnate used in firearms) gradually accumulates at firearms training grounds, contaminating the local environment and exposing range employees to a risk of lead poisoning.
Entry routes.
Heavy metals enter plant, animal and human tissues via air inhalation, diet and manual handling. Motor vehicle emissions are a major source of airborne contaminants including arsenic, cadmium, cobalt, nickel, lead, antimony, vanadium, zinc, platinum, palladium and rhodium. Water sources (groundwater, lakes, streams and rivers) can be polluted by heavy metals leaching from industrial and consumer waste; acid rain can exacerbate this process by releasing heavy metals trapped in soils. Plants are exposed to heavy metals through the uptake of water; animals eat these plants; ingestion of plant- and animal-based foods are the largest sources of heavy metals in humans. Absorption through skin contact, for example from contact with soil, is another potential source of heavy metal contamination. Toxic heavy metals can bioaccumulate in organisms as they are hard to metabolize.
Detrimental effects.
Heavy metals "can bind to vital cellular components, such as structural proteins, enzymes, and nucleic acids, and interfere with their functioning." Symptoms and effects can vary according to the metal or metal compound, and the dose involved. Broadly, long-term exposure to toxic heavy metals can have carcinogenic, central and peripheral nervous system and circulatory effects. For humans, typical presentations associated with exposure to any of the "classical" toxic heavy metals, or chromium (another toxic heavy metal) or arsenic (a metalloid), are shown in the table.
History.
The toxic effects of arsenic, mercury and lead were known to the ancients but methodical studies of the overall toxicity of heavy metals appear to date from only 1868. In that year, Wanklyn and Chapman speculated on the adverse effects of the heavy metals "arsenic, lead, copper, zinc, iron and manganese" in drinking water. They noted an "absence of investigation" and were reduced to "the necessity of pleading for the collection of data." In 1884, Blake described an apparent connection between toxicity and the atomic weight of an element. The following sections provide historical thumbnails for the "classical" toxic heavy metals (arsenic, mercury and lead) and some more recent examples (chromium and cadmium).
Arsenic.
Arsenic, as realgar () and orpiment (), was known in ancient times. Strabo (64–50 BCE – c. AD 24?), a Greek geographer and historian, wrote that only slaves were employed in realgar and orpiment mines since they would inevitably die from the toxic effects of the fumes given off from the ores. Arsenic-contaminated beer poisoned over 6,000 people in the Manchester area of England in 1900, and is thought to have killed at least 70 victims. Clare Luce, American ambassador to Italy from 1953 to 1956, suffered from arsenic poisoning. Its source was traced to flaking arsenic-laden paint on the ceiling of her bedroom. She may also have eaten food contaminated by arsenic in flaking ceiling paint in the embassy dining room. Ground water contaminated by arsenic, as of 2014, "is still poisoning millions of people in Asia."
Mercury.
The first emperor of unified China, Qin Shi Huang, it is reported, died of ingesting mercury pills that were intended to give him eternal life. The phrase "mad as a hatter" is likely a reference to mercury poisoning among milliners (so-called "mad hatter disease"), as mercury-based compounds were once used in the manufacture of felt hats in the 18th and 19th century. Historically, gold amalgam (an alloy with mercury) was widely used in gilding, leading to numerous casualties among the workers. It is estimated that during the construction of Saint Isaac's Cathedral alone, 60 workers died from the gilding of the main dome. Outbreaks of methylmercury poisoning occurred in several places in Japan during the 1950s due to industrial discharges of mercury into rivers and coastal waters. The best-known instances were in Minamata and Niigata. In Minamata alone, more than 600 people died due to what became known as Minamata disease. More than 21,000 people filed claims with the Japanese government, of which almost 3000 became certified as having the disease. In 22 documented cases, pregnant women who consumed contaminated fish showed mild or no symptoms but gave birth to infants with severe developmental disabilities. Since the industrial Revolution, mercury levels have tripled in many near-surface seawaters, especially around Iceland and Antarctica.
Lead.
The adverse effects of lead were known to the ancients. In the 2nd century BC the Greek botanist Nicander described the colic and paralysis seen in lead-poisoned people. Dioscorides, a Greek physician who is thought to have lived in the 1st century CE, wrote that lead "makes the mind give way". Lead was used extensively in Roman aqueducts from about 500 BC to 300 AD. Julius Caesar's engineer, Vitruvius, reported, "water is much more wholesome from earthenware pipes than from lead pipes. For it seems to be made injurious by lead, because white lead is produced by it, and this is said to be harmful to the human body." During the Mongol period in China (1271−1368 AD), lead pollution due to silver smelting in the Yunnan region exceeded contamination levels from modern mining activities by nearly four times. In the 17th and 18th centuries, people in Devon were afflicted by a condition referred to as Devon colic; this was discovered to be due to the imbibing of lead-contaminated cider. In 2013, the World Health Organization estimated that lead poisoning resulted in 143,000 deaths, and "contribute to 600,000 new cases of children with intellectual disabilities", each year. In north-east America, in the city of Flint, Michigan, lead contamination in drinking water has been an issue since 2014. The source of the contamination has been attributed to "corrosion in the lead and iron pipes that distribute water to city residents". In 2015, drinking water lead levels in north-eastern Tasmania, Australia, were reported to reach over 50 times national drinking water guidelines. The source of the contamination was attributed to “a combination of dilapidated drinking water infrastructure, including lead jointed pipelines, end-of-life polyvinyl chloride pipes and household plumbing.”
Chromium.
Chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known since at least the late 19th century. In 1890, Newman described the elevated cancer risk of workers in a chromate dye company. Chromate-induced dermatitis was reported in aircraft workers during World War II. In 1963, an outbreak of dermatitis, ranging from erythema to exudative eczema, occurred amongst 60 automobile factory workers in England. The workers had been wet-sanding chromate-based primer paint that had been applied to car bodies. In Australia, chromium was released from the Newcastle Orica explosives plant on August 8, 2011. Up to 20 workers at the plant were exposed as were 70 nearby homes in Stockton. The town was only notified three days after the release and the accident sparked a major public controversy, with Orica criticised for playing down the extent and possible risks of the leak, and the state Government attacked for their slow response to the incident.
Cadmium.
Cadmium exposure is a phenomenon of the early 20th century, and onwards. In Japan in 1910, the Mitsui Mining and Smelting Company began discharging cadmium into the Jinzugawa river, as a byproduct of mining operations. Residents in the surrounding area subsequently consumed rice grown in cadmium-contaminated irrigation water. They experienced softening of the bones and kidney failure. The origin of these symptoms was not clear; possibilities raised at the time included "a regional or bacterial disease or lead poisoning." In 1955, cadmium was identified as the likely cause and in 1961 the source was directly linked to mining operations in the area. In February 2010, cadmium was found in Wal-Mart exclusive Miley Cyrus jewelry. Wal-Mart continued to sell the jewelry until May, when covert testing organised by Associated Press confirmed the original results. In June 2010 cadmium was detected in the paint used on promotional drinking glasses for the movie Shrek Forever After, sold by McDonald's Restaurants, triggering a recall of 12 million glasses.
Remediation.
In humans, heavy metal poisoning is generally treated by the administration of chelating agents.
These are chemical compounds, such as (calcium disodium ethylenediaminetetraacetate) that convert heavy metals to chemically inert forms that can be excreted without further interaction with the body. Chelates are not without side effects and can also remove beneficial metals from the body. Vitamin and mineral supplements are sometimes co-administered for this reason.
Soils contaminated by heavy metals can be remediated by one or more of the following technologies: isolation; immobilization; toxicity reduction; physical separation; or extraction. "Isolation" involves the use of caps, membranes or below-ground barriers in an attempt to quarantine the contaminated soil. "Immobilization" aims to alter the properties of the soil so as to hinder the mobility of the heavy contaminants. "Toxicity reduction" attempts to oxidise or reduce the toxic heavy metal ions, via chemical or biological means into less toxic or mobile forms. "Physical separation" involves the removal of the contaminated soil and the separation of the metal contaminants by mechanical means. "Extraction" is an on or off-site process that uses chemicals, high-temperature volatization, or electrolysis to extract contaminants from soils. The process or processes used will vary according to contaminant and the characteristics of the site.
Benefits.
Some elements otherwise regarded as toxic heavy metals are essential, in small quantities, for human health. These elements include vanadium, manganese, iron, cobalt, copper, zinc, selenium, strontium and molybdenum. A deficiency of these essential metals may increase susceptibility to heavy metal poisoning.

</doc>
<doc id="47672" url="https://en.wikipedia.org/wiki?curid=47672" title="Beriev">
Beriev

The Beriev Aircraft Company, formerly Beriev Design Bureau, is a Russian aircraft manufacturer (design office prefix Be), specializing in amphibious aircraft. The company was founded in Taganrog in the 1934 as OKB-49 by Georgy Mikhailovich Beriev (born February 13, 1903), and since that time has designed and produced more than 20 different models of aircraft for civilian and military purposes, as well as customized models. Today the Company employs some 3000 specialists and is developing and manufacturing amphibious aircraft.
Pilots flying Beriev seaplanes have broken 228 world aviation records. The records are registered and acknowledged by the Fédération Aéronautique Internationale. In November 1989 BERIEV Aircraft Company became the only defense industry enterprise to win the Prize for Quality awarded by the Government of Russia.

</doc>
<doc id="47673" url="https://en.wikipedia.org/wiki?curid=47673" title="Aermacchi">
Aermacchi

Aermacchi was an Italian aircraft manufacturer. Formerly known as Aeronautica Macchi, the company was founded in 1912 by Giulio Macchi at Varese in north-western Lombardy as Nieuport-Macchi, to build Nieuport monoplanes under licence for the Italian military. With a factory located on the shores of Lake Varese, the firm originally manufactured a series of Nieuport designs, as well as seaplanes.
After the Second World War, the company began producing motorcycles as a way to fill the post-war need for cheap, efficient transportation.
The company later specialised in civil and military pilot training aircraft. In July 2003, Aermacchi was integrated into the Finmeccanica Group as Alenia Aermacchi, which increased its shareholding to 99%.
Military trainers.
Since the beginning, the design and production of military trainers have been Alenia Aermacchi's core business.
The products include: 
Military collaboration.
Alenia Aermacchi has cooperated in international military programs:
Alenia Aermacchi takes part in the AMX program with Alenia Aeronautica and Embraer of Brazil with a total share of 24%. Alenia Aermacchi develops and manufactures the fuselage forward and rear sections and installs some avionic equipment in the aircraft. A Mid-Life Updating program is required by the Italian Air Force to upgrade the aircraft capabilities.
Alenia Aermacchi designs and produces wing pylons and wing tips, roots, trailing edges and flaps, which represents a 5% share in the overall program.
Alenia Aermacchi has a share of more than 4% in the Eurofighter program, for the design and development of wing pylons, twin missile and twin store carriers, ECM pods, carbon fiber structures and titanium engine cowlings.
After participating in the G-222 transport aircraft program, the company is involved in the new Military Transport Aircraft C-27J Spartan, for the production of outer wings.
Civil programs.
Since the mid-1990s, Alenia Aermacchi has participated in programs for the supply of engine nacelles for civil aircraft. It produces cold parts for engine nacelles: inlets, fan cowls and EBU, the systems-to-engine interface. 
In 1999, the company established a joint venture (MHD) with Hurel-Dubois (presently Hurel-Hispano, of SNECMA group), a French company specializing in the development and manufacture of thrust reversers, to obtain the full responsibility for the development of nacelles installed on maximum 100-seat aircraft.
Motorcycles.
Aermacchi began producing motorcycles after World War II. 
In 1960, US business motorcycles purchased 50% of Aermacchi's motorcycle division. The remaining motorcycle holdings were sold in 1974 to AMF-Harley-Davidson, with motorcycles continuing to be made at Varese. The business was sold to Cagiva in 1978.

</doc>
<doc id="47674" url="https://en.wikipedia.org/wiki?curid=47674" title="Blackburn Aircraft">
Blackburn Aircraft

Blackburn Aircraft Limited was a British aircraft manufacturer that concentrated mainly on naval and maritime aircraft during the first part of the 20th century.
History.
Blackburn Aircraft was founded by Robert Blackburn, who built his first aircraft in Leeds in 1908.
The Blackburn Aeroplane & Motor Company was created in 1914, established in a new factory built at Brough, East Riding of Yorkshire in 1916, where Robert's brother Norman Blackburn was later Managing Director. By acquiring the Cirrus-Hermes company in 1937, Blackburn started producing aircraft engines, the Blackburn Cirrus range.
By 1937, pressure to re-arm was growing and the Yorkshire factory was approaching capacity. A fortuitous friendship between Maurice Denny, managing director of Denny Bros., the Dumbarton ship building company, and Robert Blackburn resulted in the building of a new Blackburn factory at Barge Park, Dumbarton where production of the Blackburn Botha commenced in 1939.
The company's name was changed to Blackburn Aircraft Limited in 1939, and the company amalgamated with General Aircraft Limited in 1949 as Blackburn and General Aircraft Limited, reverting to Blackburn Aircraft Limited by 1958.
As part of the rationalisation of British aircraft manufacturers, its aircraft production and engine operations were absorbed into Hawker Siddeley and Bristol Siddeley respectively. The Blackburn name was dropped completely in 1963.
An American company, Blackburn Aircraft Corp., was incorporated in Detroit on 20 May 1929 to acquire design and patent rights of the aircraft of Blackburn Airplane & Motor Co., Ltd. in the USA. It was owned 90% by Detroit Aircraft Corp. and 10% by Blackburn Airplane & Motor Co., Ltd. Agreements covered such rights in North and South America, excepting Brazil and certain rights in Canada and provided that all special tools and patterns were to be supplied by the UK company at cost.

</doc>
<doc id="47676" url="https://en.wikipedia.org/wiki?curid=47676" title="Data (Star Trek)">
Data (Star Trek)

Lieutenant Commander Data ( ) is a character in the fictional "Star Trek" universe portrayed by actor Brent Spiner. He appears in the television series ' and the feature films "Star Trek Generations", ', ' and '.
An artificial intelligence and synthetic life form designed and built by Doctor Noonien Soong, Data is a self-aware, sapient, sentient, and anatomically fully functional android who serves as the second officer and chief operations officer aboard the Federation starships USS "Enterprise"-D and USS "Enterprise"-E. His positronic brain allows him impressive computational capabilities. Data experienced ongoing difficulties during the early years of his life with understanding various aspects of human behavior and was unable to feel emotion or understand certain human idiosyncrasies, inspiring him to strive for his own humanity. This goal eventually led to the addition of an "emotion chip", also created by Soong, to Data's positronic net. Although Data's endeavor to increase his humanity and desire for human emotional experience is a significant plot point (and source of humor) throughout the series, he consistently shows a nuanced sense of wisdom, sensitivity, and curiosity, garnering immense respect from his peers and colleagues.
Data is in many ways a successor to the original ""s Spock (Leonard Nimoy), in that the character offers an "outsider's" perspective on humanity, even briefly working with Spock in the two-part "Next Generation" episode, .
Development.
Gene Roddenberry told Brent Spiner that over the course of the series, Data was to become "more and more like a human until the end of the show, when he would be very close, but still not quite there. That was the idea and that's the way that the writers took it." Spiner felt that Data exhibited the Chaplinesque characteristics of a sad, tragic clown. To get into his role as Data, Spiner used the character of Robby the Robot from the film "Forbidden Planet" as a role model.
Commenting on Data's perpetual albino-like appearance, he said: "I spent more hours of the day in make-up than out of make-up", so much so that he even called it a way of method acting. Spiner also portrayed Data's manipulative and malignant brother Lore (a role he found much easier to play, because the character was "more like me"), and Data's creator, Dr. Noonien Soong. Additionally, he portrayed another Soong-type android, B-4, in the film "Star Trek Nemesis", and also one of Soong's ancestors in three episodes of "". Spiner said his favorite Data scene takes place in "", when Data plays poker on the holodeck with a re-creation of the famous physicist Stephen Hawking, played by Hawking himself.
Spiner reprised his role of Data in the "" series finale "These Are the Voyages..." in an off-screen speaking part. Spiner felt that he had visibly aged out of the role and that Data was best presented as a youthful figure.
Depiction.
Television series and films.
Dialog in "Datalore" establishes some of Data's backstory. It is stated that he was deactivated in 2336 on Omicron Theta before an attack by the Crystalline Entity, a spaceborne creature which converts life forms to energy for sustenance. He was found and reactivated by Starfleet personnel two years later. Data went to Starfleet Academy from 2341–45 (he describes himself as "Class of '78" to Riker in "Encounter at Farpoint", but that may refer to the stardate and not the year that he graduated) and then served in Starfleet aboard the USS "Trieste". He was assigned to the "Enterprise" under Captain Jean-Luc Picard in 2364. In "Datalore", Data discovers his amoral brother, Lore, and learns that he was created after Lore. Lore fails in an attempt to betray the "Enterprise" to the Crystalline Entity, and Wesley Crusher beams Data's brother into space at the episode's conclusion.
In "", Data reunites with Dr. Soong (also portrayed by Spiner). There he meets again with Lore, who steals the emotion chip Soong meant for Data to receive. Lore then fatally wounds Soong. Lore returns in the two-part episode "", using the emotion chip to control Data and make him help with Lore's attempt to make the Borg entirely artificial lifeforms. Data eventually deactivates Lore, and recovers, but does not install the damaged emotion chip.
In "", a Starfleet judge rules that Data is not Starfleet property. The episode establishes that Data has a storage capacity of 800 quadrillion bits, (100 PB or 88.81784197 PiB) and a total linear computational speed of 60 trillion operations per second.
Data's family is expanded in "", which introduces Lal, a robot based on Data's neural interface and who Data refers to as his daughter. Lal “dies” shortly after activation. Later, his mother Julianna appears in the episode "" and reunites with Data, though the crew discovers she was an android duplicate built by Soong after the real Julianna's death, programmed to die after a long life, and to believe she is the true Julianna, unaware of the fact she is an android. Faced with the decision, Data chooses not to disclose this to her and allow her the chance to continue on with her normal life.
In "", the two-hour concluding episode of "The Next Generation", Captain Picard travels between three different time periods. The Picard of 25 years into the future goes with La Forge to seek advice from Professor Data, a luminary physicist who holds the Lucasian Chair at Cambridge University.
In "", because 'data' is a heterophone, a word with multiple pronunciations, Commander Data clarifies to the newly arrived ship's chief medical officer, Dr. Katherine Pulaski's, the correct pronunciation of his name as "day'ta", not "dah'ta".
Although several androids, robots, and artificial intelligences were seen in the original "Star Trek" series, Data was often referred to as being unique in the galaxy as being the only sentient android known to exist (save the other androids created by Soong).
In the film "Star Trek Generations", Data finally installs the emotion chip he retrieved from Lore, and experiences the full scope of emotions. However, those emotions proved difficult to control and Data struggled to master them. However, by the events of "", Data managed to gain complete control of the chip, which includes deactivating it to maintain his performance efficiency.
In the film "", Data beams Picard off an enemy ship before destroying it, sacrificing himself and saving the captain and crew of the Enterprise. However, Data previously copied his core memories into B-4, his lost brother who is introduced in the movie. This was done with the reluctant help of Geordi LaForge who voiced concerns about how this could cause B-4 to be nothing more than an exact duplicate of Data.
Spin-off works.
In the comic book miniseries "" (the official prequel to the reboot "Star Trek" film), Data, having successfully transferred his positronic pathways and memories into B-4, now commands the "Enterprise"-E in 2387 in its mission to stop the Romulan Nero. Spock compares Data's "resurrection" with his own and years earlier.
In the novels published by Pocket Books and set after "Nemesis", Data returned in 2384 by having his memories and neural net transferred from B-4 into a new body which contained the memory engrams of Data's creator Doctor Noonian Soong after he was dying and being attacked by Lore years earlier. Data then takes control of the body after Soong deletes himself. After a tearful reunion with his old shipmates, Picard offers to reactivate Data's commission and to rejoin the crew but Data declines as he says he requires time. Several months later, with the help of the "Enterprise" crew, he is able to obtain the help necessary to resurrect his daughter, Lal.
In the Prologue to the novel adaptation for Encounter at Farpoint, by David Gerrold, Data chose his own name, due to his love for, and identification with, knowledge. 
Data also appeared in the crossover graphic novel series "", set in 2368, in which the Borg Collective joins forces with the Cybermen when the latter invade their universe. Data and the crew of the "Enterprise"-D form an alliance of their own with the Eleventh Doctor-who immediately recognizes Data as an android upon seeing him-and his companions, Amy Pond and Rory Williams. The group later forms a reluctant truce with the Borg, who have been betrayed by the Cybermen and are in danger of falling to them. Data and the others manage to restore the Borg Collective and destroy the Cybermen, but their Borg liaison then attempts to seize control of the Doctor's TARDIS. The time machine's intelligence then briefly transfers itself into Data to escape the Borg's control, and the empowered Data overpowers the Borg and throws him out into the Time Vortex.
Characteristics.
Data is immune to nearly all biological diseases and other weaknesses that can affect humans and other carbon-based lifeforms. This benefits the "Enterprise" many times, such as when Data is the and . One exception however was in the episode "The Naked Now" where Data was also a victim of the Tsiolkovsky polywater virus. Data does not require life support to function and does not register a bio-signature. The crew of the "Enterprise"-D must modify their scanners to detect positronic signals in order to locate and keep track of him on away-missions. Another unique feature of Data's construction is the ability to be dismantled and then re-assembled for later use. This is used as a plot element in the episode "" where Data's head (an artifact excavated on Earth from the late 19th century) is reattached to his body after nearly 500 years. Another example is in the episode "", where Data intentionally damages his body to break a high-current electrical arc, and then Riker takes his head to engineering to solve an engine problem.
Data is vulnerable to technological hazards such as computer viruses, certain levels of energy discharges, ship malfunctions (when connected to the "Enterprise" main computer for experiments), and shutdowns whether through remote control shutdown devices or through use of his "off switch". Data has also been "possessed" through technological means such as: Ira Graves's transfer of consciousness into his neural net; Dr. Soong's "calling" him; and an alien library that placed several different personalities into him. Data cannot swim unless aided by his built in flotation device, yet he is waterproof and can perform tasks underwater without the need to surface. Data is also impervious to sensory tactile emotion such as pain or pleasure. In "" the Borg Queen grafted artificial skin to his forearm. Data was then able to feel pain when a Borg drone slashed at his arm, and pleasure when the Borg Queen blew on the skin's hair follicles. Despite being mechanical in nature, Data is treated as an equal member of the "Enterprise" crew. Being a mechanical construct, technicians such as Chief Engineer LaForge prove to be more appropriate to treat his mechanical or cognitive function failures than the ship's doctor. His positronic brain becomes deactivated, and then repaired and reactivated by Geordi on several occasions.
Data is physically the strongest member of the "Enterprise" crew and also is, in ability to process and calculate information rapidly, the most intelligent member. He is able to survive in atmospheres that most carbon-based life forms would consider inhospitable, including the lack of an atmosphere or the vacuum of space; however, as an android, he is the most emotionally challenged and, with the addition of Dr. Soong's emotions chip, the most emotionally unstable member of the crew. Before the emotions chip, Data was unable to grasp basic emotion and imagination, leading him to download personality subroutines into his programming when participating in holographic recreational activities (most notably during Dixon Hill and Sherlock Holmes holoprograms) and during romantic encounters (most notably with Tasha Yar and Jenna D'Sora). Yet none of those personalities are his own and are immediately put away at the conclusion of their usefulness. Also, the abilities of Data's hearing are explained in the episodes "" and "" where his hearing is more sensitive than a dog's and that he can identify several hundred different distinct sound patterns simultaneously, but for aesthetics purposes limits it to about ten. Throughout the series, Data develops a frequently humorous affinity for theatrical acting and singing. This is most definitively demonstrated in "" where Picard and Worf distract an erratically behaving Data by singing two parts of "A British Tar", compelling Data to sing the third part.
Because of Julianna Soong's inability to conceive children, Data has at least five robotic siblings (two of which are Lore and B-4). Later on, his "mother" is revealed also to be his positronic sister as the real Julianna Soong died and was replaced with an identical Soong-type android, the most advanced one that Dr. Soong was known to have built. Data constructed a daughter, which he named "Lal" in the episode "". This particular android exceeded her father in basic human emotion when she felt fear toward Starfleet's scientific interests in her. Eventually, this was the cause of a cascade failure in her neural net and she died as a result.
Spot.
Spot is Data's pet cat and a recurring character in the show. Spot appears in several episodes during "TNG"s last four seasons, as well as in the feature films "Star Trek Generations" and "". She first appears in the episode "Data's Day".
Despite her name, Spot is not actually patterned with spots. Spot originally appears as a male Somali cat, but later appears as a female orange tabby cat, eventually giving birth to kittens ("TNG": "").
Data creates several hundred food supplement variations for Spot and composes the poem "Ode to Spot" in the cat's honor (""). A computer error which occurs later in the series (in the episode "A Fistful of Datas") causes some of the ship's food replicators to create only Spot's supplements and replaces portions of a play with the ode's text.
In "Genesis", the morphogenetic virus "Barclay's protomorphosis disease" temporarily mutates Spot into an iguana-like reptile. Spot's kittens are not affected, leading to the discovery of the mechanism and a cure for the virus.
Spot is notoriously unfriendly to most people other than Data. Commander William Riker once received serious scratches while trying to feed Spot (""). Geordi La Forge borrowed her to experience taking care of a cat, but she knocked over a vase and teapot and damaged his furniture (""). When Data asked Worf to take care of Spot, Worf proved to be allergic to her and sneezed in her face, angering her (""). However, she did get along with Lieutenant Reginald Barclay, so when Data had to leave on a mission at the same time Spot's kittens were due, he persuaded Barclay to take care of her ("Genesis").
Reception.
Like Spock, Data became a sex symbol and Spiner's fan mail came mostly from women. He described the letters as "romantic mail" that was "really written to Data; he's a really accessible personality".
Robotics engineers regard Data (along with the Droids from the "Star Wars" movies) as the pre-eminent face of robots in the public's perception of their field. On April 9, 2008, Data was inducted into Carnegie Mellon University's Robot Hall of Fame during a ceremony at the Carnegie Science Center in Pittsburgh, Pennsylvania.
The Beat Fleet, a Croatian hip hop band, wrote a song called "Data" for their album "Galerija Tutnplok" dedicated to Data. The release of this album coincided with reruns of "Star Trek: The Next Generation" being shown on Croatian Radiotelevision. In 2005, the nerdcore group The Futuristic Sex Robotz released a song about Data entitled "The Positronic Pimp." Cuban-American musician Voltaire has also performed a song about Data entitled "The Sexy Data Tango" on his LP Banned on Vulcan and later his album BiTrektual. Punk rock group Warp 11 also has a song, My Electric Man, from their album Boldly Go Down on Me, as well as numerous other references in other songs.

</doc>
<doc id="47679" url="https://en.wikipedia.org/wiki?curid=47679" title="Hawker Pacific Aerospace">
Hawker Pacific Aerospace

Hawker Pacific Aerospace (HPA) is a MRO-Service (Maintenance, Repair and Overhaul) company which offers landing gear and hydraulic MRO services for all major aircraft types. These include all current Airbus, Boeing, Bombardier and Embraer models as well as Helicopters. 
Lufthansa Technik (LHT), a subsidiary of Lufthansa German Airlines, acquired Hawker Pacific in 2002. Within the LHT network Hawker is able to provide an around the world landing gear service.
HPA’s corporate headquarters located in Sun Valley, Los Angeles, California near the Bob Hope Burbank International Airport.
History.
Hawker Pacific Aerospace was formed in 1980. In 1991, Hawker Siddeley was absorbed by BTR Aerospace Group. In 1994, Hawker Pacific merged with Dunlop Aviation Inc and in 1996, Hawker Pacific Aerospace was sold by BTR and became a stand-alone company.
In 1998, Hawker Pacific completed its initial public offering of common stock and used the proceeds to acquire the landing gear, flap track and flap carriage operation from British Airways.
In 2002, Lufthansa Technik acquired 100% ownership of Hawker Pacific Aerospace including its UK location. Hawker was added to Lufthansa Technik's Landing Gear Division to form a global network for Landing Gear MRO services.
In January 2011, Hawker Pacific Aerospace UK was sold to Lufthansa Technik and renamed into Lufthansa Technik Landing Gear Services UK.
Business divisions.
Hawker Pacific Aerospace has three business divisions:

</doc>
<doc id="47682" url="https://en.wikipedia.org/wiki?curid=47682" title="599">
599

__NOTOC__
Year 599 (DXCIX) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. The denomination 599 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47683" url="https://en.wikipedia.org/wiki?curid=47683" title="598">
598

__NOTOC__
Year 598 (DXCVIII) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. The denomination 598 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47684" url="https://en.wikipedia.org/wiki?curid=47684" title="596">
596

__NOTOC__
Year 596 (DXCVI) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar. The denomination 596 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47685" url="https://en.wikipedia.org/wiki?curid=47685" title="595">
595

__NOTOC__
Year 595 (DXCV) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. The denomination 595 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47687" url="https://en.wikipedia.org/wiki?curid=47687" title="Cosmic ray">
Cosmic ray

Cosmic rays are immensely high-energy radiation, mainly originating outside the Solar System. They may produce showers of secondary particles that penetrate and impact the Earth's atmosphere and sometimes even reach the surface. Composed primarily of high-energy protons and atomic nuclei, they are of mysterious origin. Data from the "Fermi" space telescope (2013) have been interpreted as evidence that a significant fraction of primary cosmic rays originate from the supernovae of massive stars. However, this is not thought to be their only source. Active galactic nuclei probably also produce cosmic rays.
Etymology.
The term "ray" is a historical accident, as cosmic rays were at first, and wrongly, thought to be mostly electromagnetic radiation. In common scientific usage high-energy particles with intrinsic mass are known as "cosmic" rays, while photons, which are quanta of electromagnetic radiation (and so have no intrinsic mass) are known by their common names, such as "gamma rays" or "X-rays", depending on their energy.
Composition.
Of primary cosmic rays, which originate outside of Earth's atmosphere, about 99% are the nuclei (stripped of their electron shells) of well-known atoms, and about 1% are solitary electrons (similar to beta particles). Of the nuclei, about 90% are simple protons, i. e. hydrogen nuclei; 9% are alpha particles, identical to helium nuclei, and 1% are the nuclei of heavier elements, called HZE ions. A very small fraction are stable particles of antimatter, such as positrons or antiprotons. The precise nature of this remaining fraction is an area of active research. An active search from Earth orbit for anti-alpha particles has failed to detect them.
Earthly effects.
Cosmic rays attract great interest practically, due to the damage they inflict on microelectronics and life outside the protection of an atmosphere and magnetic field, and scientifically, because the energies of the most energetic ultra-high-energy cosmic rays (UHECRs) have been observed to approach 3 × 1020 eV, about 40 million times the energy of particles accelerated by the Large Hadron Collider. One can show that such enormous energies might be achieved by means of the Centrifugal mechanism of acceleration in Active galactic nuclei. At 50 J, the highest-energy ultra-high-energy cosmic rays have energies comparable to the kinetic energy of a baseball. As a result of these discoveries, there has been interest in investigating cosmic rays of even greater energies. Most cosmic rays, however, do not have such extreme energies; the energy distribution of cosmic rays peaks at .
History.
After the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896, it was generally believed that atmospheric electricity, ionization of the air, was caused only by radiation from radioactive elements in the ground or the radioactive gases or isotopes of radon they produce. Measurements of ionization rates at increasing heights above the ground during the decade from 1900 to 1910 showed a decrease that could be explained as due to absorption of the ionizing radiation by the intervening air.
Discovery.
In 1909 Theodor Wulf developed an electrometer, a device to measure the rate of ion production inside a hermetically sealed container, and used it to show higher levels of radiation at the top of the Eiffel Tower than at its base. However, his paper published in "Physikalische Zeitschrift" was not widely accepted. In 1911 Domenico Pacini observed simultaneous variations of the rate of ionization over a lake, over the sea, and at a depth of 3 meters from the surface. Pacini concluded from the decrease of radioactivity underwater that a certain part of the ionization must be due to sources other than the radioactivity of the Earth. 
In 1912, Victor Hess carried three enhanced-accuracy Wulf electrometers to an altitude of 5300 meters in a free balloon flight. He found the ionization rate increased approximately fourfold over the rate at ground level. Hess ruled out the Sun as the radiation's source by making a balloon ascent during a near-total eclipse. With the moon blocking much of the Sun's visible radiation, Hess still measured rising radiation at rising altitudes. He concluded "The results of my observation are best explained by the assumption that a radiation of very great penetrating power enters our atmosphere from above." In 1913–1914, Werner Kolhörster confirmed Victor Hess' earlier results by measuring the increased ionization rate at an altitude of 9 km. Hess received the Nobel Prize in Physics in 1936 for his discovery.
The Hess balloon flight took place on 7 August 1912. By sheer coincidence, exactly 100 years later on 7 August 2012, the Mars Science Laboratory rover used its Radiation Assessment Detector (RAD) instrument to begin measuring the radiation levels on another planet for the first time. On 31 May 2013, NASA scientists reported that a possible manned mission to Mars may involve a greater radiation risk than previously believed, based on the amount of energetic particle radiation detected by the RAD on the Mars Science Laboratory while traveling from the Earth to Mars in 2011–2012.
Identification.
In the 1920s the term "cosmic rays" was coined by Robert Millikan who made measurements of ionization due to cosmic rays from deep under water to high altitudes and around the globe. Millikan believed that his measurements proved that the primary cosmic rays were gamma rays, i.e., energetic photons. And he proposed a theory that they were produced in interstellar space as by-products of the fusion of hydrogen atoms into the heavier elements, and that secondary electrons were produced in the atmosphere by Compton scattering of gamma rays. But then, sailing from Java to the Netherlands in 1927, Jacob Clay found evidence, later confirmed in many experiments, of a variation of cosmic ray intensity with latitude, which indicated that the primary cosmic rays are deflected by the geomagnetic field and must therefore be charged particles, not photons. In 1929, Bothe and Kolhörster discovered charged cosmic-ray particles that could penetrate 4.1 cm of gold. Charged particles of such high energy could not possibly be produced by photons from Millikan's proposed interstellar fusion process.
In 1930, Bruno Rossi predicted a difference between the intensities of cosmic rays arriving from the east and the west that depends upon the charge of the primary particles – the so-called "east-west effect." Three independent experiments found that the intensity is, in fact, greater from the west, proving that most primaries are positive. During the years from 1930 to 1945, a wide variety of investigations confirmed that the primary cosmic rays are mostly protons, and the secondary radiation produced in the atmosphere is primarily electrons, photons and muons. In 1948, observations with nuclear emulsions carried by balloons to near the top of the atmosphere showed that approximately 10% of the primaries are helium nuclei (alpha particles) and 1% are heavier nuclei of the elements such as carbon, iron, and lead.
During a test of his equipment for measuring the east-west effect, Rossi observed that the rate of near-simultaneous discharges of two widely separated Geiger counters was larger than the expected accidental rate. In his report on the experiment, Rossi wrote "... it seems that once in a while the recording equipment is struck by very extensive showers of particles, which causes coincidences between the counters, even placed at large distances from one another." In 1937 Pierre Auger, unaware of Rossi's earlier report, detected the same phenomenon and investigated it in some detail. He concluded that high-energy primary cosmic-ray particles interact with air nuclei high in the atmosphere, initiating a cascade of secondary interactions that ultimately yield a shower of electrons, and photons that reach ground level.
Soviet physicist Sergey Vernov was the first to use radiosondes to perform cosmic ray readings with an instrument carried to high altitude by a balloon. On 1 April 1935, he took measurements at heights up to 13.6 kilometers using a pair of Geiger counters in an anti-coincidence circuit to avoid counting secondary ray showers.
Homi J. Bhabha derived an expression for the probability of scattering positrons by electrons, a process now known as Bhabha scattering. His classic paper, jointly with Walter Heitler, published in 1937 described how primary cosmic rays from space interact with the upper atmosphere to produce particles observed at the ground level. Bhabha and Heitler explained the cosmic ray shower formation by the cascade production of gamma rays and positive and negative electron pairs.
Energy distribution.
Measurements of the energy and arrival directions of the ultra-high energy primary cosmic rays by the techniques of "density sampling" and "fast timing" of extensive air showers were first carried out in 1954 by members of the Rossi Cosmic Ray Group at the Massachusetts Institute of Technology. The experiment employed eleven scintillation detectors arranged within a circle 460 meters in diameter on the grounds of the Agassiz Station of the Harvard College Observatory. From that work, and from many other experiments carried out all over the world, the energy spectrum of the primary cosmic rays is now known to extend beyond 1020 eV. A huge air shower experiment called the Auger Project is currently operated at a site on the pampas of Argentina by an international consortium of physicists, led by James Cronin, winner of the 1980 Nobel Prize in Physics from the University of Chicago, and Alan Watson of the University of Leeds. Their aim is to explore the properties and arrival directions of the very highest-energy primary cosmic rays. The results are expected to have important implications for particle physics and cosmology, due to a theoretical Greisen–Zatsepin–Kuzmin limit to the energies of cosmic rays from long distances (about 160 million light years) which occurs above 1020 eV because of interactions with the remnant photons from the Big Bang origin of the universe.
High-energy gamma rays (>50 MeV photons) were finally discovered in the primary cosmic radiation by an MIT experiment carried on the OSO-3 satellite in 1967. Components of both galactic and extra-galactic origins were separately identified at intensities much less than 1% of the primary charged particles. Since then, numerous satellite gamma-ray observatories have mapped the gamma-ray sky. The most recent is the Fermi Observatory, which has produced a map showing a narrow band of gamma ray intensity produced in discrete and diffuse sources in our galaxy, and numerous point-like extra-galactic sources distributed over the celestial sphere.
Sources of cosmic rays.
Early speculation on the sources of cosmic rays included a 1934 proposal by Baade and Zwicky suggesting cosmic rays originated from supernovae. A 1948 proposal by Horace W. Babcock suggested that magnetic variable stars could be a source of cosmic rays. Subsequently in 1951, Y. Sekido "et al." identified the Crab Nebula as a source of cosmic rays. Since then, a wide variety of potential sources for cosmic rays began to surface, including supernovae, active galactic nuclei, quasars, and gamma-ray bursts.
Later experiments have helped to identify the sources of cosmic rays with greater certainty. In 2009, a paper presented at the International Cosmic Ray Conference (ICRC) by scientists at the Pierre Auger Observatory showed ultra-high energy cosmic rays (UHECRs) originating from a location in the sky very close to the radio galaxy Centaurus A, although the authors specifically stated that further investigation would be required to confirm Cen A as a source of cosmic rays. However, no correlation was found between the incidence of gamma-ray bursts and cosmic rays, causing the authors to set upper limits as low as 3.4 × 10−6 erg cm−2 on the flux of 1 GeV-1 TeV cosmic rays from gamma-ray bursts.
In 2009, supernovae were said to have been "pinned down" as a source of cosmic rays, a discovery made by a group using data from the Very Large Telescope. This analysis, however, was disputed in 2011 with data from PAMELA, which revealed that "spectral shapes of and helium nuclei are different and cannot be described well by a single power law", suggesting a more complex process of cosmic ray formation. In February 2013, though, research analyzing data from "Fermi" revealed through an observation of neutral pion decay that supernovae were indeed a source of cosmic rays, with each explosion producing roughly 3 × 1042 - 3 × 1043 J of cosmic rays. However, supernovae do not produce all cosmic rays, and the proportion of cosmic rays that they do produce is a question which cannot be answered without further study.
Types.
Cosmic rays can be divided into two types, Galactic Cosmic Rays ("GCR"), high energy particles originating outside the solar system, and Solar energetic particles, high energy particles (predominantly protons) emitted by the sun, primarily in solar particle events. However, the term "cosmic ray" is often used to refer to only the GCR flux. Despite the nomenclature "galactic", GCRs may originate within or outside the galaxy (as discussed in the source section above).
Cosmic rays originate as primary cosmic rays, which are those originally produced in various astrophysical processes. Primary cosmic rays are composed primarily of protons and alpha particles (99%), with a small amount of heavier nuclei (~1%) and an extremely minute proportion of positrons and antiprotons. Secondary cosmic rays, caused by a decay of primary cosmic rays as they impact an atmosphere, include neutrons, pions, positrons, and muons. Of these four, the latter three were first detected in cosmic rays.
Primary cosmic rays.
Primary cosmic rays primarily originate from outside the Solar System and sometimes even the Milky Way. When they interact with Earth's atmosphere, they are converted to secondary particles. The mass ratio of helium to hydrogen nuclei, 28%, is similar to the primordial elemental abundance ratio of these elements, 24%. The remaining fraction is made up of the other heavier nuclei that are typical nucleosynthesis end products, primarily lithium, beryllium, and boron. These nuclei appear in cosmic rays in much greater abundance (~1%) than in the solar atmosphere, where they are only about 10−11 as abundant as helium. Cosmic rays made up of charged nuclei heavier than helium are called HZE ions. Due to the high charge and heavy nature of HZE ions, their contribution to an astronaut's radiation dose in space is significant even though they are relatively scarce.
This abundance difference is a result of the way secondary cosmic rays are formed. Carbon and oxygen nuclei collide with interstellar matter to form lithium, beryllium and boron in a process termed cosmic ray spallation. Spallation is also responsible for the abundances of scandium, titanium, vanadium, and manganese ions in cosmic rays produced by collisions of iron and nickel nuclei with interstellar matter.
Primary cosmic ray antimatter.
Satellite experiments have found evidence of positrons and a few antiprotons in primary cosmic rays, amounting to less than 1% of the particles in primary cosmic rays. These do not appear to be the products of large amounts of antimatter from the Big Bang, or indeed complex antimatter in the universe. Rather, they appear to consist of only these two elementary particles, newly made in energetic processes.
Preliminary results from the presently operating Alpha Magnetic Spectrometer ("AMS-02") on board the International Space Station show that positrons in the cosmic rays arrive with no directionality, and with energies that range from 10 to 250 GeV. In September, 2014, new results with almost twice as much data were presented in a talk at CERN and published in Physical Review Letters. A new measurement of positron fraction up to 500 GeV was reported, showing that positron fraction peaks at a maximum of about 16% of total electron+positron events, around an energy of 275 ± 32 GeV. At higher energies, up to 500 GeV, the ratio of positrons to electrons begins to fall again. The absolute flux of positrons also begins to fall before 500 GeV, but peaks at energies far higher than electron energies, which peak about 10 GeV. These results on interpretation have been suggested to be due to positron production in annihilation events of massive dark matter particles.
Cosmic ray antiprotons also have a much higher average energy than their normal-matter counterparts (protons). They arrive at Earth with a characteristic energy maximum of 2 GeV, indicating their production in a fundamentally different process from cosmic ray protons, which on average have only one-sixth of the energy.
There is no evidence of complex antimatter atomic nuclei, such as antihelium nuclei (i.e., anti-alpha particles), in cosmic rays. These are actively being searched for. A prototype of the "AMS-02" designated "AMS-01", was flown into space aboard the on STS-91 in June 1998. By not detecting any antihelium at all, the "AMS-01" established an upper limit of 1.1×10−6 for the antihelium to helium flux ratio.
Secondary cosmic rays.
When cosmic rays enter the Earth's atmosphere they collide with atoms and molecules, mainly oxygen and nitrogen. The interaction produces a cascade of lighter particles, a so-called air shower secondary radiation that rains down, including x-rays, muons, protons, alpha particles, pions, electrons, and neutrons. All of the produced particles stay within about one degree of the primary particle's path.
Typical particles produced in such collisions are neutrons and charged mesons such as positive or negative pions and kaons. Some of these subsequently decay into muons, which are able to reach the surface of the Earth, and even penetrate for some distance into shallow mines. The muons can be easily detected by many types of particle detectors, such as cloud chambers, bubble chambers or scintillation detectors. The observation of a secondary shower of particles in multiple detectors at the same time is an indication that all of the particles came from that event.
Cosmic rays impacting other planetary bodies in the Solar System are detected indirectly by observing high energy gamma ray emissions by gamma-ray telescope. These are distinguished from radioactive decay processes by their higher energies above  about 10 MeV.
Cosmic-ray flux.
The flux of incoming cosmic rays at the upper atmosphere is dependent on the solar wind, the Earth's magnetic field, and the energy of the cosmic rays. At distances of ~94 AU from the Sun, the solar wind undergoes a transition, called the termination shock, from supersonic to subsonic speeds. The region between the termination shock and the heliopause acts as a barrier to cosmic rays, decreasing the flux at lower energies (≤ 1 GeV) by about 90%. However, the strength of the solar wind is not constant, and hence it has been observed that cosmic ray flux is correlated with solar activity.
In addition, the Earth's magnetic field acts to deflect cosmic rays from its surface, giving rise to the observation that the flux is apparently dependent on latitude, longitude, and azimuth angle. The magnetic field lines deflect the cosmic rays towards the poles, giving rise to the aurorae.
The combined effects of all of the factors mentioned contribute to the flux of cosmic rays at Earth's surface. The following table of participial frequencies reach the planet and are inferred from lower energy radiation reaching the ground
In the past, it was believed that the cosmic ray flux remained fairly constant over time. However, recent research suggests 1.5 to 2-fold millennium-timescale changes in the cosmic ray flux in the past forty thousand years.
The magnitude of the energy of cosmic ray flux in interstellar space is very comparable to that of other deep space energies: cosmic ray energy density averages about one electron-volt per cubic centimeter of interstellar space, or ~1 eV/cm3, which is comparable to the energy density of visible starlight at 0.3 eV/cm3, the galactic magnetic field energy density (assumed 3 microgauss) which is ~0.25 eV/cm3, or the cosmic microwave background (CMB) radiation energy density at ~ 0.25 eV/cm3.
Detection methods.
There are several ground-based methods of detecting cosmic rays currently in use. The first detection method is called the air Cherenkov telescope, designed to detect low-energy (<200 GeV) cosmic rays by means of analyzing their Cherenkov radiation, which for cosmic rays are gamma rays emitted as they travel faster than the speed of light in their medium, the atmosphere. While these telescopes are extremely good at distinguishing between background radiation and that of cosmic-ray origin, they can only function well on clear nights without the Moon shining, and have very small fields of view and are only active for a few percent of the time. Another Cherenkov telescope uses water as a medium through which particles pass and produce Cherenkov radiation to make them detectable.
Extensive air shower (EAS) arrays, a second detection method, measure the charged particles which pass through them. EAS arrays measure much higher-energy cosmic rays than air Cherenkov telescopes, and can observe a broad area of the sky and can be active about 90% of the time. However, they are less able to segregate background effects from cosmic rays than can air Cherenkov telescopes. EAS arrays employ plastic scintillators in order to detect particles.
Another method was developed by Robert Fleischer, P. Buford Price, and Robert M. Walker for use in high-altitude balloons. In this method, sheets of clear plastic, like 0.25 mm Lexan polycarbonate, are stacked together and exposed directly to cosmic rays in space or high altitude. The nuclear charge causes chemical bond breaking or ionization in the plastic. At the top of the plastic stack the ionization is less, due to the high cosmic ray speed. As the cosmic ray speed decreases due to deceleration in the stack, the ionization increases along the path. The resulting plastic sheets are "etched" or slowly dissolved in warm caustic sodium hydroxide solution, that removes the surface material at a slow, known rate. The caustic sodium hydroxide dissolves the plastic at a faster rate along the path of the ionized plastic. The net result is a conical etch pit in the plastic. The etch pits are measured under a high-power microscope (typically 1600x oil-immersion), and the etch rate is plotted as a function of the depth in the stacked plastic.
This technique yields a unique curve for each atomic nucleus from 1 to 92, allowing identification of both the charge and energy of the cosmic ray that traverses the plastic stack. The more extensive the ionization along the path, the higher the charge. In addition to its uses for cosmic-ray detection, the technique is also used to detect nuclei created as products of nuclear fission.
A fourth method involves the use of cloud chambers to detect the secondary muons created when a pion decays. Cloud chambers in particular can be built from widely available materials and can be constructed even in a high-school laboratory. A fifth method, involving bubble chambers, can be used to detect cosmic ray particles.
Another method detects the light from nitrogen fluorescence caused by the excitation of nitrogen in the atmosphere by the shower of particles moving through the atmosphere. This method allows for accurate detection of the direction from which the cosmic ray came.
Finally, the CMOS devices in pervasive smartphone cameras have been proposed as a practical distributed network to detect air showers from ultra-high energy cosmic rays (UHECRs) which is at least comparable with that of conventional cosmic ray detectors. The app, which is currently in beta and accepting applications, is CRAYFIS (Cosmic RAYs Found In Smartphones).
Effects.
Changes in atmospheric chemistry.
Cosmic rays ionize the nitrogen and oxygen molecules in the atmosphere, which leads to a number of chemical reactions. One of the reactions results in ozone depletion. Cosmic rays are also responsible for the continuous production of a number of unstable isotopes in the Earth's atmosphere, such as carbon-14, via the reaction:
Cosmic rays kept the level of carbon-14 in the atmosphere roughly constant (70 tons) for at least the past 100,000 years, until the beginning of above-ground nuclear weapons testing in the early 1950s. This is an important fact used in radiocarbon dating used in archaeology.
Role in ambient radiation.
Cosmic rays constitute a fraction of the annual radiation exposure of human beings on the Earth, averaging 0.39 mSv out of a total of 3 mSv per year (13% of total background) for the Earth's population. However, the background radiation from cosmic rays increases with altitude, from 0.3 mSv per year for sea-level areas to 1.0 mSv per year for higher-altitude cities, raising cosmic radiation exposure to a quarter of total background radiation exposure for populations of said cities. Airline crews flying long distance high-altitude routes can be exposed to 2.2 mSv of extra radiation each year due to cosmic rays, nearly doubling their total ionizing radiation exposure.
Effect on electronics.
Cosmic rays have sufficient energy to alter the states of circuit components in electronic integrated circuits, causing transient errors to occur, such as corrupted data in electronic memory devices, or incorrect performance of CPUs, often referred to as "soft errors" (not to be confused with software errors caused by programming mistakes/bugs). This has been a problem in electronics at extremely high-altitude, such as in satellites, but with transistors becoming smaller and smaller, this is becoming an increasing concern in ground-level electronics as well. Studies by IBM in the 1990s suggest that computers typically experience about one cosmic-ray-induced error per 256 megabytes of RAM per month. To alleviate this problem, the Intel Corporation has proposed a cosmic ray detector that could be integrated into future high-density microprocessors, allowing the processor to repeat the last command following a cosmic-ray event.
Cosmic rays were suspected as a possible cause of an in-flight incident in 2008 where an Airbus A330 airliner of Qantas twice plunged hundreds of feet after an unexplained malfunction in its flight control system. Many passengers and crew members were injured, some seriously. After this incident, the accident investigators for NW Australia gave the ruling that," there was a design flaw in the air data inertial reference unit and a limitation of the aircraft's computer software. The serious Qantas A330 upset was caused by a long dormant computer flaw triggered by chance." 
This has prompted a software upgrade to all A330 and A340 airliners, worldwide, so that any data spikes in this system are filtered out electronically.
Significance to aerospace travel.
Galactic cosmic rays are one of the most important barriers standing in the way of plans for interplanetary travel by crewed spacecraft.
Cosmic rays also pose a threat to electronics placed aboard outgoing probes. In 2010, a malfunction aboard the Voyager 2 space probe was credited to a single flipped bit, probably caused by a cosmic ray. Strategies such as physical or magnetic shielding for spacecraft have been considered in order to minimize the damage to electronics and human beings caused by cosmic rays.
Flying high, passengers and crews of jet airliners are exposed to at least 10 times the cosmic ray dose that people at sea level receive. Aircraft flying polar routes near the geomagnetic poles are at particular risk.
Role in lightning.
Cosmic rays have been implicated in the triggering of electrical breakdown in lightning. It has been proposed that essentially all lightning is triggered through a relativistic process, "runaway breakdown", seeded by cosmic ray secondaries. Subsequent development of the lightning discharge then occurs through "conventional breakdown" mechanisms.
Postulated role in climate change.
A role of cosmic rays directly or via solar-induced modulations in climate change was suggested by Edward P. Ney in 1959 and by Robert E. Dickinson in 1975. The idea has been revived in recent years, most notably by Henrik Svensmark, who has argued that because solar variation modulates the cosmic ray flux on Earth, they would consequently affect the rate of cloud formation and hence the climate. However, other scientists have vigorously criticized Svensmark for sloppy and inconsistent work: one example is adjustment of cloud data that understates error in lower cloud data, but not in high cloud data; another example is "incorrect handling of the physical data" resulting in graphs that do not show the correlations they claim to show.
In any case, 97% of climate scientists support the conclusions in the 2007 IPCC synthesis report, which strongly attributes a major role in the ongoing global warming to human-produced gases such as carbon dioxide, methane, nitrous oxide, and halocarbons, and has stated that models including natural forcings only (including aerosol forcings, which cosmic rays are considered by some to contribute to) would result in far less warming than has actually been observed or predicted in models including anthropogenic forcings.
Svensmark is one of several scientists outspokenly opposed to the mainstream scientific assessment of global warming. His estimates on the magnitude of the effect of GCR (galactic cosmic rays) on global warming continue to be refuted in the mainstream scientific press. For instance, a November 2013 study showed that less than 14 percent of global warming since the 1950s could be attributed to cosmic ray rate, and while the models showed a small correlation every 22 years, the cosmic ray rate did not match the changes in temperature, indicating that it was not a causal relationship. Another 2013 study found, contrary to Svensmark's claims, "no statistically significant correlations between cosmic rays and global albedo or globally averaged cloud height."
Research and experiments.
There are a number of cosmic-ray research initiatives.

</doc>
<doc id="47691" url="https://en.wikipedia.org/wiki?curid=47691" title="Fork (chess)">
Fork (chess)

In chess, a fork is a tactic whereby a single piece makes two or more direct attacks simultaneously. Most commonly two pieces are threatened, which is also sometimes called a "double attack". The attacker usually aims to gain material by capturing one of the opponent's pieces. The defender often finds it difficult to counter two or more threats in a single move. The attacking piece is called the "forking" piece; the pieces attacked are said to be "forked". A piece that is defended can still said to be forked if the forking piece has a lower value.
Besides attacking pieces, a target of a fork can be a direct mating threat (for example, attacking an unprotected knight while simultaneously setting up a battery of queen and bishop to threaten mate). Or a target can be an implied threat (for example, a knight may attack an unprotected piece while simultaneously threaten to fork queen and rook).
Forks are often used as part of a combination which may involve other types of chess tactics as well.
Forking piece.
The type of fork is named after the type of forking piece. For example, a "knight fork" is a knight move that attacks two or more opponent's pieces simultaneously. Any type of piece can perform a fork—including the king—and any type of piece can be forked. A fork is most effective when it is forcing, such as when the king is put in check.
Knights are often used for forks. Their unique L-shaped move means that they can attack any other type of piece, including the powerful queen, without being attacked by their targets.
The queen is also often used to fork, but since the queen is usually more valuable than the pieces it attacks, this typically gains material only when the pieces attacked are undefended or if one is undefended and the opposing king is checked. The possibility of a "queen fork" is a very real threat when the queen is in the open, as is often the case in endgames. If a player wants to force an exchange of queens, forking the opposing queen and king (or an undefended piece) with a protected queen can be useful.
Pawns other than rook pawns (those on the a- and h-files) can also be used to fork by attacking two enemy pieces diagonally—one to the left, the other to the right.
Example from a game.
This example is from the first round of the FIDE World Chess Championship 2004 between Mohamed Tissir and Alexey Dreev. After 33... Nf2+ 34. Kg1 Nd3, White resigned. In the final position the black knight forks the white queen and rook; after the queen moves away, Black will win the exchange.
Example from an opening.
In the Two Knights Defense (1.e4 e5 2.Nf3 Nc6 3.Bc4 Nf6) after 4. Nc3, Black can eliminate White's e4-pawn immediately with 4... Nxe4! due to the "fork trick" 5. Nxe4 d5—regaining either the bishop or the knight. 
Escaping forks.
Forks can possibly be escaped. A forked piece such as the queen might check the enemy king, a zwischenzug, giving time to move the second forked piece to safety on the next move.
Other terms.
A fork of the king and queen, the highest material-gaining fork possible, is sometimes called a "royal fork". A fork of the opponent's king, queen, and one (or both) rooks is sometimes called a "grand fork". A knight fork of the opponent's king, queen, and possibly other pieces is sometimes called a "family fork" or "family check".

</doc>
<doc id="47692" url="https://en.wikipedia.org/wiki?curid=47692" title="Backyard Blitz">
Backyard Blitz

Backyard Blitz was a Logie Award winning Australian lifestyle and DIY television program that aired on the Nine Network between 2000 through to 2007 before its cancellation. It was hosted by Jamie Durie and was produced by Don Burke.
The show featured a very similar premise to the show "Ground Force", in which a team of gardeners employed by the show descend on a supposedly worthy individual's place and improve the garden for the cameras within a specified time limit. This similarity in fact led to legal action being taken by the rival Seven Network who at the time was set to debut an Australian version of "Ground Force".
The show like many of its other lifestyle brethren was mainly watched by older viewers and was widely derided by younger viewers and television critics. However it was a strong ratings performer.
On 14 November 2006 "Backyard Blitz" was axed by the Nine Network after seven years on air. Don Burke, whose own show "Burke's Backyard" was broadcast by Nine for nearly 18 years before it was axed in 2004, said his production company was "quite shocked by this decision". In mid-2007 Nine aired the six remaining unaired episodes that were filmed before the show was cancelled. In 2008, it aired a spin-off show Domestic Blitz hosted by Shelley Craft and Scott Cam
Presenters.
The four regular presenters on the show were landscaper Jamie Durie (the main host and the shows lead landscaper), Scott Cam (builder/carpenter), Nigel Ruck (landscaper) and Jody Rigby (horticulturist).
Awards.
During the shows run, it has won (and been nominated) for several Logie Awards. The show won six consecutive 'Most Popular Lifestyle Program' awards (2002–2006). It was nominated in the same category in 2007 but lost. Jamie Durie won the 'Most Popular New Male Talent' in 2001 and was nominated for 'Most Popular TV Presenter' in 2005 and 2006 for his role on the show.

</doc>
<doc id="47693" url="https://en.wikipedia.org/wiki?curid=47693" title="Jamie Durie">
Jamie Durie

Jamie Paul Durie OAM (born 3 June 1970) is an Australian-born international horticulturalist and landscape designer. He is founder and Director of Durie Design, a television host and producer, the author of nine best-selling books, an environmentalist and a humanitarian.
Durie has been the television host of the Seven Network's "The Outdoor Room" and the US PBS series "The Victory Garden".
Early life.
Durie was born in Manly, an oceanside suburb in Sydney, to a Sri Lankan mother and Australian father. He spent most of his early childhood in the mining town of Tom Price in north Western Australia. During the 1980s he was also working at night club Jamison Street in Sydney with the male review boys. A former model, Durie performed with the international cabaret group Manpower Australia in the early 1990s. It was during his tours with Manpower, and his creative roles within the production, that Durie began to develop his interest in garden design and architecture. He then returned to study design and horticulture in Sydney for four years at the age of 26. Durie has received 33 International Design Awards since.
Television career.
Durie has hosted and featured in over 35 prime-time lifestyle shows and appeared on television shows around the world. The top-rating television shows he has hosted or featured in include "The Outdoor Room", "Australia's Best Backyards", "The Block", "Backyard Blitz", "Torvil and Dean's Dancing on Ice", "Disney on Ice", "Cirque Du Soleil Specials", and "White Room Challenge".
He has received seven Australian TV Logie awards including the Logie award for Most Popular New Male Talent and six consecutive Logie awards for hosting the Most Popular Australian Lifestyle Program for "Backyard Blitz".
Durie appeared regularly on "The Oprah Winfrey Show" following his first appearance in November 2006. Durie appeared on a number of Winfrey's shows during his five-year contract to Harpo and publicly gives credit to Oprah for his USA career.
In the US, Durie's "The Outdoor Room on HGTV" has had four seasons. He has hosted nine independent shows on HGTV including "HGTV Showdown", "HGTV Dream Home", "HGTV Green Home", "The Rose Parade" and hosts America's longest running gardening program on PBS, "The Victory Garden". Durie has appeared in other programs in Australia and the USA including "Dancing with the Stars", "60 Minutes", "Celebrity Millionaire", "The Price is Right", "Better Homes and Gardens", "Hard Copy", "Entertainment Tonight", "E news", "NBC Today Show", "Donahue", "Lifestyles of the Rich and Famous", and was host and judge on the US series "Top Design" in Australia.
Books and online publishing.
Durie is the author of nine best-selling books: (in order of release) "Patio—Garden Design and Inspiration", "The Outdoor Room", "Outside", "The Source Book" (Editions 1 and 2), "Outdoor Kids", "Inspired", "Jamie Durie's The Outdoor Room" and "100 Gardens".
Durie and his team produce and publish jamiedurie.com, jamieduriedesign.com and in January 2012 Durie released his own iPhone app, Garden Design With Jamie Durie.
He is founder and Editorial Director of "The Outdoor Room" magazine.
Landscape design.
After completing four years of study in horticulture and design, Durie founded the landscape design company Patio Landscape Architecture and Design in 1998. Patio was renamed Durie Design in 2010.
Durie has received 34 design awards. He has been invited to exhibit and compete at international garden shows and won gold medals in Australia, Singapore, Japan, New Zealand and the Chelsea Flower Show in the UK. He continues to focus on large-scale resort and hotel designs globally with designs in Australia, the US, Canada, Spain, Thailand, Indonesia, Singapore, Dubai, Bahrain, Hong Kong and the Caribbean. His latest large scale resort project involved designing and rejuvenating the landscape, furniture, sculpture and gardens at Hayman Island and its private estates on Australia's Great Barrier Reef, after they were devastated by tropical cyclones Anthony and Yasi.
Furniture and product design.
In 2003 Durie designed and launched a range of outdoor furniture and gardening products under the brand name "Patio by Jamie Durie". The range includes furniture, décor, gardening tools, lighting, textiles, BBQs, ornaments and accessories. All the timber products in the range are environmentally responsible FSC certified timber. It used to be sold through Kmart, and is now sold exclusively through Big W Australia.
Durie designed the Jamie Durie Signature range of products in collaboration with other artists and designers, including a range of paints with Porters Paints, textiles with Cloth, Durie Design outdoor furniture range with David Knott, a studio design with Modern-Shed and a range of rugs with the Rug Collection.
Environment and charity.
Durie is an advocate for environmental conservation. He trained with former US Vice-President Al Gore as a Climate Change Presenter (now named Climate Reality), launched Clean up the World Day with the UN in New York, and is an Ambassador for Greenpeace, Australian Conservation Foundation, Planet Ark and National Tree Day. He also hosted the Australian Conservation Foundation's Spirituality and Sustainability Forum with the Dalai Lama, and is an ambassador and former board member of the Royal Botanic Gardens Foundation, Sydney. Durie is an ambassador for the Forest Stewardship Council and Earth Hour.
Humanitarian work.
Durie has donated his time and resources to several charities, including Plan International, the Children's Cancer Institute, FSHD, Sydney Children's Hospital and the Children's Hospital at Westmead, the Variety Club, Rotary, the Children First Foundation, and Westmead Millennium Institute. He was executive producer and host of "Jamie's Journey – Hope for Uganda's Children" and "Jamie's Journey with the Children of India", both produced in conjunction with Plan International to raise awareness of HIV Aids in Uganda and highlight the challenges and successes of Plan's early childhood care programs in India.
Personal.
Durie's parents, Ron "Dave" and Joy (who was born in Sri Lanka), divorced when he was aged 10. He and his brother Chris moved to the Gold Coast, Queensland with their mother.
Durie has a daughter, Taylor (born 1997), from a relationship with Michelle Gennock. He has since been engaged to Terasa Livingstone and Siobhan Way.

</doc>
<doc id="47694" url="https://en.wikipedia.org/wiki?curid=47694" title="Pin (chess)">
Pin (chess)

In chess, a pin is a situation brought on by an attacking piece in which a defending piece cannot move without exposing a more valuable defending piece on its other side to capture by the attacking piece. "To pin" refers to the action of the attacking piece inducing the pin, and the defending piece so restricted is described as pinned.
Only pieces that can move an indefinite number of squares in a horizontal, vertical, or diagonal line, "i.e.", bishops, rooks and queens, can pin opposing pieces. Kings, knights, and pawns cannot pin. Any piece may be pinned except the king, as the king must be immediately removed from check under all circumstances.
Types.
Absolute pin.
An "absolute pin" is one where the piece shielded by the pinned piece is the king. In this case it is illegal to move the pinned piece out of the line of attack, as that would place one's king in check. 
Relative pin.
A "relative pin" is one where the piece shielded by the pinned piece is a piece other than the king, but typically more valuable than the pinned piece. Moving such a pinned piece is legal, but may not be prudent as the shielded piece would then be vulnerable to capture. (See diagram at right.)
Partial pin.
If a rook or queen is pinned along a file or rank, or a bishop or queen is pinned along a diagonal, the pin is a "partial pin": the pinned unit can still move along its line but cannot leave that line. A partially pinned unit may break its own pin by capturing the pinning piece; however, a partial pin can still be advantageous to the pinning player, for instance if the queen is pinned by a rook or bishop, and the pinning piece is defended, so that capturing it with the queen would lose material. Note that a queen can only ever be partially pinned, as it can move in any linear direction.
It is possible for two opposing pieces to be partially pinning each other. It is also possible for one piece to be pinned in one direction (line of attack) and partially pinned in another, or otherwise pinned in two or more directions.
Situational pin.
Sometimes in a chess game position, a piece may be considered to be in a "situational pin". In a situational pin, moving the pinned piece out of the line of attack will result in a situation detrimental to the player of the pinned piece, such as a checkmate. Although a situational pin is not an absolute pin and the pinned piece can still be moved according to the rules, moving out of line of attack can result in a bad situation or even immediate loss of the game.
Consider the chess position shown at right. The black bishop has just moved from e6 to d5, making itself unprotected and available for capture by the white knight on b4. It is now white's turn to move. White should not capture the black bishop or otherwise move the knight because after 1.Nxd5, 1...Rb1+ wins white's rook, because the king is forced to move away from the check, thereby exposing the rook to attack (a skewer).
Pin combinations.
Pinning can also be used in combination with other tactics. For example, a piece can be pinned to prevent it from moving to attack, or a defending piece can be pinned as part of tactic undermining an opponent's defense. 
A pinned piece can usually no longer be counted on as a defender of another friendly piece (that is out of the pinning line of attack) or as an attacker of an opposing piece (out of the pinning line). However, a pinned piece can still check the opposing king - and therefore still can defend friendly pieces against captures made by the enemy king.
Unpinning.
The act of breaking a pin is "unpinning". This can be executed in a number of ways: the piece creating the pin can be captured; another unit can be moved onto the line of the pin; or the unit to which a piece is pinned can be moved.
Although a pin is not a tactic in itself, it can be useful in tactical situations. One tactic which takes advantage of a pin can be called "working the pin". In this tactic, other pieces from the pinning piece's side attack the opposing pinned piece. Since the pinned piece cannot move out of the line of attack, the pinned piece's player may move other pieces to defend the pinned piece, but the pinning player may yet attack with even more pieces, etc. 
Pins commonly seen in game play.
A pin that often occurs in openings is the move Bb5 which, if Black has moved ...Nc6 and ...d6 or ...d5, pins the knight on c6, because moving the knight would expose the king on e8 to check. (The same may, of course, occur on the other flank, with a bishop on g5, or by Black on White, with a bishop on b4 or g4.)
A common way to win the queen is to pin her to the king with a rook: for instance with the black queen on e5 and the black king on e8 and no other pieces on the e-file, the move Re1 by White would pin Black's queen.
Example of a pin in a real game.
In the diagram at right with white to move next, Black is threatening the following rook sacrifice leading to mate. 
The pawn on g2 cannot take the rook on h3 because the queen on g3 is pinning the pawn with a vertical line of attack. The only move to prevent the above moves is 27.Nf4 which temporarily blocks black's bishop from protecting his queen, but to no avail. The black bishop can take the knight by 27...Bxf4 renewing the same threat of mate in 2, or Black can respond as follows to mate anyway: 
In this case, white could not take the mating rook now on f3 with the g2 pawn because the queen on h2 would now be pinning the pawn with a horizontal line of attack. With mate against him being inevitable, white resigned after move 26. 

</doc>
<doc id="47696" url="https://en.wikipedia.org/wiki?curid=47696" title="Sport utility vehicle">
Sport utility vehicle

A sport utility vehicle (SUV, sometimes called a sports utility wagon) is a vehicle similar to a station wagon or estate car that is usually equipped with four-wheel drive for on-road or off-road ability. Some SUVs include the towing capacity of a pickup truck with the passenger-carrying space of a minivan or large sedan.
Definitions.
According to the Merriam-Webster dictionary, a "sport utility vehicle" is "a rugged automotive vehicle similar to a station wagon but built on a light-truck chassis". The "SUV" term is defined as "a large vehicle that is designed to be used on rough surfaces but that is often used on city roads or highways." The "SUV" acronym "is still used to describe nearly anything with available all-wheel drive and raised ground clearance."
North America.
There are a number of definitions for an SUV. Most government regulations simply have categories for "off-highway vehicles," which in turn are lumped in with pickup trucks and minivans as light trucks." The auto industry has not settled on one definition.
Nevertheless, four-wheel-drive SUVs are considered light trucks in North America (and two-wheel-drive SUVs up to the 2011 model year) where they were regulated less strictly than passenger cars under two laws in the United States, the Energy Policy and Conservation Act for fuel economy, and the Clean Air Act for emissions. Starting in 2004, the United States Environmental Protection Agency (EPA) began to hold sport utility vehicles to the same tailpipe emissions standards as cars.
Many people question "how can an SUV be called a truck?" Although the original definition of the "light truck" classification included pickups and delivery vans, usually SUVs and minivans are included in this category because these vehicles are designed to "permit greater cargo-carying capacity than passenger carrying volume. Manufacturing, emissions, and safety regulations in the U.S. classify "an SUV is a truck"; however, for local licensing and traffic enforcement, "an SUV may be a truck or a car" because the classification of these vehicles varies from state to state. For industry production statistics, SUVs are counted in the light truck product segment.
Other markets.
The term is not used in all countries, and outside North America the terms "off-road vehicle", "four-wheel drive" or "four-by-four" (abbreviated to "4WD" or "4×4") or simply use of the brand name to describe the vehicle like "Jeep" or "Land Rover" are more common.
In Europe, the term SUV has a similar meaning, but being newer than in the U.S. it only applies to the newer street oriented one, where-as "Jeep", "Land Rover" or 4x4 are used for the off-roader oriented ones. Not all SUVs have four-wheel drive capabilities, and not all four-wheel-drive passenger vehicles are SUVs. Although some SUVs have off-road capabilities, they often play only a secondary role, and SUVs often do not have the ability to switch among two-wheel and four-wheel-drive high gearing and four-wheel-drive low gearing. While automakers tout an SUV's off-road prowess with advertising and naming, the daily use of SUVs is largely on paved roads.
In India, all SUVs are classified in the "Utility Vehicle" category per the Society of Indian Automobile Manufacturers (SIAM) definitions and carry a 27% excise tax. Those that are long, have a engine or larger, along with of ground clearance, are subject to a 30% excise duty.
Designs.
Although designs vary, SUVs were built with a body-on-frame chassis similar to that found on light trucks. Early SUVs were mostly two-door models, and often available with removable tops. Most SUVs are two-box design featuring an engine compartment with a combined passenger and cargo compartment, as in a station wagon body. Mid-size and full-size SUVs have two of three rows of seats with a cargo area directly behind the last row of seats.
The original 1984 Jeep Cherokee (XJ) made by American Motors combined passenger car comfort features with truck chassis strength in a unibody structure for ease of driving in difficult conditions, at the same time establishing the modern SUV market segment and responsible its growth during the late-1980s and early-1990s. The compact-sized XJ Cherokee was available in both two- and four-door versions, becoming one of the most popular SUVs ever made with over 2.8 million built between 1984 and 2001.
Consumer demand pushed the SUV market towards four doors, and by 2002 all full-size two-door SUVs were gone from the market. The Jeep Wrangler remained as a compact two-door body style, and was also joined by a four-door variant starting with the 2007 model year, the Wrangler Unlimited. Another trend was that the SUV label "brought up negative associations with large size and poor gas mileage" of the body-on-frame versions causing automakers to repurpose their volume production passenger car platforms with tall interior packaging, configurations for either passenger or cargo, higher ground-clearance, and even all-wheel-drive capability that became known as crossover vehicles offering "the practicality of an SUV to the drivability and fuel efficiency of a car."
SUVs are known for high ground clearance, upright, boxy body, and high H-point. This can make them more susceptible to dangerous roll over due to their high center of gravity. For example; although built on the same platform, the BMW X5 SUV has higher rollover risk of 17.4% compared to the 9.3% of the BMW 5 Series sedan.
A mini SUV (also called subcompact SUV or subcompact crossover) is a class of small sport utility vehicles. The term usually applies to crossovers based on a supermini (B-segment cars in Europe) platform.
A compact SUV is a class of smaller SUVs that are commonly built with less cargo and passenger space, and often with smaller engines resulting in better fuel economy, the term is often interchangeable with crossover SUV.
A mid-size SUV is a class of medium-size SUVs whose size typically falls between that of a full-size and a compact SUV. This term is not commonly used outside North America, where fullsize and midsize SUVs are considered similar.
A full-size SUV is a class of large-size SUVs that are most often larger than midsize SUVs. They have greater cargo and passenger space than midsize SUVs. Full Size SUVs are usually given higher safety ratings than their smaller counterparts.
An extended length SUV, also sometimes called a long-wheel based SUV, are vehicles that are similar to a full-size SUV, except that these vehicles have a larger cargo area (around ) and passenger space that can seat up to 8 or 9 people (with the available third row seating that when folded or removed adds more cargo space). Although these extended length SUVs are mostly sold in North America because of their size and the roads are made and designed differently, they can also be found in other countries, exported to such places like The Philippines and The Middle East. The vehicles are to in length and can be distinguished by the rear wheel area not touching the rear doors.
History.
Origins.
Early SUVs were descendants from commercial and military vehicles as the World War II Jeep and Land Rover.
The earliest examples of longer-wheelbase wagon-type SUVs were the Chevrolet Carryall Suburban (1935, RWD only), GAZ-61 (1938, 4×4), Willys Jeep Station Wagon (1948), Pobeda M-72 (GAZ-M20/1955), which Russian references credit as possibly being the first modern SUV (with unitary body rather than body-on-frame), International Harvester Travelall (1953), Land Rover Series II 109 (1958), and the International Harvester Scout 80 (1961). These were followed by the more 'modern' Jeep Wagoneer (1963), International Harvester Scout II (1971), Ford Bronco (1966), Toyota Land Cruiser FJ-55 (1968), the Chevrolet Blazer/GMC Jimmy (1969), and the Land Rover Range Rover (1970). The actual term "sport utility vehicle" did not come into wide popular usage until the late 1980s; many of these vehicles were marketed during their era as station wagons.
According to Robert Casey, the transportation curator at the Henry Ford Museum, the Jeep Cherokee (XJ) was the first true sport utility vehicle in the modern understanding of the term. Developed under the leadership of AMC's François Castaing and marketed to urban families as a substitute for a traditional car (and especially station wagons, which were still fairly popular at the time), the Cherokee had four-wheel drive in a more manageable size (compared to the full-size Wagoneer), as well as a plush interior resembling a station wagon. With the introduction of more luxurious models and a much more powerful 4-liter engine, sales of the Cherokee increased even higher as the price of gasoline fell, and the term "sport utility vehicle" began to be used in the national press for the first time. "The advent and immediate success of AMC/Jeep's compact four-door Cherokee turned the truck industry upside down."
The corporate average fuel economy (CAFE) standard was ratified in the 1970s to regulate the fuel economy of passenger vehicles. Car manufacturers evaded the regulation by selling SUVs as work vehicles. The popularity of SUV increased among urban drivers in the last 25 years, and particularly in the last decade. Consequently, modern SUVs are available with luxury vehicle features, and some crossover models adopt lower ride heights to accommodate on-road driving.
Keith Bradsher explained the rise of the SUV with American Motors' (AMC) lobbying the United States Environmental Protection Agency (EPA) for a waiver of the United States Clean Air Act. The EPA subsequently designated AMC's compact Cherokee as a "light truck", and the company marketed the vehicle to everyday drivers. AMC's effort to affect rulemaking changing the official definition of their new model then led to the SUV boom when other auto makers marketed their own models in response to the Cherokee taking sales from their regular cars.
Popularity.
SUVs became popular in the United States, Canada, India and Australia in the 1990s and early-2000s. U.S. automakers could enjoy profit margins of $10,000 per SUV, while losing a few hundred dollars on a compact car. For example, the Ford Excursion could net the company $18,000, while they could not break even with the Ford Focus unless the buyer chose options, leading Detroit's big three automakers to focus on SUVs over small cars.
The higher cost of union labor in the U.S. and Canada compared to the lower wages of non-union workers at non-U.S. companies like Toyota, made it unprofitable for American auto makers to build small cars in the U.S. For example, the General Motors factory in Arlington, Texas where rear-wheel-drive cars were built, such as the Chevrolet Caprice, Buick Roadmaster, and Cadillac Fleetwood Brougham was converted to truck and SUV production, putting an end to full-size family station wagon and overall terminating production of rear-wheel drive full-size cars. Due to the shift in the Big Three's strategy, many long-running cars like the Ford Taurus, Buick Century and Pontiac Grand Prix fell behind their Japanese competitors in features and image (relying more on fleet sales instead of retail and/or heavy incentive discounts); some were discontinued.
Buyers were drawn to SUVs' large cabins, higher ride height, and perceived safety. Full-size SUVs often offered features such as three-row seating, to effectively replace full-size station wagons and minivans. Wagons were seen as old-fashioned. Additionally, full-size SUVs have greater towing capabilities than conventional cars, and can haul trailers, travel trailers (caravans) and boats. Increased ground clearance is useful in climates with heavy snowfall. The very low oil prices of the 1990s helped to keep down running costs. The SUV was one of the most popular choices of vehicle for female drivers in the U.S. The 1990 Ford Explorer was also popular despite it being one of several Ford SUV models described as "dangerous vehicles" through the 2000s. By 1994, SUVs outsold minivans in the United States although they were also not ranked high in safety. Peaking in the late-1990s and early to mid-2000s, SUVs sales temporarily declined due to high oil prices and a declining economy. The traditional truck-based SUV is gradually being supplanted by the crossover SUV, which uses an automobile platform for lighter weight and better fuel efficiency.
Social scientists have drawn on popular folklore such as urban legends to illustrate how marketers have been able to capitalize on the feelings of strength and security offered by SUVs. Popular tales include narratives where mothers save the family from armed robbery and other incidents by taking the automobile off road, for example.
In Australia, SUV sales were helped by the fact that SUVs had much lower import duty than passenger cars did, so that they cost less than similarly equipped imported sedans. However, this gap was gradually narrowed, and in January 2010 the import duty on cars was lowered to match the 5 percent duty on SUVs.
Sales of SUVs and other light trucks fell in the mid-2000s because of high oil prices and declining economy. In June 2008, General Motors announced plans to close four truck and SUV plants, including the Oshawa Truck Assembly. The company cited decreased sales of large vehicles in the wake of rising fuel prices. The business model of focusing on SUVs and light trucks, at the expense of more fuel-efficient compact and midsized cars, is blamed for declining sales and profits among Detroit's Big Three automakers since the mid–late-2000s. The Big Three were slower to adapt than their Japanese rivals in producing small cars to meet growing demand due to inflexible manufacturing facilities, which made it unprofitable to build small cars. However, starting in 2010 SUV and light truck sales have started an upward trend due to lower gas prices and a revival of the North American economy. In 2013, General Motors saw its sales for its large SUVs increased by 74%, making them the largest producer of SUVs in the United States. However, the "small and compact SUVs, when compared with other vehicles in the light truck segment, has made this vehicle segment the third highest selling vehicle segment in the automotive market in 2013." With the redesigned GM and Ford large SUVs being introduced in 2014 (for the 2015 model year), it has seen a slight resurgence among consumers due to better fuel economy and new engines, along with updated and newer features.
Use in remote areas.
SUVs are sometimes driven off-road on farms and in remote areas of such places as the Australian Outback, Africa, the Middle East, Alaska, Canada, Iceland, South America, Russia and parts of Asia which have limited paved roads and require a vehicle to have all-terrain handling, increased range, and storage capacity. The scarcity of spare parts and the need to carry out repairs quickly resulted in the popularity of vehicles with the bare minimum of electric and hydraulic systems, such as the basic versions of the Land Rover, Jeep Wrangler, Nissan Patrol and Toyota Land Cruiser. SUVs for urban driving have traditionally been developed from their more rugged all-terrain counterparts. For example, the Hummer H1 was developed from the HMMWV, originally developed for the military of the United States.
As many SUV owners never used the off-road capabilities of their vehicle, newer SUVs (including crossovers) now have lower ground clearance and suspension designed primarily for paved roads.
Some buyers choose SUVs because they have more interior space than sedans of similar sizes. In areas with gravel roads in summer and snow and ice in winter, four-wheel drives offer a safety advantage due to their traction advantages under these conditions.
The sport utility vehicles have also gained popularity in some areas of Mexico, especially in desert areas or in cities where drivers frequently encounter potholes, detours, high water and rough roads. Increasing use is also attributed to the high number of dirt roads outside major population centers, resulting in washboard and mud in the rainy seasons.
Use in recreation and motorsport.
Some highly modified SUVs, together with their more rugged off-road counterparts, are also used to explore places otherwise unreachable by other vehicles. In Australia, China, Europe, South Africa, South America and the United States at least, 4WD clubs have been formed for this purpose. Modified SUVs also take part in races, including the Paris-Dakar Rally, the Baja racing series, TREC events, King of the Hammers in California and the Australian Outback.
The Trophee Andros ice-racing series is another competition where SUVs participate as well.
Many 4×4 mud racing events and other activities take place throughout the US organized by clubs and associations.
Luxury SUV.
Numerous luxury vehicles in the form of SUVs and pickup trucks are being produced. Luxury SUV is principally a marketing term to sell fancier vehicles that may have higher performance, comfort, technology, or brand image. The term lacks both measurability and verifiability, and it is applied to a broad range of SUV sizes and types.
Nevertheless, the marketing category was created in 1966 with Kaiser Jeep's luxurious Super Wagoneer. It was the first SUV to offer a V8 engine, automatic transmission, and luxury car trim and equipment in a serious off-road model. It came with bucket seating, air conditioning, sun roof, and even a vinyl roof. Land Rover followed suit in 1970 by introducing the Range Rover. The trend continued with other competitors adding comfort features to their rudimentary and truck-based models.
The production of luxury models increased in the late-1990s with vehicles such as the Lincoln Navigator and Cadillac Escalade. These luxury SUVs generated higher profit margins than non-luxury SUVs did. For some auto makers, luxury SUVs were the first SUV models they produced. Some of these models are not traditional SUVs based on light truck as they are classified as crossovers.
The luxury SUV class encompasses both smaller 5-passenger SUVs and larger 7-passenger SUVs, with luxury features both inside of the cabin but also in the outside. Buyers looking for a luxury vehicle that offers more cargo capacity than a sedan may prefer a luxury SUV. This is also a vehicle aimed for those who prefer an SUV with a little more style.
Luxury SUVs typically offer the most expected safety features including side airbags, ABS and traction control, and many of them also come with electronic stability control, crash resistant door pillars, dynamic head restraints and back-up sensing systems.
The U.S. News & World Report Rankings and Reviews ranks premium midsize SUVs and crossovers based on an in-depth analysis by its editors of published auto ratings, reviews and test drives. Ranking is based on the score on performance, exterior, interior, safety, and reliability obtained by the vehicles.
Other names.
In Australia and New Zealand, the term "SUV" is not widely used, except by motoring organizations, the press, and industry bodies. Passenger class vehicles designed for off-road use are known as 'four-wheel drives', '4WDs', or '4×4s'. Some manufacturers do refer to their products as SUVs, but others invented names such as XUV, (HSV Avalanche XUV or GMC Envoy XUV) or action utility vehicles (AUVs). The term 'AWD', or all-wheel drive, is used for any vehicle which drives on all four wheels, but may not be designed for off-road use. 'Crossover' is a marketing term for a vehicle that is both four-wheel-drive and primarily a road car.
In Norway the term "Børstraktor" (Stock Exchange Tractor) serves a similar purpose.
In Finland the term "katumaasturi" is commonly used to designate SUVs. It roughly translates to street-off-roader, or street-4×4. This marks the difference with what is called "maasturi" which is a vehicle with off-road capability.

</doc>
<doc id="47697" url="https://en.wikipedia.org/wiki?curid=47697" title="Timecode">
Timecode

A timecode (alternatively, time code) is a sequence of numeric codes generated at regular intervals by a timing synchronization system.
Video and film timecode.
In video production and filmmaking, SMPTE timecode is used extensively for synchronization, and for logging and identifying material in recorded media. During filmmaking or video production shoot, the camera assistant will typically log the start and end timecodes of shots, and the data generated will be sent on to the editorial department for use in referencing those shots. This shot-logging process was traditionally done by hand using pen and paper, but is now typically done using shot-logging software running on a laptop computer that is connected to the time code generator or the camera itself.
The SMPTE family of timecodes are almost universally used in film, video and audio production, and can be encoded in many different formats, including:
Keykode, while not a timecode, is used to identify specific film frames in film post-production that uses physical film stock. Keykode data is normally used in conjunction with SMPTE time code.
Rewritable consumer timecode is a proprietary consumer video timecode system that is not frame-accurate, and is therefore not used in professional post-production.
Other time code formats.
Time codes for purposes other than video and audio production include:

</doc>
<doc id="47698" url="https://en.wikipedia.org/wiki?curid=47698" title="Victoria Adams">
Victoria Adams

Victoria Adams may refer to:

</doc>
<doc id="47700" url="https://en.wikipedia.org/wiki?curid=47700" title="Coral">
Coral

Corals are marine invertebrates in the class Anthozoa of phylum Cnidaria. They typically live in compact colonies of many identical individual polyps. The group includes the important reef builders that inhabit tropical oceans and secrete calcium carbonate to form a hard skeleton.
A coral "group" is a colony of myriad genetically identical polyps. Each polyp is a sac-like animal typically only a few millimeters in diameter and a few centimeters in length. A set of tentacles surround a central mouth opening. An exoskeleton is excreted near the base. Over many generations, the colony thus creates a large skeleton that is characteristic of the species. Individual heads grow by asexual reproduction of polyps. Corals also breed sexually by spawning: polyps of the same species release gametes simultaneously over a period of one to several nights around a full moon.
Although some corals can catch small fish and plankton, using stinging cells on their tentacles, most corals obtain the majority of their energy and nutrients from photosynthetic unicellular dinoflagellates in the genus "Symbiodinium" that live within their tissues. These are commonly known as zooxanthellae and the corals that contain them are zooxanthellate corals. Such corals require sunlight and grow in clear, shallow water, typically at depths shallower than . Corals are major contributors to the physical structure of the coral reefs that develop in tropical and subtropical waters, such as the enormous Great Barrier Reef off the coast of Queensland, Australia.
Other corals do not rely on zooxanthellae and can live in much deeper water, with the cold-water genus "Lophelia" surviving as deep as . Some have been found on the Darwin Mounds, north-west of Cape Wrath, Scotland. Corals have also been found as far north as off the coast of Washington State and the Aleutian Islands.
Taxonomy.
In his "Scala Naturae", Aristotle classified corals as "zoophyta" ("plant-animals"), animals that had characteristics of plants and were therefore hypothetically in between animals and plants. The Persian polymath Al-Biruni (d. 1048) classified sponges and corals as animals, arguing that they respond to touch. Nevertheless, people believed corals to be plants until the eighteenth century, when William Herschel used a microscope to establish that coral had the characteristic thin cell membranes of an animal.
The phylogeny of Anthozoans is not clearly understood and a number of different models have been proposed. Within the Hexacorallia, the sea anemones, coral anemones and stony corals may constitute a monophyletic grouping united by their eight-fold symmetry and cnidocyte trait. The Octocorallia appears to be monophyletic, and primitive members of this group may have been stolonate. The cladogram presented here comes from a 2014 study by Stampar et al. which was based on the divergence of mitochondrial DNA within the group and on nuclear markers.
Corals are classified in the class Anthozoa of the phylum Cnidaria. They are divided into three subclasses, Hexacorallia, Octocorallia, and Ceriantharia. The Hexacorallia include the stony corals, the sea anemones and the zoanthids. These groups have polyps that generally have 6-fold symmetry. The Octocorallia include blue coral, soft corals, sea pens, and gorgonians (sea fans and sea whips). These groups have polyps with 8-fold symmetry, each polyp having eight tentacles and eight mesenteries. Ceriantharia are the tube-dwelling anemones.
Fire corals are not true corals, being in the order Anthomedusa (sometimes known as Anthoathecata) of the class Hydrozoa.
Anatomy.
Corals are sessile animals in the class Anthozoa and differ from most other cnidarians in not having a medusa stage in their life cycle. The body unit of the animal is a polyp. Most corals are colonial, the initial polyp budding to produce another and the colony gradually developing from this small start. In stony corals, also known as hard corals, the polyps produce a skeleton composed of calcium carbonate to strengthen and protect the organism. This is deposited by the polyps and by the coenosarc, the living tissue that connects them. The polyps sit in cup-shaped depressions in the skeleton known as corallites. Colonies of stony coral are very variable in appearance; a single species may adopt an encrusting, plate-like, bushy, columnar or massive solid structure, the various forms often being linked to different types of habitat, with variations in light level and water movement being significant.
In soft corals, there is no stony skeleton but the tissues are often toughened by the presence of tiny skeletal elements known as sclerites, which are made from calcium carbonate. Soft corals are very variable in form and most are colonial. A few soft corals are stolonate, but the polyps of most are connected by sheets of coenosarc. In some species this is thick and the polyps are deeply embedded. Some soft corals are encrusting or form lobes. Others are tree-like or whip-like and have a central axial skeleton embedded in the tissue matrix. This is composed either of a fibrous protein called gorgonin or of a calcified material. In both stony and soft corals, the polyps can be retracted, with stony corals relying on their hard skeleton and cnidocytes for defence against predators, with soft corals generally relying on chemical defences in the form of toxic substances present in the tissues known as terpenoids.
The polyps of stony corals have six-fold symmetry while those of soft corals have eight. The mouth of each polyp is surrounded by a ring of tentacles. In stony corals these are cylindrical and taper to a point, but in soft corals they are pinnate with side branches known as pinnules. In some tropical species these are reduced to mere stubs and in some they are fused to give a paddle-like appearance. In most corals, the tentacles are retracted by day and spread out at night to catch plankton and other small organisms. Shallow water species of both stony and soft corals can be zooxanthellate, the corals supplementing their plankton diet with the products of photosynthesis produced by these symbionts. The polyps interconnect by a complex and well-developed system of gastrovascular canals, allowing significant sharing of nutrients and symbionts.
Ecology.
Feeding.
Polyps feed on a variety of small organisms, from microscopic zooplankton to small fish. The polyp's tentacles immobilize or kill prey using their nematocysts. These cells carry venom which they rapidly release in response to contact with another organism. A dormant nematocyst discharges in response to nearby prey touching the trigger (cnidocil). A flap (operculum) opens, and its stinging apparatus fires the barb into the prey. The venom is injected through the hollow filament to immobilise the prey; the tentacles then manoeuvre the prey to the mouth.
The tentacles then contract to bring the prey into the stomach. Once the prey is digested, the stomach reopens, allowing the elimination of waste products and the beginning of the next hunting cycle. They can scavenge drifting organic molecules and dissolved organic molecules.
Intracellular symbionts.
Many corals, as well as other cnidarian groups such as "Aiptasia" (a sea anemone) form a symbiotic relationship with a class of dinoflagellate algae, zooxanthellae of the genus "Symbiodinium". "Aiptasia", a familiar pest among coral reef aquarium hobbyists, serves as a valuable model organism in the study of cnidarian-algal symbiosis. Typically, each polyp harbors one species of algae. Via photosynthesis, these provide energy for the coral, and aid in calcification. As much as 30% of the tissue of a polyp may be plant material.
The algae benefit from a safe place to live and consume the polyp's carbon dioxide and nitrogenous waste. Due to the strain the algae can put on the polyp, stress on the coral often drives them to eject the algae. Mass ejections are known as coral bleaching, because the algae contribute to coral's brown coloration; other colors, however, are due to host coral pigments, such as green fluorescent proteins (GFPs). Ejection increases the polyp's chance of surviving short-term stress—they can regain algae, possibly of a different species at a later time. If the stressful conditions persist, the polyp eventually dies.
Reproduction.
Corals can be both gonochoristic (unisexual) and hermaphroditic, each of which can reproduce sexually and asexually. Reproduction also allows coral to settle in new areas.
Sexual.
Corals predominantly reproduce sexually. About 25% of hermatypic corals (stony corals) form single sex (gonochoristic) colonies, while the rest are hermaphroditic.
Broadcasters.
About 75% of all hermatypic corals "broadcast spawn" by releasing gametes—eggs and sperm—into the water to spread offspring. The gametes fuse during fertilization to form a microscopic larva called a planula, typically pink and elliptical in shape. A typical coral colony forms several thousand larvae per year to overcome the odds against formation of a new colony.
Synchronous spawning is very typical on the coral reef, and often, even when multiple species are present, all corals spawn on the same night. This synchrony is essential so male and female gametes can meet. Corals rely on environmental cues, varying from species to species, to determine the proper time to release gametes into the water. The cues involve temperature change, lunar cycle, day length, and possibly chemical signalling. Synchronous spawning may form hybrids and is perhaps involved in coral speciation. The immediate cue is most often sunset, which cues the release. The spawning event can be visually dramatic, clouding the usually clear water with gametes.
Brooders.
Brooding species are most often ahermatypic (not reef-building) in areas of high current or wave action. Brooders release only sperm, which is negatively buoyant, sinking on to the waiting egg carriers who harbor unfertilized eggs for weeks. Synchronous spawning events sometimes occurs even with these species. After fertilization, the corals release planula that are ready to settle.
Planulae.
Planulae exhibit positive phototaxis, swimming towards light to reach surface waters, where they drift and grow before descending to seek a hard surface to which they can attach and begin a new colony. They also exhibit positive sonotaxis, moving towards sounds that emanate from the reef and away from open water. High failure rates afflict many stages of this process, and even though millions of gametes are released by each colony, few new colonies form. The time from spawning to settling is usually two to three days, but can be up to two months. The larva grows into a polyp and eventually becomes a coral head by asexual budding and growth.
Asexual.
Within a coral head, the genetically identical polyps reproduce asexually, either by budding (gemmation) or by dividing, whether longitudinally or transversely.
Budding involves splitting a smaller polyp from an adult. As the new polyp grows, it forms its body parts. The distance between the new and adult polyps grows, and with it, the coenosarc (the common body of the colony; see coral anatomy). Budding can be intratentacular, from its oral discs, producing same-sized polyps within the ring of tentacles, or extratentacular, from its base, producing a smaller polyp.
Division forms two polyps that each become as large as the original. Longitudinal division begins when a polyp broadens and then divides its coelenteron (body), effectively splitting along its length. The mouth divides and new tentacles form. The two polyps thus created then generate their missing body parts and exoskeleton. Transversal division occurs when polyps and the exoskeleton divide transversally into two parts. This means one has the basal disc (bottom) and the other has the oral disc (top); the new polyps must separately generate the missing pieces.
Asexual reproduction offers the benefits of high reproductive rate, delaying senescence, and replacement of dead modules, as well as geographical distribution.
Colony division.
Whole colonies can reproduce asexually, forming two colonies with the same genotype. The possible mechanisms include fission, bailout and fragmentation. Fission occurs in some corals, especially among the family Fungiidae, where the colony splits into two or more colonies during early developmental stages. Bailout occurs when a single polyp abandons the colony and settles on a different substrate to create a new colony. Fragmentation involves individuals broken from the colony during storms or other disruptions. The separated individuals can start new colonies.
Reefs.
Many corals in the order Scleractinia are hermatypic, meaning that they are involved in building reefs. Most such corals obtain some of their energy from zooxanthellae in the genus "Symbiodinium". These are symbiotic photosynthetic dinoflagellates which require sunlight; reef-forming corals are therefore found mainly in shallow water. They secrete calcium carbonate to form hard skeletons that become the framework of the reef. However, not all reef-building corals in shallow water contain zooxanthellae, and some deep water species, living at depths to which light cannot penetrate, form reefs but do not harbour the symbionts.
There are various types of shallow-water coral reef, including fringing reefs, barrier reefs and atolls; most occur in tropical and subtropical seas. They are very slow-growing, adding perhaps one centimetre (0.4 in) in height each year. The Great Barrier Reef is thought to have been laid down about two million years ago. Over time, corals fragment and die, sand and rubble accumulates between the corals, and the shells of clams and other molluscs decay to form a gradually evolving calcium carbonate structure. Coral reefs are extremely diverse marine ecosystems hosting over 4,000 species of fish, massive numbers of cnidarians, molluscs, crustaceans, and many other animals.
Evolutionary history.
Although corals first appeared in the Cambrian period, some , fossils are extremely rare until the Ordovician period, 100 million years later, when rugose and tabulate corals became widespread. Paleozoic corals often contained numerous endobiotic symbionts.
Tabulate corals occur in limestones and calcareous shales of the Ordovician and Silurian periods, and often form low cushions or branching masses of calcite alongside rugose corals. Their numbers began to decline during the middle of the Silurian period, and they became extinct at the end of the Permian period, .
Rugose or horn corals became dominant by the middle of the Silurian period, and became extinct early in the Triassic period. The rugose corals existed in solitary and colonial forms, and were also composed of calcite.
The scleractinian corals filled the niche vacated by the extinct rugose and tabulate species. Their fossils may be found in small numbers in rocks from the Triassic period, and became common in the Jurassic and later periods. Scleractinian skeletons are composed of a form of calcium carbonate known as aragonite. Although they are geologically younger than the tabulate and rugose corals, the aragonite of their skeletons is less readily preserved, and their fossil record is accordingly less complete.
At certain times in the geological past, corals were very abundant. Like modern corals, these ancestors built reefs, some of which ended as great structures in sedimentary rocks. Fossils of fellow reef-dwellers algae, sponges, and the remains of many echinoids, brachiopods, bivalves, gastropods, and trilobites appear along with coral fossils. This makes some corals useful index fossils. Coral fossils are not restricted to reef remnants, and many solitary fossils may be found elsewhere, such as "Cyclocyathus", which occurs in England's Gault clay formation.
Status.
Threats.
Coral reefs are under stress around the world. In particular, coral mining, agricultural and urban runoff, pollution (organic and inorganic), overfishing, blast fishing, disease, and the digging of canals and access into islands and bays are localized threats to coral ecosystems. Broader threats are sea temperature rise, sea level rise and pH changes from ocean acidification, all associated with greenhouse gas emissions. In 1998, 16% of the world's reefs died as a result of increased water temperature.
Approximately 10% of the world's coral reefs are dead. About 60% of the world's reefs are at risk due to human-related activities. The threat to reef health is particularly strong in Southeast Asia, where 80% of reefs are endangered. Over 50% of the world's coral reefs may be destroyed by 2030; as a result, most nations protect them through environmental laws.
In the Caribbean and tropical Pacific, direct contact between ~40–70% of common seaweeds and coral causes bleaching and death to the coral via transfer of lipid-soluble metabolites. Seaweed and algae proliferate given adequate nutrients and limited grazing by herbivores such as parrotfish.
Water temperature changes of more than 1–2 °C (1.8–3.6 °F) or salinity changes can kill some species of coral. Under such environmental stresses, corals expel their Symbiodinium; without them coral tissues reveal the white of their skeletons, an event known as coral bleaching.
Submarine springs found along the coast of Mexico's Yucatán Peninsula produce water with a naturally low pH (relatively high acidity) providing conditions similar to those expected to become widespread as the oceans absorb carbon dioxide. Surveys discovered multiple species of live coral that appeared to tolerate the acidity. The colonies were small and patchily distributed, and had not formed structurally complex reefs such as those that compose the nearby Mesoamerican Barrier Reef System.
Protection.
Marine Protected Areas (MPAs), Biosphere reserves, marine parks, national monuments world heritage status, fishery management and habitat protection can protect reefs from anthropogenic damage.
Many governments now prohibit removal of coral from reefs, and inform coastal residents about reef protection and ecology. While local action such as habitat restoration and herbivore protection can reduce local damage, the longer-term threats of acidification, temperature change and sea-level rise remain a challenge.
To eliminate destruction of corals in their indigenous regions, projects have been started to grow corals in non-tropical countries.
Relation to humans.
Local economies near major coral reefs benefit from an abundance of fish and other marine creatures as a food source. Reefs also provide recreational scuba diving and snorkeling tourism. These activities can damage coral but international projects such as Green Fins that encourage dive and snorkel centres to follow a Code of Conduct have been proven to mitigate these risks.
Live coral is highly sought after for aquaria. Soft corals are easier to maintain in captivity than hard corals.
Jewelry.
Corals' many colors give it appeal for necklaces and other jewelry. Intensely red coral is prized as a gemstone. Sometimes called fire coral, it is not the same as fire coral. Red coral is very rare because of overharvesting.
Medicine.
In medicine, chemical compounds from corals are used for cancer, AIDS, pain, and other uses. Coral skeletons, e.g. "Isididae" are also used for bone grafting in humans.
Coral Calx, known as Praval Bhasma in Sanskrit, is widely used in traditional system of Indian medicine as a supplement in the treatment of a variety of bone metabolic disorders associated with calcium deficiency.
Construction.
Coral reefs in places such as the East African coast are used as a source of building material. Ancient (fossil) coral limestone, notably including the Coral Rag Formation of the hills around Oxford (England), was once used as a building stone, and can be seen in some of the oldest buildings in that city including the Saxon tower of St Michael at the Northgate, St. George's Tower of Oxford Castle, and the mediaeval walls of the city.
Climate research.
Annual growth bands in some corals, such as the deep sea bamboo corals ("Isididae"), may be among the first signs of the effects of ocean acidification on marine life. The growth rings allow geologists to construct year-by-year chronologies, a form of incremental dating, which underlie high-resolution records of past climatic and environmental changes using geochemical techniques.
Certain species form communities called microatolls, which are colonies whose top is dead and mostly above the water line, but whose perimeter is mostly submerged and alive. Average tide level limits their height. By analyzing the various growth morphologies, microatolls offer a low resolution record of sea level change. Fossilized microatolls can also be dated using Radiocarbon dating. Such methods can help to reconstruct Holocene sea levels.
Increasing sea temperatures in tropical regions (~1 degree C) the last century have caused major coral bleaching, death, and therefore shrinking coral populations since although they are able to adapt and acclimate, it is uncertain if this evolutionary process will happen quickly enough to prevent major reduction of their numbers.
Though coral have large sexually-reproducing populations, their evolution can be slowed by abundant asexual reproduction. Gene flow is variable among coral species. According to the biogeography of coral species gene flow cannot be counted on as a dependable source of adaptation as they are very stationary organisms. Also, coral longevity might factor into their adaptivity.
However, adaptation to climate changes has been demonstrated in many cases. These are usually due to a shift in coral and zooxanthellae genotypes. These shifts in allelic frequencies have progressed toward more tolerant types of zooxanthellae. Scientists found that a certain scleractinian zooxanthella is becoming more common where sea temperature is high. Symbionts able to tolerate warmer water seem to photosynthesise more slowly, implying an evolutionary trade-off.
In the Gulf of Mexico, where sea temperatures are rising, cold-sensitive staghorn and elkhorn coral have shifted in location.
Not only have the symbionts and specific species been shown to shift, but there seems to be a certain growth rate favorable to selection. Slower-growing but more heat-tolerant corals have become more common. The changes in temperature and acclimation are complex. Some reefs in current shadows represent a refugium location that will help them adjust to the disparity in the environment even if eventually the temperatures may rise more quickly there than in other locations. This separation of populations by climatic barriers causes a realized niche to shrink greatly in comparison to the old fundamental niche.
Geochemistry.
Corals are shallow, colonial organisms that integrate δ18O and trace elements into their skeletal aragonite (polymorph of calcite) crystalline structures, as they grow. Geochemistry anomalies within the crystalline structures of corals represent functions of temperature, salinity and oxygen isotopic composition. Such geochemical analysis can help with climate modeling.
Strontium/calcium ratio anomaly.
Time can be attributed to coral geochemistry anomalies by correlating strontium/calcium minimums with sea surface temperature (SST) maximums to data collected from NINO 3.4 SSTA.
Oxygen isotope anomaly.
The comparison of coral strontium/calcium minimums with sea surface temperature maximums, data recorded from NINO 3.4 SSTA, time can be correlated to coral strontium/calcium and δ18O variations. To confirm accuracy of the annual relationship between Sr/Ca and δ18O variations, a perceptible association to annual coral growth rings confirms the age conversion. Geochronology is established by the blending of Sr/Ca data, growth rings, and stable isotope data. El Nino-Southern Oscillation (ENSO) is directly related to climate fluctuations that influence coral δ18O ratio from local salinity variations associated with the position of the South Pacific convergence zone (SPCZ) and can be used for ENSO modeling.
Sea surface temperature and sea surface salinity.
The global moisture budget is primarily being influenced by tropical sea surface temperatures from the position of the Intertropical Convergence Zone (ITCZ). The Southern Hemisphere has a unique meteorological feature positioned in the southwestern Pacific Basin called the South Pacific Convergence Zone (SPCZ), which contains a perennial position within the Southern Hemisphere. During ENSO warm periods, the SPCZ reverses orientation extending from the equator down south through Solomon Islands, Vanuatu, Fiji and towards the French Polynesian Islands; and due east towards South America affecting geochemistry of corals in tropical regions.
Geochemical analysis of skeletal coral can be linked to sea surface salinity (SSS) and sea surface temperature (SST), from El Nino 3.4 SSTA data, of tropical oceans to seawater δ18O ratio anomalies from corals. ENSO phenomenon can be related to variations in sea surface salinity (SSS) and sea surface temperature (SST) that can help model tropical climate activities.
Limited climate research on current species.
Climate research on live coral species is limited to a few studied species. Studying "Porites" coral provides a stable foundation for geochemical interpretations that is much simpler to physically extract data in comparison to "Platygyra" species where the complexity of "Platygyra" species skeletal structure creates difficulty when physically sampled, which happens to be one of the only multidecadal living coral records used for coral paleoclimate modeling.
Aquaria.
The saltwater fishkeeping hobby has increasingly expanded, over recent years, to include reef tanks, fish tanks that include large amounts of live rock on which coral is allowed to grow and spread. These tanks are either kept in a natural-like state, with algae (sometimes in the form of an algae scrubber) and a deep sand bed providing filtration, or as "show tanks", with the rock kept largely bare of the algae and microfauna that would normally populate it, in order to appear neat and clean.
The most popular kind of coral kept is soft coral, especially zoanthids and mushroom corals, which are especially easy to grow and propagate in a wide variety of conditions, because they originate in enclosed parts of reefs where water conditions vary and lighting may be less reliable and direct. More serious fishkeepers may keep small polyp stony coral, which is from open, brightly lit reef conditions and therefore much more demanding, while large polyp stony coral is a sort of compromise between the two.
Aquaculture.
Coral aquaculture, also known as "coral farming" or "coral gardening", is the cultivation of corals for commercial purposes or coral reef restoration. Aquaculture is showing promise as a potentially effective tool for restoring coral reefs, which have been declining around the world. The process bypasses the early growth stages of corals when they are most at risk of dying. Coral fragments known as "seeds" are grown in nurseries then replanted on the reef. Coral is farmed by coral farmers who live locally to the reefs and farm for reef conservation or for income. It is also farmed by scientists for research, by businesses for the supply of the live and ornamental coral trade and by private aquarium hobbyists.
Gallery.
"Further images: and "

</doc>
