<doc id="41097" url="https://en.wikipedia.org/wiki?curid=41097" title="Nuclear electromagnetic pulse">
Nuclear electromagnetic pulse

An electromagnetic pulse (commonly abbreviated as EMP, pronounced ) is a burst of electromagnetic radiation. Nuclear explosions create a characteristic pulse of electromagnetic radiation called a nuclear EMP or NEMP.
The resulting rapidly changing electric and magnetic fields may couple with electrical and electronic systems to produce damaging current and voltage surges. The specific characteristics of any particular nuclear EMP event vary according to a number of factors, the greatest of which is the altitude of the detonation.
The term "electromagnetic pulse" generally excludes optical (infrared, visible, ultraviolet) and ionizing (such as X-ray and gamma radiation) ranges.
In military terminology, a nuclear warhead detonated hundreds of kilometers above the Earth's surface is known as a high-altitude electromagnetic pulse (HEMP) device. Effects of a HEMP device depend on factors including the altitude of the detonation, energy yield, gamma ray output, interactions with the Earth's magnetic field and electromagnetic shielding of targets.
History.
The fact that an electromagnetic pulse is produced by a nuclear explosion was known in the earliest days of nuclear weapons testing. The magnitude of the EMP and the significance of its effects, however, were not immediately realized.
During the first United States nuclear test on 16 July 1945, electronic equipment was shielded due to Enrico Fermi's expectation of the electromagnetic pulse. The official technical history for that first nuclear test states, "All signal lines were completely shielded, in many cases doubly shielded. In spite of this many records were lost because of spurious pickup at the time of the explosion that paralyzed the recording equipment."  During British nuclear testing in 1952–1953 instrumentation failures were attributed to "radioflash", which was their term for EMP.
The first openly reported observation of the unique aspects of high-altitude nuclear EMP occurred during the helium balloon lofted Yucca nuclear test of the Hardtack I series on 28 April 1958. In that test, the electric field measurements from the 1.7 kiloton weapon went off the scale of the test instruments and was estimated to be about 5 times the oscilloscope limits. The Yucca EMP was initially positive-going whereas low-altitude bursts were negative pulses. Also, the polarization of the Yucca EMP signal was horizontal, whereas low-altitude nuclear EMP was vertically polarized. In spite of these many differences, the unique EMP results were dismissed as a possible wave propagation anomaly.
The high-altitude nuclear tests of 1962, as discussed below, confirmed the unique results of the Yucca high-altitude test and increased the awareness of high-altitude nuclear EMP beyond the original group of defense scientists.
The larger scientific community became aware of the significance of the EMP problem after a three-article series on nuclear EMP was published in 1981 by William J. Broad in "Science".
Starfish Prime.
In July 1962, a 1.44 megaton United States nuclear test in space, above the mid-Pacific Ocean, called the Starfish Prime test, demonstrated to nuclear scientists that the magnitude and effects of a high-altitude nuclear explosion were much larger than had been previously calculated. Starfish Prime made those effects known to the public by causing electrical damage in Hawaii, about away from the detonation point, knocking out about 300 streetlights, setting off numerous burglar alarms and damaging a microwave link.
Starfish Prime was the first success in the series of United States high-altitude nuclear tests in 1962 known as Operation Fishbowl. Subsequent tests gathered more data on the high-altitude EMP phenomenon.
The Bluegill Triple Prime and Kingfish high-altitude nuclear tests of October and November 1962 in Operation Fishbowl provided data that was clear enough to enable physicists to accurately identify the physical mechanisms behind the electromagnetic pulses.
The EMP damage of the Starfish Prime test was quickly repaired because of the ruggedness (compared to today) of Hawaii's electrical and electronic infrastructure.
The relatively small magnitude of the Starfish Prime EMP in Hawaii (about 5.6 kilovolts/metre) and the relatively small amount of damage (for example, only 1 to 3 percent of streetlights extinguished) led some scientists to believe, in the early days of EMP research, that the problem might not be significant. Newer calculations showed that if the Starfish Prime warhead had been detonated over the northern continental United States, the magnitude of the EMP would have been much larger (22 to 30 kv/m) because of the greater strength of the Earth's magnetic field over the United States, as well as its different orientation at high latitudes. These calculations, combined with the accelerating reliance on EMP-sensitive microelectronics, heightened awareness that EMP could be a significant problem.
Soviet Test 184.
In 1962, the Soviet Union also performed three EMP-producing nuclear tests in space over Kazakhstan, the last in the "Soviet Project K nuclear tests". Although these weapons were much smaller (300 kiloton) than the Starfish Prime test, they were over a populated, large land mass and at a location where the Earth's magnetic field was greater, the damage caused by the resulting EMP was reportedly much greater than in Starfish Prime. The geomagnetic storm–like E3 pulse from Test 184 induced a current surge in a long underground power line that caused a fire in the power plant in the city of Karaganda.
After the collapse of the Soviet Union, the level of this damage was communicated informally to U.S. scientists. After the 1991 collapse of the Soviet Union, there was a period of a few years of cooperation between United States and Russian scientists on the HEMP phenomenon. In addition, funding was secured to enable Russian scientists to formally report on some of the Soviet EMP results in international scientific journals. As a result, formal documentation of some of the EMP damage in Kazakhstan exists but is still sparse in the open scientific literature, especially in relation to the level of damage that was indicated in the open reports.
For one of the K Project tests, Soviet scientists instrumented a section of telephone line in the area that they expected to be affected by the pulse. The monitored telephone line was divided into sub-lines of in length, separated by repeaters. Each sub-line was protected by fuses and by gas-filled overvoltage protectors. The EMP from the 22 October (K-3) nuclear test (also known as Test 184) blew all of the fuses and fired all of the overvoltage protectors in all of the sub-lines.
Published reports, including a 1998 IEEE article, have stated that there were significant problems with ceramic insulators on overhead electrical power lines during the tests. A 2010 technical report written for Oak Ridge National Laboratory stated that "Power line insulators were damaged, resulting in a short circuit on the line and some lines detaching from the poles and falling to the ground."
Characteristics of nuclear EMP.
Nuclear EMP is a complex multi-pulse, usually described in terms of three components, as defined by the International Electrotechnical Commission (IEC).
The three components of nuclear EMP, as defined by the IEC, are called "E1", "E2" and "E3".
E1.
The E1 pulse is the very fast component of nuclear EMP. E1 is a very brief but intense electromagnetic field that induces very high voltages in electrical conductors. E1 causes most of its damage by causing electrical breakdown voltages to be exceeded. E1 can destroy computers and communications equipment and it changes too quickly for ordinary surge protectors to provide effective protection against it, although there are special fast-acting surge protectors that will block the E1 pulse.
E1 is produced when gamma radiation from the nuclear detonation ionizes (strips electrons from) atoms in the upper atmosphere. This is known as the Compton effect and the resulting current is called the "Compton current". The electrons travel in a generally downward direction at relativistic speeds (more than 90 percent of the speed of light). In the absence of a magnetic field, this would produce a large, radial pulse of electric current propagating outward from the burst location confined to the source region (the region over which the gamma photons are attenuated). The Earth's magnetic field deflects the electron flow at a right angle to the field, leading to synchrotron radiation emitted by the electrons. Because the outward traveling gamma pulse is propagating at the speed of light, the synchrotron radiation of the Compton electrons adds coherently, leading to a radiated electromagnetic signal. This interaction produces a very large, but very brief, electromagnetic pulse over the affected area.
Several physicists worked on the problem of identifying the mechanism of the uniquely large E1 pulse produced by a nuclear weapon detonated at high altitude (HEMP). The correct mechanism was finally identified by Conrad Longmire of Los Alamos National Laboratory in 1963.
Conrad Longmire gives numerical values for a typical case of E1 pulse produced by a second-generation nuclear weapon such as those of Operation Fishbowl in 1962. The typical gamma rays given off by the weapon have an energy of about 2 MeV (mega-electron volts). The gamma rays transfer about half of their energy to the ejected free electrons, giving an energy of about 1 MeV.
In a vacuum and absent a magnetic field, the electrons would travel with a current density of tens of amperes per square metre. Because of the downward tilt of the Earth's magnetic field at high latitudes, the area of peak field strength is a U-shaped region to the equatorial side of the nuclear detonation. As shown in the diagram at the right, for nuclear detonations over the continental United States, this U-shaped region is south of the detonation point. Near the equator, where the Earth's magnetic field is more nearly horizontal, the E1 field strength is more nearly symmetrical around the burst location.
At geomagnetic field strengths typical of the central United States, central Europe or Australia, these initial electrons spiral around the magnetic field lines with a typical radius of about 85 metres (about 280 feet). These initial electrons are stopped by collisions with other air molecules at an average distance of about 170 metres (a little less than 580 feet). This means that most of the electrons are stopped by collisions with air molecules before completing a full spiral around the field lines.
This interaction of the very rapidly moving negatively charged electrons with the magnetic field radiates a pulse of electromagnetic energy. The pulse typically rises to its peak value in some 5 nanoseconds. Its magnitude typically decays to half of its peak value within 200 nanoseconds. (By the IEC definition, this E1 pulse ends 1000 nanoseconds after it begins.) This process occurs simultaneously on about 1025 electrons.  The simultaneous action of the very large number of electrons causes the resulting electromagnetic pulses from each electron to radiate coherently, thus adding to produce a single very large amplitude, but very narrow, radiated electromagnetic pulse.
Secondary collisions cause subsequent electrons to lose energy before they reach ground level. The electrons generated by these subsequent collisions have such reduced energy that they do not contribute significantly to the E1 pulse.
These 2 MeV gamma rays typically produce an E1 pulse near ground level at moderately high latitudes that peaks at about 50,000 volts per metre. This is a peak power density of 6.6 megawatts per square metre.
The ionization process in the mid-stratosphere causes this region to become an electrical conductor, a process that blocks the production of further electromagnetic signals and causes the field strength to saturate at about 50,000 volts per metre. The strength of the E1 pulse depends upon the number and intensity of the gamma rays and upon the rapidity of the gamma ray burst. Strength is also somewhat dependent upon altitude.
There are reports of "super-EMP" nuclear weapons that are able to exceed the 50,000 volt per metre limit by the nearly instantaneous release of a burst of much higher gamma radiation levels than are known to be produced by second-generation nuclear weapons. The reality and possible construction details of these weapons are classified and unconfirmed in the open scientific literature.
E2.
The E2 component is generated by scattered gamma rays and inelastic gammas produced by neutrons. This E2 component is an "intermediate time" pulse that, by the IEC definition, lasts from about 1 microsecond to 1 second after the explosion. E2 has many similarities to lightning, although lightning-induced E2 may be considerably larger than a nuclear E2. Because of the similarities and the widespread use of lightning protection technology, E2 is generally considered to be the easiest to protect against.
According to the United States EMP Commission, the main problem with E2 is the fact that it immediately follows E1, which may have damaged the devices that would normally protect against E2.
The EMP Commission Executive Report of 2004 states, "In general, it would not be an issue for critical infrastructure systems since they have existing protective measures for defense against occasional lightning strikes. The most significant risk is synergistic, because the E2 component follows a small fraction of a second after the first component's insult, which has the ability to impair or destroy many protective and control features. The energy associated with the second component thus may be allowed to pass into and damage systems."
E3.
The E3 component is very different from E1 and E2. E3 is a very slow pulse, lasting tens to hundreds of seconds. It is caused by the nuclear detonation's temporary distortion of the Earth's magnetic field. The E3 component has similarities to a geomagnetic storm caused by a solar flare.
Because of the similarity between solar-induced geomagnetic storms and nuclear E3, it has become common to refer to solar-induced geomagnetic storms as "solar EMP." "Solar EMP", however, does not include an E1 or E2 component.
Generation.
Factors that control weapon effectiveness include altitude, yield, construction details, target distance, intervening geographical features, and local strength of the Earth's magnetic field.
Weapon altitude.
According to an internet primer published by the Federation of American Scientists
Thus, for equipment to be affected, the weapon needs to be above the visual horizon.
The altitude indicated above is greater than that of the International Space Station and many low Earth orbit satellites. Large weapons could have a dramatic impact on satellite operations and communications such as occurred during Operation Fishbowl. The damaging effects on orbiting satellites are usually due to factors other than EMP. In the Starfish Prime nuclear test, most damage was to the satellites' solar panels while passing through radiation belts created by the explosion.
For detonations within the atmosphere, the situation is more complex. Within the range of gamma ray deposition, simple laws no longer hold as the air is ionised and there are other EMP effects, such as a radial electric field due to the separation of Compton electrons from air molecules, together with other complex phenomena. For a surface burst, absorption of gamma rays by air would limit the range of gamma ray deposition to approximately 10 miles, while for a burst in the lower-density air at high altitudes, the range of deposition would be far greater.
Weapon yield.
Typical nuclear weapon yields used during Cold War planning for EMP attacks were in the range of 1 to 10 megatons This is roughly 50 to 500 times the size of the Hiroshima and Nagasaki bombs. Physicists have testified at United States Congressional hearings that weapons with yields of 10 kilotons or less can produce a large EMP.
The EMP at a fixed distance from an explosion increases at most as the square root of the yield (see the illustration to the right). This means that although a 10 kiloton weapon has only 0.7% of the energy release of the 1.44-megaton Starfish Prime test, the EMP will be at least 8% as powerful. Since the E1 component of nuclear EMP depends on the prompt gamma ray output, which was only 0.1% of yield in Starfish Prime but can be 0.5% of yield in low yield pure nuclear fission weapons, a 10 kiloton bomb can easily be 5 x 8% = 40% as powerful as the 1.44 megaton Starfish Prime at producing EMP.
The total prompt gamma ray energy in a fission explosion is 3.5% of the yield, but in a 10 kiloton detonation the triggering explosive around the bomb core absorbs about 85% of the prompt gamma rays, so the output is only about 0.5% of the yield. In the thermonuclear Starfish Prime the fission yield was less than 100% and the thicker outer casing absorbed about 95% of the prompt gamma rays from the pusher around the fusion stage. Thermonuclear weapons are also less efficient at producing EMP because the first stage can pre-ionize the air which becomes conductive and hence rapidly shorts out the Compton currents generated by the fusion stage. Hence, small pure fission weapons with thin cases are far more efficient at causing EMP than most megaton bombs.
This analysis, however, only applies to the fast E1 and E2 components of nuclear EMP. The geomagnetic storm-like E3 component of nuclear EMP is more closely proportional to the total energy yield of the weapon.
Target distance.
In nuclear EMP all of the components of the electromagnetic pulse are generated outside of the weapon.
For high-altitude nuclear explosions, much of the EMP is generated far from the detonation (where the gamma radiation from the explosion hits the upper atmosphere). This electric field from the EMP is remarkably uniform over the large area affected.
According to the standard reference text on nuclear weapons effects published by the U.S. Department of Defense, "The peak electric field (and its amplitude) at the Earth's surface from a high-altitude burst will depend upon the explosion yield, the height of the burst, the location of the observer, and the orientation with respect to the geomagnetic field. As a general rule, however, the field strength may be expected to be tens of kilovolts per metre over most of the area receiving the EMP radiation."
The text also states that, "... over most of the area affected by the EMP the electric field strength on the ground would exceed 0.5"E"max. For yields of less than a few hundred kilotons, this would not necessarily be true because the field strength at the Earth's tangent could be substantially less than 0.5"E"max."
In other words, the electric field strength in the entire area that is affected by the EMP will be fairly uniform for weapons with a large gamma ray output. For smaller weapons, the electric field may fall at a faster rate as distance increases.
Effects.
On aircraft.
Many nuclear detonations have taken place using bombs. The B-29 aircraft that delivered the nuclear weapons at Hiroshima and Nagasaki did not lose power due to electrical damage, because electrons (ejected from the air by gamma rays) are stopped quickly in normal air for bursts below roughly , so they are not significantly deflected by the Earth's magnetic field.
If the aircraft carrying the Hiroshima and Nagasaki bombs had been within the intense nuclear radiation zone when the bombs exploded over those cities, then they would have suffered effects from the charge separation (radial) EMP. But this only occurs within the severe blast radius for detonations below about 10 km altitude.
During Operation Fishbowl, EMP disruptions were suffered aboard a KC-135 photographic aircraft flying from the detonations at burst altitudes. The vital electronics were less sophisticated than today's and the aircraft was able to land safely.
Vacuum tube versus solid state electronics.
Older, vacuum tube (valve) based equipment is generally much less vulnerable to nuclear EMP than newer solid state equipment. Soviet Cold War–era military aircraft often had avionics based on vacuum tubes due to limited solid-state capabilities and a belief that the vacuum-tube gear would be more likely to survive.
Other components in vacuum tube circuitry can be damaged by EMP. Vacuum tube equipment was damaged in the 1962 testing. The solid state PRC-77 VHF manpackable 2-way radio survived extensive EMP testing. The earlier PRC-25, nearly identical except for a vacuum tube final amplification stage, was tested in EMP simulators, but was not certified to remain fully functional.
Post–Cold War attack scenarios.
The United States military services developed, and in some cases published, hypothetical EMP attack scenarios.
The United States EMP Commission was created by the United States Congress in 2001. The commission is formally known as the Commission to Assess the Threat to the United States from Electromagnetic Pulse (EMP) Attack.
The Commission brought together notable scientists and technologists to compile several reports. In 2008, the EMP Commission released the "Critical National Infrastructures Report". This report describes the likely consequences of a nuclear EMP on civilian infrastructure. Although this report covered the United States, most of the information can be generalized to other industrialized countries. The 2008 report was a followup to a more generalized report issued by the commission in 2004.
In written testimony delivered to the United States Senate in 2005, an EMP Commission staff member reported:
The United States EMP Commission determined that long-known protections are almost completely absent in the civilian infrastructure of the United States and that large parts of US military services were less-protected against EMP than during the Cold War. In public statements, the EMP experts on the EMP Commission recommended making electronic equipment and electrical components resistant to EMP — and maintaining spare parts inventories that would enable prompt repairs. The United States EMP Commission did not look at the civilian infrastructures of other nations.
In 2011 the Defense Science Board published a report about the ongoing efforts to defend critical military and civilian systems against EMP and other nuclear weapons effects.
Common misconceptions.
A 2010 technical report written for the US government's Oak Ridge National Laboratory included a brief section addressing common EMP myths. The remainder of this section is a direct quotation from that Oak Ridge report regarding common HEMP Myths:
Protecting infrastructure.
In 2013, the US House of Representatives considered the "Secure High-voltage Infrastructure for Electricity from Lethal Damage Act" that would provide surge protection for some 300 large transformers around the country.
The problem of protecting civilian infrastructure from electromagnetic pulse has also been intensively studied throughout the European Union, and in particular by the United Kingdom.
In fiction and popular culture.
Especially since the 1980s, Nuclear EMP weapons have gained a significant presence in fiction and popular culture.
The popular media often depict EMP effects incorrectly, causing misunderstandings among the public and even professionals, and official efforts have been made in the United States to set the record straight. See, for example, the Oak Ridge quotation in the above section of this article on "Common Misconceptions." Also, the United States Space Command commissioned science educator Bill Nye to produce a video called "Hollywood vs. EMP" so that Hollywood fiction would not confuse those who must deal with real EMP events. The U.S. Space Command video is not available to the general public.

</doc>
<doc id="41098" url="https://en.wikipedia.org/wiki?curid=41098" title="Electromagnetic radiation and health">
Electromagnetic radiation and health

Electromagnetic radiation can be classified into two types: ionizing radiation and non-ionizing radiation, based on its capability of ionizing atoms and breaking chemical bonds. Ultraviolet and higher frequencies, such as X-rays or gamma rays are ionizing, and these pose their own special hazards: see "radiation" and "radiation poisoning". The electric currents that flow through power sockets have associated line-frequency electromagnetic fields. Various kinds of higher-frequency radiowaves are used to transmit information – whether via TV antennas, radio stations or mobile phone base stations.
By far the most common health hazard of radiation is sunburn, which causes over one million new skin cancers annually.
Types of hazards.
Electrical hazards.
Very strong radiation can induce current capable of delivering an electric shock to persons or animals. It can also overload and destroy electrical equipment. The induction of currents by oscillating magnetic fields is also the way in which solar storms disrupt the operation of electrical and electronic systems, causing damage to and even the explosion of power distribution transformers, blackouts (as occurred in 1989), and interference with electromagnetic signals ("e.g." radio, TV, and telephone signals). 
Fire hazards.
Extremely high power electromagnetic radiation can cause electric currents strong enough to create sparks (electrical arcs) when an induced voltage exceeds the breakdown voltage of the surrounding medium ("e.g." air at 3.0 MV/m). These sparks can then ignite flammable materials or gases, possibly leading to an explosion.
This can be a particular hazard in the vicinity of explosives or pyrotechnics, since an electrical overload might ignite them. This risk is commonly referred to as Hazards of Electromagnetic Radiation to Ordnance (HERO) by the United States Navy (USN). United States Military Standard 464A (MIL-STD-464A) mandates assessment of HERO in a system, but USN document OD 30393 provides design principles and practices for controlling electromagnetic hazards to ordnance.
On the other hand, the risk related to fueling is known as Hazards of Electromagnetic Radiation to Fuel (HERF). NAVSEA OP 3565 Vol. 1 could be used to evaluate HERF, which states a maximum power density of 0.09 W/m² for frequencies under 225 MHz (i.e. 4.2 meters for a 40 W emitter).
Biological hazards.
The best understood biological effect of electromagnetic fields is to cause dielectric heating. For example, touching or standing around an antenna while a high-power transmitter is in operation can cause severe burns. These are exactly the kind of burns that would be caused inside a microwave oven.
This heating effect varies with the power and the frequency of the electromagnetic energy. A measure of the heating effect is the specific absorption rate or SAR, which has units of watts per kilogram (W/kg). The IEEE and many national governments have established safety limits for exposure to various frequencies of electromagnetic energy based on SAR, mainly based on ICNIRP Guidelines, which guard against thermal damage.
There are publications which support the existence of complex biological effects of weaker "non-thermal" electromagnetic fields (see Bioelectromagnetics), including weak ELF magnetic fields and modulated RF and microwave fields. Fundamental mechanisms of the interaction between biological material and electromagnetic fields at non-thermal levels are not fully understood.
A 2009 study at the University of Basel in Switzerland found that intermittent (but not continuous) exposure of human cells to a 50 Hz electromagnetic field at a flux density of 1 mT (or 10 G) induced a slight but significant increase of DNA fragmentation in the Comet assay. However that level of exposure is already above current established safety exposure limits.
Lighting.
Fluorescent lights.
Fluorescent light bulbs and tubes internally produce ultraviolet light. Normally this is converted to visible light by the phosphor film inside a protective coating. When the film is cracked by mishandling or faulty manufacturing then UV may escape at levels that could cause sunburn or even skin cancer.
LED lights.
Blue light, emitting at wavelengths of 400–500 nanometers, suppresses the production of melatonin produced by the pineal gland. The effect is disruption of a human being's biological clock resulting in poor sleeping and rest periods.
EMR effects on the human body by frequency.
While the most acute exposures to harmful levels of electromagnetic radiation are immediately realized as burns, the health effects due to chronic or occupational exposure may not manifest effects for months or years.
Extremely-low-frequency RF.
High-power extremely-low-frequency RF with electric field levels in the low kV/m range are known to induce perceivable currents within the human body that create an annoying tingling sensation. These currents will typically flow to ground through a body contact surface such as the feet, or arc to ground where the body is well insulated.
Shortwave frequency RF.
Shortwave diathermy heating of human tissue only heats tissues that are good electrical conductors, such as blood vessels and muscle. Adipose tissue (fat) receives little heating by induction fields because an electrical current is not actually going through the tissues.
Radio frequency fields.
Apart from some suspicion that the electromagnetic fields emitted by mobile phones may be responsible for an increased risk of glioma and acoustic neuroma, the fields otherwise pose no risk to human health. This designation of mobile phone signals as "possibly carcinogenic" by the World Health Organization (WHO) (e.g. it's IARC, see below) has often been misinterpreted as indicating that of some measure of risk has been observed however the designation indicates only that the possibility could not be conclusively ruled out using the available data.
In 2011, International Agency for Research on Cancer (IARC) classified mobile phone radiation as Group 2B possibly carcinogenic ("not" Group 2A probably carcinogenic nor the dangerous Group 1). That means that there "could be some risk" of carcinogenicity, so additional research into the long-term, heavy use of mobile phones needs to be conducted. The WHO added in June 2011 that "to date, no adverse health effects have been established as being caused by mobile phone use", a point they reiterated in October 2014.
Microwaves.
Microwave exposure at low-power levels below the specific absorption rate set by government regulatory bodies is considered harmless non-ionizing radiation and has no effect on the human body. Levels above the specific absorption rate set by the US Federal Communications Commission (FCC) are those they considered to be potentially harmful. ANSI standards for safe exposure levels to RF and microwave radiation are set to a SAR level of 4 W/kg, the threshold before hazardous thermical effects occur due to energy absorption in the body. A safety factor of ten was then incorporated to arrive at the final recommended protection guidelines of a SAR exposure threshold of 0.4 W/kg for RF and microwave radiation. There is disagreement over exactly what levels of RF radiation are safe, particularly with regard to low levels of exposure. Russia and eastern European countries set SAR thresholds for microwaves and RF much lower than western countries.
Two areas of the body, the eyes and the testes, can be particularly susceptible to heating by RF energy because of the relative lack of available blood flow to dissipate the excessive heat load. Laboratory experiments have shown that short-term exposure to high levels of RF radiation (100–200 mW/cm²) can cause cataracts in rabbits. Temporary sterility, caused by such effects as changes in sperm count and in sperm motility, is possible after exposure of the testes to high-level RF radiation.
Long-term exposure to high-levels of microwaves, is recognized, from experimental animal studies and epidemiological studies in humans, to cause cataracts. The mechanism is unclear but may include changes in heat sensitive enzymes that normally protect cell proteins in the lens. Another mechanism that has been advanced is direct damage to the lens from pressure waves induced in the aqueous humor.
Exposure to sufficiently high-power microwave RF is known to create effects ranging from a burning sensation on the skin and microwave auditory effect, to extreme pain at the mid-range, to physical microwave burns and blistering of skin and internals at high power levels.
Millimeter waves.
Recent technology advances in the developments of millimeter wave scanners for airport security and WiGig for Personal area networks have opened the 60 GHz and above microwave band to SAR exposure regulations. Previously, microwave applications in these bands were for point-to-point satellite communication with minimal human exposure. Radiation levels in the millimeter wavelength represent the high microwave band or close to Infrared wavelengths.
Infrared.
Infrared wavelengths longer than 750 nm can produce changes in the lens of the eye. Glassblower's cataract is an example of a heat injury that damages the anterior lens capsule among unprotected glass and iron workers. Cataract-like changes can occur in workers who observe glowing masses of glass or iron without protective eyewear for many hours a day.
Another important factor is the distance between the worker and the source of radiation. In the case of arc welding, infrared radiation decreases rapidly as a function of distance, so that farther than three feet away from where welding takes place, it does not pose an ocular hazard anymore but, ultraviolet radiation still does. This is why welders wear tinted glasses and surrounding workers only have to wear clear ones that filter UV.
Visible light.
Moderate and high-power lasers are potentially hazardous because they can burn the retina of the eye, or even the skin. To control the risk of injury, various specifications – for example ANSI Z136 in the US, and IEC 60825 internationally – define "classes" of lasers depending on their power and wavelength. These regulations also prescribe required safety measures, such as labeling lasers with specific warnings, and wearing laser safety goggles during operation (see laser safety).
As with its infrared and ultraviolet radiation dangers, welding creates an intense brightness in the visible light spectrum, which may cause temporary flash blindness. Some sources state that there is no minimum safe distance for exposure to these radiation emissions without adequate eye protection.
Ultraviolet.
Short-term exposure to strong ultraviolet sunlight causes sunburn within hours of exposure.
Ultraviolet light, specifically UV-B, has been shown to cause cataracts and there is some evidence that sunglasses worn at an early age can slow its development in later life. Most UV light from the sun is filtered out by the atmosphere and consequently airline pilots often have high rates of cataracts because of the increased levels of UV radiation in the upper atmosphere. It is hypothesized that depletion of the ozone layer and a consequent increase in levels of UV light on the ground may increase future rates of cataracts. Note that the lens filters UV light, so once that is removed via surgery, one may be able to see UV light.
Prolonged exposure to ultraviolet radiation from the sun can lead to melanoma and other skin malignancies. Clear evidence establishes ultraviolet radiation, especially the non-ionizing medium wave UVB, as the cause of most non-melanoma skin cancers, which are the most common forms of cancer in the world. UV rays can also cause wrinkles, liver spots, moles, and freckles. In addition to sunlight, other sources include tanning beds, and bright desk lights. Damage is cumulative over one's lifetime, so that permanent effects may not be evident for some time after exposure.
Ultraviolet radiation of wavelengths shorter than 300 nm (actinic rays) can damage the corneal epithelium. This is most commonly the result of exposure to the sun at high altitude, and in areas where shorter wavelengths are readily reflected from bright surfaces, such as snow, water, and sand. UV generated by a welding arc can similarly cause damage to the cornea, known as "arc eye" or welding flash burn, a form of photokeratitis.

</doc>
<doc id="41099" url="https://en.wikipedia.org/wiki?curid=41099" title="Electromagnetic survivability">
Electromagnetic survivability

In telecommunication, electromagnetic survivability is the ability of a system, subsystem, or equipment to resume functioning without evidence of degradation following temporary exposure to an adverse electromagnetic environment. 
The system, subsystem, or equipment performance may be degraded during exposure to the adverse electromagnetic environment, but the system will not experience permanent damage, such as component burnout, that will prevent proper operation when the adverse electromagnetic environment is removed.

</doc>
<doc id="41100" url="https://en.wikipedia.org/wiki?curid=41100" title="Electronic deception">
Electronic deception

In telecommunication, the term electronic deception means the deliberate radiation, reradiation, alteration, suppression, absorption, denial, enhancement, or reflection of electromagnetic energy in a manner intended to convey misleading information and to deny valid information to an enemy or to enemy electronics-dependent weapons.
Among the types of electronic deception are:

</doc>
<doc id="41101" url="https://en.wikipedia.org/wiki?curid=41101" title="Electronic switching system">
Electronic switching system

In telecommunications, an electronic switching system (ESS) is a telephone switch that uses digital electronics and computerized control to interconnect telephone circuits for the purpose of establishing telephone calls.
The generations of telephone switches before the advent of electronic switching in the 1950s used purely electro-mechanical relay systems and analog voice paths. These early machines typically utilized the step-by-step technique. The first generation of electronic switching systems in the 1960s were not entirely digital in nature, but used reed relay-operated metallic paths or crossbar switches operated by stored program control (SPC) systems.
First announced in 1955, the first customer trial installation of an all-electronic central office commenced in Morris, Illinois in November 1960 by Bell Laboratories. The first prominent large-scale electronic switching system was the Number One Electronic Switching System (1ESS) of the Bell System in the United States, introduced in Succasunna, New Jersey, in May 1965.
Later electronic switching systems implemented the digital representation of the electrical audio signals on subscriber loops by digitizing the analog signals and processing the resulting data for transmission between central offices. Time-division multiplexing (TDM) technology permitted the simultaneous transmission of multiple telephone calls on a single wire connection between central offices or other electronic switches, resulting in dramatic capacity improvements of the telephone network.
With the advances of digital electronics starting in the 1960s telephone switches employed semiconductor device components in increasing measure.
In the late 20th century most telephone exchanges without TDM processing were eliminated and the term "electronic switching system" became largely a historical distinction for the older SPC systems.

</doc>
<doc id="41102" url="https://en.wikipedia.org/wiki?curid=41102" title="Electronic warfare support measures">
Electronic warfare support measures

In military telecommunications, the terms Electronic Support (ES) or Electronic Support Measures (ESM) describe the division of electronic warfare involving actions taken under direct control of an operational commander to detect, intercept, identify, locate, record, and/or analyze sources of radiated electromagnetic energy for the purposes of immediate threat recognition (such as warning that fire control RADAR has locked on a combat vehicle, ship, or aircraft) or longer-term operational planning. Thus, Electronic Support provides a source of information required for decisions involving Electronic Protection (EP), Electronic Attack (EA), avoidance, targeting, and other tactical employment of forces. Electronic Support data can be used to produce signals intelligence (SIGINT), communications intelligence (COMINT) and electronics intelligence (ELINT).
Electronic support measures gather intelligence through passive "listening" to electromagnetic radiations of military interest. Electronic support measures can provide (1) initial detection or knowledge of foreign systems, (2) a library of technical and operational data on foreign systems, and (3) tactical combat information utilizing that library. ESM collection platforms can remain electronically silent and detect and analyze RADAR transmissions beyond the RADAR detection range because of the greater power of the transmitted electromagnetic pulse with respect to a reflected echo of that pulse. United States airborne ESM receivers are designated in the AN/ALR series.
Desirable characteristics for electromagnetic surveillance and collection equipment include (1) wide-spectrum or bandwidth capability because foreign frequencies are initially unknown, (2) wide dynamic range because signal strength is initially unknown, (3) narrow bandpass to discriminate the signal of interest from other electromagnetic radiation on nearby frequencies, and (4) good angle-of arrival measurement for bearings to locate the transmitter. The frequency spectrum of interest ranges from 30 MHz to 50 GHz. Multiple receivers are typically required for surveillance of the entire spectrum, but tactical receivers may be functional within a specific signal strength threshold of a smaller frequency range.

</doc>
<doc id="41103" url="https://en.wikipedia.org/wiki?curid=41103" title="Electro-optic effect">
Electro-optic effect

An electro-optic effect is a change in the optical properties of a material in response to an electric field that varies slowly compared with the frequency of light. The term encompasses a number of distinct phenomena, which can be subdivided into
In December 2015, two further electro-optic effects of type (b) were theoretically predicted to exist but have not, as yet, been experimentally observed.
Changes in absorption can have a strong effect on refractive index for wavelengths near the absorption edge, due to the Kramers–Kronig relation.
Using a less strict definition of the electro-optic effect allowing also electric fields oscillating at optical frequencies, one could also include nonlinear absorption (absorption depends on the light intensity) to category a) and the optical Kerr effect (refractive index depends on the light intensity) to category b). Combined with the photoeffect and photoconductivity, the electro-optic effect gives rise to the photorefractive effect.
The term ""electro-optic"" is often erroneously used as a synonym for ""optoelectronic"".
Main applications.
Electro-optic modulators.
Electro-optic modulators are usually built with electro-optic crystals exhibiting the Pockels effect. The transmitted beam is phase modulated with the electric signal applied to the crystal. Amplitude modulators can be built by putting the electro-optic crystal between two linear polarizers or in one path of a Mach–Zehnder interferometer.
Additionally, Amplitude modulators can be constructed by deflecting the beam into and out of a small aperture such as a fiber. This design can be low loss (<3 dB) and polarization independent depending on the crystal configuration.
Electro-optic deflectors.
Electro-optic deflectors utilize prisms of electro-optic crystals. The index of refraction is changed by the Pockels effect, thus changing the direction of propagation of the beam inside the prism. Electro-optic deflectors have only a small number of resolvable spots, but possess a fast response time. There are few commercial models available at this time. This is because of competing acousto-optic deflectors, the small number of resolvable spots and the relatively high price of electro-optic crystals.

</doc>
<doc id="41104" url="https://en.wikipedia.org/wiki?curid=41104" title="Electro-optic modulator">
Electro-optic modulator

Electro-optic modulator (EOM) is an optical device in which a signal-controlled element exhibiting the electro-optic effect is used to modulate a beam of light. The modulation may be imposed on the phase, frequency, amplitude, or polarization of the beam. Modulation bandwidths extending into the gigahertz range are possible with the use of laser-controlled modulators.
The electro-optic effect is the change in the refractive index of a material resulting from the application of a DC or low-frequency electric field. This is caused by forces that distort the position, orientation, or shape of the molecules constituting the material. Generally, a nonlinear optical material (organic polymers have the fastest response rates, and thus are best for this application) with an incident static or low frequency optical field will see a modulation of its refractive index.
The simplest kind of EOM consists of a crystal, such as lithium niobate, whose refractive index is a function of the strength of the local electric field. That means that if lithium niobate is exposed to an electric field, light will travel more slowly through it. But the phase of the light leaving the crystal is directly proportional to the length of time it takes that light to pass through it. Therefore, the phase of the laser light exiting an EOM can be controlled by changing the electric field in the crystal.
Note that the electric field can be created by placing a parallel plate capacitor across the crystal. Since the field inside a parallel plate capacitor depends linearly on the potential, the index of refraction depends linearly on the field (for crystals where Pockels effect dominates), and the phase depends linearly on the index of refraction, the phase modulation must depend linearly on the potential applied to the EOM.
The voltage required for inducing a phase change of formula_1 is called the half-wave voltage (formula_2). For a Pockels cell, it is usually hundreds or even thousands of volts, so that a high-voltage amplifier is required. Suitable electronic circuits can switch such large voltages within a few nanoseconds, allowing the use of EOMs as fast optical switches.
Liquid crystal devices are electro-optical phase modulators if no polarizers are used.
Phase Modulation.
A very common application of EOMs is for creating sidebands in a monochromatic laser beam. To see how this works, first imagine that the strength of a laser beam with frequency formula_3 entering the EOM is given by
Now suppose we apply a sinusoidally varying potential voltage to the EOM with frequency formula_5 and small amplitude formula_6. This adds a time dependent phase to the above expression,
Since formula_6 is small, we can use the Taylor expansion for the exponential
to which we apply a simple identity for sine,
This expression we interpret to mean that we have the original carrier signal plus two small sidebands, one at formula_11 and another at formula_12. Notice however that we only used the first term in the Taylor expansion - in truth there are an infinite number of sidebands. There is a useful identity involving Bessel functions called the Jacobi-Anger expansion which can be used to derive
which gives the amplitudes of all the sidebands. Notice that if one modulates the amplitude instead of the phase, one gets only the first set of sidebands, 
Amplitude modulation.
A phase modulating EOM can also be used as an amplitude modulator by using a Mach-Zehnder interferometer. A beam splitter divides the laser light into two paths, one of which has a phase modulator as described above. The beams are then recombined. Changing the electric field on the phase modulating path will then determine whether the two beams interfere constructively or destructively at the output, and thereby control the amplitude or intensity of the exiting light. This device is called a Mach-Zehnder modulator.
Polarization modulation.
Depending on the type and orientation of the nonlinear crystal, and on the direction of the applied electric field, the phase delay can depend on the polarization direction. A Pockels cell can thus be seen as a voltage-controlled waveplate, and it can be used for modulating the polarization state. For a linear input polarization (often oriented at 45° to the crystal axis), the output polarization will in general be elliptical, rather than simply a linear polarization state with a rotated direction.

</doc>
<doc id="41105" url="https://en.wikipedia.org/wiki?curid=41105" title="Electro-optics">
Electro-optics

Electro-optics is a branch of electrical engineering and material physics involving components, devices (e.g. Lasers, LEDs, waveguides etc.) and systems which operate by the propagation and interaction of light with various tailored materials. It is essentially the same, as what is popularly described today as photonics. It is not only concerned with the "Electro-Optic effect". Thus it concerns the interaction between the electromagnetic (optical) and the electrical (electronic) states of materials.
Electro-optical devices.
The electro-optic effect relates to a change in the optical properties of the medium, which is usually a change in the birefringence, and not simply the refractive index.
In a Kerr cell, the change in birefringence is proportional to the square of the electric field, and the material is usually a liquid. In a Pockels cell, the change in birefringence varies linearly with the electric field, and the material is a crystal.
Non-crystalline, solid electro-optical materials have caught interest because of their low cost of production. These organic, polymer-based materials are also known as organic EO material, plastic EO material, or polymer EO material. They consist of nonlinear optical chromophores in a polymer lattice. The nonlinear optical chromophores produce Pockel's effect.

</doc>
<doc id="41106" url="https://en.wikipedia.org/wiki?curid=41106" title="Elliptical polarization">
Elliptical polarization

In electrodynamics, elliptical polarization is the polarization of electromagnetic radiation such that the tip of the electric field vector describes an ellipse in any fixed plane intersecting, and normal to, the direction of propagation. An elliptically polarized wave may be resolved into two linearly polarized waves in phase quadrature, with their polarization planes at right angles to each other. Since the electric field can rotate clockwise or counterclockwise as it propagates, elliptically polarized waves exhibit chirality.
Other forms of polarization, such as circular and linear polarization, can be considered to be special cases of elliptical polarization.
Mathematical description of elliptical polarization.
The classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is (cgs units)
for the magnetic field, where k is the wavenumber,
is the angular frequency of the wave propagating in the +z direction, and formula_4 is the speed of light.
Here formula_5 is the amplitude of the field and
is the normalized Jones vector. This is the most complete representation of polarized electromagnetic radiation and corresponds in general to elliptical polarization.
Polarization ellipse.
At a fixed point in space (or for fixed z), the electric vector formula_7 traces out an ellipse in the x-y plane. The semi-major and semi-minor axes of the ellipse have lengths a and b, respectively, that are given by
and 
where formula_10.
The orientation of the ellipse is given by the angle formula_11 the semi-major axis makes with the x-axis. This angle can be calculated from
If formula_13, the wave is linearly polarized. The ellipse collapses to a straight line formula_14) oriented at an angle formula_15. This is the case of superposition of two simple harmonic motions (in phase), one in the x direction with an amplitude formula_16, and the other in the y direction with an amplitude formula_17. When formula_18 increases from zero, i.e., assumes positive values, the line evolves into an ellipse that is being traced out in the counterclockwise direction (looking into the propagating wave); this then corresponds to Left-Handed Elliptical Polarization; the semi-major axis is now oriented at an angle formula_19. Similarly, if formula_18 becomes negative from zero, the line evolves into an ellipse that is being traced out in the clockwise direction; this corresponds to Right-Handed Elliptical Polarization. 
If formula_21 and formula_22, formula_23, i.e.,the wave is circularly polarized. When formula_24, the wave is left-circularly polarized, and when formula_25, the wave is right-circularly polarized.

</doc>
<doc id="41107" url="https://en.wikipedia.org/wiki?curid=41107" title="Emphasis (telecommunications)">
Emphasis (telecommunications)

In telecommunications emphasis is the intentional alteration of the amplitude-vs.-frequency characteristics of the signal to reduce adverse effects of noise in a communication system. Typically some part of the input is boosted during conversion to a signal, and then later the output is correspondingly attenuated so that the original input comes through unaffected while noise is reduced due to not having been boosted, only attenuated.
The whole system of pre-emphasis and de-emphasis is called emphasis.
The high-frequency signal components are emphasized to produce a more equal modulation index for the transmitted frequency spectrum, and therefore a better signal-to-noise ratio for the entire frequency range.
Emphasis is commonly used in LP records and FM broadcasting.
Pre-emphasis.
In processing electronic audio signals, pre-emphasis refers to a system process designed to increase (within a frequency band) the magnitude of some (usually higher) frequencies with respect to the magnitude of other (usually lower) frequencies in order to improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation distortion or saturation of recording media in subsequent parts of the system. The mirror operation is called de-emphasis, and the system as a whole is called emphasis.
Pre-emphasis is achieved with a pre-emphasis network which is essentially a calibrated filter. The frequency response is decided by special time constants. The cutoff frequency can be calculated from that value.
Pre-emphasis is commonly used in telecommunications, digital audio recording, record cutting, in FM broadcasting transmissions, and in displaying the spectrograms of speech signals.
One example of this is the RIAA equalization curve on 33 rpm and 45 rpm vinyl records. Another is the Dolby noise-reduction system as used with magnetic tape.
In high speed digital transmission, pre-emphasis is used to improve signal quality at the output of a data transmission. In transmitting signals at high data rates, the transmission medium may introduce distortions, so pre-emphasis is used to distort the transmitted signal to correct for this distortion. When done properly this produces a received signal which more closely resembles the original or desired signal, allowing the use of higher frequencies or producing fewer bit errors.
Pre-emphasis is employed in frequency modulation or phase modulation transmitters to equalize the modulating signal drive power in terms of deviation ratio. The receiver demodulation process includes a reciprocal network, called a de-emphasis network, to restore the original signal power distribution.
De-emphasis.
In telecommunication, de-emphasis is the complement of pre-emphasis, in the antinoise system called emphasis. De-emphasis is a system process designed to decrease, (within a band of frequencies), the magnitude of some (usually higher) frequencies with respect to the magnitude of other (usually lower) frequencies in order to improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation differences or saturation of recording media in subsequent parts of the system.
Special time constants dictate the frequency response curve, from which one can calculate the cutoff frequency.
Pre-emphasis is commonly used in audio digital recording, record cutting and FM radio transmission.
In serial data transmission, de-emphasis has a different meaning, which is to reduce the level of all bits except the first one after a transition. That causes the high frequency content due to the transition to be emphasized compared to the low frequency content which is de-emphasized. This is a form of transmitter equalization; it compensates for losses over the channel which are larger at higher frequencies. Well known serial data standards such as PCI Express, SATA and SAS require transmitted signals to use de-emphasis.
Red Book Audio.
Although rarely used, there exists the capability for standardized emphasis in Red Book CD mastering. As CD's were intended to work on 14 bit audio, a specification for 'pre-emphasis' was included to compensate for quantization noise. After production spec was set at 16 bits, quantization noise became less of a concern, but emphasis remained an option through standards revisions. The pre-emphasis is described as a first-order filter with a gain of 10 dB (at 20 dB/decade) and time constants 50 μs and 15 μs.

</doc>
<doc id="41108" url="https://en.wikipedia.org/wiki?curid=41108" title="Encode">
Encode

Encode or encoder may refer to:

</doc>
<doc id="41109" url="https://en.wikipedia.org/wiki?curid=41109" title="End distortion">
End distortion

End distortion: In start-stop teletypewriter operation, the shifting of the end of all marking pulses, except the stop pulse, from their proper positions in relation to the beginning of the next start pulse. 
Shifting of the end of the stop pulse is a deviation in character time and rate rather than an end distortion. 
Spacing end distortion is the termination of marking pulses before the proper time. Marking end distortion is the continuation of marking pulses past the proper time. 
The magnitude of the distortion is expressed as a percentage of an ideal pulse length.

</doc>
<doc id="41110" url="https://en.wikipedia.org/wiki?curid=41110" title="End-of-Transmission character">
End-of-Transmission character

In telecommunication, an End-of-Transmission character (EOT) is a transmission control character. Its intended use is to indicate the conclusion of a transmission that may have included one or more texts and any associated message headings.
An EOT is often used to initiate other functions, such as releasing circuits, disconnecting terminals, or placing receive terminals in a standby condition. Its most common use today is to cause a Unix terminal driver to signal end of file and thus exit programs that are awaiting input.
In ASCII and Unicode, the character is encoded at . It can be referred to as , ^D in caret notation. Unicode provides the character for when EOT needs to be displayed graphically. In addition, can also be used as a graphic representation of EOT; it is defined in Unicode as "symbol for End of Transmission".
Meaning in Unix.
The EOT character in Unix is different from the Control-Z in DOS. The DOS Control-Z byte is actually sent and/or placed in files to indicate where the text ends. In contrast the Control-D causes the Unix terminal driver to signal the EOF condition, which is not a character, while the byte has no special meaning if actually read or written from a file or terminal.
In Unix the end-of-file character (by default EOT) causes the terminal driver to make available all characters in its input buffer immediately; normally the driver would collect characters until it sees an end-of-line character. If the input buffer is empty (because no characters have been typed since the last end-of-line or end-of-file), a program reading from the terminal reads a count of zero bytes. In Unix, such a condition is understood as having reached the end of the file.
This can be demonstrated with the cat program on Unix-based operating systems such as Linux: Run the codice_1 command with no arguments, so it accepts its input from the keyboard and prints output to the screen. Type a few characters without pressing , then type . The characters typed to that point are sent to cat, which then writes them to the screen. If is typed without typing any characters first, the input stream is terminated and the program ends. An actual EOT is obtained by typing then .
If the terminal driver is in "raw" mode, it no longer interprets control characters, and the EOT character is sent unchanged to the program, which is free to interpret it any way it likes. A program may then decide to handle the EOT byte as an indication that it should end the text; this would then be similar to how is handled by DOS programs.
Usage in mainframe computer system communications protocols.
The EOT character is used in legacy communications protocols by mainframe computer manufacturers such as IBM, Burroughs Corporation, and the BUNCH. Terminal transmission control protocols such as IBM 3270 Poll/Select, or Burroughs TD830 Contention Mode protocol use the EOT character to terminate a communications sequence between two cooperating stations (such as a host multiplexer or Input/Output terminal).
A single Poll (ask the station for data) or Select (send data to the station) operation will include two round-trip send-reply operations between the polling station and the station being polled, the final operation being transmission of a single EOT character to the initiating station.

</doc>
<doc id="41111" url="https://en.wikipedia.org/wiki?curid=41111" title="Endurability">
Endurability

In telecommunication, endurability is the property of a system, subsystem, equipment, or process that enables it to continue to function within specified performance limits for an extended period of time, usually months, despite a severe natural or man-made disturbance, such as a nuclear attack, or a loss of external logistic or utility support. 
Endurability is not compromised by temporary failures when the local capability exists to restore and maintain the system, subsystem, equipment, or process to an acceptable performance level.

</doc>
<doc id="41112" url="https://en.wikipedia.org/wiki?curid=41112" title="Enhanced service">
Enhanced service

Enhanced service is service offered over commercial carrier transmission facilities used in interstate communications, that employs computer processing applications that act on the format, content, code, protocol, or similar aspects of the subscriber's transmitted information; provides the subscriber with additional, different, or restructured information; or involves subscriber interaction with stored information.

</doc>
<doc id="41113" url="https://en.wikipedia.org/wiki?curid=41113" title="Epoch (reference date)">
Epoch (reference date)

In the fields of chronology and periodization, an epoch is an instant in time chosen as the origin of a particular era. The "epoch" then serves as a reference point from which time is measured. Time measurement units are counted from the epoch so that the date and time of events can be specified unambiguously.
Events taking place before the epoch can be dated by counting negatively from the epoch, though in pragmatic periodization practice, epochs are defined for the past, and another epoch is used to start the next era, therefore serving as the ending of the older preceding era. The whole purpose and criteria of such definitions are to clarify and co-ordinate scholarship about a period, at times, across disciplines.
Epochs are generally chosen to be convenient or significant by a consensus of the time scale's initial users, or by authoritarian fiat. The epoch moment or date is usually defined by "a specific clear event", condition, or criterion — the epoch event or epoch criterion — from which the period or era or age is usually characterized or described.
Calendars.
Each calendar era starts from an arbitrary epoch, which is often chosen to commemorate an important historical or mythological event. Many current and historical calendar eras exist, each with its own epoch.
Astronomy.
In astronomy, an epoch is a specific moment in time for which celestial coordinates or orbital elements are specified, and from which other orbital parametrics are thereafter calculated in order to predict future position. The applied tools of the mathematics disciplines of Celestial mechanics or its subfield Orbital mechanics (both predict orbital paths and positions) about a center of gravity are used to generate an ephemeris (plural: "ephemerides"; from the Greek word "ephemeros" = daily) which is a table of values that gives the positions of astronomical objects in the sky at a given time or times, or a formula to calculate such given the proper time offset from the epoch. Such calculations generally result in an elliptical path on a plane defined by some point on the orbit, and the two foci of the ellipse. Viewing from another orbiting body, following its own trace and orbit, creates shifts in three dimensions in the spherical trigonometry used to calculate relative positions. Interestingly, these dynamics in three dimensions are also elliptical, which means the ephemeris need only specify one set of equations to be a useful predictive tool to predict future location of the object of interest.
Over time, inexactitudes and other errors accumulate, creating more and greater errors of prediction, so ephemeris factors need to be recalculated from time to time, and that requires a new epoch to be defined. Different astronomers or groups of astronomers used to define epochs to suit themselves, but in these days of speedy communications, the epochs are generally defined in an international agreement, so astronomers worldwide can collaborate more effectively. It was inefficient and error prone for data observed by one group to need translation (mathematic transformation) so other groups could compare information.
J2000.0.
The current standard epoch is called "J2000.0" This is defined by international agreement to be equivalent to:
Computing.
The time kept internally by a computer system is usually expressed as the number of time units that have elapsed since a specified epoch, which is nearly always specified as midnight Universal Time on some particular date.
Software timekeeping systems vary widely in the granularity of their time units; some systems may use time units as large as a day, while others may use nanoseconds. For example, for an epoch date of midnight UTC (00:00) on January 1, 1900, and a time unit of a second, the time of the midnight (24:00) between January 1 and 2, 1900 is represented by the number 86400, the number of seconds in one day. When times prior to the epoch need to be represented, it is common to use the same system, but with negative numbers.
These representations of time are mainly for internal use. If an end user interaction with dates and times is required, the software will nearly always convert this internal number into a date and time representation that is comprehensible to humans.
Notable epoch dates in computing.
The following table lists epoch dates used by popular software and other computer-related systems. The time in these systems is stored as the quantity of a particular time unit (days, seconds, nanoseconds, etc.) that has elapsed since a stated time (usually midnight UTC at the beginning of the given date).
Problems with epoch-based computer time representation.
Computers do not generally store arbitrarily large numbers. Instead, each number stored by a computer is allotted a fixed amount of space. Therefore, when the number of time units that have elapsed since a system's epoch exceeds the largest number that can fit in the space allotted to the time representation, the time representation overflows, and problems can occur. While a system's behavior after overflow occurs is not necessarily predictable, in most systems the number representing the time will reset to zero, and the computer system will think that the current time is the epoch time again.
Most famously, older systems which counted time as the number of years elapsed since the epoch of January 1, 1900 and which only allotted enough space to store the numbers 0 through 99, experienced the Year 2000 problem. These systems (if not corrected beforehand) would interpret the date January 1, 2000 as January 1, 1900, leading to unpredictable errors at the beginning of the year 2000.
Even systems which allocate more storage to the time representation are not immune from this kind of error. Many Unix-like operating systems which keep time as seconds elapsed from the epoch date of January 1, 1970, and allot timekeeping enough storage to store numbers as large as will experience an overflow problem on January 19, 2038 if not fixed beforehand. This is known as the Year 2038 problem. A correction involving doubling the storage allocated to timekeeping on these systems will allow them to represent dates more than 290 billion years into the future.
Other more subtle timekeeping problems exist in computing, such as accounting for leap seconds, which are not observed with any predictability or regularity. Additionally, applications which need to represent historical dates and times (for example, representing a date prior to the switch from the Julian calendar to the Gregorian calendar) must use specialized timekeeping libraries.
Finally, some software must maintain compatibility with older software that does not keep time in strict accordance with traditional timekeeping systems. For example, Microsoft Excel observes the fictional date of February 29, 1900 in order to maintain compatibility with older versions of Lotus 1-2-3. Lotus 1-2-3 observed the date due to an error; by the time the error was discovered, it was too late to fix it—"a change now would disrupt formulas which were written to accommodate this anomaly".
Epoch in satellite-based time systems.
There are at least six satellite navigation systems, all of which function by transmitting time signals. Of the only two satellite systems with global coverage, GPS calculates its time signal from an epoch, whereas GLONASS calculates time as an offset from UTC, with the UTC input adjusted for leap seconds. Of the only two other systems aiming for global coverage, Galileo calculates from an epoch and Beidou calculates from UTC without adjustment for leap seconds. GPS also transmits the offset between UTC time and GPS time, and must update this offset every time there is a leap second, requiring GPS receiving devices to handle the update correctly. In contrast, leap seconds are transparent to GLONASS users.
The complexities of calculating UTC from an epoch are explained by the European Space Agency in Galileo documentation under "Equations to correct system timescale to reference timescale" 

</doc>
<doc id="41115" url="https://en.wikipedia.org/wiki?curid=41115" title="Equivalent noise resistance">
Equivalent noise resistance

In telecommunication, an equivalent noise resistance is a quantitative representation in resistance units of the spectral density of a noise-voltage generator, given by
formula_1
where formula_2 is the spectral density, formula_3 is the Boltzmann's constant, formula_4 is the standard noise temperature (290 K), so formula_5.
"Note:" The equivalent noise resistance in terms of the mean-square noise-generator voltage, "e"2, within a frequency increment, Δ "f", is given by

</doc>
<doc id="41116" url="https://en.wikipedia.org/wiki?curid=41116" title="Equivalent pulse code modulation noise">
Equivalent pulse code modulation noise

In telecommunication, equivalent pulse code modulation noise (PCM) is the amount of thermal noise power on a frequency-division multiplexing (FDM) or wire channel necessary to approximate the same judgment of speech quality created by quantizing noise in a PCM channel. 

</doc>
<doc id="41118" url="https://en.wikipedia.org/wiki?curid=41118" title="Error">
Error

An error (from the Latin "error", meaning "wandering") is an action which is inaccurate or incorrect. In some usages, an error is synonymous with a mistake (for instance, a cook who misses a step from a recipe might describe it as either an error or a mistake), though in technical contexts the two are often distinguished. For instance, in statistics "error" refers to the difference between the value which has been computed and the correct value.
Human behavior.
One reference differentiates between "error" and "mistake" as follows:
In human behavior the norms or expectations for behavior or its consequences can be derived from the intention of the actor or from the expectations of other individuals or from a social grouping or from social norms. (See deviance.) Gaffes and faux pas can be labels for certain instances of this kind of error. More serious departures from social norms carry labels such as misbehavior and labels from the legal system, such as misdemeanor and crime. Departures from norms connected to religion can have other labels, such as sin.
Oral and written language.
An individual language user's deviations from standard language norms in grammar, syntax, pronunciation and punctuation are sometimes referred to as errors. However, in light of the role of language usage in everyday social class distinctions, many feel that linguistics should be descriptive rather than prescriptive to avoid reinforcing dominant class value judgments about what linguistic forms should and should not be used. One may distinguish various kinds of linguistic errors – some, such as aphasia or speech disorders, where the user is unable to say what they intend to, are generally considered errors, while cases where natural, intended speech is non-standard (as in dialects), are considered correct speech in descriptive linguistics, but errors in prescriptive linguistics. See also Error analysis (linguistics).
Gaffe.
A gaffe is a verbal mistake, usually made in a social environment. The mistake may come from saying something that is true, but inappropriate. It may also be an erroneous attempt to reveal a truth. Finally, gaffes can be malapropisms, grammatical errors or other verbal and gestural weaknesses or revelations through body language.
Actually revealing factual or social truth through words or body language, however, can commonly result in embarrassment or, when the gaffe has negative connotations, friction between people involved.
Philosophers and psychologists interested in the nature of the gaffe include Freud and Gilles Deleuze. Deleuze, in his "Logic of Sense", places the gaffe in a developmental process that can culminate in stuttering.
Sports writers and journalists commonly use "gaffe" to refer to any kind of mistake, e.g., a dropped ball by a player in a baseball game.
Medicine.
See medical error for a description of error in medicine.
Science and engineering.
In statistics, an error (or "residual") is not a "mistake" but rather a difference between a computed, estimated, or measured value and the accepted true, specified, or theoretically correct value.
In science and engineering in general an error is defined as a difference between the desired and actual performance or behavior of a system or object. This definition is the basis of operation for many types of control systems, in which error is defined as the difference between a set point and the process value. An example of this would be the thermostat in a home heating system—the operation of the heating equipment is controlled by the difference (the error) between the thermostat setting and the sensed air temperature. Another approach is related to considering a scientific hypothesis as true or false, giving birth to two types of errors: Type 1 and Type 2. The first one is when a true hypothesis is considered false, while the second is the reverse (a false one is considered true).
Engineers seek to design devices, machines and systems and in such a way as to mitigate or preferably avoid the effects of error, whether unintentional or not. Such errors in a system can be latent design errors that may go unnoticed for years, until the right set of circumstances arises that cause them to become active. Other errors in engineered systems can arise due to human error, which includes cognitive bias. Human factors engineering is often applied to designs in an attempt to minimize this type of error by making systems more forgiving or error-tolerant.
Numerical analysis.
Numerical analysis provides a variety of techniques to represent (store) and compute approximations to mathematical numerical values. Errors arise from a trade-off between efficiency (space and computation time) and precision, which is limited anyway, since (using common floating-point arithmetic) only a finite amount of values can be represented exactly. The discrepancy between the exact mathematical value and the stored/computed value is called the approximation error.
Cybernetics.
The word "cybernetics" stems from the Greek Κυβερνήτης ("kybernētēs", steersman, governor, pilot, or rudder — the same root as government). In applying corrections to the trajectory or course being steered cybernetics can be seen as the most general approach to error and its correction for the achievement of any goal. The term was suggested by Norbert Wiener to describe a new science of control and information in the animal and the machine. Wiener's early work was on noise.
The cybernetician Gordon Pask held that the error that drives a servomechanism can be seen as a difference between a pair of analogous concepts in a servomechanism: the current state and the goal state. Later he suggested error can also be seen as an innovation or a contradiction depending on the context and perspective of interacting (observer) participants. The founder of management cybernetics, Stafford Beer, applied these ideas most notably in his Viable System Model.
Biology.
In biology, an error is said to occur when perfect fidelity is lost in the copying of information. For example, in an asexually reproducing species, an error (or mutation) has occurred for each DNA nucleotide that differs between the child and the parent. Many of these mutations can be harmful, but unlike other types of errors, some are neutral or even beneficial. Mutations are an important force driving evolution. Mutations that make organisms more adapted to their environment increase in the population through natural selection as organisms with favorable mutations have more offspring.
Philately.
In philately, an error refers to a postage stamp or piece of postal stationery that exhibits a printing or production mistake that differentiates it from a normal specimen or from the intended result. Examples are stamps printed in the wrong color or missing one or more colors, printed with a vignette inverted in relation to its frame, produced without any perforations on one or more sides when the normal stamps are perforated, or printed on the wrong type of paper. Legitimate errors must always be produced and sold unintentionally. Such errors may or may not be scarce or rare. A design error may refer to a mistake in the design of the stamp, such as a mislabeled subject, even if there are no printing or production mistakes.
Law.
In appellate review, error typically refers to mistakes made by a trial court or some other court of first instance in applying the law in a particular legal case. This may involve such mistakes as improper admission of evidence, inappropriate instructions to the jury, or applying the wrong standard of proof.
Governmental policy.
Within United States government intelligence agencies, such as Central Intelligence Agency agencies, error refers to intelligence error, as previous assumptions that used to exist at a senior intelligence level within senior intelligence agencies, but has since been disproven, and is sometimes eventually listed as unclassified, and therefore more available to the American public and citizenry of the United States. The Freedom of information act provides American citizenry with a means to read intelligence reports that were mired in error. Per United States Central Intelligence Agency's website (as of August, 2008) intelligence error is described as:
"Intelligence errors are factual inaccuracies in analysis resulting from poor or missing data; intelligence failure is systemic organizational surprise resulting from incorrect, missing, discarded, or inadequate hypotheses."
Numismatics.
In numismatics, an error refers to a coin or medal that has a minting mistake, similar to errors found in philately. Because the U.S. Bureau of the Mint keeps a careful eye on all potential errors, errors on U.S. coins are very few and usually very scarce. Examples of numismatic errors: extra metal attached to a coin, a clipped coin caused by the coin stamp machine stamping a second coin too early, double stamping of a coin. A coin that has been overdated, e.g.: 1942/41, is also considered an error.

</doc>
<doc id="41119" url="https://en.wikipedia.org/wiki?curid=41119" title="Burst error">
Burst error

In telecommunication, a burst error or error burst is a contiguous sequence of symbols, received over a data transmission channel, such that the first and last symbols are in error and there exists no contiguous subsequence of "m" correctly received symbols within the error burst.
The integer parameter "m" is referred to as the guard band of the error burst. The last symbol in a burst and the first symbol in the following burst are accordingly separated by "m" correct bits or more. The parameter "m" should be specified when describing an error burst.
For example, imagine sending a packet containing all of the letters of the alphabet, A through Z. If the recipient's computer "opens" the packet and finds that the first letter in the sequence is "Q" and the last letter in the sequence is "R," that is a burst error. The "burst" of data in the packet is corrupt.
Although in the example the first and last letters are defined as corrupt, that does not mean that every letter within the packet is damaged. Imagine that every other letter is as it should be; only position one, "A," and position 26, "Z," have been damaged. The number of correct bits of information between the damaged ends is called the guard band. In this case, the guard band would be 24, because there are 24 correct letters separating the two damaged ones.
Channel model.
The Gilbert–Elliott model is a simple channel model introduced by Edgar Gilbert and E. O. Elliott widely used for describing burst error patterns in transmission channels, that enables simulations of the digital error performance of communications links. It is based on a Markov chain with two states "G" (for good or gap) and "B" (for bad or burst). In state "G" the probability of transmitting a bit correctly is "k" and in state "B" it is "h". Usually, it is assumed that "k" = 1. Gilbert provided equations for deriving the other three parameters ("G" and "B" state transition probabilities and "h") from a given success/failure sequence. In his example, the sequence was too short to correctly find "h" (a negative probability was found) and so Gilbert assumed that "h" = 0.5.

</doc>
<doc id="41123" url="https://en.wikipedia.org/wiki?curid=41123" title="Escape character">
Escape character

In computing and telecommunication, an escape character is a character which invokes an alternative interpretation on subsequent characters in a character sequence. An escape character is a particular case of metacharacters. Generally, the judgement of whether something is "an escape character" or not depends on context.
Definition.
Escape characters are part of the syntax for many programming languages, data formats, and communication protocols. For a given alphabet an escape character's purpose is to start character sequences (so named escape sequences), which have to be interpreted differently from the same characters occurring without the prefixed escape character. An escape character may not have its own meaning, so all escape sequences are of two or more characters.
There are usually two functions of escape sequences. The first is to encode a syntactic entity, such as device commands or special data, which cannot be directly represented by the alphabet. The second use, referred to as "character quoting", is to represent characters, which cannot be typed in current context, or would have an undesired interpretation. In the latter case an escape sequence is a digraph consisting of an escape character itself and a "quoted" character.
Control character.
Generally, an escape character is not a particular case of (device) control characters, nor vice versa. If we define control characters as non-graphic, or as having a special meaning for an output device (e.g. printer or text terminal) then any escape character for this device is a control one. But escape characters used in programming (such as the backslash, "\") are graphic, hence are not control characters. Conversely most (but not all) of the ASCII "control characters" have some control function in isolation, therefore are not escape characters.
In many programming languages, an escape character also forms some escape sequences which are referred to control characters. For example, line break has an escape sequence of .
Examples.
JavaScript.
JavaScript uses the \ (backslash) as an escape character for:
Note that the \v and \0 escapes are not allowed in JSON strings.
ASCII escape character.
The ASCII "escape" character (octal: \033, hexadecimal: \x1B, or ^[, or, in decimal, 27) is used in many output devices to start a series of characters called a control sequence or escape sequence. Typically, the escape character was sent first in such a sequence to alert the device that the following characters were to be interpreted as a control sequence rather than as plain characters, then one or more characters would follow to specify some detailed action, after which the device would go back to interpreting characters normally. For example, the sequence of ^[, followed by the printable characters codice_1, would cause a DEC VT102 terminal to move its cursor to the 10th cell of the 2nd line of the screen. This was later developed to ANSI escape codes covered by the ANSI X3.64 standard. The escape character also starts each command sequence in the Hewlett Packard Printer Command Language.
Early reference to the term "escape character" is found in Bob Bemer's IBM technical publications. Apparently, it is he who invented this mechanism, during his work on the ASCII character set.
The Escape key is usually found on standard PC keyboards. However it is commonly absent from keyboards for PDAs and other devices not designed primarily for ASCII communications. The DEC VT220 series was one of the few popular keyboards that did not have a dedicated Esc key, instead using one of the keys above the main keypad. In user interfaces of the 1970s–1980s it was not uncommon to use this key as an escape character, but in modern desktop computers such use is dropped. Sometimes the key was identified with AltMode (for alternative mode). Even with no dedicated key, the escape character code could be generated by typing '[' while simultaneously holding down the Control key, 'Ctrl'.
Programming and data formats.
Many modern programming languages specify the doublequote character (codice_2) as a delimiter for a string literal. The backslash (codice_3) escape character typically provides two ways to include doublequotes inside a string literal, either by modifying the meaning of the doublequote character embedded in the string (codice_4 becomes codice_2), or by modifying the meaning of a sequence of characters including the hexadecimal value of a doublequote character (codice_6 becomes codice_2).
C, C++, Java, and Ruby all allow exactly the same two backslash escape styles. The PostScript language and Microsoft Rich Text Format also use backslash escapes. The quoted-printable encoding uses the equals sign as an escape character.
URL and URI use %-escapes to quote characters with a special meaning, as for non-ASCII characters. The ampersand (codice_8) character may be considered as an escape character in SGML and derived formats such as HTML and XML.
Some programming languages also provide other ways to represent special characters in literals, without requiring an escape character (see e.g. delimiter collision).
Communication protocols.
The Point-to-Point Protocol uses the 0x7D octet (\175, or ASCII: } ) as an escape character. The octet immediately following should be XORed by 0x20 before being passed to a higher level protocol. This is applied to both 0x7D itself and the control character 0x7E (which is used in PPP to mark the beginning and end of a frame) when those octets need to be transmitted by a higher level protocol encapsulated by PPP, as well as other octets negotiated when the link is established. That is, when a higher level protocol wishes to transmit 0x7D, it is transmitted as the sequence 0x7D 0x5D, and 0x7E is transmitted as 0x7D 0x5E.
Bourne shell.
In Bourne shell (sh), the asterisk (codice_9) and question mark (codice_10) characters are wildcard characters expanded via globbing. Without a preceding escape character, an codice_9 will expand to the names of all files in the working directory that do not start with a period iff there are such files, otherwise codice_9 remains unexpanded. So to refer to a file literally called "*", the shell must be told not to interpret it in this way, by preceding it with a backslash (codice_3). This modifies the interpretation of the asterisk (codice_9). Compare:
Windows Command Prompt.
The Windows command-line interpreter uses a caret character (codice_15) to escape reserved characters that have special meanings (in particular: codice_16). The DOS command-line interpreter, though it supports similar syntax, does not support this.
For example, on the Windows Command Prompt, this will result in a syntax error.
whereas this will output the string: codice_17
See also.
Not to be confused with:

</doc>
<doc id="41124" url="https://en.wikipedia.org/wiki?curid=41124" title="Essential service (telecommunications)">
Essential service (telecommunications)

In telecommunication, an essential service (critical service) is a network-provided service feature in which a priority dial tone is furnished. Essential service is typically provided to fewer than 10% of network users, and recommended for use in conjunction with NS/EP telecommunications services.

</doc>
<doc id="41125" url="https://en.wikipedia.org/wiki?curid=41125" title="Exchange">
Exchange

Exchange may refer to:

</doc>
<doc id="41126" url="https://en.wikipedia.org/wiki?curid=41126" title="Exempted addressee">
Exempted addressee

In telecommunication, an exempted addressee is an organization, activity, or person included in the collective address group of a message and deemed by the message originator as having no need for the information in the message. 
Exempted addressees may be explicitly excluded from the collective address group for the particular message to which the exemption applies.

</doc>
<doc id="41128" url="https://en.wikipedia.org/wiki?curid=41128" title="Extended superframe">
Extended superframe

In telecommunications, extended superframe (ESF) is a T1 framing standard.
ESF is sometimes called D5 Framing because it was first used in the D5 channel bank, invented in the 1980s.
It is preferred to its predecessor, superframe, because it includes a cyclic redundancy check (CRC) and 4000 bit/s channel capacity for a data link channel (used to pass out-of-band data between equipment.) It requires less frequent synchronization than the earlier superframe format, and provides on-line, real-time monitoring of circuit capability and operating condition.
An extended superframe is 24 frames long, and the framing bit of each frame is used in the following manner:
The CRC is computed using the polynomial over all 23×193 = 4632 bits (framing and data) of the previous superframe, but with its CRC bits forced to 1 for the purpose of CRC computation. The purpose of this small CRC is not to take any immediate action, but to keep statistics on the performance of the link.
Like the predecessor superframe, every sixth frame's least-significant data bit can be used for robbed-bit signaling of call supervision state. However, there are four such bits (ABCD) per channel per extended superframe, rather than the two bits (AB) provided per superframe. 
Unlike the superframe, it is possible to avoid robbed-bit signalling and send call supervision over the data link instead.

</doc>
<doc id="41130" url="https://en.wikipedia.org/wiki?curid=41130" title="Extinction ratio">
Extinction ratio

In telecommunications, extinction ratio ("r"e) is the ratio of two optical power levels of a digital signal generated by an optical source, "e.g.," a laser diode. The extinction ratio may be expressed as a fraction, in dB, or as a percentage. It may be given by
where "P"1 is the optical power level generated when the light source is on, and "P"0 is the power level generated when the light source is off.

</doc>
<doc id="41131" url="https://en.wikipedia.org/wiki?curid=41131" title="Eye pattern">
Eye pattern

In telecommunication, an eye pattern, also known as an eye diagram, is an oscilloscope display in which a digital signal from a receiver is repetitively sampled and applied to the vertical input, while the data rate is used to trigger the horizontal sweep. It is so called because, for several types of coding, the pattern looks like a series of eyes between a pair of rails. It is an experimental tool for the evaluation of the combined effects of channel noise and intersymbol interference on the performance of a baseband pulse-transmission system. It is the synchronised superposition of all possible realisations of the signal of interest viewed within a particular signalling interval.
Several system performance measures can be derived by analyzing the display. If the signals are too long, too short, poorly synchronized with the system clock, too high, too low, too noisy, or too slow to change, or have too much undershoot or overshoot, this can be observed from the eye diagram. An open eye pattern corresponds to minimal signal distortion. Distortion of the signal waveform due to intersymbol interference and noise appears as closure of the eye pattern.
Measurements.
There are many measurements that can be obtained from an Eye Diagram:
Amplitude Measurements
Time Measurements

</doc>
<doc id="41132" url="https://en.wikipedia.org/wiki?curid=41132" title="Telecommunications facility">
Telecommunications facility

In telecommunications, a facility is defined by Federal Standard 1037C as:
In Canada.
Under Canadian federal and Québécois provincial law, a telecommunications facility, for the purposes of determining whether GST applies, is defined by §123(1) of the GST Act to be "any facility, apparatus, or other thing (including any wire, cable, radio, optical, or other electromagnetic system, or any similar technical system or any part thereof) that is used or is capable of being used for telecommunications". This is a very broad definition that includes a wide range of things from satellites and earth stations, to telephones and fax machines. The consequence of its application is that even a simple LAN connector jack can be considered to be a telecommunications facility in Canada, for tax purposes.

</doc>
<doc id="41133" url="https://en.wikipedia.org/wiki?curid=41133" title="Facsimile converter">
Facsimile converter

In telecommunication, the term facsimile converter has the following meanings: 
1. In a facsimile receiver, a device that changes the signal modulation from frequency-shift keying (FSK) to amplitude modulation (AM). 
2. In a facsimile transmitter, a device that changes the signal modulation from amplitude modulation (AM) to frequency-shift keying (FSK).

</doc>
<doc id="41134" url="https://en.wikipedia.org/wiki?curid=41134" title="Fade margin">
Fade margin

In telecommunication, the term fade margin (fading margin) has the following meanings:

</doc>
<doc id="41135" url="https://en.wikipedia.org/wiki?curid=41135" title="Fading distribution">
Fading distribution

In telecommunications, a fading distribution is the probability distribution of the value of signal fading relative to a specified reference level.
In the case of phase interference fading, the time distribution of the instantaneous field strength usually approximates a Rayleigh distribution when several signal components of equal amplitude are present. 
The field strength is usually measured in volts per meter. 
The fading distribution may also be measured in terms of power level, where the unit of measure is usually watts per square meter and the expression is in decibels.

</doc>
<doc id="41136" url="https://en.wikipedia.org/wiki?curid=41136" title="Fail-safe">
Fail-safe

A fail-safe device is one that, in the event of a specific type of failure, responds in a way that will cause no harm, or at least a minimum of harm, to other devices or to personnel.
Fail-safe and fail-secure are similar but distinct concepts. Fail-safe means that a device will not endanger lives or property when it fails. Fail-secure means that access or data will not fall into the wrong hands in a failure. Sometimes the approaches suggest opposite solutions. For example, if a building catches fire, fail-safe systems would unlock doors to ensure quick escape and allow firefighters inside, while fail-secure would lock doors to prevent unauthorized access to the building.
A system's being "fail-safe" means not that failure is impossible or improbable, but rather that the system's design prevents or mitigates unsafe consequences of the system's failure. That is, if and when a "fail-safe" system "fails", it is "safe" or at least no less safe than when it was operating correctly.
Since many types of failure are possible, it must be specified to what failure a component is fail safe. For example, a system may be fail-safe in the event of a power outage (electrical failure), but may not be fail safe in the event of mechanical failures.
Examples.
Mechanical or physical.
Examples include:
Electrical or electronic.
Examples include:
Procedural.
As well as physical devices and systems fail-safe procedures can be created so that if a procedure is not carried out or carried out incorrectly no dangerous action results. For example:
Other terminology.
Fail-safe (foolproof) devices are also known as "poka-yoke" devices. "Poka-yoke", a Japanese term, was coined by Shigeo Shingo, a quality expert. "Safe to fail" refers to civil engineering designs such as the Room for the River project in Netherlands and the Thames Estuary 2100 Plan which incorporate flexible adaptation strategies or climate change adaptation which provide for, and limit, damage, should severe events such as 500-year floods occur.

</doc>
<doc id="41138" url="https://en.wikipedia.org/wiki?curid=41138" title="Fall time">
Fall time

In electronics, fall time (pulse decay time) formula_1 is the time taken for the amplitude of a pulse to decrease (fall) from a specified value (usually 90% of the peak value exclusive of overshoot or undershoot) to another specified value (usually 10% of the maximum value exclusive of overshoot or undershoot). 
Limits on undershoot and oscillation (also known as ringing and hunting) are sometimes additionally stated when specifying fall time limits.

</doc>
<doc id="41142" url="https://en.wikipedia.org/wiki?curid=41142" title="Fast packet switching">
Fast packet switching

In telecommunications, fast packet switching is a variant of packet switching that increases the throughput by eliminating overhead associated with flow control and error correction functions, which are either offloaded to upper layer networking protocols or removed altogether. ATM and Frame Relay are two major implementations of fast packet switching.

</doc>
<doc id="41143" url="https://en.wikipedia.org/wiki?curid=41143" title="Fault">
Fault

Fault may refer to:

</doc>
<doc id="41144" url="https://en.wikipedia.org/wiki?curid=41144" title="Fault management">
Fault management

In network management, fault management is the set of functions that detect, isolate, and correct malfunctions in a telecommunications network, compensate for environmental changes, and include maintaining and examining error logs, accepting and acting on error detection notifications, tracing and identifying faults, carrying out sequences of diagnostics tests, correcting faults, reporting error conditions, and localizing and tracing faults by examining and manipulating database information.
When a fault or event occurs, a network component will often send a notification to the network operator using a protocol such as SNMP. An alarm is a persistent indication of a fault that clears only when the triggering condition has been resolved. A current list of problems occurring on the network component is often kept in the form of an active alarm list such as is defined in RFC 3877, the Alarm MIB. A list of cleared faults is also maintained by most network management systems.
Fault management systems may use complex filtering systems to assign alarms to severity levels. These can range in severity from debug to emergency, as in the syslog protocol. Alternatively, they could use the ITU X.733 Alarm Reporting Function's perceived severity field. This takes on values of cleared, indeterminate, critical, major, minor or warning. Note that the latest version of the syslog protocol draft under development within the IETF includes a mapping between these two different sets of severities. It is considered good practice to send a notification not only when a problem has occurred, but also when it has been resolved. The latter notification would have a severity of clear.
A fault management console allows a network administrator or system operator to monitor events from multiple systems and perform actions based on this information. Ideally, a fault management system should be able to correctly identify events and automatically take action, either launching a program or script to take corrective action, or activating notification software that allows a human to take proper intervention (i.e. send e-mail or SMS text to a mobile phone). Some notification systems also have escalation rules that will notify a chain of individuals based on availability and severity of alarm.
Types.
There are two primary ways to perform fault management - these are active and passive. Passive fault management is done by collecting alarms from devices (normally via SNMP(simple network management protocol)) when something happens in the devices. In this mode, the fault management system only knows if a device it is monitoring is intelligent enough to generate an error and report it to the management tool. However, if the device being monitored fails completely or locks up, it won't throw an alarm and the problem will not be detected. Active fault management addresses this issue by actively monitoring devices via tools such as ping to determine if the device is active and responding. If the device stops responding, active monitoring will throw an alarm showing the device as unavailable and allows for the proactive correction of the problem.
Fault management includes any tools or procedure for testing, diagnosing or repairing the network when a failure occurs.

</doc>
<doc id="41145" url="https://en.wikipedia.org/wiki?curid=41145" title="FCC (disambiguation)">
FCC (disambiguation)

__NOTOC__
FCC may refer to:

</doc>
<doc id="41146" url="https://en.wikipedia.org/wiki?curid=41146" title="FCC registration program">
FCC registration program

In telecommunication, FCC registration program is the Federal Communications Commission (FCC) program and associated directives intended to assure that all connected terminal equipment and protective circuitry will not harm the public switched telephone network or certain private line services. 
"Note 1:" The FCC registration program requires the registering of terminal equipment and protective circuitry in accordance with Subpart C of part 68, Title 47 of the "Code of Federal Regulations." This includes the assignment of identification numbers to the equipment and the testing of the equipment. 
"Note 2:" The FCC registration program contains no requirement that accepted terminal equipment be compatible with, or function with, the network. 

</doc>
<doc id="41147" url="https://en.wikipedia.org/wiki?curid=41147" title="Feed">
Feed

Feed or The Feed may refer to:

</doc>
<doc id="41148" url="https://en.wikipedia.org/wiki?curid=41148" title="Optical amplifier">
Optical amplifier

An optical amplifier is a device that amplifies an optical signal directly, without the need to first convert it to an electrical signal. An optical amplifier may be thought of as a laser without an optical cavity, or one in which feedback from the cavity is suppressed. Optical amplifiers are important in optical communication and laser physics.
There are several different physical mechanisms that can be used to amplify a light signal, which correspond to the major types of optical amplifiers. In doped fibre amplifiers and bulk lasers, stimulated emission in the amplifier's gain medium causes amplification of incoming light. In semiconductor optical amplifiers (SOAs), electron-hole recombination occurs. In Raman amplifiers, Raman scattering of incoming light with phonons in the lattice of the gain medium produces photons coherent with the incoming photons. Parametric amplifiers use parametric amplification.
Laser amplifiers.
Almost any laser active gain medium can be pumped to produce gain for light at the wavelength of a laser made with the same material as its gain medium. Such amplifiers are commonly used to produce high power laser systems. Special types such as regenerative amplifiers and chirped-pulse amplifiers are used to amplify ultrashort pulses.
Doped fibre amplifiers.
Doped fibre amplifiers (DFAs) are optical amplifiers that use a doped optical fibre as a gain medium to amplify an optical signal. They are related to fibre lasers. The signal to be amplified and a pump laser are multiplexed into the doped fibre, and the signal is amplified through interaction with the doping ions. The most common example is the Erbium Doped Fibre Amplifier (EDFA), where the core of a silica fibre is doped with trivalent erbium ions and can be efficiently pumped with a laser at a wavelength of 980 nm or 1,480 nm, and exhibits gain in the 1,550 nm region.
An "erbium-doped waveguide amplifier" ("EDWA") is an optical amplifier that uses a waveguide to boost an optical signal.
Amplification is achieved by stimulated emission of photons from dopant ions in the doped fibre. The pump laser excites ions into a higher energy from where they can decay via stimulated emission of a photon at the signal wavelength back to a lower energy level. The excited ions can also decay spontaneously (spontaneous emission) or even through nonradiative processes involving interactions with phonons of the glass matrix. These last two decay mechanisms compete with stimulated emission reducing the efficiency of light amplification.
The "amplification window" of an optical amplifier is the range of optical wavelengths for which the amplifier yields a usable gain. The amplification window is determined by the spectroscopic properties of the dopant ions, the glass structure of the optical fibre, and the wavelength and power of the pump laser.
Although the electronic transitions of an isolated ion are very well defined, broadening of the energy levels occurs when the ions are incorporated into the glass of the optical fibre and thus the amplification window is also broadened. This broadening is both homogeneous (all ions exhibit the same broadened spectrum) and inhomogeneous (different ions in different glass locations exhibit different spectra). Homogeneous broadening arises from the interactions with phonons of the glass, while inhomogeneous broadening is caused by differences in the glass sites where different ions are hosted. Different sites expose ions to different local electric fields, which shifts the energy levels via the Stark effect. In addition, the Stark effect also removes the degeneracy of energy states having the same total angular momentum (specified by the quantum number J). Thus, for example, the trivalent erbium ion (Er+3) has a ground state with J = 15/2, and in the presence of an electric field splits into J + 1/2 = 8 sublevels with slightly different energies. The first excited state has J = 13/2 and therefore a Stark manifold with 7 sublevels. Transitions from the J = 13/2 excited state to the J= 15/2 ground state are responsible for the gain at 1.5 µm wavelength. The gain spectrum of the EDFA has several peaks that are smeared by the above broadening mechanisms. The net result is a very broad spectrum (30 nm in silica, typically). The broad gain-bandwidth of fibre amplifiers make them particularly useful in 
wavelength-division multiplexed communications systems as a single amplifier can be utilized to amplify all signals being carried on a fibre and whose wavelengths fall within the gain window.
Basic principle of EDFA.
A relatively high-powered beam of light is mixed with the input signal using a wavelength selective coupler (WSC). The input signal and the excitation light must be at significantly different wavelengths.
The mixed light is guided into a section of fibre with erbium ions included in the core.
This high-powered light beam excites the erbium ions to their higher-energy state.
When the photons belonging to the signal at a different wavelength from the pump light meet the excited erbium atoms, the erbium atoms give up some of their energy to the signal and return to their lower-energy state.
A significant point is that the erbium gives up its energy in the form of additional photons which are exactly in the same phase and direction as the signal being amplified. So the signal is amplified along its direction of travel only. This is not unusual - when an atom “lases” it always gives up its energy in the same direction and phase as the incoming light. Thus all of the additional signal power is guided in the same fibre mode as the incoming signal. There is usually an isolator placed at the output to prevent reflections returning from the attached fibre. Such reflections disrupt amplifier operation and in the extreme case can cause the amplifier to become a laser. The erbium doped amplifier is a high gain amplifier.
Noise.
The principal source of noise in DFAs is Amplified Spontaneous Emission (ASE), which has a spectrum approximately the same as the gain spectrum of the amplifier. Noise figure in an ideal DFA is 3 dB, while practical amplifiers can have noise figure as large as 6–8 dB.
As well as decaying via stimulated emission, electrons in the upper energy level can also decay by spontaneous emission, which occurs at random, depending upon the glass structure and inversion level. Photons are emitted spontaneously in all directions, but a proportion of those will be emitted in a direction that falls within the numerical aperture of the fibre and are thus captured and guided by the fibre. Those photons captured may then interact with other dopant ions, and are thus amplified by stimulated emission. The initial spontaneous emission is therefore amplified in the same manner as the signals, hence the term "Amplified Spontaneous Emission". ASE is emitted by the amplifier in both the forward and reverse directions, but only the forward ASE is a direct concern to system performance since that noise will co-propagate with the signal to the receiver where it degrades system performance. Counter-propagating ASE can, however, lead to degradation of the amplifier's performance since the ASE can deplete the inversion level and thereby reduce the gain of the amplifier.
Gain saturation.
Gain is achieved in a DFA due to population inversion of the dopant ions. The inversion level of a DFA is set, primarily, by the power of the pump wavelength and the power at the amplified wavelengths. As the signal power increases, or the pump power decreases, the inversion level will reduce and thereby the gain of the amplifier will be reduced. This effect is known as gain saturation – as the signal level increases, the amplifier saturates and cannot produce any more output power, and therefore the gain reduces. Saturation is also commonly known as gain compression.
To achieve optimum noise performance DFAs are operated under a significant amount of gain compression (10 dB typically), since that reduces the rate of spontaneous emission, thereby reducing ASE. Another advantage of operating the DFA in the gain saturation region is that small fluctuations in the input signal power are reduced in the output amplified signal: smaller input signal powers experience larger (less saturated) gain, while larger input powers see less gain.
The leading edge of the pulse is amplified, until the saturation energy of the gain medium is reached. In some condition, the width (FWHM) of the pulse is reduced.
Inhomogeneous broadening effects.
Due to the inhomogeneous portion of the linewidth broadening of the dopant ions, the gain spectrum has an inhomogeneous component and gain saturation occurs, to a small extent, in an inhomogeneous manner. This effect is known as "spectral hole burning" because a high power signal at one wavelength can 'burn' a hole in the gain for wavelengths close to that signal by saturation of the inhomogeneously broadened ions. Spectral holes vary in width depending on the characteristics of the optical fibre in question and the power of the burning signal, but are typically less than 1 nm at the short wavelength end of the C-band, and a few nm at the long wavelength end of the C-band. The depth of the holes are very small, though, making it difficult to observe in practice.
Polarization effects.
Although the DFA is essentially a polarization independent amplifier, a small proportion of the dopant ions interact preferentially with certain polarizations and a small dependence on the polarization of the input signal may occur (typically < 0.5 dB). This is called Polarization Dependent Gain (PDG). 
The absorption and emission cross sections of the ions can be modeled as ellipsoids with the major axes aligned at random in all directions in different glass sites. The random distribution of the orientation of the ellipsoids in a glass produces a macroscopically isotropic medium, but a strong pump laser induces an anisotropic distribution by selectively exciting those ions that are more aligned with the optical field vector of the pump. Also, those excited ions aligned with the signal field produce more stimulated emission. The change in gain is thus dependent on the alignment of the polarizations of the pump and signal lasers – i.e. whether the two lasers are interacting with the same sub-set of dopant ions or not. 
In an ideal doped fibre without birefringence, the PDG would be inconveniently large. Fortunately, in optical fibres small amounts of birefringence are always present and, furthermore, the fast and slow axes vary randomly along the fibre length. A typical DFA has several tens of meters, long enough to already show this randomness of the birefringence axes. These two combined effects (which in transmission fibres give rise to polarization mode dispersion) produce a misalignment of the relative polarizations of the signal and pump lasers along the fibre, thus tending to average out the PDG. The result is that PDG is very difficult to observe in a single amplifier (but is noticeable in links with several cascaded amplifiers).
Erbium-doped optical fibre amplifiers.
The erbium-doped fibre amplifier (EDFA) is the most deployed fibre amplifier as its amplification window coincides with the third transmission window of silica-based optical fibre.
Two bands have developed in the third transmission window – the "Conventional", or C-band, from approximately 1525 nm – 1565 nm, and the "Long", or L-band, from approximately 1570 nm to 1610 nm. Both of these bands can be amplified by EDFAs, but it is normal to use two different amplifiers, each optimized for one of the bands.
The principal difference between C- and L-band amplifiers is that a longer length of doped fibre is used in L-band amplifiers. The longer length of fibre allows a lower inversion level to be used, thereby giving at longer wavelengths (due to the band-structure of Erbium in silica) while still providing a useful amount of gain.
EDFAs have two commonly-used pumping bands – 980 nm and 1480 nm. The 980 nm band has a higher absorption cross-section and is generally used where low-noise performance is required. The absorption band is relatively narrow and so wavelength stabilised laser sources are typically needed. The 1480 nm band has a lower, but broader, absorption cross-section and is generally used for higher power amplifiers. A combination of 980 nm and 1480 nm pumping is generally utilised in amplifiers.
The optical fibre amplifier was invented by H. J. Shaw and Michel Digonnet at Stanford University, California, in the early 1980s. The EDFA was first demonstrated several years later by a group including David N. Payne, R. Mears, I.M Jauncey and L. Reekie, from the University of Southampton in collaboration with a group from AT&T Bell Laboratories, E. Desurvire, P. Becker, and J. Simpson and the Italian Company Pirelli . The dual-stage optical amplifier which enabled Dense Wave Division Multiplexing (DWDM,) was invented by Stephen B. Alexander at Ciena Corporation.
Doped fibre amplifiers for other wavelength ranges.
Thulium doped fibre amplifiers have been used in the S-band (1450–1490 nm) and Praseodymium doped amplifiers in the 1300 nm region. However, those regions have not seen any significant commercial use so far and so those amplifiers have not been the subject of as much development as the EDFA. However, Ytterbium doped fibre lasers and amplifiers, operating near 1 micrometre wavelength, have many applications in industrial processing of materials, as these devices can be made with extremely high output power (tens of kilowatts).
Semiconductor optical amplifier.
Semiconductor optical amplifiers (SOAs) are amplifiers which use a semiconductor to provide the gain medium. These amplifiers have a similar structure to Fabry–Pérot laser diodes but with anti-reflection design elements at the end faces. Recent designs include anti-reflective coatings and tilted wave guide and window regions which can reduce end face reflection to less than 0.001%. Since this creates a loss of power from the cavity which is greater than the gain, it prevents the amplifier from acting as a laser. Another type of SOA consists of two regions. One part has a structure of a Fabry-Pérot laser diode and the other has a tapered geometry in order to reduce the power density on the output facet.
Semiconductor optical amplifiers are typically made from group III-V compound semiconductors such as GaAs/AlGaAs, InP/InGaAs, InP/InGaAsP and InP/InAlGaAs, though any direct band gap semiconductors such as II-VI could conceivably be used. Such amplifiers are often used in telecommunication systems in the form of fibre-pigtailed components, operating at signal wavelengths between 0.85 µm and 1.6 µm and generating gains of up to 30 dB.
The semiconductor optical amplifier is of small size and electrically pumped. It can be potentially less expensive than the EDFA and can be integrated with semiconductor lasers, modulators, etc. However, the performance is still not comparable with the EDFA. The SOA has higher noise, lower gain, moderate polarization dependence and high nonlinearity with fast transient time. The main advantage of SOA is that all four types of nonlinear operations (cross gain modulation, cross phase modulation, wavelength conversion and four wave mixing) can be conducted. Furthermore, SOA can be run with a low power laser.
This originates from the short nanosecond or less upper state lifetime, so that the gain reacts rapidly to changes of pump or signal power and the changes of gain also cause phase changes which can distort the signals.
This nonlinearity presents the most severe problem for optical communication applications. However it provides the possibility for gain in different wavelength regions from the EDFA. "Linear optical amplifiers" using gain-clamping techniques have been developed.
High optical nonlinearity makes semiconductor amplifiers attractive for all optical signal processing like all-optical switching and wavelength conversion. There has been much research on semiconductor optical amplifiers as elements for optical signal processing, wavelength conversion, clock recovery, signal demultiplexing, and pattern recognition.
Vertical-cavity SOA.
A recent addition to the SOA family is the vertical-cavity SOA (VCSOA). These devices are similar in structure to, and share many features with, vertical-cavity surface-emitting lasers (VCSELs). The major difference when comparing VCSOAs and VCSELs is the reduced mirror reflectivities used in the amplifier cavity. With VCSOAs, reduced feedback is necessary to prevent the device from reaching lasing threshold. Due to the extremely short cavity length, and correspondingly thin gain medium, these devices exhibit very low single-pass gain (typically on the order of a few percent) and also a very large free spectral range (FSR). The small single-pass gain requires relatively high mirror reflectivities to boost the total signal gain. In addition to boosting the total signal gain, the use of the resonant cavity structure results in a very narrow gain bandwidth; coupled with the large FSR of the optical cavity, this effectively limits operation of the VCSOA to single-channel amplification. Thus, VCSOAs can be seen as amplifying filters.
Given their vertical-cavity geometry, VCSOAs are resonant cavity optical amplifiers that operate with the input/output signal entering/exiting normal to the wafer surface. In addition to their small size, the surface normal operation of VCSOAs leads to a number of advantages, including low power consumption, low noise figure, polarization insensitive gain, and the ability to fabricate high fill factor two-dimensional arrays on a single semiconductor chip. These devices are still in the early stages of research, though promising preamplifier results have been demonstrated. Further extensions to VCSOA technology are the demonstration of wavelength tunable devices. These MEMS-tunable vertical-cavity SOAs utilize a microelectromechanical systems (MEMS) based tuning mechanism for wide and continuous tuning of the peak gain wavelength of the amplifier. SOAs have a more rapid gain response, which is in the order of 1 to 100 ps.
Tapered amplifiers.
For high output power and broader wavelength range, tapered amplifiers are used. These amplifiers consist of a lateral single-mode section and a section with a tapered structure, where the laser light is amplified. The tapered structure leads to a reduction of the power density at the output facet.
Typical parameters:
Raman amplifier.
In a Raman amplifier, the signal is intensified by Raman amplification. Unlike the EDFA and SOA the amplification effect is achieved by a nonlinear interaction between the signal and a pump laser within an optical fibre. There are two types of Raman amplifier: distributed and lumped. A distributed Raman amplifier is one in which the transmission fibre is utilised as the gain medium by multiplexing a pump wavelength with signal wavelength, while a lumped Raman amplifier utilises a dedicated, shorter length of fibre to provide amplification. In the case of a lumped Raman amplifier highly nonlinear fibre with a small core is utilised to increase the interaction between signal and pump wavelengths and thereby reduce the length of fibre required.
The pump light may be coupled into the transmission fibre in the same direction as the signal (co-directional pumping), in the opposite direction (contra-directional pumping) or both. Contra-directional pumping is more common as the transfer of noise from the pump to the signal is reduced.
The pump power required for Raman amplification is higher than that required by the EDFA, with in excess of 500 mW being required to achieve useful levels of gain in a distributed amplifier. Lumped amplifiers, where the pump light can be safely contained to avoid safety implications of high optical powers, may use over 1 W of optical power.
The principal advantage of Raman amplification is its ability to provide distributed amplification within the transmission fibre, thereby increasing the length of spans between amplifier and regeneration sites. The amplification bandwidth of Raman amplifiers is defined by the pump wavelengths utilised and so amplification can be provided over wider, and different, regions than may be possible with other amplifier types which rely on dopants and device design to define the amplification 'window'.
Raman amplifiers have some fundamental advantages. First, Raman gain exists in every fiber, which provides a cost-effective means of upgrading from the terminal ends. Second, the gain is nonresonant, which means that gain is available over the entire transparency region of the fiber ranging from approximately 0.3 to 2μm. A third advantage of Raman amplifiers is that the gain spectrum can be tailored by adjusting the pump wavelengths. For instance, multiple pump lines can be used to increase the optical bandwidth, and the pump distribution determines the gain flatness. Another advantage of Raman amplification is that it is a relatively broad-band amplifier with a bandwidth > 5 THz, and the gain is reasonably flat over a wide wavelength range.
However, a number of challenges for Raman amplifiers prevented their earlier adoption. First, compared to the EDFAs, Raman amplifiers have relatively poor pumping efficiency at lower signal powers. Although a disadvantage, this lack of pump efficiency also makes gain clamping easier in Raman amplifiers. Second, Raman amplifiers require a longer gain fiber. However, this disadvantage can be mitigated by combining gain and the dispersion compensation in a single fiber. A third disadvantage of Raman amplifiers is a fast response time, which gives rise to new sources of noise, as further discussed below. Finally, there are concerns of nonlinear penalty in the amplifier for the WDM signal channels.
"Note: The text of an earlier version of this article was taken from the public domain Federal Standard 1037C."
Optical parametric amplifier.
An optical parametric amplifier allows the amplification of a weak signal-impulse in a noncentrosymmetric nonlinear medium (e.g. Beta barium borate (BBO)). In contrast to the previously mentioned amplifiers, which are mostly used in telecommunication environments, this type finds its main application in expanding the frequency tunability of ultrafast solid-state lasers (e.g. Ti:sapphire). By using a noncollinear interaction geometry optical parametric amplifiers are capable of extremely broad amplification bandwidths.
Recent achievements.
The adoption of high power fiber lasers as an industrial material processing tool has been ongoing for several years and is now expanding into other markets including the medical and scientific markets. One key enhancement enabling penetration into the scientific market has been the improvements in high finesse fiber amplifiers, which are now capable of delivering single frequency linewidths (<5 kHz) together with excellent beam quality and stable linearly polarized output. Systems meeting these specifications, have steadily progressed in the last few years from a few Watts of output power, initially to the 10s of Watts and now into the 100s of Watts power level. This power scaling has been achieved with developments in the fiber technology, such as the adoption of stimulated brillouin scattering (SBS) suppression/mitigation techniques within the fiber, along with improvements in the overall amplifier design. The latest generation of high finesse, high power fiber amplifiers now deliver power levels exceeding what is available from commercial solid-state single frequency sources and are opening up new scientific applications as a result of the higher power levels and stable optimized performance.
Implementations.
There are several simulation tools that can be used to design optical amplifiers. Popular commercial tools have been developed by Optiwave Systems and VPI Systems.

</doc>
<doc id="41149" url="https://en.wikipedia.org/wiki?curid=41149" title="Fiber Distributed Data Interface">
Fiber Distributed Data Interface

Fiber Distributed Data Interface (FDDI) is a standard for data transmission in a local area network.
It uses optical fiber as its standard underlying physical medium, although it was also later specified to use copper cable, in which case it may be called CDDI (Copper Distributed Data Interface), standardized as TP-PMD (Twisted-Pair Physical Medium-Dependent), also referred to as TP-DDI (Twisted-Pair Distributed Data Interface). 
Description.
FDDI provides a 100 Mbit/s optical standard for data transmission in local area network that can extend in range up to . Although FDDI logical topology is a ring-based token network, it did not use the IEEE 802.5 token ring protocol as its basis; instead, its protocol was derived from the IEEE 802.4 token bus "timed token" protocol. In addition to covering large geographical areas, FDDI local area networks can support thousands of users. FDDI offers both a Dual-Attached Station (DAS), counter-rotating token ring topology and a Single-Attached Station (SAS), token bus passing ring topology.
FDDI, as a product of American National Standards Institute X3T9.5 (now X3T12), conforms to the Open Systems Interconnection (OSI) model of functional layering using other protocols. The standards process started in the mid 1980s.
FDDI-II, a version of FDDI described in 1989, added circuit-switched service capability to the network so that it could also handle voice and video signals. Work started to connect FDDI networks to synchronous optical networking (SONET) technology.
A FDDI network contains two rings, one as a secondary backup in case the primary ring fails. The primary ring offers up to 100 Mbit/s capacity. When a network has no requirement for the secondary ring to do backup, it can also carry data, extending capacity to 200 Mbit/s. The single ring can extend the maximum distance; a dual ring can extend . FDDI had a larger maximum-frame size (4,352 bytes) than the standard Ethernet family, which only supports a maximum-frame size of 1,500 bytes, allowing better effective data rates in some cases.
Topology.
Designers normally constructed FDDI rings in a network topology such as a "dual ring of trees". A small number of devices, typically infrastructure devices such as routers and concentrators rather than host computers, were "dual-attached" to both rings. Host computers then connect as single-attached devices to the routers or concentrators. The dual ring in its most degenerate form simply collapses into a single device. Typically, a computer-room contained the whole dual ring, although some implementations deployed FDDI as a metropolitan area network.
FDDI requires this network topology because the dual ring actually passes through each connected device and requires each such device to remain continuously operational.
The standard actually allows for optical bypasses, but network engineers consider these unreliable and error-prone. Devices such as workstations and minicomputers that might not come under the control of the network managers are not suitable for connection to the dual ring.
As an alternative to using a dual-attached connection, a workstation can obtain the same degree of resilience through a dual-homed connection made simultaneously to two separate devices in the same FDDI ring. One of the connections becomes active while the other one is automatically blocked. If the first connection fails, the backup link takes over with no perceptible delay.
Frame format.
The FDDI data frame format is:
Where PA is the preamble, SD is a start delimiter, FC is frame control, DA is the destination address, SA is the source address, PDU is the protocol data unit (or packet data unit), FCS is the frame check Sequence (or checksum), and ED/FS are the end delimiter and frame status.
The Internet Engineering Task Force defined a standard for transmission of the Internet Protocol (which would be the protocol data unit in this case) over FDDI.
It was first proposed in June 1989 and revised in 1990.
Some aspects of the protocol were compatible with the IEEE 802.2 standard for logical link control. For example, the 48-bit MAC addresses that became popular with the Ethernet family. Thus other protocols such as the Address Resolution Protocol (ARP) could be common as well.
Deployment.
FDDI was considered an attractive campus backbone network technology in the early to mid 1990s since existing Ethernet networks only offered 10 Mbit/s data rates and token ring networks only offered 4 Mbit/s or 16 Mbit/s rates. Thus it was a relatively high-speed choice of that era.
By 1994, vendors included Cisco Systems, National Semiconductor, Network Peripherals, SysKonnect (acquired by Marvell Technology Group), and 3Com.
FDDI was effectively made obsolete in local networks by Fast Ethernet which offered the same 100 Mbit/s speeds, but at a much lower cost and, since 1998, by Gigabit Ethernet due to its speed, and even lower cost, and ubiquity.
Standards.
FDDI standards included:

</doc>
<doc id="41150" url="https://en.wikipedia.org/wiki?curid=41150" title="Field strength">
Field strength

In physics, field strength (also signal strength) means either the "magnitude" of a vector-valued field (e.g., in volts per meter, V/m, for an electric field E) or its square, the "intensity" (in watts per square meter, W/m2, for E as above). 
For example, electromagnetic field results in both electric field strength and magnetic field strength.
As an application, in radio frequency telecommunications, the signal strength excites a receiving antenna and thereby induce a voltage at a specific frequency and polarization in order to provide an input signal to a radio receiver. Field strength meters are used for such applications as cellular, broadcasting, wi-fi and a wide variety of other radio-related applications.

</doc>
<doc id="41151" url="https://en.wikipedia.org/wiki?curid=41151" title="File server">
File server

In computing, a file server (or fileserver) is a computer attached to a network that has the primary purpose of providing a location for shared disk access, i.e. shared storage of computer files (such as documents, sound files, photographs, movies, images, databases, etc.) that can be accessed by the workstations that are attached to the same computer network. The term "server" highlights the role of the machine in the client–server scheme, where the "clients" are the workstations using the storage. A file server is not intended to perform computational tasks, and does not run programs on behalf of its clients.
It is designed primarily to enable the storage and retrieval of data while the computation is carried out by the workstations.
File servers are commonly found in schools and offices, where users use a LAN to connect their client computers.
Types of file servers.
A file server may be dedicated or non-dedicated. A dedicated server is designed specifically for use as a file server, with workstations attached for reading and writing files and databases.
File servers may also be categorized by the method of access: Internet file servers are frequently accessed by File Transfer Protocol (FTP) or by HTTP (but are different from web servers, that often provide dynamic web content in addition to static files). Servers on a LAN are usually accessed by SMB/CIFS protocol (Windows and Unix-like) or NFS protocol (Unix-like systems).
Database servers, that provide access to a shared database via a database device driver, are "not" regarded as file servers as they may require Record locking.
Design of file servers.
In modern businesses the design of file servers is complicated by competing demands for storage space, access speed, recoverability, ease of administration, security, and budget. This is further complicated by a constantly changing environment, where new hardware and technology rapidly obsolesces old equipment, and yet must seamlessly come online in a fashion compatible with the older machinery. To manage throughput, peak loads, and response time, vendors may utilize queuing theory to model how the combination of hardware and software will respond over various levels of demand. Servers may also employ dynamic load balancing scheme to distribute requests across various pieces of hardware.
The primary piece of hardware equipment for servers over the last couple of decades has proven to be the hard disk drive. Although other forms of storage are viable (such as magnetic tape and solid-state drives) disk drives have continued to offer the best fit for cost, performance, and capacity.
Storage.
Since the crucial function of a file server is storage, technology has been developed to operate multiple disk drives together as a team, forming a disk array. A disk array typically has cache (temporary memory storage that is faster than the magnetic disks), as well as advanced functions like RAID and storage virtualization. Typically disk arrays increase level of availability by using redundant components other than RAID, such as power supplies. Disk arrays may be consolidated or virtualized in a SAN.
Network-attached storage.
Network-attached storage (NAS) is file-level computer data storage connected to a computer network providing data access to a heterogeneous group of clients. NAS devices specifically are distinguished from file servers generally in a NAS being a computer appliance – a specialized computer built from the ground up for serving files – rather than a general purpose computer being used for serving files (possibly with other functions). In discussions of NASs, the term "file server" generally stands for a contrasting term, referring to general purpose computers only.
NAS systems are networked appliances containing one or more hard drives, often arranged into logical, redundant storage containers or RAID arrays. Network Attached Storage removes the responsibility of file serving from other servers on the network. They typically provide access to files using network file sharing protocols such as NFS, SMB/CIFS (Server Message Block/Common Internet File System), or AFP.
Security.
File servers generally offer some form of system security to limit access to files to specific users or groups. In large organizations, this is a task usually delegated to what is known as directory services such as openLDAP, Novell's eDirectory or Microsoft's Active Directory.
These servers work within the hierarchical computing environment which treat users, computers, applications and files as distinct but related entities on the network and grant access based on user or group credentials. In many cases, the directory service spans many file servers, potentially hundreds for large organizations. In the past, and in smaller organizations, authentication could take place directly at the server itself.

</doc>
<doc id="41152" url="https://en.wikipedia.org/wiki?curid=41152" title="Filled cable">
Filled cable

In telecommunication, a filled cable is a cable that has a non-hygroscopic material, usually a gel called icky-pick, inside the jacket or sheath. 
The nonhygroscopic material fills the spaces between the interior parts of the cable, preventing moisture from entering minor leaks in the sheath and migrating inside the cable. 
A metallic cable, such as a coaxial cable or a metal waveguide, filled with a dielectric material, is not considered as a filled cable.
Further reading.
See Telcordia GR-421-CORE, "Generic Requirements for Metallic Telecommunications Cables," for filled, polyolefin-insulated conductor (PIC) cable requirements.

</doc>
<doc id="41155" url="https://en.wikipedia.org/wiki?curid=41155" title="Firmware">
Firmware

In electronic systems and computing, firmware is a type of software that provides control, monitoring and data manipulation of engineered products and systems. Typical examples of devices containing firmware are embedded systems (such as traffic lights, consumer appliances, remote controls and digital watches), computers, computer peripherals, mobile phones, and digital cameras. The firmware contained in these devices provides the low-level control program for the device. As of 2013, most firmware can be updated.
Firmware is held in non-volatile memory devices such as ROM, EPROM, or flash memory. Changing the firmware of a device may rarely or never be done during its economic lifetime; some firmware memory devices are permanently installed and cannot be changed after manufacture. Common reasons for updating firmware include fixing bugs or adding features to the device. This may require ROM integrated circuits to be physically replaced, or flash memory to be reprogrammed through a special procedure. Firmware such as the ROM BIOS of a personal computer may contain only elementary basic functions of a device and may only provide services to higher-level software. Firmware such as the program of an embedded system may be the only program that will run on the system and provide all of its functions.
Before integrated circuits, other firmware devices included a discrete semiconductor diode matrix. The Apollo guidance computer had firmware consisting of a specially manufactured core memory plane, called "core rope memory", where data were stored by physically threading wires through (1) or around (0) the core storing each data bit.
History.
Ascher Opler coined the term "firmware" in a 1967 "Datamation" article. Originally, it meant the contents of a writable control store (a small specialized high speed memory), containing microcode that defined and implemented the computer's instruction set, and that could be reloaded to specialize or modify the instructions that the central processing unit (CPU) could execute. As originally used, firmware contrasted with hardware (the CPU itself) and software (normal instructions executing on a CPU). It was not composed of CPU machine instructions, but of lower-level microcode involved in the implementation of machine instructions. It existed on the boundary between hardware and software; thus the name "firmware". Over time, popular usage extended the word "firmware" to denote anything ROM-resident, including processor machine instructions for BIOS, bootstrap loaders, or specialized applications.
Until the mid-1990s, updating firmware typically involved replacing a storage medium containing firmware, usually a socketed ROM integrated circuit. Flash memory allows firmware to be updated without physically removing an integrated circuit from the system. An error during the update process may make the device non-functional, or "bricked", which the system is vulnerable to when the parts of flash storage containing the core software or update program are erased and reprogrammed. If the update process is interrupted abruptly, this core software may not be operational to run the device and retry the update, which can be avoided by having a protected read-only section of the flash storage that holds the core software. The downside for the manufacturer is that this read-only software cannot be corrected in the field, so it must be tested to a very high degree before the release.
Applications.
Personal computers.
In some respects, the various firmware components are as important as the operating system in a working computer. However, unlike most modern operating systems, firmware rarely has a well-evolved automatic mechanism of updating itself to fix any functionality issues detected after shipping the unit.
The BIOS may be "manually" updated by a user, using a small utility program. In contrast, firmware in storage devices (harddisks, DVD drives, flash storage) rarely gets updated, even when flash (rather than ROM) storage is used for the firmware; there are no standardized mechanisms for detecting or updating firmware versions.
Most computer peripherals are themselves special-purpose computers. Devices such as printers, scanners, cameras and USB flash drives have internally stored firmware; some devices may also permit field upgrading of their firmware.
Some low-cost peripherals no longer contain non-volatile memory for firmware, and instead rely on the host system to transfer the device control program from a disk file or CD.
Automobiles.
Since 1996 most automobiles have employed an on-board computer and various sensors to detect mechanical problems. , modern vehicles also employ computer-controlled ABS systems and computer-operated transmission control units (TCUs). The driver can also get in-dash information while driving in this manner, such as real-time fuel economy and tire pressure readings. Local dealers can update most vehicle firmware.
Examples.
Examples of firmware include:
Flashing.
Flashing involves the overwriting of existing firmware or data, contained in EEPROM or flash memory modules present in an electronic device, with new data. This can be done to upgrade a device or to change the provider of a service associated with the function of the device, such as changing from one mobile phone service provider to another or installing a new operating system. If firmware is upgradable, it is often done via a program from the provider, and will often allow the old firmware to be saved before upgrading so it can be reverted to if the process fails, or if the newer version performs worse.
Firmware hacking.
Sometimes, third parties create an unofficial new or modified ("aftermarket") version of firmware to provide new features or to unlock hidden functionality; this is referred to as custom firmware (also "Custom Firmware" in the video game console community). An example is Rockbox as a firmware replacement for portable media players. There are many homebrew projects for video game consoles, which often unlock general-purpose computing functionality in previously limited devices (e.g., running Doom on iPods).
Firmware hacks usually take advantage of the firmware update facility on many devices to install or run themselves. Some, however, must resort to exploits in order to run, because the manufacturer has attempted to lock the hardware to stop it from running unlicensed code.
Most firmware hacks are free software.
HDD firmware hacks.
The Moscow-based Kaspersky Lab discovered that a group of developers it refers to as the "Equation Group" has developed hard disk drive firmware modifications for various drive models, containing a trojan horse that allows data to be stored on the drive in locations that will not be erased even if the drive is formatted or wiped. Although the Kaspersky Lab report did not explicitly claim that this group is part of the United States National Security Agency (NSA), evidence obtained from the code of various Equation Group software suggests that they are part of the NSA.
Researchers from the Kaspersky Lab categorized the undertakings by Equation Group as the most advanced hacking operation ever uncovered, also documenting around 500 infections caused by the Equation Group in at least 42 countries.
Security risks.
Mark Shuttleworth, founder of the Ubuntu Linux distribution, has described proprietary firmware as a security risk, saying that "firmware on your device is the NSA's best friend" and calling firmware "a trojan horse of monumental proportions". He has asserted that low-quality, nonfree firmware is a major threat to system security: "Your biggest mistake is to assume that the NSA is the only institution abusing this position of trust in fact, it's reasonable to assume that all firmware is a cesspool of insecurity, courtesy of incompetence of the highest degree from manufacturers, and competence of the highest degree from a very wide range of such agencies". As a potential solution to this problem, he has called for declarative firmware, which would describe "hardware linkage and dependencies" and "should not include executable code".
Custom firmware hacks have also focused on injecting malware into devices such as smartphones or USB devices. One such smartphone injection was demonstrated on the Symbian OS at MalCon, a hacker convention. A USB device firmware hack called "BadUSB" was presented at Black Hat USA 2014 conference, demonstrating how a USB flash drive microcontroller can be reprogrammed to spoof various other device types in order to take control of a computer, exfiltrate data, or spy on the user. Other security researchers have worked further on how to exploit the principles behind BadUSB, releasing at the same time the source code of hacking tools that can be used to modify the behavior of different USB devices.

</doc>
<doc id="41156" url="https://en.wikipedia.org/wiki?curid=41156" title="Fixed access">
Fixed access

Fixed access: In personal communications service (PCS), terminal access to a network in which there is a set relationship between a terminal and the access interface. A single "identifier" serves for both the access interface and the terminal. If the terminal moves to another access interface, that terminal assumes the identity of the new interface.

</doc>
<doc id="41157" url="https://en.wikipedia.org/wiki?curid=41157" title="Flag sequence">
Flag sequence

Flag sequence: In data transmission or processing, a sequence of bits used to delimit, "i.e." mark the beginning and end of a frame. 
"Note 1:" An 8-bit sequence is usually used as the flag sequence; for example, the 8-bit flag sequence 01111110. 
"Note 2:" Flag sequences are used in bit-oriented protocols, such as Advanced Data Communication Control Procedures (ADCCP), Synchronous Data Link Control (SDLC), and High-Level Data Link Control (HDLC).

</doc>
<doc id="41158" url="https://en.wikipedia.org/wiki?curid=41158" title="Flat weighting">
Flat weighting

In a noise-measuring set, flat weighting is a noise weighting based on an amplitude-frequency characteristic that is flat over a frequency range that must be stated. 

</doc>
<doc id="41159" url="https://en.wikipedia.org/wiki?curid=41159" title="Flood search routing">
Flood search routing

In a telephone network, flood search routing is non-deterministic routing in which a dialed number received at a switch is transmitted to all switches, "i.e.," flooded, in the area code directly connected to that switch; if the dialed number is not an affiliated subscriber at that switch, the number is then retransmitted to all directly connected switches, and then routed through the switch that has the dialed number corresponding to the particular user end instrument affiliated with it. All digits of the numbering plan are used to identify a particular subscriber. Flood search routing allows subscribers to have telephone numbers independent of switch codes. Flood search routing provides the highest probability that a telephone call will go through even though a number of switches and links fail.
Flood search routing is used in military telecommunication systems, such as the mobile subscriber equipment (MSE) system.

</doc>
<doc id="41160" url="https://en.wikipedia.org/wiki?curid=41160" title="Flutter (electronics and communication)">
Flutter (electronics and communication)

In electronics and communication, flutter is the rapid variation of signal parameters, such as amplitude, phase, and frequency. Examples of electronic flutter are:
Aeroelastic flutter.
In the field of mechanics and structures, Aeroelastic flutter is an aeroelastic phenomenon where a body's own aerodynamic forces couple with its natural mode of vibration to produce rapid periodic motion.
Aeroelastic flutter occurs under steady flow conditions, when a structure's aerodynamic forces are affected by and in turn affect the movement of the structure. This sets up a positive feedback loop exciting the structure's free vibration. Flutter is self-starting and results in large amplitude vibration which often lead to rapid failure.
The aerodynamic conditions required for flutter vary with the structure's external design and flexibility, but can range from very low velocities to supersonic flows. Large or flexible structures such as pipes, suspension bridges, chimneys and tall buildings are prone to flutter. Designing to avoid flutter is a fundamental requirement for rigid airfoils (fixed wing aircraft and helicopters) as well as for aircraft propellers and gas turbine blades.
Prediction of flutter prior to modern unsteady computational fluid dynamics was based on empirical testing. As a result, many pioneering designs failed due to unforeseen vibrations. The most famous of these was the opening of the original Tacoma Narrows Suspension Bridge in mid 1940, which failed spectacularly 4 months later during a sustained 67 km/h crosswind and became known as Galloping Gertie for its flutter movement.
During the 1950s over 100 incidents were recorded of military or civilian aircraft being lost or damaged due to unforeseen flutter events. While as recently as the 1990s jet engine flutter has grounded military aircraft.
Techniques to avoid flutter include changes to the structure's aerodynamics, stiffening the structure to change the excitation frequency and increasing the damping within the structure.
See also.
"Electronic Flutter"
"Structural Flutter"

</doc>
<doc id="41161" url="https://en.wikipedia.org/wiki?curid=41161" title="Flywheel effect">
Flywheel effect

The flywheel effect is the continuation of oscillations in an oscillator circuit after the control stimulus has been removed. This is usually caused by interacting inductive and capacitive elements in the oscillator. Circuits undergoing such oscillations are said to be flywheeling.
The flywheel effect may be desirable, such as in phase-locked loops used in synchronous systems, or undesirable, such as in voltage-controlled oscillators.
Flywheel effect is used in class C modulation where efficiency of modulation can be achieved as high as 90%.

</doc>
<doc id="41164" url="https://en.wikipedia.org/wiki?curid=41164" title="Foreign exchange service">
Foreign exchange service

Foreign exchange service may refer to:

</doc>
<doc id="41165" url="https://en.wikipedia.org/wiki?curid=41165" title="Foreign instrumentation signals intelligence">
Foreign instrumentation signals intelligence

In telecommunication, the term foreign instrumentation signals intelligence (FISINT) has the following meanings: 
1. Intelligence information derived from electromagnetic emissions associated with the testing and operational deployment of foreign aerospace, surface, and subsurface systems. 
2. Technical information and intelligence information derived from the intercept of foreign instrumentation signals by other than the intended recipients. Foreign instrumentation signals intelligence is a category of signals intelligence. 
Foreign instrumentation signals include but are not limited to signals from telemetry, beaconry, electronic interrogators, tracking/fusing/arming/firing command systems, and video data links.

</doc>
<doc id="41166" url="https://en.wikipedia.org/wiki?curid=41166" title="Forward echo">
Forward echo

Forward echo: In a transmission line, a reflection propagating in the same direction as the original wave and consisting of energy reflected back by one discontinuity and then forward again by another discontinuity. Forward echoes can be supported by reflections caused by splices or other discontinuities in the transmission medium (e.g. optical fiber, twisted pair, or coaxial tube). In metallic lines, they may be supported by impedance mismatches between the source or load and the characteristic impedance of the transmission medium. They may cause attenuation distortion.

</doc>
<doc id="41167" url="https://en.wikipedia.org/wiki?curid=41167" title="Forward error correction">
Forward error correction

In telecommunication, information theory, and coding theory, forward error correction (FEC) or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels.
The central idea is the sender encodes the message in a redundant way by using an error-correcting code (ECC).
The American mathematician Richard Hamming pioneered this field in the 1940s and invented the first error-correcting code in 1950: the Hamming (7,4) code.
The redundancy allows the receiver to detect a limited number of errors that may occur anywhere in the message, and often to correct these errors without retransmission. FEC gives the receiver the ability to correct errors without needing a reverse channel to request retransmission of data, but at the cost of a fixed, higher forward channel bandwidth. FEC is therefore applied in situations where retransmissions are costly or impossible, such as one-way communication links and when transmitting to multiple receivers in multicast. FEC information is usually added to mass storage devices to enable recovery of corrupted data, and is widely used in modems.
FEC processing in a receiver may be applied to a digital bit stream or in the demodulation of a digitally modulated carrier. For the latter, FEC is an integral part of the initial analog-to-digital conversion in the receiver. The Viterbi decoder implements a soft-decision algorithm to demodulate digital data from an analog signal corrupted by noise. Many FEC coders can also generate a bit-error rate (BER) signal which can be used as feedback to fine-tune the analog receiving electronics.
The noisy-channel coding theorem establishes bounds on the theoretical maximum information transfer rate of a channel with some given noise level.
Some advanced FEC systems come very close to the theoretical maximum.
The maximum fractions of errors or of missing bits that can be corrected is determined by the design of the FEC code, so different forward error correcting codes are suitable for different conditions.
How it works.
FEC is accomplished by adding redundancy to the transmitted information using an algorithm. A redundant bit may be a complex function of many original information bits. The original information may or may not appear literally in the encoded output; codes that include the unmodified input in the output are systematic, while those that do not are non-systematic.
A simplistic example of FEC is to transmit each data bit 3 times, which is known as a (3,1) repetition code. Through a noisy channel, a receiver might see 8 versions of the output, see table below.
This allows an error in any one of the three samples to be corrected by "majority vote" or "democratic voting". The correcting ability of this FEC is:
Though simple to implement and widely used, this triple modular redundancy is a relatively inefficient FEC. Better FEC codes typically examine the last several dozen, or even the last several hundred, previously received bits to determine how to decode the current small handful of bits (typically in groups of 2 to 8 bits).
Averaging noise to reduce errors.
FEC could be said to work by "averaging noise"; since each data bit affects many transmitted symbols, the corruption of some symbols by noise usually allows the original user data to be extracted from the other, uncorrupted received symbols that also depend on the same user data.
Most telecommunication systems use a fixed channel code designed to tolerate the expected worst-case bit error rate, and then fail to work at all if the bit error rate is ever worse.
However, some systems adapt to the given channel error conditions: some instances of hybrid automatic repeat-request use a fixed FEC method as long as the FEC can handle the error rate, then switch to ARQ when the error rate gets too high;
adaptive modulation and coding uses a variety of FEC rates, adding more error-correction bits per packet when there are higher error rates in the channel, or taking them out when they are not needed.
Types of FEC.
The two main categories of FEC codes are block codes and convolutional codes.
There are many types of block codes, but among the classical ones the most notable is Reed-Solomon coding because of its widespread use on the Compact disc, the DVD, and in hard disk drives. Other examples of classical block codes include Golay, BCH, Multidimensional parity, and Hamming codes.
Hamming ECC is commonly used to correct NAND flash memory errors.
This provides single-bit error correction and 2-bit error detection.
Hamming codes are only suitable for more reliable single level cell (SLC) NAND.
Denser multi level cell (MLC) NAND requires stronger multi-bit correcting ECC such as BCH or Reed–Solomon.
NOR Flash typically does not use any error correction.
Classical block codes are usually decoded using hard-decision algorithms, which means that for every input and output signal a hard decision is made whether it corresponds to a one or a zero bit. In contrast, convolutional codes are typically decoded using soft-decision algorithms like the Viterbi, MAP or BCJR algorithms, which process (discretized) analog signals, and which allow for much higher error-correction performance than hard-decision decoding.
Nearly all classical block codes apply the algebraic properties of finite fields. Hence classical block codes are often referred to as algebraic codes.
In contrast to classical block codes that often specify an error-detecting or error-correcting ability, many modern block codes such as LDPC codes lack such guarantees. Instead, modern codes are evaluated in terms of their bit error rates.
Most forward error correction correct only bit-flips, but not bit-insertions or bit-deletions.
In this setting, the Hamming distance is the appropriate way to measure the bit error rate.
A few forward error correction codes are designed to correct bit-insertions and bit-deletions, such as Marker Codes and Watermark Codes.
The Levenshtein distance is a more appropriate way to measure the bit error rate when using such codes.
Concatenated FEC codes for improved performance.
Classical (algebraic) block codes and convolutional codes are frequently combined in concatenated coding schemes in which a short constraint-length Viterbi-decoded convolutional code does most of the work and a block code (usually Reed-Solomon) with larger symbol size and block length "mops up" any errors made by the convolutional decoder. Single pass decoding with this family of error correction codes can yield very low error rates, but for long range transmission conditions (like deep space) iterative decoding is recommended.
Concatenated codes have been standard practice in satellite and deep space communications since Voyager 2 first used the technique in its 1986 encounter with Uranus. The Galileo craft used iterative concatenated codes to compensate for the very high error rate conditions caused by having a failed antenna.
Low-density parity-check (LDPC).
Low-density parity-check (LDPC) codes are a class of recently re-discovered highly efficient linear block
codes made from many single parity check (SPC) codes. They can provide performance very close to the channel capacity (the theoretical maximum) using an iterated soft-decision decoding approach, at linear time complexity in terms of their block length. Practical implementations rely heavily on decoding the constituent SPC codes in parallel.
LDPC codes were first introduced by Robert G. Gallager in his PhD thesis in 1960,
but due to the computational effort in implementing encoder and decoder and the introduction of Reed–Solomon codes,
they were mostly ignored until recently.
LDPC codes are now used in many recent high-speed communication standards, such as DVB-S2 (Digital video broadcasting), WiMAX (IEEE 802.16e standard for microwave communications), High-Speed Wireless LAN (IEEE 802.11n), 10GBase-T Ethernet (802.3an) and G.hn/G.9960 (ITU-T Standard for networking over power lines, phone lines and coaxial cable). Other LDPC codes are standardized for wireless communication standards within 3GPP MBMS (see fountain codes).
Turbo codes.
Turbo coding is an iterated soft-decoding scheme that combines two or more relatively simple convolutional codes and an interleaver to produce a block code that can perform to within a fraction of a decibel of the Shannon limit. Predating LDPC codes in terms of practical application, they now provide similar performance.
One of the earliest commercial applications of turbo coding was the CDMA2000 1x (TIA IS-2000) digital cellular technology developed by Qualcomm and sold by Verizon Wireless, Sprint, and other carriers. It is also used for the evolution of CDMA2000 1x specifically for Internet access, 1xEV-DO (TIA IS-856). Like 1x, EV-DO was developed by Qualcomm, and is sold by Verizon Wireless, Sprint, and other carriers (Verizon's marketing name for 1xEV-DO is "Broadband Access", Sprint's consumer and business marketing names for 1xEV-DO are "Power Vision" and "Mobile Broadband", respectively).
Local decoding and testing of codes.
Sometimes it is only necessary to decode single bits of the message, or to check whether a given signal is a codeword, and do so without looking at the entire signal. This can make sense in a streaming setting, where codewords are too large to be classically decoded fast enough and where only a few bits of the message are of interest for now. Also such codes have become an important tool in computational complexity theory, e.g., for the design of probabilistically checkable proofs.
Locally decodable codes are error-correcting codes for which single bits of the message can be probabilistically recovered by only looking at a small (say constant) number of positions of a codeword, even after the codeword has been corrupted at some constant fraction of positions. Locally testable codes are error-correcting codes for which it can be checked probabilistically whether a signal is close to a codeword by only looking at a small number of positions of the signal.
Interleaving.
Interleaving is frequently used in digital communication and storage systems to improve the performance of forward error correcting codes. Many communication channels are not memoryless: errors typically occur in bursts rather than independently. If the number of errors within a code word exceeds the error-correcting code's capability, it fails to recover the original code word. Interleaving ameliorates this problem by shuffling source symbols across several code words, thereby creating a more uniform distribution of errors. Therefore, interleaving is widely used for burst error-correction.
The analysis of modern iterated codes, like turbo codes and LDPC codes, typically assumes an independent distribution of errors. Systems using LDPC codes therefore typically employ additional interleaving across the symbols within a code word.
For turbo codes, an interleaver is an integral component and its proper design is crucial for good performance. The iterative decoding algorithm works best when there are not short cycles in the factor graph that represents the decoder; the interleaver is chosen to avoid short cycles.
Interleaver designs include:
In multi-carrier communication systems, interleaving across carriers may be employed to provide frequency diversity, e.g., to mitigate frequency-selective fading or narrowband interference.
Example.
Transmission without interleaving:
Here, each group of the same letter represents a 4-bit one-bit error-correcting codeword. The codeword cccc is altered in one bit and can be corrected, but the codeword dddd is altered in three bits, so either it cannot be decoded at all or it might be decoded incorrectly.
With interleaving:
In each of the codewords aaaa, eeee, ffff, gggg, only one bit is altered, so one-bit error-correcting code will decode everything correctly.
Transmission without interleaving:
The term "AnExample" ends up mostly unintelligible and difficult to correct.
With interleaving:
No word is completely lost and the missing letters can be recovered with minimal guesswork.
Disadvantages of interleaving.
Use of interleaving techniques increases total delay. This is because the entire interleaved block must be received before the packets can be decoded. Also interleavers hide the structure of errors; without an interleaver, more advanced decoding algorithms can take advantage of the error structure and achieve more reliable communication than a simpler decoder combined with an interleaver.

</doc>
<doc id="41168" url="https://en.wikipedia.org/wiki?curid=41168" title="Forward scatter">
Forward scatter

In telecommunication and astronomy, forward scatter is the deflection—by diffraction, nonhomogeneous refraction, or nonspecular reflection by particulate matter of dimensions that are large with respect to the wavelength in question but small with respect to the beam diameter—of a portion of an incident electromagnetic wave, in such a manner that the energy so deflected propagates in a direction that is within 90° of the direction of propagation of the incident wave (i.e., the phase angle is greater than 90°). 
The scattering process may be polarization-sensitive, "i.e.", incident waves that are identical in every respect but their polarization may be scattered differently.
Comets.
Forward scattering can make a back-lit comet appear significantly brighter because the dust and ice crystals are reflecting and enhancing the apparent brightness of the comet by scattering that light towards the observer. Comets studied forward-scattering in visible-thermal photometry include C/1927 X1 (Skjellerup–Maristany), C/1975 V1 (West), and C/1980 Y1 (Bradfield). Comets studied forward-scattering in SOHO non-thermal C3 coronograph photometry include 96P/Machholz and C/2004 F4 (Bradfield). The brightness of the great comets C/2006 P1 (McNaught) and Comet Skjellerup–Maristany near perihelion were enhanced by forward scattering.

</doc>
<doc id="41169" url="https://en.wikipedia.org/wiki?curid=41169" title="Frequency of optimum transmission">
Frequency of optimum transmission

Frequency of optimum transmission (FOT), in the transmission of radio waves via ionospheric reflection, is the highest effective (i.e. working) frequency that is predicted to be usable for a specified path and time for 90% of the days of the month. The FOT is normally just below the value of the maximum usable frequency (MUF). In the prediction of usable frequencies, the FOT is commonly taken as 15% below the monthly median value of the MUF for the specified time and path. 
The FOT is usually the most effective frequency for ionospheric reflection of radio waves between two specified points on Earth. 
Synonyms for this term include:

</doc>
<doc id="41170" url="https://en.wikipedia.org/wiki?curid=41170" title="Four-wire circuit">
Four-wire circuit

In telecommunication, a four-wire circuit is a two-way circuit using two paths so arranged that the respective signals are transmitted in one direction only by one path and in the other direction by the other path. Late in the 20th century, almost all connections between telephone exchanges were four-wire circuits, while conventional phone lines into residences and businesses were two-wire circuits.
The four-wire circuit gets its name from the fact that, historically, a balanced pair of conductors were used in each of two directions for full-duplex operation. The name may still be applied to, for example, optical fibers, even though only one fiber is required for transmission in each direction. A system can separate the frequency directions by frequency duplex and realize the benefits of a four-wire circuit even while the same wire pair is used in both directions.

</doc>
<doc id="41171" url="https://en.wikipedia.org/wiki?curid=41171" title="Four-wire terminating set">
Four-wire terminating set

A four-wire terminating set (4WTS) is a balanced transformer used to perform a conversion between four-wire and two-wire operation in telecommunication systems.
For example, a 4-wire circuit may, by means of a 4-wire terminating set, be connected to a 2-wire telephone set. Also, a pair of 4-wire terminating sets may be used to introduce an intermediate 4-wire circuit into a 2-wire circuit, in which loop repeaters may be situated to amplify signals in each direction without positive feedback and oscillation. 
The 4WTS differs from a simple hybrid coil in being equipped to adjust its impedance to maximize return loss.
Four-wire terminating sets were largely supplanted by resistance hybrids in the late 20th century.

</doc>
<doc id="41172" url="https://en.wikipedia.org/wiki?curid=41172" title="Frame (networking)">
Frame (networking)

A frame is a digital data transmission unit in computer networking and telecommunication. A frame typically includes frame synchronization features consisting of a sequence of bits or symbols that indicate to the receiver the beginning and end of the payload data within the stream of symbols or bits it receives. If a receiver is connected to the system in the middle of a frame transmission, it ignores the data until it detects a new frame synchronization sequence. 
In the OSI model of computer networking, a frame is the protocol data unit at the data link layer. Frames are the result of the final layer of encapsulation before the data is transmitted over the physical layer. A frame is "the unit of transmission in a link layer protocol, and consists of a link layer header followed by a packet."
Each frame is separated from the next by an interframe gap.
A frame is a series of bits generally composed of framing bits, the packet payload, and a frame check sequence.
Examples are Ethernet frames, Point-to-Point Protocol (PPP) frames, Fibre Channel frames, and V.42 modem frames.
In telecommunications, specifically in time-division multiplex (TDM) and time-division multiple access (TDMA) variants, a frame is a cyclically repeated data block that consists of a fixed number of time slots, one for each logical TDM channel or TDMA transmitter. In this context, a frame is typically an entity at the physical layer. TDM application examples are SONET/SDH and the ISDN circuit switched B-channel, while TDMA examples are the 2G and 3G circuit-switched cellular voice services. The frame is also an entity for time-division duplex, where the mobile terminal may transmit during some timeslots and receive during others.
Often frames of several different sizes are nested inside each other.
For example, when people use Point-to-Point Protocol over asynchronous serial communication, the 8 bits of each individual byte are framed by start and stop bits,
the payload data bytes in a network packet are framed by the header and footer,
and several packets can be framed with

</doc>
<doc id="41173" url="https://en.wikipedia.org/wiki?curid=41173" title="Frame rate">
Frame rate

Frame rate, also known as frame frequency, is the frequency (rate) at which an imaging device displays consecutive images called frames. The term applies equally to film and video cameras, computer graphics, and motion capture systems. Frame rate is expressed in frames per second (FPS).
Frame rate and human vision.
The temporal sensitivity and resolution of human vision varies depending on the type and characteristics of visual stimulus, and it differs between individuals. The human visual system can process 10 to 12 separate images per second and perceive them individually, and sequences at higher rates are perceived as motion. Modulated light (such as a computer display) is perceived as stable by the majority of participants in studies when the rate is higher than 50 Hz through 90 Hz. This perception of modulated light as steady is known as the flicker fusion threshold. However, when the modulated light is non-uniform and contains an image, the flicker fusion threshold can be much higher. With regard to image recognition, people have been found to recognize a specific image in an unbroken series of different images, each of which lasts as little as 13 milliseconds. Persistence of vision sometimes accounts for very short single-millisecond visual stimulus having a perceived duration of between 100 ms and 400 ms. Multiple stimuli that are very short are sometimes perceived as a single stimulus, such as a 10 ms green flash of light immediately followed by a 10 ms red flash of light perceived as a single yellow flash of light.
Film.
Silent films.
Early silent films had stated frame rates anywhere from 16 to 24 FPS, but since the cameras were hand-cranked, the rate often changed during the scene to fit the mood. Projectionists could also change the frame rate in the theater by adjusting a rheostat controlling the voltage powering the film-carrying mechanism in the projector. Silent films were often intended to be shown at higher frame rates than those used during filming. These frame rates were enough for the sense of motion, but it was perceived as jerky motion. By using projectors with dual- and triple-blade shutters, the rate was multiplied two or three times as seen by the audience. Thomas Edison said that 46 frames per second was the minimum needed by the visual cortex: "Anything less will strain the eye." In the mid to late 1920s, the frame rate for silent films increased to between 20 and 26 FPS.
Sound films.
When sound film was introduced in 1926, variations in film speed were no longer tolerated as the human ear is more sensitive to changes in audio frequency. Many theaters had shown silent films at 22 to 26 FPS which is why 24 FPS was chosen for sound. From 1927 to 1930, as various studios updated equipment, the rate of 24 FPS became standard for 35 mm sound film. At 24 FPS the film travels through the projector at a rate of per second. This allowed for simple two-blade shutters to give a projected series of images at 48 per second, satisfying Edison's recommendation. Many modern 35 mm film projectors use three-blade shutters to give 72 images per second—each frame is flashed on screen three times.
Digital video and television standards.
There are three main frame rate standards in the TV and digital cinema business: 24p, 25p, and 30p. However, there are many variations on these as well as newer emerging standards. Frames per second are often expressed as hertz (1 Hz = 1 Fps).

</doc>
<doc id="41174" url="https://en.wikipedia.org/wiki?curid=41174" title="Constitution of Vermont">
Constitution of Vermont

The Constitution of the State of Vermont is the fundamental body of law of the U.S. State of Vermont. It was adopted in 1793 following Vermont's admission to the Union in 1791 and is largely based upon the 1777 Constitution of the Vermont Republic which was ratified at Windsor in the Old Constitution House and amended in 1786. At 8,295 words, it is the shortest U.S. state constitution.
History.
1777.
From 1777 to 1791, Vermont was an independent country, often referred to in the present day as the Vermont Republic. During that time it was usually called the State of Vermont but sometimes called the Commonwealth of Vermont or the Republic of Vermont. Its first constitution, drafted in 1777, was among the most far-reaching in guaranteeing personal freedoms and individual rights. In particular, it banned adult slavery, saying male slaves become free at the age of 21 and females at 18. The 1777 constitution's Declaration of Rights of the Inhabitants of the State of Vermont anticipated the United States Bill of Rights by a dozen years. The first chapter, a "Declaration of Rights of the Inhabitants of the State of Vermont", is followed by a "Plan or Frame of Government" outlining the structure of governance. It provided that the governor would be elected by the freemen of the state, who could vote regardless of whether they owned property, that each town would be represented in the legislative assembly, that there would be a court of law in each county, and that the legislative assembly and the governor's council would jointly hold legislative power.
1786.
In 1786, the Constitution was extensively revised to establish a far greater separation of powers than what had prevailed under the 1777 Constitution. In particular, it forbade anyone to simultaneously hold more than one of certain offices, including those of judges, legislators, members of the governor's council, the governor, and the surveyor-general. It also provided that the legislature could no longer function as a court of appeals nor otherwise intervene in cases before the courts, as it had often done.
The 1786 Constitution continued in effect when, in 1791, Vermont made the transition from independence to the status of one of the states of the Union. In particular, the governor, the members of the governor's council, and other officers of the state, including judges in all courts, simply continued their terms of office that were already underway.
1793.
The 1793 Constitution was adopted two years after Vermont's admission to the Union and continues in effect, with various later amendments, to this day. It eliminated all mention of grievances against King George III and against the State of New York. In 1790, New York's legislature finally renounced its claims that Vermont was a part of New York, the cessation of those claims being effective if and when Congress decided to admit Vermont to the Union.
Council of Censors.
"In order that the freedom of this Commonwealth may be preserved inviolate" the 1777 constitution established a Council of Censors. This body consisted of thirteen elected members, chosen every seven years, but not from the Council or General Assembly. They were to check that "the legislative and executive branches of government have performed their duty as guardians of the people". They also had the power to call a convention, if needed, to amend the constitution. This council had been based on a similar element of the Pennsylvania Constitution of 1776.
In 1786, the constitution was amended with language proposed by the 1785 Council of Censors, their first meeting, and adopted by the 1786 Constitutional Convention. The section on the Council of Censors remained generally unchanged, with only an added clarification of scope.
In 1793, the constitution was amended with language proposed by the 1792 Council of Censors and adopted by the 1793 Constitutional Convention. The Council now had the "power to send for persons, papers, and records".
In 1870, the constitution was amended with language proposed by the 1869 Council of Censors, their last meeting, and adopted by the 1870 Constitutional Convention. The Council of Censors was abolished and replaced by a new procedure to amend the constitution.
Amending the constitution.
The Vermont Constitution, Chapter 2, Section 72 establishes the procedure for amending the constitution. The Vermont General Assembly, the state's bi-cameral legislature, has the sole power to propose amendments to the Constitution of Vermont. The process must be initiated by a Senate that has been elected in an "off-year", that is, an election that does not coincide with the election of the U.S. president. An amendment must originate in the Senate and be approved by a two-thirds vote. It must then receive a majority vote in the House. Then, after a newly elected legislature is seated, the amendment must receive a majority vote in each chamber, first in the Senate, then in the House. The proposed amendment must then be presented to the voters as a referendum and receive a majority of the votes cast.
1990s revision to gender-neutral language.
In 1991 and again in 1993, the Vermont General Assembly approved a constitutiuonal amendment authorizing the justices of the Vermont Supreme Court to revise the Constitution in "gender-inclusive" language, replacing gender-specific terms. (Examples: "men" and "women" were replaced by "persons" and the "Freeman's Oath," which requires all newly registered voters in the state to swear by, was renamed the "Voters' Oath"). The revision was ratified by the voters in the general election of November 8, 1994. Vermont is one of six states whose constitutions are written in gender-neutral language.

</doc>
<doc id="41175" url="https://en.wikipedia.org/wiki?curid=41175" title="Frame slip">
Frame slip

In the reception of framed data, a frame slip is the loss of synchronization between a received frame and the receiver clock signal, causing a frame misalignment event, and resulting in the loss of the data contained in the received frame. 
A frame slip should not be confused with a dropped frame where synchronization is not lost, as in the case of buffer overflow, for example.

</doc>
<doc id="41176" url="https://en.wikipedia.org/wiki?curid=41176" title="Frame synchronization">
Frame synchronization

In telecommunication, frame synchronization or framing is the process by which, while receiving a stream of framed data, incoming frame alignment signals (i.e., a distinctive bit sequences or syncwords) are identified (that is, distinguished from data bits), permitting the data bits within the frame to be extracted for decoding or retransmission.
Framing.
If the transmission is temporarily interrupted, or a bit slip event occurs, the receiver must re-synchronize.
The transmitter and the receiver must agree ahead of time on which frame synchronization scheme they will use.
Common frame synchronization schemes are:
Frame synchronizer.
In telemetry applications, a "frame synchronizer" is used to frame-align a serial pulse code-modulated (PCM) binary stream.
The frame synchronizer immediately follows the bit synchronizer in most telemetry applications. Without frame synchronization, decommutation is impossible.
The frame synchronization pattern is a known binary pattern which repeats at a regular interval within the PCM stream. The frame synchronizer recognizes this pattern and aligns the data into minor frames or sub-frames. Typically the frame sync pattern is followed by a counter (sub-frame ID) which dictates which minor or sub-frame in the series is being transmitted. This becomes increasingly important in the decommutation stage where all data is deciphered as to what attribute was sampled. Different commutations require a constant awareness of which section of the major frame is being decoded.

</doc>
<doc id="41177" url="https://en.wikipedia.org/wiki?curid=41177" title="Framing">
Framing

Framing may refer to:

</doc>
<doc id="41179" url="https://en.wikipedia.org/wiki?curid=41179" title="Free-space path loss">
Free-space path loss

In telecommunication, free-space path loss (FSPL) is the loss in signal strength of an electromagnetic wave that would result from a line-of-sight path through free space (usually air), with no obstacles nearby to cause reflection or diffraction. It is defined in "Standard Definitions of Terms for Antennas", IEEE Std 145-1983, as "The loss between two isotropic radiators in free space, expressed as a power ratio." Usually it is expressed in dB, although the IEEE standard does not say that. So it assumes that the antenna gain is a power ratio of 1.0, or 0 dB. It does not include any loss associated with hardware imperfections, or the effects of any antennas gain. A discussion of these losses may be found in the article on link budget. The FSPL is rarely used standalone, but rather as a part of the Friis transmission equation, which includes the gain of antennas.
Free-space path loss formula.
Free-space path loss is proportional to the square of the distance between the transmitter and receiver, and also proportional to the square of the frequency of the radio signal.
The equation for FSPL is
where:
This equation is only accurate in the far field where spherical spreading can be assumed; it does not hold close to the transmitter.
Free-space path loss in decibels.
A convenient way to express FSPL is in terms of dB:
where the units are as before.
For typical radio applications, it is common to find formula_3 measured in units of GHz and formula_4 in km, in which case the FSPL equation becomes
For formula_10 in meters and kilohertz, respectively, the constant becomes formula_11 .
For formula_10 in meters and megahertz, respectively, the constant becomes formula_13 .
For formula_10 in kilometers and megahertz, respectively, the constant becomes formula_15 .
Physical explanation.
The FSPL expression above often leads to the erroneous belief that free space attenuates an electromagnetic wave according to its frequency. This is not the case, as there is no physical mechanism that could cause this. The expression for FSPL actually encapsulates two effects.
Distance dependency.
Dependency of the FSPL on distance is caused by the spreading out of electromagnetic energy in free space and is described by the inverse square law, i.e.
where:
Note that this is not a frequency-dependent effect.
Frequency dependency.
The frequency dependency is somewhat more confusing. The question is often asked: Why should path loss, which is just a geometric inverse-square loss, be a function of frequency? The answer is that path loss is "defined" on the use of an isotropic receiving antenna (formula_20). This can be seen if we derive the FSPL from the Friis transmission equation.
Hence path loss is a convenient tool; it represents a hypothetical received-power loss that would occur if the receiving antenna were isotropic. Therefore, the FSPL can be viewed as a convenient collection of terms that have been assigned the unfortunate name "path loss". This name calls up an image of purely geometric effect and fails to emphasize the requirement that formula_20. A better choice of the name would have been "unity-gain propagation loss". 
Hence frequency dependency of the path loss is caused by the frequency dependency of the receiving antenna's aperture in case the antenna gain is fixed. Antenna aperture in turn determines how well an antenna can pick up power from an incoming electromagnetic wave.
Dependency of antenna aperture from antenna gain is described by the formula:
This formula represents a well-known fact, that the lower the frequency (the longer the wavelength), the bigger antenna is needed to achieve certain antenna gain. Therefore, for a theoretical isotropic antenna (formula_24), the received power formula_25 is described by a formula:
where formula_27 is a power density of an electromagnetic wave at a location of theoretical isotropic receiving antenna. Note that this is entirely dependent on wavelength, which is how the frequency-dependent behaviour arises.
In simple terms the frequency dependency of the path loss can be explained like this: with the increase of the frequency the requirement to keep the gain of the receiving antenna intact will cause an antenna aperture to be decreased, which will result in less energy being captured with the smaller antenna, which is similar to increasing the path loss in the situation when receiving antenna gain would not have been fixed.

</doc>
<doc id="41180" url="https://en.wikipedia.org/wiki?curid=41180" title="Freeze frame television">
Freeze frame television

Freeze frame television: Television in which fixed ("still") images (the frames of the video) are transmitted sequentially at a rate far too slow to be perceived as continuous motion by human vision. The receiving device typically holds each frame in memory, displaying it until the next complete frame is available.
For an image of specified quality, "e.g.", resolution and color fidelity, freeze-frame television has a lower bandwidth requirement than that of full-motion television. For this reason, NASA, which refers to this technique as "sequential still video", uses it on UHF when Ku band full-motion video signals are not available.

</doc>
<doc id="41181" url="https://en.wikipedia.org/wiki?curid=41181" title="F region">
F region

The F region of the ionosphere is home to the F layer of ionization, also called the Appleton–Barnett layer, after the English physicist Edward Appleton and New Zealander Miles Barnett. As with other ionospheric sectors, 'layer' implies a concentration of plasma, while 'region' is the volume that contains the said layer. The F region contains ionized gases at a height of around 150–800 km above sea level, placing it in the Earth’s thermosphere, a hot region in the upper atmosphere, and also in the heterosphere, where chemical composition varies with height. Generally speaking, the F region has the highest concentration of free electrons and ions anywhere in the atmosphere. It may be thought of as comprising two layers, the F1-and F2-layers.
The F-region is located directly above the E region (formerly the Kennelly-Heaviside layer) and below the protonosphere. It acts as a dependable reflector of radio signals as it is not affected by atmospheric conditions, although its ionic composition varies with the sunspot cycle. It reflects normal-incident frequencies at or below the critical frequency (approximately 10 MHz) and partially absorbs waves of higher frequency.
The F region is the region of the ionosphere which is very important for HF radio wave propagation. This F region is very anomalous in nature.
F1 and F2 layers.
The F1 layer is the lower sector of the F layer and exists from about 150 to 220 km above the surface of the Earth and only during daylight hours. It is composed of a mixture of molecular ions O2+ and NO+, and atomic ions O+. Above the F1 region, atomic oxygen becomes the dominant constituent because lighter particles tend to occupy higher altitudes above the turbopause (at ~100 km). This atomic oxygen provides the O+ atomic ions that make up the F2 layer.
The F1 layer has approximately 5 × 105 e/cm3 (free electrons per cubic centimeter) at noontime and minimum sunspot activity, and increases to roughly 2 × 106 e/cm3 during maximum sunspot activity. The density falls off to below 104 e/cm3 at night.
Usage in Radio Communication.
Critical F2 layer frequencies are the ones that will not go through the F2 layer.

</doc>
<doc id="41183" url="https://en.wikipedia.org/wiki?curid=41183" title="Frequency administration">
Frequency administration

In telecommunication, frequency assignment authority is the power granted for the administration, designation or delegation to an agency or administrator via treaty or law, to specify frequencies, frequency channels or frequency bands, in the electromagnetic spectrum for use in radiocommunication services, radio stations or ISM applications.
Frequency administration is – according to "Article 1.2" of the International Telecommunication Union´s (ITU) Radio Regulations (RR) – defined as "«Any governmental department or service responsible for discharging the obligations undertaken in the Constitution of the International Telecommunication Union, in the Convention of the International Telecommunication Union and in the Administrative Regulations (CS 1002)".» Definitions identical to those contained in the Annex to the Constitution or the Annex to the Convention of the International Telecommunication Union (Geneva, 1992) are marked “(CS)” or “(CV)” respectively. 
International frequency assignment authority is vested in the Radiocommunication Bureau of the International Telecommunication Union (ITU).
United States.
In the United States, primary frequency assignment authority is exercised by the National Telecommunications and Information Administration (NTIA) for the Federal Government and by the Federal Communications Commission (FCC) for non-Federal Government organizations.
Europe.
In Europe each country has regulatory input into the progress of European and international policy, standards, and legislation governing these sectors through their respective "frequency administration".
Frequency management for Europe is driven by a number of organisations. These include the:
In July 2002, the European Commission also established the European Regulators Group for Electronic Communications Networks and Services; creating, for the first time, a formal structure for interaction and coordination between the European Commission and regulators in all EU Member States to ensure consistent application of European legislation.

</doc>
<doc id="41184" url="https://en.wikipedia.org/wiki?curid=41184" title="Frequency averaging">
Frequency averaging

In telecommunication, the term frequency averaging has the following meanings: 
In frequency averaging, all oscillators are assigned equal weight in determining the ultimate network frequency. 
In terms of musical note frequency, the averaging of the frequency of low or high notes in a solo instrumental piece is a technique used to match different instruments together so they may be played together. The musical note frequency calculation formula is used: F=(2^12/n)*440, where n equals the amount of positive or negative steps away from the base note of A4(440 hertz) and F equals the frequency. The formula is used in calculating the frequency of each note in the piece. The values are then added together and divided the amount of notes. This is the average frequency of those notes. It is said that such techniques were used by classical composers, especially those who involved mathematics heavily in their music. 

</doc>
<doc id="41185" url="https://en.wikipedia.org/wiki?curid=41185" title="Frequency-change signaling">
Frequency-change signaling

In telecommunication, frequency-change signaling is a telegraph signaling method in which one or more particular frequencies correspond to each desired signaling condition of a telegraph code. The transition from one set of frequencies to the other may be a continuous or a discontinuous change in the frequency or phase.

</doc>
<doc id="41186" url="https://en.wikipedia.org/wiki?curid=41186" title="Frequency compatibility">
Frequency compatibility

In telecommunication, the term frequency compatibility has the following meanings: 
1. Of an electronic device, the extent to which it will operate at its designed performance level in its intended operational environment (including the presence of interference) without causing interference to other devices. 
2. The degree to which an electrical or electronic device or devices operating on or responding to a specified frequency or frequencies is capable of functioning with other such devices.
See also electromagnetic compatibility

</doc>
<doc id="41187" url="https://en.wikipedia.org/wiki?curid=41187" title="Frequency deviation">
Frequency deviation

Frequency deviation (formula_1) is used in FM radio to describe the maximum instantaneous difference between an FM modulated frequency and the nominal carrier frequency. The term is sometimes mistakenly used as synonymous with frequency drift, which is an unintended offset of an oscillator from its nominal frequency.
The frequency deviation of a radio is of particular importance in relation to bandwidth, because less deviation means that more channels can fit into the same amount of frequency spectrum. The FM broadcasting range (87.5–108 MHz, NOTE: In some countries the 87.5–88.0 MHz part of the band is not used) uses a channel spacing of 200 kHz, with a maximum frequency deviation of 75 kHz, leaving a 25 kHz buffer above the highest and below the lowest frequency to reduce interaction with other channels. AM broadcasting uses a channel spacing of 10 kHz, but with amplitude modulation frequency deviation is irrelevant. 
FM applications use peak deviations of 75 kHz (200 kHz spacing), 5 kHz (25 kHz spacing), 2.5 kHz (12.5 kHz spacing), and 2 kHz (8.33 kHz spacing).

</doc>
<doc id="41188" url="https://en.wikipedia.org/wiki?curid=41188" title="Frequency-exchange signaling">
Frequency-exchange signaling

In telegraphy, frequency-exchange signaling or two-source frequency keying is frequency-change signaling in which the change from one significant condition to another is accompanied by decay in amplitude of one or more frequencies and by buildup in amplitude of one or more other frequencies.
Frequency-exchange signaling applies to supervisory signaling and user-information transmission. 

</doc>
<doc id="41189" url="https://en.wikipedia.org/wiki?curid=41189" title="Frequency frogging">
Frequency frogging

In telecommunication, the term frequency frogging has the following meanings: 
"Note:" Frequency frogging is accomplished by having modulators, which are integrated into specially designed repeaters, translate a low-frequency group to a high-frequency group, and vice versa. A channel will appear in the low group for one repeater section and will then be translated to the high group for the next section because of frequency frogging. This results in nearly constant attenuation with frequency over two successive repeater sections, and eliminates the need for large slope equalization and adjustments. Singing and crosstalk are minimized because the high-level output of a repeater is at a different frequency than the low-level input to other repeaters. It also diminishes group delay distortion. A repeater that receives on the high band from both direction and sends on the low band is called Hi-Lo; the other kind Lo-Hi.

</doc>
<doc id="41191" url="https://en.wikipedia.org/wiki?curid=41191" title="Frequency sharing">
Frequency sharing

In telecommunication, frequency sharing is the assignment to or use of the same radio frequency by two or more stations that are separated geographically or that use the frequency at different times. 
Frequency sharing reduces the potential for mutual interference where the assignment of different frequencies to each user is not practical or possible. 
In a communications net, frequency sharing does not pertain to stations that use the same frequency.

</doc>
<doc id="41192" url="https://en.wikipedia.org/wiki?curid=41192" title="Frequency shift">
Frequency shift

In the physical sciences and in telecommunication, the term frequency shift may refer to:

</doc>
<doc id="41193" url="https://en.wikipedia.org/wiki?curid=41193" title="Frequency-shift keying">
Frequency-shift keying

Frequency-shift keying (FSK) is a frequency modulation scheme in which digital information is transmitted through discrete frequency changes of a carrier signal. The technology is used for communication systems such as amateur radio, caller ID and emergency broadcasts. The simplest FSK is binary FSK (BFSK). BFSK uses a pair of discrete frequencies to transmit binary (0s and 1s) information. With this scheme, the "1" is called the mark frequency and the "0" is called the space frequency. The time domain of an FSK modulated carrier is illustrated in the figures to the right.
Implementations of FSK Modems.
Reference implementations of FSK modems exist and are documented in detail. The demodulation of a binary FSK signal can be done using the Goertzel algorithm very efficiently, even on low-power microcontrollers.
Other forms of FSK.
Continuous-phase frequency-shift keying.
In principle FSK can be implemented by using completely independent free-running oscillators, and switching between them at the beginning of each symbol period.
In general, independent oscillators will not be at the same phase and therefore the same amplitude at the switch-over instant,
causing sudden discontinuities in the transmitted signal.
In practice, many FSK transmitters use only a single oscillator, and the process of switching to a different frequency at the beginning of each symbol period preserves the phase.
The elimination of discontinuities in the phase (and therefore elimination of sudden changes in amplitude) reduces sideband power, reducing interference with neighboring channels.
Gaussian frequency-shift keying.
Rather than directly modulating the frequency with the digital data symbols, "instantaneously" changing the frequency at the beginning of each symbol period, Gaussian frequency-shift keying (GFSK) filters the data pulses with a Gaussian filter to make the transitions smoother. This filter has the advantage of reducing sideband power, reducing interference with neighboring channels, at the cost of increasing intersymbol interference. It is used by DECT, Bluetooth, Cypress WirelessUSB, Nordic Semiconductor, Texas Instruments LPRF, Z-Wave and Wavenis devices. For basic data rate Bluetooth the minimum deviation is 115 kHz.
A GFSK modulator differs from a simple frequency-shift keying modulator in that before the baseband waveform (levels −1 and +1) goes into the FSK modulator, it is passed through a Gaussian filter to make the transitions smoother so to limit its spectral width. Gaussian filtering is a standard way for reducing spectral width; it is called "pulse shaping" in this application.
In ordinary non-filtered FSK, at a jump from −1 to +1 or +1 to −1, the modulated waveform changes rapidly, which introduces large out-of-band spectrum. If we change the pulse going from −1 to +1 as −1, −.98, −.93 ... +.93, +.98, +1, and we use this smoother pulse to determine the carrier frequency, the out-of-band spectrum will be reduced.
Minimum-shift keying.
Minimum frequency-shift keying or minimum-shift keying (MSK) is a particular spectrally efficient form of coherent FSK. In MSK, the difference between the higher and lower frequency is identical to half the bit rate. Consequently, the waveforms that represent a 0 and a 1 bit differ by exactly half a carrier period. The maximum frequency deviation is δ = 0.25 "fm", where "fm" is the maximum modulating frequency. As a result, the modulation index "m" is 0.5. This is the smallest FSK modulation index that can be chosen such that the waveforms for 0 and 1 are orthogonal.
Gaussian minimum shift keying.
A variant of MSK called Gaussian minimum shift keying (GMSK) is used in the GSM mobile phone standard.
Audio FSK.
"Audio frequency-shift keying" (AFSK) is a modulation technique by which digital data is represented by changes in the frequency (pitch) of an audio tone, yielding an encoded signal suitable for transmission via radio or telephone. Normally, the transmitted audio alternates between two tones: one, the "mark", represents a binary one; the other, the "space", represents a binary zero.
AFSK differs from regular frequency-shift keying in performing the modulation at baseband frequencies. In radio applications, the AFSK-modulated signal normally is being used to modulate an RF carrier (using a conventional technique, such as AM or FM) for transmission.
AFSK is not always used for high-speed data communications, since it is far less efficient in both power and bandwidth than most other modulation modes. In addition to its simplicity, however, AFSK has the advantage that encoded signals will pass through AC-coupled links, including most equipment originally designed to carry music or speech.
AFSK is used in the U.S. based Emergency Alert System to notify stations of the type of emergency, locations affected, and the time of issue without actually hearing the text of the alert.
Continuous 4 level FM.
Phase 1 radios in the Project 25 system use continuous 4 level FM (C4FM) modulation.
Applications.
In 1910, Reginald Fessenden invented a two-tone method of transmitting Morse code. Dots and dashes were replaced with different tones of equal length. The intent was to minimize transmission time.
Some early CW transmitters employed an arc converter that could not be conveniently keyed. Instead of turning the arc on and off, the key slightly changed the transmitter frequency in a technique known as the "compensation-wave method". The compensation-wave was not used at the receiver. Spark transmitters used for this method consumed a lot of bandwidth and caused interference, so it was discouraged by 1921.
Most early telephone-line modems used audio frequency-shift keying (AFSK) to send and receive data at rates up to about 1200 bits per second. The Bell 103 and Bell 202 modems used this technique. Even today, North American caller ID uses 1200 baud AFSK in the form of the Bell 202 standard. Some early microcomputers used a specific form of AFSK modulation, the Kansas City standard, to store data on audio cassettes. AFSK is still widely used in amateur radio, as it allows data transmission through unmodified voiceband equipment. 
AFSK is also used in the United States’ Emergency Alert System to transmit warning information. It is used at higher bitrates for Weathercopy used on Weatheradio by NOAA in the U.S.
The CHU shortwave radio station in Ottawa, Canada broadcasts an exclusive digital time signal encoded using AFSK modulation.
FSK is commonly used in Caller ID and remote metering applications: see FSK standards for use in Caller ID and remote metering for more details

</doc>
<doc id="41194" url="https://en.wikipedia.org/wiki?curid=41194" title="Frequency standard">
Frequency standard

A frequency standard is a stable oscillator used for frequency calibration or reference. A frequency standard generates a fundamental frequency with a high degree of accuracy and precision. Harmonics of this fundamental frequency are used to provide reference points.
Since time is the reciprocal of frequency, it is relatively easy to derive a time standard from a frequency standard. A standard clock comprises a frequency standard, a device to count off the cycles of the oscillation emitted by the frequency standard, and a means of displaying or outputting the result.
Frequency standards in a network or facility are sometimes administratively designated as "primary" or "secondary". The terms "primary" and "secondary", as used in this context, should not be confused with the respective technical meanings of these words in the discipline of precise time and frequency.
Frequency reference.
A frequency reference is an instrument used for providing a stable frequency of some kind. There are different sorts of frequency references, acoustic ones such as tuning forks but also electrical ones that emit a signal of a certain frequency (a frequency standard).
Among the most stable frequency references in the world are cesium standards, including cesium fountains, and hydrogen masers. Cesium standards are widely recognized as having better long-term stability, whereas hydrogen masers can attain superior short-term performance; therefore, several national standards laboratories use ensembles of cesium standards and hydrogen masers in order to combine the best attributes of both.
The carrier of time signal transmitters, LORAN-C transmitters and of several long wave and medium wave broadcasting stations is derived from an atomic clock and can be therefore used as frequency standard.

</doc>
<doc id="41196" url="https://en.wikipedia.org/wiki?curid=41196" title="Fresnel zone">
Fresnel zone

A Fresnel zone ( ), named for physicist Augustin-Jean Fresnel, is a series of concentric ellipsoidal regions of alternating double strength and half strength volumes of a wave's propagation, caused by a wave following multiple paths as it passes by an object and is partially refracted by it, resulting in constructive and destructive interference as the different length paths go in and out of phase.
Fresnel zones are seen in optics, radio communications, electrodynamics, seismology, acoustics, gravitational radiation, and other situations involving the radiation of waves and multipath propagation.
This is the cause of the picket-fencing effect when either the radio transmitter or receiver is moving and the high and low signal strength zones are above and below the receiver cut off threshold.
Importance of Fresnel zones.
Fresnel zones are concentric ellipses (1, 2, 3) centred on the direct transmission path (AB).
The 1st zone is the ellipse with chords 1/2 wavelength longer than the direct path (ACB).
If a reflective object is very near the direct path, the signal will experience a 180o phase shift and cancel the direct wave at the receiver.
If a reflective object is tangent to the 1st zone, the electromagnetic wave will be shifted 180o because of the increased path length, undergo an additional 180o phase shift due to the reflection, and reinforce the direct wave at the receiver. Consequently, there should be no reflective objects in the 1st Fresnel zone.
If unobstructed, radio waves will travel in a straight line from the transmitter to the receiver. But if there are reflective surfaces along the path, such as bodies of water or smooth terrain, the radio waves reflecting off those surfaces may arrive either out of phase or in phase with the signals that travel directly to the receiver. Waves that reflect off of surfaces within an even Fresnel zone are out of phase with the direct-path wave and reduce the power of the received signal. Waves that reflect off of surfaces within an odd Fresnel zone are in phase with the direct-path wave and can enhance the power of the received signal. Sometimes this results in the counter-intuitive finding that reducing the height of an antenna increases the signal-to-noise ratio.
Fresnel provided a means to calculate where the zones are—where a given obstacle will cause mostly in phase or mostly out of phase reflections between the transmitter and the receiver. Obstacles in the first Fresnel zone will create signals with a path-length phase shift of 0 to 180 degrees, in the second zone they will be 180 to 360 degrees out of phase, and so on. Even numbered zones have the maximum phase cancelling effect and odd numbered zones may actually add to the signal power.
To maximize receiver strength, one needs to minimize the effect of obstruction loss by removing obstacles from the radio frequency line of sight (RF LoS). The strongest signals are on the direct line between transmitter and receiver and always lie in the first Fresnel zone.
Determining Fresnel zone clearance.
The concept of Fresnel zone clearance may be used to analyze interference by obstacles near the path of a radio beam. The first zone must be kept largely free from obstructions to avoid interfering with the radio reception. However, some obstruction of the Fresnel zones can often be tolerated. 
For establishing Fresnel zones, first determine the RF Line of Sight (RF LoS), which in simple terms is a straight line between the transmitting and receiving antennas. Now the zone surrounding the RF LoS is said to be the Fresnel zone.
The general equation for calculating the Fresnel zone radius at any point P in between the endpoints of the link is the following:
where,
Fn = The nth Fresnel Zone radius in metres
d1 = The distance of P from one end in metres
d2 = The distance of P from the other end in metres
formula_2 = The wavelength of the transmitted signal in metres
The cross sectional radius of each Fresnel zone is the longest at the midpoint of the RF LoS, shrinking to a point at the antenna on each end. For practical applications, it is often useful to know the maximum radius of the first Fresnel zone. From the above formula, the following formulas can be derived, using formula_3, formula_4, and formula_5. Now we have an easy way to calculate the radius of the first Fresnel zone (F1 in the above equation), knowing the distance between the two antennas and the frequency of the transmitted signal.
In SI:

</doc>
<doc id="41197" url="https://en.wikipedia.org/wiki?curid=41197" title="Front-to-back ratio">
Front-to-back ratio

In telecommunication, the term front-to-back ratio ("also known as front-to-rear ratio") can mean:
The ratio compares the antenna gain in a specified direction, "i.e.", azimuth, usually that of maximum gain, to the gain in a direction 180° from the specified azimuth. A front-to-back ratio is usually expressed in dB. 
In point-to-point microwave antennas, a "high performance" antenna usually has a higher front to back ratio than other antennas. For example, an unshrouded 38 GHz microwave dish may have a front to back ratio of 64 dB, while the same size reflector equipped with a shroud would have a front to back ratio of 70 dB. Other factors affecting the front to back ratio of a parabolic microwave antenna include the material of the dish and the precision with which the reflector itself was formed.
In other electrical engineering the front to back ratio is a ratio of parameters used to characterize rectifiers or other devices, in which electric current, signal strength, resistance, or other parameters, in one direction is compared with that in the opposite direction. 

</doc>
<doc id="41199" url="https://en.wikipedia.org/wiki?curid=41199" title="FTS2000">
FTS2000

Federal Telecommunications System 2000 (FTS2000) is a long distance telecommunications service for the United States federal government, including services such as switched voice service for voice or data up to 4.8 kbit/s, switched data at 56 kbit/s and 64 kbit/s, switched digital integrated service for voice, data, image, and video up to 1.544 Mbit/s, packet switched service for data in packet form, video transmission for both compressed and wideband video, and dedicated point-to-point private line for voice and data. 
"Note 1:" Use of FTS2000 contract services is mandatory for use by U.S. Government agencies for all acquisitions subject to 40 U.S.C. 759. 
"Note 2:" No U.S. Government information processing equipment or customer premises equipment other than that which are required to provide an FTS2000 service are furnished. 
"Note 3:" The FTS2000 contractors will be required to provide service directly to an agency's terminal equipment interface. For example, the FTS2000 contractor might provide a terminal adapter to an agency location in order to connect FTS2000 ISDN services to the agency's terminal equipment. 
"Note 4:" GSA awarded two 10-year, fixed-price contracts covering FTS2000 services on December 7, 1988. 
"Note 5:" The Warner Amendment excludes the mandatory use of FTS2000 in instances related to maximum security.
FTS2000 was completed in 2000, then replaced by FTS2001, and thereafter, in 2008, by Networx.

</doc>
<doc id="41200" url="https://en.wikipedia.org/wiki?curid=41200" title="Full width at half maximum">
Full width at half maximum

Full width at half maximum (FWHM) is an expression of the extent of a function given by the difference between the two extreme values of the independent variable at which the dependent variable is equal to half of its maximum value. In other words, it is the width of a spectrum curve measured between those points on the "y"-axis which are half the maximum amplitude.
Half width at half maximum (HWHM) is half of the FWHM.
FWHM is applied to such phenomena as the duration of pulse waveforms and the spectral width of sources used for optical communications and the resolution of spectrometers.
The term full duration at half maximum (FDHM) is preferred when the independent variable is time.
The convention of "width" meaning "half maximum" is also widely used in signal processing to define bandwidth as "width of frequency range where less than half the signal's power is attenuated", i.e., the power is at least half the maximum. In signal processing terms, this is at most −3 dB of attenuation, called "half power point".
If the considered function is the density of a normal distribution of the form
where "σ" is the standard deviation and "x"0 is the expected value, then the relationship between FWHM and the standard deviation is
The width does not depend on the expected value "x"0; it is invariant under translations.
In spectroscopy half the width at half maximum (here "γ"), HWHM, is in common use. For example, a Lorentzian/Cauchy distribution of height can be defined by
Another important distribution function, related to solitons in optics, is the hyperbolic secant:
Any translating element was omitted, since it does not affect the FWHM. For this impulse we have:
where "arsech" is the inverse hyperbolic secant.

</doc>
<doc id="41201" url="https://en.wikipedia.org/wiki?curid=41201" title="Functional profile">
Functional profile

In telecommunication, a functional profile is a standardization document that characterizes the requirements of a standard or group of standards, and specifies how the options and ambiguities in the standard(s) should be interpreted or implemented to (a) provide a particular information technology function, (b) provide for the development of uniform, recognized tests, and (c) promote interoperability among different network elements and terminal equipment that implement a specific profile.

</doc>
<doc id="41202" url="https://en.wikipedia.org/wiki?curid=41202" title="Fuse">
Fuse

Fuse or fuze may refer to:

</doc>
<doc id="41203" url="https://en.wikipedia.org/wiki?curid=41203" title="Garble">
Garble


</doc>
<doc id="41204" url="https://en.wikipedia.org/wiki?curid=41204" title="Gateway">
Gateway

Gateway may refer to:

</doc>
<doc id="41205" url="https://en.wikipedia.org/wiki?curid=41205" title="Gating">
Gating

Gating may refer to:
Other.
and see also

</doc>
<doc id="41206" url="https://en.wikipedia.org/wiki?curid=41206" title="Gaussian beam">
Gaussian beam

In optics, a Gaussian beam is a beam of monochromatic electromagnetic radiation whose transverse magnetic and electric field amplitude profiles are given by the Gaussian function; this also implies a Gaussian intensity (irradiance) profile. This fundamental (or TEM00) transverse gaussian mode describes the intended output of most (but not all) lasers, as such a beam can be focused into the most concentrated spot. When such a beam is refocused by a lens, the transverse "phase" dependence is altered; this results in a "different" Gaussian beam. The electric and magnetic field amplitude profiles along any such circular Gaussian beam (for a given wavelength and polarization) are determined by a single parameter: the so-called waist "w0". At any position "z" relative to the waist (focus) along a beam having a specified "w0", the field amplitudes and phases are thereby determined as detailed below.
The equations below assume a beam with a circular cross-section at all values of "z"; this can be seen by noting that a single transverse dimension, "r", appears. Beams with elliptical cross-sections, or with waists at different positions in "z" for the two transverse dimensions (astigmatic beams) can also be described as Gaussian beams, but with distinct values of "w0" and of the "z=0" location for the two transverse dimensions "x" and "y".
Arbitrary solutions of the paraxial Helmholtz equation can be expressed as combinations of Hermite–Gaussian modes (whose amplitude profiles are separable in "x" and "y" using Cartesian coordinates) or similarly as combinations of Laguerre–Gaussian modes (whose amplitude profiles are separable in "r" and "θ" using cylindrical coordinates). At any point along the beam "z" these modes include the same Gaussian factor as the fundamental Gaussian mode multiplying the additional geometrical factors for the specified mode. However different modes propagate with a different Gouy phase which is why the net transverse profile due to a superposition of modes evolves in "z", whereas the propagation of any "single" Hermite–Gaussian (or Laguerre–Gaussian) mode retains the same form along a beam.
Although there are other possible modal decompositions, these families of solutions are the most useful for problems involving compact beams, that is, where the optical power is rather closely confined along an axis. Even when a laser is "not" operating in the fundamental Gaussian mode, its power will generally be found among the lowest-order modes using these decompositions, as the spatial extent of higher order modes will tend to exceed the bounds of a laser's resonator (cavity). "Gaussian beam" normally implies radiation confined to the fundamental (TEM00) Gaussian mode.
Mathematical form.
The Gaussian beam is a transverse electromagnetic (TEM) mode. The mathematical expression for the electric field amplitude is a solution to the paraxial Helmholtz equation. Assuming polarization in the "x" direction and propagation in the +"z" direction, the electric field in phasor (complex) notation is given by:
where
There is also an understood time dependence formula_11 multiplying such phasor quantities; the actual field at a point in time and space is given by the real part of that complex quantity.
The wave's magnetic field has a slightly more complicated form (it is directed in y since the electric field polarization was stipulated to be in x). It can be found through the Maxwell equation
where formula_13 is the magnetic permeability of the medium. It becomes
where the constant "η" is the characteristic impedance of the medium in which the beam is propagating. For free space, η = η0 ≈ 377 Ω.
Evolving beam width.
At a position "z" along the beam (measured from the focus), the spot size parameter "w" is given by
where
is called the Rayleigh range as further discussed below.
Evolving radius of curvature.
The curvature of the wavefronts is zero at the beam waist and also approaches zero as z → ±∞. It is equal to 1/"R" where "R"("z") is the "radius of curvature" as a function of position along the beam, given by
Gouy phase.
The so-called "Gouy phase" of the beam at "z" is given by:
The Gouy phase results in an increase in the apparent wavelength near the waist. The phase velocity near the waist exceeds the speed of light in the medium, just as it can in a waveguide. The Gouy phase shift along the beam remains within the range ±π/2 (for a fundamental Gaussian beam) and is not observable in most experiments. However it is of theoretical importance and takes on a greater range for higher-order Gaussian modes.
Elliptical and astigmatic beams.
Many laser beams have an elliptical cross-section. Also common are beams with waist positions which are different for the two transverse dimensions, called astigmatic beams. These beams can be dealt with using the above two evolution equations, but with distinct values of each parameter for "x" and "y" and distinct definitions of the "z" = 0 point. The Gouy phase is a single value calculated correctly by summing the contribution from each dimension, with a Gouy phase within the range ± π/4 contributed by each dimension.
An elliptical beam will invert its ellipticity ratio as it propagates from the far field to the waist. The dimension which was the larger far from the waist, will be the smaller near the waist.
Beam parameters.
The geometric dependence of the fields of a Gaussian beam are governed by the light's wavelength λ ("in" the dielectric medium, if not free space) and the following beam parameters, all of which are connected as detailed in the following sections.
Beam waist.
The shape of a Gaussian beam of a given wavelength λ is governed solely by one parameter, the "beam waist" "w"0. This is a measure of the beam size at the point of its focus ("z"=0 in the above equations) where the beam width "w(z)" (as defined above) is the smallest (and likewise where the intensity on-axis ("r"=0) is the largest). From this parameter the other parameters describing the beam geometry are determined. This includes the Rayleigh range "z"R and asymptotic beam divergence θ, as detailed below.
Rayleigh range and confocal parameter.
The "Rayleigh distance" or range "z"R is determined given a Gaussian beam's waist size:
At a distance from the waist equal to the Rayleigh range "z"R, the width "w" of the beam is formula_20 larger than it is at the focus where "w" = "w"0, the beam waist. That also implies that the on-axis ("r=0") intensity there is one half of the peak intensity (at "z"=0). That point along the beam also happens to be where the wavefront curvature (1/"R") is greatest.
The distance between the two points "z" = ±"z"R is called the "confocal parameter" or "depth of focus" of the beam.
Beam divergence.
Although the tails of a Gaussian function never actually reach zero, for the purposes of the following discussion, let us call the "edge" of a beam the radius where "r" = "w(z)". That is where the intensity has dropped to 1/e2 of its on-axis value. Now, for formula_21 the parameter formula_7 increases linearly with formula_3. This means that far from the waist, the beam "edge" (in the above sense) is cone-shaped. The angle between lines along that cone (whose formula_24) and the central axis of the beam (formula_25) is called the "divergence" of the beam. It is given by
The total angular spread of the beam far from the waist is then given by
Because the divergence is inversely proportional to the spot size, for a given wavelength λ, a Gaussian beam that is focused to a small spot diverges rapidly as it propagates away from the focus. Conversely, to "minimize" the divergence of a laser beam in the far field (and increase its peak intensity at large distances) it must have a large crossection ("w"0) at the waist (and thus a large diameter where it is launched, since "w"(z) is never less than "w"0 ). This relationship between beam width and divergence is a fundamental characteristic of diffraction, and of the Fourier transform which describes Fraunhofer diffraction. A beam with any specified amplitude profile also obeys this inverse relationship, but the fundamental Gaussian mode is a special case where the product of beam size at focus and far-field divergence is smaller than for any other case.
Since the gaussian beam model uses the paraxial approximation, it fails when wavefronts are tilted by more than about 30° from the axis of the beam. From the above expression for divergence, this means the Gaussian beam model is only accurate for beams with waists larger than about formula_28.
Laser beam quality is quantified by the beam parameter product (BPP). For a Gaussian beam, the BPP is the product of the beam's divergence and waist size formula_29. The BPP of a real beam is obtained by measuring the beam's minimum diameter and far-field divergence, and taking their product. The ratio of the BPP of the real beam to that of an ideal Gaussian beam at the same wavelength is known as "M"2 ("M squared"). The "M"2 for a Gaussian beam is one. All real laser beams have "M"2 values greater than one, although very high quality beams can have values very close to one.
The numerical aperture of a Gaussian beam is defined to be formula_30, where "n" is the index of refraction of the medium through which the beam propagates. This means that the Rayleigh range is related to the numerical aperture by 
Power and intensity.
Power through an aperture.
The power "P" passing through a circle of radius "r" in the transverse plane at position "z" is
where
is the total power transmitted by the beam.
For a circle of radius formula_34, the fraction of power transmitted through the circle is
Similarly, about 90 percent of the beam's power will flow through a circle of radius formula_36, 95 percent through a circle of radius formula_37, and 99 percent through a circle of radius formula_38.
Peak intensity.
The peak intensity at an axial distance formula_3 from the beam waist can be calculated as the limit of the enclosed power within a circle of radius formula_2, divided by the area of the circle formula_41 as the circle shrinks:
The limit can be evaluated using L'Hôpital's rule:
Complex beam parameter.
The spot size and curvature of a Gaussian beam as a function of "z" along the beam can also be encoded in the complex beam parameter formula_44 given by:
Introducing this complication leads to a simplification of the Gaussian beam field equation as shown below. It can be seen that the reciprocal of "q(z)" contains the wavefront curvature and relative on-axis intensity in its real and imaginary parts, respectively:
The complex beam parameter simplifies the mathematical analysis of Gaussian beam propagation, and especially in the analysis of optical resonator cavities using ray transfer matrices.
Then using this form, the earlier equation for the electric (or magnetic) field is greatly simplified. If we call "u" the relative field strength of an elliptical Gaussian beam (with the elliptical axes in the "x" and "y" directions) then it can be separated in "x" and "y" according to:
where 
where formula_50 and formula_51 are the complex beam parameters in the "x" and "y" directions.
For the common case of a circular beam profile, formula_52 and formula_53, which yields
where in this case the combined mode number "N = |l| + 2p". As before, the transverse amplitude variations are contained in the last two factors on the upper line of the equation, which again includes the basic Gaussian drop off in "r" but now multiplied by a Laguerre polynomial. The effect of the rotational mode number "l", in addition to affecting the Laguerre polynomial, is mainly contained in the "phase" factor "exp(-ilφ)", in which the beam profile is advanced ( or retarded) by "l" complete 2π phases in one rotation around the beam (in φ). This is an example of an optical vortex of topological charge "l", and can be associated with the orbital angular momentum of light in that mode.
Ince-Gaussian modes.
In elliptic coordinates, one can write the higher-order modes using Ince polynomials. The even and odd Ince-Gaussian modes are given by 
where formula_56 and formula_57 are the radial and angular elliptic coordinates
defined by
formula_60 are the even Ince
polynomials of order formula_61 and degree formula_62 where formula_63 is the ellipticity parameter. The Hermite-Gaussian and Laguerre-Gaussian modes are a special case of the Ince-Gaussian modes for formula_64 and formula_65 respectively.
Hypergeometric-Gaussian modes.
There is another important class of paraxial wave modes in cylindrical coordinates in which the complex amplitude is proportional to a confluent hypergeometric function.
These modes have a singular phase profile and are eigenfunctions of the photon orbital angular momentum. Their intensity profiles are characterized by a single brilliant ring; like Laguerre–Gaussian modes, their intensities fall to zero at the center (on the optical axis) except for the fundamental (0,0) mode. A mode's complex amplitude can be written in terms of the normalized (dimensionless) radial coordinate formula_66 and the normalized longitudinal coordinate formula_67. as follows:
where the rotational index formula_70 is an integer, and formula_71 is real valued, formula_72 is the gamma function and formula_73 is a confluent hypergeometric
function.
Some subfamilies of hypergeometric-Gaussian (HyGG) modes can be listed as the modified Bessel-Gaussian modes, the modified exponential Gaussian modes, and the modified Laguerre–Gaussian modes.
The set of hypergeometric-Gaussian modes is overcomplete and is not an orthogonal set of modes. In spite of its complicated field profile, HyGG modes have a very simple profile at the pupil plane (formula_74):
u(\rho,\phi,0) \propto \rho^

</doc>
<doc id="41207" url="https://en.wikipedia.org/wiki?curid=41207" title="Gel">
Gel

A gel is a solid jelly-like material that can have properties ranging from soft and weak to hard and tough. Gels are defined as a substantially dilute cross-linked system, which exhibits no flow when in the steady-state. By weight, gels are mostly liquid, yet they behave like solids due to a three-dimensional cross-linked network within the liquid. It is the crosslinking within the fluid that gives a gel its structure (hardness) and contributes to the adhesive stick (tack). In this way gels are a dispersion of molecules of a liquid within a solid in which the solid is the continuous phase and the liquid is the discontinuous phase. The word "gel" was coined by 19th-century Scottish chemist Thomas Graham by clipping from "gelatine". 
Composition.
Gels consist of a solid three-dimensional network that spans the volume of a liquid medium and ensnares it through surface tension effects. This internal network structure may result from physical bonds (physical gels) or chemical bonds (chemical gels), as well as crystallites or other junctions that remain intact within the extending fluid. Virtually any fluid can be used as an extender including water (hydrogels), oil, and air (aerogel). Both by weight and volume, gels are mostly fluid in composition and thus exhibit densities similar to those of their constituent liquids. Edible jelly is a common example of a hydrogel and has approximately the density of water.
Polyionic polymers.
Polyionic polymers are polymers with an ionic functional group. The ionic charges prevent the formation of tightly coiled polymer chains. This allows them to contribute more to viscosity in their stretched state, because the stretched-out polymer takes up more space. See polyelectrolyte for more information.
Types.
Hydrogels.
A hydrogel is a network of polymer chains that are hydrophilic, sometimes found as a colloidal gel in which water is the dispersion medium. Hydrogels are highly absorbent (they can contain over 90% water) natural or synthetic polymeric networks.
Hydrogels also possess a degree of flexibility very similar to natural tissue, due to their significant water content.
The first appearance of the term 'hydrogel' in the literature was in 1894.
Common uses for hydrogels include:
Other, less common uses include
Common ingredients include polyvinyl alcohol, sodium polyacrylate, acrylate polymers and copolymers with an abundance of hydrophilic groups.
Natural hydrogel materials are being investigated for tissue engineering; these materials include agarose, methylcellulose, hyaluronan, and other naturally derived polymers.
Organogels.
An organogel is a non-crystalline, non-glassy thermoreversible (thermoplastic) solid material composed of a liquid organic phase entrapped in a three-dimensionally cross-linked network. The liquid can be, for example, an organic solvent, mineral oil, or vegetable oil. The solubility and particle dimensions of the structurant are important characteristics for the elastic properties and firmness of the organogel. Often, these systems are based on self-assembly of the structurant molecules. (An example of formation of an undesired thermoreversible network is the occurrence of wax crystallization in petroleum.)
Organogels have potential for use in a number of applications, such as in pharmaceuticals, cosmetics, art conservation, and food.
Xerogels.
A xerogel is a solid formed from a gel by drying with unhindered shrinkage. Xerogels usually retain high porosity (15–50%) and enormous surface area (150–900 m2/g), along with very small pore size (1–10 nm). When solvent removal occurs under supercritical conditions, the network does not shrink and a highly porous, low-density material known as an "aerogel" is produced. Heat treatment of a xerogel at elevated temperature produces viscous sintering (shrinkage of the xerogel due to a small amount of viscous flow) and effectively transforms the porous gel into a dense glass.
Nanocomposite hydrogels.
Nanocomposite hydrogels are also known as hybrid hydrogels, can be defined as highly hydrated polymeric networks, either physically or covalently crosslinked with each other and/or with nanoparticles or nanostructures. Nanocomposite hydrogels can mimic native tissue properties, structure and microenvironment due to their hydrated and interconnected porous structure. A wide range of nanoparticles, such as carbon-based, polymeric, ceramic, and metallic nanomaterials can be incorporated within the hydrogel structure to obtain nanocomposites with tailored functionality. Nanocomposite hydrogels can be engineered to possess superior physical, chemical, electrical, and biological properties.
Properties.
Many gels display thixotropy – they become fluid when agitated, but resolidify when resting.
In general, gels are apparently solid, jelly-like materials. 
By replacing the liquid with gas it is possible to prepare aerogels, materials with exceptional properties including very low density, high specific surface areas, and excellent thermal insulation properties.
Animal-produced gels.
Some species secrete gels that are effective in parasite control. For example, the long-finned pilot whale secretes an enzymatic gel that rests on the outer surface of this animal and helps prevent other organisms from establishing colonies on the surface of these whales' bodies.
Hydrogels existing naturally in the body include mucus, the vitreous humor of the eye, cartilage, tendons and blood clots. Their viscoelastic nature results in the soft tissue component of the body, disparate from the mineral-based hard tissue of the skeletal system. Researchers are actively developing synthetically derived tissue replacement technologies derived from hydrogels, for both temporary implants (degradable) and permanent implants (non-degradable). A review article on the subject discusses the use of hydrogels for nucleus pulposus replacement, cartilage replacement, and synthetic tissue models.
Applications.
Many substances can form gels when a suitable thickener or gelling agent is added to their formula. This approach is common in manufacture of wide range of products, from foods to paints and adhesives.
In fiber optics communications, a soft gel resembling "hair gel" in viscosity is used to fill the plastic tubes containing the fibers. The main purpose of the gel is to prevent water intrusion if the buffer tube is breached, but the gel also buffers the fibers against mechanical damage when the tube is bent around corners during installation, or flexed. Additionally, the gel acts as a processing aid when the cable is being constructed, keeping the fibers central whilst the tube material is extruded around it.
RNA Triple-helix hydrogels for cancer therapy.
Recently, Conde et al. developed a self-assembled dual-color RNA triple-helix structure comprising two miRNAs, a miR mimic (tumor suppressor miRNA) and an antagomiR (oncomiR inhibitor) provides outstanding capability to synergistically abrogate tumors. The authors hypothesized that efficacious delivery of the RNA triple-helix hydrogel nanoconjugates would be achieved by coating the breast tumor with the adhesive hydrogel scaffold that they have previously shown able to enhance the stability of embedded nanoparticles used for local gene and drug delivery using smart gold nanobeacons.
Conjugation of RNA triple-helices to dendrimers allows the formation of stable triplex nanoparticles, which form an RNA-triple-helix adhesive scaffold on interaction with dextran aldehyde, the latter able to chemically interact and adhere to natural tissue amines in the tumor. The authors also show that the self-assembled RNA-triple-helix conjugates remain functional in vitro and in vivo, and that they lead to nearly 90% levels of tumor shrinkage two weeks post-gel implantation in a triple-negative breast-cancer mouse model. These findings suggest that the RNA-triple-helix hydrogels can be used as an efficient anticancer platform to locally modulate the expression of endogenous miRs in cancer.
The authors reported on a novel strategy for concomitant oncomiR inhibition and tumor suppressor miR replacement therapy using a RNA triple-helix hydrogel scaffold that affords highly efficacious local anticancer therapy. Self-assembled RNA triple-helix conjugates remain functional in vitro with high selective uptake and control over miR expression compared to their respective single-stranded or double-stranded forms. Hence, cancer gene delivery systems should provide potent, selective and specific treatment to tumor cells only, unlike the standard delivery of most conventional chemotherapeutic drugs. This approach can be implemented to design self-assembled triplex structures from any other miR combination, or from other genetic materials including antisense DNA or siRNA to treat a range of diseases.

</doc>
<doc id="41210" url="https://en.wikipedia.org/wiki?curid=41210" title="Geostationary orbit">
Geostationary orbit

A geostationary orbit, geostationary Earth orbit or geosynchronous equatorial orbit (GEO) is a circular orbit above the Earth's equator and following the direction of the Earth's rotation. An object in such an orbit has an orbital period equal to the Earth's rotational period (one sidereal day) and thus appears motionless, at a fixed position in the sky, to ground observers. Communications satellites and weather satellites are often placed in geostationary orbits, so that the satellite antennas (located on Earth) that communicate with them do not have to rotate to track them, but can be pointed permanently at the position in the sky where the satellites are located. Using this characteristic, ocean color satellites with visible and near-infrared light sensors (e.g. the Geostationary Ocean Color Imager (GOCI)) can also be operated in geostationary orbit in order to monitor sensitive changes of ocean environments.
A geostationary orbit is a particular type of geosynchronous orbit, the distinction being that while an object in geosynchronous orbit returns to the same point in the sky at the same time each day, an object in geostationary orbit never leaves that position.
The notion of a geosynchronous satellite for communication purposes was first published in 1928 (but not widely so) by Herman Potočnik. The first appearance of a geostationary orbit in popular literature was in the first Venus Equilateral story by George O. Smith, but Smith did not go into details. British science fiction author Arthur C. Clarke disseminated the idea widely, with more details on how it would work, in a 1945 paper entitled "Extra-Terrestrial Relays — Can Rocket Stations Give Worldwide Radio Coverage?", published in "Wireless World" magazine. Clarke acknowledged the connection in his introduction to "The Complete Venus Equilateral". The orbit, which Clarke first described as useful for broadcast and relay communications satellites, is sometimes called the Clarke Orbit. Similarly, the Clarke Belt is the part of space about above sea level, in the plane of the equator, where near-geostationary orbits may be implemented. The Clarke Orbit is about in circumference.
Practical uses.
Most commercial communications satellites, broadcast satellites and SBAS satellites operate in geostationary orbits. A geostationary transfer orbit is used to move a satellite from low Earth orbit (LEO) into a geostationary orbit. The first satellite placed into a geostationary orbit was the Syncom-3, launched by a Delta D rocket in 1964.
A worldwide network of operational geostationary meteorological satellites is used to provide visible and infrared images of Earth's surface and atmosphere. These satellite systems include:
A statite, a hypothetical satellite that uses a solar sail to modify its orbit, could theoretically hold itself in a geostationary "orbit" with different altitude and/or inclination from the "traditional" equatorial geostationary orbit.
Orbital stability.
A geostationary orbit can only be achieved at an altitude very close to and directly above the equator. This equates to an orbital velocity of and an orbital period of 1,436 minutes, which equates to almost exactly one sidereal day (23.934461223 hours). This ensures that the satellite will match the Earth's rotational period and has a stationary footprint on the ground. All geostationary satellites have to be located on this ring.
A combination of lunar gravity, solar gravity, and the flattening of the Earth at its poles causes a precession motion of the orbital plane of any geostationary object, with an orbital period of about 53 years and an initial inclination gradient of about 0.85° per year, achieving a maximal inclination of 15° after 26.5 years. To correct for this orbital perturbation, regular orbital stationkeeping manoeuvres are necessary, amounting to a delta-v of approximately 50 m/s per year.
A second effect to be taken into account is the longitude drift, caused by the asymmetry of the Earth – the equator is slightly elliptical. There are two stable (at 75.3°E and 104.7°W) and two unstable (at 165.3°E and 14.7°W) equilibrium points. Any geostationary object placed between the equilibrium points would (without any action) be slowly accelerated towards the stable equilibrium position, causing a periodic longitude variation. The correction of this effect requires orbit control manoeuvres with a maximal delta-v of about 2 m/s per year, depending on the desired longitude.
Solar wind and radiation pressure also exert small forces on satellites; over time, these cause them to slowly drift away from their prescribed orbits.
In the absence of servicing missions from the Earth or a renewable propulsion method, the consumption of thruster propellant for station keeping places a limitation on the lifetime of the satellite. Hall-effect thrusters, which are currently in use, have the potential to prolong the service life of a satellite by providing high-efficiency electric propulsion.
Communications.
Satellites in geostationary orbits are far enough away from Earth that communication latency becomes significant — about a quarter of a second for a trip from one ground-based transmitter to the satellite and back to another ground-based transmitter; close to half a second for a round-trip communication from one Earth station to another and then back to the first.
For example, for ground stations at latitudes of "φ" = ±45° on the same meridian as the satellite, the time taken for a signal to pass from Earth to the satellite and back again can be computed using the cosine rule, given the geostationary orbital radius "r" (derived below), the Earth's radius "R" and the speed of light "c", as
This delay presents problems for latency-sensitive applications such as voice communication.
Geostationary satellites are directly overhead at the equator and become lower in the sky the further north or south one travels. As the observer's latitude increases, communication becomes more difficult due to factors such as atmospheric refraction, Earth's thermal emission, line-of-sight obstructions, and signal reflections from the ground or nearby structures. At latitudes above about 81°, geostationary satellites are below the horizon and cannot be seen at all. Because of this, some Russian communication satellites have used elliptical Molniya and Tundra orbits, which have excellent visibility at high latitudes.
Orbit allocation.
Satellites in geostationary orbit must all occupy a single ring above the equator. The requirement to space these satellites apart to avoid harmful radio-frequency interference during operations means that there are a limited number of orbital "slots" available, thus only a limited number of satellites can be operated in geostationary orbit. This has led to conflict between different countries wishing access to the same orbital slots (countries near the same longitude but differing latitudes) and radio frequencies. These disputes are addressed through the International Telecommunication Union's allocation mechanism. In the 1976 Bogotá Declaration, eight countries located on the Earth's equator claimed sovereignty over the geostationary orbits above their territory, but the claims gained no international recognition.
Limitations to usable life of geostationary satellites.
When they run out of thruster fuel, the satellites are at the end of their service life, as they are no longer able to stay in their allocated orbital position. The transponders and other onboard systems generally outlive the thruster fuel and, by stopping N–S station keeping, some satellites can continue to be used in inclined orbits (where the orbital track appears to follow a figure-eight loop centred on the equator), or else be elevated to a "graveyard" disposal orbit.
Derivation of geostationary altitude.
In any circular orbit, the centripetal force required to maintain the orbit (Fc) is provided by the gravitational force on the satellite (Fg). To calculate the geostationary orbit altitude, one begins with this equivalence:
By Newton's second law of motion, we can replace the forces F with the mass "m" of the object multiplied by the acceleration felt by the object due to that force:
We note that the mass of the satellite "m" appears on both sides — geostationary orbit is independent of the mass of the satellite. So calculating the altitude simplifies into calculating the point where the magnitudes of the centripetal acceleration required for orbital motion and the gravitational acceleration provided by Earth's gravity are equal.
The centripetal acceleration's magnitude is:
where "ω" is the angular speed, and "r" is the orbital radius as measured from the Earth's center of mass.
The magnitude of the gravitational acceleration is:
where "M" is the mass of Earth, , and "G" is the gravitational constant,
Equating the two accelerations gives:
The product "GM" is known with much greater precision than either factor alone; it is known as the geocentric gravitational constant "μ" = . Hence
The angular speed "ω" is found by dividing the angle travelled in one revolution (360° = 2π rad) by the orbital period (the time it takes to make one full revolution). In the case of a geostationary orbit, the orbital period is one sidereal day, or ). This gives
The resulting orbital radius is . Subtracting the Earth's equatorial radius, , gives the altitude of .
Orbital speed is calculated by multiplying the angular speed by the orbital radius:
By the same formula, we can find the geostationary-type orbit of an object in relation to Mars (this type of orbit above is referred to as an areostationary orbit if it is above Mars). The geocentric gravitational constant "GM" (which is "μ") for Mars has the value of 42,828 km3s−2, and the known rotational period ("T") of Mars is 88,642.66 seconds. Since "ω" = 2/"T", using the formula above, the value of "ω" is found to be approx 7.088218×10−5 s−1. Thus "r"3 = 8.5243×1012 km3, whose cube root is 20,427 km; subtracting the equatorial radius of Mars (3396.2 km), we have 17,031 km.

</doc>
<doc id="41211" url="https://en.wikipedia.org/wiki?curid=41211" title="Graded-index fiber">
Graded-index fiber

In fiber optics, a graded-index or gradient-index fiber is an optical fiber whose core has a refractive index that decreases with increasing radial distance from the optical axis of the fiber.
Because parts of the core closer to the fiber axis have a higher refractive index than the parts near the cladding, light rays follow sinusoidal paths down the fiber. The most common refractive index profile for a graded-index fiber is very nearly parabolic. The parabolic profile results in continual refocusing of the rays in the core, and minimizes modal dispersion.
Multi-mode optical fiber can be built with either graded index or step index. The advantage of the multi-mode graded index compared to the multi-mode step index is the considerable decrease in modal dispersion. Modal dispersion can be further decreased by selecting a smaller core size (less than 5-10μm) and forming a single mode step index fiber.
This type of fiber is normalized by the International Telecommunications Union ITU-T at recommendation G.651.1.
Pulse dispersion.
Pulse dispersion in a graded index optical fiber is given by
formula_1     
where
formula_2 is the difference in refractive indices of core and cladding,
formula_3 is the refractive index of the cladding,
formula_4 is the length of the fiber taken for observing the pulse dispersion,
formula_5 is the speed of light, and
formula_6 is the constant of graded index profile.

</doc>
<doc id="41212" url="https://en.wikipedia.org/wiki?curid=41212" title="Grade of service">
Grade of service

In telecommunication engineering, and in particular teletraffic engineering, the quality of voice service is specified by two measures: the grade of service (GoS) and the quality of service (QoS).
Grade of service is the probability of a call in a circuit "group" being blocked or delayed for more than a specified interval, expressed as a vulgar fraction or decimal fraction. This is always with reference to the busy hour when the traffic intensity is the greatest. Grade of service may be viewed independently from the perspective of incoming versus outgoing calls, and is not necessarily equal in each direction or between different source-destination pairs.
On the other hand, the quality of service which a "single" circuit is designed or conditioned to provide, e.g. voice grade or program grade is called the quality of service. Quality criteria for such circuits may include equalization for amplitude over a specified band of frequencies, or in the case of digital data transported via analogue circuits, may include equalization for phase. Criteria for mobile quality of service in cellular telephone circuits include the probability of abnormal termination of the call.
What is Grade of Service and how is it measured?
When a user attempts to make a telephone call, the routing equipment handling the call has to determine whether to accept the call, reroute the call to alternative equipment, or reject the call entirely. Rejected calls occur as a result of heavy traffic loads (congestion) on the system and can result in the call either being delayed or lost. If a call is delayed, the user simply has to wait for the traffic to decrease, however if a call is lost then it is removed from the system.
The Grade of Service is one aspect of the quality a customer can expect to experience when making a telephone call. In a Loss System, the Grade of Service is described as that proportion of calls that are lost due to congestion in the busy hour. 
For a Lost Call system, the Grade of Service can be measured using "Equation 1". 
For a delayed call system, the Grade of Service is measured using three separate terms:
Where and when is Grade of Service measured?
The Grade of Service can be measured using different sections of a network. When a call is routed from one end to another, it will pass through several exchanges. If the Grade of Service is calculated based on the number of calls rejected by the final circuit group, then the Grade of Service is determined by the final circuit group blocking criteria. If the Grade of Service is calculated based on the number of rejected calls between exchanges, then the Grade of Service is determined by the exchange-to-exchange blocking criteria.
The Grade of Service should be calculated using both the access networks and the core networks as it is these networks that allow a user to complete an end-to-end connection. Furthermore, the Grade of Service should be calculated from the average of the busy hour traffic intensities of the 30 busiest traffic days of the year. This will cater for most scenarios as the traffic intensity will seldom exceed the reference level.
The grade of service is a measure of the ability of a user to access a trunk system during the busiest hour. The busy is based upon customer demand at the busiest hour during a week month or year.
Class of Service.
Different telecommunications applications require different Qualities of Service. For example, if a telecommunications service provider decides to offer different qualities of voice connection, then a premium voice connection will require a better connection quality compared to an ordinary voice connection. Thus different Qualities of Service are appropriate, depending on the intended use. To help telecommunications service providers to market their different services, each service is placed into a specific class. Each Class of Service determines the level of service required.
To identify the Class of Service for a specific service, the network’s switches and routers examine the call based on several factors. Such factors can include:
Quality of Service in broadband networks.
In broadband networks, the Quality of Service is measured using two criteria. The first criterion is the probability of packet losses or delays in already accepted calls. The second criterion refers to the probability that a new incoming call will be rejected or blocked. To avoid the former, broadband networks limit the number of active calls so that packets from established calls will not be lost due to new calls arriving. As in circuit-switched networks, the Grade of Service can be calculated for individual switches or for the whole network.
Maintaining a Grade of Service.
The telecommunications provider is usually aware of the required Grade of Service for a particular product. To achieve and maintain a given Grade of Service, the operator must ensure that sufficient telecommunications circuits or routes are available to meet a specific level of demand. It should also be kept in mind that too many circuits will create a situation where the operator is providing excess capacity which may never be used, or at the very least may be severely underutilized. This adds costs which must be borne by other parts of the network. To determine the correct number of circuits that are required, telecommunications service providers make use of Traffic Tables. An example of a Traffic Table can be viewed in "Figure 1". It follows that in order for a telecommunications network to continue to offer a given Grade of Service, the number of circuits provided in a circuit group must increase (non-linearly) if the traffic intensity increases.
Erlang's lost call assumptions.
To calculate the Grade of Service of a specified group of circuits or routes, Agner Krarup Erlang used a set of assumptions that relied on the network losing calls when all circuits in a group were busy. These assumptions are:
From these assumptions Erlang developed the Erlang-B formula which describes the probability of congestion in a circuit group. The probability of congestion gives the Grade of Service experienced.
Calculating the Grade of Service.
To determine the Grade of Service of a network when the traffic load and number of circuits are known, telecommunications network operators make use of "Equation 2", which is the Erlang-B equation. 
"A" = Expected traffic intensity in Erlangs,
"N" = Number of circuits in group.
This equation allows operators to determine whether each of their circuit groups meet the required Grade of Service, simply by monitoring the reference traffic intensity. 

</doc>
<doc id="41214" url="https://en.wikipedia.org/wiki?curid=41214" title="Graphic character">
Graphic character

In ISO/IEC 646 (commonly known as ASCII) and related standards including ISO 8859 and Unicode, a graphic character is any character intended to be written, printed, or otherwise displayed in a form that can be read by humans. In other words, it is any encoded character that is associated with one or more glyphs.
ISO/IEC 646.
In ISO 646, graphic characters are contained in rows 2 through 7 of the code table. However, two of the characters in these rows, namely the space character SP at row 2 column 0 and the delete character DEL (also called the rubout character) at row 7 column 15, require special mention.
The space is considered to be "both" a graphic character and a control character in ISO 646; this is probably due to it having a visible form on computer terminals but a control function (of moving the print head) on teletypes. 
The delete character is strictly a control character, not a graphic character. This is true not only in ISO 646, but also in all related standards including Unicode. However, many modern character sets deviate from ISO 646, and as a result a graphic character might occupy the position originally reserved for the delete character.
Unicode.
In Unicode, Graphic characters are those with General Category Letter, Mark, Number, Punctuation, Symbol or Zs=space. Other code points (General categories Control, Zl=line separator, Zp=paragraph separator) are Format, Control, Private Use, Surrogate, Noncharacter or Reserved (unassigned).
Spacing and non-spacing characters.
Most graphic characters are spacing characters, which means that each instance of a spacing character has to occupy some area in a graphic representation. For a teletype or a typewriter this implies moving of the carriage after typing of a character. In the context of text mode display, each spacing character occupies one rectangular character box of equal sizes. Or maybe two adjacent ones, for non-alphabetic characters of East Asian languages. If a text is rendered using proportional fonts, widths of character boxes are not equal, but are positive.
There exists also "non-spacing" graphic characters. Most of non-spacing characters are "modifiers", also called combining characters in Unicode, such as diacritical marks. Although non-spacing graphic characters are uncommon in traditional code pages, there are many such in Unicode. A combining character has its distinct glyph, but it applies to a character box of another character, a spacing one. In some historical systems such as line printers this was implemented as overstrike.
Note that not all modifiers are non-spacing – there exists Spacing Modifier Letters Unicode block.

</doc>
<doc id="41215" url="https://en.wikipedia.org/wiki?curid=41215" title="Ground (electricity)">
Ground (electricity)

For different type of earthing system refer Earthing system.
In electrical engineering, ground or earth is the reference point in an electrical circuit from which voltages are measured, a common return path for electric current, or a direct physical connection to the Earth.
In electrical power distribution systems, a protective ground conductor is an essential part of the safety Earthing system.
Electrical circuits may be connected to ground (earth) for several reasons. In mains powered equipment, exposed metal parts are connected to ground to prevent user contact with dangerous voltage if electrical insulation fails. Connections to ground limit the build-up of static electricity when handling flammable products or electrostatic-sensitive devices. In some telegraph and power transmission circuits, the earth itself can be used as one conductor of the circuit, saving the cost of installing a separate return conductor (see single-wire earth return).
For measurement purposes, the Earth serves as a (reasonably) constant potential reference against which other potentials can be measured. An electrical ground system should have an appropriate current-carrying capability to serve as an adequate zero-voltage reference level. In electronic circuit theory, a "ground" is usually idealized as an infinite source or sink for charge, which can absorb an unlimited amount of current without changing its potential. Where a real ground connection has a significant resistance, the approximation of zero potential is no longer valid. Stray voltages or earth potential rise effects will occur, which may create noise in signals or if large enough will produce an electric shock hazard.
The use of the term ground (or earth) is so common in electrical and electronics applications that circuits in portable electronic devices such as cell phones and media players as well as circuits in vehicles may be spoken of as having a "ground" connection without any actual connection to the Earth, despite "common" being a more appropriate term for such a connection. This is usually a large conductor attached to one side of the power supply (such as the "ground plane" on a printed circuit board) which serves as the common return path for current from many different components in the circuit.
History.
Long-distance electromagnetic telegraph systems from 1820 onwards used two or more wires to carry the signal and return currents. It was then discovered, probably by the German scientist Carl August Steinheil in 1836–1837, that the ground could be used as the return path to complete the circuit, making the return wire unnecessary. However, there were problems with this system, exemplified by the transcontinental telegraph line constructed in 1861 by the Western Union Company between Saint Joseph, Missouri, and Sacramento, California. During dry weather, the ground connection often developed a high resistance, requiring water to be poured on the ground rod to enable the telegraph to work or phones to ring.
Later, when telephony began to replace telegraphy, it was found that the currents in the earth induced by power systems, electrical railways, other telephone and telegraph circuits, and natural sources including lightning caused unacceptable interference to the audio signals, and the two-wire or 'metallic circuit' system was reintroduced around 1883.
Radio communications.
An electrical connection to earth can be used as a reference potential for radio frequency signals for certain kinds of antennas. The part directly in contact with the earth - the "earth electrode" - can be as simple as a metal rod or stake driven into the earth, or a connection to buried metal water piping (the pipe must be conductive). Because high frequency signals can flow to earth due to capacitative effects, capacitance to ground is an important factor in effectiveness of signal grounds. Because of this, a complex system of buried rods and wires can be effective. An ideal signal ground maintains a fixed potential (zero) regardless of how much electric current flows into ground or out of ground. Low impedance at the signal frequency of the electrode-to-earth connection determines its quality, and that quality is improved by increasing the surface area of the electrode in contact with the earth, increasing the depth to which it is driven, using several connected ground rods, increasing the moisture content of the soil, improving the conductive mineral content of the soil, and increasing the land area covered by the ground system.
Some types of transmitting antenna systems in the VLF, LF, MF and lower SW range must have a good ground to operate efficiently. For example, a vertical monopole antenna requires a ground plane that often consists of an interconnected network of wires running radially away from the base of the antenna for a distance about equal to the height of the antenna. Sometimes a counterpoise is used as a ground plane, supported above the ground.
Building wiring installations.
Electrical power distribution systems are often connected to ground to limit the voltage that can appear on distribution circuits. A distribution system insulated from ground may attain a high potential due to transient voltages caused by arcing, static electricity, or accidental contact with higher potential circuits. A ground connection of the system dissipates such potentials and limits the rise in voltage of the grounded system.
In a mains electricity (AC power) wiring installation, the term "ground conductor" typically refers to three different conductors or conductor systems as listed below.
"Equipment earthing conductors" provide an electrical connection between non-current-carrying metallic parts of equipment and the earth. According to the U.S. National Electrical Code (NEC), the reason for doing this is to limit the voltage imposed by lightning, line surges, and contact with higher voltage lines. The equipment earthing conductor is usually also used as the equipment bonding conductor (see below).
"Equipment bonding conductors" provide a low impedance path between non-current-carrying metallic parts of equipment and one of the conductors of that electrical system's source, so that if a part becomes energized for any reason, such as a frayed or damaged conductor, a short circuit will occur and operate a circuit breaker or fuse to disconnect the faulted circuit. The earth itself has no role in this fault-clearing process since current must return to its source; however, the sources are very frequently connected to earth. (see Kirchhoff's circuit laws). By bonding (interconnecting) all exposed non-current carrying metal objects together, they should remain near the same potential thus reducing the chance of a shock. This is especially important in bathrooms where one may be in contact with several different metallic systems such as supply and drain pipes and appliance frames. The equipment bonding conductor is usually also used as the equipment earthing conductor (see above).
A (GEC) connects one leg of an electrical system to one or more earth electrodes. This is called "system grounding" and most systems are required to be grounded. The U.S. NEC and the UK's BS 7671 list systems that are required to be grounded. The grounding electrode conductor connects the leg of the electrical system that is the "neutral wire" to the grounding electrode(s). The grounding electrode conductor is also usually bonded to pipework and structural steel in larger structures. According to the NEC, the purpose of earthing an electrical system is to limit the voltage to earth imposed by lightning events and contact with higher voltage lines, and also to stabilize the voltage to earth during normal operation. In the past, water supply pipes were often used as grounding electrodes, but this was banned where plastic pipes are popular. This type of ground applies to radio antennas and to lightning protection systems.
Permanently installed electrical equipment usually also has permanently connected grounding conductors. Portable electrical devices with metal cases may have them connected to earth ground by a pin in the interconnecting plug (see Domestic AC power plugs and sockets). The size of power ground conductors is usually regulated by local or national wiring regulations.
Earthing systems.
In electricity supply systems, an earthing (grounding) system defines the electrical potential of the conductors relative to that of the Earth's conductive surface. The choice of earthing system has implications for the safety and electromagnetic compatibility of the power supply. Regulations for earthing systems vary considerably between different countries.
A functional earth connection serves a purpose other than providing protection against electrical shock. In contrast to a protective earth connection, a functional earth connection may carry a current during the normal operation of a device. Functional earth connections may be required by devices such as surge suppression and electromagnetic-compatibility filters, some types of antennas and various measurement instruments. Generally the protective earth is also used as a functional earth, though this requires care in some situations.
Impedance grounding.
Distribution power systems may be solidly grounded, with one circuit conductor directly connected to an earth grounding electrode system. Alternatively, some amount of electrical impedance may be connected between the distribution system and ground, to limit the current that can flow to earth. The impedance may be a resistor, or an inductor (coil). In a high-impedance grounded system, the fault current is limited to a few amperes (exact values depend on the voltage class of the system); a low-impedance grounded system will permit several hundred amperes to flow on a fault. A large solidly-grounded distribution system may have thousands of amperes of ground fault current.
In a polyphase AC system, an artificial neutral grounding system may be used. Although no phase conductor is directly connected to ground, a specially constructed transformer (a "zig zag" transformer) blocks the power frequency current from flowing to earth, but allows any leakage or transient current to flow to ground.
Low-resistance grounding systems use a neutral grounding resistor (NGR) to limit the fault current to 25 A or greater. Low resistance grounding systems will have a time rating (say, 10 seconds) that indicates how long the resistor can carry the fault current before overheating. A ground fault protection relay must trip the breaker to protect the circuit before overheating of the resistor occurs.
High-resistance grounding (HRG) systems use an NGR to limit the fault current to 25 A or less. They have a continuous rating, and are designed to operate with a single-ground fault. This means that the system will not immediately trip on the first ground fault. If a second ground fault occurs, a ground fault protection relay must trip the breaker to protect the circuit. On an HRG system, a sensing resistor is used to continuously monitor system continuity. If an open-circuit is detected (e.g., due to a broken weld on the NGR), the monitoring device will sense voltage through the sensing resistor and trip the breaker. Without a sensing resistor, the system could continue to operate without ground protection (since an open circuit condition would mask the ground fault) and transient overvoltages could occur.
Ungrounded systems.
Where the danger of electric shock is high, special ungrounded power systems may be used to minimize possible leakage current to ground. Examples of such installations include patient care areas in hospitals, where medical equipment is directly connected to a patient and must not permit any power-line current to pass into the patient's body. Medical systems include monitoring devices to warn of any increase of leakage current. On wet construction sites or in shipyards, isolation transformers may be provided so that a fault in a power tool or its cable does not expose users to shock hazard.
Circuits used to feed sensitive audio/video production equipment or measurement instruments may be fed from an isolated ungrounded technical power system to limit the injection of noise from the power system.
Power transmission.
In single-wire earth return (SWER) AC electrical distribution systems, costs are saved by using just a single high voltage conductor for the power grid, while routing the AC return current through the earth. This system is mostly used in rural areas where large earth currents will not otherwise cause hazards.
Some high-voltage direct-current (HVDC) power transmission systems use the ground as second conductor. This is especially common in schemes with submarine cables, as sea water is a good conductor. Buried grounding electrodes are used to make the connection to the earth. The site of these electrodes must be chosen carefully to prevent electrochemical corrosion on underground structures.
A particular concern in design of electrical substations is earth potential rise. When very large fault currents are injected into the earth, the area around the point of injection may rise to a high potential with respect to distant points. This is due to the limited finite conductivity of the layers of soil in the earth. The gradient of the voltage (changing voltage within a distance) may be so high that two points on the ground may be at significantly different potentials, creating a hazard to anyone standing on the ground in the area. Pipes, rails, or communication wires entering a substation may see different ground potentials inside and outside the substation, creating a dangerous touch voltage.
Electronics.
Signal grounds serve as return paths for signals and power (at extra low voltages, less than about 50 V) within equipment, and on the signal interconnections between equipment. Many electronic designs feature a single return that acts as a reference for all signals. Power and signal grounds often get connected, usually through the metal case of the equipment. Designers of printed circuit boards must take care in the layout of electronic systems so that high-power or rapidly-switching currents in one part of a system do not inject noise into low-level sensitive parts of a system due to some common impedance in the grounding traces of the layout.
Circuit ground versus earth.
Voltage is measured on an interval scale, which means that only differences can be measured. To measure the voltage of a single point, a reference point must be selected to measure against. This common reference point is called "ground" and considered to have zero voltage. This signal ground may or may not be connected to a power ground. A system where the system ground is not connected to another circuit or to earth (though there may still be AC coupling) is often referred to as a floating ground or double-insulated.
Functional grounds.
Some devices require a connection to the mass of earth to function correctly, as distinct from any purely protective role. Such a connection is known as a functional earth- for example some long wavelength antenna structures require a functional earth connection, which generally should not be indiscriminately connected to the supply protective earth, as the introduction of transmitted radio frequencies into the electrical distribution network is both illegal and potentially dangerous. Because of this separation, a purely functional ground should not normally be relied upon to perform a protective function
To avoid accidents, such functional grounds are normally wired in white or cream cable, and not green or green/yellow.
Separating low signal ground from a noisy ground.
In television stations, recording studios, and other installations where signal quality is critical, a special signal ground known as a "technical ground" (or "technical earth", "special earth", and "audio earth") is often installed, to prevent ground loops. This is basically the same thing as an AC power ground, but no general appliance ground wires are allowed any connection to it, as they may carry electrical interference. For example, only audio equipment is connected to the technical ground in a recording studio. In most cases, the studio's metal equipment racks are all joined together with heavy copper cables (or flattened copper tubing or busbars) and similar connections are made to the technical ground. Great care is taken that no general chassis grounded appliances are placed on the racks, as a single AC ground connection to the technical ground will destroy its effectiveness. For particularly demanding applications, the main technical ground may consist of a heavy copper pipe, if necessary fitted by drilling through several concrete floors, such that all technical grounds may be connected by the shortest possible path to a grounding rod in the basement.
Lightning protection systems.
Lightning protection systems are designed to mitigate the effects of lightning through connection to extensive grounding systems that provide a large surface area connection to earth. The large area is required to dissipate the high current of a lightning strike without damaging the system conductors by excess heat. Since lightning strikes are pulses of energy with very high frequency components, grounding systems for lightning protection tend to use short straight runs of conductors to reduce the self-inductance and skin effect.
Bonding.
Strictly speaking, the terms "grounding" or "earthing" are meant to refer to an electrical connection to ground/earth. Bonding is the practice of intentionally electrically connecting metallic items not designed to carry electricity. This brings all the bonded items to the same electrical potential as a protection from electrical shock. The bonded items can then be connected to ground to bring them to earth potential.
Ground (earth) mat.
In an electrical substation a ground (earth) mat is a mesh of conductive material installed at places where a person would stand to operate a switch or other apparatus; it is bonded to the local supporting metal structure and to the handle of the switchgear, so that the operator will not be exposed to a high differential voltage due to a fault in the substation.
In the vicinity of electrostatic sensitive devices, a ground (earth) mat or grounding (earthing) mat is used to ground static electricity generated by people and moving equipment. There are two types used in static control: Static Dissipative Mats, and Conductive Mats.
A static dissipative mat that rests on a conductive surface (commonly the case in military facilities) are typically made of 3 layers (3-ply) with static dissipative vinyl layers surrounding a conductive substrate which is electrically attached to ground (earth). For commercial uses, static dissipative rubber mats are traditionally used that are made of 2 layers (2-ply) with a tough solder resistant top static dissipative layer that makes them last longer than the vinyl mats, and a conductive rubber bottom. Conductive mats are made of carbon and used only on floors for the purpose of drawing static electricity to ground as quickly as possible. Normally conductive mats are made with cushioning for standing and are referred to as "anti-fatigue" mats.
For a static dissipative mat to be reliably grounded it must be attached to a path to ground. Normally, both the mat and the wrist strap are connected to ground by using a common point ground system (CPGS).
In computer repair shops and electronics manufacturing workers must be grounded before working on devices sensitive to voltages capable of being generated by humans. For that reason static dissipative mats can be and are also used on production assembly floors as "floor runner" along the assembly line to draw static generated by people walking up and down.
Isolation.
Isolation is a mechanism that defeats grounding. It is frequently used with low-power consumer devices, and when electronics engineers, hobbyists, or repairmen are working on circuits that would normally be operated using the power line voltage. Isolation can be accomplished by simply placing a "1:1 wire ratio" transformer with an equal number of turns between the device and the regular power service, but applies to any type of transformer using two or more coils electrically insulated from each other.
For an isolated device, touching a single powered conductor does not cause a severe shock, because there is no path back to the other conductor through the ground. However, shocks and electrocution may still occur if both poles of the transformer are contacted by bare skin. Previously it was suggested that repairmen "work with one hand behind their back" to avoid touching two parts of the device under test at the same time, thereby preventing a circuit from crossing through the chest and interrupting cardiac rhythms/ causing cardiac arrest.
Generally every AC power line transformer acts as an isolation transformer, and every step up or down has the potential to form an isolated circuit. However, this isolation would prevent failed devices from blowing fuses when shorted to their ground conductor. The isolation that could be created by each transformer is defeated by always having one leg of the transformers grounded, on both sides of the input and output transformer coils. Power lines also typically ground one specific wire at every pole, to ensure current equalization from pole to pole if a short to ground is occurring.
In the past, grounded appliances have been designed with internal isolation to a degree that allowed the simple disconnection of ground by cheater plugs without apparent problem (a dangerous practice, since the safety of the resulting floating equipment relies on the insulation in its power transformer). Modern appliances however often include power entry modules which are designed with deliberate capacitive coupling between the AC power lines and chassis, to suppress electromagnetic interference. This results in a significant leakage current from the power lines to ground. If the ground is disconnected by a cheater plug or by accident, the resulting leakage current can cause mild shocks, even without any fault in the equipment. Even small leakage currents are a significant concern in medical settings, as the accidental disconnection of ground can introduce these currents into sensitive parts of the human body. As a result, medical power supplies are designed to have low capacitance.
Class II appliances and power supplies (such as cell phone chargers) do not provide any ground connection, and are designed to isolate the output from input. Safety is ensured by double-insulation, so that two failures of insulation are required to cause a shock.

</doc>
<doc id="41216" url="https://en.wikipedia.org/wiki?curid=41216" title="Ground constants">
Ground constants

In telecommunication, ground constants are the electrical parameters of earth, such as conductivity, permittivity, and magnetic permeability. 
The values of these parameters vary with the local chemical composition and density of the Earth. For a propagating electromagnetic wave, such as a surface wave propagating along the surface of the Earth, these parameters vary with frequency and direction. 

</doc>
<doc id="41217" url="https://en.wikipedia.org/wiki?curid=41217" title="Ground loop">
Ground loop

Ground loop may refer to:

</doc>
<doc id="41218" url="https://en.wikipedia.org/wiki?curid=41218" title="Ground plane">
Ground plane

In electrical engineering, a ground plane is an electrically conductive surface, usually connected to electrical ground. The term has two different meanings in separate areas of electrical engineering. In antenna theory, a ground plane is a conducting surface large in comparison to the wavelength, such as the Earth, which is connected to the transmitter's ground wire and serves as a reflecting surface for radio waves. In printed circuit boards, a ground plane is a large area of copper foil on the board which is connected to the power supply ground terminal and serves as a return path for current from different components on the board.
Radio antenna theory.
In telecommunication, a "ground plane" is a flat or nearly flat horizontal conducting surface that serves as part of an antenna, to reflect the radio waves from the other antenna elements. The plane does not necessarily have to be connected to ground. Ground planes are particularly used with microstrip antennas and printed monopole antennas. A microstrip patch, placed above a ground plane, forms a resonant cavity having magnetic wall-boundary on its four sides. Ground plane shape and size play major roles in determining its radiation characteristics including gain.
To function as a ground plane, the conducting surface must be at least a quarter of the wavelength ("λ"/4) of the radio waves in size. In lower frequency antennas, such as the mast radiators used for broadcast antennas, the Earth itself (or a body of water such as a salt marsh or ocean) is used as a ground plane. For higher frequency antennas, in the VHF or UHF range, the ground plane can be smaller, and metal disks, screens and wires are used as ground planes. At upper VHF and UHF, the metal skin of a car or aircraft can serve as a ground plane for whip antennas projecting from it. The ground plane doesn't have to be a continuous surface. In the "ground plane antenna" style whip antenna, the "plane" consists of several wires "λ"/4 long radiating from the base of a quarter-wave whip antenna.
The radio waves from an antenna element that reflect off a ground plane appear to come from a mirror image of the antenna located on the other side of the ground plane. In a monopole antenna, the radiation pattern of the monopole plus the virtual "image antenna" make it appear as a two element center-fed dipole antenna. So a monopole mounted over an ideal ground plane has a radiation pattern identical to a dipole antenna. The feedline from the transmitter or receiver is connected between the bottom end of the monopole element and the ground plane. The ground plane must have good conductivity; any resistance in the ground plane is in series with the antenna, and serves to dissipate power from the transmitter.
Printed circuit boards.
A "ground plane" on a printed circuit board (PCB) is a large area or layer of copper foil connected to the circuit's ground point, usually one terminal of the power supply. It serves as the return path for current from many different components.
A ground plane is often made as large as possible, covering most of the area of the PCB which is not occupied by circuit traces. In multilayer PCBs, it is often a separate layer covering the entire board. This serves to make circuit design easier, allowing the designer to ground any component without having to run additional traces; component leads needing grounding are routed directly through a hole in the board to the ground plane on another layer. The large area of copper also conducts the large return currents from many components without significant voltage drops, ensuring that the ground connection of all the components are at the same reference potential.
In digital and radio frequency PCBs, the major reason for using large ground planes is to reduce electrical noise and interference through ground loops and to prevent crosstalk between adjacent circuit traces. When digital circuits switch state, large current pulses flow from the active devices (transistors or integrated circuits) through the ground circuit. If the power supply and ground traces have significant impedance, the voltage drop across them may create noise voltage pulses that disturb other parts of the circuit (ground bounce). The large conducting area of the ground plane has much lower impedance than a circuit trace, so the current pulses cause less disturbance.
In addition, a ground plane under printed circuit traces can reduce crosstalk between adjacent traces. When two traces run parallel, an electrical signal in one can be coupled into the other through electromagnetic induction by magnetic field lines from one linking the other; this is called crosstalk. When a ground plane layer is present underneath, it forms a transmission line with the trace. The oppositely-directed return currents flow through the ground plane directly beneath the trace. This confines most of the electromagnetic fields to the area near the trace and consequently reduces crosstalk.
A power plane is often used in addition to a ground plane in a multilayer circuit board, to distribute DC power to the active devices. The two facing areas of copper create a large parallel plate decoupling capacitor that prevents noise from being coupled from one circuit to another through the power supply.
Ground planes are sometimes split and then connected by a thin trace. This allows the separation of analog and digital sections of a board or the inputs and outputs of amplifiers. The thin trace has low enough impedance to keep the two sides very close to the same potential while keeping the ground currents of one side from coupling into the other side, causing ground loop.

</doc>
<doc id="41220" url="https://en.wikipedia.org/wiki?curid=41220" title="Group alerting and dispatching system">
Group alerting and dispatching system

In telecommunication, a group alerting and dispatching system is a service feature that (a) enables a controlling telephone to place a call to a specified number of telephones simultaneously, (b) enables the call to be recorded, (c) if any of the called lines is busy, enables the equipment to camp on until the busy line is free, and (d) rings the free line and plays the recorded message.

</doc>
<doc id="41222" url="https://en.wikipedia.org/wiki?curid=41222" title="Group delay and phase delay">
Group delay and phase delay

In signal processing, group delay is the time delay of the amplitude envelopes of the various sinusoidal components of a signal through a device under test, and is a function of frequency for each component. Phase delay, in contrast, is the time delay of the "phase" as opposed to the time delay of the "amplitude envelope".
All frequency components of a signal are delayed when passed through a device such as an amplifier, a loudspeaker, or propagating through space or a medium, such as air. This signal delay will be different for the various frequencies unless the device has the property of being linear phase. (Linear phase and minimum phase are often confused. They are quite different.) The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device. This changes the shape of the signal in addition to any constant delay or scale change. A sufficiently large delay variation can cause problems such as poor fidelity in audio or intersymbol interference (ISI) in the demodulation of digital information from an analog carrier signal. High speed modems use adaptive equalizers to compensate for non-constant group delay.
Introduction.
Group delay is a useful measure of time distortion, and is calculated by differentiating, with respect to frequency, the phase response of the device under test (DUT): the group delay is a measure of the slope of the phase response at any given frequency. Variations in group delay cause signal distortion, just as deviations from linear phase cause distortion.
In linear time-invariant (LTI) system theory, control theory, and in digital or analog signal processing, the relationship between the input signal, formula_1, to output signal, formula_2, of an LTI system is governed by a convolution operation:
Or, in the frequency domain,
where
and
Here formula_8 is the time-domain impulse response of the LTI system and formula_9, formula_10, formula_11, are the Laplace transforms of the input formula_1, output formula_2, and impulse response formula_8, respectively. formula_11 is called the transfer function of the LTI system and, like the impulse response formula_8, "fully" defines the input-output characteristics of the LTI system.
Suppose that such a system is driven by a quasi-sinusoidal signal, that is, a sinusoid whose amplitude envelope formula_17 is slowly-changing relative to the phase frequency formula_18 of the sinusoid. Mathematically, this means that the driving signal has the form
subject to the assumption
Then the output of such an LTI system is very well approximated as
Here formula_22 and formula_23, the group delay and phase delay respectively, are given by the expressions below (and potentially are functions of the angular frequency formula_18).
In a linear phase system (with non-inverting gain), both formula_22 and formula_23 are constant (i.e. independent of formula_18) and equal, and their common value equals the overall delay of the system; and the unwrapped phase shift of the system (namely formula_28) is negative, with magnitude increasing linearly with frequency formula_18.
More generally, it can be shown that for an LTI system with transfer function formula_11 driven by a complex sinusoid of unit amplitude,
the output is
where the phase shift formula_33 is
Additionally, it can be shown that the group delay, formula_22, and phase delay, formula_23, are frequency-dependent, and they can be computed from the phase shift formula_33 by
Group delay in optics.
In physics, and in particular in optics, the term group delay has the following meanings:
It is often desirable for the group delay to be constant across all frequencies; otherwise there is temporal smearing of the signal. Because group delay is formula_45, as defined in (1), it therefore follows that a constant group delay can be achieved if the transfer function of the device or medium has a linear phase response (i.e., formula_46 where the group delay formula_47 is a constant).
The degree of nonlinearity of the phase indicates the deviation of the group delay from a constant.
Group delay in audio.
Group delay has some importance in the audio field and especially in the sound reproduction field. Many components of an audio reproduction chain, notably loudspeakers and multiway loudspeaker crossover networks, introduce group delay in the audio signal. It is therefore important to know the threshold of audibility of group delay with respect to frequency, especially if the audio chain is supposed to provide high fidelity reproduction. The best thresholds of audibility table has been provided by .
Flanagan, Moore and Stone conclude that at 1, 2 and 4 kHz, a group delay of about 1.6 ms is audible with headphones in a non-reverberant condition.

</doc>
<doc id="41223" url="https://en.wikipedia.org/wiki?curid=41223" title="Guided ray">
Guided ray

A guided ray (also bound ray or trapped ray) is a ray of light in a multi-mode optical fiber, which is confined by the core. For step index fiber, light entering the fiber will be guided if it falls within the acceptance cone of the fiber, that is if it makes an angle with the fiber axis that is less than the acceptance angle,
where
This result can be derived from Snell's law by considering the critical angle.
Rays that fall within this angular range are reflected from the core-cladding boundary by total internal reflection, and so are confined by the core. The confinement of light by the fiber can also be described in terms of bound modes or guided modes. This treatment is necessary when considering singlemode fiber, since the ray model does not accurately describe the propagation of light in this type of fiber.

</doc>
<doc id="41224" url="https://en.wikipedia.org/wiki?curid=41224" title="Hagelbarger code">
Hagelbarger code

In telecommunication, a Hagelbarger code is a convolutional code that enables error bursts to be corrected provided that there are relatively long error-free intervals between the error bursts. 
In the Hagelbarger code, inserted parity check bits are spread out in time so that an error burst is not likely to affect more than one of the groups in which parity is checked.

</doc>
<doc id="41225" url="https://en.wikipedia.org/wiki?curid=41225" title="Halftone characteristic">
Halftone characteristic

In a facsimile system the halftone characteristic is either:

</doc>
<doc id="41226" url="https://en.wikipedia.org/wiki?curid=41226" title="Hamming code">
Hamming code

In telecommunication, Hamming codes are a family of linear error-correcting codes that generalize the Hamming(7,4)-code, and were invented by Richard Hamming in 1950. Hamming codes can detect up to two-bit errors or correct one-bit errors without detection of uncorrected errors. By contrast, the simple parity code cannot correct errors, and can detect only an odd number of bits in error. Hamming codes are perfect codes, that is, they achieve the highest possible rate for codes with their block length and minimum distance of three.
In mathematical terms, Hamming codes are a class of binary linear codes. For each integer there is a code with block length and message length . Hence the rate of Hamming codes is , which is the highest possible for codes with minimum distance of three (i.e., the minimal number of bit changes needed to go from any code word to any other code word is three) and block length . The parity-check matrix of a Hamming code is constructed by listing all columns of length that are non-zero, which means that the dual code of the Hamming code is the punctured Hadamard code. The parity-check matrix has the property that any two columns are pairwise linearly independent.
Due to the limited redundancy that Hamming codes add to the data, they can only detect and correct errors when the error rate is low. This is the case in computer memory (ECC memory), where bit errors are extremely rare and Hamming codes are widely used. In this context, an extended Hamming code having one extra parity bit is often used. Extended Hamming codes achieve a Hamming distance of four, which allows the decoder to distinguish between when at most one one-bit error occurs and when any two-bit errors occur. In this sense, extended Hamming codes are single-error correcting and double-error detecting, abbreviated as SECDED.
History.
Richard Hamming, the inventor of Hamming codes, worked at Bell Labs in the 1940s on the Bell Model V computer, an electromechanical relay-based machine with cycle times in seconds. Input was fed in on punched cards, which would invariably have read errors. During weekdays, special code would find errors and flash lights so the operators could correct the problem. During after-hours periods and on weekends, when there were no operators, the machine simply moved on to the next job.
Hamming worked on weekends, and grew increasingly frustrated with having to restart his programs from scratch due to the unreliability of the card reader. Over the next few years, he worked on the problem of error-correction, developing an increasingly powerful array of algorithms. In 1950, he published what is now known as Hamming Code, which remains in use today in applications such as ECC memory.
Codes predating Hamming.
A number of simple error-detecting codes were used before Hamming codes, but none was as effective as Hamming codes in the same overhead of space.
Parity.
Parity adds a single bit that indicates whether the number of ones (bit-positions with values of one) in the preceding data was even or odd. If an odd number of bits is changed in transmission, the message will change parity and the error can be detected at this point; however, the bit that changed may have been the parity bit itself. The most common convention is that a parity value of one indicates that there is an odd number of ones in the data, and a parity value of zero indicates that there is an even number of ones. If the number of bits changed is even, the check bit will be valid and the error will not be detected.
Moreover, parity does not indicate which bit contained the error, even when it can detect it. The data must be discarded entirely and re-transmitted from scratch. On a noisy transmission medium, a successful transmission could take a long time or may never occur. However, while the quality of parity checking is poor, since it uses only a single bit, this method results in the least overhead.
Two-out-of-five code.
A two-out-of-five code is an encoding scheme which uses five bits consisting of exactly three 0s and two 1s. This provides ten possible combinations, enough to represent the digits 0–9. This scheme can detect all single bit-errors, all odd numbered bit-errors and some even numbered bit-errors (for example the flipping of both 1-bits). However it still cannot correct for any of these errors.
Repetition.
Another code in use at the time repeated every data bit multiple times in order to ensure that it was sent correctly. For instance, if the data bit to be sent is a 1, an "repetition code" will send 111. If the three bits received are not identical, an error occurred during transmission. If the channel is clean enough, most of the time only one bit will change in each triple. Therefore, 001, 010, and 100 each correspond to a 0 bit, while 110, 101, and 011 correspond to a 1 bit, as though the bits count as "votes" towards what the intended bit is. A code with this ability to reconstruct the original message in the presence of errors is known as an "error-correcting" code. This triple repetition code is a Hamming code with since there are two parity bits, and data bit.
Such codes cannot correctly repair all errors, however. In our example, if the channel flips two bits and the receiver gets 001, the system will detect the error, but conclude that the original bit is 0, which is incorrect. If we increase the number of times we duplicate each bit to four, we can detect all two-bit errors but cannot correct them (the votes "tie"); at five repetitions, we can correct all two-bit errors, but not all three-bit errors.
Moreover, the repetition code is extremely inefficient, reducing throughput by three times in our original case, and the efficiency drops drastically as we increase the number of times each bit is duplicated in order to detect and correct more errors.
Hamming codes.
If more error-correcting bits are included with a message, and if those bits can be arranged such that different incorrect bits produce different error results, then bad bits could be identified. In a seven-bit message, there are seven possible single bit errors, so three error control bits could potentially specify not only that an error occurred but also which bit caused the error.
Hamming studied the existing coding schemes, including two-of-five, and generalized their concepts. To start with, he developed a nomenclature to describe the system, including the number of data bits and error-correction bits in a block. For instance, parity includes a single bit for any data word, so assuming ASCII words with seven bits, Hamming described this as an "(8,7)" code, with eight bits in total, of which seven are data. The repetition example would be "(3,1)", following the same logic. The code rate is the second number divided by the first, for our repetition example, 1/3.
Hamming also noticed the problems with flipping two or more bits, and described this as the "distance" (it is now called the "Hamming distance", after him). Parity has a distance of 2, so one bit flip can be detected, but not corrected and any two bit flips will be invisible. The (3,1) repetition has a distance of 3, as three bits need to be flipped in the same triple to obtain another code word with no visible errors. It can correct one-bit errors or detect but not correct two-bit errors. A (4,1) repetition (each bit is repeated four times) has a distance of 4, so flipping three bits can be detected, but not corrected. When three bits flip in the same group there can be situations where attempting to correct will produce the wrong code word. In general, a code with distance "k" can detect but not correct errors.
Hamming was interested in two problems at once: increasing the distance as much as possible, while at the same time increasing the code rate as much as possible. During the 1940s he developed several encoding schemes that were dramatic improvements on existing codes. The key to all of his systems was to have the parity bits overlap, such that they managed to check each other as well as the data.
General algorithm.
The following general algorithm generates a single-error correcting (SEC) code for any number of bits.
The form of the parity is irrelevant. Even parity is simpler from the perspective of theoretical mathematics, but there is no difference in practice.
This general rule can be shown visually:
Shown are only 20 encoded bits (5 parity, 15 data) but the pattern continues indefinitely. The key thing about Hamming Codes that can be seen from visual inspection is that any given bit is included in a unique set of parity bits. To check for errors, check all of the parity bits. The pattern of errors, called the error syndrome, identifies the bit in error. If all parity bits are correct, there is no error. Otherwise, the sum of the positions of the erroneous parity bits identifies the erroneous bit. For example, if the parity bits in positions 1, 2 and 8 indicate an error, then bit 1+2+8=11 is in error. If only one parity bit indicates an error, the parity bit itself is in error.
As you can see, if you have formula_1 parity bits, it can cover bits from 1 up to formula_2. If we subtract out the parity bits, we are left with formula_3 bits we can use for the data. As formula_1 varies, we get all the possible Hamming codes:
If, in addition, an overall parity bit (bit 0) is included, the code can detect (but not correct) any two-bit error, making a SECDED code. The overall parity indicates whether the total number of errors is even or odd. If the basic Hamming code detects an error, but the overall parity says that there are an even number of errors, an uncorrectable 2-bit error has occurred.
Hamming codes with additional parity (SECDED).
Hamming codes have a minimum distance of 3, which means that the decoder can detect and correct a single error, but it cannot distinguish a double bit error of some codeword from a single bit error of a different codeword. Thus, they can detect double-bit errors only if correction is not attempted.
To remedy this shortcoming, Hamming codes can be extended by an extra parity bit. This way, it is possible to increase the minimum distance of the Hamming code to 4, which allows the decoder to distinguish between single bit errors and two-bit errors. Thus the decoder can detect and correct a single error and at the same time detect (but not correct) a double error. If the decoder does not attempt to correct errors, it can detect up to three errors.
This extended Hamming code is popular in computer memory systems, where it is known as "SECDED" (abbreviated from "single error correction, double error detection"). Particularly popular is the (72,64) code, a truncated (127,120) Hamming code plus an additional parity bit, which has the same space overhead as a (9,8) parity code.
[7,4] Hamming code.
In 1950, Hamming introduced the [7,4] Hamming code. It encodes four data bits into seven bits by adding three parity bits. It can detect and correct single-bit errors. With the addition of an overall parity bit, it can also detect (but not correct) double-bit errors.
Construction of G and H.
The matrix 
formula_5 is called a (canonical) generator matrix of a linear ("n","k") code,
and formula_6 is called a parity-check matrix.
This is the construction of G and H in standard (or systematic) form. Regardless of form, G and H for linear block codes must satisfy
formula_7, an all-zeros matrix.
Since 4, 3 = ["n", "k", "d"] = − 1, 2"m"−1−"m", "m". The parity-check matrix H of a Hamming code is constructed by listing all columns of length "m" that are pair-wise independent.
Thus H is a matrix whose left side is all of the nonzero n-tuples where order of the n-tuples in the columns of matrix does not matter. The right hand side is just the ("n" − "k")-identity matrix. 
So G can be obtained from H by taking the transpose of the left hand side of H with the identity k-identity matrix on the left hand side of G.
The code generator matrix formula_8 and the parity-check matrix formula_9 are:
formula_10
and
formula_11
Finally, these matrices can be mutated into equivalent non-systematic codes by the following operations:
Encoding.
From the above matrix we have 2k = 24 = 16 codewords.
The codewords formula_12 of this binary code can be obtained from formula_13. With formula_14 with formula_15 exist in formula_16 (A field with two elements namely 0 and 1).
Thus the codewords are all the 4-tuples ("k"-tuples).
Therefore,
(1,0,1,1) gets encoded as (1,0,1,1,0,1,0).
[7,4] Hamming code with an additional parity bit.
The Hamming code can easily be extended to an [8,4 code by adding an extra parity bit on top of the (7,4) encoded word (see Hamming(7,4)).
This can be summed up with the revised matrices:
and
Note that H is not in standard form. To obtain G, elementary row operations can be used to obtain an equivalent matrix to H in systematic form:
For example, the first row in this matrix is the sum of the second and third rows of H in non-systematic form. Using the systematic construction for Hamming codes from above, the matrix A is apparent and the systematic form of G is written as
The non-systematic form of G can be row reduced (using elementary row operations) to match this matrix.
The addition of the fourth row effectively computes the sum of all the codeword bits (data and parity) as the fourth parity bit.
For example, 1011 is encoded into 01100110 where blue digits are data; red digits are parity from the Hamming code; and the green digit is the parity added by [8,4 code.
The green digit makes the parity of the [7,4] code even.
Finally, it can be shown that the minimum distance has increased from 3, as with the code, to 4 with the [8,4 code. Therefore, the code can be defined as [8,4] Hamming code. 

</doc>
<doc id="41227" url="https://en.wikipedia.org/wiki?curid=41227" title="Hamming distance">
Hamming distance

In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of "substitutions" required to change one string into the other, or the minimum number of "errors" that could have transformed one string into the other.
A major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field.
Examples.
The Hamming distance between:
Properties.
For a fixed length "n", the Hamming distance is a metric on the set of the words of length n (also known as a Hamming space), as it fulfills the conditions of non-negativity, identity of indiscernibles and symmetry, and it can be shown by complete induction that it satisfies the triangle inequality as well. The Hamming distance between two words "a" and "b" can also be seen as the Hamming weight of "a"−"b" for an appropriate choice of the − operator.
For binary strings "a" and "b" the Hamming distance is equal to the number of ones (population count) in "a" XOR "b". The metric space of length-"n" binary strings, with the Hamming distance, is known as the "Hamming cube"; it is equivalent as a metric space to the set of distances between vertices in a hypercube graph. One can also view a binary string of length "n" as a vector in formula_1 by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an "n"-dimensional hypercube, and the Hamming distance of the strings is equivalent to the Manhattan distance between the vertices.
Error detection and error correction.
The Hamming distance is used to define some essential notions in coding theory, such as error detecting and error correcting codes. In particular, a code "C" is said to be "k-errors detecting" if any two codewords "c"1 and "c"2 from "C" that have a Hamming distance less than "k" coincide; otherwise, a code is "k"-errors detecting if, and only if, the minimum Hamming distance between any two of its codewords is at least "k"+1.
A code "C" is said to be "k-errors correcting" if, for every word "w" in the underlying Hamming space "H", there exists at most one codeword "c" (from "C") such that the Hamming distance between "w" and "c" is less than "k". In other words, a code is "k"-errors correcting if, and only if, the minimum Hamming distance between any two of its codewords is at least 2"k"+1. This is more easily understood geometrically as any closed balls of radius "k" centered on distinct codewords being disjoint. These balls are also called Hamming spheres in this context.
Thus a code with minimum Hamming distance "d" between its codewords can detect at most "d"-1 errors and can correct ⌊("d"-1)/2⌋ errors. The latter number is also called the "packing radius" or the "error-correcting capability" of the code.
History and applications.
The Hamming distance is named after Richard Hamming, who introduced it in his fundamental paper on Hamming codes "Error detecting and error correcting codes" in 1950. Hamming weight analysis of bits is used in several disciplines including information theory, coding theory, and cryptography.
It is used in telecommunication to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the signal distance. For "q"-ary strings over an alphabet of size "q" ≥ 2 the Hamming distance is applied in case of the q-ary symmetric channel, while the Lee distance is used for phase-shift keying or more generally channels susceptible to synchronization errors because the Lee distance accounts for errors of ±1. If "q" = 2 or "q" = 3 both distances coincide because Z/2Z and Z/3Z are also fields, but Z/4Z is not a field but only a ring.
The Hamming distance is also used in systematics as a measure of genetic distance.
However, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric like the Levenshtein distance is more appropriate.
Algorithm example.
The Python3 function codice_1 computes the Hamming distance between
two strings (or other iterable objects) of equal length, by creating a sequence of Boolean values indicating mismatches and matches between corresponding positions in the two inputs, and then summing the sequence with False and True values being interpreted as zero and one.
<syntaxhighlight lang="python3">
def hammingDistance(s1, s2):
</syntaxhighlight>
Or in Ruby language the function codice_1 could be:
<syntaxhighlight lang="ruby">
def hammingDistance(s1, s2)
end
</syntaxhighlight>
The following C function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the bitwise exclusive or of the two inputs, and then finds the Hamming weight of the result (the number of nonzero bits) using an algorithm of that repeatedly finds and clears the lowest-order nonzero bit. Some compilers support the __builtin_popcount function which can calculate this using specialized processor hardware where available.
<syntaxhighlight lang="c">
int hamming_distance(unsigned x, unsigned y)
</syntaxhighlight>

</doc>
<doc id="41229" url="https://en.wikipedia.org/wiki?curid=41229" title="Handshaking">
Handshaking

In information technology, telecommunications, and related fields, handshaking is an automated process of negotiation that dynamically sets parameters of a communications channel established between two entities before normal communication over the channel begins. It follows the physical establishment of the channel and precedes normal information transfer.
The handshaking process usually takes place in order to establish rules for communication when a computer sets about communicating with a foreign device. When a computer communicates with another device like a modem, printer, or network server, it needs to handshake with it to establish a connection.
Handshaking can negotiate parameters that are acceptable to equipment and systems at both ends of the communication channel, including information transfer rate, coding alphabet, parity, interrupt procedure, and other protocol or hardware features.
Handshaking is a technique of communication between two entities. However, within TCP/IP RFCs, the term "handshake" is most commonly used to reference the TCP three-way handshake. For example, the term "handshake" is not present in RFCs covering FTP or SMTP. One exception is Transport Layer Security, TLS, setup, FTP RFC 4217. In place of the term "handshake", FTP RFC 3659 substitutes the term "conversation" for the passing of commands.
A simple handshaking protocol might only involve the receiver sending a message meaning "I received your last message and I am ready for you to send me another one." A more complex handshaking protocol might allow the sender to ask the receiver if it is ready to receive or for the receiver to reply with a negative acknowledgement meaning "I did not receive your last message correctly, please resend it" (e.g., if the data was corrupted en route).
Handshaking facilitates connecting relatively heterogeneous systems or equipment over a communication channel without the need for human intervention to set parameters. 
Examples.
TCP three-way handshake.
Establishing a normal TCP connection requires three separate steps:
One of the most important factors of three-way handshake is that, in order to exchange the starting sequence number the two sides plan to use, the client first sends a segment with its own initial sequence number formula_1, then the server responds by sending a segment with its own sequence number formula_2 and the acknowledgement number formula_3, and finally the client responds by sending a segment with acknowledgement number formula_4.
The reason for the client and server not using the default sequence number such as 0 for establishing connection is to protect against two incarnations of the same connection reusing the same sequence number too soon, which means a segment from an earlier incarnation of a connection might interfere with a later incarnation of the connection.
SMTP.
The Simple Mail Transfer Protocol (SMTP) is the key Internet standard for email transmission. It includes handshaking to negotiate authentication, encryption and maximum message size. 
TLS handshake.
When a Transport Layer Security (SSL or TLS) connection starts, the record encapsulates a "control" protocol—the handshake messaging protocol (content type 22). This protocol is used to exchange all the information required by both sides for the exchange of the actual application data by TLS. It defines the messages formatting or containing this information and the order of their exchange. These may vary according to the demands of the client and server—i.e., there are several possible procedures to set up the connection. This initial exchange results in a successful TLS connection (both parties ready to transfer application data with TLS) or an alert message (as specified below).
The protocol is used to negotiate the secure attributes of a session. (RFC 5246, p. 37)
WPA2 wireless.
The WPA2 standard for wireless uses a four-way handshake defined in IEEE 802.11i-2004.
Dial-up access modems.
One classic example of handshaking is that of dial-up modems, which typically negotiate communication parameters for a brief period when a connection is first established, and thereafter use those parameters to provide optimal information transfer over the channel as a function of its quality and capacity. The "squealing" (which is actually a sound that changes in pitch 100 times every second) noises made by some modems with speaker output immediately after a connection is established are in fact the sounds of modems at both ends engaging in a handshaking procedure; once the procedure is completed, the speaker might be silenced, depending on the settings of operating system or the application controlling the modem.

</doc>
<doc id="41230" url="https://en.wikipedia.org/wiki?curid=41230" title="Hard copy">
Hard copy

In information handling, a hard copy is a permanent reproduction, or copy, in the form of a physical object, of any media suitable for direct use by a person (in particular paper), of displayed or transmitted data. Examples of hard copy include teleprinter pages, continuous printed tapes, computer printouts, and radio photo prints.
Hard Copy Records, such as printed forms, tab cards, and OCR forms, are best designed using a record layout.
Magnetic tapes, diskettes, and non-printed punched paper tapes are not hard copies.
The term "hard copy" predates the age of the digital computer. In the process of producing printed books and newspapers, hard copy refers to a manuscript or typewritten document that has been edited and proofread, and is ready for typesetting, or being read on-air in a radio or television broadcast. This traditional meaning has been all but forgotten in the wake of the information revolution.
"Dead-tree edition".
"Dead-tree edition" refers to a printed paper version of a written work, as opposed to digital alternatives such as a web page. It is a dysphemism for "hard copy". Variations include "dead-tree format" and "dead-tree-ware". "Dead-tree" refers to trees being cut down for raw material for producing paper. Newspapers are, sometimes pejoratively, referred to as "the dead-tree-press". "The Guardian" website on 29 November 2006 wrote:Maybe this is more a multimedia victory for Jeff Randall himself: he did manage a "dead-tree" front page, web scoop, vodcast and major plug on the 10 O'clock news.
A related saying among computer fans is ""You can't grep dead trees"", from the Unix command grep meaning to search the contents of text files. This means that an advantage of keeping documents in digital form rather than on paper is that they can be more easily searched for specific contents. An exception are texts stored as digital images (digital facsimile), as they cannot be easily searched, except by sophisticated means such as optical character recognition or examining the image metadata. On the other hand, paper copies have tremendous data integrity in proper conditions.

</doc>
<doc id="41231" url="https://en.wikipedia.org/wiki?curid=41231" title="Hard sectoring">
Hard sectoring

Hard sectoring in a magnetic or optical data storage device is a form of sectoring which uses a physical mark or hole in the recording medium to reference sector locations.
In older 8- and 5-inch floppy disks, hard sectoring was implemented by punching sector holes in the disk to mark the start
of each sector. These were equally spaced holes, at a common radius. This was in addition to the index hole, situated between two sector holes, to mark the start of the entire track of sectors. When the index or sector hole was recognized by an optical sensor, a sector signal was generated. Timing electronics or software would use the faster timing of the index hole between sector holes, to generate an index signal. Data read and write is faster in this technique than soft sectoring as no operations are to be performed regarding the starting and ending points of tracks.

</doc>
<doc id="41232" url="https://en.wikipedia.org/wiki?curid=41232" title="Harmonic">
Harmonic

The term harmonic in its strictest sense describes any member of the harmonic series. The term is employed in various disciplines, including music and acoustics, electronic power transmission, radio technology, etc. It is typically applied to repeating signals, such as sinusoidal waves. A harmonic of such a wave is a wave with a frequency that is a positive integer multiple of the frequency of the original wave, known as the fundamental frequency. The original wave is also called 1st harmonic, the following harmonics are known as higher harmonics. As all harmonics are periodic at the fundamental frequency, the sum of harmonics is also periodic at that frequency. For example, if the fundamental frequency is 50 Hz, a common AC power supply frequency, the frequencies of the first three higher harmonics are 100 Hz (2nd harmonic), 150 Hz (3rd harmonic), 200 Hz (4th harmonic) and any addition of waves with these frequencies is periodic at 50 Hz.
Characteristics.
Most acoustic instruments emit complex tones containing many individual partials (component simple tones or sinusoidal waves), but the untrained human ear typically does not perceive those partials as separate phenomena. Rather, a musical note is perceived as one sound, the quality or timbre of that sound being a result of the relative strengths of the individual partials. 
Many acoustic oscillators, such as the human voice or a bowed violin string, produce complex tones that are more or less periodic, and thus are composed of partials that are near matches to integer multiples of the fundamental frequency and therefore resemble the ideal harmonics and are called "harmonic partials" or simply "harmonics" for convenience (although it's not strictly accurate to call a partial a harmonic, the first being real and the second being ideal). Oscillators that produce harmonic partials behave somewhat like 1-dimensional resonators, and are often long and thin, such as a guitar string or a column of air open at both ends (as with the modern orchestral transverse flute). Wind instruments whose air column is open at only one end, such as trumpets and clarinets, also produce partials resembling harmonics. However they only produce partials matching the odd harmonics, at least in theory. The reality of acoustic instruments is such that none of them behaves as perfectly as the somewhat simplified theoretical models would predict.
Partials whose frequencies are not integer multiples of the fundamental are referred to as "inharmonic partials". 
Some acoustic instruments emit a mix of harmonic and inharmonic partials but still produce an effect on the ear of having a definite fundamental pitch, such as pianos, strings plucked pizzicato, vibraphones, marimbas, and certain pure-sounding bells or chimes. Antique singing bowls are known for producing multiple harmonic partials or multiphonics.
Other oscillators, such as cymbals, drum heads, and other percussion instruments, naturally produce an abundance of inharmonic partials and do not imply any particular pitch, and therefore cannot be used melodically or harmonically in the same way other instruments can.
Partials, overtones, and harmonics.
A overtone is any partial higher than the lowest partial in a compound tone. The relative strengths and frequency relationships of the component partials determine the timbre of an instrument. The similarity between the terms overtone and partial sometimes leads to their being loosely used interchangeably in a musical context, but they are counted differently, leading to some possible confusion. In the special case of instrumental timbres whose component partials closely match a harmonic series (such as with most strings and winds) rather than being inharmonic partials (such as with most pitched percussion instruments), it is also convenient to call the component partials "harmonics" but not strictly correct (because harmonics are numbered the same even when missing, while partials and overtones are only counted when present). This chart demonstrates how the three types of names (partial, overtone, and harmonic) are counted (assuming that the harmonics are present):
In many musical instruments, it is possible to play the upper harmonics without the fundamental note being present. In a simple case (e.g., recorder) this has the effect of making the note go up in pitch by an octave, but in more complex cases many other pitch variations are obtained. In some cases it also changes the timbre of the note. This is part of the normal method of obtaining higher notes in wind instruments, where it is called "overblowing". The extended technique of playing multiphonics also produces harmonics. On string instruments it is possible to produce very pure sounding notes, called harmonics or "flageolets" by string players, which have an eerie quality, as well as being high in pitch. Harmonics may be used to check at a unison the tuning of strings that are not tuned to the unison. For example, lightly fingering the node found halfway down the highest string of a cello produces the same pitch as lightly fingering the node 1/3 of the way down the second highest string. For the human voice see Overtone singing, which uses harmonics. 
While it is true that electronically produced periodic tones (e.g. square waves or other non-sinusoidal waves) have "harmonics" that are whole number multiples of the fundamental frequency, practical instruments do not all have this characteristic. For example higher "harmonics"' of piano notes are not true harmonics but are "overtones" and can be very sharp, i.e. a higher frequency than given by a pure harmonic series. This is especially true of instruments other than stringed or brass/woodwind ones, e.g., xylophone, drums, bells etc., where not all the overtones have a simple whole number ratio with the fundamental frequency.
The fundamental frequency is the reciprocal of the period of the periodic phenomenon. 
Harmonics on stringed instruments.
The following table displays the stop points on a stringed instrument, such as the guitar (guitar harmonics), at which gentle touching of a string will force it into a harmonic mode when vibrated. String harmonics (flageolet tones) are described as having a "flutelike, silvery quality that can be highly effective as a special color" when used and heard in orchestration. It is unusual to encounter natural harmonics higher than the fifth partial on any stringed instrument except the double bass, on account of its much longer strings.
Artificial harmonics.
Although harmonics are most often used on open strings, occasionally a score will call for an artificial harmonic, produced by playing an overtone on a stopped string. As a performance technique, it is accomplished by using two fingers on the fingerboard, the first to shorten the string to the desired fundamental, with the second touching the node corresponding to the appropriate harmonic.
Other information.
Harmonics may be either used or considered as the basis of just intonation systems. 
Composer Arnold Dreyblatt is able to bring out different harmonics on the single string of his modified double bass by slightly altering his unique bowing technique halfway between hitting and bowing the strings. 
Composer Lawrence Ball uses harmonics to generate music electronically.

</doc>
<doc id="41233" url="https://en.wikipedia.org/wiki?curid=41233" title="H channel">
H channel

In the Integrated Services Digital Network (ISDN), a high-speed channel comprising multiple aggregated low-speed channels to accommodate bandwidth-intensive applications such as file transfer, videoconferencing, and high-quality audio. An H channel is formed of multiple bearer B channels bonded together in a primary rate access (PRA) or primary rate interface (PRI) frame in support of applications with bandwidth requirements that exceed the B channel rate of 64 kbit/s. The channels, once bonded, remain so end-to-end, from transmitter to receiver, through the ISDN network. The feature is known variously as multirate ISDN, Nx64, channel aggregation, and bonding.
H channels are implemented as:

</doc>
<doc id="41234" url="https://en.wikipedia.org/wiki?curid=41234" title="Heterodyne">
Heterodyne

Heterodyning is a radio signal processing technique invented in 1901 by Canadian inventor-engineer Reginald Fessenden, in which new frequencies are created by combining or mixing two frequencies. Heterodyning is used to shift one frequency range into another, new one, and is also involved in the processes of modulation and demodulation. The two frequencies are combined in a nonlinear signal-processing device such as a vacuum tube, transistor, or diode, usually called a "mixer". In the most common application, two signals at frequencies and are mixed, creating two new signals, one at the sum of the two frequencies, and the other at the difference . These new frequencies are called heterodynes. Typically only one of the new frequencies is desired, and the other signal is filtered out of the output of the mixer. Heterodynes are related to the phenomenon of "beats" in acoustics.
A major application of the heterodyne process is in the superheterodyne radio receiver circuit, which is used in virtually all modern radio receivers.
History.
In 1901, Reginald Fessenden demonstrated a direct-conversion heterodyne receiver or beat receiver as a method of making continuous wave radiotelegraphy signals audible. Fessenden's receiver did not see much application because of its local oscillator's stability problem. While complex isochronous electromechanical oscillators existed, a stable yet inexpensive local oscillator would not be available until Lee de Forest's invention of the triode vacuum tube oscillator. In a 1905 patent, Fessenden stated the frequency stability of his local oscillator was one part per thousand.
Early spark gap radio transmitters sent information exclusively by means of radio telegraphy. In radio telegraphy, the characters of text messages are translated into the short duration dots and long duration dashes of Morse code that are broadcast as bursts of radio waves. The heterodyne detector was not needed to hear the signals produced by these spark gap transmitters. The transmitted damped wave signals were amplitude modulated at an audio frequency by the spark. A simple detector produced an audible buzzing sound in the radiotelegraph operator's headphones that could be transcribed back into alpha-numeric characters.
With the advent of the arc converter, continuous wave (CW) transmitters were adopted. CW Morse code signals are not amplitude modulated, so a different detector was needed. The direct-conversion detector was invented to make continuous wave radio-frequency signals audible.
The "heterodyne" or "beat" receiver has a local beat frequency oscillator (BFO) that produces a radio signal adjusted to be close in frequency to the incoming signal being received. When the two signals are mixed, a "beat" frequency equal to the difference between the two frequencies is created. By adjusting the local oscillator frequency correctly, the beat frequency is in the audio range, and can be heard as a tone in the receiver's earphones whenever the transmitter signal is present. Thus the Morse code "dots" and "dashes" are audible as beeping sounds. This technique is still used in radio telegraphy, the local oscillator now being called the beat frequency oscillator or BFO. Fessenden coined the word "heterodyne" from the Greek roots "hetero-" "different", and "dyn-" "power" (cf. ).
Superheterodyne receiver.
The most important and widely used application of the heterodyne technique is in the superheterodyne receiver (superhet), invented by U.S. engineer Edwin Howard Armstrong in 1918. In this circuit, the incoming radio frequency signal from the antenna is mixed with a signal from a local oscillator and converted by the heterodyne technique to a somewhat lower fixed frequency signal called the intermediate frequency (IF). This IF signal is amplified and filtered, before being applied to a detector which extracts the audio signal, which is sent to the loudspeaker.
The advantage of this technique is that the different frequencies of the different stations received are all converted to the "same" IF before amplification and filtering. The complicated amplifier and bandpass filter stages, which in previous receivers had to be made tunable to work at the different station frequencies, in the superheterodyne can be built to work at one fixed frequency, the IF, simplifying their design. Another advantage is that the IF is at a considerably lower frequency than the RF frequency of the incoming radio signal.
The superior superheterodyne system replaced the earlier TRF and regenerative receiver designs, and since the 1930s almost all commercial radio receivers have been superheterodynes.
Applications.
Heterodyning, also called "frequency conversion", is used very widely in communications engineering to generate new frequencies and move information from one frequency channel to another. Besides its use in the superheterodyne circuit which is found in almost all radio and television receivers, it is used in radio transmitters, modems, satellite communications and set-top boxes, radar, radio telescopes, telemetry systems, cell phones, cable television converter boxes and headends, microwave relays, metal detectors, atomic clocks, and military electronic countermeasures (jamming) systems.
Up and down converters.
In large scale telecommunication networks such as telephone network trunks, microwave relay networks, cable television systems, and communication satellite links, large bandwidth capacity links are shared by many individual communication channels by using heterodyning to move the frequency of the individual signals up to different frequencies, which share the channel. This is called frequency division multiplexing (FDM).
For example, a coaxial cable used by a cable television system can carry 500 television channels at the same time because each one is given a different frequency, so they don't interfere with one another. At the cable source or headend, electronic upconverters convert each incoming television channel to a new, higher frequency. They do this by mixing the television signal frequency, "fCH" with a local oscillator at a much higher frequency , creating a heterodyne at the sum , which is added to the cable. At the consumer's home, the cable set top box has a downconverter that mixes the incoming signal at frequency with the same local oscillator frequency creating the difference heterodyne, converting the television channel back to its original frequency: . Each channel is moved to a different higher frequency. The original lower basic frequency of the signal is called the baseband, while the higher channel it is moved to is called the passband.
Analog videotape recording.
Many analog videotape systems rely on a downconverted color subcarrier in order to record color information in their limited bandwidth. These systems are referred to as "heterodyne systems" or "color-under systems". For instance, for NTSC video systems, the VHS (and S-VHS) recording system converts the color subcarrier from the NTSC standard 3.58 MHz to ~629 kHz. PAL VHS color subcarrier is similarly downconverted (but from 4.43 MHz). The now-obsolete 3/4" U-matic systems use a heterodyned ~688 kHz subcarrier for NTSC recordings (as does Sony's Betamax, which is at its basis a 1/2″ consumer version of U-matic), while PAL U-matic decks came in two mutually incompatible varieties, with different subcarrier frequencies, known as Hi-Band and Low-Band. Other videotape formats with heterodyne color systems include Video-8 and Hi8.
The heterodyne system in these cases is used to convert quadrature phase-encoded and amplitude modulated sine waves from the broadcast frequencies to frequencies recordable in less than 1 MHz bandwidth. On playback, the recorded color information is heterodyned back to the standard subcarrier frequencies for display on televisions and for interchange with other standard video equipment.
Some U-matic (3/4″) decks feature 7-pin mini-DIN connectors to allow dubbing of tapes without a heterodyne up-conversion and down-conversion, as do some industrial VHS, S-VHS, and Hi8 recorders.
Music synthesis.
The theremin, an electronic musical instrument, traditionally uses the heterodyne principle to produce a variable audio frequency in response to the movement of the musician's hands in the vicinity of one or more antennas, which act as capacitor plates. The output of a fixed radio frequency oscillator is mixed with that of an oscillator whose frequency is affected by the variable capacitance between the antenna and the thereminist as that person moves her or his hand near the pitch control antenna. The difference between the two oscillator frequencies produces a tone in the audio range.
The ring modulator is a type of heterodyne incorporated into some synthesizers or used as a stand-alone audio effect.
Optical heterodyning.
Optical heterodyne detection (an area of active research) is an extension of the heterodyning technique to higher (visible) frequencies. This technique could greatly improve optical modulators, increasing the density of information carried by optical fibers. It is also being applied in the creation of more accurate atomic clocks based on directly measuring the frequency of a laser beam. See NIST subtopic 9.07.9-4.R for a description of research on one system to do this.
Since optical frequencies are far beyond the manipulation capacity of any feasible electronic circuit, all photon detectors are inherently energy detectors not oscillating electric field detectors. However, since energy detection is inherently "square-law" detection, it intrinsically mixes any optical frequencies present on the detector. Thus, sensitive detection of specific optical frequencies necessitates optical heterodyne detection, in which two different (close-by) wavelengths of light illuminate the detector so that the oscillating electrical output corresponds to the difference between their frequencies. This allows extremely narrow band detection (much narrower than any possible color filter can achieve) as well as precision measurements of phase and frequency of a light signal relative to a reference light source, as in a laser Doppler vibrometer.
This phase sensitive detection has been applied for Doppler measurements of wind speed, and imaging through dense media. The high sensitivity against background light is especially useful for lidar.
In optical Kerr effect (OKE) spectroscopy, optical heterodyning of the OKE signal and a small part of the probe signal produces a mixed signal consisting of probe, heterodyne OKE-probe and homodyne OKE signal. The probe and homodyne OKE signals can be filtered out, leaving the heterodyne signal for detection.
Mathematical principle.
Heterodyning is based on the trigonometric identity:
The product on the left hand side represents the multiplication ("mixing") of a sine wave with another sine wave. The right hand side shows that the resulting signal is the difference of two sinusoidal terms, one at the sum of the two original frequencies, and one at the difference, which can be considered to be separate signals.
Using this trigonometric identity, the result of multiplying two sine wave signals, formula_2 and formula_3 can be calculated:
The result is the sum of two sinusoidal signals, one at the sum and one at the difference of the original frequencies
Mixer.
The two signals are combined in a device called a "mixer". It can be seen from the previous section that an ideal mixer would be a device that multiplies the two signals. Some widely used mixer circuits, such as the Gilbert cell, operate in this way, but they are limited to lower frequencies. However, any "nonlinear" electronic component will also multiply signals applied to it, producing heterodyne frequencies in its output, so a variety of nonlinear components are used as mixers. A nonlinear component is one in which the output current or voltage is a nonlinear function of its input. Most circuit elements in communications circuits are designed to be linear. This means they obey the superposition principle; if "F"("v") is the output of a linear element with an input of "v":
So if two sine wave signals at frequencies and are applied to a linear device, the output is simply the sum of the outputs when the two signals are applied separately with no product terms. Thus, the function must be nonlinear to create heterodynes (mixer products). A perfect multiplier only produces mixer products at the sum and difference frequencies , but more general nonlinear functions produce higher order mixer products: for integers and . Some mixer designs, such as double-balanced mixers, suppress some high order undesired products, while other designs, such as harmonic mixers exploit high order differences.
Examples of nonlinear components that are used as mixers are vacuum tubes and transistors biased near cutoff (class C), and diodes. Ferromagnetic core inductors driven into saturation can also be used at lower frequencies. In nonlinear optics, crystals that have nonlinear characteristics are used to mix laser light beams to create heterodynes at optical frequencies.
Output of a mixer.
To demonstrate mathematically how a nonlinear component can multiply signals and generate heterodyne frequencies, the nonlinear function "F" can be expanded in a power series (MacLaurin series):
To simplify the math, the higher order terms above will be indicated by an ellipsis (". . .") and only the first terms will be shown. Applying the two sine waves at frequencies and to this device:
It can be seen that the second term above contains a product of the two sine waves. Simplifying with trigonometric identities:
So the output contains sinusoidal terms with frequencies at the sum and difference of the two original frequencies. It also contains terms at the original frequencies and at multiples of the original frequencies , , , , etc.; the latter are called harmonics, as well as more complicated terms at frequencies of , called intermodulation products. These unwanted frequencies, along with the unwanted heterodyne frequency, must be filtered out of the mixer output by an electronic filter to leave the desired heterodyne.

</doc>
<doc id="41236" url="https://en.wikipedia.org/wiki?curid=41236" title="Heuristic routing">
Heuristic routing

Heuristic routing is a system used to describe how deliveries are made when problems in a network topology arise. Heuristic is an adjective used in relation to methods of learning, discovery, or problem solving. Routing is the process of selecting paths to specific destinations. Heuristic routing is used for traffic in the telecommunications networks and transport networks of the world.
Heuristic routing is achieved using specific algorithms to determine a better, although not always optimal, path to a destination. When an interruption in a network topology occurs, the software running on the networking electronics can calculate another route to the desired destination via an alternate available path.
According to :
The heuristic approach to problem solving consists of applying human intelligence, experience, common sense and certain rules of thumb (or heuristics) to develop an acceptable, but not necessarily an optimum, solution to a problem. Of course, determining what constitutes an acceptable solution is part of the task of deciding which approach to use; but broadly defined, an acceptable solution is one that is both reasonably good (close to optimum) and derived within reasonable effort, time, and cost constraints. Often the effort (manpower, computer, and other resources) required, the time limits on when the solution is needed, and the cost to compile, process, and analyze all the data required for deterministic or other complicated procedures preclude their usefulness or favor the faster, simpler heuristic approach. Thus, the heuristic approach is generally used when deterministic techniques or are not available, economical, or practical.
Heuristic routing allows a measure of route optimization in telecommunications networks based on recent empirical knowledge of the state of the network. Data, such as time delay, may be extracted from incoming messages, during specified periods and over different routes, and used to determine the optimum routing for transmitting data back to the sources. 
IP routing.
The IP routing protocols in use today are based on one of two algorithms: "distance vector" or "link state". Distance vector algorithms broadcast routing information to all neighboring routers. Link state routing protocols build a topographical map of the entire network based on updates from neighbor routers, and then use the Dijkstra algorithm to compute the shortest path to each destination. Metrics used are based on the number of hops, delay, throughput, traffic, and reliability.

</doc>
<doc id="41237" url="https://en.wikipedia.org/wiki?curid=41237" title="Hierarchical routing">
Hierarchical routing

Hierarchical routing is a method of routing in networks that is based on hierarchical addressing.
Background.
Most Transmission Control Protocol/Internet Protocol (TCP/IP) routing is based on a two-level hierarchical routing in which an IP address is divided into a network portion and a host portion. Gateways use only the network portion until an IP datagram reaches a gateway that can deliver it directly. Additional levels of hierarchical routing are introduced by the addition of subnetworks.
Description.
Hierarchical routing is the procedure of arranging routers in a hierarchical manner. A good example would be to consider a corporate intranet. Most corporate intranets consist of a high speed backbone network. Connected to this backbone are routers which are in turn connected to a particular workgroup. These workgroups occupy a unique LAN. The reason this is a good arrangement is because even though there might be dozens of different workgroups, the span (maximum hop count to get from one host to any other host on the network) is 2. Even if the workgroups divided their LAN network into smaller partitions, the span could only increase to 4 in this particular example. 
Considering alternative solutions with every router connected to every other router, or if every router was connected to 2 routers, shows the convenience of hierarchical routing. It decreases the complexity of network topology, increases routing efficiency, and causes much less congestion because of fewer routing advertisements. With hierarchical routing, only core routers connected to the backbone are aware of all routes. Routers that lie within a LAN only know about routes in the LAN. Unrecognized destinations are passed to the default route.

</doc>
<doc id="41239" url="https://en.wikipedia.org/wiki?curid=41239" title="High-performance equipment">
High-performance equipment

High-performance equipment describes telecommunications equipment that
"Note:" Requirements for global and tactical high-performance equipment may differ.

</doc>
<doc id="41240" url="https://en.wikipedia.org/wiki?curid=41240" title="Hop">
Hop

Hop or hops usually refers to a kind of small jump, usually using only one leg can also be on two.
It can also refer to:

</doc>
<doc id="41242" url="https://en.wikipedia.org/wiki?curid=41242" title="Horn">
Horn

Horn or Horns may refer to:

</doc>
<doc id="41243" url="https://en.wikipedia.org/wiki?curid=41243" title="Hotline">
Hotline

A hotline is a point-to-point communications link in which a call is automatically directed to the preselected destination without any additional action by the user when the end instrument goes off-hook. An example would be a phone that automatically connects to emergency services on picking up the receiver. Therefore, dedicated hotline phones do not need a rotary dial or keypad. A hotline can also be called an automatic signaling, ringdown, or off-hook service.
For crises and service.
True hotlines cannot be used to originate calls other than to preselected destinations. However, in common or colloquial usage, a "hotline" often refers to a call center reachable by dialing a standard telephone number, or sometimes the phone numbers themselves.
This is especially the case with 24-hour, noncommercial numbers, such as police tip hotlines or suicide crisis hotlines, which are manned around the clock and thereby give the appearance of real hotlines. Increasingly, however, the term is found being applied to any customer service telephone number.
Between states.
USA–Russia.
The most famous hotline between states is the Moscow–Washington hotline, which is also known as the "red telephone", although telephones have never been used in this capacity. This direct communications link was established on 20 June 1963, in the wake of the Cuban Missile Crisis, and utilized teletypewriter technology, later replaced by telecopier and then by electronic mail.
USA–UK.
Already during World War II—two decades before the Washington–Moscow hotline was established—there was a hotline between No. 10 Downing Street and the Cabinet War Room bunker under the Treasury, Whitehall; with the White House in Washington. From 1943–1946, this link was made secure by using the very first voice encryption machine, called SIGSALY.
Russia–China.
A hotline connection between Moscow and Beijing was used during the 1969 frontier confrontation between the two countries. The Chinese however refused the Russian peace attempts and ended the communications link. After a reconciliation between the former enemies, the hotline between China and Russia was revived in 1996.
Russia–France.
On his visit to the Soviet Union in 1966, French President Charles de Gaulle announced that a hotline would be established between Paris and Moscow. The line was upgraded from a telex to a high-speed fax machine in 1989.
Russia–UK.
A London–Moscow hotline was not formally established until a treaty of friendship between the two countries in 1992. An upgrade was announced when Foreign Secretary William Hague visited Moscow in 2011.
India–Pakistan.
On 20 June 2004, both India and Pakistan agreed to extend a nuclear testing ban and to set up an Islamabad–New Delhi hotline between their foreign secretaries aimed at preventing misunderstandings that might lead to nuclear war. The hotline was set up with the assistance of United States military officers.
USA–China.
The United States and China set up a defense hotline in 2008, but it has rarely been used in crises.
China–India.
India and China announced a hotline for the foreign ministers of both countries while reiterating their commitment to strengthening ties and building "mutual political trust". As of August 2015 the hotline was yet to be made operational.
China–Japan.
In February 2013, the Senkaku Islands dispute gave renewed impetus to a China–Japan hotline, which had been agreed to but due to rising tensions had not been established.
North and South Korea.
The Seoul–Pyongyang hotline was opened on 18 August 1972 and maintained by the Red Cross. North Korea deactivated the hotline on 11 March 2013, as part of increasing tensions on the Korean peninsula. North Korea reopened the hotline on 7 June 2013.
USA–India.
In August 2015 the hotline between the White House and New Delhi became operational. The decision of establishing this hotline was taken during Obama's visit to India in January 2015. This is the first hotline connecting an Indian Prime Minister to a head of state. 

</doc>
<doc id="41244" url="https://en.wikipedia.org/wiki?curid=41244" title="Hybrid (biology)">
Hybrid (biology)

In biology a hybrid, also known as cross breed, is the result of mixing, through sexual reproduction, two animals or plants of different breeds, varieties, species or genera. Using genetic terminology, it may be defined as follows.
From a taxonomic perspective, hybrid refers to:
Terminology.
The term hybrid is derived from Latin "hybrida", meaning the "offspring of a tame sow and a wild boar", "child of a freeman and slave", etc. The term came into popular use in English in the 19th century, though examples of its use have been found from the early 17th century.
There is a popular convention of naming hybrids by forming portmanteau words. 
The template for this is the naming of tiger-lion hybrids as liger and tigon in the 1920s.
This was playfully (but unsystematically) extended to a number of other hybrids, or hypothetical hybrids, such as beefalo (1960s), humanzee (1980s), cama (1998).
Types of hybrids.
Depending on the parents, there are a number of different types of hybrids;
Interspecific hybrids.
Interspecific hybrids are bred by mating two species, normally from within the same genus. The offspring display traits and characteristics of both parents. The offspring of an interspecific cross are very often sterile; thus, hybrid sterility prevents the movement of genes from one species to the other, keeping both species distinct. Sterility is often attributed to the different number of chromosomes the two species have, for example donkeys have 62 chromosomes, while horses have 64 chromosomes, and mules and hinnies have 63 chromosomes. Mules, hinnies, and other normally sterile interspecific hybrids cannot produce viable gametes, because differences in chromosome structure prevent appropriate pairing and segregation during meiosis, meiosis is disrupted, and viable sperm and eggs are not formed. However, fertility in female mules has been reported with a donkey as the father.
Most often other processes occurring in plants and animals keep gametic isolation and species distinction. Species often have different mating or courtship patterns or behaviors, the breeding seasons may be distinct and even if mating does occur antigenic reactions to the sperm of other species prevent fertilization or embryo development. Hybridisation is much more common among organisms that spawn indiscriminately, like soft corals and among plants.
While it is possible to predict the genetic composition of a backcross "on average", it is not possible to accurately predict the composition of a particular backcrossed individual, due to random segregation of chromosomes. In a species with two pairs of chromosomes, a twice backcrossed individual would be predicted to contain 12.5% of one species' genome (say, species A). However, it may, in fact, still be a 50% hybrid if the chromosomes from species A were lucky in two successive segregations, and meiotic crossovers happened near the telomeres. The chance of this is fairly high: formula_1 (where the "two times two" comes about from two rounds of meiosis with two chromosomes); however, this probability declines markedly with chromosome number and so the actual composition of a hybrid will be increasingly closer to the predicted composition.
Hybrid species.
While not very common, a few animal species have been recognized as being the result of hybridization. The Lonicera fly is an example of a novel animal species that resulted from natural hybridization. The American red wolf appears to be a hybrid species between gray wolf and coyote, although its taxonomic status has been a subject of controversy. The European edible frog appears to be a species, but is actually a semi-permanent hybrid between pool frogs and marsh frogs. The edible frog population is dependent on the presence of at least one of the parents species to be maintained.
Hybrid species of plants are much more common than animals. Many of the crop species are hybrids, and hybridization appears to be an important factor in speciation in some plant groups.
Hybrid plants.
Many hybrids are created by humans, but natural hybrids occur as well. Plant species hybridize more readily than animal species, and the resulting hybrids are more often fertile hybrids and may reproduce, though there still exist sterile hybrids and selective hybrid elimination where the offspring are less able to survive and are thus eliminated before they can reproduce. A number of plant species are the result of hybridization and polyploidy with many plant species easily cross pollinating and producing viable seeds, the distinction between each species is often maintained by geographical isolation or differences in the flowering period. Since plants hybridize frequently without much work, they are often created by humans in order to produce improved plants. These improvements can include the production of more or improved seeds, fruits or other plant parts for consumption, or to make a plant more winter or heat hardy or improve its growth and/or appearance for use in horticulture. Much work is now being done with hybrids to produce more disease resistant plants for both agricultural and horticultural crops. In many groups of plants hybridization has been used to produce larger and more showy flowers and new flower colors. Hybridization may be restricted to the desired parent species through the use of pollination bags.
Many plant genera and species have their origins in polyploidy. Autopolyploidy results from the sudden multiplication in the number of chromosomes in typical normal populations caused by unsuccessful separation of the chromosomes during meiosis. Tetraploids (plants with four sets of chromosomes rather than two) are common in a number of different groups of plants and over time these plants can differentiate into distinct species from the normal diploid line. In "Oenothera lamarchiana" the diploid species has 14 chromosomes, this species has spontaneously given rise to plants with 28 chromosomes that have been given the name "Oenothera gigas". When hybrids are formed between the tetraploids and the diploid population, the resulting offspring tend to be sterile triploids, thus effectively stopping the intermixing of genes between the two groups of plants (unless the diploids, in rare cases, produce unreduced gametes).
Another form of polyploidy called allopolyploidy occurs when two different species mate and produce polyploid hybrids. Usually the typical chromosome number is doubled, and the four sets of chromosomes can pair up during meiosis, thus the polyploids can produce offspring. Usually, these offspring can mate and reproduce with each other but cannot back-cross with the parent species. Allopolyploids may be able to adapt to new habitats that neither of their parent species inhabited.
Sterility in a non-polyploid hybrid is often a result of chromosome number; if parents are of differing chromosome pair number, the offspring will have an odd number of chromosomes, leaving them unable to produce chromosomally balanced gametes. While this is undesirable in a crop such as wheat, where growing a crop which produces no seeds would be pointless, it is an attractive attribute in some fruits. Triploid bananas and watermelons are intentionally bred because they produce no seeds (and are parthenocarpic).
Heterosis.
Hybrids are sometimes stronger than either parent variety, a phenomenon most common with plant hybrids, which when present is known as "hybrid vigor" (heterosis) or heterozygote advantage. A transgressive phenotype is a phenotype displaying more extreme characteristics than either of the parent lines. Plant breeders make use of a number of techniques to produce hybrids, including line breeding and the formation of complex hybrids. An economically important example is hybrid maize (corn), which provides a considerable seed yield advantage over open pollinated varieties. Hybrid seed dominates the commercial maize seed market in the United States, Canada and many other major maize producing countries.
Examples of plant hybrids.
The multiplication symbol × (not italicised) indicates a hybrid in the Latin binomial nomenclature. Placed before the binomial it indicates a hybrid between species from different genera (intergeneric hybrid):-
Interspecific plant hybrids include:
Some natural hybrids:
Hybrids in nature.
Hybridization between two closely related species is actually a common occurrence in nature but is also being greatly influenced by anthropogenic changes as well. Hybridization is a naturally occurring genetic process where individuals from two genetically distinct populations mate. As stated above, it can occur both intraspecifically, between different distinct populations within the same species, and interspecifically, between two different species. Hybrids can be either sterile/not viable or viable/fertile. This affects the kind of effect that this hybrid will have on its and other populations that it interacts with. Many hybrid zones are known where the ranges of two species meet, and hybrids are continually produced in great numbers. These hybrid zones are useful as biological model systems for studying the mechanisms of speciation (Hybrid speciation). Recently DNA analysis of a bear shot by a hunter in the North West Territories confirmed the existence of naturally-occurring and fertile grizzly–polar bear hybrids.
Anthropogenic hybridization.
Changes to the environment caused by humans, such as fragmentation and Introduced species, are becoming more widespread. This increases the challenges in managing certain populations that are experiencing introgression, and is a focus of conservation genetics.
Introduced species and habitat fragmentation.
Humans have introduced species worldwide to environments for a long time, both intentionally such as establishing a population to be used as a biological control, and unintentionally such as accidental escapes of individuals out of agriculture. This causes drastic global effects on various populations, including through hybridization.
When habitats become broken apart, one of two things can occur, genetically speaking. The first is that populations that were once connected can be cut off from one another, preventing their genes from interacting. Occasionally, this will result in a population of one species breeding with a population of another species as a means of surviving such as the case with the red wolves. Their population numbers being so small, they needed another means of survival. Habitat fragmentation also led to the influx of generalist species into areas where they would not have been, leading to competition and in some cases interbreeding/incorporation of a population into another. In this way, habitat fragmentation is essentially an indirect method of introducing species to an area.
The hybridization continuum.
There is a kind of continuum with three semi-distinct categories dealing with anthropogenic hybridization: hybridization without Introgression, hybridization with widespread introgression, and essentially a Hybrid swarm. Depending on where a population falls along this continuum, the management plans for that population will change. Hybridization is currently an area of great discussion within Wildlife management and habitat management. Global climate change is creating other changes such as difference in population distributions which are indirect causes for an increase in anthropogenic hybridization.
Consequences.
Hybridization can be a less discussed way toward extinction than within detection of where a population lies along the hybrid continuum. The dispute of hybridization is how to manage the resulting hybrids. When a population experiences hybridization with substantial introgression, there still exists parent types of each set of individuals. When a complete hybrid swarm is created, all the individuals are hybrids.
Management of hybrids.
Conservationists disagree on when is the proper time to give up on a population that is becoming a hybrid swarm or to try and save the still existing pure individuals. Once it becomes a complete mixture, we should look to conserve those hybrids to avoid their loss. Most leave it as a case-by-case basis, depending on detecting of hybrids within the group. It is nearly impossible to regulate hybridization via policy because hybridization can occur beneficially when it occurs "naturally" and there is the matter of protecting those previously mentioned hybrid swarms because if they are the only remaining evidence of prior species, they need to be conserved as well.
Expression of parental traits in hybrids.
When two distinct types of organisms breed with each other, the resulting hybrids typically have intermediate traits (e.g., one parent has red flowers, the other has white, and the hybrid, pink flowers). Commonly, hybrids also combine traits seen only separately in one parent or the other (e.g., a bird hybrid might combine the yellow head of one parent with the orange belly of the other).
In a hybrid, any trait that falls outside the range of parental variation is termed heterotic. Heterotic hybrids do have new traits, that is, they are not intermediate. "Positive heterosis" produces more robust hybrids, they might be stronger or bigger; while the term "negative heterosis" refers to weaker or smaller hybrids. Heterosis is common in both animal and plant hybrids. For example, hybrids between a lion and a tigress ("ligers") are much larger than either of the two progenitors, while a tigon (lioness × tiger) is smaller. Also the hybrids between the common pheasant ("Phasianus colchicus") and domestic fowl ("Gallus gallus") are larger than either of their parents, as are those produced between the common pheasant and hen golden pheasant ("Chrysolophus pictus"). Spurs are absent in hybrids of the former type, although present in both parents.
Genetic mixing and extinction.
Regionally developed ecotypes can be threatened with extinction when new alleles or genes are introduced that alter that ecotype. This is sometimes called genetic mixing. Hybridization and introgression of new genetic material can lead to the replacement of local genotypes if the hybrids are more fit and have breeding advantages over the indigenous ecotype or species. These hybridization events can result from the introduction of non native genotypes by humans or through habitat modification, bringing previously isolated species into contact. Genetic mixing can be especially detrimental for rare species in isolated habitats, ultimately affecting the population to such a degree that none of the originally genetically distinct population remains.
Effect on biodiversity and food security.
In agriculture and animal husbandry, the Green Revolution's use of conventional hybridization increased yields by breeding "high-yielding varieties". The replacement of locally indigenous breeds, compounded with unintentional cross-pollination and crossbreeding (genetic mixing), has reduced the gene pools of various wild and indigenous breeds resulting in the loss of genetic diversity. Since the indigenous breeds are often well-adapted to local extremes in climate and have immunity to local pathogens this can be a significant genetic erosion of the gene pool for future breeding. Therefore, commercial plant geneticists strive to breed "widely adapted" cultivars to counteract this tendency.
Limiting factors.
A number of conditions exist that limit the success of hybridization, the most obvious is great genetic diversity between most species. But in animals and plants that are more closely related hybridization barriers can include morphological differences, differing times of fertility, mating behaviors and cues, physiological rejection of sperm cells or the developing embryo.
In plants, barriers to hybridization include blooming period differences, different pollinator vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and structural differences of the chromosomes.
Mythical, legendary and religious hybrids.
Ancient folktales often contain mythological creatures, sometimes these are described as hybrids (e.g., hippogriff as the offspring of a griffin and a horse, and the Minotaur which is the offspring of Pasiphaë and a white bull). More often they are kind of chimera, i.e., a composite of the physical attributes of two or more kinds of animals, mythical beasts, and often humans, with no suggestion that they are the result of interbreeding, e.g., harpies, mermaids, and centaurs.
In the Bible, the Old Testament contains several passages which talk about a first generation of hybrid giants who were known as the Nephilim. The Book of Genesis (6:4) states that "the sons of God went to the daughters of humans and had children by them". As a result, the offspring was born as hybrid giants who became mighty heroes of old and legendary famous figures of ancient times. In addition, the Book of Numbers (13:33) says that the descendants of Anak came from the Nephilim, whose bodies looked exactly like men, but with an enormous height. According to the apocryphal Book of Enoch the Nephilim were wicked sons of fallen angels who had lusted with attractive women.

</doc>
<doc id="41245" url="https://en.wikipedia.org/wiki?curid=41245" title="Hybrid balance">
Hybrid balance

In telecommunications, a hybrid balance is an expression of the degree of electrical symmetry between two impedances connected to two conjugate sides of a hybrid coil or resistance hybrid. It is usually expressed in dB. 
If the respective impedances of the branches of the hybrid that are connected to the conjugate sides of the hybrid are known, hybrid balance may be computed by the formula for return loss. 

</doc>
<doc id="41246" url="https://en.wikipedia.org/wiki?curid=41246" title="Hybrid coil">
Hybrid coil

A signal arriving on one branch is divided between the two adjacent branches but does not appear at the opposite branch. In the schematic diagram, the signal into W splits between X and Z, and no signal passes to Y. Similarly, signals into X split to W and Y with none to Z, etc. 
Correct operation requires matched characteristic impedance at all four ports.
Explanation.
Hybrid coil circuit diagrams.
Hybrids are realized using transformers. Two versions of transformer hybrids were used, the single transformer version providing unbalanced outputs with one end grounded, and the double transformer version providing balanced ports. 
Double transformer hybrid.
When both the 2-wire and the 4-wire circuits must be balanced, double transformer hybrids are used, as shown at right. Signal into port W splits between X and Z, but due to reversed connection to the windings, cancel at port Y. Signal into port X goes to W and Y. But due to reversed connection to ports W and Y, Z gets no signal. Thus the pairs, W & Y, X & Z, are conjugates.
Applications.
Hybrids are used in telephones (see telephone hybrid) to reduce the sidetone, or volume of microphone output that was fed back to the earpiece. Without this, the phone user's own voice would be louder in the earpiece than the other party's. Such hybrids also had their windings so arranged as to act as an impedance matching transformer, matching the low-impedance carbon button transmitter to the higher impedance parts of the system. Today, the transformer version of the hybrid has been replaced by resistor networks and compact IC versions, which uses integrated circuit electronics to do the job of the hybrid coil.
Radio-frequency hybrids are used to split radio signals, including television. The splitter divides the antenna signal to feed multiple receivers.

</doc>
<doc id="41247" url="https://en.wikipedia.org/wiki?curid=41247" title="Hybrid routing">
Hybrid routing

Hybrid routing is the routing of telephone calls in which numbering plans and routing tables are used to permit the colocation, in the same area code, of switches using a deterministic routing scheme with switches using a non-deterministic routing scheme, such as flood search routing. 
"Note:" Routing tables are constructed with no duplicate numbers, so that direct distance dialing service can be provided to all network subscribers. This may require the use of 10-digit telephone numbers.

</doc>
<doc id="41248" url="https://en.wikipedia.org/wiki?curid=41248" title="Hydroxyl ion absorption">
Hydroxyl ion absorption

Hydroxyl ion absorption is the absorption in optical fibers of electromagnetic waves, including the near-infrared, due to the presence of trapped hydroxyl ions remaining from water as a contaminant. 
The hydroxyl (OH−) ion, can penetrate glass during or after product fabrication, resulting in significant attenuation of discrete optical wavelengths, "e.g.", centred at 1.383 μm, used for communications via optical fibres.

</doc>
<doc id="41250" url="https://en.wikipedia.org/wiki?curid=41250" title="Identifier">
Identifier

An identifier is a name that identifies (that is, labels the identity of) either a unique object or a unique "class" of objects, where the "object" or class may be an idea, physical object (or class thereof), or physical [noncountable substance (or class thereof). The abbreviation ID often refers to identity, identification (the process of identifying), or an identifier (that is, an instance of identification). An identifier may be a word, number, letter, symbol, or any combination of those.
The words, numbers, letters, or symbols may follow an encoding system (wherein letters, digits, words, or symbols "stand for" (represent) ideas or longer names) or they may simply be arbitrary. When an identifier follows an encoding system, it is often referred to as a code or ID code. Identifiers that do not follow any encoding scheme are often said to be arbitrary IDs; they are arbitrarily assigned and have no greater meaning. (Sometimes identifiers are called "codes" even when they are actually arbitrary, whether because the speaker believes that they have deeper meaning or simply because he is speaking casually and imprecisely.)
ID codes inherently carry metadata along with them. (For example, when you know that the food package in front of you has the identifier "2011-09-25T15:42Z-MFR5-P02-243-45", you not only have that data, you also have the metadata that tells you that it was packaged on September 25, 2011, at 3:42pm UTC, manufactured by Licensed Vendor Number 5, at the Peoria, IL, USA plant, in Building 2, and was the 243rd package off the line in that shift, and was inspected by Inspector Number 45.) Arbitrary identifiers carry no metadata. (For example, if your food package just says 100054678214, its ID may not tell you anything except identity—no date, manufacturer name, production sequence rank, or inspector number.)
In some cases, even arbitrary identifiers such as sequential serial numbers leak too much information (see German tank problem).
Opaque identifiers—identifiers designed to avoid leaking even that small amount of information—include "really opaque pointers" and Version 4 UUIDs.
The unique identifier (UID) is an identifier that refers to "only one instance"—only one particular object in the universe. A part number is an identifier, but it is not a "unique" identifier—for that, a serial number is needed, to identify "each instance" of the part design. Thus the "identifier" "Model T" identifies the "class" "(model)" of automobiles that Ford's Model T comprises; whereas the "unique identifier" "Model T Serial Number 159,862" identifies one specific member of that class—that is, one particular Model T car, owned by one specific person.
The concepts of "name" and "identifier" are denotatively equal, and the terms are thus denotatively synonymous; but they are not always connotatively synonymous, because code names and ID numbers are often connotatively distinguished from names in the sense of traditional natural language naming. For example, both "Jamie Zawinski" and "Netscape employee number 20" are identifiers for the same specific human being; but normal English-language connotation may consider "Jamie Zawinski" a "name" and not an "identifier", whereas it considers "Netscape employee number 20" an "identifier" but not a "name". This is an emic indistinction rather than an etic one.
Metadata.
In metadata, an identifier is a language-independent label, sign or token that uniquely identifies an object within an identification scheme.
The suffix identifier is also used as a representation term when naming a data element.
In computer science.
In computer science, identifiers (IDs) are lexical tokens that name entities. Identifiers are used extensively in virtually all information processing systems. Identifying entities makes it possible to refer to them, which is essential for any kind of symbolic processing.
In computer languages.
In computer languages, identifiers are tokens (also called symbols) which name language entities. Some of the kinds of entities an identifier might denote include variables, types, labels, subroutines, and packages.
Which character sequences constitute identifiers depends on the lexical grammar of the language. A common rule is alphanumeric sequences, with underscore also allowed, and with the condition that it not begin with a digit (to simplify lexing by avoiding confusing with integer literals) – so codice_1 are allowed, but codice_2 is not – this is the definition used in earlier versions of C and C++, Python 2, and many other languages. Later versions of these languages, along with many other modern languages support almost all Unicode characters in an identifier. However, a common restriction is not to permit whitespace characters and language operators; this simplifies tokenization by making it free-form and context-free. For example, forbidding codice_3 in identifiers (due to its use as a binary operation) means that codice_4 and codice_5 can be tokenized the same, while if it were allowed, codice_4 would be an identifier, not an addition. Whitespace in identifier is particularly problematic, as if spaces are allowed in identifiers, then a clause such as codice_7 is legal, with codice_8 as an identifier, but tokenizing this requires the phrasal context of being in the condition of an if clause. Some languages do allow spaces in identifiers, however, such as ALGOL 68 and some ALGOL variants – for example, the following is a valid statement: codice_9 which could be entered as codice_10 (keywords are represented in boldface, concretely via stropping). In ALGOL this was possible because keywords are syntactically differentiated, so there is no risk of collision or ambiguity, spaces are eliminated during the line reconstruction phase, and the source was processed via scannerless parsing, so lexing could be context-sensitive.
In most languages, some character sequences have the lexical form of an identifier but are known as keywords – for example, codice_11 is frequently a keyword for an if clause, but lexically is of the same form as codice_12 or codice_13 namely a sequence of letters. This overlap can be handled in various ways: these may be forbidden from being identifiers – which simplifies tokenization and parsing – in which case they are reserved words; they may both be allowed but distinguished in other ways, such as via stropping; or keyword sequences may be allowed as identifiers and which sense is determined from context, which requires a context-sensitive lexer. Non-keywords may also be reserved words (forbidden as identifiers), particularly for forward compatibility, in case a word may become a keyword in future. In a few languages, e.g., PL/1, the distinction is not clear.
For implementations of programming languages that are using a compiler, identifiers are often only compile time entities. That is, at runtime the compiled program contains references to memory addresses and offsets rather than the textual identifier tokens (these memory addresses, or offsets, having been assigned by the compiler to each identifier).
In languages that support reflection, such as interactive evaluation of source code (using an interpreter or an incremental compiler), identifiers are also runtime entities, sometimes even as first-class objects that can be freely manipulated and evaluated. In Lisp, these are called symbols.
Compilers and interpreters do not usually assign any semantic meaning to an identifier based on the actual character sequence used. However, there are exceptions.
For example:
In some languages such as Go, identifiers uniqueness is based on their spelling and their visibility.
In HTML an identifier is one of the possible attributes of an HTML element. It is unique within the document.
Ambiguity.
Identifiers (IDs) versus Unique identifiers (UIDs).
Many resources may carry multiple identifiers. Typical examples are: 
The inverse is also possible, where multiple resources are represented with the same identifier (discussed below).
Implicit context and namespace conflicts.
Many codes and nomenclatural systems originate within a small namespace. Over the years, some of them bleed into larger namespaces (as people interact in ways they formerly hadn't, e.g., cross-border trade, scientific collaboration, military alliance, and general cultural interconnection or assimilation). When such dissemination happens, the limitations of the original naming convention, which had formerly been latent and moot, become painfully apparent, often necessitating retronymy, synonymity, translation/transcoding, and so on. Such limitations generally accompany the shift away from the original context to the broader one. Typically the system shows implicit context (context was formerly assumed, and narrow), lack of capacity (e.g., low number of possible IDs, reflecting the outmoded narrow context), lack of extensibility (no features defined and reserved against future needs), and lack of specificity and disambiguating capability (related to the context shift, where longstanding uniqueness encounters novel nonuniqueness). Within computer science, this problem is called naming collision. The story of the origination and expansion of the CODEN system provides a good case example in a recent-decades, technical-nomenclature context. The capitalization variations seen with specific designators reveals an instance of this problem occurring in natural languages, where the proper noun/common noun distinction (and its complications) must be dealt with. A universe in which every object had a UID would not need any namespaces, which is to say that it would constitute one gigantic namespace; but human minds could never keep track of, or semantically interrelate, so many UIDs.
Identifiers in various disciplines.
A small sample of various identifiers

</doc>
<doc id="41251" url="https://en.wikipedia.org/wiki?curid=41251" title="Image antenna">
Image antenna

In telecommunications and antenna design, an image antenna is an electrical mirror-image of an antenna element formed by the radio waves reflecting from a conductive surface called a ground plane, such as the surface of the earth. It is used as a geometrical technique in calculating the radiation pattern of the antenna.
When a radio antenna is mounted above a conductive surface such as the earth, the radio waves directed down toward the surface reflect off it. The radiation received at a distant point is the sum of two contributions: the waves that travel directly from the antenna to the point, and the waves that reach the point after reflecting off the ground plane. Because of the reflection, these second waves appear to come from a second antenna behind the plane, just as a visible object in front of a flat mirror forms a virtual image that seems to lie behind the mirror. The radiation pattern of the antenna is exactly the same as it would be if the ground plane were replaced by a mirror image of the antenna, located an equal distance behind the plane. This second apparent source of radio waves is the image antenna.
The image antenna is used in calculating electric field vectors, magnetic field vectors, and electromagnetic fields emanating from the real antenna, particularly in the vicinity of the antenna and along the ground. Each charge and current in the real antenna has its counterpart in the image, and may also be considered as a source of radiation. 
To form an image of the antenna above it, the ground plane need not be grounded to the Earth. Many antenna types, such as reflective array antennas, use flat surfaces of metal or metal screen to reflect radio waves from the antenna elements, and these can be analyzed using image antennas. If there is more than one reflective surface in the antenna, as in a corner reflector antenna, each surface forms its own image of the antenna elements. In order to form an image, the ground plane surface must generally have dimensions of at least a quarter-wavelength of the radio waves used.

</doc>
<doc id="41254" url="https://en.wikipedia.org/wiki?curid=41254" title="Improved-definition television">
Improved-definition television

Improved-definition television (IDTV) or enhanced-quality television transmitters and receivers exceed the performance requirements of the NTSC standard, while remaining within the general parameters of NTSC emissions standards. 
IDTV improvements may be made at the television transmitter or receiver. Improvements include enhancements in encoding, digital filtering, scan interpolation, interlaced line scanning, and ghost cancellation. 
IDTV improvements must allow the TV signal to be transmitted and received in the standard 4:3 aspect ratio.

</doc>
<doc id="41255" url="https://en.wikipedia.org/wiki?curid=41255" title="Independent clock">
Independent clock

In telecommunications networks, independent clocks are free-running precision clocks located at the nodes which are used for synchronization.
Variable storage buffers, installed to accommodate variations in transmission delay between nodes, are made large enough to accommodate small time (phase) departures among the nodal clocks that control transmission. Traffic may occasionally be interrupted to allow the buffers to be emptied of some or all of their stored data.

</doc>
<doc id="41256" url="https://en.wikipedia.org/wiki?curid=41256" title="Index-matching material">
Index-matching material

In optics, an index-matching material is a substance, usually a liquid, cement (adhesive), or gel, which has an index of refraction that closely approximates that of another object (such as a lens, material, fiber-optic, etc.).
When two substances with the same index are next to each other, light passes from one to the other with neither reflection nor refraction. As such, they are used for various purposes in science, engineering, and art.
For example, in a popular home experiment, a glass rod is made almost invisible by immersing it in an index-matched transparent fluid such as mineral spirits.
In microscopy.
In light microscopy, oil immersion is a technique used to increase the resolution of a microscope. This is achieved by immersing both the objective lens and the specimen in a transparent oil of high refractive index, thereby increasing the numerical aperture of the objective lens.
Immersion oils are transparent oils that have specific optical and viscosity characteristics necessary for use in microscopy. Typical oils used have an index of refraction around 1.515. An oil immersion objective is an objective lens specially designed to be used in this way. The index of the oil is typically chosen to match the index of the microscope lens glass, and of the cover slip.
For more details, see the main article, oil immersion. Some microscopes also use other index-matching materials besides oil; see water immersion objective and solid immersion lens.
In fiber optics.
In fiber optics and telecommunications, an index-matching material may be used in conjunction with pairs of mated connectors or with mechanical splices to reduce signal reflected in the guided mode (known as return loss) (see: Optical fiber connector). Without the use of an index-matching material, Fresnel reflections will occur at the smooth end faces of a fiber unless there is no fiber-air interface or other significant mismatch in refractive index. These reflections may be as high as -14 dB ("i.e.," 14 dB below the optical power of the incident signal). When the reflected signal returns to the transmitting end, it may be reflected again and return to the receiving end at a level that is (28 plus twice the fiber loss) dB below the direct signal. The reflected signal will also be delayed by twice the delay time introduced by the fiber. The twice-reflected, delayed signal superimposed on the direct signal may noticeably degrade an analog baseband intensity-modulated video signal. Conversely, for digital transmission, the reflected signal will often have no practical effect on the detected signal seen at the decision point of the digital optical receiver except in marginal cases where bit-error ratio is significant. However, certain digital transmitters such as those employing a Distributed Feedback Laser may be affected by back reflection and then fall outside specifications such as Side Mode Suppression Ratio, potentially degrading system bit error ratio, so networking standards intended for DFB lasers may specify a back-reflection tolerance such as -10 dB for transmitters so that they remain within specification even without index matching. This back-reflection tolerance might be achieved using an optical isolator or by way of reduced coupling efficiency.
For some applications, instead of standard polished connectors (e.g. FC/PC), angle polished connectors (e.g. FC/APC) may be used, whereby the non-perpendicular polish angle greatly reduces the ratio of reflected signal launched into the guided mode even in the case of a fiber-air interface.
In art conservation.
If a sculpture is broken into several pieces, art conservators may reattach the pieces using an adhesive such as Paraloid B-72 or epoxy. If the sculpture is made of a transparent or semitransparent material (such as glass), the seam where the pieces are attached will usually be much less noticeable if the refractive index of the adhesive matches the refractive index of the surrounding object. Therefore, art conservators may measure the index of objects and then use an index-matched adhesive. Similarly, losses (missing sections) in transparent or semitransparent objects are often filled using an index-matched material.
In optical component adhesives.
Certain optical components, such as a Wollaston prism or Nicol prism, are made of multiple transparent pieces that are directly attached to each other. The adhesive is usually index-matched to the pieces. Historically, Canada balsam was used in this application, but it is now more common to use epoxy or other synthetic adhesives.

</doc>
<doc id="41258" url="https://en.wikipedia.org/wiki?curid=41258" title="Inductive coupling">
Inductive coupling

In electrical engineering, two conductors are referred to as mutual-inductively coupled or magnetically coupled when they are configured such that change in current through one wire induces a voltage across the ends of the other wire through electromagnetic induction. The amount of inductive coupling between two conductors is measured by their mutual inductance.
The coupling between two wires can be increased by winding them into coils and placing them close together on a common axis, so the magnetic field of one coil passes through the other coil. The two coils may be physically contained in a single unit, as in the primary and secondary sides of a transformer, or may be separated. Coupling may be intentional or unintentional. 
Unintentional coupling is called cross-talk, and is a form of electromagnetic interference. Inductive coupling favors low frequency energy sources. High frequency energy sources generally use capacitive coupling.
An inductively coupled transponder comprises an electronic data carrying device, usually a single microchip, and a large coil that functions as an antenna. Inductively coupled transponders are almost always operated passively.
Magnetic coupling transfers torque from one magnetic gear to another.
Some diver propulsion vehicles and remotely operated underwater vehicles use magnetic coupling
to transfer torque from the electric motor to the prop. Magnetic gearing is also being explored for use in utility scale wind turbines as a means of enhancing reliability.
The magnetic coupling has several advantages over a traditional stuffing box.
Uses.
Devices that use inductive coupling include:
Low frequency induction.
Low frequency induction is an unwanted form of inductive coupling, which can occur when a metallic pipeline is installed parallel to a high-voltage power line. The pipeline, which is a conductor, and is insulated from the earth by its protective coating, can develop voltages which are hazardous to personnel operating valves or otherwise contacting the pipeline.

</doc>
<doc id="41259" url="https://en.wikipedia.org/wiki?curid=41259" title="Information-bearer channel">
Information-bearer channel

In telecommunications, an information-bearer channel is one of: 

</doc>
<doc id="41262" url="https://en.wikipedia.org/wiki?curid=41262" title="Information-transfer transaction">
Information-transfer transaction

A transaction is a change of state, an information-transfer transaction is a transaction in which one of the following changes occurs: content, ownership, location, format, etc. An information-transfer transaction usually consists of three consecutive phases called the access phase, the information transfer phase, and the disengagement phase. Examples of these consecutive phases are the copying and transporting of information. Once a transaction occurs there are also costs to consider, which are associated with that certain transaction. When it comes to the transfer of information some transaction costs include time and means (money).
History of Information-transfer transactions.
There are many social systems and devices that have contributed to information-transfer transactions; starting from people writing letters using postal systems to emailing using information technology. Two main examples of information-transfer transactions technology development is the copying and transportation of information.
History of Copying.
Copying is the process of duplicating information with the change of location or format of the original information. The transfer transaction of information through copying has been going on for ages and there has been many advances in technology to decrease the time it takes to make copies of said information. The art of copying started with people having to write a copy out by hand, then the printing press, all the way to digital copying with ICTs. These developments lead to quicker information-transfer transactions in the form of distributing copies of original information to others through a changes of location or format.
History of Transporting.
Transporting is the movement of information with the change of location or ownership of the original information. The transfer transaction of information through transporting has been going on for ages and there has been social and technological developments to decrease the time it takes for information to change ownership or location. The transportation of information started with people sending letters by foot, then by horse, the public and international postal service, all the way down to technology networks. It is these developments which lead to the ability to send information further and quicker through information-transfer transactions.
Transaction Costs.
Every time a transaction occurs, there are always costs to be considered. In the case of information-transfer transactions, one most consider the costs of time and means (money). Both of these costs are corollated with one another in that to decrease one, you must increase the other. For example, say Person #1 sends a letter through the mail, while Person #2 sends letters through email. For Person #1 to send their letter they had to buy paper, means of writing, envelopes, stamps, etc., while Person #2 needed to buy a source of electricity, internet, computer technology, etc. to send an email. It seems like Person #1 has lower transaction costs then Person #2 in terms of means; however, when you look at both information-transfer transactions in terms of time that is a different story. For Person #1 although they had little costs in sending their letter, the time it takes for the transfer of that letter is about 3+ days, while Person #2's transfer through email happens in less than minutes, but they endured high mean costs. Therefore, for information-transfer transaction times to decrease, the costs of means have to increase and vice versa.
Telecommunication.
In telecommunication, an information-transfer transaction is a coordinated sequence of user and telecommunications system actions that cause information present at a source user to become present at a destination user.

</doc>
<doc id="41264" url="https://en.wikipedia.org/wiki?curid=41264" title="Input">
Input

Input may refer to:

</doc>
<doc id="41265" url="https://en.wikipedia.org/wiki?curid=41265" title="Insertion gain">
Insertion gain

In telecommunication, insertion gain is the gain resulting from the insertion of a device in a transmission line, expressed as the ratio of the signal power delivered to that part of the line following the device to the signal power delivered to that same part before insertion. If the resulting number is less than unity, an ""insertion loss"" is indicated. 
Insertion gain is usually expressed in dB.

</doc>
<doc id="41266" url="https://en.wikipedia.org/wiki?curid=41266" title="Insertion loss">
Insertion loss

In telecommunications, insertion loss is the loss of signal power resulting from the insertion of a device in a transmission line or optical fiber and is usually expressed in decibels (dB). 
If the power transmitted to the load before insertion is "P"T and the power received by the load after insertion is "P"R, then the insertion loss in dB is given by,
Electronic filters.
Insertion loss is a figure of merit for an electronic filter and this data is generally specified with a filter. Insertion loss is defined as a ratio of the signal level in a test configuration without the filter installed (|"V"1|) to the signal level with the filter installed (|"V"2|). This ratio is described in dB by the following equation:
Note that, for most filters, |"V"2| will be smaller than |"V"1|. In this case, the insertion loss is positive and measures how much smaller the signal is after adding the filter.
Link with Scattering parameters.
In case the two measurement ports use the same reference impedance, the insertion loss (formula_3) is defined as: 
and not, as often mistakenly thought, by: 
Here formula_6 and formula_7 are two of the scattering parameters. It is the extra loss produced by the introduction of the DUT between the 2 reference planes of the measurement. The extra loss can be introduced by intrinsic loss in the DUT and/or mismatch. In case of extra loss the insertion loss is defined to be positive.

</doc>
<doc id="41267" url="https://en.wikipedia.org/wiki?curid=41267" title="Inside plant">
Inside plant

In telecommunication, the term inside plant has the following meanings: 
Around the turn of the 21st century, DSLAMs became an important part of telephone company inside plant. Inside plant will also have distribution frames and other equipment including passive optical network (name depends on the Service Provider).
Power.
A typical power system for a switching office in an inside plant consists of the elements listed below: 
For safety and reliability reasons, it is desirable that all telecommunications loads be DC powered with minimal AC-powered devices used. Telcordia GR-513, Power Requirements in Telecommunications Plant (LSSGR Section 13), contains detailed industry requirements for using power in an inside plant.
Both integrated and isolated bonding networks as per Telcordia GR-295, Mesh and Isolated Bonding Networks: Definition and Application to Telephone Central Offices, are a technically viable means to ground and bond the equipment in a safe and effective manner. However, the integrated or mesh bonding schemes are preferred over isolated bonding networks because of the added costs and efforts required to manage, control, and maintain the isolation for the equipment, particularly during equipment upgrade and modifications to the plant. This preference is based on a pragmatic desire for lower costs and ease of management, and to simplify operations during plant modifications/upgrades.
For a comprehensive analysis of the energy efficiency and environmental soundness of a power system, one should ideally consider a wider range of factors than strict energy conversion AC-to-DC power efficiency. These environmental factors cover a wider vision known as Industrial Ecology within which each manufacturing step of the products need to be considered from a Design for Environment (DfE) factors standpoint.

</doc>
<doc id="41268" url="https://en.wikipedia.org/wiki?curid=41268" title="Intelligent Network">
Intelligent Network

The Intelligent Network (IN) is the standard network architecture specified in the ITU-T Q.1200 series recommendations. It is intended for fixed as well as mobile telecom networks. It allows operators to differentiate themselves by providing value-added services in addition to the standard telecom services such as PSTN, ISDN and GSM services on mobile phones.
The intelligence is provided by network nodes on the service layer, distinct from the switching layer of the core network, as opposed to solutions based on intelligence in the core switches or telephone equipment. The IN nodes are typically owned by telecommunications operators (telecommunications service providers).
IN is supported by the Signaling System #7 (SS7) protocol between telephone network switching centers and other network nodes owned by network operators.
History and key concepts.
The IN concepts, architecture and protocols were originally developed as standards by the ITU-T which is the standardization committee of the International Telecommunication Union, prior to this a number of telecommunications providers had proprietary IN solutions. The primary aim of the IN was to enhance the core telephony services offered by traditional telecommunications networks, which usually amounted to making and receiving voice calls, sometimes with call divert. This core would then provide a basis upon which operators could build services in addition to those already present on a standard telephone exchange.
A complete description of the IN emerged in a set of ITU-T standards named Q.1210 to Q.1219, or Capability Set One (CS-1) as they became known. The standards defined a complete architecture including the architectural view, state machines, physical implementation and protocols. They were universally embraced by telecom suppliers and operators, although many variants were derived for use in different parts of the world (see Variants below).
Following the success of CS-1, further enhancements followed in the form of CS-2. Although the standards were completed, they were not as widely implemented as CS-1, partly because of the increasing power of the variants, but also partly because they addressed issues which pushed traditional telephone exchanges to their limits.
The major driver behind the development of the IN system was the need for a more flexible way of adding sophisticated services to the existing network. Before IN was developed, all new feature and/or services that were to be added had to be implemented directly in the core switch systems. This made for very long release cycles as the bug hunting and testing had to be extensive and thorough to prevent the network from failing. With the advent of IN, most of these services (such as toll free numbers and geographical number portability) were moved out of the core switch systems and into self-serving nodes (IN), thus creating a modular and more secure network that allowed the service providers themselves to develop variations and value-added services to their networks without submitting a request to the core switch manufacturer and wait for the long development process. The initial use of IN technology was for number translation services, e.g. when translating toll free numbers to regular PSTN numbers. But much more complex services have since been built on IN, such as Custom Local Area Signaling Services (CLASS) and prepaid telephone calls.
SS7 architecture.
The main concepts (functional view) surrounding IN services or architecture are connected with SS7 architecture:
Protocols.
The core elements described above use standard protocols to communicate with each other. The use of standard protocols allows different manufacturers to concentrate on different parts of the architecture and be confident that they will all work together in any combination.
The interfaces between the SSP and the SCP are SS7 based and may look familiar to those familiar with TCP/IP protocols. The SS7 protocols implement much of the OSI seven-layer model. This means that the IN standards only had to define the application layer which was called the Intelligent Networks Application Part or INAP. The INAP messages are encoded using ASN.1.
The interface between the SCP and the SDP is defined in the standards to be an X.500 Directory Access Protocol or DAP. A more lightweight interface called LDAP has emerged from the IETF which is considerably simpler to implement, so many SCPs have implemented that instead.
Variants.
The core CS-1 specifications were adopted and extended by other standards bodies. European flavours were developed by ETSI, American flavours were developed by ANSI and Japanese variants also exist. The main reasons for producing variants in each region was to ensure interoperability between equipment manufactured and deployed locally (for example different versions of the underlying SS7 protocols exist between the regions).
New functionality was also added which meant that variants diverged from each other and the main ITU-T standard. The biggest variant was called Customised Applications for Mobile networks Enhanced Logic, or CAMEL for short. This allowed for extensions to be made for the mobile phone environment, and allowed mobile phone operators to offer the same IN services to subscribers while they are roaming as they receive in the home network.
CAMEL has become a major standard in its own right and is currently maintained by 3GPP. The last major release of the standard was CAMEL phase 4. It is the only IN standard currently being actively worked on.
Bellcore (subsequently Telcordia Technologies) developed the Advanced Intelligent Network (AIN) as the variant of Intelligent Network for North America and performed the standardization of the AIN on behalf of the major US operators.
The original goal of AIN was AIN 1.0, which was specified in the early 1990s ("AIN Release 1", Bellcore SR-NWT-002247, 1993). AIN 1.0 proved technically infeasible to implement, which led to the definition of simplified AIN 0.1 and AIN 0.2 specifications. In North America, Telcordia SR-3511 (originally known as TA-1129+) and GR-1129-CORE protocols serve to link switches with the IN systems such as Service Control Points (SCPs) or Service Nodes.
SR-3511 details a TCP/IP-based protocol which directly connects the SCP and Service Node. GR-1129-CORE provides generic requirements for an ISDN based protocol which connects the SCP to the Service Node via the SSP.
Future.
While activity in development of IN standards has declined in recent years, there are many systems deployed across the world which use this technology. The architecture has proved to be not only stable, but also a continuing source of revenue with new services added all the time. Manufacturers continue to support the equipment and obsolescence is not an issue.
Nevertheless, new technologies and architectures are emerging, especially in the area of VoIP and SIP. More attention is being paid to the use of APIs in preference to protocols like INAP and new standards have emerged in the form of JAIN and Parlay. From a technical view, the SCE is beginning to move away from its proprietary graphical origins and is moving towards a Java application server environment.

</doc>
<doc id="41269" url="https://en.wikipedia.org/wiki?curid=41269" title="Intensity modulation">
Intensity modulation

In optical communications, intensity modulation (IM) is a form of modulation in which the optical power output of a source is varied in accordance with some characteristic of the modulating signal. The envelope of the modulated optical signal is an analog of the modulating signal in the sense that the instantaneous power of the envelope is an analog of the characteristic of interest in the modulating signal. 
Recovery of the modulating signal is usually by direct detection, not heterodyning. However, optical heterodyne detection is possible and has been actively studied since 1979. Bell Laboratories had a working, but impractical, system in 1969. Heterodyne and homodyne systems are of interest because they are expected to produce an increase in sensitivity of up to allowing longer hops between islands for instance. Such systems also have the important advantage of very narrow channel spacing in optical frequency-division multiplexing (OFDM) systems. OFDM is a step beyond wavelength-division multiplexing (WDM). Normal WDM using direct detection does not achieve anything like the close channel spacing of radio frequency FDM.

</doc>
<doc id="41270" url="https://en.wikipedia.org/wiki?curid=41270" title="Intercept">
Intercept

Intercept may refer to:

</doc>
<doc id="41271" url="https://en.wikipedia.org/wiki?curid=41271" title="Interchangeability">
Interchangeability

Interchangeability can refer to:

</doc>
<doc id="41272" url="https://en.wikipedia.org/wiki?curid=41272" title="Interchange circuit">
Interchange circuit

In telecommunication, an interchange circuit is a circuit that facilitates the exchange of data and signaling information between data terminal equipment (DTE) and data circuit-terminating equipment (DCE). 
An interchange circuit can carry many types of signals and provide many types of service features, such as control signals, timing signals, and common return functions.

</doc>
<doc id="41273" url="https://en.wikipedia.org/wiki?curid=41273" title="Intercharacter interval">
Intercharacter interval

In telecommunications, the intercharacter interval is the time interval between the end of the stop signal of one character and the beginning of the start signal of the next character of an asynchronous transmission. 
The intercharacter interval may be of any duration. The signal sense of the intercharacter interval is always the same as the sense of the stop element, "i.e.", "1" or "mark."

</doc>
<doc id="41274" url="https://en.wikipedia.org/wiki?curid=41274" title="Interconnect facility">
Interconnect facility

Interconnect facility: In a communications network, one or more communications links that (a) are used to provide local area communications service among several locations and (b) collectively form a node in the network. 
An interconnect facility may include network control and administrative circuits as well as the primary traffic circuits. 
An interconnect facility may use any medium available and may be redundant.

</doc>
<doc id="41275" url="https://en.wikipedia.org/wiki?curid=41275" title="Interface">
Interface

Interface may refer to:

</doc>
<doc id="41276" url="https://en.wikipedia.org/wiki?curid=41276" title="Interface functionality">
Interface functionality

In telephony, interface functionality is the characteristic of interfaces that allows operators to support transmission, switching, and signaling functions identical to those used in the enhanced services provided by the carrier. 
As part of its comparably efficient interconnection (CEI) offering, the carrier must make available standardized telephone networking hardware and software interfaces that are able to support transmission, switching, and signaling functions identical to those used in the enhanced services provided by the carrier.

</doc>
<doc id="41277" url="https://en.wikipedia.org/wiki?curid=41277" title="Interface standard">
Interface standard

In telecommunications, an interface standard is a standard that describes one or more functional characteristics (such as code conversion, line assignments, or protocol compliance) or physical characteristics (such as electrical, mechanical, or optical characteristics) necessary to allow the exchange of information between two or more (usually different) systems or pieces of equipment. Communications protocols are an example. 
An interface standard may include operational characteristics and acceptable levels of performance. 
In the military community, interface standards permit command and control functions to be performed using communication and computer systems.

</doc>
<doc id="41278" url="https://en.wikipedia.org/wiki?curid=41278" title="Interference filter">
Interference filter

An interference filter or dichroic filter is an optical filter that reflects one or more spectral bands or lines and transmits others, while maintaining a nearly zero coefficient of absorption for all wavelengths of interest. An interference filter may be high-pass, low-pass, bandpass, or band-rejection. 
An interference filter consists of multiple thin layers of dielectric material having different refractive indices. There also may be metallic layers. In its broadest meaning, interference filters comprise also etalons that could be implemented as tunable interference filters. Interference filters are wavelength-selective by virtue of the interference effects that take place between the incident and reflected waves at the thin-film boundaries. The important characteristic of the filter is the form of the leaving signal. It is considered that the best form is a rectangle.
Bandpass filters are normally designed for normal incidence. However, when the angle of incidence of the incoming light is increased from zero, the central wavelength of the filter decreases, resulting in partial tunability. The transmission band widens and the maximum transmission decreases. If λ"c" is the central wavelength, λ0 is the central wavelength at normal incidence, and "n"* is the filter effective index of refraction, then:
For example, for λ0=1550 nm, "n"*=1.5, Δλ = λ0−λc=32 nm, the rotation angle is
θ = 17.7°. This corresponds to C-band or L-band in 1550 nm fiber-optic communications window. Equipped with a stepper motor and electronics, a tunable optical filter that tunes center transmission wavelength over C-band or L-band by remote control can be achieved. See diagram below for its working principle and tunable optical filter devices

</doc>
<doc id="41280" url="https://en.wikipedia.org/wiki?curid=41280" title="Intermediate distribution frame">
Intermediate distribution frame

An intermediate distribution frame (IDF) is a distribution frame in a central office or customer premises, which cross-connects the user cable media to individual user line circuits and may serve as a distribution point for multipair cables from the main distribution frame (MDF) or combined distribution frame (CDF) to individual cables connected to equipment in areas remote from these frames.
IDFs are used for telephone exchange central office, customer-premises equipment, wide area network (WAN), and local area network (LAN) environments, among others.
In central office environments the IDF may contain circuit termination equipment from various auxiliary components. In WAN and LAN environments IDFs can hold devices of different types including backup systems, (hard drives or other media as self-contained, or as RAIDs, CD-ROMs, etc.), networking (switches, hubs, routers), and connections (fiber optics, coaxial, category cables) and so on.

</doc>
<doc id="41281" url="https://en.wikipedia.org/wiki?curid=41281" title="Intermediate-field region">
Intermediate-field region

Intermediate-field region: For an antenna, the transition region--lying between the near-field region and the far-field region--in which the field strength of an electromagnetic wave is dependent upon the inverse distance, inverse square of the distance, and the inverse cube of the distance from the antenna. For an antenna that is small compared to the wavelength in question, the intermediate-field region is considered to exist at all distances between 0.1 wavelength and 1.0 wavelength from the antenna. "Synonyms": intermediate field, intermediate zone, transition zone.

</doc>
<doc id="41284" url="https://en.wikipedia.org/wiki?curid=41284" title="IP address spoofing">
IP address spoofing

In computer networking, IP address spoofing or IP spoofing is the creation of Internet Protocol (IP) packets with a forged source IP address, with the purpose of concealing the identity of the sender or impersonating another computing system.
Background.
The basic protocol for sending data over the Internet network and many other computer networks is the Internet Protocol ("IP"). The header of each IP packet contains, among other things, the numerical source and destination address of the packet. The source address is normally the address that the packet was sent from. By forging the header so it contains a different address, an attacker can make it appear that the packet was sent by a different machine. The machine that receives spoofed packets will send a response back to the forged source address, which means that this technique is mainly used when the attacker does not care about the response or the attacker has some way of guessing the response.
Applications.
IP spoofing is most frequently used in denial-of-service attacks. In such attacks, the goal is to flood the victim with overwhelming amounts of traffic, and the attacker does not care about receiving responses to the attack packets. Packets with spoofed addresses are thus suitable for such attacks. They have additional advantages for this purpose—they are more difficult to filter since each spoofed packet appears to come from a different address, and they hide the true source of the attack. Denial of service attacks that use spoofing typically randomly choose addresses from the entire IP address space, though more sophisticated spoofing mechanisms might avoid unroutable addresses or unused portions of the IP address space. The proliferation of large botnets makes spoofing less important in denial of service attacks, but attackers typically have spoofing available as a tool, if they want to use it, so defenses against denial-of-service attacks that rely on the validity of the source IP address in attack packets might have trouble with spoofed packets. Backscatter, a technique used to observe denial-of-service attack activity in the Internet, relies on attackers' use of IP spoofing for its effectiveness.
IP spoofing can also be a method of attack used by network intruders to defeat network security measures, such as authentication based on IP addresses. This method of attack on a remote system can be extremely difficult, as it involves modifying thousands of packets at a time. This type of attack is most effective where trust relationships exist between machines. For example, it is common on some corporate networks to have internal systems trust each other, so that users can log in without a username or password provided they are connecting from another machine on the internal network (and so must already be logged in). By spoofing a connection from a trusted machine, an attacker may be able to access the target machine without authentication.
Legitimate uses of IP spoofing.
Spoofed IP packets are not incontrovertible evidence of malicious intent: in performance testing of websites, hundreds or even thousands of "vusers" (virtual users) may be created, each executing a test script against the website under test, in order to simulate what will happen when the system goes "live" and a large number of users log on at once.
Since each user will normally have its own IP address, commercial testing products (such as HP LoadRunner and WebLOAD) can use IP spoofing, allowing each user its own "return address" as well.
Services vulnerable to IP spoofing.
Configuration and services that are vulnerable to IP spoofing:
Defense against spoofing attacks.
Packet filtering is one defense against IP spoofing attacks. The gateway to a network usually performs ingress filtering, which is blocking of packets from outside the network with a source address inside the network. This prevents an outside attacker spoofing the address of an internal machine. Ideally the gateway would also perform egress filtering on outgoing packets, which is blocking of packets from inside the network with a source address that is not inside. This prevents an attacker within the network performing filtering from launching IP spoofing attacks against external machines.
It is also recommended to design network protocols and services so that they do not rely on the IP source address for authentication.
Upper layers.
Some upper layer protocols provide their own defense against IP spoofing attacks. For example, Transmission Control Protocol (TCP) uses sequence numbers negotiated with the remote machine to ensure that arriving packets are part of an established connection. Since the attacker normally can't see any reply packets, the sequence number must be guessed in order to hijack the connection. The poor implementation in many older operating systems and network devices, however, means that TCP sequence numbers can be predicted.
Other definitions.
The term spoofing is also sometimes used to refer to "header forgery", the insertion of false or misleading information in e-mail or netnews headers. Falsified headers are used to mislead the recipient, or network applications, as to the origin of a message. This is a common technique of spammers and sporgers, who wish to conceal the origin of their messages to avoid being tracked down.

</doc>
<doc id="41285" url="https://en.wikipedia.org/wiki?curid=41285" title="Interoperability">
Interoperability

Interoperability is a property of a product or system, whose interfaces are completely understood, to work with other products or systems, present or future, without any restricted access or implementation.
Other elements of definition.
While the term was initially defined for information technology or systems engineering services to allow for information exchange, a more broad definition takes into account social, political, and organizational factors that impact system to system performance. Task of building coherent services for users when the individual components are technically different and managed by different organizations
Syntactic interoperability.
If two or more systems are capable of communicating with each other, they exhibit syntactic interoperability Specified data formats, communication protocols and the like are fundamental. XML or SQL standards are among the tools of syntactic interoperability. This is also true for lower-level data formats, such as ensuring alphabetical characters are stored in a same variation of ASCII or a Unicode format (for English or international text) in all the communicating systems.
Semantic interoperability.
Beyond the ability of two or more computer systems to exchange information, semantic interoperability is the ability to automatically interpret the information exchanged meaningfully and accurately in order to produce useful results as defined by the end users of both systems. To achieve semantic interoperability, both sides must refer to a common information exchange reference model. The content of the information exchange requests are unambiguously defined: what is sent is the same as what is understood. The possibility of promoting this result by user-driven convergence of disparate interpretations of the same information has been object of study by research prototypes such as S3DB.
Cross-domain interoperability.
Multiple social, organizational, political, legal entities working together for a common interest and/or information exchange.
Interoperability and open standards.
Interoperability must be distinguished from open standards. Although the goal of each is to provide effective and efficient exchange between computer systems, the mechanisms for accomplishing that goal differ. Open standards imply interoperability "ab-initio," i.e. by definition, while interoperability does not, by itself, imply wider exchange between a range of products, or similar products from several different vendors, or even between past and future revisions of the same product. Interoperability may be developed "post-facto," as a special measure between two products, while excluding the rest, or when a vendor is forced to adapt its system to make it interoperable with a dominant system.
Open standards.
Open standards rely on a broadly consultative and inclusive group including representatives from vendors, academics and others holding a stake in the development. That discusses and debates the technical and economic merits, demerits and feasibility of a proposed common protocol. After the doubts and reservations of all members are addressed, the resulting common document is endorsed as a "common standard." This document is subsequently released to the public, and henceforth becomes an "open standard." It is usually published and is available freely or at a nominal cost to any and all comers, with "no further encumbrances." Various vendors and individuals (even those who were not part of the original group) can use the standards document to make products that implement the common protocol defined in the standard, and are thus interoperable by design, with no specific liability or advantage for any customer for choosing one product over another on the basis of standardised features. The vendors' products compete on the quality of their implementation, user interface, ease of use, performance, price, and a host of other factors, while keeping the customers data intact and transferable even if he chooses to switch to another competing product for business reasons.
"Post Facto" interoperability.
"Post-facto" interoperability may be the result of the absolute market dominance of a particular product in contravention of any applicable standards, or if any effective standards were not present at the time of that product's introduction. The vendor behind that product can then choose to "ignore" any forthcoming standards and not co-operate in any standardisation process at all, using its near-monopoly to insist that its product sets the "de facto" standard by its very market dominance. This is not a problem if the product's implementation is open "and" minimally encumbered, but it may as well be both closed and heavily encumbered (e.g. by patent claims). Because of the network effect, achieving interoperability with such a product is both critical for any other vendor if it wishes to remain relevant in the market, and difficult to accomplish because of lack of co-operation on equal terms with the original vendor, who may well see the new vendor as a potential competitor and threat. The newer implementations often rely on clean-room reverse engineering in the absence of technical data to achieve interoperability. The original vendors can provide such technical data to others, often in the name of 'encouraging competition,' but such data is invariably encumbered, and may be of limited use. Availability of such data is "not" equivalent to an open standard, because:
Telecommunications.
In telecommunication, the term can be defined as:
In two-way radio, interoperability is composed of three dimensions:
Search.
Search interoperability refers to the ability of two or more information collections to be searched by a single query.
Specifically related to web-based search, the challenge of interoperability stems from the fact designers of web resources typically have little or no need to concern themselves with exchanging information with other web resources. Federated Search technology, which does not place format requirements on the data owner, has emerged as one solution to search interoperability challenges. In addition, standards, such as OAI-PMH, RDF, and SPARQL, have emerged recently that also help address the issue of search interoperability related to web resources. Such standards also address broader topics of interoperability, such as allowing data mining.
Software.
With respect to software, the term interoperability is used to describe the capability of different programs to exchange data via a common set of exchange formats, to read and write the same file formats, and to use the same protocols. (The ability to execute the same binary code on different processor platforms is 'not' contemplated by the definition of interoperability.) The lack of interoperability can be a consequence of a lack of attention to standardization during the design of a program. Indeed, interoperability is not taken for granted in the non-standards-based portion of the computing world.
According to ISO/IEC 2382-01, "Information Technology Vocabulary, Fundamental Terms", interoperability is defined as follows: ""The capability to communicate, execute programs, or transfer data among various functional units in a manner that requires the user to have little or no knowledge of the unique characteristics of those units"."
Note that the definition is somewhat ambiguous because the "user" of a program can be another program and, if the latter is a portion of the set of program that is required to be interoperable, it might well be that it does need to have knowledge of the characteristics of other units.
This definition focuses on the technical side of interoperability, while it has also been pointed out that interoperability is often more of an organizational issue: often interoperability has a significant impact on the organizations concerned, raising issues of ownership (do people want to share their data?) or are they dealing with information silos, labor relations (are people prepared to undergo training?) and usability. In this context, a more apt definition is captured in the term "business process interoperability".
Interoperability can have important economic consequences; for example, research has estimated the cost of inadequate interoperability in the U.S. capital facilities industry to be $15.8 billion a year. If competitors' products are not interoperable (due to causes such as patents, trade secrets or coordination failures), the result may well be monopoly or market failure. For this reason, it may be prudent for user communities or governments to take steps to encourage interoperability in various situations. At least 30 international bodies and countries have implemented eGovernment-based interoperability framework initiatives called e-GIF while in the United States there is the NIEM initiative. Standards Defining Organizations (SDOs) provide open public software specifications to facilitate interoperability; examples include the Oasis-Open organization and buildingSMART (formerly the International Alliance for Interoperability). As far as user communities, Neutral Third Party is creating standards for business process interoperability. Another example of a neutral party is the RFC documents from the Internet Engineering Task Force (IETF).
The OSLC (Open Service for Lifecycle Collaboration) Community is working on finding a common standard in order that software tools can share and exchange data e.g. bugs, tasks, requirements etc. The final goal is to agree on an open standard for interoperability of open source Lifecycle Management|ALM tools.
Organizations dedicated to interoperability.
Many organizations are dedicated to interoperability. All have in common that they want to push the development of the World Wide Web towards the semantic web. Some concentrate on eGovernment, eBusiness or data exchange in general. Internationally, Network Centric Operations Industry Consortium facilitates global interoperability across borders, language and technical barriers. In Europe, for instance, the European Commission and its IDABC programme issue the European Interoperability Framework. IDABC was succeeded by the ISA programme. They also initiated the Semantic Interoperability Centre Europe (SEMIC.EU). A European Land Information Service (EULIS) was established in 2006, as a consortium of European National Land Registers. The aim of the service is to establish a single portal through which customers are provided with access to information about individual properties, about land and property registration services, and about the associated legal environment. In the United States, the government's CORE.gov service provides a collaboration environment for component development, sharing, registration, and reuse and related to this is the National Information Exchange Model (NIEM) work and component repository. The National Institute of Standards and Technology serves as an agency for measurement standards.
Medical industry.
New technology is being introduced in hospitals and labs at an ever-increasing rate. The need for “plug-and-play” interoperability – the ability to take a medical device out of its box and easily make it work with one’s other devices – has attracted great attention from both healthcare providers and industry.
The HIMSS Board approved the following definition of interoperability on April 5, 2013:
"In healthcare, interoperability is the ability of different information technology systems and software applications to communicate, exchange data, and use the information that has been exchanged. Data exchange schema and standards should permit data to be shared across clinicians, lab, hospital, pharmacy, and patient regardless of the application or application vendor. Interoperability means the ability of health information systems to work together within and across organizational boundaries in order to advance the effective delivery of healthcare for individuals and communities. There are three levels of health information technology interoperability: 1) Foundational; 2) Structural; and 3) Semantic."
They also have introduced ConCert at HIMSS15 a comprehensive interoperability testing and certification program. One of the first products that has completed testing in this program is an iPatientCare EHR system.
eGovernment.
Speaking from an eGovernment perspective, interoperability refers to the collaboration ability of cross-border services for citizens, businesses and public administrations. Exchanging data can be a challenge due to language barriers, different specifications of formats and varieties of categorisations. Many more hindrances can be identified.
If data is interpreted differently, collaboration is limited, takes longer and is not efficient. For instance if a citizen of country A wants to purchase land in country B, the person will be asked to submit the proper address data. Address data in both countries include: Full name details, street name and number as well as a post code. The order of the address details might vary. In the same language it is not an obstacle to order the provided address data; but across language barriers it becomes more and more difficult. If the language requires other characters it is almost impossible, if no translation tools are available.
Hence eGovernment applications need to exchange data in a semantically interoperable manner. This saves time and money and reduces sources of errors. Fields of practical use are found in every policy area, be it justice, trade or participation etc. Clear concepts of interpretation patterns are required.
Public safety.
Interoperability is an important issue for law enforcement, fire fighting, EMS, and other public health and safety departments, because first responders need to be able to communicate during wide-scale emergencies. It has been a major area of investment and research over the last 12 years. Traditionally, agencies could not exchange information because they operated widely disparate hardware that was incompatible. Agencies' information systems such as computer-aided dispatch systems (CAD) and records management systems (RMS) functioned largely in isolation, so-called "information islands." Agencies tried to bridge this isolation with inefficient, stop-gap methods while large agencies began implementing limited interoperable systems. These approaches were inadequate and, in the U.S.A., the lack of interoperability in the public safety realm become evident during the 9/11 attacks on the Pentagon and World Trade Center structures. Further evidence of a lack of interoperability surfaced when agencies tackled the aftermath of the Hurricane Katrina disaster.
In contrast to the overall national picture, some states, including Utah, have already made great strides forward. The Utah Highway Patrol and other departments in Utah have created a statewide data-sharing network using technology from a company based in Bountiful, Utah, FATPOT Technologies.
The Commonwealth of Virginia is one of the leading states in the United States in improving interoperability and is continually recognized as a National Best Practice by Department of Homeland Security (DHS). Virginia's proven practitioner-driven Governance Structure ensures that all the right players are involved in decision making, training & exercises, planning efforts, etc. The Interoperability Coordinator leverages a regional structure to better allocate grant funding around the Commonwealth so that all areas have an opportunity to improve communications interoperability. Virginia's strategic plan for communications is updated yearly to include new initiatives for the Commonwealth – all projects and efforts are tied to this plan, which is aligned with the National Emergency Communications Plan, authored by Department of Homeland Security's Office of Emergency Communications (OEC).
The State of Washington seeks to enhance interoperability statewide. The State Interoperability Executive Committee (SIEC), established by the legislature in 2003, works to assist emergency responder agencies (police, fire, sheriff, medical, hazmat, etc.) at all levels of government (city, county, state, tribal, federal) to define interoperability for their local region.
Washington recognizes collaborating on system design and development for wireless radio systems enables emergency responder agencies to efficiently provide additional services, increase interoperability, and reduce long-term costs.
This important work saves the lives of emergency personnel and the citizens they serve.
The U.S. government is making a concerted effort to overcome the nation's lack of public safety interoperability. The Department of Homeland Security's Office for Interoperability and Compatibility (OIC) is pursuing the SAFECOM and CADIP and Project 25 programs, which are designed to help agencies as they integrate their CAD and other IT systems.
The OIC launched CADIP in August 2007. This project will partner the OIC with agencies in several locations, including Silicon Valley. This program will use case studies to identify the best practices and challenges associated with linking CAD systems across jurisdictional boundaries. These lessons will create the tools and resources public safety agencies can use to build interoperable CAD systems and communicate across local, state, and federal boundaries.
Forces.
"Force interoperability" is defined in NATO as the ability of the forces of two or more nations to train, exercise and operate effectively together in the execution of assigned missions and tasks. Additionally NATO defines interoperability more generally as the ability to act together coherently, effectively and efficiently to achieve Allied tactical, operational and strategic objectives.
At the strategic level, interoperability is an enabler for coalition building. It facilitates meaningful contributions by coalition partners. At this level, interoperability issues center on harmonizing the world views, strategies, doctrines, and force structures. Interoperability is an element of coalition willingness to work together over the long term to achieve and maintain shared interests against common threats. Interoperability at the operational and tactical levels is where strategic/political interoperability and technological interoperability come together to help allies shape the environment, manage crises, and win wars. The benefits of interoperability at the operational and tactical levels generally derive from the fungibility or interchangeability of force elements and units. "Technological interoperability" reflects the interfaces between organizations and systems. It focuses on communications and computers but also involves the technical capabilities of systems and the resulting mission compatibility or incompatibility between the systems and data of coalition partners. At the technological level, the benefits of interoperability come primarily from their impacts at the operational and tactical levels in terms of enhancing fungibility and flexibility.
Achieving software interoperability.
Software Interoperability is achieved through five interrelated ways:
Each of these has an important role in reducing variability in intercommunication software and enhancing a common understanding of the end goal to be achieved.
Interoperability as a question of power and market dominance.
Interoperability tends to be regarded as an issue for experts and its implications for daily living are sometimes underrated. The European Union Microsoft competition case shows how interoperability concerns important questions of power relationships. In 2004, the European Commission found that Microsoft had abused its market power by deliberately restricting interoperability between Windows work group servers and non-Microsoft work group servers. By doing so, Microsoft was able to protect its dominant market position for work group server operating systems, the heart of corporate IT networks. Microsoft was ordered to disclose complete and accurate interface documentation, which will enable rival vendors to compete on an equal footing (“the interoperability remedy”). As of June 2005 the Commission is market testing a new proposal by Microsoft to do this, having rejected previous proposals as insufficient.
Interoperability has also surfaced in the Software patent debate in the European Parliament (June/July 2005). Critics claim that because patents on techniques required for interoperability are kept under RAND (reasonable and non discriminatory licensing) conditions, customers will have to pay license fees twice: once for the product and, in the appropriate case, once for the patent protected programme the product uses.
Railways.
Railways have greater or lesser interoperability depending on conforming to standards of gauge, couplings, brakes, signalling, communications, loading gauge, structure gauge, and operating rules, to mention a few parameters. For passenger rail service, different railway platform height and width clearance standards may also cause interoperability problems.
North American freight railroads are highly interoperable, but systems in Europe, Asia, Africa, Central and South America, and Australia are much less so. The parameter most difficult to overcome (at reasonable cost) is incompatibility of gauge, though variable gauge axle systems are increasingly used.

</doc>
<doc id="41286" url="https://en.wikipedia.org/wiki?curid=41286" title="Interposition trunk">
Interposition trunk

In telecommunication, the term interposition trunk has the following meanings: 
1. A single direct transmission channel, "e.g.," voice-frequency circuit, between two positions of a large switchboard to facilitate the interconnection of other circuits appearing at the respective switchboard positions. 
2. Within a technical control facility, a single direct transmission circuit, between positions in a testboard or patch bay, which circuit facilitates testing or patching between the respective positions. 

</doc>
<doc id="41287" url="https://en.wikipedia.org/wiki?curid=41287" title="Intersymbol interference">
Intersymbol interference

In telecommunication, Intersymbol Interference (ISI) is a form of distortion of a signal in which one symbol interferes with subsequent symbols. This is an unwanted phenomenon as the previous symbols have similar effect as noise, thus making the communication less reliable. The spreading of the pulse beyond its allotted time interval causes it to interfere with neighboring pulses. ISI is usually caused by multipath propagation or the inherent non-linear frequency response of a channel causing successive symbols to "blur" together.
The presence of ISI in the system introduces errors in the decision device at the receiver output. Therefore, in the design of the transmitting and receiving filters, the objective is to minimize the effects of ISI, and thereby deliver the digital data to its destination with the smallest error rate possible.
Ways to fight intersymbol interference include adaptive equalization and error correcting codes.
Causes.
Multipath propagation.
One of the causes of intersymbol interference is multipath propagation in which a wireless signal from a transmitter reaches the receiver via multiple paths. The causes of this include reflection (for instance, the signal may bounce off buildings), refraction (such as through the foliage of a tree) and atmospheric effects such as atmospheric ducting and ionospheric reflection. Since the various paths can be of different lengths, this results in the different versions of the signal arriving at the receiver at different times. These delays mean that part or all of a given symbol will be spread into the subsequent symbols, thereby interfering with the correct detection of those symbols. Additionally, the various paths often distort the amplitude and/or phase of the signal, thereby causing further interference with the received signal.
Bandlimited channels.
Another cause of intersymbol interference is the transmission of a signal through a bandlimited channel, i.e., one where the frequency response is zero above a certain frequency (the cutoff frequency). Passing a signal through such a channel results in the removal of frequency components above this cutoff frequency. In addition, components of the frequency below the cutoff frequency may also be attenuated by the channel.
This filtering of the transmitted signal affects the shape of the pulse that arrives at the receiver. The effects of filtering a rectangular pulse not only change the shape of the pulse within the first symbol period, but it is also spread out over the subsequent symbol periods. When a message is transmitted through such a channel, the spread pulse of each individual symbol will interfere with following symbols.
Bandlimited channels are present in both wired and wireless communications. The limitation is often imposed by the desire to operate multiple independent signals through the same area/cable; due to this, each system is typically allocated a piece of the total bandwidth available. For wireless systems, they may be allocated a slice of the electromagnetic spectrum to transmit in (for example, FM radio is often broadcast in the 87.5 MHz - 108 MHz range). This allocation is usually administered by a government agency; in the case of the United States this is the Federal Communications Commission (FCC). In a wired system, such as an optical fiber cable, the allocation will be decided by the owner of the cable.
The bandlimiting can also be due to the physical properties of the medium - for instance, the cable being used in a wired system may have a cutoff frequency above which practically none of the transmitted signal will propagate.
Communication systems that transmit data over bandlimited channels usually implement pulse shaping to avoid interference caused by the bandwidth limitation. If the channel frequency response is flat and the shaping filter has a finite bandwidth, it is possible to communicate with no ISI at all. Often the channel response is not known beforehand, and an adaptive equalizer is used to compensate the frequency response.
Effects on eye patterns.
One way to study ISI in a PCM or data transmission system experimentally is to apply the received wave to the vertical deflection plates of an oscilloscope and to apply a sawtooth wave at the transmitted symbol rate R (R = 1/T) to the horizontal deflection plates. The resulting display is called an eye pattern because of its resemblance to the human eye for binary waves. The interior region of the eye pattern is called the eye opening. An eye pattern provides a great deal of information about the performance of the pertinent system.
An eye pattern, which overlays many samples of a signal, can give a graphical representation of the
signal characteristics. The first image below is the eye pattern for a binary phase-shift keying (PSK) system in which a one is represented by an amplitude of -1 and a zero by an amplitude of +1. The current sampling time is at the center of the image and the previous and next sampling times are at the edges of the image. The various transitions from one sampling time to another (such as one-to-zero, one-to-one and so forth) can clearly be seen on the diagram.
The noise margin - the amount of noise required to cause the receiver to get an error - is given by the distance between the signal and the zero amplitude point at the sampling time; in other words, the further from zero at the sampling time the signal is the better. For the signal to be correctly interpreted, it must be sampled somewhere between the two points where the zero-to-one and one-to-zero transitions cross. Again, the further apart these points are the better, as this means the signal will be less sensitive to errors in the timing of the samples at the receiver.
The effects of ISI are shown in the second image which is an eye pattern of the same system when operating over a multipath channel. The effects of receiving delayed and distorted versions of the signal can be seen in the loss of definition of the signal transitions. It also reduces both the noise margin and the window in which the signal can be sampled, which shows that the performance of the system will be worse (i.e. it will have a greater bit error ratio).
Countering ISI.
There are several techniques in telecommunication and data storage that try to work around the problem of intersymbol interference.
Intentional Intersymbol interference.
There exist also coded modulation systems that intentionally builds a controlled amount of ISI into the system at the transmitter side, this is known under the name Faster-than-Nyquist Signaling. Such a design trades a computational complexity penalty at the receiver against a Shannon capacity gain of the overall transceiver system. See for a recent survey of this technique.

</doc>
<doc id="41288" url="https://en.wikipedia.org/wiki?curid=41288" title="Inverse-square law">
Inverse-square law

[[Image:Inverse square law.svg|thumb|420px|S represents the light source, while r represents the measured points. The lines represent the flux emanating from the source. The total number of flux lines depends on the strength of the source and is constant with increasing distance.
A greater density of flux lines (lines per unit area) means a stronger field. The density of flux lines is inversely proportional to the square of the distance from the source because the surface area of a sphere increases with the square of the radius. Thus the strength of the field is inversely proportional to the square of the distance from the source.]]
In physics, an inverse-square law is any physical law stating that a specified physical quantity or intensity is inversely proportional to the square of the distance from the source of that physical quantity. The fundamental cause for this can be understood as geometric dilution corresponding to point-source radiation into three-dimensional space (see diagram). Mathematically formulated:
It can also be mathematically expressed as:

</doc>
<doc id="41289" url="https://en.wikipedia.org/wiki?curid=41289" title="Ionospheric reflection">
Ionospheric reflection

Ionospheric reflection is a bending, through a complex process involving reflection and refraction, of electromagnetic waves propagating in the ionosphere back toward the Earth. 
The amount of bending depends on the extent of penetration (which is a function of frequency), the angle of incidence, polarization of the wave, and ionospheric conditions, such as the ionization density. It is negatively affected by incidents of ionospheric absorption.

</doc>
<doc id="41290" url="https://en.wikipedia.org/wiki?curid=41290" title="Ionospheric sounding">
Ionospheric sounding

In telecommunication and radio science, an ionospheric sounding is a technique that provides real-time data on high-frequency ionospheric-dependent radio propagation, using a basic system consisting of a synchronized transmitter and receiver. 
The time delay between transmission and reception is translated into effective ionospheric layer altitude. Vertical incident sounding uses a collocated transmitter and receiver and involves directing a range of frequencies vertically to the ionosphere and measuring the values of the reflected returned signals to determine the effective ionosphere layer altitude. This technique is also used to determine the critical frequency. Oblique sounders use a transmitter at one end of a given propagation path, and a synchronized receiver, usually with an oscilloscope-type display (ionogram), at the other end. The transmitter emits a stepped- or swept-frequency signal which is displayed or measured at the receiver. The measurement converts time delay to effective altitude of the ionospheric layer. The ionogram display shows the effective altitude of the ionospheric layer as a function of frequency.

</doc>
<doc id="41291" url="https://en.wikipedia.org/wiki?curid=41291" title="Isochronous">
Isochronous

A sequence of events is isochronous if the events occur regularly, or at equal time intervals. The term "isochronous" is used in several technical contexts, but usually refers to the primary subject maintaining a constant period or interval (the reciprocal of frequency), despite variations in other measurable factors in the same system.
Isochronous timing differs from synchronous timing, in that the latter refers to relative timing between two or more sequences of events.

</doc>
<doc id="41292" url="https://en.wikipedia.org/wiki?curid=41292" title="Isochronous burst transmission">
Isochronous burst transmission

Isochronous burst transmission is a method of transmission. In a data network where the information-bearer channel rate is higher than the input data signaling rate, transmission is performed by interrupting, at controlled intervals, the data stream being transmitted. 
"Note 1:" Burst transmission in isochronous form enables communication between data terminal equipment (DTE) and data networks that operate at dissimilar data signaling rates, such as when the information-bearer channel rate is higher than the DTE output data signaling rate. 
"Note 2:" The binary digits are transferred at the information-bearer channel rate. The data transfer is interrupted at intervals in order to produce the required average data signaling rate. 
"Note 3:" The interruption is always for an integral number of unit intervals. 
"Note 4:" Isochronous burst transmission has particular application where envelopes are being transferred between data circuit terminating equipment (DCE) and only the bytes contained within the envelopes are being transferred between the DCE and the DTE. "Synonyms": burst isochronous "(deprecated)", interrupted isochronous transmission.

</doc>
<doc id="41293" url="https://en.wikipedia.org/wiki?curid=41293" title="Isochronous signal">
Isochronous signal

In telecommunication, an isochronous signal is a signal in which the time interval separating any two significant instants is equal to the unit interval or a multiple of the unit interval. Variations in the time intervals are constrained within specified limits.
"Isochronous" is a characteristic of one signal, while "synchronous" indicates a relationship between two or more signals.

</doc>
<doc id="41295" url="https://en.wikipedia.org/wiki?curid=41295" title="Jerkiness">
Jerkiness

Jerkiness, sometimes called strobing or choppy, describes the perception of individual still images in a motion picture.
Motion pictures are made from still images shown in rapid sequence. Provided there is sufficient continuity between the images and provided the sequence is shown fast enough, the central nervous system interprets the sequence as continuous motion.
In conventional cinematography, the images are filmed and displayed at 24 frames per second, at which speed jerkiness is not normally discernible.
Television screens refresh at even higher frequencies. PAL and SÉCAM television (the standards in Europe) refresh at 25 or 50 (HDTV) frames per second. NTSC television displays (the standard in North America) refresh at 29.97 frames per second.
However, some technologies cannot process or carry data fast enough for sufficiently high frame rates. For example, viewing motion pictures by Internet connection generally necessitates a greatly reduced frame rate, making jerkiness clearly apparent.
Animated cartoon films are typically made at reduced frame rates (accomplished by shooting several film frames of the individual drawings) so as to limit production costs, with the result that jerkiness tends to be apparent.
Strobing can also refer to cross colour and Moiré patterning:
The former is where any high frequency luminance content of the picture, close to the TV systems colour sub-carrier frequency, is interpreted by the analogue receiver's decoder as colour information.
Moiré is where an interference pattern is produced by fine scene detail beating with the line (or even pixel) structure of the device used to analyse or display the scene.

</doc>
<doc id="41296" url="https://en.wikipedia.org/wiki?curid=41296" title="Jitter">
Jitter

Jitter is the deviation from true periodicity of a presumed periodic signal in electronics and telecommunications, often in relation to a reference clock source. Jitter may be observed in characteristics such as the frequency of successive pulses, the signal amplitude, or phase of periodic signals. Jitter is a significant, and usually undesired, factor in the design of almost all communications links (e.g., USB, PCI-e, SATA, OC-48). In clock recovery applications it is called "timing jitter".
Jitter can be quantified in the same terms as all time-varying signals, e.g., root mean square (RMS), or peak-to-peak displacement. Also like other time-varying signals, jitter can be expressed in terms of spectral density (frequency content).
"Jitter period" is the interval between two times of maximum effect (or minimum effect) of a signal characteristic that varies regularly with time. "Jitter frequency", the more commonly quoted figure, is its inverse. ITU-T G.810 classifies jitter frequencies below 10 Hz as wander and frequencies at or above 10 Hz as jitter.
Jitter may be caused by electromagnetic interference (EMI) and crosstalk with carriers of other signals. Jitter can cause a display monitor to flicker, affect the performance of processors in personal computers, introduce clicks or other undesired effects in audio signals, and loss of transmitted data between network devices. The amount of tolerable jitter depends on the affected application.
Sampling jitter.
In analog to digital and digital to analog conversion of signals, the sampling is normally assumed to be periodic with a fixed period—the time between every two samples is the same. If there is jitter present on the clock signal to the analog-to-digital converter or a digital-to-analog converter, the time between samples varies and instantaneous signal error arises. The error is proportional to the slew rate of the desired signal and the absolute value of the clock error. Various effects such as noise (random jitter), or spectral components (periodic jitter) can come about depending on the pattern of the jitter in relation to the signal. In some conditions, less than a nanosecond of jitter can reduce the effective bit resolution of a converter with a Nyquist frequency of 22 kHz to 14 bits.
This is a consideration in high-frequency signal conversion, or where the clock signal is especially prone to interference.
Packet jitter in computer networks.
In the context of computer networks, jitter is the variation in latency as measured in the variability over time of the packet latency across a network. A network with constant latency has no variation (or jitter). Packet jitter is expressed as an average of the deviation from the network mean latency. However, for this use, the term is imprecise. The standards-based term is "packet delay variation" (PDV). PDV is an important quality of service factor in assessment of network performance.
Compact disc seek jitter.
In the context of digital audio extraction from compact discs, seek jitter causes extracted audio samples to be doubled-up or skipped entirely if the Compact Disc drive re-seeks. The problem occurs because the Red Book does not require block-accurate addressing during seeking. As a result, the extraction process may restart a few samples early or late, resulting in doubled or omitted samples. These glitches often sound like tiny repeating clicks during playback. A successful approach to correction in software involves performing overlapping reads and fitting the data to find overlaps at the edges. Most extraction programs perform seek jitter correction. CD manufacturers avoid seek jitter by extracting the entire disc in one continuous read operation, using special CD drive models at slower speeds so the drive does not re-seek.
A jitter meter is a testing instrument for measuring clock jitter values, and is used in manufacturing DVD and CD-ROM discs.
Due to additional sector level addressing added in the Yellow Book, CD-ROM data discs are not subject to seek jitter.
Jitter metrics.
For clock jitter, there are three commonly used metrics: "absolute jitter", "period jitter", and "cycle to cycle jitter".
Absolute jitter is the absolute difference in the position of a clock's edge from where it would ideally be.
Period jitter (aka "cycle jitter") is the difference between any one clock period and the ideal/average clock period. Accordingly, it can be thought of as the discrete-time derivative of absolute jitter. Period jitter tends to be important in synchronous circuitry like digital state machines where the error-free operation of the circuitry is limited by the shortest possible clock period, and the performance of the circuitry is limited by the average clock period. Hence, synchronous circuitry benefits from minimizing period jitter, so that the shortest clock period approaches the average clock period.
Cycle-to-cycle jitter is the difference in length/duration of any two adjacent clock periods. Accordingly, it can be thought of as the discrete-time derivative of period jitter. It can be important for some types of clock generation circuitry used in microprocessors and RAM interfaces.
Since they have different generation mechanisms, different circuit effects, and different measurement methodology, it is useful to quantify them separately.
In telecommunications, the unit used for the above types of jitter is usually the "Unit Interval" (abbreviated "UI") which quantifies the jitter in terms of a fraction of the ideal period of a bit. This unit is useful because it scales with clock frequency and thus allows relatively slow interconnects such as T1 to be compared to higher-speed internet backbone links such as OC-192. Absolute units such as "picoseconds" are more common in microprocessor applications. Units of "degrees" and "radians" are also used.
If jitter has a Gaussian distribution, it is usually quantified using the standard deviation of this distribution (aka. "RMS"). Often, jitter distribution is significantly non-Gaussian. This can occur if the jitter is caused by external sources such as power supply noise. In these cases, "peak-to-peak" measurements are more useful. Many efforts have been made to meaningfully quantify distributions that are neither Gaussian nor have meaningful peaks (which is the case in all real jitter). All have shortcomings but most tend to be good enough for the purposes of engineering work. Note that typically, the reference point for jitter is defined such that the "mean" jitter is 0.
In networking, in particular IP networks such as the Internet, jitter can refer to the variation (statistical dispersion) in the delay of the packets.
Types.
Random jitter.
Random Jitter, also called Gaussian jitter, is unpredictable electronic timing noise. Random jitter typically follows a Gaussian distribution or Normal distribution. It usually follows this pattern because most noise or jitter in an electrical circuit is caused by thermal noise, which has a Gaussian distribution. Another reason for random jitter to have a distribution like this is due to the central limit theorem. The central limit theorem states that composite effect of many uncorrelated noise sources, regardless of the distributions, approaches a Gaussian distribution.
One of the main differences between random and deterministic jitter is that deterministic jitter is bounded and random jitter is unbounded.
Deterministic jitter.
Deterministic jitter is a type of clock timing jitter or data signal jitter that is predictable and reproducible. The peak-to-peak value of this jitter is bounded, and the bounds can easily be observed and predicted. Deterministic jitter can either be correlated to the data stream (data-dependent jitter) or uncorrelated to the data stream (bounded uncorrelated jitter). Examples of data-dependent jitter are duty-cycle dependent jitter (also known as duty-cycle distortion) and intersymbol interference.
Deterministic jitter (or DJ) has a known non-Gaussian probability distribution.
Total jitter.
Total jitter ("T") is the combination of random jitter ("R") and deterministic jitter ("D"):
in which the value of "n" is based on the bit error rate (BER) required of the link.
A common bit error rate used in communication standards such as Ethernet is 10−12.
Testing.
Testing for jitter and its measurement is of growing importance to electronics engineers because of increased clock frequencies in digital electronic circuitry to achieve higher device performance. Higher clock frequencies have commensurately smaller eye openings, and thus impose tighter tolerances on jitter. For example, modern computer motherboards have serial bus architectures with eye openings of 160 picoseconds or less. This is extremely small compared to parallel bus architectures with equivalent performance, which may have eye openings on the order of 1000 picoseconds.
Testing of device performance for jitter tolerance often involves the injection of jitter into electronic components with specialized test equipment.
Jitter is measured and evaluated in various ways depending on the type of circuitry under test. For example, jitter in serial bus architectures is measured by means of eye diagrams, according to industry accepted standards. A less direct approach—in which analog waveforms are digitized and the resulting data stream analyzed—is employed when measuring pixel jitter in frame grabbers. In all cases, the goal of jitter measurement is to verify that the jitter will not disrupt normal operation of the circuitry.
There are standards for jitter measurement in serial bus architectures. The standards cover jitter tolerance, jitter transfer function and jitter generation, with the required values for these attributes varying among different applications. Where applicable, compliant systems are required to conform to these standards.
Mitigation.
Anti-jitter circuits.
Anti-jitter circuits (AJCs) are a class of electronic circuits designed to reduce the level of jitter in a regular pulse signal. AJCs operate by re-timing the output pulses so they align more closely to an idealised pulse signal. They are widely used in clock and data recovery circuits in digital communications, as well as for data sampling systems such as the analog-to-digital converter and digital-to-analog converter. Examples of anti-jitter circuits include phase-locked loop and delay-locked loop. Inside digital to analog converters jitter causes unwanted high-frequency distortions. In this case it can be suppressed with high fidelity clock signal usage.
Jitter buffers.
Jitter buffers or de-jitter buffers are used to counter jitter introduced by queuing in packet switched networks so that a continuous playout of audio (or video) transmitted over the network can be ensured. The maximum jitter that can be countered by a de-jitter buffer is equal to the buffering delay introduced before starting the play-out of the mediastream. In the context of packet-switched networks, the term "packet delay variation" is often preferred over "jitter".
Some systems use sophisticated delay-optimal de-jitter buffers that are capable of adapting the buffering delay to changing network jitter characteristics. These are known as adaptive de-jitter buffers and the adaptation logic is based on the jitter estimates computed from the arrival characteristics of the media packets. Adaptive de-jittering involves introducing discontinuities in the media play-out, which may appear offensive to the listener or viewer. Adaptive de-jittering is usually carried out for audio play-outs that feature a VAD/DTX encoded audio, that allows the lengths of the silence periods to be adjusted, thus minimizing the perceptual impact of the adaptation.
Dejitterizer.
A dejitterizer is a device that reduces jitter in a digital signal. A dejitterizer usually consists of an elastic buffer in which the signal is temporarily stored and then retransmitted at a rate based on the average rate of the incoming signal. A dejitterizer is usually ineffective in dealing with low-frequency jitter, such as waiting-time jitter.
Filtering.
A filter can be designed to minimize the effect of sampling jitter. For more information, see the paper by S. Ahmed and T. Chen entitled, "Minimizing the effects of sampling jitters in wireless sensors networks".
Video and image jitter.
Video or image jitter occurs when the horizontal lines of video image frames are randomly displaced due to the corruption of synchronization signals or electromagnetic interference during video transmission. Model based dejittering study has been carried out under the framework of digital image/video restoration.

</doc>
<doc id="41297" url="https://en.wikipedia.org/wiki?curid=41297" title="Joint multichannel trunking and switching system">
Joint multichannel trunking and switching system

The Joint multichannel trunking and switching system is that composite multichannel trunking and switching system formed from assets of the Services, the Defense Communications System, other available systems, and/or assets controlled by the Joint Chiefs of Staff to provide an operationally responsive, survivable communication system, preferably in a mobile/transportable/recoverable configuration, for the joint force commander in an area of operations. 

</doc>
<doc id="41298" url="https://en.wikipedia.org/wiki?curid=41298" title="Journal">
Journal

__NOTOC__
A journal (through French from Latin "diurnalis", daily) has several related meanings:
The word "journalist", for one whose business is writing for the public press and nowadays also other media, has been in use since the end of the 17th century.
Public journal.
A public journal is a record of day-by-day events in a parliament or congress. It is also called minutes or records.
Business and accounting.
The term "journal" is also used in business:

</doc>
<doc id="41299" url="https://en.wikipedia.org/wiki?curid=41299" title="Justify">
Justify

Justify may refer to:
In music:

</doc>
<doc id="41300" url="https://en.wikipedia.org/wiki?curid=41300" title="Kendall effect">
Kendall effect

In telecommunications the Kendall effect is a spurious pattern or other distortion in a facsimile.
It is caused by unwanted modulation products which arise from the transmission of the carrier signal, and appear in the form of a rectified baseband that interferes with the lower sideband of the carrier. 
The Kendall effect occurs principally when the single-sideband width is greater than half of the facsimile carrier frequency.

</doc>
<doc id="41303" url="https://en.wikipedia.org/wiki?curid=41303" title="K-factor">
K-factor

K-factor or K factor may refer to:
In telecommunications:

</doc>
<doc id="41304" url="https://en.wikipedia.org/wiki?curid=41304" title="Knife-edge effect">
Knife-edge effect

In electromagnetic wave propagation, the knife-edge effect or edge diffraction is a redirection by diffraction of a portion of the incident radiation that strikes a well-defined obstacle such as a mountain range or the edge of a building.
The knife-edge effect is explained by Huygens–Fresnel principle, which states that a well-defined obstruction to an electromagnetic wave acts as a secondary source, and creates a new wavefront. This new wavefront propagates into the geometric shadow area of the obstacle.
The knife-edge effect is an outgrowth of the half-plane problem, originally solved by Arnold Sommerfeld using a plane wave spectrum formulation. A generalization of the halfplane problem is the wedge problem, solvable as a boundary value problem in cylindrical coordinates. The solution in cylindrical coordinates was then extended to the optical regime by Joseph B. Keller, who introduced the notion of diffraction coefficients through his geometrical theory of diffraction (GTD). Pathak and Kouyoumjian extended the (singular) Keller coefficients via the uniform theory of diffraction (UTD).

</doc>
<doc id="41305" url="https://en.wikipedia.org/wiki?curid=41305" title="Label (disambiguation)">
Label (disambiguation)

A label is any kind of tag attached to something so as to identify the object or its content.
Label may also refer to:

</doc>
<doc id="41306" url="https://en.wikipedia.org/wiki?curid=41306" title="Lambert's cosine law">
Lambert's cosine law

In optics, Lambert's cosine law says that the radiant intensity or luminous intensity observed from an ideal diffusely reflecting surface or ideal diffuse radiator is directly proportional to the cosine of the angle "θ" between the direction of the incident light and the surface normal. The law is also known as the cosine emission law or Lambert's emission law. It is named after Johann Heinrich Lambert, from his "Photometria", published in 1760.
A surface which obeys Lambert's law is said to be "Lambertian", and exhibits Lambertian reflectance. Such a surface has the same radiance when viewed from any angle. This means, for example, that to the human eye it has the same apparent brightness (or luminance). It has the same radiance because, although the emitted power from a given area element is reduced by the cosine of the emission angle, the apparent size (solid angle) of the observed area, as seen by a viewer, is decreased by a corresponding amount. Therefore, its radiance (power per unit solid angle per unit projected source area) is the same.
Lambertian scatterers and radiators.
When an area element is radiating as a result of being illuminated by an external source, the irradiance (energy or photons/time/area) landing on that area element will be proportional to the cosine of the angle between the illuminating source and the normal. A Lambertian scatterer will then scatter this light according to the same cosine law as a Lambertian emitter. This means that although the radiance of the surface depends on the angle from the normal to the illuminating source, it will not depend on the angle from the normal to the observer. For example, if the moon were a Lambertian scatterer, one would expect to see its scattered brightness appreciably diminish towards the terminator due to the increased angle at which sunlight hit the surface. The fact that it does not diminish illustrates that the moon is not a Lambertian scatterer, and in fact tends to scatter more light into the oblique angles than would a Lambertian scatterer.
The emission of a Lambertian radiator does not depend upon the amount of incident radiation, but rather from radiation originating in the emitting body itself. For example, if the sun were a Lambertian radiator, one would expect to see a constant brightness across the entire solar disc. The fact that the sun exhibits limb darkening in the visible region illustrates that it is not a Lambertian radiator. A black body is an example of a Lambertian radiator.
Details of equal brightness effect.
The situation for a Lambertian surface (emitting or scattering) is illustrated in Figures 1 and 2. For conceptual clarity we will think in terms of photons rather than energy or luminous energy. The wedges in the circle each represent an equal angle "dΩ", and for a Lambertian surface, the number of photons per second emitted into each wedge is proportional to the area of the wedge.
It can be seen that the length of each wedge is the product of the diameter of the circle and cos("θ"). It can also be seen that the maximum rate of photon emission per unit solid angle is along the normal and diminishes to zero for "θ" = 90°. In mathematical terms, the radiance along the normal is "I" photons/(s·cm2·sr) and the number of photons per second emitted into the vertical wedge is "I" "dΩ" "dA". The number of photons per second emitted into the wedge at angle "θ" is "I" cos("θ") "dΩ" "dA".
Figure 2 represents what an observer sees. The observer directly above the area element will be seeing the scene through an aperture of area "dA"0 and the area element "dA" will subtend a (solid) angle of "dΩ"0. We can assume without loss of generality that the aperture happens to subtend solid angle "dΩ" when "viewed" from the emitting area element. This normal observer will then be recording "I" "dΩ" "dA" photons per second and so will be measuring a radiance of
The observer at angle "θ" to the normal will be seeing the scene through the same aperture of area "dA"0 and the area element "dA" will subtend a (solid) angle of "dΩ"0 cos("θ"). This observer will be recording "I" cos("θ") "dΩ" "dA" photons per second, and so will be measuring a radiance of
which is the same as the normal observer.
Relating peak luminous intensity and luminous flux.
In general, the luminous intensity of a point on a surface varies by direction; for a Lambertian surface, that distribution is defined by the cosine law, with peak luminous intensity in the normal direction. Thus when the Lambertian assumption holds, we can calculate the total luminous flux, formula_3, from the peak luminous intensity, formula_4, by integrating the cosine law:
and so
where formula_9 is the determinant of the Jacobian matrix for the unit sphere, and realizing that formula_4 is luminous flux per steradian. Similarly, the peak intensity will be formula_11 of the total radiated luminous flux. For Lambertian surfaces, the same factor of formula_12 relates luminance to luminous emittance, radiant intensity to radiant flux, and radiance to radiant emittance. Radians and steradians are, of course, dimensionless and so "rad" and "sr" are included only for clarity.
Example: A surface with a luminance of say 100 cd/m2 (= 100 nits, typical PC monitor) will, if it is a perfect Lambert emitter, have a luminous emittance of 314 lm/m2. If its area is 0.1 m2 (~19" monitor) then the total light emitted, or luminous flux, would thus be 31.4 lm.
Uses.
Lambert's cosine law in its reversed form (Lambertian reflection) implies that the apparent brightness of a Lambertian surface is proportional to the cosine of the angle between the surface normal and the direction of the incident light.
This phenomenon is, among others, used when creating mouldings, with the effect of creating light- and dark-shaded stripes on a structure or object without having to change the material or apply pigment. The contrast of dark and light areas gives definition to the object. Mouldings are strips of material with various cross-sections used to cover transitions between surfaces or for decoration.

</doc>
<doc id="41308" url="https://en.wikipedia.org/wiki?curid=41308" title="Launch angle">
Launch angle

In fiber optic telecommunications, the launch angle has the following meanings: 

</doc>
<doc id="41309" url="https://en.wikipedia.org/wiki?curid=41309" title="Launch numerical aperture">
Launch numerical aperture

In telecommunications, launch numerical aperture (LNA) is the numerical aperture of an optical system used to couple (launch) power into an optical fiber. 
LNA may differ from the stated NA of a final focusing element if, for example, that element is underfilled or the focus is other than that for which the element is specified. 
LNA is one of the parameters that determine the initial distribution of power among the modes of an optical fiber.

</doc>
<doc id="41310" url="https://en.wikipedia.org/wiki?curid=41310" title="Layer">
Layer

Layer may refer to:

</doc>
<doc id="41311" url="https://en.wikipedia.org/wiki?curid=41311" title="Layered system">
Layered system

In telecommunication, a layered system is a system in which components are grouped, "i.e.", layered, in a hierarchical arrangement, such that lower layers provide functions and services that support the functions and services of higher layers. 
Systems of ever-increasing complexity and capability can be built by adding or changing the layers to improve overall system capability while using the components that are still in place.

</doc>
<doc id="41312" url="https://en.wikipedia.org/wiki?curid=41312" title="Leaky mode">
Leaky mode

A leaky mode or tunneling mode in an optical fiber or other waveguide is a mode having an electric field that decays monotonically for a finite distance in the transverse direction but becomes oscillatory everywhere beyond that finite distance. Such a mode gradually "leaks" out of the waveguide as it travels down it, producing attenuation even if the waveguide is perfect in every respect. In order for a leaky mode to be definable as a mode, the relative amplitude of the oscillatory part (the leakage rate) must be sufficiently small that the mode substantially maintains its shape as it decays.
Leaky modes correspond to leaky rays in the terminology of geometric optics.
The propagation of light through optical fibre can take place via meridional rays or skew rays. These skew rays suffer only partial reflection while meridional rays are completely guided. Thus the modes allowing propagation of skew rays are called leaky modes. 
Some optical power is lost into clad due to these modes.

</doc>
<doc id="41314" url="https://en.wikipedia.org/wiki?curid=41314" title="Limiting">
Limiting

Limiting: Any process by which a specified characteristic (usually amplitude) of the output of a device is prevented from exceeding a predetermined value. 
Limiting can refer to non-linear clipping, in which a signal is passed through normally but "sheared off" when it would normally exceed a certain threshold. It can also refer to a type of variable-gain audio level compression, in which the gain of an amplifier is changed very quickly to prevent the signal from going over a certain amplitude.

</doc>
<doc id="41316" url="https://en.wikipedia.org/wiki?curid=41316" title="Linear polarization">
Linear polarization

In electrodynamics, linear polarization or plane polarization of electromagnetic radiation is a confinement of the electric field vector or magnetic field vector to a given plane along the direction of propagation. See polarization for more information.
The orientation of a linearly polarized electromagnetic wave is defined by the direction of the electric field vector. For example, if the electric field vector is vertical (alternately up and down as the wave travels) the radiation is said to be vertically polarized.
Mathematical description of linear polarization.
The classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is (cgs units)
for the magnetic field, where k is the wavenumber,
is the angular frequency of the wave, and formula_4 is the speed of light.
Here formula_5 is the amplitude of the field and
is the Jones vector in the x-y plane.
The wave is linearly polarized when the phase angles formula_7 are equal,
This represents a wave polarized at an angle formula_9 with respect to the x axis. In that case, the Jones vector can be written
The state vectors for linear polarization in x or y are special cases of this state vector.
If unit vectors are defined such that
and
then the polarization state can be written in the "x-y basis" as

</doc>
<doc id="41317" url="https://en.wikipedia.org/wiki?curid=41317" title="Line code">
Line code

In telecommunication, a line code is a code chosen for use within a communications system for transmitting a digital signal down a line. Line coding is often used for digital data transport. Some line codes are digital baseband modulation or digital baseband transmission methods, and these are baseband line codes that are used when the line can carry DC components.
Line coding.
Line coding consists of representing the digital signal to be transported, by a waveform that is optimally tuned for the specific properties of the physical channel (and of the receiving equipment). The pattern of voltage, current or photons used to represent the digital data on a transmission link is called "line encoding". The common types of line encoding are unipolar, polar, bipolar, and Manchester encoding.
For reliable clock recovery at the receiver, one usually imposes a maximum run length constraint on the generated channel sequence, i.e., the maximum number of consecutive ones or zeros is bounded to a reasonable number. A clock period is recovered by observing transitions in the received sequence, so that a maximum run length guarantees such clock recovery, while sequences without such a constraint could seriously hamper the detection quality.
After line coding, the signal is put through a "physical channel", either a "transmission medium" or "data storage medium".
Sometimes the characteristics of two very different-seeming channels are similar enough that the same line code is used for them. The most common physical channels are:
Some of the more common or binary line codes include:
Each line code has advantages and disadvantages. The particular line code used is chosen to meet one or more of the following criteria:
Disparity.
The disparity of a bit pattern is the difference in the number of one bits vs the number of zero bits. The running disparity is the running total of the disparity of all previously transmitted words.
Unfortunately, most long-distance communication channels cannot transport a DC component. The DC component is also called the disparity, the bias, or the DC coefficient. The simplest possible line code, unipolar, gives too many errors on such systems, because it has an unbounded DC component.
Most line codes eliminate the DC component such codes are called DC-balanced, zero-DC, DC-free, zero-bias, DC equalized, etc.
There are three ways of eliminating the DC component:
Polarity.
Bipolar line codes have two polarities, are generally implemented as RZ, and have a radix of three since there are three distinct output levels One of the principle advantages of this type of code, is that it can completely eliminate any DC component. This is important if the signal must pass through a transformer or a long transmission line.
Unfortunately, several long-distance communication channels have polarity ambiguity.
To compensate, several people have designed polarity-insensitive transmission systems.
There are three ways of providing unambiguous reception of "0" bits or "1" bits over such channels:
Synchronization.
Line coding should make it possible for the receiver to synchronize itself to the phase of the received signal. If the synchronization is not ideal, then the signal to be decoded will not have optimal differences (in amplitude) between the various digits or symbols used in the line code. This will increase the error probability in the received data.
Biphase line codes require at least one transition per bit time. This makes it easier to synchronize the transceivers and detect errors, however, the baud rate is greater than that of NRZ codes.
Other considerations.
It is also preferred for the line code to have a structure that will enable error detection.
Note that the line-coded signal and a signal produced at a terminal may differ, thus requiring translation.
A line code will typically reflect technical requirements of the transmission medium, such as optical fiber or shielded twisted pair. These requirements are unique for each medium, because each one has different behavior related to interference, distortion, capacitance and loss of amplitude.
Common line codes.
Optical line codes:

</doc>
<doc id="41319" url="https://en.wikipedia.org/wiki?curid=41319" title="Link">
Link

Link or Links may refer to:

</doc>
<doc id="41320" url="https://en.wikipedia.org/wiki?curid=41320" title="Link level">
Link level

Link level: In the hierarchical structure of a primary or secondary station, the conceptual level of control or data processing logic that controls the data link. 
"Note:" Link-level functions provide an interface between the station high-level logic and the data link. Link-level functions include (a) transmit bit injection and receive bit extraction, (b) address and control field interpretation, (c) command response generation, transmission and interpretation, and (d) frame check sequence computation and interpretation.

</doc>
<doc id="41321" url="https://en.wikipedia.org/wiki?curid=41321" title="Link quality analysis">
Link quality analysis

In adaptive high-frequency (HF) radio, link quality analysis (LQA) is the overall process by which measurements of signal quality are made, assessed, and analyzed.
In LQA, signal quality is determined by measuring, assessing, and analyzing link parameters, such as bit error ratio (BER), and the levels of the ratio of signal-plus-noise-plus-distortion to noise-plus-distortion (SINAD). Measurements are stored at—and exchanged between—stations, for use in making decisions about link establishment.
For adaptive HF radio, LQA is automatically performed and is usually based on analyses of pseudo-BERs and SINAD readings.

</doc>
<doc id="41323" url="https://en.wikipedia.org/wiki?curid=41323" title="Load">
Load

Load may refer to:

</doc>
<doc id="41325" url="https://en.wikipedia.org/wiki?curid=41325" title="Loading characteristic">
Loading characteristic

Loading characteristic: In multichannel telephone systems, a plot, for the busy hour, of the equivalent mean power and the peak power as a function of the number of voice channels. 
The equivalent power of a multichannel signal referred to the zero transmission level point is a function of the number of channels and has for its basis a specified voice channel mean power.

</doc>
