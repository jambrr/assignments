<doc id="44005" url="https://en.wikipedia.org/wiki?curid=44005" title="The English Patient">
The English Patient

The English Patient is a 1992 novel by Michael Ondaatje. The book follows four dissimilar people brought together at an Italian villa during the Italian Campaign of World War II. The four main characters are: an unrecognisably burned man—the titular patient, presumed to be English; his Canadian Army nurse, a Sikh British Army sapper, and a Canadian thief. The story occurs during the North African Campaign and centres on the incremental revelations of the patient's actions prior to his injuries, and the emotional effects of these revelations on the other characters. The book won the Booker Prize and the Governor General's Award.
Plot synopsis.
The novel's historical backdrop is the North African/Italian Campaigns of World War II. The story is told out of sequence, moving back and forth between the severely burned "English" patient's memories from before his accident and current events at the bomb-damaged Villa San Girolamo, an Italian monastery, where he is being cared for by Hana, a troubled young Canadian Army nurse. The English patient's only possession is a well-worn, and heavily annotated copy of Herodotus's "The Histories" that has survived the fiery parachute drop. Hearing the book constantly being read aloud to him brings about detailed recollections of his desert explorations, yet he is unable to recall his own name. Instead, he chooses to believe the assumption by others that he is an Englishman based on the sound of his voice. The patient is in fact László de Almásy, a Hungarian Count and desert explorer, one of many members of a British cartography group.
Caravaggio, an Italian-Canadian in the British foreign intelligence service since the late 1930s, befriended Hana's father before the latter died in the war. He learns that Hana is at the villa caring for a patient. He had remained in North Africa to spy when the German forces gain control and then transfers to Italy. He is eventually caught, interrogated, and tortured; they even cut off his thumbs. Caravaggio bears physical and psychological scars from his painful war experience for which he seeks vengeance.
Two British soldiers yell at Hana to stop her from playing a piano since the Germans often booby-trapped them. One of the soldiers, Kip, an Indian Sikh, a trained sapper, specializes in bomb and ordnance disposal. Kip decides to stay at the villa to attempt to clear it of unexploded ordnance. Kip and the English patient immediately become friends.
The English patient, sedated by morphine, begins to reveal everything: he fell in love with the Englishwoman Katharine Clifton who, with her husband Geoffrey, accompanied Almásy's desert exploration team. Almásy was mesmerized by Katharine's voice as she read Herodotus' "Histories" out loud by the campfire. They soon began a very intense affair, but she cut it short, claiming that Geoffrey would go mad if he were to discover them.
Geoffrey offers to return Almásy to Cairo on his plane since the expedition will break camp with the coming of war. Almásy is unaware that Katharine is aboard the plane as it flies low over him and then crashes. Geoffrey is killed outright. Katharine is injured internally and Almásy leaves her in the Cave of Swimmers. Caravaggio tells Almásy that British Intelligence knew about the affair. Almásy makes a three-day trek to British-controlled El Taj for help. When he arrives, he is detained as a spy because of his name, despite telling them about Katharine's predicament. He later guides German spies across the desert to Cairo. Almásy retrieves Katharine's body from the Cave and, while flying back, the decrepit plane leaks oil onto him and both of them catch fire. He parachutes from the plane and is found by the Bedouin.
The novel ends with Kip learning that America has bombed Hiroshima and Nagasaki. He departs from Villa San Girolamo, estranged from his white companions. This is another manifestation of the offshoots of war and the dominance of race, along with the strive for power.
Characters.
Almásy.
Count Ladislaus de Almásy is the titlular character who comes under Hana's care in Italy after being burned unrecognizably in Africa. Although Hungarian by birth, because he has lived without government identification or many verifiable long-term interactions, his accent prompts the authorities around him to perceive an English affiliation and to refer to him as the English Patient. Almásy serves as a blank canvas onto which the other characters project their experience during this time in Italy. For example, Hana treats him tenderly to redeem herself for not being by the side of her father when he was engulfed in flames and died. She provides comfort to the English Patient that she could not provide to her own father.
The rejection of a nationalistic identity enables Almásy to rationalize his duplicitous actions with his associates. He socializes with, and is a mapmaker for, the British before the war, then uses that information to smuggle German spies across northern Africa. Almásy is portrayed in a sympathetic light, partly because he tells his own story, but also because he always adheres to his own moral code.
Almásy is also at the centre of one of the novel's love stories. He is involved in an adulterous relationship with Katharine Clifton, which eventually leads to her death and the death of her husband, Geoffrey Clifton. Katharine is the figure who leads Almásy to sensuality. He falls in love with her voice as she reads Herodotus. Sensuality, both sexual and observational, is a major theme in the novel.
The character is loosely based on László Almásy, a well-known desert explorer in 1930s Egypt, who helped the German side in World War II. Almásy did not suffer burns or die in Italy, but survived the war and lived until 1951.
Hana.
Hana is a twenty-year-old Canadian Army nurse torn between her youth and her maturity. Being a good nurse, she quickly learns that she cannot become emotionally attached to her patients. She calls them all "buddy," and forgets them immediately once they die. Her lover, a Canadian officer, is killed and because of this, Hana comes to believe that she is cursed and that all those around her are doomed to die.
In contrast, upon hearing of her father's death Hana has an emotional breakdown. She then puts all of her energy into caring for the English Patient. She washes his wounds and provides him with morphine. When the hospital is abandoned, Hana refuses to leave staying with her patient. She sees Almásy as saint like and falls in love with his pure nature.
In addition to her relationship with Almásy, Hana also forms a strong relationship with Kip during his stay at the villa.
Kip.
Kirpal (Kip) Singh is an Indian Sikh who has volunteered with the British military for sapper bomb disposal training under Lord Suffolk. This act of patriotism is not shared by his Indian nationalist brother; the skepticism of his unit's white peers discourages a sense of community for Kip.
Lord Suffolk, an eccentric English nobleman, has developed techniques to dismantle complicated, unexploded bombs in what is a very dangerous occupation. Kip feels a sense of belonging in a community when he is welcomed into the Suffolk household. Lord Suffolk and his sapper team are killed while attempting to dismantle a new type of bomb. Their deaths cause Kip's emotional withdrawal to become more pronounced. It should be noted that Charles Howard, 20th Earl of Suffolk, was a real person who did dismantle of bombs and was killed while attempting to dismantle one.
Kip is transferred to another unit in Italy where he and his partner hear a piano playing. As they enter the villa, they come across Hana and urge her to stop playing as the Germans were known to sabotage musical instruments. Kip stays on at the villa to clear any remaining unexploded bombs, mines, or other booby-traps. Kip feels a sense of community and confidence when he becomes Hana's lover. Kip sees the interactions of the Westerners at the villa as those of a group that disregards nationality. They get together and celebrate Hana's twenty-first birthday, a symbol of their friendship and Kip's acceptance. However, when he learns of the nuclear bombing of Hiroshima Kip is thoroughly shocked. He leaves immediately, unable to believe that Westerners would do such a thing. Kip goes back to India and never returns, though he never stops recalling the effect of Hana in his life.
David Caravaggio.
David Caravaggio is a Canadian thief whose profession is legitimized by the war, as the Allies needed crafty people to steal Axis documents. He is a long-time friend of Hana's father and becomes known as "the man with bandaged hands" when he arrives at the villa; the bandages cover his severed thumbs the result of an interrogation by the Italians in Florence. He recalls that Ranuccio Tommasoni ordered the interrogation tactic. This is a reference to a man by the same name who was murdered by the historical Caravaggio in 1606.
The mental and physical outcome of the torture is that Caravaggio has "lost his nerve" and ability to steal. Hana remembers him as a very human thief. He would always be distracted by the human element while doing a job. For instance, if an advent calendar was on the wrong day, he would fix it. She also has deep feelings of love for Caravaggio. At times, Caravaggio seems to display a romantic love towards Hana. Caravaggio and Almásy share a morphine addiction. Caravaggio works this to his advantage to confirm his suspicion that Almásy is not English.
Katharine Clifton.
Katharine is the childhood friend and recently wedded wife of Geoffrey Clifton, whom she married after their days at Oxford University. The day after their wedding, she and Geoffrey flew to join Almásy's expedition. She entertained the camp in the evening by reading aloud from Almásy's copy of Heroditus' "Histories", after which she and Almásy began an affair. Geoffrey discovered the affair after she had ended it, and she is wracked with guilt. Geoffrey attempts to kill all three of them by crashing his plane while they are flying. After Geoffrey is killed in the crash, Katharine admits that she always loved Almásy.
Geoffrey Clifton.
Geoffrey is Katharine's cuckolded husband, on a secret mission for the British government to make detailed aerial maps of North Africa; his joining the Almásy expedition is only a ruse. The plane he claims to be his own was appropriated by the Crown, and he leaves his wife with the other expedition members while on his mission, leading to her infidelity.
Awards.
The novel won the 1992 Booker Prize and the 1992 Governor General's Award.
Film adaptation.
The book was adapted into a 1996 film by Anthony Minghella, starring Ralph Fiennes, Kristin Scott Thomas, Willem Dafoe, and Juliette Binoche. The film received nine Academy Awards—including Best Picture and Director—at the 69th Academy Awards.

</doc>
<doc id="44008" url="https://en.wikipedia.org/wiki?curid=44008" title="Adaptive Transform Acoustic Coding">
Adaptive Transform Acoustic Coding

Adaptive Transform Acoustic Coding (ATRAC) is a family of proprietary audio compression algorithms developed by Sony. MiniDisc was the first commercial product to incorporate ATRAC in 1992. ATRAC allowed a relatively small disc like MiniDisc to have the same running time as CD while storing audio information with minimal loss in perceptible quality. Improvements to the codec in the form of ATRAC3, ATRAC3plus, and ATRAC Advanced Lossless followed in 1999, 2002, and 2006 respectively.
Other MiniDisc manufacturers such as Sharp and Panasonic also implemented their own versions of the ATRAC codec.
General bitrate quality.
ATRAC's original 292 kbit/s bitrate was designed to be close to CD quality acoustically. This is the bitrate used on original MiniDiscs. Years later ATRAC was improved and is generally considered better than earlier versions at similar bitrates. For purposes of comparison, CDs are encoded at 1411.2 kbit/s, and lossless encoders can encode most CDs below 1000 kbit/s, with significant bitrate reduction for easier-to-encode content such as voice.
Performance.
According to ATRAC engineers, ATRAC algorithms were developed in close cooperation with LSI development engineers within Sony in order to deliver on a tangible product that could encode at high speeds and with minimal power consumption. This is in contrast to other codecs developed on computers with no regard for the constraints of portable hardware. This is reflected in the design of the ATRAC codecs, which tend to emphasize processing smaller numbers of samples at a time to save memory at the cost of compression efficiency and additional multiplies. These trade-offs are entirely logical on DSP systems, where memory is often at a premium compared to multiplier performance.
Sony Walkmans offer better battery life when playing ATRAC files as compared to MP3 files. However, as Sony only pushed ATRAC compatibility in Sony Ericsson Walkman series phones in the Japanese market, it is not supported in GSM/UMTS market phones. Sony's Xplod series of car audio CD players support ATRAC CDs. Minidiscs with ATRAC format songs have, in the past, been supported on Eclipse brand car stereos.
ATRAC1.
ATRAC1 was first used in Sony's own theater format SDDS system in the 1990s, and in this context is a direct competitor to Dolby Digital (AC3) and DTS. SDDS uses ATRAC1 with 8 channel encoding, and with a total encoding rate over all the channels of 1168 kbit/s.
Two stacked quadrature mirror filters split the signal into 3 parts:
Full stereo (i.e., independent channel) encoding with a data rate of 292 kbit/s.
Quality is generally transparent for many people (meaning that it is not possible to tell an ATRAC encoding from the source). This is most possible when using the latest algorithm, Type-S, or Type-R (Type-S only improves LP modes). Like most other audio compression codecs, some signals will "trip" the codec and cause artifacts, though these are not usually severe enough to be obvious.
High-frequency lowpass depends on the complexity of the material; some encodings have content clear up to 22.05 kHz.
ATRAC1 can also be used in mono (one channel) mode, doubling recording time.
FFmpeg has an implementation of an ATRAC1 decoder.
ATRAC3 (LP2 and LP4 Modes).
Like ATRAC1 and MP3, ATRAC3 is also a hybrid subband-MDCT encoder, but with several differences.
In ATRAC3, Three stacked QMF split the signal into 4 parts:
The four subbands are then MDCT encoded using a fixed-length transform. Unlike nearly all modern formats, the transform length cannot be varied to optimize coding transients. Instead, a simpler transient encoding technique called gain control is used, in which the gain of different subbands is varied during a transient prior to MDCT and then restored during decoding after the inverse MDCT to try to smooth over transients. Additionally, prior to quantization, tonal components are subtracted from the signal and independently quantized. During decoding, they are separately reconstructed and added back to reform the original MDCT coefficients.
Sony claims the major advantage of ATRAC3 is its coding efficiency, which was tuned for portable DSP which provides less computing power and battery life. However, as ATRAC is a hybrid subband-MDCT codec that is algorithmically very similar to MP3, any advantage is probably exaggerated. Furthermore, compared to newer formats such as Windows Media Audio which use a simple MDCT rather than a hybrid, ATRAC3 must perform an additional and computationally expensive inverse-QMF, although the hybrid system does significantly reduce memory usage, which was likely a factor given the limited memory available when ATRAC was originally developed.
This uses a 132 kbit/s data rate, the quality of which is advertised to be similar to that of MP3 encoded at a similar bit rate.
However, in an independent double-blind test (2004/05) without format encoding parameters reference against Ogg Vorbis, AAC, and LAME VBR MP3, ATRAC3 came last. Unfortunately, due to lack of transparency in ATRAC encoder versioning, it is not known if the ATRAC3 encoder tested was optimal, and subsequent investigation was inconclusive. It is possible that newer ATRAC3 encoders offer better performance.
This reduces the data rate to 66 kbit/s (half that of LP2), partly by using joint stereo coding and a lowpass filter around 13.5 kHz. It allows 324 minutes to be recorded on an 80-minute MiniDisc, with the same padding required as LP2.
FFmpeg has an implementation of an ATRAC3 decoder, which was converted to fixed precision and implemented in the Rockbox series of firmwares for ARM, Coldfire and MIPS processors. RealAudio8 is a high-bitrate implementation of ATRAC3 (up to 352.8kbit/s).
The PlayStation 3 video game "" uses 224 simultaneous streams of ATRAC3 compressed audio, with between one and eight channels per stream at sample rates between 24 and 48 kHz, each filtered using 512 frequency bands of adaptive equalisation, routed via six reverb units running on the same SPU co-processor (one of eight on the PS3's Cell chip), alongside 7.1 channel hybrid third-order Ambisonic mixing.
ATRAC3plus.
This codec is used in Sony Hi-MD Walkman devices (e.g., "Hi-LP and Hi-SP"), Network Walkman players, Memory Stick players, VAIO Pocket, PS3 and PSP console, and ATRAC CD players. It is a hybrid subband/MDCT codec based on a 16 channel QMF followed by a 128-point MDCT. Prior to MDCT coding, Generalized Harmonic Analysis (GHA) is used to extract tonal components, an improved version of the process used in ATRAC3. As in previous ATRAC versions, gain control is used to control preecho rather than variable sized transforms, although different MDCT windows are apparently possible.
In the recently released SonicStage version 3.2 and 3.3 some more bitrates have been introduced, the available bitrates are: 48, 64, 96, 128, 160, 192, 256, 320 and 352 kbit/s. The newer bitrates are not always compatible with all older hardware decoders, however, some of the older hardware has been found to be compatible with certain newer ATRAC3plus bitrates.
MiniDiscs recorded in this format are incompatible with older players.
In a test conducted by an independent firm, but financed by Sony, it was concluded that ATRAC3plus at 64 kbit/s is equal in subjective sound quality to an obsolete MP3 encoder at 128 kbit/s. Performance against modern high quality MP3 encoders was not evaluated.
ATRAC Advanced Lossless.
ATRAC Advanced Lossless is a "scalable" lossless audio codec that records a lossy ATRAC3 or ATRAC3plus stream, and supplements it with a stream of correction information stored within the file itself that allows the original signal to be reproduced, if desired. A player/decoder can extract and use just the ATRAC3 or ATRAC3plus data, or it can combine that with the correction stream to perfectly reproduce the original audio information. This allows the file to be decoded as either lossless or lossy. It is implemented in such a way that allows the file size to be smaller than uncompressed or compressed versions of the same file. Compression is approximately 30–80% of the original file. Benefits of scalable compression include providing backward compatibility, such that older devices that are not AAL-aware can still have the ATRAC3 stream available for playback without understanding the AAL format, and faster transfer speed between portable audio devices and PC.
ATRAC Advanced Lossless is widely supported in older Walkman players and SonicStage version 4 or later. Sonic Stage 4 allows download of ATRAC Advanced Lossless to Minidisc Players, PlayStation Portable, and PlayStation 3. Recent Walkman players do not support ATRAC Advanced Lossless/ATRAC. Sony has all but dropped the ATRAC related codecs in the USA and Europe and their SonicStage powered 'Connect' Music Service (Sony's equivalent of iTunes) on 31 March 2008. However, it is being continued in Japan and various other countries.
Comparable technology.
AAL's use of a "core" (lossy) and "residual" (correction) stream is similar to the idea behind Opus, MPEG-4 SLS, DTS-HD Master Audio, Dolby True HD and Ogg Vorbis bitrate peeling. In fact, AAL was the first to be released in the commercial market with this scheme for backward compatibility.
WavPack hybrid mode and OptimFROG DualStream are in the same category, but store the correction stream in a separate file.
ATRAC9.
According to Sony ATRAC9 is a high-compression audio codec optimized for games, offering low delay (granularity) and low CPU and memory usage.
Used in PS4 and PS Vita consoles. Audio middleware such as FMOD and Audiokinetic Wwise supports it.
External links.
News portal, discussion forums and downloads related to ATRAC.

</doc>
<doc id="44009" url="https://en.wikipedia.org/wiki?curid=44009" title="MiniDisc">
MiniDisc

The MiniDisc (MD) is a magneto-optical disc-based data storage device offering a capacity of 74 minutes and, later, 80 minutes, of digitized audio or 1 gigabyte of Hi-MD data. The Sony brand audio players were on the market from September 1992 until March 2013.
MiniDisc was announced by Sony in September 1992 and released that November for sale in Japan and in December in Europe, Canada, the USA and other countries . The music format was originally based exclusively on ATRAC audio data compression, but the option of linear PCM digital recording was later introduced to attain audio quality comparable to that of a compact disc. MiniDiscs were very popular in Japan but made a limited impact elsewhere.
Sony announced they would cease development of MD devices, with the last of the players sold by March 2013.
Market history.
In 1983, just a year after the introduction of the Compact Disc, Immink and Braat presented the first experiments with erasable magneto-optical Compact Discs during the 73rd AES 
Convention in Eindhoven. It took, however, almost ten years before their idea was commercialized.
Sony's MiniDisc was one of two rival digital systems, introduced in 1992, that were both targeted as a replacement for the Philips analog cassette audio tape system: the other was Digital Compact Cassette (DCC), created by Philips and Matsushita. Sony had originally intended for Digital Audio Tape (DAT) to be the dominant home digital audio recording format, replacing the analog cassette. Due to technical delays, DAT was not launched until 1989, and by then the U.S. dollar had fallen so far in relation to the yen that the introductory DAT machine Sony had intended to market for about $400 in the late 1980s now had to retail for $800 or even $1000 to break even, putting it out of reach for most users.
Relegating DAT for professional use, Sony set to work to come up with a simpler, more economical digital home format. By the time Sony came up with MiniDisc in late 1992, Philips had introduced a competing system, DCC (the digital compact cassette). This created marketing confusion very similar to the Beta versus VHS battle of the late 1970s and early 1980s. Sony attempted to license MD technology to other manufacturers, with JVC, Sharp, Pioneer, Panasonic and others all producing their own MD systems. However, non-Sony machines were not widely available in North America, and companies such as Technics and Radio Shack tended to promote DCC instead.
Despite having a loyal customer base (primarily musicians and audio enthusiasts), MiniDisc met with only limited success. It was relatively popular in Japan during the 1990s but did not enjoy comparable sales in other world markets. Since then, Recordable CDs, flash memory and HDD-based digital audio players introduced in 1998 have become increasingly popular as playback devices.
The initial low uptake of MiniDisc was attributed to the small number of pre-recorded albums available on MD as a relatively small number of record labels embraced the format. The initial high cost of equipment and blank media was also a factor. Stationary MiniDisc player/recorders never got into the lower price ranges, and most consumers had to connect the portable player to the hi-fi in order to record. This inconvenience contrasted with the earlier common use of cassette decks as a standard part of an ordinary hi-fi set-up.
MiniDisc technology was faced with new competition (CD Consortium) from the recordable compact disc (CD-R) when it became more affordable to consumers in 1996. Initially, Sony believed that it would take a decade for CD-R prices to become affordable (starting at about $12 per blank CD-R disk in 1994). But the prices fell very quickly, to the point where CD-R blanks sank below $1.00 by the late 1990s, compared to around $2.00 for similar 80-minute MiniDisc blanks.
The biggest competition for MiniDisc came from the emergence of MP3 players. With the Diamond Rio player in 1998, the mass market began to eschew physical media in favor of file-based systems.
By 2007, because of the waning popularity of the format and the increasing popularity of solid-state MP3 players, Sony was producing only one model, the Hi-MD MZ-RH1 (also available as the MZ-M200 in North America packaged with a Sony microphone and limited Macintosh software support).
The introduction of the MZ-RH1 allowed users to freely move uncompressed digital recordings back and forth from the MiniDisc to a computer without the copyright protection limitations previously imposed upon the NetMD series. This allowed the MiniDisc to better compete with HD recorders and MP3 players. However, even pro users like broadcasters and news reporters had already abandoned MiniDisc in favor of solid-state recorders, due to their long recording times, open digital content sharing, high-quality digital recording capabilities and reliable, lightweight design.
On 7 July 2011, Sony announced that it would no longer ship MiniDisc Walkman products as of September 2011, effectively killing the format.
On 1 February 2013, Sony issued a press release on the Nikkei stock exchange that it will cease shipment of MD devices, with last of the players to be sold in March 2013. However, they will continue to sell media and offer repair services.
MD Data.
MD Data, a version for storing computer data, was announced by Sony in 1993 but never gained significant ground. Its media were incompatible with standard audio MiniDiscs, which has been cited as one of the main reasons behind the format's failure.
MD Data could not write to audio-MDs, only the considerably more expensive data blanks. In 1997, MD-Data2 blanks were introduced, which held 650 MB of data. They were only implemented in Sony's short-lived MD-based camcorder (the DCM-M1) as well as a small number of MultiTrack Recorders; Sony's MDM-X4, Tascam's 564 (which could also record using standard MD-Audio discs, albeit only 2 tracks), and Yamaha's MD-8, MD-4, & MD4S.
The Hi-MD format, introduced in 2004, marked a return to the data storage arena with its 1 GB discs and ability to act as a USB drive. Hi-MD units allow the recording and playback of audio and data on the same disc, and are compatible (both audio and data) with standard MiniDisc media. (An 80-minute Minidisc blank can be formatted to store 305MB of data)
Design.
Physical characteristics.
The disc is permanently housed in a cartridge (68×72×5 mm) with a sliding door, similar to the casing of a 3.5" floppy disk. This shutter is opened automatically by a mechanism upon insertion. The audio discs can either be recordable (blank) or premastered. Recordable MiniDiscs use a magneto-optical system to record data. A laser heats one side of the disc to its Curie point, making the material in the disc susceptible to a magnetic field. A magnetic head on the other side of the disc alters the polarity of the heated area, recording the digital data onto the disk. Playback is accomplished with the laser alone: taking advantage of the Faraday effect, the player senses the polarisation of the reflected light and thus interprets a 1 or a 0. Recordable MDs can be recorded on repeatedly; Sony claims up to one million times. As of May 2005, there were 74 minute and 80 minute discs available. 60 minute blanks, which were widely available in the early years of the format's introduction, were phased out long before and are rarely seen.
MiniDiscs use a mastering process and optical playback system that is very similar to CDs. The recorded signal of the premastered pits and of the recordable MD are also very similar. Eight-to-Fourteen Modulation (EFM) and a modification of CD's CIRC code, called Advanced Cross Interleaved Reed-Solomon Code (ACIRC) are employed.
Differences from cassette and CDs.
MiniDiscs use rewritable magneto-optical storage to store the data. Unlike the Digital Compact Cassette, or the analog compact audio cassette, the disc is a random-access medium, making seek time very fast. MiniDiscs can be edited very quickly even on portable machines. Tracks can be split, combined, moved or deleted with ease either on the player or uploaded to PC (only with the latest version of Sony's PC based SonicStage V4.3 software) and edited there. Transferring data from an MD unit to a non-Windows machine can only be done in real time, preferably via optical I/O, by connecting the audio out port of the MD to an available audio in port of the computer. With the release of the Hi-MD format, Sony began to release Macintosh compatible software. However, the Mac compatible software is still not compatible with legacy MD formats (SP, LP2, LP4). This means that using an MD recorded on a legacy unit or in a legacy format still requires a Windows machine for non-real time transfers.
At the beginning of the disc there is a table of contents (TOC, also known as "System File" area of the disc), which stores the start positions of the various tracks, as well as meta information (Title, Artist) about them and free blocks. Unlike the conventional cassette, a recorded song does not need to be stored as one piece on the disk, it can be stored in several fragments, similar to a hard drive. Early MiniDisc equipment had a fragment granularity of 4 seconds audio. Fragments smaller than the granularity are not kept track of, which may lead to the usable capacity of a disc actually shrinking. Also, no means of defragmenting the disc are provided in consumer grade equipment.
All "consumer-grade" MiniDisc devices feature a copy-protection scheme known as Serial Copy Management System. An unprotected disc or song can be copied without limit, but the copies can no longer be digitally copied. However, as a concession to this the most recent Hi-MD players can upload to PC a Digitally Recorded file which can subsequently be resaved as a WAV (PCM) file and thus replicated.
Audio data compression.
The digitally encoded audio signal on a MiniDisc has traditionally been data-compressed using the ATRAC format (Adaptive TRansform Acoustic Coding). This is in fact a 'psychoacoustic' data reduction system. It omits some of the musical content. It is claimed by Sony that the content that is omitted is inaudible anyway. Some original sounds have been known to defeat ATRAC which introduces (typically) a crackle or whistle onto the data stream.
ATRAC was devised for MiniDisc so that the same amount of audio a CD can carry can fit on a disc far smaller than the CD (which contains uncompressed 16-bit stereo linear PCM audio). ATRAC reduces the 1.4 Mbit/s of a CD to a 292 kbit/s data stream, roughly a 5:1 reduction.
ATRAC was also used on nearly all FLASH memory Walkman devices until the 8 series but is now only used in Sony's MiniDisc devices (as of November 2008) as ATRAC is fundamental to the MiniDisc specification.
Sony's ATRAC codec differs from uncompressed PCM in that it is a psychoacoustic lossy audio data reduction scheme and is such that the recorded signal does not require decompression on replay. Although it is intended that the reproduced signal may sound nearly identical to the original as far as the listener is concerned, it differs sufficiently that listening on a high quality audio system will betray the difference (other true compression schemes generally share this characteristic to a greater or lesser degree).
There have been four versions of the ATRAC data reduction system, each claimed (by Sony) to more accurately reflect the original audio. Early version players are guaranteed to play later version ATRAC audio because, as stated, there is no processing required for replay. Version 1 could only be copied on consumer equipment three or four times before artifacts became objectionable (the ATRAC on the recording machine attempts to data reduce the already reduced signal). By version 4, the potential number of generations of copy had increased to around fifteen to twenty depending on audio content.
The latest versions of Sony's ATRAC are ATRAC3 and ATRAC3plus, both of which are true lossy compression schemes and both require decompression on replay. Original ATRAC3 at 132 kbit/s (also known as ATRAC-LP2 mode) is the format that used to be used by Sony's Connect audio download store (now defunct). ATRAC3plus was not used in order to retain backwards compatibility with earlier NetMD players.
In MiniDisc's latest progression, Hi-MD, uncompressed CD-quality linear PCM audio recording and playback is offered, placing Hi-MD on a par with CD-quality audio. Hi-MD also supports both ATRAC3 and ATRAC3plus in varying bitrates, but not the original ATRAC.
Anti-skip.
MiniDisc has a feature that prevents disc skipping under all but the most extreme conditions. Older CD players had once been a source of annoyance to users as they were prone to mistracking from vibration and shock. MiniDisc solved this problem by reading the data into a memory buffer at a higher speed than was required before being read out to the digital-to-analog converter at the standard rate required by the format. The size of the buffer varies by model.
If the MiniDisc player were bumped, playback could continue unimpeded while the laser repositioned itself to continue reading data from the disc. This feature allows the player to stop the spindle motor for long periods, increasing battery life. The memory buffer concept introduced by MiniDisc was soon incorporated into portable CD players as well, and in hard drive based digital audio players.
A buffer of at least six seconds is required on all MiniDisc players, be they portable or stationary full-sized units. This is needed to ensure uninterrupted playback in the presence of fragmentation.
Operation.
The data structure and operation of a MiniDisc is similar to that of a computer's hard disk drive. The bulk of the disc contains data pertaining to the music itself, and a small section contains the Table of Contents (TOC), providing the playback device with vital information about the number and location of tracks on the disc. Tracks and discs can be named. Tracks may easily be added, erased, combined and divided, and their preferred order of playback modified. Erased tracks are not actually erased at the time, but are marked so. When a disc becomes full, the recorder can simply slot track data into sections where erased tracks reside. This can lead to some fragmentation but unless many erasures and replacements are performed, the only likely problem is excessive searching, reducing battery life.
The data structure of the MiniDisc, where music is recorded in a single stream of bytes while the TOC contains pointers to track positions, allows for gapless playback of music, something which the majority of competing portable players, including most MP3 players, fail to implement properly. (Notable exceptions are CD players, as well as all recent iPods.)
At the end of recording, after the "Stop" button has been pressed, the MiniDisc may continue to write music data for a few seconds from its memory buffers. During this time, it may display a message ("Data Save", on at least some models) and the case will not open. After the audio data is written out, the final step is to write the TOC track denoting the start and endpoints of the recorded data. Sony notes in the manual that one should not interrupt the power or expose the unit to undue physical shock during this period.
Copy protection.
All MiniDisc-recorders used the SCMS copy protection system which uses two bits in the S/PDIF digital audio stream and on disc to differentiate between "protected" vs. "unprotected" audio, and between "original" vs. "copy":
Recording from an analogue source resulted in a disc marked "protected" and "original" allowing one further copy to be made (this contrasts with the SCMS on the Digital Compact Cassette where analogue recording was marked as "unprotected").
Of those recorder/players that could be connected to a PC via a USB lead, although it was possible to transfer audio from the PC to the MiniDisc recorder, for many years it was not possible to transfer audio the other way. This restriction existed in both the SonicStage software and in the MiniDisc player itself. SonicStage V3.4 was the first version of the software where this restriction was removed, but it still required a MiniDisc recorder/player that also had the restriction removed. The Hi-MD model MZ-RH1, was the first such player available.
Format extensions.
MDLP.
In 2000, Sony announced MDLP (MiniDisc Long Play), which added new recording modes based on a new codec called ATRAC3. In addition to the standard, high-quality mode, now called SP, MDLP adds LP2 mode, which allows twice as much recording time (160 minutes on an 80-minute disc) of good-quality stereo sound, and LP4, which allows four times more recording time (320 minutes on an 80-minute disc) of medium-quality stereo sound.
The bitrate of the standard SP mode is 292 kbit/s, and it uses separate stereo coding with discrete left and right channels. LP2 mode uses a bitrate of 132 kbit/s and also uses separate stereo coding. The last mode, LP4 has a bitrate of 66 kbit/s and uses joint stereo coding. The sound quality is noticeably poorer than the first two modes, but is sufficient for many uses.
Tracks recorded in LP2 or LP4 mode play back as silence on non-MDLP players.
NetMD.
NetMD recorders allow music files to be transferred from a computer to a recorder (but not in the other direction) over a USB connection. In LP4 mode, speeds of up to 32× real-time are possible and three Sony NetMD recorders (MZ-N10, MZ-N910, and MZ-N920) are capable of speeds up to 64× real-time. NetMD recorders all support MDLP.
NetMD is a proprietary protocol, and it is currently impossible to use it without proprietary software, such as SonicStage. Thus, it cannot be used under non-Windows machines. A free *nix based implementation, libnetmd, is being developed, but it cannot be used to upload music ().
Hi-MD.
Hi-MD is the further development of the MiniDisc-format. It was introduced in 2004. Hi-MD media will not play on non Hi-MD equipment, including NetMD players.
Recording modes.
Modes marked in green are available for recordings made on the player, while those marked in red are only available for music downloaded from a PC. Capacities are official Sony figures; real world figures are usually slightly higher. Second generation Hi-MD players also support MP3 compression natively, in a multitude of bitrates. Recently, 352 kbit/s and 192 kbit/s "ATRAC3plus" have also been made available for 1st and 2nd generation Hi-MDs.

</doc>
<doc id="44012" url="https://en.wikipedia.org/wiki?curid=44012" title="Anton Bruckner">
Anton Bruckner

Anton Bruckner (; ) was an Austrian composer known for his symphonies, masses, and motets. The first are considered emblematic of the final stage of Austro-German Romanticism because of their rich harmonic language, strongly polyphonic character, and considerable length. Bruckner's compositions helped to define contemporary musical radicalism, owing to their dissonances, unprepared modulations, and roving harmonies.
Unlike other musical radicals such as Richard Wagner and Hugo Wolf who fit the "enfant terrible" mould, Bruckner showed extreme humility before other musicians, Wagner in particular. This apparent dichotomy between Bruckner the man and Bruckner the composer hampers efforts to describe his life in a way that gives a straightforward context for his music.
His works, the symphonies in particular, had detractors, most notably the influential Austrian critic Eduard Hanslick, and other supporters of Johannes Brahms who pointed to their large size and use of repetition, as well as to Bruckner's propensity for revising many of his works, often with the assistance of colleagues, and his apparent indecision about which versions he preferred. On the other hand, Bruckner was greatly admired by subsequent composers including his friend Gustav Mahler, who described him as "half simpleton, half God".
Biography.
Early life.
Anton Bruckner was born in Ansfelden (then a village, now a suburb of Linz) on 4 September 1824. The ancestors of Bruckner's family were farmers and craftsmen; their history can be traced to as far back as the 16th century. They lived near a bridge south of Sindelburg, which led to their being called "Pruckhner an der Pruckhen" (bridgers on the bridge). Bruckner's grandfather was appointed schoolmaster in Ansfelden in 1776; this position was inherited by Bruckner's father, Anton Bruckner senior, in 1823. It was a poorly paid but well-respected position in the rural environment. Music was a part of the school curriculum, and Bruckner's father was his first music teacher. Bruckner learned to play the organ early as a child. He entered school when he was six, proved to be a hard-working student, and was promoted to upper class early. While studying, Bruckner also helped his father in teaching the other children. After Bruckner received his confirmation in 1833, Bruckner's father sent him to another school in Hörsching. The schoolmaster, Johann Baptist Weiß, was a music enthusiast and respected organist. Here, Bruckner completed his school education and learned to play the organ excellently. Around 1835 Bruckner wrote his first composition, a "Pange lingua" – one of the compositions which he revised at the end of his life. When his father became ill, Anton returned to Ansfelden to help him in his work.
Teacher's education.
Bruckner's father died in 1837, when Bruckner was 13 years old. The teacher's position and house were given to a successor, and Bruckner was sent to the Augustinian monastery in Sankt Florian to become a choirboy. In addition to choir practice, his education included violin and organ lessons. Bruckner was in awe of the monastery's great organ, which was built during the late baroque era and rebuilt in 1837, and he sometimes played it during church services. Later, the organ was to be called the "Bruckner Organ". Despite his musical abilities, Bruckner's mother sent her son to a teaching seminar in Linz in 1841. After completing the seminar with an excellent grade, he was sent as a teacher's assistant to a school in Windhaag. The living standards and pay were horrible, and Bruckner was constantly humiliated by his superior, teacher Franz Fuchs. Despite the difficult situation, Bruckner never complained or rebelled; a belief of inferiority was to remain one of Bruckner's main personal characteristics during his whole life. He would stay at Windhaag from age 17–19, teaching subjects that did not have to do with music. Prelate Michael Arneth noticed Bruckner's bad situation in Windhaag and awarded him a teacher's assistant position in the vicinity of the monastic town of Sankt Florian, sending him to Kronstorf an der Enns for two years. Here he would be able to have more of a part in musical activity. The time in Kronstorf was a much happier one for Bruckner. Compared to the few works he wrote in Windhaag, the Kronstorf compositions from 1843–1845 show a significantly improved artistic ability, and finally the beginnings of what could be called "the Bruckner style". Among the Kronstorf works is the vocal piece "Asperges" (WAB 4), which the young teacher's assistant, out of line of his position, signed with "Anton Bruckner m.p.ria. Comp". This has been interpreted as a lone early sign of Bruckner's artistic ambitions. Otherwise, little is known of Bruckner's life plans and intentions.
Organist in Sankt Florian.
After the Kronstorf period, Bruckner returned to Sankt Florian in 1845, where, for the next 10 years, he would work as a teacher and an organist. In May 1845, Bruckner passed an examination, which allowed him to begin work as an assistant teacher in one of the village schools of Sankt Florian. He continued to improve his education by taking further courses, passing an examination giving him the permission to also teach in higher education institutes, receiving the grade "very good" in all disciplines. In 1848 he was appointed an organist in Sankt Florian and in 1851 this was made a regular position. In Sankt Florian, most of the repertoire consisted of the music of Michael Haydn, Johann Georg Albrechtsberger and Franz Joseph Aumann.
Study period.
In 1855, Bruckner, aspiring to become a student of the famous Vienna music theorist Simon Sechter, showed the master his "Missa solemnis" (WAB 29), written a year earlier, and was accepted. The education, which included skills in music theory and counterpoint among others, took place mostly via correspondence, but also included long in-person sessions in Vienna. Sechter's teaching would have a profound influence on Bruckner. Later, when Bruckner began teaching music himself, he would base his curriculum on Sechter's book "Die Grundsätze der musikalischen Komposition" (Leipzig 1853/54). In 1861, Bruckner studied further with Otto Kitzler, who was nine years younger than him and who introduced him to the music of Richard Wagner, which Bruckner studied extensively from 1863 onwards. Bruckner considered the earliest orchestral works (the "study" Symphony in F minor, the three orchestral pieces, the March in D minor and the Overture in G minor, which he composed in 1862-1863), mere school exercises, done under the supervision of Otto Kitzler. He continued his studies to the age of 40. Broad fame and acceptance did not come until he was over 60 (after the premiere of his Seventh Symphony in 1884). In 1861 he had already made the acquaintance of Franz Liszt who, like Bruckner, had a strong, Catholic religious faith and who first and foremost was a harmonic innovator, initiating the new German school together with Wagner. In May 1861 he made his concert debut, as both composer and conductor of his "Ave Maria", set in seven parts. Soon after Bruckner had ended his studies under Sechter and Kitzler, he wrote his first mature work, the Mass in D Minor. From 1861 to 1868, he alternated his time between Vienna and Sankt Florian. He wished to ensure he knew how to make his music modern, but he also wanted to spend time in a more religious setting.
The Vienna period.
In 1868, after Sechter had died, Bruckner hesitantly accepted Sechter's post as a teacher of music theory at the Vienna Conservatory, during which time he concentrated most of his energy on writing symphonies. These symphonies, however, were poorly received, at times considered "wild" and "nonsensical". His students at the Conservatory included Richard Robert.
He later accepted a post at the Vienna University in 1875, where he tried to make music theory a part of the curriculum. Overall, he was unhappy in Vienna, which was musically dominated by the critic Eduard Hanslick. At the time, there was a feud between advocates of the music of Wagner and Brahms; by aligning himself with Wagner, Bruckner made an unintentional enemy out of Hanslick. He was not without supporters, though. "Deutsche Zeitung"'s music critic Theodor Helm, and famous conductors such as Arthur Nikisch and Franz Schalk constantly tried to bring his music to the public, and for this purpose proposed "improvements" for making Bruckner's music more acceptable to the public. While Bruckner allowed these changes, he also made sure in his will to bequeath his original scores to the Vienna National Library, confident of their musical validity.
In addition to his symphonies, Bruckner wrote masses, motets and other sacred choral works, and a few chamber works, including a string quintet. Unlike his romantic symphonies, some of Bruckner's choral works are often conservative and contrapuntal in style; however, the Te Deum, Helgoland, Psalm 150 and at least one Mass demonstrate innovative and radical uses of chromaticism.
Biographers generally characterize Bruckner as a "simple" provincial man, and many biographers have complained that there is huge discrepancy between Bruckner's life and his work. For example, Karl Grebe said: "his life doesn't tell anything about his work, and his work doesn't tell anything about his life, that's the uncomfortable fact any biography must start from." Anecdotes abound as to Bruckner's dogged pursuit of his chosen craft and his humble acceptance of the fame that eventually came his way. Once, after a rehearsal of his Fourth Symphony in 1881, the well-meaning Bruckner tipped the conductor Hans Richter: "When the symphony was over," Richter related, "Bruckner came to me, his face beaming with enthusiasm and joy. I felt him press a coin into my hand. 'Take this' he said, 'and drink a glass of beer to my health.'" Richter, of course, accepted the coin, a Maria Theresa thaler, and wore it on his watch-chain ever after.
Bruckner was a renowned organist in his day, impressing audiences in France in 1869, and England in 1871, giving six recitals on a new Henry Willis organ at Royal Albert Hall in London and five more at the Crystal Palace. Though he wrote no major works for the organ, his improvisation sessions sometimes yielded ideas for the symphonies. He taught organ performance at the Conservatory; among his students were Hans Rott and Franz Schmidt. Gustav Mahler, who called Bruckner his "forerunner", attended the conservatory at this time.
Bruckner was a lifelong bachelor who made numerous unsuccessful marriage proposals to teenage girls. One such was the daughter of a friend, called Louise; in his grief he is believed to have written the cantata "Entsagen" (Renunciation). His affection for teenage girls led to an accusation of impropriety where he taught music, and while he was exonerated, he decided to concentrate on teaching boys afterwards. His calendar for 1874 details the names of girls who appealed to him, and the list of such girls in all his diaries was very long. In 1880 he fell for a 17-year-old peasant girl in the cast of the Oberammergau Passion Play. His interest in young girls seems to have been motivated by his fear of sin; he believed that (unlike older women) he could be certain that he was marrying a virgin. His unsuccessful proposals to teenagers continued when he was past his 70th birthday; one prospect, Berlin hotel chambermaid Ida Buhz, came near to marrying him but broke off the engagement when she refused to convert to Catholicism. He suffered from periodic attacks of depression, with his numerous failed attempts to find a female companion only adding to his unhappiness.
In July 1886, the Emperor decorated him with the Order of Franz Joseph. He most likely retired from his position at the University of Vienna in 1892, at the age of 68. He wrote a great deal of music that he used to help teach his students.
Bruckner died in Vienna in 1896 at the age of 72. He is buried in the crypt of the monastery church at Sankt Florian, immediately below his favorite organ. He had always had a morbid fascination with death and dead bodies, and left explicit instructions regarding the embalming of his corpse.
The Anton Bruckner Private University for Music, Drama, and Dance, an institution of higher education in Linz, close to his native Ansfelden, was named after him in 1932 ("Bruckner Conservatory Linz" until 2004). The Bruckner Orchester Linz was also named in his honor.
Compositions.
Sometimes Bruckner's works are referred to by WAB numbers, from the "Werkverzeichnis Anton Bruckner", a catalogue of Bruckner's works edited by Renate Grasberger.
The revision issue has generated controversy. A common explanation for the multiple versions is that Bruckner was willing to revise his work on the basis of harsh, uninformed criticism from his colleagues. "The result of such advice was to awaken immediately all the insecurity in the non-musical part of Bruckner's personality," musicologist Deryck Cooke writes. "Lacking all self-assurance in such matters, he felt obliged to bow to the opinions of his friends, 'the experts,' to permit ... revisions and even to help make them in some cases." This explanation was widely accepted when it was championed by Bruckner scholar Robert Haas, who was the chief editor of the first critical editions of Bruckner's works published by the International Bruckner Society; it continues to be found in the majority of program notes and biographical sketches concerning Bruckner. Haas's work was endorsed by the Nazis and so fell out of favour after the war as the Allies enforced denazification.
Haas's rival Leopold Nowak was appointed to produce a whole new critical edition of Bruckner's works. He and others such as Benjamin Korstvedt and conductor Leon Botstein argued that Haas's explanation is at best idle speculation, at worst a shady justification of Haas's own editorial decisions. Also, it has been pointed out that Bruckner often started work on a symphony just days after finishing the one before. As Cooke writes, "In spite of continued opposition and criticism, and many well-meaning exhortations to caution from his friends, he looked neither to right nor left, but simply got down to work on the next symphony." The matter of Bruckner's authentic texts and the reasons for his changes to them remains politicised and uncomfortable.
Symphonies.
"Bruckner expanded the concept of the symphonic form in ways that have never been witnessed before or since. … When listening to a Bruckner symphony, one encounters some of the most complex symphonic writing ever created. As scholars study Bruckner's scores they continue to revel in the complexity of Bruckner's creative logic."
Bruckner composed eleven symphonies, the first, the Study Symphony in F minor in 1863, the last, the unfinished Symphony No. 9 in D minor in 1893-1896. With the exception of Symphony No. 4 ("Romantic"), none of Bruckner's symphonies have subtitles, and most of their nicknames did not originate with the composer.
Style.
Bruckner's symphonies are scored for a fairly standard orchestra of woodwinds in pairs, four horns, two or three trumpets, three trombones, tuba (from the second version of the Fourth), timpani and strings. The later symphonies increase this complement, but not by much. Notable is the use of Wagner tubas in his last three symphonies. Only the Eighth has harp, and percussion besides timpani (though legend has it the Seventh is supposed to have a cymbal crash at the exact moment Wagner died). Bruckner's style of orchestral writing was criticized by his Viennese contemporaries, but by the middle of the twentieth century, musicologists recognized that his orchestration was modeled after the sound of his primary instrument, the pipe organ, "i.e.", alternating between two groups of instruments, as when changing from one manual of the organ to another.
Structure.
The structure of Bruckner's symphonies is in a way an extension of that of Beethoven's symphonies. Bruckner's symphonies are in four movements. 
Nicholas Temperley writes in the "New Grove Dictionary of Music and Musicians (1980)" that Bruckner
alone succeeded in creating a new school of symphonic writing... Some have classified him as a conservative, some as a radical. Really he was neither, or alternatively was a fusion of both... is music, though Wagnerian in its orchestration and in its huge rising and falling periods, patently has its roots in older styles. Bruckner took Beethoven's Ninth Symphony as his starting-point... The introduction to the first movement, beginning mysteriously and climbing slowly with fragments of the first theme to the gigantic full statement of that theme, was taken over by Bruckner; so was the awe-inspiring coda of the first movement. The scherzo and slow movement, with their alternation of melodies, are models for Bruckner's spacious middle movements, while the finale with a grand culminating hymn is a feature of almost every Bruckner symphony.
Bruckner is the first composer since Schubert about whom it is possible to make such generalizations. His symphonies deliberately followed a pattern, each one building on the achievements of its predecessors... His melodic and harmonic style changed little, and it had as much of Schubert in it as of Wagner... His technique in the development and transformation of themes, learnt from Beethoven, Liszt and Wagner, was unsurpassed, and he was almost the equal of Brahms in the art of melodic variation.
Cooke adds, also in the "New Grove",
Despite its general debt to Beethoven and Wagner, the "Bruckner Symphony" is a unique conception, not only because of the individuality of its spirit and its materials, but even more because of the absolute originality of its formal processes. At first, these processes seemed so strange and unprecedented that they were taken as evidence of sheer incompetence... Now it is recognized that Bruckner's unorthodox structural methods were inevitable... Bruckner created a new and monumental type of symphonic organism, which abjured the tense, dynamic continuity of Beethoven, and the broad, fluid continuity of Wagner, in order to express something profoundly different from either composer, something elemental and metaphysical.
In a concert review, Bernard Holland described parts of the first movements of Bruckner's sixth and seventh symphonies as follows: "There is the same slow, broad introduction, the drawn-out climaxes that grow, pull back and then grow some more – a sort of musical coitus interruptus."
In the 2001 Second Edition of the "New Grove", Mark Evan Bonds called the Bruckner symphonies "monumental in scope and design, combining lyricism with an inherently polyphonic design... Bruckner favored an approach to large-scale form that relied more on large-scale thematic and harmonic juxtaposition. Over the course of his output, one senses an ever-increasing interest in cyclic integration that culminates in his masterpiece, the Symphony No. 8 in C minor, a work whose final page integrates the main themes of all four movements simultaneously."
The Bruckner Problem.
"The Bruckner Problem" is a term that refers to the difficulties and complications resulting from the numerous contrasting versions and editions that exist for most of the symphonies. The term gained currency following the publication (in 1969) of an article dealing with the subject, "The Bruckner Problem Simplified," by musicologist Deryck Cooke, which brought the issue to the attention of English-speaking musicians.
The first versions of the Bruckner's symphonies often presented an instrumental, contrapuntal and rhythmic complexity (Brucknerian rhythm "2 + 3", use of quintolets), the originality of which has not been understood and considered unperformable by the musicians. In order to make them "performable", the symphonies, except Symphonies No. 6 and No. 7, have been revised several times. Consequently, there are several versions and editions, mainly of Symphonies 3, 4 and 8, which have been deeply emended by Bruckner's friends and associates, and it is not always possible to tell whether the emendations had Bruckner's direct authorization.
Looking for authentic versions of the symphonies, Robert Haas produced during the 1930s a first critical edition of Bruckner's works based on the original scores. After World War II other scholars (Leopold Nowak, William Carragan, Benjamin-Gunnar Cohrs "et al.") carried on with this work.
Sacred choral works.
Bruckner was a devoutly religious man, and composed numerous sacred works. He wrote a Te Deum, five psalm settings (including Psalm 150 in the 1890s), a Festive cantata, a Magnificat, about forty motets (among them eight settings of "Tantum ergo", and three settings of both "Christus factus est" and "Ave Maria"), and at least seven Masses.
The three early masses ("Windhaager Messe", "Kronstorfer Messe" and "Messe für den Gründonnerstag"), composed between 1842 and 1844, were short Austrian "Landmessen" for use in local churches and did not always set all the numbers of the ordinary. His Requiem in D minor of 1849 is the earliest work Bruckner himself considered worthy of preservation. It shows the clear influence of Mozart's Requiem (also in D minor) and similar works of Michael Haydn. The seldom performed "Missa solemnis", composed in 1854 for Friedrich Mayer's elevation, was the last major work Bruckner composed before he started to study with Simon Sechter, with the possible exception of Psalm 146, a large work, for SATB soloists, double choir and orchestra.
The three Masses, which Bruckner wrote in the 1860s and revised later on in his life, are more often performed. The Masses numbered 1 in D minor and 3 in F minor are for solo singers, mixed choir, organ "ad libitum" and orchestra, while No. 2 in E minor is for mixed choir and a small group of wind instruments, and was written in an attempt to meet the Cecilians halfway. The Cecilians wanted to rid church music of instruments entirely. No. 3 was clearly meant for concert, rather than liturgical performance, and it is the only one of his Masses in which he set the first line of the Gloria, "Gloria in excelsis Deo", and of the Credo, "Credo in unum Deum", to music. In concert performances of the other Masses, these lines are intoned by a tenor soloist in the way a priest would, with a line of plainsong.
Secular vocal works.
As a young man Bruckner sang in men's choirs and wrote music for them. Bruckner's secular choral music was mostly written for choral societies. The texts are always in German. Some of these works were written specifically for private occasions such as weddings, funerals, birthdays or name-days, many of these being dedicated to friends and acquaintances of the composer. This music is rarely performed. Biographer Derek Watson characterizes the pieces for men's choir as being "of little concern to the non-German listener". Of about 30 such pieces, a most unusual and evocative composition is the song "Abendzauber" (1878) for men's choir, man soloist, yodelers and four horns.
Bruckner composed also 20 lieder, of which only a few have been issued. The lieder, which Bruckner composed in 1861-1862 during his tuition by Otto Kitzler, have not been edited or WAB classified. The last-known owner of this important source (Ms. Kress, Munich) deceased, apparently without descendants. In 2013 the Austrian National Library was able to acquire a facsimile of the autograph manuscript hitherto unavailable to the public. The facsimile is edited by Paul Hawkshaw and Erich Wolfgang Partsch in Band XXV of Bruckner's "".
Bruckner composed also five name-day cantatas, as well as two patriotic cantatas, "Germanenzug" and "Helgoland", on texts by August Silberstein. "Germanenzug" (WAB 70), composed in 1863–1864, was Bruckner's first published work. "Helgoland" (WAB 71), for SATB men's choir and large orchestra, was composed in 1893 and was Bruckner's last completed composition and the only secular vocal work, which he thought worthy enough to bequeath to the Vienna National Library.
Other works.
During his apprenticeship with Otto Kitzler, Bruckner composed three short orchestral pieces and a March in D minor as orchestration exercises. At that time he also wrote an Overture in G minor. These works, which are occasionally included in recordings of the symphonies, show already hints of Bruckner's emerging style.
A String Quartet in C minor and the additional Rondo in C minor, also composed in 1862, were discovered decades after Bruckner's death. The later String Quintet in F Major of 1879, contemporaneous with the Fifth and Sixth symphonies, has been frequently performed. The Intermezzo in D minor, which was intended to replace its scherzo, is not frequently performed.
A "Symphonisches Präludium" (Symphonic Prelude) in C minor was discovered by Mahler scholar Paul Banks in the Vienna National Library in 1974 in a piano duet transcription. Banks ascribed it to Gustav Mahler, and let orchestrate it by Albrecht Gürsching. In 1985 Wolfgang Hiltl, who had retrieved the original score by Rudolf Krzyzanowski, let publish it by Doblinger (issued in 2002). According to scholar Benjamin-Gunnar Cohrs, the stylistic examination of this "prelude" shows that it is all Bruckner's. Possibly Bruckner had given a draft-score to his pupil Krzyzanowski, which already contained the string parts and some important lines for woodwind and brass, as an exercise in instrumentation.
Bruckner's Two Aequali of 1847 for three trombones are solemn, brief works. The Military march of 1865 is an occasional work as a gesture of appreciation for the "Militär-Kapelle der Jäger-Truppe" of Linz. "Abendklänge" of 1866 is a short character piece for violin and piano.
Bruckner also wrote a Lancer-Quadrille () and a few other small works for piano. Most of this music was written for teaching purposes. Sixteen other pieces for piano, which Bruckner composed in 1862 during his tuition by Kitzler, have not been edited or WAB classified.
Bruckner was a renowned organist at the St Florian's Priory, where he improvised frequently. Those improvisations were usually not transcribed, so that only a few of his work for organ has survived. The five Preludes in E-flat major (1836–1837), classified WAB 127 and WAB 128, as well as a few other WAB-unclassified works, which have been found in Bruckner's "Präludienbuch", are probably not by Bruckner.
Bruckner never wrote an opera, and as much as he was a fan of Wagner's music dramas, he was uninterested in drama. In 1893 he thought about writing an opera called "Astra" based on a novel by Gertrud Bollé-Hellmund.
Although he attended performances of Wagner's operas, he was much more interested in the music than the plot. After seeing Wagner's "Götterdämmerung", he asked: "Tell me, why did they burn the woman at the end?" Nor did Bruckner ever write an oratorio.
Bruckner Gesamtausgabe.
Published by Musikwissenschaftlicher Verlag in Vienna, the "" (Bruckner's Critical Complete Edition) comprises three successive editions.
Reception in the 20th century.
Because of the long duration and vast orchestral canvas of much of his music, Bruckner's popularity has greatly benefited from the introduction of long-playing media and from improvements in recording technology.
Decades after his death, the Nazis strongly approved of Bruckner's music because they saw it as expressing the zeitgeist of the German volk, and Hitler even consecrated a bust of Bruckner in a widely photographed ceremony in 1937 at Regensburg's Walhalla temple. Bruckner's music was among the most popular in Nazi Germany and the Adagio from his Seventh Symphony was broadcast by German radio (Deutscher Reichsrundfunk) when it broadcast the news of Hitler's death on 1 May 1945. However, this did not hurt Bruckner's standing in the postwar media, and several movies and TV productions in Europe and the United States have used excerpts from his music ever since the 1950s, as they already did in the 1930s. Nor did the Israel Philharmonic Orchestra ever ban Bruckner's music as they have Wagner's, even recording the Eighth Symphony with Zubin Mehta.
Bruckner's symphonic works, much maligned in Vienna in his lifetime, now have an important place in the tradition and musical repertoire of the Vienna Philharmonic Orchestra.
The life of Bruckner was portrayed in Jan Schmidt-Garre's 1995 film "Bruckner's Decision", which focuses on his recovery in the Austrian spa. Ken Russell's TV movie "The Strange Affliction of Anton Bruckner", starring Peter Mackriel, also fictionalizes Bruckner's real-life stay at a sanatorium because of obsessive-compulsive disorder (or 'numeromania' as it was then described).
In addition, "Visconti used the music of Bruckner for his "Senso" (1953), its plot concerned with the Austrian invasion of Italy in the 1860s." The score by Carl Davis for the restoration of the 1925 film "Ben-Hur" takes "inspiration from Bruckner to achieve reverence in biblical scenes."

</doc>
<doc id="44014" url="https://en.wikipedia.org/wiki?curid=44014" title="Henry Hudson">
Henry Hudson

Sir Henry Hudson (died 1611) was an English sea explorer and navigator in the early 17th century.
Hudson made two attempts on behalf of English merchants to find a prospective Northwest Passage to Cathay (today's China) via a route above the Arctic Circle. Hudson explored the region around modern New York metropolitan area while looking for a western route to Asia while in the employment of the Dutch East India Company. He explored the river which eventually was named for him, and laid thereby the foundation for Dutch colonization of the region.
Hudson discovered a strait and immense bay on his final expedition while searching for the Northwest Passage. In 1611, after wintering on the shore of James Bay, Hudson wanted to press on to the west, but most of his crew mutinied. The mutineers cast Hudson, his son and 7 others adrift; the Hudsons, and those cast off at their side, were never seen again.
His name also lives on with the Hudson's Bay Company that explored and worked in the vast Hudson Bay watershed. This large company, and one of the longest lasting, was very successful with fur trading all across North America. Canada acquired a massive portion of land from the purchase of Hudson's Bay Company lands. 
Birth and early life.
Details of Hudson's birth and early life are mostly unknown. Some sources have identified Hudson as having been born in about 1565, but others date his birth to around 1570. Other historians assert even less certainty; Mancall, for instance, states that ' was probably born in the 1560s," while Pennington gives no date at all. Hudson is thought to have spent many years at sea, beginning as a cabin boy and gradually working his way up to ship's captain.
1607 and 1608 voyage.
In 1607, the Muscovy Company of England hired Hudson to find a northerly route to the Pacific coast of Asia. The English were battling the Dutch for northwest routes. It was thought at the time that, because the sun shone for three months in the northern latitudes in the summer, the ice would melt and a ship could make it across the top of the world. 
Hudson sailed on 1 May with a crew of ten men and a boy on the 80-ton "Hopewell". They reached the east coast of Greenland on 14 June, coasting it northward until the 22nd. Here they named a headland "Young's Cape", a "very high mount, like a round castle" near it "Mount of God's Mercy" and land at 73° N "Hold-with-Hope". After turning east, they sighted "Newland" (i.e Spitsbergen) on the 27th, near the mouth of the great bay Hudson later simply named the "Great Indraught" (Isfjorden). On 13 July Hudson and his crew thought they had sailed as far north as 80° 23' N, but more likely only reached 79° 23' N. The following day they entered what Hudson later in the voyage named "Whales Bay" (Krossfjorden and Kongsfjorden), naming its northwestern point "Collins Cape" (Kapp Mitra) after his boatswain, William Collins. They sailed north the following two days. On the 16th they reached as far north as Hakluyt's Headland (which Thomas Edge claims Hudson named on this voyage) at 79° 49' N, thinking they saw the land continue to 82° N (Svalbard's northernmost point is 80° 49' N) when really it trended to the east. Encountering ice packed along the north coast, they were forced to turn back south. Hudson wanted to make his return "by the north of Greenland to Davis his Streights (Davis Strait), and so for Kingdom of England," but ice conditions would have made this impossible. The expedition returned to Tilbury Hope on the Thames on 15 September.
Many authors have wrongly stated that it was the discovery of large numbers of whales in Spitsbergen waters by Hudson during this voyage that led to several nations sending whaling expeditions to the islands. While he did indeed report seeing many whales, it was not his reports that led to the trade, but that by Jonas Poole in 1610 which led to the establishment of English whaling and the voyages of Nicholas Woodcock and Willem Cornelisz. van Muyden in 1612 that led to the establishment of Dutch, French and Spanish whaling.
In 1608, English merchants of the East India and Muscovy Companies again sent Hudson in the "Hopewell" on another attempt at a passage to the Indies, this time to the east around northern Russia. Leaving London on 22 April, the ship traveled almost 2,500 miles, making it to Novaya Zemlya well above the Arctic Circle in July, but even in the summer the ice was impenetrable and they turned back, arriving at Gravesend on 26 August.
Hudson's alleged discovery of Jan Mayen.
According to Thomas Edge, "William Hudson" in 1608 discovered an island he named "Hudson's Tutches" (Touches) at 71°, the latitude of Jan Mayen. However, he only could have come across Jan Mayen in 1607 (if he had made an illogical detour) and made no mention of it in his journal. There is also no cartographical proof of this supposed discovery. Jonas Poole in 1611 and Robert Fotherby in 1615 both had possession of Hudson's journal while searching for his elusive Hold-with-Hope (on the east coast of Greenland), but neither had any knowledge of his (later) alleged discovery of Jan Mayen. The latter actually found Jan Mayen, thinking it a "new" discovery and naming it "Sir Thomas Smith's Island".
1609 voyage.
In 1609 Hudson was chosen by merchants of the Dutch East India Company in the Netherlands to find an easterly passage to Asia. While awaiting orders and supplies in Amsterdam, he heard rumors of a northwest route to the Pacific through North America. Hudson was told to sail through the Arctic Ocean north of Russia, into the Pacific and so to the Far East. Hudson departed Amsterdam on 4 April in command of the Dutch ship "Halve Maen." He could not complete the specified route because ice blocked the passage, as with all previous such voyages, and he turned the ship around in mid-May while somewhere east of Norway's North Cape. At that point, acting entirely outside his instructions, Hudson pointed the ship west and decided to try to seek a westerly passage through North America.
They reached the Grand Banks, south of Newfoundland, on 2 July, and in mid-July made landfall near what is now LaHave, Nova Scotia.
Here they encountered Native Americans who were accustomed to trading with the French; they were willing to trade beaver pelts, but apparently no trades occurred. The ship stayed in the area about ten days, the crew replacing a broken mast and fishing for food. On the 25th a dozen men from the "Halve Maen", using muskets and small cannon, went ashore and assaulted the village near their anchorage. They drove the people from the settlement and took their boat and other property (probably pelts and trade goods).
On 4 August the ship was at Cape Cod, from which Hudson sailed south to the entrance of the Chesapeake Bay. Rather than entering the Chesapeake he explored the coast to the north, finding Delaware Bay but continuing on north. On 3 September he reached the estuary of the river that initially was called the "North River" or "Mauritius" and now carries his name. He was not the first European to discover the estuary, though, as it had been known since the voyage of Giovanni da Verrazzano in 1524. On 6 September 1609 John Colman of his crew was killed by Indians with an arrow to his neck. Hudson sailed into the upper bay on 11 September, and the following day began a journey up what is now known as the Hudson River. Over the next ten days his ship ascended the river, reaching a point about where the present-day capital of Albany is located.
On 23 September, Hudson decided to return to Europe. He put in at Dartmouth, England on 7 November, and was detained by authorities who wanted access to his log. He managed to pass the log to the Dutch ambassador to England, who sent it, along with his report, to Amsterdam.
While exploring the river, Hudson had traded with several native groups, mainly obtaining furs. His voyage was used to establish Dutch claims to the region and to the fur trade that prospered there when a trading post was established at Albany in 1614. New Amsterdam on Manhattan Island became the capital of New Netherland in 1625.
1610–1611 voyage.
In 1610, Hudson managed to get backing for another voyage, this time under the English flag. The funding came from the Virginia Company and the British East India Company. At the helm of his new ship, the "Discovery", he stayed to the north (some claim he deliberately stayed too far south on his Dutch-funded voyage), reaching Iceland on 11 May, the south of Greenland on 4 June, and then rounding the southern tip of Greenland.
Excitement was very high due to the expectation that the ship had finally found the Northwest Passage through the continent. On 25 June, the explorers reached what is now the Hudson Strait at the northern tip of Labrador. Following the southern coast of the strait on 2 August, the ship entered Hudson Bay. Hudson spent the following months mapping and exploring its eastern shores, but he and his crew did not find a passage to Asia. In November, however, the ship became trapped in the ice in the James Bay, and the crew moved ashore for the winter.
Mutiny.
When the ice cleared in the spring of 1611, Hudson planned to use his "Discovery" to further explore Hudson Bay with the continuing goal of discovering the Passage; however, most of the members of his crew ardently desired to return home. Matters came to a head and much of the crew mutinied in June.
Descriptions of the successful mutiny are one-sided, because the only survivors who could tell their story were the mutineers and those who went along with the mutiny. Allegedly in the latter class was ship's navigator Abacuk Pricket, a survivor who kept a journal that was to become a key source for the narrative of the mutiny. According to Pricket, the leaders of the mutiny were Henry Greene and Robert Juet. Pricket's narrative tells how the mutineers set Hudson, his teenage son John, and seven crewmen—men who were either sick and infirm or loyal to Hudson—adrift from the "Discovery" in a small shallop, an open boat, effectively marooning them in Hudson Bay. The Pricket journal reports that the mutineers provided the castaways with clothing, powder and shot, some pikes, an iron pot, some meal, and other miscellaneous items.
After the mutiny, Captain Hudson's shallop broke out oars and tried to keep pace with the "Discovery" for some time. Pricket recalled that the mutineers finally tired of the David-Goliath pursuit and unfurled additional sails aboard the "Discovery", enabling the larger vessel to leave the tiny open boat behind. Hudson and the other seven aboard the shallop were never seen again, and their fate is unknown.
Pricket's journal and testimony have been severely criticized for bias, on two grounds. Firstly, prior to the mutiny the alleged leaders of the uprising, Greene and Juet, had been friends and loyal seamen of Captain Hudson. Secondly, Greene and Juet did not survive the return voyage to England. Pricket knew he and the other survivors of the mutiny would be tried in England for piracy, and it would have been in his interest, and the interest of the other survivors, to put together a narrative that would place the blame for the mutiny upon men who were no longer alive to defend themselves.
In any case, the Pricket narrative became the controlling story of the expedition's disastrous end. Only 8 of the 13 mutinous crewmen survived the return voyage to Europe. They were arrested in England, and some were indeed put on trial, but no punishment was ever imposed for the mutiny. One theory holds that the survivors were considered too valuable as sources of information for it to be wise to execute them, as they had traveled to the New World and could describe sailing routes and conditions. Perhaps for this reason, they were charged with murder—of which they were acquitted—rather than mutiny, of which they most certainly would have been convicted and executed.
Legacy.
The gulf or bay discovered by Hudson is twice the size of the Baltic Sea, and its many large estuaries afford access to otherwise landlocked parts of Western Canada and the Arctic. This allowed the Hudson's Bay Company to exploit a lucrative fur trade along its shores for more than two centuries, growing powerful enough to influence the history and present international boundaries of Western North America. Hudson Strait became the entrance to the Arctic for all ships engaged in the historic search for the Northwest Passage from the Atlantic side (though modern voyages take more northerly routes).
The Hudson River in New York and New Jersey, explored earlier by Hudson, is named after him, as are Hudson County, New Jersey, the Henry Hudson Bridge, the Henry Hudson Parkway, and the town of Hudson, New York.
He, along with his marooned crewmates, appear as mythic characters in the famous story "Rip Van Winkle" by Washington Irving. He also appears in the time-travel novel "Torn" by Margaret Peterson Haddix.

</doc>
<doc id="44017" url="https://en.wikipedia.org/wiki?curid=44017" title="Candle">
Candle

A candle is an ignitable wick embedded in wax or another flammable solid substance such as tallow that provides light, and in some cases, a fragrance. It can also be used to provide heat, or used as a method of keeping time.
A candle manufacturer is traditionally known as a chandler. Various devices have been invented to hold candles, from simple tabletop candle holders to elaborate chandeliers.
For a candle to burn, a heat source (commonly a naked flame) is used to light the candle's wick, which melts and vaporizes a small amount of fuel (the wax). Once vaporized, the fuel combines with oxygen in the atmosphere to ignite and form a constant flame. This flame provides sufficient heat to keep the candle burning via a self-sustaining chain of events: the heat of the flame melts the top of the mass of solid fuel; the liquefied fuel then moves upward through the wick via capillary action; the liquefied fuel finally vaporizes to burn within the candle's flame.
As the mass of solid fuel is melted and consumed, the candle becomes shorter. Portions of the wick that are not emitting vaporized fuel are consumed in the flame. The incineration of the wick limits the exposed length of the wick, thus maintaining a constant burning temperature and rate of fuel consumption. Some wicks require regular trimming with scissors (or a specialized wick trimmer), usually to about one-quarter inch (~0.7 cm), to promote slower, steady burning, and also to prevent smoking. In early times, the wick needed to be trimmed quite frequently. Special candle-scissors, referred to as "snuffers" were produced for this purpose in the 20th century and were often combined with an extinguisher. In modern candles, the wick is constructed so that it curves over as it burns. This ensures that the end of the wick gets oxygen and is then consumed by fire—a self-trimming wick.
Etymology.
The word candle comes from Middle English "candel", from Old English and from Anglo-Norman "candele", both from Latin "candēla", from "candēre", to shine.
History.
The earliest surviving candles originated in China around 200 BC, and were made from whale fat. European candles of antiquity were made from various forms of natural fat, tallow, and wax. In Ancient Rome, candles were made of tallow due to the prohibitive cost of beeswax. It is possible that they also existed in Ancient Greece, but imprecise terminology makes it difficult to determine.
In the Middle Ages in Europe, tallow candles were the most common candle. By the 13th century, candle making had become a guild craft in England and France. The candle makers (chandlers) went from house to house making candles from the kitchen fats saved for that purpose, or made and sold their own candles from small candle shops. Beeswax, compared to animal-based tallow, burned cleanly, without smoky flame. Rather than the foul and terrible odor of tallow, it emits a fresh smell. Beeswax candles were expensive, and relatively few people could afford to burn them in their homes in medieval Europe. However, they were widely used for church ceremonies.
In the 18th century, spermaceti, oil produced by the sperm whale, was used to produce a superior candle. Late in the 18th century, colza oil and rapeseed oil came into use as much cheaper substitutes.
Modern era.
The manufacture of candles became an industrialized mass market in the mid 19th century. In 1834, Joseph Morgan, a pewterer from Manchester, England, patented a machine that revolutionised candle making. It allowed for continuous production of molded candles by using a cylinder with a moveable piston to eject candles as they solidified. This more efficient mechanized production produced about 1,500 candles per hour. This allowed candles to become an easily affordable commodity for the masses. Candlemakers also began to fashion wicks out of tightly braided (rather than simply twisted) strands of cotton. This technique makes wicks curl over as they burn, maintaining the height of the wick and therefore the flame. Because much of the excess wick is incinerated, these are referred to as "self-trimming" or "self-consuming" wicks.
In the mid-1850s, James Young succeeded in distilling paraffin wax from coal and oil shales at Bathgate in West Lothian and developed a commercially viable method of production. Paraffin could be used to make inexpensive candles of high quality. It was a bluish-white wax, burned cleanly, and left no unpleasant odor, unlike tallow candles. By the end of the 19th century, most candles being manufactured consisted of paraffin wax and stearic acid.
By the late 19th century, Price's Candles, based in London, was the largest candle manufacturer in the world. Founded by William Wilson in 1830, the company pioneered the implementation of the technique of steam distillation, and was thus able to manufacture candles from a wide range of raw materials, including skin fat, bone fat, fish oil and industrial greases.
Despite advances in candle making, the candle industry declined rapidly upon the introduction of superior methods of lighting, including kerosene and lamps and the 1879 invention of the incandescent light bulb. From this point on, candles came to be marketed as more of a decorative item.
Use.
Before the invention of electric lighting, candles and oil lamps were commonly used for illumination. In areas without electricity, they are still used routinely. Until the 20th century, candles were more common in northern Europe. In southern Europe and the Mediterranean, oil lamps predominated.
In the developed world today, candles are used mainly for their aesthetic value and scent, particularly to set a soft, warm, or romantic ambiance, for emergency lighting during electrical power failures, and for religious or ritual purposes. 
Other uses.
With the fairly consistent and measurable burning of a candle, a common use of candles was to tell the time. The candle designed for this purpose might have time measurements, usually in hours, marked along the wax. The Song dynasty in China (960–1279) used candle clocks.
By the 18th century, candle clocks were being made with weights set into the sides of the candle. As the candle melted, the weights fell off and made a noise as they fell into a bowl.
In the days leading to Christmas some people burn a candle a set amount to represent each day, as marked on the candle. The type of candle used in this way is called the "Advent candle", although this term is also used to refer to a candle that decorates an Advent wreath.
Components.
Wax.
For most of recorded history candles were tallow (a by-product of beef-fat rendering) and beeswax until the mid 1800s at which point they were made mainly from spermaceti (spurring larger demand for whale oil), and stearin (initially manufactured from animal fats but now produced almost exclusively from palm waxes). Today, most candles are made from paraffin wax, a product of petroleum refining.
Candles can also be made from:
The size of the flame and corresponding rate of burning is controlled largely by the candle wick.
Production methods utilize extrusion moulding. More traditional production methods entail melting the solid fuel by the controlled application of heat. The liquid is then poured into a mould or a wick is repeatedly immersed in the liquid to create a dipped tapered candle. Often fragrance oils, essential oils or aniline-based dye is added.
Wick.
A candle wick works by capillary action, drawing ("wicking") the melted wax or fuel up to the flame. When the liquid fuel reaches the flame, it vaporizes and combusts. The candle wick influences how the candle burns. Important characteristics of the wick include diameter, stiffness, fire-resistance, and tethering.
A candle wick is a piece of string or cord that holds the flame of a candle. Commercial wicks are made from braided cotton. The wick's capillarity determines the rate at which the melted hydrocarbon is conveyed to the flame. If the capillarity is too great, the molten wax streams down the side of the candle. Wicks are often infused with a variety of chemicals to modify their burning characteristics. For example, it is usually desirable that the wick not glow after the flame is extinguished. Typical agents are ammonium nitrate and ammonium sulfate.
Characteristics.
Light.
Based on measurements of a taper-type, paraffin wax candle, a modern candle typically burns at a steady rate of about 0.1 g/min, releasing heat at roughly 80 W. The light produced is about 13 lumens, for a luminous efficacy of about 0.16 lumens per watt (luminous efficacy of a source) – almost a hundred times lower than an incandescent light bulb.
The luminous intensity of a typical candle is approximately one candela. The SI unit, candela, was in fact based on an older unit called the "candlepower", which represented the luminous intensity emitted by a candle made to particular specifications (a "standard candle"). The modern unit is defined in a more precise and repeatable way, but was chosen such that a candle's luminous intensity is still about one candela.
Temperature.
The hottest part of the flame is just above the very dull blue part to one side of the flame, at the base. At this point, the flame is about 1,400 °C. However note that this part of the flame is very small and releases little heat energy. The blue color is due to chemiluminescence, while the visible yellow color is due to radiative emission from hot soot particles. The soot is formed through a series of complex chemical reactions, leading from the fuel molecule through molecular growth, until multi-carbon ring compounds are formed. The thermal structure of a flame is complex, hundreds of degrees over very short distances leading to extremely steep temperature gradients. On average, the flame temperature is about 1,000 °C. 
The color temperature is approximately 1,000 K.
Candle flame.
A candle flame is formed because wax vaporizes on burning. It has three distinct regions. The innermost zone, directly above the wick, contains wax that has been vaporized but that is unburnt. It is the darkest zone. The middle zone is yellow and luminous. As it is an oxygen depleted zone, insufficient oxygen exists to burn all of the wax vapor. As such, partial combustion of wax takes place. The zone also contains unburnt carbon vapor. The temperature in this region is hotter than the innermost zone, but cooler than the outer zone. The outer zone is the area where the flame is the hottest and complete combustion of wax takes place. It is light blue in color and not normally visible.
The main determinant of the height of a candle flame is the diameter of the wick. This is evidenced in TeaLights where the wick is very thin and the flame, which is for mainly decorative purposes, is very small. Candles whose main purpose is illumination use a much thicker wick.
History of study.
One of Michael Faraday's significant works was "The Chemical History of a Candle", where he gives an in-depth analysis of the evolutionary development, workings and science of candles.
Hazards.
According to the U.S. National Fire Protection Association, candles are one of the leading sources of residential fires in the U.S. with almost 10% of civilian injuries and 6% of civilian fatalities from fire attributed to candles.
A candle flame that is longer than its laminar smoke point will emit soot. Proper wick trimming will substantially reduce soot emissions from most candles.
The liquid wax is hot and can cause skin burns, but the amount and temperature are generally rather limited and the burns are seldom serious. The best way to avoid getting burned from splashed wax is to use a candle snuffer instead of blowing on the flame. A candle snuffer is usually a small metal cup on the end of a long handle. Placing the snuffer over the flame cuts off the oxygen supply. Snuffers were common in the home when candles were the main source of lighting before electric lights were available. Ornate snuffers, often combined with a taper for lighting, are still found in those churches which regularly use large candles.
Glass candle-holders are sometimes cracked by thermal shock from the candle flame, particularly when the candle burns down to the end. When burning candles in glass holders or jars, users should avoid lighting candles with chipped or cracked containers, and stop use once 1/2 inch or less of wax remains.
A former worry regarding the safety of candles was that a lead core was used in the wicks to keep them upright in container candles. Without a stiff core, the wicks of a container candle could sag and drown in the deep wax pool. Concerns rose that the lead in these wicks would vaporize during the burning process, releasing lead vapors — a known health and developmental hazard. Lead core wicks have not been common since the 1970s. Today, most metal-cored wicks use zinc or a zinc alloy, which has become the industry standard. Wicks made from specially treated paper and cotton are also available.
Regulation.
International markets have developed a range of standards and regulations to ensure compliance, while maintaining and improving safety, including:
Accessories.
Candle holders.
Decorative candleholders, especially those shaped as a pedestal, are called candlesticks; if multiple candle tapers are held, the term "candelabrum" is also used. The root form of "chandelier" is from the word for candle, but now usually refers to an electric fixture. The word "chandelier" is sometimes now used to describe a hanging fixture designed to hold multiple tapers.
Many candle holders use a friction-tight socket to keep the candle upright. In this case, a candle that is slightly too wide will not fit in the holder, and a candle that is slightly too narrow will wobble. Candles that are too big can be trimmed to fit with a knife; candles that are too small can be fitted with aluminium foil. Traditionally, the candle and candle holders were made in the same place, so they were appropriately sized, but international trade has combined the modern candle with existing holders, which makes the ill-fitting candle more common. This friction tight socket is only needed for the federals and the tapers. For tea light candles, there are a variety of candle holders, including small glass holders and elaborate multi-candle stands. The same is true for votives. Wall sconces are available for tea light and votive candles. For pillar-type candles, the assortment of candle holders is broad. A fireproof plate, such as a glass plate or small mirror, is a candle holder for a pillar-style candle. A pedestal of any kind, with the appropriate-sized fireproof top, is another option. A large glass bowl with a large flat bottom and tall mostly vertical curved sides is called a hurricane. The pillar-style candle is placed at the bottom center of the hurricane. A hurricane on a pedestal is sometimes sold as a unit.
A bobèche is a drip-catching ring, which may also be affixed to a candle holder, or used independently of one. Bobèches can range from ornate metal or glass, to simple plastic, cardboard, or wax paper. Use of paper or plastic bobèches is common at events where candles are distributed to a crowd or audience, such as Christmas carolers or people at other concerts/festivals.
Candle followers.
These are glass or metal tubes with an internal stricture partway along, which sit around the top of a lit candle. As the candle burns, the wax melts and the follower holds the melted wax in, whilst the stricture rests on the topmost solid portion of wax. Candle followers are often deliberately heavy or 'weighted', to ensure they move down as the candle burns lower, maintaining a seal and preventing wax escape. The purpose of a candle follower is threefold:
Candle followers are often found in churches on altar candles.
Candle snuffers.
Candle snuffers are instruments used to extinguish burning candles by smothering the flame with a small metal cup that is suspended from a long handle, and thus depriving it of oxygen. An older meaning refers to a scissor-like tool used to trim the wick of a candle. With skill, this could be done without extinguishing the flame. The instrument now known as a candle snuffer was formerly called an "extinguisher" or "douter".

</doc>
<doc id="44019" url="https://en.wikipedia.org/wiki?curid=44019" title="205">
205

__NOTOC__
Year 205 (CCV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Aurelius and Geta (or, less frequently, year 958 "Ab urbe condita"). The denomination 205 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="44020" url="https://en.wikipedia.org/wiki?curid=44020" title="DSP">
DSP

DSP may refer to: 

</doc>
<doc id="44021" url="https://en.wikipedia.org/wiki?curid=44021" title="Utopian and dystopian fiction">
Utopian and dystopian fiction

The utopia and its derivative, the dystopia, are genres of literature that explore social and political structures. Utopian fiction portrays a setting that agrees with the author's ethos and is portrayed as having various attributes that readers often find to be characteristic of that which they would like to implement in reality, or utopia, as the setting for a novel. Dystopian fiction--commonly referred to as dystopic fiction--(sometimes combined with, but distinct from apocalyptic literature) is the opposite: the portrayal of a setting that completely disagrees with the author's ethos and is portrayed as having various attributes that readers often find to be characteristic of that which they would like to avoid in reality, or dystopia. Many novels combine both, often as a metaphor for the different directions humanity can take in its choices, ending up with one of two possible futures. Both utopias and dystopias are commonly found in science fiction and other speculative fiction genres, and arguably are by definition a type of speculative fiction.
More than 400 utopian works were published prior to the year 1900 in the English language alone, with more than a thousand others during the twentieth century.
Subgenres.
Utopian fiction.
The word "utopia" was first used in direct context by Sir Thomas More in his 1516 work "Utopia". The word "utopia" resembles both the Greek words "no place", "outopos", and "good place", "eutopos". In his book, which was written in Latin, More sets out a vision of an ideal society. As the title suggests, the work presents an ambiguous and ironic projection of the ideal state. The whimsical nature of the text can be confirmed by the narrator of Utopia's second book, Raphael Hythloday. The Greek root of Hythloday suggests an 'expert in nonsense'. An earlier example of a Utopian work from classical antiquity is Plato's "The Republic", in which he outlines what he sees as the ideal society and its political system. Later examples can be seen in Samuel Johnson's "The History of Rasselas, Prince of Abissinia" and Samuel Butler's "Erewhon", which uses an anagram of "nowhere" as its title. This, like much of the utopian literature, can be seen as utopian satire which is most notable in the inversion of illness and crime which Butler portrays, with punishment for the former and treatment for the latter.
Dystopian fiction.
Dystopia is defined as an alternate society characterized by a focus on that which is contrary to the author's ethos, portraying it as mass poverty, public mistrust and suspicion, police state, and/or oppression. Most authors of dystopian fiction explore at least one reason why things are that way, often as an analogy for similar issues in the real world. In the words of Keith M. Booker, dystopian literature is used to "provide fresh perspectives on problematic social and political practices that might otherwise be taken for granted or considered natural and inevitable".
Dystopias usually extrapolate elements of contemporary society and are read by many as political warnings. Many dystopias claim to be utopias. Samuel Butler's "Erewhon" can be seen as a dystopia because of the way sick people are punished as criminals while thieves are "cured" in hospitals, which the inhabitants of Erewhon see as natural and right, i.e. utopian (as mocked in Voltaire's "Candide"). The 1921 novel "We" by Yevgeny Zamyatin predicts a post-apocalyptic future in which society is entirely based on logic and modeled after mechanical systems; also, George Orwell cited it as an influence on his "Nineteen Eighty-Four". Aldous Huxley's novel "Brave New World" started as a parody of utopian fiction, and projected into the year 2540 industrial and social changes he perceived in 1931, leading to industrial success by a coercively persuaded population divided into five castes, three of which are artificially, systematically and deliberately mentally challenged from birth to match their intellects to various menial careers, which is rewarded with all their material needs being met and with hedonism. The World State kills everyone 60 years old or older. Orwell's "Nineteen Eighty-Four" is a novel about Oceania, that also offers its people a recreational substance, but the substance is an alcoholic beverage that to put it mildly is unpleasant. Oceania conditions its population through propaganda that Oceania requires to be proficient and to not contain anything that would contradict anything else contained in the propaganda at that particular time and in this sense as convincing as possible, but to contradict what it says at different times so heavy-handedly that anyone can easily see that it is inauthentic if and only if they do not have total amnesia. Oceania carefully deters its population from any contrary thought, but it has hate sessions in which they are trained to take offense at dissent. These tactics make it as difficult as possible to avoid thinking contrarily, and it is suggested that this difficulty is intended by the Inner Party. Anthony Burgess' 1962 novel "A Clockwork Orange" is set in a future England that has a subculture of extreme youth violence, and details the protagonist's experiences with the state intent on changing his character at their whim. Margaret Atwood's "The Handmaid's Tale" describes a future North America governed by strict religious rules that all and only the privileged defy. Examples of young adult dystopian fiction include "The Hunger Games" by Suzanne Collins, "The House of the Scorpion" by Nancy Farmer, "Divergent", "Insurgent" by Veronica Roth, "The Maze Runner" by James Dashner, and "Delirium" by Lauren Oliver. Video games often include dystopias as well; notable examples include the "Fallout" series, BioShock, and the later games of the "Half-Life" series.
Combinations.
Jonathan Swift's "Gulliver's Travels" is sometimes linked with utopian (and dystopian) literature, because it shares the general preoccupation with ideas of the good (and bad) society. Of the countries Lemuel Gulliver visits, Brobdingnag and Country of the Houyhnhnms approach a utopia; the others have significant dystopian aspects.
Many works combine elements of both utopias and dystopias. Typically, an observer from our world will journey to another place or time and see one society the author considers ideal, and another representing the worst possible outcome. The point is usually that the choices we make now may lead to a better or worse potential future world. Ursula K. Le Guin's "Always Coming Home" fulfils this model, as does Marge Piercy's "Woman on the Edge of Time". In Starhawk's "The Fifth Sacred Thing" there is no time-travelling observer, but her ideal society is invaded by a neighbouring power embodying evil repression. In Aldous Huxley's "Island", in many ways a counterpoint to his better-known "Brave New World", the fusion of the best parts of Buddhist philosophy and Western technology is threatened by the "invasion" of oil companies. As another example, in the "Unwanteds" series by Lisa McMann, a paradox occurs where the outcasts from a complete dystopia are treated to absolute utopia, and therefore believe that those who were privileged in said dystopia were actually the unlucky ones. 
In another literary model, the imagined society journeys between elements of utopia and dystopia over the course of the novel or film. At the beginning of "The Giver" by Lois Lowry, the world is described as a utopia, but as the book progresses, the world's dystopian aspects are revealed.
Ecotopian fiction.
A subgenre of this is ecotopian fiction, where the author posits either a utopian or dystopian world revolving around environmental conservation or destruction. Danny Bloom coined the term "cli fi" in 2006, with a Twitter boost from Margaret Atwood in 2011, to cover climate change-related fiction, but the theme has existed for decades. Novels dealing with overpopulation, such as Harry Harrison's "Make Room! Make Room!" (made into movie "Soylent Green"), were popular in the 1970s, reflecting the popular concern with the effects of overpopulation on the environment. The novel "Nature's End" by Whitley Strieber and James Kunetka (1986) posits a future in which overpopulation, pollution, climate change, and resulting superstorms, have led to a popular mass-suicide political movement. Some other examples of ecological dystopias are depictions of Earth in the films "Wall-E" and "Avatar".
While eco-dystopias are more common, a small number of works depicting what might be called eco-utopia, or eco-utopian trends, have also been influential. These include Ernest Callenbach's "Ecotopia", an important 20th century example of this genre. Kim Stanley Robinson has written a number of books dealing with environmental themes, including the Mars trilogy. Most notably, however, his "Three Californias Trilogy" contrasted an eco-dystopia with an eco-utopia, and a sort of middling-future. Robinson has also edited an anthology of short ecotopian fiction, called "".
There are a few dystopias that have an "anti-ecological" theme. These are often characterized by a government that is overprotective of nature or a society that has lost most modern technology and struggles for survival. A good example of this is the novel "Riddley Walker".
Feminist utopias.
Another subgenre is feminist utopias and the overlapping category of feminist science fiction. Writer Sally Miller Gearhart calls this sort of fiction political: it contrasts the present world with an idealized society, criticizes contemporary values and conditions, sees men or masculine systems as the major cause of social and political problems (e.g. war), and presents women as equal to or superior to men, having ownership over their reproductive functions. A common solution to gender oppression or social ills in feminist utopian fiction is to remove men, either showing isolated all-female societies as in Charlotte Perkins Gilman's "Herland", or societies where men have died out or been replaced, as in Joanna Russ's "A Few Things I Know About Whileaway", where "the poisonous binary gender" has died off. Marge Piercy's novel "Woman on the Edge of Time" keeps human biology, but removes pregnancy and childbirth from the gender equation by resorting to artificial wombs, while allowing both women and men the nurturing experience of breastfeeding.
Utopias have explored the ramification of gender being either a societal construct or a hard-wired imperative. In Mary Gentle's "Golden Witchbreed", gender is not chosen until maturity, and gender has no bearing on social roles. In contrast, Doris Lessing's "The Marriages Between Zones Three, Four and Five" (1980) suggests that men's and women's values are inherent to the sexes and cannot be changed, making a compromise between them essential. In "My Own Utopia" (1961) by Elizabeth Mann Borghese, gender exists but is dependent upon age rather than sex — genderless children mature into women, some of whom eventually become men.
Utopic single-gender worlds or single-sex societies have long been one of the primary ways to explore implications of gender and gender-differences. In speculative fiction, female-only worlds have been imagined to come about by the action of disease that wipes out men, along with the development of technological or mystical method that allow female parthenogenic reproduction. The resulting society is often shown to be utopian by feminist writers. Many influential feminist utopias of this sort were written in the 1970s; the most often studied examples include Joanna Russ's "The Female Man", Suzy McKee Charnas's "Walk to the End of the World" and "Motherlines", and Marge Piercy's "Woman on the Edge of Time". Utopias imagined by male authors have generally included equality between sexes, rather than separation. Such worlds have been portrayed most often by lesbian or feminist authors; their use of female-only worlds allows the exploration of female independence and freedom from patriarchy. The societies may not necessarily be lesbian, or sexual at all — "Herland" (1915) by Charlotte Perkins Gilman is a famous early example of a sexless society. Charlene Ball writes in "Women's Studies Encyclopedia" that use of speculative fiction to explore gender roles has been more common in the United States than in Europe and elsewhere.
Feminist dystopians have become prevalent in Young Adult, or YA, literature in recent years, focusing on the relationship between gender identity and the teenager. For instance, the "Birthmarked" trilogy by Caragh O'Brien focuses on a teenage midwife in a future post-apocalyptic world while the second novel in the series places the teenage heroine Gaia in a matriarchal society.
Cultural impact.
Étienne Cabet's work "Travels in Icaria" caused a group of followers to leave France in 1848 and travel to the United States to found a series of utopian settlements in Texas, Illinois, Iowa, California, and elsewhere. These groups lived in communal settings and lasted until 1898.

</doc>
<doc id="44022" url="https://en.wikipedia.org/wiki?curid=44022" title="206">
206

__NOTOC__
Year 206 (CCVI) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Umbrius and Gavius (or, less frequently, year 959 "Ab urbe condita"). The denomination 206 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
</onlyinclude>

</doc>
<doc id="44024" url="https://en.wikipedia.org/wiki?curid=44024" title="207">
207

__NOTOC__
Year 207 (CCVII) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Maximus and Severus (or, less frequently, year 960 "Ab urbe condita"). The denomination 207 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="44025" url="https://en.wikipedia.org/wiki?curid=44025" title="208">
208

__NOTOC__
Year 208 (CCVIII) was a leap year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Aurelius and Geta (or, less frequently, year 961 "Ab urbe condita"). The denomination 208 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Parthia.
</onlyinclude>

</doc>
<doc id="44026" url="https://en.wikipedia.org/wiki?curid=44026" title="United States National Security Council">
United States National Security Council

The White House National Security Council (NSC) is the principal forum used by the President of the United States for consideration of national security and foreign policy matters with senior national security advisors and Cabinet officials and is part of the Executive Office of the President of the United States. Since its inception under Harry S. Truman, the function of the Council has been to advise and assist the president on national security and foreign policies. The Council also serves as the president's principal arm for coordinating these policies among various government agencies. The Council has counterparts in the national security councils of many other nations.
History.
The National Security Council was created in 1947 by the National Security Act. It was created because policymakers felt that the diplomacy of the State Department was no longer adequate to contain the USSR in light of the tension between the Soviet Union and the United States. The intent was to ensure coordination and concurrence among the Army, Marine Corps, Navy, Air Force and other instruments of national security policy such as the Central Intelligence Agency (CIA), also created in the National Security Act.
On May 26, 2009, President Barack Obama merged the White House staff supporting the Homeland Security Council (HSC) and the National Security Council into one National Security Staff (NSS). The HSC and NSC each continue to exist by statute as bodies supporting the President.
The decision process inside the structure has become less and less formal but influence of the Council has become stronger and stronger. Detailed history of the National Security Council under each Presidential administration since its inception can be found at:
Membership.
The National Security Council is chaired by the President. Its members are the Vice President (statutory), the Secretary of State (statutory), the Secretary of Defense (statutory), the National Security Advisor (non-statutory), and the Secretary of Treasury (non-statutory).
The Chairman of the Joint Chiefs of Staff is the statutory military advisor to the Council, the Director of National Intelligence is the statutory intelligence advisor, and the Director of National Drug Control Policy is the statutory drug control policy advisor. The Chief of Staff to the President, Counsel to the President, and the Assistant to the President for Economic Policy are also regularly invited to attend NSC meetings. The Attorney General, the Director of the Office of Management and Budget and The Director of the Central Intelligence Agency are invited to attend meetings pertaining to their responsibilities. The heads of other executive departments and agencies, as well as other senior officials, are invited to attend meetings of the NSC when appropriate.
Authority.
The National Security Council was established by the National Security Act of 1947 (PL 235 – 61 Stat. 496; U.S.C. 402), amended by the National Security Act Amendments of 1949 (63 Stat. 579; 50 U.S.C. 401 et seq.). Later in 1949, as part of the Reorganization Plan, the Council was placed in the Executive Office of the President.
High Value Detainee Interrogation Group.
The High Value Detainee Interrogation Group reports to the NSC.
Kill authorizations.
A secret National Security Council panel may pursue the killing of an individual who has been called a suspected terrorist. In this case, no public record of this decision or any operation to kill the suspect will be made available. No laws govern criteria for killing such suspects, nor mandate the existence of the panel.
National Security Advisor Susan Rice, who has helped codify targeted killing criteria by creating the Disposition Matrix database, has described the Obama Administration targeted killing policy by stating that "in order to ensure that our counterterrorism operations involving the use of lethal force are legal, ethical, and wise, President Obama has demanded that we hold ourselves to the highest possible standards and processes".
It is unknown who has been placed on the kill list; Mark Hosenball, a Reuters reporter, alleges Anwar al-Awlaki was on the list.
On February 4, 2013, NBC published a leaked Department of Justice memo providing a summary of the rationale used to justify targeted killing of US citizens who are senior operational leaders of Al-Qa'ida or associated forces.

</doc>
<doc id="44027" url="https://en.wikipedia.org/wiki?curid=44027" title="Permutation">
Permutation

In mathematics, the notion of permutation relates to the act of arranging all the members of a set into some sequence or order, or if the set is already ordered, rearranging (reordering) its elements, a process called permuting. These differ from combinations, which are selections of some members of a set where order is disregarded. For example, written as tuples, there are six permutations of the set {1,2,3}, namely: (1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2), and (3,2,1). These are all the possible orderings of this three element set. As another example, an anagram of a word, all of whose letters are different, is a permutation of its letters. In this example, the letters are already ordered in the original word and the anagram is a reordering of the letters. The study of permutations of finite sets is a topic in the field of combinatorics.
Permutations occur, in more or less prominent ways, in almost every area of mathematics. They often arise when different orderings on certain finite sets are considered, possibly only because one wants to ignore such orderings and needs to know how many configurations are thus identified. For similar reasons permutations arise in the study of sorting algorithms in computer science.
The number of permutations of distinct objects is  factorial usually written as , which means the product of all positive integers less than or equal to .
In algebra and particularly in group theory, a permutation of a set is defined as a bijection from to itself. That is, it is a function from to for which every element occurs exactly once as an image value. This is related to the rearrangement of the elements of in which each element is replaced by the corresponding . The collection of such permutations form a group called the symmetric group of . The key to this group's structure is the fact that the composition of two permutations (performing two given rearrangements in succession) results in another rearrangement. Permutations may "act" on structured objects by rearranging their components, or by certain replacements (substitutions) of symbols.
In elementary combinatorics, the -permutations, or partial permutations, are the ordered arrangements of distinct elements selected from a set. When is equal to the size of the set, these are the permutations of the set.
History.
The rule to determine the number of permutations of "n" objects was known in Indian culture at least as early as around 1150: the Lilavati by the Indian mathematician Bhaskara II contains a passage that translates to
The product of multiplication of the arithmetical series beginning and increasing by unity and continued to the number of places, will be the variations of number with specific figures.
Fabian Stedman in 1677 described factorials when explaining the number of permutations of bells in change ringing. Starting from two bells: "first, "two" must be admitted to be varied in two ways" which he illustrates by showing 1 2 and 2 1. He then explains that with three bells there are "three times two figures to be produced out of three" which again is illustrated. His explanation involves "cast away 3, and 1.2 will remain; cast away 2, and 1.3 will remain; cast away 1, and 2.3 will remain". He then moves on to four bells and repeats the casting away argument showing that there will be four different sets of three. Effectively this is an recursive process. He continues with five bells using the "casting away" method and tabulates the resulting 120 combinations. At this point he gives up and remarks:
Now the nature of these methods is such, that the changes on one number comprehends the changes on all lesser numbers, ... insomuch that a compleat Peal of changes on one number seemeth to be formed by uniting of the compleat Peals on all lesser numbers into one entire body;
Stedman widens the consideration of permutations; he goes on to consider the number of permutations of the letters of the alphabet and horses from a stable of 20.
A first case in which seemingly unrelated mathematical questions were studied with the help of permutations occurred around 1770, when Joseph Louis Lagrange, in the study of polynomial equations, observed that properties of the permutations of the roots of an equation are related to the possibilities to solve it. This line of work ultimately resulted, through the work of Évariste Galois, in Galois theory, which gives a complete description of what is possible and impossible with respect to solving polynomial equations (in one unknown) by radicals. In modern mathematics there are many similar situations in which understanding a problem requires studying certain permutations related to it.
Definition and one-line notation.
There are two equivalent common ways of regarding permutations, sometimes called the "active" and "passive" forms, or in older terminology "substitutions" and "permutations". Which form is preferable depends on the type of questions being asked in a given discipline.
The "active" way to regard permutations of a set "S" (finite or infinite) is to define them as the bijections from "S" to itself. Thus, the permutations are thought of as functions which can be composed with each other, forming groups of permutations. From this viewpoint, the elements of "S" have no internal structure and are just labels for the objects being moved: one may refer to permutations of any set of "n" elements as "permutations on "n" letters".
In Cauchy's "two-line notation", one lists the elements of "S" in the first row, and for each one its image below it in the second row. For instance, a particular permutation of the set "S" = {1,2,3,4,5} can be written as:
this means that "σ" satisfies "σ"(1)=2, "σ"(2)=5, "σ"(3)=4, "σ"(4)=3, and "σ"(5)=1. The elements of "S" may appear in any order in the first row. This permutation could also be written as:
The "passive" way to regard a permutation of the set "S" is an "ordered arrangement" (or listing, or linearly ordered arrangement, or sequence without repetition) of the elements of "S". This is related to the active form as follows. If there is a "natural" order for the elements of "S", say formula_3, then one uses this for the first row of the two-line notation:
Under this assumption, one may omit the first row and write the permutation in "one-line notation" as formula_5, that is, an ordered arrangement of S. Care must be taken to distinguish one-line notation from the cycle notation described later. In mathematics literature, a common usage is to omit parentheses for one-line notation, while using them for cycle notation. The one-line notation is also called the "word representation" of a permutation. The example above would then be 2 5 4 3 1 since the natural order 1 2 3 4 5 would be assumed for the first row. (It is typical to use commas to separate these entries only if some have two or more digits.) This form is more compact, and is common in elementary combinatorics and computer science. It is especially useful in applications where the elements of "S" or the permutations are to be compared as larger or smaller.
Other uses of the term "permutation".
The concept of a permutation as an ordered arrangement admits several generalizations that are not permutations but have been called permutations in the literature.
"k"-permutations of "n".
A weaker meaning of the term "permutation", sometimes used in elementary combinatorics texts, designates those ordered arrangements in which no element occurs more than once, but without the requirement of using all the elements from a given set. These are not permutations except in special cases, but are natural generalizations of the ordered arrangement concept. Indeed, this use often involves considering arrangements of a fixed length "k" of elements taken from a given set of size "n", in other words, these "k"-permutations of "n" are the different ordered arrangements of a "k"-element subset of an "n"-set (sometimes called variations in the older literature.) These objects are also known as partial permutations or as sequences without repetition, terms that avoid confusion with the other, more common, meaning of "permutation". The number of such formula_6-permutations of formula_7 is denoted variously by such symbols as formula_8, formula_9, formula_10, formula_11, or formula_12, and its value is given by the product
which is 0 when , and otherwise is equal to
The product is well defined without the assumption that formula_7 is a non-negative integer and is of importance outside combinatorics as well; it is known as the Pochhammer symbol formula_16 or as the formula_6-th falling factorial power formula_18 of formula_7.
This usage of the term "permutation" is closely related to the term "combination". A "k"-element combination of an "n"-set "S" is a "k" element subset of "S", the elements of which are not ordered. By taking all the "k" element subsets of "S" and ordering each of them in all possible ways we obtain all the "k"-permutations of "S". The number of "k"-combinations of an "n"-set, "C"("n","k"), is therefore related to the number of "k"-permutations of "n" by:
These numbers are also known as binomial coefficients and denoted formula_21.
C, E, G, I, N, R}, the sequence ICE is a 3-permutation, RING and RICE are 4-permutations, NICER and REIGN are 5-permutations, and CRINGE is a 6-permutation; since the latter uses all letters, it is a permutation of the given set in the ordinary combinatorial sense. ENGINE on the other hand is not a permutation, because of the repetitions: it uses the elements E and N twice.
Let "n" be the size of "S", the number of elements available for selection. In constructing a "k"-permutation, there are "n"  possible choices for the first element of the sequence, and this is then number of 1-permutations. Once it has been chosen, there are elements of "S" left to choose from, so a second element can be chosen in ways, giving a total "n" × ("n" − 1) possible 2-permutations. For each successive element of the sequence, the number of possibilities decreases by 1  which leads to the number of
This gives in particular the number of "n"-permutations (which contain all elements of "S" once, and are therefore simply permutations of "S"):
a number that occurs so frequently in mathematics that it is given a compact notation ""n"!", and is called ""n" factorial". These "n"-permutations are the longest sequences without repetition of elements of "S", which is reflected by the fact that the above formula for the number of "k"-permutations gives zero whenever "k" > "n".
The number of "k"-permutations of a set of "n" elements is sometimes denoted by "P"("n","k") or a similar notation (usually accompanied by a notation for the number of "k"-combinations of a set of "n" elements in which the ""P"" is replaced by ""C""). That notation is rarely used in other contexts than that of counting "k"-permutations, but the expression for the number does arise in many other situations. Being a product of "k" factors starting at "n" and decreasing by unit steps, it is called the "k"-th falling factorial power of "n":
though many other names and notations are in use, as detailed at Pochhammer symbol. When "k" ≤ "n" the factorial power can be completed by additional factors: "n""k" × ("n" − "k")! = "n"!, which allows writing
The right hand side is often given as expression for the number of "k"-permutations, but its main merit is using the compact factorial notation. Expressing a product of "k" factors as a quotient of potentially much larger products, where all factors in the denominator are also explicitly present in the numerator, is not particularly efficient; as a method of computation there is the additional danger of overflow or rounding errors. It should also be noted that the expression is undefined when "k" > "n", whereas in those cases the number "n""k" of "k"-permutations is just 0.
Permutations with repetition.
Ordered arrangements of the elements of a set "S" of length "n" where repetition is allowed are called "n"-tuples, but have sometimes been referred to as permutations with repetition although they are not permutations in general. They are also called words over the alphabet "S" in some contexts. If the set "S" has "k" elements, the number of "n"-tuples over "S" is:
There is no restriction on how often an element can appear in an "n"-tuple, but if restrictions are placed on how often an element can appear, this formula is no longer valid.
Permutations of multisets.
If "M" is a finite multiset, then a multiset permutation is an ordered arrangement of elements of "M" in which each element appears exactly as often as is its multiplicity in "M". An anagram of a word having some repeated letters is an example of a multiset permutation. If the multiplicities of the elements of "M" (taken in some order) are formula_25, formula_26, ..., formula_27 and their sum (i.e., the size of "M") is "n", then the number of multiset permutations of "M" is given by the multinomial coefficient,
For example, the number of distinct anagrams of the word MISSISSIPPI is:
A k-permutation of a multiset "M" is a sequence of length "k" of elements of "M" in which each element appears "at most" its multiplicity in "M" times (an element's "repetition number").
Circular permutations.
Permutations, when considered as arrangements, are sometimes referred to as "linearly ordered" arrangements. In these arrangements there is a first element, a second element, and so on. If, however, the objects are arranged in a circular manner this distinguished ordering no longer exists, that is, there is no "first element" in the arrangement, any element can be considered as the start of the arrangement. The arrangements of objects in a circular manner are called circular permutations. These can be formally defined as equivalence classes of ordinary permutations of the objects, for the equivalence relation generated by moving the final element of the linear arrangement to its front.
Two circular permutations are equivalent if one can be rotated into the other (that is, cycled without changing the relative positions of the elements). The following two circular permutations on four letters are considered to be the same.
The circular arrangements are to be read counterclockwise, so the following two are not equivalent since no rotation can bring one to the other.
The number of circular permutations of a set "S" with "n" elements is ("n" - 1)!.
Permutations in group theory.
The set of all permutations of any given set "S" forms a group, with the composition of maps as the product operation and the identity function as the neutral element of the group. This is the symmetric group of "S", denoted by Sym("S"). Up to isomorphism, this symmetric group only depends on the cardinality of the set (called the "degree" of the group), so the nature of elements of "S" is irrelevant for the structure of the group. Symmetric groups have been studied mostly in the case of finite sets, so, confined to this case, one can assume without loss of generality that "S" = {1,2...,"n"} for some natural number "n". This is then the symmetric group of degree "n", usually written as .
Any subgroup of a symmetric group is called a permutation group. By Cayley's theorem any group is isomorphic to some permutation group, and every finite group to a subgroup of some finite symmetric group.
Cycle notation.
This alternative notation describes the effect of repeatedly applying the permutation, thought of as a function from a set onto itself. It expresses the permutation as a product of cycles corresponding to the orbits of the permutation; since distinct orbits are disjoint, this is referred to as "decomposition into disjoint cycles". 
Starting from some element "x" of "S", one writes the sequence ("x" "σ"("x") "σ"("σ"("x")) ...) of successive images under "σ", until the image returns to "x", at which point one closes the parenthesis rather than repeat "x". The set of values written down forms the orbit (under "σ") of "x", and the parenthesized expression gives the corresponding cycle of "σ". One then continues by choosing a new element "y" of "S" outside the previous orbit and writing down the cycle starting at "y"; and so on until all elements of "S" are written in cycles. Since for every new cycle the starting point can be chosen in different ways, there are in general many different cycle notations for the same permutation; for the example above one has:
A cycle ("x") of length 1 occurs when σ("x") = "x", and is commonly omitted from the cycle notation, provided the set "S" is clear: for any element "x" ∈ "S" not appearing in a cycle, one implicitly assumes σ("x") = "x". The identity permutation, which consists only of 1-cycles, can be denoted by a single 1-cycle (x), by the number 1, or by "id". 
A cycle ("x"1 "x"2 ... "x""k") of length "k" is called a "k"-cycle. Written by itself, it denotes a permutation in its own right, which maps "x""i" to "x""i"+1 for , and "x""k" to "x"1, while implicitly mapping all other elements of "S" to themselves (omitted 1-cycles). Therefore the individual cycles in the cycle notation can be interpreted as factors in a composition product. Since the orbits are disjoint, the corresponding cycles commute under composition, and so can be written in any order. The cycle decomposition is essentially unique: apart from the reordering the cycles in the product, there are no other ways to write "σ" as a product of cycles. Each individual cycle can be written in different ways, as in the example above where (5 1 2) and (1 2 5) and (2 5 1) all denote the same cycle, though note that (5 2 1) denotes a different cycle.
An element in a 1-cycle ("x"), corresponding to σ("x") = "x", is called a fixed point of the permutation σ. A permutation with no fixed points is called a derangement. Cycles of length two are called transpositions; such permutations merely exchange the place of two elements, implicitly leaving the others fixed. Since the orbits of a permutation partition the set "S", for a finite set of size "n", the lengths of the cycles of a permutation σ form a partition of "n" called the cycle type of σ. There is a "1" in the cycle type for every fixed point of σ, a "2" for every transposition, and so on. The cycle type of β = (1 2 5)(3 4)(6 8)(7), is (3,2,2,1) which is sometimes written in a more compact form as (11,22,31).
The number of "n"-permutations with "k" disjoint cycles is the signless Stirling number of the first kind, denoted by formula_31.
Abstract groups vs. permutations vs. group actions.
Permutation groups have more structure than abstract groups, and different realizations of a group as a permutation group need not be equivalent as permutations. For instance is naturally a permutation group, in which any transposition has cycle type (2,1); but the proof of Cayley's theorem realizes as a subgroup of (namely the permutations of the 6 elements of itself), in which permutation group transpositions have cycle type (2,2,2). Finding the minimal-order symmetric group containing a subgroup isomorphic to a given group (sometimes called minimal faithful degree representation) is a rather difficult problem. So in spite of Cayley's theorem, the study of permutation groups differs from the study of abstract groups, being a branch of representation theory.
Much of the power of permutations can be regained in an abstract setting by considering group actions instead. A group action actually permutes the elements of a set according to the recipe provided by the abstract group. For example, acts faithfully and transitively on a set with exactly three elements (by permuting them).
Product and inverse.
The product of two permutations is defined as their composition as functions, in other words "σ·π" is the function that maps any element "x" of the set to "σ"("π"("x")). Note that the rightmost permutation is applied to the argument first,
because of the way function application is written. Some authors prefer the leftmost factor acting first,
but to that end permutations must be written to the "right" of their argument, for instance as an exponent, where "σ" acting on "x" is written "x""σ"; then the product is defined by "x""σ·π" = ("x""σ")"π". However this gives a "different" rule for multiplying permutations; this article uses the definition where the rightmost permutation is applied first.
Since the composition of two bijections always gives another bijection, the product of two permutations is again a permutation. In two-line notation, the product of two permutations is obtained by rearranging the columns of the second (leftmost) permutation so that its first row is identical with the second row of the first (rightmost) permutation. The product can then be written as the first row of the first permutation over the second row of the modified second permutation. For example, given the permutations,
the product "QP" is:
In cyclic notation this same product would be given by:
In general the composition of two permutations is not commutative, so that "PQ" can be different from "QP" as in the above example:
Since function composition is associative, so is the product operation on permutations: ("σ·π")·"ρ" = "σ"·("π·ρ"). Therefore, products of more than two permutations are usually written without adding parentheses to express grouping; they are also usually written without a dot or other sign to indicate multiplication.
The identity permutation, which maps every element of the set to itself, is the neutral element for this product. In two-line notation, the identity is
Since bijections have inverses, so do permutations, and the inverse "σ"−1 of "σ" is again a permutation. Explicitly, whenever "σ"("x")="y" one also has "σ"−1("y")="x". In two-line notation the inverse can be obtained by interchanging the two lines (and sorting the columns if one wishes the first line to be in a given order). For instance
In cycle notation one can reverse the order of the elements in each cycle to obtain a cycle notation for its inverse. Thus,
Having an associative product, a neutral element, and inverses for all its elements, makes the set of all permutations of "S" into a group, called the symmetric group of "S".
Properties.
Every permutation of a finite set can be expressed as the product of transpositions.
Moreover, although many such expressions for a given permutation may exist, there can never be among them both expressions with an even number and expressions with an odd number of transpositions. All permutations are then classified as even or odd, according to the parity of the transpositions in any such expression.
Multiplying permutations written in cycle notation follows no easily described pattern, and the cycles of the product can be entirely different from those of the permutations being composed. However the cycle structure is preserved in the special case of conjugating a permutation "σ" by another permutation "π", which means forming the product "π·σ·π"−1. Here the cycle notation of the result can be obtained by taking the cycle notation for "σ" and applying "π" to all the entries in it.
Matrix representation.
One can represent a permutation of {1, 2, ..., "n"} as an "n"×"n" matrix. There are two natural ways to do so, but only one for which multiplications of matrices corresponds to multiplication of permutations in the same order: this is the one that associates to "σ" the matrix "M" whose entry "M""i","j" is 1 if "i" = "σ"("j"), and 0 otherwise. The resulting matrix has exactly one entry 1 in each column and in each row, and is called a "permutation matrix". <br>
Here (file) is a list of these matrices for permutations of 4 elements. The Cayley table on the right shows these matrices for permutations of 3 elements.
Permutation of components of a sequence.
As with any group, one can consider actions of a symmetric group on a set, and there are many ways in which such an action can be defined. For the symmetric group of {1, 2, ..., "n"} there is one particularly natural action, namely the action by permutation on the set "X""n" of sequences of "n" symbols taken from some set "X". As with the matrix representation, there are two natural ways in which the result of permuting a sequence ("x"1,"x"2...,"x""n") by "σ" can be defined, but only one is compatible with the multiplication of permutations (so as to give a left action of the symmetric group on "X""n"); with the multiplication rule used in this article this is the one given by
This means that each component "x""i" ends up at position "σ"("i") in the sequence permuted by "σ".
Permutations of totally ordered sets.
In some applications, the elements of the set being permuted will be compared with each other. This requires that the set "S" has a total order so that any two elements can be compared. The set {1, 2, ..., "n"} is totally ordered by the usual "≤" relation and so it is the most frequently used set in these applications, but in general, any totally ordered set will do. In these applications, the ordered arrangement view of a permutation is needed to talk about the "positions" in a permutation.
Here are a number of properties that are directly related to the total ordering of "S".
Ascents, descents, runs and excedances.
An "ascent" of a permutation "σ" of "n" is any position "i" < "n" where the following value is bigger than the current one. That is, if "σ" = "σ"1"σ"2..."σ""n", then "i" is an ascent if "σ""i" < "σ""i"+1.
For example, the permutation 3452167 has ascents (at positions) 1,2,5,6.
Similarly, a "descent" is a position "i" < "n" with "σ""i" > "σ""i"+1, so every "i" with formula_40 either is an ascent or is a descent of "σ".
An "ascending run" of a permutation is a nonempty increasing contiguous subsequence of the permutation that cannot be extended at either end; it corresponds to a maximal sequence of successive ascents (the latter may be empty: between two successive descents there is still an ascending run of length 1). By contrast an "increasing subsequence" of a permutation is not necessarily contiguous: it is an increasing sequence of elements obtained from the permutation by omitting the values at some positions.
For example, the permutation 2453167 has the ascending runs 245, 3, and 167, while it has an increasing subsequence 2367.
If a permutation has "k" − 1 descents, then it must be the union of "k" ascending runs.
The number of permutations of "n" with "k" ascents is (by definition) the Eulerian number formula_41; this is also the number of permutations of "n" with "k" descents. Some authors however define the Eulerian number formula_41 as the number of permutations with "k" ascending runs, which corresponds to "k"-1 descents.
An excedance of a permutation "σ"1"σ"2..."σ""n" in an index "j" such that "σ""j" > "j". If the inequality is not strict, i.e. "σ""j" ≥ "j", then "j" is called a "weak excedance". The number of "n"-permutations with "k" excedances coincides with the number of "n"-permutations with "k" descents.
Canonical cycle notation (aka standard form).
In some combinatorial contexts it useful to fix a certain order or the elements in the cycles and of the (disjoint) cycles themselves. Miklós Bóna calls the following ordering choices the "canonical cycle notation":
For example, (312)(54)(8)(976) is a permutation in canonical cycle notation. Note that the canonical cycle notation does not omit one-cycles.
Richard P. Stanley calls the same choice of representation the "standard representation" of a permutation. and Martin Aigner uses the term "standard form" for the same notion. Sergey Kitaev also uses the "standard form" terminology, but reverses both choices, i.e. each cycle lists its least element first and the cycles are sorted in decreasing order of their least/first elements. In the remainder of this article, we use the first of these dual forms as the standard/canonical one.
Foata's transition lemma (or the fundamental bijection).
A natural question arises as to the relationship of the one-line and the canonical cycle notation. For example, considering the permutation (2)(31), which is in canonical cycle notation, if we erase its cycle parentheses, we obtain a different permutation in one-line notation, namely 231. (The permutation (2)(31) is actually 321 in one-line notation.) Foata's transition lemma establishes the nature of this correspondence as a bijection on the set of "n"-permutations (to itself). Richard P. Stanley calls this correspondence the "fundamental bijection".
Let formula_43 be the parentheses-erasing transformation. The inverse of formula_44 is a bit less intuitive. Starting with the one-line notation formula_45, the first cycle in canonical cycle notation must start with formula_46. As long as the subsequent elements are smaller than formula_46, we are in the same cycle. The second cycle starts at the smallest index formula_48 such that formula_49. In other words, formula_50 is larger than everything else to its left, so it is called a "left-to-right maximum". Every cycle in the canonical cycle notation starts with a left-to-right maximum.
For example, in the one-line notation 312548976, 5 is the first element larger than 3, so the first cycle must be (312). Then 8 is the next element larger than 5, so the second cycle is (54). Since 9 is larger than 8, (8) is a cycle by itself. Finally, 9 is larger than all the remaining elements to its right, so the last cycle is (976).
As a first corollary, the number of n-permutations with exactly "k" left-to-right maxima is also equal to the signless Stirling number of the first kind, formula_31. Furthermore, Foata's mapping takes an "n"-permutation with "k"-weak excedances to an "n"-permutations with "k" − 1 ascents. For example, (2)(31) = 321 has two weak excedances (at index 1 and 2), whereas "f"(321)=231 has one ascent (at index 1, i.e. from 2 to 3).
Inversions.
An "inversion" of a permutation "σ" is a pair ("i","j") of positions where the entries of a permutation are in the opposite order: formula_52 and formula_53. So a descent is just an inversion at two adjacent positions. For example, the permutation "σ" = 23154 has three inversions: (1,3), (2,3), (4,5), for the pairs of entries (2,1), (3,1), (5,4).
Sometimes an inversion is defined as the pair of values ("σ""i","σ""j") itself whose order is reversed; this makes no difference for the "number" of inversions, and this pair (reversed) is also an inversion in the above sense for the inverse permutation "σ"−1. The number of inversions is an important measure for the degree to which the entries of a permutation are out of order; it is the same for "σ" and for "σ"−1. To bring a permutation with "k" inversions into order (i.e., transform it into the identity permutation), by successively applying (right-multiplication by) adjacent transpositions, is always possible and requires a sequence of "k" such operations. Moreover any reasonable choice for the adjacent transpositions will work: it suffices to choose at each step a transposition of "i" and where "i" is a descent of the permutation as modified so far (so that the transposition will remove this particular descent, although it might create other descents). This is so because applying such a transposition reduces the number of inversions by 1; also note that as long as this number is not zero, the permutation is not the identity, so it has at least one descent. Bubble sort and insertion sort can be interpreted as particular instances of this procedure to put a sequence into order. Incidentally this procedure proves that any permutation "σ" can be written as a product of adjacent transpositions; for this one may simply reverse any sequence of such transpositions that transforms "σ" into the identity. In fact, by enumerating all sequences of adjacent transpositions that would transform "σ" into the identity, one obtains (after reversal) a "complete" list of all expressions of minimal length writing "σ" as a product of adjacent transpositions.
The number of permutations of "n" with "k" inversions is expressed by a Mahonian number, it is the coefficient of "X""k" in the expansion of the product
which is also known (with "q" substituted for "X") as the q-factorial ["n"]"q"! . The expansion of the product appears in Necklace (combinatorics).
Permutations in computing.
Numbering permutations.
One way to represent permutations of "n" is by an integer "N" with 0 ≤ "N" < "n"!, provided convenient methods are given to convert between the number and the representation of a permutation as an ordered arrangement (sequence). This gives the most compact representation of arbitrary permutations, and in computing is particularly attractive when "n" is small enough that "N" can be held in a machine word; for 32-bit words this means "n" ≤ 12, and for 64-bit words this means "n" ≤ 20. The conversion can be done via the intermediate form of a sequence of numbers "d""n", "d""n"−1, ..., "d"2, "d"1, where "d""i" is a non-negative integer less than "i" (one may omit "d"1, as it is always 0, but its presence makes the subsequent conversion to a permutation easier to describe). The first step then is to simply express "N" in the factorial number system, which is just a particular mixed radix representation, where for numbers up to "n"! the bases for successive digits are "n", , ..., 2, 1. The second step interprets this sequence as a Lehmer code or (almost equivalently) as an inversion table.
In the Lehmer code for a permutation "σ", the number "d""n" represents the choice made for the first term "σ"1, the number "d""n"−1 represents the choice made for the second term
"σ"2 among the remaining elements of the set, and so forth. More precisely, each "d""n"+1−"i" gives the number of "remaining" elements strictly less than the term "σ""i". Since those remaining elements are bound to turn up as some later term "σ""j", the digit "d""n"+1−"i" counts the "inversions" ("i","j") involving "i" as smaller index (the number of values "j" for which "i" < "j" and "σ""i" > "σ""j"). The inversion table for "σ" is quite similar, but here "d""n"+1−"k" counts the number of inversions ("i","j") where "k" = "σ""j" occurs as the smaller of the two values appearing in inverted order. Both encodings can be visualized by an "n" by "n" Rothe diagram (named after Heinrich August Rothe) in which dots at ("i","σ""i") mark the entries of the permutation, and a cross at ("i","σ""j") marks the inversion ("i","j"); by the definition of inversions a cross appears in any square that comes both before the dot ("j","σ""j") in its column, and before the dot ("i","σ""i") in its row. The Lehmer code lists the numbers of crosses in successive rows, while the inversion table lists the numbers of crosses in successive columns; it is just the Lehmer code for the inverse permutation, and vice versa.
To effectively convert a Lehmer code "d""n", "d""n"−1, ..., "d"2, "d"1 into a permutation of an ordered set "S", one can start with a list of the elements of "S" in increasing order, and for "i" increasing from 1 to "n" set "σ""i" to the element in the list that is preceded by "d""n"+1−"i" other ones, and remove that element from the list. To convert an inversion table "d""n", "d""n"−1, ..., "d"2, "d"1 into the corresponding permutation, one can traverse the numbers from "d"1 to "d""n" while inserting the elements of "S" from largest to smallest into an initially empty sequence; at the step using the number "d" from the inversion table, the element from "S" inserted into the sequence at the point where it is preceded by "d" elements already present. Alternatively one could process the numbers from the inversion table and the elements of "S" both in the opposite order, starting with a row of "n" empty slots, and at each step place the element from "S" into the empty slot that is preceded by "d" other empty slots.
Converting successive natural numbers to the factorial number system produces those sequences in lexicographic order (as is the case with any mixed radix number system), and further converting them to permutations preserves the lexicographic ordering, provided the Lehmer code interpretation is used (using inversion tables, one gets a different ordering, where one starts by comparing permutations by the "place" of their entries 1 rather than by the value of their first entries). The sum of the numbers in the factorial number system representation gives the number of inversions of the permutation, and the parity of that sum gives the signature of the permutation. Moreover the positions of the zeroes in the inversion table give the values of left-to-right maxima of the permutation (in the example 6, 8, 9) while the positions of the zeroes in the Lehmer code are the positions of the right-to-left minima (in the example positions the 4, 8, 9 of the values 1, 2, 5); this allows computing the distribution of such extrema among all permutations. A permutation with Lehmer code "d""n", "d""n"−1, ..., "d"2, "d"1 has an ascent if and only if .
Algorithms to generate permutations.
In computing it may be required to generate permutations of a given sequence of values. The methods best adapted to do this depend on whether one wants some randomly chosen permutations, or all permutations, and in the latter case if a specific ordering is required. Another question is whether possible equality among entries in the given sequence is to be taken into account; if so, one should only generate distinct multiset permutations of the sequence.
An obvious way to generate permutations of "n" is to generate values for the Lehmer code (possibly using the factorial number system representation of integers up to "n"!), and convert those into the corresponding permutations. However, the latter step, while straightforward, is hard to implement efficiently, because it requires "n" operations each of selection from a sequence and deletion from it, at an arbitrary position; of the obvious representations of the sequence as an array or a linked list, both require (for different reasons) about "n"2/4 operations to perform the conversion. With "n" likely to be rather small (especially if generation of all permutations is needed) that is not too much of a problem, but it turns out that both for random and for systematic generation there are simple alternatives that do considerably better. For this reason it does not seem useful, although certainly possible, to employ a special data structure that would allow performing the conversion from Lehmer code to permutation in "O"("n" log "n") time.
Random generation of permutations.
For generating random permutations of a given sequence of "n" values, it makes no difference whether one applies a randomly selected permutation of "n" to the sequence, or chooses a random element from the set of distinct (multiset) permutations of the sequence. This is because, even though in case of repeated values there can be many distinct permutations of "n" that result in the same permuted sequence, the number of such permutations is the same for each possible result. Unlike for systematic generation, which becomes unfeasible for large "n" due to the growth of the number "n"!, there is no reason to assume that "n" will be small for random generation.
The basic idea to generate a random permutation is to generate at random one of the "n"! sequences of integers "d"1,"d"2...,"d""n" satisfying (since "d"1 is always zero it may be omitted) and to convert it to a permutation through a bijective correspondence. For the latter correspondence one could interpret the (reverse) sequence as a Lehmer code, and this gives a generation method first published in 1938 by Ronald Fisher and Frank Yates.
While at the time computer implementation was not an issue, this method suffers from the difficulty sketched above to convert from Lehmer code to permutation efficiently. This can be remedied by using a different bijective correspondence: after using "d""i" to select an element among "i" remaining elements of the sequence (for decreasing values of "i"), rather than removing the element and compacting the sequence by shifting down further elements one place, one swaps the element with the final remaining element. Thus the elements remaining for selection form a consecutive range at each point in time, even though they may not occur in the same order as they did in the original sequence. The mapping from sequence of integers to permutations is somewhat complicated, but it can be seen to produce each permutation in exactly one way, by an immediate induction. When the selected element happens to be the final remaining element, the swap operation can be omitted. This does not occur sufficiently often to warrant testing for the condition, but the final element must be included among the candidates of the selection, to guarantee that all permutations can be generated.
The resulting algorithm for generating a random permutation of "a""a"[1, ..., "a"["n" − 1] can be described as follows in pseudocode:
This can be combined with the initialization of the array "a"["i"] = "i" as follows:
If "d""i"+1 = "i", the first assignment will copy an uninitialized value, but the second will overwrite it with the correct value "i".
Generation in lexicographic order.
There are many ways to systematically generate all permutations of a given sequence.
One classical algorithm, which is both simple and flexible, is based on finding the next permutation in lexicographic ordering, if it exists. It can handle repeated values, for which case it generates the distinct multiset permutations each once. Even for ordinary permutations it is significantly more efficient than generating values for the Lehmer code in lexicographic order (possibly using the factorial number system) and converting those to permutations. To use it, one starts by sorting the sequence in (weakly) increasing order (which gives its lexicographically minimal permutation), and then repeats advancing to the next permutation as long as one is found. The method goes back to Narayana Pandita in 14th century India, and has been frequently rediscovered ever since.
The following algorithm generates the next permutation lexicographically after a given permutation. It changes the given permutation in-place.
For example, given the sequence 2, 3, 4 which starts in a weakly increasing order, and given that the index is zero-based, the steps are as follows: 
Following this algorithm, the next lexicographic permutation will be and the 24th permutation will be [4,3,2,1 at which point "a"["k"] < "a"["k" + 1] does not exist, indicating that this is the last permutation.
Generation with minimal changes.
An alternative to the above algorithm, the Steinhaus–Johnson–Trotter algorithm, generates an ordering on all the permutations of a given sequence with the property that any two consecutive permutations in its output differ by swapping two adjacent values. This ordering on the permutations was known to 17th-century English bell ringers, among whom it was known as "plain changes". One advantage of this method is that the small amount of change from one permutation to the next allows the method to be implemented in constant time per permutation. The same can also easily generate the subset of even permutations, again in constant time per permutation, by skipping every other output permutation.
An alternative to Steinhaus–Johnson–Trotter is Heap's algorithm, said by Robert Sedgewick in 1977 to be the fastest algorithm of generating permutations in applications.
Meandric permutations.
Meandric systems give rise to "meandric permutations", a special subset of "alternate permutations". An alternate permutation of the set {1,2...,2"n"} is a cyclic permutation (with no fixed points) such that the digits in the cyclic notation form alternate between odd and even integers. Meandric permutations are useful in the analysis of RNA secondary structure. Not all alternate permutations are meandric. A modification of Heap's algorithm has been used to generate all alternate permutations of order "n" (that is, of length 2"n") without generating all (2"n")! permutations. Generation of these alternate permutations is needed before they are analyzed to determine if they are meandric or not.
The algorithm is recursive. The following table exhibits a step in the procedure. In the previous step, all alternate permutations of length 5 have been generated. Three copies of each of these have a "6" added to the right end, and then a different transposition involving this last entry and a previous entry in an even position is applied (including the identity, i.e., no transposition).
Software implementations.
Calculator functions.
Many scientific calculators and computing software have a built-in function for calculating the number of "k"-permutations of "n".
Spreadsheet functions.
Most spreadsheet software also provides a built-in function for calculating the number of "k"-permutations of "n", called PERMUT in many popular spreadsheets.
Applications.
Permutations are used in the interleaver component of the error detection and correction algorithms, such as turbo codes,
for example 3GPP Long Term Evolution mobile telecommunication standard uses these ideas (see 3GPP technical specification 36.212 ).
Such applications raise the question of fast generation of permutations satisfying certain desirable properties. One of the methods is based on the permutation polynomials. Also as a base for optimal hashing in Unique Permutation Hashing.

</doc>
<doc id="44028" url="https://en.wikipedia.org/wiki?curid=44028" title="National Security Advisor (United States)">
National Security Advisor (United States)

The Assistant to the President for National Security Affairs, commonly referred to as the National Security Advisor or at times informally termed the NSC advisor, is a senior aide in the Executive Office of the President, based at the West Wing of the White House, who serves as the chief in-house advisor to the President of the United States on national security issues.
The APNSA also participates in the meetings of the National Security Council and usually chairs the Principal Committee meetings with the Secretary of State and Secretary of Defense (i.e. the meetings not attended by the President). The APNSA is supported by the National Security Council staff that produces research and briefings for the APNSA to review and present, either to the National Security Council or directly to the President.
The current Assistant to the President for National Security Affairs is Susan Rice, who assumed the role on July 1, 2013.
Role.
The Assistant to the President for National Security Affairs (APNSA) is appointed by the President without confirmation by the Senate. The influence and role of the National Security Advisor varies from administration to administration; and depends, not only on the qualities of the person appointed to the position, but also on the style and management philosophy of the incumbent President. Ideally, the APNSA serves as an honest broker of policy options for the President in the field of national security, rather than as an advocate for his or her own policy agenda.
However, the APNSA is a staff position in the Executive Office of the President and does not have line or budget authority over either the Department of State nor the Department of Defense, unlike the Secretary of State and the Secretary of Defense, who are Senate-confirmed officials with statutory authority over their departments; but the APNSA is able to offer daily advice (due to the proximity) to the President independently of the vested interests of the large bureaucracies and clientele of those departments.
In times of crisis, the National Security Advisor is likely to operate from the White House Situation Room, or the Presidential Emergency Operations Center (as on September 11, 2001), updating the President on the latest events in a crisis situation.
History.
The National Security Council was created at the start of the Cold War under the National Security Act of 1947 to coordinate defense, foreign affairs, international economic policy, and intelligence; this was part of a large reorganization that saw the creation of the Department of Defense and the Central Intelligence Agency. In 1949, the NSC became part of the president's executive office. The National Security Act of 1947 did not create the position of the National Security Advisor per se, but it did create an executive secretary in charge of the staff.
Robert Cutler became the first National Security Advisor in 1953. The system has remained largely unchanged since then, particularly since Kennedy's time, with powerful National Security Advisors and strong staff but a lower importance given to formal NSC meetings. This continuity persists despite the tendency of each new president to replace the advisor and senior NSC staff.
Henry Kissinger, President Richard Nixon's National Security Advisor, enhanced the importance of the role, controlling the flow of information to the President and meeting him multiple times per day. Henry Kissinger also holds the distinction of serving as National Security Advisor and United States Secretary of State at the same time from September 22, 1973 until November 3, 1975.
List of National Security Advisors.
Brent Scowcroft is the only person to have held the job twice, in two different administrations: in the Ford administration and in the George H.W. Bush administration. Robert Cutler also held the job twice, both times under Dwight D. Eisenhower.

</doc>
<doc id="44029" url="https://en.wikipedia.org/wiki?curid=44029" title="Executive Office of the President">
Executive Office of the President

The Executive Office of the President (EOPOTUS or EOP) consists of the immediate staff of the current President of the United States and multiple levels of support staff reporting to the President. The EOP is headed by the White House Chief of Staff, currently Denis McDonough. The size of the White House staff has increased dramatically since 1939, and has grown to include an array of policy experts in various fields.
History.
In 1939, during Franklin D. Roosevelt's second term in office, the foundations of the modern White House staff were created. Based on the recommendations of a presidentially commissioned panel of political science and public administration experts that was known as the Brownlow Committee, Roosevelt was able to get Congress to approve the Reorganization Act of 1939. The Act led to Reorganization Plan No. 1, which created the EOP, which reported directly to the president. The EOP encompassed two subunits at its outset: the White House Office (WHO) and the Bureau of the Budget, the predecessor to today's Office of Management and Budget, which had been created in 1921 and originally located in the Treasury Department. It absorbed most of the functions of the National Emergency Council. Initially, the new staff system appeared more ambitious on paper than in practice; the increase in the size of the staff was quite modest at the start. But it laid the groundwork for the large and organizationally complex White House staff that would emerge during the presidencies of Roosevelt's successors.
Roosevelt's efforts are also notable in contrast to those of his predecessors in office. During the nineteenth century, presidents had few staff resources. Thomas Jefferson had one messenger and one secretary at his disposal, both of whose salaries were paid by the president personally. It was not until 1857 that Congress appropriated money ($2,500) for the hiring of one clerk. By Ulysses S. Grant's presidency (1869–1877), the staff had grown to three. By 1900, the White House staff included one "secretary to the president" (then the title of the president's chief aide), two assistant secretaries, two executive clerks, a stenographer, and seven other office personnel. Under Warren G. Harding, the size of the staff expanded to thirty-one, although most were clerical positions. During Herbert Hoover's presidency, two additional secretaries to the president were added by Congress, one of whom Hoover designated as his Press Secretary. From 1933 to 1939, even as he greatly expanded the scope of the federal government's policies and powers in response to the Great Depression, Roosevelt muddled through: his "brains trust" of top advisers, although working directly for the President, often were appointed to vacant positions in agencies and departments, whence they drew their salaries since the White House lacked statutory or budgetary authority to create new staff positions.
From 1939 through the present, the situation changed dramatically. New units within the EOP were created, some by statute, some by executive order of the president. Among the most important are the Council of Economic Advisers (1946), the National Security Council and its staff (1947), the Office of the U.S. Trade Representative (1963), the Council on Environmental Quality (1970), the Office of Science and Technology Policy (1976), the Office of Administration (1977), and the Office of National Drug Control Policy (1989). Under George W. Bush, additional units were added, such as the Office of Homeland Security (2001), which later became a Cabinet department, and the Office of Faith-based and Community Initiatives (2001). Precise estimates as to the size and budget of the EOP are difficult to come by. Many people who work on the staff are "detailed" from other federal departments and agencies, and budgetary expenses are often charged elsewhere, for example Defense Department staff for the White House Military Office. Ballpark estimates indicate some 2,000 to 2,500 persons serve in EOP staff positions with policy-making responsibilities, with a budget of $300 to $400 million (George W. Bush's budget request for Fiscal Year 2005 was for $341 million in support of 1,850 personnel).
Organization.
Senior staff within the Executive Office of the President have the title Assistant to the President, second-level staff have the title Deputy Assistant to the President, and third-level staff have the title Special Assistant to the President.
Very few EOP (Executive Office of the President) officials are required to be confirmed by the U.S. Senate, although there are a handful of exceptions to this rule (e.g., the Director of the Office of Management and Budget, the Chair and members of the Council of Economic Advisers, and the United States Trade Representative). The core White House Staff appointments do not require Senate approval. The staff of the Executive Office of the President is managed by the White House Chief of Staff.
The information in the following table is current as of April 12, 2014. Only principal executives are listed; for subordinate officers, see individual office pages.

</doc>
<doc id="44030" url="https://en.wikipedia.org/wiki?curid=44030" title="Office of Management and Budget">
Office of Management and Budget

The Office of Management and Budget (OMB) is the largest office within the Executive Office of the President of the United States (EOP). The main function of OMB is to produce the President's Budget. OMB also measures the quality of agency programs, policies, and procedures to see if they comply with the president's policies.
The current OMB Director is Shaun Donovan, who was nominated by the president following the nomination of Sylvia Mathews Burwell to become the United States Secretary of Health and Human Services. Donovan was confirmed by the Senate in a 75–22 vote.
History.
The Bureau of the Budget, OMB's predecessor, was established in 1921 as a part of the Department of the Treasury by the Budget and Accounting Act of 1921, which was signed into law by president Warren G. Harding. The Bureau of the Budget was moved to the Executive Office of the President in 1939 and was run by Harold D. Smith during the government's rapid expansion of spending during the Second World War. James L. Sundquist, a staffer at the Bureau of the Budget described the relationship between the President and the Bureau as extremely close and of subsequent Bureau Directors as politicians and not public administrators.
The Bureau was reorganized into the Office of Management and Budget in 1970 during the Nixon administration. The first OMB included Roy Ash (head), Paul O'Neill (assistant director), Fred Malek (deputy director) and Frank Zarb (associate director) and two dozen others. In 2008, the president earned $400,000 a year .The president's salary is set by Congress and cannot be changed during a president's term of office.
In the 1990s, OMB was reorganized to remove the distinction between management staff and budgetary staff by combining the dual roles into each given program examiner within the Resource Management Offices.
Purpose.
OMB prepares the President's budget proposal to Congress and supervises the administration of the executive branch agencies. OMB evaluates the effectiveness of agency programs, policies, and procedures, assesses competing funding demands among agencies, and sets funding priorities. OMB ensures that agency reports, rules, testimony, and proposed legislation are consistent with the president's budget and with administration policies.
OMB also oversees and coordinates the administration's procurement, financial management, information, and regulatory policies. In each of these areas, OMB's role is to help improve administrative management, to develop better performance measures and coordinating mechanisms, and to reduce any unnecessary burdens on the public.
OMB's critical missions are:
Structure.
Overview.
The Office is made up mainly of career appointed staff who provide continuity across changes of party and persons in the White House. Six positions within OMBthe Director, the Deputy Director, the Deputy Director for Management, and the administrators of the Office of Information and Regulatory Affairs, the Office of Federal Procurement Policy, and the Office of Federal Financial Management are presidentially appointed and Senate-confirmed positions.
The largest component of the Office of Management and Budget are the four Resource Management Offices which are organized along functional lines mirroring the U.S. federal government, each led by an OMB associate director. Approximately half of all OMB staff are assigned to these offices, the majority of whom are designated as program examiners. Program examiners can be assigned to monitor one or more federal agencies or may be deployed by a topical area, such as monitoring issues relating to U.S. Navy warships. These staff have dual responsibility for both management and budgetary issues, as well as responsibility for giving expert advice on all aspects relating to their programs. Each year they review federal agency budget requests and help decide what resource requests will be sent to Congress as part of the president's budget. They perform in-depth program evaluations using the Program Assessment Rating Tool, review proposed regulations, agency testimony, analyze pending legislation, and oversee the aspects of the president's management agenda including agency management scorecards. They are often called upon to provide analysis information to any EOP staff member. They also provide important information to those assigned to the statutory offices within OMB, which are Office of Information and Regulatory Affairs, the Office of Federal Procurement Policy, the Office of Federal Financial Management, and the Office of E-Government & Information Technology whose job it is to specialize in issues such as federal regulations or procurement policy and law.
Other offices are OMB-wide support offices which include the Office of General Counsel, the Office of Legislative Affairs, the Budget Review Division (BRD), and the Legislative Reference Division. The BRD performs government-wide budget coordination and is largely responsible for the technical aspects relating to the release of the president's budget each February. With respect to the estimation of spending for the executive branch, the BRD serves a purpose parallel to that of the Congressional Budget Office for the estimation of spending for Congress, the Department of the Treasury for the estimation of revenues for the executive branch, and the Joint Committee on Taxation for the estimation of revenues for Congress.
The Legislative Reference Division has the important role of being the central clearing house across the federal government for proposed legislation or testimony by federal officials. It distributes proposed legislation and testimony to all relevant federal reviewers and distils the comments into a consensus opinion of the Administration about the proposal. They are also responsible for writing an Enrolled Bill Memorandum to the president once a bill is presented by both bodies of Congress for the president's signature. The Enrolled Bill Memorandum details the particulars of the bill, opinions on the bill from relevant federal departments, and an overall opinion about whether the bill should be signed into law or vetoed. They also issue Statements of Administration Policy that let Congress know the White House's official position on proposed legislation.
List of directors.
Source:

</doc>
<doc id="44032" url="https://en.wikipedia.org/wiki?curid=44032" title="Corporation for Public Broadcasting">
Corporation for Public Broadcasting

The Corporation for Public Broadcasting (CPB) is an American non-profit corporation created by an act of the United States Congress and funded by the United States federal government to promote and help support public broadcasting. 
CPB’s mission is to ensure universal access to non-commercial, high-quality content and telecommunications services. It does so by distributing more than 70% of its funding to more than 1,400 locally owned public radio and television stations.
History.
The CPB was created on November 7, 1967, when U.S. president Lyndon B. Johnson signed the Public Broadcasting Act of 1967. The new organization initially collaborated with the then existing National Educational Television network. 
In March 27, 1968, CPB registers as a nonprofit corporation in the District of Columbia.
In 1969, the CPB talked to private groups to start the Public Broadcasting Service (PBS). 
In February 26, 1970, the CPB formed National Public Radio (NPR), a radio network consisting of public stations. Unlike PBS, NPR produces as well as distributes programming. 
In May 31, 2002, CPB, through a first round of funding from a special appropriation, helped public television stations making the transition (completed by 2009) to digital broadcasting.
Funding of and by the corporation.
The CPB's annual budget is composed almost entirely of an annual appropriation from Congress plus interest on those funds.
Ninety-five percent (95%) of CPB's appropriation goes directly to content development, community services, and other local station and system needs. 
For fiscal year 2014, its appropriation was US$445.5 million, including $.5M in interest earned. The distribution of these funds was as follows:
Public broadcasting stations are funded by a combination of private donations from listeners and viewers, foundations and corporations. Funding for public television comes in roughly equal parts from government (at all levels) and the private sector. 
Stations that receive CPB funds must meet certain requirements, such as the maintenance or provision of open meetings, open financial records, a community advisory board, equal employment opportunity, and lists of donors and political activities.
Political composition of the CPB Board.
The CPB is governed by a board of directors consisting of nine members. They are selected by the President of the United States, confirmed by the Senate, and serve six-year terms. As of May 2015, the board was composed of four Republicans and four Democrats. According to the Public Broadcasting Act of 1967, the president cannot appoint persons of the same political party to more than five of the nine CPB board seats.
<http://www.cpb.org/aboutpb/act//ref>
In 2004 and 2005, people from the PBS and NPR complained that the CPB was starting to push a conservative agenda. Board members replied that they were merely seeking balance. Polls of the PBS and NPR audiences in 2002 and 2003 indicated that few felt that the groups' news reports contained bias, and those that saw a slant were split as to which side they believed the reports favored.
The charge of a conservative agenda came to a head in 2005. Kenneth Tomlinson, chair of the CPB board from September 2003 until September 2005, angered PBS and NPR supporters by unilaterally commissioning a conservative colleague to conduct a study of alleged bias in the PBS show "NOW with Bill Moyers", and by appointing two conservatives as CPB Ombudsmen. On November 3, 2005, Tomlinson resigned from the board, prompted by a report of his tenure by the CPB Inspector General, Kenneth Konz, requested by Democrats in the U.S. House of Representatives. The report was made public on November 15. It states:
We found evidence that the Corporation for Public Broadcasting (CPB) former Chairman violated statutory provisions and the Director’s Code of Ethics by dealing directly with one of the creators of a new public affairs program during negotiations with the Public Broadcasting Service (PBS) and the CPB over creating the show. Our review also found evidence that suggests “political tests” were a major criteria used by the former Chairman in recruiting a President/Chief Executive Officer (CEO) for CPB, which violated statutory prohibitions against such practices.
Objectivity and balance requirements.
The Public Broadcasting Act of 1967 requires the CPB to operate with a "strict adherence to objectivity and balance in all programs or series of programs of a controversial nature". It also requires it to regularly review national programming for objectivity and balance, and to report on "its efforts to address concerns about objectivity and balance".

</doc>
<doc id="44035" url="https://en.wikipedia.org/wiki?curid=44035" title="Franco-Prussian War">
Franco-Prussian War

The Franco-Prussian War or Franco-German War (, lit. "German-French War", , lit. "Franco-German War"), often referred to in France as the War of 1870 (19 July 1871), was a conflict between the Second French Empire and the German states of the North German Confederation led by the Kingdom of Prussia. The conflict was caused by Prussian ambitions to extend German unification. Some historians argue that the Prussian chancellor Otto von Bismarck planned to provoke a French attack in order to draw the southern German states—Baden, Württemberg, Bavaria and Hesse-Darmstadt—into an alliance with the North German Confederation dominated by Prussia, while others contend that Bismarck did not plan anything and merely exploited the circumstances as they unfolded.
According to some historians, Bismarck adroitly created a diplomatic crisis over the succession to the Spanish throne, then edited a dispatch about a meeting between King William of Prussia and the French ambassador, to make it appear that the French had been insulted. The French press and parliament demanded a war, which the generals of Napoleon III assured him that France would win. Napoleon and his Prime Minister, Émile Ollivier, for their parts sought war to solve their problems with political disunity in France. On 16 July 1870, the French parliament voted to declare war on the German Kingdom of Prussia and hostilities began three days later. The German coalition mobilised its troops much more quickly than the French and rapidly invaded northeastern France. The German forces were superior in numbers, had better training and leadership and made more effective use of modern technology, particularly railroads and artillery.
A series of swift Prussian and German victories in eastern France, culminating in the Siege of Metz and the Battle of Sedan, saw the army of the Second Empire decisively defeated (Napoleon III had been captured at Sedan on 2 September). A Government of National Defence declared the Third Republic in Paris on 4 September and continued the war for another five months; the German forces fought and defeated new French armies in northern France. Following the Siege of Paris, the capital fell on 28 January 1871 and then a revolutionary uprising called the Paris Commune seized power in the capital and held it for two months, until it was bloodily suppressed by the regular French army at the end of May 1871.
The German states proclaimed their union as the German Empire under the Prussian king, Wilhelm I, uniting Germany as a nation-state. The Treaty of Frankfurt of 10 May 1871 gave Germany most of Alsace and some parts of Lorraine, which became the Imperial territory of Alsace-Lorraine ("Reichsland Elsaß-Lothringen").The German conquest of France and the unification of Germany upset the European balance of power, that had existed since the Congress of Vienna in 1815 and Otto von Bismarck maintained great authority in international affairs for two decades. French determination to regain Alsace-Lorraine and fear of another Franco-German war, along with British apprehension about the balance of power, became factors in the causes of World War I.
Causes.
The causes of the Franco-Prussian War are deeply rooted in the events surrounding the unification of Germany. In the aftermath of the Austro–Prussian War of 1866, Prussia had annexed numerous territories and formed the North German Confederation. This new power destabilized the European balance of power established by the Congress of Vienna in 1815 after the Napoleonic Wars. Napoleon III, then the emperor of France, demanded compensations in Belgium and on the left bank of the Rhine to secure France's strategic position, which the Prussian chancellor, Otto von Bismarck, flatly refused. Prussia then turned its attention towards the south of Germany, where it sought to incorporate the southern German kingdoms, Bavaria, Württemberg, Baden and Hesse-Darmstadt, into a unified Prussia-dominated Germany. France was strongly opposed to any further alliance of German states, which would have significantly strengthened the Prussian military.
In Prussia, some officials considered a war against France both inevitable and necessary to arouse German nationalism in those states that would allow the unification of a great German empire. This aim was epitomized by Prussian Chancellor Otto von Bismarck's later statement: "I did not doubt that a Franco-German war must take place before the construction of a United Germany could be realised." Bismarck also knew that France should be the aggressor in the conflict to bring the southern German states to side with Prussia, hence giving Germans numerical superiority. Many Germans also viewed the French as the traditional destabilizer of Europe, and sought to weaken France to prevent further breaches of the peace.
The immediate cause of the war resided in the candidacy of a Leopold of Hohenzollern-Sigmaringen, a Prussian prince, to the throne of Spain. France feared encirclement by an alliance between Prussia and Spain. The Hohenzollern prince's candidacy was withdrawn under French diplomatic pressure, but Otto von Bismarck goaded the French into declaring war by altering a telegram sent by William I. Releasing the Ems Dispatch to the public, Bismarck made it sound as if the king had treated the French envoy in a demeaning fashion, which inflamed public opinion in France.
Some historians argue that Napoleon III also sought war, particularly as a result of the diplomatic failure, in 1866, to obtain any concessions following the Austro-Prussian War, and he believed he would win a conflict with Prussia. They also argue that he wanted a war to resolve growing domestic political problems. Other historians, notably French historian Pierre Milza, dispute this. On 8 May 1870, shortly before the war, French voters had overwhelmingly supported Napoleon III's program in a national plebiscite, with 7,358,000 votes yes against 1,582,000 votes no, an increase of support of two million votes since the legislative elections in 1869. According to Milza, the Emperor had no need for a war to increase his popularity.
The Ems telegram had exactly the effect on French public opinion that Bismarck had intended. "This text produced the effect of a red flag on the Gallic bull", Bismarck later wrote. Gramont, the French foreign minister, declared that he felt "he had just received a slap". The leader of the monarchists in Parliament, Adolphe Thiers, spoke for moderation, arguing that France had won the diplomatic battle and there was no reason for war, but he was drowned out by cries that he was a traitor and a Prussian. Napoleon's new prime minister, Emile Ollivier, declared that France had done all that it could humanly and honorably do to prevent the war, and that he accepted the responsibility "with a light heart." A crowd of 15–20,000 people, carrying flags and patriotic banners, marched through the streets of Paris, demanding war. On 19 July 1870 a declaration of war was sent to the Prussian government. The southern German states immediately sided with Prussia.
Opposing forces.
The French Army consisted in peacetime of approximately 400,000 soldiers, some of them regulars, others conscripts who until 1869 served the comparatively long period of seven years with the colours. Some of them were veterans of previous French campaigns in the Crimean War, Algeria, the Franco-Austrian War in Italy, and in the Franco-Mexican War. However, following the "Seven Weeks War" between Prussia and Austria four years earlier, it had been calculated that the French Army could field only 288,000 men to face the Prussian Army when potentially 1,000,000 would be required. Under Marshal Adolphe Niel, urgent reforms were made. Universal conscription (rather than by ballot, as previously) and a shorter period of service gave increased numbers of reservists, who would swell the army to a planned strength of 800,000 on mobilisation. Those who for any reason were not conscripted were to be enrolled in the "Garde Mobile", a militia with a nominal strength of 400,000. However, the Franco-Prussian War broke out before these reforms could be completely implemented. The mobilisation of reservists was chaotic and resulted in large numbers of stragglers, while the "Garde Mobile" were generally untrained and often mutinous.
French infantry were equipped with the breech-loading Chassepot rifle, one of the most modern mass-produced firearms in the world at the time. With a rubber ring seal and a smaller bullet, the Chassepot had a maximum effective range of some with a short reloading time. French tactics emphasised the defensive use of the Chassepot rifle in trench-warfare style fighting—the so-called "feu de bataillon". The artillery was equipped with rifled, muzzle-loaded La Hitte guns. The army also possessed a precursor to the machine-gun: the mitrailleuse, which could unleash significant, concentrated firepower but nevertheless lacked range and was comparatively immobile, and thus prone to being easily overrun. The mitrailleuse was mounted on an artillery gun carriage and grouped in batteries in a similar fashion to cannon.
The army was nominally led by Napoleon III, with Marshals Francois Achille Bazaine and Patrice de Mac-Mahon in command of the field armies. However, there was no previously arranged plan of campaign in place. The only campaign plan prepared between 1866 and 1870 was a defensive one.
The Prussian Army was composed not of regulars but conscripts. Service was compulsory for all of men of military age, and thus Prussia and its North and South German allies could mobilise and field some 1,000,000 soldiers in time of war. German tactics emphasised encirclement battles like Cannae and using artillery offensively whenever possible. Rather than advancing in a column or line formation, Prussian infantry moved in small groups that were harder to target by artillery or French defensive fire. The sheer number of soldiers available made encirclement "en masse" and destruction of French formations relatively easy.
The army was still equipped with the Dreyse needle gun of Battle of Königgrätz fame, which was by this time showing the age of its 25-year-old design. The rifle had a range of only and lacked the rubber breech seal that permitted aimed shots. The deficiencies of the needle gun were more than compensated for by the famous Krupp 6-pounder (3 kg) steel breech-loading cannons being issued to Prussian artillery batteries. Firing a contact-detonated shell, the Krupp gun had a longer range and a higher rate of fire than the French bronze muzzle loading cannon, which relied on faulty time fuses.
The Prussian army was controlled by the General Staff, under Field Marshal Helmuth von Moltke. The Prussian army was unique in Europe for having the only such organisation in existence, whose purpose in peacetime was to prepare the overall war strategy, and in wartime to direct operational movement and organise logistics and communications. The officers of the General Staff were hand-picked from the Prussian "Kriegsakademie" (War Academy). Moltke embraced new technology, particularly the railroad and telegraph, to coordinate and accelerate mobilisation of large forces.
French Army incursion.
Preparations for the offensive.
On 28 July 1870 Napoleon III left Paris for Metz and assumed command of the newly titled Army of the Rhine, some 202,448 strong and expected to grow as the French mobilization progressed. Marshal MacMahon took command of I Corps (4 infantry divisions) near Wissembourg, Marshal François Canrobert brought VI Corps (four infantry divisions) to Châlons-sur-Marne in northern France as a reserve and to guard against a Prussian advance through Belgium.
A pre-war plan laid out by the late Marshal Niel called for a strong French offensive from Thionville towards Trier and into the Prussian Rhineland. This plan was discarded in favour of a defensive plan by Generals Charles Frossard and Bartélemy Lebrun, which called for the Army of the Rhine to remain in a defensive posture near the German border and repel any Prussian offensive. As Austria along with Bavaria, Württemberg and Baden were expected to join in a revenge war against Prussia, I Corps would invade the Bavarian Palatinate and proceed to "free" the South German states in concert with Austro-Hungarian forces. VI Corps would reinforce either army as needed.
Unfortunately for Frossard's plan, the Prussian army was mobilizing far more rapidly than expected. The Austro-Hungarians, still smarting after their defeat by Prussia in the Austro-Prussian War, were treading carefully before stating that they would only commit to France's cause if the southern Germans viewed the French positively. This did not materialize as the South German states had come to Prussia's aid and were mobilizing their armies against France.
Occupation of Saarbrücken.
Napoleon III was under immense domestic pressure to launch an offensive before the full might of Moltke's forces was mobilized and deployed. Reconnaissance by Frossard's forces had identified only the Prussian 16th Infantry Division guarding the border town of Saarbrücken, right before the entire Army of the Rhine. Accordingly, on 31 July the Army marched forward toward the Saar River to seize Saarbrücken.
General Frossard's II Corps and Marshal Bazaine's III Corps crossed the German border on 2 August, and began to force the Prussian 40th Regiment of the 16th Infantry Division from the town of Saarbrücken with a series of direct attacks. The Chassepot rifle proved its worth against the Dreyse rifle, with French riflemen regularly outdistancing their Prussian counterparts in the skirmishing around Saarbrücken. However the Prussians resisted strongly, and the French suffered 86 casualties to the Prussian 83 casualties. Saarbrücken also proved to be a major obstacle in terms of logistics. Only one railway there led to the German hinterland but could be easily defended by a single force, and the only river systems in the region ran along the border instead of inland. While the French hailed the invasion as the first step towards the Rhineland and later Berlin, General Le Bœuf and Napoleon III were receiving alarming reports from foreign news sources of Prussian and Bavarian armies massing to the southeast in addition to the forces to the north and northeast.
Moltke had indeed massed three armies in the area—the Prussian First Army with 50,000 men, commanded by General Karl von Steinmetz opposite Saarlouis, the Prussian Second Army with 134,000 men commanded by Prince Friedrich Karl opposite the line Forbach–Spicheren, and the Prussian Third Army with 120,000 men commanded by Crown Prince Friedrich Wilhelm, poised to cross the border at Wissembourg.
Prussian Army advance.
Battle of Wissembourg.
Upon learning from captured Prussian soldiers and a local area police chief, that the Prussian Crown Prince's Third Army was just from Saarbrücken near the town of Wissembourg, General Le Bœuf and Napoleon III decided to retreat to defensive positions. General Frossard, without instructions, hastily withdrew the elements of the Army of the Rhine in Saarbrücken back to Spicheren and Forbach.
Marshal MacMahon, now closest to Wissembourg, spread his four divisions over to react to any Prussian invasion. This organization of forces was due to a lack of supplies, forcing each division to seek out basic provisions along with the representatives of the army supply arm that was supposed to aid them. What made a bad situation much worse was the conduct of General Auguste-Alexandre Ducrot, commander of the 1st Division. He told General Abel Douay, commander of the 2nd Division, on 1 August that ""The information I have received makes me suppose that the enemy has no considerable forces very near his advance posts, and has no desire to take the offensive"". Two days later, he told MacMahon that he had not found ""a single enemy post ... it looks to me as if the menace of the Bavarians is simply bluff"". Even though Ducrot shrugged off the possibility of an attack by the Germans, MacMahon tried to warn the other divisions of his army, without success.
The first action of the Franco-Prussian War took place on 4 August 1870. This battle saw the unsupported division of General Douay of I Corps, with some attached cavalry, which was posted to watch the border, attacked in overwhelming but uncoordinated fashion by the German 3rd Army. During the day, elements of a Bavarian and two Prussian corps became engaged and were aided by Prussian artillery, which blasted holes in the defenses of the town. Douay held a very strong position initially, thanks to the accurate long-range fire of the Chassepots but his force was too thinly stretched to hold it. Douay was killed in the late morning when a caisson of the divisional mitrailleuse battery exploded near him; the encirclement of the town by the Prussians threatened the French avenue of retreat.
The fighting within the town had become extremely intense, becoming a door to door battle of survival. Despite a never-ending attack of Prussian infantry, the soldiers of the 2nd Division kept to their positions. The people of the town of Wissembourg finally surrendered to the Germans. The French troops who did not surrender retreated westward, leaving behind and wounded and another and all of their remaining ammunition. The final attack by the Prussian troops also cost The German cavalry then failed to pursue the French and lost touch with them. The attackers had an initial superiority of numbers, a broad deployment which made envelopment highly likely but the effectiveness of French Chassepot rifle-fire inflicted costly repulses on infantry attacks, until the French infantry had been extensively bombarded by the Prussian artillery.
Battle of Spicheren.
The Battle of Spicheren, on 5 August, was the second of three critical French defeats. Moltke had originally planned to keep Bazaine's army on the Saar River until he could attack it with the 2nd Army in front and the 1st Army on its left flank, while the 3rd Army closed towards the rear. The aging General von Steinmetz made an overzealous, unplanned move, leading the 1st Army south from his position on the Moselle. He moved straight toward the town of Spicheren, cutting off Prince Frederick Charles from his forward cavalry units in the process.
On the French side, planning after the disaster at Wissembourg had become essential. General Le Bœuf, flushed with anger, was intent upon going on the offensive over the Saar and countering their loss. However, planning for the next encounter was more based upon the reality of unfolding events rather than emotion or pride, as Intendant General Wolff told him and his staff that supply beyond the Saar would be impossible. Therefore, the armies of France would take up a defensive position that would protect against every possible attack point, but also left the armies unable to support each other.
While the French army under General MacMahon engaged the German 3rd Army at the Battle of Wörth, the German 1st Army under Steinmetz finished their advance west from Saarbrücken. A patrol from the German 2nd Army under Prince Friedrich Karl of Prussia spotted decoy fires close and Frossard's army farther off on a distant plateau south of the town of Spicheren, and took this as a sign of Frossard's retreat. Ignoring Moltke's plan again, both German armies attacked Frossard's French 2nd Corps, fortified between Spicheren and Forbach.
The French were unaware of German numerical superiority at the beginning of the battle as the German 2nd Army did not attack all at once. Treating the oncoming attacks as merely skirmishes, Frossard did not request additional support from other units. By the time he realized what kind of a force he was opposing, it was too late. Seriously flawed communications between Frossard and those in reserve under Bazaine slowed down so much that by the time the reserves received orders to move out to Spicheren, German soldiers from the 1st and 2nd armies had charged up the heights. Because the reserves had not arrived, Frossard erroneously believed that he was in grave danger of being outflanked as German soldiers under General von Glume were spotted in Forbach. Instead of continuing to defend the heights, by the close of battle after dusk he retreated to the south. The German casualties were relatively high due to the advance and the effectiveness of the chassepot rifle. They were quite startled in the morning when they had found out that their efforts were not in vain—Frossard had abandoned his position on the heights.
Battle of Wörth.
The Battle of Wörth (also known as Fröschwiller or Reichshoffen) began when the two armies clashed again on 6 August near Wörth in the town of Fröschwiller, about from Wissembourg. The Crown Prince of Prussia's 3rd army had, on the quick reaction of his Chief of Staff General von Blumenthal, drawn reinforcements which brought its strength up to 140,000 troops. The French had been slowly reinforced and their force numbered only 35,000. Although badly outnumbered, the French defended their position just outside Fröschwiller. By afternoon, the Germans had suffered or wounded and the French had lost a similar number of casualties and another taken prisoner, a loss of about 50%. The Germans captured Fröschwiller which sat on a hilltop in the centre of the French line. Having lost any hope for victory and facing a massacre, the French army disengaged and retreated in a westerly direction towards Bitche and Saverne, hoping to join French forces on the other side of the Vosges mountains. The German 3rd army did not pursue the French but remained in Alsace and moved slowly south, attacking and destroying the French garrisons in the vicinity.
Battle of Mars-La-Tour.
About 160,000 French soldiers were besieged in the fortress of Metz following the defeats on the frontier. A retirement from Metz to link up with French forces at Châlons, was ordered on 15 August and spotted by a Prussian cavalry patrol under Major Oskar von Blumenthal. Next day a grossly outnumbered Prussian force of 30,000 men of III Corps (of the 2nd Army) under General Konstantin von Alvensleben, found the French Army near Vionville, east of Mars-la-Tour.
Despite odds of four to one, the III Corps launched a risky attack. The French were routed and the III Corps captured Vionville, blocking any further escape attempts to the west. Once blocked from retreat, the French in the fortress of Metz had no choice but to engage in a fight that would see the last major cavalry engagement in Western Europe. The battle soon erupted, and III Corps was shattered by incessant cavalry charges, losing over half its soldiers. The German Official History recorded and French casualties of 
On 16 August, the French had a chance to sweep away the key Prussian defense, and to escape. Two Prussian corps had attacked the French advanced guard, thinking that it was the rearguard of the retreat of the French Army of the Meuse. Despite this misjudgment the two Prussian corps held the entire French army for the whole day. Outnumbered 5 to 1, the extraordinary élan of the Prussians prevailed over gross indecision by the French. The French had lost the opportunity to win a decisive victory.
Battle of Gravelotte.
The Battle of Gravelotte, or Gravelotte–St. Privat (18 August), was the largest battle during the Franco-Prussian War. It was fought about west of Metz, where on the previous day, having intercepted the French army's retreat to the west at the Battle of Mars-La-Tour, the Prussians were now closing in to complete the destruction of the French forces. The combined German forces, under Field Marshal Count Helmuth von Moltke, were the Prussian First and Second Armies of the North German Confederation numbering about 210 infantry battalions, 133 cavalry squadrons, and 732 heavy cannons totaling 188,332 officers and men. The French Army of the Rhine, commanded by Marshal François-Achille Bazaine, numbering about 183 infantry battalions, 104 cavalry squadrons, backed by 520 heavy cannons, totaling 112,800 officers and men, dug in along high ground with their southern left flank at the town of Rozerieulles, and their northern right flank at St. Privat.
On 18 August, the battle began when at 08:00 Moltke ordered the First and Second Armies to advance against the French positions. By 12:00, General Manstein opened up the battle before the village of Amanvillers with artillery from the 25th Infantry Division. But the French had spent the night and early morning digging trenches and rifle pits while placing their artillery and their mitrailleuses in concealed positions. Finally aware of the Prussian advance, the French opened up a massive return fire against the mass of advancing Germans. The battle at first appeared to favor the French with their superior Chassepot rifle. However, the Prussian artillery was superior with the all-steel Krupp breech-loading gun. By 14:30, General Steinmetz, the commander of the First Army, unilaterally launched his VIII Corps across the Mance Ravine in which the Prussian infantry were soon pinned down by murderous rifle and mitrailleuse fire from the French positions. At 15:00, the massed guns of the VII and VIII Corps opened fire to support the attack. But by 16:00, with the attack in danger of stalling, Steinmetz ordered the VII Corps forward, followed by the 1st Cavalry Division.
By 16:50, with the Prussian southern attacks in danger of breaking up, the Prussian 3rd Guards Infantry Brigade of the Second Army opened an attack against the French positions at St. Privat which were commanded by General Canrobert. At 17:15, the Prussian 4th Guards Infantry Brigade joined the advance followed at 17:45 by the Prussian 1st Guards Infantry Brigade. All of the Prussian Guard attacks were pinned down by lethal French gunfire from the rifle pits and trenches. At 18:15 the Prussian 2nd Guards Infantry Brigade, the last of the 1st Guards Infantry Division, was committed to the attack on St. Privat while Steinmetz committed the last of the reserves of the First Army across the Mance Ravine. By 18:30, a considerable portion of the VII and VIII Corps disengaged from the fighting and withdrew towards the Prussian positions at Rezonville.
With the defeat of the First Army, Prince Frederick Charles ordered a massed artillery attack against Canrobert's position at St. Privat to prevent the Guards attack from failing too. At 19:00 the 3rd Division of Fransecky's II Corps of the Second Army advanced across Ravine while the XII Corps cleared out the nearby town of Roncourt and with the survivors of the 1st Guards Infantry Division launched a fresh attack against the ruins of St. Privat. At 20:00, the arrival of the Prussian 4th Infantry Division of the II Corps and with the Prussian right flank on Mance Ravine, the line stabilised. By then, the Prussians of the 1st Guards Infantry Division and the XII and II Corps captured St. Privat forcing the decimated French forces to withdraw. With the Prussians exhausted from the fighting, the French were now able to mount a counter-attack. General Bourbaki, however, refused to commit the reserves of the French Old Guard to the battle because, by that time, he considered the overall situation a 'defeat'. By 22:00, firing largely died down across the battlefield for the night. The next morning, the French Army of the Rhine, rather than resume the battle with an attack of its own against the battle-weary German armies, retreated to Metz where they were besieged and forced to surrender two months later.
The casualties were horrible, especially for the attacking Prussian forces. A grand total of 20,163 German troops were killed, wounded or missing in action during the August 18 battle. The French losses were 7,855 killed and wounded along with 4,420 prisoners of war (half of them were wounded) for a total of 12,275. While most of the Prussians fell under the French Chassepot rifles, most French fell under the Prussian Krupp shells. In a breakdown of the casualties, Frossard's II Corps of the Army of the Rhine suffered 621 casualties while inflicting 4,300 casualties on the Prussian First Army under Steinmetz before the Pointe du Jour. The Prussian Guards Infantry Divisions losses were even more staggering with 8,000 casualties out of 18,000 men. The Special Guards Jäger lost 19 officers, a surgeon and 431 men out of a total of 700. The 2nd Guards Infantry Brigade lost 39 officers and 1,076 men. The 3rd Guards Infantry Brigade lost 36 officers and 1,060 men. On the French side, the units holding St. Privat lost more than half their number in the village.
Siege of Metz.
With the defeat of Marshal Bazaine's Army of the Rhine at Gravelotte, the French were forced to retire to Metz, where they were besieged by over 150,000 Prussian troops of the First and Second Armies. Napoleon III and MacMahon formed the new French Army of Châlons, to march on to Metz to rescue Bazaine. Napoleon III personally led the army with Marshal MacMahon in attendance. The Army of Châlons marched northeast towards the Belgian border to avoid the Prussians before striking south to link up with Bazaine. The Prussians, under the command of Field Marshal Count Helmuth von Moltke, took advantage of this maneuver to catch the French in a pincer grip. He left the Prussian First and Second Armies besieging Metz, except three corps detached to form the Army of the Meuse under the Crown Prince of Saxony. With this army and the Prussian Third Army, Moltke marched northward and caught up with the French at Beaumont on 30 August. After a sharp fight in which they lost 5,000 men and 40 cannons, the French withdrew toward Sedan. Having reformed in the town, the Army of Châlons was immediately isolated by the converging Prussian armies. Napoleon III ordered the army to break out of the encirclement immediately. With MacMahon wounded on the previous day, General Auguste Ducrot took command of the French troops in the field.
Battle of Sedan.
On 1 September 1870, the battle opened with the Army of Châlons, with 202 infantry battalions, 80 cavalry squadrons and 564 guns, attacking the surrounding Prussian Third and Meuse Armies totaling 222 infantry battalions, 186 cavalry squadrons and 774 guns. General De Wimpffen, the commander of the French V Corps in reserve, hoped to launch a combined infantry and cavalry attack against the Prussian XI Corps. But by 11:00, Prussian artillery took a toll on the French while more Prussian troops arrived on the battlefield. The French cavalry, commanded by General Marguerite, launched three desperate attacks on the nearby village of Floing where the Prussian XI Corps was concentrated. Marguerite was killed leading the very first charge and the two additional charges led to nothing but heavy losses. By the end of the day, with no hope of breaking out, Napoleon III called off the attacks. The French lost over 17,000 men, killed or wounded, with 21,000 captured. The Prussians reported their losses at 2,320 killed, 5,980 wounded and 700 captured or missing. By the next day, on 2 September, Napoleon III surrendered and was taken prisoner with 104,000 of his soldiers. It was an overwhelming victory for the Prussians, for they not only captured an entire French army, but the leader of France as well. The defeat of the French at Sedan had decided the war in Prussia's favour. One French army was now immobilised and besieged in the city of Metz, and no other forces stood on French ground to prevent a German invasion. Nevertheless, the war would continue.
The war of the Government of National Defence.
Government of National Defence.
When the news arrived at Paris of the surrender at Sedan of Napoleon III and the Second Empire was overthrown by a popular uprising in Paris, which forced the proclamation of a Provisional Government and a Third Republic by general Trochu, Favre and Gambetta at Paris on 4 September, the new government calling itself the Government of National Defence. After the German victory at Sedan, most of the French standing army was either besieged in Metz or prisoner of the Germans, who hoped for an armistice and an end to the war. Bismarck wanted an early peace but had difficulty in finding a legitimate French authority with which to negotiate. The Government of National Defence had no electoral mandate, the Emperor was a captive and the Empress in exile but there had been no abdication "de jure" and the army was still bound by an oath of allegiance to the defunct imperial régime.
The Germans expected to negotiate an end to the war but immediately ordered an advance on Paris; by 15 September Moltke issued the orders for an investment of Paris and on 20 September the encirclement was complete. Bismarck met Favre on 18 September at the Château de Ferrières and demanded a frontier immune to a French war of revenge, which included Strasbourg, Alsace and most the Moselle department in Lorraine of which Metz was the capital. In return for an armistice for the French to elect a National Assembly, Bismarck demanded the surrender of Strasbourg and the fortress city of Toul. To allow supplies into Paris, one of the perimeter forts had to be handed over. Favre was unaware that the real aim of Bismarck in making such extortionate demands was to establish a durable peace on the new western frontier of Germany, preferably by a peace with a friendly government, on terms acceptable to French public opinion. An impregnable military frontier was an inferior alternative to him, favoured only by the militant nationalists on the German side.
While the republican government was amenable to war reparations or ceding colonial territories in Africa or in South East Asia to Prussia, Favre on behalf of the Government of National Defense, declared on 6 September that France would not "yield an inch of its territory nor a stone of its fortresses." The republic then renewed the declaration of war, called for recruits in all parts of the country and pledged to drive the German troops out of France by a . Under these circumstances, the Germans had to continue the war, yet could not pin down any proper military opposition in their vicinity. As the bulk of the remaining French armies were digging-in near Paris, the German leaders decided to put pressure upon the enemy by attacking Paris. By September 15, German troops reached the outskirts of the fortified city. On September 19, the Germans surrounded it and erected a blockade, as already established at Metz.
When the war had begun, European public opinion heavily favored the Germans; many Italians attempted to sign up as volunteers at the Prussian embassy in Florence and a Prussian diplomat visited Giuseppe Garibaldi in Caprera. Bismarck's demand for the return of Alsace caused a dramatic shift in that sentiment in Italy, which was best exemplified by the reaction of Garibaldi soon after the revolution in Paris, who told the "Movimento" of Genoa on 7 September 1870 that "Yesterday I said to you: war to the death to Bonaparte. Today I say to you: rescue the French Republic by every means." Garibaldi went to France and assumed command of the Army of the Vosges, with which he operated around Dijon till the end of the war.
Siege of Paris.
Prussian forces commenced the Siege of Paris on 19 September 1870. Faced with the blockade, the new French government called for the establishment of several large armies in the French provinces. These new bodies of troops were to march towards Paris and attack the Germans there from various directions at the same time. Armed French civilians were to create a guerilla force—the so-called "Francs-tireurs"—for the purpose of attacking German supply lines.
These developments prompted calls from the German public for a bombardment of the city. von Blumenthal, who commanded the siege, was opposed to the bombardment on moral grounds. In this he was backed by other senior military figures such as the Crown Prince and Moltke.
Loire campaign.
Dispatched from Paris as the republican government emissary, Léon Gambetta flew over the German lines in a balloon inflated with coal gas from the city's gasworks and organized the recruitment of the Armée de la Loire. Rumors about an alleged German "extermination" plan infuriated the French and strengthened their support of the new regime. Within a few weeks, five new armies totalling more than 500,000 troops were recruited.
The Germans dispatched some of their troops to the French provinces to detect, attack and disperse the new French armies before they could become a menace. The Germans were not prepared for an occupation of the whole of France.
On 10 October, hostilities began between German and French republican forces near Orléans. At first, the Germans were victorious but the French drew reinforcements and defeated the Germans at the Battle of Coulmiers on 9 November. After the surrender of Metz, more than 100,000 well-trained and experienced German troops joined the German 'Southern Army'. The French were forced to abandon Orléans on 4 December, and were finally defeated at the Battle of Le Mans . A second French army which operated north of Paris was turned back at the Battle of Amiens (27 November), the Battle of Bapaume (3 January 1871) and the Battle of St. Quentin (13 January).
Northern campaign.
Following the Army of the Loire's defeats, Gambetta turned to General Faidherbe's Army of the North. The army had achieved several small victories at towns such as Ham, La Hallue, and Amiens and was protected by the belt of fortresses in northern France, allowing Faidherbe's men to launch quick attacks against isolated Prussian units, then retreat behind the fortresses. Despite access to the armaments factories of Lille, the Army of the North suffered from severe supply difficulties, which depressed morale. In January 1871, Gambetta forced Faidherbe to march his army beyond the fortresses and engage the Prussians in open battle. The army was severely weakened by low morale, supply problems, the terrible winter weather and low troop quality, whilst general Faidherbe was unable to command due to his poor health, the result of decades of campaigning in West Africa. At the Battle of St. Quentin, the Army of the North suffered a crushing defeat and was scattered, releasing thousands of Prussian soldiers to be relocated to the East.
Eastern campaign.
Following the destruction of the French Army of the Loire, remnants of the Loire army gathered in eastern France to form the Army of the East, commanded by general Charles-Denis Bourbaki. In a final attempt to cut the German supply lines in northeast France, Bourbaki's army marched north to attack the Prussian siege of Belfort and relieve the defenders.
In the battle of the Lisaine, Bourbaki's men failed to break through German lines commanded by General August von Werder. Bringing in the German 'Southern Army', General von Manteuffel then drove Bourbaki's army into the mountains near the Swiss border. Facing annihilation, the last intact French army crossed the border and was disarmed and interned by the neutral Swiss near Pontarlier (1 February).
Armistice.
On 28 January 1871 the Government of National Defence based in Paris negotiated an armistice with the Prussians. With Paris starving, and Gambetta's provincial armies reeling from one disaster after another, French foreign minister Favre went to Versailles on 24 January to discuss peace terms with Bismarck. Bismarck agreed to end the siege and allow food convoys to immediately enter Paris (including trains carrying millions of German army rations), on condition that the Government of National Defence surrender several key fortresses outside Paris to the Prussians. Without the forts, the French Army would no longer be able to defend Paris.
Although public opinion in Paris was strongly against any form of surrender or concession to the Prussians, the Government realised that it could not hold the city for much longer, and that Gambetta's provincial armies would probably never break through to relieve Paris. President Trochu resigned on 25 January and was replaced by Favre, who signed the surrender two days later at Versailles, with the armistice coming into effect at midnight. Several sources claim that in his carriage on the way back to Paris, Favre broke into tears, and collapsed into his daughter's arms as the guns around Paris fell silent at midnight. At Tours, Gambetta received word from Paris on 30 January that the Government had surrendered. Furious, he refused to surrender and launched an immediate attack on German forces at Orleans which, predictably, failed. A delegation of Parisian diplomats arrived in Tours by train on 5 February to negotiate with Gambetta, and the following day Gambetta stepped down and surrendered control of the provincial armies to the Government of National Defence, which promptly ordered a cease-fire across France.
The war at sea.
Blockade.
When the war began, the French government ordered a blockade of the North German coasts, which the small North German navy ("Norddeutsche Bundesmarine") with only five ironclads could do little to oppose. For most of the war, the three largest German ironclads were out of service with engine troubles; only the turret ship was available to conduct operations. By the time engine repairs had been completed, the French fleet had already departed. The blockade proved only partially successful due to crucial oversights by the planners in Paris. Reservists that were supposed to be at the ready in case of war, were working in the Newfoundland fisheries or in Scotland. Only part of the 470-ship French Navy put to sea on 24 July. Before long, the French navy ran short of coal, needing per day and having a bunker capacity in the fleet of only . A blockade of Wilhelmshaven failed and conflicting orders about operations in the Baltic Sea or a return to France, made the French naval efforts futile. Spotting a blockade-runner became unwelcome because of the ; pursuit of Prussian ships quickly depleted the coal reserves of the French ships.
To relieve pressure from the expected German attack into Alsace-Lorraine, Napoleon III and the French high command planned a seaborne invasion of northern Germany as soon as war began. The French expected the invasion to divert German troops and to encourage Denmark to join in the war, with its 50,000-strong army and the Royal Danish Navy. It was discovered that Prussia had recently built defences around the big North German ports, including coastal artillery batteries with Krupp heavy artillery, which with a range of , had double the range of French naval guns. The French Navy lacked the heavy guns to engage the coastal defences and the topography of the Prussian coast made a seaborne invasion of northern Germany impossible.
The French Marines and naval infantry intended for the invasion of northern Germany were dispatched to reinforce the French Army of Châlons and fell into captivity at Sedan along with Napoleon III. A shortage of officers, following the capture of most of the professional French army at the Siege of Metz and at the Battle of Sedan, led naval officers to be sent from their ships to command hastily assembled reservists of the "Garde Mobile". As the autumn storms of the North Sea forced the return of more of the French ships, the blockade of the north German ports diminished and in September 1870 the French navy abandoned the blockade for the winter. The rest of the navy retired to ports along the English Channel and remained in port for the rest of the war.
Pacific and Caribbean.
Outside Europe, the French corvette "Dupleix" blockaded the German corvette in Nagasaki and the Battle of Havana took place between the Prussian gunboat and the French aviso "Bouvet" off Havana, Cuba, in November 1870.
Aftermath.
Analysis.
The quick German victory over the French stunned neutral observers, many of whom had expected a French victory and most of whom had expected a long war. The strategic advantages possessed by the Germans were not appreciated outside Germany until after hostilities had ceased. Other countries quickly discerned the advantages given to the Germans by their military system, and adopted many of their innovations, particularly the General Staff, universal conscription and highly detailed mobilization systems.
The Prussian General Staff developed by Moltke proved to be extremely effective, in contrast to the traditional French school. This was in large part due to the fact that the Prussian General Staff was created to study previous Prussian operations and learn to avoid mistakes. The structure also greatly strengthened Moltke's ability to control large formations spread out over significant distances. The Chief of the General Staff, effectively the commander in chief of the Prussian army, was independent of the minister of war and answered only to the monarch. The French General Staff—along with those of every other European military—was little better than a collection of assistants for the line commanders. This disorganization hampered the French commanders' ability to exercise control of their forces.
In addition, the Prussian military education system was superior to the French model; Prussian staff officers were trained to exhibit initiative and independent thinking. Indeed, this was Moltke's expectation. The French, meanwhile, suffered from an education and promotion system that stifled intellectual development. According to the military historian Dallas Irvine, the system "was almost completely effective in excluding the army's brain power from the staff and high command. To the resulting lack of intelligence at the top can be ascribed all the inexcusable defects of French military policy."
Albrecht von Roon, the Prussian Minister of War from 1859 to 1873, put into effect a series of reforms of the Prussian military system in the 1860s. Among these were two major reforms that substantially increased the military power of Germany. The first was a reorganization of the army that integrated the regular army and the "Landwehr" reserves. The second was the provision for the conscription of every male Prussian of military age in the event of mobilization. Thus, despite the population of France being greater than the population of all of the German states that participated in the war, the Germans mobilized more soldiers for battle.
At the outset of the Franco-Prussian War, 462,000 German soldiers concentrated on the French frontier while only 270,000 French soldiers could be moved to face them, the French army having lost 100,000 stragglers before a shot was fired through poor planning and administration. This was partly due to the peacetime organisations of the armies. Each Prussian Corps was based within a "Kreis" (literally "circle") around the chief city in an area. Reservists rarely lived more than a day's travel from their regiment's depot. By contrast, French regiments generally served far from their depots, which in turn were not in the areas of France from which their soldiers were drawn. Reservists often faced several days' journey to report to their depots, and then another long journey to join their regiments. Large numbers of reservists choked railway stations, vainly seeking rations and orders.
The effect of these differences was accentuated by the pre-war preparations. The Prussian General Staff had drawn up minutely detailed mobilization plans using the railway system, which in turn had been partly laid out in response to recommendations of a Railway Section within the General Staff. The French railway system, with multiple competing companies, had developed purely from commercial pressures and many journeys to the front in Alsace and Lorraine involved long diversions and frequent changes between trains. Furthermore, no system had been put in place for military control of the railways, and officers simply commandeered trains as they saw fit. Rail sidings and marshalling yards became choked with loaded wagons, with nobody responsible for unloading them or directing them to the destination.
Although Austria-Hungary and Denmark had both wished to avenge their recent military defeats against Prussia, they chose not to intervene in the war due to a lack of confidence in the French. Napoleon III also failed to cultivate alliances with the Russian Empire and the United Kingdom, partially due to the diplomatic efforts of the Prussian chancellor Otto von Bismarck, and thus faced the German states alone.
The French breech-loading rifle, the Chassepot, had a far longer range than the German needle gun; compared to . The French also had an early machine-gun type weapon, the mitrailleuse, which could fire its thirty-seven barrels at a range of around . It was developed in such secrecy, that little training with the weapon had occurred, leaving French gunners with no experience; the gun was treated like artillery and in this role it was ineffective. Worse still, once the small number of soldiers who had been trained how to use the new weapon became casualties, there were no replacements who knew how to operate the mitrailleuse.
The French were equipped with bronze, rifled muzzle-loading artillery, while the Prussians used new steel breech-loading guns, which had a far longer range and a faster rate of fire. Prussian gunners strove for a high rate of fire, which was discouraged in the French army in the belief that it wasted ammunition. In addition, the Prussian artillery batteries had 30% more guns than their French counterparts. The Prussian guns typically opened fire at a range of , beyond the range of French artillery or the Chassepot rifle. The Prussian batteries could thus destroy French artillery with impunity, before being moved forward to directly support infantry attacks.
Effects on military thought.
The events of the Franco-Prussian War had great influence on military thinking over the next forty years. Lessons drawn from the war included the need for a general staff system, the scale and duration of future wars and the tactical use of artillery and cavalry. The bold use of artillery by the Prussians, to silence French guns at long range and then to directly support infantry attacks at close range, proved to be superior to the defensive doctrine employed by French gunners. The Prussian tactics were adopted by European armies by 1914, exemplified in the French 75, an artillery piece optimised to provide direct fire support to advancing infantry. Most European armies ignored the evidence of the Russo-Japanese War of which suggested that infantry armed with new smokeless-powder rifles could engage gun crews effectively. This forced gunners to fire at longer range using indirect fire, usually from a position of cover.
At the Battle of Mars-la-Tours, the Prussian 12th Cavalry Brigade, commanded by General Adalbert von Bredow, conducted a charge against a French artillery battery. The attack was a costly success and came to be known as "von Bredow's Death Ride", which was held to prove that cavalry charges could still prevail on the battlefield. Use of traditional cavalry on the battlefields of 1914 proved to be disastrous, due to accurate, long-range rifle fire, machine-guns and artillery. Von Bredow's attack had succeeded only because of an unusually effective artillery bombardment just before the charge, along with favorable terrain that masked his approach.
Subsequent events.
Prussian reaction and withdrawal.
The Prussian Army, under the terms of the armistice, held a brief victory parade in Paris on 17 February; the city was silent and draped with black and the Germans quickly withdrew. Bismarck honoured the armistice, by allowing train loads of food into Paris and withdrawing Prussian forces to the east of the city, prior to a full withdrawal once France agreed to pay a five billion franc war indemnity. At the same time, Prussian forces were concentrated in the provinces of Alsace and Lorraine. An exodus occurred from Paris as some 200,000 people, predominantly middle-class, went to the countryside.
The Paris Commune.
During the war, the Paris National Guard, particularly in the working-class neighbourhoods of Paris, had become highly politicised and units elected officers; many refused to wear uniforms or obey commands from the national government. National guard units tried to seize power in Paris on 31 October 1870 and 22 January 1871. On 18 March 1871, when the regular army tried to remove cannons from an artillery park on Montmartre, National Guard units resisted and killed two army generals. The national government and regular army forces retreated to Versailles and a revolutionary government was proclaimed in Paris. A commune was elected, which was dominated by socialists, anarchists and revolutionaries. The red flag replaced the French tricolour and a civil war began between the Commune and the regular army, which attacked and recaptured Paris from in the (bloody week).
During the fighting, the Communards killed including the Archbishop of Paris, and burned down many government buildings, including the Tuileries Palace and the Hotel de Ville. Communards captured with weapons were routinely shot by the army and Government troops killed between 7,000 and 30,000 Communards, both during the fighting and in massacres of men, women, and children during and after the Commune. More recent histories, based on studies of the number buried in Paris cemeteries and in mass graves after the fall of the Commune, put the number killed at between 6,000 and 10,000. Twenty-six courts were established to try more than who had been arrested, which took until 1875 and imposed sentences, of which inflicted. Forced labour for life was imposed on were transported to "a fortified place" and were transported. About were held in prison hulks until released in 1872 and a great many Communards fled abroad to Britain, Switzerland, Belgium or the United States. The survivors were amnestied by a bill introduced by Gambetta in 1880 and allowed to return.
German unification and power.
The creation of a unified German Empire greatly disturbed the balance of power that had been created with the Congress of Vienna after the end of the Napoleonic Wars. Germany had established itself as a major power in continental Europe, boasting the most powerful and professional army in the world, with a navy that would soon become second only to Britain's Royal Navy. Although Britain remained the dominant world power overall, British involvement in European affairs during the late 19th century was limited, owing to its focus on colonial empire-building, allowing Germany to exercise great influence over the European mainland. Anglo-German straining of tensions was somewhat mitigated by several prominent relationships between the two powers, such as the Crown Prince's marriage with the daughter of Queen Victoria.
Poland.
In the Prussian province of Posen, with a large Polish population, there was strong support for the French and angry demonstrations at news of Prussian-German victories—a clear manifestation of Polish nationalist feeling. Calls were also made for Polish recruits to desert from the Prussian Army—though these went mainly unheeded. An alarming report on the Posen situation, sent to Bismarck on 16 August 1870, led to the quartering of reserve troop contingents in the restive province. The Franco-Prussian War thus turned out to be a significant event also in German–Polish relations, marking the beginning of a prolonged period of repressive measures by the authorities and efforts at Germanisation.
French reaction and Revanchism.
The defeat in the Franco-Prussian War led to the birth of Revanchism (literally, "revenge-ism") in France, characterised by a deep sense of bitterness, hatred and demand for revenge against Germany. This was particularly manifested in the desire for another war with Germany in order to reclaim Alsace and Lorraine. It also led to the development of right-wing ideologies emphasising the "the ideal of the guarded, self-referential nation schooled in the imperative of war", an ideology epitomised by figures such as General Georges Ernest Boulanger in the 1880s. Paintings that emphasized the humiliation of the defeat became in high demand, such as those by Alphonse de Neuville.

</doc>
<doc id="44041" url="https://en.wikipedia.org/wiki?curid=44041" title="Solvation">
Solvation

Solvation, also sometimes called dissolution, is the process of attraction and association of molecules of a solvent with molecules or ions of a solute. As ions dissolve in a solvent they spread out and become surrounded by solvent molecules. Solvation is the process of surrounding solute with solvent. It involves evening out a concentration gradient and evenly distributing the solute within the solvent.
Distinction between solvation, dissolution and solubility.
By an IUPAC definition, solvation is an interaction of a solute with the solvent, which leads to stabilization of the solute species in the solution. One may also refer to the solvated state, whereby an ion in a solution is surrounded or complexed by solvent molecules (see solvation shell). The concept of the solvation interaction can also be applied to an insoluble material, for example, solvation of functional groups on a surface of ion-exchange resin.
Solvation is, in concept, distinct from dissolution and solubility. Dissolution is a kinetic process, and is quantified by its rate. Solubility quantifies the dynamic equilibrium state achieved when the rate of dissolution equals the rate of precipitation.
The consideration of the units makes the distinction clearer. Complexation can be described by coordination number and the complex stability constants. The typical unit for dissolution rate is mol/s. The unit for solubility can be mol/kg.
Liquefaction accompanied by an irreversible chemical change is also distinct from solvation. For example, zinc cannot be solvated by hydrochloric acid, but it can be converted into the soluble salt zinc chloride by a chemical reaction.
Solvents and intermolecular interactions.
Polar solvents are those with a molecular structure that contains dipoles. Such compounds are often found to have a high dielectric constant. The polar molecules of these solvents can solvate ions because they can orient the appropriate partially charged portion of the molecule towards the ion in response to electrostatic attraction. This stabilizes the system and creates a solvation shell (or hydration shell in the case of water). Water is the most common and well-studied polar solvent, but others exist, such as ethanol, methanol, acetone, acetonitrile, dimethyl sulfoxide, and propylene carbonate. These solvents can be used to dissolve inorganic compounds such as salts.
Solvation involves different types of intermolecular interactions: hydrogen bonding, ion-dipole, and dipole-dipole attractions or van der Waals forces. The hydrogen bonding, ion-dipole, and dipole-dipole interactions occur only in polar solvents. Ion-ion interactions occur only in ionic solvents. The solvation process will be thermodynamically favored only if the overall Gibbs energy of the solution is decreased, compared to the Gibbs energy of the separated solvent and solid (or gas or liquid). This means that the change in enthalpy minus the change in entropy (multiplied by the absolute temperature) is a negative value, or that the Gibbs free energy of the system decreases.
The conductivity of a solution depends on the solvation of its ions.
Thermodynamic considerations.
For solvation to occur, energy is required to release individual ions and molecules from the crystal lattices in which they are present. This is necessary to break the attractions the ions have with each other and is equal to the solid's lattice free energy (the energy released at the formation of the lattice as the ions bonded with each other). The energy for this comes from the energy released when ions of the lattice associate with molecules of the solvent. Energy released in this form is called the free energy of solvation.
The enthalpy of solution is the solution enthalpy minus the enthalpy of the separate systems, whereas the entropy is the corresponding difference in entropy. Most gases have a negative enthalpy of solution. A negative enthalpy of solution means that the solute is less soluble at high temperatures.
Although early thinking was that a higher ratio of a cation's ion charge to ionic radius, or the charge density, resulted in more solvation, this does not stand up to scrutiny for ions like iron(III) or lanthanides and actinides, which are readily hydrolyzed to form insoluble (hydrous) oxides. As these are solids, it is apparent that they are not solvated.
Enthalpy of solvation can help explain why solvation occurs with some ionic lattices but not with others. The difference in energy between that which is necessary to release an ion from its lattice and the energy given off when it combines with a solvent molecule is called the enthalpy change of solution. A negative value for the enthalpy change of solution corresponds to an ion that is likely to dissolve, whereas a high positive value means that solvation will not occur. It is possible that an ion will dissolve even if it has a positive enthalpy value. The extra energy required comes from the increase in entropy that results when the ion dissolves. The introduction of entropy makes it harder to determine by calculation alone whether a substance will dissolve or not. A quantitative measure for solvation power of solvents is given by donor numbers.
In general, thermodynamic analysis of solutions is done by modeling them as reactions. For example; if you add sodium chloride(s) to water, the salt will dissociate into the ions sodium(+aq) and chloride(-aq). The equilibrium constant for this dissociation can be predicted by the change in Gibbs free energy of this reaction.
Max Born developed the first quantitative model for solvation of ionic compounds.
Macromolecules and assemblies.
Hydration affects electronic and vibrational properties of biomolecules.

</doc>
<doc id="44042" url="https://en.wikipedia.org/wiki?curid=44042" title="Rubi">
Rubi

Rubi may refer to:

</doc>
<doc id="44043" url="https://en.wikipedia.org/wiki?curid=44043" title="Rawa">
Rawa

Rawa may refer to:

</doc>
<doc id="44044" url="https://en.wikipedia.org/wiki?curid=44044" title="Oceanography">
Oceanography

Oceanography (compound of the Greek words ὠκεανός meaning "ocean" and γράφω meaning "write"), also known as oceanology, is the branch of Earth science that studies the ocean. It covers a wide range of topics, including ecosystem dynamics; ocean currents, waves, and geophysical fluid dynamics; plate tectonics and the geology of the sea floor; and fluxes of various chemical substances and physical properties within the ocean and across its boundaries. These diverse topics reflect multiple disciplines that oceanographers blend to further knowledge of the world ocean and understanding of processes within: astronomy, biology, chemistry, climatology, geography, geology, hydrology, meteorology and physics. Paleoceanography studies the history of the oceans in the geologic past.
History.
Early history.
Humans first acquired knowledge of the waves and currents of the seas and oceans in pre-historic times. Observations on tides were recorded by Aristotle and Strabo. Early exploration of the oceans was primarily for cartography and mainly limited to its surfaces and of the animals that fishermen brought up in nets, though depth soundings by lead line were taken.
Although Juan Ponce de León in 1513 first identified the Gulf Stream, and the current was well-known to mariners, Benjamin Franklin made the first scientific study of it and gave it its name. Franklin measured water temperatures during several Atlantic crossings and correctly explained the Gulf Stream's cause. Franklin and Timothy Folger printed the first map of the Gulf Stream in 1769-1770.
Information on the currents of the Pacific Ocean was gathered by explorers of the late 18th century, including James Cook and Louis Antoine de Bougainville. James Rennell wrote the first scientific textbooks on oceanography, detailing the current flows of the Atlantic and Indian oceans. During a voyage around the Cape of Good Hope in 1777, he mapped ""the banks and currents at the Lagullas"". He was also the first to understand the nature of the intermittent current near the Isles of Scilly, (now known as Rennell's Current).
Sir James Clark Ross took the first modern sounding in deep sea in 1840, and Charles Darwin published a paper on reefs and the formation of atolls as a result of the Second voyage of HMS Beagle in 1831-6. Robert FitzRoy published a four-volume report of the Beagle's three voyages. In 1841–1842 Edward Forbes undertook dredging in the Aegean Sea that founded marine ecology.
The first superintendent of the United States Naval Observatory (1842–1861), Matthew Fontaine Maury devoted his time to the study of marine meteorology, navigation, and charting prevailing winds and currents. His 1855 textbook "Physical Geography of the Sea" was one of the first comprehensive oceanography studies. Many nations sent oceanographic observations to Maury at the Naval Observatory, where he and his colleagues evaluated the information and distributed the results worldwide.
Modern Oceanography.
Despite all this, human knowledge of the oceans remained confined to the topmost few fathoms of the water and a small amount of the bottom, mainly in shallow areas. Almost nothing was known of the ocean depths. The Royal Navy's efforts to chart all of the world's coastlines in the mid-19th century reinforced the vague idea that most of the ocean was very deep, although little more was known. As exploration ignited both popular and scientific interest in the polar regions and Africa, so too did the mysteries of the unexplored oceans.
The seminal event in the founding of the modern science of oceanography was the 1872-76 Challenger expedition. As the first true oceanographic cruise, this expedition laid the groundwork for an entire academic and research discipline. In response to a recommendation from the Royal Society, The British Government announced in 1871 an expedition to explore world's oceans and conduct appropriate scientific investigation. Charles Wyville Thompson and Sir John Murray launched the Challenger expedition. The Challenger, leased from the Royal Navy, was modified for scientific work and equipped with separate laboratories for natural history and chemistry. Under the scientific supervision of Thomson, Challenger travelled nearly surveying and exploring. On her journey circumnavigating the globe, 492 deep sea soundings, 133 bottom dredges, 151 open water trawls and 263 serial water temperature observations were taken. Around 4,700 new species of marine life were discovered. The result was the "Report Of The Scientific Results of the Exploring Voyage of H.M.S. Challenger during the years 1873-76". Murray, who supervised the publication, described the report as "the greatest advance in the knowledge of our planet since the celebrated discoveries of the fifteenth and sixteenth centuries". He went on to found the academic discipline of oceanography at the University of Edinburgh, which remained the centre for oceanographic research well into the 20th century. Murray was the first to study marine trenches and in particular the Mid-Atlantic Ridge, and map the sedimentary deposits in the oceans. He tried to map out the world's ocean currents based on salinity and temperature observations, and was the first to correctly understand the nature of coral reef development.
In the late 19th century, other Western nations also sent out scientific expeditions (as did private individuals and institutions). The first purpose built oceanographic ship, the Albatros, was built in 1882. In 1893, Fridtjof Nansen allowed his ship, Fram, to be frozen in the Arctic ice. This enabled him to obtain oceanographic, meteorological and astronomical data at a stationary spot over an extended period.
Between 1907 and 1911 Otto Krümmel published the "Handbuch der Ozeanographie", which became influential in awakening public interest in oceanography. The four-month 1910 North Atlantic expedition headed by John Murray and Johan Hjort was the most ambitious research oceanographic and marine zoological project ever mounted until then, and led to the classic 1912 book "The Depths of the Ocean".
The first acoustic measurement of sea depth was made in 1914. Between 1925 and 1927 the "Meteor" expedition gathered 70,000 ocean depth measurements using an echo sounder, surveying the Mid-Atlantic ridge.
Sverdrup, Johnson and Fleming published "The Oceans" in 1942, which was a major landmark. "The Sea" (in three volumes, covering physical oceanography, seawater and geology) edited by M.N. Hill was published in 1962, while Rhodes Fairbridge's "Encyclopedia of Oceanography" was published in 1966.
The Great Global Rift, running along the Mid Atlantic Ridge, was discovered by Maurice Ewing and Bruce Heezen in 1953; in 1954 a mountain range under the Arctic Ocean was found by the Arctic Institute of the USSR. The theory of seafloor spreading was developed in 1960 by Harry Hammond Hess. The Ocean Drilling Program started in 1966. Deep sea vents were discovered in 1977 by John Corlis and Robert Ballard in the submersible DSV "Alvin".
In the 1950s, Auguste Piccard invented the bathyscaphe and used the "Trieste" to investigate the ocean's depths. The United States nuclear submarine Nautilus made the first journey under the ice to the North Pole in 1958. In 1962 the FLIP (Floating Instrument Platform), a 355-foot spar buoy, was first deployed.
From the 1970s, there has been much emphasis on the application of large scale computers to oceanography to allow numerical predictions of ocean conditions and as a part of overall environmental change prediction. An oceanographic buoy array was established in the Pacific to allow prediction of El Niño events.
1990 saw the start of the World Ocean Circulation Experiment (WOCE) which continued until 2002. Geosat seafloor mapping data became available in 1995.
In recent years studies advanced particular knowledge on ocean acidification, ocean heat content, ocean currents, the El Niño phenomenon, mapping of methane hydrate deposits, the carbon cycle, coastal erosion, weathering and climate feedbacks in regards to climate change interactions.
Study of the oceans is linked to understanding global climate changes, potential global warming and related biosphere concerns. The atmosphere and ocean are linked because of evaporation and precipitation as well as thermal flux (and solar insolation). Wind stress is a major driver of ocean currents while the ocean is a sink for atmospheric carbon dioxide. All these factors relate to the ocean's biogeochemical setup.
Ocean acidification.
Ocean acidification describes the decrease in ocean pH that is caused by anthropogenic carbon dioxide () emissions into the atmosphere. Seawater is slightly alkaline and had a preindustrial pH of about 8.2. More recently, anthropogenic activities have steadily increased the carbon dioxide content of the atmosphere; about 30–40% of the added CO2 is absorbed by the oceans, forming carbonic acid and lowering the pH (now below 8.1) through ocean acidification. The pH is expected to reach 7.7 by the year 2100.
An important element for the skeletons of marine animals is calcium, but calcium carbonate becomes more soluble with pressure, so carbonate shells and skeletons dissolve below the carbonate compensation depth. Calcium carbonate becomes more soluble at lower pH, so ocean acidification is likely to affect marine organisms with calcareous shells, such as oysters, clams, sea urchins and corals, and the carbonate compensation depth will rise closer to the sea surface. Affected planktonic organisms will include pteropods, coccolithophorids and foraminifera, all important in the food chain. In tropical regions, corals are likely to be severely affected as they become less able to build their calcium carbonate skeletons, in turn adversely impacting other reef dwellers.
The current rate of ocean chemistry change seems to be unprecedented in Earth's geological history, making it unclear how well marine ecosystems will adapt to the shifting conditions of the near future. Of particular concern is the manner in which the combination of acidification with the expected additional stressors of higher temperatures and lower oxygen levels will impact the seas.
Ocean currents.
Since the early ocean expeditions in oceanography, a major interest was the study of the ocean currents and temperature measurements. The tides, the Coriolis effect, changes in direction and strength of wind, salinity and temperature are the main factors determining ocean currents. The thermohaline circulation (THC) "thermo-" referring to temperature and "-haline" referring to salt content connects 4 of 5 ocean basins and is primarily dependent on the density of sea water. Ocean currents such as the Gulf Stream are wind-driven surface currents.
Ocean heat content.
Oceanic heat content (OHC) refers to the heat stored in the ocean. The changes in the ocean heat play an important role in sea level rise, because of thermal expansion. Ocean warming accounts for 90% of the energy accumulation from global warming between 1971 and 2010.
Branches.
The study of oceanography is divided into these four branches:
Oceanographic institutions.
The first international organization of oceanography was created in 1902 as the International Council for the Exploration of the Sea. In 1903 the Scripps Institution of Oceanography was founded, followed by Woods Hole Oceanographic Institution in 1930, Virginia Institute of Marine Science in 1938, and later the Lamont-Doherty Earth Observatory at Columbia University, and the School of Oceanography at University of Washington. In Britain, the National Oceanography Centre (an institute of the Natural Environment Research Council) is the successor to the UK's Institute of Oceanographic Sciences. In Australia, CSIRO Marine and Atmospheric Research (CMAR), is a leading centre. In 1921 the International Hydrographic Bureau (IHB) was formed in Monaco.

</doc>
<doc id="44048" url="https://en.wikipedia.org/wiki?curid=44048" title="Northern Province">
Northern Province

Northern Province or North Province may refer to:
In fiction, "North Province" may refer to:

</doc>
<doc id="44049" url="https://en.wikipedia.org/wiki?curid=44049" title="Chiricahua">
Chiricahua

Chiricahua ( ) are a band of Apache Native Americans, based in the Southern Plains and Southwest United States. Culturally related to other Apache peoples, Chiricahua historically shared a common area, language, customs, and intertwined family relations. At the time of European contact, they had a territory of 15 million acres (61,000 km2) in Southwestern New Mexico and Southeastern Arizona in the United States and in Northern Sonora and Chihuahua in Mexico.
Today Chiricahua are enrolled in two federally recognized tribes in the United States: the Fort Sill Apache Tribe, located near Apache, Oklahoma with a reservation in Los Lunas, New Mexico, and the Mescalero Apache Tribe of the Mescalero Reservation near Ruidoso, New Mexico the San Carlos Apache Tribe, Arizona does have Chiricahua Apache people there also.
Name.
The Chiricahua Apache were initially given their name by the Spanish. They are also known as the "Chiricagui", "Apaches de Chiricahui", "Chiricahues", "Chilicague", "Chilecagez", and "Chiricagua". The White Mountain Apache, including the "Cibecue" and "Bylas" groups of the Western Apache, called them "Ha'i’ą́há" and the San Carlos Apache called them "Hák'ą́yé" both meaning ″'Eastern Sunrise, i.e. People in the East". Navajo call the Chiricahua "Chíshí". 
Their autonym for themselves is depending on the dialect simply "Nde, Ne, Néndé, Héndé" or "Hen-de" - ″The People, Men″, they never called themselves ″Apaches″. Sometimes it is noted that all Apaches called the Americans and European settlers (with exception of the Mexicans) "pindah-lickoyee" ("White Eyes"), but this seems an appellation from Mescalero and Lipan Apache bands, the Chiricahua bands called the whites "daadatlijende" (″Blue/green eye people″) or "indaaligande" (″white skinned people″ or literally ″strange, not Apache people, which are white skinned″).
Please see the Bands section below for more names of bands and sub-bands of the Chiricahua (Central Apache).
Culture and organization.
Several loosely affiliated bands of Apache came improperly to be usually known as the Chiricahuas. These included the "Chokonen" (recte: Tsokanende), the "Chihenne" (recte: Tchihende), the "Nednai" ("Nednhi") and "Bedonkohe" (recte, both of them together: Ndendahe). Today, all are commonly referred to as Chiricahua, but they were not historically a single band nor the same Apache division, being more correctly identified, all together, as "Central Apaches".
Many other bands and groups of Apachean language-speakers ranged over eastern Arizona and the American Southwest. The bands that are grouped under the Chiricahua term today had much history together: they intermarried and lived alongside each other, and they also occasionally fought with each other. They formed short-term as well as longer alliances that have caused scholars to classify them as one people.
The Apachean groups and the Navajo peoples were part of the Athabaskan migration into the North American continent from Asia, across the Bering Strait from Siberia. As the people moved south and east into North America, groups splintered off and became differentiated by language and culture over time. Some anthropologists believe that the Apache and the Navajo were pushed south and west into what is now New Mexico and Arizona by pressure from other Great Plains Indians, such as the Comanche and Kiowa. Among the last of such splits were those that resulted in the formation of the different Apachean bands whom the later Europeans encountered: the southwestern Apache groups and the Navajo. Although both speaking forms of Southern Athabaskan, the Navajo and Apache have become culturally distinct.
History.
The Tsokanende (Chiricahua) Apache division was once led, from the beginning of the 18th century, by chiefs such as Pisago Cabezon, Relles, Posito Moraga, Yrigollen, Tapilà, Teboca, Vivora, Miguel Narbona, Esquinaline, and finally Cochise (whose name was derived from the Apache word "Cheis," meaning "having the quality of oak") and, after his death, his sons Tahzay and, later, Naiche, under the guardianship of Cochise's war chief and brother-in-law Nahilzay, and the independent chiefs Chihuahua, Ulzana, Skinya and Pionsenay; Tchihende (Mimbreño) people was led, during the same period, by chiefs as Juan Josè Compa, Fuerte a.k.a. Soldado Fiero, Mangas Coloradas, Cuchillo Negro, Delgadito, Ponce, Nana, Victorio, Loco, Mangus; Ndendahe Apache people, in the meanwhile, was led by Mahko and, after him, Mano Mocha, Coleto Amarillo, Luis, Laceres, Felipe, Natiza, and finally Juh and "Goyaałé" (known to the Americans as Geronimo). After Victorio's death, Nana, Geronimo, Mangus (youngest Mangas Coloradas' son) and youngest Cochise's son "Naiche" were the last leaders of the Central Apaches, and their mixed Apache group was the last to continue to resist U.S. government control of the American Southwest.
European-Apache relations.
From the beginning of EuropeanAmerican/Apache relations, there was conflict between them, as they competed for land and other resources, and had very different cultures. Their encounters were preceded by more than 100 years of Spanish colonial and Mexican incursions and settlement on the Apache lands. The United States settlers were newcomers to the competition for land and resources in the Southwest, but they inherited its complex history, and brought their own attitudes with them about American Indians and how to use the land. By the Treaty of Guadalupe Hidalgo of 1848, the US took on the responsibility to prevent and punish cross-border incursions by Apache who were raiding in Mexico.
The Apache viewed the United States colonists with ambivalence, and in some cases enlisted them as allies in the early years against the Mexicans. In 1852, the US and some of the Chiricahua signed a treaty, but it had little lasting effect. During the 1850s, American miners and settlers began moving into Chiricahua territory, beginning encroachment that had been renewed in the migration to the Southwest of the previous two decades.
This forced the Apachean people to change their lives as nomads, free on the land. The US Army defeated them and forced them into the confinement of reservation life, on lands ill-suited for subsistence farming, which the US proffered as the model of civilization. Today, the Chiricahua are preserving their culture as much as possible, while forging new relationships with the peoples around them. The Chiricahua are a living and vibrant culture, a part of the greater American whole and yet distinct based on their history and culture.
Hostilities.
Although they had lived peaceably with most Americans in the New Mexico Territory up to about 1860, the Chiricahua became increasingly hostile to American encroachment in the Southwest after a number of provocations had occurred between them.
In 1835, Mexico had placed a bounty on Apache scalps which further inflamed the situation. In 1837 Warm Springs Mimbreños' head chief and famed raider, Soldado Fiero a.k.a. Fuerte was killed by Mexican soldiers of the garrison at Janos (only two days' travel from Santa Rita del Cobre), and his son Cuchillo Negro succeeded him as head chief. In the same 1837, the American John (a.k.a. James) Johnson invited the Coppermine Mimbreños in the Pinos Altos area to trade with his party (near the mines at Santa Rita del Cobre, New Mexico and, when they gathered around a blanket on which "pinole" (a ground corn flour) had been placed for them, Johnson and his men opened fire on the Chihenne with rifles and a concealed cannon loaded with scrap iron, glass, and a length of chain. They killed about 20 Apache, including the chief Juan José Compá. Mangas Coloradas is said to have witnessed this attack, which inflamed his and other Apache warriors' desires for vengeance for many years; he led the survivors to safety and subsequently, together with Cuchillo Negro, took Mimbreño revenge. The historian Rex W. Strickland argued that the Apache had come to the meeting with their own intentions of attacking Johnson's party, but were taken by surprise. In 1839 scalp-hunter James Kirker was employed by Robert McKnight to re-open the road to Santa Rita del Cobre.
After the conclusion of the US/Mexican War (1848) and the Gadsden Purchase (1853), Americans began to enter the territory in greater numbers. This increased the opportunities for incidents and misunderstandings. Anyway, the Apaches, including Mangas Coloradas and Cuchillo Negro, were not at first hostile to the Americans, considering them enemies of their own Mexican enemies.
Cuchillo Negro, with Ponce, Delgadito, Victorio and other Mimbreño chiefs, signed a treaty at Fort Webster in April 1853, but, during the spring of 1857 the U.S. Army set out on a campaign, led by Col. Benjamin L.E. deBonneville, Col. Dixon S. Miles (3° Cavalry from Fort Thorn) and Col. William W. Loring (commanding a Mounted Rifles Regiment from Albuquerque), against Mogollon and Coyotero Apaches: Loring's Pueblo Indian scouts found out and attacked an Apache rancheria in the Canyon de Los Muertos Carneros (May 25, 1857), where Cuchillo Negro and some Mimbreño Apache were resting after a raid against the Navahos. Some Apaches, including Cuchillo Negro himself, were killed.
In December 1860, after several bad incidents provoked by the miners led by James H. Tevis in the Pinos Altos area, "Mangas Coloradas" went to Pinos Altos, New Mexico to try to convince the miners to move away from the area he loved and to go to the Sierra Madre and seek gold there, but they tied him to a tree and whipped him badly. His Mimbreño and Ndendahe followers and related Chiricahua bands were incensed by the treatment of their respected chief. Mangas had been just as great a chief in his prime (during the 1830s and 1840s), along with Cuchillo Negro, as Cochise was then becoming.
In 1861, the US Army seized and killed some of Cochise's relatives near Apache Pass, in what became known as the Bascom Affair. Remembering how Cochise had escaped, the Chiricahua called the incident "cut the tent." In 1863, Gen. James H. Carleton set out leading a new campaign against the Mescalero Apache, and Capt. Edmund Shirland (10° California Cavalry) invited Mangas Coloradas for a "parley" but, after he entered the U.S. camp to negotiate a peace, the great Mimbreño chief was arrested and convicted in Fort McLane, where, probably after Gen. Joseph R. West's orders, Mangas Coloradas was killed by American soldiers (Jan. 18, 1863). His body was mutilated by the soldiers, and his people were enraged by his murder. The Chiricahuas began to consider the Americans as "enemies we go against them." From that time, they waged almost constant war against US settlers and the Army for the next 23 years. Cochise, his brother-in-law Nahilzay (war chief of Cochise's people), Chihuahua, Skinya, Pionsenay, Ulzana and other warring chiefs became a nightmare to settlers and military garrisons and patrols. In the meantime, the great Victorio, Delgadito (soon killed in 1864), Nana, Loco, young Mangus (last son of Mangas Coloradas) and other minor chiefs led on the warpath the Mimbreños, Chiricahuas' cousins and allies, and Juh led the Ndendahe (Nednhi and Bedonkohe together).
In 1872, General Oliver O. Howard, with the help of Thomas Jeffords, succeeded in negotiating a peace with Cochise. The US established a Chiricahua Apache Reservation with Jeffords as US Agent, near Fort Bowie, Arizona Territory. It remained open for about 4 years, during which the chief Cochise died (from natural causes). In 1877, about three years after Cochise's death, the US moved the Chiricahua and some other Apache bands to the San Carlos Apache Indian Reservation, still in Arizona. The mountain people hated the desert environment of San Carlos, and some frequently began to leave the reservation and sometimes raid neighboring settlers. They surrendered to General Nelson Miles in 1886. The best-known warrior leader of the renegades, although he was not considered a 'chief', was the forceful and influential Geronimo. He and "Naiche" (the son of Cochise and hereditary leader after Tahzay's death) together led many of the resisters during those last few years of freedom.
They made a stronghold in the Chiricahua Mountains, part of which is now inside Chiricahua National Monument, and across the intervening Willcox Playa to the northeast, in the Dragoon Mountains (all in southeastern Arizona). In late frontier times, the Chiricahua ranged from San Carlos and the White Mountains of Arizona, to the adjacent mountains of southwestern New Mexico around what is now Silver City, and down into the mountain sanctuaries of the Sierra Madre (of northern Mexico). There they often joined with their "Nednai" Apache kin.
General George Crook, then General Miles' troops, aided by Apache scouts from other groups, pursued the exiles until they gave up. Mexico and the United States had negotiated an agreement allowing their troops in pursuit of the Apache to continue into each other's territories. This prevented the Chiricahua groups from using the border as an escape route, and as they could gain little time to rest and consider their next move, the fatigue, attrition and demoralization of the constant hunt led to their surrender.
The final 34 hold-outs, including Geronimo and Naiche, surrendered to units of General Miles' forces in September 1886. From Bowie Station, Arizona, they were entrained, along with most of the other remaining Chiricahua (as well as the Army's Apache scouts), and exiled to Fort Marion, Florida. At least two Apache warriors, Massai and Gray Lizard, escaped from their prison car and made their way back to Arizona in a journey to their ancestral lands.
After a number of Chiricahua deaths at the Fort Marion prison near St. Augustine, Florida, the survivors were moved, first to Alabama, and later to Fort Sill, Oklahoma. Geronimo's surrender ended the Indian Wars in the United States. However, another group of Chiricahua (a.k.a. the "Nameless Ones" or "Bronco Apache") were not captured by U.S. forces and refused to surrender. They escaped over the border to Mexico, and settled in the remote Sierra Madre mountains. There they built hidden camps, raided homes for cattle and other food supplies, and engaged in periodic firefights with units of the Mexican Army and police. Most were eventually captured or killed by soldiers or by private ranchers armed and deputized by the Mexican government.
Eventually, the surviving Chiricahua prisoners were moved to the Fort Sill military reservation in Oklahoma. In August 1912, by an act of the U.S. Congress, they were released from their prisoner of war status as they were thought to be no further threat. Although promised land at Fort Sill, they met resistance from local non-Apache. They were given the choice to remain at Fort Sill or to relocate to the Mescalero reservation near Ruidoso, New Mexico. Two-thirds of the group, 183 people, elected to go to New Mexico, while 78 remained in Oklahoma. Their descendants still reside in these places. At the time, they were not permitted to return to Arizona because of hostility from the long wars.
Bands.
Since the "band" as a unit was much more important than "tribe" in Chiricahua culture, the Chiricahua had no name for themselves (autonym) as a people. The name Chiricahua is most likely the Spanish rendering of the Opata word "Chihuicahui or Chiguicagui" ('mountain of the wild turkey') for the Chiricahua Mountains, later corrupted into Chiricahui/Chiricahua. The Chiricahua tribal territory encompassed today's SE Arizona, SW New Mexico, NE Sonora and NW Chihuahua. The Chiricahua range extended to the east as far as the Rio Grande Valley in New Mexico and to the west as far as the San Pedro River Valley in Arizona, north of Magdalena just below present day Hwy I-40 corridor in New Mexico and with the town Ciudad Madera (276 km northwest of the state capital, Chihuahua, and 536 km southwest of Ciudad Juárez (formerly known as Paso del Norte) on the Mexico–United States border), as their southernmost range.
According to Morris E. Opler (1941), the Chiricahuas consisted of three bands:
Schroeder (1947) lists five bands:
The Chiricahua-Warm Springs Fort Sill Apache tribe in Oklahoma say they have four bands in Fort Sill:
Today they use the word Chidikáágu (derived from the Spanish word "Chiricahua") to refer to the Chiricahua in general, and the word Indé, to refer to the Apache in general.
Other sources list these and additional bands (only the Chokonen and Chihuicahui local groups of the Chokonen band were considered by Chiricahua tribal members to be "the real Chiricahua people"):
The Chokonen, Chihenne, Nednhi, and Bedonkohe had probably up to three other groups, named respectively after their leaders or homelands. By the end of the 19th century, surviving Apache no longer identified these groups. They may have been wiped out (like the Pinaleño-Nednhi) or had joined more powerful groups. For instance, the remnant of the Carrizaleño-Nedhni camped together with their northern kin, the Janero-Nednhi.
The Carrizaleňo-Nednhi shared overlapping territory in the surroundings of Casas Grandes and Agua Nuevas with the "Tsebekinéndé", a southern Mescalero band (which was often called "Aguas Nuevas" by the Spanish). The Spanish referred to the Apache band by the same name of Tsebekinéndé. These two different Apache bands were often confused with each other. (Similar confusion arose over distinguishing the Janeros-Nednhi of the Chiricahua ("Dzilthdaklizhéndé") and the "Dzithinahndé" of the Mescalero.

</doc>
<doc id="44050" url="https://en.wikipedia.org/wiki?curid=44050" title="John Abbott (disambiguation)">
John Abbott (disambiguation)

John Abbott (1821–1893) was Prime Minister of Canada, 1891–1892.
John Abbott or Abbot may also refer to:

</doc>
<doc id="44055" url="https://en.wikipedia.org/wiki?curid=44055" title="Measurable function">
Measurable function

In mathematics, particularly in measure theory, measurable functions are structure-preserving functions between measurable spaces; as such, they form a natural context for the theory of integration. Specifically, a function between measurable spaces is said to be measurable if the preimage of each measurable set is measurable, analogous to the situation of continuous functions between topological spaces.
In probability theory, the formula_1-algebra often represents the set of available information, and a function (in this context a random variable) is measurable if and only if it represents an outcome that is knowable based on the available information. In contrast, functions that are not Lebesgue measurable are generally considered pathological, at least in the field of analysis.
Formal definition.
Let formula_2 and formula_3 be measurable spaces, meaning that formula_4 and formula_5 are sets equipped with respective formula_1-algebras formula_7 and formula_8. A function formula_9 is said to be measurable if the preimage of formula_10 under formula_11 is in formula_7 for every formula_13; i.e.
The notion of measurability depends on the sigma algebras formula_7 and formula_8. To emphasize this dependency, if formula_9 is a measurable function, we will write
Caveat.
This definition can be deceptively simple, however, as special care must be taken regarding the formula_1-algebras involved. In particular, when a function formula_20 is said to be Lebesgue measurable, what is actually meant is that formula_21 is a measurable function—that is, the domain and range represent different formula_1-algebras on the same underlying set. Here, formula_23 is the formula_1-algebra of Lebesgue measurable sets, and formula_25 is the Borel algebra on formula_26, the smallest formula_1-algebra containing all the open sets. As a result, the composition of Lebesgue-measurable functions need not be Lebesgue-measurable.
By convention a topological space is assumed to be equipped with the Borel algebra unless otherwise specified. Most commonly this space will be the real or complex numbers. For instance, a real-valued measurable function is a function for which the preimage of each Borel set is measurable. A complex-valued measurable function is defined analogously. In practice, some authors use measurable functions to refer only to real-valued measurable functions with respect to the Borel algebra. If the values of the function lie in an infinite-dimensional vector space instead of formula_26 or formula_29, usually other definitions of measurability are used, such as weak measurability and Bochner measurability.
Non-measurable functions.
Real-valued functions encountered in applications tend to be measurable; however, it is not difficult to find non-measurable functions.

</doc>
<doc id="44056" url="https://en.wikipedia.org/wiki?curid=44056" title="MD Data">
MD Data

MD Data stands for MiniDisc-Data, and is a magneto-optical medium for storing computer data. Sony wanted MD Data to replace floppy disks, but the Zip drive from Iomega ended up filling that market need and, later on, the advent of affordable CD-writers and very cheap blank CD media, coupled with the availability of memory sticks and cards proved the final straw for MD-Data. 
The technology provided 140 MB of data storage, but it was slow and expensive. Also, data drives can only read or write audio MDs when set in "play" mode, which, however, does not provide computer access to the data.
MD Data appeared in products such as an MD still camera and an MD document scanner. MD Data was also used in the late 1990s in 4- and 8-track multitrack recording decks. Meant as a step up from the popular 4-track cassette-based studios, these recorders enjoyed a brief prominence before they were replaced by relatively affordable -- and far more flexible -- direct-to-hard drive recording on Windows and Macintosh based computers. The format has always been hampered by the lack of an affordable MD-Data drive with which to manipulate and back up data using a PC. 
In 1997, Sony introduced the MD-Data2 format, which could hold 650 MB of data (as opposed to MD-Data's 140 MB). However this format was only used by Sony's DCM-M1 camcorder (capable of still images and MPEG-2 video), and a few multitrack "portastudio"-style audio recorders such as Sony's MDM-X4 and Tascam's 564, among others.
The introduction of Hi-MD in 2004 allowed any type of data (files, music, etc.) to be stored on a Hi-MD formatted MiniDisc. This allows for storage capacity of around 340MB on a standard MiniDisc and approx. 1GB on a new, higher-density Hi-MD.

</doc>
<doc id="44057" url="https://en.wikipedia.org/wiki?curid=44057" title="Galactic astronomy">
Galactic astronomy

Galactic astronomy is the study of the Milky Way galaxy and all its contents. This is in contrast to extragalactic astronomy, which is the study of everything outside our galaxy, including all other galaxies.
Galactic astronomy should not be confused with galaxy formation and evolution, which is the general study of galaxies, their formation, structure, components, dynamics, interactions, and the range of forms they take.
The Milky Way galaxy, where the Solar System belongs, is in many ways the best studied galaxy, although important parts of it are obscured from view in visible wavelengths by regions of cosmic dust. The development of radio astronomy, infrared astronomy and submillimetre astronomy in the 20th Century allowed the gas and dust of the Milky Way to be mapped for the first time.
Subcategories.
A standard set of subcategories is used by astronomical journals to split up the subject of Galactic Astronomy:

</doc>
<doc id="44058" url="https://en.wikipedia.org/wiki?curid=44058" title="Big Bang nucleosynthesis">
Big Bang nucleosynthesis

In physical cosmology, Big Bang nucleosynthesis (abbreviated BBN, also known as primordial nucleosynthesis) refers to the production of nuclei other than those of the lightest isotope of hydrogen (hydrogen-1, 1H, having a single proton as a nucleus) during the early phases of the universe. Primordial nucleosynthesis is believed by most cosmologists to have taken place from 10 seconds to 20 minutes after the Big Bang, and is calculated to be responsible for the formation of most of the universe's helium as the isotope helium-4 (4He), along with small amounts of the hydrogen isotope deuterium (2H or D), the helium isotope helium-3 (3He), and a very small amount of the lithium isotope lithium-7 (7Li). In addition to these stable nuclei, two unstable or radioactive isotopes were also produced: the heavy hydrogen isotope tritium (3H or T); and the beryllium isotope beryllium-7 (7Be); but these unstable isotopes later decayed into 3He and 7Li, as above.
Essentially all of the elements that are heavier than lithium and beryllium were created much later, by stellar nucleosynthesis in evolving and exploding stars.
Characteristics.
There are two important characteristics of Big Bang nucleosynthesis (BBN):
The key parameter which allows one to calculate the effects of BBN is the baryon/photon number ratio, which is a small number of order 6 x 10−10. This parameter corresponds to the baryon density and controls the rate at which nucleons collide and react; from this we can derive elemental abundances. Although the baryon per photon ratio is important in determining elemental abundances, the precise value makes little difference to the overall picture. Without major changes to the Big Bang theory itself, BBN will result in mass abundances of about 75% of hydrogen-1, about 25% helium-4, about 0.01% of deuterium and helium-3, trace amounts (on the order of 10−10) of lithium, and negligible heavier elements. (Traces of boron have been found in some old stars, giving rise to the question whether some boron, not really predicted by the theory, might have been produced in the Big Bang. The question is not presently resolved.) That the observed abundances in the universe are generally consistent with these abundance numbers is considered strong evidence for the Big Bang theory.
In this field, for historical reasons it is customary to quote the helium-4 fraction "by mass", symbol Y, so that 25% helium-4 means that helium-4 atoms account for 25% of the mass, but only about 8% of the nuclei would be helium-4 nuclei. Other (trace) nuclei are usually expressed as number ratios to hydrogen.
Important parameters.
The creation of light elements during BBN was dependent on a number of parameters; among those was the neutron-proton ratio (calculable from Standard Model physics) and the baryon-photon ratio.
Neutron–proton ratio.
Neutrons can react with positrons or electron neutrinos to create protons and other products in one of the following reactions:
At times much earlier than 1 sec, the n/p ratio was close to 1:1. As the temperature dropped, the equilibrium shifted in favour of protons due to their slightly lower mass, and the n/p ratio decreased. 
These reactions continue until expansion of the universe outpaces the reactions, which occurs at about T = 0.7 MeV and is called the freeze out temperature. At freeze out, the neutron-proton ratio was about 1/6. However, free neutrons are unstable with a mean life of 880 sec; some neutrons decayed in the next few minutes before fusing into any nucleus, so the ratio of total neutrons to protons after nucleosynthesis ends is about 1/7. Almost all neutrons that fused instead of decaying ended up combined into helium-4, due to the fact that helium-4 has the highest binding energy per nucleon among light elements. This predicts that about 8% of all atoms should be helium-4, leading to a mass fraction of helium-4 of about 25%, which is in line with observations. Small traces of deuterium and helium-3 remained as there was insufficient time and density for them to react and form helium-4.
Baryon–photon ratio.
The baryon–photon ratio, η, is the key parameter determining the abundance of light elements present in the early universe. Baryons can react with light elements in the following reactions:
It is evident that reactions with baryons during BBN would ultimately result in helium-4, and also that the abundance of primordial deuterium is indirectly related to the baryon density or baryon-photon ratio. That is, the larger the baryon-photon ratio the more reactions there will be and the more efficiently deuterium will be eventually transformed into helium-4. This result makes deuterium a very useful tool in measuring the baryon-to-photon ratio.
Sequence.
Big Bang nucleosynthesis began a few seconds after the big bang, when the universe had cooled sufficiently to allow deuterium nuclei to survive disruption by high-energy photons. This time is essentially independent of dark matter content, since the universe was highly radiation dominated until much later, and this dominant component controls the temperature/time relation. 
The relative abundances of protons and neutrons follow from simple thermodynamical arguments, combined with the way that the mean temperature of the universe changes over time. If the reactions needed to reach the thermodynamically favoured equilibrium values are too slow compared to the temperature change brought about by the expansion, abundances would have remained at some specific non-equilibrium value. Combining thermodynamics and the changes brought about by cosmic expansion, one can calculate the fraction of protons and neutrons based on the temperature at this point. The answer is that there are about seven protons for every neutron at the beginning of nucleosynthesis. This fraction is in favour of protons, primarily because their lower mass - with respect to the neutron - favors their production. Free neutrons decay to protons with a half-life of about 10.2 minutes, but this time-scale is longer than the first three minutes of nucleogenesis. It was during this time that a substantial fraction of neutrons were combined with protons into deuterium and then into He-4. The sequence of these reaction chains is shown on the image.
One feature of BBN is that the physical laws and constants that govern the behavior of matter at these energies are very well understood, and hence BBN lacks some of the speculative uncertainties that characterize earlier periods in the life of the universe. Another feature is that the process of nucleosynthesis is determined by conditions at the start of this phase of the life of the universe, and proceeds independently of what happened before.
As the universe expands, it cools. Free neutrons and protons are less stable than helium nuclei, and the protons and neutrons have a strong tendency to form helium-4. However, forming helium-4 requires the intermediate step of forming deuterium. Before nucleosynthesis began, the temperature was high enough for many photons to have energy greater than the binding energy of deuterium; therefore any deuterium that was formed was immediately destroyed (a situation known as the deuterium bottleneck). Hence, the formation of helium-4 is delayed until the universe became cool enough for deuterium to survive (at about T = 0.1 MeV); after which there was a sudden burst of element formation. However, very shortly thereafter, at twenty minutes after the Big Bang, the universe became too cool for any further nuclear fusion and nucleosynthesis to occur. At this point, the elemental abundances were nearly fixed, and only change was the result of the radioactive decay of some products of BBN (such as tritium).
History of theory.
The history of Big Bang nucleosynthesis began with the calculations of Ralph Alpher in the 1940s. Alpher published the Alpher–Bethe–Gamow paper that outlined the theory of light-element production in the early universe.
During the 1970s, there was a major puzzle in that the density of baryons as calculated by Big Bang nucleosynthesis was much less than the observed mass of the universe based on calculations of the expansion rate. This puzzle was resolved in large part by postulating the existence of dark matter.
Heavy elements.
Big Bang nucleosynthesis produced no elements heavier than beryllium, due to a bottleneck: the absence of a stable nucleus with 8 or 5 nucleons. This deficit of larger atoms also limited the amounts of lithium-7 and beryllium-9 produced during BBN. In stars, the bottleneck is passed by triple collisions of helium-4 nuclei, producing carbon (the triple-alpha process). However, this process is very slow, taking tens of thousands of years to convert a significant amount of helium to carbon in stars, and therefore it made a negligible contribution in the minutes following the Big Bang.
Helium-4.
Big Bang nucleosynthesis predicts a primordial abundance of about 25% helium-4 by mass, irrespective of the initial conditions of the universe. As long as the universe was hot enough for protons and neutrons to transform into each other easily, their ratio, determined solely by their relative masses, was about 1 neutron to 7 protons (allowing for some decay of neutrons into protons). Once it was cool enough, the neutrons quickly bound with an equal number of protons to form first deuterium, then helium-4. Helium-4 is very stable and is nearly the end of this chain if it runs for only a short time, since helium neither decays nor combines easily to form heavier nuclei (since there are no stable nuclei with mass numbers of 5 or 8, helium does not combine easily with either protons, or with itself). Once temperatures are lowered, out of every 16 nucleons (2 neutrons and 14 protons), 4 of these (25% of the total particles and total mass) combine quickly into one helium-4 nucleus. This produces one helium for every 12 hydrogens, resulting in a universe that is a little over 8% helium by number of atoms, and 25% helium by mass.
One analogy is to think of helium-4 as ash, and the amount of ash that one forms when one completely burns a piece of wood is insensitive to how one burns it. The resort to the BBN theory of the helium-4 abundance is necessary as there is far more helium-4 in the universe than can be explained by stellar nucleosynthesis. In addition, it provides an important test for the Big Bang theory. If the observed helium abundance is much different from 25%, then this would pose a serious challenge to the theory. This would particularly be the case if the early helium-4 abundance was much smaller than 25% because it is hard to destroy helium-4. For a few years during the mid-1990s, observations suggested that this might be the case, causing astrophysicists to talk about a Big Bang nucleosynthetic crisis, but further observations were consistent with the Big Bang theory.
Deuterium.
Deuterium is in some ways the opposite of helium-4 in that while helium-4 is very stable and very difficult to destroy, deuterium is only marginally stable and easy to destroy. The temperatures, time, and densities were sufficient to combine a substantial fraction of the deuterium nuclei to form helium-4 but insufficient to carry the process further using helium-4 in the next fusion step. BBN did not convert all of the deuterium in the universe to helium-4 due to the expansion that cooled the universe and reduced the density and so, cut that conversion short before it could proceed any further. One consequence of this is that unlike helium-4, the amount of deuterium is very sensitive to initial conditions. The denser the initial universe was, the more deuterium would be converted to helium-4 before time ran out, and the less deuterium would remain.
There are no known post-Big Bang processes which can produce significant amounts of deuterium. Hence observations about deuterium abundance suggest that the universe is not infinitely old, which is in accordance with the Big Bang theory.
During the 1970s, there were major efforts to find processes that could produce deuterium, but those revealed ways of producing isotopes other than deuterium. The problem was that while the concentration of deuterium in the universe is consistent with the Big Bang model as a whole, it is too high to be consistent with a model that presumes that most of the universe is composed of protons and neutrons. If one assumes that all of the universe consists of protons and neutrons, the density of the universe is such that much of the currently observed deuterium would have been burned into helium-4. The standard explanation now used for the abundance of deuterium is that the universe does not consist mostly of baryons, but that non-baryonic matter (also known as dark matter) makes up most of the mass of the universe. This explanation is also consistent with calculations that show that a universe made mostly of protons and neutrons would be far more "clumpy" than is observed.
It is very hard to come up with another process that would produce deuterium other than by nuclear fusion. Such a process would require that the temperature be hot enough to produce deuterium, but not hot enough to produce helium-4, and that this process should immediately cool to non-nuclear temperatures after no more than a few minutes. It would also be necessary for the deuterium to be swept away before it reoccurs.
Producing deuterium by fission is also difficult. The problem here again is that deuterium is very unlikely due to nuclear processes, and that collisions between atomic nuclei are likely to result either in the fusion of the nuclei, or in the release of free neutrons or alpha particles. During the 1970s, cosmic ray spallation was proposed as a source of deuterium. That theory failed to account for the abundance of deuterium, but led to explanations of the source of other light elements.
Measurements and status of theory.
The theory of BBN gives a detailed mathematical description of the production of the light "elements" deuterium, helium-3, helium-4, and lithium-7. Specifically, the theory yields precise quantitative predictions for the mixture of these elements, that is, the primordial abundances at the end of the big-bang.
In order to test these predictions, it is necessary to reconstruct the primordial abundances as faithfully as possible, for instance by observing astronomical objects in which very little stellar nucleosynthesis has taken place (such as certain dwarf galaxies) or by observing objects that are very far away, and thus can be seen in a very early stage of their evolution (such as distant quasars).
As noted above, in the standard picture of BBN, all of the light element abundances depend on the amount of ordinary matter (baryons) relative to radiation (photons). Since the universe is presumed to be homogeneous, it has one unique value of the baryon-to-photon ratio. For a long time, this meant that to test BBN theory against observations one had to ask: can "all" of the light element observations be explained with a "single value" of the baryon-to-photon ratio? Or more precisely, allowing for the finite precision of both the predictions and the observations, one asks: is there some "range" of baryon-to-photon values which can account for all of the observations?
More recently, the question has changed: Precision observations of the cosmic microwave background radiation with the Wilkinson Microwave Anisotropy Probe (WMAP) and Planck give an independent value for the baryon-to-photon ratio. Using this value, are the BBN predictions for the abundances of light elements in agreement with the observations?
The present measurement of helium-4 indicates good agreement, and yet better agreement for helium-3. But for lithium-7, there is a significant discrepancy between BBN and WMAP/Planck, and the abundance derived from Population II stars. The discrepancy is a factor of 2.4―4.3 below the theoretically predicted value and is considered a problem for the original models, that have resulted in revised calculations of the standard BBN based on new nuclear data, and to various reevaluation proposals for primordial proton-proton nuclear reactions, especially the abundances of 7Be(n,p)7Li versus 7Be(d,p)8Be.For a recent calculation of BBN predictions, see 
For the observational values, see the following articles:
Non-standard scenarios.
In addition to the standard BBN scenario there are numerous non-standard BBN scenarios. These should not be confused with non-standard cosmology: a non-standard BBN scenario assumes that the Big Bang occurred, but inserts additional physics in order to see how this affects elemental abundances. These pieces of additional physics include relaxing or removing the assumption of homogeneity, or inserting new particles such as massive neutrinos.
There have been, and continue to be, various reasons for researching non-standard BBN. The first, which is largely of historical interest, is to resolve inconsistencies between BBN predictions and observations. This has proved to be of limited usefulness in that the inconsistencies were resolved by better observations, and in most cases trying to change BBN resulted in abundances that were more inconsistent with observations rather than less. The second reason for researching non-standard BBN, and largely the focus of non-standard BBN in the early 21st century, is to use BBN to place limits on unknown or speculative physics. For example, standard BBN assumes that no exotic hypothetical particles were involved in BBN. One can insert a hypothetical particle (such as a massive neutrino) and see what has to happen before BBN predicts abundances that are very different from observations. This has been done to put limits on the mass of a stable tau neutrino.

</doc>
<doc id="44059" url="https://en.wikipedia.org/wiki?curid=44059" title="Harrison Ford">
Harrison Ford

Harrison Ford (born July 13, 1942) is an American actor and film producer. He gained worldwide fame for his starring roles as Han Solo in the original "Star Wars" epic space opera trilogy and the title character of the "Indiana Jones" film series. Ford is also known for his roles as Rick Deckard in the neo-noir dystopian science fiction film "Blade Runner" (1982), John Book in the thriller "Witness" (1985), and Jack Ryan in the action films "Patriot Games" (1992) and "Clear and Present Danger" (1994). Most recently, Ford reprised his role of Han Solo in "" (2015).
His career has spanned six decades and includes roles in several Hollywood blockbusters; including the epic war film "Apocalypse Now" (1979), the legal drama "Presumed Innocent" (1990), the action film "The Fugitive" (1993), the political action thriller "Air Force One" (1997) and the psychological thriller "What Lies Beneath" (2000). Seven of his films have been inducted into the National Film Registry: "American Graffiti" (1973), "The Conversation" (1974), "Star Wars" (1977), "The Empire Strikes Back" (1980), "Raiders of the Lost Ark" (1981) and "Blade Runner".
In 1997, Ford was ranked No. 1 in "Empire"s "The Top 100 Movie Stars of All Time" list. The U.S. domestic box-office grosses of Ford's films total over US$4.7 billion, with worldwide grosses surpassing $6 billion, making Ford the highest-grossing U.S. domestic box-office star. When adjusted for inflation, Ford has appeared in five of the top thirty highest-grossing movies of the U.S. box office, the most for any actress/actor. Ford is married to actress Calista Flockhart, who is known for playing the title role in the comedy-drama series "Ally McBeal".
Early life.
Ford was born at the Swedish Covenant Hospital in Chicago, to Christopher Ford (born John William Ford; 1906-1999), an advertising executive and former actor, and Dorothy (née Nidelman; 1917-2004), a former radio actress. A younger brother, Terence, was born in 1945. Ford's paternal grandparents, John Fitzgerald Ford and Florence Veronica Niehaus, were of Irish Catholic and German descent, respectively. Ford's maternal grandparents, Harry Nidelman and Anna Lifschutz, were Jewish immigrants from Minsk, Belarus (at that time a part of the Russian Empire). When asked in which religion he and his brother were raised, Ford has jokingly responded, "Democrat," "to be liberals of every stripe". In a television interview shown in August 2000, when asked about what influence his Irish Catholic and Russian Jewish ancestry may have had on his life as a person and as an artist, Ford humorously stated, "As a man I've always felt Irish, as an actor I've always felt Jewish."
Ford was active in the Boy Scouts of America, and achieved its second-highest rank, Life Scout. He worked at Napowan Adventure Base Scout camp as a counselor for the Reptile Study merit badge. Because of this, he and director Steven Spielberg later decided to depict the young Indiana Jones as a Life Scout in the film "Indiana Jones and the Last Crusade". They also jokingly reversed Ford's knowledge of reptiles into Jones' fear of snakes.
In 1960, Ford graduated from Maine East High School in Park Ridge, Illinois. His was the first student voice broadcast on his high school's new radio station, WMTH, and he was its first sportscaster during his senior year (1959–60). He attended Ripon College in Wisconsin, where he was a member of the Sigma Nu fraternity. He took a drama class in the final quarter of his senior year to get over his shyness. Ford, a self-described "late bloomer," became fascinated with acting.
Early career.
In 1964, after a season of summer stock with the Belfry Players in Wisconsin, Ford traveled to Los Angeles to apply for a job in radio voice-overs. He did not get it, but stayed in California and eventually signed a $150-a-week contract with Columbia Pictures' "New Talent" program, playing bit roles in films. His first known part was an uncredited role as a bellhop in "Dead Heat on a Merry-Go-Round" (1966). There is little record of his non-speaking roles (or "extra" work) in film. Ford was at the bottom of the hiring list, having offended producer Jerry Tokovsky after he played a bellboy in the feature. He was told by Tokovsky that when actor Tony Curtis delivered a bag of groceries, he did it like a movie star; Ford felt his job was to act like a bellboy. Ford managed to secure other roles in movies, such as "A Time for Killing (The Long Ride Home)", starring Glenn Ford, George Hamilton, and Inger Stevens.
His speaking roles continued next with "Luv" (1967), though he was still uncredited. He was finally credited as "Harrison J. Ford" in the 1967 Western film, "A Time for Killing", but the "J" did not stand for anything, since he has no middle name. It was added to avoid confusion with a silent film actor named Harrison Ford, who appeared in more than 80 films between 1915 and 1932, and died in 1957. Ford later said that he was unaware of the existence of the earlier Harrison Ford until he came upon a star with his own name on the Hollywood Walk of Fame. Ford soon dropped the "J" and worked for Universal Studios, playing minor roles in many television series throughout the late 1960s and early 1970s, including "Gunsmoke", "Ironside", "The Virginian", "The F.B.I.", "Love, American Style", and "Kung Fu". He appeared in the western "Journey to Shiloh" (1968) and had an uncredited, non-speaking role in Michelangelo Antonioni's 1970 film "Zabriskie Point", as an arrested student protester. Not happy with the roles being offered to him, Ford became a self-taught professional carpenter to support his then-wife and two small sons. While working as a carpenter, he became a stagehand for the popular rock band The Doors. He also built a sun deck for actress Sally Kellerman and a recording studio for Brazilian band leader Sérgio Mendes.
Casting director and fledgling producer Fred Roos championed the young Ford, and secured him an audition with George Lucas for the role of Bob Falfa, which Ford went on to play in "American Graffiti" (1973). Ford's relationship with Lucas would profoundly affect his career later on. After director Francis Ford Coppola's film "The Godfather" was a success, he hired Ford to expand his office and gave him small roles in his next two films, "The Conversation" (1974) and "Apocalypse Now" (1979); in the latter film he played an army officer named "G. Lucas".
Milestone franchises.
"Star Wars".
Harrison Ford's previous work in "American Graffiti" eventually landed him his first starring film role, when he was hired by Lucas to read lines for actors auditioning for parts in his then-upcoming film "Star Wars" (1977). Lucas was eventually won over by Ford's performance during these line reads and cast him as Han Solo. "Star Wars" became one of the most successful movies of all time and established Ford as a superstar. He went on to star in the similarly successful "Star Wars" sequels, "The Empire Strikes Back" (1980) and "Return of the Jedi" (1983), as well as the "Star Wars Holiday Special" (1978). Ford wanted Lucas to kill off Han Solo at the end of "Return of the Jedi", saying, "That would have given the whole film a bottom," but Lucas refused. However, in an interview in 2015, Ford admitted that "he was wrong" to want his character killed off.
Ford reprised the role of Han Solo in the sequel "" (2015). During filming on June 11, 2014, Ford suffered what is said to be a fractured ankle, when a hydraulic door fell on him. He was rushed to the hospital for treatment. Ford's son Ben released details on his father's injury, saying that his ankle would likely need a plate and screws, and that filming could be altered slightly with the crew needing to shoot Ford from the waist up for a short time until he recovers. Ford made his return to filming in mid-August, after a two-month layoff as he recovered from his injury. Ford's character was killed off in "The Force Awakens"; however, it was subsequently announced, via a casting call, that Ford would return in some capacity as Han Solo in "Episode VIII". In February 2016, when the cast for Episode VIII was confirmed, it has indicated that Ford won't reprise his role in the film. When Ford was asked if his character could come back in "some form", he replied, "Anything is possible in space." A Han Solo spin-off movie is scheduled to be made, but Ford is not involved in the production. "The Daily Mail" reported that Ford was paid £16 million plus a 0.5% share of the revenue to appear in "The Force Awakens". In the original 1977 film, Ford was paid $10,000.
"Indiana Jones".
Ford's status as a leading actor was solidified when he starred as globe-trotting archeologist Indiana Jones in the film "Raiders of the Lost Ark" (1981), a collaboration between George Lucas and Steven Spielberg. Though Spielberg was interested in casting Ford from the start, Lucas was not, due to having already worked with the actor in "American Graffiti" and "Star Wars", but he eventually relented after Tom Selleck was unable to accept.
Ford went on to star in the prequel "Indiana Jones and the Temple of Doom" (1984) and the sequel "Indiana Jones and the Last Crusade" (1989). He returned to the role yet again for a 1993 episode of the television series "The Young Indiana Jones Chronicles", and even later for the fourth film "Indiana Jones and the Kingdom of the Crystal Skull" (2008). On March 15, 2016 Walt Disney Studios announced that Ford will appear in the fifth film due for a release in July 2019.
Other film work.
Ford has been in other films, including "Heroes" (1977), "Force 10 from Navarone" (1978), and "Hanover Street" (1979). Ford also co-starred alongside Gene Wilder in the buddy-Western "The Frisco Kid" (1979), playing a bank robber with a heart of gold. He then starred as Rick Deckard in Ridley Scott's cult sci-fi classic "Blade Runner" (1982), and in a number of dramatic-action films: Peter Weir's "Witness" (1985) and "The Mosquito Coast" (1986), and Roman Polanski's "Frantic" (1988).
The 1990s brought Ford the role of Jack Ryan in Tom Clancy's "Patriot Games" (1992) and "Clear and Present Danger" (1994), as well as leading roles in Alan Pakula's "Presumed Innocent" (1990) and "The Devil's Own" (1997), Andrew Davis' "The Fugitive (1993), "Sydney Pollack's remake of "Sabrina (1995), "and Wolfgang Petersen's "Air Force One" (1997). Ford also played straight dramatic roles, including an adulterous husband in both "Presumed Innocent" (1990) and "What Lies Beneath" (2000), and a recovering amnesiac in Mike Nichols' "Regarding Henry" (1991).
Many of Ford's major film roles came to him by default through unusual circumstances: he won the role of Han Solo while reading lines for other actors, was cast as Indiana Jones because Tom Selleck was not available, and took the role of Jack Ryan supposedly due to Alec Baldwin's fee demands, although Baldwin disputes this (Baldwin had previously played the role in "The Hunt for Red October").
1990s–2010s.
Starting in the late 1990s, Ford appeared in several critically derided and commercially disappointing movies, including "Six Days Seven Nights" (1998), "Random Hearts" (1999), "" (2002), "Hollywood Homicide" (2003), "Firewall" (2006), and "Extraordinary Measures" (2010). One exception was 2000's "What Lies Beneath," which grossed over $155 million in the United States and $291 million worldwide.
In 2004, Ford declined a chance to star in the thriller "Syriana", later commenting that "I didn't feel strongly enough about the truth of the material and I think I made a mistake." The role eventually went to George Clooney, who won an Oscar and a Golden Globe for his work. Prior to that, he had passed on a role in another Stephen Gaghan-written role, Robert Wakefield in "Traffic". That role went to Michael Douglas.
In 2008, Ford enjoyed success with the release of "Indiana Jones and the Kingdom of the Crystal Skull", another Lucas/Spielberg collaboration. The film received generally positive reviews and was the second highest-grossing film worldwide in 2008. He later said he would like to star in another sequel, "...if it didn't take another 20 years to digest."
Other 2008 work included "Crossing Over", directed by Wayne Kramer. In the film, he plays an immigrations officer, working alongside Ashley Judd and Ray Liotta. He also narrated a feature documentary film about the Dalai Lama entitled "Dalai Lama Renaissance".
Ford filmed the medical drama "Extraordinary Measures" in 2009 in Portland, Oregon. Released January 22, 2010, the film also starred Brendan Fraser and Alan Ruck. Also in 2010, he co-starred in the film "Morning Glory," along with Patrick Wilson, Rachel McAdams, and Diane Keaton.
In July 2011, Ford starred alongside Daniel Craig and Olivia Wilde in the science fiction Western film "Cowboys & Aliens". To promote the film, Ford appeared at the San Diego Comic-Con International and, apparently surprised by the warm welcome, told the audience, "I just wanted to make a living as an actor. I didn't know about this."
In 2011, Ford starred in Japanese commercials advertising the video game ' for the PlayStation 3. In 2013, Ford co-starred in the corporate espionage thriller "Paranoia", with Liam Hemsworth and Gary Oldman, and directed by Robert Luketic, as well as "Ender's Game", "42", and '.
On February 26, 2015, Alcon Entertainment announced Ford would reprise his role as Rick Deckard in the sequel to "Blade Runner".
Personal life.
Marriages and family.
Ford is one of Hollywood's most private actors, guarding much of his personal life. He has two sons, Benjamin (born 1967) and Willard, with his first wife, Mary Marquardt, to whom he was married from 1964 until their divorce in 1979. With his second wife, screenwriter Melissa Mathison, whom he married in March 1983 and from whom he was separated in August 2001 and eventually divorced, he has two more children, Malcolm and Georgia (born 1990).
Ford began dating actress Calista Flockhart after meeting at the 2002 Golden Globes, and together they are parents to her adopted son, Liam (born 2001). Ford proposed to Flockhart over Valentine's Day weekend in 2009. They married on June 15, 2010, in Santa Fe, New Mexico, where Ford was filming "Cowboys & Aliens".
Ford has three grandchildren: Eliel (born 1993), Ethan (born 2000) and Waylon (2010). Son Benjamin, a chef and restaurateur, owns Ford's Filling Station, a gastropub at The Marriott, L.A. Live, Los Angeles. Son Willard is the owner of Strong Sports Gym, and was co-owner of Ford & Ching and owner of the Ludwig Clothing company.
Back injury.
In June 1983, at age 40, during the filming of "Indiana Jones and the Temple of Doom" in London, he herniated a disc in his back, forcing him to fly back to Los Angeles for an operation. He returned six weeks later.
Ankle injury.
On June 11, 2014, Ford injured his ankle during filming of "". He was airlifted to John Radcliffe Hospital in Oxford, England. Ford's wife soon traveled from the U.S. to be at his hospital bedside, as it was feared that injuries sustained on the set could be worse than previously thought. Doctors suspected that his ankle might have been broken and he might have received a pelvic injury. Producers stated that filming would continue as planned.
Aviation.
Ford is a private pilot of both fixed-wing aircraft and helicopters, and owns an ranch in Jackson, Wyoming, approximately half of which he has donated as a nature reserve. On several occasions, Ford has personally provided emergency helicopter services at the request of local authorities, in one instance rescuing a hiker overcome by dehydration.
Ford began flight training in the 1960s at Wild Rose Idlewild Airport in Wild Rose, Wisconsin, flying in a Piper PA-22 Tri-Pacer, but at $15 an hour, he could not afford to continue the training. In the mid-1990s, he bought a used Gulfstream II and asked one of his pilots, Terry Bender, to give him flying lessons. They started flying a Cessna 182 out of Jackson, Wyoming, later switching to Teterboro, New Jersey, flying a Cessna 206, the aircraft he soloed in.
On October 23, 1999, Harrison Ford was involved in the crash of a Bell 206L4 LongRanger helicopter (N36R). The NTSB accident report states that Ford was piloting the aircraft over the Lake Piru riverbed near Santa Clarita, California, on a routine training flight. While making his second attempt at an autorotation with powered recovery, Ford allowed the aircraft's altitude to drop to 150–200 feet before beginning power-up. The aircraft was unable to recover power before hitting the ground. The aircraft landed hard and began skidding forward in the loose gravel before one of its skids struck a partially embedded log, flipping the aircraft onto its side. Neither Ford nor the instructor pilot suffered any injuries, though the helicopter was seriously damaged. When asked about the incident by fellow pilot James Lipton in an interview on the TV show "Inside the Actor's Studio", Ford replied, "I broke it."
Ford keeps his aircraft at Santa Monica Airport, though the Bell 407 is often kept and flown in Jackson, Wyoming, and has been used by the actor in two mountain rescues during the actor's assigned duty time assisting the Teton County Search and Rescue. On one of the rescues, Ford recovered a hiker who had become lost and disoriented. She boarded Ford's Bell 407 and promptly vomited into one of the rescuers' caps, unaware of who the pilot was until much later; "I can't believe I barfed in Harrison Ford's helicopter!" she said later.
Ford flies his de Havilland Canada DHC-2 Beaver (N28S) more than any of his other aircraft, and has repeatedly said that he likes this aircraft and the sound of its Pratt & Whitney R-985 radial engine. According to Ford, it had been flown in the CIA's Air America operations, and was riddled with bullet holes that had to be patched up.
In March 2004, Ford officially became chairman of the Young Eagles program of the Experimental Aircraft Association (EAA). Ford was asked to take the position by Greg Anderson, Senior Vice President of the EAA at the time, to replace General Charles "Chuck" Yeager, who was vacating the post that he had held for many years. Ford at first was hesitant, but later accepted the offer and has made appearances with the Young Eagles at the EAA AirVenture Oshkosh gathering at Oshkosh, Wisconsin, for two years. In July 2005, at the gathering in Oshkosh, Ford agreed to accept the position for another two years. Ford has flown over 280 children as part of the Young Eagles program, usually in his DHC-2 Beaver, which can seat the actor and five children. He is involved with the EAA chapter in Driggs, Idaho, just over the Teton Range from Jackson, Wyoming.
As of 2009, Ford appears in internet advertisements for General Aviation Serves America, a campaign by the advocacy group Aircraft Owners and Pilots Association (AOPA). He has also appeared in several independent aviation documentaries, including "Wings Over the Rockies" (2009), "" (2014), and "Living in the Age of Airplanes" (2015). 
Ford is an honorary board member of the humanitarian aviation organization Wings of Hope.
On March 5, 2015, Ford's plane, believed to be a Ryan PT-22 Recruit, made an emergency landing on the Penmar Golf Course in Venice, California. Ford had radioed in to report that the plane had suffered engine failure. He was taken to Ronald Reagan UCLA Medical Center, where he was reported to be in fair to moderate condition. Ford suffered a broken pelvis and broken ankle during the accident, as well as other injuries.
Activism.
Environmental causes.
Ford is vice-chair of Conservation International an American nonprofit environmental organization headquartered in Arlington, Virginia. The organization's intent is to protect nature. The institution tries to combine the services or benefits of science, field work, and partnership to find global solutions to global problems. Three ways CI goes about solving nature-related problems are: 1) identifying and moving to protect locations that are crucial, such as those affecting water, food, and air; 2) working with large companies that are involved in energy and agriculture, to ensure the environment is being protected; and 3) working with governments to ensure they have the knowledge and the proper tools to construct policies that are environmentally friendly.
From its origins as an NGO dedicated to protecting tropical biodiversity, CI has evolved into an organization that works with governments, scientists, charitable foundations, and business. CI has been criticised for links to companies with a poor environmental record such as BP, Cargill, Chevron, Monsanto and Shell and for allegedly offering greenwashing services. CI has also been chastised for poor judgment in its expenditure of donors' money.
In September 2013, Ford, while filming an environmental documentary in Indonesia, interviewed the Indonesian Forestry Minister, Zulkifli Hasan. After the interview the Presidential Advisor, Andi Arief, accused Ford and his crew of "harassing state institutions" and publicly threatened them with deportation. Questions within the interview concerned the Tesso Nilo National Park, Sumatra. It was alleged the Minister of Forestry was given no prior warning of questions nor the chance to explain the challenges of catching people with illegal logging. Ford was provided an audience with the Indonesian President, Susilo Bambang Yudhoyono, during which he expressed concerns regarding Indonesia's environmental degradation and the government efforts to address climate change. In response, the President explained Indonesia's commitment to preserving its oceans and forests.
In 1993, the arachnologist Norman Platnick named a new species of spider "Calponia harrisonfordi", and in 2002, the entomologist Edward O. Wilson named a new ant species "Pheidole harrisonfordi" (in recognition of Harrison's work as Vice Chairman of Conservation International).
Since 1992, Ford has lent his voice to a series of public service messages promoting environmental involvement for EarthShare, an American federation of environmental and conservation charities.
Ford has been a spokesperson for Restore Hetch Hetchy, a non-profit organization dedicated to restoring Yosemite National Park's Hetch Hetchy Valley to its original condition.
Ford appears in the documentary series "Years of Living Dangerously", which provides reports on those affected by, and seeking solutions to climate change.
Political views.
Like his parents, Ford is a lifelong Democrat, and a close friend of former President Bill Clinton.
On September 7, 1995, Ford testified before the U.S. Senate Foreign Relations Committee in support of the Dalai Lama and an independent Tibet. In 2008, he narrated the documentary "Dalai Lama Renaissance".
In 2003, he publicly condemned the Iraq War and called for "regime change" in the United States. He also criticized Hollywood for making violent movies, and called for more gun control in the United States.
After Republican presidential candidate Donald Trump said his favorite role of Ford's was "Air Force One" because he "stood up for America", Ford reasoned that it was just a film and was doubtful that Trump's presidential bid would be successful.
Archaeology.
Following on his success portraying the archaeologist Indiana Jones, Ford also plays a part in supporting the work of professional archaeologists. He serves as a General Trustee on the Governing Board of the Archaeological Institute of America (AIA), North America's oldest and largest organization devoted to the world of archaeology. Ford assists them in their mission of increasing public awareness of archaeology and preventing looting and the illegal antiquities trade.
Awards and honors.
Ford received a nomination for the Academy Award for Best Actor for "Witness", for which he also received "Best Actor" BAFTA and Golden Globe nominations. He received the Cecil B. DeMille Award at the 2002 Golden Globe Awards and on June 2, 2003, he received a star on the Hollywood Walk of Fame. He has received three additional "Best Actor" Golden Globe nominations for "The Mosquito Coast", "The Fugitive" and "Sabrina".
He received the first ever Hero Award for his many iconic roles, including Han Solo and Indiana Jones, at the 2007 Scream Awards, and in 2008, the Spike TV's Guy's Choice Award for Brass Balls.
Ford has also been honored multiple times for his involvement in general aviation, receiving the Living Legends of Aviation Award and EAA's Freedom of Flight Award in 2009, Wright Brothers Memorial Trophy in 2010, and the Al Ueltschi Humanitarian Award in 2013. In 2013, "Flying Magazine" ranked him number 48 on their list of the 51 Heroes of Aviation.
Harrison Ford received the AFI Life Achievement Award in 2000.
External links.
Interviews

</doc>
<doc id="44060" url="https://en.wikipedia.org/wiki?curid=44060" title="Barents Sea">
Barents Sea

The Barents Sea (, "Barentsevo More" ) is a marginal sea of the Arctic Ocean, located off the northern coasts of Norway and Russia with vast majority of it lying in Russian territorial waters. Known in the Middle Ages as the Murman Sea, the sea takes its current name from the Dutch navigator Willem Barentsz.
It is a rather shallow shelf sea, with an average depth of , and is an important site for both fishing and hydrocarbon exploration. The Barents Sea is bordered by the Kola Peninsula to the south, the shelf edge towards the Norwegian Sea to the west, and the archipelagos of Svalbard to the northwest, Franz Josef Land to the north east and Novaya Zemlya to the east. Novaya Zemlya, an extension of the northern part of the Ural Mountains, separates the Barents Sea from the Kara Sea.
Geography.
The southern half of the Barents Sea, including the ports of Murmansk (Russia) and Vardø (Norway) remain ice-free year round due to the warm North Atlantic drift. In September, the entire Barents Sea is more or less completely ice-free. Until the Winter War (1939–40), Finland's territory also reached to the Barents Sea, with the harbor at Petsamo being Finland's only ice-free winter harbor.
There are three main types of water masses in the Barents Sea: Warm, salty Atlantic water (temperature >3 °C, salinity >35) from the North Atlantic drift, cold Arctic water (temperature <0 °C, salinity <35) from the north, and warm, but not very salty coastal water (temperature >3 °C, salinity <34.7). Between the Atlantic and Polar waters, a front called the Polar Front is formed. In the western parts of the sea (close to Bear Island), this front is determined by the bottom topography and is therefore relatively sharp and stable from year to year, while in the east (towards Novaya Zemlya), it can be quite diffuse and its position can vary a lot between years.
The lands of Novaya Zemlya attained most of their early Holocene coastal deglaciation approximately 10,000 years before present.
Extent.
The International Hydrographic Organization defines the limits of the "Barentsz Sea" as follows:
Other islands in the Barents Sea include Chaichy and Timanets.
Geology.
The Barents Sea was originally formed from two major continental collisions: the Caledonian orogeny, in which the Baltica and Laurentia collided to form Laurasia, and a subsequent collision between Laurasia and Western Siberia. Most of its geological history is dominated by extensional tectonics, caused by the collapse of the Caledonian and Uralian orogenic belts and the break-up of Pangaea. These events created the major rift basins that dominate the Barents Shelf, along with various platforms and structural highs. The later geological history of the Barents Sea is dominated by Late Cenozoic uplift, particularly that caused by Quaternary glaciation, which has resulted in erosion and deposition of significant sediment.
Ecology.
Due to the North Atlantic drift, the Barents Sea has a high biological production compared to other oceans of similar latitude. The spring bloom of phytoplankton can start quite early close to the ice edge, because the fresh water from the melting ice makes up a stable water layer on top of the sea water. The phytoplankton bloom feeds zooplankton such as "Calanus finmarchicus", "Calanus glacialis", "Calanus hyperboreus", "Oithona" spp., and krill. The zooplankton feeders include young cod, capelin, polar cod, whales, and little auk. The capelin is a key food for top predators such as the north-east Arctic cod, harp seals, and seabirds such as common guillemot and Brunnich's guillemot. The fisheries of the Barents Sea, in particular the cod fisheries, are of great importance for both Norway and Russia.
SIZEX-89 was an international winter experiment where the main objectives were to perform sensor signature studies of different ice types in order to develop SAR algorithms for ice variables such as ice types, ice concentrations and ice kinematics.
Although previous research suggested that predation by whales may be the cause of depleting fish stocks, more recent research suggests that marine mammal consumption has only a trivial influence on fisheries and a model examining the impact of fisheries and climate was far more accurate at describing trends in fish abundance. There is a genetically distinct polar bear population associated with the Barents Sea.
History.
Name.
The Barents Sea was formerly known to Russians as Murmanskoye Morye, or the "Sea of Murmans" (i.e., Norwegians), and it appears with this name in sixteenth-century maps, including Gerard Mercator's "Map of the Arctic" published in his 1595 atlas. Its eastern corner, in the region of the Pechora River's estuary, has been known as Pechorskoye Morye, that is, Pechora Sea.
This sea was given its present name in honor of Willem Barentsz, a Dutch navigator and explorer. Barentsz was the leader of early expeditions to the far north, at the end of the sixteenth century.
Modern Era.
Seabed mapping was completed in 1933 with the first full map produced by Russian marine geologist Maria Klenova.
The Barents Sea was also the site of a notable World War II engagement, a German surface merchant raiding attack on a British convoy that later became known as the Battle of the Barents Sea. Under the command of Oskar Kummetz, the German warships sank minelayer HMS "Bramble" and destroyer , but in turn lost destroyer and was severely damaged by British gunfire. The Germans later retreated and the British convoy arrived safely at Murmansk shortly afterwards.
During the Cold War, the Soviet Red Banner Northern Fleet used the southern reaches of the sea as a ballistic missile submarine bastion, a strategy that Russia continues. Nuclear contamination from dumped Russian naval reactors is an environmental concern in the Barents Sea.
Economy.
Political Status.
For decades there was a boundary dispute between Norway and Russia regarding the position of the boundary between their respective claims to the Barents Sea. The Norwegians favoured a median line, based on the Geneva Convention of 1958, whereas the Russians favoured a meridian based sector line, based on a Soviet decision of 1926. This led to a neutral "grey" zone between the competing claims that had an area of 175,000 sq.km, which is approximately 12% of the total area of the Barents Sea. The two countries started negotiations on the location of the boundary in 1974 and a moratorium on hydrocarbon exploration was declared in 1976.
In 2010, Norway and Russia signed an agreement that placed the boundary equidistant from their competing claims. This was ratified and went into force on 7 July 2011, opening the grey zone for hydrocarbon exploration.
Oil and Gas.
Encouraged by the success of the North Sea in the 1960s, hydrocarbon exploration in the Barents Sea got underway in 1969. The Norwegian authorities acquired seismic reflection surveys through the following years, which were analysed to understand the location of the main sedimentary basins. NorskHydro drilled the first well in 1980, which was a dry hole, and the first discoveries were made the following year - the Alke and Askeladden gas fields. Several more discoveries were made on the Norwegian side of the Barents Sea throughout the 1980s, including the important Snøhvit field. However interest in the area began to wane due to a succession of dry holes, the wells only containing gas (which was cheap at the time) and the prohibitive costs of developing wells in such a remote area. Interest in the area was reignited in the late 2000s, after the Snovhit field was finally bought into production and two new large discoveries were made.
Exploration on the Russian side got underway around the same time as that on Norwegian side, encouraged by the success in the Timan-Pechora Basin. The first wells were drilled in the early 1980s and some very large gas fields were discovered throughout this decade. The Shtokman field was discovered in 1988 and is classed as a giant gas field—currently the 5th largest gas field in the world. However, due to the same reasons that interest declined in the Norwegian side of the Barents Sea, in addition to the political instability of the 1990s, interest in the Russian side of the Barents Sea declined.
Fishing.
The Barents Sea contains the world largest remaining cod population, as well as an important stocks of haddock and capelin. Fishing is managed jointly by Russia and Norway in the form of the Joint Norwegian–Russian Fisheries Commission, established in 1976, in an attempt to keep track of how many fish are leaving the ecosystem due to fishing. The Joint Norwegian-Russian Fisheries Commission sets Total Allowable Catches (TACs) for multiple species throughout their migratory tracks. Through the Commission, Norway and Russia also exchange fishing quotas and catch statistics to ensure the TACs are not being violated. However, there are problems with the system and the effects of fishing on the Barents Sea ecosystem are not completely accurate. Cod is one of the major catches. A large portion of catches are not reported when the fishing boats land to account for profits that are being lost to high taxes and fees. Since many fishermen do not strictly follow the TACs and rules set forth by the Commission, the amount of fish being extracted annually from the Barents Sea is underestimated.
Barents Sea biodiversity and marine bioprospecting.
The Barents Sea, where temperate waters from the Gulf Stream and cold waters from the Arctic meet, is home to an enormous diversity of organisms, which are well adapted to the extreme conditions of their marine habitats. This makes these arctic species very attractive for marine bioprospecting. Marine bioprospecting may be defined as the search for bioactive molecules and compounds from marine sources having new, unique properties and the potential for commercial applications. Amongst others, applications include medicines, food and feed, textiles, cosmetics and the process industry.
The Norwegian government strategically supports the development of marine bioprospecting as it has the potential to contribute to new and sustainable wealth creation. Tromsø and the northern areas of Norway play a central role in this strategy due to excellent access to unique Arctic marine organisms and the presence of marine industries and R&D competence and infrastructure in this region.Since 2007 science and industry have cooperated closely on bioprospecting and the development and commercialization of new products.
Institutions and industry supporting marine bioprospecting in Barents Sea
MabCent-SFI is one of fourteen Research-Based Innovation Centers initiated by the Research Council of Norway, and is the only one within the field of “bioactive compounds and drug discovery” that is based on bioactives from marine organisms. MabCent-SFI maintains a focus on bioactives from Arctic and sub-Arctic organisms. By the end of 2011, MabCent has tested about 200,000 extracts, finding several hundred "hits". Through further research and development, some of these hits will become valuable "leads", i.e. characterized compounds known to possess biological effects of interest.
The commercial partners in MabCent-SFI are Biotec Pharmacon ASA and its subsidiary ArcticZymes AS, ABC BioScience AS, Lytix Biopharma AS and Pronova BioPharma ASA. ArcticZymes is also a partner in MARZymes, a project financed by the Research Council of Norway to find marine enzymes which are adapted to the extreme conditions in the Arctic. The science partners in MabCent-SFI are Marbank, a national marine biobank located in Tromsø, Marbio, a medium/high-throughput platform for screening and identification of bioactive compounds and Norstruct, a protein structure determination platform. Mabcent-SFI is hosted by the University of Tromsø.
BioTech North is an emerging biotechnology cluster of enterprises and R&D organizations, which cooperate closely with regional funding and development actors (triple helix). As bioactive molecules and compounds from Arctic marine resources form the basis of activities for the majority of the cluster members, BioTech North serves as a marine biotech cluster. The majority of BioTech North’s enterprises are active within life science applications and markets. To date the cluster contains around thirty organizations from both the private and public sector. It has received Arena status and is funded through the [ Arena] programme financed by Innovation Norway, SIVA and The Research Council of Norway. Stakeholders of BioTech North include Barents BioCentre Lab, BioStruct, Marbank, Norut, Nofima, Mabcent-SFI, University of Tromsø, Unilab, Barentzymes AS, Trofi, Scandiderma AS, Prophylix Pharma AS, Olivita, Marealis, ProCelo, Probio, Lytix Biopharma, Integorgen, d'Liver, Genøk, Cognis, Clare AS, Chitinor, Calanus AS, Biotec Betaglucans, Ayanda, ArcticZymes AS, ABC Bioscience, Akvaplanniva.

</doc>
<doc id="44061" url="https://en.wikipedia.org/wiki?curid=44061" title="Convulsion">
Convulsion

A convulsion is a medical condition where body muscles contract and relax rapidly and repeatedly, resulting in an uncontrolled shaking of the body. Because a convulsion is often a symptom of an epileptic seizure, the term "convulsion" is sometimes used as a synonym for "seizure". However, not all epileptic seizures lead to convulsions, and not all convulsions are caused by epileptic seizures. Convulsions are also consistent with an electric shock and improper Enriched Air Scuba Diving. For non-epileptic convulsions, see non-epileptic seizures.
The word "fit" is sometimes used to mean a convulsion or epileptic seizure.
Symptoms.
When a person is having a convulsion, they may experience several different symptoms. These may include: a brief blackout, confusion, drooling, loss of bowel/bladder control, sudden shaking of entire body, uncontrollable muscle spasms, temporary cessation of breathing, and many more. Symptoms usually last from a few seconds to around 15 minutes. If someone has a fit like this, it is advised to make sure they don't fall and injure themselves, cushion their head and loosen any restricting clothing/jewelry, and also call for medical help. Do not try to pin/hold them in place, as this could possibly cause harm or injury to the individual.
Causes.
Convulsions are often caused by some sort of electrical activity mishap in the brain. Oftentimes, the cause is not able to be pinpointed. Convulsions can be caused by chemicals in the blood, as well as infections like meningitis or encephalitis. A very common cause of convulsions is fevers. Other possibilities include head trauma, stroke or lack of oxygen to the brain. Sometimes the convulsion can be caused by genetic defects or brain tumors. Convulsions can also be caused by any type-1 diabetic, whose blood sugar is too low. Hypoglycemia can cause very bad convulsions until the person's blood sugar is raised to normal level.
Generalized seizures.
The most common type of seizure is called a generalized seizure, also known as a generalized convulsion. This is characterized by a loss of consciousness which may lead to the person collapsing. The body stiffens for about a minute and then jerks uncontrollably for the next minute. During this, the patient may fall and injure themselves or bite their tongue and lose control of their bladder. A familial history of this puts a person at a greater risk for developing them.

</doc>
<doc id="44062" url="https://en.wikipedia.org/wiki?curid=44062" title="X-ray astronomy">
X-ray astronomy

X-ray astronomy is an observational branch of astronomy which deals with the study of X-ray observation and detection from astronomical objects. X-radiation is absorbed by the Earth's atmosphere, so instruments to detect X-rays must be taken to high altitude by balloons, sounding rockets, and satellites. X-ray astronomy is the space science related to a type of space telescope that can see farther than standard light-absorption telescopes, such as the Mauna Kea Observatories, via x-ray radiation.
X-ray emission is expected from astronomical objects that contain extremely hot gasses at temperatures from about a million kelvin (K) to hundreds of millions of kelvin (MK). Although X-rays have been observed emanating from the Sun since the 1940s, the discovery in 1962 of the first cosmic X-ray source was a surprise. This source is called Scorpius X-1 (Sco X-1), the first X-ray source found in the constellation Scorpius. The X-ray emission of Scorpius X-1 is 10,000 times greater than its visual emission, whereas that of the Sun is about a million times less. In addition, the energy output in X-rays is 100,000 times greater than the total emission of the Sun in all wavelengths. Based on discoveries in this new field of X-ray astronomy, starting with Scorpius X-1, Riccardo Giacconi received the Nobel Prize in Physics in 2002. It is now known that such X-ray sources as Sco X-1 are compact stars, such as neutron stars or black holes. Material falling into a black hole may emit X-rays, but the black hole itself does not. The energy source for the X-ray emission is gravity. Infalling gas and dust is heated by the strong gravitational fields of these and other celestial objects.
Many thousands of X-ray sources are known. In addition, the space between galaxies in galaxy clusters is filled with a very hot, but very dilute gas at a temperature between 10 and 100 megakelvins (MK). The total amount of hot gas is five to ten times the total mass in the visible galaxies.
Sounding rocket flights.
The first sounding rocket flights for X-ray research were accomplished at the White Sands Missile Range in New Mexico with a V-2 rocket on January 28, 1949. A detector was placed in the nose cone section and the rocket was launched in a suborbital flight to an altitude just above the atmosphere.
X-rays from the Sun were detected by the U.S. Naval Research Laboratory Blossom experiment on board. An Aerobee 150 rocket was launched on June 12, 1962 and it detected the first X-rays from other celestial sources (Scorpius X-1).
The largest drawback to rocket flights is their very short duration (just a few minutes above the atmosphere before the rocket falls back to Earth) and their limited field of view. A rocket launched from the United States will not be able to see sources in the southern sky; a rocket launched from Australia will not be able to see sources in the northern sky.
X-ray Quantum Calorimeter (XQC) project.
In astronomy, the interstellar medium (or ISM) is the gas and cosmic dust that pervade interstellar space: the matter that exists between the star systems within a galaxy. It fills interstellar space and blends smoothly into the surrounding intergalactic medium. The interstellar medium consists of an extremely dilute (by terrestrial standards) mixture of ions, atoms, molecules, larger dust grains, cosmic rays, and (galactic) magnetic fields. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field.
Of interest is the hot ionized medium (HIM) consisting of a coronal cloud ejection from star surfaces at 106-107 K which emits X-rays. The ISM is turbulent and full of structure on all spatial scales. Stars are born deep inside large complexes of molecular clouds, typically a few parsecs in size. During their lives and deaths, stars interact physically with the ISM. Stellar winds from young clusters of stars (often with giant or supergiant HII regions surrounding them) and shock waves created by supernovae inject enormous amounts of energy into their surroundings, which leads to hypersonic turbulence. The resultant structures are stellar wind bubbles and superbubbles of hot gas. The Sun is currently traveling through the Local Interstellar Cloud, a denser region in the low-density Local Bubble.
To measure the spectrum of the diffuse X-ray emission from the interstellar medium over the energy range 0.07 to 1 keV, NASA launched a Black Brant 9 from White Sands Missile Range, New Mexico on May 1, 2008. The Principal Investigator for the mission is Dr. Dan McCammon of the University of Wisconsin.
Balloons.
Balloon flights can carry instruments to altitudes of up to 40 km above sea level, where they are above as much as 99.997% of the Earth's atmosphere. Unlike a rocket where data are collected during a brief few minutes, balloons are able to stay aloft for much longer. However, even at such altitudes, much of the X-ray spectrum is still absorbed. X-rays with energies less than 35 keV (5,600 aJ) cannot reach balloons. On July 21, 1964, the Crab Nebula supernova remnant was discovered to be a hard X-ray (15 – 60 keV) source by a scintillation counter flown on a balloon launched from Palestine, Texas, USA. This was likely the first balloon-based detection of X-rays from a discrete cosmic X-ray source.
High-energy focusing telescope.
The high-energy focusing telescope (HEFT) is a balloon-borne experiment to image astrophysical sources in the hard X-ray (20–100 keV) band. Its maiden flight took place in May 2005 from Fort Sumner, New Mexico, USA. The angular resolution of HEFT is ~1.5'. Rather than using a grazing-angle X-ray telescope, HEFT makes use of a novel tungsten-silicon multilayer coatings to extend the reflectivity of nested grazing-incidence mirrors beyond 10 keV. HEFT has an energy resolution of 1.0 keV full width at half maximum at 60 keV. HEFT was launched for a 25-hour balloon flight in May 2005. The instrument performed within specification and observed Tau X-1, the Crab Nebula.
High-resolution gamma-ray and hard X-ray spectrometer (HIREGS).
A balloon-borne experiments called the High-resolution gamma-ray and hard X-ray spectrometer (HIREGS) made observed in X-ray and gamma-rays It was launched from McMurdo Station, Antarctica in December 1991. Steady winds carried the balloon on a circumpolar flight lasting about two weeks.
Rockoons.
The rockoon (a portmanteau of rocket and balloon) was a solid fuel rocket that, rather than being immediately lit while on the ground, was first carried into the upper atmosphere by a gas-filled balloon. Then, once separated from the balloon at its maximum height, the rocket was automatically ignited. This achieved a higher altitude, since the rocket did not have to move through the lower thicker air layers that would have required much more chemical fuel.
The original concept of "rockoons" was developed by Cmdr. Lee Lewis, Cmdr. G. Halvorson, S. F. Singer, and James A. Van Allen during the Aerobee rocket firing cruise of the on March 1, 1949.
From July 17 to July 27, 1956, the Naval Research Laboratory (NRL) shipboard launched eight Deacon rockoons for solar ultraviolet and X-ray observations at ~30° N ~121.6° W, southwest of San Clemente Island, apogee: 120 km.
X-ray astronomy satellites.
X-ray astronomy satellites study X-ray emissions from celestial objects. Satellites, which can detect and transmit data about the X-ray emissions are deployed as part of branch of space science known as X-ray astronomy. Satellites are needed because X-radiation is absorbed by the Earth's atmosphere, so instruments to detect X-rays must be taken to high altitude by balloons, sounding rockets, and satellites.
X-ray telescopes and mirrors.
X-ray telescopes (XRTs) have varying directionality or imaging ability based on glancing angle reflection rather than refraction or large deviation reflection.
This limits them to much narrower fields of view than visible or UV telescopes. The mirrors can be made of ceramic or metal foil.
The first X-ray telescope in astronomy was used to observe the Sun. The first X-ray picture (taken with a grazing incidence telescope) of the Sun was taken in 1963, by a rocket-borne telescope. On April 19, 1960 the very first X-ray image of the sun was taken using a pinhole camera on an Aerobee-Hi rocket.
The utilization of X-ray mirrors for extrasolar X-ray astronomy simultaneously requires:
X-ray astronomy detectors.
X-ray astronomy detectors have been designed and configured primarily for energy and occasionally for wavelength detection using a variety of techniques usually limited to the technology of the time.
X-ray detectors collect individual X-rays (photons of X-ray electromagnetic radiation) and count the number of photons collected (intensity), the energy (0.12 to 120 keV) of the photons collected, wavelength (~0.008 to 8 nm), or how fast the photons are detected (counts per hour), to tell us about the object that is emitting them.
Astrophysical sources of X-rays.
Several types of astrophysical objects emit, fluoresce, or reflect X-rays, from galaxy clusters, through black holes in active galactic nuclei (AGN) to galactic objects such as supernova remnants, stars, and binary stars containing a white dwarf (cataclysmic variable stars and super soft X-ray sources), neutron star or black hole (X-ray binaries). Some solar system bodies emit X-rays, the most notable being the Moon, although most of the X-ray brightness of the Moon arises from reflected solar X-rays. A combination of many unresolved X-ray sources is thought to produce the observed X-ray background. The X-ray continuum can arise from bremsstrahlung, black-body radiation, synchrotron radiation, or what is called inverse Compton scattering of lower-energy photons by relativistic electrons, knock-on collisions of fast protons with atomic electrons, and atomic recombination, with or without additional electron transitions.
An intermediate-mass X-ray binary (IMXB) is a binary star system where one of the components is a neutron star or a black hole. The other component is an intermediate mass star.
Hercules X-1 is composed of a neutron star accreting matter from a normal star (HZ Herculis) probably due to Roche lobe overflow. X-1 is the prototype for the massive X-ray binaries although it falls on the borderline, , between high- and low-mass X-ray binaries.
Celestial X-ray sources.
The celestial sphere has been divided into 88 constellations. The International Astronomical Union (IAU) constellations are areas of the sky. Each of these contains remarkable X-ray sources. Some of them are have been identified from astrophysical modeling to be galaxies or black holes at the centers of galaxies. Some are pulsars. As with sources already successfully modeled by X-ray astrophysics, striving to understand the generation of X-rays by the apparent source helps to understand the Sun, the universe as a whole, and how these affect us on Earth. Constellations are an astronomical device for handling observation and precision independent of current physical theory or interpretation. Astronomy has been around for a long time. Physical theory changes with time. With respect to celestial X-ray sources, X-ray astrophysics tends to focus on the physical reason for X-ray brightness, whereas X-ray astronomy tends to focus on their classification, order of discovery, variability, resolvability, and their relationship with nearby sources in other constellations.
Within the constellations Orion and Eridanus and stretching across them is a soft X-ray "hot spot" known as the Orion-Eridanus Superbubble, the Eridanus Soft X-ray Enhancement, or simply the Eridanus Bubble, a 25° area of interlocking arcs of Hα emitting filaments. Soft X-rays are emitted by hot gas (T ~ 2–3 MK) in the interior of the superbubble. This bright object forms the background for the "shadow" of a filament of gas and dust. The filament is shown by the overlaid contours, which represent 100 micrometre emission from dust at a temperature of about 30 K as measured by IRAS. Here the filament absorbs soft X-rays between 100 and 300 eV, indicating that the hot gas is located behind the filament. This filament may be part of a shell of neutral gas that surrounds the hot bubble. Its interior is energized by ultraviolet (UV) light and stellar winds from hot stars in the Orion OB1 association. These stars energize a superbubble about 1200 lys across which is observed in the visual (Hα) and X-ray portions of the spectrum.
Proposed (future) X-ray observatory satellites.
There are several projects that are proposed for X-ray observatory satellites. See main article link above.
Explorational X-ray astronomy.
Usually observational astronomy is considered to occur on Earth's surface (or beneath it in neutrino astronomy). The idea of limiting observation to Earth includes orbiting the Earth. As soon as the observer leaves the cozy confines of Earth, the observer becomes a deep space explorer. Except for Explorer 1 and Explorer 3 and the earlier satellites in the series, usually if a probe is going to be a deep space explorer it leaves the Earth or an orbit around the Earth.
For a satellite or space probe to qualify as a deep space X-ray astronomer/explorer or "astronobot"/explorer, all it needs to carry aboard is an XRT or X-ray detector and leave Earth orbit.
Ulysses is launched October 6, 1990, and reached Jupiter for its "gravitational slingshot" in February 1992. It passed the south solar pole in June 1994 and crossed the ecliptic equator in February 1995. The solar X-ray and cosmic gamma-ray burst experiment (GRB) had 3 main objectives: study and monitor solar flares, detect and localize cosmic gamma-ray bursts, and in-situ detection of Jovian aurorae. Ulysses was the first satellite carrying a gamma burst detector which went outside the orbit of Mars. The hard X-ray detectors operated in the range 15–150 keV. The detectors consisted of 23-mm thick × 51-mm diameter CsI(Tl) crystals mounted via plastic light tubes to photomultipliers. The hard detector changed its operating mode depending on (1) measured count rate, (2) ground command, or (3) change in spacecraft telemetry mode. The trigger level was generally set for 8-sigma above background and the sensitivity is 10−6 erg/cm2 (1 nJ/m2). When a burst trigger is recorded, the instrument switches to record high resolution data, recording it to a 32-kbit memory for a slow telemetry read out. Burst data consist of either 16 s of 8-ms resolution count rates or 64 s of 32-ms count rates from the sum of the 2 detectors. There were also 16 channel energy spectra from the sum of the 2 detectors (taken either in 1, 2, 4, 16, or 32 second integrations). During 'wait' mode, the data were taken either in 0.25 or 0.5 s integrations and 4 energy channels (with shortest integration time being 8 s). Again, the outputs of the 2 detectors were summed.
The Ulysses soft X-ray detectors consisted of 2.5-mm thick × 0.5 cm2 area Si surface barrier detectors. A 100 mg/cm2 beryllium foil front window rejected the low energy X-rays and defined a conical FOV of 75° (half-angle). These detectors were passively cooled and operate in the temperature range −35 to −55 °C. This detector had 6 energy channels, covering the range 5–20 keV.
Theoretical X-ray astronomy.
Theoretical X-ray astronomy is a branch of theoretical astronomy that deals with the theoretical astrophysics and theoretical astrochemistry of X-ray generation, emission, and detection as applied to astronomical objects.
Like theoretical astrophysics, theoretical X-ray astronomy uses a wide variety of tools which include analytical models to approximate the behavior of a possible X-ray source and computational numerical simulations to approximate the observational data. Once potential observational consequences are available they can be compared with experimental observations. Observers can look for data that refutes a model or helps in choosing between several alternate or conflicting models.
Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model to fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.
Most of the topics in astrophysics, astrochemistry, astrometry, and other fields that are branches of astronomy studied by theoreticians involve X-rays and X-ray sources. Many of the beginnings for a theory can be found in an Earth-based laboratory where an X-ray source is built and studied.
Dynamos.
Dynamo theory describes the process through which a rotating, convecting, and electrically conducting fluid acts to maintain a magnetic field. This theory is used to explain the presence of anomalously long-lived magnetic fields in astrophysical bodies. If some of the stellar magnetic fields are really induced by dynamos, then field strength might be associated with rotation rate.
Astronomical models.
From the observed X-ray spectrum, combined with spectral emission results for other wavelength ranges, an astronomical model addressing the likely source of X-ray emission can be constructed. For example, with Scorpius X-1 the X-ray spectrum steeply drops off as X-ray energy increases up to 20 keV, which is likely for a thermal-plasma mechanism. In addition, there is no radio emission, and the visible continuum is roughly what would be expected from a hot plasma fitting the observed X-ray flux. The plasma could be a coronal cloud of a central object or a transient plasma, where the energy source is unknown, but could be related to the idea of a close binary.
In the Crab Nebula X-ray spectrum there are three features that differ greatly from Scorpius X-1: its spectrum is much harder, its source diameter is in light-years (ly)s, not astronomical units (AU), and its radio and optical synchrotron emission are strong. Its overall X-ray luminosity rivals the optical emission and could be that of a nonthermal plasma. However, the Crab Nebula appears as an X-ray source that is a central freely expanding ball of dilute plasma, where the energy content is 100 times the total energy content of the large visible and radio portion, obtained from the unknown source.
The "Dividing Line" as giant stars evolve to become red giants also coincides with the Wind and Coronal Dividing Lines. To explain the drop in X-ray emission across these dividing lines, a number of models have been proposed:
Analytical X-ray astronomy.
Analytical X-ray astronomy is applied to an astronomy puzzle in an attempt to provide an acceptable solution. Consider the following puzzle.
High-mass X-ray binaries (HMXBs) are composed of OB supergiant companion stars and compact objects, usually neutron stars (NS) or black holes (BH). Supergiant X-ray binaries (SGXBs) are HMXBs in which the compact objects orbit massive companions with orbital periods of a few days (3–15 d), and in circular (or slightly eccentric) orbits. SGXBs show typical the hard X-ray spectra of accreting pulsars and most show strong absorption as obscured HMXBs. X-ray luminosity ("L"x) increases up to 1036 erg·s−1 (1029 watts).
The mechanism triggering the different temporal behavior observed between the classical SGXBs and the recently discovered supergiant fast X-ray transients (SFXT)s is still debated.
Aim: use the discovery of long orbits (>15 d) to help discriminate between emission models and perhaps bring constraints on the models.
Method: analyze archival data on various SGXBs such as has been obtained by INTEGRAL for candidates exhibiting long orbits. Build short- and long-term light curves. Perform a timing analysis in order to study the temporal behavior of each candidate on different time scales.
Compare various astronomical models:
Draw some conclusions: for example, the SGXB SAX J1818.6-1703 was discovered by BeppoSAX in 1998, identified as a SGXB of spectral type between O9I−B1I, which also displayed short and bright flares and an unusually very low quiescent level leading to its classification as a SFXT. The analysis indicated an unusually long orbital period: 30.0 ± 0.2 d and an elapsed accretion phase of ~6 d implying an elliptical orbit and possible supergiant spectral type between B0.5-1I with eccentricities e ~ 0.3–0.4. The large variations in the X-ray flux can be explained through accretion of macro-clumps formed within the stellar wind.
Choose which model seems to work best: for SAX J1818.6-1703 the analysis best fits the model that predicts SFXTs behave as SGXBs with different orbital parameters; hence, different temporal behavior.
Stellar X-ray astronomy.
Stellar X-ray astronomy is said to have started on April 5, 1974, with the detection of X-rays from Capella. A rocket flight on that date briefly calibrated its attitude control system when a star sensor pointed the payload axis at Capella (α Aur). During this period, X-rays in the range 0.2–1.6 keV were detected by an X-ray reflector system co-aligned with the star sensor. The X-ray luminosity of "L"x = 1031 erg·s−1 (1024 W) is four orders of magnitude above the Sun's X-ray luminosity.
Eta Carinae.
New X-ray observations by the Chandra X-ray Observatory show three distinct structures: an outer, horseshoe-shaped ring about 2 light years in diameter, a hot inner core about 3 light-months in diameter, and a hot central source less than 1 light-month in diameter which may contain the superstar that drives the whole show. The outer ring provides evidence of another large explosion that occurred over 1,000 years ago. These three structures around Eta Carinae are thought to represent shock waves produced by matter rushing away from the superstar at supersonic speeds. The temperature of the shock-heated gas ranges from 60 MK in the central regions to 3 MK on the horseshoe-shaped outer structure. "The Chandra image contains some puzzles for existing ideas of how a star can produce such hot and intense X-rays," says Prof. Kris Davidson of the University of Minnesota. Davidson is principal investigator for the Eta Carina observations by the Hubble Space telescope. "In the most popular theory, X-rays are made by colliding gas streams from two stars so close together that they'd look like a point source to us. But what happens to gas streams that escape to farther distances? The extended hot stuff in the middle of the new image gives demanding new conditions for any theory to meet."
Stellar coronae.
Coronal stars, or stars within a coronal cloud, are ubiquitous among the stars in the cool half of the Hertzsprung-Russell diagram. Experiments with instruments aboard Skylab and Copernicus have been used to search for soft X-ray emission in the energy range ~0.14–0.284 keV from stellar coronae. The experiments aboard ANS succeeded in finding X-ray signals from Capella and Sirius (α CMa). X-ray emission from an enhanced solar-like corona was proposed for the first time. The high temperature of Capella's corona as obtained from the first coronal X-ray spectrum of Capella using HEAO 1 required magnetic confinement unless it was a free-flowing coronal wind.
In 1977 Proxima Centauri is discovered to be emitting high-energy radiation in the XUV. In 1978, α Cen was identified as a low-activity coronal source. With the operation of the Einstein observatory, X-ray emission was recognized as a characteristic feature common to a wide range of stars covering essentially the whole Hertzsprung-Russell diagram. The Einstein initial survey led to significant insights:
To fit the medium-resolution spectrum of UX Ari, subsolar abundances were required.
Stellar X-ray astronomy is contributing toward a deeper understanding of
Current wisdom has it that the massive coronal main sequence stars are late-A or early F stars, a conjecture that is supported both by observation and by theory.
Unstable winds.
Given the lack of a significant outer convection zone, theory predicts the absence of a magnetic dynamo in earlier A stars. In early stars of spectral type O and B, shocks developing in unstable winds are the likely source of X-rays.
Coolest M dwarfs.
Beyond spectral type M5, the classical αω dynamo can no longer operate as the internal structure of dwarf stars changes significantly: they become fully convective. As a distributed (or α2) dynamo may become relevant, both the magnetic flux on the surface and the topology of the magnetic fields in the corona should systematically change across this transition, perhaps resulting in some discontinuities in the X-ray
characteristics around spectral class dM5. However, observations do not seem to support this picture: long-time lowest-mass X-ray detection, VB 8 (M7e V), has shown steady emission at levels of X-ray luminosity ("L"X) ≈ 1026 erg·s−1 (1019 W) and flares up to an order of magnitude higher. Comparison with other late M dwarfs shows a rather continuous trend.
Strong X-ray emission from Herbig Ae/Be stars.
Herbig Ae/Be stars are pre-main sequence stars. As to their X-ray emission properties, some are
The nature of these strong emissions has remained controversial with models including
K giants.
The FK Com stars are giants of spectral type K with an unusually rapid rotation and signs of extreme activity. Their X-ray coronae are among the most luminous ("L"X ≥ 1032 erg·s−1 or 1025 W) and the hottest known with dominant temperatures up to 40 MK. However, the current popular hypothesis involves a merger of a close binary system in which the orbital angular momentum of the companion is transferred to the primary.
Pollux is the brightest star in the constellation Gemini, despite its Beta designation, and the 17th brightest in the sky. Pollux is a giant orange K star that makes an interesting color contrast with its white "twin", Castor. Evidence has been found for a hot, outer, magnetically supported corona around Pollux, and the star is known to be an X-ray emitter.
Amateur X-ray astronomy.
Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. The United States Air Force Academy (USAFA) is the home of the US's only undergraduate satellite program, and has and continues to develop the FalconLaunch sounding rockets. In addition to any direct amateur efforts to put X-ray astronomy payloads into space, there are opportunities that allow student-developed experimental payloads to be put on board commercial sounding rockets as a free-of-charge ride.
There are major limitations to amateurs observing and reporting experiments in X-ray astronomy: the cost of building an amateur rocket or balloon to place a detector high enough and the cost of appropriate parts to build a suitable X-ray detector.
History of X-ray astronomy.
In 1927, E.O. Hulburt of the US Naval Research Laboratory and associates Gregory Breit and Merle A. Tuve of the Carnegie Institution of Washington explored the possibility of equipping Robert H. Goddard's rockets to explore the upper atmosphere. "Two years later, he proposed an experimental program in which a rocket might be instrumented to explore the upper atmosphere, including detection of ultraviolet radiation and X-rays at high altitudes".
In the late 1930s, the presence of a very hot, tenuous gas surrounding the Sun was inferred indirectly from optical coronal lines of highly ionized species. The Sun has been known to be surrounded by a hot tenuous corona. In the mid-1940s radio observations revealed a radio corona around the Sun.
The beginning of the search for X-ray sources from above the Earth's atmosphere was on August 5, 1948 12:07 GMT. A US Army (formerly German) V-2 rocket as part of Project Hermes was launched from White Sands Proving Grounds. The first solar X-rays were recorded by T. Burnight.
Through the 1960s, 70s, 80s, and 90s, the sensitivity of detectors increased greatly during the 60 years of X-ray astronomy. In addition, the ability to focus X-rays has developed enormously—allowing the production of high-quality images of many fascinating celestial objects.
Major questions in X-ray astronomy.
As X-ray astronomy uses a major spectral probe to peer into source, it is a valuable tool in efforts to understand many puzzles.
Stellar magnetic fields.
Magnetic fields are ubiquitous among stars, yet we do not understand precisely why, nor have we fully understood the bewildering variety of plasma physical mechanisms that act in stellar environments. Some stars, for example, seem to have magnetic fields, fossil stellar magnetic fields left over from their period of formation, while others seem to generate the field anew frequently.
Extrasolar X-ray source astrometry.
With the initial detection of an extrasolar X-ray source, the first question usually asked is "What is the source?" An extensive search is often made in other wavelengths such as visible or radio for possible coincident objects. Many of the verified X-ray locations still do not have readily discernible sources. X-ray astrometry becomes a serious concern that results in ever greater demands for finer angular resolution and spectral radiance.
There are inherent difficulties in making X-ray/optical, X-ray/radio, and X-ray/X-ray identifications based solely on positional coincidents, especially with handicaps in making identifications, such as the large uncertainties in positional determinants made from balloons and rockets, poor source separation in the crowded region toward the galactic center, source variability, and the multiplicity of source nomenclature.
X‐ray source counterparts to stars can be identified by calculating the angular separation between source centroids and position of the star. The maximum allowable separation is a compromise between a larger value to identify as many real matches as possible and a smaller value to minimize the probability of spurious matches. "An adopted matching criterion of 40" finds nearly all possible X‐ray source matches while keeping the probability of any spurious matches in the sample to 3%."
Solar X-ray astronomy.
All of the detected X-ray sources at, around, or near the Sun are within or associated with the coronal cloud which is its outer atmosphere.
Coronal heating problem.
In the area of solar X-ray astronomy, there is the coronal heating problem. The photosphere of the Sun has an effective temperature of 5,570 K yet its corona has an average temperature of 1–2 × 106 K. However, the hottest regions are 8–20 × 106 K. The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.
It is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating. The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone. These waves travel upward and dissipate in the corona, depositing their energy in the ambient gas in the form of heat. The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller events—nanoflares.
Currently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfvén waves have been found to dissipate or refract before reaching the corona. In addition, Alfvén waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.
Coronal mass ejection.
A coronal mass ejection (CME) is an ejected plasma consisting primarily of electrons and protons (in addition to small quantities of heavier elements such as helium, oxygen, and iron), plus the entraining coronal closed magnetic field regions. Evolution of these closed magnetic structures in response to various photospheric motions over different time scales (convection, differential rotation, meridional circulation) somehow leads to the CME. Small-scale energetic signatures such as plasma heating (observed as compact soft X-ray brightening) may be indicative of impending CMEs.
The soft X-ray sigmoid (an S-shaped intensity of soft X-rays) is an observational manifestation of the connection between coronal structure and CME production. "Relating the sigmoids at X-ray (and other) wavelengths to magnetic structures and current systems in the solar atmosphere is the key to understanding their relationship to CMEs."
The first detection of a Coronal mass ejection (CME) as such was made on December 1, 1971 by R. Tousey of the US Naval Research Laboratory using OSO 7. Earlier observations of coronal transients or even phenomena observed visually during solar eclipses are now understood as essentially the same thing.
The largest geomagnetic perturbation, resulting presumably from a "prehistoric" CME, coincided with the first-observed solar flare, in 1859. The flare was observed visually by Richard Christopher Carrington and the geomagnetic storm was observed with the recording magnetograph at Kew Gardens. The same instrument recorded a crotchet, an instantaneous perturbation of the Earth's ionosphere by ionizing soft X-rays. This could not easily be understood at the time because it predated the discovery of X-rays (by Roentgen) and the recognition of the ionosphere (by Kennelly and Heaviside).
Exotic X-ray sources.
A microquasar is a smaller cousin of a quasar that is a radio emitting X-ray binary, with an often resolvable pair of radio jets.
LSI+61°303 is a periodic, radio-emitting binary system that is also the gamma-ray source, CG135+01.
Observations are revealing a growing number of recurrent X-ray transients, characterized by short outbursts with very fast rise times (tens of minutes) and typical durations of a few hours that are associated with OB supergiants and hence define a new class of massive X-ray binaries: Supergiant Fast X-ray Transients (SFXTs).
Observations made by Chandra indicate the presence of loops and rings in the hot X-ray emitting gas that surrounds Messier 87. A magnetar is a type of neutron star with an extremely powerful magnetic field, the decay of which powers the emission of copious amounts of high-energy electromagnetic radiation, particularly X-rays and gamma rays.
X-ray dark stars.
During the solar cycle, as shown in the sequence of images at right, at times the Sun is almost X-ray dark, almost an X-ray variable. Betelgeuse, on the other hand, appears to be always X-ray dark. Hardly any X-rays are emitted by red giants. There is a rather abrupt onset of X-ray emission around spectral type A7-F0, with a large range of luminosities developing across spectral class F. Altair is spectral type A7V and Vega is A0V. Altair's total X-ray luminosity is at least an order of magnitude larger than the X-ray luminosity for Vega. The outer convection zone of early F stars is expected to be very shallow and absent in A-type dwarfs, yet the acoustic flux from the interior reaches a maximum for late A and early F stars provoking investigations of magnetic activity in A-type stars along three principal lines. Chemically peculiar stars of spectral type Bp or Ap are appreciable magnetic radio sources, most Bp/Ap stars remain undetected, and of those reported early on as producing X-rays only few of them can be identified as probably single stars. X-ray observations offer the possibility to detect (X-ray dark) planets as they eclipse part of the corona of their parent star while in transit. "Such methods are particularly promising for low-mass stars as a Jupiter-like planet could eclipse a rather significant coronal area."
X-ray dark planet/comet.
X-ray observations offer the possibility to detect (X-ray dark) planets as they eclipse part of the corona of their parent star while in transit. "Such methods are particularly promising for low-mass stars as a Jupiter-like planet could eclipse a rather significant coronal area."
As X-ray detectors have become more sensitive, they have observed that some planets and other normally X-ray non-luminescent celestial objects under certain conditions emit, fluoresce, or reflect X-rays.
Comet Lulin.
NASA's Swift Gamma-ray Explorer satellite was monitoring Comet Lulin as it closed to 63 Gm of Earth. For the first time, astronomers can see simultaneous UV and X-ray images of a comet. "The solar wind—a fast-moving stream of particles from the sun—interacts with the comet's broader cloud of atoms. This causes the solar wind to light up with X-rays, and that's what Swift's XRT sees", said Stefan Immler, of the Goddard Space Flight Center. This interaction, called charge exchange, results in X-rays from most comets when they pass within about three times Earth's distance from the Sun. Because Lulin is so active, its atomic cloud is especially dense. As a result, the X-ray-emitting region extends far sunward of the comet.
Single X-ray stars.
In addition to the Sun there are many unary stars or star systems throughout the galaxy that emit X-rays. β Hydri (G2 IV) is a normal single, post main-sequence subgiant star, "T"eff = 5800 K. It exhibits coronal X-ray fluxes.
The benefit of studying single stars is that it allows measurements free of any effects of a companion or being a part of a multiple star system. Theories or models can be more readily tested. See, e.g., Betelgeuse, Red giants, and Vega and Altair.

</doc>
<doc id="44063" url="https://en.wikipedia.org/wiki?curid=44063" title="Extragalactic astronomy">
Extragalactic astronomy

Extragalactic astronomy is the branch of astronomy concerned with objects outside the Milky Way galaxy. In other words, it is the study of all astronomical objects which are not covered by galactic astronomy, the next level of galactic astronomy.
As instrumentation has improved, more distant objects can now be examined in detail. It is therefore useful to sub-divide this branch into Near-Extragalactic Astronomy and Far-Extragalactic Astronomy. The former deals with objects such as the galaxies of the Local Group, which are close enough to allow very detailed analyses of their contents (e.g. supernova remnants, stellar associations). The latter describes the study of objects sufficiently far away that only the brightest phenomena are observable.
Some topics include:

</doc>
<doc id="44069" url="https://en.wikipedia.org/wiki?curid=44069" title="Vulcan (hypothetical planet)">
Vulcan (hypothetical planet)

Vulcan is a small hypothetical planet that was proposed to exist in an orbit between Mercury and the Sun. Attempting to explain peculiarities of Mercury's orbit, the 19th-century French mathematician Urbain Le Verrier hypothesized that they were the result of another planet, which he named "Vulcan".
A number of reputable investigators became involved in the search for Vulcan, but no such planet was ever found, and the peculiarities in Mercury's orbit have now been explained by Albert Einstein's theory of general relativity. Searches of data gathered by NASA's two "STEREO" spacecraft have failed to find any vulcanoids that could have accounted for claimed observations of Vulcan. It is doubtful that there are any vulcanoids larger than in diameter. Other than Mercury, asteroid with a semi-major axis of has the smallest known semi-major axis of any known object orbiting the Sun.
Argument for existence.
In 1840, François Arago, the director of the Paris Observatory, suggested to the French mathematician Urbain Le Verrier that he work on the topic of the planet Mercury's orbital motion around the Sun. The goal of this study was to construct a model based on Sir Isaac Newton's laws of motion and gravitation. By 1843, Le Verrier published his provisional theory on the subject, which would be tested during a transit of Mercury across the face of the Sun in 1843. As it turned out, predictions from Le Verrier's theory failed to match the observations.
Le Verrier renewed his work and, in 1859, published a more thorough study of Mercury's motion. This was based on a series of meridian observations of the planet as well as 14 transits. The rigor of this study meant that any differences from observation would be caused by some unknown factor. Indeed, there still remained some discrepancy. During Mercury's orbit, its perihelion advances by a small amount each orbit, technically called perihelion precession. The phenomenon is predicted by classical mechanics, but the observed value differed from the predicted value by the small amount of 43 arcseconds per century.
Le Verrier postulated that the excess precession could be explained by the presence of a small planet inside the orbit of Mercury, and he proposed the name "Vulcan" for this object. In Roman mythology, Vulcan was the god of beneficial and hindering fire, including the fire of volcanoes, making it an apt name for a planet so close to the Sun. Le Verrier's recent success in discovering the planet Neptune using the same techniques lent veracity to his claim, and astronomers around the world attempted to observe a new planet there, but nothing was ever found.
Search.
In December 1859, Le Verrier received a letter from French physician and amateur astronomer Edmond Modeste Lescarbault, who claimed to have seen a transit of the hypothetical planet earlier in the year. Le Verrier took the train to the village of Orgères-en-Beauce, some 70 kilometres southwest of Paris, where Lescarbault had built himself a small observatory. Le Verrier arrived unannounced and proceeded to interrogate the man.
Lescarbault described in detail how, on 26 March 1859, he noticed a small black dot on the face of the Sun, which he was studying with his modest refractor. Thinking it to be a sunspot, Lescarbault was not at first surprised, but after some time had passed he realized that it was moving. Having observed the transit of Mercury in 1845, he guessed that what he was observing was another transit, but of a previously undiscovered body. He took some hasty measurements of its position and direction of motion, and using an old clock and a pendulum with which he took his patients’ pulses, he estimated the duration of the transit at 1 hour, 17 minutes and 9 seconds.
Le Verrier was satisfied that Lescarbault had seen the transit of a previously unknown planet. On 2 January 1860 he announced the discovery of Vulcan to a meeting of the Académie des Sciences in Paris. Lescarbault, for his part, was awarded the Légion d'honneur and invited to appear before numerous learned societies.
Not everyone accepted the veracity of Lescarbault's "discovery", however. An eminent French astronomer, Emmanuel Liais, who was working for the Brazilian government in Rio de Janeiro in 1859, claimed to have been studying the surface of the Sun with a telescope twice as powerful as Lescarbault's at the very moment that Lescarbault said he observed his mysterious transit. Liais, therefore, was "in a condition to deny, in the most positive manner, the passage of a planet over the sun at the time indicated".
Based on Lescarbault’s "transit", Le Verrier computed Vulcan’s orbit: it supposedly revolved about the Sun in a nearly circular orbit at a distance of 21 million kilometres, or 0.14 astronomical units. The period of revolution was 19 days and 17 hours, and the orbit was inclined to the ecliptic by 12 degrees and 10 minutes (an incredible degree of precision). As seen from the Earth, Vulcan’s greatest elongation from the Sun was 8 degrees.
Numerous reports—all of them unreliable—began to reach Le Verrier from other amateurs who claimed to have seen unexplained transits. Some of these reports referred to observations made many years earlier, and many could not be properly dated. Nevertheless, Le Verrier continued to tinker with Vulcan’s orbital parameters as each new reported sighting reached him. He frequently announced dates of future Vulcan transits, and when these failed to materialize, he tinkered with the parameters some more.
Among the earlier alleged observers of Vulcan, the following are the most noteworthy:
Shortly after eight o'clock on the morning of 29 January 1860, F A R Russell and three other people saw an alleged transit of an intra-Mercurial planet from London. An American observer, Richard Covington, many years later claimed to have seen a well-defined black spot progress across the Sun’s disk around 1860, when he was stationed in Washington Territory.
No "observations" of Vulcan were made in 1861. Then, on the morning of 22 March 1862, between eight and nine o’clock Greenwich Time, another amateur astronomer, a Mr Lummis of Manchester, England, saw a transit. His colleague whom he alerted also saw the event. Based on these two men's reports, two French astronomers, Benjamin Valz and Rodolphe Radau, independently calculated the object’s supposed orbital period, with Valz deriving a figure of 17 days and 13 hours, and Radau a figure of 19 days and 22 hours.
On 8 May 1865 another French astronomer, Aristide Coumbary, observed an unexpected transit from Istanbul, Turkey.
Between 1866 and 1878 no reliable observations of the hypothetical planet were made. Then, during the total solar eclipse of 29 July 1878, two experienced astronomers, Professor James Craig Watson, the director of the Ann Arbor Observatory in Michigan, and Lewis Swift, an amateur from Rochester, New York, both claimed to have seen a Vulcan-type planet close to the Sun. Watson, observing from Separation, Wyoming, placed the planet about 2.5 degrees southwest of the Sun, and estimated its magnitude at 4.5. Swift, who was observing the eclipse from a location near Denver, Colorado, saw what he took to be an intra-mercurial planet about 3 degrees southwest of the Sun. He estimated its brightness to be the same as that of Theta Cancri, a fifth-magnitude star which was also visible during totality, about six or seven minutes from the "planet". Theta Cancri and the planet were very nearly in line with the centre of the Sun.
Watson and Swift had reputations as excellent observers. Watson had already discovered more than twenty asteroids, while Swift had several comets named after him. Both described the colour of their hypothetical intra-mercurial planet as "red". Watson reported that it had a definite disk—unlike stars, which appear in telescopes as mere points of light—and that its phase indicated that it was approaching superior conjunction.
These are merely the more "reliable observations" of alleged intra-Mercurial planets. For half a century or more, many other observers tried to find the hypothetical Vulcan. Many false alarms were triggered by round sunspots that closely resembled planets in transit. During solar eclipses, stars close to the Sun were mistaken for planets. At one point, to reconcile different observations, at least two intra-mercurial planets were postulated.
Conclusion.
In 1877 Le Verrier died, convinced to the end of having discovered another planet. With the loss of its principal proponent, however, the search for Vulcan abated. After many years of searching, astronomers were seriously doubting the planet's existence.
In 1915 Einstein's theory of relativity, an entirely different approach to understanding gravity from classical mechanics, solved the problem. His equations predicted exactly the observed amount of advance of Mercury's perihelion without any recourse to the existence of a hypothetical Vulcan. The new theory modified the predicted orbits of all planets, but the magnitude of the differences from Newtonian theory diminishes rapidly as one gets farther from the Sun. Also, Mercury's fairly eccentric orbit makes it much easier to detect the perihelion shift than is the case for the nearly circular orbits of Venus and Earth.
Vulcanoids.
Observing a planet inside the orbit of Mercury is difficult, since the telescope must be pointed very close to the Sun, where the sky is only dark during a solar eclipse. Also, an error in pointing the telescope can result in damage for the optics, and injury to the observer. The huge amount of light present even quite far away from the Sun can produce false reflections inside the optics, thus fooling the observer into seeing things that do not exist.
The best observational strategy might be to monitor the Sun's disk for possible transits, but transits would only be seen from Earth provided the object orbits close enough to the ecliptic plane. A small, dark spot might be seen to move across the Sun's disk, as happens with transits of Mercury and Venus.
In 1915, when Einstein successfully explained the apparent anomaly in Mercury's orbit, most astronomers abandoned the search for Vulcan. A few, however, remained convinced that not all the alleged observations of Vulcan were unfounded. Among these was Henry C Courten, of Dowling College, New York. Studying photographic plates of the 1970 eclipse of the Sun, he and his associates detected several objects which appeared to be in orbits close to the Sun. Even accounting for artifacts, Courten felt that at least seven of the objects were real.
Courten believed that an intra-Mercurial planetoid between 130 and 800 kilometres in diameter was orbiting the Sun at a distance of about 0.1 AU. Other images on his eclipse plates led him to postulate the existence of an asteroid belt between Mercury and the Sun.
None of these claims has ever been substantiated after more than forty years of observation. It has been surmised, however, that some of these objects—and other alleged intra-Mercurial objects—may exist, being nothing more than previously unknown comets or small asteroids. Today, the search continues for these so-called vulcanoid asteroids, which are thought to exist in the region where Vulcan was once sought. None have been found yet and searches have ruled out any such asteroids larger than about 6 km. Neither SOHO nor STEREO has detected a planet inside the orbit of Mercury.

</doc>
<doc id="44070" url="https://en.wikipedia.org/wiki?curid=44070" title="Avro Vulcan">
Avro Vulcan

The Avro Vulcan (later Hawker Siddeley Vulcan from July 1963) is a jet-powered tailless delta wing high-altitude strategic bomber, which was operated by the Royal Air Force (RAF) from 1956 until 1984. Aircraft manufacturer A.V. Roe and Company (Avro) designed the Vulcan in response to Specification B.35/46. Of the three V bombers produced, the Vulcan was considered the most technically advanced and hence the riskiest option. Several scale aircraft, designated Avro 707, were produced to test and refine the delta wing design principles.
The Vulcan B.1 was first delivered to the RAF in 1956; deliveries of the improved Vulcan B.2 started in 1960. The B.2 featured more powerful engines, a larger wing, an improved electrical system and electronic countermeasures (ECM); many were modified to accept the Blue Steel missile. As a part of the V-force, the Vulcan was the backbone of the United Kingdom’s airborne nuclear deterrent during much of the Cold War. Although the Vulcan was typically armed with nuclear weapons, it was capable of conventional bombing missions, a capability which was used in Operation Black Buck during the Falklands War between the United Kingdom and Argentina in 1982.
The Vulcan had no defensive weaponry, initially relying upon high-speed high-altitude flight to evade interception. Electronic countermeasures were employed by the B.1 (designated B.1A) and B.2 from circa 1960. A change to low-level tactics was made in the mid-1960s. In the mid-1970s nine Vulcans were adapted for maritime radar reconnaissance operations, redesignated as B.2 (MRR). In the final years of service six Vulcans were converted to the K.2 tanker configuration for aerial refuelling.
Since retirement by the RAF one example, B.2 "XH558", named "The Spirit of Great Britain" was restored for use in display flights and air shows, whilst two other B.2s, XL426 and "XM655", are kept in taxiable condition for ground runs and demonstrations at London Southend Airport and Wellesbourne Mountford Airfield respectively. B.2 "XH558" flew for the last time in October 2015, before also being kept in taxiable condition at Robin Hood Airport, Doncaster.
Development.
Origins.
The origin of the Vulcan and the other V bombers is linked with early British atomic weapon programme and nuclear deterrent policies. Britain's atom bomb programme began with Air Staff Operational Requirement OR.1001 issued in August 1946. This anticipated a government decision in January 1947 to authorise research and development work on atomic weapons, the U.S. Atomic Energy Act of 1946 (McMahon Act) having prohibited exporting atomic knowledge, even to countries that had collaborated on the Manhattan Project. OR.1001 envisaged a weapon not to exceed in length, in diameter and in weight. The weapon had to be suitable for release from to .
In January 1947, the Ministry of Supply distributed Specification B.35/46 to UK aviation companies to satisfy Air Staff Operational Requirement OR.229 for "a medium range bomber landplane capable of carrying one bomb to a target from a base which may be anywhere in the world." A cruising speed of at heights between and was specified. The maximum weight when fully loaded ought not to exceed . In addition to a "Special" (i.e., atomic) bomb, the aircraft was to be capable of alternatively carrying a conventional bomb load of . The similar OR.230 required a "long range bomber" with a radius of action with a maximum weight of when fully loaded; this requirement was considered too exacting. A total of six companies would submit technical brochures to this specification, including Avro.
Required to tender by the end of April 1947, work began on receipt of Specification B.35/46 at Avro, led by technical director Roy Chadwick and chief designer Stuart Davies; the type designation was "Avro 698". It was obvious to the design team that conventional aircraft could not satisfy the Specification; knowing little about high-speed flight and unable to glean much from the Royal Aircraft Establishment or the US, they investigated German Second World War swept wing research. The team estimated that an otherwise conventional aircraft, with a swept wing of 45°, would have doubled the weight requirement. Realising that swept wings increase longitudinal stability, the team deleted the tail (empennage) and the supporting fuselage, it thus became a swept-back flying wing with only a rudimentary forward fuselage and a fin (vertical stabilizer) at each wingtip. The estimated weight was now only 50% over the requirement; a delta shape resulted from reducing the wingspan and maintaining the wing area by filling in the space between the wingtips, which enabled the specification to be met. Though Dr Alexander Lippisch is generally credited as the pioneer of the delta wing, Chadwick’s team had followed its own logical design process. The initial design submission had four large turbojets stacked in pairs buried in the wing either side of the centreline. Outboard of the engines were two bomb-bays.
In August 1947, Roy Chadwick was killed in the crash of the Avro Tudor 2 prototype and was succeeded by Sir William Farren. Reductions in wing thickness made it impossible to incorporate the split bomb bays and stacked engines, thus the engines were placed side-by-side in pairs either side of a single bomb-bay, the fuselage growing somewhat. The wingtip fins gave way to a single fin on the aircraft's centreline. Rival manufacturer Handley Page received a prototype contract for its crescent-winged HP.80 B.35/46 tender in November 1947. Though considered the best option, contract placement for Avro's design was delayed whilst its technical strength was established. Instructions to proceed with the construction of two Avro 698 prototypes was received in January 1948. As an insurance measure against both radical designs failing, Short Brothers received a contract for the prototype SA.4 to the less-stringent Specification B.14/46; the SA.4, later named Sperrin, was not required. In April 1948, Vickers also received authority to proceed with their Type 660 which, although falling short of the B.35/46 Specification, being of a more conventional design would be available sooner; this plane entered service as the Valiant.
Avro 707 and Avro 710.
As Avro had no flight experience of the delta wing, the company planned two smaller experimental aircraft based on the 698, the one-third scale model 707 for low-speed handling and the one-half scale model 710 for high-speed handling. Two of each were ordered. However, the 710 was cancelled when it was considered too time-consuming to develop; a high-speed variant of the 707 was designed in its place, the 707A. The first 707, "VX784", flew in September 1949 but crashed later that month killing Avro test pilot Flt Lt Eric Esler. The second low-speed 707, "VX790", built with the still uncompleted 707A’s nose section (containing an ejection seat) and redesignated 707B, flew in September 1950 piloted by Avro test pilot Wg Cdr Roland "Roly" Falk. The high speed 707A, "WD480", followed in July 1951.
Due to the delay of the 707 programme, the contribution of the 707B and 707A towards the basic design of the 698 was not considered significant, though it did highlight a need to increase the length of the nosewheel to give a ground incidence of 3.5 degrees, the optimum take-off attitude. The 707B and 707A proved the design's validity and gave confidence in the delta planform. A second 707A, "WZ736" and a two-seat 707C, "WZ744" were also constructed but they played no part in the 698's development.
Vulcan B.1 and B.2.
Prototypes and type certification.
More influential than the 707 in the 698's design was wind-tunnel testing performed by the Royal Aircraft Establishment at Farnborough, which indicated the need for a wing redesign to avoid the onset of compressibility drag which would have restricted the maximum speed. Painted gloss white, the 698 prototype "VX770" flew for the first time on 30 August 1952 piloted by Roly Falk flying solo. The prototype 698, then fitted with only the first-pilot's ejection seat and a conventional control wheel, was powered by four Rolls-Royce RA.3 Avon engines of thrust; there were no wing fuel tanks, temporary tankage was carried in the bomb bay. "VX770" made an appearance at the 1952 Society of British Aircraft Constructors' (SBAC) Farnborough Air Show the next month when Falk demonstrated an "almost vertical bank". After its Farnborough appearance, the future name of the Avro 698 was a subject of speculation; Avro had strongly recommended the name "Ottawa", in honour of the company's connection with Avro Canada. Weekly magazine "Flight" suggested "Albion" after rejecting "Avenger", "Apollo" and "Assegai". The Chief of the Air Staff preferred a V-class of bombers, the Air Council announced the following month that the 698 would be called "Vulcan" after the Roman god of fire and destruction. In January 1953, "VX770" was grounded for the installation of wing fuel tanks, Armstrong Siddeley ASSa.6 Sapphire engines of thrust and other systems; it flew again in July 1953.
The second prototype, "VX777", flew in September 1953. More representative of production aircraft, it was lengthened to accommodate a longer nose undercarriage leg, featured a visual bomb-aiming blister under the cabin and was fitted with Bristol Olympus 100 engines of thrust. At Falk’s suggestion, a fighter-style control stick replaced the control wheel. Both prototypes had almost pure delta wings with straight leading edges. During trials in July 1954, "VX777" was substantially damaged in a heavy landing at Farnborough. It was repaired and fitted with Olympus 101 engines of thrust before resuming trials in October 1955. While exploring the high speed and high altitude flight envelope, mild buffeting and other undesirable flight characteristics were experienced while approaching the speed of sound, including an alarming tendency to enter an uncontrollable dive, unacceptable to the Aeroplane and Armament Experimental Establishment (A&AEE) at Boscombe Down. The solution included the "Phase 2" wing, featuring a kinked and drooped leading edge and vortex generators on the upper surface, first tested on 707A "WD480". An auto-mach trimmer introduced a nose-up attitude when at high speeds, the control column had to be pushed rather than pulled to maintain level flight.
Meanwhile, the first production B.1, "XA889", had flown in February 1955 with the original wing. In September 1955, Falk, flying the second production B.1 "XA890" amazed crowds at the Farnborough Air Show by executing a barrel roll on his second flypast in front of the SBAC president’s tent. After two days flying, he was called in front of service and civil aviation authorities and ordered to refrain from carrying out this "dangerous" manoeuvre. Now fitted with a Phase 2 wing, "XA889" was delivered in March 1956 to the A&AEE for trials for the type’s initial Certificate of Airworthiness which it received the following month.
Further developments.
The first 15 B.1s were powered by the Olympus 101 of thrust. Many of these early examples in a metallic finish remained the property of the Ministry of Supply being retained for trials and development purposes. Those entering RAF service were delivered to No 230 Operational Conversion Unit (OCU), the first in July 1956. Later aircraft, painted in anti-flash white and powered by the Olympus 102 of thrust, began to enter squadron service in July 1957. The Olympus 102s were quickly modified to Olympus 104 standard, ultimately rated at thrust. As far back as 1952, Bristol Aero Engines had begun development of the BOl.6 (Olympus 6) rated at thrust but if fitted to the B.1, this would have re-introduced the buffet requiring further redesign of the wing.
The decision to proceed with the B.2 versions of the Vulcan was made in May 1956. It was anticipated that the first B.2 would be around the 45th aircraft of the 99 then on order. As well as being able to achieve greater heights over targets, it was believed that operational flexibility could be extended by the provision of in-flight refuelling equipment and tanker aircraft. The increasing sophistication of Soviet air defences required the fitting of electronic countermeasure (ECM) equipment and vulnerability could be reduced by the introduction of the Avro "Blue Steel" stand-off missile, then in development. In order to develop these proposals, the second Vulcan prototype "VX777" was rebuilt with the larger and thinner Phase 2C wing, improved flying control surfaces and Olympus 102 engines, first flying in this configuration in August 1957. Plans were in hand to equip all Vulcans from the 16th aircraft onwards with in-flight refuelling receiving equipment. A B.1, "XA903", was allocated for Blue Steel development work. Other B.1s were used for the development of the BOl.6 (later Olympus 200), "XA891"; a new AC electrical system, "XA893"; and ECM including jammers within a bulged tail-cone and a tail warning radar, "XA895".
The 46th production aircraft and first B.2, "XH533", first flew in September 1958 fitted with Olympus 200 engines of thrust, six months before the last B.1 "XH532" was delivered in March 1959. Rebuilding B.1s as B.2s was considered but rejected over cost. Nevertheless, to extend the B.1's service life, 28 were upgraded by Armstrong Whitworth between 1959 and 1963 to the B.1A standard, including features of the B.2 such as ECM equipment, in-flight refuelling receiving equipment, and UHF radio. The second B.2, "XH534", flew in January 1959. Powered by production Olympus 201 of thrust, it was more representative of a production aircraft, being fitted with an in-flight refuelling probe and a bulged ECM tail cone. Some subsequent B.2s were initially lacking probes and ECM tail cones, but these were fitted retrospectively. The first 10 B.2s outwardly showed their B.1 ancestry, retaining narrow engine air intakes. Anticipating even more powerful engines, the air intakes were deepened on the 11th ("XH557") and subsequent aircraft. Many of the early aircraft were retained for trials and it was the 12th B.2, "XH558", that was the first to be delivered to the RAF in July 1960. Coincidentally, "XH558" would also be the last Vulcan in service with the RAF, before being retired in 1992.
The 26th B.2, "XL317", the first of a production batch ordered in February 1956, was the first Vulcan, apart from development aircraft, capable of carrying the Blue Steel missile; 33 aircraft were delivered to the RAF with these modifications. When the Mk.2 version of Blue Steel was cancelled in favour of the Douglas GAM-87 Skybolt air-launched ballistic missile in December 1959, fittings were changed in anticipation of the new missile, one under each wing. Though Skybolt was cancelled in November 1962, many aircraft were delivered or retrofitted with "Skybolt" blisters. Later aircraft ("XL391" and "XM574" onwards) were delivered with Olympus 301 engines of thrust. Two earlier aircraft were re-engined ("XH557" and "XJ784") for trials and development work; another seven aircraft ("XL384"-"XL390") were converted circa 1963.
The last B.2 "XM657" was delivered in 1965 and the type served till 1984. Whilst in service the B.2 was continuously updated with modifications including rapid engine starting, bomb-bay fuel tanks, wing strengthening to give the fatigue life to enable the aircraft to fly at low level (a tactic introduced in the mid-60s), upgraded navigation equipment, Terrain Following Radar (TFR), standardisation on a common nuclear weapon (WE.117) and improved ECM equipment. The B.1As were not strengthened, thus all were withdrawn by 1968. Nine B.2s were modified for the Maritime Radar Reconnaissance (MRR) role and six for the airborne tanker role.
Proposed developments and cancelled projects.
The Avro 718 was a 1951 proposal for a delta-winged military transport based on the Type 698 to carry 80 troops or 110 passengers. It would have been powered by four Bristol Olympus BOl.3 engines.
The Avro Type 722 Atlantic was a 1952 proposal (announced in June 1953) for a 120-passenger delta-winged airliner based on the Type 698.
The Avro 732 was a 1956 proposal for a supersonic development of the Vulcan and would have been powered by 8 de Havilland Gyron Junior engines. Unlike the proposed Avro 721 low-level bomber of 1952 or the Avro 730 supersonic stainless steel canard bomber dating from 1954 (cancelled in 1957 before completion of the prototype), the Type 732 showed its Vulcan heritage.
In 1960, the Air Staff approached Avro with a request into a study for a Patrol Missile Carrier armed with up to six Skybolt missiles capable of a mission length of 12 hours. Avro's submission in May 1960 was the Phase 6 Vulcan, which if built would have been the Vulcan B.3. The aircraft was fitted with an enlarged wing of span with increased fuel capacity; additional fuel tanks in a dorsal spine; a new main undercarriage to carry an all-up-weight of ; and reheated Olympus 301s of thrust. An amended proposal of October 1960 inserted a plug into the forward fuselage with capacity for six crew members including a relief pilot, all facing forwards on ejection seats, and aft-fan versions of the Olympus 301.
Export proposals.
Other countries expressed interest in purchasing Vulcans but, as with the other V-bombers, no foreign sales materialised.
As early as 1954, Australia recognised that the English Electric Canberra was becoming outdated and evaluated aircraft such as the Avro Vulcan and Handley-Page Victor as potential replacements. Political pressure for a Canberra replacement only rose to a head in 1962; at which point more modern types such as the BAC TSR-2, General Dynamics F-111C, and North American A-5 Vigilante had become available. The RAF would have transferred several V-bombers, including Vulcans, for interim use by the RAAF if they had purchased the TSR-2, but the RAAF selected the F-111C.
In the early 1980s, Argentina approached the UK with a proposal to buy a number of Vulcans. An application, made in September 1981, requested the 'early availability' of a 'suitable aircraft'. With some reluctance, ministers approved the export of a single aircraft but emphasised that clearance had not been given for the sale of a larger number. A letter from the British Foreign and Commonwealth Office to the Ministry of Defence in January 1982 stated that little prospect was seen of this happening without ascertaining the Argentine interest and whether such interest was genuine: 'On the face of it, a strike aircraft would be entirely suitable for an attack on the Falklands.' Argentina invaded the Falkland Islands less than three months later.
Design.
Overview.
Despite its radical and unusual shape, the airframe was built along traditional lines. Except for the most highly stressed parts, the whole structure was manufactured from standard grades of light alloy. The airframe was broken down into a number of major assemblies: the centre section, a rectangular box containing the bomb-bay and engine bays bounded by the front and rear spars and the wing transport joints; the intakes and centre fuselage; the front fuselage, incorporating the pressure cabin; the nose; the outer wings; the leading edges; the wing trailing edge and tail end of the fuselage; the wings were not sealed and used directly as fuel tankage, but carried bladders for fuel in the void spaces of the wings; and there was a single swept tail fin with a single rudder on the trailing edge.
A five-man crew, the first pilot, co-pilot, navigator radar, navigator plotter and air electronics officer (AEO) was accommodated within the pressure cabin on two levels; the pilots sitting on Martin-Baker 3K (3KS on the B.2) ejection seats whilst on the lower level, the other crew sat facing rearwards and would abandon the aircraft via the entrance door. The original B35/46 specification sought a jettisonable crew compartment, this requirement was removed in a subsequent amendment, the rear crew's escape system was often an issue of controversy, such as when a practical refit scheme was rejected. A rudimentary sixth seat forward of the navigator radar was provided for an additional crew member; the B.2 had an additional seventh seat opposite the sixth seat and forward of the AEO. These seats were no more than cushions, a full harness and an oxygen and intercom facility. The visual bomb-aimer’s compartment could be fitted with a T4 (Blue Devil) bombsight, in many B.2s this space housed a vertically mounted Vinten F95 Mk.10 camera for assessing simulated low-level bombing runs.
Fuel was carried in 14 bag tanks, four in the centre fuselage above and to the rear of the nosewheel bay and five in each outer wing. The tanks were split into four groups of almost equal capacity, each normally feeding its respective engine though cross-feeding was possible. The centre of gravity was automatically maintained by electric timers which sequenced the booster pumps on the tanks. B.2 aircraft could be fitted with one or two additional fuel tanks in the bomb-bay.
Despite being designed before a low radar cross-section (RCS) and other stealth factors were ever a consideration, a Royal Aircraft Establishment technical note of 1957 stated that of all the aircraft so far studied, the Vulcan appeared by far the simplest radar echoing object, due to its shape: only one or two components contributed significantly to the echo at any aspect, compared with three or more on most other types.
Colour schemes.
The two prototype Vulcans were finished in gloss white. Early Vulcan B.1s left the factory in a natural metal finish; the front half of the nose radome was painted black, the rear half painted silver. Front-line Vulcan B.1s had a finish of anti-flash white and RAF "type D" roundels. Front-line Vulcan B.1As and B.2s were similar but with 'type D pale' roundels.
With the adoption of low-level attack profiles in the mid-1960s, B.1As and B.2s were given a glossy Sea Grey Medium and Dark Green disruptive pattern camouflage on the upper surfaces, white undersurfaces and "type D" roundels. (The last 13 Vulcan B.2s, XM645 onwards, were delivered thus from the factory). In the mid-1970s: Vulcan B.2s received a similar scheme with matte camouflage, Light Aircraft Grey undersides, and "low-visibility" roundels; B.2(MRR)s received a similar scheme in gloss; and the front half of the radomes were no longer painted black. Beginning in 1979, 10 Vulcans received a wrap-around camouflage of Dark Sea Grey and Dark Green because, during Red Flag exercises in the USA, defending SAM forces had found that the grey-painted undersides of the Vulcan became much more visible against the ground at high angles of bank.
Avionics.
The original Vulcan B.1 radio fit was: two 10-channel VHF transmitter/receivers ("TR-1985/TR-1986") and a 24-channel HF transmitter/receiver ("STR-18"). The Vulcan B.1A also featured an UHF transmitter/receiver ("ARC-52"). The initial B.2 radio fit was similar to the B.1A though it was ultimately fitted with the "ARC-52", a V/UHF transmitter/receiver ("PTR-175"), and a SSB HF transmitter/receiver (Collins "618T").
The Navigation and Bombing System (NBS) comprised an "H2S" Mk9 radar and a Navigation Bombing Computer (NBC) Mk1. Other navigation aids included a Marconi radio compass (ADF), "GEE" Mk3, "Green Satin" Doppler radar to determine the groundspeed and drift angle, radio and radar altimeters, and ILS. "TACAN" replaced "GEE" in the B.1A and B.2 in 1964 . Decca "Doppler 72" replaced "Green Satin" in the B.2 around 1969 A continuous display of the aircraft's position was maintained by a Ground Position Indicator (GPI). The bulk of the equipment was carried in a large extended tail cone, and a flat ECM aerial counterpoise plate mounted between the starboard tailpipes. Later equipment on the B.2 included: an L band jammer (replacing a "Red Shrimp"); the "ARI 18146" X-band jammer; replacing the Green Palm; the improved "Red Steer" Mk.2; infra-red decoys (flares); and the "ARI 18228" PWR with its aerials that gave a squared top to the fin.
Controls.
The aircraft was controlled by a fighter-type control stick and rudder bar which operated the powered flying controls (PFCs). Each PFC had a single electro-hydraulic powered flying control unit (PFCU) except the rudder which had two, one running as a back-up. Artificial feel and autostabilisation in the form of pitch and yaw dampers were provided, as well as an auto mach trimmer.
The flight instruments in the B.1 were traditional and included "G4B" compasses; Mk.4 artificial horizons; and zero reader flight display instruments. The B.1 had a Smiths Mk10 autopilot. In the B.2, these features were incorporated into the Smiths Military Flight System (MFS), the pilots' components being: two beam compasses; two director-horizons; and a Mk.10A or Mk.10B autopilot. From 1966, B.2s were fitted with the "ARI 5959" Terrain-following radar (TFR), built by General Dynamics, its commands being fed into the director-horizons.
The B.1 had four elevators (inboard) and four ailerons (outboard). In the B.2, these were replaced by eight elevons. The Vulcan was also fitted with six electrically-operated three-position (in, medium drag, high drag) airbrakes, four in the upper centre section and two in the lower. There were originally four lower airbrakes but the outboard two were deleted before the aircraft entered service. A brake parachute was installed inside the tail cone.
Electrical and hydraulic systems.
The main electrical system on the B.1/B.1A was 112V DC supplied by four 22.5kW engine-driven generators. Backup power was provided by four 24V 40Ah batteries connected in series providing 96V. Secondary electrical systems were 28V DC, single-phase 115V AC at 1600Hz, and three-phase 115V AC at 400 Hz, driven by transformers and inverters from the main system. The 28V DC system was backed up by a single 24V battery.
For greater efficiency and higher reliability, the main system on the B.2 was changed to three-phase 200V AC at 400 Hz supplied by four 40kVA engine-driven constant speed alternators. Standby supplies in the event of a main AC failure were provided by a Ram Air Turbine (RAT) driving a 17kVA alternator that could operate at high altitude down to , and an Airborne Auxiliary Power Plant (AAPP), a Rover gas turbine driving a 40kVA alternator, which could be started once the aircraft was below an altitude of . Secondary electrical supplies were similar to the B.1.
The change to an AC system was a significant improvement. The Vulcan's powered flying controls were hydraulically actuated but each Powered Flying Control Unit (PFCU) had a hydraulic pump which was driven by an electric motor. Because there was no manual reversion, a total electrical failure would result in a loss of control. The standby batteries on the B.1 were designed to give enough power for 20 minutes of flying time but this proved to be optimistic and two aircraft, "XA891" and "XA908", crashed as a result.
The main hydraulic system provided pressure for: undercarriage raising and lowering and bogie trim; nosewheel centring and steering; wheelbrakes (fitted with Maxarets); bomb doors opening and closing; and (B.2 only) AAPP air scoop lowering. Hydraulic pressure was provided by three hydraulic pumps fitted to Nos. 1, 2 and 3 engines. An electrically operated hydraulic power pack (EHPP) could be used to operate the bomb doors and recharge the brake accumulators. A compressed air (later nitrogen) system was provided for emergency undercarriage lowering.
Engine.
The Rolls-Royce Olympus, originally known as the "Bristol BE.10 Olympus", is a two-spool axial-flow turbojet that powered the Vulcan. Each Vulcan had four engines buried in the wings, positioned in pairs close to the centre of the fuselage. The engine's design began in 1947, intended to power the Bristol Aeroplane Company's own rival design to the Vulcan. A serendipitous arrangement in air intakes could cause the Vulcan to emit a distinctive "howl" when the engines were at approximately 90% power, which can be heard as the aircraft performs a flypast, such as at public airshows.
As the prototype Vulcan VX770 was ready for flight prior to the Olympus being available, it first flew using Rolls-Royce Avon RA.3 engines of thrust. These were quickly replaced by Armstrong Siddeley Sapphire ASSa.6 engines of thrust. VX770 later became a flying test bed for the Rolls-Royce Conway. The second prototype VX777 first flew with Olympus 100s of thrust. It was subsequently re-engined with Olympus 101 engines of thrust. When VX777 flew with a Phase 2C (B.2) wing in 1957, it was fitted with Olympus 102 engines of thrust.
Early B.1s were engined with the Olympus 101. Later aircraft were delivered with Olympus 102s. All Olympus 102s became the Olympus 104 of thrust on overhaul and ultimately thrust on uprating. The first B.2 flew with the second-generation Olympus 200 of thrust, design of which began in 1952. Subsequent B.2s were engined with either the uprated Olympus 201 of thrust or the Olympus 301 of thrust. The Olympus 201 was designated 202 on being fitted with a rapid air starter. The engine would later be developed into a reheated (afterburning) powerplant for the cancelled supersonic BAC TSR-2 strike bomber and the supersonic passenger transport Concorde.
Operational history.
Introduction.
In September 1956, the RAF received its first Vulcan B.1, "XA897", which immediately embarked upon a round-the-world tour. The tour was to be an important demonstration of the range and capabilities of the aircraft, it also had other benefits in the form of conducting goodwill visits in various countries; in later life Vulcans routinely visited various nations and distant parts of the former British Empire as a show of support and military protection. This first tour, however, was struck by misfortune; on 1 October 1956, while landing in bad weather at London Heathrow Airport at the completion of the world tour, "XA897" was destroyed in a fatal accident.
The first two aircraft were delivered to 230 OCU in January 1957 and the training of crews started on 21 February 1957, in the following months more aircraft were delivered to the OCU. The first OCU course to qualify was No. 1 Course, on 21 May 1957, and they went on to form the first flight of No.83 Squadron. No. 83 Squadron was the first operational squadron to use the bomber, at first using borrowed Vulcans from the OCU and on 11 July 1956 it received the first aircraft of its own. By September 1957, several Vulcans had been handed over to No.83 Squadron The second OCU course also formed a Flight of 83 Squadron, but subsequent trained crews were also used to form the second bomber squadron, 101 Squadron. The last aircraft from the first batch of 25 aircraft had been delivered by the end of 1957 to 101 Squadron.
In order to increase the mission range and flight time for Vulcan operations, in-flight refuelling capabilities were added in 1959 onwards; several Valiant bombers were refurbished as tankers to refuel the Vulcans. Continuous airborne patrols proved untenable, however, and the refuelling mechanisms across the Vulcan fleet fell into disuse in the 1960s. Both Vulcans and the other V-force aircraft routinely visited the Far East, in particular Singapore, where a fully equipped nuclear weapons storage facility had been constructed in 1959. During the Indonesia–Malaysia confrontation Britain planned to deploy three squadrons of V-bomber aircraft and 48 Red Beard tactical nuclear weapons to the region, although this was ultimately decided against, Vulcans trained in the region for both conventional and nuclear missions. Britain regularly deployed Vulcans to the Far East as a part of their contribution to SEATO operations, often to test the defenses of friendly nations in joint exercises. In the early 1970s, the RAF decided to permanently deploy two squadrons of Vulcans overseas in the Near East Air Force Bomber Wing, based at RAF Akrotiri in Cyprus; the Vulcans were withdrawn as Cypriot intercommunal violence intensified in the mid-1970s.
Vulcans did some very long range missions. In June 1961, one of them took off from RAF Scampton to Sydney, with an 18,507 km long journey, flown in only a bit more than 20 hours and three air refuellings. Vulcans frequently visited the United States during the 1960s and 1970s to participate in air shows and static displays, as well as to participate in the Strategic Air Command's Annual Bombing and Navigation Competition at such locations as Barksdale AFB, Louisiana and the former McCoy AFB, Florida, with the RAF crews representing Bomber Command and later Strike Command. Vulcans also took part in the 1960, 1961, and 1962 Operation Skyshield exercises, in which NORAD defences were tested against possible Soviet air attack, the Vulcans simulating Soviet fighter/bomber attacks against New York, Chicago and Washington. The results of the tests were classified until 1997. The Vulcan proved quite successful during the 1974 'Giant Voice' exercise, in which it had managed to avoid USAF interceptors 
Nuclear deterrent.
As part of Britain's independent nuclear deterrent, the Vulcan initially carried Britain's first nuclear weapon, the "Blue Danube" gravity bomb. "Blue Danube" was a low-kiloton yield fission bomb designed before the United States detonated the first hydrogen bomb. These were supplemented by U.S.-owned "Mk 5" bombs (made available under the Project E programme) and later by the British "Red Beard" tactical nuclear weapon. The UK had previously embarked on its own hydrogen bomb programme, and to bridge the gap until these were ready the V-bombers were equipped with an Interim Megaton Weapon based on the "Blue Danube" casing containing "Green Grass", a large pure-fission warhead of yield. This bomb was known as "Violet Club". Only five were deployed before the "Green Grass" warhead was incorporated into a developed weapon as "Yellow Sun Mk.1."
The later "Yellow Sun Mk 2", was fitted with "Red Snow", a British-built variant of the U.S. W28 warhead. "Yellow Sun Mk 2" was the first British thermonuclear weapon to be deployed, and was carried on both the Vulcan and Handley Page Victor. The Valiant retained U.S. nuclear weapons assigned to SACEUR under the dual-key arrangements. "Red Beard" was pre-positioned in Singapore for use by Vulcan and Victor bombers. From 1962, three squadrons of Vulcan B.2s and two squadrons of Victor B.2s were armed with the "Blue Steel" missile, a rocket-powered stand-off bomb, which was also fitted with the yield "Red Snow" warhead.
Operationally, RAF Bomber Command and the U.S. Strategic Air Command cooperated in the Single Integrated Operational Plan (SIOP) to ensure coverage of all major Soviet targets from 1958, 108 aircraft of the RAF's V-Bombers were assigned targets under SIOP by the end of 1959. From 1962 onwards, two jets in every major RAF base were armed with nuclear weapons and on standby permanently under the principle of Quick Reaction Alert (QRA). Vulcans on QRA standby were to be airborne within four minutes of receiving an alert, as this was identified as the amount of time between warning of a USSR nuclear strike being launched and it arriving in Britain. The closest the Vulcan came to take part in potential nuclear conflict was during the Cuban missile crisis in October 1962, where Bomber Command was moved to Alert Condition 3, an increased state of preparedness from normal operations, however stood down in early November.
The Vulcans were intended to be equipped with the American "Skybolt" Air Launched Ballistic Missile to replace the "Blue Steel", with Vulcan B.2s carrying two "Skybolts" under the wings; the last 28 B.2s were modified on the production line to fit pylons to carry the "Skybolt". A B.3 variant with increased wingspan to carry up to six "Skybolts" was proposed in 1960. When the "Skybolt" missile system was cancelled by U.S. President John F. Kennedy on the recommendation of his Secretary of Defense, Robert McNamara in 1962, "Blue Steel" was retained. To supplement it until the Royal Navy took on the deterrent role with Polaris ICBM-equipped submarines, the Vulcan bombers adopted a new mission profile of flying high during clear transit, dropping down low to avoid enemy defences on approach, and deploying a parachute-retarded bomb, the "WE.177B". However, since the aircraft had been designed for high-altitude flight, at low altitudes it could not exceed 350 knots. RAF Air Vice Marshal Ron Dick, a former Vulcan pilot, said "it is questionable whether it could have been effective flying at low level in a war against ... the Soviet Union.”
After the British Polaris submarines became operational and Blue Steel was taken out of service in 1970, the Vulcan continued to carry "WE.177B" in a tactical nuclear strike role as part of the British contribution to Europe's standing NATO forces, although they no longer held aircraft at 15 minutes readiness in peacetime. Two squadrons were also stationed in Cyprus as part of the Near East Air Force and assigned to Central Treaty Organization in a strategic strike role. With the eventual demise of the "WE.177B" and the Vulcan bombers, the Blackburn Buccaneer, SEPECAT Jaguar, and Panavia Tornado, continued with the "WE.177C" until its retirement in 1998. While not a like-for-like replacement, the multi-role Tornado interdictor/strike bomber is the successor for the roles previously filled by the Vulcan.
Conventional role.
Although in operational usage the Vulcan typically carried various nuclear armaments, the type also had a secondary conventional role. While performing conventional combat missions, the Vulcan can carry up to 21  bombs inside its bomb bay. Since the 1960s, the various Vulcan squadrons were routinely conducting conventional training missions; the aircrews were expected to be able to perform conventional bombing missions in addition to the critical nuclear strike mission the Vulcan normally performed.
The Vulcan's only combat missions took place towards the end of the type's service in 1982. During the Falklands War, the Vulcan was deployed against Argentinian forces which had occupied the Falkland Islands. This conflict was the only occasion in which any of the V-bombers would participate in conventional warfare. The missions performed by the Vulcan became known as the "Black Buck" raids, each aircraft had to fly from Ascension Island to reach Stanley on the Falklands. Victor tankers conducted the necessary air-to-air refuelling for the Vulcan to cover the distance involved; approximately of fuel was used in each mission.
Five Vulcans were selected to participate in the operation. In order to do so, each aircraft had to receive various last-minute adaptations; including modifications to the bomb bay, the reinstatement of the long out-of-use in-flight refuelling system, the installation of a new navigational system derived from the Vickers VC-10, and the updating of several onboard electronics. Underneath the wings, new pylons were fitted to carry an ECM pod and Shrike anti-radar missiles at wing hardpoint locations; these hardpoints had originally been installed for the purpose of carrying the cancelled Skybolt nuclear missile. Engineering work to retrofit these Vulcans had begun on 9 April.
On 1 May, the first mission was conducted by a single Vulcan that flew over Port Stanley and dropped its bombs on the airfield concentrating on the single runway, with one direct hit, making it unsuitable for fighter aircraft. The Vulcan's mission was quickly followed up by strikes against anti-air installations, flown by British Aerospace Sea Harriers from nearby Royal Navy carriers. Three Vulcan missions were flown against the airfield, a further two missions in which missiles were launched against radar installations; an additional two missions were cancelled. At the time, these missions held the record for the world's longest-distance raids. The ECM systems on board the Vulcans proved to be effective at jamming Argentine radars; while a Vulcan was within the theatre, other British aircraft in the vicinity had a greatly reduced chance of coming under effective fire.
On 3 June 1982, Vulcan B.2 "XM597" of No. 50 Squadron took part in the "Black Buck 6" mission against Argentinian radar sites at Stanley airfield on the Falkland Islands. While attempting to refuel for its return journey to Ascension Island, the probe broke, leaving the Vulcan with insufficient fuel, forcing a diversion to Galeão Air Force Base, Rio de Janeiro in neutral Brazil. En route, secret papers were dumped along with the two remaining AGM-45 Shrike missiles, although one failed to launch. After a mayday call, the Vulcan, escorted by Brazilian Air Force Northrop F-5 fighters, was permitted an emergency landing at Rio with very little fuel left on board. The Vulcan and her crew were detained until the end of hostilities nine days later.
Reconnaissance.
In November 1973, as a result of the planned closure of the Victor SR.2 equipped No. 543 Squadron, No. 27 Squadron reformed at RAF Scampton equipped with the Vulcan as a replacement in the maritime radar reconnaissance role. The squadron carried out patrols of the seas around the British Isles, including the strategically important GIUK gap between Iceland and the United Kingdom, flying at high level and using the Vulcan's H2S radar to monitor shipping. In peacetime, this could be followed up by visual identification and photography of targets of interest at low level. In wartime, a Vulcan would leave visual identification of potential targets to Buccaneers or Canberras, and could coordinate attacks by Buccaneers against hostile shipping. Though initially equipped with a number of B.2 aircraft, the Squadron eventually operated nine B.2 (MRR) aircraft (also known by the unofficial designation SR.2). The aircraft were modified for the role by removing the Terrain Following Radar (and its thimble radome) and adding the LORAN C radio navigation aid. The main external visual difference was the presence of a gloss paint finish, with a light grey undersurface, to protect against sea spray.
The squadron also inherited its secondary role of air sampling from No. 543 Squadron. This involved flying through plumes of airborne contamination and using onboard equipment to collect fallout released from both above ground and underground nuclear tests for later analysis at the Atomic Weapons Research Establishment at Aldermaston. Five aircraft had small pylons fitted to the redundant Skybolt hardpoints, which could be used to carry sampling pods modified from drop tanks. These pods would collect the needed samples on a filter, while an additional smaller "localiser" pod was fitted to the port wing, inboard of the main pylons.
The squadron disbanded at Scampton in March 1982, passing on its radar reconnaissance duties to the RAF's Nimrods.
Aerial refuelling role.
After the end of the Falklands War in 1982, the Vulcan B.2 was due to be withdrawn from RAF service that year. However, the Falklands campaign had consumed much of the airframe fatigue life of the RAF's Victor tankers. While Vickers VC10 tanker conversions had been ordered in 1979 and Lockheed TriStar tankers would be ordered subsequent to the conflict, as a stopgap measure six Vulcans were converted into single point tankers. The Vulcan tanker conversion was accomplished by removing the jammers from the ECM bay in the tail of the aircraft, and replacing them with a single Hose Drum Unit (HDU). An additional cylindrical bomb-bay tank was fitted, making a total of three, giving a fuel capacity of almost .
The go-ahead for converting the six aircraft was given on 4 May 1982. Just 50 days after being ordered, the first Vulcan tanker, "XH561", was delivered to RAF Waddington. The Vulcan K.2s were operated by No. 50 Squadron, along with three Vulcan B.2s, in support of UK air defence activities until it was disbanded in March 1984.
Vulcan Display Flight.
After the disbandment of No. 50 Squadron, two Vulcans continued flying with the RAF in air displays as part of the Vulcan Display Flight, based at Waddington but administered through No. 55 Squadron, based at RAF Marham. Initially displaying using XL426, in 1986 that aircraft was sold, having been replaced by XH558, which began displays in 1985. The VDF continued with XH558 until 1992, finishing operations after the Ministry of Defence determined it was too costly to run in light of budget cuts. Both aircraft subsequently entered preservation and survived, although a third, XH560, kept in reserve in the first years, was later scrapped.
Variants.
Production.
A total of 134 production Vulcans were assembled at Woodford Aerodrome, 45 to the B.1 design and 89 were B.2 models, the last being delivered to the RAF in January 1965.
Operators.
V-Bomber dispersal airfields.
In the event of transition to war, the V Bomber squadrons were to deploy four aircraft at short notice to each of 26 pre-prepared dispersal airfields around the United Kingdom. In the early 1960s the RAF ordered 20 Beagle Basset communication aircraft to move the crews to dispersal airfields; the importance of these aircraft was only brief, diminishing when the primary nuclear deterrent switched to the Royal Navy's Polaris Missile.
Surviving aircraft.
Several Vulcans survive, housed in museums in both the United Kingdom and North America (USA & Canada). One Vulcan, XH558 (G-VLCN) "Spirit of Great Britain", was used as a display aircraft by the RAF as part of the Vulcan Display Flight until 1993. After being grounded it was later restored to flight by the Vulcan To The Sky Trust and displayed as a civilian aircraft from 2008 until 2015, before being retired a second time for engineering reasons. In retirement, XH558 is to be retained at its base at Robin Hood Airport as a taxi-able aircraft, a role already performed by two other survivors, XL426 (G-VJET) based at Southend Airport, and XM655 (G-VULC), based at Wellesbourne Mountford Airfield.

</doc>
<doc id="44071" url="https://en.wikipedia.org/wiki?curid=44071" title="M61 Vulcan">
M61 Vulcan

The M61 Vulcan is a hydraulically or pneumatically driven, six-barrel, air-cooled, electrically fired Gatling-style rotary cannon which fires 20 mm rounds at an extremely high rate (typically 6,000 rounds per minute). The M61 and its derivatives have been the principal cannon armament of United States military fixed-wing aircraft for fifty years.
The M61 was originally produced by General Electric. After several mergers and acquisitions, it is currently produced by General Dynamics.
Development.
At the end of World War II, the United States Army began to consider new directions for future military aircraft guns. The higher speeds of jet-powered fighter aircraft meant that achieving an effective number of hits would be extremely difficult without a much higher volume of fire. While captured German designs (principally the Mauser MG 213C) showed the potential of the single-barrel revolver cannon, the practical rate of fire of such a design was still limited by ammunition feed and barrel wear concerns. The Army wanted something better, combining extremely high rate of fire with exceptional reliability. In 1947, the Air Force became a separate branch of the military. The new Air Force made a request for a new aircraft gun. A lesson of World War II air combat was that German, Italian and Japanese fighters could attack American aircraft from long range with their cannon main armament. American fighters with .50 cal main armament, such as the P-51 and P-47, had to be close to the enemy in order to hit and damage enemy aircraft. The 20mm Hispano carried by the P-38, while formidable against propeller driven planes, was deemed a relatively low velocity weapon in the age of jets, while other cannons were notoriously unreliable.
In response to this requirement, the Armament Division of General Electric resurrected an old idea: the multi-barrel Gatling gun. The original Gatling gun had fallen out of favor because of the need for an external power source to rotate the barrel assembly, but the new generation of turbojet-powered fighters offered sufficient electric power to operate the gun, and electric operation was more reliable than gas-operated reloading. With multiple barrels, the rate of fire per barrel could be lower than a single-barrel revolver cannon while providing a greater overall rate of fire. The idea of powering a Gatling gun from an external electric power source was not a novel idea at the end of the World War II, as Richard Jordan Gatling himself had done just that with a patent he filed in 1893
In 1946, the Army issued General Electric a contract for "Project Vulcan", a six-barrel weapon capable of firing 7,200 rounds per minute (rpm). Although European designers were moving towards heavier 30 mm weapons for better hitting power, the U.S. initially concentrated on a powerful cartridge designed for a pre-war anti-tank rifle, expecting that the cartridge's high muzzle velocity would be beneficial for improving hit ratios on high speed targets.
The first GE prototypes of the caliber T45 were ground-fired in 1949; it achieved 2,500 rpm, which was increased to 4,000 rpm by 1950. By the early 1950s, the USAF decided that high velocity alone might not be sufficient to ensure target destruction and tested 20 mm and 27 mm alternatives based on the caliber cartridge. These variants of the T45 were known as the T171 and T150 respectively, and were first tested in 1952. Eventually, the 20×102 mm cartridge was determined to have the desired balance of projectile and explosive weight and muzzle velocity.
The development of the Lockheed F-104 Starfighter revealed that the T171 Vulcan (later redesignated "M61") suffered problems with its linked ammunition, being prone to misfeed and presenting a foreign object damage (FOD) hazard with discarded links. A linkless ammunition feed system was developed for the upgraded "M61A1", which subsequently became the standard cannon armament of U.S. fighters.
In 1993, General Electric sold its aerospace division, including GE Armament Systems along with the design and production tooling for the M61 and GE's other rotary cannon, to Martin Marietta. After Martin's merger with Lockheed, the rotary cannon became the responsibility of Lockheed Martin Armament Systems. Lockheed Martin Armament Systems was later acquired by General Dynamics, who currently produce the M61 and its variants.
Description.
Each of the cannon's six barrels fires once in turn during each revolution of the barrel cluster. The multiple barrels provide both a very high rate of fire—around 100 rounds per second—and contribute to prolonged weapon life by minimizing barrel erosion and heat generation. Mean time between jams or failures is in excess of 10,000 rounds, making it an extremely reliable weapon. The success of the Vulcan Project and its subsequent progeny, the very-high-speed Gatling gun, has led to guns of the same configuration being referred to as "Vulcan cannon", which can sometimes confuse nomenclature on the subject.
Most aircraft versions of the M61 are hydraulically driven and electrically primed. The gun rotor, barrel assembly and ammunition feed system are rotated by a hydraulic drive motor through a system of flexible drive shafts. The round is fired by an electric priming system where an electric current from a firing lead passes through the firing pin to the primer as each round is rotated into the firing position.
The self-powered version, the GAU-4 (called M130 in Army service), is gas-operated, tapping gun gas from three of the six barrels to operate the gun gas driven mechanism. The self-powered Vulcan weighs about more than its electric counterpart, but requires no external power source to operate, except for an electric, inertia starter to initiate gun rotation, allowing the first rounds to be chambered and fired.
The initial "M61" used linked, belted ammunition, but the ejection of spent links created considerable (and ultimately insuperable) problems. The original weapon was soon replaced by the "M61A1", with a linkless feed system. Depending on the application, the feed system can be either single-ended (ejecting spent cases and unfired rounds) or double-ended (returning casings back to the magazine). A disadvantage of the M61 is that the bulk of the weapon, its feed system, and ammunition drum makes it difficult to fit it into a densely packed airframe.
The feed system must be custom-designed for each application, adding to the complete weapon. Most aircraft installations are double-ended, because the ejection of empty cartridges can cause a foreign-object damage (FOD) hazard for jet engines and because the retention of spent cases assists in maintaining the center of gravity of the aircraft. The first aircraft to carry the M61A1 was the C model of the F-104, starting in 1959.
A lighter version of the Vulcan developed for use on the F-22 Raptor, the "M61A2", is mechanically the same as the M61A1, but with thinner barrels to reduce overall weight to . The rotor and housing have also been modified to remove any piece of metal not absolutely needed for operation and replaces some metal components with lighter weight materials. The F/A-18E/F Super Hornet also uses this version.
The Vulcan's rate of fire is typically 6,000 rounds per minute, although some versions (such as that of the AMX and the F-106 Delta Dart) are limited to a lower rate, and others (A-7 Corsair) have a selectable rate of fire of either 4,000 or 6,000 rounds per minute. The M61A2's lighter barrels allow a somewhat higher rate of fire, up to 6,600 rounds per minute.
Ammunition.
Practically no powered rotary cannon is supplied with sufficient ammunition for a full minute of firing, due to its weight. In order to avoid using the few hundred rounds carried all at once, a burst controller is generally used to limit the number of rounds fired at each trigger pull. Bursts of from two or three up to 40 or 50 can be selected. The size of the airframe and available internal space limits the size of the ammunition drum and thus limits the ammunition capacity.
Until the late 1980s, the M61 primarily used the M50 series of ammunition in various types, typically firing a projectile at a muzzle velocity of about . A variety of Armor-Piercing Incendiary (API), High Explosive Incendiary (HEI), and training rounds are available. The M246 HEI-T-SD (High-Explosive Incendiary, Tracer, Self-Destroying) is the primary round for use against aerial targets. 
The new PGU-28/B round was developed in the mid-1980s. It is a semi-armor-piercing high-explosive incendiary (SAPHEI) round, providing improvements in range, accuracy, and power over the preceding M56A3 HEI round. PGU-28/B is a "low-drag" round designed to reduce in-flight drag and deceleration, and has a slightly increased muzzle velocity of . However, the PGU-28/B has not been without problems. A 2000 USAF safety report noted 24 premature detonation mishaps (causing serious damage in many cases) in 12 years with the SAPHEI round, compared to only two such mishaps in the entire recorded history of the M56 round. The report estimated that the current PGU-28/B had a potential failure rate 80 times higher than USAF standards permit. Due to safety issues, it was limited to emergency wartime use in 2000.
The main types of combat rounds and their main characteristics are listed in the table below.
Applications and first combat use.
The Vulcan first entered aerial combat on 4 April 1965 when four North Vietnamese Air Force MiG-17s (J-5s) attacked a force of 10 escorting North American F-100 Super Sabres (2 of which were assigned weather reconnaissance duties) and 48 Vulcan-armed and "bomb-laden" F-105 Thunderchiefs, shooting down two of the latter. The MiG Leader, Capt. Tran Hanh, and the only survivor from the four MiGs reported that U.S. jets had pursued them and that F-105s had shot down three of his aircraft, killing Lieutenants Pham Giay, Le Minh Huan, and Tran Nguyen Nam. Capt. Donald Kilgus piloting an F-100 received an official probable kill with his four M39 20 mm cannons during the engagement; however no other US pilot reported destroying any MiGs during the battle, leaving open the plausibility that at least two of the MiG-17s may have been downed by their own anti-aircraft fire.
The first confirmed Vulcan gun kill occurred on 29 June 1966 when Major Fred Tracy, flying his F-105 Thunderchief with the 421st TFS, fired 200 rounds of 20mm into a MiG-17 that had just fired a 23mm shell through one side of his cockpit and which exited out the other side. When the NVAF MiG flew in front of him after making his pass, Maj. Tracy opened fire on him.
The gun was installed in the Air Force's A-7D version of the LTV A-7 Corsair II where it replaced the earlier United States Navy A-7's Colt Mk 12 cannon and was adopted by the Navy on the A-7C and A-7E. It was integrated into the newer F-4E Phantom II variants. The F-4 was originally designed without a cannon as it was believed that missiles had made guns obsolete. Combat experience in Vietnam showed that a gun could be more effective than guided missiles in many combat situations, and that an externally carried gun pod was less effective than an internal gun; the first generation of gun pods such as the SUU-16 were not oriented with the sights of the fighter. The improved pods were self-powered and properly synchronized to the fighters sights. The next generation of fighters built post-Vietnam incorporated the M61 gun internally.
The Vulcan was later fitted into the weapons bay of some Convair F-106 Delta Dart and General Dynamics F-111 Aardvark models. It was also adopted as standard in the "teen"-series air superiority fighters, the Grumman F-14 Tomcat, the McDonnell Douglas F-15 Eagle, General Dynamics F-16 Fighting Falcon and McDonnell Douglas F/A-18 Hornet. Other aircraft include the Italian/Brazilian AMX International AMX (on Italian aircraft only), and the F-22 Raptor. It was fitted in a side-firing installation on the Fairchild AC-119, some marks of the Lockheed AC-130 gunships, and was used in the tail turrets of both the Convair B-58 Hustler and Boeing B-52H Stratofortress bombers. Japan's Mitsubishi F-1 carried one internally mounted JM61A1 Vulcan with 750 rounds.
Two gun pod versions, the SUU-16/A (also designated M12 by the US Army) and improved SUU-23/A (US Army M25), were developed in the 1960s, often used on gunless versions of the F-4. The SUU-16/A uses the electric M61A1 with a ram-air turbine to power the motor. This proved to cause serious aerodynamic drag at higher speeds, while speeds under did not provide enough airflow for the maximum rate of fire.
The subsequent SUU-23/A uses the "GAU-4/A" self-powered Vulcan, with an electric inertia starter to bring it up to speed. Both pods ejected empty cases and unfired rounds rather than retaining them. Both pods contained 1,200 rounds of ammunition, with a loaded weight of respectively. During service in the Vietnam War, the pods proved to be relatively inaccurate: the pylon mounting was not rigid enough to prevent deflection when firing, and repeated use would misalign the pod on its pylon, making matters worse.
A variant with much shorter barrels, designated the M195, was also developed for use on the M35 Armament Subsystem for use on the AH-1G Cobra helicopter. This variant fed from ammunition boxes fitted to the landing skid and was developed to provide the AH-1 helicopter with a longer-range suppressive fire system before the adoption of the M97 Universal Turret mounting the M197 cannon.
The M61 is also the basis of the US Navy Mk 15 Phalanx Close-in weapon system system and the M163 VADS Vulcan Air Defense System, using the M168 variant.

</doc>
<doc id="44077" url="https://en.wikipedia.org/wiki?curid=44077" title="Captains Courageous">
Captains Courageous

Captains Courageous is an 1897 novel, by Rudyard Kipling, that follows the adventures of fifteen-year-old Harvey Cheyne Jr., the spoiled son of a railroad tycoon, after he is saved from drowning by a Portuguese fisherman in the north Atlantic. The novel originally appeared as a serialisation in "McClure's", beginning with the November 1896 edition.
The book's title comes from the ballad "Mary Ambree", which starts, "When captains courageous, whom death could not daunt". Kipling had previously used the same title for an article on businessmen as the new adventurers, published in "The Times" of 23 November 1892.
Plot.
Protagonist Harvey Cheyne, Jr., is the son of a wealthy railroad magnate and his wife, in San Diego, California. Washed overboard from a transatlantic steamship and rescued by fishermen off the Grand Banks of Newfoundland, Harvey can neither persuade them to take him quickly to port, nor convince them of his wealth. Disko Troop, captain of the schooner "We're Here", offers him temporary membership in the crew until they return to port, and Harvey later accepts.
Through a series of trials and adventures, Harvey, with the help of the captain's son Dan Troop, becomes acclimated to the fishing lifestyle, and even skillful. Eventually, the schooner returns to port and Harvey wires his parents, who immediately hasten to Boston, Massachusetts, and thence to the fishing town of Gloucester to recover him. There, Harvey's mother rewards the seaman Manuel, who initially rescued her son; Harvey's father hires Dan to work on his prestigious tea clipper fleet; and Harvey begins his career in his father's shipping lines.
Notes.
The book was written during Kipling's time living in Brattleboro, Vermont. Kipling recalled in his autobiography:
Kipling also recalled:
The resulting account, in Chapter 9, of the Cheynes' journey from San Diego to Boston, is a classic of railway literature. The couple travel in the Cheynes' private rail car, the "Constance", and are taken from San Diego to Chicago as a special train, hauled by sixteen locomotives in succession. It takes precedence over 177 other trains. "Two and one-half minutes would be allowed for changing engines; three for watering and two for coaling". The "Constance" is attached to the scheduled express "New York Limited" to Buffalo, New York and then transferred to the New York Central for the trip across the state to Albany. Switched to the Boston and Albany Railroad, the Cheynes complete the trip to Boston in their private car, with the entire cross-country run taking 87 hours 35 minutes. 
Kipling also recalled:
Disko Troop claims to receive his given name for his birth on board his father's ship near Disko Island on the west coast of Greenland. His crewman 'Long Jack' once calls him "Discobolus".
Film, TV, theatrical, or other adaptations.
"Captains Courageous" has been adapted for film three times:
Musical theatre:
Other adaptations:

</doc>
<doc id="44082" url="https://en.wikipedia.org/wiki?curid=44082" title="Marine science">
Marine science

Marine Science is a multidisciplinary field of study and research of ocean life and physics. Overlap areas between the different marine science disciplines are increasingly targeted as in seems necessary to investigate these areas in order to fully understand the workings of the marine environment.
Basic marine sciences are:

</doc>
<doc id="44085" url="https://en.wikipedia.org/wiki?curid=44085" title="John Byron">
John Byron

Vice Admiral The Hon. John Byron, RN (8 November 1723 – 10 April 1786) was a Royal Navy officer. He was known as Foul-weather Jack because of his frequent encounters with bad weather at sea. As a midshipman, he sailed in the squadron under George Anson on his voyage around the world, though Byron made it to southern Chile, and returned to England with the captain of HMS "Wager". He was governor of Newfoundland following Hugh Palliser, who left in 1768. He circumnavigated the world as a commodore with his own squadron in 1764-1766. He fought in battles in The Seven Years' War and the American Revolution. He rose to Vice Admiral of the White before his death in 1786.
His grandsons include the poet George Gordon Byron and George Anson Byron, admiral and explorer, who were the 6th and 7th Baron Byron, respectively.
Early career.
Byron was the son of William Byron, 4th Baron Byron and Frances Berkeley. He joined the Royal Navy in 1731. In 1740, he accompanied George Anson on his voyage around the world as a midshipman aboard one of the several ships in the squadron. On 14 May 1741, HMS "Wager" under Captain Cheap (as Captain Dandy Kidd had died), was shipwrecked on the coast of Chile on what is now called Wager Island and Byron was one of the survivors. The survivors decided to split in two teams, one to make its way by boat to Rio de Janeiro on the Atlantic coast; the other, including John Byron and the Captain, to sail north along the Spanish colonial coast.
Captain Cheap at Wager Island had a party of 19 men after the deserters rejoined the camp. This included the surgeon Elliot and Lieutenant Hamilton who had been cast adrift with him plus midshipmen John Byron and Campbell who had been on the barge. They rowed up the coast but were punished by continuous rain, headwinds and waves that threatened the boats. One night while the men slept on shore, one of the boats was capsized while at anchor and was swept out to sea with its two boatkeepers. One of the men got ashore but the other drowned. As it was now impossible for them all to fit in the remaining boat, four marines were left ashore with muskets to fend for themselves. The winds prevented them from getting around the headland so they returned to pick up the marines only to find them gone. They returned to Wager Island in early February 1742. With one death on the journey, there were now 13 in the group.
A local Indian guided the men up the coast to Chiloe Island so they set out again. Two men died and after burying the bodies, the six seaman rowed off in the boat never to be seen again while Cheap, Hamilton, Byron, Campbell and the dying Elliot were on shore looking for food. The Indian then agreed to take the remaining four on by canoe for their only remaining possession, a musket. Eventually they made it to be taken prisoner by the Spanish. Fortunately the Spaniards treated them well and they were eventually taken to the inland capital of Santiago where they were released on parole. The Spaniards heard that Anson had been generous in the treatment of the prisoners he had taken and this kindness was returned.
Byron and the other three men stayed in Santiago till late 1744 and were offered passage on a French ship bound for Spain. Three accepted the passage. Campbell elected to take a mule across the Andes and joined the Spanish Admiral Pizarro in Montevideo on the "Asia" only to find Isaac Morris and the two seamen who had been abandoned in Freshwater Bay on the Atlantic coast. After time in prison in Spain, Campbell reached Britain in May 1746, followed by the other three two months later.
In England, the official court martial examined only the loss of the "Wager" in which Baynes, in nominal charge at the time, was acquitted of blame but reprimanded for omissions of duty. Disputes over what happened after the wreck were instead played out as Bulkeley and Cummins, Campbell, Morris, the cooper Young and later Byron published their own accounts, the last of which was the only one that in any way defended Cheap who had since died. Twenty nine crew members plus seven marines made it back to England.
Byron's account of his adventures and the Wager Mutiny are recounted in "The Narrative of the Honourable John Byron" (1768). His book sold well enough to be printed in several editions.
Byron was appointed captain of in December 1746.
Seven Years War.
In 1760 during the Seven Years' War, Byron commanded a squadron sent to destroy the fortifications at Louisbourg, Quebec, which had been captured by the British two years before. They wanted to ensure it could not be used by the French in Canada. In July of that year he defeated the French flotilla sent to relieve New France at the Battle of Restigouche.
Commodore, governor and vice admiral.
Between June 1764 and May 1766, Byron completed his own circumnavigation of the globe as captain of HMS "Dolphin". This was the first such circumnavigation that was accomplished in less than 2 years. During this voyage, in 1765 he took possession of the Falkland Islands on behalf of Britain on the grounds of prior discovery. His action nearly caused a war between Great Britain and Spain, as both countries had armed fleets ready to contest the sovereignty of the barren islands. Later Byron encountered islands and extant residents of the Tuamotus and Tokelau Islands, and Nikunau in the southern Gilbert Islands; he also visited Tinian in the Northern Marianas Islands.
In 1769 he was appointed governor of Newfoundland off the mainland of Canada, an office he held for the next three years.
He was promoted to rear admiral on 31 March 1775. In 1778 and 1779, he served as Commander-in-chief of the British fleet in the West Indies during the American War of Independence. He unsuccessfully attacked a French fleet under the Comte d'Estaing at the Battle of Grenada in July 1779. Byron was briefly Commander-in-Chief, North American Station from 1 October 1779. He was made vice admiral of the white in September 1780.
Family.
On 8 September 1748 he married Sophia Trevanion, daughter of John Trevanion of Caerhays in Cornwall, by whom he had two sons and seven daughters, three of whom died in infancy {a daughter Juliana Bryon married her cousin William Byron {died 1776}son of William Byron, 5th Baron Byron}. Their eldest son, John "Mad Jack" Byron, in turn fathered the poet George Gordon Byron, the future 6th Baron Byron. John Byron was also the grandfather of George Anson Byron, another admiral and explorer and later the 7th Baron Byron. He was the brother of Hon. George Byron, married to Frances Levett, daughter of Elton Levett of Nottingham, a descendant of Ambrose Elton, Esq., High Sheriff of Herefordshire in 1618 and a surgeon in Nottingham.
Death.
John Byron died on 10 April 1786 at home in London. His remains were buried in the Berkeley family vault situated beneath the chancel of the Church of St Mary the Virgin, Twickenham.
In fiction.
John Byron's experiences in the Anson voyage form the basis of the novel "The Unknown Shore" by Patrick O'Brian. It closely follows Byron's account in "The Narrative of the Honourable John Byron" (1768).

</doc>
<doc id="44086" url="https://en.wikipedia.org/wiki?curid=44086" title="Lew Wallace">
Lew Wallace

Lewis "Lew" Wallace (April 10, 1827February 15, 1905) was an American lawyer, Union general in the American Civil War, governor of the New Mexico Territory, politician, diplomat, and author from Indiana. Among his novels and biographies, Wallace is best known for his historical adventure story, "" (1880), a bestselling novel that has been called "the most influential Christian book of the nineteenth century."
Wallace's military career included service in the Mexican–American War and the American Civil War. He was appointed Indiana's adjutant general and commanded the 11th Indiana Infantry Regiment. Wallace, who attained the rank of major general, participated in the Battle of Fort Donelson, the Battle of Shiloh, and the Battle of Monocacy. He also served on the military commission for the trials of the Lincoln assassination conspirators, and presided over the military investigation of Henry Wirz, a Confederate commandant of the Andersonville prison camp.
Wallace resigned from the U.S. Army in November 1865 and briefly served as a major general in the Mexican army, before returning to the United States. Wallace was appointed governor of the New Mexico Territory (1878–81) and served as U.S. minister to the Ottoman Empire (1881–85). Wallace retired to his home in Crawfordsville, Indiana, where he continued to write until his death in 1905.
Early life and education.
Lewis "Lew" Wallace was born on April 10, 1827, in Brookville, Indiana. He was the second of four sons born to Esther French Wallace (née Test) and David Wallace. Lew's father, a graduate of the U.S. Military Academy in West Point, New York, left the military in 1822 and moved to Brookville, where he established a law practice and entered Indiana politics. David served in the Indiana General Assembly and later as the state's lieutenant governor, and governor, and as a member of Congress. Lew Wallace's maternal grandfather was circuit court judge and Congressman John Test.
In 1832 the family moved to Covington, Indiana, where Lew's mother died from tuberculosis on July 14, 1834. In December 1836, David married nineteen-year-old Zerelda Gray Sanders Wallace, who later became a prominent suffragist and temperance advocate. In 1837, after David's election as governor of Indiana, the family moved to Indianapolis.
Lew began his formal education at the age of six at a public school in Covington, but he much preferred the outdoors. Wallace had a talent for drawing and loved to read, but he was a discipline problem at school. In 1836, at the age of nine, Lew joined his older brother in Crawfordsville, Indiana, where he briefly attended Wabash Preparatory School, but soon transferred to another school more suitable for his age. In 1840, when Wallace was thirteen, his father sent him to a private academy at Centerville, Indiana, where his teacher encouraged Lew's natural affinity for writing. Wallace returned to Indianapolis the following year.
Sixteen-year-old Lew went out to earn his own wages in 1842, after his father refused to pay for more schooling. Wallace found a job copying records at the Marion County clerk's office and lived in an Indianapolis boardinghouse. He also joined the Marion Rifles, a local militia unit, and began writing his first novel, "The Fair God", but it was not published until 1873. Wallace acknowledged in his autobiography that he had never been a member of any organized religion, but he did believe "in the Christian conception of God."
By 1846, at the start of the Mexican–American War, the nineteen-year-old Wallace was studying law at his father's law office, but left that pursuit to establish a recruiting office for the Marion Volunteers in Indianapolis. He was appointed a second lieutenant, and on June 19, 1846, mustered into military service with the Marion Volunteers (also known as Company H, 1st Indiana Volunteer Infantry). Wallace rose to the position of regimental adjutant and the rank of first lieutenant while serving in the army of Zachary Taylor, but Wallace personally did not participate in combat. Wallace was mustered out of the volunteer service on June 15, 1847, and returned to Indiana, where he intended to practice law. After the war, Wallace and William B. Greer operated a Free Soil newspaper, "The Free Soil Banner," in Indianapolis.
Marriage and family.
In 1848 Wallace met Susan Arnold Elston at the Crawfordsville home of Henry Smith Lane, Wallace's former commander during the Mexican War. Susan was the daughter of Major Isaac Compton Elston, a wealthy Crawfordsville merchant, and Maria Akin Elston, whose family were Quakers from upstate New York. Susan accepted Wallace's marriage proposal in 1849, and they were married in Crawfordsville on May 6, 1852. The Wallaces had one son, Henry Lane Wallace, who was born on February 17, 1853.
Early law and military career.
Wallace was admitted to the bar in February 1849, and moved from Indianapolis to Covington, Indiana, where he established a law practice. In 1851 Wallace was elected prosecuting attorney of Indiana's 1st congressional district, but he resigned in 1853 and moved his family to Crawfordsville, in Montgomery County, Indiana. Wallace continued to practice law and was elected as a Democrat to a two-year term in the Indiana Senate in 1856. From 1849 to 1853, his office was housed in the Fountain County Clerk's Building.
While living in Crawfordsville, Wallace organized the Crawfordsville Guards Independent Militia, later called the Montgomery Guards. During the winter of 1859–60, after reading about elite units of the French Army in Algeria, Wallace adopted the Zouave uniform and their system of training for the group. The Montgomery Guards would later form the core of his first military command, the 11th Indiana Volunteer Infantry, during the American Civil War.
Civil War service.
Wallace, a staunch, pro-Union supporter who became a member of the Republican party, began his full-time military career after the Confederate attack on Fort Sumter, South Carolina, on April 12, 1861. Indiana's Republican governor, Oliver P. Morton, asked Wallace to help recruit Indiana volunteers for the Union army. Wallace, who also sought a military command, agreed to become the state's adjutant general on the condition that he would be given command of a regiment of his choice. Indiana's quota of six regimental units was filled within a week, and Wallace took command of the 11th Indiana Volunteer Infantry Regiment, which was mustered into the Union army on April 25, 1861. Wallace received his formal commission as a colonel in the Union army the following day.
On June 5, 1861, Wallace went with the 11th Indiana to Cumberland, Maryland, and on June 12, the regiment won a minor battle at Romney, Virginia, (in present-day West Virginia). The rout boosted morale for Union troops and led to the Confederate evacuation of Harpers Ferry on June 18. On September 3, 1861, Wallace was promoted to brigadier general of U.S. Army volunteers and given command of a brigade.
Forts Henry and Donelson.
On February 4 and 5, 1862, prior to the advance against Fort Henry, Union troops under the command of Brig. Gen. Ulysses S. Grant and a flotilla of Union ironclads and timberclad gunboats under the command of Flag Officer Andrew Hull Foote made their way toward the Confederate fort along the Tennessee River in western Tennessee. Wallace's brigade, which was attached to Brig. Gen. Charles F. Smith's division, was ordered to occupy Fort Heiman, an uncompleted Confederate fort across the river from Fort Henry. Wallace's troops secured the deserted fort and watched the Union attack on Fort Henry from their hilltop position. On February 6, after more than an hour of bombardment from the Union gunboats, Confederate Brig. Gen. Lloyd Tilghman, surrendered Fort Henry to Grant.
Grant's superior, Maj. Gen. Henry W. Halleck, was concerned that Confederate reinforcements would try to retake the two forts when the Union troops moved overland toward Fort Donelson, so Wallace was left in command at Fort Henry to keep the forts secure. Displeased to have been left behind, Wallace prepared his troops to move out at a moment's notice. The order came at midnight on February 13. Wallace arrived along the Cumberland River the following day and was placed in charge of the 3rd Division. Many of the men in the division were untested reinforcements. Wallace's three brigades took up position in the center of the Union line, facing Fort Donelson.
During the fierce Confederate assault on February 15, and in Grant's absence from the battlefield, Wallace acted on his own initiative to send Cruft's brigade to reinforce the beleaguered division of Brig. Gen. John A. McClernand, despite orders from Grant to hold his position and prevent the enemy from escaping and without Grant's authority to take the offensive. With the Confederates continuing to advance, Wallace led a second brigade to the right and engaged the Confederates with infantry and artillery. Wallace's decision stopped their forward movement and was key in stabilizing a defensive line for the Union troops. After the Confederate assault had been checked, Wallace led a counterattack that regained the lost ground on the Union right. On March 21, 1862, Wallace, McClernand, and C. F. Smith were promoted to major general for their efforts. Wallace, who was age thirty-four at the time of his promotion, became the youngest major general in the Union army.
Shiloh.
Wallace's most controversial command came at the battle of Shiloh, where he continued as the 3rd Division commander under Maj. Gen. Grant. The long-standing controversy developed around the contents of Wallace's written orders on April 6, the 3rd Division's movements on the first day of battle, and their late arrival on the field. On the second day of battle, Wallace's division joined reinforcements from Maj. Gen. Don Carlos Buell's army to play an important role in the Union victory.
Prior to the battle, Wallace's division had been left in reserve and was encamped near Crump's Landing. Their orders were to guard the Union’s right flank and cover the road to Bethel Station, Tennessee, where railroad lines led to Corinth, Mississippi, to the south. To protect the road from Crump's Landing and Bethel Station, Wallace sent Col. John M. Thayer's 2nd Brigade to Stoney Lonesome, west of at Crump's Landing, and the 3rd Brigade, commanded by Col. Charles Whittlesey to Adamsville, west of Crump's Landing. Col. Morgan L. Smith's 1st Brigade remained with Wallace at Crump's Landing, north of Pittsburg Landing, Tennessee.
Between 5 and 6 a.m. on April 6, 1862, Grant's army at Pittsburg Landing was surprised and nearly routed by a sudden attack from the Confederate army under Gen. Albert Sidney Johnston. Grant, who heard the early morning artillery fire, took a steamboat from his headquarters at Savannah, Tennessee, to Crump's Landing, where he gave Wallace orders to wait in reserve and be ready to move. Grant proceeded to Pittsburg Landing, where he arrived around 8:30 a.m. Grant's new orders to Wallace, which arrived between 11 and 11:30 a.m., were given verbally to an aide, who transcribed them before they were delivered. The written orders were lost during the battle, so their exact wording cannot be confirmed; however, eyewitness accounts agree that Grant ordered Wallace to join the right side of the Union army, presumably in support of Brig. Gen. William Tecumseh Sherman's 5th Division, who were encamped near Shiloh Church on the morning of April 6.
Knowledge of the area's roads played a critical role in Wallace's journey to the battlefield on April 6. In late March, after heavy rains made transportation difficult between Crump's Landing and Pittsburg Landing, Wallace's men had opened a route to Pittsburg Landing along Shunpike road, which connected to a road near Sherman's camp. Brig. Gen. W. H. L. Wallace's men at Pittsburg Landing opened the River Road (also known as the Hamburg-Savannah Road), a route farther east.
Of the two main routes that Wallace could use to move his men to the front, he chose the Shunpike road, the more direct route to reach Sherman's division near Shiloh Church. The day before the battle, Wallace wrote a letter to a fellow officer, W. H. L. Wallace, stating his intention to do so. Lew Wallace and his staff maintained after the battle that Grant's order did not specify Pittsburg Landing as their destination or indicated a specific route. However, Grant claimed in his memoirs that he had ordered Wallace to take the route nearest to the river to reach Pittsburg Landing. Historians are divided, with some stating that Wallace's explanation is the most logical.
After a second messenger from Grant arrived around noon with word to move out, Wallace's division of approximately 5,800 men began their march toward the battlefield. Between 2 and 2:30 p.m., a third messenger from Grant found Wallace along the Shunpike road, where he informed Wallace that Sherman had been forced back from Shiloh Church and was fighting closer to the river, near Pittsburg Landing. The Union army had been pushed back so far that Wallace was to the rear of the advancing Southern troops.
Wallace considered attacking the Confederates, but abandoned the idea. Instead he made a controversial decision to countermarch his troops along the Shunpike road, follow a crossroads to the River Road, and then move south to Pittsburg Landing. Rather than realigning his troops, so that the rear guard would be in the front, Wallace countermarched his column to maintain their original order, keeping his artillery in the lead position to support the Union infantry on the field After the time-consuming maneuver was completed, Wallace's troops returned to the midpoint on the Shunpike road, crossed east over a path to the River Road, and followed it south to join Grant's army on the field. Progress was slow due to the road conditions and countermarch. Wallace's division arrived at Pittsburg Landing about 6:30 p.m., after having marched about in nearly seven hours over roads that had been left in terrible conditions by recent rainstorms and previous Union marches. They gathered at the battlefield at dusk, about 7 p.m., with the fighting nearly over for the day, and took up a position on the right of the Union line.
The next day, April 7, Wallace's division held the extreme right of the Union line. Two of Wallace's batteries with the aid of a battery from the 1st Illinois Light Artillery were the first to attack at about 5:30 a.m. Sherman's and Wallace's troops helped force the Confederates to fall back, and by 3 p.m. the Confederates were retreating southwest, toward Corinth.
Shiloh controversy.
At first, the battle was viewed by the North as a victory; however, on April 23, after civilians began hearing news of the high number of casualties, the Lincoln administration asked the Union army for further explanation. Grant, who was accused of poor leadership at Shiloh, and his superior, Halleck, placed the blame on Wallace by asserting that his failure to follow orders and the delay in moving up the reserves on April 6 had nearly cost them the battle. On April 30, 1862, Halleck reorganized his army and removed Wallace and John McClernand from active duty, placing both of them in reserve.
Wallace's reputation and career as a military leader suffered a significant setback from controversy over Shiloh. He spent the remainder of his life trying to resolve the accusations and change public opinion about his role in the battle. On March 14, 1863, Wallace wrote a letter to Halleck that provided an official explanation of his actions. He also wrote Grant several letters and met with him in person more than once in an attempt to vindicate himself. On August 16, 1863, Wallace wrote Sherman for advice on the issue. Sherman urged Wallace to be patient and not to request a formal inquiry. Although Sherman brought Wallace's concerns to Grant's attention, Wallace was not given another active duty command until March 1864.
For many years Grant stood by his original version of the orders to Wallace. As late as 1884, when Grant wrote an article on Shiloh for "The Century Magazine" that appeared in its February 1885 issue, he maintained that Wallace had taken the wrong road on the first day of battle. After W. H. L. Wallace's widow gave Grant a letter that Lew Wallace had written to her the day before the battle (the one indicating his plans to use the Shunpike road to pass between Shiloh and his position west of Crump's Landing), Grant changed his mind. Grant wrote a letter to the editors at "Century", which was published in its September 1885 issue, and added a note to his memoirs to explain that Wallace's letter "modifies very materially what I have said, and what has been said by others, about the conduct of General Lew Wallace at the battle of Shiloh." While reaffirming that he had ordered Wallace to take the River Road, Grant stated that he could not be sure the exact content of Wallace's written orders, since his verbal orders were given to one of his aides and transcribed.
Grant's article in the February 1885 issue of "Century" became the basis of his chapter on Shiloh in his memoirs, which were published in 1886, and influenced many later accounts of Wallace's actions on the first day of battle. Grant acknowledged in his memoirs: "If the position of our front had not changed, the road which Wallace took would have been somewhat shorter to our right than the River road." Wallace's account of the events appeared in his autobiography, which was published posthumously in 1906. Despite his later fame and fortune as the novelist of "Ben-Hur", Wallace continued to lament, "Shiloh and its slanders! Will the world ever acquit me of them? If I were guilty I would not feel them as keenly."
Other military assignments.
On August 17, 1862, Wallace accepted a regiment command in the Department of the Ohio to help with the successful defense of Cincinnati during Braxton Bragg's incursion into Kentucky. Next, Wallace took command of Camp Chase, a prisoner-of-war camp at Columbus, Ohio, where he remained until October 30, 1862. A month later Wallace was placed in charge of a five-member commission to investigate Maj. Gen. Don Carlos Buell's conduct in response to the Confederate invasion of Kentucky. The commission criticized Buell for his retreat, but it did not find him disloyal to the Union. When the commission's work was completed on May 6, 1863, Wallace returned to Indiana to wait for a new command. In mid-July 1863, while Wallace was home, he helped protect the railroad junction at North Vernon, Indiana, from Confederate general John Hunt Morgan's raid into southern Indiana.
Monocacy.
Wallace's most notable service came on Saturday, July 9, 1864 at the Battle of Monocacy part of the Valley Campaigns of 1864. Although Confederate General Jubal A. Early and an estimated 15,000 troops defeated Wallace's troops at Monocacy Junction, Maryland, forcing them to retreat to Baltimore, the effort cost Early a chance to capture Washington, D.C. Wallace's men were able to delay the Confederate advance toward Washington for an entire day, giving the city time to organize its defenses. Early arrived in Washington at around noon on July 11, two days after defeating Wallace at Monocacy, the northernmost Confederate victory of the war, but Union reinforcements had already arrived at Fort Stevens to repel the Confederates and force their retreat to Virginia.
Wallace, who had returned to active duty on March 12, 1864, assumed command of VIII Corps, which was headquartered in Baltimore. On July 9, a combined Union force of approximately 5,800 men under Wallace's command (mostly hundred-days' men from VIII Corps) and a division under James B. Ricketts from VI Corps encountered Confederate troops at Monocacy Junction between 9 and 10 a.m. Although Wallace was uncertain whether Baltimore or Washington, D.C. was the Confederate objective, he knew his troops would have to delay the advance until Union reinforcements arrived. Wallace's men repelled the Confederate attacks for more than six hours before retreating to Baltimore.
After the battle Wallace informed Halleck that his forces fought until 5 p.m., but the Confederate troops, which he estimated at 20,000 men, had overwhelmed them. When Grant learned of the defeat, he named Maj. Gen. E. O. C. Ord as Wallace's replacement in command of VIII Corps. On July 28, after officials learned how Wallace's efforts at Monocacy helped save Washington D.C. from capture, he was reinstated as commander of VIII Corps. In Grant's memoirs, he praised Wallace's delaying tactics at Monocacy:
If Early had been but one day earlier, he might have entered the capital before the arrival of the reinforcements I had sent. ... General Wallace contributed on this occasion by the defeat of the troops under him, a greater benefit to the cause than often falls to the lot of a commander of an equal force to render by means of a victory.
Later military service.
On January 22, 1865 Grant ordered Wallace to the Rio Grande in southern Texas to investigate Confederate military operations in the area. Although Wallace was not officially authorized to offer terms, he did discuss proposals for the surrender of the Confederate troops in the Trans-Mississippi Department. Wallace provided Grant with copies of his proposals and reported on the negotiations, but no agreement was made. Before returning to Baltimore, Wallace also met with Mexican military leaders to discuss the U.S. government's unofficial efforts to aid in expelling Maximilian's French occupation forces from Mexico.
Following President Lincoln's death on April 15, 1865, Wallace was appointed to the military commission that investigated the Lincoln assassination conspirators. The commission, which began in May, was dissolved on June 30, 1865, after all eight conspirators were found guilty. In mid-August 1865, Wallace was appointed head of an eight-member military commission that investigated the conduct of Henry Wirz, the Confederate commandant in charge of the South's Andersonville prison camp. The court-martial which took nearly two months, opened on August 21, 1865. At its conclusion Wirz was found guilty and sentenced to death.
On April 30, 1865, Wallace had accepted an offer to become a major general in the Mexican army, but the agreement, which was contingent upon his resignation from the U.S. Army, was delayed by Wallace's service on the two military commissions. Wallace tendered his resignation from the U.S. Army on November 4, 1865, effective November 30, and returned to Mexico to assist the Mexican army. Although the Juárez government promised Wallace $100,000 for his services, he returned to the United States in 1867 in deep financial debt.
Political and diplomatic career.
Wallace returned to Indiana in 1867 to practice law, but the profession did not appeal to him, and he turned to politics. Wallace made two unsuccessful bids for a seat in Congress (in 1868 and 1870), and supported Republican presidential candidate Rutherford B. Hayes in the 1876 election. As a reward for his political support, Hayes appointed Wallace as governor of the New Mexico Territory, where he served from August 1878 to March 1881. His next assignment came in March 1881, when Republican president James A. Garfield appointed Wallace to an overseas diplomatic post in Constantinople, Turkey, as U.S. Minister to the Ottoman Empire. Wallace remained in this post until 1885.
Territorial governor of New Mexico.
Wallace arrived in Santa Fe, on September 29, 1878, to begin his service as governor of the New Mexico Territory during a time of lawless violence and political corruption. Wallace was involved in efforts to resolve New Mexico's Lincoln County War, a contentious and violent disagreement among the county’s residents, and tried to end a series of Apache raids on territorial settlers. In 1880, while living at the Palace of the Governors in Santa Fe, Wallace also completed the manuscript for "Ben Hur".
On March 1, 1879, after previous efforts to restore order in Lincoln County had failed, Wallace ordered the arrest of those responsible for local killings. One of the outlaws was William Henry McCarty, Jr. (alias William H. Bonney), better known as Billy the Kid. On March 17, 1879, Wallace secretly met with the Kid, who had witnessed the murder of a Lincoln County lawyer named Chapman. Wallace wanted the Kid to testify in the trial of Chapman's accused murderers, but the Kid had killed others and wanted Wallace's protection from the outlaw gang and amnesty for his crimes. During their meeting, the pair arranged for the Kid to become an informant in exchange for a full pardon of his previous crimes. Wallace supposedly assured the Kid that he would be "scot free with a pardon in your pocket for all your misdeeds." On March 20, the Kid agreed to testify against others involved in Chapman's murder. Wallace arranged for the Kid's arrest and detention in a local jail to assure his safety. After the Kid testified in court on April 14, the local district attorney revoked Wallace's bargain and refused to set the outlaw free. The Kid escaped from jail and returned to his criminal ways, which included killing additional men. The Kid was shot and killed on July 14, 1881 by Sheriff Pat Garrett, who had been appointed by local ranching interests who had tired of his rustling their herds. In the meantime, Wallace had resigned from his duties as territorial governor on March 9, 1881, and was waiting for a new political appointment.
On December 31, 2010, on his last day in office, then-Governor Bill Richardson of New Mexico declined a pardon request from supporters of the Kid, citing a "lack of conclusiveness and the historical ambiguity" over Wallace's promise of amnesty. Descendants of Wallace and Garrett were among those who opposed the pardon.
U.S. diplomat in Turkey.
On May 19, 1881, Wallace was appointed U.S. Minister to the Ottoman Empire in Constantinople (present-day Istanbul), Turkey. Wallace remained at the diplomatic post until 1885, and became a trusted friend of Sultan Abdul Hamid II. When a crisis developed between the Turkish and British governments over control of Egypt, Wallace served as an intermediary between the sultan and Lord Dufferin, the British ambassador. Although Wallace's efforts were unsuccessful, he earned respect for his efforts and a promotion in the U.S. diplomatic service.
In 1883, an editorial aimed at Wallace appeared in Havatzelet (xiii. No. 6) titled "An American and yet a Despot". The editorial caused the Havatzelet to be suspended and its editor imprisoned for forty-five days by order from Constantinople directed to the pasha of Jerusalem. The incident that lead to the editorial was the dismissal, made at Wallace's request, of Joseph Kriger, the Jewish secretary and interpreter to the pasha of Jerusalem. Wallace complained that Kriger had failed to receive him with the honor due to his rank, and refused to issue any apology for the alleged shortcoming. Havatzelet claimed that the proceeding was instigated by missionaries, whom Wallace strongly supported.
In addition to Wallace's diplomatic duties, which included protection of U.S. citizens and U.S. trade rights in the area, Wallace found time to travel and do historical research. Wallace visited Jerusalem and the surrounding area, the site for his novel, "Ben-Hur", and did research in Constantinople, the locale for "The Prince of India; or, Why Constantinople Fell", which he began writing in 1887.
The election of Grover Cleveland, the Democratic candidate for president, ended Wallace's political appointment. He resigned from the U.S. diplomatic service on March 4, 1885. The sultan wanted Wallace to continue to work in Turkey, and even made a proposal to have him represent Turkish interests in England or France, but Wallace declined and returned home to Crawfordsville.
Writing career.
Wallace confessed in his autobiography that he took up writing as a diversion from studying law. Although he wrote several books, Wallace is best known for his historical adventure story, "" (1880), which established his fame as an author.
In 1843 Wallace began writing his first novel, "The Fair God", but it was not published until 1873. The popular historical novel, with Cortez's conquest of Mexico as its central theme, was based on William H. Prescott's "History of the Conquest of Mexico". Wallace's book sold seven thousand copies in its first year. Its sales continued to rise after Wallace's reputation as an author was established with the publication of subsequent novels.
Wallace wrote the manuscript for "Ben-Hur", his second and best-known novel, during his spare time at Crawfordsville, and completed it in Santa Fe, while serving as the territorial governor of New Mexico. "Ben-Hur", an adventure story of revenge and redemption, is told from the perspective of a Jewish nobleman named Judah Ben-Hur. Because Wallace had not been to the Holy Land before writing the book, he began research to familiarize himself with the area's geography and its history at the Library of Congress in Washington, D.C. in 1873. Harper and Brothers published the book on November 12, 1880.
"Ben-Hur" made Wallace a wealthy man and established his reputation as a famous author. Sales were slow at first, only 2,800 copies were sold in the first seven months after its release, but the book became popular among readers around the world By 1886 it was earning Wallace about $11,000 in annual royalties (equivalent to $290,000 in 2015 dollars), and provided Wallace’s family with financial security. By 1889 Harper and Brothers had sold 400,000 copies and the book had been translated into several languages.
In 1900 "Ben-Hur" became the best-selling American novel of the 19th century, surpassing Harriet Beecher Stowe's "Uncle Tom's Cabin". Amy Lifson, an editor for "Humanities", identified it as the most influential Christian book of the 19th century. Others named it one of the best-selling novels of all time. At the time of "Ben-Hur'"s one hundredth anniversary in 1980, it had "never been out of print" and had been adapted for the stage and several motion pictures. One historian, Victor Davis Hanson, has argued that "Ben-Hur" drew from Wallace's life, particularly his experiences at Shiloh, and the damage it did to his reputation. The book's main character, Judah Ben-Hur, accidentally causes injury to a high-ranking Roman commander, for which he and his family suffer tribulations and calumny.
Wallace wrote subsequent novels and biographies, but "Ben-Hur" remained his most important work. Wallace considered "The Prince of India; or, Why Constantinople Fell" (1893) as his best novel. He also wrote a biography of President Benjamin Harrison, a fellow Hoosier and Civil War general, and "The Wooing of Malkatoon" (1898), a narrative poem. Wallace was writing his autobiography when he died in 1905. His wife Susan completed it with the assistance of Mary Hannah Krout, another author from Crawfordsville. It was published posthumously in 1906.
Later years.
Wallace continued to write after his return from Turkey. He also patented several of his own inventions, built a seven-story apartment building in Indianapolis, and drew up plans for a private study at his home in Crawfordsville. Wallace remained active in veterans groups, including writing a speech for the dedication of the battlefield at the Chickamauga.
Wallace's elaborate writing study, which he described as "a pleasure-house for my soul", served as his private retreat. Now called the General Lew Wallace Study and Museum, it was built between 1895 and 1898, adjacent to his residence in Crawfordsville, and set in an enclosed park. The study along with three and one-half acres of its grounds were designated a National Historic Landmark in 1976. The property is operated as a museum, open to the public.
On April 5, 1898, at the outbreak of the Spanish–American War, Wallace, at age seventy-one, offered to raise and lead a force of soldiers, but the war office refused. Undeterred, he went to a local recruiting office and attempted to enlist as a private, but was rejected again, presumably because of his age.
Wallace's service at the battle of Shiloh continued to haunt him in later life. The debate persisted in book publications, magazine articles, pamphlets, speeches, and in private correspondence. Wallace attended a reunion at Shiloh in 1894, his first return since 1862, and retraced his journey to the battlefield with veterans from the 3rd Division. He returned to Shiloh for a final time in 1901 to walk the battlefield with David W. Reed, the Shiloh Battlefield Commission's historian, and others. Wallace died before the manuscript of his memoirs was fully completed, so it is unknown whether he would have revised his final account of the battle.
Death.
Wallace died at home in Crawfordsville, on February 15, 1905, of atrophic gastritis. He was seventy-seven years old. Wallace is buried in Crawfordsville Oak Hill Cemetery.
Legacy and honors.
Wallace was a man of many interests and a lifelong adventure seeker, who remained a persistent, self-confident man of action. He was also impatient and highly sensitive to personal criticisms, especially those related to his command decisions at Shiloh. Despite Wallace's career in law and politics, combined with years of military and diplomatic service, he achieved his greatest fame as a novelist, most notably for a best-selling biblical tale, "Ben-Hur".
Following Wallace's death, the State of Indiana commissioned sculptor Andrew O'Connor to create a marble statue of Wallace dressed in a military uniform for the National Statuary Hall Collection in the U.S. Capitol. The statue was unveiled during a ceremony held on January 11, 1910. Wallace is the only novelist honored in the hall. A bronze copy of the statue is installed on the grounds of Wallace's study in Crawfordsville.
Lew Wallace High School opened in 1926 at 415 West 45th Avenue in Gary, Indiana. On June 3, 2014, the Gary School Board voted 4 to 2 to close Lew Wallace, along with five other schools.

</doc>
<doc id="44088" url="https://en.wikipedia.org/wiki?curid=44088" title="Vittorio Gassman">
Vittorio Gassman

Vittorio Gassman, Knight Grand Cross, OMRI (; born Vittorio Gassmann; 1 September 1922 – 29 June 2000), popularly known as Il Mattatore, was an Italian theatre and film actor, as well as director.
He is considered one of the greatest Italian actors and is commonly recalled as an extremely professional, versatile, magnetic interpreter, whose long career includes both important productions as well as dozens of "divertissements" (which made him greatly popular).
Biography.
Early life.
He was born in Genoa to a German father, Heinrich Gassmann, and a Pisan Jewish mother, Luisa Ambron. While still very young he moved to Rome, where he studied at the Accademia Nazionale d'Arte Drammatica.
Career.
Gassman's debut was in Milan, in 1942, with Alda Borelli in Niccodemi's "Nemica" (theatre). He then moved to Rome and acted at the "Teatro Eliseo" joining Tino Carraro and Ernesto Calindri in a team that remained famous for some time; with them he acted in a range of plays from bourgeois comedy to sophisticated intellectual theatre. In 1946, he made his film debut in "Preludio d'amore", while only one year later he appeared in five films. In 1948 he played in "Riso amaro".
It was with Luchino Visconti's company that Gassman achieved his mature successes, together with Paolo Stoppa, Rina Morelli and Paola Borboni. He played Stanley Kowalski in Tennessee Williams' "Un tram che si chiama desiderio" ("A Streetcar Named Desire"), as well as in "As You Like It" (by Shakespeare) and "Oreste" (by Vittorio Alfieri). He joined the "Teatro Nazionale" with Tommaso Salvini, Massimo Girotti, Arnoldo Foà to create a successful "Peer Gynt" (by Henrik Ibsen). With Luigi Squarzina in 1952 he co-founded and co-directed the "Teatro d'Arte Italiano", producing the first complete version of "Hamlet" in Italy, followed by rare works such as Seneca's "Thyestes" and Aeschylus's "The Persians".
In 1956 Gassman played the title role in a production of "Othello". He was so well received by his acting in the television series entitled "Il Mattatore" ("Spotlight Chaser") that "Il Mattatore" became the nickname that accompanied him for the rest of his life. Gassman's debut in the commedia all'italiana genre was rather accidental, in Mario Monicelli's "I soliti ignoti" ("Big Deal on Madonna Street", 1958). Famous movies featuring Gassman include: "Il sorpasso" (1962), "La Grande Guerra" (1962), "I mostri" (1963), "L'Armata Brancaleone" (1966), "Profumo di donna" (1974) and "C'eravamo tanto amati" (1974).
He directed "Adelchi", a lesser-known work by Alessandro Manzoni. Gassman brought this production to half a million spectators, crossing Italy with his "Teatro Popolare Itinerante" (a newer edition of the famous "Carro di Tespi"). His productions have included many of the famous authors and playwrights of the 20th century, with repeated returns to the classics of Shakespeare, Dostoyevsky and the Greek tragicians. He also founded a theatre school in Florence (Bottega Teatrale di Firenze), which educated many of the more talented actors of the current generation of Italian thespians.
In cinema, he worked frequently both in Italy and abroad. He met and fell in love with American actress Shelley Winters while she was touring Europe with fiancé Farley Granger. When Winters was forced to return to Hollywood to fulfill contractual obligations, he followed her there and married her. With his natural charisma and his fluency in English he scored a number of roles in Hollywood, including "Rhapsody" with Elizabeth Taylor and "The Glass Wall" before returning to Italy and the theatre. While rehearsing "Hamlet", he began an affair with Anna Maria Ferrero, his 16-year-old Ophelia, which ended his marriage to Winters. He and Winters were forced to work together on "Mambo" just as their marriage was unraveling, providing fodder for tabloids all over the world. He later voiced Mufasa in the Italian version of "The Lion King".
Personal life.
Gassman married three actresses: Nora Ricci (with whom he had Paola, an actress and wife of Ugo Pagliai); Shelley Winters (mother of his daughter Vittoria); and Diletta D'Andrea, by whom he had a son, Jacopo. In addition, he had an affair with actress Juliette Mayniel (mother of his son Alessandro, also an actor). In the 1990s he took part in the popular TV show "Tunnel" in which he very formally and "seriously"' recited documents such as utility bills, yellow pages and similar trivial texts, such as washing instructions for a wool sweater or cookies ingredients. He rendered them with the same professional skill that made him famous while reciting Dante's "Divine Comedy".
On 29 June 2000, Gassman died of a heart attack at his home in Rome, aged 77.

</doc>
