<doc id="41616" url="https://en.wikipedia.org/wiki?curid=41616" title="Queuing delay">
Queuing delay

In telecommunication and computer engineering, the queuing delay or queueing delay is the time a job waits in a queue until it can be executed. It is a key component of network delay. In a switched network, the time between the completion of signaling by the call originator and the arrival of a ringing signal at the call receiver. Queues may be caused by delays at the originating switch, intermediate switches, or the call receiver servicing switch. In a data network, the sum of the delays between the request for service and the establishment of a circuit to the called data terminal equipment (DTE). In a packet-switched network, the sum of the delays encountered by a packet between the time of insertion into the network and the time of delivery to the addressee. 
This term is most often used in reference to routers. When packets arrive at a router, they have to be processed and transmitted. A router can only process one packet at a time. If packets arrive faster than the router can process them (such as in a burst transmission) the router puts them into the queue (also called the buffer) until it can get around to transmitting them. Delay can also vary from packet to packet so averages and statistics are usually generated when measuring and evaluating queuing delay. 
As a queue begins to fill up due to traffic arriving faster than it can be processed, the amount of delay a packet experiences going through the queue increases. The speed at which the contents of a queue can be processed is a function of the transmission rate of the facility. This leads to the classic delay curve. The average delay any given packet is likely to experience is given by the formula 1/(μ-λ) where μ is the number of packets per second the facility can sustain and λ is the average rate at which packets are arriving to be serviced. This formula can be used when no packets are dropped from the queue. 
The maximum queuing delay is proportional to buffer size. The longer the line of packets waiting to be transmitted, the longer the average waiting time is. The router queue of packets waiting to be sent also introduces a potential cause of packet loss. Since the router has a finite amount of buffer memory to hold the queue, a router which receives packets at too high a rate may experience a full queue. In this case, the router has no other option than to simply discard excess packets.
When the transmission protocol uses the dropped-packets symptom of filled buffers to regulate its transmit rate, as the Internet's TCP does, bandwidth is fairly shared at near theoretical capacity with minimal network congestion delays. Absent this feedback mechanism the delays become both unpredictable and rise sharply, a symptom also seen as freeways approach capacity; metered onramps are the most effective solution there, just as TCP's self-regulation is the most effective solution when the traffic is packets instead of cars). This result is both hard to model mathematically and quite counterintuitive to people who lack experience with mathematics or real networks. Failing to drop packets, choosing instead to buffer an ever-increasing number of them, produces bufferbloat.
In Kendall's notation, the M/M/1/K queuing model, where K is the size of the buffer, may be used to analyze the queuing delay in a specific system. Kendall's notation should be used to calculate the queuing delay when packets are dropped from the queue. The M/M/1/K queuing model is the most basic and important queuing model for network analysis.

</doc>
<doc id="41618" url="https://en.wikipedia.org/wiki?curid=41618" title="Radiation angle">
Radiation angle

In fiber optics, the radiation angle is half the vertex angle of the cone of light emitted at the exit face of an optical fiber. 
The cone boundary is usually defined (a) by the angle at which the far-field irradiance has decreased to a specified fraction of its maximum value or (b) as the cone within which there is a specified fraction of the total radiated power at any point in the far field.

</doc>
<doc id="41619" url="https://en.wikipedia.org/wiki?curid=41619" title="Radiation mode">
Radiation mode

For an optical fiber or waveguide, a radiation mode or unbound mode is a mode which is not confined by the fiber core. Such a mode has fields that are transversely oscillatory everywhere external to the waveguide, and exists even at the limit of zero wavelength.
Specifically, a radiation mode is one for which 
where "β" is the imaginary part of the axial propagation constant, integer "l" is the azimuthal index of the mode, "n"("r") is the refractive index at radius "r", "a" is the core radius, and "k" is the free-space wave number, "k" = 2π/"λ", where "λ" is the wavelength. Radiation modes correspond to refracted rays in the terminology of geometric optics.

</doc>
<doc id="41620" url="https://en.wikipedia.org/wiki?curid=41620" title="Radiation pattern">
Radiation pattern

In the field of antenna design the term radiation pattern (or antenna pattern or far-field pattern) refers to the "directional" (angular) dependence of the strength of the radio waves from the antenna or other source.
Particularly in the fields of fiber optics, lasers, and integrated optics, the term radiation pattern may also be used as a synonym for the near-field pattern or Fresnel pattern. This refers to the "positional" dependence of the electromagnetic field in the near-field, or Fresnel region of the source. The near-field pattern is most commonly defined over a plane placed in front of the source, or over a cylindrical or spherical surface enclosing it.
The far-field pattern of an antenna may be determined experimentally at an antenna range, or alternatively, the near-field pattern may be found using a near-field scanner, and the radiation pattern deduced from it by computation. The far-field radiation pattern can also be calculated from the antenna shape by computer programs such as NEC. Other software, like HFSS can also compute the near field.
The far field radiation pattern may be represented graphically as a plot of one of a number of related variables, including; the field strength at a constant (large) radius (an amplitude pattern or field pattern), the power per unit solid angle (power pattern) and the directive gain. Very often, only the relative amplitude is plotted, normalized either to the amplitude on the antenna boresight, or to the total radiated power. The plotted quantity may be shown on a linear scale, or in dB. The plot is typically represented as a three-dimensional graph (as at right), or as separate graphs in the vertical plane and horizontal plane. This is often known as a polar diagram.
Reciprocity.
It is a fundamental property of antennas that the receiving pattern (sensitivity as a function of direction) of an antenna when used for receiving is identical to the far-field radiation pattern of the antenna when used for transmitting. This is a consequence of the reciprocity theorem of electro-magnetics and is proved below. Therefore in discussions of radiation patterns the antenna can be viewed as either transmitting or receiving, whichever is more convenient.
Typical patterns.
Since electromagnetic radiation is dipole radiation, it is not possible to build an antenna that radiates coherently equally in all directions, although such a hypothetical isotropic antenna is used as a reference to calculate antenna gain.
The simplest antennas, monopole and dipole antennas, consist of one or two straight metal rods along a common axis. These axially symmetric antennas have radiation patterns with a similar symmetry, called omnidirectional patterns; they radiate equal power in all directions perpendicular to the antenna, with the power varying only with the angle to the axis, dropping off to zero on the antenna's axis. This illustrates the general principle that if the shape of an antenna is symmetrical, its radiation pattern will have the same symmetry.
In most antennas, the radiation from the different parts of the antenna interferes at some angles. This results in zero radiation at certain angles where the radio waves from the different parts arrive out of phase, and local maxima of radiation at other angles where the radio waves arrive in phase. Therefore the radiation plot of most antennas shows a pattern of maxima called "lobes" at various angles, separated by ""nulls"" at which the radiation goes to zero. 
The larger the antenna is compared to a wavelength, the more lobes there will be. In a directive antenna in which the objective is to direct the radio waves in one particular direction, the lobe in that direction is larger than the others; this is called the ""main lobe"". The axis of maximum radiation, passing through the center of the main lobe, is called the ""beam axis"" or "boresight axis"". In some antennas, such as split-beam antennas, there may exist more than one major lobe. A minor lobe is any lobe except a major lobe.
The other lobes, representing unwanted radiation in other directions, are called ""side lobes"". The side lobe in the opposite direction (180°) from the main lobe is called the ""back lobe"". Usually it refers to a minor lobe that
occupies the hemisphere in a direction opposite to that of the major (main) lobe.
Minor lobes usually represent radiation in undesired directions, and they should be minimized. Side lobes are normally the largest of the minor lobes. The level of minor lobes is usually expressed as a ratio of the power density in the lobe in question to that of the major lobe. This ratio is often termed the side lobe ratio or side lobe level. Side lobe levels of −20 dB or smaller are usually not desirable in many applications. Attainment of a side lobe level smaller than −30 dB usually requires very careful design and construction. In most radar systems, for example, low side lobe ratios are very important to minimize false target indications through the side lobes.
Proof of reciprocity.
For a complete proof, see the reciprocity (electromagnetism) article. Here, we present a common simple proof limited to the approximation of two antennas separated by a large distance compared to the size of the antenna, in a homogeneous medium. The first antenna is the test antenna whose patterns are to be investigated; this antenna is free to point in any direction. The second antenna is a reference antenna, which points rigidly at the first antenna.
Each antenna is alternately connected to a transmitter having a particular source impedance, and a receiver having the same input impedance (the impedance may differ between the two antennas).
It is assumed that the two antennas are sufficiently far apart that the properties of the transmitting antenna are not affected by the load placed upon it by the receiving antenna. Consequently, the amount of power transferred from the transmitter to the receiver can be expressed as the product of two independent factors; one depending on the directional properties of the transmitting antenna, and the other depending on the directional properties of the receiving antenna.
For the transmitting antenna, by the definition of gain, formula_1, the radiation power density at a distance formula_2 from the antenna (i.e. the power passing through unit area) is
Here, the arguments formula_4 and formula_5 indicate a dependence on direction from the antenna, and formula_6 stands for the power the transmitter would deliver into a matched load. The gain formula_1 may be broken down into three factors; the antenna gain (the directional redistribution of the power), the radiation efficiency (accounting for ohmic losses in the antenna), and lastly the loss due to mismatch between the antenna and transmitter. Strictly, to include the mismatch, it should be called the realized gain, but this is not common usage.
For the receiving antenna, the power delivered to the receiver is
Here formula_9 is the power density of the incident radiation, and formula_10 is the antenna aperture or effective area of the antenna (the area the antenna would need to occupy in order to intercept the observed captured power). The directional arguments are now relative to the receiving antenna, and again formula_10 is taken to include ohmic and mismatch losses.
Putting these expressions together, the power transferred from transmitter to receiver is
where formula_1 and formula_10 are directionally dependent properties of the transmitting and receiving antennas respectively. For transmission from the reference
antenna (2), to the test antenna (1), that is
and for transmission in the opposite direction
Here, the gain formula_17 and effective area formula_18 of antenna 2 are fixed, because the orientation of this antenna is fixed with respect to the first.
Now for a given disposition of the antennas, the reciprocity theorem requires that the power transfer is equally effective in each direction, i.e.
whence
But the right hand side of this equation is fixed (because the orientation of antenna 2 is fixed), and so
i.e. the directional dependence of the (receiving) effective aperture and the (transmitting) gain are identical (QED). Furthermore, the constant of proportionality is the same irrespective of the nature of the antenna, and so must be the same for all antennas. Analysis of a particular antenna (such as a Hertzian dipole), shows that this constant is formula_22, where formula_23 is the free-space wavelength. Hence, for any antenna the gain and the effective aperture are related by
Even for a receiving antenna, it is more usual to state the gain than to specify the effective aperture. The power delivered to the receiver is therefore more usually written as
(see link budget). The effective aperture is however of interest for comparison with the actual physical size of the antenna.

</doc>
<doc id="41622" url="https://en.wikipedia.org/wiki?curid=41622" title="Radio equipment">
Radio equipment

Radio equipment, as defined in "Federal Information Management Regulations", is any equipment or interconnected system or subsystem of equipment (both transmission and reception) that is used to communicate over a distance by modulating and radiating electromagnetic waves in space without artificial guide. This does not include such items as microwave, satellite, or cellular telephone equipment.

</doc>
<doc id="41623" url="https://en.wikipedia.org/wiki?curid=41623" title="Radio fix">
Radio fix

In telecommunication and position fixing, the term radio fix has the following meanings:
Compare triangulation.
Obtaining a radio fix.
A single transmitter can be used to give a line of position (LOP) of the craft. The (true) bearing to the station from the craft, TB or QUJ, is composed of the true heading, TH, plus the relative bearing, RB, of the station. The bearing from the station (QTE) is found by adding 180° to the QUJ figure.
The line of position is then the line of bearing QUJ (i.e. from the station to the receiver) passing through the station.
For the diagram on the right, we have:
A radio fix on two stations can be found in exactly the same way. The intersection of the two position lines gives the position of the receiver. For the diagram on the right, the LOPs are found as before:
Remembering that the LOPs pass through their respective stations, it is now simple to find the location of the craft.
Remember too, that bearings and direction are given/recorded with respect to True North and to Magnetic North. Values used by mobile stations usually need to be converted from Magnetic to True. (Fixed stations are expected to use True).

</doc>
<doc id="41625" url="https://en.wikipedia.org/wiki?curid=41625" title="Radiometry">
Radiometry

Radiometry is a set of techniques for measuring electromagnetic radiation, including visible light. Radiometric techniques in optics characterize the distribution of the radiation's power in space, as opposed to photometric techniques, which characterize the light's interaction with the human eye. Radiometry is distinct from quantum techniques such as photon counting.
The use of radiometers to determine the temperature of objects and gasses by measuring radiation flux is called pyrometry. Handheld pyrometer devices are often marketed as infrared thermometers.
Radiometry is important in astronomy, especially radio astronomy, and plays a significant role in Earth remote sensing. The measurement techniques categorized as "radiometry" in optics are called "photometry" in some astronomical applications, contrary to the optics usage of the term.
Spectroradiometry is the measurement of absolute radiometric quantities in narrow bands of wavelength.
Integral and spectral radiometric quantities.
Integral quantities (like radiant flux) describe the total effect of radiation of all wavelengths or frequencies, while spectral quantities (like spectral power) describe the effect of radiation of a single wavelength "λ" or frequency "ν". To each integral quantity there are corresponding spectral quantities, for example the radiant flux Φe corresponds to the spectral power Φe,"λ" and Φe,"ν".
Getting an integral quantity's spectral counterpart requires a limit transition. This comes from the idea that the precisely requested wavelength photon existence probability is zero. Let us show the relation between them using the radiant flux as an example:
Integral flux, whose unit is W:
Spectral flux by wavelength, whose unit is :
where formula_3 is the radiant flux of the radiation in a small wavelength interval ["λ", "λ" + d"λ"].
The area under a plot with wavelength horizontal axis equals to the total radiant flux.
Spectral flux by frequency, whose unit is :
where formula_3 is the radiant flux of the radiation in a small frequency interval ["ν", "ν" + d"ν"].
The area under a plot with frequency horizontal axis equals to the total radiant flux.
Spectral flux multiplied by wavelength or frequency, whose unit is W, i.e. the same as the integral quantity:
The area under a plot with logarithmic wavelength or frequency horizontal axis equals to the total radiant flux.
The spectral quantities by wavelength "λ" and frequency "ν" are related by equations featuring the speed of light "c":
The integral quantity can be obtained by the spectral quantity's integration:

</doc>
<doc id="41627" url="https://en.wikipedia.org/wiki?curid=41627" title="Random number">
Random number

Random number may refer to:

</doc>
<doc id="41628" url="https://en.wikipedia.org/wiki?curid=41628" title="Receive-after-transmit time delay">
Receive-after-transmit time delay

In telecommunication, receive-after-transmit time delay is the time interval between (a) the instant of keying off the local transmitter to stop transmitting and (b) the instant the local receiver output has increased to 90% of its steady-state value in response to an rf signal from a distant transmitter. 
The rf signal from the distant transmitter must exist at the local receiver input prior to, or at the time of, keying off the local transmitter. 
Receive-after-transmit time delay applies only to half-duplex operation.

</doc>
<doc id="41629" url="https://en.wikipedia.org/wiki?curid=41629" title="Received noise power">
Received noise power

In telecommunications, received noise power is a measure of noise in a receiver. For example, the received noise power might be:

</doc>
<doc id="41630" url="https://en.wikipedia.org/wiki?curid=41630" title="Attack-time delay">
Attack-time delay

In telecommunications, attack-time delay is the time needed for a receiver or transmitter to respond to an incoming signal.
For a receiver, the attack-time delay is defined as the time interval from the instant a step radio-frequency (RF) signal, at a level equal to the receiver's threshold of sensitivity, is applied to the receiver input, to the instant when the receiver's output amplitude reaches 90% of its steady-state value. If a squelch circuit is operating, the receiver attack-time delay includes the time for the receiver to break squelch. 
For a transmitter, the attack-time delay is defined as the interval from the instant the transmitter is keyed-on to the instant the transmitted RF signal amplitude has increased to a specified level, usually 90% of its key-on steady-state value. The transmitter attack-time delay excludes the time required for automatic antenna tuning.

</doc>
<doc id="41633" url="https://en.wikipedia.org/wiki?curid=41633" title="Recorder warning tone">
Recorder warning tone

In telecommunication, a recorder warning tone is a tone transmitted over a telephone line to indicate to the called party that the calling party is recording the conversation.
In the United States, the recorder warning tone is a half-second burst of 1400 Hz applied every 15 seconds. The recorder warning tone is required by law to be generated as an integral part of any recording device used for the purpose and is required to be not under the control of the calling party. The tone is recorded together with the conversation.

</doc>
<doc id="41635" url="https://en.wikipedia.org/wiki?curid=41635" title="Recovery procedure">
Recovery procedure

In telecommunication, a recovery procedure is a process that attempts to bring a system back to a normal operating state. Examples:

</doc>
<doc id="41636" url="https://en.wikipedia.org/wiki?curid=41636" title="Reference circuit">
Reference circuit

A reference circuit is a hypothetical electric circuit of specified equivalent length and configuration, and having a defined transmission characteristic or characteristics, used primarily as a reference for measuring the performance of other, "i.e.," real, circuits or as a guide for planning and engineering of circuits and networks. 
Normally, several types of reference circuits are defined, with different configurations, because communications are required over a wide range of distances. Another type of reference circuit shows how to configure integrated circuits into function blocks, which Analog Devices provides for electrical design engineers. Analog Devices' Circuits from the Lab reference circuits are fully tested and come with the schematics, evaluation boards, and device drivers necessary for system integration. A group of related reference circuits is also called a "reference system".

</doc>
<doc id="41637" url="https://en.wikipedia.org/wiki?curid=41637" title="Reference clock">
Reference clock

A reference clock may refer to the following:

</doc>
<doc id="41638" url="https://en.wikipedia.org/wiki?curid=41638" title="Cycloid">
Cycloid

A cycloid is the curve traced by a point on the rim of a circular wheel as the wheel rolls along a straight line without slippage.
It is an example of a roulette, a curve generated by a curve rolling on another curve.
The inverted cycloid (a cycloid rotated through 180°) is the solution to the brachistochrone problem (i.e., it is the curve of fastest descent under gravity) and the related tautochrone problem (i.e., the period of an object in descent without friction inside this curve does not depend on the object's starting position).
History.
The cycloid has been called "The Helen of Geometers" as it caused frequent quarrels among 17th-century mathematicians.
Historians of mathematics have proposed several candidates for the discoverer of the cycloid. Mathematical historian Paul Tannery cited similar work by the Syrian philosopher Iamblichus as evidence that the curve was likely known in antiquity. English mathematician John Wallis writing in 1679 attributed the discovery to Nicholas of Cusa, but subsequent scholarship indicates Wallis was either mistaken or the evidence used by Wallis is now lost. Galileo Galilei's name was put forward at the end of the 19th century and at least one author reports credit being given to Marin Mersenne. Beginning with the work of Moritz Cantor and Siegmund Günther, scholars now assign priority to French mathematician Charles de Bovelles based on his description of the cycloid in his "Introductio in geometriam", published in 1503. In this work, Bovelles mistakes the arch traced by a rolling wheel as part of a larger circle with a radius 120% larger than the smaller wheel.
Galileo originated the term "cycloid" and was the first to make a serious study of the curve. According to his student Evangelista Torricelli, in 1599 Galileo attempted the quadrature of the cycloid (constructing a square with area equal to the area under the cycloid) with an unusually empirical approach that involved tracing both the generating circle and the resulting cycloid on sheet metal, cutting them out and weighing them. He discovered the ratio was roughly 3:1 but incorrectly concluded the ratio was an irrational fraction, which would have made quadrature impossible. Around 1628, Gilles Persone de Roberval likely learned of the quadrature problem from Père Marin Mersenne and effected the quadrature in 1634 by using Cavalieri's Theorem. However, this work was not published until 1693 (in his "Traité des Indivisibles").
Constructing the tangent of the cycloid dates to August 1638 when Mersenne received unique methods from Roberval, Pierre de Fermat and René Descartes. Mersenne passed these results along to Galileo, who gave them to his students Torricelli and Viviana, who were able to produce a quadrature. This result and others were published by Torricelli in 1644, which is also the first printed work on the cycloid. This led to Roberval charging Torricelli with plagiarism, with the controversy cut short by Torricelli's early death in 1647.
In 1658, Blaise Pascal had given up mathematics for theology but, while suffering from a toothache, began considering several problems concerning the cycloid. His toothache disappeared, and he took this as a heavenly sign to proceed with his research. Eight days later he had completed his essay and, to publicize the results, proposed a contest. Pascal proposed three questions relating to the center of gravity, area and volume of the cycloid, with the winner or winners to receive prizes of 20 and 40 Spanish doubloons. Pascal, Roberval and Senator Carcavy were the judges, and neither of the two submissions (by John Wallis and Antoine Lalouvère) were judged to be adequate. While the contest was ongoing, Christopher Wren sent Pascal a proposal for a proof of the rectification of the cycloid; Roberval claimed promptly that he had known of the proof for years. Wallis published Wren's proof (crediting Wren) in Wallis's "Tractus Duo", giving Wren priority for the first published proof.
Fifteen years later, Christiaan Huygens had deployed the cycloidal pendulum to improve chronometers and had discovered that a particle would traverse an inverted cycloidal arch in the same amount of time, regardless of its starting point. In 1686, Gottfried Wilhelm Leibniz used analytic geometry to describe the curve with a single equation. In 1696, Johann Bernoulli posed the brachistochrone problem, the solution of which is a cycloid.
Equations.
The cycloid through the origin, generated by a circle of radius "r", consists of the points ("x", "y"), with
where "t" is a real parameter, corresponding to the angle through which the rolling circle has rotated, measured in radians. For given "t", the circle's centre lies at "x" = "rt", "y" = "r".
Solving for "t" and replacing, the Cartesian equation is found to be:
An expression of the equation in the form "y" = "f"("x") is not possible using standard functions.
The first arch of the cycloid consists of points such that
When "y" is viewed as a function of "x", the cycloid is differentiable everywhere except at the cusps where it hits the "x"-axis, with the derivative tending toward formula_4 or formula_5 as one approaches a cusp. The map from "t" to ("x", "y") is a differentiable curve or parametric curve of class "C"∞ and the singularity where the derivative is 0 is an ordinary cusp.
The cycloid satisfies the differential equation:
Evolute.
The evolute of the cycloid has the property of being exactly the same cycloid it originates from. This can otherwise be seen from the tip of a wire initially lying on a half arc of cycloid describing a cycloid arc equal to the one it was lying on once unwrapped (see also cycloidal pendulum and arc length).
Demonstration.
There are several demonstrations of the assertion. The one presented here uses the physical definition of cycloid and the kinematic property that the instantaneous velocity of a point is tangent to its trajectory.
Referring to the picture on the right, formula_7 and formula_8 are two tangent points belonging to two rolling circles. The two circles start to roll with same speed and same direction without skidding. formula_7 and formula_8 start to draw two cycloid arcs as in the picture. Considering the line connecting formula_7 and formula_8 at an arbitrary instant (red line), it is possible to prove that "the line is anytime tangent in P2 to the lower arc and orthogonal to the tangent in P1 of the upper arc". One sees that:
Area.
One arch of a cycloid generated by a circle of radius "r" can be parameterized by
with 
formula_24
Since
the area under the arch is
This result, and some generalizations, can be obtained without calculation by Mamikon's visual calculus.
Arc length.
The arc length "S" of one arch is given by
Another immediate way to calculate the length of the cycloid given the properties of the evolute is to notice that when a wire describing an evolute has been completely unwrapped it extends itself along two diameters, a length of 4r. Because the wire does not change length during the unwrapping it follows that the length of half an arc of cycloid is 4r and a complete arc is 8r.
Cycloidal pendulum.
If a simple pendulum is suspended from the cusp of an inverted cycloid, such that the "string" is constrained between the adjacent arcs of the cycloid, and the pendulum's length is equal to that of half the arc length of the cycloid (i.e., twice the diameter of the generating circle), the bob of the pendulum also traces a cycloid path. Such a cycloidal pendulum is isochronous, regardless of amplitude. The equation of motion is given by:
The 17th-century Dutch mathematician Christiaan Huygens discovered and proved these properties of the cycloid while searching for more accurate pendulum clock designs to be used in navigation.
Related curves.
Several curves are related to the cycloid.
All these curves are roulettes with a circle rolled along a uniform curvature. The cycloid, epicycloids, and hypocycloids have the property that each is similar to its evolute. If "q" is the product of that curvature with the circle's radius, signed positive for epi- and negative for hypo-, then the curve:evolute similitude ratio is 1 + 2"q".
The classic Spirograph toy traces out hypotrochoid and epitrochoid curves.
Use in architecture.
The cycloidal arch was used by architect Louis Kahn in his design for the Kimbell Art Museum in Fort Worth, Texas. It was also used in the design of the Hopkins Center in Hanover, New Hampshire.
Use in violin plate arching.
Early research indicated that some transverse arching curves of the plates of golden age violins are closely modeled by curtate cycloid curves. Later work indicates that curtate cycloids do not serve as general models for these curves, which vary considerably.

</doc>
<doc id="41639" url="https://en.wikipedia.org/wiki?curid=41639" title="Reference noise">
Reference noise

In telecommunication, reference noise is the magnitude of circuit noise chosen as a reference for measurement. 
Many different levels with a number of different weightings are in current use, and care must be taken to ensure that the proper parameters are stated. 
Specific ones include: dBa, dBa(F1A), dBa(HA1), dBa0, dBm, dBm(psoph), dBm0, dBrn, dBrnC, dBrnC0, dBrn(f1-f2), dBrn(144-line), dBx.

</doc>
<doc id="41640" url="https://en.wikipedia.org/wiki?curid=41640" title="Reference surface">
Reference surface

In fiber optic technology, a reference surface is that surface of an optical fiber that is used to contact the transverse-alignment elements of a component such as a connector or mechanical splice. For telecommunications-grade fibers, the reference surface is the outer surface of the cladding. For plastic-clad silica (PCS) fibers, which have a strippable polymer cladding (not to be confused with the polymer overcoat of an all-silica fiber), the reference surface may be the core.

</doc>
<doc id="41641" url="https://en.wikipedia.org/wiki?curid=41641" title="Reflection coefficient">
Reflection coefficient

In physics and electrical engineering the reflection coefficient is a parameter that describes how much of an electromagnetic wave is reflected by an impedance discontinuity in the transmission medium. It is equal to the ratio of the amplitude of the reflected wave to the incident wave, with each expressed as phasors. For example, it is used in optics to calculate the amount of light that is reflected from a surface with a different index of refraction, such as a glass surface, or in an electrical transmission line to calculate how much of the electromagnetic wave is reflected by an impedance. The reflection coefficient is closely related to the "transmission coefficient". The reflectance of a system is also sometimes called a "reflection coefficient".
Different specialties have different applications for the term.
Telecommunications.
In telecommunications, the reflection coefficient is the ratio of the complex amplitude of the reflected wave to that of the incident wave. In particular, at a discontinuity in a transmission line, it is the complex ratio of the electric field strength of the reflected wave (formula_1) to that of the incident wave (formula_2). This is typically represented with a formula_3 (capital gamma) and can be written as:
The reflection coefficient may also be established using other field or circuit quantities. 
The reflection coefficient of a load is determined by its impedance formula_5 and the impedance toward the source formula_6
formula_7
Notice that a negative reflection coefficient means that the reflected wave receives a 180°, or formula_8, phase shift.
The magnitude (designated by vertical bars) of the reflection coefficient can be calculated from the standing wave ratio, formula_9:
The reflection coefficient is displayed graphically using a Smith chart.
Seismology.
Reflection coefficient is used in feeder testing for reliability of medium.
Optics and microwaves.
In optics and electromagnetics in general, "reflection coefficient" can refer to either the amplitude reflection coefficient described here, or the reflectance, depending on context. Typically, the reflectance is represented by a capital "R", while the amplitude reflection coefficient is represented by a lower-case "r".

</doc>
<doc id="41642" url="https://en.wikipedia.org/wiki?curid=41642" title="Reflection loss">
Reflection loss

In telecommunications, reflection loss occurs on a line which results in part of the energy being reflected back to the source. This can occur:

</doc>
<doc id="41643" url="https://en.wikipedia.org/wiki?curid=41643" title="Reflective array antenna">
Reflective array antenna

In telecommunications and radar, a reflective array antenna is a class of directive antennas in which multiple driven elements are mounted in front of a flat surface designed to reflect the radio waves in a desired direction. They are often used in the VHF and UHF frequency bands. VHF examples are generally large and resemble a highway billboard, so they are sometimes called billboard antennas or broadside antennas. Other names are bedspread array and bowtie array depending on the type of elements making up the antenna. The curtain array is a larger version used by shortwave radio broadcasting stations.
Reflective array antennas usually have a number of identical driven elements, fed in phase, in front of a flat, electrically large reflecting surface to produce a unidirectional beam, increasing antenna gain and reducing radiation in unwanted directions. The individual elements are most commonly half wave dipoles, although they sometimes contain parasitic elements as well as driven elements. The reflector may be a metal sheet or more commonly a wire screen. A metal screen reflects radio waves as well as a solid metal sheet as long as the holes in the screen are smaller than about one-tenth of a wavelength, so screens are often used to reduce weight and wind loads on the antenna. They usually consist of a grill of parallel wires or rods, oriented parallel to the axis of the dipole elements.
The driven elements are fed by a network of transmission lines, which divide the power from the RF source equally between the elements. This often has the circuit geometry of a tree structure.
Basic concepts.
Radio signals.
When a radio signal passes a conductor, it induces an electrical current in it. Since the radio signal fills space, and the conductor has a finite size, the induced currents add up or cancel out as they move along the conductor. A basic goal of antenna design is to make the currents add up to a maximum at the point where the energy is tapped off. To do this, the antenna elements are sized in relation to the wavelength of the radio signal, with the aim of setting up standing waves of current that are maximized at the feed point.
This means that an antenna designed to receive a particular wavelength has a natural size. To improve reception, one cannot simply make the antenna larger; this will improve the amount of signal intercepted by the antenna, which is largely a function of area, but will lower the efficiency of the reception (at a given wavelength). Thus, in order to improve reception, antenna designers often use multiple elements, combining them together so their signals add up. These are known as "antenna arrays".
Array phasing.
In order for the signals to add together, they need to arrive in-phase. Consider two dipole antennas placed in a line end-to-end, or "collinear". If the resulting array is pointed directly at the source signal, both dipoles will see the same instantaneous signal, and thus their reception will be in-phase. However, if one were to rotate the antenna so it was at an angle to the signal, the extra path from the signal to the more distant dipole means it receives the signal slightly out of phase. When the two signals are then added up, they no longer strictly reinforce each other, and the output drops. This makes the array more sensitive horizontally, while stacking the dipoles in parallel narrows the pattern vertically. This allows the designer to tailor the reception pattern, and thus the gain, by moving the elements about.
If the antenna is properly aligned with the signal, at any given instant in time, all of the elements in an array will receive the same signal and be in-phase. However, the output from each element has to be gathered up at a single feed point, and as the signals travel across the antenna to that point, their phase is changing. In a two-element array this is not a problem because the feed point can be placed between them; any phase shift taking place in the transmission lines is equal for both elements. However, if one extends this to a four-element array, this approach no longer works, as the signal from the outer pair has to travel further and will thus be at a different phase than the inner pair when it reaches the center. To ensure that they all arrive with the same phase, it is common to see additional transmission wire inserted in the signal path, or for the transmission line to be crossed over to reverse the phase if the difference is greater than a wavelength.
Reflectors.
The gain can be further improved through the addition of a "reflector". Generally any conductor in a flat sheet will act in a mirror-like fashion for radio signals, but this also holds true for non-continuous surfaces as long as the gaps between the conductors are less than about of the target wavelength. This means that wire mesh or even parallel wires or metal bars can be used, which is especially useful both for reducing the total amount of material as well as reducing wind loads.
Due to the change in signal propagation direction on reflection, the signal undergoes a reversal of phase. In order for the reflector to add to the output signal, it has to reach the elements in-phase. Generally this would require the reflector to be placed at of a wavelength behind the elements, and this can be seen in many common reflector arrays like television antennas. However, there are a number of factors that can change this distance, and actual reflector positioning varies.
Reflectors also have the advantage of reducing the signal received from the back of the antenna. Signals received from the rear and re-broadcast from the reflector have not undergone a change of phase, and do not add to the signal from the front. This greatly improves the front-to-back ratio of the antenna, making it more directional. This can be useful when a more directional signal is desired, or unwanted signals are present. There are cases when this is not desirable, and although reflectors are commonly seen in array antennas, they are not universal. For instance, while UHF television antennas often use an array of bowtie antennas with a reflector, a bowtie array without a reflector is a relatively common design in the microwave region.
Gain limits.
As more elements are added to an array, the beamwidth of the antenna's main lobe decreases, leading to an increase in gain. In theory there is no limit to this process. However, as the number of elements increases, the complexity of the required feed network that keeps the signals in-phase increases. Ultimately, the rising inherent losses in the feed network become greater than the additional gain achieved with more elements, limiting the maximum gain that can be achieved.
The gain of practical array antennas is limited to about 25 - 30 dB. Common 4-bay television antennas have gains around 10 to 12 dB, and 8-bay designs might increase this to 12 to 16 dB. The 32-element SCR-270 had a gain around 19.8 dB. Some very large reflective arrays have been constructed, notably the Soviet Duga radars which are hundreds of meters across and contain hundreds of elements. "Active" array antennas, in which groups of elements are driven by separate RF amplifiers, can have much higher gain, but are prohibitively expensive.
Since the 1980s, versions for use at microwave frequencies have been made with patch antenna elements mounted in front of a metal surface.
Radiation pattern and beam steering.
When driven in phase, the radiation pattern of the reflective array is a single main lobe perpendicular to the plane of the antenna, plus several sidelobes at equal angles to either side. The more elements used, the narrower the main lobe and the less power is radiated in the sidelobes.
The main lobe of the antenna can be steered electronically within a limited angle by phase shifting the drive signals applied to the individual elements. Each antenna element is fed through a phase shifter which can be controlled digitally, delaying each signal by a successive amount. This causes the wavefronts created by the superposition of the individual elements to be at an angle to the plane of the antenna. Antennas that use this technique are called phased arrays and are being intensively developed, particularly for use in radar systems. 
Another option for steering the beam is mounting the entire array structure on a rotating bearing and rotating it mechanically.

</doc>
<doc id="41644" url="https://en.wikipedia.org/wiki?curid=41644" title="Reflectance">
Reflectance

Reflectance of the surface of a material is its effectiveness in reflecting radiant energy. It is the fraction of incident electromagnetic power that is reflected at an interface. The reflectance spectrum or spectral reflectance curve is the plot of the reflectance as a function of wavelength.
Mathematical definitions.
Hemispherical reflectance.
Hemispherical reflectance of a surface, denoted "R", is defined as
where
Spectral hemispherical reflectance.
Spectral hemispherical reflectance in frequency and spectral hemispherical reflectance in wavelength of a surface, denoted "R"ν and "R"λ respectively, are defined as
where
Directional reflectance.
Directional reflectance of a surface, denoted "R"Ω, is defined as
where
Spectral directional reflectance.
Spectral directional reflectance in frequency and spectral directional reflectance in wavelength of a surface, denoted "R"ν,Ω and "R"λ,Ω respectively, are defined as
where
Reflectivity.
For homogeneous and semi-infinite (see halfspace) materials, reflectivity is the same as reflectance. 
Reflectivity is the square of the magnitude of the Fresnel reflection coefficient,
which is the ratio of the reflected to incident electric field; 
as such the reflection coefficient can be expressed as a complex number as determined by the Fresnel equations for a single layer, whereas the reflectance is always a positive real number.
For layered and finite media, according to the CIE, reflectivity is distinguished from reflectance by the fact that reflectivity is a value that applies to "thick" reflecting objects. When reflection occurs from thin layers of material, internal reflection effects can cause the reflectance to vary with surface thickness. Reflectivity is the limit value of reflectance as the sample becomes thick; it is the intrinsic reflectance of the surface, hence irrespective of other parameters such as the reflectance of the rear surface. Another way to interpret this is that the reflectance is the fraction of electromagnetic power reflected from a specific sample, while reflectivity is a property of the material itself, which would be measured on a perfect machine if the material filled half of all space.
Surface type.
Going back to the fact that reflectance is a directional property, most surfaces can be divided into those that give specular reflection and those that give diffuse reflection:
Most real objects have some mixture of diffuse and specular reflective properties.
Water reflectance.
Reflection occurs when light moves from a medium with one index of refraction into a second medium with a different index of refraction.
Specular reflection from a body of water is calculated by the Fresnel equations. Fresnel reflection is directional and therefore does not contribute significantly to albedo which is primarily diffuse reflection.
A real water surface may be wavy. Reflectance assuming a flat surface as given by the Fresnel equations can be adjusted to account for waviness.
Grating efficiency.
The generalization of reflectance to a diffraction grating, which disperses light by wavelength, is called diffraction efficiency.
Applications.
Reflectance is an important concept in the fields of optics, solar thermal energy, telecommunication and radar.

</doc>
<doc id="41646" url="https://en.wikipedia.org/wiki?curid=41646" title="Refractive index contrast">
Refractive index contrast

Refractive index contrast, in an optical fiber, is a measure of the relative difference in refractive index of the core and cladding. Refractive index contrast, Δ, is given by Δ = ("n"12- "n"22)/(2"n"12), where "n"1 is the maximum refractive index in the core and "n"2 is the refractive index of the homogeneous cladding. Normal optical fibers have very low refractive index contrast(ΔÂ«1)hence are weakly guided medium. The weak guiding will cause more of the Electrical field to "leak" and travel through the cladding(as evanescent waves) as compared to the strongly guided waveguides. 

</doc>
<doc id="41647" url="https://en.wikipedia.org/wiki?curid=41647" title="Reframing time">
Reframing time

In telecommunication, the reframing time (or frame-alignment recovery time) is the time interval between the instant at which a valid frame-alignment signal is available at the receiving data terminal equipment and the instant at which frame alignment is established. 
The reframing time includes the time required for replicated verification of the validity of the frame-alignment signal.

</doc>
<doc id="41648" url="https://en.wikipedia.org/wiki?curid=41648" title="Regeneration">
Regeneration

Regeneration is renewal through the internal processes of a body or system. Something which embodies this action can be described as regenerative. Many titles of cultural work and cultural and scientific concepts use the term, and may refer to:

</doc>
<doc id="41649" url="https://en.wikipedia.org/wiki?curid=41649" title="Relative transmission level">
Relative transmission level

In telecommunication, relative transmission level is the ratio of the signal power, at a given point in a transmission system, to a reference signal power. 
The ratio is usually determined by applying a standard test tone at zero transmission level point (or applying adjusted test tone power at any other point) and measuring the gain or loss to the location of interest. A distinction should be made between the standard test tone power and the expected median power of the actual signal required as the basis for the design of transmission systems.

</doc>
<doc id="41650" url="https://en.wikipedia.org/wiki?curid=41650" title="Release time (telecommunication)">
Release time (telecommunication)

In telecommunication, release time is the time interval for a circuit to respond when an enabling signal is discontinued, for example:

</doc>
<doc id="41651" url="https://en.wikipedia.org/wiki?curid=41651" title="Reliability">
Reliability

Reliability may refer to:

</doc>
<doc id="41652" url="https://en.wikipedia.org/wiki?curid=41652" title="Remote access">
Remote access

Remote access may refer to:

</doc>
<doc id="41653" url="https://en.wikipedia.org/wiki?curid=41653" title="Remote call forwarding">
Remote call forwarding

In telecommunication, a remote call forwarding is a service feature that allows calls coming to a remote call forwarding number to be automatically forwarded to any answering location designated by the call receiver.
Customers may have a remote-forwarding telephone number in a central switching office without having any other local telephone service in that office.
One common purpose for this service is to enable customers to retain their telephone number when they move to a location serviced by a different telephone exchange. The service is useful for business customers with widely-advertised numbers which appear on headed paper, vehicles and various marketing literature. When customers ring, their calls are seamlessly forwarded to the new location.
Remote call forwarding is also a means for a suburban business to obtain a city-centre local number (with its full large-city coverage area) for inbound calls; while cheaper than a foreign exchange line, this can reduce long-distance telephony costs in markets where local calls are flat-rated but trunk calls are expensive.
One alternative to RCF is Caller Redirect whereby callers simply hear an intercept message notifying them that the number has changed. Another alternative is to port the existing number to a voice over IP carrier, which is not tied to a single physical location as the subscriber may be anywhere on broadband Internet.
Remote Access to Call Forwarding.
Remote Call Forwarding (RCF) requires neither a physical telephone set nor physical input by customer to get calls forwarded.
In this respect, it differs from the (similarly named) Remote Access to Call Forwarding in that the number is attached to a physical line where it rings normally until a call is made to a remote number to enable redirection.
To activate Remote Access to Call Forwarding, a subscriber calls a provider-supplied Remote Access Directory Number, enters the telephone number of the line to be redirected along with a Personal Identification Number (PIN), a vertical service code (such as 72# or *73) and the number to which the calls are to be forwarded.
Remote Access to Call Forwarding allows incoming calls to be diverted and answered elsewhere if a subscriber cannot use their telephone normally (for instance, the number is assigned to a lost or stolen wireless handset or to a landline in need of repair service). In some cases, a business which subscribes to standard call forwarding (*72) may be able to request temporary redirection of inbound calls when calling a telco repair service number (such as 6-1-1) to report a line outage.

</doc>
<doc id="41654" url="https://en.wikipedia.org/wiki?curid=41654" title="Remote Operations Service Element protocol">
Remote Operations Service Element protocol

The Remote Operations Service Element (ROSE) is the OSI service interface, specified in ITU-T Recommendation X.219, ISO/IEC International Standard 9072-1, that (a) provides remote operation capabilities, (b) allows interaction between entities in a distributed application, and (c) upon receiving a remote operations service request, allows the receiving entity to attempt the operation and report the results of the attempt to the requesting entity. 
OSI application protocols such as X.400 and X.500 use the services provided by ROSE. The ROSE protocol itself is defined using the notation of ASN.1.

</doc>
<doc id="41655" url="https://en.wikipedia.org/wiki?curid=41655" title="Repeater">
Repeater

In telecommunications, a repeater is an electronic device that receives a signal and retransmits it at a higher level or higher power, or onto the other side of an obstruction, so that the signal can cover longer distances. There are several different types of repeaters; a "telephone repeater" is an amplifier in a telephone line, an "optical repeater" is an optoelectronic circuit that amplifies the light beam in an optical fiber cable; and a "radio repeater" is a radio receiver and transmitter that retransmits a radio signal. A broadcast relay station performs an analogous role in broadcast radio and television.
Overview.
When an information-bearing signal passes through a communication channel, it is progressively degraded due to loss of power. For example, when a telephone call passes through a wire telephone line, some of the power in the electric current which represents the audio signal is dissipated as heat in the resistance of the copper wire. The longer the wire is, the more power is lost, and the smaller the amplitude of the signal at the far end. So with a long enough wire the call will not be audible at the other end. Similarly, the farther from a radio station a receiver is, the weaker the radio signal, and the poorer the reception. A repeater is an electronic device in a communication channel that increases the power of a signal and retransmits it, allowing it to travel further. Since it amplifies the signal, it requires a source of electric power.
The term "repeater" originated with telegraphy in the 19th century, and referred to an electromechanical device (a relay) used to regenerate telegraph signals. Use of the term has continued in telephony and data communications.
In computer networking, because repeaters work with the actual physical signal, and do not attempt to interpret the data being transmitted, they operate on the physical layer, the first layer of the OSI model.
Types.
Repeaters can be divided into two types depending on the type of data they handle:
Telephone repeater.
Before the invention of electronic amplifiers, mechanically coupled carbon microphones were used as amplifiers in telephone repeaters. After the turn of the century it was found that negative resistance mercury lamps could amplify, and they were used. The invention of audion tube repeaters around 1916 made transcontinental telephony practical. In the 1930s vacuum tube repeaters using hybrid coils became commonplace, allowing the use of thinner wires. In the 1950s negative impedance gain devices were more popular, and a transistorized version called the E6 repeater was the final major type used in the Bell System before the low cost of digital transmission made all voiceband repeaters obsolete. Frequency frogging repeaters were commonplace in frequency-division multiplexing systems from the middle to late 20th century...

</doc>
<doc id="41656" url="https://en.wikipedia.org/wiki?curid=41656" title="Repeating coil">
Repeating coil

In telecommunications, a repeating coil is a voice-frequency transformer characterized by a closed magnetic core, a pair of identical balanced primary (line) windings, a pair of identical but not necessarily balanced secondary (drop) windings, and low transmission loss at voice frequencies. It permits transfer of voice currents from one winding to another by magnetic induction, matches line and drop impedances, and prevents direct conduction between the line and the drop.
It is a special application of an isolation transformer, and is often used to prevent ground loops or earth loops, which cause humming or buzzing in audio circuits. It also prevents low direct current voltages from passing.

</doc>
<doc id="41657" url="https://en.wikipedia.org/wiki?curid=41657" title="Reproduction speed">
Reproduction speed

In telecommunication, the term reproduction speed has the following meanings: 

</doc>
<doc id="41658" url="https://en.wikipedia.org/wiki?curid=41658" title="Reradiation">
Reradiation

In telecommunication, the term reradiation has the following meanings:
Near-field effects of an AM antenna may extend out two miles (3 km) or more. Cellular and microwave towers within this radius can reflect the AM signal out at a different frequency. This process results in interfering frequencies called reradiation.

</doc>
<doc id="41659" url="https://en.wikipedia.org/wiki?curid=41659" title="Resolution">
Resolution

Resolution may refer to:

</doc>
<doc id="41660" url="https://en.wikipedia.org/wiki?curid=41660" title="Resonance">
Resonance

In physics, resonance describes when a vibrating system or external force drives another system to oscillate with greater amplitude at a specific preferential frequency.
Frequencies at which the response amplitude is a relative maximum are known as the system's resonant frequencies, or resonance frequencies. At resonant frequencies, small periodic driving forces have the ability to produce large amplitude oscillations. This is because the system stores vibrational energy.
Resonance occurs when a system is able to store and easily transfer energy between two or more different storage modes (such as kinetic energy and potential energy in the case of a pendulum). However, there are some losses from cycle to cycle, called damping. When damping is small, the resonant frequency is approximately equal to the natural frequency of the system, which is a frequency of unforced vibrations. Some systems have multiple, distinct, resonant frequencies.
Resonance phenomena occur with all types of vibrations or waves: there is mechanical resonance, acoustic resonance, electromagnetic resonance, nuclear magnetic resonance (NMR), electron spin resonance (ESR) and resonance of quantum wave functions. Resonant systems can be used to generate vibrations of a specific frequency (e.g., musical instruments), or pick out specific frequencies from a complex vibration containing many frequencies (e.g., filters).
The term "resonance" (from Latin "resonantia", 'echo', from "resonare", 'resound') originates from the field of acoustics, particularly observed in musical instruments, e.g., when strings started to vibrate and to produce sound without direct excitation by the player.
Examples.
A familiar example is a playground swing, which acts as a pendulum. Pushing a person in a swing in time with the natural interval of the swing (its resonant frequency) makes the swing go higher and higher (maximum amplitude), while attempts to push the swing at a faster or slower tempo produce smaller arcs. This is because the energy the swing absorbs is maximized when the pushes are "in phase" with the swing's natural oscillations, while some of the swing's energy is actually extracted by the opposing force of the pushes when they are not.
Resonance occurs widely in nature, and is exploited in many manmade devices. It is the mechanism by which virtually all sinusoidal waves and vibrations are generated. Many sounds we hear, such as when hard objects of metal, glass, or wood are struck, are caused by brief resonant vibrations in the object. Light and other short wavelength electromagnetic radiation is produced by resonance on an atomic scale, such as electrons in atoms. Other examples are:
Theory.
The exact response of a resonance, especially for frequencies far from the resonant frequency, depends on the details of the physical system, and is usually not exactly symmetric about the resonant frequency, as illustrated for the simple harmonic oscillator above.
For a lightly damped linear oscillator with a resonance frequency Ω, the "intensity" of oscillations "I" when the system is driven with a driving frequency ω is typically approximated by a formula that is symmetric about the resonance frequency:
The intensity is defined as the square of the amplitude of the oscillations. This is a Lorentzian function, or Cauchy distribution, and this response is found in many physical situations involving resonant systems. Γ is a parameter dependent on the damping of the oscillator, and is known as the "linewidth" of the resonance. Heavily damped oscillators tend to have broad linewidths, and respond to a wider range of driving frequencies around the resonant frequency. The linewidth is inversely proportional to the Q factor, which is a measure of the sharpness of the resonance.
In electrical engineering, this approximate symmetric response is known as the "universal resonance curve", a concept introduced by Frederick E. Terman in 1932 to simplify the approximate analysis of radio circuits with a range of center frequencies and Q values.
Resonators.
A physical system can have as many resonant frequencies as it has degrees of freedom; each degree of freedom can vibrate as a harmonic oscillator. Systems with one degree of freedom, such as a mass on a spring, pendulums, balance wheels, and LC tuned circuits have one resonant frequency. Systems with two degrees of freedom, such as coupled pendulums and resonant transformers can have two resonant frequencies. As the number of coupled harmonic oscillators grows, the time it takes to transfer energy from one to the next becomes significant. The vibrations in them begin to travel through the coupled harmonic oscillators in waves, from one oscillator to the next.
Extended objects that can experience resonance due to vibrations inside them are called resonators, such as organ pipes, vibrating strings, quartz crystals, microwave and laser cavities. Since these can be viewed as being made of millions of coupled moving parts (such as atoms), they can have millions of resonant frequencies. The vibrations inside them travel as waves, at an approximately constant velocity, bouncing back and forth between the sides of the resonator. If the distance between the sides is formula_2, the length of a roundtrip is formula_3. To cause resonance, the phase of a sinusoidal wave after a roundtrip must be equal to the initial phase, so the waves reinforce the oscillation. So the condition for resonance in a resonator is that the roundtrip distance, formula_3, be equal to an integer number of wavelengths formula_5 of the wave:
If the velocity of a wave is formula_7, the frequency is formula_8 so the resonant frequencies are:
So the resonant frequencies of resonators, called normal modes, are equally spaced multiples of a lowest frequency called the fundamental frequency. The multiples are often called overtones. There may be several such series of resonant frequencies, corresponding to different modes of oscillation.
Q factor.
The Q factor or "quality factor" is a dimensionless parameter that describes how under-damped an oscillator or resonator is, or equivalently, characterizes a resonator's bandwidth relative to its center frequency.
Higher "Q" indicates a lower rate of energy loss relative to the stored energy of the oscillator, i.e., the oscillations die out more slowly. A pendulum suspended from a high-quality bearing, oscillating in air, has a high "Q", while a pendulum immersed in oil has a low "Q". To sustain a system in resonance in constant amplitude by providing power externally, the energy provided in each cycle must be less than the energy stored in the system (i.e., the sum of the potential and kinetic) by a factor of formula_10. Oscillators with high-quality factors have low damping, which tends to make them ring longer.
Sinusoidally driven resonators having higher Q factors resonate with greater amplitudes (at the resonant frequency) but have a smaller range of frequencies around the frequency at which they resonate. The range of frequencies at which the oscillator resonates is called the bandwidth. Thus, a high Q tuned circuit in a radio receiver would be more difficult to tune, but would have greater selectivity, it would do a better job of filtering out signals from other stations that lie nearby on the spectrum. High Q oscillators operate over a smaller range of frequencies and are more stable. (See oscillator phase noise.)
The quality factor of oscillators varies substantially from system to system. Systems for which damping is important (such as dampers keeping a door from slamming shut) have "Q" = . Clocks, lasers, and other systems that need either strong resonance or high frequency stability need high-quality factors. Tuning forks have quality factors around "Q" = 1000. The quality factor of atomic clocks and some high-Q lasers can reach as high as 1011 and higher.
There are many alternate quantities used by physicists and engineers to describe how damped an oscillator is that are closely related to its quality factor. Important examples include: the damping ratio, relative bandwidth, linewidth, and bandwidth measured in octaves.
Types of resonance.
Mechanical and acoustic resonance.
Mechanical resonance is the tendency of a mechanical system to absorb more energy when the frequency of its oscillations matches the system's natural frequency of vibration than it does at other frequencies. It may cause violent swaying motions and even catastrophic failure in improperly constructed structures including bridges, buildings, trains, and aircraft. When designing objects, engineers must ensure the mechanical resonance frequencies of the component parts do not match driving vibrational frequencies of motors or other oscillating parts, a phenomenon known as resonance disaster.
Avoiding resonance disasters is a major concern in every building, tower, and bridge construction project. As a countermeasure, shock mounts can be installed to absorb resonant frequencies and thus dissipate the absorbed energy. The Taipei 101 building relies on a —a tuned mass damper—to cancel resonance. Furthermore, the structure is designed to resonate at a frequency that does not typically occur. Buildings in seismic zones are often constructed to take into account the oscillating frequencies of expected ground motion. In addition, engineers designing objects having engines must ensure that the mechanical resonant frequencies of the component parts do not match driving vibrational frequencies of the motors or other strongly oscillating parts.
Clocks keep time by mechanical resonance in a balance wheel, pendulum, or quartz crystal.
The cadence of runners has been hypothesized to be energetically favorable due to resonance between the elastic energy stored in the lower limb and the mass of the runner.
Acoustic resonance is a branch of mechanical resonance that is concerned with the mechanical vibrations across the frequency range of human hearing, in other words sound. For humans, hearing is normally limited to frequencies between about 20 Hz and 20,000 Hz (20 kHz),
Acoustic resonance is an important consideration for instrument builders, as most acoustic instruments use resonators, such as the strings and body of a violin, the length of tube in a flute, and the shape of, and tension on, a drum membrane.
Like mechanical resonance, acoustic resonance can result in catastrophic failure of the object at resonance. The classic example of this is breaking a wine glass with sound at the precise resonant frequency of the glass, although this is difficult in practice.
Electrical resonance.
Electrical resonance occurs in an electric circuit at a particular "resonant frequency" when the impedance of the circuit is at a minimum in a series circuit or at maximum in a parallel circuit (or when the transfer function is at a maximum).
Optical resonance.
An optical cavity, also called an "optical resonator", is an arrangement of mirrors that forms a standing wave cavity resonator for light waves. Optical cavities are a major component of lasers, surrounding the gain medium and providing feedback of the laser light. They are also used in optical parametric oscillators and some interferometers. Light confined in the cavity reflects multiple times producing standing waves for certain resonant frequencies. The standing wave patterns produced are called "modes". Longitudinal modes differ only in frequency while transverse modes differ for different frequencies and have different intensity patterns across the cross-section of the beam. Ring resonators and whispering galleries are examples of optical resonators that do not form standing waves.
Different resonator types are distinguished by the focal lengths of the two mirrors and the distance between them; flat mirrors are not often used because of the difficulty of aligning them precisely. The geometry (resonator type) must be chosen so the beam remains stable, i.e., the beam size does not continue to grow with each reflection. Resonator types are also designed to meet other criteria such as minimum beam waist or having no focal point (and therefore intense light at that point) inside the cavity.
Optical cavities are designed to have a very large Q factor. A beam reflects a large number of times with little attenuation—therefore the frequency line width of the beam is small compared to the frequency of the laser.
Additional optical resonances are guided-mode resonances and surface plasmon resonance, which result in anomalous reflection and high evanescent fields at resonance. In this case, the resonant modes are guided modes of a waveguide or surface plasmon modes of a dielectric-metallic interface. These modes are usually excited by a subwavelength grating.
Orbital resonance.
In celestial mechanics, an orbital resonance occurs when two orbiting bodies exert a regular, periodic gravitational influence on each other, usually due to their orbital periods being related by a ratio of two small integers. Orbital resonances greatly enhance the mutual gravitational influence of the bodies. In most cases, this results in an "unstable" interaction, in which the bodies exchange momentum and shift orbits until the resonance no longer exists. Under some circumstances, a resonant system can be stable and self-correcting, so that the bodies remain in resonance. Examples are the 1:2:4 resonance of Jupiter's moons Ganymede, Europa, and Io, and the 2:3 resonance between Pluto and Neptune. Unstable resonances with Saturn's inner moons give rise to gaps in the rings of Saturn. The special case of 1:1 resonance (between bodies with similar orbital radii) causes large Solar System bodies to clear the neighborhood around their orbits by ejecting nearly everything else around them; this effect is used in the current definition of a planet.
Atomic, particle, and molecular resonance.
Nuclear magnetic resonance (NMR) is the name given to a physical resonance phenomenon involving the observation of specific quantum mechanical magnetic properties of an atomic nucleus in the presence of an applied, external magnetic field. Many scientific techniques exploit NMR phenomena to study molecular physics, crystals, and non-crystalline materials through NMR spectroscopy. NMR is also routinely used in advanced medical imaging techniques, such as in magnetic resonance imaging (MRI).
All nuclei containing odd numbers of nucleons have an intrinsic magnetic moment and angular momentum. A key feature of NMR is that the resonant frequency of a particular substance is directly proportional to the strength of the applied magnetic field. It is this feature that is exploited in imaging techniques; if a sample is placed in a non-uniform magnetic field then the resonant frequencies of the sample's nuclei depend on where in the field they are located. Therefore, the particle can be located quite precisely by its resonant frequency.
Electron paramagnetic resonance, otherwise known as "Electron Spin Resonance" (ESR) is a spectroscopic technique similar to NMR, but uses unpaired electrons instead. Materials for which this can be applied are much more limited since the material needs to both have an unpaired spin and be paramagnetic.
The Mössbauer effect is the resonant and recoil-free emission and absorption of gamma ray photons by atoms bound in a solid form.
Resonance in particle physics appears in similar circumstances to classical physics at the level of quantum mechanics and quantum field theory. However, they can also be thought of as unstable particles, with the formula above valid if the formula_11 is the decay rate and formula_12 replaced by the particle's mass M. In that case, the formula comes from the particle's propagator, with its mass replaced by the complex number formula_13. The formula is further related to the particle's decay rate by the optical theorem.
Tacoma Narrows Bridge.
The dramatically visible, rhythmic twisting that resulted in the 1940 collapse of "Galloping Gertie", the original Tacoma Narrows Bridge, is misleadingly characterized as an example of resonance phenomenon in certain textbooks. The catastrophic vibrations that destroyed the bridge were not due to simple mechanical resonance, but to a more complicated interaction between the bridge and the winds passing through it—a phenomenon known as aeroelastic flutter, which is a kind of "self-sustaining vibration" as referred to in the nonlinear theory of vibrations. Robert H. Scanlan, father of bridge aerodynamics, has written an article about this misunderstanding.
International Space Station.
The rocket engines for the International Space Station (ISS) are controlled by an autopilot. Ordinarily, uploaded parameters for controlling the engine control system for the Zvezda module make the rocket engines boost the International Space Station to a higher orbit. The rocket engines are hinge-mounted, and ordinarily the crew doesn't notice the operation. On January 14, 2009, however, the uploaded parameters made the autopilot swing the rocket engines in larger and larger oscillations, at a frequency of 0.5 Hz. These oscillations were captured on video, and lasted for 142 seconds.

</doc>
<doc id="41661" url="https://en.wikipedia.org/wiki?curid=41661" title="Response">
Response

Response may refer to:

</doc>
<doc id="41662" url="https://en.wikipedia.org/wiki?curid=41662" title="Response time (technology)">
Response time (technology)

In technology, response time is the time a system or functional unit takes to react to a given input.
Computing.
Response time is the total amount of time it takes to respond to a request for service. That service can be anything from a memory fetch, to a disk IO, to a complex database query, or loading a full web page. Ignoring transmission time for a moment, the response time is the sum of the service time and wait time. The service time is the time it takes to do the work you requested. For a given request the service time varies little as the workload increases – to do X amount of work it always takes X amount of time. The wait time is how long the request had to wait in a queue before being serviced and it varies from zero, when no waiting is required, to a large multiple of the service time, as many requests are already in the queue and have to be serviced first.
With basic queueing theory math you can calculate how the average wait time increases as the device providing the service goes from 0-100% busy. As the device becomes busier, the average wait time increases in a non-linear fashion. The busier the device is, the more dramatic the response time increases will seem as you approach 100% busy; all of that increase is caused by increases in wait time, which is the result of all the requests waiting in queue that have to run first.
Transmission time gets added to response time when your request and the resulting response has to travel over a network and it can be very significant. Transmission time can include propagation delays due to distance (the speed of light is finite), delays due to transmission errors, and data communication bandwidth limits (especially at the last mile) slowing the transmission speed of the request or the reply.
Real-time Systems.
In real-time systems the response time of a task or thread is defined as the time elapsed between the dispatch (time when task is ready to execute) to the time when it finishes its job (one dispatch). Response time is different from WCET which is the maximum time the task would take if it were to execute without interference. It is also different from deadline which is the length of time during which the task's output would be valid in the context of the specific system.
Display technologies.
Response time is the amount of time a pixel in a display takes to change. It is measured in milliseconds (ms). Lower numbers mean faster transitions and therefore fewer visible image artifacts. Older monitors with long response times would create display motion blur around moving objects, making them unacceptable for rapidly moving images. Typically response times are "usually" measured from grey-to-grey transitions, but there is no industry standard.

</doc>
<doc id="41663" url="https://en.wikipedia.org/wiki?curid=41663" title="Responsivity">
Responsivity

Responsivity measures the input–output gain of a detector system. In the specific case of a photodetector, responsivity measures the electrical output per optical input. 
The responsivity of a photodetector is usually expressed in units of either amperes or volts per watt of incident radiant power. For a system that responds linearly to its input, there is a unique responsivity. For nonlinear systems, the responsivity is the local slope. Many common photodetectors respond linearly as a function of the incident power. 
Responsivity is a function of the wavelength of the incident radiation and of the sensor properties, such as the bandgap of the material of which the photodetector is made. One simple expression for the responsivity "R" of a photodetector in which an optical signal is converted into an electric current (known as a photocurrent) is 
formula_1
where formula_2 is the quantum efficiency (the conversion efficiency of photons to electrons) of the detector for a given wavelength, formula_3 is the electron charge, formula_4 is the frequency of the optical signal, and formula_5 is Planck's constant. This expression is also given in terms of formula_6, the wavelength of the optical signal, and has units of amperes per watt (A/W).
The term responsivity is also used to summarize input–output relationship in non-electrical systems. For example, a neuroscientist may measure how neurons in the visual pathway respond to light. In this case, responsivity summarizes the change in the neural response per unit signal strength. The responsivity in these applications can have a variety of units. The signal strength typically is controlled by varying either intensity (intensity-response function) or contrast (contrast-response function). The neural response measure depends on the part of the nervous system under study. For example, at the level of the retinal cones, the response might be in photocurrent. In the central nervous system the response is usually spikes per second. In functional neuroimaging, the response measure is usually BOLD contrast. The responsivity units reflect the relevant stimulus and physiological units. 
When describing an amplifier, the more common term is gain. 
"Deprecated synonym" sensitivity. A system's sensitivity is the inverse of the stimulus level required to produce a threshold response, with the threshold typically chosen just above the noise level.

</doc>
<doc id="41664" url="https://en.wikipedia.org/wiki?curid=41664" title="Restoration">
Restoration

Restoration may refer to:

</doc>
<doc id="41665" url="https://en.wikipedia.org/wiki?curid=41665" title="Return loss">
Return loss

In telecommunications, return loss is the loss of power in the signal returned/reflected by a discontinuity in a transmission line or optical fiber. This discontinuity can be a mismatch with the terminating load or with a device inserted in the line. It is usually expressed as a ratio in decibels (dB);
Return loss is related to both standing wave ratio (SWR) and reflection coefficient (Γ). Increasing return loss corresponds to lower SWR. Return loss is a measure of how well devices or lines are matched. A match is good if the return loss is high. A high return loss is desirable and results in a lower insertion loss.
Return loss is used in modern practice in preference to SWR because it has better resolution for small values of reflected wave.
Sign.
Properly, loss quantities, when expressed in decibels, should be positive numbers. However, return loss has historically been expressed as a negative number, and this convention is still widely found in the literature.
The correct definition of return loss is the difference in dB between the incident power sent towards the Device Under Test (DUT) and the power reflected, resulting in a positive sign:
However taking the ratio of reflected to incident power results in a negative sign for return loss;
Return loss with a positive sign is identical to the magnitude of Γ when expressed in decibels but of opposite sign. That is, return loss with a negative sign is more properly called reflection coefficient. The S-parameter "S"11 from two-port network theory is frequently also called return loss, but is actually equal to Γ.
Caution is required when discussing increasing or decreasing return loss since these terms strictly have the opposite meaning when return loss is defined as a negative quantity.
Electrical.
In metallic conductor systems, reflections of a signal traveling down a conductor can occur at a discontinuity or impedance mismatch. The ratio of the amplitude of the reflected wave "Vr" to the amplitude of the incident wave "Vi" is known as the reflection coefficient formula_4.
When the source and load impedances are known values, the reflection coefficient is given by
where "Z"S is the impedance toward the source and "Z"L is the impedance toward the load.
Return loss is the negative of the magnitude of the reflection coefficient in dB. Since power is proportional to the square of the voltage, return loss is given by,
where the vertical bars indicate magnitude. Thus, a large positive return loss indicates the reflected power is small relative to the incident power, which indicates good impedance match from source to load.
When the actual transmitted (incident) power and the reflected power are known (i.e., through measurements and/or calculations), then the return loss in dB can be calculated as the difference between the incident power "P"i (in dBm) and the reflected power "P"r (in dBm),
Optical.
In optics (particularly in fiberoptics) a loss that takes place at discontinuities of refractive index, especially at an air-glass interface such as a fiber endface. At those interfaces, a fraction of the optical signal is reflected back toward the source. This reflection phenomenon is also called "Fresnel reflection loss," or simply "Fresnel loss."
Fiber optic transmission systems use lasers to transmit signals over optical fiber, and a high optical return loss (ORL) can cause the laser to stop transmitting correctly. The measurement of ORL is becoming more important in the characterization of optical networks as the use of wavelength-division multiplexing increases. These systems use lasers that have a lower tolerance for ORL, and introduce elements into the network that are located in close proximity to the laser.
where formula_10 is the reflected power and formula_11 is the incident, or input, power.

</doc>
<doc id="41666" url="https://en.wikipedia.org/wiki?curid=41666" title="RF power margin">
RF power margin

In telecommunication, the term RF power margin has the following meanings: 

</doc>
<doc id="41667" url="https://en.wikipedia.org/wiki?curid=41667" title="Ringaround">
Ringaround

In telecommunication, the term ringaround has the following meanings:

</doc>
<doc id="41669" url="https://en.wikipedia.org/wiki?curid=41669" title="Ringdown">
Ringdown

In telephony, ringdown is a method of signaling an operator in which telephone ringing current is sent over the line to operate a lamp or cause the operation of a self-locking relay known as a "drop".
Ringdown is used in manual operation, as distinguished from automatic signaling by dialing a number. The signal consists of a continuous or pulsed alternating current (AC) signal transmitted over the line. It may be used with or without a telephone switchboard. The term originated in magneto telephone signaling in which cranking the magneto generator, either integrated into the telephone set or housed in a connected ringer box, would not only "ring" its bell but also cause a drop to fall "down" at the telephone exchange switchboard, marked with the number of the line to which the magneto telephone instrument was connected. 
The last ringdown telephone exchange in the United States was located at Bryant Pond, Maine, had 400+ subscribers, and converted to dial service in October 1983.
Ringdown operator.
In telephone systems where calls from distant automated exchanges arrive for manual subscribers or non-dialable points, there often would be a ringdown operator (reachable from the distant operator console by dialling NPA+181) who would manually ring the desired subscriber on a party line or toll station. On some systems, this function was carried out by the inward operator (NPA+121). In both cases, this is a telephone operator at the destination who provides assistance solely to other operators on inbound toll calls; the ringdown operator nominally cannot be dialled directly by the subscriber.
Non-operator use.
In an application "not" involving a telephone operator, a two-point automatic ringdown circuit, or ringdown, has a telephone at each end. When the telephone at one end goes off-hook, the phone at the other end instantly rings. No dialing is involved and therefore telephone sets without dials are sometimes used.
Many ringdown circuits work in both directions. In some cases a circuit is designed to work in one direction only. That is, going off-hook at one end (end A) rings the other (end B). Going off-hook at end B has no effect at end A.
Ringdown features are often part of a key telephone system. In the wire spring relay key service units of the Bell System 1A2, a model 216 automatic ringdown was used to operate the circuit. In the 400-series units, a number of different KTUs operate (supervise) a ringdown, including the model 415. In other situations, the ringdown is powered and operated by equipment inside the telephone exchange.
In the case of enterprises with a private branch exchange (PBX) switch, the ringdown can be operated by the PBX key. The switch is programmed to ring a specific extension (the called phone) when a defined extension (the calling phone) goes off-hook. The PBX does not offer dial tone to the calling extension: it only detects on-hook or off-hook status.
Voice over IP adapters can be networked and configured to provide automatic ringdown by selecting a dial plan which replaces the empty string with a predefined number or SIP address, dialed immediately. (Some Cisco VoIP phones and analog adapters treat a dial plan of (S0 <:1234567890>) as a hotline configuration which dials 1-234-567890 zero seconds after the telephone is taken off-hook, for instance).
These circuits are used:
In some cases, automatic ringdown circuits have one-to-many configurations. When one phone goes off-hook, a group of phones is made to ring simultaneously.
In cases where one or both ends of the circuit terminate in a key telephone system, a well-designed system will have no hold feature on the ringdown circuit unless supervision provides a Calling Party Control (CPC) signal.
PLAR.
PLAR stands for Private Line Automatic Ringdown. It is a type of analog signalling often used with telephone based intercom systems. When a device is taken off-hook, it applies a ringing voltage to the circuit. Other devices on the same pair will ring. Then, when another device is answered, a call will be maintained over the circuit at normal voltage. The telephone company switch is not involved in the process, making this a private line.

</doc>
<doc id="41670" url="https://en.wikipedia.org/wiki?curid=41670" title="Ringer equivalence number">
Ringer equivalence number

The ringer equivalence number (REN) is a telecommunications measure that represents the electrical loading effect of a telephone ringer on a telephone line. In the United States, the REN was first defined by U.S. Code of Federal Regulations, Title 47, Part 68, based on the load that a standard Bell System model 500 telephone represented, and was later determined in accordance with ANSI/TIA-968-B (August 2009).
Although the REN was developed in the United States, analogous measurement systems exist internationally. In some countries, particularly in Commonwealth nations, the REN is also known as the ringer approximated loading number (RAL).
Definition.
A ringer equivalency number of 1 represents the loading effect of a single traditional telephone ringing circuit, such as that within the Western Electric model 500 telephone. The REN of modern telephone equipment may be significantly lower than 1. For example, externally powered digital-ring telephones may have a REN as low as 0.1, while modern analog-ring telephones, in which the ringer is powered from the telephone line, typically have a REN of approximately 0.8.
In the United States, the FCC Part 68 specification defined 1 REN as equivalent to a 6930 Ω resistor in series with an 8 µF (microfarad) capacitor. The modern ANSI/TIA-968-B specification (August 2009) defines 1 REN as an impedance of at (type A ringer), or from 15 Hz to 68 Hz (type B ringer).
Maximum REN loading.
The total REN load on a subscriber line is the sum of the REN loads of all devices (phone, fax, a separate answerphone, etc.) connected to the line; this number expresses the overall loading effect of the subscriber equipment on the central office ringing current source. Subscriber telephone lines are usually limited to support a load of 5 REN or less.
If the total allowable ringer load is exceeded, the phone circuit may fail to ring or otherwise malfunction. For example, call waiting, caller ID and ADSL services are often affected by high ringer load. 20th century equipment tends to contribute to a larger REN than new equipment.
Some analog telephone adapters for Internet telephony require analog telephones with low REN, for example, the AT&T 210 is a basic phone which does not require an external electrical connection and has a REN of 0.9B.
International usage.
In the United Kingdom a maximum of 4 is allowed on any British Telecom (BT) line.
In Australia it is called ringer equivalence number and a maximum of 3 is allowed on any Telstra or Optus Line.
In Canada it is called a Load Number (LN); which must not exceed 100. The LN of each device represents the percentage of total load allowed.
In Europe 1 REN used to be equivalent to an 1800 Ω resistor in series with a 1 µF capacitor. The latest ETSI specification (2003–09) calls for 1 REN to be greater than 16 kΩ at 25 Hz and 50 Hz.

</doc>
<doc id="41671" url="https://en.wikipedia.org/wiki?curid=41671" title="Ring latency">
Ring latency

In a ring network, such as Token Ring, ring latency is the time required for a signal to propagate once around the ring. Ring latency may be measured in seconds or in bits at the data transmission rate. Ring latency includes signal propagation delays in (a) the ring medium, (b) the drop cables, and (c) the data stations connected to the ring network. 

</doc>
<doc id="41672" url="https://en.wikipedia.org/wiki?curid=41672" title="Round-trip delay time">
Round-trip delay time

In telecommunications, the round-trip delay time (RTD) or round-trip time (RTT) is the length of time it takes for a signal to be sent plus the length of time it takes for an acknowledgment of that signal to be received. This time delay therefore consists of the propagation times between the two points of a signal.
Computer networks.
In the context of computer networks, the signal is generally a data packet, and the RTT is also known as the ping time. An internet user can determine the RTT by using the ping command.
Space technology.
In space technology, the "round-trip delay time" or "round trip light time" is the time light (and hence any signal) takes to go to a space probe and return.
Protocol design.
Network links with both a high bandwidth and a high RTT can have a very large amount of data (the bandwidth-delay product) "in flight" at any given time. Such "long fat pipes" require a special protocol design. One example is the TCP window scale option.
The RTT was originally estimated in TCP by:
Where α is constant weighting factor(0 ≤ α < 1). Choosing a value α close to 1 makes the weighted average immune to changes that last a short time (e.g., a single segment that encounters long delay). Choosing a value for α close to 0 makes the weighted average respond to changes in delay very quickly.
This was improved by the Jacobson/Karels algorithm, which takes standard deviation into account as well.
Once a new RTT is calculated, it is entered into the equation above to obtain an average RTT for that connection, and the procedure continues for every new calculation.

</doc>
<doc id="41673" url="https://en.wikipedia.org/wiki?curid=41673" title="Routing indicator">
Routing indicator

In telecommunication, the term routing indicator (RI) has the following meanings: 
A Routing Indicator is a group of letters assigned to identify a station within a tape relay network to facilitate routing of traffic. It indicates the status of the station and may indicate its geographical area. The following factors are reflected in routing indicator assignment:
Routing indicators consist of not less than four, and not more than seven
letters, including suffixes. The intent of allocated letters and of letter position is as follows:

</doc>
<doc id="41674" url="https://en.wikipedia.org/wiki?curid=41674" title="Rubidium standard">
Rubidium standard

A rubidium standard or rubidium atomic clock is a frequency standard in which a specified hyperfine transition of electrons in rubidium-87 atoms is used to control the output frequency. It is the most inexpensive, compact, and widely used type of atomic clock, used to control the frequency of television stations, cell phone base stations, in test equipment, and global navigation satellite systems like GPS. Commercial rubidium clocks are less accurate than cesium atomic clocks, which serve as primary frequency standards, so the rubidium clock is a secondary frequency standard. However, rubidium fountains are currently being developed that are even more stable than caesium fountain clocks.
All commercial rubidium frequency standards operate by disciplining a crystal oscillator to the rubidium hyperfine transition of . The intensity of light from a rubidium discharge lamp that reaches a photodetector through a resonance cell will drop by about 0.1% when the rubidium vapor in the resonance cell is exposed to microwave power near the transition frequency. The crystal oscillator is stabilized to the rubidium transition by detecting the light dip while sweeping an RF synthesizer (referenced to the crystal) through the transition frequency.

</doc>
<doc id="41675" url="https://en.wikipedia.org/wiki?curid=41675" title="Rural radio service">
Rural radio service

Rural Radiotelephone Service (RRTS) provides basic, analog communications service between locations deemed so remote that traditional wireline service or service by other means is not feasible. RRTS uses channelized radio to provide radiotelephone services such as Basic Exchange Telephone Radio Service between a fixed subscriber location and a remote central office, private line service between a two fixed locations or interconnection between two or more central offices. RRTS does not enable mobile communications.
Licensing.
In the United States, the Federal Communications Commission issues initial Rural Radiotelephone Service licenses on a site-by-site basis. Once a license is issued, the licensee can sell or lease the license to another party.
The FCC service rules for Rural Radiotelephone are located in 47 C.F.R. Part 22 Subpart F.
Technical information.
In the United States, the ULS radio service code and description for Rural Radiotelephone licenses is CR – Rural Radiotelephone. The licensed spectrum is divided in 44 channels of 20 kHz each.

</doc>
<doc id="41676" url="https://en.wikipedia.org/wiki?curid=41676" title="Saturation">
Saturation

Saturation, saturated, unsaturation or unsaturated may refer to:

</doc>
<doc id="41677" url="https://en.wikipedia.org/wiki?curid=41677" title="Scan">
Scan

Scan may refer to:
Acronyms:
Businesses:
Electronics or computer related:
Medical:
Other uses:

</doc>
<doc id="41679" url="https://en.wikipedia.org/wiki?curid=41679" title="Schematic">
Schematic

A schematic, or schematic diagram, is a representation of the elements of a system using abstract, graphic symbols rather than realistic pictures. A schematic usually omits all details that are not relevant to the information the schematic is intended to convey, and may add unrealistic elements that aid comprehension. For example, a subway map intended for riders may represent a subway station with a dot; the dot doesn't resemble the actual station at all but gives the viewer information without unnecessary visual clutter. A schematic diagram of a chemical process uses symbols to represent the vessels, piping, valves, pumps, and other equipment of the system, emphasizing their interconnection paths and suppressing physical details. In an electronic circuit diagram, the layout of the symbols may not resemble the layout in the circuit. In the schematic diagram, the symbolic elements are arranged to be more easily interpreted by the viewer.
Types.
Schematics and other types of diagrams, e.g.,
A semi-schematic diagram combines some of the abstraction of a purely schematic diagram with other elements displayed as realistically as possible, for various reasons. It is a compromise between a purely abstract diagram (e.g. the schematic of the Washington Metro) and an exclusively realistic representation (e.g. the corresponding aerial view of Washington).
Electrical and electronic industry.
In electrical and electronic industry, a schematic diagram is often used to describe the design of equipment. Schematic diagrams are often used for the maintenance and repair of electronic and electromechanical systems. Original schematics were done by hand, using standardized templates or pre-printed adhesive symbols, but today electronic design automation software (EDA or "electrical CAD") is often used.
In electronic design automation, until the 1980s schematics were virtually the only formal representation for circuits. More recently, with the progress of computer technology, other representations were introduced and specialized computer languages were developed, since with the explosive growth of the complexity of electronic circuits, traditional schematics are becoming less practical. For example, hardware description languages are indispensable for modern digital circuit design.
Schematics for electronic circuits are prepared by designers using EDA (electronic design automation) tools called schematic capture tools or schematic entry tools. These tools go beyond simple drawing of devices and connections. Usually they are integrated into the whole IC design flow and linked to other EDA tools for verification and simulation of the circuit under design.
In electric power systems design, a schematic drawing called a one-line diagram is frequently used to represent substations, distribution systems or even whole electrical power grids. These diagrams simplify and compress the details that would be repeated on each phase of a three-phase system, showing only one element instead of three. Electrical diagrams for switchgear often have common device functions designate by standard function numbers.
Schematics in repair manuals.
Schematic diagrams are used extensively in repair manuals to help users understand the interconnections of parts, and to provide graphical instruction to assist in dismantling and rebuilding mechanical assemblies. Many automotive and motorcycle repair manuals devote a significant number of pages to schematic diagrams.

</doc>
<doc id="41680" url="https://en.wikipedia.org/wiki?curid=41680" title="Scrambler">
Scrambler

In telecommunications, a scrambler is a device that transposes or inverts signals or otherwise encodes a message at the senders side to make the message unintelligible at a receiver not equipped with an appropriately set descrambling device. Whereas encryption usually refers to operations carried out in the digital domain, scrambling usually refers to operations carried out in the analog domain. Scrambling is accomplished by the addition of components to the original signal or the changing of some important component of the original signal in order to make extraction of the original signal difficult. Examples of the latter might include removing or changing vertical or horizontal sync pulses in television signals; televisions will not be able to display a picture from such a signal. Some modern scramblers are actually encryption devices, the name remaining due to the similarities in use, as opposed to internal operation.
In telecommunications and recording, a "scrambler" (also referred to as a "randomizer") is a device that manipulates a data stream before transmitting. The manipulations are reversed by a "descrambler" at the receiving side. Scrambling is widely used in satellite, radio relay communications and PSTN modems. A scrambler can be placed just before a FEC coder, or it can be placed after the FEC, just before the modulation or line code. A scrambler in this context has nothing to do with encrypting, as the intent is not to render the message unintelligible, but to give the transmitted data useful engineering properties.
A scrambler replaces sequences (referred to as "whitening sequences") into other sequences without removing undesirable sequences, and as a result it changes the probability of occurrence of vexatious sequences. Clearly it is not foolproof as there are input sequences that yield all-zeros, all-ones, or other undesirable periodic output sequences. A scrambler is therefore not a good substitute for a line code, which, through a coding step, removes unwanted sequences.
Purposes of scrambling.
A scrambler (or randomizer) can be either:
There are two main reasons why scrambling is used:
Scramblers are essential components of physical layer system standards besides interleaved coding and modulation. They are usually defined based on linear feedback shift registers (LFSRs) due to their good statistical properties and ease of implementation in hardware.
It is common for physical layer standards bodies to refer to lower-layer (physical layer and link layer) encryption as scrambling as well. This may well be because (traditional) mechanisms employed are based on feedback shift registers as well.
Some standards for digital television, such as DVB-CA and MPE, refer to encryption at the link layer as scrambling.
Types of scramblers.
Additive (synchronous) scramblers.
"Additive scramblers" (they are also referred to as "synchronous") transform the input data stream by applying a pseudo-random binary sequence (PRBS) (by modulo-two addition). Sometimes a pre-calculated PRBS stored in the Read-only memory is used, but more often it is generated by a linear feedback shift register (LFSR).
In order to assure a synchronous operation of the transmitting and receiving LFSR (that is, "scrambler" and "descrambler"), a "sync-word" must be used.
A sync-word is a pattern that is placed in the data stream through equal intervals (that is, in each frame). A receiver searches for a few sync-words in adjacent frames and hence determines the place when its LFSR must be reloaded with a pre-defined "initial state".
The "additive descrambler" is just the same device as the additive scrambler.
Additive scrambler/descrambler is defined by the polynomial of its LFSR (for the scrambler on the picture above, it is formula_1) and its "initial state".
Multiplicative (self-synchronizing) scramblers.
"Multiplicative scramblers" (also known as "feed-through") are called so because they perform a "multiplication" of the input signal by the scrambler's transfer function in Z-space. They are discrete linear time-invariant systems.
A multiplicative scrambler is recursive and a multiplicative descrambler is non-recursive. Unlike additive scramblers, multiplicative scramblers do not need the frame synchronization, that is why they are also called "self-synchronizing". Multiplicative scrambler/descrambler is defined similarly by a polynomial (for the scrambler on the picture it is formula_2), which is also a "transfer function" of the descrambler.
Comparison of scramblers.
Scramblers have certain drawbacks: 
Noise.
The first voice scramblers were invented at Bell Labs in the period just before World War II. These sets consisted of electronics that could mix two signals or alternatively "subtract" one signal back out again. The two signals were provided by a telephone and a record player. A matching pair of records was produced, each containing the same recording of noise. The recording was played into the telephone, and the mixed signal was sent over the wire. The noise was then subtracted out at the far end using the matching record, leaving the original voice signal intact. Eavesdroppers would hear only the noisy signal, unable to understand the voice.
One of those, used (among other duties) for telephone conversations between Winston Churchill and Franklin D. Roosevelt was intercepted and unscrambled by the Germans. At least one German engineer had worked at Bell Labs before the war and came up with a way to break them. Later versions were sufficiently different that the German team was unable to unscramble them. Early versions were known as "A-3" (from AT&T Corporation). An unrelated device called SIGSALY was used for higher-level voice communications.
The noise was provided on large shellac phonograph records made in pairs, shipped as needed, and destroyed after use. This worked, but was enormously awkward. Just achieving synchronization of the two records proved difficult. Post-war electronics made such systems much easier to work with by creating pseudo-random noise based on a short input tone. In use, the caller would play a tone into the phone, and both scrambler units would then listen to the signal and synchronize to it. This provided limited security, however, as any listener with a basic knowledge of the electronic circuitry could often produce a machine of similar-enough settings to break into the communications.
Cryptographic.
It was the need to synchronize the scramblers that suggested to James H. Ellis the idea for non-secret encryption which ultimately led to the invention of both the RSA encryption algorithm and Diffie–Hellman key exchange well before either was reinvented publicly by Rivest, Shamir, and Adleman, or by Diffie and Hellman.
The latest scramblers are not scramblers in the truest sense of the word, but rather digitizers combined with encryption machines. In these systems the original signal is first converted into digital form, and then the digital data is encrypted and sent. Using modern public-key systems, these "scramblers" are much more secure than their earlier analog counterparts. Only these types of systems are considered secure enough for sensitive data.
Voice inversion scrambling can be as simple as inverting the frequency bands around a static point to various complex methods of changing the inversion point randomly and in real time and using multiple bands.
The "scramblers" used in cable television are designed to prevent casual signal theft - not to provide any real security. Early versions of these devices simply "inverted" one important component of the TV signal, re-inverting it at the client end for display. Later devices were only slightly more complex, filtering out that component entirely and then adding it by examining other portions of the signal. In both cases the circuitry could be easily built by any reasonably knowledgeable hobbyist. "See Television encryption"
Electronic kits for scrambling and descrambling are available from hobbyist suppliers. Scanner enthusiasts often use them to listen in to scrambled communications at car races and some public service transmissions. It is also common in FRS radios. This is an easy way to learn about scrambling.
The term "scrambling" is sometimes incorrectly used when jamming is meant.

</doc>
<doc id="41681" url="https://en.wikipedia.org/wiki?curid=41681" title="Screen">
Screen

Screen or Screens may refer to:

</doc>
<doc id="41682" url="https://en.wikipedia.org/wiki?curid=41682" title="Secondary frequency standard">
Secondary frequency standard

In telecommunications, a secondary frequency standard is a frequency standard that does not have inherent accuracy, and therefore must be calibrated against a primary frequency standard. 
Secondary standards include crystal oscillators and rubidium standards. A crystal oscillator depends for its frequency on its physical dimensions, which vary with fabrication and environmental conditions. A rubidium standard is a secondary standard even though it uses atomic transitions, because it takes the form of a gas cell through which an optical signal is passed. The gas cell has inherent inaccuracies because of gas pressure variations, including those induced by temperature variations. There are also variations in the concentrations of the required buffer gases, which variations cause frequency deviations.

</doc>
<doc id="41684" url="https://en.wikipedia.org/wiki?curid=41684" title="Security">
Security

Security is the degree of resistance to, or protection from, harm. It applies to any vulnerable and valuable asset, such as a person, dwelling, community, item, nation, or organization.
As noted by the Institute for Security and Open Methodologies (ISECOM) in the OSSTMM 3, security provides "a form of protection where a separation is created between the assets and the threat." These separations are generically called "controls," and sometimes include changes to the asset or the threat.
Security is said to have two dialogues. Negative dialogue is about danger, risk, threat and etc. Positive dialogue is about opportunities, interests, profits, and etc. Negative dialogue needs military equipment, armies, or police. Positive dialogue needs social capital, education, or social interaction.
Perceived security compared to real security.
Perception of security may be poorly mapped to measureable objective security. For example, the fear of earthquakes has been reported to be more common than the fear of slipping on the bathroom floor although the latter kills many more people than the former. Similarly, the perceived effectiveness of security measures is sometimes different from the actual security provided by those measures. The presence of security protections may even be taken for security itself. For example, two computer security programs could be interfering with each other and even cancelling each other's effect, while the owner believes s/he is getting double the protection.
Security theater is a critical term for deployment of measures primarily aimed at raising subjective security without a genuine or commensurate concern for the effects of that measure on objective security. For example, some consider the screening of airline passengers based on static databases to have been Security Theater and Computer Assisted Passenger Prescreening System to have created a "decrease" in objective security.
Perception of security can increase objective security when it affects or deters malicious behavior, as with visual signs of security protections, such as video surveillance, alarm systems in a home, or an anti-theft system in a car such as a vehicle tracking system or warning sign. Since some intruders will decide not to attempt to break into such areas or vehicles, there can actually be less damage to windows in addition to protection of valuable objects inside. Without such advertisement, an intruder might, for example, approach a car, break the window, and then flee in response to an alarm being triggered. Either way, perhaps the car itself and the objects inside aren't stolen, but with "perceived security" even the windows of the car have a lower chance of being damaged.
Categorizing security.
There is an immense literature on the analysis and categorization of security. Part of the reason for this is that, in most security systems, the "weakest link in the chain" is the most important. The situation is asymmetric since the 'defender' must cover all points of attack while the attacker need only identify a single weak point upon which to concentrate.
IT realm
Physical realm
Political
Monetary
Security concepts.
Certain concepts recur throughout different fields of security:
Home security.
Home security is something applicable to all of us and involves the hardware in place on a property, and personal security practices. The hardware would be the doors, locks, alarm systems, lighting that is installed on your property. Personal security practices would be ensuring doors are locked, alarms activated, windows closed and many other routine tasks which act to prevent a burglary.
Computer security.
Computer security, also known as cybersecurity or IT security, is security applied to computing devices such as computers and smartphones, as well as computer networks such as private and public networks, including the whole Internet. The field includes all five components: hardware, software, data, people, and procedures by which digital equipment, information and services are protected from unintended or unauthorized access, change or destruction, and is of growing importance due to the increasing reliance of computer systems in most societies. It includes physical security to prevent theft of equipment and information security to protect the data on that equipment. Those terms generally do not refer to physical security, but a common belief among computer security experts is that a physical security breach is one of the worst kinds of security breaches as it generally allows full access to both data and equipment.
Security management in organizations.
In the corporate world, various aspects of security are historically addressed separately - notably by distinct and often noncommunicating departments for IT security, physical security, and fraud prevention. Today there is a greater recognition of the interconnected nature of security requirements, an approach variously known as holistic security, "all hazards" management, and other terms.
Inciting factors in the convergence of security disciplines include the development of digital video surveillance technologies (see Professional video over IP) and the digitization and networking of physical control systems (see SCADA). Greater interdisciplinary cooperation is further evidenced by the February 2005 creation of the Alliance for Enterprise Security Risk Management, a joint venture including leading associations in security (ASIS), information security (ISSA, the Information Systems Security Association), and IT audit (ISACA, the Information Systems Audit and Control Association).
In 2007 the International Organisation for Standardization (ISO) released ISO 28000 - Security Management Systems for the supply chain. Although the title supply chain is included, this Standard specifies the requirements for a security management system, including those aspects critical to security assurance for any organisation or enterprise wishing to manage the security of the organisation and its activities.
ISO 28000 is the foremost risk based security system and is suitable for managing both public and private regulatory security, customs and industry based security schemes and requirements.
See also.
Concepts
Branches

</doc>
<doc id="41685" url="https://en.wikipedia.org/wiki?curid=41685" title="Security kernel">
Security kernel

In telecommunication, the term security kernel has the following meanings: 

</doc>
<doc id="41686" url="https://en.wikipedia.org/wiki?curid=41686" title="Security management">
Security management

Security management is the identification of an organization's assets (including information assets), followed by the development, documentation, and implementation of policies and procedures for protecting these assets.
An organisation uses such security management procedures as information classification, risk assessment, and risk analysis to identify threats, categorise assets, and rate system vulnerabilities so that they can implement effective controls.
Loss prevention.
Loss prevention focuses on what your critical assets are and how you are going to protect them. A key component to loss prevention is assessing the potential threats to the successful achievement of the goal. This must include the potential opportunities that further the object (why take the risk unless there's an upside?) Balance probability and impact determine and implement measures to minimize or eliminate those threats.
Security risk management.
Management of security risks applies the principles of risk management to the management of security threats. It consists of identifying threats (or risk causes), assessing the effectiveness of existing controls to face those threats, determining the risks' consequence(s), prioritizing the risks by rating the likelihood and impact, classifying the type of risk and selecting an appropriate risk option or risk response.
Risk options.
Risk avoidance.
The first choice to be considered. The possibility of eliminating the existence of criminal opportunity or avoiding the creation of such an opportunity is always the best solution, when additional considerations or factors are not created as a result of this action that would create a greater risk. As an example, removing all the cash from a retail outlet would eliminate the opportunity for stealing the cash–but it would also eliminate the ability to conduct business.
Risk reduction.
When avoiding or eliminating the criminal opportunity conflicts with the ability to conduct business, the next step is the reduction of the opportunity and potential loss to the lowest level consistent with the function of the business. In the example above, the application of risk reduction might result in the business keeping only enough cash on hand for one day’s operation.
Risk spreading.
Assets that remain exposed after the application of reduction and avoidance are the subjects of risk spreading. This is the concept that limits loss or potential losses by exposing the perpetrator to the probability of detection and apprehension prior to the consummation of the crime through the application of perimeter lighting, barred windows and intrusion detection systems. The idea here is to reduce the time available to steal assets and escape without apprehension
Risk transfer.
Transferring risks to other alternatives when those risks have not been reduced to acceptable levels. The two primary methods of accomplishing risk transfer are to insure the assets or raise prices to cover the loss in the event of a criminal act. Generally speaking, when the first three steps have been properly applied, the cost of transferring risks is much lower.
Risk acceptance.
All remaining risks must simply be assumed by the business as a risk of doing business. Included with these accepted losses are deductibles which have been made as part of the insurance coverage.

</doc>
<doc id="41687" url="https://en.wikipedia.org/wiki?curid=41687" title="Self-synchronizing code">
Self-synchronizing code

In coding theory, especially in telecommunications, a self-synchronizing code is a uniquely decodable code in which the symbol stream formed by a portion of one code word, or by the overlapped portion of any two adjacent code words, is not a valid code word. Put another way, a set of strings (called "code words") over an alphabet is called a self-synchronizing code if for each string obtained by concatenating two code words, the substring starting at the second symbol and ending at the second-last symbol does not contain any code word as substring. Every self-synchronizing code is a prefix code, but not all prefix codes are self-synchronizing.
Other terms for self-synchronizing code are synchronized code or, ambiguously, comma-free code. A self-synchronizing code permits the proper framing of transmitted code words provided that no uncorrected errors occur in the symbol stream; external synchronization is not required. Self-synchronizing codes also allow recovery from uncorrected errors in the stream; with most prefix codes, an uncorrected error in a single bit may propagate errors further in the stream and make the subsequent data corrupted.
Importance of self-synchronizing codes is not limited to data transmission. Self-synchronization also facilitates some cases of data recovery, for example of a digitally encoded text.
Synchronizing word.
A code over an alphabet has a "synchronizing word" (aka "syncword") in if 
A prefix code is synchronized if and only if it has a synchronizing word.

</doc>
<doc id="41688" url="https://en.wikipedia.org/wiki?curid=41688" title="Semiautomatic switching system">
Semiautomatic switching system

In telecommunication, the term semiautomatic switching system has the following meanings: 

</doc>
<doc id="41690" url="https://en.wikipedia.org/wiki?curid=41690" title="Sensitivity">
Sensitivity

Sensitivity may refer to:

</doc>
<doc id="41691" url="https://en.wikipedia.org/wiki?curid=41691" title="Separate-channel signaling">
Separate-channel signaling

Separate-channel signaling is a form of signaling in which the whole or a part of one or more channels in a multichannel system is used to provide for supervisory and control signals for the message traffic channels. 
The same channels, such as frequency bands or time slots, that are used for signaling are not used for message traffic. 

</doc>
<doc id="41694" url="https://en.wikipedia.org/wiki?curid=41694" title="Service termination point">
Service termination point

In telecommunication, service termination point is the last point of service rendered by a commercial carrier under applicable tariffs.
Usually, the service termination point is on the customer premises and corresponds to the demarcation point. The customer is responsible for equipment and operation from the service termination point to user end instruments.

</doc>
<doc id="41695" url="https://en.wikipedia.org/wiki?curid=41695" title="Shadow loss">
Shadow loss

In telecommunication, the term shadow loss has the following meanings: 

</doc>
<doc id="41696" url="https://en.wikipedia.org/wiki?curid=41696" title="Shannon's law">
Shannon's law

Shannon's law may refer to:

</doc>
<doc id="41697" url="https://en.wikipedia.org/wiki?curid=41697" title="Sheath">
Sheath

Sheath pronounced as , may refer to:

</doc>
<doc id="41698" url="https://en.wikipedia.org/wiki?curid=41698" title="Shield">
Shield

A shield is a type of personal armor, meant to intercept attacks, either by stopping projectiles such as arrows or redirecting a hit from a sword, mace, battle axe or similar weapon to the side of the shield-bearer.
Shields vary greatly in size, ranging from large panels that protect the user's whole body to small models (such as the buckler) that were intended for hand-to-hand-combat use. Shields also vary a great deal in thickness; whereas some shields were made of relatively deep, absorbent, wooden planking to protect soldiers from the impact of spears and crossbow bolts, others were thinner and lighter and designed mainly for deflecting blade strikes.
In prehistory and during the era of the earliest civilizations, shields were made of wood, animal hide, woven reeds or wicker. In classical antiquity, the Migration Period and the Middle Ages, they were normally constructed of poplar tree, lime or another split-resistant timber, covered in some instances with a material such as leather or rawhide and often reinforced with a metal boss, rim or banding. They were carried by foot soldiers, knights and cavalry.
Depending on time and place, shields could be round, oval, square, rectangular, triangular, bilobal or scalloped. Sometimes they took on the form of kites or flatirons, or had rounded tops on a rectangular base with perhaps an eyehole, to look through when used with combat. The shield was held by a central grip or by straps that went over or around the user's arm.
Often shields were decorated with a painted pattern or an animal representation to show their army or clan. These designs developed into systematized heraldic devices during the High Middle Ages for purposes of battlefield identification. Even after the introduction of gunpowder and firearms to the battlefield, shields continued to be used by certain groups. In the 18th century, for example, Scottish Highland fighters liked to wield small shields known as targes, and as late as the 19th century, some non-industrialized peoples (such as Zulu warriors) employed them when waging war.
In the 20th and 21st century, shields have been used by military and police units that specialize in anti-terrorist actions, hostage rescue, riot control and siege-breaking. The modern term usually refers to a device that is held in the hand or attached to the arm, as opposed to an armored suit or a bullet-proof vest. Shields are also sometimes mounted on vehicle-mounted weapons to protect the operator.
Prehistory and antiquity.
The oldest form of shield was a protection device designed to block attacks by hand weapons, such as swords, axes and maces, or ranged weapons like sling-stones and arrows. Shields have varied greatly in construction over time and place. Sometimes shields were made of metal, but wood or animal hide construction was much more common; wicker and even turtle shells have been used. Many surviving examples of metal shields are generally felt to be ceremonial rather than practical, for example the Yetholm-type shields of the Bronze Age, or the Iron Age Battersea shield.
Size and weight varied greatly. Lightly armored warriors relying on speed and surprise would generally carry light shields ("pelte") that were either small or thin. Heavy troops might be equipped with robust shields that could cover most of the body. Many had a strap called a guige that allowed them to be slung over the user's back when not in use or on horseback. During the 14th–13th century BC, the Sards or Shardana, working as mercenaries for the Egyptian pharaoh Ramses II, utilized either large or small round shields against the Hittites. The Mycenaean Greeks used two types of shields: the "figure-of-eight" shield and a rectangular "tower" shield. These shields were made primarily from a wicker frame and then reinforced with leather. Covering the body from head to foot, the figure-of-eight and tower shield offered most of the warrior's body a good deal of protection in head-to-head combat. The Ancient Greek hoplites used a round, bowl-shaped wooden shield that was reinforced with bronze and called an "aspis". Another name for this type of shield is a "hoplon". The hoplon shield inspired the name for hoplite soldiers. The hoplon was also the longest-lasting and most famous and influential of all of the ancient Greek shields. The Spartans used the aspis to create the Greek phalanx formation. Their shields offered protection not only for themselves but for their comrades to their left and right. Examples of Germanic wooden shields circa 350 BC – 500 AD survive from weapons sacrifices in Danish bogs.
The heavily armored Roman legionaries carried large shields ("scuta") that could provide far more protection, but made swift movement a little more difficult. The "scutum" originally had an oval shape, but gradually the curved tops and sides were cut to produce the familiar rectangular shape most commonly seen in the early Imperial legions. Famously, the Romans used their shields to create a tortoise-like formation called a "testudo" in which entire groups of soldiers would be enclosed in an armoured box to provide protection against missiles. Many ancient shield designs featured incuts of one sort or another. This was done to accommodate the shaft of a spear, thus facilitating tactics requiring the soldiers to stand close together forming a wall of shields.
Middle Ages.
Typical in the early European Middle Ages were round shields with light, non-splitting wood like linden, fir, alder or poplar, usually reinforced with leather cover on one or both sides and occasionally metal rims, encircling a metal shield boss. These light shields suited a fighting style where each incoming blow is intercepted with the boss in order to deflect it.
The Normans introduced the kite shield around the 10th century, which was rounded at the top and tapered at the bottom. This gave some protection to the user's legs, without adding too much to the total weight of the shield. 
The kite shield predominantly features enarmes, leather straps used to grip the shield tight to the arm. Used by foot and mounted troops alike, it gradually came to replace the round shield as the common choice until the end of the 12th century, when more efficient limb armour allowed the shields to grow shorter, and be entirely replaced by the 14th century.
As body armour improved, knight's shields became smaller, leading to the familiar heater shield style. Both kite and heater style shields were made of several layers of laminated wood, with a gentle curve in cross section. The heater style inspired the shape of the symbolic heraldic shield that is still used today. Eventually, specialised shapes were developed such as the "bouche", which had a lance rest cut into the upper corner of the lance side, to help guide it in combat or tournament. Free standing shields called pavises, which were propped up on stands, were used by medieval crossbowmen who needed protection while reloading.
In time, some armoured foot knights gave up shields entirely in favour of mobility and two-handed weapons. Other knights and common soldiers adopted the buckler, giving rise to the term "swashbuckler". The buckler is a small round shield, typically between 8 and 16 inches (20–40 cm) in diameter. The buckler was one of very few types of shield that were usually made of metal. Small and light, the buckler was easily carried by being hung from a belt; it gave little protection from missiles and was reserved for hand-to-hand combat where it served both for protection and offence. The buckler's use began in the Middle Ages and continued well into the 16th century.
In Italy, the targa, parma and rotella were used by common people, fencers and even knights. The development of plate armour made shields less and less common as it eliminated the need for a shield. Lightly armoured troops continued to use shields after men-at-arms and knights ceased to use them. Shields continued in use even after gunpowder powered weapons made them essentially obsolete on the battlefield. In the 18th century, the Scottish clans used a small, round targe that was partially effective against the firearms of the time, although it was arguably more often used against British infantry bayonets and cavalry swords in close-in fighting.
During the 19th century, non-industrial cultures with little access to guns were still using war shields. Zulu warriors carried large lightweight shields called Ishlangu made from a single ox hide supported by a wooden spine. This was used in combination with a short spear (assegai) and/or club.
Although the size of a shield would vary due to personal preference and role, most were thin compared to common belief, a misconception aided by the depiction of heavy shields in films. When used in fighting, shields were most effective when used to deflect glancing blows. By deflecting a sword blow to the side, rather than blocking it head on, the attacker could be rendered open to a counterattack. This technique allowed the shield to be made lighter and more easily wielded, while reducing the amount of energy and risk of injury posed to the shield-bearer.
Modern day shields.
Law enforcement shields.
Shields for protection from armed attack are still used by many police forces around the world. These modern shields are usually intended for two broadly distinct purposes. The first type, riot shields, are used for riot control and can be made from metal or polymers such as polycarbonate Lexan or Makrolon or boPET Mylar. These typically offer protection from relatively large and low velocity projectiles, such as rocks and bottles, as well as blows from fists or clubs. Synthetic riot shields are normally transparent, allowing full use of the shield without obstructing vision. Similarly, metal riot shields often have a small window at eye level for this purpose. These riot shields are most commonly used to block and push back crowds when the users stand in a "wall" to block protesters, and to protect against shrapnel, projectiles, molotov cocktails, and during hand-to-hand combat.
The second type of modern police shield is the bullet-resistant tactical shield. These shields are typically manufactured from advanced synthetics such as Kevlar and are designed to be bulletproof, or at least bullet resistant. Two types of shields are available:
Tactical shields often have a firing port so that the officer holding the shield can fire a weapon while being protected by the shield, and they often have a bulletproof glass viewing port. They are typically employed by specialist police, such as SWAT teams in high risk entry and siege scenarios, such as hostage rescue and breaching gang compounds, as well as in antiterrorism operations. Tactical shields often have a large signs stating "POLICE" (or the name of a force, such as "US MARSHALS") to indicate that the user is a law enforcement officer.
Gun shields.
With the widespread use of machine guns in World War I and in subsequent conflicts, battlegrounds were swept with automatic weapons fire. While soldiers who are in foxholes or trenches are protected from this fire, soldiers who are manning mounted machine guns were vulnerable to being hit. Since WW I, there have been a variety of attempts to install armored gun shields on tripod-mounted machine guns or vehicle-mounted weapons to protect machine gunners. Transportation devices with mounted guns that may have gun shields to protect the gunner include jeeps, Humvees, armored cars, and boats.
Non-martial applications.
Many non-martial devices also employ shielding of a kind—not usually a single device worn on an arm but various protective plates or other insulation positioned where needed. Space craft have heat shields to ensure a safe re-entry. Electronics uses shielding to reduce electrical noise and crosstalk between signals. Better-quality patch cables used in audio and electronic music have shielding to reduce interference and noise. People and systems that must work in the presence of ionizing radiation (X-rays) such as dentists, hospital technicians, and patients undergoing X-rays are protected with lead shielding clothing.
Emblems that resemble heraldic shields are also called shields. Movie studio Warner Bros. uses a shield emblazoned with WB as its logo. The Looney Tunes cartoons, released through Warner Bros., open with the WB shield zooming through concentric circles. Also, the Superman logo uses a shield emblazoned with the letter "S".

</doc>
<doc id="41699" url="https://en.wikipedia.org/wiki?curid=41699" title="Shift register">
Shift register

In digital circuits, a shift register is a cascade of flip flops, sharing the same clock, in which the output of each flip-flop is connected to the 'data' input of the next flip-flop in the chain, resulting in a circuit that shifts by one position the 'bit array' stored in it, 'shifting in' the data present at its input and 'shifting out' the last bit in the array, at each transition of the clock input.
More generally, a shift register may be multidimensional, such that its 'data in' and stage outputs are themselves bit arrays: this is implemented simply by running several shift registers of the same bit-length in parallel.
Shift registers can have both parallel and serial inputs and outputs. These are often configured as 'serial-in, parallel-out' (SIPO) or as 'parallel-in, serial-out' (PISO). There are also types that have both serial and parallel input and types with serial and parallel output. There are also 'bidirectional' shift registers which allow shifting in both directions: L→R or R→L. The serial input and last output of a shift register can also be connected to create a 'circular shift register'.
Serial-in and Serial-out (SISO).
Destructive readout.
These are the simplest kind of shift registers. The data string is presented at 'Data In', and is shifted right one stage each time 'Data Advance' is brought high. At each advance, the bit on the far left (i.e. 'Data In') is shifted into the first flip-flop's output. The bit on the far right (i.e. 'Data Out') is shifted out and lost. 
The data are stored after each flip-flop on the 'Q' output, so there are four storage 'slots' available in this arrangement, hence it is a 4-bit Register. To give an idea of the shifting pattern, imagine that the register holds 0000 (so all storage slots are empty). As 'Data In' presents 1,0,1,1,0,0,0,0 (in that order, with a pulse at 'Data Advance' each time—this is called clocking or strobing) to the register, this is the result. The left hand column corresponds to the left-most flip-flop's output pin, and so on.
So the serial output of the entire register is 10110000. It can be seen that if data were to be continued to input, it would get exactly what was put in, but offset by four 'Data Advance' cycles. This arrangement is the hardware equivalent of a queue. Also, at any time, the whole register can be set to zero by bringing the reset (R) pins high.
This arrangement performs "destructive readout" - each datum is lost once it has been shifted out of the right-most bit.
Serial-in, parallel-out (SIPO).
This configuration allows conversion from serial to parallel format. Data is input serially, as described in the SISO section above. Once the data has been clocked in, it may be either read off at each output simultaneously, or it can be shifted out
In this configuration, each flip-flop is edge triggered. The initial flip-flop operates at the given clock frequency. Each subsequent flip-flop halves the frequency of its predecessor, which doubles its duty cycle. As a result, it takes twice as long for the rising/falling edge to trigger each subsequent flip-flop; this staggers the serial input in the time domain, leading to parallel output.
In cases where the parallel outputs should not change during the serial loading process, it is desirable to use a latched or buffered output. In a latched shift register (such as the 74595) the serial data is first loaded into an internal buffer register, then upon receipt of a load signal the state of the buffer register is copied into a set of output registers. In general, the practical application of the serial-in/parallel-out shift register is to convert data from serial format on a single wire to parallel format on multiple wires.
Parallel-in, Serial-out (PISO).
This configuration has the data input on lines D1 through D4 in parallel format, being D1 the MSB. To write the data to the register, the Write/Shift control line must be held LOW. To shift the data, the W/S control line is brought HIGH and the registers are clocked. The arrangement now acts as a SISO shift register, with D1 as the Data Input. However, as long as the number of clock cycles is not more than the length of the data-string, the Data Output, Q, will be the parallel data read off in order. The animation below shows the write/shift sequence, including the internal state of the shift register. 
Uses.
One of the most common uses of a shift register is to convert between serial and parallel interfaces. This is useful as many circuits work on groups of bits in parallel, but serial interfaces are simpler to construct. Shift registers can be used as simple delay circuits. Several bidirectional shift registers could also be connected in parallel for a hardware implementation of a stack.
SIPO registers are commonly attached to the output of microprocessors when more General Purpose Input/Output pins are required than are available. This allows several binary devices to be controlled using only two or three pins, but slower than parallel I/O - the devices in question are attached to the parallel outputs of the shift register, then the desired state of all those devices can be sent out of the microprocessor using a single serial connection. Similarly, PISO configurations are commonly used to add more binary inputs to a microprocessor than are available - each binary input (i.e. a button or more complicated circuitry) is attached to a parallel input of the shift register, then the data is sent back via serial to the microprocessor using several fewer lines than originally required.
Shift registers can also be used as pulse extenders. Compared to monostable multivibrators, the timing has no dependency on component values, however it requires external clock and the timing accuracy is limited by a granularity of this clock. Example: Ronja Twister, where five 74164 shift registers create the core of the timing logic this way (schematic).
In early computers, shift registers were used to handle data processing: two numbers to be added were stored in two shift registers and clocked out into an arithmetic and logic unit (ALU) with the result being fed back to the input of one of the shift registers (the accumulator) which was one bit longer since binary addition can only result in an answer that is the same size or one bit longer. 
Many computer languages include instructions to 'shift right' and 'shift left' the data in a register, effectively dividing by two or multiplying by two for each place shifted.
Very large serial-in serial-out shift registers (thousands of bits in size) were used in a similar manner to the earlier delay line memory in some devices built in the early 1970s. Such memories were sometimes called "circulating memory". For example, the Datapoint 3300 terminal stored its display of 25 rows of 72 columns of upper-case characters using fifty-four 200-bit shift registers, arranged in six tracks of nine packs each, providing storage for 1800 six-bit characters. The shift register design meant that scrolling the terminal display could be accomplished by simply pausing the display output to skip one line of characters.
History.
One of the first known examples of a shift register was in the Mark 2 Colossus, a code-breaking machine built in 1944. It was a six-stage device built of vacuum tubes and thyratrons. A shift register was also used in the IAS machine, built by John von Neumann and others at the Institute for Advanced Study in the late 1940s.

</doc>
<doc id="41700" url="https://en.wikipedia.org/wiki?curid=41700" title="Shot noise">
Shot noise

Shot noise or Poisson noise is a type of electronic noise which can be modeled by a Poisson process.
In electronics shot noise originates from the discrete nature of electric charge. Shot noise also occurs in photon counting in optical devices, where shot noise is associated with the particle nature of light.
Origin.
It is known that in a statistical experiment such as tossing a fair coin and counting the occurrences of heads and tails, the numbers of heads and tails after a great many throws will differ by only a tiny percentage, while after only a few throws outcomes with a significant excess of heads over tails or vice versa are common; if an experiment with a few throws is repeated over and over, the outcomes will fluctuate a lot. (It can be proven that the relative fluctuations reduce as the reciprocal square root of the number of throws, a result valid for all statistical fluctuations, including shot noise.)
Shot noise exists because phenomena such as light and electric current consist of the movement of discrete (also called "quantized") 'packets'. Consider light—a stream of discrete photons—coming out of a laser pointer and hitting a wall to create a visible spot. The fundamental physical processes that govern light emission are such that these photons are emitted from the laser at random times; but the many billions of photons needed to create a spot are so many that the brightness, the number of photons per unit time, varies only infinitesimally with time. However, if the laser brightness is reduced until only a handful of photons hit the wall every second, the relative fluctuations in number of photons, i.e., brightness, will be significant, just as when tossing a coin a few times. These fluctuations are shot noise.
The concept of shot noise was first introduced in 1918 by Walter Schottky who studied fluctuations of current in vacuum tubes.
Shot noise may be dominant when the finite number of particles that carry energy (such as electrons in an electronic circuit or photons in an optical device) is sufficiently small so that uncertainties due to the Poisson distribution, which describes the occurrence of independent random events, are of significance. It is important in electronics, telecommunications, optical detection, and fundamental physics.
The term can also be used to describe any noise source, even if solely mathematical, of similar origin. For instance, particle simulations may produce a certain amount of "noise", where due to the small number of particles simulated, the simulation exhibits undue statistical fluctuations which don't reflect the real-world system. The magnitude of shot noise increases according to the square root of the expected number of events, such as the electric current or intensity of light. But since the strength of the signal itself increases more rapidly, the "relative" proportion of shot noise decreases and the signal to noise ratio (considering only shot noise) increases anyway. Thus shot noise is most frequently observed with small currents or low light intensities that have been amplified.
For large numbers, the Poisson distribution approaches a normal distribution about its mean, and the elementary events (photons, electrons, etc.) are no longer individually observed, typically making shot noise in actual observations indistinguishable from true Gaussian noise. Since the standard deviation of shot noise is equal to the square root of the average number of events "N", the signal-to-noise ratio (SNR) is given by:
Thus when "N" is very large, the signal-to-noise ratio is very large as well, and any "relative" fluctuations in "N" due to other sources are more likely to dominate over shot noise. However when the other noise source is at a fixed level, such as thermal noise, or grows slower than formula_2, increasing "N" (the DC current or light level, etc.) can lead to dominance of shot noise.
Properties.
Electronic devices.
Shot noise in electronic circuits consists of random fluctuations of the electric current in a DC current which originate due to fact that current actually consists of a flow of discrete charges (electrons). Because the electron has such a tiny charge, however, shot noise is of relative insignificance in many (but not all) cases of electrical conduction. For instance 1 ampere of current consists of about electrons per second; even though this number will randomly vary by several billion in any given second, such a fluctuation is minuscule compared to the current itself. In addition, shot noise is often less significant as compared with two other noise sources in electronic circuits, flicker noise and Johnson–Nyquist noise. However, shot noise is temperature and frequency independent, in contrast to Johnson–Nyquist noise, which is proportional to temperature, and flicker noise, with the spectral density decreasing with the frequency. Therefore at high frequencies and low temperatures shot noise may become the dominant source of noise.
With very small currents and considering shorter time scales (thus wider bandwidths) shot noise can be significant. For instance, a microwave circuit operates on time scales of less than a nanosecond and if we were to have a current of 16 nanoamperes that would amount to only 100 electrons passing every nanosecond. According to Poisson statistics the "actual" number of electrons in any nanosecond would vary by 10 electrons rms, so that one sixth of the time less than 90 electrons would pass a point and one sixth of the time more than 110 electrons would be counted in a nanosecond. Now with this small current viewed on this time scale, the shot noise amounts to 1/10 of the DC current itself.
The result by Schottky, based on the assumption that the statistics of electrons passage is Poissonian, reads for the spectral noise density at the frequency formula_3,
where formula_5 is the electron charge, and formula_6 is the average current created by the electron stream. The noise spectral power is frequency independent, which means the noise is white. This is the classical result in the sense that it does not take into account that electrons obey Fermi–Dirac statistics. This can be combined with the Landauer formula, which relates the average current with the transmission eigenvalues formula_7 of the contact through which the current is measured (formula_8 labels transport channels). In the simplest case these transmission eigenvalues can be taken energy independent, the Landauer formula is 
where formula_10 is the applied voltage. This provides for 
commonly referred to as the Poisson value of shot noise, formula_12. The correct result takes into account the quantum statistics of electrons and reads (at zero temperature)
It was obtained in the 1990s by Khlus, Lesovik (independently the single-channel case), and Büttiker (multi-channel case). This noise is white and is always suppressed with respect to the Poisson value. The degree of suppression, formula_14, is known as the Fano factor. Noises produced by different transport channels are independent. Fully open (formula_15) and fully closed (formula_16) channels produce no noise, since there are no irregularities in the electron stream.
At finite temperature, a closed expression for noise can be written as well. It interpolates between shot noise (zero temperature) and Nyquist-Johnson noise (high temperature).
Effects of interactions.
While this is the result when the electrons contributing to the current occur completely randomly, unaffected by each other, there are important cases in which these natural fluctuations are largely suppressed due to a charge build up. Take the previous example in which an average of 100 electrons go from point A to point B every nanosecond. During the first half of a nanosecond we would expect 50 electrons to arrive at point B on the average, but in a particular half nanosecond there might well be 60 electrons which arrive there. This will create a more negative electric charge at point B than average, and that extra charge will tend to "repel" the further flow of electrons from leaving point A during the remaining half nanosecond. Thus the net current integrated over a nanosecond will tend more to stay near its average value of 100 electrons rather than exhibiting the expected fluctuations (10 electrons rms) we calculated. This is the case in ordinary metallic wires and in metal film resistors, where shot noise is almost completely cancelled due to this anti-correlation between the motion of individual electrons, acting on each other through the coulomb force. 
However this reduction in shot noise does not apply when the current results from random events at a potential barrier which all the electrons must overcome due to a random excitation, such as by thermal activation. This is the situation in p-n junctions, for instance. A semiconductor diode is thus commonly used as a noise source by passing a particular DC current through it.
Shot noise is distinct from voltage and current fluctuations expected in thermal equilibrium; this occurs without any applied DC voltage or current flowing. These fluctuations are known as Johnson–Nyquist noise or thermal noise and increase in proportion to the Kelvin temperature of any resistive component. However both are instances of white noise and thus cannot be distinguished simply by observing them even though their origins are quite dissimilar.
Since shot noise is a Poisson process due to the finite charge of an electron, one can compute the root mean square current fluctuations as being of a magnitude
where "q" is the elementary charge of an electron, Δ"f" is the bandwidth in hertz over which the noise is considered, and "I" is the DC current flowing.
For a current of 100 mA, measuring the current noise over a bandwidth of 1 Hz, we obtain
If this noise current is fed through a resistor a noise voltage of
would be generated. Coupling this noise through a capacitor, one could supply a noise power of
to a matched load.
Optics.
In optics, shot noise describes the fluctuations of the number of photons detected (or simply counted in the abstract) due to their occurrence independent of each other. This is therefore another consequence of discretization, in this case of the energy in the electromagnetic field in terms of photons. In the case of photon "detection", the relevant process is the random conversion of photons into photo-electrons for instance, thus leading to a larger effective shot noise level when using a detector with a quantum efficiency below unity. Only in an exotic squeezed coherent state can the number of photons measured per unit time have fluctuations smaller than the square root of the expected number of photons counted in that period of time. Of course there are other mechanisms of noise in optical signals which often dwarf the contribution of shot noise. When these are absent, however, optical detection is said to be "photon noise limited" as only the shot noise (also known as "quantum noise" or "photon noise" in this context) remains.
Shot noise is easily observable in the case of photomultipliers and avalanche photodiodes used in the Geiger mode, where individual photon detections are observed. However the same noise source is present with higher light intensities measured by any photo detector, and is directly measurable when it dominates the noise of the subsequent electronic amplifier. Just as with other forms of shot noise, the fluctuations in a photo-current due to shot noise scale as the square-root of the average intensity: 
The shot noise of a coherent optical beam (having no other noise sources) is a fundamental physical phenomenon, reflecting quantum fluctuations in the electromagnetic field (due to the so-called zero-point energy). This sets a lower bound on the noise introduced by quantum amplifiers which preserve the phase of an optical signal.

</doc>
<doc id="41701" url="https://en.wikipedia.org/wiki?curid=41701" title="Sideband">
Sideband

In radio communications, a sideband is a band of frequencies higher than or lower than the carrier frequency, containing power as a result of the modulation process. The sidebands consist of all the Fourier components of the modulated signal except the carrier. All forms of modulation produce sidebands.
Amplitude modulation of a carrier signal normally results in two mirror-image sidebands. The signal components above the carrier frequency constitute the upper sideband (USB), and those below the carrier frequency constitute the lower sideband (LSB). In conventional AM transmission, the carrier and both sidebands are present, sometimes called double sideband amplitude modulation (DSB-AM).
In some forms of AM, the carrier may be removed, producing double sideband with suppressed carrier (DSB-SC). An example is the stereophonic difference (L-R) information transmitted in stereo FM broadcasting on a 38 kHz subcarrier. The receiver locally regenerates the subcarrier by doubling a special 19 kHz pilot tone, but in other DSB-SC systems, the carrier may be regenerated directly from the sidebands by a Costas loop or squaring loop. This is common in digital transmission systems such as BPSK where the signal is continually present.
If part of one sideband and all of the other remain, it is called vestigial sideband, used mostly with television broadcasting, which would otherwise take up an unacceptable amount of bandwidth. Transmission in which only one sideband is transmitted is called single-sideband transmission or SSB. SSB is the predominant voice mode on shortwave radio other than shortwave broadcasting. Since the sidebands are mirror images, which sideband is used is a matter of convention.
In SSB, the carrier is suppressed, significantly reducing the electrical power (by up to 12 dB) without affecting the information in the sideband. This makes for more efficient use of transmitter power and RF bandwidth, but a beat frequency oscillator must be used at the receiver to reconstitute the carrier. Another way to look at an SSB receiver is as an RF-to-audio frequency transposer: in USB mode, the dial frequency is subtracted from each radio frequency component to produce a corresponding audio component, while in LSB mode each incoming radio frequency component is subtracted from the dial frequency. 
Sidebands can also interfere with adjacent channels. The part of the sideband that would overlap the neighboring channel must be suppressed by filters, before or after modulation (often both). In Broadcast band frequency modulation (FM), subcarriers above 75 kHz are limited to a small percentage of modulation and are prohibited above 99 kHz altogether to protect the ±75 kHz normal deviation and ±100 kHz channel boundaries. Amateur radio and public service FM transmitters generally utilize ±5 kHz deviation.

</doc>
<doc id="41702" url="https://en.wikipedia.org/wiki?curid=41702" title="Signal compression">
Signal compression

Signal compression is the use of various techniques to reduce the amount of data that must be transmitted from one point to another in order to convey information.
Types of signal compression include:

</doc>
<doc id="41703" url="https://en.wikipedia.org/wiki?curid=41703" title="Signaling (telecommunications)">
Signaling (telecommunications)

In telecommunication, signaling (signalling in British English) has the following meanings: 
Signaling systems may be classified based on several principal characteristics.
In-band and out-of-band signaling.
In the public switched telephone network (PSTN), in-band signaling is the exchange of call control information within the same channel that the telephone call itself is using. An example is dual-tone multi-frequency signaling (DTMF), which is used on most telephone lines to customer premises.
Out-of-band signaling is telecommunication signaling on a dedicated channel separate from that used for the telephone call. Out-of-band signaling has been used since Signaling System No. 6 (SS6) was introduced in the 1970s, and also in Signalling System No. 7 (SS7) in 1980 which became the standard for signaling among exchanges ever since.
Line versus register signaling.
Line signaling is concerned with conveying information on the state of the line or channel, such as on-hook, off-hook (answer supervision and disconnect supervision, together referred to as "supervision"), ringing current (alerting), and recall. In the middle 20th century, supervision signals on long-distance trunks in North America were usually inband, for example at 2600 Hz, necessitating a notch filter to prevent interference. Late in the century, all supervisory signals were out of band. With the advent of digital trunks, supervision signals are carried by robbed bits or other bits in the E1-carrier dedicated to signaling.
Register signaling is concerned with conveying addressing information, such as the calling and/or called telephone number. In the early days of telephony, with operator handling calls, the addressing formation is by voice as "Operator, connect me to Mr. Smith please". In the first half of the 20th century, addressing formation is by using a rotary dial, which rapidly breaks the line current into pulses, with the number of pulses conveying the address. Finally, starting in the second half of the century, address signaling is by DTMF.
Channel-associated versus common-channel signaling.
Channel-associated signaling (CAS) employs a signaling channel which is dedicated to a specific bearer channel.
Common-channel signaling (CCS) employs a signaling channel which conveys signaling information relating to multiple bearer channels. These bearer channels therefore have their signaling channel in common.
Compelled signaling.
Compelled signaling refers to signaling where receipt of each signal from an originating register needs to be explicitly acknowledged before the next signal is able to be sent.
Most forms of R2 register signaling are compelled (see R2 signaling), while R1 multi-frequency signaling is not.
The term is only relevant in the case of signaling systems that use discrete signals (e.g. a combination of tones to denote one digit), as opposed to signaling systems which are message-oriented (such as SS7 and ISDN Q.931) where each message is able to convey multiple items of formation (e.g. multiple digits of the called telephone number).
Subscriber versus trunk signaling.
Subscriber signaling refers to signaling between the telephone and the telephone exchange. Trunk signaling is signaling between exchanges.
Classification.
Every signaling system can be characterized along each of the above axes of classification. A few examples:
Whereas common-channel signaling systems are out-of-band by definition, and in-band signaling systems are also necessarily channel-associated, the above metering pulse example demonstrates that there exist channel-associated signaling systems which are out-of-band.

</doc>
<doc id="41705" url="https://en.wikipedia.org/wiki?curid=41705" title="Signal-to-crosstalk ratio">
Signal-to-crosstalk ratio

The signal-to-crosstalk ratio at a specified point in a circuit is the ratio of the power of the wanted signal to the power of the unwanted signal from another channel. 
The signals are adjusted in each channel so that they are of equal power at the zero transmission level point in their respective channels. 
The signal-to-crosstalk ratio is usually expressed in dB. 

</doc>
<doc id="41706" url="https://en.wikipedia.org/wiki?curid=41706" title="Signal-to-noise ratio">
Signal-to-noise ratio

Signal-to-noise ratio (abbreviated SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. It is defined as the ratio of signal power to the noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise. While SNR is commonly quoted for electrical signals, it can be applied to any form of signal (such as isotope levels in an ice core or biochemical signaling between cells).
The signal-to-noise ratio, the bandwidth, and the channel capacity of a communication channel are connected by the Shannon–Hartley theorem.
Signal-to-noise ratio is sometimes used informally to refer to the ratio of useful information to false or irrelevant data in a conversation or exchange. For example, in online discussion forums and other online communities, off-topic posts and spam are regarded as "noise" that interferes with the "signal" of appropriate discussion.
Definition.
Signal-to-noise ratio is defined as the ratio of the power of a signal (meaningful information) and the power of background noise (unwanted signal):
where "P" is average power. Both signal and noise power must be measured at the same or equivalent points in a system, and within the same system bandwidth.
If the variance of the signal and noise are known, and the signal is zero-mean:
If the signal and the noise are measured across the same impedance, then the SNR can be obtained by calculating the square of the amplitude ratio:
where "A" is root mean square (RMS) amplitude (for example, RMS voltage).
Decibels.
Because many signals have a very wide dynamic range, signals are often expressed using the logarithmic decibel scale. Based upon the definition of decibel, signal and noise may be expressed in decibels (dB) as
and
In a similar manner, SNR may be expressed in decibels as
Using the definition of SNR
Using the quotient rule for logarithms
Substituting the definitions of SNR, signal, and noise in decibels into the above equation results in an important formula for calculating the signal to noise ratio in decibels, when the signal and noise are also in decibels:
In the above formula, P is measured in units of power, such as Watts or miliWatts, and signal-to-noise ratio is a pure number.
However, when the signal and noise are measured in Volts or Amperes, which are measures of amplitudes, they must be squared to be proportionate to power as shown below:
Dynamic range.
The concepts of signal-to-noise ratio and dynamic range are closely related. Dynamic range measures the ratio between the strongest un-distorted signal on a channel and the minimum discernible signal, which for most purposes is the noise level. SNR measures the ratio between an arbitrary signal level (not necessarily the most powerful signal possible) and noise. Measuring signal-to-noise ratios requires the selection of a representative or "reference" signal. In audio engineering, the reference signal is usually a sine wave at a standardized nominal or alignment level, such as 1 kHz at +4 dBu (1.228 VRMS).
SNR is usually taken to indicate an "average" signal-to-noise ratio, as it is possible that (near) instantaneous signal-to-noise ratios will be considerably different. The concept can be understood as normalizing the noise level to 1 (0 dB) and measuring how far the signal 'stands out'.
Difference from conventional power.
In physics, the average power of an AC signal is defined as the average value of voltage times current; for resistive (non-reactive) circuits, where voltage and current are in phase, this is equivalent to the product of the rms voltage and current:
But in signal processing and communication, one usually assumes that formula_13 so that factor is usually not included while measuring power or energy of a signal. This may cause some confusion among readers, but the resistance factor is not significant for typical operations performed in signal processing, or for computing power ratios. For most cases, the power of a signal would be considered to be simply
where 'A' is the amplitude of the AC signal.
Alternative definition.
An alternative definition of SNR is as the reciprocal of the coefficient of variation, i.e., the ratio of mean to standard deviation of a signal or measurement:
where formula_16 is the signal mean or expected value and formula_17 is the standard deviation of the noise, or an estimate thereof. Notice that such an alternative definition is only useful for variables that are always non-negative (such as photon counts and luminance). Thus it is commonly used in image processing, where the SNR of an image is usually calculated as the ratio of the mean pixel value to the standard deviation of the pixel values over a given neighborhood. Sometimes SNR is defined as the square of the alternative definition above.
The "Rose criterion" (named after Albert Rose) states that an SNR of at least 5 is needed to be able to distinguish image features at 100% certainty. An SNR less than 5 means less than 100% certainty in identifying image details.
Yet another alternative, very specific and distinct definition of SNR is employed to characterize sensitivity of imaging systems; see Signal-to-noise ratio (imaging).
Related measures are the "contrast ratio" and the "contrast-to-noise ratio".
SNR for various modulation systems.
Amplitude modulation.
Channel signal-to-noise ratio is given by
where W is the bandwidth and formula_19 is modulation index
Output signal-to-noise ratio (of AM receiver) is given by
Frequency modulation.
Channel signal-to-noise ratio is given by
Output signal-to-noise ratio is given by
Improving SNR in practice.
All real measurements are disturbed by noise. This includes electronic noise, but can also include external events that affect the measured phenomenon — wind, vibrations, gravitational attraction of the moon, variations of temperature, variations of humidity, etc., depending on what is measured and of the sensitivity of the device. It is often possible to reduce the noise by controlling the environment. Otherwise, when the characteristics of the noise are known and are different from the signals, it is possible to filter it or to process the signal.
For example, it is sometimes possible to use a lock-in amplifier to modulate and confine the signal within a very narrow bandwidth and then filter the detected signal to the narrow band where it resides, thereby eliminating most of the broadband noise. When the signal is constant or periodic and the noise is random, it is possible to enhance the SNR by averaging the measurement. In this case the noise goes down as the square root of the number of averaged samples.
Digital signals.
When a measurement is digitized, the number of bits used to represent the measurement determines the maximum possible signal-to-noise ratio. This is because the minimum possible noise level is the error caused by the quantization of the signal, sometimes called Quantization noise. This noise level is non-linear and signal-dependent; different calculations exist for different signal models. Quantization noise is modeled as an analog error signal summed with the signal before quantization ("additive noise").
This theoretical maximum SNR assumes a perfect input signal. If the input signal is already noisy (as is usually the case), the signal's noise may be larger than the quantization noise. Real analog-to-digital converters also have other sources of noise that further decrease the SNR compared to the theoretical maximum from the idealized quantization noise, including the intentional addition of dither.
Although noise levels in a digital system can be expressed using SNR, it is more common to use Eb/No, the energy per bit per noise power spectral density.
The modulation error ratio (MER) is a measure of the SNR in a digitally modulated signal.
Fixed point.
For "n"-bit integers with equal distance between quantization levels (uniform quantization) the dynamic range (DR) is also determined.
Assuming a uniform distribution of input signal values, the quantization noise is a uniformly distributed random signal with a peak-to-peak amplitude of one quantization level, making the amplitude ratio 2"n"/1. The formula is then:
This relationship is the origin of statements like "16-bit audio has a dynamic range of 96 dB". Each extra quantization bit increases the dynamic range by roughly 6 dB.
Assuming a full-scale sine wave signal (that is, the quantizer is designed such that it has the same minimum and maximum values as the input signal), the quantization noise approximates a sawtooth wave with peak-to-peak amplitude of one quantization level and uniform distribution. In this case, the SNR is approximately
Floating point.
Floating-point numbers provide a way to trade off signal-to-noise ratio for an increase in dynamic range. For n bit floating-point numbers, with n-m bits in the mantissa and m bits in the exponent:
Note that the dynamic range is much larger than fixed-point, but at a cost of a worse signal-to-noise ratio. This makes floating-point preferable in situations where the dynamic range is large or unpredictable. Fixed-point's simpler implementations can be used with no signal quality disadvantage in systems where dynamic range is less than 6.02m. The very large dynamic range of floating-point can be a disadvantage, since it requires more forethought in designing algorithms.
Optical SNR.
Optical signals have a carrier frequency that is much higher than the modulation frequency (about 200 THz and more). This way the noise covers a bandwidth that is much wider than the signal itself. The resulting signal influence relies mainly on the filtering of the noise. To describe the signal quality without taking the receiver into account, the optical SNR (OSNR) is used. The OSNR is the ratio between the signal power and the noise power in a given bandwidth. Most commonly a reference bandwidth of 0.1 nm is used. This bandwidth is independent of the modulation format, the frequency and the receiver. For instance an OSNR of 20dB/0.1 nm could be given, even the signal of 40 GBit DPSK would not fit in this bandwidth. OSNR is measured with an optical spectrum analyzer.
Types and abbreviations.
Signal to noise ratio may be abbreviated as SNR and less commonly as S/N. PSNR stands for Peak signal-to-noise ratio. GSNR stands for Geometric Signal-to-Noise Ratio. SINR is the Signal-to-noise-plus-interference ratio.

</doc>
<doc id="41707" url="https://en.wikipedia.org/wiki?curid=41707" title="Signal transition">
Signal transition

Signal transition, when referring to the modulation of a carrier signal, is a change from one significant condition to another. 
Examples of signal transitions are a change from one electric current, voltage, or power level to another; a change from one optical power level to another; a phase shift; or a change from one frequency or wavelength to another. 
Signal transitions are used to create signals that represent information, such as "0" and "1" or "mark" and "space".

</doc>
<doc id="41710" url="https://en.wikipedia.org/wiki?curid=41710" title="Simple Network Management Protocol">
Simple Network Management Protocol

Simple Network Management Protocol (SNMP) is an Internet-standard protocol for collecting and organizing information about managed devices on IP networks and for modifying that information to change device behavior. Devices that typically support SNMP include routers, switches, servers, workstations, printers, modem racks and more.
SNMP is widely used in network management systems to monitor network-attached devices for conditions that warrant administrative attention. SNMP exposes management data in the form of variables on the managed systems, which describe the system configuration. These variables can then be queried (and sometimes set) by managing applications.
SNMP is a component of the Internet Protocol Suite as defined by the Internet Engineering Task Force (IETF). It consists of a set of standards for network management, including an application layer protocol, a database schema, and a set of data objects.
Overview and basic concepts.
In typical uses of SNMP one or more administrative computers, called "managers", have the task of monitoring or managing a group of hosts or devices on a computer network. Each managed system executes, at all times, a software component called an "agent" which reports information via SNMP to the manager.
An SNMP-managed network consists of three key components:
A "managed device" is a network node that implements an SNMP interface that allows unidirectional (read-only) or bidirectional (read and write) access to node-specific information. Managed devices exchange node-specific information with the NMSs. Sometimes called network elements, the managed devices can be any type of device, including, but not limited to, routers, access servers, switches, cable modems, bridges, hubs, IP telephones, IP video cameras, computer hosts, and printers.
An "agent" is a network-management software module that resides on a managed device. An agent has local knowledge of management information and translates that information to or from an SNMP-specific form.
A "network management station" (NMS) executes applications that monitor and control managed devices. NMSes provide the bulk of the processing and memory resources required for network management. One or more NMSes may exist on any managed network.
Management information base (MIB).
SNMP agents expose management data on the managed systems as variables. The protocol also permits active management tasks, such as modifying and applying a new configuration through remote modification of these variables. The variables accessible via SNMP are organized in hierarchies. These hierarchies, and other metadata (such as type and description of the variable), are described by Management Information Bases (MIBs).
SNMP itself does not define which information (which variables) a managed system should offer. Rather, SNMP uses an extensible design, where the available information is defined by management information bases (MIBs). MIBs describe the structure of the management data of a device subsystem; they use a hierarchical namespace containing object identifiers (OID). Each OID identifies a variable that can be read or set via SNMP. MIBs use the notation defined by Structure of Management Information Version 2.0 (SMIv2, RFC 2578), a subset of ASN.1.
Protocol details.
SNMP operates in the Application Layer of the Internet Protocol Suite (Layer 7 of the OSI model). The SNMP agent receives requests on UDP port 161. The manager may send requests from any available source port to port 161 in the agent. The agent response will be sent back to the source port on the manager. The manager receives notifications ("Traps" and "InformRequests") on port 162. The agent may generate notifications from any available port. When used with Transport Layer Security or Datagram Transport Layer Security requests are received on port 10161 and traps are sent to port 10162.
SNMPv1 specifies five core protocol data units (PDUs). Two other PDUs, "GetBulkRequest" and "InformRequest" were added in SNMPv2 and the "Report" PDU was added in SNMPv3.
All SNMP PDUs are constructed as follows:
The seven SNMP protocol data unit (PDU) types are as follows:
Development and usage.
Version 1.
SNMP version 1 (SNMPv1) is the initial implementation of the SNMP protocol. SNMPv1 operates over protocols such as User Datagram Protocol (UDP), Internet Protocol (IP), OSI Connectionless Network Service (CLNS), AppleTalk Datagram-Delivery Protocol (DDP), and Novell Internet Packet Exchange (IPX). SNMPv1 is widely used and is the de facto network-management protocol in the Internet community.
The first Requests for Comments (RFC)s for SNMP, now known as SNMPv1, appeared in 1988:
These protocols were obsoleted by:
After a short time, RFC 1156 (MIB-1) was replaced by the more often used:
Version 1 has been criticized for its poor security. Authentication of clients is performed only by a "community string", in effect a type of password, which is transmitted in cleartext. The '80s design of SNMP V1 was done by a group of collaborators who viewed the officially sponsored OSI/IETF/NSF (National Science Foundation) effort (HEMS/CMIS/CMIP) as both unimplementable in the computing platforms of the time as well as potentially unworkable. SNMP was approved based on a belief that it was an interim protocol needed for taking steps towards large scale deployment of the Internet and its commercialization. In that time period Internet-standard authentication/security was both a dream and discouraged by focused protocol design groups.
Version 2.
SNMPv2 (RFC 1441–RFC 1452), revises version 1 and includes improvements in the areas of performance, security, confidentiality, and manager-to-manager communications. It introduced "GetBulkRequest", an alternative to iterative GetNextRequests for retrieving large amounts of management data in a single request. However, the new party-based security system in SNMPv2, viewed by many as overly complex, was not widely accepted. This version of SNMP reached the Proposed Standard level of maturity, but was deemed obsoleted by later versions.
"Community-Based Simple Network Management Protocol version 2", or "SNMPv2c", is defined in RFC 1901–RFC 1908. SNMPv2c comprises SNMPv2 "without" the controversial new SNMP v2 security model, using instead the simple community-based security scheme of SNMPv1. This version is one of relatively few standards to meet the IETF's Draft Standard maturity level, and was widely considered the "de facto" SNMPv2 standard. It too was later obsoleted, by SNMPv3.
"User-Based Simple Network Management Protocol version 2", or "SNMPv2u", is defined in RFC 1909–RFC 1910. This is a compromise that attempts to offer greater security than SNMPv1, but without incurring the high complexity of SNMPv2. A variant of this was commercialized as "SNMP v2*", and the mechanism was eventually adopted as one of two security frameworks in SNMP v3.
SNMPv1 & SNMPv2c interoperability.
As presently specified, SNMPv2c is incompatible with SNMPv1 in two key areas: message formats and protocol operations. SNMPv2c messages use different header and protocol data unit (PDU) formats from SNMPv1 messages. SNMPv2c also uses two protocol operations that are not specified in SNMPv1. Furthermore, RFC 2576 defines two possible SNMPv1/v2c coexistence strategies: proxy agents and bilingual network-management systems.
Proxy agents.
An SNMPv2 agent can act as a proxy agent on behalf of SNMPv1 managed devices, as follows:
The proxy agent maps SNMPv1 trap messages to SNMPv2 trap messages and then forwards them to the NMS.
Bilingual network-management system.
Bilingual SNMPv2 network-management systems support both SNMPv1 and SNMPv2. To support this dual-management environment, a management application in the bilingual NMS must contact an agent. The NMS then examines information stored in a local database to determine whether the agent supports SNMPv1 or SNMPv2. Based on the information in the database, the NMS communicates with the agent using the appropriate version of SNMP.
Version 3.
Although SNMPv3 makes no changes to the protocol aside from the addition of cryptographic security, it looks much different due to new textual conventions, concepts, and terminology.
SNMPv3 primarily added security and remote configuration enhancements to SNMP.
Due to lack of security with the use of SNMP, network administrators were using other means, such as telnet for configuration, accounting, and fault management.
SNMPv3 addresses issues related to the large-scale deployment of SNMP, accounting, and fault management. Currently, SNMP is predominantly used for monitoring and performance management.
SNMPv3 defines a secure version of SNMP and also facilitates remote configuration of the SNMP entities.
SNMPv3 provides a secure environment for the management of systems covering the following:
SNMPv3 focuses on two main aspects, namely security and administration. The security aspect is addressed by offering both strong authentication and data encryption for privacy. The administration aspect is focused on two parts, namely notification originators and proxy forwarders.
SNMPv3 defines a number of security-related capabilities. The initial specifications defined the USM and VACM, which were later followed by a transport security model that provided support for SNMPv3 over SSH and SNMPv3 over TLS and DTLS.
Security has been the biggest weakness of SNMP since the beginning. Authentication in SNMP Versions 1 and 2 amounts to nothing more than a password (community string) sent in clear text between a manager and agent.
Each SNMPv3 message contains security parameters which are encoded as an octet string. The meaning of these security parameters depends on the security model being used.
SNMPv3 provides important security features:
In practice, SNMP implementations often support multiple versions: typically SNMPv1, SNMPv2c, and SNMPv3.
Implementation issues.
SNMP implementations vary across platform vendors. In some cases, SNMP is an added feature, and is not taken seriously enough to be an element of the core design. Some major equipment vendors tend to over-extend their proprietary command line interface (CLI) centric configuration and control systems.
SNMP's seemingly simple tree structure and linear indexing may not always be understood well enough within the internal data structures that are elements of a platform's basic design. Consequently, processing SNMP queries on certain data sets may result in higher CPU utilization than necessary. One example of this would be large routing tables, such as BGP or IGP.
Some SNMP values (especially tabular values) require specific knowledge of table indexing schemes, and these index values are not necessarily consistent across platforms. This can cause correlation issues when fetching information from multiple devices that may not employ the same table indexing scheme (for example fetching disk utilization metrics, where a specific disk identifier is different across platforms.)
Resource indexing.
Modular devices may dynamically increase or decrease their SNMP indices (a.k.a. instances) whenever slotted hardware is added or removed. Although this is most common with hardware, virtual interfaces have the same effect. Index values are typically assigned at boot time and remain fixed until the next reboot. Hardware or virtual entities added while the device is 'live' may have their indices assigned at the end of the existing range and possibly reassigned at the next reboot. Network inventory and monitoring tools need to have the device update capability by properly reacting to the cold start trap from the device reboot in order to avoid corruption and mismatch of polled data.
Index assignments for a SNMP device instance may change from poll to poll mostly as a result of changes initiated by the system administrator. If information is needed for a particular interface, it is imperative to determine the SNMP index before retrieving the data needed. Generally, a description table like ifDescr will map a user friendly name like Serial 0/1 (Blade 0, port 1) to an SNMP index. However, this is not necessarily the case for a specific SNMP value, and can be arbitrary for an SNMP implementation.
Security implications.
A person who is unfamiliar with the SNMP design rationale and/or cryptography, may ask why a challenge-response handshake was not used to improve security. The reasons are:
Autodiscovery.
SNMP by itself is simply a protocol for collecting and organizing information about managed devices (network and device monitoring), and modifying that information on these devices, causing change in their behavior (network management). Most toolsets implementing SNMP offer some form of discovery mechanism, a standardized collection of data common to most platforms and devices, to get a new user or implementor started. One of these features is often a form of automatic discovery, where new devices discovered in the network are polled automatically. For SNMPv1 and SNMPv2c, this presents a security risk, in that SNMP read communities will be broadcast in cleartext to the target device. SNMPv3 mitigates this risk, however it does not protect against traffic analysis and potential network topology discovery by an adversary. While security requirements and risk profiles vary from organization to organization, care should be taken when using a feature like automatic discovery, especially in mixed-tenant datacenters, server hosting and colocation facilities, and similar environments.

</doc>
<doc id="41712" url="https://en.wikipedia.org/wiki?curid=41712" title="Simplex signaling">
Simplex signaling

Simplex signaling (SX) is signaling in which two conductors are used for a single channel, and a center-tapped coil, or its equivalent, is used to split the signaling current equally between the two conductors. The return path for the current is through ground. 
It is distinct from a phantom circuit in which the return current path for power or signaling is provided through different signal conductors.
SX signaling may be one-way, for intra-central-office use, or the simplex legs may be connected to form full duplex signaling circuits that function like composite (CX) signaling circuits with E&M lead control.
Simplex is also used to describe a powering method where one or more signal conductors carries direct current to power a remote device, which sends its output signal back on the same conductor. Phantom powering as used in audio is a form of simplex powering, as the return current flows through the ground or shield conductor.

</doc>
<doc id="41714" url="https://en.wikipedia.org/wiki?curid=41714" title="SINAD">
SINAD

Signal-to-noise and distortion ratio (SINAD) is a measure of the quality of a signal from a communications device, often defined as:
where formula_2 is the average power of the signal, noise and distortion components. SINAD is usually expressed in dB and is quoted alongside the receiver RF sensitivity, to give a quantitative evaluation of the receiver sensitivity. Note that with this definition, unlike SNR, a SINAD reading can never be less than 1 (i.e. it is always positive when quoted in dB).
When calculating the distortion, it is common to exclude the DC components.
Due to widespread use, SINAD has collected several different definitions. SINAD is commonly defined as:
Regardless of the exact definition, it is always true that a lower SINAD value means worse performance of the system. As the received RF signal becomes weaker it becomes progressively lost in the noise and distortion generated by receiver, demodulation and audio output drive circuits. By convention, the minimum acceptable SINAD level that will not swamp intelligible speech is 12dB for a narrow band FM voice radio system.
Commercial radio specifications.
A typical example, quoted from a commercial hand held VHF or UHF radio, might be:
which is stating that the receiver will produce intelligible speech with a signal at its input as low as 0.25μV. Radio receiver designers will test the product in a laboratory following a procedure. A typical example procedure is as follows:
According to the radio designer, intelligible speech can be detected 12dB above the receiver's noise floor (noise and distortion). Regardless on how accurate this output power is regarding intelligible speech, having a standard output SINAD allows for easy comparison between radio receiver input sensitivities. This value is typical for VHF commercial radio while 0.35μV is probably more typical for UHF. In the real world lower SINAD values (more noise) can still result in intelligible speech but it is tiresome work to listen to a voice in that much noise.

</doc>
<doc id="41715" url="https://en.wikipedia.org/wiki?curid=41715" title="Single-frequency signaling">
Single-frequency signaling

Single-frequency signaling (SF) is line signaling (in telephony) in which dial pulses or supervisory signals are conveyed by a single voice-frequency tone in each direction. SF and similar systems were used in 20th-century carrier systems.
An SF signaling unit converts DC signaling (usually, at least in long distance circuits, E&M signaling) to a format (characterized by the presence or absence of a single voice-frequency tone), which is suitable for transmission over an AC path, "e.g.", a carrier system. The SF tone is present in the on-hook or idle state and absent during the seized state. In the seized state, dial pulses are conveyed by bursts of SF tone, corresponding to the interruptions in dc continuity created by a rotary dial or other DC dialing mechanism. 
The SF tone may occupy a small portion of the user data channel spectrum, "e.g.," 1600 Hz or 2600 Hz (SF "in-band signaling)". There may be a notch filter at the precise SF frequency, either filtering the circuit at all times or only when the circuit is off-hook, to prevent the user from inadvertently disconnecting a call if the users voice has a sufficiently strong spectral content at the SF frequency, a falsing condition known as "talk-off". Notoriously, this property was exploited by blue boxers and other toll fraudsters. The SF tone may also be just outside the user voice band, "e.g.," 3600 Hz. 
The Defense Data Network (DDN) transmitted DC line signaling pulses or supervisory signals, or both, over carrier channels or cable pairs on a four wire circuit basis using a 2600 Hz signal tone. The conversion into tones, or vice versa, is done by SF signal units.
SF was developed in the early 20th century and standardized in middle century. It declined in the 1970s due to the adoption of T-carrier, and was largely abandoned late in the century in favor of common-channel signaling.

</doc>
<doc id="41716" url="https://en.wikipedia.org/wiki?curid=41716" title="Single-mode optical fiber">
Single-mode optical fiber

[[Image:Singlemode fibre structure.svg|thumb|right|The structure of a typical single-mode fiber.
1. Core 8 µm diameter
2. Cladding 125 µm dia.
3. Buffer 250 µm dia.
4. Jacket 400 µm dia.]]
In fiber-optic communication, a single-mode optical fiber (SMF) is an optical fiber designed to carry light only directly down the fiber - the transverse mode. Modes are the possible solutions of the Helmholtz equation for waves, which is obtained by combining Maxwell's equations and the boundary conditions. These modes define the way the wave travels through space, i.e. how the wave is distributed in space. Waves can have the same mode but have different frequencies. This is the case in single-mode fibers, where we can have waves with different frequencies, but of the same mode, which means that they are distributed in space in the same way, and that gives us a single ray of light. Although the ray travels parallel to the length of the fiber, it is often called transverse mode since its electromagnetic vibrations occur perpendicular (transverse) to the length of the fiber. The 2009 Nobel Prize in Physics was awarded to Charles K. Kao for his theoretical work on the single-mode optical fiber.
History.
At the Corning Glass Works (now Corning Inc.), Robert Maurer, Donald Keck and Peter Schultz started with fused silica, a material that can be made extremely pure, but has a high melting point and a low refractive index. They made cylindrical preforms by depositing purified materials from the vapor phase, adding carefully controlled levels of dopants to make the refractive index of the core slightly higher than that of the cladding, without raising attenuation dramatically. In September 1970, they announced they had made single-mode fibers with attenuation at the 633-nanometer helium-neon line below 20 dB/km.
Professor Huang Hongjia of the Chinese Academy of Sciences, developed coupling wave theory in the field of microwave theory. He led a research team that successfully developed Single-mode optical fiber in 1980.
Characteristics.
Like multi-mode optical fibers, single mode fibers do exhibit modal dispersion resulting from multiple spatial modes but with narrower modal dispersion. Single mode fibers are therefore better at retaining the fidelity of each light pulse over longer distances than multi-mode fibers. For these reasons, single-mode fibers can have a higher bandwidth than multi-mode fibers. Equipment for single mode fiber is more expensive than equipment for multi-mode optical fiber, but the single mode fiber itself is usually cheaper in bulk. 
A typical single mode optical fiber has a core diameter between 8 and 10.5 µm and a cladding diameter of 125 µm. There are a number of special types of single-mode optical fiber which have been chemically or physically altered to give special properties, such as dispersion-shifted fiber and nonzero dispersion-shifted fiber. Data rates are limited by polarization mode dispersion and chromatic dispersion. , data rates of up to 10 gigabits per second were possible at distances of over with commercially available transceivers (Xenpak). By using optical amplifiers and dispersion-compensating devices, state-of-the-art DWDM optical systems can span thousands of kilometers at 10 Gbit/s, and several hundred kilometers at 40 Gbit/s.
The lowest-order bounds mode is ascertained for the wavelength of interest by solving Maxwell's equations for the boundary conditions imposed by the fiber, which are determined by the core diameter and the refractive indices of the core and cladding. The solution of Maxwell's equations for the lowest order bound mode will permit a pair of orthogonally polarized fields in the fiber, and this is the usual case in a communication fiber.
In step-index guides, single-mode operation occurs when the normalized frequency, "V", is less than or equal to 2.405. For power-law profiles, single-mode operation occurs for a normalized frequency, "V", less than approximately 
where "g" is the profile parameter.
In practice, the orthogonal polarizations may not be associated with degenerate modes.
OS1 and OS2 are standard single-mode optical fiber used with wavelengths 1310 nm and 1550 nm (size 9/125 µm) with a maximum attenuation of 1 dB/km (OS1) and .4 dB/km (OS2). OS1 is defined in ISO/IEC 11801, and OS2 is defined in ISO/IEC 24702.
Connectors.
Optical fiber connectors are used to join optical fibers where a connect/disconnect capability is required. The basic connector unit is a connector assembly. A connector assembly consists of an adapter and two connector plugs. 
Due to the sophisticated polishing and tuning procedures that may be incorporated into optical connector manufacturing, connectors are generally assembled onto optical fiber in a supplier’s manufacturing facility. However, the assembly and
polishing operations involved can be performed in the field, for example to make cross-connect jumpers to size.
Optical fiber connectors are used in telephone company central offices, at installations on customer premises, and in outside plant applications. Their uses include:
Outside plant applications may involve locating connectors underground in subsurface enclosures that may be subject to flooding, on outdoor walls, or on utility poles. The closures that enclose them may be hermetic, or may be “free-breathing.” Hermetic closures will prevent the connectors within being subjected to temperature swings unless they are breached. Free-breathing enclosures will subject them to temperature and humidity swings, and possibly to condensation and biological action from airborne bacteria, insects, etc. Connectors in the underground plant may be subjected to groundwater immersion if the closures containing them are breached or improperly assembled.
The latest industry requirements for optical fiber connectors are in Telcordia GR-326, "Generic Requirements for Singlemode Optical Connectors and Jumper Assemblies".
A "multi-fiber" optical connector is designed to simultaneously join multiple optical fibers together, with each optical fiber being joined to only one other optical fiber.
The last part of the definition is included so as not to confuse multi-fiber connectors with a branching component, such as a coupler. The latter joins one optical fiber to two or more other optical fibers.
Multi-fiber optical connectors are designed to be used wherever quick and/or repetitive connects and disconnects of a group of fibers are needed. Applications include telecommunications companies’ Central Offices (COs), installations on customer premises, and Outside Plant (OSP) applications.
The multi-fiber optical connector can be used in the creation of a low-cost switch for use in fiber optical testing. Another application is in cables delivered to a user with pre-terminated multi-fiber jumpers. This would reduce the need for field splicing, which could greatly reduce the amount of hours necessary for placing an optical fiber cable in a telecommunications network. This, in turn, would result in savings for the installer of such cable.
Industry requirements for multi-fiber optical connectors are covered in GR-1435, "Generic Requirements for Multi-Fiber Optical Connectors".
Fiber Optic Switches.
An optical switch is a component with two or more ports that selectively transmits, redirects, or blocks an optical signal in a transmission medium. According to Telcordia GR-1073, an optical switch must be actuated to select or change between states. The actuating signal (also referred to as the control signal) is usually electrical, but in principle, could be optical or mechanical. (The control signal format may be Boolean and may be a separate signal; or, in the case of optical actuation, the control signal may be encoded in the input data signal. Switch performance is generally intended to be independent of wavelength within the component passband.)

</doc>
<doc id="41717" url="https://en.wikipedia.org/wiki?curid=41717" title="S interface">
S interface

The S interface or S reference point, also known as S0, is a User–network interface reference point for basic rate access in an Integrated Services Digital Network (ISDN) environment, that
The S interface is electrically equivalent to the T interface, and the two are jointly referred to as the S/T interface.

</doc>
<doc id="41718" url="https://en.wikipedia.org/wiki?curid=41718" title="Skew">
Skew

Skew may refer to:
In mathematics:
In statistics:
In chemistry
In optics:
In engineering:
In Roofing':
Sometimes reinforced with slate fragments on laying.
In finance:
In telecommunications:
In computers:
In aviation:
In fantasy baseball:
In carpentry or construction:
See also.
SKU refers to a Stock-keeping unit, a unique identifier for each distinct product and service that can be purchased in business.

</doc>
<doc id="41719" url="https://en.wikipedia.org/wiki?curid=41719" title="Skip zone">
Skip zone

A skip zone, also called a silent zone or zone of silence, is a region where a radio transmission can not be received. The zone is located between regions both closer and farther from the transmitter where reception is possible.
When using medium to high frequency radio telecommunication, there are radio waves which travel both parallel to the ground, and towards the ionosphere, referred to as a ground wave and sky wave, respectively. A skip zone is an annular region between the farthest points at which the ground wave can be received and the nearest point at which the refracted sky waves can be received. Within this region, no signal can be received because, due to the conditions of the local ionosphere, the relevant sky waves are not reflected but penetrate the ionosphere. 
The skip zone is a natural phenomenon that cannot be influenced by technical means. Its width depends on the height and shape of the ionosphere and, particularly, on the local ionospheric maximum electron density characterized by critical frequency foF2. It varies mainly with this parameter, being larger for low foF2. With a fixed working frequency it is large by night and may even disappear by day. Transmitting at night is most effective for long distance communication but the skip zone becomes significantly larger. 
Very high frequency waves and higher normally travel through the ionosphere wherefore communication via skywave is exceptional. A highly ionized Es-Layer that occasionally may appear in Summer may produce such an example.
Another method of decreasing the skip zone is by decreasing the frequency of the radio waves. Decreasing the frequency is akin to increasing the ionospheric width. A point is eventually reached when decreasing the frequency results in a zero distance skip zone. In other words, a frequency exists for which vertically incident radio waves will always be refracted back to the Earth. This frequency is equivalent to the ionospheric plasma frequency and is also known as the ionospheric critical frequency, or foF2.
Skip zone is the subject of a film 'SKIPZONE' made in 1992 by UK artist, Peter Lee-Jones. It refers to areas in Scottish Highlands where it is difficult to obtain radio and TV reception.

</doc>
<doc id="41720" url="https://en.wikipedia.org/wiki?curid=41720" title="Slant range">
Slant range

In radio electronics, especially radar terminology, slant range is the line-of-sight distance between two points which are not at the same level relative to a specific datum.
An example of slant range is the distance to an aircraft flying at high altitude with respect to that of the radar antenna. The slant range (1) is the hypotenuse of the triangle represented by the altitude of the aircraft and the distance between the radar antenna and the aircraft's ground track (point (3) on the earth directly below the aircraft). In the absence of altitude information, for example from a height finder, the aircraft location would be plotted farther (2) from the antenna than its actual ground track.

</doc>
<doc id="41721" url="https://en.wikipedia.org/wiki?curid=41721" title="Slave clock">
Slave clock

In telecommunication and horology, a slave clock is a clock that depends for its accuracy on another clock, a master clock. Many modern clocks are synchronized, either through the Internet or by radio time signals, to a worldwide time standard called Coordinated Universal Time (UTC) based on a network of master atomic clocks in many countries. For scientific purposes, precision clocks can be synchronized to within a few nanoseconds by dedicated satellite channels. Slave clock synchronization is usually achieved by phase-locking the slave clock signal to a signal received from the master clock. To adjust for the transit time of the signal from the master clock to the slave clock, the phase of the slave clock may be adjusted with respect to the signal from the master clock so that both clocks are in phase. Thus, the time markers of both clocks, at the output of the clocks, occur simultaneously.
Before the computer era, the term referred to satellite electrical clocks that are synchronized periodically by an electrical pulse issued by a master clock. From the late 19th to the mid 20th centuries, electrical master/slave clock systems were widely used in public buildings and business offices, with all the clocks in the building synchronized through electric wires to a central master clock.
These older styles of slave clocks either keep time by themselves, and are periodically corrected by the master clock, or require impulses from the master clock to advance. Many slave clocks of these types remain in operation, most commonly in schools.
Pictures.
Mechanical slave clocks from the 1950s and 1960s era.

</doc>
<doc id="41722" url="https://en.wikipedia.org/wiki?curid=41722" title="Slave station">
Slave station

Slave station may refer to:

</doc>
<doc id="41724" url="https://en.wikipedia.org/wiki?curid=41724" title="Slip">
Slip

Slip or SLIP may refer to:

</doc>
<doc id="41725" url="https://en.wikipedia.org/wiki?curid=41725" title="Spatial application">
Spatial application

A spatial application is a technological application (such as video) requiring high spatial resolution, possibly at the expense of reduced temporal positioning accuracy, "i.e.," increased jerkiness. 
Examples of spatial applications include the requirement to display small characters and to resolve fine detail in still video, or in motion video that contains very limited motion.

</doc>
<doc id="41727" url="https://en.wikipedia.org/wiki?curid=41727" title="Specific detectivity">
Specific detectivity

Specific detectivity, or D*, for a photodetector is a figure of merit used to characterize performance, equal to the reciprocal of noise-equivalent power (NEP), normalized per square root of the sensor's area and frequency bandwidth (reciprocal of twice the integration time).
Specific detectivity is given by formula_1, where formula_2 is the area of the photosensitive region of the detector and formula_3 is the frequency bandwidth. It is commonly expressed in "Jones" units (formula_4) in honor of Robert Clark Jones who originally defined it.
Given that noise-equivalent power can be expressed as a function of the responsivity formula_5 (in units of formula_6 or formula_7) and the noise spectral density formula_8 (in units of formula_9 or formula_10) as formula_11, it's common to see the specific detectivity expressed as formula_12.
It is often useful to express the specific detectivity in terms of relative noise levels present in the device. A common expression is given below.
formula_13
With "q" as the electronic charge, formula_14 is the wavelength of interest, "h" is Planck's constant, "c" is the speed of light, "k" is Boltzmann's constant, "T" is the temperature of the detector, formula_15 is the zero-bias dynamic resistance area product (often measured experimentally, but also expressible in noise level assumptions), formula_16 is the quantum efficiency of the device, and formula_17 is the total flux of the source (often a blackbody) in photons/sec/cm².
Detectivity measurement.
Detectivity can be measured from a suitable optical setup using known parameters.
You will need a known light source with known irradiance at a given standoff distance. The incoming light source will be chopped at a certain frequency, and then each wavelet will be integrated over a given time constant over a given number of frames.
In detail, we compute the bandwidth formula_18directly from the integration time constant formula_19.
formula_20
Next, an rms signal and noise needs to be measured from a set of formula_21 frames. This is done either directly by the instrument, or done as post-processing.
formula_22
formula_23
Now, the computation of the radiance formula_24 in W/sr/cm² must be computed where cm² is the emitting area. Next, emitting area must be converted into a projected area and the solid angle; this product is often called the etendue. This step can be obviated by the use of a calibrated source, where the exact number of photons/s/cm² is known at the detector. If this is unknown, it can be estimated using the black-body radiation equation, detector active area formula_25 and the etendue. This ultimately converts the outgoing radiance of the black body in W/sr/cm² of emitting area into one of W observed on the detector.
The broad-band responsivity, is then just the signal weighted by this wattage.
formula_26
Where,
From this metric noise-equivalent power can be computed by taking the noise level over the responsivity.
formula_32
Similarly, noise-equivalent irradiance can be computed using the responsivity in units of photons/s/W instead of in units of the signal.
Now, the detectivity is simply the noise-equivalent power normalized to the bandwidth and detector area.
formula_33

</doc>
<doc id="41728" url="https://en.wikipedia.org/wiki?curid=41728" title="Speckle pattern">
Speckle pattern

A speckle pattern is an intensity pattern produced by the mutual interference of a set of wavefronts. This phenomenon has been investigated by scientists since the time of Newton, but speckles have come into prominence since the invention of the laser and have now found a variety of applications.
Speckle patterns typically occur in diffuse reflections of monochromatic light such as laser light. Such reflections may occur on materials such as paper, white paint, rough surfaces, or in media with a large number of scattering particles in space, such as airborne dust or in cloudy liquids.
Explanation.
The speckle effect is a result of the interference of many waves of the same frequency, having different phases and amplitudes, which add together to give a resultant wave whose amplitude, and therefore intensity, varies randomly. If each wave is modelled by a vector, then it can be seen that if a number of vectors with random angles are added together, the length of the resulting vector can be anything from zero to the sum of the individual vector lengths—a 2-dimensional random walk, sometimes known as a drunkard's walk. In the limit of many interfering waves the distribution of intensities (which go as the square of the vector's length) becomes exponential formula_1, where formula_2 is the mean intensity.
When a surface is illuminated by a light wave, according to diffraction theory, each point on an illuminated surface acts as a source of secondary spherical waves. The light at any point in the scattered light field is made up of waves which have been scattered from each point on the illuminated surface. If the surface is rough enough to create path-length differences exceeding one wavelength, giving rise to phase changes greater than 2π, the amplitude, and hence the intensity, of the resultant light varies randomly.
If light of low coherence (i.e., made up of many wavelengths) is used, a speckle pattern will not normally be observed, because the speckle patterns produced by individual wavelengths have different dimensions and will normally average one another out. However, speckle patterns can be observed in polychromatic light in some conditions.
Subjective speckles.
When an image is formed of a rough surface which is illuminated by a coherent light (e.g. a laser beam), a speckle pattern is observed in the image plane; this is called a “subjective speckle pattern” – see image above. It is called "subjective" because the detailed structure of the speckle pattern depends on the viewing system parameters; for instance, if the size of the lens aperture changes, the size of the speckles change. If the position of the imaging system is altered, the pattern will gradually change and will eventually be unrelated to the original speckle pattern.
This can be explained as follows. Each point in the image can be considered to be illuminated by a finite area in the object. The size of this area is determined by the diffraction-limited resolution of the lens which is given by the Airy disk whose diameter is 2.4λu/D, where λ is the wavelength of the light, u is the distance between the object and the lens, and D is the diameter of the lens aperture. (This is a simplified model of diffraction-limited imaging).
The light at neighbouring points in the image has been scattered from areas which have many points in common and the intensity of two such points will not differ much. However, two points in the image which are illuminated by areas in the object which are separated by the diameter of the Airy disk, have light intensities which are unrelated. This corresponds to a distance in the image of 2.4λv/D where v is the distance between the lens and the image. Thus, the ‘size’ of the speckles in the image is of this order.
The change in speckle size with lens aperture can be observed by looking at a laser spot on a wall directly, and then through a very small hole. The speckles will be seen to increase significantly in size.
Objective speckles.
The light at a given point in the speckle pattern is made up of contributions from the whole of the scattering surface. The relative phases of these waves vary across the surface, so that the sum of the individual waves varies randomly. The pattern is the same regardless of how it is imaged, just as if it were a painted pattern.
The "size" of the speckles is a function of the wavelength of the light, the size of the laser beam which illuminates the first surface, and the distance between this surface and the surface where the speckle pattern is formed. This is the case because when the angle of scattering changes such that the relative path difference between light scattered from the centre of the illuminated area compared with light scattered from the edge of the illuminated changes by λ, the intensity becomes uncorrelated. Dainty derives an expression for the mean speckle size as λz/L where L is the width of the illuminated area and z is the distance between the object and the location of the speckle pattern.
Near-field speckles.
Objective speckles are usually obtained in the far field (also called Fraunhofer region, that
is the zone where Fraunhofer diffraction happens). This means that they are generated "far" from 
the object that emits or scatters light. Speckles can be observed also close to the
scattering object, in the near field (also called Fresnel region, that is, the region where Fresnel diffraction happens). This kind of speckles are called Near Field Speckles. 
See near and far field for a more rigorous definition of "near" and "far".
The statistical properties of a far-field speckle pattern (i.e., the speckle form and dimension)
depend on the form and dimension of the region hit by laser light.
By contrast, a very interesting feature of near field speckles is that
their statistical properties are closely related to the form and structure of the scattering object:
objects that scatter at high angles generate small near field speckles, and "vice versa".
Under Rayleigh-Gans condition, in particular, speckle dimension mirrors the average dimension
of the scattering objects, while, in general, the statistical properties of near field
speckles generated by a sample
depend on the light scattering distribution.
Actually, the condition under which the near field speckles appear has been
described as more strict than the usual Fresnel condition.
Optical vortices in speckle patterns.
Speckle interference pattern may be decomposed in the 
sum of plane waves. There exist a set of points where amplitude 
of electromagnetic field is exactly zero. These points 
had been recognized as " dislocations of wave trains " 
These phase dislocations of electromagnetic field are known as optical vortices.
There is a circular energy flow around each
vortex core. Thus each vortex in the speckle pattern carries optical angular momentum. The angular momentum density is given by:
formula_3
Typically vortices appear in speckle pattern in pairs. These vortex - antivortex pairs are placed randomly in space. One may show that electromagnetic angular momentum of each vortex pair is close to zero. In phase conjugating mirrors based on Stimulated Brillouin scattering optical vortices excite acoustical vortices.
Apart from formal decomposition in Fourier series the speckle pattern may be composed for plane waves emitted by tilted regions of the phase plate. This approach significantly simplifies numerical modelling. formula_4 numerical emulation demonstrates the intertwining of vortices which leads to formation of "ropes " in optical speckle.
Applications.
When lasers were first invented, the speckle effect was considered to be a severe drawback in using lasers to illuminate objects, particularly in holographic imaging because of the grainy image produced. It was later realized that speckle patterns could carry information about the object's surface deformations, and this effect is exploited in holographic interferometry and electronic speckle pattern interferometry. The speckle effect is also used in speckle imaging and in eye testing using speckle.
Speckle is the chief limitation of coherent lidar and coherent imaging in optical heterodyne detection.
In the case of near field speckles, the statistical properties depend on the light scattering
distribution of a given sample. This allows the use of near field speckle analysis to detect the scattering distribution; this is the so-called near-field scattering
technique.
When the speckle pattern changes in time, due to changes in the illuminated surface, the phenomenon is known as dynamic speckle, and it can be used to measure activity, by means of, for example,an optical flow sensor (optical computer mouse). In biological materials, the phenomenon is known as biospeckle.
Reduction.
Speckle is considered to be a problem in laser based display systems like the Laser TV. Speckle is usually quantified by the speckle contrast. Speckle contrast reduction is essentially the creation of many independent speckle patterns, so that they average out on the retina/detector. This can be achieved by,
Rotating diffusers—which destroys the spatial coherence of the laser light—can also be used to reduce the speckle. Moving/vibrating screens may also be solutions. The Mitsubishi Laser TV appears to use such a screen which requires special care according to their product manual. A more detailed discussion on laser speckle reduction can be found in 
Synthetic array heterodyne detection was developed to reduce speckle noise in coherent optical imaging and coherent DIAL lidar.
In scientific applications, a spatial filter can be used to reduce speckle.

</doc>
<doc id="41729" url="https://en.wikipedia.org/wiki?curid=41729" title="Spectral width">
Spectral width

In telecommunications, spectral width is the wavelength interval over which the magnitude of all spectral components is equal to or greater than a specified fraction of the magnitude of the component having the maximum value.
In optical communications applications, the usual method of specifying spectral width is the full width at half maximum. This is the same convention used in bandwidth, defined as the frequency range where power drops by less than half (at most −3 dB).
The FWHM method may be difficult to apply when the spectrum has a complex shape. Another method of specifying spectral width is a special case of root-mean-square deviation where the independent variable is wavelength, λ, and "f" (λ) is a suitable radiometric quantity.
The "relative spectral width", Δλ/λ, is frequently used where Δλ is obtained according to note 1, and λ is the center wavelength.

</doc>
<doc id="41730" url="https://en.wikipedia.org/wiki?curid=41730" title="Speed of service">
Speed of service

In telecommunication, speed of service is the time for a message to be received, for example:

</doc>
<doc id="41732" url="https://en.wikipedia.org/wiki?curid=41732" title="Spill-forward feature">
Spill-forward feature

In telecommunication, a spill-forward feature is a service feature, in the operation of an intermediate office, that, acting on incoming trunk service treatment indications, assumes routing control of the call from the originating office. This increases the chances of completion by offering the call to more trunk groups than are available in the originating office.

</doc>
<doc id="41734" url="https://en.wikipedia.org/wiki?curid=41734" title="Spread spectrum">
Spread spectrum

In telecommunication and radio communication, spread-spectrum techniques are methods by which a signal (e.g. an electrical, electromagnetic, or acoustic signal) generated with a particular bandwidth is deliberately spread in the frequency domain, resulting in a signal with a wider bandwidth. These techniques are used for a variety of reasons, including the establishment of secure communications, increasing resistance to natural interference, noise and jamming, to prevent detection, and to limit power flux density (e.g. in satellite downlinks).
Spread-spectrum telecommunications.
This is a technique in which a telecommunication signal is transmitted on a bandwidth considerably larger than the frequency content of the original information. Frequency hopping is a basic modulation technique used in spread spectrum signal transmission.
Spread-spectrum telecommunications is a signal structuring technique that employs direct sequence, frequency hopping, or a hybrid of these, which can be used for multiple access and/or multiple functions. This technique decreases the potential interference to other receivers while achieving privacy. Spread spectrum generally makes use of a sequential noise-like signal structure to spread the normally narrowband information signal over a relatively wideband (radio) band of frequencies. The receiver correlates the received signals to retrieve the original information signal. Originally there were two motivations: either to resist enemy efforts to jam the communications (anti-jam, or AJ), or to hide the fact that communication was even taking place, sometimes called low probability of intercept (LPI).
Frequency-hopping spread spectrum (FHSS), direct-sequence spread spectrum (DSSS), time-hopping spread spectrum (THSS), chirp spread spectrum (CSS), and combinations of these techniques are forms of spread spectrum. Each of these techniques employs pseudorandom number sequences — created using pseudorandom number generators — to determine "and" control the spreading pattern of the signal across the allocated bandwidth. Ultra-wideband (UWB) is another modulation technique that accomplishes the same purpose, based on transmitting short duration pulses. Wireless standard IEEE 802.11 uses either FHSS or DSSS in its radio interface.
Invention of frequency hopping.
Frequency-hopping may date back to radio pioneer Jonathan Zenneck's 1908 German book "Wireless Telegraphy" although he states that Telefunken was using it previously. It saw limited use by the German military in World War I, was put forward by Polish engineer Leonard Danilewicz in 1929, showed up in a patent in the 1930s by Willem Broertjes (, issued Aug. 2, 1932), and in the top-secret US Army Signal Corps World War II communications system named SIGSALY.
During World War II, Golden Age of Hollywood actress Hedy Lamarr and Avant garde composer George Antheil developed and patented, intended jamming-resistant, radio guidance system for use in Allied torpedoes under US Patent 2,292,387, receiving it on August 11, 1942. Their approach was unique in that frequency coordination was done with paper player piano rolls - a novel approach which was never put in practice.
Spread-spectrum clock signal generation.
Spread-spectrum clock generation (SSCG) is used in some synchronous digital systems, especially those containing microprocessors, to reduce the spectral density of the electromagnetic interference (EMI) that these systems generate. A synchronous digital system is one that is driven by a clock signal and, because of its periodic nature, has an unavoidably narrow frequency spectrum. In fact, a perfect clock signal would have all its energy concentrated at a single frequency (the desired clock frequency) and its harmonics. Practical synchronous digital systems radiate electromagnetic energy on a number of narrow bands spread on the clock frequency and its harmonics, resulting in a frequency spectrum that, at certain frequencies, can exceed the regulatory limits for electromagnetic interference (e.g. those of the FCC in the United States, JEITA in Japan and the IEC in Europe).
Spread-spectrum clocking avoids this problem by using one of the methods previously described to reduce the peak radiated energy and, therefore, its electromagnetic emissions and so comply with electromagnetic compatibility (EMC) regulations.
It has become a popular technique to gain regulatory approval because it requires only simple equipment modification. It is even more popular in portable electronics devices because of faster clock speeds and increasing integration of high-resolution LCD displays into ever smaller devices. Since these devices are designed to be lightweight and inexpensive, traditional passive, electronic measures to reduce EMI, such as capacitors or metal shielding, are not viable. Active EMI reduction techniques such as spread-spectrum clocking are needed in these cases.
However, spread-spectrum clocking, like other kinds of dynamic frequency change, can also create challenges for designers. Principal among these is clock/data misalignment, or clock skew.
Note that this method does not reduce total radiated energy, and therefore systems are not necessarily less likely to cause interference. Spreading energy over a larger bandwidth effectively reduces electrical and magnetic readings within narrow bandwidths. Typical measuring receivers used by EMC testing laboratories divide the electromagnetic spectrum into frequency bands approximately 120 kHz wide. If the system under test were to radiate all its energy in a narrow bandwidth, it would register a large peak. Distributing this same energy into a larger bandwidth prevents systems from putting enough energy into any one narrowband to exceed the statutory limits. The usefulness of this method as a means to reduce real-life interference problems is often debated, since it is perceived that spread-spectrum clocking hides rather than resolves higher radiated energy issues by simple exploitation of loopholes in EMC legislation or certification procedures. This situation results in electronic equipment sensitive to narrow bandwidth(s) experiencing much less interference, while those with broadband sensitivity, or even operated at other frequencies (such as a radio receiver tuned to a different station), will experience more interference.
FCC certification testing is often completed with the spread-spectrum function enabled in order to reduce the measured emissions to within acceptable legal limits. However, the spread-spectrum functionality may be disabled by the user in some cases. As an example, in the area of personal computers, some BIOS writers include the ability to disable spread-spectrum clock generation as a user setting, thereby defeating the object of the EMI regulations. This might be considered a loophole, but is generally overlooked as long as spread-spectrum is enabled by default.
An ability to disable spread-spectrum clocking in computer systems is considered useful for overclocking, as spread spectrum can lower maximum clock speed achievable due to clock skew.

</doc>
<doc id="41735" url="https://en.wikipedia.org/wiki?curid=41735" title="Squelch">
Squelch

In telecommunications, squelch is a circuit function that acts to suppress the audio (or video) output of a receiver in the absence of a sufficiently strong desired input signal. Squelch is widely used in two-way radios and radio scanners to suppress the sound of channel noise when the radio is not receiving a transmission. Squelch can be 'opened', which allows all signals entering the receivers discriminator tap to be heard. This can be useful when trying to hear distant, or otherwise weak signals.
Carrier squelch.
A carrier squelch or noise squelch is the most simple variant of all. It operates strictly on the signal strength, such as when a television mutes the audio or blanks the video on "empty" channels, or when a walkie talkie mutes the audio when no signal is present. In some designs, the squelch threshold is preset. For example, television squelch settings are usually preset. Receivers in base stations or repeaters at remote mountain top sites are usually not adjustable remotely from the control point.
In devices such as two-way radios (also known as radiotelephones), the squelch on a local receiver can be adjusted with a knob, others have push buttons or a sequence of button presses. This setting adjusts the threshold at which signals will open (un-mute) the audio channel. Backing off the control will turn on the audio, and the operator will hear white noise (also called "static" or squelch noise) when there is no signal present. The usual operation is to adjust the control until the channel just shuts off - then only a small threshold signal is needed to turn on the speaker. However, if a weak signal is annoying, the operator can set the control a little higher thereby adjusting the squelch to open only when stronger signals are received.
A typical FM two-way radio carrier squelch circuit is noise operated. It takes out the voice components of the receive audio by passing the detected audio through a high-pass filter. A typical filter might pass frequencies over 4,000 Hz (4 kHz). The squelch control adjusts the gain of an amplifier which varies the level of noise coming out of the filter. The audio output of the filter and amplifier is rectified and produces a DC voltage when noise is present. The presence of continuous noise on an idle channel creates a DC voltage which turns the receiver audio off. When a signal with little or no noise is received, the noise-derived voltage goes away and the receiver audio is unmuted. Some applications have the receiver tied to other equipment that uses the audio muting control voltage as a "signal present" indication, for example in a repeater the act of the receiver unmuting will switch on the transmitter.
Tone squelch and selective calling.
Tone squelch, or other forms of selective calling, is sometimes used to solve interference problems. Where more than one user is on the same channel ("co-channel" users), selective calling addresses a subset of all receivers. Instead of turning on the receive audio for any signal, the audio turns on only in the presence of the correct selective calling code. This is akin to the use of a lock on a door. A carrier squelch is unlocked and will let any signal in. Selective calling locks out all signals except ones with the correct key to the lock (the correct code).
In non-critical uses, selective calling can also be used to hide the presence of interfering signals such as receiver-produced intermodulation. Receivers with poor specifications—such as inexpensive police scanners or low-cost mobile radios—cannot reject the strong signals present in urban environments. The interference will still be present. It will still degrade system performance but by using selective calling the user will not have to hear the noises produced by receiving the interference.
Four different techniques are commonly used. Selective calling can be regarded as a form of in-band signalling.
CTCSS.
CTCSS (Continuous Tone-Coded Squelch System) continuously superimposes any one of about 50 low-pitch audio tones on the transmitted signal, ranging from 67 to 254 Hz. The original tone set was 10, then 32 tones, and has been expanded even further over the years. CTCSS is often called "PL tone" (for "Private Line", a trademark of Motorola), or simply "tone squelch". General Electric's implementation of CTCSS is called "Channel Guard" (or "CG"). RCA Corporation used the name "Quiet Channel", or "QC". There are many other company-specific names used by radio vendors to describe compatible options. Any CTCSS system that has compatible tones is interchangeable. Old and new radios with CTCSS and radios across manufacturers are compatible.
SelCall.
Selcall (Selective Calling) transmits a burst of up to five inband audio tones at the beginning of each transmission. This feature (sometimes called "tone burst") is common in European systems. Early systems used one tone (commonly called "Tone Burst"). Several tones were used, the most common being 1,750 Hz, which is still used in European amateur radio repeater systems. The addressing scheme provided by one tone was not enough, so a two tone system was devised - one tone followed by a second tone (sometimes called a "1+1" system). Later on, Motorola marketed a system called "Quik-Call" that used two simultaneous tones followed by two more simultaneous tones (sometimes called a "2+2" system) that was heavily used by fire department dispatch systems in the USA. Later selective call systems used paging system technology that made use of five sequential tones. In the same way that a single CTCSS tone would be used on an entire group of radios, a single five-tone sequence is used in a group of radios.
DCS.
DCS (Digital-Coded Squelch), generically known as "CDCSS" (Continuous Digital-Coded Squelch System), was designed as the digital replacement for CTCSS. In the same way that a single CTCSS tone would be used on an entire group of radios, the same DCS code is used in a group of radios. DCS is also referred to as "Digital Private Line" (or "DPL"), another trademark of Motorola, and likewise, General Electric's implementation of DCS is referred to a "Digital Channel Guard" (or "DCG"). DCS is also called "DTCS" (Digital Tone Code Squelch) by Icom, other names by other manufacturers. Radios with DCS options are generally compatible provided the radio's encoder-decoder will use the same code as radios in the existing system.
DCS adds a 134.4 bps (sub-audible) bitstream to the transmitted audio. The code word is a 23-bit Golay (23,12) code which has the ability to detect and correct errors of 3 or fewer bits. The word consists of 12 data bits followed by 11 check bits. The last 3 data bits are a fixed '001', this leaves 9 code bits (512 possibilities) which are conventionally represented as a 3-digit octal number. Note that the first bit transmitted is the LSB, so the code is "backwards" from the transmitted bit order. Only 83 of the 512 possible codes are available, to prevent falsing due to alignment collisions.
XTCSS.
XTCSS is the newest signalling technique and it provides 99 codes with the added advantage of "silent operation". XTCSS fitted radios are purposed to enjoy more privacy and flexibility of operation. XTCSS is implemented as a combination of CTCSS and in-band signalling.
Uses.
Squelch was invented first and is still in wide use in two-way/three-way radio, especially in the amateur radio world. Squelch of any kind is used to indicate loss of signal, which is used to keep commercial and amateur radio repeaters from transmitting continually. Since a carrier squelch receiver cannot tell a valid carrier from a spurious signal (noise, etc.), CTCSS is often used as well, as it avoids false keyups. Use of CTCSS is especially helpful on congested frequencies or on frequency bands prone to skip and during band openings.
It is a bad idea to use any coded squelch system to hide interference issues in systems with life-safety or public-safety uses such as police, fire, search and rescue or ambulance company dispatching. Adding tone or digital squelch to a radio system does not solve interference issues, it just covers them up. The presence of interfering signals should be corrected rather than masked. Interfering signals masked by tone squelch will produce apparently random missed messages. The intermittent nature of interfering signals will make the problem difficult to reproduce and troubleshoot. Users will not understand why they cannot hear a call, and will lose confidence in their radio system.
Professional wireless microphones use squelch to avoid reproducing noise when the receiver does not receive enough signal from the microphone. Most professional models have adjustable squelch, usually set with a screwdriver adjustment on the receiver.

</doc>
<doc id="41736" url="https://en.wikipedia.org/wiki?curid=41736" title="Standard telegraph level">
Standard telegraph level

In telecommunication, standard telegraph level ("STL") is the power per individual telegraph channel required to yield the standard composite data level. 
For example, for a composite data level of -13 dBm at 0-dBm transmission level point (0TLP), the "STL" would be approximately -25 dBm for a 16-channel VFCT terminal computed from "STL" = - (13+10log10" n" ), where "n" is the number of telegraph channels and the "STL" is in dBm. 

</doc>
<doc id="41737" url="https://en.wikipedia.org/wiki?curid=41737" title="Standard test signal">
Standard test signal

In telecommunication, a standard test signal is a single-frequency signal with standardized level used for testing the peak power transmission capability and for measuring the total harmonic distortion of circuits or parts of an electric circuit. 
Standardized test signal levels and frequencies are listed in MIL-STD-188-100 and in the Code of Federal Regulations Title 47, part 68.

</doc>
<doc id="41738" url="https://en.wikipedia.org/wiki?curid=41738" title="Standard test tone">
Standard test tone

In telecommunication, a standard test tone is a pure tone with a standardized level generally used for level alignment of single links and of links in tandem. 
For standardized test signal levels and frequencies, see MIL-STD-188-100 for United States Department of Defense (DOD) use, and the Code of Federal Regulations Title 47, part 68 for other Government agencies.

</doc>
<doc id="41739" url="https://en.wikipedia.org/wiki?curid=41739" title="Standard time and frequency signal">
Standard time and frequency signal

The Standard Time and Frequency Signal (STFS) is a Radiocommunication service providing the transmission of specified frequency and time signal, of stated high precision, intended for general reception in the United States and beyond. The radio signals are broadcast on very precise carrier frequencies by the U.S. Naval Observatory and the National Institute of Standards and Technology (NIST), formerly the National Bureau of Standards (NBS). The technical specification of that particular service is in line to the provisions of the International Telecommunication Union´s (ITU) Radio Regulations (RR)
Utilization in other countries.
A similar service is operated in the United Kingdom by the National Physical Laboratory, broadcasting from Anthorn radio station in north-west England, and by the BBC using the 198 kHz carrier of the Radio 4 national radio station with a frequency accuracy of 1 part in 1011.

</doc>
<doc id="41740" url="https://en.wikipedia.org/wiki?curid=41740" title="Standby">
Standby

Standby may refer to:

</doc>
<doc id="41741" url="https://en.wikipedia.org/wiki?curid=41741" title="Standing wave">
Standing wave

In physics, a standing wave – also known as a stationary wave – is a wave in a medium in which each point on the axis of the wave has an associated constant amplitude. The locations at which the amplitude is minimum are called nodes, and the locations where the amplitude is maximum are called antinodes.
Standing waves were first discovered by Franz Melde, who coined the term "standing wave" around 1860.
This phenomenon can occur because the medium is moving in the opposite direction to the wave, or it can arise in a stationary medium as a result of interference between two waves traveling in opposite directions. The most common cause of standing waves is the phenomenon of resonance, in which standing waves occur inside a resonator due to interference between waves reflected back and forth at the resonator's resonant frequency.
For waves of equal amplitude traveling in opposing directions, there is on average no net propagation of energy.
Moving medium.
As an example of the first type, under certain meteorological conditions standing waves form in the atmosphere in the lee of mountain ranges. Such waves are often exploited by glider pilots.
Standing waves and hydraulic jumps also form on fast flowing river rapids and tidal currents such as the Saltstraumen maelstrom. Many standing river waves are popular river surfing breaks.
Opposing waves.
As an example of the second type, a "standing wave" in a transmission line is a wave in which the distribution of current, voltage, or field strength is formed by the superposition of two waves of the same frequency propagating in opposite directions. The effect is a series of nodes (zero displacement) and anti-nodes (maximum displacement) at fixed points along the transmission line. Such a standing wave may be formed when a wave is transmitted into one end of a transmission line and is reflected from the other end by an impedance mismatch, "i.e.", discontinuity, such as an open circuit or a short. The failure of the line to transfer power at the standing wave frequency will usually result in attenuation distortion.
In practice, losses in the transmission line and other components mean that a perfect reflection and a pure standing wave are never achieved. The result is a "partial standing wave", which is a superposition of a standing wave and a traveling wave. The degree to which the wave resembles either a pure standing wave or a pure traveling wave is measured by the standing wave ratio (SWR).
Another example is standing waves in the open ocean formed by waves with the same wave period moving in opposite directions. These may form near storm centres, or from reflection of a swell at the shore, and are the source of microbaroms and microseisms.
Mathematical description.
In one dimension, two waves with the same frequency, wavelength and amplitude traveling in opposite directions will interfere and produce a standing wave or stationary wave. For example: a wave traveling to the right along a taut string and hitting the end will reflect back in the other direction along the string, and the two waves will superpose to produce a standing wave. The reflected wave has to have the same amplitude and frequency as the incoming wave.
If the string is held at both ends, forcing zero movement at the ends, the ends become zeroes or "nodes" of the wave. The length of the string then becomes a measure of which waves the string will entertain: the longest wavelength is called the "fundamental". Half a wavelength of the fundamental fits on the string. Shorter wavelengths also can be supported as long as multiples of half a wavelength fit on the string. The frequencies of these waves all are multiples of the fundamental, and are called "harmonics" or "overtones". For example, a guitar player can select an overtone by putting a finger on a string to force a node at the proper position between the ends of the string, suppressing all harmonics that do not share this node.
Harmonic waves travelling in opposite directions can be represented by the equations below:
and
where:
So the resultant wave "y" equation will be the sum of "y1" and "y2":
Using the trigonometric sum-to-product identity for 'sin("u") + sin("v")' to simplify:
This describes a wave that oscillates in time, but has a spatial dependence that is stationary: sin("kx"). At locations "x" = 0, "λ"/2, "λ", 3"λ"/2, ... called the nodes the amplitude is always zero, whereas at locations "x" = "λ"/4, 3"λ"/4, 5"λ"/4, ... called the anti-nodes, the amplitude is maximum. The distance between two conjugative nodes or anti-nodes is "λ"/2.
Standing waves can also occur in two- or three-dimensional resonators. With standing waves on two-dimensional membranes such as drumheads, illustrated in the animations above, the nodes become nodal lines, lines on the surface at which there is no movement, that separate regions vibrating with opposite phase. These nodal line patterns are called Chladni figures. In three-dimensional resonators, such as musical instrument sound boxes and microwave cavity resonators, there are nodal surfaces.
Examples.
One easy example to understand standing waves is two people shaking either end of a jump rope. If they shake in sync the rope can form a regular pattern of waves oscillating up and down, with stationary points along the rope where the rope is almost still (nodes) and points where the arc of the rope is maximum (antinodes)
Sound waves.
Standing waves are also observed in physical media such as strings and columns of air. Any waves traveling along the medium will reflect back when they reach the end. This effect is most noticeable in musical instruments where, at various multiples of a vibrating string or air column's natural frequency, a standing wave is created, allowing harmonics to be identified. Nodes occur at fixed ends and anti-nodes at open ends. If fixed at only one end, only odd-numbered harmonics are available. At the open end of a pipe the anti-node will not be exactly at the end as it is altered by its contact with the air and so end correction is used to place it exactly. The density of a string will affect the frequency at which harmonics will be produced; the greater the density the lower the frequency needs to be to produce a standing wave of the same harmonic.
Light.
Standing waves are also observed in optical media such as optical wave guides, optical cavities, etc. Lasers use optical cavities in the form of a pair of facing mirrors. The gain medium in the cavity (such as a crystal) emits light coherently, exciting standing waves of light in the cavity. The wavelength of light is very short (in the range of nanometers, 10−9 m) so the standing waves are microscopic in size. One use for standing light waves is to measure small distances, using optical flats.
Mechanical waves.
Standing waves can be mechanically induced into solid medium using resonance. One easy to understand example is two people shaking either end of a jump rope. If they shake in sync, the rope will form a regular pattern with nodes and antinodes and appear to be stationary, hence the name standing wave. Similarly a cantilever beam can have a standing wave imposed on it by applying a base excitation. In this case the free end moves the greatest distance laterally compared to any location along the beam. Such a device can be used as a sensor to track changes in frequency or phase of the resonance of the fiber. One application is as a measurement device for dimensional metrology.
Seismic waves.
Standing surface waves on the Earth are observed as free oscillations of the Earth.
Faraday waves.
The Faraday wave is a non-linear standing wave at the air-liquid interface induced by hydrodynamic instability. It can be used as a liquid-based template to assemble microscale materials.

</doc>
<doc id="41742" url="https://en.wikipedia.org/wiki?curid=41742" title="Standing wave ratio">
Standing wave ratio

In radio engineering and telecommunications, standing wave ratio (SWR) is a measure of impedance matching of loads to the characteristic impedance of a transmission line or waveguide. Impedance mismatches result in standing waves along the transmission line, and SWR is defined as the ratio of the partial standing wave's amplitude at an antinode (maximum) to the amplitude at a node (minimum) along the line.
The SWR is usually thought of in terms of the maximum and minimum AC voltages along the transmission line, thus called the voltage standing wave ratio or VSWR (sometimes pronounced "vizwar"). For example, the VSWR value 1.2:1 denotes an AC voltage due to standing waves along the transmission line reaching a peak value 1.2 times that of the minimum AC voltage along that line. The SWR can as well be defined as the ratio of the maximum amplitude to minimum amplitude of the transmission line's currents, electric field strength, or the magnetic field strength. Neglecting transmission line loss, these ratios are identical.
The power standing wave ratio (PSWR) is defined as the square of the VSWR, however this terminology has no physical relation to actual powers involved in transmission.
The SWR can be measured with an instrument called an SWR meter. Since SWR is defined relative to the transmission line's characteristic impedance, the SWR meter must be constructed for that impedance; in practice most transmission lines used in these applications are coaxial cables with an impedance of either 50 or 75 ohms. Checking the SWR is a standard procedure in a radio station, for instance, to verify impedance matching of the antenna to the transmission line (and transmitter). Unlike connecting an impedance analyzer (or "impedance bridge") directly to the antenna (or other load), the SWR does not measure the actual impedance of the load, but quantifies the magnitude of the impedance mismatch just performing a measurement on the transmitter side of the transmission line.
Impedance matching.
SWR is used as a measure of impedance matching of a load to the characteristic impedance of a transmission line carrying radio frequency (RF) signals. This especially applies to transmission lines connecting radio transmitters and receivers with their antennas, as well as similar uses of RF cables such as cable television connections to TV receivers and distribution amplifiers. Impedance matching is achieved when the source impedance is the complex conjugate of the load impedance. The easiest way of achieving this, and the way that minimizes losses along the transmission line, is for both the source and load to be real, that is, pure resistances, equal to the characteristic impedance of the transmission line. When there is a mismatch between the load impedance and the transmission line, part of the forward wave sent toward the load is reflected back along the transmission line towards the source. The source then sees a different impedance than it expects which can lead to lesser (or in some cases, more) power being supplied by it, the result being very sensitive to the electrical length of the transmission line.
Such a mismatch is usually undesired and results in standing waves along the transmission line which magnifies transmission line losses (significant at higher frequencies and for longer cables). The SWR is a measure of the depth of those standing waves and is therefore a measure of the matching of the load to the transmission line. A matched load would result in an SWR of 1:1 implying no reflected wave. An infinite SWR represents complete reflection by a load unable to absorb electrical power, with all the incident power reflected back towards the source.
It should be understood that the match of a load to the transmission line is different from the match of a "source" to the transmission line or the match of a source to the load "seen through" the transmission line. For instance, if there is a perfect match between the load impedance "Z"load and the source impedance "Z"source="Z"*load, that perfect match will remain if the source and load are connected through a transmission line with an electrical length of one half wavelength (or a multiple of one half wavelengths) using a transmission line of "any" characteristic impedance "Z"0. However the SWR will generally not be 1:1, depending only on "Z"load and "Z"0. With a different length of transmission line, the source will see a different impedance than "Z"load which may or may not be a good match to the source. Sometimes this is deliberate, as when a quarter-wave matching section is used to improve the match between an otherwise mismatched source and load.
However typical RF sources such as transmitters and signal generators are designed to look into a purely resistive load impedance such as 50Ω or 75Ω, corresponding to common transmission lines' characteristic impedances. In those cases, matching the load to the transmission line, "Z"load="Z"0, "always" insures that the source will see the same load impedance as if the transmission line weren't there. This is identical to a 1:1 SWR. This condition ( "Z"load="Z"0) also means that the load seen by the source is independent of the transmission line's electrical length. Since the electrical length of a physical segment of transmission line depends on the signal frequency, violation of this condition means that the impedance seen by the source through the transmission line becomes a function of frequency (especially if the line is long), even if "Z"load is frequency-independent. So in practice, a good SWR (near 1:1) implies a transmitter's output seeing the exact impedance it expects for optimum and safe operation.
Relationship to the reflection coefficient.
The voltage component of a standing wave in a uniform transmission line consists of the forward wave (with complex amplitude formula_1) superimposed on the reflected wave (with complex amplitude formula_2).
A wave is partly reflected when a transmission line is terminated with other than a pure resistance equal to its characteristic impedance. The reflection coefficient formula_3 is defined thus:
formula_3 is a complex number that describes both the magnitude and the phase shift of the reflection. The simplest cases with formula_3 "measured at the load" are:
The SWR directly corresponds to the magnitude of formula_3.
At some points along the line the forward and reflected waves interfere constructively, exactly in phase, with the resulting amplitude formula_11 given by the sum of their those waves' amplitudes:
At other points, the waves interfere 180° out of phase with the amplitudes partially cancelling:
The voltage standing wave ratio is then equal to:
Since the magnitude of formula_3 always falls in the range [0,1], the SWR is always greater than or equal to unity. Note that the "phase" of "V"f and "V"r vary along the transmission line in opposite directions to each other. Therefore the complex valued reflection coefficient formula_3 varies as well, but only in phase. With the SWR dependent "only" on the complex magnitude of formula_3, it can be seen that the SWR measured at "any" point along the transmission line (neglecting transmission line losses) obtains an identical reading.
Since the power of the forward and reflected waves are proportional to the square of the voltage components due to each wave, SWR can be expressed in terms of forward and reflected power as follows:
In fact, most SWR meters operate by measuring both the forward power and the reflected power. Normalizing the power readings according to the forward power, a reading of the reflected power is then directly read off a meter in terms of SWR.
In the special case of a load "RL" which is purely resistive but unequal to the characteristic impedance of the transmission line "Z0", the SWR is given simply by their ratio:
with the ±1 chosen to obtain a value greater than unity.
The standing wave pattern.
Using complex notation for the voltage amplitudes, for a signal at frequency ν, the actual (real) voltages Vactual as a function of time "t" are understood to relate to the complex voltages according to:
Thus taking the real part of the complex quantity inside the parenthesis, the actual voltage consists of a sine wave at frequency ν with a peak amplitude equal to the complex magnitude of V, and with a phase given by the phase of the complex V. Then with the position along a transmission line given by x, with the line ending in a load located at x0, the complex amplitudes of the forward and reverse waves would be written as:
for some complex amplitude A (corresponding to the forward wave at x0). Here "k" is the wavenumber due to the guided wavelength along the transmission line. Note that some treatments use phasors where the time dependence is according to formula_22 and spatial dependence (for a wave in the +x direction) of formula_23. Either convention obtains the same result for Vactual.
According to the superposition principle the net voltage present at any point x on the transmission line is equal to the sum of the voltages due to the forward and reflected waves:
Since we are interested in the variations of the "magnitude" of Vnet along the line (as a function of x), we shall solve instead for the squared magnitude of that quantity, which simplifies the mathematics. To obtain the squared magnitude we multiply the above quantity by its complex conjugate:
Depending on the phase of the third term, one can see that the maximum and minimum values of Vnet (the square root of the quantity in the equations) are (1 + |Γ|)|A| and (1 − |Γ|)|A| respectively, for a standing wave ratio of:
as we had earlier asserted. Along the line, the above expression for formula_27 is seen to oscillate sinusoidally between formula_28 and formula_29 with a period of 2π/2k. This is "half" of the guided wavelength λ = 2π/k for the frequency ν. That can be seen as due to interference between two waves of that frequency which are travelling in "opposite" directions.
For example, at a frequency ν=20 MHz (free space wavelength of 15 m) in a transmission line whose velocity factor is 2/3, the guided wavelength (distance between voltage peaks of the forward wave alone) would be λ = 10 m. At instances when the forward wave at x = 0 is at zero phase (peak voltage) then at x = 10 m it would also be at zero phase, but at x = 5 m it would be at 180° phase (peak "negative" voltage). On the other hand, the magnitude of the voltage due to a standing wave produced by its addition to a reflected wave, would have a wavelength between peaks of only λ/2 = 5 m. Depending on the location of the load and phase of reflection, there might be a peak in the magnitude of Vnet at x = 1.3 m. Then there would be another peak found where |Vnet|=Vmax at x = 6.3 m, whereas it would find minima of the standing wave |Vnet| = Vmin at x = 3.8 m, 8.8 m, etc.
Practical implications of SWR.
The most common case for measuring and examining SWR is when installing and tuning transmitting antennas. When a transmitter is connected to an antenna by a feed line, the driving point impedance of the antenna must be resistive and matching the characteristic impedance of the feed line in order for the transmitter to see the impedance it was designed for (the impedance of the feed line, usually 50 or 75 ohms).
The impedance of a particular antenna design can vary due to a number of factors that cannot always be clearly identified. This includes the transmitter frequency (as compared to the antenna's design or resonant frequency), the antenna's height above the ground and proximity to large metal structures, and variations in the exact size of the conductors used to construct the antenna.
When an antenna and feed line do not have matching impedances, the transmitter sees an unexpected impedance, where it might not be able to produce its full power, and can even damage the transmitter in some cases.
The reflected power in the transmission line increases the average current and therefore losses in the transmission line compared to power actually delivered to the load.
It is the interaction of these reflected waves with forward waves which causes standing wave patterns, with the negative repercussions we have noted.
Matching the impedance of the antenna to the impedance of the feed line can sometimes be accomplished through adjusting the antenna itself, but otherwise is possible using an antenna tuner, an impedance matching device. Installing the tuner between the feed line and the antenna allows for the feed line to see a load close to its characteristic impedance, while sending most of the transmitter's power (a small amount may be dissipated within the tuner) to be radiated by the antenna despite its otherwise unacceptable feed point impedance. Installing a tuner in between the transmitter and the feed line can also transform the impedance seen at the transmitter end of the feed line to one preferred by the transmitter. However in the latter case, the feed line still has a high SWR present, with the resulting increased feed line losses unmitigated.
The magnitude of those losses are dependent on the type of transmission line, and its length. They always increase with frequency. For example, a certain antenna used well away from its resonant frequency may have an SWR of 6:1. For a frequency of 3.5 MHz, with that antenna fed through 75 meters of RG-8A coax, the loss due to standing waves would be 2.2 dB. However the same 6:1 mismatch through 75 meters of RG-8A coax would incur 10.8 dB of loss at 146 MHz. Thus, a better match of the antenna to the feed line, that is, a lower SWR, becomes increasingly important with increasing frequency, even if the transmitter is able to accommodate the impedance seen (or an antenna tuner is used between the transmitter and feed line).
Power standing wave ratio.
The term "power standing wave ratio" (PSWR) is sometimes referred to, and defined as the square of the voltage standing wave ratio. The term is widely cited as "misleading." In the words of Gridley:
In other words, there are no actual powers being compared. Patently a misnomer, the term "power standing wave ratio" is not the ratio of any two physical quantities.
However it does correspond to one type of measurement of SWR using what was formerly a standard measuring instrument at microwave frequencies. A slotted line involves a waveguide (or air-filled coaxial line) in which a small sensing antenna measures the electric field along the transmission line "directly". The electric field strength is commonly measured using a crystal detector or Schottky barrier diode. These detectors have a square law output for low levels of input. Readings therefore corresponded to the square of the electric field along the slot, "E"2("x"), with maximum and minimum readings of "E"2max and "E"2min found as the probe is moved along the slot. The ratio of these yields the PSWR directly, the square root of which is the VSWR.
Implications of SWR on medical applications.
SWR can also have a detrimental impact upon the performance of microwave based medical applications. In microwave electrosurgery an antenna that is placed directly into tissue may not always have an optimal match with the feedline resulting in an SWR. The presence of SWR can affect monitoring components used to measure power levels impacting the reliability of such measurements.

</doc>
<doc id="41743" url="https://en.wikipedia.org/wiki?curid=41743" title="Star coupler">
Star coupler

A star coupler is a device that takes in an input signal and splits it into several output signals.
In fiber optics, and especially in telecommunications, a star coupler is a passive optical device, used in network applications. An optical signal introduced into any input port is distributed to all output ports. Because of the way a passive star coupler is constructed, the number of ports is usually a power of 2; "i.e.", two input ports and two output ports (a "two-port" coupler, customarily called a "directional coupler," or "splitter" ); four input ports and four output ports (a "four-port" coupler); eight input ports and eight output ports (an "eight-port" coupler), "etc".
DEC computers.
A star coupler was also the name of a device sold by Digital Equipment Corporation (now part of Hewlett-Packard) of Maynard, Massachusetts. In this case, the star coupler interconnected links to computers via coaxial cable rather than optical fibres, but the function was essentially the same. The signal that was distributed was 70 Mb/s computer interconnect (CI) data and the star coupler provided two redundant paths of either 8 or 16 ports each. Digital's star coupler was developed for use with the VAX- and later Alpha-based computers running Digital's VMS operating system, to provide a passive, highly reliable interconnect for Digital's cluster technology.

</doc>
<doc id="41744" url="https://en.wikipedia.org/wiki?curid=41744" title="Start signal">
Start signal

In telecommunication, a start signal is a signal that prepares a device to receive data or to perform a function. 
In asynchronous serial communication, start signals are used at the beginning of a character that prepares the receiving device for the reception of the code elements. 
A start signal is limited to one signal element usually having the duration of a unit interval. 

</doc>
<doc id="41746" url="https://en.wikipedia.org/wiki?curid=41746" title="Statement">
Statement

Statement may refer to:

</doc>
<doc id="41748" url="https://en.wikipedia.org/wiki?curid=41748" title="Step-index profile">
Step-index profile

For an optical fiber, a step-index profile is a refractive index profile characterized by a uniform refractive index within the core and a sharp decrease in refractive index at the core-cladding interface so that the cladding is of a lower refractive index. The step-index profile corresponds to a power-law index profile with the profile parameter approaching infinity. The step-index profile is used in most single-mode fibers and some multimode fibers.
A step-index fiber is characterized by the core and cladding refractive indices "n1" and "n2" and the core and cladding radii a and b. Examples of standard core and cladding diameters 2a/2b are 8/125, 50/125, 62.5/125, 85/125, or 100/140 (units of µm). The fractional refractive-index change formula_1. The value of n1 is typically between 1.44 and 1.46, and formula_2 is typically between 0.001 and 0.02.
Step-index optical fiber is generally made by doping high-purity fused silica glass (SiO2) with different concentrations of materials like titanium, germanium, or boron.
Pulse dispersion in a step index optical fiber is given by
formula_3
where
formula_4 is the difference in refractive indices of core and cladding.
formula_5 is the refractive index of core
formula_6 is the length of the optical fiber under observation
formula_7

</doc>
<doc id="41749" url="https://en.wikipedia.org/wiki?curid=41749" title="Stopband">
Stopband

A stopband is a band of frequencies, between specified limits, through which a circuit, such as a filter or telephone circuit, does not allow signals to pass, or the attenuation is above the required stopband attenuation level. Depending on application, the required attenuation within the stopband may typically be a value between 20 and 120 dB higher than the nominal passband attenuation, which often is 0 dB.
The lower and upper "limiting frequencies", also denoted lower and upper stopband corner frequencies, are the frequencies where the stopband and the transition bands meet in a filter specification. The stopband of a low-pass filter is the frequencies from the stopband corner frequency (which is slightly higher than the passband 3 dB cut-off frequency) up to the infinite frequency. The stopband of a high-pass filter consists of the frequencies from 0 hertz to a stopband corner frequency (slightly lower than the passband cut-off frequency).
A band-stop filter has one stopband, specified by two non-zero and non-infinite corner frequencies. The difference between the limits in the band-stop filter is the stopband bandwidth, which usually is expressed in hertz.
A bandpass filter typically has two stopbands. The shape factor of a bandpass filter is the relationship between the 3 dB bandwidth, and the difference between the stopband limits.

</doc>
<doc id="41750" url="https://en.wikipedia.org/wiki?curid=41750" title="Stop signal">
Stop signal

In telecommunication, a stop signal is a signal that marks the end of part of a transmission, for example:

</doc>
<doc id="41751" url="https://en.wikipedia.org/wiki?curid=41751" title="Store-and-forward switching center">
Store-and-forward switching center

In telecommunication, a store-and-forward switching center is a message switching center in which a message is accepted from the originating user, "i.e.," sender, when it is offered, held in a physical storage, and forwarded to the destination user, "i.e.," receiver, in accordance with the priority placed upon the message by the originating user and the availability of an outgoing channel.
Store and forward switching centers are usually implemented in mobile service stations where the messages that are sent from the sender is first sent to these centers. If the destination address isn't available then the center stores this message and tries sending it later. This improves the probability of the message to be delivered. In the other case, if the destination is available at that time, then the message is immediately sent.

</doc>
<doc id="41752" url="https://en.wikipedia.org/wiki?curid=41752" title="Stressed environment">
Stressed environment

Stressed environment: In radio communications, an environment that is under the influence of extrinsic factors that degrade communications integrity, such as when (a) the benign communications medium is disturbed by natural or man-made events (such as an intentional nuclear burst), (b) the received signal is degraded by natural or man-made interference (such as jamming signals or co-channel interference), (c) an interfering signal can reconfigure the network, and/or (d) an adversary threatens successful communications, in which case radio signals may be encrypted in order to deny the adversary an intelligible message, traffic flow information, network information, or automatic link establishment (ALE) control information.

</doc>
<doc id="41754" url="https://en.wikipedia.org/wiki?curid=41754" title="Sublayer">
Sublayer

In telecommunication, the term sublayer has the following meanings: 

</doc>
<doc id="41757" url="https://en.wikipedia.org/wiki?curid=41757" title="Substitution method">
Substitution method

In optical fiber technology, the substitution method is a method of measuring the transmission loss of a fiber. It consists of:
The substitution method has certain shortcomings with regard to its accuracy, but its simplicity makes it a popular field test method. It is conservative, in that if it were used to measure the individual losses of several long fibers, and the long fibers were concatenated, the total loss obtained (excluding splice losses) would be expected to be lower than the sum of the individual fiber losses. 
Some modern optical power meters have the capability to set to zero the reference level measured at the output of the reference fiber, so that the transmission loss of the fiber under test may be read out directly.

</doc>
<doc id="41760" url="https://en.wikipedia.org/wiki?curid=41760" title="Summation check">
Summation check

In telecommunication, the term summation check (sum check) has the following meanings: 

</doc>
<doc id="41761" url="https://en.wikipedia.org/wiki?curid=41761" title="Supervisory program">
Supervisory program

A supervisory program or supervisor is a computer program, usually part of an operating system, that controls the execution of other routines and regulates work scheduling, input/output operations, error actions, and similar functions and regulates the flow of work in a data processing system. 
It can also refer to a program that allocates computer component space and schedules computer events by task queuing and system interrupts. Control of the system is returned to the supervisory program frequently enough to ensure that demands on the system are met.
Historically, this term was essentially associated with IBM's line of mainframe operating systems starting with OS/360. In other operating systems, the supervisor is generally called the kernel.
In the 1970s, IBM further abstracted the supervisor state from the hardware, resulting in a hypervisor that enabled full virtualization, i.e. the capacity to run multiple operating systems on the same machine totally independently from each other. Hence the first such system was called "Virtual Machine" or "VM".

</doc>
<doc id="41762" url="https://en.wikipedia.org/wiki?curid=41762" title="Reduced-carrier transmission">
Reduced-carrier transmission

Reduced-carrier transmission is an amplitude modulation (AM) transmission in which the carrier signal level is reduced to reduce wasted electrical power. Suppressed-carrier transmission is a special case in which the carrier level is reduced below that required for demodulation by a normal receiver. 
Reduction of the carrier level permits higher power levels in the sidebands than would be possible with conventional AM transmission. Carrier power must be restored by the receiving station to permit demodulation, usually by means of a beat frequency oscillator (BFO). Failure of the BFO to match the original carrier frequency when receiving such a signal will cause a heterodyne.
Suppressed carriers are often used for single sideband (SSB) transmissions, such as for amateur radio on shortwave. That system is referred to in full as SSB suppressed carrier (SSBSC) or (SSB-SC). International broadcasters agreed in 1985 to also use SSBSC entirely by 2015, though IBOC and IBAC digital radio (namely Digital Radio Mondiale) seems likely to make this irrelevant.
FM stereo transmissions use a double-sideband suppressed carrier (DSBSC) signal from a stereo generator, together with a pilot tone of exactly half the original carrier frequency. This allows reconstitution of the original stereo carrier, and hence the stereo signal.

</doc>
<doc id="41763" url="https://en.wikipedia.org/wiki?curid=41763" title="Surface wave">
Surface wave

In physics, a surface wave is a mechanical wave that propagates along the interface between differing media, usually as a gravity wave between two fluids with different densities. A surface wave can also be an elastic (or a seismic) wave, such as with a "Rayleigh" or "Love" wave. It can also be an electromagnetic wave guided by a refractive index gradient. In radio transmission, a ground wave is a surface wave that propagates close to the surface of the Earth.
Mechanical waves.
In seismology, several types of surface waves are encountered. Surface waves, in this mechanical sense, are commonly known as either "Love waves" (L waves) or "Rayleigh waves". A seismic wave is a wave that "travels through the Earth, often as the result of an earthquake or explosion." Love waves have transverse motion (movement is perpendicular to the direction of travel, like light waves), whereas Rayleigh waves have both longitudinal (movement parallel to the direction of travel, like sound waves) and transverse motion. Seismic waves are studied by seismologists and measured by a seismograph or seismometer. Surface waves span a wide frequency range, and the period of waves that are most damaging is usually 10 seconds or longer. Surface waves can travel around the globe many times from the largest earthquakes. Surface waves are caused when P waves and S waves come to the surface.
The term "surface wave" can describe waves over an ocean, even when they are approximated by Airy functions and are more properly called creeping waves. Examples are the waves at the surface of water and air (ocean surface waves), or ripples in the sand at the interface with water or air. Another example is internal waves, which can be transmitted along the interface of two water masses of different densities.
In theory of Hearing physiology, the Traveling Wave (TW) of Von Bekesy, resulted from an acoustic surface wave of the basilar membrane into the cochlear duct. His theory pretended to explain every features of the auditory sensation owing to these passive mechanical phenomena. But Jozef Zwislocki and later David Kemp, showed that that was irrealistic and that an active feedback was necessary.
Electromagnetic waves.
"Ground waves" refer to the propagation of radio waves parallel to and adjacent to the surface of the Earth, following the curvature of the Earth. These "surface waves" are also known loosely as the Norton surface wave, the Zenneck surface wave, Sommerfeld waves, and gliding waves. See also Dyakonov surface waves (DSW) propagating at the interface of transparent materials with different symmetry.
Radio propagation.
Lower frequencies, below 3 MHz, travel efficiently as ground waves. This is because they are more strongly diffracted around obstacles due to their long wavelengths, allowing them to follow the Earth's curvature. The Earth has one refractive index and the atmosphere has another, thus constituting an interface that supports the surface wave transmission. Ground waves propagate in vertical polarization, with their magnetic field horizontal and electric field (close to) vertical. At VLF, the Ionosphere and earth's surface act as a waveguide.
Conductivity of the surface affects the propagation of ground waves, with more conductive surfaces such as sea water providing better propagation. Increasing the conductivity in a surface results in less dissipation. The refractive indices are subject to spatial and temporal changes. Since the ground is not a perfect electrical conductor, ground waves are attenuated as they follow the earth’s surface. The wavefronts initially are vertical, but the ground, acting as a lossy dielectric, causes the wave to tilt forward as it travels. This directs some of the energy into the earth where it is dissipated, so that the signal decreases exponentially.
Most long-distance low frequency (LF) "longwave" radio communication (between 30 kHz and 300 kHz) is a result of groundwave propagation. Mediumwave radio transmissions (frequencies between 300 kHz and 3000 kHz), including AM broadcast band, travel both as groundwaves and, for longer distances at night, as skywaves. Ground losses become lower at lower frequencies, greatly increasing the coverage of AM stations using the lower end of the band. The VLF and LF frequencies are mostly used for military communications, especially with ships and submarines. The lower the frequency the better the waves penetrate sea water. Even extremely low frequency waves (below 3 kHz) have been used to communicate with deeply submerged submarines.
Surface waves have been used in over-the-horizon radar, which operates mainly at frequencies between 2 and 20 MHz over the sea, which has a sufficiently high conductivity to convey the surface waves to and from a reasonable distance (up to 100 km or more; over-horizon radar also uses skywave propagation at much greater distances). In the development of radio, surface waves were used extensively. Early commercial and professional radio services relied exclusively on long wave, low frequencies and ground-wave propagation. To prevent interference with these services, amateur and experimental transmitters were restricted to the higher (HF) frequencies, felt to be useless since their ground-wave range was limited. Upon discovery of the other propagation modes possible at medium wave and short wave frequencies, the advantages of HF for commercial and military purposes became apparent. Amateur experimentation was then confined only to authorized frequencies in the range.
Mediumwave and shortwave reflect off the ionosphere at night, which is known as skywave. During daylight hours, the lower "D" layer of the ionosphere forms and absorbs lower frequency energy. This prevents skywave propagation from being very effective on mediumwave frequencies in daylight hours. At night, when the "D" layer dissipates, mediumwave transmissions travel better by skywave. Ground waves "do not" include ionospheric and tropospheric waves.
The propagation of sound waves through the ground taking advantage of the earths ability to more efficiently transmit low frequency is known as Audio ground wave (AGW).
Microwave field theory.
Within microwave field theory, the interface of a dielectric and conductor supports "surface wave transmission". Surface waves have been studied as part of transmission lines and some may be considered as single-wire transmission lines.
Characteristics and utilizations of the electrical surface wave phenomenon include:

</doc>
<doc id="41764" url="https://en.wikipedia.org/wiki?curid=41764" title="Survivability">
Survivability

Survivability is the ability to remain alive or continue to exist. The term has more specific meaning in certain contexts.
Ecological.
Following disruptive forces such as flood, fire, disease, war, or climate change some species of flora, fauna, and local life forms are likely to survive more successfully than others because of consequent changes to their surrounding biophysical conditions.
Engineering.
In engineering, survivability is the quantified ability of a system, subsystem, equipment, process, or procedure to continue to function during and after a natural or man-made disturbance; e.g. nuclear electromagnetic pulse from the detonation of a nuclear weapon.
For a given application, survivability must be qualified by specifying the range of conditions over which the entity will survive, the minimum acceptable level or post-disturbance functionality, and the maximum acceptable downtime.
Military.
In the military environment, survivability is defined as the ability to remain mission capable after a single engagement. Engineers working in survivability are often responsible for improving four main system elements: 
The European Survivability Workshop introduced the concept of "Mission Survivability" whilst retaining the three core areas above, either pertaining to the "survivability" of a platform through a complete mission, or the "survivability" of the mission itself (i.e. probability of mission success). Recent studies have also introduced the concept of "Force Survivability" which relates to the ability of a force rather than an individual platform to remain "mission capable".
There is no clear prioritisation of the three elements; this will depend on the characteristics and role of the platform. Some platform types, such as submarines and airplanes, minimise their susceptibility and may, to some extent, compromise in the other areas. Main Battle Tanks minimise vulnerability through the use of heavy armours. Present day surface warship designs tend to aim for a balanced combination of all three areas.
Naval.
Survivability denotes the ability of a ship and its on-board systems to remain functional and continue designated mission in a man-made hostile environment. The naval vessels are designed to operate in a man-made hostile environment, and therefore the survivability is a vital feature required from them. The naval vessel’s survivability is a complicated subject affecting the whole life cycle of the vessel, and should be considered from the initial design phase of every war ship.
The classical definition of naval survivability includes three main aspects, which are susceptibility, vulnerability, and recoverability; although, recoverability is often subsumed within vulnerability.
Susceptibility consists of all the factors that expose the ship to the weapons effects in a combat environment. These factors in general are the operating conditions, the threat, and the features of the ship itself. The operating conditions, such as sea state, weather and atmospheric conditions, vary considerably, and their influence is difficult to address (hence they are often not accounted for in survivability assessment). The threat is dependent on the weapons directed against the ship and weapon’s performance, such as the range. The features of the ship in this sense include platform signatures (radar, infrared, acoustic, magnetic), the defensive systems on board, such as surface-to-air missiles, EW and decoys, and also the tactics employed by the platform in countering the attack (aspects such as speed, maneuverability, chosen aspect presented to the threat).
Vulnerability refers to the ability of the vessel to withstand the short-term effects of the threat weapon. Vulnerability is an attribute typical to the vessel and therefore heavily
affected by the vessel’s basic characteristics such as size, subdivision, armouring, and other hardening features, and also the design of the ship's systems, in particular the location of equipment, degrees of redundancy and separation, and the presence within a system of single point failures. Recoverability refers to vessel’s ability to restore and maintain its functionality after sustaining damage. Thus, recoverability is dependent on the actions aimed to neutralize the effects of the damage. These actions include firefighting, limiting the extent of flooding, and dewatering. Besides the equipment, the crew also has a vital role in recoverability.
Combat vehicle crew.
The crews of military combat vehicles face numerous lethal hazards which are both diverse and constantly evolving. Improvised Explosive Devices (IEDs), mines, and enemy fire are examples of such persistent and variable threats. Historically, measures taken to mitigate these hazards were concerned with protecting the vehicle itself, but due to this achieving only limited protection, the focus has now shifted to safeguarding the crew within from an ever-broadening range of threats, including 
Radio Controlled IEDs (RCIEDs), blast, fragmentation, heat stress, and dehydration.
The expressed goal of "crew survivability" is to ensure vehicle occupants are best protected. It goes beyond simply ensuring crew have the appropriate protective equipment and has expanded to include measuring the overpressure and blunt impact forces experienced by a vehicle from real blast incidents in order to develop medical treatment and improve overall crew survivability. Sustainable crew survivability is dependent on the effective integration of knowledge, training, and equipment:
Prevention and training.
Threat intelligence identifying trends, emerging technologies, and attack tactics used by enemy forces enables crews to implement procedures that will reduce their exposure to unnecessary risks. Such intelligence also allows for more effective pre-deployment training programs where personnel can be taught the most up-to-date developments in IED concealment, for example, or undertake tailored training that will enable them to identify the likely attack strategy of enemy forces. In addition, with expert, current threat intelligence, the most effective equipment can be procured or rapidly developed in support of operations.
Network.
Definitions of network survivability.
"The capability of a system to fulfill its mission, in a timely manner, in the presence of such as attacks or large-scale natural disasters. Survivability is a subset of resilience."
“The capability of a system to fulfill its mission, in a timely manner, in the presence of attacks, failures, or accidents.”

</doc>
<doc id="41765" url="https://en.wikipedia.org/wiki?curid=41765" title="Switched loop">
Switched loop

In telephony, a Switched loop is a circuit that automatically releases a connection from an attendant console or switchboard, once the connection has been made to the appropriate terminal. 
Loop buttons or jacks are used to answer incoming listed directory number calls, dial "0" internal calls, transfer requests, and intercepted calls. The attendant can handle only one telephone call at a time. "Synonym": released loop.

</doc>
<doc id="41767" url="https://en.wikipedia.org/wiki?curid=41767" title="Synchronism">
Synchronism

Synchronism is deliberately achieved coincidence at a specified point of time. 
Telecommunication.
In telecommunication the term synchronism has the following meanings: 
1. The state of being synchronous.
2. For repetitive events with the same, multiple, or submultiple repetition rates, a relationship among the events such that a significant instant of one event bears a fixed time relationship to a corresponding instant in another event.
3. The simultaneous occurrence of two or more events at the same instant on the same coordinated time standard.
Art and entertainment.
The term may refer to:

</doc>
<doc id="41768" url="https://en.wikipedia.org/wiki?curid=41768" title="Synchronizing">
Synchronizing

In telecommunication, the term synchronizing has the following meanings: 
In the civilian community, the noun ""synchronization" " is preferred to ""synchronizing"."

</doc>
<doc id="41769" url="https://en.wikipedia.org/wiki?curid=41769" title="Synchronous network">
Synchronous network

In telecommunications, a synchronous network is a network in which clocks are controlled to run, ideally, at identical rates, or at the same mean rate with a fixed relative phase displacement, within a specified limited range. 
Ideally, the clocks are synchronous, but they may be mesochronous in practice. By common usage, such mesochronous networks are frequently described as "synchronous".

</doc>
<doc id="41770" url="https://en.wikipedia.org/wiki?curid=41770" title="Synchronous orbit">
Synchronous orbit

A synchronous orbit is an orbit in which an orbiting body (usually a satellite) has a period equal to the average rotational period of the body being orbited (usually a planet), and in the same direction of rotation as that body.
Simplified meaning.
A synchronous orbit is where something (like a satellite) orbiting a body (like a planet) is orbiting an exact equal line around the object like, orbiting the equator thus orbiting the same speed the planet is spinning.
Properties.
A satellite in a synchronous orbit that is both equatorial and circular will appear to be suspended motionless above a point on the orbited planet's equator. For synchronous satellites orbiting Earth, this is also known as a geostationary orbit. However, a synchronous orbit need not be equatorial; nor circular. A body in a non-equatorial synchronous orbit will appear to oscillate north and south above a point on the planet's equator, whereas a body in an elliptical orbit will appear to oscillate eastward and westward. As seen from the orbited body the combination of these two motions produces a figure-8 pattern called an analemma.
Nomenclature.
There are many specialized terms for synchronous orbits depending on the body orbited. The following are some of the more common ones. A synchronous orbit around Earth that is circular and lies in the equatorial plane is called a geostationary orbit. The more general case, when the orbit is inclined to Earth's equator or is non-circular is called a geosynchronous orbit. The corresponding terms for synchronous orbits around Mars are areostationary and areosynchronous orbits.
Examples.
An astronomical example is Pluto's moon Charon.
Much more commonly, synchronous orbits are employed by artificial satellites used for communication, such as geostationary satellites.
For natural satellites, which can attain a synchronous orbit only by tidally locking their parent body, it always goes in hand with synchronous rotation of the satellite. This is because the smaller body becomes tidally locked faster, and by the time a synchronous orbit is achieved, it has had a locked synchronous rotation for a long time already.

</doc>
<doc id="41771" url="https://en.wikipedia.org/wiki?curid=41771" title="System integrity">
System integrity

In telecommunications, the term system integrity has the following meanings: 

</doc>
<doc id="41772" url="https://en.wikipedia.org/wiki?curid=41772" title="System lifecycle">
System lifecycle

The system lifecycle in systems engineering is a view of a system or proposed system that addresses all phases of its existence to include system conception, design and development, production and/or construction, distribution, operation, maintenance and support, retirement, phase-out and disposal.
Conceptual design.
The conceptual design stage is the stage where an identified need is examined, requirements for potential solutions are defined, potential solutions are evaluated and a system specification is developed. The system specification represents the technical requirements that will provide overall guidance for system design. Because this document determines all future development, the stage cannot be completed until a conceptual design review has determined that the system specification properly addresses the motivating need.
Key steps within the conceptual design stage include:
Preliminary system design.
During this stage of the system lifecycle, subsystems that perform the desired system functions are designed and specified in compliance with the system specification. Interfaces between subsystems are defined, as well as overall test and evaluation requirements. At the completion of this stage, a development specification is produced that is sufficient to perform detailed design and development.
Key steps within the preliminary design stage include:
Detail design and development.
This stage includes the development of detailed designs that brings initial design work into a completed with form of specifications. This work includes the specification of interfaces between the system and its intended environment and a comprehensive evaluation of the systems logistical, maintenance and support requirements. The detail design and development is responsible for producing the product, process and material specifications and may result in substantial changes to the development specification.
Key steps within the detail design and development stage include:
Production and construction.
During the production and/or construction stage the product is built or assembled in accordance with the requirements specified in the product, process and material specifications and is deployed and tested within the operational target environment. System assessments are conducted in order to correct deficiencies and adapt the system for continued improvement.
Key steps within the product construction stage include:
Utilization and support.
Once fully deployed, the system is used for its intended operational role and maintained within its operational environment.
Key steps within the utilization and support stage include:
Phase-out and disposal.
Effectiveness and efficiency of the system must be continuously evaluated to determine when the product has met its maximum effective lifecycle. Considerations include: Continued existence of operational need, matching between operational requirements and system performance, feasibility of system phase-out versus maintenance, and availability of alternative systems.

</doc>
<doc id="41773" url="https://en.wikipedia.org/wiki?curid=41773" title="Systems control">
Systems control

Systems control, in a communications system, is the control and implementation of a set of functions that:

</doc>
<doc id="41774" url="https://en.wikipedia.org/wiki?curid=41774" title="Systems design">
Systems design

Systems design is the process of defining the architecture, components, modules, interfaces, and data for a system to satisfy specified requirements. Systems design could be seen as the application of systems theory to product development. There is some overlap with the disciplines of systems analysis, systems architecture and systems engineering.
Overview.
If the broader topic of product development "blends the perspective of marketing, design, and manufacturing into a single approach to product development," then design is the act of taking the marketing information and creating the design of the product to be manufactured. Systems design is therefore the process of defining and developing systems to satisfy specified requirements of the user.
Until the 1990s, systems design had a crucial and respected role in the data processing industry. In the 1990s, standardization of hardware and software resulted in the ability to build modular systems. The increasing importance of software running on generic platforms has enhanced the discipline of software engineering.
Object-oriented analysis and design methods are becoming the most widely used methods for computer systems design. The UML has become the standard language in object-oriented analysis and design. It is widely used for modeling software systems and is increasingly used for high designing non-software systems and organizations.
Architectural design.
The architectural design of a system emphasizes on the design of the systems architecture which describes the structure, behavior, and more views of that system and analysis.
Logical design.
The logical design of a system pertains to an abstract representation of the data flows, inputs and outputs of the system. This is often conducted via modelling, using an over-abstract (and sometimes graphical) model of the actual system. In the context of systems, designs are included. Logical design includes entity-relationship diagrams (ER diagrams).
Physical design.
The physical design relates to the actual input and output processes of the system. This is explained in terms of how data is input into a system, how it is verified/authenticated, how it is processed, and how it is displayed.
In physical design, the following requirements about the system are decided.
Put another way, the physical portion of systems design can generally be broken down into three sub-tasks:
User Interface Design is concerned with how users add information to the system and with how the system presents information back to them. Data Design is concerned with how the data is represented and stored within the system. Finally, Process Design is concerned with how data moves through the system, and with how and where it is validated, secured and/or transformed as it flows into, through and out of the system. At the end of the systems design phase, documentation describing the three sub-tasks is produced and made available for use in the next phase.
Physical design, in this context, does not refer to the tangible physical design of an information system. To use an analogy, a personal computer's physical design involves input via a keyboard, processing within the CPU, and output via a monitor, printer, etc. It would not concern the actual layout of the tangible hardware, which for a PC would be a monitor, CPU, motherboard, hard drive, modems, video/graphics cards, USB slots, etc.
It involves a detailed design of a user and a product database structure processor and a control processor. The H/S personal specification is developed for the proposed system.
Alternative design methodologies.
Rapid application development (RAD).
Rapid application development (RAD) is a methodology in which a systems designer produces prototypes for an end-user. The end-user reviews the prototype, and offers feedback on its suitability. This process is repeated until the end-user is satisfied with the final system.
Joint application design (JAD).
Joint application design (JAD) is a methodology which evolved from RAD, in which a systems designer consults with a group consisting of the following parties:
JAD involves a number of stages, in which the group collectively develops an agreed pattern for the design and implementation of the system.

</doc>
<doc id="41775" url="https://en.wikipedia.org/wiki?curid=41775" title="Tactical communications">
Tactical communications

Tactical communications are military communications in which information of any kind, especially orders and military intelligence, are conveyed from one command, person, or place to another upon a battlefield, particularly during the conduct of combat. It includes any kind of delivery of information, whether verbal, written, visual or auditory, and can be sent in a variety of ways. In modern times, this is usually done by electronic means. Tactical communications do not include communications provided to tactical forces by the Defense Communications System to non-tactical military commands, to tactical forces by civil organizations, nor does it include strategic communication.
Early Means.
The earliest way of communicating with others in a battle was by the commander's voice or by human messenger. Someone would have to run from one commander to a subordinate to tell them what to do. Once the horse was domesticated messages could travel much faster. A very fast way to send information was to use either drums, trumpets or flags. Each sound or banner would have a pre-determined significance for the soldier who would respond accordingly. Auditory signals were only as effective, though, as the receiver's ability to hear them. The din of battle or long distances could make using noise less effective. They were also limited in the amount of information they could convey; the information must be simple, such as "attack" or "retreat".
Visual cues, such as flags or smoke signals required the receiver to have a clear line of sight to the signal, and know when and where to look for them. Intricate warning systems have though always been used such as scouting towers with fires to signal incoming threats - this could occur at the tactical as well as the strategic level. The armies of the 19th century used two flags in combinations that replicated the alphabet. This allowed commanders the ability to send any order they wanted as they needed to, but still relied on line-of-sight. During the Siege of Paris (1870-1871) the defending French effectively used carrier pigeons to relay information between tactical units.
The Wireless Revolution.
Although visual communication flew at the speed of light, it relied on a direct line of sight between the sender and the receiver. Telegraphs helped theater commanders to move large armies about, but one certainly could not count on using immobile telegraph lines on a changing battlefield. At the end of the 19th century the disparate units across any field were instantaneously joined to their commanders by the invention and mass production of the radio. At first the radio could only broadcast tones, so messages were sent via Morse code. The first field radios used by the United States Army saw action in the Spanish–American War (1898) and the Philippine Insurrection (1899-1902). At the same time as radios were deployed the field telephone was developed and made commercially viable. This caused a new signal occupation specialty to be developed: lineman.
The Digital Battlefield.
Security was a problem. If you broadcast your plans over radio waves, anyone with a similar radio listening to the same frequency could hear your plans. Advances in electronics, particularly during World War II, allowed for electronic scrambling of radio broadcasts, which permitted messages to be encrypted with ciphers too complex for humans to crack without the assistance of a similar, high-tech machine, such as the German Enigma machine. Once computer science advanced, large amounts of data could be sent over the airwaves in quick bursts of signals and more complex encryption was allowed.
Communication between armies were of course much more difficult before the electronic age and could only be achieved with messengers on horseback or by foot and with time delays according to the distance the messenger needed to travel. Advances in long-range communications aided the commander on the battlefield, for then they could receive news of any outside force or factor that could impact the conduct of a battle.
Sources.
History of Communications. Bakersfield, CA: William Penn School, 2011. http://library.thinkquest.org/5729/ 
Raines, Rebecca Robbins. "Getting the Message Through: A Branch History of the U.S. Army Signal Corps" (Washington: Center of Military History, US Army, 1999).
Rienzi, Thomas Matthew. "Vietnam Studies: Communications-Electronics 1962-1970. (Washington: Department of the Army, 1985.
"Signal Corps History." Augusta, GA: United States Army Signal Center, 2012. http://www.signal.army.mil/ocos/rdiv/histarch/schist.asp

</doc>
<doc id="41776" url="https://en.wikipedia.org/wiki?curid=41776" title="Tactical communications system">
Tactical communications system

In telecommunication, a tactical communications system is a communications system that (a) is used within, or in direct support of, tactical forces, (b) is designed to meet the requirements of changing tactical situations and varying environmental conditions, (c) provides securable communications, such as voice, data, and video, among mobile users to facilitate command and control within, and in support of, tactical forces, and (d) usually requires extremely short installation times, usually on the order of hours, in order to meet the requirements of frequent relocation. 

</doc>
<doc id="41777" url="https://en.wikipedia.org/wiki?curid=41777" title="Tactical data information link–A">
Tactical data information link–A

In telecommunication, a tactical data information link—A (TADIL—A) is a netted link in which one unit acts as a net control station and interrogates each unit by roll call. Once interrogated, that unit transmits its data to the net. This means that each unit receives all the information transmitted. This is a direct transfer of data and no relaying is involved.

</doc>
<doc id="41778" url="https://en.wikipedia.org/wiki?curid=41778" title="Tape relay">
Tape relay

A tape relay is a method of retransmitting teletypewriter traffic from one channel to another, in which messages arriving on an incoming channel are recorded in the form of perforated tape, this punched tape then being either fed directly and automatically into an outgoing channel, or manually transferred to an automatic transmitter for transmission on an outgoing channel. Tape relay, sometimes informally called "torn tape operation", was commonplace during much of the 20th century. 

</doc>
<doc id="41779" url="https://en.wikipedia.org/wiki?curid=41779" title="T-carrier">
T-carrier

The T-carrier is a member of the series of carrier systems developed by AT&T Bell Laboratories for digital transmission of multiplexed telephone calls. The first version, the Transmission System 1 (T-1), was introduced in 1962 in the Bell System, and could transmit up to 24 telephone calls simultaneously over a single transmission line of copper wire. Subsequent specifications carried multiples of the basic T1 (1.544 Mbit/s) data rates, such as T2 (6.312 Mbit/s) with 96 channels, T3 (44.736 Mbit/s) with 672 channels, and others.
Transmission System 1.
T-1 is a hardware specification for telecommunications trunking. A "trunk" is a single transmission channel between two points on the network: each point is either a switching center or a node (such as a telephone).
Initially, T-1 trunks were used only to connect major telephone exchanges, via the same twisted pair copper wire that the analog trunks used. If the exchanges were too far apart, a repeater boosted the signal.
Before the digital T-1 system, carrier wave systems such as 12-channel carrier systems worked by frequency division multiplexing; each call was an analog signal. A T-1 trunk could transmit 24 telephone calls at a time, because it used a digital carrier signal called Digital Signal 1 (DS-1). DS-1 is a communications protocol for multiplexing the bitstreams of up to 24 telephone calls, along with two special bits: a "framing bit" (for frame synchronization) and a "maintenance-signaling bit". T-1's maximum data transmission rate is 1.544 megabits per second.
Throughout Europe and most of the rest of the world there is a comparable transmission system called E-carrier, which is not directly compatible with T-carrier.
Legacy.
Existing frequency-division multiplexing carrier systems worked well for connections between distant cities, but required expensive modulators, demodulators and filters for every voice channel. For connections within metropolitan areas, Bell Labs in the late 1950s sought cheaper terminal equipment. Pulse-code modulation allowed sharing a coder and decoder among several voice trunks, so this method was chosen for the T1 system introduced into local use in 1961. In later decades, the cost of digital electronics declined to the point that an individual codec per voice channel became commonplace, but by then the other advantages of digital transmission had become entrenched.
The most common legacy of this system is the line rate speeds. "T1" now means any data circuit that runs at the original 1.544 Mbit/s line rate. Originally the T1 format carried 24 pulse-code modulated, time-division multiplexed speech signals each encoded in 64 kbit/s streams, leaving 8 kbit/s of framing information which facilitates the synchronization and demultiplexing at the receiver. The T2 and T3 circuit channels carry multiple T1 channels multiplexed, resulting in transmission rates of 6.312 and 44.736 Mbit/s, respectively. A T3 line comprises 28 T1 lines, each operating at total signaling rate of 1.544 Mbit/s. It is possible to get a fractional T3 line, meaning a T3 line with some of the 28 lines turned off, resulting in a slower transfer rate but typically at reduced cost.
Supposedly, the 1.544 Mbit/s rate was chosen because tests done by AT&T Long Lines in Chicago were conducted underground. The test site was typical of Bell System outside plant of the time in that, to accommodate loading coils, cable vault manholes were physically apart, which determined the repeater spacing. The optimum bit rate was chosen empirically—the capacity was increased until the failure rate was unacceptable, then reduced to leave a margin. Companding allowed acceptable audio performance with only seven bits per PCM sample in this original T1/D1 system. The later D3 and D4 channel banks had an extended frame format, allowing eight bits per sample, reduced to seven every sixth sample or frame when one bit was "robbed" for signaling the state of the channel. The standard does not allow an all zero sample which would produce a long string of binary zeros and cause the repeaters to lose bit sync. However, when carrying data (Switched 56) there could be long strings of zeros, so one bit per sample is set to "1" (jam bit 7) leaving 7 bits × 8,000 frames per second for data.
A more detailed understanding of how the rate of 1.544 Mbit/s was divided into channels is as follows. (This explanation glosses over T1 voice communications, and deals mainly with the numbers involved.) Given that the telephone system nominal voiceband (including guardband) is 4,000 Hz, the required digital sampling rate is 8,000 Hz (see Nyquist rate). Since each T1 frame contains 1 byte of voice data for each of the 24 channels, that system needs then 8,000 frames per second to maintain those 24 simultaneous voice channels. Because each frame of a T1 is 193 bits in length (24 channels × 8 bits per channel + 1 framing bit = 193 bits), 8,000 frames per second is multiplied by 193 bits to yield a transfer rate of 1.544 Mbit/s (8,000 × 193 = 1,544,000).
Initially, T1 used Alternate Mark Inversion (AMI) to reduce frequency bandwidth and eliminate the DC component of the signal. Later B8ZS became common practice. For AMI, each mark pulse had the opposite polarity of the previous one and each space was at a level of zero, resulting in a three level signal which however only carried binary data. Similar British 23 channel systems at 1.536 megabaud in the 1970s were equipped with ternary signal repeaters, in anticipation of using a 3B2T or 4B3T code to increase the number of voice channels in future, but in the 1980s the systems were merely replaced with European standard ones. American T-carriers could only work in AMI or B8ZS mode.
The AMI or B8ZS signal allowed a simple error rate measurement. The D bank in the central office could detect a bit with the wrong polarity, or "bipolarity violation" and sound an alarm. Later systems could count the number of violations and reframes and otherwise measure signal quality and allow a more sophisticated alarm indication signal system.
Historical note on the 193-bit T1 frame.
The decision to use a 193-bit frame was made in 1958. To allow for the identification of information bits within a frame, two alternatives were considered. Assign (a) just one extra bit, or (b) additional eight bits per frame. The 8-bit choice is cleaner, resulting in a 200-bit frame, twenty-five 8-bit channels, of which 24 are traffic and one 8-bit channel available for operations, administration, and maintenance (OA&M). AT&T chose the single bit per frame not to reduce the required bit rate (1.544 vs 1.6 Mbit/s), but because AT&T Marketing worried that "if 8 bits were chosen for OA&M function, someone would then try to sell this as a voice channel and you wind up with nothing."
Soon after commercial success of T1 in 1962, the T1 engineering team realized the mistake of having only one bit to serve the increasing demand for housekeeping functions. They petitioned AT&T management to change to 8-bit framing. This was flatly turned down because it would make installed systems obsolete.
Having this hindsight, some ten years later, CEPT chose eight bits for framing the European E1, although as feared the extra channel is sometimes appropriated for voice or data.
Higher T.
In the 1970s, Bell Labs developed higher rate systems. T-1C with a more sophisticated modulation scheme carried 3 Mbit/s, on those balanced pair cables that could support it. T-2 carried 6.312 Mbit/s, requiring a special low-capacitance cable with foam insulation. This was standard for Picturephone. T-4 and T-5 used coaxial cables, similar to the old L-carriers used by AT&T Long Lines. TD microwave radio relay systems were also fitted with high rate modems to allow them to carry a DS1 signal in a portion of their FM spectrum that had too poor quality for voice service. Later they carried DS3 and DS4 signals. During the 1980s companies such as RLH Industries, Inc. developed T1 over optical fiber. The industry soon developed and evolved with multiplexed T1 transmission schemes.
Digital signal cross-connect.
DS1 signals are interconnected typically at Central Office locations at a common metallic cross-connect point known as a 
DSX-1. When a DS1 is transported over metallic outside plant cable, the signal travels over conditioned cable pairs known as a T1 span. A T1 span can have up to +-130 Volts of DC power superimposed on the associated four wire cable pairs to line or "Span" power line repeaters, and T1 NIU's (T1 Smartjacks). T1 span repeaters are typically engineered up to apart, depending on cable gauge, and at no more than 36 dB of loss before requiring a repeated span. There can be no cable bridge taps or Load Coils across any pairs.
T1 copper spans are being replaced by optical transport systems, but if a copper (Metallic) span is used, the T1 is typically carried over an HDSL encoded copper line. Four wire HDSL does not require as many repeaters as conventional T1 spans. Newer two wire HDSL (HDSL-2) equipment transports a full 1.544 Mbit/s T1 over a single copper wire pair up to approximately twelve thousand (12,000) feet (3.5 km), if all 24 gauge cable is used. HDSL-2 does not employ multiple repeaters as does conventional four wire HDSL, or newer HDSL-4 systems.
One advantage of HDSL is its ability to operate with a limited number of bridge taps, with no tap being closer than from any HDSL transceiver. Both two or four wire HDSL equipment transmits and receives over the same cable wire pair, as compared to conventional T1 service that utilizes individual cable pairs for transmit or receive.
DS3 signals are rare except within buildings, where they are used for interconnections and as an intermediate step before being muxed onto a SONET circuit. This is because a T3 circuit can only go about between repeaters. A customer who orders a DS3 usually receives a SONET circuit run into the building and a multiplexer mounted in a utility box. The DS3 is delivered in its familiar form, two coax cables (1 for send and 1 for receive) with BNC connectors on the ends.
Sources:
Bit robbing.
Twelve DS1 frames make up a single T1 Superframe (T1 SF). Each T1 Superframe is composed of two signaling frames. All T1 DS0 channels that employ in-band signaling will have its eighth bit over written, or "robbed" from the full 64 kbit/s DS0 payload, by either a logical ZERO or ONE bit to signify a circuit signaling state or condition. Hence robbed bit signaling will restrict a DS0 channel to a rate of only 56 kbit/s during two of the twelve DS1 frames that make up a T1 SF framed circuit. T1 SF framed circuits yield two independent signaling channels (A&B) T1 ESF framed circuits four signaling frames in a twenty four frame extended frame format that yield four independent signaling channels (A, B, C, and D).
56 kbit/s DS0 channels are associated with digital data service (DDS) services typically do not utilize the eighth bit of the DS0 as voice circuits that employ A&B out of band signaling. One exception is Switched 56kbit/s DDS. In DDS, bit eight is used to identify DTE request to send (RTS) condition. With Switched 56 DDS, bit eight is pulsed (alternately set to logical ZERO and ONE) to transmit two state dial pulse signaling information between a SW56 DDS CSU/DSU, and a digital end office switch.
The use of robbed-bit signaling in America has decreased significantly as a result of Signaling System No 7 (SS7) on inter-office dial trunks. With SS7, the full 64 kbit/s DS0 channel is available for use on a connection, and allows 64 kbit/s, and 128 kbit/s ISDN data calls to exist over a switched trunk network connection if the supporting T1 carrier entity is optioned B8ZS (Clear Channel Capable).
Carrier pricing.
Carriers price DS1 lines in many different ways. However, most boil down to two simple components: local loop (the cost the local incumbent charges to transport the signal from the end user's central office, otherwise known as a CO, to the point of presence, otherwise known as a POP, of the carrier) and the port (the cost to access the telephone network or the Internet through the carrier's network). Typically, the port price is based upon access speed and yearly commitment level while the loop is based on geography. The farther the CO and POP, the more the loop costs.
The loop price has several components built into it, including the mileage calculation (performed in V/H coordinates, not standard GPS coordinates) and the telco piece. Each local Bell operating company—namely Verizon, AT&T Inc., and Qwest—charge T-carriers different price per mile rates. Therefore, the price calculation has two distance steps: geomapping and the determination of local price arrangements.
While most carriers utilize a geographic pricing model as described above, some Competitive Local Exchange Carriers (CLECs), such as TelePacific, Integra Telecom, tw telecom, Windstream, Level 3 Communications, and XO Communications offer national pricing.
Under this DS1 pricing model, a provider charges the same price in every geography it services. National pricing is an outgrowth of increased competition in the T-carrier market space and the commoditization of T-carrier products. Providers that have adopted a national pricing strategy may experience widely varying margins as their suppliers, the Bell operating companies (e.g., Verizon, AT&T Inc., and Qwest), maintain geographic pricing models, albeit at wholesale prices.
For voice DS1 lines, the calculation is mostly the same, except that the port (required for Internet access) is replaced by LDU (otherwise known as Long Distance Usage). Once the price of the loop is determined, only voice-related charges are added to the total. In short, the total price = loop + LDU x minutes used.

</doc>
<doc id="41781" url="https://en.wikipedia.org/wiki?curid=41781" title="Technical control facility">
Technical control facility

In telecommunication, a technical control facility (TCF) is a telecommunications facility, or a designated and specially configured part thereof, that (a) contains the equipment necessary for ensuring fast, reliable, and secure exchange of information, (b) typically includes distribution frames and associated panels, jacks, and switches and monitoring, test, conditioning, and orderwire equipment, and (c) allows telecommunications systems control personnel to exercise operational control of communications paths and facilities, make quality analyses of communications and communications channels, monitor operations and maintenance functions, recognize and correct deteriorating conditions, restore disrupted communications, provide requested on-call circuits, and take or direct such actions as may be required and practical to provide effective telecommunications services. 

</doc>
<doc id="41782" url="https://en.wikipedia.org/wiki?curid=41782" title="Telecommunications service">
Telecommunications service

In telecommunication, a telecommunications service is a service provided by a telecommunications provider, or a specified set of user-information transfer capabilities provided to a group of users by a telecommunications system.
The telecommunications service user is responsible for the information content of the message. The telecommunications service provider has the responsibility for the acceptance, transmission, and delivery of the message.
For purposes of regulation by the Federal Communications Commission under the U.S. Communications Act of 1934 and Telecommunications Act of 1996, the definition of telecommunications service is "the offering of telecommunications for a fee directly to the public, or to such classes of users as to be effectively available directly to the public, regardless of the facilities used." "Telecommunications", in turn, is defined as "the transmission, between or among points specified by the user, of information of the user’s choosing, without change in the form or content of the information as sent and received." 

</doc>
<doc id="41783" url="https://en.wikipedia.org/wiki?curid=41783" title="Teleconference">
Teleconference

The telecommunications system may support the teleconference by providing one or more of the following: audio, video, and/or data services by one or more means, such as telephone, computer, telegraph, teletypewriter, radio, and television.
Internet teleconferencing.
Internet teleconferencing includes internet telephone conferencing, videoconferencing, web conferencing, and augmented reality conferencing.
Internet telephony involves conducting a teleconference over the Internet or a Wide Area Network. One key technology in this area is Voice over Internet Protocol (VOIP). Popular software for personal use includes Skype, Google Talk, Windows Live Messenger and Yahoo! Messenger.
A working example of an augmented reality conferencing was demonstrated at the Salone di Mobile in Milano by AR+RFID Lab. is another AR teleconferencing tool.
Software and service providers.
Notable vendors with articles:

</doc>
<doc id="41784" url="https://en.wikipedia.org/wiki?curid=41784" title="Teletraining">
Teletraining

Teletraining is training that 
"Synonyms"

</doc>
<doc id="41785" url="https://en.wikipedia.org/wiki?curid=41785" title="Terminal adapter">
Terminal adapter

ISDN.
In ISDN terminology, a terminal adapter or TA is a device that connects a "terminal" (computer) to the ISDN network.
The TA therefore fulfills a similar function to the ones a modem has on the POTS network, and is therefore sometimes called an ISDN modem. The latter term, however, is partially misleading as there is no modulation or demodulation performed.
There are devices on the market that combine the functions of an ISDN TA with those of a classical modem (with an ISDN line interface). These combined TA/modems permit connections from both ISDN and analog-line/modem counterparts. In addition, a TA may contain an interface and codec for one or more analog telephone lines (aka "a/b line"), allowing an existing POTS installation to be upgraded to ISDN without changing phones.
Terminal adapters typically connect to a basic rate interface (S0, sometimes also U0). On the "terminal" side, the most popular interfaces are RS-232 serial and USB; others like V.35 or RS-449 are only of historical interest.
Devices connecting ISDN to a network (e.g. Ethernet) commonly include routing functionality; while they technically include a TA function, they are referred to as (ISDN) routers.
Mobile Networks.
In Mobile networks, the "terminal adapter" is used by the Terminal equipment to access the Mobile termination, using AT commands (see Hayes command set).
In 2G (such as GSM or CDMA), the terminal adapter is a theoretically optional while in 3G (such as W-CDMA), the terminal adapter is mandatory and is part of the Mobile Termination.
Automation industry.
In the automation industry, a terminal adapter is a passive device that converts a connector like the 8P8C (RJ-45) modular connector or 9 pin D-Sub into a terminal block to facilitate wiring. It is often used when daisy-chain wiring is necessary on a multi-node serial communication network like RS-485 or RS-422.

</doc>
<doc id="41786" url="https://en.wikipedia.org/wiki?curid=41786" title="Terminal equipment">
Terminal equipment

In telecommunication, the term terminal equipment has the following meanings:

</doc>
<doc id="41787" url="https://en.wikipedia.org/wiki?curid=41787" title="Ternary signal">
Ternary signal

In telecommunication, a ternary signal is a signal that can assume, at any given instant, one of three states or significant conditions, such as power level, phase position, pulse duration, or frequency.
Examples of ternary signals are (a) a pulse that can have a positive, zero, or negative voltage value at any given instant (PAM-3), (b) a sine wave that can assume phases of 0°, 120°, or 240° relative to a clock pulse (3-PSK), and (c) a carrier signal that can assume any one of three different frequencies depending on three different modulation signal significant conditions (3-FM).
Some examples of PAM-3 line codes that use ternary signals are:
3-PSK can be seen as falling between "binary phase-shift keying" (BPSK) which uses two phases, and "quadrature phase-shift keying" (QPSK) which uses four phases.

</doc>
<doc id="41789" url="https://en.wikipedia.org/wiki?curid=41789" title="Thermodynamic temperature">
Thermodynamic temperature

Thermodynamic temperature is the absolute measure of temperature and is one of the principal parameters of thermodynamics.
Thermodynamic temperature is defined by the third law of thermodynamics in which the theoretically lowest temperature is the null or zero point. At this point, absolute zero, the particle constituents of matter have minimal motion and can become no colder. In the quantum-mechanical description, matter at absolute zero is in its ground state, which is its state of lowest energy. Thermodynamic temperature is often also called absolute temperature, for two reasons: one, proposed by Kelvin, that it does not depend on the properties of a particular material; two that it refers to an absolute zero according to the properties of the ideal gas.
The International System of Units specifies a particular scale for thermodynamic temperature. It uses the Kelvin scale for measurement and selects the triple point of water at as the fundamental fixing point. Other scales have been in use historically. The Rankine scale, using the degree Fahrenheit as its unit interval, is still in use as part of the English Engineering Units in the United States in some engineering fields. ITS-90 gives a practical means of estimating the thermodynamic temperature to a very high degree of accuracy.
Roughly, the temperature of a body at rest is a measure of the mean of the energy of the translational, vibrational and rotational motions of matter's particle constituents, such as molecules, atoms, and subatomic particles. The full variety of these kinetic motions, along with potential energies of particles, and also occasionally certain other types of particle energy in equilibrium with these, make up the total internal energy of a substance. Internal energy is loosely called the heat energy or thermal energy in conditions when no work is done upon the substance by its surroundings, or by the substance upon the surroundings. Internal energy may be stored in a number of ways within a substance, each way constituting a "degree of freedom". At equilibrium, each degree of freedom will have on average the same energy: formula_1 where formula_2 is the Boltzmann constant, unless that degree of freedom is in the quantum regime. The internal degrees of freedom (rotation, vibration, etc.) may be in the quantum regime at room temperature, but the translational degrees of freedom will be in the classical regime except at extremely low temperatures (fractions of kelvins) and it may be said that, for most situations, the thermodynamic temperature is specified by the average translational kinetic energy of the particles.
Overview.
Temperature is a measure of the random submicroscopic motions and vibrations of the particle constituents of matter. These motions comprise the internal energy of a substance. More specifically, the thermodynamic temperature of any bulk quantity of matter is the measure of the average kinetic energy per classical (i.e., non-quantum) degree of freedom of its constituent particles. "Translational motions" are almost always in the classical regime. Translational motions are ordinary, whole-body movements in three-dimensional space in which particles move about and exchange energy in collisions. "Figure 1" below shows translational motion in gases; "" below shows translational motion in solids. Thermodynamic temperature's null point, absolute zero, is the temperature at which the particle constituents of matter are as close as possible to complete rest; that is, they have minimal motion, retaining only quantum mechanical motion. Zero kinetic energy remains in a substance at absolute zero (see "Thermal energy at absolute zero", below).
Throughout the scientific world where measurements are made in SI units, thermodynamic temperature is measured in kelvins (symbol: K). Many engineering fields in the U.S. however, measure thermodynamic temperature using the Rankine scale.
By international agreement, the unit "kelvin" and its scale are defined by two points: absolute zero, and the triple point of Vienna Standard Mean Ocean Water (water with a specified blend of hydrogen and oxygen isotopes). Absolute zero, the lowest possible temperature, is defined as being precisely 0 K "and" −273.15 °C. The triple point of water is defined as being precisely 273.16 K "and" 0.01 °C. This definition does three things:
Temperatures expressed in kelvins are converted to degrees Rankine simply by multiplying by 1.8 as follows: "T"°R = 1.8"T"K, where "T"K and "T"°R are temperatures in kelvin and degrees Rankine respectively. Temperatures expressed in degrees Rankine are converted to kelvins by "dividing" by 1.8 as follows: "T"K = .
Practical realization.
Although the Kelvin and Celsius scales are defined using absolute zero (0 K) and the triple point of water (273.16 K and 0.01 °C), it is impractical to use this definition at temperatures that are very different from the triple point of water. ITS-90 is then designed to represent the thermodynamic temperature as closely as possible throughout its range. Many different thermometer designs are required to cover the entire range. These include helium vapor pressure thermometers, helium gas thermometers, standard platinum resistance thermometers (known as SPRTs, PRTs or Platinum RTDs) and monochromatic radiation thermometers.
For some types of thermometer the relationship between the property observed (e.g., length of a mercury column) and temperature, is close to linear, so for most purposes a linear scale is sufficient, without point-by-point calibration. For others a calibration curve or equation is required. The mercury thermometer, invented before the thermodynamic temperature was understood, originally "defined" the temperature scale; its linearity made readings correlate well with true temperature, i.e. the "mercury" temperature scale was a close fit to the true scale.
The relationship of temperature, motions, conduction, and thermal energy.
The nature of kinetic energy, translational motion, and temperature.
The thermodynamic temperature is a measure of the average energy of the translational, vibrational, and rotational motions of matter's particle constituents (molecules, atoms, and subatomic particles). The full variety of these kinetic motions, along with potential energies of particles, and also occasionally certain other types of particle energy in equilibrium with these, contribute the total internal energy (loosely, the thermal energy) of a substance. Thus, internal energy may be stored in a number of ways (degrees of freedom) within a substance. When the degrees of freedom are in the classical regime ("unfrozen") the temperature is very simply related to the average energy of those degrees of freedom at equilibrium. The three translational degrees of freedom are unfrozen except for the very lowest temperatures, and their kinetic energy is simply related to the thermodynamic temperature over the widest range. The heat capacity, which relates heat input and temperature change, is discussed below.
The relationship of kinetic energy, mass, and velocity is given by the formula "Ek" = "mv"2. Accordingly, particles with one unit of mass moving at one unit of velocity have precisely the same kinetic energy, and precisely the same temperature, as those with four times the mass but half the velocity.
Except in the quantum regime at extremely low temperatures, the thermodynamic temperature of any "bulk quantity" of a substance (a statistically significant quantity of particles) is directly proportional to the mean average kinetic energy of a specific kind of particle motion known as "translational motion." These simple movements in the three "x", "y", and "z"–axis dimensions of space means the particles move in the three spatial "degrees of freedom." The temperature derived from this translational kinetic energy is sometimes referred to as "kinetic temperature" and is equal to the thermodynamic temperature over a very wide range of temperatures. Since there are three translational degrees of freedom (e.g., motion along the x, y, and z axes), the translational kinetic energy is related to the kinetic temperature by:
where:
While the Boltzmann constant is useful for finding the mean kinetic energy of a particle, it's important to note that even when a substance is isolated and in thermodynamic equilibrium (all parts are at a uniform temperature and no heat is going into or out of it), the translational motions of individual atoms and molecules occur across a wide range of speeds (see animation in "Figure 1" above). At any one instant, the proportion of particles moving at a given speed within this range is determined by probability as described by the Maxwell–Boltzmann distribution. The graph shown here in "Fig. 2 " shows the speed distribution of 5500 K helium atoms. They have a "most probable" speed of 4.780 km/s. However, a certain proportion of atoms at any given instant are moving faster while others are moving relatively slowly; some are momentarily at a virtual standstill (off the "x"–axis to the right). This graph uses "inverse speed" for its "x"–axis so the shape of the curve can easily be compared to the curves in "" below. In both graphs, zero on the "x"–axis represents infinite temperature. Additionally, the "x" and "y"–axis on both graphs are scaled proportionally.
The high speeds of translational motion.
Although very specialized laboratory equipment is required to directly detect translational motions, the resultant collisions by atoms or molecules with small particles suspended in a fluid produces Brownian motion that can be seen with an ordinary microscope. The translational motions of elementary particles are "very" fast and temperatures close to absolute zero are required to directly observe them. For instance, when scientists at the NIST achieved a record-setting cold temperature of 700 nK (billionths of a kelvin) in 1994, they used optical lattice laser equipment to adiabatically cool caesium atoms. They then turned off the entrapment lasers and directly measured atom velocities of 7 mm per second in order to calculate their temperature.  Formulas for calculating the velocity and speed of translational motion are given in the following footnote.
The internal motions of molecules and specific heat.
There are other forms of internal energy besides the kinetic energy of translational motion. As can be seen in the animation at right, molecules are complex objects; they are a population of atoms and thermal agitation can strain their internal chemical bonds in three different ways: via rotation, bond length, and bond angle movements. These are all types of "internal degrees of freedom". This makes molecules distinct from "monatomic" substances (consisting of individual atoms) like the noble gases helium and argon, which have only the three translational degrees of freedom. Kinetic energy is stored in molecules' internal degrees of freedom, which gives them an "internal temperature". Even though these motions are called "internal", the external portions of molecules still move—rather like the jiggling of a stationary water balloon. This permits the two-way exchange of kinetic energy between internal motions and translational motions with each molecular collision. Accordingly, as energy is removed from molecules, both their kinetic temperature (the temperature derived from the kinetic energy of translational motion) and their internal temperature simultaneously diminish in equal proportions. This phenomenon is described by the equipartition theorem, which states that for any bulk quantity of a substance in equilibrium, the kinetic energy of particle motion is evenly distributed among all the active (i.e. unfrozen) degrees of freedom available to the particles. Since the internal temperature of molecules are usually equal to their kinetic temperature, the distinction is usually of interest only in the detailed study of non-local thermodynamic equilibrium (LTE) phenomena such as combustion, the sublimation of solids, and the diffusion of hot gases in a partial vacuum.
The kinetic energy stored internally in molecules causes substances to contain more internal energy at any given temperature and to absorb additional internal energy for a given temperature increase. This is because any kinetic energy that is, at a given instant, bound in internal motions is not at that same instant contributing to the molecules' translational motions. This extra thermal energy simply increases the amount of energy a substance absorbs for a given temperature rise. This property is known as a substance's specific heat capacity.
Different molecules absorb different amounts of thermal energy for each incremental increase in temperature; that is, they have different specific heat capacities. High specific heat capacity arises, in part, because certain substances' molecules possess more internal degrees of freedom than others do. For instance, nitrogen, which is a diatomic molecule, has "five" active degrees of freedom at room temperature: the three comprising translational motion plus two rotational degrees of freedom internally. Since the two internal degrees of freedom are essentially unfrozen, in accordance with the equipartition theorem, nitrogen has five-thirds the specific heat capacity per mole (a specific number of molecules) as do the monatomic gases. Another example is gasoline (see table showing its specific heat capacity). Gasoline can absorb a large amount of thermal energy per mole with only a modest temperature change because each molecule comprises an average of 21 atoms and therefore has many internal degrees of freedom. Even larger, more complex molecules can have dozens of internal degrees of freedom.
The diffusion of thermal energy: Entropy, phonons, and mobile conduction electrons.
"Heat conduction "is the diffusion of thermal energy from hot parts of a system to cold. A system can be either a single bulk entity or a plurality of discrete bulk entities. The term "bulk" in this context means a statistically significant quantity of particles (which can be a microscopic amount). Whenever thermal energy diffuses within an isolated system, temperature differences within the system decrease (and entropy increases).
One particular heat conduction mechanism occurs when translational motion, the particle motion underlying temperature, transfers momentum from particle to particle in collisions. In gases, these translational motions are of the nature shown above in "Fig. 1. "As can be seen in that animation, not only does momentum (heat) diffuse throughout the volume of the gas through serial collisions, but entire molecules or atoms can move forward into new territory, bringing their kinetic energy with them. Consequently, temperature differences equalize throughout gases very quickly—especially for light atoms or molecules; convection speeds this process even more.
Translational motion in "solids", however, takes the form of "phonons "(see "Fig. 4" at right). Phonons are constrained, quantized wave packets that travel at a given substance's speed of sound. The manner in which phonons interact within a solid determines a variety of its properties, including its thermal conductivity. In electrically insulating solids, phonon-based heat conduction is "usually" inefficient and such solids are considered "thermal insulators" (such as glass, plastic, rubber, ceramic, and rock). This is because in solids, atoms and molecules are locked into place relative to their neighbors and are not free to roam.
Metals however, are not restricted to only phonon-based heat conduction. Thermal energy conducts through metals extraordinarily quickly because instead of direct molecule-to-molecule collisions, the vast majority of thermal energy is mediated via very light, mobile "conduction electrons." This is why there is a near-perfect correlation between metals' thermal conductivity and their electrical conductivity. Conduction electrons imbue metals with their extraordinary conductivity because they are "delocalized" (i.e., not tied to a specific atom) and behave rather like a sort of quantum gas due to the effects of "zero-point energy" (for more on ZPE, see "Note 1" below). Furthermore, electrons are relatively light with a rest mass only th that of a proton. This is about the same ratio as a .22 Short bullet (29 grains or 1.88 g) compared to the rifle that shoots it. As Isaac Newton wrote with his third law of motion,
However, a bullet accelerates faster than a rifle given an equal force. Since kinetic energy increases as the square of velocity, nearly all the kinetic energy goes into the bullet, not the rifle, even though both experience the same force from the expanding propellant gases. In the same manner, because they are much less massive, thermal energy is readily borne by mobile conduction electrons. Additionally, because they're delocalized and "very" fast, kinetic thermal energy conducts extremely quickly through metals with abundant conduction electrons.
The diffusion of thermal energy: Black-body radiation.
Thermal radiation is a byproduct of the collisions arising from various vibrational motions of atoms. These collisions cause the electrons of the atoms to emit thermal photons (known as black-body radiation). Photons are emitted anytime an electric charge is accelerated (as happens when electron clouds of two atoms collide). Even "individual molecules" with internal temperatures greater than absolute zero also emit black-body radiation from their atoms. In any bulk quantity of a substance at equilibrium, black-body photons are emitted across a range of wavelengths in a spectrum that has a bell curve-like shape called a Planck curve (see graph in "Fig. 5" at right). The top of a Planck curve (the peak emittance wavelength) is located in a particular part of the electromagnetic spectrum depending on the temperature of the black-body. Substances at extreme cryogenic temperatures emit at long radio wavelengths whereas extremely hot temperatures produce short gamma rays (see "Table of common temperatures").
Black-body radiation diffuses thermal energy throughout a substance as the photons are absorbed by neighboring atoms, transferring momentum in the process. Black-body photons also easily escape from a substance and can be absorbed by the ambient environment; kinetic energy is lost in the process.
As established by the Stefan–Boltzmann law, the intensity of black-body radiation increases as the fourth power of absolute temperature. Thus, a black-body at 824 K (just short of glowing dull red) emits "60 times" the radiant power as it does at 296 K (room temperature). This is why one can so easily feel the radiant heat from hot objects at a distance. At higher temperatures, such as those found in an incandescent lamp, black-body radiation can be the principal mechanism by which thermal energy escapes a system.
Table of thermodynamic temperatures.
The full range of the thermodynamic temperature scale, from absolute zero to absolute hot, and some notable points between them are shown in the table below.
A The 2500 K value is approximate.<br>
B For a true blackbody (which tungsten filaments are not). Tungsten filaments’ emissivity is greater at shorter wavelengths, which makes them appear whiter.<br>
C Effective photosphere temperature.<br>
D For a true blackbody (which the plasma was not). The Z machine’s dominant emission originated from 40 MK electrons (soft x–ray emissions) within the plasma.
The heat of phase changes.
The kinetic energy of particle motion is just one contributor to the total thermal energy in a substance; another is "phase transitions", which are the potential energy of molecular bonds that can form in a substance as it cools (such as during condensing and freezing). The thermal energy required for a phase transition is called "latent heat." This phenomenon may more easily be grasped by considering it in the reverse direction: latent heat is the energy required to "break" chemical bonds (such as during evaporation and melting). Almost everyone is familiar with the effects of phase transitions; for instance, steam at 100 °C can cause severe burns much faster than the 100 °C air from a hair dryer. This occurs because a large amount of latent heat is liberated as steam condenses into liquid water on the skin.
Even though thermal energy is liberated or absorbed during phase transitions, pure chemical elements, compounds, and eutectic alloys "exhibit no temperature change whatsoever" while they undergo them (see "Fig. 7," below right). Consider one particular type of phase transition: melting. When a solid is melting, crystal lattice chemical bonds are being broken apart; the substance is transitioning from what is known as a "more ordered state" to a "less ordered state". In "Fig. 7, "the melting of ice is shown within the lower left box heading from blue to green.
At one specific thermodynamic point, the melting point (which is 0 °C across a wide pressure range in the case of water), all the atoms or molecules are, on average, at the maximum energy threshold their chemical bonds can withstand without breaking away from the lattice. Chemical bonds are all-or-nothing forces: they either hold fast, or break; there is no in-between state. Consequently, when a substance is at its melting point, every joule of added thermal energy only breaks the bonds of a specific quantity of its atoms or molecules, converting them into a liquid of precisely the same temperature; no kinetic energy is added to translational motion (which is what gives substances their temperature). The effect is rather like popcorn: at a certain temperature, additional thermal energy can't make the kernels any hotter until the transition (popping) is complete. If the process is reversed (as in the freezing of a liquid), thermal energy must be removed from a substance.
As stated above, the thermal energy required for a phase transition is called "latent heat." In the specific cases of melting and freezing, it's called "enthalpy of fusion" or "heat of fusion." If the molecular bonds in a crystal lattice are strong, the heat of fusion can be relatively great, typically in the range of 6 to 30 kJ per mole for water and most of the metallic elements. If the substance is one of the monatomic gases, (which have little tendency to form molecular bonds) the heat of fusion is more modest, ranging from 0.021 to 2.3 kJ per mole. Relatively speaking, phase transitions can be truly energetic events. To completely melt ice at 0 °C into water at 0 °C, one must add roughly 80 times the thermal energy as is required to increase the temperature of the same mass of liquid water by one degree Celsius. The metals' ratios are even greater, typically in the range of 400 to 1200 times. And the phase transition of boiling is much more energetic than freezing. For instance, the energy required to completely boil or vaporize water (what is known as "enthalpy of vaporization") is roughly "540 times" that required for a one-degree increase.
Water's sizable enthalpy of vaporization is why one's skin can be burned so quickly as steam condenses on it (heading from red to green in "Fig. 7 "above). In the opposite direction, this is why one's skin feels cool as liquid water on it evaporates (a process that occurs at a sub-ambient wet-bulb temperature that is dependent on relative humidity). Water's highly energetic enthalpy of vaporization is also an important factor underlying why "solar pool covers" (floating, insulated blankets that cover swimming pools when not in use) are so effective at reducing heating costs: they prevent evaporation. For instance, the evaporation of just 20 mm of water from a 1.29-meter-deep pool chills its water 8.4 degrees Celsius (15.1 °F).
Internal energy.
The total energy of all particle motion translational and internal, including that of conduction electrons, plus the potential energy of phase changes, plus zero-point energy comprise the "internal energy" of a substance.
Internal energy at absolute zero.
As a substance cools, different forms of internal energy and their related effects simultaneously decrease in magnitude: the latent heat of available phase transitions is liberated as a substance changes from a less ordered state to a more ordered state; the translational motions of atoms and molecules diminish (their kinetic temperature decreases); the internal motions of molecules diminish (their internal temperature decreases); conduction electrons (if the substance is an electrical conductor) travel "somewhat" slower; and black-body radiation's peak emittance wavelength increases (the photons' energy decreases). When the particles of a substance are as close as possible to complete rest and retain only ZPE-induced quantum mechanical motion, the substance is at the temperature of absolute zero ("T"=0).
Note that whereas absolute zero is the point of zero thermodynamic temperature and is also the point at which the particle constituents of matter have minimal motion, absolute zero is not necessarily the point at which a substance contains zero thermal energy; one must be very precise with what one means by "internal energy". Often, all the phase changes that "can" occur in a substance, "will" have occurred by the time it reaches absolute zero. However, this is not always the case. Notably, "T"=0 helium remains liquid at room pressure and must be under a pressure of at least to crystallize. This is because helium's heat of fusion (the energy required to melt helium ice) is so low (only 21 joules per mole) that the motion-inducing effect of zero-point energy is sufficient to prevent it from freezing at lower pressures. Only if under at least of pressure will this latent thermal energy be liberated as helium freezes while approaching absolute zero. A further complication is that many solids change their crystal structure to more compact arrangements at extremely high pressures (up to millions of bars, or hundreds of gigapascals). These are known as "solid-solid phase transitions" wherein latent heat is liberated as a crystal lattice changes to a more thermodynamically favorable, compact one.
The above complexities make for rather cumbersome blanket statements regarding the internal energy in "T"=0 substances. Regardless of pressure though, what "can" be said is that at absolute zero, all solids with a lowest-energy crystal lattice such those with a "closest-packed arrangement" (see "Fig. 8," above left) contain minimal internal energy, retaining only that due to the ever-present background of zero-point energy.  One can also say that for a given substance at constant pressure, absolute zero is the point of lowest "enthalpy" (a measure of work potential that takes internal energy, pressure, and volume into consideration). Lastly, it is always true to say that all "T"=0 substances contain zero kinetic thermal energy. 
Practical applications for thermodynamic temperature.
Thermodynamic temperature is useful not only for scientists, it can also be useful for lay-people in many disciplines involving gases. By expressing variables in absolute terms and applying Gay–Lussac's law of temperature/pressure proportionality, solutions to everyday problems are straightforward; for instance, calculating how a temperature change affects the pressure inside an automobile tire. If the tire has a relatively cold pressure of 200 kPa-gage , then in absolute terms (relative to a vacuum), its pressure is 300 kPa-absolute. Room temperature ("cold" in tire terms) is 296 K. If the tire pressure is 20 °C hotter (20 kelvins), the solution is calculated as  = 6.8% greater thermodynamic temperature "and" absolute pressure; that is, a pressure of 320 kPa-absolute, which is 220 kPa-gage.
Definition of thermodynamic temperature.
The thermodynamic temperature is defined by the second law of thermodynamics and its consequences. The thermodynamic temperature can be shown to have special properties, and in particular can be seen to be uniquely defined (up to some constant multiplicative factor) by considering the efficiency of idealized heat engines. Thus the "ratio" "T"2/"T"1 of two temperatures"T"1 and"T"2 is the same in all absolute scales.
Strictly speaking, the temperature of a system is well-defined only if it is at thermal equilibrium. From a microscopic viewpoint, a material is at thermal equilibrium if the quantity of heat between its individual particles cancel out. There are many possible scales of temperature, derived from a variety of observations of physical phenomena.
Loosely stated, temperature differences dictate the direction of heat between two systems such that their combined energy is maximally distributed among their lowest possible states. We call this distribution "entropy". To better understand the relationship between temperature and entropy, consider the relationship between heat, work and temperature illustrated in the Carnot heat engine. The engine converts heat into work by directing a temperature gradient between a higher temperature heat source, "T"H, and a lower temperature heat sync, "T"C, through a gas filled piston. The work done per cycle is equal to the difference between the heat supplied to the engine by "T"H, "q"H, and the heat supplied to "T"C by the engine, "q"C. The efficiency of the engine is the work divided by the heat put into the system or
where wcy is the work done per cycle. Thus the efficiency depends only on qC/qH.
Carnot's theorem states that all reversible engines operating between the same heat reservoirs are equally efficient.
Thus, any reversible heat engine operating between temperatures "T"1 and "T"2 must have the same efficiency, that is to say, the efficiency is the function of only temperatures
In addition, a reversible heat engine operating between temperatures "T"1 and "T"3 must have the same efficiency as one consisting of two cycles, one between "T"1 and another (intermediate) temperature "T"2, and the second between "T"2 and"T"3. If this were not the case, then energy (in the form of "Q") will be wasted or gained, resulting in different overall efficiencies every time a cycle is split into component cycles; clearly a cycle can be composed of any number of smaller cycles.
With this understanding of "Q"1, "Q"2 and "Q"3, we note also that mathematically,
But the first function is "NOT" a function of "T"2, therefore the product of the final two functions "MUST" result in the removal of "T"2 as a variable. The only way is therefore to define the function f as follows:
and
so that
i.e. The ratio of heat exchanged is a function of the respective temperatures at which they occur. We can choose any monotonic function for our formula_12; it is a matter of convenience and convention that we choose formula_13. Choosing then "one" fixed reference temperature (i.e. triple point of water), we establish the thermodynamic temperature scale.
It is to be noted that such a definition coincides with that of the ideal gas derivation; also it is this "definition" of the thermodynamic temperature that enables us to represent the Carnot efficiency in terms of "T"H and "T"C, and hence derive that the (complete) Carnot cycle is isentropic:
Substituting this back into our first formula for efficiency yields a relationship in terms of temperature:
Notice that for "T"C=0 the efficiency is 100% and that efficiency becomes greater than 100% for "T"C<0, which cases are unrealistic. Subtracting the right hand side of Equation 4 from the middle portion and rearranging gives
where the negative sign indicates heat ejected from the system. The generalization of this equation is Clausius theorem, which suggests the existence of a state function "S" (i.e., a function which depends only on the state of the system, not on how it reached that state) defined (up to an additive constant) by
where the subscript indicates heat transfer in a reversible process. The function "S" corresponds to the entropy of the system, mentioned previously, and the change of "S" around any cycle is zero (as is necessary for any state function). Equation 5 can be rearranged to get an alternative definition for temperature in terms of entropy and heat (to avoid logic loop, we should first define entropy through statistical mechanics):
For a system in which the entropy "S" is a function "S"("E") of its energy "E", the thermodynamic temperature "T" is therefore given by
so that the reciprocal of the thermodynamic temperature is the rate of increase of entropy with energy.

</doc>
<doc id="41790" url="https://en.wikipedia.org/wiki?curid=41790" title="Third-order intercept point">
Third-order intercept point

In telecommunications, a third-order intercept point (IP3 or TOI) is a measure for weakly nonlinear systems and devices, for example receivers, linear amplifiers and mixers. It is based on the idea that the device nonlinearity can be modeled using a low-order polynomial, derived by means of Taylor series expansion. The third-order intercept point relates nonlinear products caused by the third-order nonlinear term to the linearly amplified signal, in contrast to the second-order intercept point that uses second order terms.
The intercept point is a purely mathematical concept, and does not correspond to a practically occurring physical power level. In many cases, it lies far beyond the damage threshold of the device.
Definitions.
Two different definitions for intercept points are in use:
It is worth noticing that these definitions differ by 9.5 dB (20 log10 3), so care should be taken when using existing equations, models or measurement data.
The intercept point is obtained graphically by plotting the output power versus the input power both on logarithmic scales (e.g., decibels). Two curves are drawn; one for the linearly amplified signal at an input tone frequency, one for a nonlinear product.
On a logarithmic scale, the function "xn" translates into a straight line with slope of "n". Therefore, the linearly amplified signal will exhibit a slope of 1. A third-order nonlinear product will increase by 3 dB in power when the input power is raised by 1 dB.
Both curves are extended with straight lines of slope 1 and n (3 for a third-order intercept point). The point where the curves intersect is the intercept point. It can be read off from the input or output power axis, leading to input or output intercept point, respectively (IIP3/OIP3).
Input and output intercept point differ by the small-signal gain of the device.
Practical considerations.
The concept of intercept point is based on the assumption of a weakly nonlinear system, meaning that higher-order nonlinear terms are small enough to be negligible.
In practice, the weakly nonlinear assumption may not hold for the upper end of the input power range, be it during measurement or during use of the amplifier. As a consequence, measured or simulated data will deviate from the ideal slope of "n".
The intercept point according to its basic definition should be determined by drawing the straight lines with slope 1 and n through the measured data at the smallest possible power level (possibly limited towards lower power levels by instrument or device noise).
It is a frequent mistake to derive intercept points by either changing the slope of the straight lines, or fitting them to points measured at too high a power level. In certain situations such a measure can be useful, but it is not an intercept point according to definition. Its value depends on the measurement conditions that need to be documented, whereas the IP according to definition is mostly unambiguous; although there is some dependency on frequency and tone spacing, depending on the physics of the device-under-test.
One of the useful applications of third order intercept point is as a rule-of-thumb measure to estimate nonlinear products. When comparing systems or devices for linearity, then, a higher intercept point is better. It can be seen that the spacing between two straight lines with slopes of 3 and 1 closes with slope 2.
For example, assume a device with an input-referred third-order intercept point of 10 dBm is driven with a test signal of −5 dBm. This power is 15 dB below the intercept point, therefore nonlinear products will appear at approximately 2x15 dB below the test signal power at the device output (in other words, 3×15 dB below the output-referred third-order intercept point).
A rule-of-thumb that holds for many linear radio frequency amplifiers is that the 1 dB compression point falls approximately 10 dB below the third-order intercept point.
Theory.
The third-order intercept point (TOI) is a property of the device transfer function "O" (see diagram).
This transfer function relates the output signal voltage level to the input signal voltage level. We assume a “linear” device having a transfer function whose small signal form may be expressed in terms of a power series containing only odd terms, making the transfer function an odd function of input signal voltage, i.e., "O"[−"s"("t")] = −"O"["s"("t")]. Where the signals passing through the actual device are modulated sinusoidal voltage waveforms (e.g., RF amplifier), device nonlinearities can be expressed in terms of how they affect individual sinusoidal signal components. For example, say the input voltage signal is the sine wave
and the device transfer function produces an output of the form
where "G" is the amplifier gain and "D"3 is cubic distortion. We may substitute the first equation into the second and, using the trigonometric identity
we obtain the device output voltage waveform as
The output waveform contains the original waveform, cos("ωt"), plus a new harmonic term, cos(3"ωt"), the "third-order". The coefficient of the cos("ωt") harmonic has two terms, one that varies linearly with "V" and one that varies with the cube of "V". In fact, the coefficient of cos("ωt") has nearly the same form as the transfer function, except for the factor ¾ on the cubic term. In other words, as signal level "V" is increased, the level of the cos("ωt") term in the output eventually levels off, similar to how the transfer function levels off. Of course, the coefficients of the higher-order harmonics will increase (with increasing "V") as the coefficient of the cos("ωt") term levels off (the power has to go somewhere).
If we now restrict our attention to that portion of the cos("ωt") coefficient which varies linearly with "V", and then ask ourselves, at what input voltage level, "V", will the coefficients of the first and third order terms have equal magnitudes (i.e., where the magnitudes intersect), we find that this happens when
which is the Third-Order Intercept Point (TOI). So, we see that the TOI input power level is simply 4/3 times the ratio of the gain and the cubic distortion term in the device transfer function. The smaller the cubic term is in relation to the gain, the more linear the device is and the higher the TOI is, which clearly makes sense. The TOI, being related to the magnitude squared of the input voltage waveform, is a power quantity, typically measured in milliwatts (mW). The TOI is always beyond operational power levels because the output power saturates before reaching this level.
The TOI is closely related to the amplifier's "1 dB compression point," which is defined as that point at which the "total" coefficient of the cos("ωt") term is 1 dB below the "linear portion" of that coefficient. We can relate the 1 dB compression point to the TOI as follows. Since 1 dB = 20 log10 1.122, we may say, in a voltage sense, that the 1 dB compression point occurs when
or
or
In a power sense ("V"2 is a power quantity), a factor of 0.10875 corresponds to −9.636 dB, so by this approximate analysis, the 1 dB compression point occurs roughly 9.6 dB below the TOI.
"Recall:" decibel figure = 10 dB × log10(power ratio) = 20 dB × log10(voltage ratio).

</doc>
<doc id="41791" url="https://en.wikipedia.org/wiki?curid=41791" title="Threshold">
Threshold

Threshold may refer to:

</doc>
<doc id="41792" url="https://en.wikipedia.org/wiki?curid=41792" title="Time-assignment speech interpolation">
Time-assignment speech interpolation

In telecommunication, a time-assignment speech interpolation (TASI) was an analog technique used on certain long transmission links to increase voice-transmission capacity.
TASI takes advantage of the fact that in typical person-person conversation, speech in a single direction occurs for approximately 40% of the time, the remaining time being occupied with pauses and/or silence. Statistical analysis demonstrated that for an average voice channel usage of 40%, over 74 speech conversations could be handled using 37 full Duplex speech circuits thereby doubling potential revenue for a small capital outlay relative to a highly expensive cable. e.g. £12.5 million (£263 million as of 2014) cost of the TAT-1 cable on which TASI was implemented.
TASI worked by switching additional users onto any channel temporarily idled because an original user has stopped speaking. When the original user resumes speaking, that user would, in turn, be switched to any channel that happened to be idle. The speech detector function is called voice activity detection. Clipping or loss of speech would occur for all conversations that needed to be assigned to an available idle channel and in practice lasted at least 17 ms whilst information required to re-connect both parties was signalled by the TASI control circuits. An additional freezeout period lasting between 0 and 500 ms would depend on the instantaneous loading of voice circuits. In actual use, these delays presented few problems in typical conversations.
One of the issues with using this type of technology was that the users listening on an idled channel can sometimes hear the conversation that has been switched onto it. Generally the sound heard was of very low volume and individual words are not distinguishable. See also crosstalk for a similar phenomenon in telecommunications. Another potential issue was ensuring that non-voice type circuits (e.g. Music or radio type circuits where pauses would occur infrequently) were not routed via TASI speech channels since these could seriously degrade the level of service where callers would encounter frequent clipped speech and breaks in the conversation.
TASI was invented by Bell Labs in the early 1960s to increase the capacity of transatlantic telephone cables. It was one of their first applications requiring electronic switching of voice circuits.
Later Digital Circuit Multiplication Equipment included TASI as a feature, not as distinct hardware.

</doc>
<doc id="41793" url="https://en.wikipedia.org/wiki?curid=41793" title="Time code ambiguity">
Time code ambiguity

In telecommunication, time code ambiguity is the shortest interval between successive repetitions of the same time code value.
For example, in a time code in which year-of-century (the '72' in 10/04/72) is the most slowly changing field, the time code ambiguity would be 100 years; it is ambiguous whether this value refers to a date in 1872, 1972 or some other century. For a digital clock in which hours and minutes up to a maximum of 11:59 are displayed, the time code ambiguity would be 12 hours.
The Year 2000 problem is an example of the pitfalls of time code ambiguity. Very often dates are now recorded with 4 digit years (10/04/1972). Assuming that the use of a 4-digit year field would continue, even in the far future, this would change the time code ambiguity from 100 years to 10 000 years.

</doc>
<doc id="41795" url="https://en.wikipedia.org/wiki?curid=41795" title="Minimum spanning tree">
Minimum spanning tree

A minimum spanning tree is a spanning tree of a connected, undirected graph. It connects all the vertices together with the minimal total weighting for its edges.
A single graph can have many different spanning trees. We can also assign a "weight" to each edge, which is a number representing how unfavorable it is, and use this to assign a weight to a spanning tree by computing the sum of the weights of the edges in that spanning tree. A minimum spanning tree (MST) or minimum weight spanning tree is then a spanning tree with weight less than or equal to the weight of every other spanning tree. More generally, any undirected graph (not necessarily connected) has a minimum spanning forest, which is a union of minimum spanning trees for its connected components.
There are quite a few use cases for minimum spanning trees. One example would be a telecommunications company which is trying to lay out cables in new neighborhood. If it is constrained to bury the cable only along certain paths (e.g. along roads), then there would be a graph representing which points are connected by those paths. Some of those paths might be more expensive, because they are longer, or require the cable to be buried deeper; these paths would be represented by edges with larger weights. Currency is an acceptable unit for edge weight — there is no requirement for edge lengths to obey normal rules of geometry such as the triangle inequality. A "spanning tree" for that graph would be a subset of those paths that has no cycles but still connects to every house; there might be several spanning trees possible. A "minimum spanning tree" would be one with the lowest total cost, thus would represent the least expensive path for laying the cable.
Properties.
Possible multiplicity.
"There may be several minimum spanning trees of the same weight having a minimum number of edges"; in particular, if all the edge weights of a given graph are the same, then every spanning tree of that graph is minimum.
If there are "n" vertices in the graph, then each spanning tree has "n"-1 edges.
Uniqueness.
"If each edge has a distinct weight then there will be only one, unique minimum spanning tree". This is true in many realistic situations, such as the telecommunications company example above, where it's unlikely any two paths have "exactly" the same cost. This generalizes to spanning forests as well.
If the edge weights are not unique, only the (multi-)set of weights in minimum spanning trees is unique, that is the same for all minimum spanning trees.
Proof:
Minimum-cost subgraph.
If the weights are "positive", then a minimum spanning tree is in fact a minimum-cost subgraph connecting all vertices, since subgraphs containing cycles necessarily have more total weight.
Cycle property.
"For any cycle "C" in the graph, if the weight of an edge "e" of "C" is larger than the individual weights of all other edges of "C", then this edge cannot belong to a MST."
Proof: Assume the contrary, i.e. that "e" belongs to an MST T1. Then deleting "e" will break T1 into two subtrees with the two ends of "e" in different subtrees. The remainder of "C" reconnects the subtrees, hence there is an edge "f" of "C" with ends in different subtrees, i.e., it reconnects the subtrees into a tree T2 with weight less than that of T1, because the weight of "f" is less than the weight of "e".
Cut property.
"For any cut "C" of the graph, if the weight of an edge "e" in the cut-set of "C" is strictly smaller than the weights of all other edges of the cut-set of "C", then this edge belongs to all MSTs of the graph."
Proof: assume the contrary, i.e., in the figure at right, make edge BC (weight 6) part of the MST T instead of edge e (weight 4). Adding e to T will produce a cycle, while replacing BC with e would produce MST of smaller weight. Thus, a tree containing BC is not a MST, a contradiction that violates our assumption. By a similar argument, if more than one edge is of minimum weight across a cut, then each such edge is contained in some minimum spanning tree.
Minimum-cost edge.
"If the edge of a graph with the minimum cost "e" is unique, then this edge is included in any MST."
Proof: if "e" was not included in the MST, removing any of the (larger cost) edges in the cycle formed after adding "e" to the MST, would yield a spanning tree of smaller weight.
Contraction.
"If T is a tree of MST edges, then we can "contract" T into a single vertex while maintaining the invariant that the MST of the contracted graph plus T gives the MST for the graph before contraction.
Algorithms.
In all of the algorithms below, "m" is the number of edges in the graph and "n" is the number of vertices.
Classic algorithms.
The first algorithm for finding a minimum spanning tree was developed by Czech scientist Otakar Borůvka in 1926 (see Borůvka's algorithm). Its purpose was an efficient electrical coverage of Moravia. The algorithm proceeds in a sequence of stages. In each stage, called "Boruvka step", it identifies a forest "F" consisting of the minimum-weight edge incident to each vertex in the graph "G", then forms the graph G1=G\F as the input to the next step. Here G\F denotes the graph derived from G by contracting edges in F (by the Cut property, these edges belong to the MST). Each Boruvka step takes linear time. Since the number of vertices is reduced by at least half in each step, Boruvka's algorithm takes O("m" log "n") time.
A second algorithm is Prim's algorithm, which was invented by Jarnik in 1930 and rediscovered by Prim in 1957 and Dijkstra in 1959. Basically, it grows the MST (T) one edge at a time. Initially, T contains an arbitrary vertex. In each step, T is augmented with the least-weight edge (x,y) such that x is in T and y is not yet in T. By the Cut property, all edges added to T are in the MST. Its run-time is either O("m" log "n") or O("m" + "n" log "n"), depending on the data-structures used.
A third algorithm commonly in use is the Kruskal's algorithm, which also takes O("m" log "n") time.
A fourth algorithm, not as commonly used, is the reverse-delete algorithm, which is the reverse of Kruskal's algorithm. Its runtime is O("m" log "n" (log log "n")3).
All these four are greedy algorithms. Since they run in polynomial time, the problem of finding such trees is in FP, and related decision problems such as determining whether a particular edge is in the MST or determining if the minimum total weight exceeds a certain value are in P.
Faster algorithms.
Several researchers have tried to find more computationally-efficient algorithms.
In a comparison model, in which the only allowed operations on edge weights are pairwise comparisons, found a linear time randomized algorithm based on a combination of Borůvka's algorithm and the reverse-delete algorithm.
The fastest non-randomized comparison-based algorithm with known complexity, by Bernard Chazelle, is based on the soft heap, an approximate priority queue. Its running time is "O"("m" α("m","n")), where α is the classical functional inverse of the Ackermann function. The function α grows extremely slowly, so that for all practical purposes it may be considered a constant no greater than 4; thus Chazelle's algorithm takes very close to linear time.
Linear-time algorithms in special cases.
Dense graphs.
If the graph is dense (i.e. "m"/"n" ≥ log log log "n"), then a deterministic algorithm by Fredman and Tarjan finds the MST in time O("m"). The algorithm executes a number of phases. Each phase executes Prim's algorithm many times, each for a limited number of steps. The run-time of each phase is O("m"+"n"). If the number of vertices before a phase is formula_2, the number of vertices remaining after a phase is at most formula_3. Hence, at most formula_4 phases are needed, which gives a linear run-time for dense graphs.
There are other algorithms that work in linear time on dense graphs.
Integer weights.
If the edge weights are integers represented in binary, then deterministic algorithms are known that solve the problem in "O"("m" + "n") integer operations.
Whether the problem can be solved "deterministically" for a "general graph" in "linear time" by a comparison-based algorithm remains an open question.
Decision trees.
Given graph "G" where the nodes and edges are fixed but the weights are unknown, it is possible to construct a binary decision tree (DT) for calculating the MST for any permutation of weights. Each internal node of the DT contains a comparison between two edges, e.g. "Is the weight of the edge between "x" and "y" larger than the weight of the edge between "w" and "z"?". The two children of the node correspond to the two possible answers "yes" or "no". In each leaf of the DT, there is a list of edges from "G" that correspond to an MST. The runtime complexity of a DT is the largest number of queries required to find the MST, which is just the depth of the DT. A DT for a graph "G" is called "optimal" if it has the smallest depth of all correct DTs for "G".
For every integer "r", it is possible to find optimal decision trees for all graphs on "r" vertices by brute-force search. This search proceeds in two steps.
A. Generating all potential DTs
B. Identifying the correct DTs
To check if a DT is correct, it should be checked on all possible permutations of the edge weights.
Hence, the total time required for finding an optimal DT for "all" graphs with "r" vertices is: formula_14, which is less than: formula_15.
Optimal algorithm.
Seth Pettie and Vijaya Ramachandran have found a provably optimal deterministic comparison-based minimum spanning tree algorithm. The following is a simplified description of the algorithm.
The runtime of all steps in the algorithm is O("m"), "except for the step of using the decision trees". We don't know the runtime of this step, but we know that it is optimal - no algorithm can do better than the optimal decision tree.
Thus, this algorithm has the peculiar property that it is "provably optimal" although its runtime complexity is "unknown".
Parallel and distributed algorithms.
Research has also considered parallel algorithms for the minimum spanning tree problem.
With a linear number of processors it is possible to solve the problem in formula_17 time.
Other specialized algorithms have been designed for computing minimum spanning trees of a graph so large that most of it must be stored on disk at all times. These "external storage" algorithms, for example as described in "Engineering an External Memory Minimum Spanning Tree Algorithm" by Roman, Dementiev et al., can operate, by authors' claims, as little as 2 to 5 times slower than a traditional in-memory algorithm. They rely on efficient external storage sorting algorithms and on graph contraction techniques for reducing the graph's size efficiently.
The problem can also be approached in a distributed manner. If each node is considered a computer and no node knows anything except its own connected links, one can still calculate the distributed minimum spanning tree.
MST on complete graphs.
Alan M. Frieze showed that given a complete graph on "n" vertices, with edge weights that are independent identically distributed random variables with distribution function formula_18 satisfying formula_19, then as "n" approaches +∞ the expected weight of the MST approaches formula_20, where formula_21 is the Riemann zeta function. Frieze and Steele also proved convergence in probability. Svante Janson proved a central limit theorem for weight of the MST.
For uniform random weights in formula_22, the exact expected size of the minimum spanning tree has been computed for small complete graphs.
Applications.
Minimum spanning trees have direct applications in the design of networks, including computer networks, telecommunications networks, transportation networks, water supply networks, and electrical grids (which they were first invented for, as mentioned above). They are invoked as subroutines in algorithms for other problems, including the Christofides algorithm for approximating the traveling salesman problem, approximating the multi-terminal minimum cut problem (which is equivalent in the single-terminal case to the maximum flow problem),
and approximating the minimum-cost weighted perfect matching.
Other practical applications based on minimal spanning trees include:
Related problems.
The problem of finding the Steiner tree of a subset of the vertices, that is, minimum tree that spans the given subset, is known to be NP-Complete.
A related problem is the "k"-minimum spanning tree ("k"-MST), which is the tree that spans some subset of "k" vertices in the graph with minimum weight.
A set of "k-smallest spanning trees" is a subset of "k" spanning trees (out of all possible spanning trees) such that no spanning tree outside the subset has smaller weight. (Note that this problem is unrelated to the "k"-minimum spanning tree.)
The Euclidean minimum spanning tree is a spanning tree of a graph with edge weights corresponding to the Euclidean distance between vertices which are points in the plane (or space).
The rectilinear minimum spanning tree is a spanning tree of a graph with edge weights corresponding to the rectilinear distance between vertices which are points in the plane (or space).
In the distributed model, where each node is considered a computer and no node knows anything except its own connected links, one can consider distributed minimum spanning tree. The mathematical definition of the problem is the same but there are different approaches for a solution.
The capacitated minimum spanning tree is a tree that has a marked node (origin, or root) and each of the subtrees attached to the node contains no more than a "c" nodes. "c" is called a tree capacity. Solving CMST optimally is NP-hard, but good heuristics such as Esau-Williams and Sharma produce solutions close to optimal in polynomial time.
The degree constrained minimum spanning tree is a minimum spanning tree in with each vertex is connected to no more than "d" other vertices, for some given number "d". The case "d" = 2 is a special case of the traveling salesman problem, so the degree constrained minimum spanning tree is NP-hard in general.
For directed graphs, the minimum spanning tree problem is called the Arborescence problem and can be solved in quadratic time using the Chu–Liu/Edmonds algorithm.
A maximum spanning tree is a spanning tree with weight greater than or equal to the weight of every other spanning tree.
Such a tree can be found with algorithms such as Prim's or Kruskal's after multiplying the edge weights by -1 and solving
the MST problem on the new graph. A path in the maximum spanning tree is the widest path in the graph between its two endpoints: among all possible paths, it maximizes the weight of the minimum-weight edge.
Maximum spanning trees find applications in parsing algorithms for natural languages
and in training algorithms for conditional random fields.
The dynamic MST problem concerns the update of a previously computed MST after an edge weight change in the original graph or the insertion/deletion of a vertex.
The minimum labeling spanning tree problem is to find a spanning tree with least types of labels if each edge in a graph is associated with a label from a finite label set instead of a weight.
A bottleneck edge is the highest weighted edge in a spanning tree. A spanning tree is a minimum bottleneck spanning tree (or MBST) if the graph does not contain a spanning tree with a smaller bottleneck edge weight. A MST is necessarily a MBST (provable by the cut property), but a MBST is not necessarily a MST.

</doc>
<doc id="41796" url="https://en.wikipedia.org/wiki?curid=41796" title="Time-division multiplexing">
Time-division multiplexing

Time-division multiplexing (TDM) is a method of transmitting and receiving independent signals over a common signal path by means of synchronized switches at each end of the transmission line so that each signal appears on the line only a fraction of time in an alternating pattern. It is used when the data rate of the transmission medium exceeds that of signal to be transmitted. This form of signal multiplexing was developed in telecommunications for telegraphy systems in the late 19th century, but found its most common application in digital telephony in the second half of the 20th century.
History.
Time-division multiplexing was first developed for applications in telegraphy to route multiple transmissions simultaneously over a single transmission line. In the 1870s, Émile Baudot developed a time-multiplexing system of multiple Hughes telegraph machines.
In 1953 a 24-channel TDM was placed in commercial operation by RCA Communications to send audio information between RCA's facility on Broad Street, New York, their transmitting station at Rocky Point and the receiving station at Riverhead, Long Island, New York. The communication was by a microwave system throughout Long Island. The experimental TDM system was developed by RCA Laboratories between 1950 and 1953.
In 1962, engineers from Bell Labs developed the first D1 channel banks, which combined 24 digitized voice calls over a four-wire copper trunk between Bell central office analogue switches. A channel bank sliced a 1.544 Mbit/s digital signal into 8,000 separate frames, each composed of 24 contiguous bytes. Each byte represented a single telephone call encoded into a constant bit rate signal of 64 kbit/s. Channel banks used the fixed position (temporal alignment) of one byte in the frame to identify the call it belonged to.
Technology.
Time-division multiplexing is used primarily for digital signals, but may be applied in analog multiplexing in which two or more signals or bit streams are transferred appearing simultaneously as sub-channels in one communication channel, but are physically taking turns on the channel. The time domain is divided into several recurrent "time slots" of fixed length, one for each sub-channel. A sample byte or data block of sub-channel 1 is transmitted during time slot 1, sub-channel 2 during time slot 2, etc. One TDM frame consists of one time slot per sub-channel plus a synchronization channel and sometimes error correction channel before the synchronization. After the last sub-channel, error correction, and synchronization, the cycle starts all over again with a new frame, starting with the second sample, byte or data block from sub-channel 1, etc.
Application examples.
TDM can be further extended into the time division multiple access (TDMA) scheme, where several stations connected to the same physical medium, for example sharing the same frequency channel, can communicate. Application examples include:
Multiplexed digital transmission.
In circuit-switched networks, such as the public switched telephone network (PSTN), it is desirable to transmit multiple subscriber calls over the same transmission medium to effectively utilize the bandwidth of the medium. TDM allows transmitting and receiving telephone switches to create channels ("tributaries") within a transmission stream. A standard DS0 voice signal has a data bit rate of 64 kbit/s. A TDM circuit runs at a much higher signal bandwidth, permitting the bandwidth to be divided into time frames (time slots) for each voice signal which is multiplexed onto the line by the transmitter. If the TDM frame consists of "n" voice frames, the line bandwidth is "n"*64 kbit/s.
Each voice time slot in the TDM frame is called a channel. In European systems, standard TDM frames contain 30 digital voice channels (E1), and in American systems (T1), they contain 24 channels. Both standards also contain extra bits (or bit time slots) for signaling and synchronization bits.
Multiplexing more than 24 or 30 digital voice channels is called "higher order multiplexing". Higher order multiplexing is accomplished by multiplexing the standard TDM frames. For example, a European 120 channel TDM frame is formed by multiplexing four standard 30 channel TDM frames. At each higher order multiplex, four TDM frames from the immediate lower order are combined, creating multiplexes with a bandwidth of "n"*64 kbit/s, where "n" = 120, 480, 1920, etc.
Telecommunications systems.
There are three types of synchronous TDM: T1, SONET/SDH, and ISDN.
Plesiochronous digital hierarchy (PDH) was developed as a standard for multiplexing higher order frames. PDH created larger numbers of channels by multiplexing the standard Europeans 30 channel TDM frames. This solution worked for a while; however PDH suffered from several inherent drawbacks which ultimately resulted in the development of the Synchronous Digital Hierarchy (SDH). The requirements which drove the development of SDH were these:
SDH has become the primary transmission protocol in most PSTN networks. It was developed to allow streams 1.544 Mbit/s and above to be multiplexed, in order to create larger SDH frames known as Synchronous Transport Modules (STM). The STM-1 frame consists of smaller streams that are multiplexed to create a 155.52 Mbit/s frame. SDH can also multiplex packet based frames e.g. Ethernet, PPP and ATM.
While SDH is considered to be a transmission protocol (Layer 1 in the OSI Reference Model), it also performs some switching functions, as stated in the third bullet point requirement listed above. The most common SDH Networking functions are these:
SDH network functions are connected using high-speed optic fibre. Optic fibre uses light pulses to transmit data and is therefore extremely fast. Modern optic fibre transmission makes use of wavelength-division multiplexing (WDM) where signals transmitted across the fibre are transmitted at different wavelengths, creating additional channels for transmission. This increases the speed and capacity of the link, which in turn reduces both unit and total costs.
Statistical time-division multiplexing.
Statistical time division multiplexing (STDM) is an advanced version of TDM in which both the address of the terminal and the data itself are transmitted together for better routing. Using STDM allows bandwidth to be split over one line. Many college and corporate campuses use this type of TDM to distribute bandwidth.
On a 10-Mbit line entering a network, STDM can be used to provide 178 terminals with a dedicated 56k connection (178 * 56k = 9.96Mb). A more common use however is to only grant the bandwidth when that much is needed. STDM does not reserve a time slot for each terminal, rather it assigns a slot when the terminal is requiring data to be sent or received.
In its primary form, TDM is used for circuit mode communication with a fixed number of channels and constant bandwidth per channel. Bandwidth reservation distinguishes time-division multiplexing from statistical multiplexing such as statistical time division multiplexing. In pure TDM, the time slots are recurrent in a fixed order and pre-allocated to the channels, rather than scheduled on a packet-by-packet basis.
In dynamic TDMA, a scheduling algorithm dynamically reserves a variable number of time slots in each frame to variable bit-rate data streams, based on the traffic demand of each data stream. Dynamic TDMA is used in:
Asynchronous time-division multiplexing (ATDM), is an alternative nomenclature in which STDM designates synchronous time-division multiplexing, the older method that uses fixed time slots.

</doc>
<doc id="41797" url="https://en.wikipedia.org/wiki?curid=41797" title="Time-domain reflectometer">
Time-domain reflectometer

A time-domain reflectometer (TDR) is an electronic instrument that uses time-domain reflectometry to characterize and locate faults in metallic cables (for example, twisted pair wire or coaxial cable). It can also be used to locate discontinuities in a connector, printed circuit board, or any other electrical path. The equivalent device for optical fiber is an optical time-domain reflectometer.
Description.
A TDR measures reflections along a conductor. In order to measure those reflections, the TDR will transmit an incident signal onto the conductor and listen for its reflections. If the conductor is of a uniform impedance and is properly terminated, then there will be no reflections and the remaining incident signal will be absorbed at the far-end by the termination. Instead, if there are impedance variations, then some of the incident signal will be reflected back to the source. A TDR is similar in principle to radar.
Reflection.
Generally, the reflections will have the same shape as the incident signal, but their sign and magnitude depend on the change in impedance level. If there is a step increase in the impedance, then the reflection will have the same sign as the incident signal; if there is a step decrease in impedance, the reflection will have the opposite sign. The magnitude of the reflection depends not only on the amount of the impedance change, but also upon the loss in the conductor.
The reflections are measured at the output/input to the TDR and displayed or plotted as a function of time. Alternatively, the display can be read as a function of cable length because the speed of signal propagation is almost constant for a given transmission medium.
Because of its sensitivity to impedance variations, a TDR may be used to verify cable impedance characteristics, splice and connector locations and associated losses, and estimate cable lengths.
Incident signal.
TDRs use different incident signals. Some TDRs transmit a pulse along the conductor; the resolution of such instruments is often the width of the pulse. Narrow pulses can offer good resolution, but they have high frequency signal components that are attenuated in long cables. The shape of the pulse is often a half cycle sinusoid. For longer cables, wider pulse widths are used.
Fast rise time steps are also used. Instead of looking for the reflection of a complete pulse, the instrument is concerned with the rising edge, which can be very fast. A 1970s technology TDR used steps with a rise time of 25 ps.
Still other TDRs transmit complex signals and detect reflections with correlation techniques. See spread-spectrum time-domain reflectometry.
Example traces.
These traces were produced by a time-domain reflectometer made from common lab equipment connected to approximately of coaxial cable having a characteristic impedance of 50 ohms. The propagation velocity of this cable is approximately 66 % of the speed of light in a vacuum.
These traces were produced by a commercial TDR using a step waveform with a 25 ps risetime, a sampling head with a 35 ps risetime, and an SMA cable. The far end of the SMA cable was left open or connected to different adapters. It takes about 3 ns for the pulse to travel down the cable, reflect, and reach the sampling head. A second reflection (at about 6 ns) can be seen in some traces; it is due to the reflection seeing a small mismatch at the sampling head and causing another "incident" wave to travel down the cable.
Explanation.
Consider the case where the far end of the cable is shorted (that is, terminated into zero ohms impedance). When the rising edge of the pulse is launched down the cable, the voltage at the launching point "steps up" to a given value instantly and the pulse begins propagating down the cable towards the short. When the pulse hits the short, no energy is absorbed at the far end. Instead, an opposing pulse reflects back from the short towards the launching end. It is only when this opposing reflection finally reaches the launch point that the voltage at this launching point abruptly drops back to zero, signalling the fact that there is a short at the end of the cable. That is, the TDR has no indication that there is a short at the end of the cable until its emitted pulse can travel down the cable at roughly the speed of light and the echo can return up the cable at the same speed. It is only after this round-trip delay that the short can be perceived by the TDR. Assuming that one knows the signal propagation speed in the particular cable-under-test, then in this way, the distance to the short can be measured.
A similar effect occurs if the far end of the cable is an open circuit (terminated into an infinite impedance). In this case, though, the reflection from the far end is polarized identically with the original pulse and adds to it rather than cancelling it out. So after a round-trip delay, the voltage at the TDR abruptly jumps to twice the originally-applied voltage.
Note that a theoretical perfect termination at the far end of the cable would entirely absorb the applied pulse without causing any reflection. In this case, it would be impossible to determine the actual length of the cable. Luckily, perfect terminations are very rare and some small reflection is nearly always caused.
The magnitude of the reflection is referred to as the reflection coefficient or ρ. The coefficient ranges from 1 (open circuit) to -1 (short circuit). The value of zero means that there is no reflection. The reflection coefficient is calculated as follows:
formula_1
Where Zo is defined as the characteristic impedance of the transmission medium and Zt is the impedance of the termination at the far end of the transmission line.
Any discontinuity can be viewed as a termination impedance and substituted as Zt. This includes abrupt changes in the characteristic impedance. As an example, a trace width on a printed circuit board doubled at its midsection would constitute a discontinuity. Some of the energy will be reflected back to the driving source; the remaining energy will be transmitted. This is also known as a scattering junction.
Usage.
Time domain reflectometers are commonly used for in-place testing of very long cable runs, where it is impractical to dig up or remove what may be a kilometers-long cable. They are indispensable for preventive maintenance of telecommunication lines, as TDRs can detect resistance on joints and connectors as they corrode, and increasing insulation leakage as it degrades and absorbs moisture, long before either leads to catastrophic failures. Using a TDR, it is possible to pinpoint a fault to within centimetres.
TDRs are also very useful tools for technical surveillance counter-measures, where they help determine the existence and location of wire taps. The slight change in line impedance caused by the introduction of a tap or splice will show up on the screen of a TDR when connected to a phone line.
TDR equipment is also an essential tool in the failure analysis of modern high-frequency printed circuit boards with signal traces crafted to emulate transmission lines. By observing reflections, any unsoldered pins of a ball grid array device can be detected. Short circuited pins can also be detected in a similar fashion.
The TDR principle is used in industrial settings, in situations as diverse as the testing of integrated circuit packages to measuring liquid levels. In the former, the time domain reflectometer is used to isolate failing sites in the same. The latter is primarily limited to the process industry.
TDR in level measurement.
In a TDR-based level measurement device, the device generates an impulse that propagates down a thin waveguide (referred to as a probe) – typically a metal rod or a steel cable. When this impulse hits the surface of the medium to be measured, part of the impulse reflects back up the waveguide. The device determines the fluid level by measuring the time difference between when the impulse was sent and when the reflection returned. The sensors can output the analyzed level as a continuous analog signal or switch output signals. In TDR technology, the impulse velocity is primarily affected by the permittivity of the medium through which the pulse propagates, which can vary greatly by the moisture content and temperature of the medium. In many cases, this effect can be corrected without undue difficulty. In some cases, such as in boiling and/or high temperature environments, the correction can be difficult. In particular, determining the froth (foam) height and the collapsed liquid level in a frothy / boiling medium can be very difficult.
TDR used in anchor cables in dams.
The Dam Safety Interest Group of CEA Technologies, Inc. (CEATI), a consortium of electrical power organizations, has applied Spread-spectrum time-domain reflectometry to identify potential faults in concrete dam anchor cables. The key benefit of Time Domain reflectometry over other testing methods is the non-destructive method of these tests.
TDR used in the earth and agricultural sciences.
A TDR is used to determine moisture content in soil and porous media. Over the last two decades, substantial advances have been made measuring moisture in soil, grain, food stuff, and sediment. The key to TDR’s success is its ability to accurately determine the permittivity (dielectric constant) of a material from wave propagation, due to the strong relationship between the permittivity of a material and its water content, as demonstrated in the pioneering works of Hoekstra and Delaney (1974) and Topp et al. (1980). Recent reviews and reference work on the subject include, Topp and Reynolds (1998), Noborio (2001), Pettinellia et al. (2002), Topp and Ferre (2002) and Robinson et al. (2003). The TDR method is a transmission line technique, and determines apparent permittivity (Ka) from the travel time of an electromagnetic wave that propagates along a transmission line, usually two or more parallel metal rods embedded in soil or sediment. The probes are typically between 10 and 30 cm long and connected to the TDR via coaxial cable.
TDR in geotechnical usage.
Time domain reflectometry has also been utilized to monitor slope movement in a variety of geotechnical settings including highway cuts, rail beds, and open pit mines (Dowding & O'Connor, 1984, 2000a, 2000b; Kane & Beck, 1999). In stability monitoring applications using TDR, a coaxial cable is installed in a vertical borehole passing through the region of concern. The electrical impedance at any point along a coaxial cable changes with deformation of the insulator between the conductors. A brittle grout surrounds the cable to translate earth movement into an abrupt cable deformation that shows up as a detectable peak in the reflectance trace. Until recently, the technique was relatively insensitive to small slope movements and could not be automated because it relied on human detection of changes in the reflectance trace over time. Farrington and Sargand (2004) developed a simple signal processing technique using numerical derivatives to extract reliable indications of slope movement from the TDR data much earlier than by conventional interpretation.
Another application of TDRs in geotechnical engineering is to determine the soil moisture content. This can be done by placing the TDRs in different soil layers and measurement of the time of start of precipitation and the time that TDR indicate an increase in the soil moisture content. The depth of the TDR (d) is a known factor and the other is the time it takes the drop of water to reach that depth (t); therefore the speed of water Infiltration (hydrology) (v) can be determined. This is a good method to assess the effectiveness of Best Management Practices (BMPs) in reducing stormwater Surface runoff.
TDR in semiconductor device analysis.
Time domain reflectometry is used in semiconductor failure analysis as a non-destructive method for the location of defects in semiconductor device packages. The TDR provides an electrical signature of individual conductive traces in the device package, and is useful for determining the location of opens and shorts.
TDR in aviation wiring maintenance.
Time domain reflectometry, specifically spread-spectrum time-domain reflectometry is used on aviation wiring for both preventative maintenance and fault location. Spread spectrum time domain reflectometry has the advantage of precisely locating the fault location within thousands of miles of aviation wiring. Additionally, this technology is worth considering for real time aviation monitoring, as spread spectrum reflectometry can be employed on live wires.
This method has been shown to be useful to locating intermittent electrical faults.

</doc>
<doc id="41799" url="https://en.wikipedia.org/wiki?curid=41799" title="Time standard">
Time standard

A time standard is a specification for measuring time: either the rate at which time passes; or points in time; or both. In modern times, several time specifications have been officially recognized as standards, where formerly they were matters of custom and practice. An example of a kind of time standard can be a time scale, specifying a method for measuring divisions of time. A standard for civil time can specify both time intervals and time-of-day.
Standardized time measurements are made using a clock to count periods of some cyclic change, which may be either the changes of a natural phenomenon or of an artificial machine.
Historically, time standards were often based on the Earth's rotational period. From the late 17th century to the 19th century it was assumed that the Earth's daily rotational rate was constant. Astronomical observations of several kinds, including eclipse records, studied in the 19th century, raised suspicions that the rate at which Earth rotates is gradually slowing and also shows small-scale irregularities, and this was confirmed in the early twentieth century. Time standards based on Earth rotation were replaced (or initially supplemented) for astronomical use from 1952 onwards by an "ephemeris time" standard based on the Earth's orbital period and in practice on the motion of the Moon. The invention in 1955 of the caesium atomic clock has led to the replacement of older and purely astronomical time standards, for most practical purposes, by newer time standards based wholly or partly on atomic time.
Various types of second and day are used as the basic time interval for most time scales. Other intervals of time (minutes, hours, and years) are usually defined in terms of these two.
Time standards based on Earth rotation.
Apparent solar time ('apparent' is often used in English-language sources, but 'true' in French astronomical literature) is based on the solar day, which is the period between one solar noon (passage of the real Sun across the meridian) and the next. A solar day is approximately 24 hours of mean time. Because the Earth's orbit around the sun is elliptical, and because of the obliquity of the Earth's axis relative to the plane of the orbit (the ecliptic), the apparent solar day varies a few dozen seconds above or below the mean value of 24 hours. As the variation accumulates over a few weeks, there are differences as large as 16 minutes between apparent solar time and mean solar time (see Equation of time). However, these variations cancel out over a year. There are also other perturbations such as Earth's wobble, but these are less than a second per year.
Sidereal time is time by the stars. A sidereal rotation is the time it takes the Earth to make one revolution with respect to the stars, approximately 23 hours 56 minutes 4 seconds. For accurate astronomical work on land, it was usual to observe sidereal time rather than solar time to measure mean solar time, because the observations of 'fixed' stars could be measured and reduced more accurately than observations of the Sun (in spite of the need to make various small compensations, for refraction, aberration, precession, nutation and proper motion). It is well known that observations of the Sun pose substantial obstacles to the achievement of accuracy in measurement. In former times, before the distribution of accurate time signals, it was part of the routine work at any observatory to observe the sidereal times of meridian transit of selected 'clock stars' (of well-known position and movement), and to use these to correct observatory clocks running local mean sidereal time; but nowadays local sidereal time is usually generated by computer, based on time signals.
Mean solar time was originally apparent solar time corrected by the equation of time. Mean solar time was sometimes derived, especially at sea for navigational purposes, by observing apparent solar time and then adding to it a calculated correction, the equation of time, which compensated for two known irregularities, caused by the ellipticity of the Earth's orbit and the obliquity of the Earth's equator and polar axis to the ecliptic (which is the plane of the Earth's orbit around the sun).
Greenwich Mean Time (GMT) was originally mean time deduced from meridian observations made at the Royal Greenwich Observatory (RGO). The principal meridian of that observatory was chosen in 1884 by the International Meridian Conference to be the Prime Meridian. GMT either by that name or as 'mean time at Greenwich' used to be an international time standard, but is no longer so; it was initially renamed in 1928 as Universal Time (UT) (partly as a result of ambiguities arising from the changed practice of starting the astronomical day at midnight instead of at noon, adopted as from 1 January 1925). The more current refined version of UT, UT1, is still in reality mean time at Greenwich. Greenwich Mean Time is still the legal time in the UK (in winter, and as adjusted by one hour for summer time). But Coordinated Universal Time (UTC) (an atomic-based time scale which is always kept within 0.9 second of UT1) is in common actual use in the UK, and the name GMT is often inaccurately used to refer to it. (See articles Greenwich Mean Time, Universal Time, Coordinated Universal Time and the sources they cite.)
Universal Time (UT) is mean solar time at 0° longitude; some implementations are
Time standards for planetary motion calculations.
Ephemeris time and its successor time scales described below have all been intended for astronomical use, e.g. in planetary motion calculations, with aims including uniformity, in particular, freedom from irregularities of Earth rotation. Some of these standards are examples of dynamical time scales and/or of coordinate time scales.
For applications at the Earth's surface, ET's official replacement was Terrestrial Dynamical Time (TDT), since redefined as Terrestrial Time (TT). For the calculation of ephemerides, TDB was officially recommended to replace ET, but deficiencies were found in the definition of TDB (though not affecting Teph), and these led to the IAU defining and recommending further time scales, Barycentric Coordinate Time (TCB) for use in the solar system as a whole, and Geocentric Coordinate Time (TCG) for use in the vicinity of the Earth. As defined, TCB (as observed from the Earth's surface) is of divergent rate relative to all of ET, Teph and TDT/TT; and the same is true, to a lesser extent, of TCG. The ephemerides of sun, moon and planets in current widespread and official use continue to be those calculated at the Jet Propulsion Laboratory (updated as from 2003 to DE405) using as argument Teph. 
In 1991, in order to clarify the relationships between space-time coordinates, new time scales were introduced, each with a different frame of reference. Terrestrial Time is time at Earth's surface. Geocentric Coordinate Time is a coordinate time scale at Earth's center. Barycentric Coordinate Time is a coordinate time scale at the center of mass of the solar system, which is called the barycenter. Barycentric Dynamical Time is a dynamical time at the barycenter.
Constructed time standards.
International Atomic Time (TAI) is the primary international time standard from which other time standards, including UTC, are calculated. TAI is kept by the BIPM (International Bureau of Weights and Measures), and is based on the combined input of many atomic clocks around the world, each corrected for environmental and relativistic effects. It is the primary realisation of Terrestrial Time.
Coordinated Universal Time (UTC) is an atomic time scale designed to approximate Universal Time. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the "leap second". To date these steps have always been positive.
Standard time or civil time in a region deviates a fixed, round amount, usually a whole number of hours, from some form of Universal Time, now usually UTC. The offset is chosen such that a new day starts approximately while the sun is crossing the nadir meridian. See Time zone. Alternatively the difference is not really fixed, but it changes twice a year a round amount, usually one hour, see Daylight saving time.
Other time scales.
Julian day number is a count of days elapsed since Greenwich mean noon on 1 January 4713 B.C., Julian proleptic calendar. The Julian Date is the Julian day number followed by the fraction of the day elapsed since the preceding noon. Conveniently for astronomers, this avoids the date skip during an observation night.
Modified Julian day (MJD) is defined as MJD = JD - 2400000.5. An MJD day thus begins at midnight, civil date. Julian dates can be expressed in UT, TAI, TDT, etc. and so for precise applications the timescale should be specified, e.g. MJD 49135.3824 TAI.

</doc>
<doc id="41800" url="https://en.wikipedia.org/wiki?curid=41800" title="T interface">
T interface

A T-interface or T reference point is used for basic rate access in an Integrated Services Digital Network (ISDN) environment. It is a User–network interface reference point that is characterized by a four-wire, 144 kbit/s (2B+D) user rate.
Other characteristics of a T-interface are:
The T interface is electrically equivalent to the S interface, and the two are jointly referred to as the S/T interface.

</doc>
<doc id="41801" url="https://en.wikipedia.org/wiki?curid=41801" title="Toll switching trunk">
Toll switching trunk

In telecommunication, a toll switching trunk or toll connecting trunk is a trunk connecting one or more end offices to a toll center as the first stage of concentration for intertoll or long distance traffic. 
Operator assistance or participation may be an optional function. In U.S. common carrier telephony service, a toll center designated "Class 4C" is an office where assistance in completing incoming calls is provided in addition to other traffic; a toll center designated "Class 4P" is an office where operators handle only outbound calls, or where switching is performed without operator assistance.

</doc>
<doc id="41802" url="https://en.wikipedia.org/wiki?curid=41802" title="Total harmonic distortion">
Total harmonic distortion

The total harmonic distortion, or THD, of a signal is a measurement of the harmonic distortion present and is defined as the ratio of the sum of the powers of all harmonic components to the power of the fundamental frequency. THD is used to characterize the linearity of audio systems and the power quality of electric power systems. Distortion factor is a closely related term, sometimes used as a synonym.
In audio systems, lower distortion means the components in a loudspeaker, amplifier or microphone or other equipment produce a more accurate reproduction of an audio recording.
In radiocommunications, lower THD means pure signal emission without causing interferences to other
electronic devices. Moreover, the problem of distorted and not eco-friendly radio emissions appear to be also very important in the context of spectrum sharing and spectrum sensing.
In power systems, lower THD means reduction in peak currents, heating, emissions, and core loss in motors.
Definitions and examples.
To understand a system with an input and an output, such as an audio amplifier, we start with an ideal system where the transfer function is linear and time-invariant. When a signal passes through a non-ideal, non-linear device, additional content is added at the harmonics of the original frequencies. THD is a measurement of the extent of that distortion.
When the main performance criterion is the ″purity″ of the original sine wave (in other words, the contribution of the original frequency with respect to its harmonics), the measurement is most commonly defined as the ratio of the RMS amplitude of a set of higher harmonic frequencies to the RMS amplitude of the first harmonic, or fundamental, frequency
where "Vn" is the RMS voltage of "n"th harmonic and "n" = 1 is the fundamental frequency.
In practice, the THDF is commonly used in audio distortion specifications (percentage THD); however, THD is a non-standardized specification and the results between manufacturers are not easily comparable. Since individual harmonic amplitudes are measured, it is required that the manufacturer disclose the test signal frequency range, level and gain conditions, and number of measurements taken. It is possible to measure the full 20–20 kHz range using a sweep (though distortion for a fundamental above 10 kHz is inaudible). For all signal processing equipment, except microphone preamplifiers, the preferred gain setting is unity. For microphone preamplifiers, standard practice is to use maximum gain.
Measurements for calculating the THD are made at the output of a device under specified conditions. The THD is usually expressed in percent or in dB relative to the fundamental as distortion attenuation.
A variant definition uses the fundamental plus harmonics as the reference, though usage is discouraged:
These can be distinguished as THDF (for "fundamental"), and THDR (for "root mean square"). THDR cannot exceed 100%. At low distortion levels, the difference between the two calculation methods is negligible. For instance, a signal with THDF of 10% has a very similar THDR of 9.95%. However, at higher distortion levels the discrepancy becomes large. For instance, a signal with THDF 266% has a THDR of 94%. A pure square wave with infinite harmonics has THDF of 48.3%, or THDR of 43.5%.
Some use the term "distortion factor" as a synonym for THDR, while others use it as a synonym for THDF.
THD+N.
THD+N means total harmonic distortion plus noise. This measurement is much more common and more comparable between devices. It is usually measured by inputting a sine wave, notch filtering the output, and comparing the ratio between the output signal with and without the sine wave:
Like the THD measurement, this is a ratio of RMS amplitudes, and can be measured as THDF (bandpassed or calculated fundamental as the denominator) or, more commonly, as THDR (total distorted signal as the denominator). Audio Precision measurements are THDR, for instance.
A meaningful measurement must include the bandwidth of measurement. This measurement includes effects from ground loop power line hum, high-frequency interference, intermodulation distortion between these tones and the fundamental, and so on, in addition to harmonic distortion. For psychoacoustic measurements, a weighting curve is applied such as A-weighting or ITU-R BS.468, which is intended to accentuate what is most audible to the human ear, contributing to a more accurate measurement.
For a given input frequency and amplitude, THD+N is reciprocal to SINAD, provided that both measurements are made over the same bandwidth.
Measurement.
The distortion of a waveform relative to a pure sinewave can be measured either by using a THD analyzer to analyse the output wave into its constituent harmonics and noting the amplitude of each relative to the fundamental; or by cancelling out the fundamental with a notch filter and measuring the remaining signal, which will be total aggregate harmonic distortion plus noise.
Given a sinewave generator of very low inherent distortion, it can be used as input to amplification equipment, whose distortion at different frequencies and signal levels can be measured by examining the output waveform.
There is electronic equipment both to generate sinewaves and to measure distortion; but a general-purpose digital computer equipped with a sound card can carry out harmonic analysis with suitable software. Different software can be used to generate sinewaves, but the inherent distortion may be too high for measurement of very low-distortion amplifiers.
Interpretation.
For many purposes different types of harmonics are not equivalent. For instance, crossover distortion at a given THD is much more audible than clipping distortion at the same THD, since the harmonics produced are at higher frequencies, which are not as easily masked by the fundamental. A single THD number is inadequate to specify audibility, and must be interpreted with care. Taking THD measurements at different output levels would expose whether the distortion is clipping (which increases with level) or crossover (which decreases with level).
THD is an average of a number of harmonics equally weighted, even though research performed decades ago identifies that lower order harmonics are harder to hear at the same level, compared with higher order ones. In addition, even order harmonics are said to be generally harder to hear than odd order. A number of formulas that attempt to correlate THD with actual audibility have been published, however none have gained mainstream use.
Examples.
For many standard signals, the above criterion may be calculated analytically in a closed-form. For example, a pure square wave has THDF equal to
The sawtooth signal possesses
The pure symmetrical triangle wave has THDF of 
For the rectangular pulse train with the "duty cycle" "μ" (called sometimes the "cyclic ratio"),
the THDF has the form
and logically, reaches the minimum (≈0.483) when the signal becomes symmetrical "μ"=0.5, "i.e." the pure square wave. Appropriate filtering of these signals may drastically reduce the resulting THD. For instance, the pure square wave filtered by the Butterworth low-pass filter of the second-order (with the cutoff frequency set equal to the fundamental frequency) has THDF of 5.3%, while the same signal filtered by the fourth-order filter has THDF of 0.6%. However, analytic computation of the THDF for complicated waveforms and filters often represents a difficult task, and the resulting expressions may be quite laborious to obtain. For example, the closed-form expression for the THDF of the sawtooth wave filtered by the first-order Butterworth low-pass filter is simply
while that for the same signal filtered by the second-order Butterworth filter is given by 
a rather cumbersome formula
Yet, the closed-form expression for the THDF of the pulse train filtered by the "p"th-order Butterworth low-pass filter is even more complicated and has the following form
where "μ" is the duty cycle, 0<"μ"<1, and 
see for more details.

</doc>
<doc id="41804" url="https://en.wikipedia.org/wiki?curid=41804" title="Traffic intensity">
Traffic intensity

In telecommunication networks, traffic intensity is a measure of the average occupancy of a server or resource during a specified period of time, normally a busy hour. It is measured in traffic units (erlangs) and defined as the ratio of the time during which a facility is cumulatively occupied to the time this facility is available for occupancy. 
In a digital network, the traffic intensity is:
where
A traffic intensity greater than one erlang means that the rate at which bits arrive exceeds the rate bits can be transmitted and queuing delay will grow without bound (if the traffic intensity stays the same). If the traffic intensity is less than one erlang, then the router can handle more average traffic. 
Telecommunication operators are vitally interested in traffic intensity, as it dictates the amount of equipment they must supply.

</doc>
<doc id="41805" url="https://en.wikipedia.org/wiki?curid=41805" title="Transceiver">
Transceiver

A transceiver is a device comprising both a transmitter and a receiver which are combined and share common circuitry or a single housing. When no circuitry is common between transmit and receive functions, the device is a transmitter-receiver. The term originated in the early 1920s. Similar devices include transponders, transverters, and repeaters.
Radio technology.
In radio terminology, a transceiver means a unit which contains both a receiver and a transmitter. From the beginning days
of radio the receiver and transmitter were separate units and remained so until around 1920. Amateur radio or "ham" radio operators can build their own equipment and it is now easier to design and build a simple unit containing both of the functions: transmitting and receiving. Almost all modern amateur radio equipment is now a transceiver but there is an active market for pure radio receivers, mainly for shortwave listening (SWL) operators. An example of a transceiver would be a walkie-talkie or a CB radio.
Telephony.
On a wired telephone, the handset contains the transmitter and receiver for the audio and in the 20th century was usually wired to the base unit by tinsel wire. The whole unit is colloquially referred to as a "receiver." On a mobile telephone or other radiotelephone, the entire unit is a transceiver, for both audio and radio.
A cordless telephone uses an audio and radio transceiver for the handset, and a radio transceiver for the base station. If a speakerphone is included in a wired telephone base or in a cordless base station, the base also becomes an audio transceiver in addition to the handset.
A modem is similar to a transceiver, in that it sends and receives a signal, but a modem uses modulation and demodulation. It modulates a signal being transmitted and demodulates a signal being received.
Ethernet.
Transceivers are called Medium Attachment Units (MAUs) in IEEE 802.3 documents and were widely used in 10BASE2 and 10BASE5 Ethernet networks. Fiber-optic gigabit and 10 Gigabit Ethernet utilize transceivers known as GBIC, SFP, SFP+, XFP, XAUI and CFP.

</doc>
<doc id="41807" url="https://en.wikipedia.org/wiki?curid=41807" title="Transmission">
Transmission


</doc>
<doc id="41808" url="https://en.wikipedia.org/wiki?curid=41808" title="Transmission block">
Transmission block

In telecommunication, the term transmission block has the following meanings: 
A transmission block is usually terminated by an End Of Block character (EOB), End Transmission Block character (ETB), End Of Text character (ETX), or End Of Transmission character (EOT).

</doc>
<doc id="41810" url="https://en.wikipedia.org/wiki?curid=41810" title="Transmission-level point">
Transmission-level point

In a telecommunications system, a transmission-level point (TLP) is a test point or interface, "i.e." a physical point in an electronic circuit where a test signal may be inserted or measured, and for which the nominal power of the test signal is specified.
A zero dBm transmission-level point (or synonym zero-transmission-level point) is a transmission-level point where the nominal test signal power is 0dBm.
In practice, the abbreviation TLP is usually used, and it is modified by the nominal level for the point in question. For example, where the nominal level is 0 dBm, the expression 0 dBm TLP, or simply, 0TLP, is used. Where the nominal level is −16 dBm, the expression −16 dBm TLP, or −16TLP, is used.
The nominal transmission level at a specified TLP is a function of system design and is an expression of the design gain or loss.
Voice-channel transmission levels, i.e. TLPs, are usually specified for a frequency of approximately 1,000 Hz.
The TLP at a point at which an end instrument, e.g. a telephone set, is connected is usually specified as 0 dBm.

</doc>
<doc id="41811" url="https://en.wikipedia.org/wiki?curid=41811" title="Transmission line">
Transmission line

In communications and electronic engineering, a transmission line is a specialized cable or other structure designed to carry alternating current of radio frequency, that is, currents with a frequency high enough that their wave nature must be taken into account. Transmission lines are used for purposes such as connecting radio transmitters and receivers with their antennas, distributing cable television signals, trunklines routing calls between telephone switching centres, computer network connections and high speed computer data buses.
This article covers two-conductor transmission line such as parallel line (ladder line), coaxial cable, stripline, and microstrip. Some sources also refer to waveguide, dielectric waveguide, and even optical fibre as transmission line, however these lines require different analytical techniques and so are not covered by this article; see Waveguide (electromagnetism).
Overview.
Ordinary electrical cables suffice to carry low frequency alternating current (AC), such as mains power, which reverses direction 100 to 120 times per second, and audio signals. However, they cannot be used to carry currents in the radio frequency range or higher, which reverse direction millions to billions of times per second, because the energy tends to radiate off the cable as radio waves, causing power losses. Radio frequency currents also tend to reflect from discontinuities in the cable such as connectors and joints, and travel back down the cable toward the source. These reflections act as bottlenecks, preventing the signal power from reaching the destination. Transmission lines use specialized construction, and impedance matching, to carry electromagnetic signals with minimal reflections and power losses. The distinguishing feature of most transmission lines is that they have uniform cross sectional dimensions along their length, giving them a uniform "impedance", called the characteristic impedance, to prevent reflections. Types of transmission line include parallel line (ladder line, twisted pair), coaxial cable, stripline, and microstrip. The higher the frequency of electromagnetic waves moving through a given cable or medium, the shorter the wavelength of the waves. Transmission lines become necessary when the length of the cable is longer than a significant fraction of the transmitted frequency's wavelength.
At microwave frequencies and above, power losses in transmission lines become excessive, and waveguides are used instead, which function as "pipes" to confine and guide the electromagnetic waves. Some sources define waveguides as a type of transmission line; however, this article will not include them. At even higher frequencies, in the terahertz, infrared and light range, waveguides in turn become lossy, and optical methods, (such as lenses and mirrors), are used to guide electromagnetic waves.
The theory of sound wave propagation is very similar mathematically to that of electromagnetic waves, so techniques from transmission line theory are also used to build structures to conduct acoustic waves; and these are called acoustic transmission lines.
History.
Mathematical analysis of the behaviour of electrical transmission lines grew out of the work of James Clerk Maxwell, Lord Kelvin and Oliver Heaviside. In 1855 Lord Kelvin formulated a diffusion model of the current in a submarine cable. The model correctly predicted the poor performance of the 1858 trans-Atlantic submarine telegraph cable. In 1885 Heaviside published the first papers that described his analysis of propagation in cables and the modern form of the telegrapher's equations.
Applicability.
In many electric circuits, the length of the wires connecting the components can for the most part be ignored. That is, the voltage on the wire at a given time can be assumed to be the same at all points. However, when the voltage changes in a time interval comparable to the time it takes for the signal to travel down the wire, the length becomes important and the wire must be treated as a transmission line. Stated another way, the length of the wire is important when the signal includes frequency components with corresponding wavelengths comparable to or less than the length of the wire.
A common rule of thumb is that the cable or wire should be treated as a transmission line if the length is greater than 1/10 of the wavelength. At this length the phase delay and the interference of any reflections on the line become important and can lead to unpredictable behaviour in systems which have not been carefully designed using transmission line theory.
The four terminal model.
For the purposes of analysis, an electrical transmission line can be modelled as a two-port network (also called a quadrupole network), as follows:
In the simplest case, the network is assumed to be linear (i.e. the complex voltage across either port is proportional to the complex current flowing into it when there are no reflections), and the two ports are assumed to be interchangeable. If the transmission line is uniform along its length, then its behaviour is largely described by a single parameter called the "characteristic impedance", symbol Z0. This is the ratio of the complex voltage of a given wave to the complex current of the same wave at any point on the line. Typical values of Z0 are 50 or 75 ohms for a coaxial cable, about 100 ohms for a twisted pair of wires, and about 300 ohms for a common type of untwisted pair used in radio transmission.
When sending power down a transmission line, it is usually desirable that as much power as possible will be absorbed by the load and as little as possible will be reflected back to the source. This can be ensured by making the load impedance equal to Z0, in which case the transmission line is said to be "matched".
Some of the power that is fed into a transmission line is lost because of its resistance. This effect is called "ohmic" or "resistive" loss (see ohmic heating). At high frequencies, another effect called "dielectric loss" becomes significant, adding to the losses caused by resistance. Dielectric loss is caused when the insulating material inside the transmission line absorbs energy from the alternating electric field and converts it to heat (see dielectric heating). The transmission line is modelled with a resistance (R) and inductance (L) in series with a capacitance (C) and conductance (G) in parallel. The resistance and conductance contribute to the loss in a transmission line.
The total loss of power in a transmission line is often specified in decibels per metre (dB/m), and usually depends on the frequency of the signal. The manufacturer often supplies a chart showing the loss in dB/m at a range of frequencies. A loss of 3 dB corresponds approximately to a halving of the power.
High-frequency transmission lines can be defined as those designed to carry electromagnetic waves whose wavelengths are shorter than or comparable to the length of the line. Under these conditions, the approximations useful for calculations at lower frequencies are no longer accurate. This often occurs with radio, microwave and optical signals, metal mesh optical filters, and with the signals found in high-speed digital circuits.
Telegrapher's equations.
The telegrapher's equations (or just telegraph equations) are a pair of linear differential equations which describe the voltage and current on an electrical transmission line with distance and time. They were developed by Oliver Heaviside who created the "transmission line model", and are based on Maxwell's Equations.
The transmission line model represents the transmission line as an infinite series of two-port elementary components, each representing an infinitesimally short segment of the transmission line:
The model consists of an "infinite series" of the elements shown in the figure, and the values of the components are specified "per unit length" so the picture of the component can be misleading. formula_1, formula_2, formula_3, and formula_4 may also be functions of frequency. An alternative notation is to use formula_9, formula_10, formula_11 and formula_12 to emphasize that the values are derivatives with respect to length. These quantities can also be known as the primary line constants to distinguish from the secondary line constants derived from them, these being the propagation constant, attenuation constant and phase constant.
The line voltage formula_13 and the current formula_14 can be expressed in the frequency domain as
When the elements formula_1 and formula_4 are negligibly small the transmission line is considered as a lossless structure. In this hypothetical case, the model depends only on the formula_2 and formula_3 elements which greatly simplifies the analysis. For a lossless transmission line, the second order steady-state Telegrapher's equations are:
These are wave equations which have plane waves with equal propagation speed in the forward and reverse directions as solutions. The physical significance of this is that electromagnetic waves propagate down transmission lines and in general, there is a reflected component that interferes with the original signal. These equations are fundamental to transmission line theory.
If formula_1 and formula_4 are not neglected, the Telegrapher's equations become:
where is the propagation constant
and the characteristic impedance can be expressed as
The solutions for formula_13 and formula_14 are:
The constants formula_33 and formula_34 must be determined from boundary conditions. For a voltage pulse formula_35, starting at formula_36 and moving in the positive formula_37-direction, then the transmitted pulse formula_38 at position formula_37 can be obtained by computing the Fourier Transform, formula_40, of formula_35, attenuating each frequency component by formula_42, advancing its phase by formula_43, and taking the inverse Fourier Transform. The real and imaginary parts of formula_44 can be computed as
where atan2 is the two-parameter arctangent, and
For small losses and high frequencies, to first order in formula_49 and formula_50 one obtains
Noting that an advance in phase by formula_53 is equivalent to a time delay by formula_54, formula_55 can be simply computed as
Input impedance of transmission line.
The characteristic impedance of a transmission line is the ratio of the amplitude of a "single" voltage wave to its current wave. Since most transmission lines also have a reflected wave, the characteristic impedance is generally not the impedance that is measured on the line.
The impedance measured at a given distance, , from the load impedance may be expressed as,
where is the propagation constant and formula_58 is the voltage reflection coefficient at the load end of the transmission line. Alternatively, the above formula can be rearranged to express the input impedance in terms of the load impedance rather than the load voltage reflection coefficient:
Input impedance of lossless transmission line.
For a lossless transmission line, the propagation constant is purely imaginary, , so the above formulas can be rewritten as,
where formula_61 is the wavenumber.
In calculating , the wavelength is generally different inside the transmission line to what it would be in free-space and the velocity constant of the material the transmission line is made of needs to be taken into account when doing such a calculation.
Special cases of lossless transmission lines.
Half wave length.
For the special case where formula_62 where n is an integer (meaning that the length of the line is a multiple of half a wavelength), the expression reduces to the load impedance so that
formula_63
for all . This includes the case when , meaning that the length of the transmission line is negligibly small compared to the wavelength. The physical significance of this is that the transmission line can be ignored (i.e. treated as a wire) in either case.
Quarter wave length.
For the case where the length of the line is one quarter wavelength long, or an odd multiple of a quarter wavelength long, the input impedance becomes
Z_\mathrm{in}=\frac

</doc>
<doc id="41812" url="https://en.wikipedia.org/wiki?curid=41812" title="Transmission medium">
Transmission medium

A transmission medium is a material substance (solid, liquid, gas, or plasma) that can propagate energy waves. For example, the transmission medium for sounds is usually air, but solids and liquids may also act as transmission media for sound.
The absence of a material medium in vacuum may also constitute a transmission medium for electromagnetic waves such as light and radio waves. While material substance is not required for electromagnetic waves to propagate, such waves are usually affected by the transmission media they pass through, for instance by absorption or by reflection or refraction at the interfaces between media.
The term transmission medium also refers to a technical device that employs the material substance to transmit or guide waves. Thus, an optical fiber or a copper cable is a transmission medium. Not only this but also is able to guide the transmission of networks.
A transmission medium can be classified as a:
Electromagnetic radiation can be transmitted through an optical medium, such as optical fiber, or through twisted pair wires, coaxial cable, or dielectric-slab waveguides. It may also pass through any physical material that is transparent to the specific wavelength, such as water, air, glass, or concrete. Sound is, by definition, the vibration of matter, so it requires a physical medium for transmission, as do other kinds of mechanical waves and heat energy. Historically, science incorporated various aether theories to explain the transmission medium. However, it is now known that electromagnetic waves do not require a physical transmission medium, and so can travel through the "vacuum" of free space. Regions of the insulative vacuum can become conductive for electrical conduction through the presence of free electrons, holes, or ions.
Transmission and reception of data is performed in four steps.
Telecommunications.
A physical medium in data communications is the transmission path over which a signal propagates.
Many transmission media are used as communications channel.
For telecommunications purposes in the United States, Federal Standard 1037C, transmission media are classified as one of the following:
One of the most common physical medias used in networking is copper wire. Copper wire to carry signals to long distances using relatively low amounts of power. The unshielded twisted pair (UTP) is eight strands of copper wire, organized into four pairs.
Another example of a physical medium is optical fiber, which has emerged as the most commonly used transmission medium for long-distance communications. Optical fiber is a thin strand of glass that guides light along its length. Four major factors favor optical fiber over copper- data rates, distance, installation, and costs. Optical fiber can carry huge amounts of data compared to copper. It can be run for hundreds of miles without the need for signal repeaters, in turn, reducing maintenance costs and improving the reliability of the communication system because repeaters are a common source of network failures. Glass is lighter than copper allowing for less need for specialized heavy-lifting equipment when installing long-distance optical fiber. Optical fiber for indoor applications cost approximately a dollar a foot, the same as copper.
Multimode and single mode are two types of commonly used optical fiber. Multimode fiber uses LEDs as the light source and can carry signals over shorter distances, about 2 kilometers. Single mode can carry signals over distances of tens of miles.
Wireless media may carry surface waves or skywaves, either longitudinally or transversely, and are so classified.
In both communications, communication is in the form of electromagnetic waves. With guided transmission media, the waves are guided along a physical path; examples of guided media include phone lines, twisted pair cables, coaxial cables, and optical fibers. Unguided transmission media are methods that allow the transmission of data without the use of physical means to define the path it takes. Examples of this include microwave, radio or infrared. Unguided media provide a means for transmitting electromagnetic waves but do not guide them; examples are propagation through air, vacuum and seawater.
The term direct link is used to refer to the transmission path between two devices in which signals propagate directly from transmitters to receivers with no intermediate devices, other than amplifiers or repeaters used to increase signal strength. This term can apply to both guided and unguided media.
Types of transmissions.
A transmission may be simplex, half-duplex, or full-duplex.
In simplex transmission, signals are transmitted in only one direction; one station is a transmitter and the other is the receiver. In the half-duplex operation, both stations may transmit, but only one at a time. In full duplex operation, both stations may transmit simultaneously. In the latter case, the medium is carrying signals in both directions at same time.
There are two types of transmission media:
Guided Media:
Unguided Media: Transmission media then looking at analysis of using them unguided transmission media is data signals that flow through the air. They are not guided or bound to a channel to follow. Following are unguided media used for data communication:

</doc>
<doc id="41813" url="https://en.wikipedia.org/wiki?curid=41813" title="Transmit-after-receive time delay">
Transmit-after-receive time delay

In telecommunication, transmit-after-receive time delay is the time interval from removal of RF energy at the local receiver input until the local transmitter is automatically keyed on and the transmitted rf signal amplitude has increased to 90% of its steady-state value. "An Exception:" High-frequency (HF) transceiver equipment is normally not designed with an interlock between receiver squelch and transmitter on-off key. The transmitter can be keyed on at any time, independent of whether or not a signal is being received at the receiver input.

</doc>
<doc id="41817" url="https://en.wikipedia.org/wiki?curid=41817" title="Transponder">
Transponder

In telecommunication, a transponder is one of two types of devices. In air navigation or radio frequency identification, a flight transponder is a device that emits an identifying signal in response to an interrogating received signal. In a communications satellite, a transponder gathers signals over a range of uplink frequencies and re-transmits them on a different set of downlink frequencies to receivers on Earth, often without changing the content of the received signal or signals.
The term is a portmanteau for "trans"mitter-res"ponder". It is variously abbreviated as XPDR, XPNDR, TPDR or TP.
Satellite/broadcast communications.
A communications satellite’s channels are called transponders, because each is a separate transceiver or repeater. With digital video data compression and multiplexing, several video and audio channels may travel through a single transponder on a single wideband carrier. Original analog video only has one channel per transponder, with subcarriers for audio and automatic transmission identification service (ATIS). Non-multiplexed radio stations can also travel in single channel per carrier (SCPC) mode, with multiple carriers (analog or digital) per transponder. This allows each station to transmit directly to the satellite, rather than paying for a whole transponder, or using landlines to send it to an earth station for multiplexing with other stations.
Optical communications.
In optical fiber communications, a transponder is the element that sends and receives the optical signal from a fiber. A transponder is typically characterized by its data rate and the maximum distance the signal can travel.
The term "transponder" can apply to different items with important functional differences, mentioned across academic and commercial literature:
As a result, difference in transponder functionality also might influence the functional description of related optical modules like transceivers and muxponders.
Aviation.
Another type of transponder occurs in identification friend or foe systems in military aviation and in air traffic control secondary surveillance radar (beacon radar) systems for general aviation and commercial aviation. Primary radar works best with large all-metal aircraft, but not so well on small, composite aircraft. Its range is also limited by terrain and rain or snow and also detects unwanted objects such as automobiles, hills and trees. Furthermore, it cannot always estimate the altitude of an aircraft. Secondary radar overcomes these limitations but it depends on a transponder in the aircraft to respond to interrogations from the ground station to make the plane more visible.
Depending on the type of interrogation, the transponder sends back a transponder code (or "squawk code", Mode A) or altitude information (Mode C) to help air traffic controllers to identify the aircraft and to maintain separation between planes. Another mode called Mode S (Mode Select) is designed to help avoiding over-interrogation of the transponder (having many radars in busy areas) and to allow automatic collision avoidance. Mode S transponders are "backwards compatible" with Modes A & C. Mode S is mandatory in controlled airspace in many countries. Some countries have also required, or are moving towards requiring, that all aircraft be equipped with Mode S, even in uncontrolled airspace. However, in the field of general aviation there have been objections to these moves, because of the cost, size, limited benefit to the users in uncontrolled airspace, and, in the case of balloons and gliders, the power requirements during long flights.
Marine.
The International Maritime Organization's International Convention for the Safety of Life at Sea requires the Automatic Identification System (AIS) to be fitted aboard international voyaging ships with , and all passenger ships regardless of size. Although AIS transmitters/receivers are generally called transponders they generally transmit autonomously, although coast stations can interrogate class B transponders on smaller vessels for additional information. In addition, navigational aids often have transponders called RACON (radar beacons) designed to make them stand out on a ship's radar screen.
Automotive.
Many modern automobiles have keys with transponders hidden inside the plastic head of the key. The user of the car may not even be aware that the transponder is there, because there are no buttons to press. When a key is inserted into the ignition lock cylinder and turned, the car's computer sends a radio signal to the transponder. Unless the transponder replies with a valid code, the computer will not allow the engine to be started. Transponder keys have no battery; they are energized by the radio signal itself.
Road.
Electronic toll collection systems such as E-ZPass in the eastern United States use RFID transponders to identify vehicles. Highway 407 in Ontario is one of the world's first completely automated toll highways.
Motorsport.
Transponders are used in motorsport for lap timing purposes. A cable loop is dug into the race circuit near to the start/finish line. Each car has an active transponder with a unique ID code. When the racing car passes the start/finish line the lap time and the racing position is shown on the score board.
Passive and active RFID systems are used in off-road events such as Enduro and Hare and Hounds racing, the riders have a transponder on their person, normally on their arm. When they complete a lap they swipe or touch the receiver which is connected to a computer and log their lap time. The Casimo Group Ltd make a system which does this.
NASCAR uses transponders and cable loops placed at numerous points around the track to determine the lineup during a caution period. This system replaced a dangerous race back to the start-finish line.
Underwater.
Sonar transponders operate under water and are used to measure distance and form the basis of underwater location marking, position tracking and navigation.
Gated communities.
Transponders may also be used by residents to enter their gated communities. However, having more than one transponder causes problems. If a resident's car with simple transponder is parked in the vicinity, any vehicle can come up to the automated gate, triggering the gate interrogation signal, which may get an acceptable response from the resident's car. Such units properly installed might involve beamforming, unique transponders for each vehicle, or simply obliging vehicles to be stored away from the gate.

</doc>
<doc id="41819" url="https://en.wikipedia.org/wiki?curid=41819" title="Transposition">
Transposition

Transposition may refer to:

</doc>
<doc id="41820" url="https://en.wikipedia.org/wiki?curid=41820" title="Transverse redundancy check">
Transverse redundancy check

In telecommunications, a transverse redundancy check (TRC) or vertical redundancy check is a redundancy check for synchronized parallel bits applied once per bit time, across the bit streams. This requires additional parallel channels for the check bit or bits.
The term usually applies to a single parity bit, although it could also be used to refer to a larger Hamming code.
The adjective "transverse" is most often used when it is used in combination with additional error control coding, such as a longitudinal redundancy check. Although parity alone can only detect and not correct errors, it can be part of a system for correcting errors.
An example of a TRC is the parity written to the 9th track of a 9 track tape.

</doc>
<doc id="41821" url="https://en.wikipedia.org/wiki?curid=41821" title="Tree structure">
Tree structure

A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. It is named a "tree structure" because the classic representation resembles a tree, even though the chart is generally upside down compared to an actual tree, with the "root" at the top and the "leaves" at the bottom.
A tree structure is conceptual, and appears in several forms. For a discussion of tree structures in specific fields, see Tree (data structure) for computer science: insofar as it relates to graph theory, see tree (graph theory), or also tree (set theory). Other related pages are listed below.
Terminology and properties.
The tree elements are called "nodes".
The lines connecting elements are called "branches". 
Nodes without children are called leaf nodes, "end-nodes", or "leaves".
Every finite tree structure has a member that has no superior. This member is called the "root" or root node. The root is the starting node. But the converse is not true: infinite tree structures may or may not have a root node.
The names of relationships between nodes model the kinship terminology of family relations. The gender-neutral names "parent" and "child" have largely displaced the older "father" and "son" terminology, although the term "uncle" is still used for other nodes at the same level as the parent.
In the example, "encyclopedia" is the parent of "science" and "culture", its children. "Art" and "craft" are siblings, and children of "culture", which is their parent and thus one of their ancestors. Also, "encyclopedia", as the root of the tree, is the ancestor of "science", "culture", "art" and "craft". Finally, "science", "art" and "craft", as leaves, are ancestors of no other node.
Tree structures can depict all kinds of taxonomic knowledge, such as family trees, the biological evolutionary tree, the evolutionary tree of a language family, the grammatical structure of a language (a key example being S → NP VP, meaning a sentence is a noun phrase and a verb phrase, with each in turn having other components which have other components), the way web pages are logically ordered in a web site, mathematical trees of integer sets, et cetera.
The Oxford English Dictionary records use of both the terms "tree structure" and "tree-diagram" from 1965 in Noam Chomsky's "Aspects of the Theory of Syntax".
In a tree structure there is one and only one path from any point to any other point.
Computer science uses tree structures extensively ("see" Tree (data structure) and telecommunications.)
For a formal definition see set theory, and for a generalization in which children are not necessarily successors, see prefix order.
Representing trees.
There are many ways of visually representing tree structures.
Almost always, these boil down to variations, or combinations,
of a few basic styles:
Classical node-link diagrams.
Classical node-link diagrams, that connect nodes together with line segments.
Nested sets.
Nested sets that use enclosure/containment to show parenthood, examples include TreeMaps and fractal maps.
Layered "icicle" diagrams.
Layered "icicle" diagrams that use alignment/adjacency.
Outlines and tree views.
Lists or diagrams that use indentation, sometimes called "outlines" or "tree views".
Nested parentheses.
((art,craft)culture,science)encyclopedia
or
encyclopedia(culture(art,craft),science)
A correspondence to nested parentheses was first noticed by Sir Arthur Cayley.
Radial trees.
Trees can also be represented radially.
Further reading.
Identification of some of the basic styles of tree structures can be found in:

</doc>
<doc id="41822" url="https://en.wikipedia.org/wiki?curid=41822" title="Troposphere">
Troposphere

The troposphere is the lowest portion of Earth's atmosphere, and is also where all weather takes place. It contains approximately 75% of the atmosphere's mass and 99% of its water vapour and aerosols.
The average depth of the troposphere is approximately in the middle latitudes. It is deeper in the tropics, up to , and shallower near the polar regions, approximately in winter. The lowest part of the troposphere, where friction with the Earth's surface influences air flow, is the planetary boundary layer. This layer is typically a few hundred meters to deep depending on the landform and time of day. The border between the troposphere and stratosphere, is marked by a temperature inversion called the tropopause.
The word troposphere derives from the for "turn, turn toward, trope" and "-sphere" (as in, the Earth), reflecting the fact that rotational turbulent mixing plays an important role in the troposphere's structure and behaviour. Most of the phenomena we associate with day-to-day weather occur in the troposphere.
Pressure and temperature structure.
Composition.
The chemical composition of the troposphere is essentially uniform, with the notable exception of water vapor. The source of water vapor is at the surface through the processes of evaporation. The temperature of the troposphere decreases with height, and saturation vapor pressure decreases strongly as temperature drops, so the amount of water vapor that can exist in the atmosphere decreases strongly with height. Thus the proportion of water vapor is normally greatest near the surface and decreases with height.
Pressure.
The pressure of the atmosphere is maximum at sea level and decreases with altitude. This is because the atmosphere is very nearly in hydrostatic equilibrium, so that the pressure is equal to the weight of air above a given point. The change in pressure with height, therefore can be equated to the density with this hydrostatic equation:
where:
Since temperature in principle also depends on altitude, one needs a second equation to determine the pressure as a function of height, as discussed in the next section.
Temperature.
The temperature of the troposphere generally decreases as altitude increases. The rate at which the temperature decreases, formula_2, is called the environmental lapse rate (ELR). 
The ELR is nothing more than the difference in temperature between the surface and the tropopause divided by the height. The ELR assumes that there is no mixing of the layers of air, i.e. that there is no vertical convection, nor winds that would create turbulence and hence mixing of the layers of air. The reason for this temperature difference is that the ground absorbs most of the sun's energy, which then heats the lower levels of the atmosphere with which it is in contact. Meanwhile the radiation of heat at the top of the atmosphere results in the cooling of that part of the atmosphere.
As parcels of air in the atmosphere rise and fall, they undergo changes in temperature and pressure for reasons described below. The rate of change of the temperature in the parcel of air may be less than or more than the ELR as a result of the expansion or compression of that parcel.
When a parcel of air rises, it expands, because the pressure is lower at higher altitudes. As the air parcel expands, it pushes the surrounding air outward, transferring energy in the form of work from that parcel to the atmosphere. As energy transfer to a parcel of air by way of heat is very slow, it is assumed to not exchange energy by way of heat with the environment. Such a process is called an adiabatic process (no energy transfer by way of heat). Since the rising parcel of air is losing energy as work to the surrounding atmosphere and no energy is transferred as heat from the atmosphere to make up for the loss, the parcel of air is losing energy, which manifests itself as a decrease in the temperature of the air parcel. The reverse, of course, will be true for a parcel of air that is sinking and is being compressed.
Since the process of compression and expansion of an air parcel can be considered reversible and no energy is transferred into or out of the parcel, such a process is considered isentropic, and there is no change in entropy as the air parcel rises and falls, formula_3. Since the heat exchanged formula_4 is related to the entropy change formula_5 by formula_6, the equation governing the temperature as a function of height for a thoroughly mixed atmosphere is 
where "S" is the entropy. The above equation states that the entropy of the atmosphere does not change with height. The rate at which temperature decreases with height under such conditions is called the adiabatic lapse rate.
For "dry" air, which is approximately an ideal gas, we can proceed further. The adiabatic equation for an ideal gas is 
where formula_9 is the heat capacity ratio (formula_9=7/5, for air). Combining with the equation for the pressure, one arrives at the dry adiabatic lapse rate,
If the air contains water vapor, then cooling of the air can cause the water to condense, and the behavior is no longer that of an ideal gas. If the air is at the saturated vapor pressure, then the rate at which temperature drops with height is called the saturated adiabatic lapse rate. More generally, the actual rate at which the temperature drops with altitude is called the environmental lapse rate. 
In the troposphere, the average environmental lapse rate is a drop of about 6.5 °C for every 1 km (1,000 meters) in increased height.
The environmental lapse rate (the actual rate at which temperature drops with height, formula_12) is not usually equal to the adiabatic lapse rate (or correspondingly, formula_13). If the upper air is warmer than predicted by the adiabatic lapse rate (formula_14), then when a parcel of air rises and expands, it will arrive at the new height at a lower temperature than its surroundings. In this case, the air parcel is denser than its surroundings, so it sinks back to its original height, and the air is stable against being lifted. If, on the contrary, the upper air is cooler than predicted by the adiabatic lapse rate, then when the air parcel rises to its new height it will have a higher temperature and a lower density than its surroundings, and will continue to accelerate upward.
The troposphere is heated from below by latent heat, longwave radiation, and sensible heat. Surplus heating and vertical expansion of the troposphere occurs in the tropics. At middle latitudes, tropospheric temperatures decrease from an average of 15 °C at sea level to about -55 °C at the tropopause. At the poles, tropospheric temperature only decreases from an average of 0 °C at sea level to about -45 °C at the tropopause. At the equator, tropospheric temperatures decrease from an average of 20 °C at sea level to about -70 to -75 °C at the tropopause. The troposphere is thinner at the poles and thicker at the equator. The average thickness of the tropical tropopause is roughly 7 kilometers greater than the average tropopause thickness at the poles.
Tropopause.
The tropopause is the boundary region between the troposphere and the stratosphere.
Measuring the temperature change with height through the troposphere and the stratosphere identifies the location of the tropopause. In the troposphere, temperature decreases with altitude. In the stratosphere, however, the temperature remains constant for a while and then increases with altitude. The region of the atmosphere where the lapse rate changes from positive (in the troposphere) to negative (in the stratosphere), is defined as the tropopause. Thus, the tropopause is an inversion layer, and there is little mixing between the two layers of the atmosphere.
Atmospheric flow.
The flow of the atmosphere generally moves in a west to east direction. This, however, can often become interrupted, creating a more north to south or south to north flow. These scenarios are often described in meteorology as zonal or meridional. These terms, however, tend to be used in reference to localised areas of atmosphere (at a synoptic scale). A fuller explanation of the flow of atmosphere around the Earth as a whole can be found in the three-cell model.
Zonal flow.
A zonal flow regime is the meteorological term meaning that the general flow pattern is west to east along the Earth's latitude lines, with weak shortwaves embedded in the flow. The use of the word "zone" refers to the flow being along the Earth's latitudinal "zones". This pattern can buckle and thus become a meridional flow.
Meridional flow.
When the zonal flow buckles, the atmosphere can flow in a more longitudinal (or meridional) direction, and thus the term "meridional flow" arises. Meridional flow patterns feature strong, amplified troughs of low pressure and ridges of high pressure, with more north-south flow in the general pattern than west-to-east flow.
Three-cell model.
The three cells model of the atmosphere attempts to describe the actual flow of the Earth's atmosphere as a whole. It divides the Earth into the tropical (Hadley cell), mid latitude (Ferrel cell), and polar (polar cell) regions, to describe energy flow and global atmospheric circulation (mass flow). Its fundamental principle is that of balance - the energy that the Earth absorbs from the sun each year is equal to that which it loses to space by radiation. This overall Earth energy balance, however, does not apply in each latitude due to the varying strength of the sun in each "cell" as a result of the tilt of the Earth's axis in relation to its orbit. The result is a circulation of the atmosphere that transports warm air poleward from the tropics and cold air equatorward from the poles. The effect of the three cells is the tendency to even out the heat and moisture in the Earth's atmosphere around the planet.
Synoptic scale observations and concepts.
Forcing.
Forcing is a term used by meteorologists to describe the situation where a change or an event in one part of the atmosphere causes a strengthening change in another part of the atmosphere. It is usually used to describe connections between upper, middle or lower levels (such as upper-level divergence causing lower level convergence in cyclone formation), but also be to describe such connections over lateral distance rather than height alone. In some respects, teleconnections could be considered a type of forcing.
Divergence and convergence.
An area of convergence is one in which the total mass of air is increasing with time, resulting in an increase in pressure at locations below the convergence level (recall that atmospheric pressure is just the total weight of air above a given point). Divergence is the opposite of convergence - an area where the total mass of air is decreasing with time, resulting in falling pressure in regions below the area of divergence. Where divergence is occurring in the upper atmosphere, there will be air coming in to try to balance the net loss of mass (this is called the principle of mass conservation), and there is a resulting upward motion (positive vertical velocity). Another way to state this is to say that regions of upper air divergence are conducive to lower level convergence, cyclone formation, and positive vertical velocity. Therefore, identifying regions of upper air divergence is an important step in forecasting the formation of a surface low pressure area.

</doc>
<doc id="41823" url="https://en.wikipedia.org/wiki?curid=41823" title="Tropospheric wave">
Tropospheric wave

In telecommunication, a tropospheric wave is a radio wave that is propagated by reflection from a place of abrupt change in the dielectric constant, or its gradient, in the troposphere. In some cases, a ground wave may be so altered that new components appear to arise from reflection in regions of rapidly changing dielectric constant. When these components are distinguishable from the other components, they are called ""tropospheric waves.""

</doc>
<doc id="41825" url="https://en.wikipedia.org/wiki?curid=41825" title="Trunk">
Trunk

Trunk may refer to:
In biology:
Containers:
Other uses:

</doc>
<doc id="41826" url="https://en.wikipedia.org/wiki?curid=41826" title="Trusted computing base">
Trusted computing base

The trusted computing base (TCB) of a computer system is the set of all hardware, firmware, and/or software components that are critical to its security, in the sense that bugs or vulnerabilities occurring inside the TCB might jeopardize the security properties of the entire system. By contrast, parts of a computer system outside the TCB must not be able to misbehave in a way that would leak any more privileges than are granted to them in accordance to the security policy.
The careful design and implementation of a system's trusted computing base is paramount to its overall security. Modern operating systems strive to reduce the size of the TCB so that an exhaustive examination of its code base (by means of manual or computer-assisted software audit or program verification) becomes feasible.
Definition and characterization.
The term trusted computing base goes back to Rushby, who defined it as the combination of kernel and trusted processes. The latter refers to processes which are allowed to violate the system's access-control rules.
In the classic paper "Authentication in Distributed Systems: Theory and Practice" Lampson et al. define the TCB of a computer system as simply
Both definitions, while clear and convenient, are neither theoretically exact nor intended to be, as e.g. a network server process under a UNIX-like operating system might fall victim to a security breach and compromise an important part of the system's security, yet is not part of the operating system's TCB. The Orange Book, another classic computer security literature reference, therefore provides a more formal definition of the TCB of a computer system, as
The Orange Book further explains that
In other words, a given piece of hardware or software is a part of the TCB if and only if it has been designed to be a part of the mechanism that provides its security to the computer system. In operating systems, this typically consists of the kernel (or microkernel) and a select set of system utilities (for example, setuid programs and daemons in UNIX systems). In programming languages that have security features designed in such as Java and E, the TCB is formed of the language runtime and standard library.
Properties of the TCB.
Predicated upon the security policy.
It should be pointed out that as a consequence of the above Orange Book definition, the boundaries of the TCB depend closely upon the specifics of how the security policy is fleshed out. In the network server example above, even though, say, a Web server that serves a multi-user application is not part of the operating system's TCB, it has the responsibility of performing access control so that the users cannot usurp the identity and privileges of each other. In this sense, it definitely is part of the TCB of the larger computer system that comprises the UNIX server, the user's browsers and the Web application; in other words, breaching into the Web server through e.g. a buffer overflow may not be regarded as a compromise of the operating system proper, but it certainly constitutes a damaging exploit on the Web application.
This fundamental relativity of the boundary of the TCB is exemplifed by the concept of the target of evaluation (TOE) in the Common Criteria security process: in the course of a Common Criteria security evaluation, one of the first decisions that must be made is the boundary of the audit in terms of the list of system components that will come under scrutiny.
A prerequisite to security.
Systems that don't have a trusted computing base as part of their design do not provide security of their own: they are only secure insofar as security is provided to them by external means (e.g. a computer sitting in a locked room without a network connection may be considered secure depending on the policy, regardless of the software it runs). This is because, as David J. Farber et al. put it, "n a computer system, the integrity of lower layers is typically treated as axiomatic by higher layers". As far as computer security is concerned, reasoning about the security properties of a computer system requires being able to make sound assumptions about what it can, and more importantly, cannot do; however, barring any reason to believe otherwise, a computer is able to do everything that a general Von Neumann machine can. This obviously includes operations that would be deemed contrary to all but the simplest security policies, such as divulging an email or password that should be kept secret; however, barring special provisions in the architecture of the system, there is no denying that the computer "could be programmed" to perform these undesirable tasks.
These special provisions that aim at preventing certain kinds of actions from being executed, in essence, constitute the trusted computing base. For this reason, the Orange Book (still a reference on the design of secure operating systems design ) characterizes the various security assurance levels that it defines mainly in terms of the structure and security features of the TCB.
Software parts of the TCB need to protect themselves.
As outlined by the aforementioned Orange Book, software portions of the trusted computing base need to protect themselves against tampering to be of any effect. This is due to the von Neumann architecture implemented by virtually all modern computers: since machine code can be processed as just another kind of data, it can be read and overwritten by any program barring special memory management provisions that subsequently have to be treated as part of the TCB. Specifically, the trusted computing base must at least prevent its own software from being written to.
In many modern CPUs, the protection of the memory that hosts the TCB is achieved by adding in a specialized piece of hardware called the memory management unit (MMU), which is programmable by the operating system to allow and deny access to specific ranges of the system memory to the programs being run. Of course, the operating system is also able to disallow such programming to the other programs. This technique is called supervisor mode; compared to more crude approaches (such as storing the TCB in ROM, or equivalently, using the Harvard architecture), it has the advantage of allowing the security-critical software to be upgraded in the field, although allowing secure upgrades of the trusted computing base poses bootstrap problems of its own.
Trusted vs. trustworthy.
As stated above, trust in the trusted computing base is required to make any progress in ascertaining the security of the computer system. In other words, the trusted computing base is “trusted” first and foremost in the sense that it "has" to be trusted, and not necessarily that it is trustworthy. Real-world operating systems routinely have security-critical bugs discovered in them, which attests of the practical limits of such trust.
The alternative is formal software verification, which uses mathematical proof techniques to show the absence of bugs. Researchers at NICTA and its spinout Open Kernel Labs have recently performed such a formal verification of [http://ssrg.nicta.com.au/projects/seL4/], a member of the L4 microkernel family, proving functional correctness of the C implementation of the kernel.
This makes seL4 the first operating-system kernel which closes the gap between trust and trustworthiness, assuming the mathematical proof and the compiler are free from error.
TCB size.
Due to the aforementioned need to apply costly techniques such as formal verification or manual review, the size of the TCB has immediate consequences on the economics of the TCB assurance process, and the trustworthiness of the resulting product (in terms of the mathematical expectation of the number of bugs not found during the verification or review). In order to reduce costs and security risks, the TCB should therefore be kept as small as possible. This is a key argument in the debate preferring microkernels to monolithic kernels.
Examples.
AIX materializes the trusted computing base as an optional component in its install-time package management system.

</doc>
<doc id="41827" url="https://en.wikipedia.org/wiki?curid=41827" title="Turnkey">
Turnkey

A turnkey or a turnkey project (also spelled turn-key) is a type of project that is constructed so that it could be sold to any buyer as a completed product. This is contrasted with build to order, where the constructor builds an item to the buyer's exact specifications, or when an incomplete product is sold with the assumption that the buyer would complete it.
A turnkey project or contract as described by Duncan Wallace (1984) is:
A turnkey computer system is a complete computer including hardware, operating system and application(s) designed and sold to satisfy specific business requirements.
Common usage.
Turnkey refers to something that is ready for immediate use, generally used in the sale or supply of goods or services. The word is a reference to the fact that the customer, upon receiving the product, just needs to turn the ignition key to make it operational, or that the key just needs to be turned over to the customer. Turnkey is often used to describe a home built on the developer's land with the developer's financing ready for the customer to move in. If a contractor builds a "turnkey home" they frame the structure and finish the interior. Everything is completed down to the cabinets and carpet. "Turnkey" is commonly used in the construction industry, for instance, in which it refers to the bundling of materials and labour by Home Builder or General Contractor. 'Turnkey' is also commonly used in motorsports to describe a car being sold with drivetrain (engine, transmission, etc.) to contrast with a vehicle sold without one so that other components may be re-used.
Similarly, this term may be used to advertise the sale of an established business, including all the equipment necessary to run it, or by a business-to-business supplier providing complete packages for business start-up. An example would be the creation of a "turnkey hospital" which would be building a complete medical centre with installed medical equipment.
Specific usage.
Turnkey products are synonymous to "off-the-shelf" solutions and not customized.
In drilling, the term indicates an arrangement where a contractor must fully complete a well up to some milestone to receive any payment (in exchange for greater compensation upon completion).

</doc>
<doc id="41828" url="https://en.wikipedia.org/wiki?curid=41828" title="Two-out-of-five code">
Two-out-of-five code

In telecommunication, a two-out-of-five code is an m of n code that provides exactly ten possible combinations, and thus is popular for representing decimal digits using five bits. There are ways to assign weights to each bit such that the set bits sum to the desired value, with an exception for zero.
According to Federal Standard 1037C:
The weights give a unique encoding for most digits, but allow two encodings for 3: 0+3 or 10010 and 1+2 or 01100. The former is used to encode the digit 3, and the latter is used to represent the otherwise unrepresentable zero.
The IBM 7070, IBM 7072, and IBM 7074 computers used this code to represent each of the ten decimal digits in a machine word, although they numbered the bit positions 0-1-2-3-4, rather than with weights. Each word also had a sign flag, encoded using a two-out-of-three code, that could be A Alphanumeric, − Minus, or + Plus. When copied to a digit, the three bits were placed in bit positions 0-3-4. (Thus producing the numeric values 3, 6 and 9, respectively.)
A variant is the U.S. Post Office POSTNET barcode, used to represent the ZIP+4 code for automated mail sorting and routing equipment. This uses two tall bars as ""ones"" and three short bars as ""zeros"". Here, the weights assigned to the bit positions are 7-4-2-1-0. Again, zero is encoded specially, using the 7+4 combination (binary 11000) that would naturally encode 11. This method was also used in North American telephone Multi-frequency and crossbar switching systems.[http://simplethinking.com/photo/phone/vintage/xbr%20relays.shtml]
The USPS Postal Alpha Numeric Encoding Technique (PLANET) uses the same weights, but with the opposite bar-height convention.
The following table represents decimal digits from 0 to 9 in various two-out-of-five code systems:
The limit on the number of bits set is similar to, but strictly stronger than, a parity check. All constant-weight codes, including the two-out-of-five code, can not only detect any single-bit error, but also detect any unidirectional errors -- any case where all errors in a codeword are of a single type (0→1 or 1→0).

</doc>
<doc id="41829" url="https://en.wikipedia.org/wiki?curid=41829" title="NSA product types">
NSA product types

The U.S. National Security Agency (NSA) ranks cryptographic products or algorithms by a certification called product types. Product types are defined in the National Information Assurance Glossary (CNSSI No. 4009) which defines Type 1, Type 2, products and Type 3, and Type 4 algorithms.
Type 1 product.
A Type 1 product is a device or system certified by NSA for use in cryptographically securing classified U.S. Government information. A Type 1 product is defined as: 
Type 1 certification is a rigorous process that includes testing and formal analysis of (among other things) cryptographic security, functional security, tamper resistance, emissions security (EMSEC/TEMPEST), and security of the product manufacturing and distribution process.
Type 2 product.
Type 2 products are unclassified cryptographic equipment, assemblies, or components, endorsed by the National Security Agency (NSA), for use in telecommunications and automated information systems for the protection of national security information, as defined as ""Any telecommunications or information system operated by the United States Government, the function, operation, or use of which: 1. involves intelligence activities; 2. involves cryptologic activities related to national security; 3. involves command and control of military forces; 4. involves equipment that is an integral part of a weapon or weapon system; or 5. is critical to the direct fulfillment of military or intelligence missions and does not include a system that is to be used for routine administrative and business applications (including payroll, finance, logistics, and personnel management applications). (Title 40 U.S.C. Section 1452, Information Technology Management Reform Act of 1996.)"" (40 USC 1452)
Type 3 algorithm.
A Type 3 algorithm is a device for use with Sensitive, But Unclassified (SBU) information on non-national security systems, defined as ""Cryptographic algorithm registered by the National Institute of Standards and Technology (NIST) and published as a Federal Information Processing Standard (FIPS) for use in protecting unclassified sensitive information or commercial information." Approved encryption algorithms include three-key Triple DES, and AES (although AES can also be used in NSA-certified Type 1 products). Approvals for DES, two-key Triple DES and Skipjack have been withdrawn as of 2015.
Type 4 algorithm.
A Type 4 algorithm is an encryption algorithm that has been registered with NIST but is not a Federal Information Processing Standard (FIPS). Type 4 algorithms may not be used to protect classified information. 

</doc>
<doc id="41831" url="https://en.wikipedia.org/wiki?curid=41831" title="Telephony">
Telephony

Telephony ( ) is the field of technology involving the development, application, and deployment of telecommunication services for the purpose of electronic transmission of voice, fax, or data, between distant parties. The history of telephony is intimately linked to the invention and development of the telephone.
Telephony is commonly referred to as the construction or operation of telephones and telephonic systems and as a system of telecommunications in which telephonic equipment is employed in the transmission of speech or other sound between points, with or without the use of wires. The term is also used frequently to refer to computer hardware, software, and computer network systems, that perform functions traditionally performed by telephone equipment. In this context the technology is specifically referred to as Internet telephony, or voice over Internet Protocol (VoIP).
Overview.
The first telephones were connected directly in pairs. Each user had a separate telephone wired to the locations he might wish to reach. This quickly became inconvenient and unmanageable when people wanted to communicate with more than a few people. The inventions of the telephone exchange provided the solution for establishing telephone connections with any other telephone in service in the local area. Each telephone was connected to the exchange via one wire pair, the local loop. Nearby exchanges in other service areas were connected with trunk lines and long distance service could be established by relaying the calls through multiple exchanges.
Initially the switchboards were manually operated by an attendant, a "switchboard operator". When a customer cranked a handle on the telephone, it turned on an indicator on the board in front of the operator who would plug the operator headset into that jack and offer service. The caller had to ask for the called party by name, later by number, and the operator connected one end of a circuit into the called party jack to alert them. If the called station answered the operator disconnected their headset and complete the station-to-station circuit. Trunk calls were made with the assistance of other operators at other exchangers in the network.
In modern times, most telephones are plugged into telephone jacks. The jacks are connected by inside wiring to a drop wire which connects the building to a cable. Cables usually bring a large number of drop wires from all over a district access network to one wire center or telephone exchange. When a telephone user wants to make a telephone call, equipment at the exchange examines the dialed telephone number and connects that telephone line to another in the same wire center, or to a trunk to a distant exchange. Most of the exchanges in the world are interconnected through a system of larger switching systems, forming the public switched telephone network (PSTN).
After the middle of the 20th century, fax and data became important secondary users of the network created to carry voices, and late in the century, parts of the network were upgraded with ISDN and DSL to improve handling of such traffic.
Today, telephony uses digital technology (digital telephony) in the provisioning of telephone services and systems. Telephone calls can be provided digitally, but may be restricted to cases in which the last mile is digital, or where the conversion between digital and analog signals takes place inside the telephone. This advancement has reduced costs in communication, and improved the quality of voice services. The first implementation of this, ISDN, permitted all data transport from end-to-end speedily over telephone lines. This service was later made much less important due to the ability to provide digital services based on the IP protocol.
Since the advent of personal computer technology in the 1980s, computer telephony integration (CTI) has progressively provided more sophisticated telephony services, initiated and controlled by the computer, such as making and receiving voice, fax, and data calls with telephone directory services and caller identification. The integration of telephony software and computer systems is a major development in the evolution of the automated office. The term is used in describing the computerized services of call centers, such as those that direct your phone call to the right department at a business you're calling. It's also sometimes used to describe the ability to use your personal computer to initiate and manage phone calls (in which case you can think of your computer as your personal call center). CTI is not a new concept and has been used in the past in large telephone networks, but only dedicated call centers could justify the costs of the required equipment installation. Primary telephone service providers are offering information services such as automatic number identification, which is a telephone service architecture that separates CTI services from call switching and will make it easier to add new services. Dialed Number Identification Service (DNIS) on a scale is wide enough for its implementation to bring real value to business or residential telephone usage. A new generation of applications (middleware) is being developed as a result of standardization and availability of low cost computer telephony links.
Recent developments.
The term's scope has been broadened with the advent of the different new communication technologies. In its broadest sense, the terms encompasses phone communication, Internet calling, mobile communication, faxing, voicemail and video conferencing. Telephony's initial idea returns to POTS, (an acronym for "plain old telephone service") technically called the PSTN (public-switched telephone network).
This system is being fiercely challenged by and to a great extent yielding to Voice over IP (VoIP) technology, which is also commonly referred to as IP Telephony and Internet Telephony. IP telephony is a modern form of telephony which uses the TCP/IP protocol popularized by the Internet to transmit digitized voice data. Also, unlike traditional phone service, IP telephony service is relatively unregulated by government. In the United States, the Federal Communications Commission (FCC) regulates phone-to-phone connections, but says they do not plan to regulate connections between a phone user and an IP telephony service provider.Using the Internet, calls travel as packets of data on shared lines, avoiding the tolls of the PSTN. The challenge in IP telephony is to deliver the voice, fax, or video packets in a dependable flow to the user. Much of IP telephony focuses on that challenge.
Digital telephony.
Starting with the introduction of the transistor, invented in 1947 by Bell Laboratories, to amplification and switching circuits in the 1950s, and through development of computer-based electronic switching systems, the public switched telephone network (PSTN) has gradually evolved towards automation and digitization of signaling and audio transmissions.
Digital telephony is the use of digital electronics in the operation and provisioning of telephony systems and services. Since the 1960s a digital core network has replaced the traditional analog transmission and signaling systems, and much of the access network has also been digitized.
Digital telephony has dramatically improved the capacity, quality, and cost of the network. End-to-end analog telephone networks were first modified in the early 1960s by upgrading transmission networks with Digital Signal 1 (DS1/T1) carrier systems, designed to support the basic 3 kHz voice channel by sampling the bandwidth-limited analog voice signal and encoding using PCM. While digitization allows wideband voice on the same channel, the improved quality of a wider analog voice channel did not find a large market in the PSTN.
Later transmission methods such as SONET and fiber optic transmission further advanced digital transmission. Although analog carrier systems existed that multiplexed multiple analog voice channels onto a single transmission medium, digital transmission allowed lower cost and more channels multiplexed on the transmission medium. Today the end instrument often remains analog but the analog signals are typically converted to digital signals at the serving area interface (SAI), central office (CO), or other aggregation point. Digital loop carriers (DLC) place the digital network ever closer to the customer premises, relegating the analog local loop to legacy status.
IP telephony.
A specialization of digital telephony, Internet Protocol (IP) telephony involves the application of digital networking technology that was the foundation to the Internet to create, transmit, and receive telecommunications sessions over computer networks. Internet telephony is commonly known as voice over Internet Protocol (VoIP), reflecting the principle, but it has been referred with many other terms. VoIP has proven to be a disruptive technology that is rapidly replacing traditional telephone infrastructure technologies. As of January 2005, up to 10% of telephone subscribers in Japan and South Korea have switched to this digital telephone service. A January 2005 "Newsweek" article suggested that Internet telephony may be "the next big thing". As of 2006, many VoIP companies offer service to consumers and businesses.
IP telephony uses an Internet connection and hardware IP phones, analog telephone adapters, or softphone computer applications to transmit conversations encoded as data packets. In addition to replacing plain old telephone service (POTS), IP telephony services compete with mobile phone services by offering free or lower cost connections via WiFi hotspots. VoIP is also used on private networks which may or may not have a connection to the global telephone network.
Social impact research.
Direct person-to-person communication includes non-verbal cues expressed in facial and other bodily articulation, that cannot be transmitted in traditional voice telephony. Video telephony restores such interactions to varying degrees. Social Context Cues Theory is a model to measure the success of different types of communication in maintaining the non-verbal cues present in face-to-face interactions. The research examines many different cues, such as the physical context, different facial expressions, body movements, tone of voice, touch and smell.
Various communication cues are lost with the usage of the telephone. The communicating parties are not able to identify the body movements, and lack touch and smell. Although this diminished ability to identify social cues is well known, Wiesenfeld, Raghuram, and Garud point out that there is a value and efficiency to the type of communication for different tasks. They examine work places in which different types of communication, such as the telephone, are more useful than face-to-face interaction.
The expansion of communication to mobile telephone service has created a different filter of the social cues than the land-line telephone. The use of instant messaging, such as "texting", on mobile telephones has created a sense of community. In "The Social Construction of Mobile Telephony" it is suggested that each phone call and text message is more than an attempt to converse. Instead, it is a gesture which maintains the social network between family and friends. Although there is a loss of certain social cues through telephones, mobile phones bring new forms of expression of different cues that are understood by different audiences. New language additives attempt to compensate for the inherent lack of non-physical interaction.
Another social theory supported through telephony is the Media Dependency Theory. This theory concludes that people use media or a resource to attain certain goals. This theory states that there is a link between the media, audience, and the large social system. Telephones, depending on the person, help attain certain goals like accessing information, keeping in contact with others, sending quick communication, entertainment, etc.

</doc>
