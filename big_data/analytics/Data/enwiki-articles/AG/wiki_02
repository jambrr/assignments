<doc id="39027" url="https://en.wikipedia.org/wiki?curid=39027" title="Mike Tyson">
Mike Tyson

Michael Gerard "Mike" Tyson (; born June 30, 1966) is an American former professional boxer. He held the undisputed world heavyweight championship and holds the record as the youngest boxer to win the WBC, WBA and IBF heavyweight titles at 20 years, 4 months, and 22 days old. Tyson won his first 19 professional bouts by knockout, 12 of them in the first round. He won the WBC title in 1986 after defeating Trevor Berbick by a TKO in the second round. In 1987, Tyson added the WBA and IBF titles after defeating James Smith and Tony Tucker. This made him the first heavyweight boxer to simultaneously hold the WBA, WBC and IBF titles, and the only heavyweight to successively unify them.
In 1988, Tyson became the lineal champion when he knocked out Michael Spinks in 91 seconds. Tyson successfully defended the world heavyweight championship nine times, including victories over Larry Holmes and Frank Bruno. In 1990, he lost his titles to underdog James "Buster" Douglas, by knockout in round 10. Attempting to regain the titles, he defeated Donovan Ruddock twice in 1991, but pulled out of a fight with undisputed heavyweight champion Evander Holyfield due to injury. In 1992, Tyson was convicted of raping Desiree Washington and sentenced to six years in prison but was released after serving three years. After his release, he engaged in a series of comeback fights. In 1996, he won the WBC and WBA titles after defeating Frank Bruno and Bruce Seldon by knockout. With his defeat of Bruno, Tyson joined Floyd Patterson, Muhammad Ali, Tim Witherspoon, Evander Holyfield, and George Foreman as the only men in boxing history to that point to have regained a heavyweight championship after having lost it. After being stripped of the WBC title, Tyson lost his WBA crown to Evander Holyfield in November 1996 by an eleventh-round TKO. Their 1997 rematch ended when Tyson was disqualified for biting part of Holyfield's ear off.
In 2002, he fought for the world heavyweight title at the age of 35, losing by knockout to Lennox Lewis. Tyson retired from professional boxing in 2006, after being knocked out in consecutive matches against Danny Williams and Kevin McBride. Tyson declared bankruptcy in 2003, despite having received over $30 million for several of his fights and $300 million during his career. At the time it was reported that he had approximately $23 million of debt. Tyson was well known for his ferocious and intimidating boxing style as well as his controversial behavior inside and outside the ring. Nicknamed "Iron", and "Kid Dynamite" in his early career and later known as "The Baddest Man on the Planet", Tyson is considered one of the best heavyweights of all time. He was ranked No. 16 on "The Ring"s list of 100 greatest punchers of all time, and No. 1 in the ESPN.com list of "The hardest hitters in heavyweight history". Sky Sports rated him as "the scariest boxer ever" and described him as "perhaps the most ferocious fighter to step into a professional ring." He has been inducted into the International Boxing Hall of Fame and the World Boxing Hall of Fame.
Early life.
Tyson was born in Brooklyn, New York. He has an elder brother named Rodney (born c. 1961) and had an elder sister named Denise, who died of a heart attack at age 24 in February 1990.
Tyson's biological father is listed as "Purcell Tyson" (who was from Jamaica) on his birth certificate, but the man Tyson had known as his father was Jimmy Kirkpatrick. Kirkpatrick was from Grier Town, North Carolina (a predominantly black neighborhood that was annexed by the city of Charlotte), where he was one of the neighborhood's top baseball players. Kirkpatrick married and had a son, Tyson's half-brother Jimmie Lee Kirkpatrick, who would help to integrate Charlotte high school football in 1965. In 1959, Jimmy Kirkpatrick left his family and moved to Brooklyn, where he met Tyson's mother, Lorna Mae (Smith) Tyson. Mike Tyson was born in 1966. Kirkpatrick frequented pool halls, gambled and hung out on the streets. "My father was just a regular street guy caught up in the street world," Tyson said. Kirkpatrick abandoned the Tyson family around the time Mike was born, leaving Tyson's mother to care for the children on her own. Kirkpatrick died in 1992.
The family lived in Bedford-Stuyvesant until their financial burdens necessitated a move to Brownsville when Tyson was 10 years old. Tyson's mother died six years later, leaving 16-year-old Tyson in the care of boxing manager and trainer Cus D'Amato, who would become his legal guardian. Tyson later said, "I never saw my mother happy with me and proud of me for doing something: she only knew me as being a wild kid running the streets, coming home with new clothes that she knew I didn't pay for. I never got a chance to talk to her or know about her. Professionally, it has no effect, but it's crushing emotionally and personally."
Throughout his childhood, Tyson lived in and around high-crime neighborhoods. According to an interview in "Details", his first fight was with a bigger youth who had pulled the head off one of Tyson's pigeons. Tyson was repeatedly caught committing petty crimes and fighting those who ridiculed his high-pitched voice and lisp. By the age of 13, he had been arrested 38 times. He ended up at the Tryon School for Boys in Johnstown, New York. Tyson's emerging boxing ability was discovered there by Bobby Stewart, a juvenile detention center counselor and former boxer. Stewart considered Tyson to be an outstanding fighter and trained him for a few months before introducing him to Cus D'Amato. Tyson dropped out of high school as a junior. He would later be awarded an honorary Doctorate in Humane Letters from Central State University in 1989.
Kevin Rooney also trained Tyson, and he was occasionally assisted by Teddy Atlas, although he was dismissed by D'Amato when Tyson was 15. Rooney eventually took over all training duties for the young fighter.
Tyson's brother is a physician assistant in the trauma center of the Los Angeles County-University of Southern California Medical Center. He has always been very supportive of his brother's career and was often seen at Tyson's boxing matches in Las Vegas, Nevada. When asked about their relationship, Mike has been quoted saying, "My brother and I see each other occasionally and we love each other"; and, "My brother was always something and I was nothing."
Career.
Amateur career.
Tyson won gold medals at the 1981 and 1982 Junior Olympic Games, defeating Joe Cortez in 1981 and beating Kelton Brown in 1982. Brown's corner threw in the towel in the first round. He holds the Junior Olympic record for quickest knockout (8 seconds). He won every bout at the Junior Olympic Games by knockout.
He fought Henry Tillman twice as an amateur, losing both bouts by decision. Tillman went on to win heavyweight gold at the 1984 Summer Olympics in Los Angeles.
Rise to stardom.
Tyson made his professional debut as an 18-year-old on March 6, 1985, in Albany, New York. He defeated Hector Mercedes via a first round knockout. He had 15 bouts in his first year as a professional. Fighting frequently, Tyson won 26 of his first 28 fights by KO or TKO; 16 of those came in the first round. The quality of his opponents gradually increased to journeyman fighters and borderline contenders, like James Tillis, David Jaco, Jesse Ferguson, Mitch Green and Marvis Frazier. His win streak attracted media attention and Tyson was billed as the next great heavyweight champion. D'Amato died in November 1985, relatively early into Tyson's professional career, and some speculate that his death was the catalyst to many of the troubles Tyson was to experience as his life and career progressed.
Tyson's first nationally televised bout took place on February 16, 1986, at Houston Field House in Troy, New York against journeyman heavyweight Jesse Ferguson. Tyson knocked down Ferguson with an uppercut in the fifth round that broke Ferguson's nose. During the sixth round, Ferguson began to hold and clinch Tyson in an apparent attempt to avoid further punishment. After admonishing Ferguson several times to obey his commands to box, the referee finally stopped the fight near the middle of the sixth round. The fight was initially ruled a win for Tyson by disqualification (DQ) of his opponent. The ruling was "adjusted" to a win by technical knockout (TKO) after Tyson's corner protested that a DQ win would end Tyson's string of knockout victories, and that a knockout would have been the inevitable result. The rationale offered for the revised outcome was that the fight was actually stopped because Ferguson could not (rather than would not) continue boxing.
On November 22, 1986, Tyson was given his first title fight against Trevor Berbick for the World Boxing Council (WBC) heavyweight championship. Tyson won the title by second round TKO, and at the age of 20 years and 4 months became the youngest heavyweight champion in history. Tyson's dominant performance brought many accolades. Donald Saunders wrote: "The noble and manly art of boxing can at least cease worrying about its immediate future, now it has discovered a heavyweight champion fit to stand alongside Dempsey, Tunney, Louis, Marciano and Ali."
Because of Tyson's strength, many fighters were intimidated by him. This was backed up by his outstanding hand speed, accuracy, coordination, power, and timing. Tyson was also noted for his defensive abilities. Holding his hands high in the Peek-a-Boo style taught by his mentor Cus D'Amato, he slipped and weaved out of the way of the opponent's punches while closing the distance to deliver his own punches. One of Tyson's trademark combinations was a right hook to his opponent's body followed by a right uppercut to his opponent's chin; very few boxers would remain standing if caught by this combination. Lorenzo Boyd, Jesse Ferguson and Jose Ribalta were among the boxers knocked down by the combination.
Undisputed champion.
Expectations for Tyson were extremely high, and he embarked on an ambitious campaign to fight all of the top heavyweights in the world. Tyson defended his title against James Smith on March 7, 1987, in Las Vegas, Nevada. He won by unanimous decision and added Smith's World Boxing Association (WBA) title to his existing belt. 'Tyson mania' in the media was becoming rampant. He beat Pinklon Thomas in May with a knockout in the sixth round. On August 1 he took the International Boxing Federation (IBF) title from Tony Tucker in a twelve round unanimous decision. He became the first heavyweight to own all three major belts – WBA, WBC, and IBF – at the same time. Another fight, in October of that year, ended with a victory for Tyson over 1984 Olympic super heavyweight gold medalist Tyrell Biggs by knockout in the seventh round.
During this time, Tyson came to the attention of gaming company Nintendo. After witnessing one of Tyson's fights, Nintendo of America president Minoru Arakawa was impressed by the fighter's "power and skill", prompting him to suggest Tyson be included in the upcoming Nintendo Entertainment System port of the "Punch Out!!" arcade game. In 1987, Nintendo released "Mike Tyson's Punch-Out!!", which was well received and sold more than a million copies.
Tyson had three fights in 1988. He faced Larry Holmes on January 22, 1988, and defeated the legendary former champion by a fourth round KO. This was the only knockout loss Holmes suffered in 75 professional bouts. In March, Tyson then fought contender Tony Tubbs in Tokyo, Japan, fitting in an easy two-round victory amid promotional and marketing work.
On June 27, 1988, Tyson faced Michael Spinks. Spinks, who had taken the heavyweight championship from Larry Holmes via a 15-round decision in 1985, had not lost his title in the ring but was not recognized as champion by the major boxing organizations. Holmes had previously given up all but the IBF title, and that was eventually stripped from Spinks after he elected to fight Gerry Cooney (winning by a 5th-round TKO) rather than IBF Number 1 Contender Tony Tucker, as the Cooney fight provided him a larger purse. However, Spinks did become the lineal champion by beating Holmes and many (including "Ring" magazine) considered him to have a legitimate claim to being the true heavyweight champion. The bout was, at the time, the richest fight in history and expectations were very high. Boxing pundits were predicting a titanic battle of styles, with Tyson's aggressive infighting conflicting with Spinks' skillful out-boxing and footwork. The fight ended after 91 seconds when Tyson knocked Spinks out in the first round; many consider this to be the pinnacle of Tyson's fame and boxing ability. Spinks, previously unbeaten, would never fight professionally again.
Controversy and upset.
During this period, Tyson's problems outside boxing were also starting to emerge. His marriage to Robin Givens was heading for divorce, and his future contract was being fought over by Don King and Bill Cayton. In late 1988, Tyson parted with manager Bill Cayton and fired longtime trainer Kevin Rooney, the man many credit for honing Tyson's craft after the death of D'Amato. Following Rooney's departure, critics alleged that Tyson began to jab less to work inside and use the Peek-a-Boo style sporadically. Tyson insisted he hadn't altered the style that made him a world champion. In 1989, Tyson had only two fights amid personal turmoil. He faced the popular British boxer Frank Bruno in February. Bruno managed to stun Tyson at the end of the 1st round, although Tyson went on to knock out Bruno in the fifth round. Tyson then knocked out Carl "The Truth" Williams in one round in July.
By 1990, Tyson seemed to have lost direction, and his personal life was in disarray amidst reports of less vigorous training prior to the Douglas match. In a fight on February 11, 1990, he lost the undisputed championship to Buster Douglas in Tokyo. Tyson was a huge betting favorite; indeed, the Mirage, the only casino to put out odds for the fight, made Tyson a 42/1 favorite. However, Douglas was at an emotional peak after losing his mother to a stroke 23 days prior to the fight; Douglas fought the fight of his life. Contrary to reports that Tyson was out of shape, it has been noted at the time of the fight that he had pronounced muscles, an absence of body fat and weighed 220 and 1/2 pounds, only two pounds more than he had weighed when he beat Michael Spinks 20 months earlier. Mentally, however, Tyson was unprepared. Tyson failed to find a way past Douglas's quick jab that had a reach advantage over his own. Tyson did catch Douglas with an uppercut in the eighth round and knock him to the floor, but Douglas recovered sufficiently to hand Tyson a heavy beating in the subsequent two rounds. (After the fight, the Tyson camp would complain that the count was slow and that Douglas had taken longer than ten seconds to get to his feet.) Just 35 seconds into the 10th round, Douglas unleashed a brutal uppercut, followed by a four-punch combination of hooks that sent Tyson to the canvas for the first time in his career. He was counted out by referee Octavio Meyran.
The knockout victory by Douglas over Tyson, the previously undefeated "baddest man on the planet" and arguably the most feared boxer in professional boxing at that time, has been described as one of the most shocking upsets in modern sports history.
After Douglas.
After the loss, Tyson recovered with first-round knockouts of Henry Tillman and Alex Stewart in his next two fights. Tyson's victory over Tillman, the 1984 Olympic heavyweight gold medalist, enabled Tyson to avenge his amateur losses at Tillman's hands. These bouts set up an elimination match for another shot at the undisputed world heavyweight championship, which Evander Holyfield had taken from Douglas in his first defense of the title.
Tyson, who was the number one contender, faced number two contender Donovan "Razor" Ruddock on March 18, 1991, in Las Vegas. Ruddock was seen as the most dangerous heavyweight around and was thought of as one of the hardest punching heavyweights. Tyson and Ruddock went back and forth for most of the fight, until referee Richard Steele controversially stopped the fight during the seventh round in favor of Tyson. This decision infuriated the fans in attendance, sparking a post-fight melee in the audience. The referee had to be escorted from the ring.
Tyson and Ruddock met again on June 28 that year, with Tyson knocking down Ruddock twice and winning a 12 round unanimous decision. A fight between Tyson and Holyfield for the undisputed championship was scheduled for November 8, 1991 at Caesars Palace in Las Vegas, but Tyson pulled out after sustaining a rib cartilage injury during training.
Rape conviction, prison, and conversion.
Tyson was arrested in July 1991 for the rape of 18-year-old Desiree Washington, Miss Black Rhode Island, in an Indianapolis hotel room. Tyson's rape trial took place in the Marion County superior court from January 26, 1992 to February 10, 1992.
Desiree Washington testified that she received a phone call from Tyson at 1:36 am on July 19, 1991 inviting her to a party. Having joined Tyson in his limousine, Washington testified that Tyson made sexual advances towards her. She testified that upon arriving at his hotel room, Tyson pinned her down on his bed and raped her despite her pleas to stop. She ran out of the room and asked Tyson's chauffeur to drive her back to her hotel. Partial corroboration of Washington's story came via testimony from Tyson's chauffeur, Virginia Foster, who confirmed Desiree Washington's state of shock. Further testimony came from Thomas Richardson, the emergency room physician who examined Washington more than 24 hours after the incident and confirmed that Washington's physical condition was consistent with rape.
Under lead defense lawyer Vincent J. Fuller's direct examination, Tyson claimed that everything had taken place with Washington's full cooperation and he claimed not to have forced himself upon her. When he was cross-examined by lead prosecutor Gregory Garrison, Tyson denied claims that he had misled Washington and insisted that she wanted to have sex with him. Because of Tyson's hostile and defensive responses to the questions during cross-examination, some have speculated that his behavior made him unlikable to the jury who saw him as brutish and arrogant. Tyson was convicted on the rape charge on February 10, 1992 after the jury deliberated for nearly 10 hours.
Alan Dershowitz, acting as Tyson's counsel, filed an appeal urging error of law in the Court's exclusion of evidence of the victim's past sexual conduct, the exclusion of three potential defense witnesses, and the lack of a jury instruction on honest and reasonable mistake of fact. The Indiana Court of Appeals ruled against Tyson in a 2–1 vote.
On March 26, 1992, Tyson was sentenced to six years in prison followed by four years on probation. He was assigned to the Indiana Youth Center (now the Plainfield Correctional Facility) in April 1992, and he was released in March 1995 after serving three years. Hakeem Olajuwon claims that during his incarceration, Tyson converted to Islam.
Due to his conviction, Tyson is required to register as a "tier II" sex offender under federal law.
Comeback.
After being paroled from prison, Tyson easily won his comeback bouts against Peter McNeeley and Buster Mathis Jr.. Tyson's first comeback fight grossed more than US$96 million worldwide, including a United States record $63 million for PPV television. The fight was purchased by 1.52 million homes, setting both PPV viewership and revenue records. The 89-second fight elicited criticism that Tyson's management lined up "tomato cans" to ensure easy victories for his return. "TV Guide" included the Tyson-McNeeley fight in their list of the 50 Greatest TV Sports Moments of All Time in 1998.
Tyson regained one belt by easily winning the WBC title from Frank Bruno in March 1996. It was the second fight between the two, and Tyson knocked Bruno out in the third round. Tyson added the WBA belt by defeating champion Bruce Seldon in one round in September that year. Seldon was severely criticized and mocked in the popular press for seemingly collapsing to innocuous punches from Tyson.
Tyson–Holyfield fights.
"Tyson vs. Holyfield I".
Tyson attempted to defend the WBA title against Evander Holyfield, who was in the fourth fight of his own comeback. Holyfield had retired in 1994 following the loss of his championship to Michael Moorer. It was said that Don King and others saw former champion Holyfield, who was 34 at the time of the fight and a huge underdog, as a washed-up fighter.
On November 9, 1996, in Las Vegas, Nevada, Tyson faced Holyfield in a title bout dubbed "Finally." In a surprising turn of events, Holyfield, who was given virtually no chance to win by numerous commentators, defeated Tyson by TKO when referee Mitch Halpern stopped the bout in round 11. Holyfield became the second boxer to win a heavyweight championship belt three times. Holyfield's victory was marred by allegations from Tyson's camp of Holyfield's frequent headbutts during the bout. Although the headbutts were ruled accidental by the referee, they would become a point of contention in the subsequent rematch.
"Tyson vs. Holyfield II" and aftermath.
Tyson and Holyfield fought again on June 28, 1997. Originally, Halpern was supposed to be the referee, but after Tyson's camp protested, Halpern stepped aside in favor of Mills Lane. The highly anticipated rematch was dubbed "The Sound and the Fury", and it was held at the Las Vegas MGM Grand Garden Arena, site of the first bout. It was a lucrative event, drawing even more attention than the first bout and grossing $100 million. Tyson received $30 million and Holyfield $35 million, the highest paid professional boxing purses until 2007. The fight was purchased by 1.99 million households, setting a pay-per-view buy rate record that stood until the May 5, 2007, De La Hoya-Mayweather boxing match.
Soon to become one of the most controversial events in modern sports, the fight was stopped at the end of the third round, with Tyson disqualified for biting Holyfield on both ears. The first time Tyson bit him, the match was temporarily stopped. Referee Mills Lane deducted two points from Tyson and the fight resumed. However, after the match resumed, Tyson did it again; Tyson was disqualified and Holyfield won the match. One bite was severe enough to remove a piece of Holyfield's right ear, which was found on the ring floor after the fight. Tyson later stated that his actions were retaliation for Holyfield repeatedly headbutting him without penalty. In the confusion that followed the ending of the bout and announcement of the decision, a near riot erupted in the arena and several people were injured.
As a subsequent fallout from the incident, US$3 million was immediately withheld from Tyson's $30-million purse by the Nevada state boxing commission (the most it could legally hold back at the time). Two days after the fight, Tyson issued a statement, apologizing to Holyfield for his actions and asked not to be banned for life over the incident. Tyson was roundly condemned in the news media but was not without defenders. Novelist and commentator Katherine Dunn wrote a column that criticized Holyfield's sportsmanship in the controversial bout and charged the news media with being biased against Tyson.
On July 9, 1997, Tyson's boxing license was rescinded by the Nevada State Athletic Commission in a unanimous voice vote; he was also fined US$3 million and ordered to pay the legal costs of the hearing. As most state athletic commissions honor sanctions imposed by other states, this effectively made Tyson unable to box in the United States. The revocation was not permanent, as the commission voted 4–1 to restore Tyson's boxing license on October 18, 1998.
During his time away from boxing in 1998, Tyson made a guest appearance at WrestleMania XIV as an enforcer for the main event match between Shawn Michaels and Steve Austin. During this time, Tyson was also an unofficial member of D-Generation X. Tyson was paid $3 million for being guest enforcer of the match at WrestleMania XIV.
1999 to 2005.
After Holyfield.
In January 1999, Tyson returned to the ring to fight the South African Francois Botha, in another fight that ended in controversy. While Botha initially controlled the fight, Tyson allegedly attempted to break Botha's arms during a tie-up and both boxers were cautioned by the referee in the ill-tempered bout. Botha was ahead on points on all scorecards and was confident enough to mock Tyson as the fight continued. Nonetheless, Tyson landed a straight right-hand in the fifth round that knocked out Botha. Critics noticed Tyson stopped using the bob and weave defense altogether following this return.
Legal problems caught up with Tyson once again. On February 5, 1999, Tyson was sentenced to a year's imprisonment, fined $5,000, and ordered to serve two years probation and perform 200 hours of community service for assaulting two motorists after a traffic accident on August 31, 1998. He served nine months of that sentence. After his release, he fought Orlin Norris on October 23, 1999. Tyson knocked down Norris with a left hook thrown after the bell sounded to end the first round. Norris injured his knee when he went down and said he was unable to continue the fight. Consequently, the bout was ruled a no contest.
In 2000, Tyson had three fights. The first was staged at the MEN Arena, Manchester, England against Julius Francis. Following controversy as to whether Tyson should be allowed into the country, he took four minutes to knock out Francis, ending the bout in the second round. He also fought Lou Savarese in June 2000 in Glasgow, winning in the first round; the fight lasted only 38 seconds. Tyson continued punching after the referee had stopped the fight, knocking the referee to the floor as he tried to separate the boxers. In October, Tyson fought the similarly controversial Andrew Golota, winning in round three after Gołota was unable to continue due to a broken jaw. The result was later changed to no contest after Tyson refused to take a pre-fight drug test and then tested positive for marijuana in a post-fight urine test. Tyson fought only once in 2001, beating Brian Nielsen in Copenhagen with a seventh round TKO.
Lewis vs. Tyson.
Tyson once again had the opportunity to fight for a heavyweight championship in 2002. Lennox Lewis held the WBC, IBF, IBO and Lineal titles at the time. As promising amateurs, Tyson and Lewis had sparred at a training camp in a meeting arranged by Cus D'Amato in 1984. Tyson sought to fight Lewis in Nevada for a more lucrative box-office venue, but the Nevada Boxing Commission refused him a license to box as he was facing possible sexual assault charges at the time.
Two years prior to the bout, Tyson had made several inflammatory remarks to Lewis in an interview following the Savarese fight. The remarks included the statement "I want your heart, I want to eat your children." On January 22, 2002, the two boxers and their entourages were involved in a brawl at a New York press conference to publicize the planned event. A few weeks later, the Nevada State Athletic Commission refused to grant Tyson a license for the fight, forcing the promoters to make alternative arrangements. After multiple states balked at granting Tyson a license, the fight eventually occurred on June 8 at the Pyramid Arena in Memphis, Tennessee. Lewis dominated the fight and knocked out Tyson with a right hook in the eighth round. Tyson was respectful after the fight and praised Lewis on his victory. This fight was the highest-grossing event in pay-per-view history at that time, generating $106.9 million from 1.95 million buys in the USA.
Late career, bankruptcy and retirement.
In another Memphis fight on February 22, 2003, Tyson beat fringe contender Clifford Etienne 49 seconds into round one. The pre-fight was marred by rumors of Tyson's lack of fitness. Some said that he took time out from training to party in Las Vegas and get a new facial tattoo. This would be Tyson's final professional victory in the ring.
In August 2003, after years of financial struggles, Tyson finally filed for bankruptcy. In 2003, amid all his economic troubles, he was named by "Ring Magazine" at number 16, right behind Sonny Liston, among the 100 greatest punchers of all time.
On August 13, 2003, Tyson entered the ring for a face-to-face confrontation against K-1 fighting phenom Bob Sapp immediately after Sapp's win against Kimo Leopoldo in Las Vegas. K-1 signed Tyson to a contract with the hopes of making a fight happen between the two, but Tyson's felony history made it impossible for him to obtain a visa to enter Japan, where the fight would have been most profitable. Alternative locations were discussed, but the fight ultimately failed to happen.
On July 30, 2004, Tyson faced British boxer Danny Williams in another comeback fight, this time staged in Louisville, Kentucky. Tyson dominated the opening two rounds. The third round was even, with Williams getting in some clean blows and also a few illegal ones, for which he was penalized. In the fourth round, Tyson was unexpectedly knocked out. After the fight, it was revealed that Tyson was trying to fight on one leg, having torn a ligament in his other knee in the first round. This was Tyson's fifth career defeat. He underwent surgery for the ligament four days after the fight. His manager, Shelly Finkel, claimed that Tyson was unable to throw meaningful right-hand punches since he had a knee injury.
On June 11, 2005, Tyson stunned the boxing world by quitting before the start of the seventh round in a close bout against journeyman Kevin McBride. In the 2008 documentary "Tyson", he stated that he fought McBride for a payday, that he did not anticipate winning, that he was in poor physical condition and fed up with taking boxing seriously. After losing three of his last four fights, Tyson said he would quit boxing because he felt he had lost his passion for the sport.
When Tyson fired everyone working for him and got new accountants in 2000, they prepared a statement showing he started the year $3.3 million in the hole but made $65.7 million. "The problem was that I spent $62 million that year,' Tyson said, "I just said to myself, Wow, this is over. Now I can go out and really have fun.". In August 2007, Tyson pleaded guilty to drug possession and driving under the influence in an Arizona court, which stemmed from an arrest in December where authorities said Tyson, who has a long history of legal problems, admitted to using cocaine that day and to being addicted to the drug.
Exhibition tour.
To help pay off his debts, Tyson returned to the ring in 2006 for a series of four-round exhibitions against journeyman heavyweight Corey "T-Rex" Sanders in Youngstown, Ohio. Tyson, without headgear at 5 ft 10.5 in and 216 pounds, was in great shape, but far from his prime against Sanders, with headgear at 6 ft 8 in and 293 pounds, a loser of his last seven pro bouts and nearly blind from a detached retina in his left eye. Tyson appeared to be "holding back" in these exhibitions to prevent an early end to the "show". "If I don't get out of this financial quagmire there's a possibility I may have to be a punching bag for somebody. The money I make isn't going to help my bills from a tremendous standpoint, but I'm going to feel better about myself. I'm not going to be depressed," explained Tyson about the reasons for his "comeback".
Legacy.
A 1998 ranking of "The Greatest Heavyweights of All-Time" by Ring magazine placed Tyson at No.14 on the list. Despite criticism of facing underwhelming competition during his run as champion, Tyson's knockout power and intimidation factor made him the sport's most dynamic box office attraction. According to Douglas Quenqua of "The New York Times", "The began with Mike Tyson, considered by many to be the last great heavyweight champion, losing his title to the little-known Buster Douglas. Seven years later, Mr. Tyson bit Evander Holyfield's ear in a heavyweight champion bout — hardly a proud moment for the sport."
In Ring Magazine's list of the 80 Best Fighters of the Last 80 Years, released in 2002, Tyson was ranked at No. 72. He is ranked No. 16 on "Ring Magazine"'s 2003 list of 100 greatest punchers of all time.
On June 12, 2011, Tyson was inducted to the International Boxing Hall of Fame alongside legendary Mexican champion Julio César Chávez, light welterweight champion Kostya Tszyu, and actor/screenwriter Sylvester Stallone.
After professional boxing.
In an interview with "USA Today" published on June 3, 2005, Tyson said, "My whole life has been a waste – I've been a failure." He continued: "I just want to escape. I'm really embarrassed with myself and my life. I want to be a missionary. I think I could do that while keeping my dignity without letting people know they chased me out of the country. I want to get this part of my life over as soon as possible. In this country nothing good is going to come of me. People put me so high; I wanted to tear that image down." Tyson began to spend much of his time tending to his 350 pigeons in Paradise Valley, an upscale enclave near Phoenix, Arizona.
Tyson has stayed in the limelight by promoting various websites and companies. In the past Tyson had shunned endorsements, accusing other athletes of putting on a false front to obtain them. Tyson has held entertainment boxing shows at a casino in Las Vegas and started a tour of exhibition bouts to pay off his numerous debts.
On December 29, 2006, Tyson was arrested in Scottsdale, Arizona, on suspicion of DUI and felony drug possession; he nearly crashed into a police SUV shortly after leaving a nightclub. According to a police probable-cause statement, filed in Maricopa County Superior Court, "admitted to using [drugs today and stated he is an addict and has a problem." Tyson pleaded not guilty on January 22, 2007 in Maricopa County Superior Court to felony drug possession and paraphernalia possession counts and two misdemeanor counts of driving under the influence of drugs. On February 8 he checked himself into an inpatient treatment program for "various addictions" while awaiting trial on the drug charges.
On September 24, 2007, Mike Tyson pleaded guilty to possession of cocaine and driving under the influence. He was convicted of these charges in November 2007 and sentenced to 24 hours in jail, 360 hours community service and 3 years probation. Prosecutors had requested a year-long jail sentence, but the judge praised Tyson for seeking help with his drug problems. On November 11, 2009, Mike Tyson was arrested after getting into a scuffle at Los Angeles International airport with a photographer. No charges were filed.
Tyson has taken acting roles in movies and television, most famously playing a fictionalized version of himself in the 2009 film "The Hangover". Tyson has continued to appear in the WWE. 
In September 2011, Tyson gave an interview in which he made comments about former Alaska governor Sarah Palin that included crude and violent descriptions of interracial sex. These comments were then reprinted on the Daily Caller website. Journalist Greta van Susteren criticized Tyson and the Daily Caller over the comments, which she described as "smut" and "violence against women".
After debuting a one-man show in Las Vegas, Tyson teamed up with director Spike Lee and brought the show to Broadway in August 2012. In February 2013, Tyson took his one-man show "Mike Tyson: Undisputed Truth" on a 36-city, three-month national tour. Tyson talks about his personal and professional life on stage. The one-man show was aired on HBO on November 16, 2013.
In October 2012, Tyson launched the Mike Tyson Cares Foundation. The mission of the Mike Tyson Cares Foundation is to "give kids a fighting chance" by providing innovative centers that provide for the comprehensive needs of kids from broken homes.
In August 2013, Tyson teamed up with Acquinity Interactive CEO Garry Jonas to form Iron Mike Productions, a boxing promotions company, formerly known as Acquinity Sports.
In September 2013, Tyson was featured on a six-episode television series on Fox Sports 1 that documented his personal and private life entitled "Being Mike Tyson".
In November 2013, Tyson released his book "Undisputed Truth", which also made it on The New York Times Best Seller list. An animated series named "Mike Tyson Mysteries", featuring Tyson solving mysteries in the style of Scooby-Doo, premiered on Adult Swim in late October 2014.
In early March 2015, Tyson appeared on the track "Iconic" on Madonna's album "Rebel Heart". Tyson says some lines at the beginning of the song. 
In late March 2015, "Ip Man 3" was announced. With Donnie Yen reprising his role as the titular character, Bruce Lee's martial arts master, Ip Man, while Mike Tyson has been confirmed to join the cast. Principal photography began on March 25, 2015, and was premiered in Hong Kong on 16 December 2015.
Personal life.
Tyson resides in Seven Hills, Nevada. He has been married three times. He has fathered seven children, one deceased, by three women; in addition to his biological children, Tyson includes the oldest daughter of his second wife as one of his own.
His first marriage was to actress Robin Givens, from February 7, 1988 to February 14, 1989. Givens was famous for her work on the sitcom "Head of the Class." Tyson's marriage to Givens was especially tumultuous, with allegations of violence, spousal abuse and mental instability on Tyson's part. Matters came to a head when Tyson and Givens gave a joint interview with Barbara Walters on the ABC TV newsmagazine show "20/20" in September 1988, in which Givens described life with Tyson as "torture, pure hell, worse than anything I could possibly imagine." Givens also described Tyson as "manic depressive" on national television while Tyson looked on with an intent and calm expression. A month later, Givens announced that she was seeking a divorce from the allegedly abusive Tyson. They had no children but she reported having had a miscarriage; Tyson reports that she was never pregnant and only used that to get him to marry her. During their marriage, the couple lived in a mansion in Bernardsville, New Jersey.
His second marriage was to Monica Turner from April 19, 1997 to January 14, 2003. At the time of the divorce filing, Turner worked as a pediatric resident at Georgetown University Medical Center in Washington DC. She is the sister of Michael Steele, the former Lieutenant Governor of Maryland and former Republican National Committee Chairman. Turner filed for divorce from Tyson in January 2002, claiming that he committed adultery during their five-year marriage, an act that "has neither been forgiven nor condoned." The couple had two children; son Amir, and daughter Rayna.
On May 25, 2009, Tyson's four-year-old daughter Exodus was found by her seven-year-old brother Miguel, unconscious and tangled in a cord, dangling from an exercise treadmill. The child's mother untangled her, administered CPR and called for medical attention. She died of her injuries on May 26, 2009.
Eleven days after his daughter's death, Tyson wed for the third time, to longtime girlfriend Lakiha "Kiki" Spicer, age 32, exchanging vows on Saturday, June 6, 2009, in a short, private ceremony at the La Bella Wedding Chapel at the Las Vegas Hilton. They have two children; daughter, Milan, and son, Morocco.
Tyson has been diagnosed with bipolar disorder. While on the American talk show "The View" in early May 2010, Tyson revealed that he is now forced to live paycheck to paycheck. He went on to say: "I'm totally destitute and broke. But I have an awesome life, I have an awesome wife who cares about me. ... I'm totally broke. I had a lot of fun. It broke just happened. I'm very grateful. I don't deserve to have the wife that I have; I don't deserve the kids that I have, but I do, and I'm very grateful."
In March 2011, Tyson appeared on "The Ellen DeGeneres Show" to discuss his new Animal Planet reality series, "Taking on Tyson." In the interview with DeGeneres, Tyson discussed some of the ways he had improved his life in the past two years, including sober living and a vegan diet. However, in August 2013 he admitted publicly that he had lied about his sobriety and was on the verge of death from alcoholism.
In December 2013, during an interview with Fox News, Tyson talked about his progress with sobriety and how being in the company of good people has made him want to be a better and more humble person. Tyson also talked about religion and said that he is very grateful to be a Muslim and that he needs Allah. He also revealed that he is no longer vegan after four years.
In 2015, Tyson announced that he was supporting Donald Trump's presidential candidacy.
In popular culture.
At the height of his fame and career in the late 1980s and throughout the 1990s, Tyson was one of the most recognized sports personalities in the world. Apart from his many sporting accomplishments, his outrageous and controversial behavior in the ring and in his private life has kept him in the public eye and in the courtroom. As such, Tyson has appeared in myriad popular media in cameo appearances in film and television. He has also been featured in video games and as a subject of parody or satire.
The Blackstreet single "Booti Call" was written about Tyson's rape trial and conviction. Boogie Down Productions' 1992 song "Say Gal" also addressed the rape trial.
The film "Tyson" was released in 1995 and was directed by Uli Edel. It explores the life of Mike Tyson, from the death of his guardian and trainer Cus D'Amato to his rape conviction. Tyson is played by Michael Jai White.
Published in 2007, author Joe Layden's book "The Last Great Fight: The Extraordinary Tale of Two Men and How One Fight Changed Their Lives Forever", chronicled the lives of Tyson and Douglas before and after their heavyweight championship fight. The book received positive reviews and claimed the fight was essentially the beginning of the end of boxing's popularity in mainstream sports.
In 2008, the critically acclaimed documentary "Tyson" premiered at the annual Cannes Film Festival in France. The film was directed by James Toback and has interviews with Tyson and clips of his fights and from his personal life.
The Felice Brothers, a folk-rock band from Upstate New York, released a song on their 2011 album "Celebration, Florida" titled "Cus's Catskill Gym". The song tells the story, albeit briefly, of Mike Tyson and a few notable characters and moments in his life.
He is the titular character in "Mike Tyson Mysteries", which started airing on October 27, 2014 on Adult Swim.
Awards and honors.
Source:

</doc>
<doc id="39029" url="https://en.wikipedia.org/wiki?curid=39029" title="Bamboo">
Bamboo

The bamboos are a subfamily (Bambusoideae) of flowering perennial evergreen plants in the grass family Poaceae.
Giant bamboos are the largest members of the grass family. In bamboo, the internodal regions of the stem are usually hollow and the vascular bundles in the cross section are scattered throughout the stem instead of in a cylindrical arrangement. The dicotyledonous woody xylem is also absent. The absence of secondary growth wood causes the stems of monocots, including the palms and large bamboos, to be columnar rather than tapering.
Bamboos include some of the fastest-growing plants in the world, due to a unique rhizome-dependent system. Certain species of bamboo can grow 91 cm (3 ft) within a 24-hour period, at a rate of almost 4 cm (1.5 in) an hour (a growth around 1 mm every 90 seconds, or one inch every 40 minutes). Bamboos are of notable economic and cultural significance in South Asia, Southeast Asia and East Asia, being used for building materials, as a food source, and as a versatile raw product. Bamboo has a higher compressive strength than wood, brick, or concrete and a tensile strength that rivals steel.
The word bamboo comes from the Kannada term "bambu", which was introduced to English through Indonesian and Malay.
Systematics and taxonomy.
Bamboos have long been considered the most primitive grasses, mostly because of the presence of bracteate, indeterminate inflorescences, "pseudospikelets", and flowers with three lodicules, six stamens, and three stigmata. Following more recent molecular phylogenetic research, many tribes and genera of grasses formerly included in the Bambusoideae are now classified in other subfamilies, e.g. the Anomochlooideae, the Puelioideae, and the Ehrhartoideae. The subfamily in its current sense belongs to the BOP clade of grasses, where it is sister to the Pooideae (bluegrasses and relatives).
The bamboos contains three clades classified as tribes, and these strongly correspond with geographic divisions, representing the New World herbaceous species (Olyreae), tropical woody bamboos (Bambuseae), and temperate woody bamboos (Arundinarieae). The woody bamboos do not form a monophyletic group; instead, the tropical woody and herbaceous bamboos are sister to the temperate woody bamboos. Altogether, more than 1,400 species are placed in 115 genera.
21 genera:
91 genera:
16 genera:
"Acidosasa, Ampelocalamus, Arundinaria, Borinda, Chimonocalamus, Drepanostachyum" ("Himalayacalamus"), "Fargesia, Ferrocalamus, Gaoligongshania, Gelidocalamus, Indocalamus, Oligostachyum, Pseudosasa, Sasa, Thamnocalamus, Yushania".
Distribution.
Bamboo species are found in diverse climates, from cold mountains to hot tropical regions. They occur across East Asia, from 50°N latitude in Sakhalin through to Northern Australia, and west to India and the Himalayas. They also occur in sub-Saharan Africa, and in the Americas from the USA mid-Atlantic states south to Argentina and Chile, reaching their southernmost point at 47°S latitude. Continental Europe is not known to have any native species of bamboo.
Recently, some attempts have been made to grow bamboo on a commercial basis in the Great Lakes region of east-central Africa, especially in Rwanda. In the United States, several companies are growing, harvesting, and distributing species such as "Phyllostachys nigra" (Henon) and "Phyllostachys edulis" (Moso).
Ecology.
Bamboos include some of the fastest-growing plants on Earth, with reported growth rates up to in 24 hours. However, the growth rate is dependent on local soil and climatic conditions, as well as species, and a more typical growth rate for many commonly cultivated bamboos in temperate climates is in the range of per day during the growing period. Primarily growing in regions of warmer climates during the late Cretaceous period, vast fields existed in what is now Asia. Some of the largest timber bamboo can grow over tall, and be as large as in diameter. However, the size range for mature bamboo is species-dependent, with the smallest bamboos reaching only several inches high at maturity. A typical height range that would cover many of the common bamboos grown in the United States is , depending on species. Anji County of China, known as the "Town of Bamboo", provides the optimal climate and soil conditions to grow, harvest, and process some of the most valued bamboo poles available worldwide.
Unlike all trees, individual bamboo culms emerge from the ground at their full diameter and grow to their full height in a single growing season of three to four months. During these several months, each new shoot grows vertically into a culm with no branching out until the majority of the mature height is reached. Then, the branches extend from the nodes and leafing out occurs. In the next year, the pulpy wall of each culm slowly hardens. During the third year, the culm hardens further. The shoot is now considered a fully mature culm. Over the next 2–5 years (depending on species), fungus begins to form on the outside of the culm, which eventually penetrates and overcomes the culm. Around 5–8 years later (species- and climate-dependent), the fungal growths cause the culm to collapse and decay. This brief life means culms are ready for harvest and suitable for use in construction within about three to seven years. Individual bamboo culms do not get any taller or larger in diameter in subsequent years than they do in their first year, and they do not replace any growth lost from pruning or natural breakage. Bamboo has a wide range of hardiness depending on species and locale. Small or young specimens of an individual species produce small culms initially. As the clump and its rhizome system mature, taller and larger culms are produced each year until the plant approaches its particular species limits of height and diameter.
Many tropical bamboo species die at or near freezing temperatures, while some of the hardier temperate bamboos can survive temperatures as low as . Some of the hardiest bamboo species can be grown in places as cold as USDA plant hardiness zones 5–6, although they typically defoliate and may even lose all above-ground growth, yet the rhizomes survive and send up shoots again the next spring. In milder climates, such as USDA zone 8 and above, some hardy bamboo may remain fully leafed out year-round.
Mass flowering.
Most bamboo species flower infrequently. In fact, many only flower at intervals as long as 65 or 120 years. These taxa exhibit mass flowering (or gregarious flowering), with all plants in a particular 'cohort' flowering over a several-year period. Any plant derived through clonal propagation from this cohort will also flower regardless of whether it has been planted in a different location. The longest mass flowering interval known is 130 years, and it is for the species "Phyllostachys bambusoides" (Sieb. & Zucc.). In this species, all plants of the same stock flower at the same time, regardless of differences in geographic locations or climatic conditions, and then the bamboo dies. The lack of environmental impact on the time of flowering indicates the presence of some sort of "alarm clock" in each cell of the plant which signals the diversion of all energy to flower production and the cessation of vegetative growth. This mechanism, as well as the evolutionary cause behind it, is still largely a mystery.
One hypothesis to explain the evolution of this semelparous mass flowering is the predator satiation hypothesis, which argues that by fruiting at the same time, a population increases the survival rate of its seeds by flooding the area with fruit, so even if predators eat their fill, seeds will still be left over. By having a flowering cycle longer than the lifespan of the rodent predators, bamboos can regulate animal populations by causing starvation during the period between flowering events. Thus, the death of the adult clone is due to resource exhaustion, as it would be more effective for parent plants to devote all resources to creating a large seed crop than to hold back energy for their own regeneration.
Another, the fire cycle hypothesis, states that periodic flowering followed by death of the adult plants has evolved as a mechanism to create disturbance in the habitat, thus providing the seedlings with a gap in which to grow. This argues that the dead culms create a large fuel load, and also a large target for lightning strikes, increasing the likelihood of wildfire. Because bamboos can be aggressive as early successional plants, the seedlings would be able to outstrip other plants and take over the space left by their parents.
However, both have been disputed for different reasons. The predator satiation hypothesis does not explain why the flowering cycle is 10 times longer than the lifespan of the local rodents, something not predicted. The bamboo fire cycle hypothesis is considered by a few scientists to be unreasonable; they argue that fires only result from humans and there is no natural fire in India. This notion is considered wrong based on distribution of lightning strike data during the dry season throughout India. However, another argument against this is the lack of precedent for any living organism to harness something as unpredictable as lightning strikes to increase its chance of survival as part of natural evolutionary progress.
More recently, a mathematical explanation for the extreme length of the flowering cycles has been offered, involving both the stabilizing selection implied by the predator satiation hypothesis and others, and the fact that plants that flower at longer intervals tend to release more seeds. The hypothesis claims that bamboo flowering intervals grew by integer multiplication. A mutant bamboo plant flowering at a noninteger multiple of its population's flowering interval would release its seeds alone, and would not enjoy the benefits of collective flowering (such as protection from predators). However, a mutant bamboo plant flowering at an integer multiple of its population's flowering interval would release its seeds only during collective flowering events, and would release more seeds than the average plant in the population. It could, therefore, take over the population, establishing a flowering interval that is an integer multiple of the previous flowering interval. The hypothesis predicts that observed bamboo flowering intervals should factorize into small prime numbers.
The mass fruiting also has direct economic and ecological consequences, however. The huge increase in available fruit in the forests often causes a boom in rodent populations, leading to increases in disease and famine in nearby human populations. For example, devastating consequences occur when the "Melocanna bambusoides" population flowers and fruits once every 30–35 years around the Bay of Bengal. The death of the bamboo plants following their fruiting means the local people lose their building material, and the large increase in bamboo fruit leads to a rapid increase in rodent populations. As the number of rodents increases, they consume all available food, including grain fields and stored food, sometimes leading to famine. These rats can also carry dangerous diseases, such as typhus, typhoid, and bubonic plague, which can reach epidemic proportions as the rodents increase in number. The relationship between rat populations and bamboo flowering was examined in a 2009 "Nova" documentary .
In any case, flowering produces masses of seeds, typically suspended from the ends of the branches. These seeds give rise to a new generation of plants that may be identical in appearance to those that preceded the flowering, or they may produce new cultivars with different characteristics, such as the presence or absence of striping or other changes in coloration of the culms.
Several bamboo species are never known to set seed even when sporadically flowering has been reported. "Bambusa vulgaris", "Bambusa balcooa", and "Dendrocalamus stocksii" are common examples of such bamboo.
Animal diet.
Soft bamboo shoots, stems, and leaves are the major food source of the giant panda of China, the red panda of Nepal, and the bamboo lemurs of Madagascar. Rats eat the fruits as described above. Mountain gorillas of Africa also feed on bamboo, and have been documented consuming bamboo sap which was fermented and alcoholic; chimpanzees and elephants of the region also eat the stalks.
The larvae of the bamboo borer (the moth "Omphisa fuscidentalis") of Laos, Myanmar, Thailand, and Yunnan Province, China, feeds off the pulp of live bamboo. In turn, these caterpillars are considered a local delicacy.
Cultivation.
Commercial timber.
Timber is harvested from both cultivated and wild stands, and some of the larger bamboos, particularly species in the genus "Phyllostachys", are known as "timber bamboos".
Harvesting.
Bamboo used for construction purposes must be harvested when the culms reach their greatest strength and when sugar levels in the sap are at their lowest, as high sugar content increases the ease and rate of pest infestation.
Harvesting of bamboo is typically undertaken according to these cycles:
1) Lifecycle of the culm:
As each individual culm goes through a 5– to 7-year lifecycle, culms are ideally allowed to reach this level of maturity prior to full capacity harvesting. The clearing out or thinning of culms, particularly older decaying culms, helps to ensure adequate light and resources for new growth. Well-maintained clumps may have a productivity three to four times that of an unharvested wild clump. Consistent with the lifecycle described above, bamboo is harvested from two to three years through to five to seven years, depending on the species.
2) Annual cycle:
As all growth of new bamboo occurs during the wet season, disturbing the clump during this phase will potentially damage the upcoming crop. Also during this high-rainfall period, sap levels are at their highest, and then diminish towards the dry season. Picking immediately prior to the wet/growth season may also damage new shoots. Hence, harvesting is best a few months prior to the start of the wet season.
3) Daily cycle:
During the height of the day, photosynthesis is at its peak, producing the highest levels of sugar in sap, making this the least ideal time of day to harvest. Many traditional practitioners believe the best time to harvest is at dawn or dusk on a waning moon.
Leaching.
Leaching is the removal of sap after harvest. In many areas of the world, the sap levels in harvested bamboo are reduced either through leaching or postharvest photosynthesis.
Examples of this practice include:
In the process of water leaching, the bamboo is dried slowly and evenly in the shade to avoid cracking in the outer skin of the bamboo, thereby reducing opportunities for pest infestation.
Durability of bamboo in construction is directly related to how well it is handled from the moment of planting through harvesting, transportation, storage, design, construction, and maintenance. Bamboo harvested at the correct time of year and then exposed to ground contact or rain will break down just as quickly as incorrectly harvested material.
Ornamental bamboos.
The two general patterns for the growth of bamboo are "clumping" (sympodial) and "running" (monopodial). Clumping bamboo species tend to spread slowly, as the growth pattern of the rhizomes is to simply expand the root mass gradually, similar to ornamental grasses. "Running" bamboos, though, need to be controlled during cultivation because of their potential for aggressive behavior. They spread mainly through their roots and/or rhizomes, which can spread widely underground and send up new culms to break through the surface. Running bamboo species are highly variable in their tendency to spread; this is related to both the species and the soil and climate conditions. Some can send out runners of several metres a year, while others can stay in the same general area for long periods. If neglected, over time, they can cause problems by moving into adjacent areas.
Bamboos seldom and unpredictably flower, and the frequency of flowering varies greatly from species to species. Once flowering takes place, a plant declines and often dies entirely. Although a few species of bamboo are always in flower at any given time, collectors desiring to grow specific bamboo typically obtain their plants as divisions of already-growing plants, rather than waiting for seeds to be produced.
Regular observations indicate major growth directions and locations. Once the rhizomes are cut, they are typically removed; however, rhizomes take a number of months to mature, and an immature, severed rhizome usually ceases growing if left in-ground. If any bamboo shoots come up outside of the bamboo area afterwards, their presence indicates the precise location of the removed rhizome. The fibrous roots that radiate from the rhizomes do not produce more bamboo.
Bamboo growth is also controlled by surrounding the plant or grove with a physical barrier. Typically, concrete and specially rolled HDPE plastic are the materials used to create the barrier, which is placed in a 60– to 90-cm-deep ditch around the planting, and angled out at the top to direct the rhizomes to the surface. (This is only possible if the barrier is installed in a straight line.) If the containment area is small, this method can be detrimental to ornamental bamboo, as the bamboo within can become rootbound and start to display the signs of any unhealthy containerized plant. In addition, rhizomes can escape over the top, or beneath the barrier if it is not deep enough. Strong rhizomes and tools can penetrate plastic barriers, so care must be taken. In small areas, regular maintenance may be the best method for controlling the running bamboos. Barriers and edging are unnecessary for clump-forming bamboos, although these may eventually need to have portions removed if they become too large.
The ornamental plant sold in containers and marketed as "lucky bamboo" is actually an entirely unrelated plant, "Dracaena sanderiana". It is a resilient member of the lily family that grows in the dark, tropical rainforests of Southeast Asia and Africa. Lucky bamboo has long been associated with the Eastern practice of "feng shui" and images of the plant widely available on the Web are often used to depict bamboo. On a similar note, Japanese knotweed is also sometimes mistaken for a bamboo, but it grows wild and is considered an invasive species.
"Phyllostachys" species of bamboo are also considered invasive and illegal to sell or propagate in some areas of the US.
Uses.
Culinary.
Although the shoots (new culms that come out of the ground) of bamboo contain a toxin taxiphyllin (a cyanogenic glycoside) that produces cyanide in the gut, proper processing renders them edible. They are used in numerous Asian dishes and broths, and are available in supermarkets in various sliced forms, in both fresh and canned versions. The golden bamboo lemur ingests many times the quantity of the taxiphyllin-containing bamboo that would kill a human.
The bamboo shoot in its fermented state forms an important ingredient in cuisines across the Himalayas. In Assam, India, for example, it is called "khorisa". In Nepal, a delicacy popular across ethnic boundaries consists of bamboo shoots fermented with turmeric and oil, and cooked with potatoes into a dish that usually accompanies rice ("alu tama (आलु तामा)" in Nepali).
In Indonesia, they are sliced thin and then boiled with "santan" (thick coconut milk) and spices to make a dish called "gulai rebung". Other recipes using bamboo shoots are "sayur lodeh" (mixed vegetables in coconut milk) and "lun pia" (sometimes written "lumpia": fried wrapped bamboo shoots with vegetables). The shoots of some species contain toxins that need to be leached or boiled out before they can be eaten safely.
Pickled bamboo, used as a condiment, may also be made from the pith of the young shoots.
The sap of young stalks tapped during the rainy season may be fermented to make "ulanzi" (a sweet wine) or simply made into a soft drink. Bamboo leaves are also used as wrappers for steamed dumplings which usually contains glutinous rice and other ingredients.
Pickled bamboo shoots (Nepali: तामा "tama") are cooked with black-eyed beans as a delicacy food in Nepal. Many Nepalese restaurant around the world serve this dish as "aloo bodi tama". Fresh bamboo shoots are sliced and pickled with mustard seeds and turmeric and kept in glass jar in direct sunlight for the best taste. It is used alongside many dried beans in cooking during winters. Baby shoots (Nepali: "tusa") of a very different variety of bamboo (Nepali: निगालो Nigalo) native to Nepal is cooked as a curry in hilly regions.
In Sambalpur, India, the tender shoots are grated into juliennes and fermented to prepare "kardi". The name is derived from the Sanskrit word for bamboo shoot, "karira". This fermented bamboo shoot is used in various culinary preparations, notably "amil", a sour vegetable soup. It is also made into pancakes using rice flour as a binding agent. The shoots that have turned a little fibrous are fermented, dried, and ground to sand-sized particles to prepare a garnish known as "hendua". It is also cooked with tender pumpkin leaves to make sag green leaves.
In Konkani cuisine, the tender shoots ("kirlu") are grated and cooked with crushed jackfruit seeds to prepare "kirla sukke".
The empty hollow in the stalks of larger bamboo is often used to cook food in many Asian cultures. Soups are boiled and rice is cooked in the hollows of fresh stalks of bamboo directly over a flame. Similarly, steamed tea is sometimes rammed into bamboo hollows to produce compressed forms of Pu-erh tea. Cooking food in bamboo is said to give the food a subtle but distinctive taste.
In addition, bamboo is frequently used for cooking utensils within many cultures, and is used in the manufacture of chopsticks. In modern times, some see bamboo tools as an ecofriendly alternative to other manufactured utensils.
Medicine.
Bamboo is used in traditional Chinese medicine for treating infections and healing. In northern Indian state of Assam, the fermented bamboo paste known as "khorisa" is known locally as a folk remedy for the treatment of impotence, infertility, and menstrual pains.
Construction.
Bamboo, like true wood, is a natural composite material with a high strength-to-weight ratio useful for structures.
In its natural form, bamboo as a construction material is traditionally associated with the cultures of South Asia, East Asia, and the South Pacific, to some extent in Central and South America, and by extension in the aesthetic of Tiki culture. In China and India, bamboo was used to hold up simple suspension bridges, either by making cables of split bamboo or twisting whole culms of sufficiently pliable bamboo together. One such bridge in the area of Qian-Xian is referenced in writings dating back to 960 AD and may have stood since as far back as the third century BC, due largely to continuous maintenance.
Bamboo has also long been used as scaffolding; the practice has been banned in China for buildings over six stories, but is still in continuous use for skyscrapers in Hong Kong. In the Philippines, the nipa hut is a fairly typical example of the most basic sort of housing where bamboo is used; the walls are split and woven bamboo, and bamboo slats and poles may be used as its support. In Japanese architecture, bamboo is used primarily as a supplemental and/or decorative element in buildings such as fencing, fountains, grates, and gutters, largely due to the ready abundance of quality timber.
Various structural shapes may be made by training the bamboo to assume them as it grows. Squared sections of bamboo are created by compressing the growing stalk within a square form. Arches may similarly be created by forcing the bamboo's growth into the desired form, costing much less than it would to obtain the same shape with regular wood timber. More traditional forming methods, such as the application of heat and pressure, may also be used to curve or flatten the cut stalks.
Bamboo can be cut and laminated into sheets and planks. This process involves cutting stalks into thin strips, planing them flat, and boiling and drying the strips; they are then glued, pressed, and finished. Long used in China and Japan, entrepreneurs started developing and selling laminated bamboo flooring in the West during the mid-1990s; products made from bamboo laminate, including flooring, cabinetry, furniture, and even decorations, are currently surging in popularity, transitioning from the boutique market to mainstream providers such as Home Depot. The bamboo goods industry (which also includes small goods, fabric, etc.) is expected to be worth $25 billion by 2012. The quality of bamboo laminate varies among manufacturers and varies according to the maturity of the plant from which it was harvested (six years being considered the optimum); the sturdiest products fulfill their claims of being up to three times harder than oak hardwood while others may be softer than standard hardwood.
Bamboo intended for use in construction should be treated to resist insects and rot. The most common solution for this purpose is a mixture of borax and boric acid. Another process involves boiling cut bamboo to remove the starches that attract insects.
Bamboo has been used as reinforcement for concrete in those areas where it is plentiful, though dispute exists over its effectiveness in the various studies done on the subject. Bamboo does have the necessary strength to fulfil this function, but untreated bamboo will swell with water absorbed from the concrete, causing it to crack. Several procedures must be followed to overcome this shortcoming.
Several institutes, businesses, and universities are researching the use of bamboo as an ecological construction material. In the United States and France, it is possible to get houses made entirely of bamboo, which are earthquake- and cyclone-resistant and internationally certified. In Bali, Indonesia, an international K-12 school, the Green School, is constructed entirely of bamboo, for its beauty and advantages as a sustainable resource. Three ISO standards are given for bamboo as a construction material.
In parts of India, bamboo is used for drying clothes indoors, both as a rod high up near the ceiling to hang clothes on, and as a stick wielded with acquired expert skill to hoist, spread, and to take down the clothes when dry. It is also commonly used to make ladders, which apart from their normal function, are also used for carrying bodies in funerals. In Maharashtra, the bamboo groves and forests are called "Veluvana", the name "velu" for bamboo is most likely from Sanskrit, while "vana" means forest.
Furthermore, bamboo is also used to create flagpoles for saffron-coloured, Hindu religious flags, which can be seen fluttering across India, especially in Bihar and Uttar Pradesh, as well as in Guyana and Suriname in South America.
Bamboo was used for the structural members of the India pavilion at Expo 2010 in Shanghai. The pavilion is the world’s largest bamboo dome, about in diameter, with bamboo beams/members overlaid with a ferro-concrete slab, waterproofing, copper plate, solar PV panels, a small windmill, and live plants. A total of of bamboo was used. The dome is supported on 18-m-long steel piles and a series of steel ring beams. The bamboo was treated with borax and boric acid as a fire retardant and insecticide and bent in the required shape. The bamboo sections were joined with reinforcement bars and concrete mortar to achieve the necessary lengths.
Textiles.
Since the fibers of bamboo are very short (less than ), they are not usually transformed into yarn by a natural process. The usual process by which textiles labeled as being made of bamboo are produced uses only rayon made from the fibers with heavy employment of chemicals. To accomplish this, the fibers are broken down with chemicals and extruded through mechanical spinnerets; the chemicals include lye, carbon disulfide, and strong acids. Retailers have sold both end products as "bamboo fabric" to cash in on bamboo's current ecofriendly cachet; however, the Canadian Competition Bureau and the US Federal Trade Commission, as of mid-2009, are cracking down on the practice of labeling bamboo rayon as natural bamboo fabric. Under the guidelines of both agencies, these products must be labeled as rayon with the optional qualifier "from bamboo".
As a writing surface.
Bamboo was in widespread use in early China as a medium for written documents. The earliest surviving examples of such documents, written in ink on string-bound bundles of bamboo strips (or "slips"), date from the fifth century BC during the Warring States period. However, references in earlier texts surviving on other media make it clear that some precursor of these Warring States period bamboo slips was in use as early as the late Shang period (from about 1250 BC).
Bamboo or wooden strips were the standard writing material during the Han dynasty, and excavated examples have been found in abundance. Subsequently, paper began to displace bamboo and wooden strips from mainstream uses, and by the fourth century AD, bamboo slips had been largely abandoned as a medium for writing in China.
Several paper industries are surviving on bamboo forests. Ballarpur (Chandrapur, Maharstra) paper mills use bamboo for paper production.
Bamboo fiber has been used to make paper in China since early times. A high-quality, handmade paper is still produced in small quantities. Coarse bamboo paper is still used to make spirit money in many Chinese communities.
Bamboo pulps are mainly produced in China, Myanmar, Thailand, and India, and are used in printing and writing papers. The most common bamboo species used for paper are "Dendrocalamus asper" and "Bambusa blumeana". It is also possible to make dissolving pulp from bamboo. The average fiber length is similar to hardwoods, but the properties of bamboo pulp are closer to softwood pulps due to it having a very broad fiber length distribution. With the help of molecular tools, it is now possible to distinguish the superior fiber-yielding species/varieties even at juvenile stages of their growth, which can help in unadulterated merchandise production.
Weapons.
Bamboo has often been used to construct weapons and is still incorporated in several Asian martial arts.
Other uses.
Bamboo has traditionally been used to make a wide range of everyday utensils, particularly in Japan, where archaeological excavations have uncovered bamboo baskets dating to the Late Jomon period (2000–1000 BC).
Bamboo has a long history of use in Asian furniture. Chinese bamboo furniture is a distinct style based on a millennia-long tradition.
Several manufacturers offer bamboo bicycles, surfboards, snowboards, and skateboards.
Due to its flexibility, bamboo is also used to make fishing rods. The split cane rod is especially prized for fly fishing.
Bamboo has been traditionally used in Malaysia as a firecracker called a "meriam buluh" or bamboo cannon. Four-foot-long sections of bamboo are cut, and a mixture of water and calcium carbide are introduced. The resulting acetylene gas is ignited with a stick, producing a loud bang.
Bamboo can be used in water desalination. A bamboo filter is used to remove the salt from seawater.
Food is cooked in East Timor in bamboo in fire. This is called "tukir".
Many minority groups in remote areas that have water access in Asia use bamboo that is 3–5 years old to make rafts. They use 8 to 12 poles, 6–7 m (20 to 24 ft) long, laid together side by side to a width of about 1 m (3.3 ft). Once the poles are lined up together, they cut a hole crosswise through the poles at each end and use a small bamboo pole pushed through that hole like a screw to hold all the long bamboo poles together. Floating houses use whole bamboo stalks tied together in a big bunch to support the house floating in the water. Bamboo is also used to make eating utensils such as chopsticks, trays, and tea scoops.
The Song Dynasty (960–1279 AD) Chinese scientist and polymath Shen Kuo (1031–1095) used the evidence of underground petrified bamboo found in the dry northern climate of Yan'an, Shanbei region, Shaanxi province to support his geological theory of gradual climate change.
Symbolism and culture.
Bamboo's long life makes it a Chinese symbol of uprightness, while in India it is a symbol of friendship. The rarity of its blossoming has led to the flowers' being regarded as a sign of impending famine. This may be due to rats feeding upon the profusion of flowers, then multiplying and destroying a large part of the local food supply. The most recent flowering began in May 2006 (see Mautam). Bamboo is said to bloom in this manner only about every 50 years (see 28–60 year examples in FAO: 'gregarious' species table).
In Chinese culture, the bamboo, plum blossom, orchid, and chrysanthemum (often known as "méi lán zhú jú" 梅兰竹菊) are collectively referred to as the Four Gentlemen. These four plants also represent the four seasons and, in Confucian ideology, four aspects of the "junzi" ("prince" or "noble one"). The pine ("sōng" 松), the bamboo ("zhú" 竹), and the plum blossom ("méi" 梅) are also admired for their perseverance under harsh conditions, and are together known as the "Three Friends of Winter" (岁寒三友 "suìhán sānyǒu") in Chinese culture. The "Three Friends of Winter" is traditionally used as a system of ranking in Japan, for example in sushi sets or accommodations at a traditional "ryokan". Pine ("matsu" 松) is of the first rank, bamboo ("také" 竹) is of second rank, and plum ("ume" 梅) is of the third.
The Bozo ethnic group of West Africa take their name from the Bambara phrase "bo-so", which means "bamboo house". Bamboo is also the national plant of St. Lucia.
Bamboo, noble and useful.
Bamboo, one of the "Four Gentlemen" (bamboo, orchid, plum blossom and chrysanthemum), plays such an important role in traditional Chinese culture that it is even regarded as a behavior model of the gentleman. As bamboo has features such as uprightness, tenacity, and hollow heart, people endow bamboo with integrity, elegance, and plainness, though it is not physically strong. Countless poems praising bamboo written by ancient Chinese poets are actually metaphorically about people who exhibited these characteristics. According to laws, an ancient poet, Bai Juyi (772–846), thought that to be a gentleman, a man does not need to be physically strong, but he must be mentally strong, upright, and perseverant. Just as a bamboo is hollow-hearted, he should open his heart to accept anything of benefit and never have arrogance or prejudice.
Bamboo is not only a symbol of a gentleman, but also plays an important role in Buddhism, which was introduced into China in the first century. As canons of Buddhism forbids cruelty to animals, flesh and egg were not allowed in the diet. The tender bamboo shoot ("sǔn"筍 in Chinese) thus became a nutritious alternative. Preparation methods developed over thousands of years have come to be incorporated into Asian cuisines, especially for monks. A Buddhist monk, Zan Ning, wrote a manual of the bamboo shoot called ""Sǔn Pǔ"筍譜" offering descriptions and recipes for many kinds of bamboo shoots.
Bamboo shoot has always been a traditional dish on the Chinese dinner table, especially in southern China.
In ancient times, those who could afford a big house with a yard would plant bamboo in their garden.
In Japan, a bamboo forest sometimes surrounds a Shinto shrine as part of a sacred barrier against evil. Many Buddhist temples also have bamboo groves.
Bamboo plays an important part of the culture of Vietnam. Bamboo symbolizes the spirit of Vovinam (a Vietnamese martial arts): "cương nhu phối triển" (coordination between hard and soft (martial arts)). Bamboo also symbolizes the Vietnamese hometown and Vietnamese soul: the gentlemanlike, straightforwardness, hard working, optimism, unity, and adaptability. A Vietnamese proverb says, "When the bamboo is old, the bamboo sprouts appear", the meaning being Vietnam will never be annihilated; if the previous generation dies, the children take their place. Therefore, the Vietnam nation and Vietnamese value will be maintained and developed eternally. Traditional Vietnamese villages are surrounded by thick bamboo hedges ("lũy tre").
In mythology.
Several Asian cultures, including that of the Andaman Islands, believe humanity emerged from a bamboo stem.
In Philippine mythology, one of the more famous creation accounts tells of the first man, "Malakás" ("Strong"), and the first woman, "Maganda" ("Beautiful"), each emerged from one half of a split bamboo stem on an island formed after the battle between Sky and Ocean. In Malaysia, a similar story includes a man who dreams of a beautiful woman while sleeping under a bamboo plant; he wakes up and breaks the bamboo stem, discovering the woman inside. The Japanese folktale "Tale of the Bamboo Cutter" ("Taketori Monogatari") tells of a princess from the Moon emerging from a shining bamboo section. Hawaiian bamboo "('ohe)" is a "kinolau" or body form of the Polynesian creator god Kāne.
A bamboo cane is also the weapon of Vietnamese legendary hero, Saint Giong, who had grown up immediately and magically since the age of three because of his wish to liberate his land from Ân invaders. An ancient Vietnamese legend (The Hundred-knot Bamboo Tree) tells of a poor, young farmer who fell in love with his landlord's beautiful daughter. The farmer asked the landlord for his daughter's hand in marriage, but the proud landlord would not allow her to be bound in marriage to a poor farmer. The landlord decided to foil the marriage with an impossible deal; the farmer must bring him a "bamboo tree of 100 nodes". But Gautama Buddha ("Bụt") appeared to the farmer and told him that such a tree could be made from 100 nodes from several different trees. "Bụt" gave to him four magic words to attach the many nodes of bamboo: "Khắc nhập, khắc xuất", which means "joined together immediately, fell apart immediately". The triumphant farmer returned to the landlord and demanded his daughter. Curious to see such a long bamboo, the landlord was magically joined to the bamboo when he touched it, as the young farmer said the first two magic words. The story ends with the happy marriage of the farmer and the landlord's daughter after the landlord agreed to the marriage and asked to be separated from the bamboo.
In a Chinese legend, the Emperor Yao gave two of his daughters to the future Emperor Shun as a test for his potential to rule. Shun passed the test of being able to run his household with the two emperor's daughters as wives, and thus Yao made Shun his successor, bypassing his unworthy son. Later, Shun drowned in the Xiang River. The tears his two bereaved wives let fall upon the bamboos growing there explains the origin of spotted bamboo. The two women later became goddesses.

</doc>
<doc id="39031" url="https://en.wikipedia.org/wiki?curid=39031" title="R. J. Mitchell">
R. J. Mitchell

Reginald Joseph Mitchell CBE, FRAeS, (20 May 1895 – 11 June 1937) was an English aeronautical engineer, who worked for Supermarine Aviation. Between 1920 and 1936 he designed many aircraft. He is best remembered for his racing seaplanes, which culminated in the Supermarine S.6B, and an iconic Second World War fighter, the Supermarine Spitfire.
Early life.
R.J. Mitchell was born at 115 Congleton Road, Butt Lane, Kidsgrove, Staffordshire, England. After leaving Hanley High School, a co-educational grammar school in Stoke-on-Trent, at the age of 16, he gained an apprenticeship at Kerr Stuart & Co. of Fenton, a locomotive engineering works. At the end of his apprenticeship he worked in the drawing office at Kerr Stuart and studied engineering and mathematics at night school.
Early career.
In 1917 he joined the Supermarine Aviation Works at Southampton. Advancing quickly within the company, Mitchell was appointed Chief Designer in 1919. He was made Chief Engineer in 1920 and Technical Director in 1927. He was so highly regarded that when Vickers took over Supermarine in 1928, one of the conditions was that Mitchell stay as a designer for the next five years.
Between 1920 and 1936, Mitchell designed 24 aircraft. As Supermarine was primarily a seaplane manufacturer, this included several flying boats such as the Supermarine Sea Eagle, the Supermarine Sea King, the Supermarine Walrus, and Supermarine Stranraer, and racing seaplanes. Mitchell also designed light aircraft, fighters, and bombers.
He was first noted in this period for his work on a series of racing seaplanes, built by Supermarine to compete in the Schneider Trophy competition. The Supermarine S.4 was entered in 1925, but crashed before the race. Two Supermarine S.5 aircraft were entered in 1927, and finished first and second. The Supermarine S.6 won in 1929. The final entry in the series, the Supermarine S.6B, marked the culmination of Mitchell's quest to "perfect the design of the racing seaplane". The S.6B won the Trophy in 1931 and broke the world air speed record 17 days later.
Mitchell was awarded the CBE in 1932 for his contribution to high-speed flight.
Supermarine Spitfire fighter.
The technical skill that Mitchell used in the design of the Spitfire was developed in the evolution of the Schneider Trophy seaplanes. The significance of the many earlier planes is often overlooked when people refer to Mitchell, as is the fact that he was very concerned about developments in Germany and feared that British defence needed to be strengthened, especially in the air.
In 1931 the Air Ministry issued specification F7/30 for a fighter aircraft to replace the Gloster Gauntlet. Mitchell's proposed design, the Type 224 was one of three designs for which the Air Ministry ordered prototypes.
The Type 224 first flew on 19 February 1934, but was eventually rejected by the RAF for unsatisfactory performance. While the 224 was being built, Mitchell was authorised by Supermarine in 1933 to proceed with a new design, the Type 300, an all-metal monoplane that would become the Supermarine Spitfire. This was originally a private venture by Supermarine, but the RAF quickly became interested and the Air Ministry financed a prototype.
Many of the technical advances in the Spitfire had been made by others: the thin elliptical wings were designed by Canadian aerodynamicist Beverley Shenstone, and shared some similarities with the Heinkel He 70 Blitz; the under-wing radiators had been designed by the RAE, while monocoque construction had been first developed in the United States. Mitchell's genius was bringing it all together with his experience of high speed flight and the Type 224.
The first prototype Spitfire, serial "K5054", flew for the first time on 5 March 1936 at Eastleigh, Hampshire. In later tests, it reached 349 mph; consequently, before the prototype had completed its official trials, the RAF ordered 310 production Spitfires. Mitchell is reported to have said that "Spitfire was just the sort of bloody silly name they would choose."
Later years.
In August 1933, Mitchell underwent a colostomy to treat rectal cancer. Despite this, he continued to work, not only on the Spitfire, but also on a four-engined bomber, the Type 317. Unusually for an aircraft designer in those days, he took flying lessons and got his pilot's licence in July 1934.
In 1936 cancer was diagnosed again, and subsequently, in early 1937, Mitchell gave up work, although he was often seen watching the Spitfire being tested. Mitchell went to the American Foundation in Vienna for a month, but died on 11 June 1937 at age 42. His ashes were interred at South Stoneham Cemetery, Hampshire, four days later.
Legacy.
Mitchell was succeeded as Chief Designer at Supermarine by Joseph Smith, who was responsible for the further development of the Spitfire. Nevertheless, Mitchell's design was so sound that the Spitfire was continually improved throughout World War II. Over 22,000 Spitfires and derivatives were built.
Mitchell's career was dramatized in the biographical film, "The First of the Few" (1942). He was portrayed by Leslie Howard, who also produced and directed the film, released in the United States as "Spitfire" (1943).
The school Mitchell attended, Hanley High School, was renamed Mitchell High School in his honour in 1989. Also, the primary school just by his birthplace (in Butt Lane, Kidsgrove, Stoke-on-Trent; built in 1909) was dedicated to him as Reginald Mitchell County Primary School.
While working on the Spitfire at Woolston and Eastleigh, Mitchell lived with his wife and son in Portswood, Southampton, at a home he had built to his own design, Hazeldene, at 2 Russell Place. An English Heritage blue plaque was dedicated there on September 8, 2005, a week before the 65th anniversary of the Battle of Britain. Earlier that year a blue plaque from the Newcastle Civic Society was dedicated at Mitchell's first home in Butt Lane.
Gordon Mitchell told his father's story in a 1986 book, revised in 1997 and updated in 2002 as "R. J. Mitchell: Schooldays to Spitfire".
On March 5, 2004, Mitchell unveiled a 3/4-scale representation of the prototype Spitfire K5054 at the entrance to Southampton Airport (formerly known as Eastleigh Aerodrome), Southampton, on the 68th anniversary of its first flight. The sculpture was created by Alan Manning.
American philanthropist Sidney Frank unveiled a statue of R. J. Mitchell at London's Science Museum on September 15, 2005. Made from hundreds of small pieces of Welsh slate, the statue shows Mitchell standing at his drawing board. The slate drawing board's surface is incised with the image of the June 1936 drawing of the prototype Spitfire K5054. The stone sculpture was created by Stephen Kettle and given to the museum by the Sidney E. Frank Foundation. Frank also commissioned a web site commemorating Mitchell's life and achievements.
In 2006 Gordon Mitchell received a letter from Prince Philip, Duke of Edinburgh, supporting his efforts to rename Southampton Airport in honor of his father. A tireless proponent of his father's legacy, Mitchell died in 2009, aged 88.
The Mitchell Arts Centre in Stoke-on-Trent, opened in 1957, was constructed in his honour.
In 2012, a road at the Advanced Manufacturing Park in Rotherham, England, was named Mitchell Way in honour of the Spitfire designer.

</doc>
<doc id="39032" url="https://en.wikipedia.org/wiki?curid=39032" title="Rolls-Royce Merlin">
Rolls-Royce Merlin

The Rolls-Royce Merlin is a British liquid-cooled V-12 piston aero engine of 27-litres (1,650 cu in) capacity. Rolls-Royce designed the engine and first ran it in 1933 as a private venture. Initially known as the PV-12, it was later called "Merlin" following the company convention of naming its piston aero engines after birds of prey.
After several modifications, the first production variants of the PV-12 were completed in 1936. The first operational aircraft to enter service using the Merlin were the Fairey Battle, Hawker Hurricane and Supermarine Spitfire. More Merlins were made for the four-engined Avro Lancaster heavy bomber than for any other aircraft; however, the engine is most closely associated with the Spitfire, starting with the Spitfire's maiden flight in 1936. A series of rapidly applied developments, brought about by wartime needs, markedly improved the engine's performance and durability.
One of the most successful aircraft engines of the World War II era, some fifty marks of Merlin were built by Rolls-Royce in Derby, Crewe and Glasgow, as well as by Ford of Britain at their Trafford Park factory, near Manchester. Post-war, the Merlin was largely superseded by the Rolls-Royce Griffon for military use, with most Merlin variants being designed and built for airliners and military transport aircraft. Production ceased in 1950 after 160,000 engines had been delivered. In addition, the Packard V-1650 was a version of the Merlin built in the United States, and was the principal engine used in the North American P-51 Mustang.
Merlin engines remain in Royal Air Force service today with the Battle of Britain Memorial Flight, and power many restored aircraft in private ownership worldwide.
Design and development.
Origin.
In the early 1930s, Rolls-Royce started planning its future aero-engine development programme and realised there was a need for an engine larger than their 21-litre (1,296 cu in) Kestrel which was being used with great success in a number of 1930s aircraft. Consequently, work was started on a new -class design known as the PV-12, with PV standing for "Private Venture, 12-cylinder", as the company received no government funding for work on the project. The PV-12 was first run on 15 October 1933 and first flew in a Hawker Hart biplane (serial number "K3036") on 21 February 1935. The engine was originally designed to use the evaporative cooling system then in vogue. This proved unreliable and when supplies of ethylene glycol from the U.S. became available, the engine was adapted to use a conventional liquid-cooling system. The Hart was subsequently delivered to Rolls-Royce where, as a Merlin testbed, it completed over 100 hours of flying with the Merlin C and E engines.
In 1935, the Air Ministry issued a specification, F10/35, for new fighter aircraft with a minimum airspeed of . Fortunately, two designs had been developed: the Supermarine Spitfire and the Hawker Hurricane; the latter designed in response to another specification, F36/34. Both were designed around the PV-12 instead of the Kestrel, and were the only contemporary British fighters to have been so developed. Production contracts for both aircraft were placed in 1936, and development of the PV-12 was given top priority as well as government funding. Following the company convention of naming its piston aero engines after birds of prey, Rolls-Royce named the engine the "Merlin" after a small, Northern Hemisphere falcon ("Falco columbarius").
Two more Rolls-Royce engines developed just prior to the war were added to the company's range. The Rolls-Royce Peregrine was an updated, supercharged development of their V-12 Kestrel design, while the 42-litre (2,560 cu in) Rolls-Royce Vulture used four Kestrel-sized cylinder blocks fitted to a single crankcase and driving a common crankshaft, forming an X-24 layout. This was to be used in larger aircraft such as the Avro Manchester.
Although the Peregrine appeared to be a satisfactory design, it was never allowed to mature since Rolls-Royce's priority was refining the Merlin. As a result, the Peregrine saw use in only two aircraft: the Westland Whirlwind fighter and one of the Gloster F.9/37 prototypes. The Vulture was fitted to the Avro Manchester bomber, but proved unreliable in service and the planned fighter using it – the Hawker Tornado – was cancelled as a result. With the Merlin itself soon pushing into the range, the Peregrine and Vulture were both cancelled in 1943, and by mid-1943 the Merlin was supplemented in service by the larger Griffon. The Griffon incorporated several design improvements and ultimately superseded the Merlin.
Development.
Initially the new engine was plagued with problems, such as failure of the accessory gear trains and coolant jackets, and several different construction methods were tried before the basic design of the Merlin was set. Early production Merlins were also unreliable: Common problems were cylinder head cracking, coolant leaks, and excessive wear to the camshafts and crankshaft main bearings.
Early engines.
The prototype and developmental engine types were the:
Production engines.
The Merlin II and III series were the first main production versions of the engine. The Merlin III was the first version to incorporate a "universal" propeller shaft, allowing either de Havilland or Rotol manufactured propellers to be used.
The first major version to incorporate changes brought about through experience in operational service was the XX, which was designed to run on 100 octane fuel. This fuel allowed higher manifold pressures, which were achieved by increasing the boost from the centrifugal supercharger. The Merlin XX also utilised the two-speed superchargers designed by Rolls-Royce, resulting in increased power at higher altitudes than previous versions. Another improvement, introduced with the Merlin X, was the use of a 70%–30% water-glycol coolant mix rather than the 100% glycol of the earlier versions. This substantially improved engine life and reliability, removed the fire hazard of the flammable ethylene glycol, and reduced the oil leaks that had been a problem with the early Merlin I, II and III series.
The process of improvement continued, with later versions running on higher octane ratings, delivering more power. Fundamental design changes were also made to all key components, again increasing the engine's life and reliability. By the end of the war the "little" engine was delivering over 1,600 horsepower (1,200 kW) in common versions, and as much as 2,030 horsepower (1,540 kW) in the Merlin 130/131 versions specifically designed for the de Havilland Hornet. Ultimately, during tests conducted by Rolls-Royce at Derby, an RM.17.SM achieved 2,640 horsepower (1,969 kW) at 36 lb boost (103"Hg) on 150 octane fuel with water injection. With the end of the war, work on improving Merlin power output was halted and the development effort was concentrated on civil derivatives of the Merlin.
Basic component overview (Merlin 61).
"From Jane's":
Technical improvements.
Most of the Merlin's technical improvements resulted from more efficient superchargers, designed by Stanley Hooker, and the introduction of aviation spirits with increased octane ratings. Numerous detail changes were made internally and externally to the engine to withstand increased power ratings and to incorporate advances in engineering practices.
Ejector exhausts.
The Merlin consumed an enormous volume of air at full power (equivalent to the volume of a single-decker bus "per" minute), and with the exhaust gases exiting at 1,300 mph (2,100 km/h) it was realised that useful thrust could be gained simply by angling the gases backwards instead of venting sideways.
During tests, 70 pounds-force (310 N; 32 kgf) thrust at , or roughly 70 horsepower (52 kW) was obtained which increased the level maximum speed of the Spitfire by to . The first versions of the ejector exhausts featured round outlets, while subsequent versions of the system used "fishtail" style outlets which marginally increased thrust and reduced exhaust glare for night flying.
In September 1937 the Spitfire prototype, "K5054," was fitted with ejector type exhausts. Later marks of the Spitfire used a variation of this exhaust system fitted with forward-facing intake ducts to distribute hot air out to the wing-mounted guns to prevent freezing and stoppages at high altitudes, replacing an earlier system that used heated air from the engine coolant radiator. The latter system had become ineffective due to improvements to the Merlin itself which allowed higher operating altitudes where air temperatures are lower. Ejector exhausts were also fitted to other Merlin-powered aircraft.
Supercharger.
Central to the success of the Merlin was the supercharger. A.C. Lovesey, an engineer who was a key figure in the design of the Merlin, delivered a lecture on the development of the Merlin in 1946; in this extract he explained the importance of the supercharger:
As the Merlin evolved so too did the supercharger; the latter fitting into three broad categories:
The Merlin supercharger was originally designed to allow the engine to generate maximum power at an altitude of about . In 1938 Stanley Hooker, an Oxford graduate in applied mathematics, explained "... I soon became very familiar with the construction of the Merlin supercharger and carburettor ... Since the supercharger was at the rear of the engine it had come in for pretty severe design treatment, and the air intake duct to the impeller looked very squashed ..." Tests conducted by Hooker showed the original intake design was inefficient, limiting the performance of the supercharger. Hooker subsequently designed a new air intake duct with improved flow characteristics which increased maximum power at a higher altitude of over ; and also improved the design of both the impeller, and the diffuser which controlled the airflow to it. These modifications led to the development of the single-stage Merlin XX and 45 series.
A significant advance in supercharger design was the incorporation in 1938 of a two-speed drive (designed by the French company Farman) to the impeller of the Merlin X. The later Merlin XX incorporated the two-speed drive as well as several improvements that enabled the production rate of Merlins to be increased. The low-ratio gear, which operated from takeoff to an altitude of , drove the impeller at 21,597 rpm and developed 1,240 horsepower (925 kW) at that height; while the high gear's (25,148 rpm) power rating was 1,175 horsepower (876 kW) at . These figures were achieved at 2,850 rpm engine speed using +9 pounds per square inch (1.66 atm)(48") boost.
In 1940, after receiving a request in March of that year from the Ministry of Aircraft Production for a high-rated () Merlin for use as an alternative engine to the turbocharged Hercules VIII used in the prototype high-altitude Vickers Wellington V bomber, Rolls-Royce started experiments on the design of a two-stage supercharger and an engine fitted with this was bench-tested in April 1941, eventually becoming the Merlin 60. The basic design used a modified Vulture supercharger for the first stage while a Merlin 46 supercharger was used for the second. A liquid-cooled intercooler on top of the supercharger casing was used to prevent the compressed air/fuel mixture from becoming too hot. Also considered was an exhaust-driven turbocharger but, although a lower fuel consumption was an advantage the added weight and the need to add extra ducting for the exhaust flow and waste-gates, meant that this option was rejected in favour of the two-stage supercharger. Fitted with the two-stage two-speed supercharger, the Merlin 60 series gained 300 horsepower (224 kW) at over the Merlin 45 series, at which altitude a Spitfire IX was nearly faster than a Spitfire V.
The two-stage Merlin family was extended in 1943 with the Merlin 66 which had its supercharger geared for increased power ratings at low altitudes, and the Merlin 70 series that were designed to deliver increased power at high altitudes.
While the design of the two-stage supercharger forged ahead, Rolls-Royce also continued to develop the single-stage supercharger, resulting in 1942 in the development of a smaller "cropped" impeller for the Merlin 45M and 55M; both of these engines developed greater power at low altitudes. In squadron service the LF.V variant of the Spitfire fitted with these engines became known as the "clipped, clapped, and cropped Spitty" to indicate the shortened wingspan, the less-than-perfect condition of the used airframes, and the cropped supercharger impeller.
Carburettor developments.
The use of carburettors was calculated to give a higher specific power output, due to the lower temperature, hence greater density, of the fuel/air mixture compared to injected systems. However, the Merlin's float controlled carburettor meant that both Spitfires and Hurricanes were unable to pitch nose down into a steep dive. The contemporary Bf 109E, which had direct fuel injection, could "bunt" into a high-power dive to escape attack, leaving the pursuing aircraft behind because its fuel had been forced out of the carburettor's float chamber by the effects of negative "g"-force ("g"). RAF fighter pilots soon learned to "half-roll" their aircraft before diving to pursue their opponents. "Miss Shilling's orifice", a restrictor in the fuel supply line to restrict flow together with a diaphragm fitted in the float chamber to contain fuel under negative G, went some way towards curing the fuel starvation in a dive; however, at less than maximum power a fuel-rich mixture still resulted. Another improvement was made by moving the fuel outlet from the bottom of the S.U. carburettor to exactly halfway up the side, which allowed the fuel to flow equally well under negative or positive g.
Further improvements were introduced throughout the Merlin range: 1943 saw the introduction of a Bendix-Stromberg pressure carburettor that injected fuel at 5 pounds per square inch (34 kPa; 0.34 bar) through a nozzle directly into the supercharger, and was fitted to Merlin 66, 70, 76, 77 and 85 variants. The final development, which was fitted to the 100-series Merlins, was an S.U. injection carburettor that injected fuel into the supercharger using a fuel pump driven as a function of crankshaft speed and engine pressures.
Improved fuels.
At the start of the war the Merlin I, II and III ran on the then standard 87 octane aviation spirit and could generate just over 1,000 horsepower (750 kW) from its 27-litre (1,650-cu in) displacement: the maximum boost pressure at which the engine could be run using 87 octane fuel was +6 pounds per square inch (141 kPa; 1.44 atm). However, as early as 1938, at the 16th Paris Air Show, Rolls-Royce displayed two versions of the Merlin rated to use 100 octane fuel. The Merlin R.M.2M was capable of 1,265 horsepower (943 kW) at 7,870 feet (2,400 m), 1,285 horsepower (958 kW) at 9,180 feet (2,800 m) and 1,320 horsepower (984 kW) on take-off; while a Merlin X with a two-speed supercharger in high gear generated 1,150 horsepower (857 kW) at 15,400 feet (4,700 m) and 1,160 horsepower (865 kW) at 16,730 feet (5,100 m).
From late 1939, 100 octane fuel became available from the U.S., West Indies, Persia, and, in smaller quantities, domestically, consequently, "...in the first half of 1940 the RAF transferred all Hurricane and Spitfire squadrons to 100 octane fuel." Small modifications were made to Merlin II and III series engines, allowing an increased (emergency) boost pressure of +12 pounds per square inch (183 kPa; 1.85 atm). At this power setting these engines were able to produce 1,310 horsepower (977 kW) at while running at 3,000 revolutions per minute. Increased boost could be used indefinitely as there was no mechanical time limit mechanism but pilots were advised not to use increased boost for more than a maximum of five minutes and it was considered a "definite overload condition on the engine"; if the pilot resorted to emergency boost he had to report this on landing, when it was noted in the engine log book, while the engineering officer was required to examine the engine and reset the throttle gate. Later versions of the Merlin ran only on 100 octane fuel and the five-minute combat limitation was raised to +18 pounds per square inch (224 kPa; 2.3 atm).
In late 1943 trials were run of a new "100/150" grade (150 octane) fuel, recognised by its bright-green colour and "awful smell". Initial tests were conducted using of tetraethyllead (T.E.L.) for every one imperial gallon of 100 octane fuel (or 1.43 cc/L or 0.18 U.S. fl oz/U.S. gal), but this mixture resulted in a build-up of lead in the combustion chambers, causing excessive fouling of the spark plugs. Better results were achieved by adding 2.5% mono methyl aniline (M.M.A.) to 100 octane fuel. The new fuel allowed the five-minute boost rating of the Merlin 66 to be raised to +25 pounds per square inch (272 kPa; 2.7 atm). With this boost rating the Merlin 66 generated 2,000 hp (1,491 Kw) at sea-level and 1,860 hp (1,387 Kw) at .
Starting in March 1944, the Merlin 66-powered Spitfire IXs of two Air Defence of Great Britain (ADGB) squadrons were cleared to use the new fuel for operational trials, and it was put to good use in the summer of 1944 when it enabled Spitfire L.F. Mk. IXs to intercept V-1 flying bombs coming in at low altitudes. 100/150 grade fuel was also used by Mosquito night fighters of the ADGB to intercept V-1s. In early February 1945, Spitfires of the 2 TAF also began using 100/150 grade fuel.
Production.
Production of the Rolls-Royce Merlin was driven by the forethought and determination of Ernest Hives, who at times was enraged by the apparent complacency and lack of urgency encountered in his frequent correspondence with Air Ministry and local authority officials. Hives was an advocate of shadow factories, and, sensing the imminent outbreak of war, pressed ahead with plans to produce the Merlin in sufficient numbers for the rapidly expanding Royal Air Force. Despite the importance of uninterrupted production, several factories were affected by industrial action. By the end of its production run in 1950, 168,176 Merlin engines had been built; over 112,000 in Britain and more than 55,000 under licence in the U.S.
Derby.
The existing Rolls-Royce facilities at Osmaston, Derby were not suitable for large-scale engine production although the floor space had been increased by some 25% between 1935 and 1939; nevertheless, Hives planned to build the first two- or three hundred engines there until engineering teething troubles had been resolved. To fund this expansion, the Air Ministry had provided a total of ₤1,927,000 by December 1939. Having a workforce that consisted mainly of design engineers and highly skilled men, the Derby factory carried out the majority of development work on the Merlin, with flight testing carried out at nearby RAF Hucknall.
Total Merlin production at Derby was 32,377.
The original factory closed in March 2008, but Rolls-Royce plc still maintains a large presence in Derby.
Crewe.
To meet the increasing demand for Merlin engines, Rolls-Royce started building work on a new factory at Crewe in May 1938, with engines leaving the factory in 1939. The Crewe factory had convenient road and rail links to their existing facilities at Derby. Production at Crewe was originally planned to use unskilled labour and sub-contractors with which Hives felt there would be no particular difficulty, but the number of required sub-contracted parts such as crankshafts, camshafts and cylinder liners eventually fell short and the factory was expanded to manufacture these parts "in house".
Initially the local authority promised to build 1,000 new houses to accommodate the workforce by the end of 1938, but by February 1939 it had only awarded a contract for 100. Hives was incensed by this complacency and threatened to move the whole operation, but timely intervention by the Air Ministry improved the situation. In 1940 a strike took place when women replaced men on capstan lathes, the workers' union insisting this was a skilled labour job; however, the men returned to work after 10 days.
Total Merlin production at Crewe was 26,065.
The factory was used postwar for the production of Bentley motor cars, and in 1998 Volkswagen AG bought both the marque and the factory. Today it is known as Bentley Crewe.
Glasgow.
Hives further recommended that a factory be built near Glasgow to take advantage of the abundant local work force and the supply of steel and forgings from Scottish manufacturers. In September 1939, the Air Ministry allocated ₤4,500,000 for a new Shadow factory. This government-funded and -operated factory was built at Hillington starting in June 1939 with workers moving into the premises in October, one month after the outbreak of war. The factory was fully occupied by September 1940. A housing crisis also occurred at Glasgow where Hives again asked the Air Ministry to step in.
With 16,000 employees, the Glasgow factory was one of the largest industrial operations in Scotland. Unlike the Derby and Crewe plants which relied significantly on external subcontractors, it produced almost all the Merlin's components itself. Engines began to leave the production line in November 1940, and by June 1941 monthly output had reached 200, increasing to more than 400 per month by March 1942. In total 23,675 engines were produced. Worker absenteeism became a problem after some months due to the physical and mental effects of wartime conditions such as the frequent occupation of air-raid shelters. It was agreed to cut the punishing working hours slightly to 82 hours a week, with one half-Sunday per month awarded as holiday. Record production is reported to have been 100 engines in one day.
Immediately after the war the site repaired and overhauled Merlin and Griffon engines, and continued to manufacture spare parts. Finally, following the production of the Rolls-Royce Avon turbojet and others, the factory was closed in 2005.
Manchester.
The Ford Motor Company was asked to produce Merlins at Trafford Park, Stretford, near Manchester, and building work on a new factory was started in May 1940 on a site. Built with two distinct sections to minimise potential bomb damage, it was completed in May 1941 and bombed in the same month. At first, the factory had difficulty in attracting suitable labour, and large numbers of women, youths and untrained men had to be taken on. Despite this, the first Merlin engine came off the production line one month later and it was building the engine at a rate of 200 per week by 1943, at which point the joint factories were producing 18,000 Merlins per year. In his autobiography "Not much of an Engineer", Sir Stanley Hooker states: "... once the great Ford factory at Manchester started production, Merlins came out like shelling peas ...".
Some 17,316 people worked at the Trafford Park plant, including 7,260 women and two resident doctors and nurses. Merlin production started to run down in August 1945, and finally ceased on 23 March 1946.
Total Merlin production at Trafford Park was 30,428.
Packard V-1650.
As the Merlin was considered to be so important to the war effort, negotiations were soon started to establish an alternative production line outside the UK. Rolls-Royce staff visited a number of North American automobile manufacturers in order to select one to build the Merlin in the U.S. or Canada. Henry Ford rescinded an initial offer to build the engine in the U.S. in July 1940, and the Packard Motor Car Company was subsequently selected to take on the $130,000,000 Merlin order. Agreement was reached in September 1940, and the first Packard-built engine, a Merlin XX designated the V-1650-1, ran in August 1941.
Total Merlin production by Packard was 55,523.
Six development engines were also made by Continental Motors, Inc.
Variants.
This is a list of representative Merlin variants, describing some of the mechanical changes made during development of the Merlin. Engines of the same power output were typically assigned different model numbers based on supercharger or propeller gear ratios, differences in cooling system or carburettors, engine block construction, or arrangement of engine controls. Power ratings quoted are usually maximum "military" power. All but the Merlin 131 and 134 engines were "right-hand tractor", "i.e." the propeller rotated clockwise when viewed from the rear. In addition to the mark numbers, Merlin engines were allocated experimental numbers by the Ministry of Supply (MoS) – e.g.: RM 8SM for the Merlin 61 and some variants – while under development; these numbers are noted where possible. Merlin engines used in Spitfires, apart from the Merlin 61, used a propeller reduction ratio of .477:1. Merlins used in bombers and other fighters used a ratio of .42:1.
"Data from Bridgman (Jane's) unless otherwise noted:"
Applications.
In chronological order, the first operational aircraft powered by the Merlin to enter service were the Fairey Battle, Hawker Hurricane, and Supermarine Spitfire. Although the engine is most closely associated with the Spitfire, the four-engined Avro Lancaster was the most numerous application, followed by the twin-engined de Havilland Mosquito.
"List from Lumsden 2003"
Postwar.
At the end of World War II, new versions of the Merlin (the 600- and 700-series) were designed and produced for use in commercial airliners such as the Avro Tudor, military transport aircraft such as the Avro York, and the Canadair North Star which performed in both roles. These engines were basically military specification with some minor changes to suit the different operating environment.
A Spanish-built version of the Messerschmitt Bf 109 G-2, the 1954 Hispano Aviación HA-1112-M1L "Buchon", was built in Hispano's factory in Seville with the Rolls-Royce Merlin 500/45 engine of  – a fitting powerplant for the last-produced version of the famous Messerschmitt fighter, as the Bf 109 V1 prototype aircraft had been powered by the Rolls-Royce Kestrel V-12 engine in 1935.
The CASA 2.111 was another Spanish-built version of a German aircraft, the Heinkel He 111, that was adapted to use the Merlin after the supply of Junkers Jumo 211F-2 engines ran out at the end of the war. A similar situation existed with the Fiat G.59 when available stocks of the Italian licence-built version of the Daimler-Benz DB 605 engine ran short.
Alternative applications.
A non-supercharged version of the Merlin using a larger proportion of steel and iron components was produced for use in tanks. This engine, the Rolls-Royce Meteor, in turn led to the smaller Rolls-Royce Meteorite. In 1943, further Meteor development was handed over to Rover, in exchange for Rover's gas turbine interests.
In 1938, Rolls-Royce started work on modifying some Merlins which were later to be used in British MTBs, MGBs, and RAF Air-Sea Rescue Launches. For these the superchargers were modified single-stage units and the engine was re-engineered for use in a marine environment. Some 70 engines were converted before priority was given to producing aero engines.
Experiments were carried out by the Irish Army involving replacing the Bedford engine of a Churchill tank with a Rolls-Royce Merlin engine salvaged from an Irish Air Corps Seafire aircraft. The experiment was not a success, although the reasons are not recorded.
Survivors.
One of the most successful of the World War II era aircraft engines, the Merlin continues to be used in many restored World War II vintage aircraft all over the world. The Royal Air Force Battle of Britain Memorial Flight is a notable current operator of the Merlin. In England the Shuttleworth Collection owns and operates a Merlin-powered Hawker Sea Hurricane IB and a Supermarine Spitfire VC – the Hurricane can be seen flying at home displays throughout the summer months, while the Spitfire is currently undergoing a major restoration.
Engines on display.
Preserved examples of the Rolls-Royce Merlin are on display at the following museums:

</doc>
<doc id="39033" url="https://en.wikipedia.org/wiki?curid=39033" title="Air Ministry">
Air Ministry

The Air Ministry was a department of the British Government with the responsibility of managing the affairs of the Royal Air Force, that existed from 1918 to 1964. It was under the political authority of the Secretary of State for Air.
Organisations before the Air Ministry.
The Air Committee.
On 13 April 1912, less than two weeks after the creation of the Royal Flying Corps (which initially consisted of both a naval and a military wing), an Air Committee was established to act as an intermediary between the Admiralty and the War Office in matters relating to aviation. The new Air Committee was composed of representatives of the two war ministries, and although it could make recommendations, it lacked executive authority. The recommendations of the Air Committee had to be ratified by the Admiralty Board and the Imperial General Staff and, in consequence, the Committee was not particularly effective. The increasing separation of army and naval aviation from 1912 to 1914 only exacerbated the Air Committee's ineffectiveness and the Committee did not meet after the outbreak of the First World War.
The Joint War Air Committee.
By 1916 the lack of co-ordination of the Army's Royal Flying Corps and the Navy's Royal Naval Air Service had led to serious problems, not only in the procurement of aircraft engines, but also in the air defence of Great Britain. It was the supply problems to which an attempt at rectification was first made. The War Committee meeting on 15 February 1916 decided immediately to establish a standing joint naval and military committee to co-ordinate both the design and the supply of materiel for the two air services. This committee was titled the Joint War Air Committee, and its chairman was Lord Derby. It was also at the meeting on 15 February that Curzon proposed the creation of an Air Ministry. As with the pre-war Air Committee, the Joint War Air Committee lacked any executive powers and therefore was not effective. After only eight sittings, Lord Derby resigned from the Committee, stating that "It appears to me quite impossible to bring the two wings closer together ... unless and until the whole system of the Air Service is changed and they are amalgamated into one service."
Membership.
The Joint War Air Committee was composed as follows:
Advisory Members were also appointed as required.
The Air Board.
The first Air Board.
The next attempt to establish effective co-ordination between the two air services was the creation of an Air Board. The first Air Board came into being on 15 May 1916 with Lord Curzon as its chairman. The inclusion of Curzon, a Cabinet Minister, and other political figures was intended to give the Air Board greater status than the Joint War Air Committee. In October 1916 the Air Board published its first report which was highly critical of the arrangements within the British air services. The report noted that although the Army authorities were ready and willing to provide information and take part in meetings, the Navy were often absent from Board meetings and frequently refused to provide information on naval aviation.
The second Air Board.
In January 1917 the Prime Minister David Lloyd George replaced the chairman Lord Curzon with Lord Cowdray. Godfrey Paine, who served in the newly created post of Fifth Sea Lord and Director of Naval Aviation, sat on the board and this high level representation from the Navy helped to improve matters. Additionally, as responsibility for the design of aircraft had been moved out of single service hands and given to the Ministry of Munitions, some of the problems of inter-service competition were avoided.
Establishment of the Air Ministry.
Despite attempts at reorganization of the Air Board, the earlier problems failed to be completely resolved. In addition, the growing number of German air raids against Great Britain led to public disquiet and increasing demands for something to be done. As a result, Lloyd George, the British Prime Minister, established a committee composed of himself and General Jan Smuts, which was tasked with investigating the problems with the British air defences and organizational difficulties which had beset the Air Board.
Towards the end of the First World War, on 17 August 1917, General Smuts presented a report to the War Council on the future of air power. Because of its potential for the 'devastation of enemy lands and the destruction of industrial and populous centres on a vast scale', he recommended a new air service be formed that would be on a level with the Army and Royal Navy. The new air service was to receive direction from a new ministry and on 29 November 1917 the Air Force Bill received Royal Assent and the Air Ministry was formed just over a month later on 2 January 1918. Lord Rothermere was appointed the first Air Minister. On 3 January, the Air Council was constituted as follows:
The Air Ministry initially met in the Hotel Cecil on the Strand. Later, in 1919, it moved to Adastral House on Kingsway. The creation of the Air Ministry resulted in the disestablishment of the Army Council's post of Director-General of Military Aeronautics.
History - 1920s.
In April 1921 David Lloyd George appointed Frederick Guest as Secretary of State for Air. During his eighteen months in office he played “a minor part in the desperate struggle to maintain the air force's institutional independence in the face of hostile attacks from the War Office and the Admiralty”. More importantly in the long term he was also responsible for the appointment of Sir Sefton Brancker to develop civil aviation.
With the fall of Lloyd George in October 1922 the Secretary of State for Air was, until January 1924 and again from November 1924 to June 1929, Sir Samuel Hoare. From May 1923 he was the first Secretary to have a seat in the Cabinet (Churchill had been in the Cabinet as Secretary for War as well as for Air). His time at the Air Ministry was marked by several important developments that were to cement the status of the Royal Air Force as a separate entity, play a part in the growth of civil aviation and to develop the awareness of the public about aviation.
An early priority for Sir Hugh Trenchard, Chief of the Air Staff 1919-1930, was to establish the officer cadet training college at Cranwell as a permanent establishment. It was Hoare's job to negotiate with the Treasury for the necessary funds. After much resistance Hoare managed to include in his estimates of 1929 a provision for permanent buildings. The foundation stone of the Royal Air Force College Cranwell was laid in 1929 and formally opened in 1934.
Trenchard had conceived the idea of a university air officer training corps, a sort of Territorial Army for the R.A.F. Hoare and particularly his well connected Parliamentary Private Secretary the academic Sir Geoffrey Butler, then created University Air Squadrons, at Cambridge University then at Oxford University in October 1925, without, however the militarism of the Officer Training Corps and in close collaboration with scientific and engineering work of the Universities.
The Air Ministry was also responsible for civil aviation. Early on Hoare set up the Civil Air Transport Subsidies Committee under the Chairmanship of Sir Hubert Hambling, to look at the system of subsidies to competing air lines. They reported in February 1923 favouring a single commercial company to run Britain's air routes. In March 1924 Imperial Airways was created from a merger of the four largest airlines.
In January 1924 a Labour government took power and Lord Thomson was made Secretary of State for Air. A supporter of airships he encouraged the Air Ministry to develop the construction of R101 at the Royal Airship Works at Cardington.
In November 1924 Hoare returned to the Air Ministry. He was interested in developing air links to Empire and Dominion countries, particularly India and South Africa. He negotiated a subsidy from the Treasury for Imperial Airways to open a service from Cairo to India. Hoare with his wife, Lady Maud, flew on the inaugural 13 day flight to Delhi leaving Croydon 26 December 1926 arriving on 8 January 1927. The air route to Cape Town, after much negotiation, was finalised in 1929, before he left office,but only commenced in 1932.
The third aspect of Hoare's time at the Air Ministry (after the R.A.F. and civil airlines) was to make public opinion sympathetic to air power and air travel. His much publicised flight to India in 1926-7 was part of this. He also realised the importance of the Schneider Trophy and was instrumental in making sure that the R.A.F was involved. Britain's winning entries in 1927, 1929 and 1931 were flown by R.A.F. pilots and the teams partially subsidised by the Air Ministry.
Activities.
Aircraft production.
The Air Ministry issued specifications for aircraft that British aircraft companies would supply prototypes to. These were then assessed, if ordered the Ministry assigned the aircraft name. (see List Of Air Ministry Specifications).
The ordering procedure used I.T.P. (Intention to Proceed) contract papers; these specified a maximum fixed price, which could (after investigation) be less. But when Lord Nuffield got the I.T.P. contract papers for a Wolseley radial aero engine, which would have required re-orientation of their offices with an army of chartered accountants, he decided to deal only with the War Office and the Admiralty, not the Air Ministry. So the aero engine project was abandoned in 1936, see Airspeed. Nevil Shute Norway wrote that the loss of such a technically advanced engine was a great loss to Britain as well as Airspeed, and blamed the over-cautious high civil servants of the Air Ministry. When he had asked Lord Nuffield to retain the engine, Nuffield said: "I tell you, Norway ... I sent that I.T.P. thing back to them, and I told them they could put it where the monkey put the nuts!" 
In later years the actual production of aircraft was the responsibility of the Ministry of Aircraft Production (1940–46), the Ministry of Supply (1946–59), the Ministry of Aviation (1959–67) and finally the Ministry of Technology (1967–70).
Weather Forecasting.
The Air Ministry was responsible for weather forecasting over the UK, from 1919 it being the government department responsible for the Meteorological Office.
As a result of the need for weather information for aviation, the Meteorological Office located many of its observation and data collection points on RAF stations.
WWII technology.
In the 1930s, the Air Ministry commissioned a scientific study of propagating electromagnetic energy which concluded that a death ray was impractical but detection of aircraft appeared feasible. Robert Watson-Watt demonstrated a working prototype and patented the device in 1935 (British Patent GB593017) The device served as the base for the Chain Home network of radars to defend Great Britain.
By April 1944, the ministry's air Intelligence branch had succeeded in its intelligence efforts regarding "the beams, the Bruneval Raid, the Gibraltar barrage, radar, Window, heavy water, and the German nightfighters" (R.V. Jones). Other WWII technology & warfare efforts included the branch's V-1 and V-2 Intelligence activities.
Disestablishment.
In 1964 the Air Ministry merged with the Admiralty and the War Office to form the Ministry of Defence.

</doc>
<doc id="39034" url="https://en.wikipedia.org/wiki?curid=39034" title="J. Robert Oppenheimer">
J. Robert Oppenheimer

Julius Robert Oppenheimer (April 22, 1904 – February 18, 1967) was an American theoretical physicist and professor of physics at the University of California, Berkeley. As the wartime head of the Los Alamos Laboratory, Oppenheimer is among those who are called the "father of the atomic bomb" for their role in the Manhattan Project, the World War II project that developed the first nuclear weapons used in the atomic bombings of Hiroshima and Nagasaki. The first atomic bomb was detonated on July 16, 1945, in the Trinity test in New Mexico; Oppenheimer remarked later that it brought to mind words from the "Bhagavad Gita": "Now I am become Death, the destroyer of worlds."
After the war Oppenheimer became chairman of the influential General Advisory Committee of the newly created United States Atomic Energy Commission, and used that position to lobby for international control of nuclear power to avert nuclear proliferation and a nuclear arms race with the Soviet Union. After provoking the ire of many politicians with his outspoken opinions during the Second Red Scare, he suffered the revocation of his security clearance in a much-publicized hearing in 1954, and was effectively stripped of his direct political influence; he continued to lecture, write and work in physics. Nine years later President John F. Kennedy awarded (and Lyndon B. Johnson presented) him with the Enrico Fermi Award as a gesture of political rehabilitation.
Oppenheimer's achievements in physics include the Born–Oppenheimer approximation for molecular wavefunctions, work on the theory of electrons and positrons, the Oppenheimer–Phillips process in nuclear fusion, and the first prediction of quantum tunneling. With his students he also made important contributions to the modern theory of neutron stars and black holes, as well as to quantum mechanics, quantum field theory, and the interactions of cosmic rays. As a teacher and promoter of science, he is remembered as a founding father of the American school of theoretical physics that gained world prominence in the 1930s. After World War II, he became director of the Institute for Advanced Study in Princeton.
Early life.
Childhood and education.
Oppenheimer was born in New York City on April 22, 1904, the son of Julius Oppenheimer, a wealthy Jewish textile importer who had immigrated to the United States from Germany in 1888, and Ella Friedman, a painter. Julius came to America with no money, no baccalaureate studies, and no knowledge of the English language. He got a job in a textile company and within a decade was an executive with the company. Ella was from Baltimore. The Oppenheimers were non-observant Ashkenazi Jews. In 1912 the family moved to an apartment on the 11th floor of 155 Riverside Drive, near West 88th Street, Manhattan, an area known for luxurious mansions and town houses. Their art collection included works by Pablo Picasso and Édouard Vuillard, and at least three original paintings by Vincent van Gogh. Robert had a younger brother, Frank, who also became a physicist.
Oppenheimer was initially schooled at Alcuin Preparatory School, and in 1911 entered the Ethical Culture Society School. This had been founded by Felix Adler to promote a form of ethical training based on the Ethical Culture movement, whose motto was "Deed before Creed". His father had been a member of the Society for many years, serving on its board of trustees from 1907 to 1915. Oppenheimer was a versatile scholar, interested in English and French literature, and particularly in mineralogy. He completed the third and fourth grades in one year, and skipped half the eighth grade. During his final year, he became interested in chemistry. He entered Harvard College a year late, at age 18, because he suffered an attack of colitis while prospecting in Joachimstal during a family summer vacation in Europe. To help him recover from the illness, his father enlisted the help of his English teacher Herbert Smith who took him to New Mexico, where Oppenheimer fell in love with horseback riding and the southwestern United States.
In addition to majoring in chemistry, he was also required by Harvard's rules to study history, literature, and philosophy or mathematics. He compensated for his late start by taking six courses each term and was admitted to the undergraduate honor society Phi Beta Kappa. In his first year he was admitted to graduate standing in physics on the basis of independent study, which meant he was not required to take the basic classes and could enroll instead in advanced ones. A course on thermodynamics taught by Percy Bridgman attracted him to experimental physics. He graduated summa cum laude in three years.
Studies in Europe.
In 1924 Oppenheimer was informed that he had been accepted into Christ's College, Cambridge. He wrote to Ernest Rutherford requesting permission to work at the Cavendish Laboratory. Bridgman provided Oppenheimer with a recommendation, which conceded that Oppenheimer's clumsiness in the laboratory made it apparent his forte was not experimental but rather theoretical physics. Rutherford was unimpressed, but Oppenheimer went to Cambridge in the hope of landing another offer. He was ultimately accepted by J. J. Thomson on condition that he complete a basic laboratory course. He developed an antagonistic relationship with his tutor, Patrick Blackett, who was only a few years his senior. While on vacation, as recalled by his friend Francis Fergusson, Oppenheimer once confessed that he had left an apple doused with noxious chemicals on Blackett's desk. While Ferguson's account is the only detailed version of this event, Oppenheimer's parents were alerted by the university authorities who considered placing him on probation, a fate prevented by his parents successfully lobbying the authorities.
A tall, thin chain smoker, who often neglected to eat during periods of intense thought and concentration, Oppenheimer was marked by many of his friends as having self-destructive tendencies. A disturbing event occurred when he took a vacation from his studies in Cambridge to meet up with his friend Francis Fergusson in Paris. Fergusson noticed that Oppenheimer was not well and to help distract him from his depression told Oppenheimer that he (Fergusson) was to marry his girlfriend Frances Keeley. Oppenheimer did not take the news well. He jumped on Fergusson and tried to strangle him. Although Fergusson easily fended off the attack, the episode convinced him of Oppenheimer's deep psychological troubles. Plagued throughout his life by periods of depression, Oppenheimer once told his brother, "I need physics more than friends".
In 1926 he left Cambridge for the University of Göttingen to study under Max Born. Göttingen was one of the world's leading centers for theoretical physics. Oppenheimer made friends who went on to great success, including Werner Heisenberg, Pascual Jordan, Wolfgang Pauli, Paul Dirac, Enrico Fermi and Edward Teller. He was known for being too enthusiastic in discussion, sometimes to the point of taking over seminar sessions. This irritated some of Born's other students so much that Maria Goeppert presented Born with a petition signed by herself and others threatening a boycott of the class unless he made Oppenheimer quiet down. Born left it out on his desk where Oppenheimer could read it, and it was effective without a word being said.
He obtained his Doctor of Philosophy degree in March 1927 at age 23, supervised by Born. After the oral exam, James Franck, the professor administering, reportedly said, "I'm glad that's over. He was on the point of questioning "me"." Oppenheimer published more than a dozen papers at Göttingen, including many important contributions to the new field of quantum mechanics. He and Born published a famous paper on the Born–Oppenheimer approximation, which separates nuclear motion from electronic motion in the mathematical treatment of molecules, allowing nuclear motion to be neglected to simplify calculations. It remains his most cited work.
Early professional work.
Educational work.
Oppenheimer was awarded a United States National Research Council fellowship to the California Institute of Technology (Caltech) in September 1927. Bridgman also wanted him at Harvard, so a compromise was reached whereby he split his fellowship for the 1927–28 academic year between Harvard in 1927 and Caltech in 1928. At Caltech he struck up a close friendship with Linus Pauling, and they planned to mount a joint attack on the nature of the chemical bond, a field in which Pauling was a pioneer, with Oppenheimer supplying the mathematics and Pauling interpreting the results. Both the collaboration and their friendship were nipped in the bud when Pauling began to suspect Oppenheimer of becoming too close to his wife, Ava Helen Pauling. Once, when Pauling was at work, Oppenheimer had arrived at their home and invited Ava Helen to join him on a tryst in Mexico. Though she refused and reported the incident to her husband, the invitation, and her apparent nonchalance about it, disquieted Pauling and he ended his relationship with Oppenheimer. Oppenheimer later invited him to become head of the Chemistry Division of the Manhattan Project, but Pauling refused, saying he was a pacifist.
In the autumn of 1928, Oppenheimer visited Paul Ehrenfest's institute at the University of Leiden, the Netherlands, where he impressed by giving lectures in Dutch, despite having little experience with the language. There he was given the nickname of "Opje", later anglicized by his students as "Oppie". From Leiden he continued on to the ETH in Zurich to work with Wolfgang Pauli on quantum mechanics and the continuous spectrum. Oppenheimer respected and liked Pauli and may have emulated his personal style as well as his critical approach to problems.
On returning to the United States, Oppenheimer accepted an associate professorship from the University of California, Berkeley, where Raymond T. Birge wanted him so badly that he expressed a willingness to share him with Caltech.
Before his Berkeley professorship began, Oppenheimer was diagnosed with a mild case of tuberculosis and, with his brother Frank, spent some weeks at a ranch in New Mexico, which he leased and eventually purchased. When he heard the ranch was available for lease, he exclaimed, "Hot dog!", and later called it "Perro Caliente", literally "hot dog" in Spanish. Later he used to say that "physics and desert country" were his "two great loves". He recovered from the tuberculosis and returned to Berkeley, where he prospered as an advisor and collaborator to a generation of physicists who admired him for his intellectual virtuosity and broad interests. His students and colleagues saw him as mesmerizing: hypnotic in private interaction, but often frigid in more public settings. His associates fell into two camps: one that saw him as an aloof and impressive genius and aesthete, the other that saw him as a pretentious and insecure poseur. His students almost always fell into the former category, adopting his walk, speech, and other mannerisms, and even his inclination for reading entire texts in their original languages. Hans Bethe said of him:
He worked closely with Nobel Prize-winning experimental physicist Ernest O. Lawrence and his cyclotron pioneers, helping them understand the data their machines were producing at the Lawrence Berkeley National Laboratory. In 1936 Berkeley promoted him to full professor at a salary of $3300 per annum. In return he was asked to curtail his teaching at Caltech, so a compromise was reached whereby Berkeley released him for six weeks each year, enough to teach one term at Caltech.
Scientific work.
Oppenheimer did important research in theoretical astronomy (especially as related to general relativity and nuclear theory), nuclear physics, spectroscopy, and quantum field theory, including its extension into quantum electrodynamics. The formal mathematics of relativistic quantum mechanics also attracted his attention, although he doubted its validity. His work predicted many later finds, which include the neutron, meson and neutron star.
Initially, his major interest was the theory of the continuous spectrum and his first published paper, in 1926, concerned the quantum theory of molecular band spectra. He developed a method to carry out calculations of its transition probabilities. He calculated the photoelectric effect for hydrogen and X-rays, obtaining the absorption coefficient at the K-edge. His calculations accorded with observations of the X-ray absorption of the sun, but not hydrogen. Years later it was realized that the sun was largely composed of hydrogen and that his calculations were indeed correct.
Oppenheimer also made important contributions to the theory of cosmic ray showers and started work that eventually led to descriptions of quantum tunneling. In 1931 he co-wrote a paper on the "Relativistic Theory of the Photoelectric Effect" with his student Harvey Hall, in which, based on empirical evidence, he correctly disputed Dirac's assertion that two of the energy levels of the hydrogen atom have the same energy. Subsequently, one of his doctoral students, Willis Lamb, determined that this was a consequence of what became known as the Lamb shift, for which Lamb was awarded the Nobel Prize in Physics in 1955.
Oppenheimer worked with his first doctoral student, Melba Phillips, on calculations of artificial radioactivity under bombardment by deuterons. When Ernest Lawrence and Edwin McMillan bombarded nuclei with deuterons they found the results agreed closely with the predictions of George Gamow, but when higher energies and heavier nuclei were involved, the results did not conform to the theory. In 1935, Oppenheimer and Phillips worked out a theory now known as the Oppenheimer–Phillips process to explain the results, a theory still in use today.
As early as 1930, Oppenheimer wrote a paper essentially predicting the existence of the positron, after a paper by Paul Dirac proposed that electrons could have both a positive charge and negative energy. Dirac's paper introduced an equation, known as the Dirac equation, which unified quantum mechanics, special relativity and the then-new concept of electron spin, to explain the Zeeman effect. Oppenheimer, drawing on the body of experimental evidence, rejected the idea that the predicted positively charged electrons were protons. He argued that they would have to have the same mass as an electron, whereas experiments showed that protons were much heavier than electrons. Two years later, Carl David Anderson discovered the positron, for which he received the 1936 Nobel Prize in Physics.
In the late 1930s Oppenheimer became interested in astrophysics, probably through his friendship with Richard Tolman, resulting in a series of papers. In the first of these, a 1938 paper co-written with Robert Serber entitled "On the Stability of Stellar Neutron Cores", Oppenheimer explored the properties of white dwarfs. This was followed by a paper co-written with one of his students, George Volkoff, "On Massive Neutron Cores", in which they demonstrated that there was a limit, the so-called Tolman–Oppenheimer–Volkoff limit, to the mass of stars beyond which they would not remain stable as neutron stars and would undergo gravitational collapse. Finally, in 1939, Oppenheimer and another of his students, Hartland Snyder, produced a paper "On Continued Gravitational Attraction", which predicted the existence of what are today known as black holes. After the Born–Oppenheimer approximation paper, these papers remain his most cited, and were key factors in the rejuvenation of astrophysical research in the United States in the 1950s, mainly by John A. Wheeler.
Oppenheimer's papers were considered difficult to understand even by the standards of the abstract topics he was expert in. He was fond of using elegant, if extremely complex, mathematical techniques to demonstrate physical principles, though he was sometimes criticized for making mathematical mistakes, presumably out of haste. "His physics was good", said his student Snyder, "but his arithmetic awful".
Oppenheimer published only five scientific papers, one of which was in biophysics, after World War II, and none after 1950. Murray Gell-Mann, a later Nobelist who, as a visiting scientist, worked with him at the Institute for Advanced Study in 1951, offered this opinion: 
Oppenheimer's diverse interests sometimes interrupted his focus on projects. In 1933 he learned Sanskrit and met the Indologist Arthur W. Ryder at Berkeley. He read the "Bhagavad Gita" in the original Sanskrit, and later he cited it as one of the books that most shaped his philosophy of life. His close confidant and colleague, Nobel Prize winner Isidor Rabi, later gave his own interpretation:
In spite of this, observers such as Nobel Prize-winning physicist Luis Alvarez have suggested that if he had lived long enough to see his predictions substantiated by experiment, Oppenheimer might have won a Nobel Prize for his work on gravitational collapse, concerning neutron stars and black holes. In retrospect, some physicists and historians consider this to be his most important contribution, though it was not taken up by other scientists in his own lifetime. The physicist and historian Abraham Pais once asked Oppenheimer what he considered to be his most important scientific contributions; Oppenheimer cited his work on electrons and positrons, not his work on gravitational contraction. Oppenheimer was nominated for the Nobel Prize for physics three times, in 1945, 1951 and 1967, but never won.
Private and political life.
During the 1920s, Oppenheimer remained aloof from worldly matters. He claimed that he did not read newspapers or listen to the radio, and had only learned of the Wall Street crash of 1929 some six months after it occurred while on a walk with Ernest Lawrence. He once remarked that he never cast a vote until the 1936 election. However, from 1934 on, he became increasingly concerned about politics and international affairs. In 1934, he earmarked three percent of his salary—about $100 a year—for two years to support German physicists fleeing from Nazi Germany. During the 1934 West Coast Waterfront Strike, he and some of his students, including Melba Phillips and Bob Serber, attended a longshoremen's rally. Oppenheimer repeatedly attempted to get Serber a position at Berkeley but was blocked by Birge, who felt that "one Jew in the department was enough".
Oppenheimer's mother died in 1931, and he became closer to his father who, although still living in New York, became a frequent visitor in California. When his father died in 1937 leaving $392,602 to be divided between Oppenheimer and his brother Frank, Oppenheimer immediately wrote out a will leaving his estate to the University of California for graduate scholarships. Like many young intellectuals in the 1930s, he was a supporter of social reforms that were later alleged to be communist ideas. He donated to many progressive efforts which were later branded as "left-wing" during the McCarthy era. The majority of his allegedly radical work consisted of hosting fund raisers for the Republican cause in the Spanish Civil War and other anti-fascist activity. He never openly joined the Communist Party, though he did pass money to liberal causes by way of acquaintances who were alleged to be Party members. In 1936, Oppenheimer became involved with Jean Tatlock, the daughter of a Berkeley literature professor and a student at Stanford University School of Medicine. The two had similar political views; she wrote for the "Western Worker", a Communist Party newspaper.
Tatlock broke up with Oppenheimer in 1939, after a tempestuous relationship. In August that year he met Katherine ("Kitty") Puening Harrison, a radical Berkeley student and former Communist Party member. Harrison had been married three times previously. Her first marriage lasted only a few months. Her second husband was Joe Dallet, an active member of the Communist party, who was killed in the Spanish Civil War. Kitty returned to the United States where she obtained a Bachelor of Arts degree in botany from the University of Pennsylvania. There she married Richard Harrison, a physician and medical researcher, in 1938. In June 1939 Kitty and Harrison moved to Pasadena, California, where he became chief of radiology at a local hospital and she enrolled as a graduate student at the University of California, Los Angeles. Oppenheimer and Kitty created a minor scandal by sleeping together after one of Tolman's parties. In the summer of 1940 she stayed with Oppenheimer at his ranch in New Mexico. She finally asked Harrison for a divorce when she found out she was pregnant. When he refused, she obtained an instant divorce in Reno, Nevada, and took Oppenheimer as her fourth husband on November 1, 1940.
Their first child Peter was born in May 1941, and their second child, Katherine ("Toni"), was born in Los Alamos, New Mexico, on December 7, 1944. During his marriage, Oppenheimer continued his affair with Jean Tatlock. Later their continued contact became an issue in his security clearance hearings because of Tatlock's Communist associations. Many of Oppenheimer's closest associates were active in the Communist Party in the 1930s or 1940s. They included his brother Frank, Frank's wife Jackie, Kitty, Jean Tatlock, his landlady Mary Ellen Washburn, and several of his graduate students at Berkeley.
When he joined the Manhattan Project in 1942, Oppenheimer wrote on his personal security questionnaire that he had been "a member of just about every Communist Front organization on the West Coast". Years later he claimed that he did not remember saying this, that it was not true, and that if he had said anything along those lines, it was "a half-jocular overstatement". He was a subscriber to the "People's World", a Communist Party organ, and he testified in 1954, "I was associated with the Communist movement." From 1937 to 1942, Oppenheimer was a member at Berkeley of what he called a "discussion group", which was later identified by fellow members, Haakon Chevalier and Gordon Griffiths, as a "closed" (secret) unit of the Communist Party for Berkeley faculty.
The Federal Bureau of Investigation (FBI) opened a file on Oppenheimer in March 1941. It recorded that he attended a meeting in December 1940 at Chevalier's home that was also attended by the Communist Party's California state secretary William Schneiderman, and its treasurer Isaac Folkoff. The FBI noted that Oppenheimer was on the Executive Committee of the American Civil Liberties Union, which it considered a Communist front organization. Shortly thereafter, the FBI added Oppenheimer to its Custodial Detention Index, for arrest in case of national emergency. Debates over Oppenheimer's Party membership or lack thereof have turned on very fine points; almost all historians agree he had strong left-wing sympathies during this time and interacted with Party members, though there is considerable dispute over whether he was officially a member of the Party. At his 1954 security clearance hearings, he denied being a member of the Communist Party, but identified himself as a fellow traveler, which he defined as someone who agrees with many of the goals of Communism, but without being willing to blindly follow orders from any Communist party apparatus.
Throughout the development of the atomic bomb, Oppenheimer was under investigation by both the FBI and the Manhattan Project's internal security arm for his past left-wing associations. He was followed by Army security agents during a trip to California in June 1943 to visit his former girlfriend, Jean Tatlock, who was suffering from depression. Oppenheimer spent the night in her apartment. Tatlock committed suicide on January 4, 1944, which left Oppenheimer deeply grieved. In August 1943, he volunteered to Manhattan Project security agents that George Eltenton, whom he did not know, had solicited three men at Los Alamos for nuclear secrets on behalf of the Soviet Union. When pressed on the issue in later interviews, Oppenheimer admitted that the only person who had approached him was his friend Haakon Chevalier, a Berkeley professor of French literature, who had mentioned the matter privately at a dinner at Oppenheimer's house. Brigadier General Leslie R. Groves, Jr., the director of the Manhattan Project, thought Oppenheimer was too important to the project to be ousted over this suspicious behavior. On July 20, 1943, he wrote to the Manhattan Engineer District:
Manhattan Project.
Los Alamos.
On October 9, 1941, shortly before the United States entered World War II, President Franklin D. Roosevelt approved a crash program to develop an atomic bomb. In May 1942, National Defense Research Committee Chairman James B. Conant, who had been one of Oppenheimer's lecturers at Harvard, invited Oppenheimer to take over work on fast neutron calculations, a task that Oppenheimer threw himself into with full vigor. He was given the title "Coordinator of Rapid Rupture", specifically referring to the propagation of a fast neutron chain reaction in an atomic bomb. One of his first acts was to host a summer school for bomb theory at his building in Berkeley. The mix of European physicists and his own students—a group including Robert Serber, Emil Konopinski, Felix Bloch, Hans Bethe and Edward Teller—busied themselves calculating what needed to be done, and in what order, to make the bomb.
In June 1942, the US Army established the Manhattan Engineer District to handle its part in the atom bomb project, beginning the process of transferring responsibility from the Office of Scientific Research and Development to the military. In September, Groves was appointed director of what became known as the Manhattan Project. Groves selected Oppenheimer to head the project's secret weapons laboratory, a choice which surprised many, as Oppenheimer had left-wing political views, and no record as a leader of large projects. The fact that he did not have a Nobel Prize, and might not have the prestige to direct fellow scientists, did concern Groves. However, he was impressed by Oppenheimer's singular grasp of the practical aspects of designing and constructing an atomic bomb, and by the breadth of his knowledge. As a military engineer, Groves knew that this would be vital in an interdisciplinary project that would involve not just physics, but chemistry, metallurgy, ordnance and engineering. Groves also detected in Oppenheimer something that many others did not, an "overweening ambition" that Groves reckoned would supply the drive necessary to push the project to a successful conclusion. Isidor Rabi considered the appointment "a real stroke of genius on the part of General Groves, who was not generally considered to be a genius".
Oppenheimer and Groves decided that for security and cohesion they needed a centralized, secret research laboratory in a remote location. Scouting for a site in late 1942, Oppenheimer was drawn to New Mexico, not far from his ranch. On November 16, 1942, Oppenheimer, Groves and others toured a prospective site. Oppenheimer feared that the high cliffs surrounding the site would make his people feel claustrophobic, while the engineers were concerned with the possibility of flooding. He then suggested and championed a site that he knew well: a flat mesa near Santa Fe, New Mexico, which was the site of a private boys' school called the Los Alamos Ranch School. The engineers were concerned about the poor access road and the water supply, but otherwise felt that it was ideal. The Los Alamos Laboratory was built on the site of the school, taking over some of its buildings, while many others were erected in great haste. There Oppenheimer assembled a group of the top physicists of the time, which he referred to as the "luminaries".
Initially Los Alamos was supposed to be a military laboratory, and Oppenheimer and other researchers were to be commissioned into the Army. He went so far as to order himself a lieutenant colonel's uniform and take the Army physical test, which he failed. Army doctors considered him underweight at , diagnosed his chronic cough as tuberculosis and were concerned about his chronic lumbosacral joint pain. The plan to commission scientists fell through when Robert Bacher and Isidor Rabi balked at the idea. Conant, Groves, and Oppenheimer devised a compromise whereby the laboratory was operated by the University of California under contract to the War Department. It soon turned out that Oppenheimer had hugely underestimated the magnitude of the project; Los Alamos grew from a few hundred people in 1943 to over 6,000 in 1945.
Oppenheimer at first had difficulty with the organizational division of large groups, but rapidly learned the art of large-scale administration after he took up permanent residence on the mesa. He was noted for his mastery of all scientific aspects of the project and for his efforts to control the inevitable cultural conflicts between scientists and the military. He was an iconic figure to his fellow scientists, as much a symbol of what they were working toward as a scientific director. Victor Weisskopf put it thus:
In 1943 development efforts were directed to a plutonium gun-type fission weapon called "Thin Man". Initial research on the properties of plutonium was done using cyclotron-generated plutonium-239, which was extremely pure but could only be created in tiny amounts. When Los Alamos received the first sample of plutonium from the X-10 Graphite Reactor in April 1944 a problem was discovered: reactor-bred plutonium had a higher concentration of plutonium-240, making it unsuitable for use in a gun-type weapon. In July 1944, Oppenheimer abandoned the gun design in favor of an implosion-type weapon. Using chemical explosive lenses, a sub-critical sphere of fissile material could be squeezed into a smaller and denser form. The metal needed to travel only very short distances, so the critical mass would be assembled in much less time. In August 1944 Oppenheimer implemented a sweeping reorganization of the Los Alamos laboratory to focus on implosion. He concentrated the development efforts on the gun-type device, a simpler design that only had to work with uranium-235, in a single group, and this device became Little Boy in February 1945. After a mammoth research effort, the more complex design of the implosion device, known as the "Christy gadget" after Robert Christy, another student of Oppenheimer's, was finalized in a meeting in Oppenheimer's office on February 28, 1945.
In May 1945 an Interim Committee was created to advise and report on wartime and postwar policies regarding the use of nuclear energy. The Interim Committee in turn established a scientific panel consisting of Compton, Fermi, Lawrence and Oppenheimer to advise it on scientific issues. In its presentation to the Interim Committee the scientific panel offered its opinion not just on the likely physical effects of an atomic bomb, but on its likely military and political impact. This included opinions on such sensitive issues as whether or not the Soviet Union should be advised of the weapon in advance of its use against Japan.
Trinity.
The joint work of the scientists at Los Alamos resulted in the first artificial nuclear explosion near Alamogordo on July 16, 1945, on a site that Oppenheimer codenamed "Trinity" in mid-1944. He later said this name was from one of John Donne's Holy Sonnets. According to the historian Gregg Herken, this naming could have been an allusion to Jean Tatlock, who had committed suicide a few months previously and had in the 1930s introduced Oppenheimer to Donne's work. Oppenheimer later recalled that, while witnessing the explosion, he thought of a verse from the Hindu holy book, the "Bhagavad Gita" (XI,12):
Years later he would explain that another verse had also entered his head at that time: namely, the famous verse:
"" (XI,32), which he translated as "I am become Death, the destroyer of worlds."
In 1965, he was persuaded to quote again for a television broadcast:
According to his brother, at the time Oppenheimer simply exclaimed, "It worked." A contemporary account by Brigadier General Thomas Farrell, who was present in the control bunker at the site with Oppenheimer, summarized his reaction as follows:
Physicist Isidor Rabi noticed Oppenheimer's disconcerting triumphalism: "I'll never forget his walk; I'll never forget the way he stepped out of the car ... his walk was like "High Noon" ... this kind of strut. He had done it." At an assembly at Los Alamos on August 6 (the evening of the atomic bombing of Hiroshima), Oppenheimer took to the stage and clasped his hands together "like a prize-winning boxer" while the crowd cheered. He noted his regret the weapon had not been available in time to use against Nazi Germany. However, he and many of the project staff were very upset about the bombing of Nagasaki, as they did not feel the second bomb was necessary from a military point of view. He traveled to Washington on August 17 to hand-deliver a letter to Secretary of War Henry L. Stimson expressing his revulsion and his wish to see nuclear weapons banned. In October 1945 Oppenheimer was granted an interview with President Harry S Truman. The meeting, however, went badly, after Oppenheimer remarked he felt he had "blood on my hands." The remark infuriated Truman and put an end to the meeting. Truman later told his Undersecretary of State Dean Acheson "I don't want to see that son-of-a-bitch in this office ever again."
For his services as director of Los Alamos, Oppenheimer was awarded the Medal for Merit from President Harry S Truman in 1946.
Postwar activities.
After the bombings of Hiroshima and Nagasaki, the Manhattan Project became public knowledge; and Oppenheimer became a national spokesman for science, emblematic of a new type of technocratic power. He became a household name and his face appeared on the covers of "Life" and "Time". Nuclear physics became a powerful force as all governments of the world began to realize the strategic and political power that came with nuclear weapons. Like many scientists of his generation, he felt that security from atomic bombs would come only from a transnational organization such as the newly formed United Nations, which could institute a program to stifle a nuclear arms race.
Institute for Advanced Study.
In November 1945, Oppenheimer left Los Alamos to return to Caltech, but he soon found that his heart was no longer in teaching. In 1947, he accepted an offer from Lewis Strauss to take up the directorship of the Institute for Advanced Study in Princeton, New Jersey. This meant moving back east and leaving Ruth Tolman, the wife of his friend Richard Tolman, with whom he had begun an affair after leaving Los Alamos. The job came with a salary of $20,000 per annum, plus rent-free accommodation in the director's house, a 17th-century manor with a cook and groundskeeper, surrounded by of woodlands.
Oppenheimer brought together intellectuals at the height of their powers and from a variety of disciplines to solve the most pertinent questions of the age. He directed and encouraged the research of many well-known scientists, including Freeman Dyson, and the duo of Chen Ning Yang and Tsung-Dao Lee, who won a Nobel Prize for their discovery of parity non-conservation. He also instituted temporary memberships for scholars from the humanities, such as T. S. Eliot and George F. Kennan. Some of these activities were resented by a few members of the mathematics faculty, who wanted the institute to stay a bastion of pure scientific research. Abraham Pais said that Oppenheimer himself thought that one of his failures at the institute was being unable to bring together scholars from the natural sciences and the humanities.
A series of conferences in New York from 1947 through 1949 saw physicists switch back from war work to theoretical issues. Under Oppenheimer's direction, physicists tackled the greatest outstanding problem of the pre-war years: infinite, divergent, and non-sensical expressions in the quantum electrodynamics of elementary particles. Julian Schwinger, Richard Feynman and Shin'ichiro Tomonaga tackled the problem of regularization, and developed techniques which became known as renormalization. Freeman Dyson was able to prove that their procedures gave similar results. The problem of meson absorption and Hideki Yukawa's theory of mesons as the carrier particles of the strong nuclear force were also tackled. Probing questions from Oppenheimer prompted Robert Marshak's innovative two-meson hypothesis: that there were actually two types of mesons, pions and muons. This led to Cecil Frank Powell's breakthrough and subsequent Nobel Prize for the discovery of the pion.
Atomic Energy Commission.
As a member of the Board of Consultants to a committee appointed by Truman, Oppenheimer strongly influenced the Acheson–Lilienthal Report. In this report, the committee advocated creation of an international Atomic Development Authority, which would own all fissionable material and the means of its production, such as mines and laboratories, and atomic power plants where it could be used for peaceful energy production. Bernard Baruch was appointed to translate this report into a proposal to the United Nations, resulting in the Baruch Plan of 1946. The Baruch Plan introduced many additional provisions regarding enforcement, in particular requiring inspection of the Soviet Union's uranium resources. The Baruch Plan was seen as an attempt to maintain the United States' nuclear monopoly and was rejected by the Soviets. With this, it became clear to Oppenheimer that an arms race was unavoidable, due to the mutual suspicion of the United States and the Soviet Union, which even Oppenheimer was starting to distrust.
After the Atomic Energy Commission (AEC) came into being in 1947 as a civilian agency in control of nuclear research and weapons issues, Oppenheimer was appointed as the Chairman of its General Advisory Committee (GAC). From this position he advised on a number of nuclear-related issues, including project funding, laboratory construction and even international policy—though the GAC's advice was not always heeded. As Chairman of the GAC, Oppenheimer lobbied vigorously for international arms control and funding for basic science, and attempted to influence policy away from a heated arms race. When the government questioned whether to pursue a crash program to develop an atomic weapon based on nuclear fusion—the hydrogen bomb—Oppenheimer initially recommended against it, though he had been in favor of developing such a weapon during the Manhattan Project. He was motivated partly by ethical concerns, feeling that such a weapon could only be used strategically against civilian targets, resulting in millions of deaths. He was also motivated by practical concerns, however, as at the time there was no workable design for a hydrogen bomb. Oppenheimer felt that resources would be better spent creating a large force of fission weapons. He and others were especially concerned about nuclear reactors being diverted from plutonium to tritium production. They were overridden by Truman, who announced a crash program after the Soviet Union tested their first atomic bomb in 1949. Oppenheimer and other GAC opponents of the project, especially James Conant, felt personally shunned and considered retiring from the committee. They stayed on, though their views on the hydrogen bomb were well known.
In 1951, however, Edward Teller and mathematician Stanislaw Ulam developed what became known as the Teller-Ulam design for a hydrogen bomb. This new design seemed technically feasible and Oppenheimer changed his opinion about developing the weapon. As he later recalled:
Security hearing.
The FBI under J. Edgar Hoover had been following Oppenheimer since before the war, when he showed Communist sympathies as a professor at Berkeley and had been close to members of the Communist Party, including his wife and brother. He had been under close surveillance since the early 1940s, his home and office bugged, his phone tapped and his mail opened. The FBI furnished Oppenheimer's political enemies with incriminating evidence about his Communist ties. These enemies included Strauss, an AEC commissioner who had long harbored resentment against Oppenheimer both for his activity in opposing the hydrogen bomb and for his humiliation of Strauss before Congress some years earlier; regarding Strauss's opposition to the export of radioactive isotopes to other nations, Oppenheimer had memorably categorized these as "less important than electronic devices but more important than, let us say, vitamins."
On June 7, 1949, Oppenheimer testified before the House Un-American Activities Committee, where he admitted that he had associations with the Communist Party in the 1930s. He testified that some of his students, including David Bohm, Giovanni Rossi Lomanitz, Philip Morrison, Bernard Peters and Joseph Weinberg, had been Communists at the time they had worked with him at Berkeley. Frank Oppenheimer and his wife Jackie testified before the HUAC and admitted that they had been members of the Communist Party. Frank was subsequently fired from his University of Minnesota position. Unable to find work in physics for many years, he became instead a cattle rancher in Colorado. He later taught high school physics and was the founder of the San Francisco Exploratorium.
Oppenheimer had found himself in the middle of more than one controversy and power struggle in the years from 1949 to 1953. Edward Teller, who had been so uninterested in work on the atomic bomb at Los Alamos during the war that Oppenheimer had given him time instead to work on his own project of the hydrogen bomb, had eventually left Los Alamos in 1951 to help found, in 1952, a second laboratory at what would become the Lawrence Livermore National Laboratory. There, he could be free of Los Alamos control to develop the hydrogen bomb. Long-range thermonuclear "strategic" weapons delivered by jet bombers would necessarily be under control of the new United States Air Force (USAF). Oppenheimer had for some years pushed for smaller "tactical" nuclear weapons which would be more useful in a limited theater against enemy troops and which would be under control of the Army. The two services fought for control of nuclear weapons, often allied with different political parties. The USAF, with Teller pushing its program, gained ascendance in the Republican-controlled administration following the election of Dwight D. Eisenhower as president in 1952.
Strauss and Senator Brien McMahon, author of the 1946 McMahon Act, pushed Eisenhower to revoke Oppenheimer's security clearance. On December 21, 1953, Strauss told Oppenheimer that his security clearance had been suspended, pending resolution of a series of charges outlined in a letter, and discussed his resigning. Oppenheimer chose not to resign and requested a hearing instead. The charges were outlined in a letter from Kenneth D. Nichols, General Manager of the AEC. The hearing that followed in April–May 1954, which was initially confidential and not made public, focused on Oppenheimer's past Communist ties and his association during the Manhattan Project with suspected disloyal or Communist scientists. The US Department of Energy made public the full text of the transcript in October 2014.
One of the key elements in this hearing was Oppenheimer's earliest testimony about George Eltenton's approach to various Los Alamos scientists, a story that Oppenheimer confessed he had fabricated to protect his friend Haakon Chevalier. Unknown to Oppenheimer, both versions were recorded during his interrogations of a decade before. He was surprised on the witness stand with transcripts of these, which he had not been given a chance to review. In fact, Oppenheimer had never told Chevalier that he had finally named him, and the testimony had cost Chevalier his job. Both Chevalier and Eltenton confirmed mentioning that they had a way to get information to the Soviets, Eltenton admitting he said this to Chevalier and Chevalier admitting he mentioned it to Oppenheimer, but both put the matter in terms of gossip and denied any thought or suggestion of treason or thoughts of espionage, either in planning or in deed. Neither was ever convicted of any crime.
Teller testified that he considered Oppenheimer loyal, but that: This led to outrage by the scientific community and Teller's virtual expulsion from academic science. Groves, threatened by the FBI as having been potentially part of a coverup about the Chevalier contact in 1943, likewise testified against Oppenheimer. Many top scientists, as well as government and military figures, testified on Oppenheimer's behalf. Inconsistencies in his testimony and his erratic behavior on the stand, at one point saying he had given a "cock and bull story" and that this was because he "was an idiot", convinced some that he was unstable, unreliable and a possible security risk. Oppenheimer's clearance was revoked one day before it was due to lapse anyway. Isidor Rabi's comment was that Oppenheimer was merely a government consultant at the time anyway and that if the government "didn't want to consult the guy, then don't consult him."
During his hearing, Oppenheimer testified willingly on the left-wing behavior of many of his scientific colleagues. Had Oppenheimer's clearance not been stripped then he might have been remembered as someone who had "named names" to save his own reputation. As it happened, Oppenheimer was seen by most of the scientific community as a martyr to McCarthyism, an eclectic liberal who was unjustly attacked by warmongering enemies, symbolic of the shift of scientific creativity from academia into the military. Wernher von Braun summed up his opinion about the matter with a quip to a Congressional committee: "In England, Oppenheimer would have been knighted."
In a seminar at the Woodrow Wilson Institute on May 20, 2009, based on an extensive analysis of the Vassiliev notebooks taken from the KGB archives, John Earl Haynes, Harvey Klehr and Alexander Vassiliev confirmed that Oppenheimer never was involved in espionage for the Soviet Union. The KGB tried repeatedly to recruit him, but was never successful; Oppenheimer did not betray the United States. In addition, he had several persons removed from the Manhattan Project who had sympathies to the Soviet Union. Haynes, Klehr and Vassiliev also state Oppenheimer "was, in fact, a concealed member of the CPUSA in the late 1930s". According to biographer Ray Monk: "He was, in a very practical and real sense, a supporter of the Communist Party. Moreover, in terms of the time, effort and money spent on Party activities, he was a very committed supporter".
Final years and death.
Starting in 1954, Oppenheimer spent several months of the year living on the island of Saint John in the U.S. Virgin Islands. In 1957, he purchased a tract of land on Gibney Beach, where he built a spartan home on the beach. He spent a considerable amount of time sailing with his daughter Toni and wife Kitty.
Increasingly concerned about the potential danger to humanity arising from scientific discoveries, Oppenheimer joined with Albert Einstein, Bertrand Russell, Joseph Rotblat and other eminent scientists and academics to establish what would eventually become the World Academy of Art and Science in 1960. Significantly, after his public humiliation, he did not sign the major open protests against nuclear weapons of the 1950s, including the Russell–Einstein Manifesto of 1955, nor, though invited, did he attend the first Pugwash Conferences on Science and World Affairs in 1957.
In his speeches and public writings, Oppenheimer continually stressed the difficulty of managing the power of knowledge in a world in which the freedom of science to exchange ideas was more and more hobbled by political concerns. Oppenheimer delivered the Reith Lectures on the BBC in 1953, which were subsequently published as "Science and the Common Understanding". In 1955 Oppenheimer published "The Open Mind", a collection of eight lectures that he had given since 1946 on the subject of nuclear weapons and popular culture. Oppenheimer rejected the idea of nuclear gunboat diplomacy. "The purposes of this country in the field of foreign policy", he wrote, "cannot in any real or enduring way be achieved by coercion." In 1957 the philosophy and psychology departments at Harvard invited Oppenheimer to deliver the William James Lectures. An influential group of Harvard alumni led by Edwin Ginn that included Archibald Roosevelt protested against the decision. Some 1,200 people packed into Sanders Theatre to hear Oppenheimer's six lectures, entitled "The Hope of Order". Oppenheimer delivered the Whidden Lectures at McMaster University in 1962, and these were published in 1964 as "The Flying Trapeze: Three Crises for Physicists".
Deprived of political power, Oppenheimer continued to lecture, write and work on physics. He toured Europe and Japan, giving talks about the history of science, the role of science in society, and the nature of the universe. In September 1957, France made him an Officer of the Legion of Honor, and on May 3, 1962, he was elected a Foreign Member of the Royal Society in Britain. At the urging of many of Oppenheimer's political friends who had ascended to power, President John F. Kennedy awarded Oppenheimer the Enrico Fermi Award in 1963 as a gesture of political rehabilitation. Edward Teller, the winner of the previous year's award, had also recommended Oppenheimer receive it, in the hope that it would heal the rift between them. A little over a week after Kennedy's assassination, his successor, President Lyndon Johnson, presented Oppenheimer with the award, "for contributions to theoretical physics as a teacher and originator of ideas, and for leadership of the Los Alamos Laboratory and the atomic energy program during critical years." Oppenheimer told Johnson: "I think it is just possible, Mr. President, that it has taken some charity and some courage for you to make this award today." The rehabilitation implied by the award was partly symbolic, as Oppenheimer still lacked a security clearance and could have no effect on official policy, but the award came with a $50,000 tax-free stipend, and its award outraged many prominent Republicans in Congress. The late President Kennedy's widow Jacqueline, still living in the White House, made it a point to meet with Oppenheimer to tell him how much her husband had wanted him to have the medal. While still a senator in 1959, Kennedy had been instrumental in voting to narrowly deny Oppenheimer's enemy Lewis Strauss a coveted government position as Secretary of Commerce, effectively ending Strauss' political career. This was partly due to lobbying by the scientific community on behalf of Oppenheimer.
Oppenheimer was diagnosed with throat cancer in late 1965 and, after inconclusive surgery, underwent unsuccessful radiation treatment and chemotherapy late in 1966. He fell into a coma on February 15, 1967, and died at his home in Princeton, New Jersey, on February 18, aged 62. A memorial service was held at Alexander Hall at Princeton University a week later, attended by 600 of his scientific, political and military associates including Bethe, Groves, Kennan, Lilienthal, Rabi, Smyth and Wigner. His brother Frank and the rest of his family were also there, as was the historian Arthur M. Schlesinger, Jr., the novelist John O'Hara, and George Balanchine, the director of the New York City Ballet. Bethe, Kennan and Smyth gave brief eulogies. Oppenheimer was cremated and his ashes were placed in an urn. Kitty took his ashes to St. John and dropped the urn into the sea off the coast, within sight of the beach house.
When Kitty died of an intestinal infection complicated by pulmonary embolism in October 1972, Oppenheimer's ranch in New Mexico was inherited by their son Peter, and the beach property was inherited by their daughter Katherine "Toni" Oppenheimer Silber. Toni was refused security clearance for her chosen vocation as a United Nations translator after the FBI brought up the old charges against her father. In January 1977, three months after the end of her second marriage, she committed suicide at age 32. She left the property to "the people of St. John for a public park and recreation area." The original house, built too close to the coast, succumbed to a hurricane, but today, the Virgin Islands Government maintains a Community Center in the area.
Legacy.
When Oppenheimer was ejected from his position of political influence in 1954, he symbolized for many the folly of scientists thinking they could control how others would use their research. He has also been seen as symbolizing the dilemmas involving the moral responsibility of the scientist in the nuclear world. The hearings were motivated both by politics, as Oppenheimer was seen as a representative of the previous administration, and by personal considerations stemming from his enmity with Lewis Strauss. The ostensible reason for the hearing and the issue that aligned Oppenheimer with the liberal intellectuals, Oppenheimer's opposition to hydrogen bomb development, was based as much on technical grounds as on moral ones. Once the technical considerations were resolved, he supported Teller's hydrogen bomb because he believed that the Soviet Union would inevitably construct one too. Rather than consistently oppose the "Red-baiting" of the late 1940s and early 1950s, Oppenheimer testified against some of his former colleagues and students, both before and during his hearing. In one incident, his damning testimony against former student Bernard Peters was selectively leaked to the press. Historians have interpreted this as an attempt by Oppenheimer to please his colleagues in the government and perhaps to divert attention from his own previous left-wing ties and those of his brother. In the end it became a liability when it became clear that if Oppenheimer had really doubted Peters' loyalty, his recommending him for the Manhattan Project was reckless, or at least contradictory.
Popular depictions of Oppenheimer view his security struggles as a confrontation between right-wing militarists (symbolized by Teller) and left-wing intellectuals (symbolized by Oppenheimer) over the moral question of weapons of mass destruction. The question of the scientists' responsibility toward humanity inspired Bertolt Brecht's drama "Galileo" (1955), left its imprint on Friedrich Dürrenmatt's "Die Physiker", and is the basis of the opera "Doctor Atomic" by John Adams (2005), which was commissioned to portray Oppenheimer as a modern-day Faust. Heinar Kipphardt's play "In the Matter of J. Robert Oppenheimer", after appearing on West German television, had its theatrical release in Berlin and Munich in October 1964. Oppenheimer's objections resulted in an exchange of correspondence with Kipphardt, in which the playwright offered to make corrections but defended the play. It premiered in New York in June 1968, with Joseph Wiseman in the Oppenheimer role. "New York Times" theater critic Clive Barnes called it an "angry play and a partisan play" that sided with Oppenheimer but portrayed the scientist as a "tragic fool and genius". Oppenheimer had difficulty with this portrayal. After reading a transcript of Kipphardt's play soon after it began to be performed, Oppenheimer threatened to sue the playwright, decrying "improvisations which were contrary to history and to the nature of the people involved."
Later Oppenheimer told an interviewer:
The 1980 BBC TV serial "Oppenheimer", starring Sam Waterston, won three BAFTA Television Awards. "The Day After Trinity", a 1980 documentary about J. Robert Oppenheimer and the building of the atomic bomb, was nominated for an Academy Award and received a Peabody Award. Oppenheimer's life has been explored in the play "Oppenheimer" by Tom Morton-Smith. In addition to his use by authors of fiction, there are numerous biographies, including "" (2005) by Kai Bird and Martin J. Sherwin which won the Pulitzer Prize for Biography or Autobiography for 2006. A centennial conference and exhibit were held in 2004 at Berkeley, with the proceedings of the conference published in 2005 as "Reappraising Oppenheimer: Centennial Studies and Reflections". His papers are in the Library of Congress.
As a scientist, Oppenheimer is remembered by his students and colleagues as being a brilliant researcher and engaging teacher, the founder of modern theoretical physics in the United States. Because his scientific attentions often changed rapidly, he never worked long enough on any one topic and carried it to fruition to merit the Nobel Prize, although his investigations contributing to the theory of black holes may have warranted the prize had he lived long enough to see them brought into fruition by later astrophysicists. An asteroid, 67085 Oppenheimer, was named in his honor, as was the lunar crater Oppenheimer.
As a military and public policy advisor, Oppenheimer was a technocratic leader in a shift in the interactions between science and the military and the emergence of "Big Science". During World War II, scientists became involved in military research to an unprecedented degree. Because of the threat fascism posed to Western civilization, they volunteered in great numbers both for technological and organizational assistance to the Allied effort, resulting in such powerful tools as radar, the proximity fuse and operations research. As a cultured, intellectual, theoretical physicist who became a disciplined military organizer, Oppenheimer represented the shift away from the idea that scientists had their "head in the clouds" and that knowledge on such previously esoteric subjects as the composition of the atomic nucleus had no "real-world" applications.
Two days before the Trinity test, Oppenheimer expressed his hopes and fears in a quotation from the "Bhagavad Gita":

</doc>
<doc id="39038" url="https://en.wikipedia.org/wiki?curid=39038" title="Hanford Site">
Hanford Site

The Hanford Site is a mostly decommissioned nuclear production complex operated by the United States federal government on the Columbia River in the U.S. state of Washington. The site has been known by many names, including: Hanford Project, Hanford Works, Hanford Engineer Works and Hanford Nuclear Reservation. Established in 1943 as part of the Manhattan Project in Hanford, south-central Washington, the site was home to the B Reactor, the first full-scale plutonium production reactor in the world. Plutonium manufactured at the site was used in the first nuclear bomb, tested at the Trinity site, and in Fat Man, the bomb detonated over Nagasaki, Japan.
During the Cold War, the project expanded to include nine nuclear reactors and five large plutonium processing complexes, which produced plutonium for most of the more than 60,000 weapons in the U.S. nuclear arsenal. Nuclear technology developed rapidly during this period, and Hanford scientists produced major technological achievements. Many early safety procedures and waste disposal practices were inadequate, and government documents have confirmed that Hanford's operations released significant amounts of radioactive materials into the air and the Columbia River.
The weapons production reactors were decommissioned at the end of the Cold War, and decades of manufacturing left behind of high-level radioactive waste stored within 177 storage tanks, an additional of solid radioactive waste, and of contaminated groundwater beneath the site. In 2011, DOE emptied 149 single-shell tanks by pumping nearly all of the liquid waste out into 28 newer double-shell tanks. DOE later found water intruding into at least 14 single-shell tanks and that one of them had been leaking about per year into the ground since about 2010. In 2012, DOE discovered a leak also from a double-shell tank caused by construction flaws and corrosion in the bottom, and that 12 double-shell tanks have similar construction flaws. Since then, DOE changed to monitoring single-shell tanks monthly and double-shell tanks every 3 years, and also changed monitoring methods. In March 2014, DOE announced further delays in the construction of the Waste Treatment Plant, which will affect the schedule for removing waste from the tanks.
Intermittent discoveries of undocumented contamination have slowed the pace and raised the cost of cleanup.
In 2007, the Hanford site represented two-thirds of the nation's high-level radioactive waste by volume. Hanford is currently the most contaminated nuclear site in the United States and is the focus of the nation's largest environmental cleanup. Besides the cleanup project, Hanford also hosts a commercial nuclear power plant, the Columbia Generating Station, and various centers for scientific research and development, such as the Pacific Northwest National Laboratory and the LIGO Hanford Observatory.
On November 10, 2015, it was designated as part of the Manhattan Project National Historical Park alongside other sites in Oak Ridge and Los Alamos.
Geography.
The Hanford Site occupies —roughly equivalent to half of the total area of Rhode Island—within Benton County, Washington. This land is closed to the general public. It is a desert environment receiving under 10 inches of annual precipitation, covered mostly by shrub-steppe vegetation. The Columbia River flows along the site for approximately , forming its northern and eastern boundary. The original site was and included buffer areas across the river in Grant and Franklin counties. Some of this land has been returned to private use and is now covered with orchards and irrigated fields. In 2000, large portions of the site were turned over to the Hanford Reach National Monument. The site is divided by function into three main areas. The nuclear reactors were located along the river in an area designated as the 100 Area; the chemical separations complexes were located inland in the Central Plateau, designated as the 200 Area; and various support facilities were located in the southeast corner of the site, designated as the 300 Area.
The site is bordered on the southeast by the Tri-Cities, a metropolitan area composed of Richland, Kennewick, Pasco, and smaller communities, and home to over 230,000 residents. Hanford is a primary economic base for these cities.
Early history.
The confluence of the Yakima, Snake, and Columbia rivers has been a meeting place for native peoples for centuries. The archaeological record of Native American habitation of this area stretches back over ten thousand years. Tribes and nations including the Yakama, Nez Perce, and Umatilla used the area for hunting, fishing, and gathering plant foods. Hanford archaeologists have identified numerous Native American sites, including "pit house villages, open campsites, fishing sites, hunting/kill sites, game drive complexes, quarries, and spirit quest sites", and two archaeological sites were listed on the National Register of Historic Places in 1976. Native American use of the area continued into the 20th century, even as the tribes were relocated to reservations. The Wanapum people were never forced onto a reservation, and they lived along the Columbia River in the Priest Rapids Valley until 1943. Euro-Americans began to settle the region in the 1860s, initially along the Columbia River south of Priest Rapids. They established farms and orchards supported by small-scale irrigation projects and railroad transportation, with small town centers at Hanford, White Bluffs, and Richland.
Manhattan Project.
During World War II, the Uranium Committee of the federal Office of Scientific Research and Development (OSRD) sponsored an intensive research project on plutonium. The research contract was awarded to scientists at the University of Chicago Metallurgical Laboratory (Met Lab). At the time, plutonium was a rare element that had only recently been isolated in a University of California laboratory. The Met Lab researchers worked on producing chain-reacting "piles" of uranium to convert it to plutonium and finding ways to separate plutonium from uranium. The program was accelerated in 1942, as the United States government became concerned that scientists in Nazi Germany were developing a nuclear weapons program.
Site selection.
In September 1942, the Army Corps of Engineers placed the newly formed Manhattan Project under the command of General Leslie R. Groves, charging him with the construction of industrial-size plants for manufacturing plutonium and uranium. Groves recruited the DuPont Company to be the prime contractor for the construction of the plutonium production complex. DuPont recommended that it be located far away from the existing uranium production facility at Oak Ridge, Tennessee. The ideal site was described by these criteria:
In December 1942, Groves dispatched his assistant Colonel Franklin T. Matthias and DuPont engineers to scout potential sites. Matthias reported that Hanford was "ideal in virtually all respects," except for the farming towns of White Bluffs and Hanford. General Groves visited the site in January and established the Hanford Engineer Works, codenamed "Site W". The federal government quickly acquired the land under its eminent domain authority and relocated some 1,500 residents of Hanford, White Bluffs, and nearby settlements, as well as the Wanapum people, Confederated Tribes and Bands of the Yakama Nation, the Confederated Tribes of the Umatilla Indian Reservation, and the Nez Perce Tribe.
Construction begins.
The Hanford Engineer Works (HEW) broke ground in March 1943 and immediately launched a massive and technically challenging construction project. DuPont advertised for workers in newspapers for an unspecified "war construction project" in southeastern Washington, offering "attractive scale of wages" and living facilities.
The construction workers (who reached a peak of 44,900 in June 1944) lived in a construction camp near the old Hanford townsite. The administrators and engineers lived in the government town established at Richland Village, which eventually had accommodation in 4,300 family units and 25 dormitories.
Construction of the nuclear facilities proceeded rapidly. Before the end of the war in August 1945, the HEW built 554 buildings at Hanford, including three nuclear reactors (105-B, 105-D, and 105-F) and three plutonium processing canyons (221-T, 221-B, and 221-U), each long.
To receive the radioactive wastes from the chemical separations process, the HEW built "tank farms" consisting of 64 single-shell underground waste tanks (241-B, 241-C, 241-T, and 241-U). The project required of roads, of railway, and four electrical substations. The HEW used of concrete and 40,000 short tons (36,000 t) of structural steel and consumed $230 million between 1943 and 1946.
Plutonium production.
The B Reactor (105-B) at Hanford was the first large-scale plutonium production reactor in the world. It was designed and built by DuPont based on an experimental design by Enrico Fermi, and originally operated at 250 megawatts (thermal). The reactor was graphite moderated and water cooled. It consisted of a , graphite cylinder lying on its side, penetrated through its entire length horizontally by 2,004 aluminium tubes. (180 t) of uranium slugs, diameter by long, sealed in aluminium cans went into the tubes. Cooling water was pumped through the aluminium tubes around the uranium slugs at the rate of per minute.
Construction on B Reactor began in August 1943 and was completed on September 13, 1944. The reactor went critical in late September and, after overcoming nuclear poisoning, produced its first plutonium on November 6, 1944. Plutonium was produced in the Hanford reactors when a uranium-238 atom in a fuel slug absorbed a neutron to form uranium-239. U-239 rapidly undergoes beta decay to form neptunium-239, which rapidly undergoes a second beta decay to form plutonium-239. The irradiated fuel slugs were transported by rail to three huge remotely operated chemical separation plants called "canyons" that were about away. A series of chemical processing steps separated the small amount of plutonium that was produced from the remaining uranium and the fission waste products. This first batch of plutonium was refined in the 221-T plant from December 26, 1944, to February 2, 1945, and delivered to the Los Alamos laboratory in New Mexico on February 5, 1945.
Two identical reactors, D Reactor and F reactor, came online in December 1944 and February 1945, respectively. By April 1945, shipments of plutonium were headed to Los Alamos every five days, and Hanford soon provided enough material for the bombs tested at Trinity and dropped over Nagasaki. Throughout this period, the Manhattan Project maintained a top secret classification. Until news arrived of the bomb dropped on Hiroshima, fewer than one percent of Hanford's workers knew they were working on a nuclear weapons project. General Groves noted in his memoirs that "We made certain that each member of the project thoroughly understood his part in the total effort; that, and nothing more."
Initially six reactors or "piles" were proposed, when the plutonium was to be used in the gun-type Thin Man bomb. In mid-1944 a simple gun-type bomb was found to be impractical for plutonium, and the more advanced Fat Man bomb required less plutonium. The number of piles was reduced to four and then three; and the number of chemical separation plants from four to three.
Technological innovations.
In the short time frame of the Manhattan Project, Hanford engineers produced many significant technological advances. As no one had ever built an industrial-scale nuclear reactor before, scientists were unsure how much heat would be generated by fission during normal operations. Seeking the greatest possible production while maintaining an adequate safety margin, DuPont engineers installed ammonia-based refrigeration systems with the D and F reactors to further chill the river water before its use as reactor coolant.
Another difficulty the engineers struggled with was how to deal with radioactive contamination. Once the canyons began processing irradiated slugs, the machinery would become so radioactive that it would be unsafe for humans ever to come in contact with it. The engineers therefore had to devise methods to allow for the replacement of any component via remote control. They came up with a modular cell concept, which allowed major components to be removed and replaced by an operator sitting in a heavily shielded overhead crane. This method required early practical application of two technologies that later gained widespread use: Teflon, used as a gasket material, and closed-circuit television, used to give the crane operator a better view of the process.
Cold War expansion.
In September 1946, the General Electric Company assumed management of the Hanford Works under the supervision of the newly created Atomic Energy Commission. As the Cold War began, the United States faced a new strategic threat in the rise of the Soviet nuclear weapons program. In August 1947, the Hanford Works announced funding for the construction of two new weapons reactors and research leading to the development of a new chemical separations process. With this announcement, Hanford entered a new phase of expansion.
By 1963, the Hanford Site was home to nine nuclear reactors along the Columbia River, five reprocessing plants on the central plateau, and more than 900 support buildings and radiological laboratories around the site. Extensive modifications and upgrades were made to the original three World War II reactors, and a total of 177 underground waste tanks were built. Hanford was at its peak production from 1956 to 1965. Over the entire 40 years of operations, the site produced about of plutonium, supplying the majority of the 60,000 weapons in the U.S. arsenal. Uranium-233 was also produced.
Decommissioning.
Most of the reactors were shut down between 1964 and 1971, with an average individual life span of 22 years. The last reactor, N Reactor, continued to operate as a dual-purpose reactor, being both a power reactor used to feed the civilian electrical grid via the Washington Public Power Supply System (WPPSS) and a plutonium production reactor for nuclear weapons. N Reactor operated until 1987. Since then, most of the Hanford reactors have been entombed ("cocooned") to allow the radioactive materials to decay, and the surrounding structures have been removed and buried. The B-Reactor has not been cocooned and is accessible to the public on occasional guided tours. It was listed on the National Register of Historic Places in 1992, and some historians advocate converting it into a museum. B reactor was designated a National Historic Landmark by the National Park Service on August 19, 2008.
Later operations.
The United States Department of Energy assumed control of the Hanford Site in 1977. Although uranium enrichment and plutonium breeding were slowly phased out, the nuclear legacy left an indelible mark on the Tri-Cities. Since World War II, the area had developed from a small farming community to a booming "Atomic Frontier" to a powerhouse of the nuclear-industrial complex. Decades of federal investment created a community of highly skilled scientists and engineers. As a result of this concentration of specialized skills, the Hanford Site was able to diversify its operations to include scientific research, test facilities, and commercial nuclear power production.
, operational facilities located at the Hanford Site include:
The Department of Energy and its contractors offer tours of the site. Sixty public tours, each five hours long, were planned for 2009. The tours are free, require advance reservation via the department's web site, and are limited to U.S. citizens at least 18 years of age.
Environmental concerns.
A huge volume of water from the Columbia River was required to dissipate the heat produced by Hanford's nuclear reactors. From 1944 to 1971, pump systems drew cooling water from the river and, after treating this water for use by the reactors, returned it to the river. Before its release into the river, the used water was held in large tanks known as retention basins for up to six hours. Longer-lived isotopes were not affected by this retention, and several terabecquerels entered the river every day. The federal government kept knowledge about these radioactive releases secret. Radiation was later measured 200 miles downstream as far west as the Washington and Oregon coasts.
The plutonium separation process resulted in the release of radioactive isotopes into the air, which were carried by the wind throughout southeastern Washington and into parts of Idaho, Montana, Oregon, and British Columbia. Downwinders were exposed to radionuclides, particularly iodine-131, with the heaviest releases during the period from 1945 to 1951. These radionuclides entered the food chain via dairy cows grazing on contaminated fields; hazardous fallout was ingested by communities who consumed radioactive food and milk. Most of these airborne releases were a part of Hanford's routine operations, while a few of the larger releases occurred in isolated incidents. In 1949, an intentional release known as the "Green Run" released 8,000 curies of iodine-131 over two days. Another source of contaminated food came from Columbia River fish, an impact felt disproportionately by Native American communities who depended on the river for their customary diets. A U.S. government report released in 1992 estimated that 685,000 curies of radioactive iodine-131 had been released into the river and air from the Hanford site between 1944 and 1947.
Beginning in the 1960s, scientists with the U.S. Public Health Service published reports about radioactivity released from Hanford, and there were protests from the health departments of Oregon and Washington. In response to an article in the Spokane Spokesman Review in September 1985, the Department of Energy announced to declassify environmental records and, in February 1986, released 19,000 pages of previously unavailable historical documents about Hanford's operations. The Washington State Department of Health collaborated with the citizen-led Hanford Health Information Network (HHIN) to publicize data about the health effects of Hanford's operations. HHIN reports concluded that residents who lived downwind from Hanford or who used the Columbia River downstream were exposed to elevated doses of radiation that placed them at increased risk for various cancers and other diseases. A mass tort lawsuit brought by two thousand Hanford downwinders against the federal government has been in the court system for many years. Two of six plaintiffs who went to trial in 2005 were awarded $500,000 in damages.
On February 15, 2013, Governor Jay Inslee announced that a tank storing radioactive waste at the site had been leaking liquids on average of 150 to 300 gallons per year. He said that the leak posed no immediate health risk to the public, but said that should not be an excuse for not doing anything. On February 22, 2013, the Governor stated that "6 more tanks at Hanford site" were "leaking radioactive waste" , there are 177 tanks at Hanford, 149 of which have a single shell. Historically single shell tanks were used for storing radioactive liquid waste and designed to last 20 years. By 2005, some liquid waste was transferred from single shell tanks to (safer) double shell tanks. A substantial amount of residue remains in the older single shell tanks with one containing an estimated 447,000 gallons of radioactive sludge, for example. It is believed that up to six of these "empty" tanks are leaking. Two tanks are reportedly leaking at a rate of 300 gallons (1,136 liters) per year each, while the remaining four tanks are leaking at a rate of 15 gallons (57 liters) per year each.
Since 2003, radioactive materials are known to be leaking from Hanford into the environment. "The highest tritium concentration detected in riverbank springs during 2002 was 58,000 pCi/L (2,100 Bq/L) at the Hanford Townsite. The highest iodine-129 concentration of 0.19 pCi/L (0.007 Bq/L) was also found in a Hanford Townsite spring. The WHO guidelines for radionuclides in drinking-water limits levels of iodine-129 at 1 Bq/L, and tritium at 10,000 Bq/L. Concentrations of radionuclides including tritium, technetium-99, and iodine-129 in riverbank springs near the Hanford Townsite have generally been increasing since 1994. This is an area where a major groundwater plume from the 200 East Area intercepts the river ... Detected radionuclides include strontium-90, technetium-99, iodine-129, uranium-234, −235, and −238, and tritium. Other detected contaminants include arsenic, chromium, chloride, fluoride, nitrate, and sulfate."
Occupational health concerns.
Since 1987, workers have reported exposure to harmful vapors after working around underground nuclear storage tanks, with no solution found. More than 40 workers in 2014 alone reported smelling vapors and became ill with "nosebleeds, headaches, watery eyes, burning skin, contact dermatitis, increased heart rate, difficulty breathing, coughing, sore throats, expectorating, dizziness and nausea, ... Several of these workers have long-term disabilities." Doctors checked workers and cleared them to return to work. Monitors worn by tank workers have found no samples with chemicals close to the federal limit for occupational exposure.
In August 2014, OSHA ordered the facility to rehire a contractor and pay $220,000 in back wages for firing them for whistleblowing on safety concerns at the site.
On November 19, 2014, Washington Attorney General Bob Ferguson said the state planned to sue the DOE and its contractor to protect workers from hazardous vapors at Hanford. A 2014 report by the DOE Savannah River National Laboratory initiated by 'Washington River Protection Solutions' found that DOE's methods to study vapor releases were inadequate, particularly, that they did not account for short but intense vapor releases. They recommended "proactively sampling the air inside tanks to determine its chemical makeup; accelerating new practices to prevent worker exposures; and modifying medical evaluations to reflect how workers are exposed to vapors".
Cleanup era.
On June 25, 1988, the Hanford site was divided into four areas and proposed for inclusion on the National Priorities List. On May 15, 1989, the Washington Department of Ecology, the United States Environmental Protection Agency, and the Department of Energy entered into the Tri-Party Agreement, which provides a legal framework for environmental remediation at Hanford. the agencies are engaged in the world's largest environmental cleanup, with many challenges to be resolved in the face of overlapping technical, political, regulatory, and cultural interests. The cleanup effort is focused on three outcomes: restoring the Columbia River corridor for other uses, converting the central plateau to long-term waste treatment and storage, and preparing for the future. The cleanup effort is managed by the Department of Energy under the oversight of the two regulatory agencies. A citizen-led Hanford Advisory Board provides recommendations from community stakeholders, including local and state governments, regional environmental organizations, business interests, and Native American tribes. Citing the 2014 Hanford Lifecycle Scope Schedule and Cost report, the 2014 estimated cost of the remaining Hanford clean up is $113.6 billion – more than $3 billion per year for the next six years, with a lower cost projection of approximately $2 billion per year until 2046. About 11,000 workers are on site to consolidate, clean up, and mitigate waste, contaminated buildings, and contaminated soil. Originally scheduled to be complete within thirty years, the cleanup was less than half finished by 2008. Of the four areas that were formally listed as Superfund sites on October 4, 1989, only one has been removed from the list following cleanup.
While major releases of radioactive material ended with the reactor shutdown in the 1970s and many of the most dangerous wastes are contained, there are continued concerns about contaminated groundwater headed toward the Columbia River and about workers' health and safety.
The most significant challenge at Hanford is stabilizing the of high-level radioactive waste stored in 177 underground tanks. By 1998, about a third of these tanks had leaked waste into the soil and groundwater. , most of the liquid waste has been transferred to more secure double-shelled tanks; however, of liquid waste, together with of salt cake and sludge, remains in the single-shelled tanks. DOE lacks information about the extent to which the 27 double-shell tanks may be susceptible to corrosion. Without determining the extent to which the factors that contributed to the leak in AY-102 were similar to the other 27 double-shell tanks, DOE cannot be sure how long its double-shell tanks can safely store waste. That waste was originally scheduled to be removed by 2018. , the revised deadline was 2040. Nearby aquifers contain an estimated of contaminated groundwater as a result of the leaks. , of radioactive waste is traveling through the groundwater toward the Columbia River. This waste is expected to reach the river in 12 to 50 years if cleanup does not proceed on schedule. The site includes of solid radioactive waste.
Under the Tri-Party Agreement, lower-level hazardous wastes are buried in huge lined pits that will be sealed and monitored with sophisticated instruments for many years. Disposal of plutonium and other high-level wastes is a more difficult problem that continues to be a subject of intense debate. As an example, plutonium-239 has a half-life of 24,100 years, and a decay of ten half-lives is required before a sample is considered to cease its radioactivity. In 2000, the Department of Energy awarded a $4.3 billion contract to Bechtel, a San Francisco-based construction and engineering firm, to build a vitrification plant to combine the dangerous wastes with glass to render them stable. Construction began in 2002. The plant was originally scheduled to be operational by 2011, with vitrification completed by 2028. , according to a study by the General Accounting Office, there were a number of serious unresolved technical and managerial problems. estimated costs were $13.4 billion with commencement of operations estimated to be in 2022 and about 3 decades of operation.
In May 2007, state and federal officials began closed-door negotiations about the possibility of extending legal cleanup deadlines for waste vitrification in exchange for shifting the focus of the cleanup to urgent priorities, such as groundwater remediation. Those talks stalled in October 2007. In early 2008, a $600 million cut to the Hanford cleanup budget was proposed. Washington state officials expressed concern about the budget cuts, as well as missed deadlines and recent safety lapses at the site, and threatened to file a lawsuit alleging that the Department of Energy is in violation of environmental laws. They appeared to step back from that threat in April 2008 after another meeting of federal and state officials resulted in progress toward a tentative agreement.
During excavations from 2004 to 2007 a sample of purified plutonium was uncovered inside a safe in a waste trench, and has been dated to about the 1940s, making it the second-oldest sample of purified plutonium known to exist. Analyses published in 2009 concluded that the sample originated at Oak Ridge, and was one of several sent to Hanford for optimization tests of the T-Plant until Hanford could produce its own plutonium. Documents refer to such a sample, belonging to "Watt's group", which was disposed of in its safe when a radiation leak was suspected.
Some of the radioactive waste at Hanford was supposed to be stored in the planned Yucca Mountain nuclear waste repository, but after that project was cancelled due to the opposition of citizens of Nevada, Washington State sued. They were joined by South Carolina. Their first suit was dismissed, and second suits have been filed.
A potential radioactive leak was reported in 2013; the clean up was estimated to have cost $40 billion with $115 billion more required.
Hanford organizations.
The Hanford site operations were initially directed by Colonel Franklin Matthias of the U.S. Army Corps of Engineers. Postwar the Atomic Energy Commission took over, and then the Energy Research and Development Administration. Hanford operations are currently directed by the U.S. Department of Energy. It has been operated under government contract by various private companies over the years – the table which follows summarizes the operating contractors through 2000.

</doc>
<doc id="39039" url="https://en.wikipedia.org/wiki?curid=39039" title="Lawrence Livermore National Laboratory">
Lawrence Livermore National Laboratory

Lawrence Livermore National Laboratory (LLNL) is a federal research facility in Livermore, California, founded by the University of California in 1952. A Federally Funded Research and Development Center (FFRDC), it is primarily funded by the United States Department of Energy (DOE) and managed and operated by Lawrence Livermore National Security, LLC (LLNS), a partnership of the University of California, Bechtel, Babcock & Wilcox, URS, and Battelle Memorial Institute in affiliation with the Texas A&M University System. The laboratory was honored in 2012 by having the synthetic chemical element livermorium named after it.
Background.
LLNL is self-described as "a premier research and development institution for science and technology applied to national security." Its principal responsibility is ensuring the safety, security and reliability of the nation’s nuclear weapons through the application of advanced science, engineering and technology. The Laboratory also applies its special expertise and multidisciplinary capabilities to preventing the proliferation and use of weapons of mass destruction, bolstering homeland security and solving other nationally important problems, including energy and environmental security, basic science and economic competitiveness.
LLNL is home to many unique facilities and a number of the most powerful computer systems in the world, according to the TOP500 list, including Blue Gene/L, the world's fastest computer from 2004 until Los Alamos National Laboratory's IBM Roadrunner supercomputer surpassed it in 2008. On June 18, 2012, LLNL re-took the lead on the latest edition of the list of the world’s Top 500 supercomputers with IBM Sequoia, a 16.32 petaflops system packing more than 1.5 million custom Power cores. As of 2015, there are two computer systems that are faster. China's Tianhe-2 and Oak Ridge National Lab's Titan. It is based on the same IBM BlueGene/Q architecture used in three other top ten systems which also were the most power efficient on the list. Since 1978, LLNL has received a total of 118 R&D 100 Awards, including five in 2007. The awards are given annually by the editors of "R&D Magazine" to the most innovative ideas of the year.
The Laboratory is located on a one-square-mile (2.6 km2) site at the eastern edge of Livermore. It also operates a remote experimental test site, called Site 300, situated about southeast of the main lab site. LLNL has an annual budget of about $1.5 billion and a staff of roughly 5,800 employees.
Origins.
LLNL was established in 1952 as the University of California Radiation Laboratory at Livermore, an offshoot of the existing UC Radiation Laboratory at Berkeley. It was intended to spur innovation and provide competition to the nuclear weapon design laboratory at Los Alamos in New Mexico, home of the Manhattan Project that developed the first atomic weapons. Edward Teller and Ernest Lawrence, director of the Radiation Laboratory at Berkeley, are regarded as the co-founders of the Livermore facility.
The new laboratory was sited at a former naval air station of World War II. It was already home to several UC Radiation Laboratory projects that were too large for its location in the hills above the Berkeley campus, including one of the first experiments in the magnetic approach to confined thermonuclear reactions (i.e. fusion). About half an hour southeast of Berkeley, the Livermore site provided much greater security for classified projects than an urban university campus.
Lawrence tapped 32-year-old Herbert York, a former graduate student of his, to run Livermore. Under York, the Lab had four main programs: Project Sherwood (the Magnetic Fusion Program), Project Whitney (the weapons design program), diagnostic weapon experiments (both for the Los Alamos and Livermore laboratories), and a basic physics program. York and the new lab embraced the Lawrence "big science" approach, tackling challenging projects with physicists, chemists, engineers, and computational scientists working together in multidisciplinary teams.
Lawrence died in August 1958 and shortly after, the university's board of regents named both laboratories for him, as the Lawrence Radiation Laboratory.
Historically, the Berkeley and Livermore laboratories have had very close relationships on research projects, business operations and staff. The Livermore Lab was established initially as a branch of the Berkeley Laboratory. The Livermore Lab was not officially severed administratively from the Berkeley Lab until 1971. To this day, in official planning documents and records, Lawrence Berkeley National Laboratory is designated as Site 100, Lawrence Livermore National Lab as Site 200, and LLNL's remote test location as Site 300.
The laboratory was renamed Lawrence Livermore Laboratory (LLL) in 1971. On October 1, 2007 LLNS assumed management of LLNL from the University of California, which had exclusively managed and operated the Laboratory since its inception 55 years before. The laboratory was honored in 2012 by having the synthetic chemical element livermorium named after it. The LLNS takeover of the Laboratory has been controversial. In May 2013, an Alameda County jury awarded over $2.7 million to five former Laboratory employees who were among 430 employees LLNS laid off during 2008. The jury found that LLNS breached a contractual obligation to terminate the employees only for “reasonable cause.” The five plaintiffs also have pending age discrimination claims against LLNS, which will be heard by a different jury in a separate trial. There are 125 co-plaintiffs awaiting trial on similar claims against LLNS. The May 2008 layoff was the first layoff at the Laboratory in nearly 40 years.
On March 14, 2011, the City of Livermore officially expanded the city's boundaries to annex LLNL and move it within the city limits. The unanimous vote by the Livermore City Council expanded Livermore’s southeastern boundaries to cover 15 land parcels covering that comprise the LLNL site. Prior to this, the site was in an unincorporated area of Alameda County. The LLNL campus continues to be owned by the federal government.
Nuclear weapons projects.
From its inception, Livermore focused on innovative weapon design concepts; as a result, its first three nuclear tests were unsuccessful. The lab persevered and its subsequent designs proved increasingly successful. In 1957, the Livermore Lab was selected to develop the warhead for the Navy's Polaris missile. This warhead required numerous innovations to fit a nuclear warhead into the relatively small confines of the missile nosecone.
During the Cold War, scores of Livermore-designed warheads entered service. These were used in missiles ranging in size from the Lance surface-to-surface tactical missile to the megaton-class Spartan antiballistic missile. Over the years, LLNL designed the following warheads: W27 (Regulus cruise missile; 1955; joint with Los Alamos), W38 (Atlas/Titan ICBM; 1959), B41 (B52 bomb; 1957), W45 (Little John/Terrier missiles; 1956), W47 (Polaris SLBM; 1957), W48 (155-mm howitzer; 1957), W55 (submarine rocket; 1959), W56 (Minuteman ICBM; 1960), W58 (Polaris SLBM; 1960), W62 (Minuteman ICBM; 1964), W68 (Poseidon SLBM; 1966), W70 (Lance missile; 1969), W71 (Spartan missile; 1968), W79 (8-in. artillery gun; 1975), W82 (155-mm howitzer; 1978), B83 (modern strategic bomb; 1979), and W87 (Peacekeeper/MX ICBM; 1982). The W87, and the B83 are the only LLNL designs still in the U.S. nuclear stockpile.
With the collapse of the Soviet Union in 1991 and the end of the Cold War, the United States began a moratorium on nuclear testing and development of new nuclear weapon designs. To sustain existing warheads for the indefinite future, a science-based Stockpile Stewardship Program (SSP) was defined that emphasized the development and application of greatly improved technical capabilities to assess the safety, security, and reliability of existing nuclear warheads without the use of nuclear testing. Confidence in the performance of weapons, without nuclear testing, is maintained through an ongoing process of stockpile surveillance, assessment and certification, and refurbishment or weapon replacement.
With no new designs of nuclear weapons, the warheads in the U.S. stockpile must continue to function far past their original expected lifetimes. As components and materials age, problems can arise. Stockpile Life Extension Programs can extend system lifetimes, but they also can introduce performance uncertainties and require maintenance of outdated technologies and materials. Because there is concern that it will become increasingly difficult to maintain high confidence in the current warheads for the long term, the Department of Energy/National Nuclear Security Administration initiated the Reliable Replacement Warhead (RRW) Program. RRW designs could reduce uncertainties, ease maintenance demands, and enhance safety and security. In March 2007, the LLNL design was chosen for the Reliable Replacement Warhead. Since that time, Congress has not allocated funding for any further development of the RRW.
The Livermore Action Group organized many mass protests, from 1981 to 1984, against nuclear weapons which were being produced by the Lawrence Livermore National Laboratory. Peace activists Ken Nightingale and Eldred Schneider were involved. On June 22, 1982, more than 1,300 anti-nuclear protesters were arrested in a non-violent demonstration. More recently, there has been an annual protest against nuclear weapons research at Lawrence Livermore. In August 2003, 1,000 people protested at Livermore Labs against "new-generation nuclear warheads". In the 2007 protest, 64 people were arrested. More than 80 people were arrested in March 2008 while protesting at the gates. 31 people were arrested in August 2013 during a protest marking the 68th anniversary of the atomic bombings of Hiroshima and Nagasaki, including famous whistle blower and author of the Pentagon Papers, Daniel Ellsberg.
In the 1980s, Lawrence's widow petitioned the Regents of the University of California on several occasions to remove her husband's name from the Livermore laboratory, due to its focus on nuclear weapons. She outlived her husband by more than 44 years and died in Walnut Creek at the age of 92 in January 2003.
Plutonium research.
LLNL conducts research into the properties and behavior of plutonium to learn how plutonium performs as it ages and how it behaves under high pressure (e.g., with the impact of high explosives). Plutonium has seven temperature-dependent solid allotropes. Each possesses a different density and crystal structure. Alloys of plutonium are even more complex; multiple phases can be present in a sample at any given time. Experiments are being conducted at LLNL and elsewhere to measure the structural, electrical and chemical properties of plutonium and its alloys and to determine how these materials change over time. Such measurements will enable scientists to better model and predict plutonium's long-term behavior in the aging stockpile.
The Lab’s plutonium research is conducted in a specially designed, ultra-safe, and highly secure facility called the SuperBlock. Work with highly enriched uranium is also conducted here. In March 2008, the National Nuclear Security Administration (NNSA) presented its preferred alternative for the transformation of the nation’s nuclear weapons complex. Under this plan, LLNL would be a center of excellence for nuclear design and engineering, a center of excellence for high explosive research and development, and a science magnet in high-energy-density (i.e., laser) physics. In addition, most of its special nuclear material would be removed and consolidated at a more central, yet-to-be-named site.
On September 30, 2009, the NNSA announced that about two thirds of the special nuclear material (e.g., plutonium) at LLNL requiring the highest level of security protection had been removed from LLNL. The move was part of NNSA's efforts initiated in October 2006 to consolidate special nuclear material at five sites by 2012, with significantly reduced square footage at those sites by 2017. The federally mandated project intends to improve security and reduce security costs, and is part of NNSA's overall effort to transform the Cold War era "nuclear weapons" enterprise into a 21st-century "nuclear security" enterprise. The original date to remove all high-security nuclear material from LLNL, based on equipment capability and capacity, was 2014. NNSA and LLNL developed a timeline to remove this material as early as possible, accelerating the target completion date to 2012.
Global security program.
The Lab’s work in global security aims to reduce and mitigate the dangers posed by the spread or use of weapons of mass destruction and by threats to energy and environmental security. Livermore has been working on global security and homeland security for decades, predating both the collapse of the Soviet Union in 1991 and the September 11, 2001, terrorist attacks. LLNL staff have been heavily involved in the cooperative nonproliferation programs with Russia to secure at-risk weapons materials and assist former weapons workers in developing peaceful applications and self-sustaining job opportunities for their expertise and technologies. In the mid-1990s, Lab scientists began efforts to devise improved biodetection capabilities, leading to miniaturized and autonomous instruments that can detect biothreat agents in a few minutes instead of the days to weeks previously required for DNA analysis.
Today, Livermore researchers address the full spectrum of threats – radiological/nuclear, chemical, biological, explosives, and cyber. They combine physical and life sciences, engineering, computations, and analysis to develop technologies that solve real-world problems. Activities are grouped into five programs:
Other programs.
LLNL supports capabilities in a broad range of scientific and technical disciplines, applying current capabilities to existing programs and developing new science and technologies to meet future national needs.
Lawrence Livermore National Laboratory has worked out several energy technologies in the field of coal gasification, shale oil extraction, geothermal energy, advanced battery research, solar energy, and fusion energy. Main oil shale processing technologies worked out by the Lawrence Livermore National Laboratory are LLNL HRS (hot-recycled-solid), LLNL RISE ("in situ" extraction technology) and LLNL radiofrequency technologies.
Key accomplishments.
Over its 60-year history, Lawrence Livermore has made many scientific and technological achievements, including:
On July 17, 2009 LLNL announced that the Laboratory had received eight R&D 100 Awards – more than it had ever received in the annual competition. The previous LLNL record of seven awards was reached five times – in 1987, 1988, 1997, 1998 and 2006.
Also known as the “Oscars of invention”, the awards are given each year for the development of cutting-edge scientific and engineering technologies with commercial potential.
The awards raise LLNL’s total number of awards since 1978 to 129. The winning technologies were:
Largest computers.
Throughout its history, LLNL has been a leader in computers and scientific computing. Even before the Livermore Lab opened its doors, E.O. Lawrence and Edward Teller recognized the importance of computing and the potential of computational simulation. Their purchase of one of the first UNIVAC computers, set the precedent for LLNL’s history of acquiring and exploiting the fastest and most capable supercomputers in the world. A succession of increasingly powerful and fast computers have been used at the Lab over the years:
The November 2007 release of the 30th TOP500 list of the 500 most powerful computer systems in the world, has LLNL’s Blue Gene/L computer in first place for the seventh consecutive time. Five other LLNL computers are in the top 100. The November 2008 release of the TOP500 list places the Blue Gene/L supercomputer behind the Pleiades supercomputer in NASA/Ames Research Center, the Jaguar supercomputer in Oak Ridge National Laboratory, and the IBM Roadrunner supercomputer in Los Alamos National Laboratory. Currently, the Blue Gene/L computer can sustain 478.2 trillion operations per second, with a peak of 596.4 trillion operations per second.
On June 22, 2006, researchers at LLNL announced that they had devised a scientific software application that sustained 207.3 trillion operations per second. The record performance was made at LLNL on Blue Gene/L, the world's fastest supercomputer with 131,072 processors. The record was a milestone in the evolution of predictive science, a field in which researchers use supercomputers to answer questions about such subjects as: materials science simulations, global warming, and reactions to natural disasters.
LLNL has a long history of developing computing software and systems. Initially, there was no commercially available software, and computer manufacturers considered it the customer’s responsibility to develop their own. Users of the early computers had to write not only the codes to solve their technical problems, but also the routines to run the machines themselves. Today, LLNL computer scientists focus on creating the highly complex physics models, visualization codes, and other unique applications tailored to specific research requirements. A great deal of software also has been written by LLNL personnel to optimize the operation and management of the computer systems, including operating system extensions such as CHAOS (Linux Clustering) and resource management packages such as SLURM. The Peloton procurements in late 2006 (Atlas and other computers) were the first in which a commercial resource management package, Moab, was used to manage the clusters.
Livermore Valley Open Campus (LVOC).
In August 2009 a joint venture was announced between Sandia National Laboratories/California campus and LLNL to create an open, unclassified research and development space called the Livermore Valley Open Campus (LVOC). The motivation for the LVOC stems from current and future national security challenges that require increased coupling to the private sector to understand threats and deploy solutions in areas such as high performance computing, energy and environmental security, cyber security, economic security, and non-proliferation.
The LVOC is modeled after research and development campuses found at major industrial research parks and other U.S. Department of Energy laboratories with campus-like security, a set of business and operating rules devised to enhance and accelerate international scientific collaboration and partnerships with U.S. government agencies, industry and academia. Ultimately, the LVOC will consist of an approximately 110-acre parcel along the eastern edge of the Livermore Laboratory and Sandia sites, and will house additional conference space, collaboration facilities and a visitor's center to support educational and research activities.
Objectives of LVOC
Initial research areas for the LVOC:
The architecture of the LVOC is planned in stages; first steps including:
Sponsors.
LLNL's principal sponsor is the Department of Energy/National Nuclear Security Administration (DOE/NNSA) Office of Defense Programs, which supports its stockpile stewardship and advanced scientific computing programs. Funding to support LLNL's global security and homeland security work comes from the DOE/NNSA Office of Defense Nuclear Nonproliferation as well as the Department of Homeland Security. LLNL also receives funding from DOE’s Office of Science, Office of Civilian Radioactive Waste Management, and Office of Nuclear Energy. In addition, LLNL conducts work-for-others research and development for various Defense Department sponsors, other federal agencies, including NASA, Nuclear Regulatory Commission (NRC), National Institutes of Health, and Environmental Protection Agency, a number of California State agencies, and private industry.
Budget.
For Fiscal Year 2009 LLNL spent $1.497 billion on research and laboratory operations activities:
Research/Science Budget:
Site Management/Operations Budget:
Directors.
The LLNL Director is appointed by the Board of Governors of Lawrence Livermore National Security, LLC (LLNS) and reports to the board. The Laboratory Director also serves as the President of LLNS. Over the course of its history, the following eminent scientists have served as LLNL Director:
Organization.
The LLNL Director is supported by a senior executive team consisting of the Deputy Director, the Deputy Director for Science and Technology, Principal Associate Directors, and other senior executives who manage areas/functions directly reporting to the Laboratory Director.
The Directors Office is organized into these functional areas/offices:
The Laboratory is organized into four principal directorates, each headed by a Principal Associate Director:
Three other directorates are each headed by an Associate Director who reports to the LLNL Director:
Corporate management.
The LLNL Director reports to the Lawrence Livermore National Security, LLC (LLNS) Board of Governors, a group of key scientific, academic, national security and business leaders from the LLNS partner companies that jointly own and control LLNS. The LLNS Board of Governors has a total of 16 positions, with six of these Governors constituting an Executive Committee. All decisions of the Board are made by the Governors on the Executive Committee. The other Governors are advisory to the Executive Committee and do not have voting rights.
The University of California is entitled to appoint three Governors to the Executive Committee, including the Chair. Bechtel is also entitled to appoint three Governors to the Executive Committee, including the Vice Chair. One of the Bechtel Governors must be a representative of Babcock & Wilcox (B&W) or the Washington Division of URS Corporation (URS), who is nominated jointly by B&W and URS each year, and who must be approved and appointed by Bechtel. The Executive Committee has a seventh Governor who is appointed by Battelle; they are non-voting and advisory to the Executive Committee. The remaining Board positions are known as Independent Governors (also referred to as Outside Governors), and are selected from among individuals, preferably of national stature, and can not be employees or officers of the partner companies.
The University of California-appointed Chair has tie-breaking authority over most decisions of the Executive Committee. The Board of Governors is the ultimate governing body of LLNS and is charged with overseeing the affairs of LLNS in its operations and management of LLNL.
LLNS managers and employees who work at LLNL, up to and including the President/Laboratory Director, are generally referred to as Laboratory Employees. All Laboratory Employees report directly or indirectly to the LLNS President. While most of the work performed by LLNL is funded by the federal government, Laboratory employees are paid by LLNS which is responsible for all aspects of their employment including providing health care benefits and retirement programs.
Within the Board of Governors, authority resides in the Executive Committee to exercise all rights, powers, and authorities of LLNS, excepting only certain decisions that are reserved to the parent companies. The LLNS Executive Committee is free to appoint officers or other managers of LLNS and LLNL, and may delegate its authorities as it deems appropriate to such officers, employees, or other representatives of LLNS/LLNL. The Executive Committee may also retain auditors, attorneys, or other professionals as necessary. For the most part the Executive Committee has appointed senior managers at LLNL as the primary officers of LLNS. As a practical matter most operational decisions are delegated to the President of LLNS, who is also the Laboratory Director. The positions of President/Laboratory Director and Deputy Laboratory Director are filled by joint action of the Chair and Vice Chair of the Executive Committee, with the University of California nominating the President/Laboratory Director and Bechtel nominating the Deputy Laboratory Director.
The current LLNS Chairman is Norman J. Pattiz - founder and chairman of Westwood One, America's largest radio network, and he also currently serves on the Board of Regent of the University of California. The Vice Chairman is J. Scott Ogilvie - president of Bechtel Systems & Infrastructure, Inc., he serves on the Board of Directors of Bechtel Group, Inc. (BGI) and on the BGI Audit Committee.
Public Protests.
The Livermore Action Group organized many mass protests, from 1981 to 1984, against nuclear weapons which were being produced by the Lawrence Livermore National Laboratory. Peace activists Ken Nightingale and Eldred Schneider were involved. On June 22, 1982, more than 1,300 anti-nuclear protesters were arrested in a nonviolent demonstration. More recently, there has been an annual protest against nuclear weapons research at Lawrence Livermore. In August 2003, 1,000 people protested at Livermore Labs against "new-generation nuclear warheads". In the 2007 protest, 64 people were arrested. More than 80 people were arrested in March 2008 while protesting at the gates.

</doc>
<doc id="39040" url="https://en.wikipedia.org/wiki?curid=39040" title="Ernest Lawrence">
Ernest Lawrence

Ernest Orlando Lawrence (August 8, 1901 – August 27, 1958) was a pioneering American nuclear scientist and winner of the Nobel Prize in Physics in 1939 for his invention of the cyclotron. He is known for his work on uranium-isotope separation for the Manhattan Project, for founding the Lawrence Berkeley National Laboratory and the Lawrence Livermore National Laboratory.
A graduate of the University of South Dakota and University of Minnesota, Lawrence obtained a PhD in physics at Yale in 1925. In 1928, he was hired as an associate professor of physics at the University of California, becoming the youngest full professor there two years later. In its library one evening, Lawrence was intrigued by a diagram of an accelerator that produced high-energy particles. He contemplated how it could be made compact, and came up with an idea for a circular accelerating chamber between the poles of an electromagnet. The result was the first cyclotron.
Lawrence went on to build a series of ever larger and more expensive cyclotrons. His Radiation Laboratory became an official department of the University of California in 1936, with Lawrence as its director. In addition to the use of the cyclotron for physics, Lawrence also supported its use in research into medical uses of radioisotopes. During World War II, Lawrence developed electromagnetic isotope separation at the Radiation Laboratory. It used devices known as calutrons, a hybrid of the standard laboratory mass spectrometer and cyclotron. A huge electromagnetic separation plant was built at Oak Ridge, Tennessee, which came to be called Y-12. The process was inefficient, but it worked.
After the war, Lawrence campaigned extensively for government sponsorship of large scientific programs, and was a forceful advocate of "Big Science", with its requirements for big machines and big money. Lawrence strongly backed Edward Teller's campaign for a second nuclear weapons laboratory, which Lawrence located in Livermore, California. After his death, the Regents of the University of California renamed the Lawrence Livermore National Laboratory and Lawrence Berkeley National Laboratory after him. Chemical element number 103 was named lawrencium in his honor after its discovery at Berkeley in 1961.
Early life.
Ernest Orlando Lawrence was born in Canton, South Dakota on August 8, 1901. His parents, Carl Gustavus and Gunda (née Jacobson) Lawrence, were both the offspring of Norwegian immigrants who had met while teaching at the high school in Canton, where his father was also the superintendent of schools. He had a younger brother, John H. Lawrence, who would become a physician, and was a pioneer in the field of nuclear medicine. Growing up, his best friend was Merle Tuve, who would also go on to become a highly accomplished nuclear physicist.
Lawrence attended the public schools of Canton and Pierre, then enrolled at St. Olaf College in Northfield, Minnesota, but transferred after a year to the University of South Dakota in Vermillion. He completed his bachelor's degree in chemistry in 1922, and his Master of Arts (M.A.) degree in physics from the University of Minnesota in 1923 under the supervision of William Francis Gray Swann. For his master's thesis, Lawrence built an experimental apparatus that rotated an ellipsoid through a magnetic field.
Lawrence followed Swann to the University of Chicago, and then to Yale University in New Haven, Connecticut, where Lawrence completed his Doctor of Philosophy (Ph.D.) degree in physics in 1925 as a Sloane Fellow, writing his doctoral thesis on the photoelectric effect in potassium vapor. He was elected a member of Sigma Xi, and, on Swann's recommendation, received a National Research Council fellowship. Instead of using it to travel to Europe, as was customary at the time, he remained at Yale University with Swann as a researcher.
With Jesse Beams from the University of Virginia, Lawrence continued to research the photoelectric effect. They showed that photoelectrons appeared within 2 x 10−9 seconds of the photons striking the photoelectric surface—close to the limit of measurement at the time. Reducing the emission time by switching the light source on and off rapidly made the spectrum of energy emitted broader, in conformance with Werner Heisenberg's uncertainty principle.
Early career.
In 1926 and 1927, Lawrence received offers of assistant professorships from the University of Washington in Seattle and the University of California at a salary of $3,500 per annum. Yale promptly matched the offer of the assistant professorship, but at a salary of $3,000. Lawrence chose to stay at the more prestigious Yale, but because he had never been an instructor, the appointment was resented by some of his fellow faculty, and in the eyes of many it still did not compensate for his South Dakota immigrant background.
Lawrence was hired as an associate professor of physics at the University of California in 1928, and two years later became a full professor, becoming the university's youngest professor. Robert Gordon Sproul, who became university president the day after Lawrence became a professor, was a member of the Bohemian Club, and he sponsored Lawrence's membership in 1932. Through this club, Lawrence met William Henry Crocker, Edwin Pauley, and John Francis Neylan. They were influential men who helped him obtain money for his energetic nuclear particle investigations. There was great hope for medical uses to come from the development of particle physics, and this led to much of the early funding for advances Lawrence was able to obtain.
While at Yale, Lawrence met Mary Kimberly (Molly) Blumer, the eldest of four daughters of George Blumer, the dean of the Yale School of Medicine. They first met in 1926 and became engaged in 1931, and were married on May 14, 1932, at Trinity Church on the Green in New Haven, Connecticut. They had six children: Eric, Margaret, Mary, Robert, Barbara, and Susan. Lawrence named his son Robert after theoretical physicist Robert Oppenheimer, his closest friend in Berkeley. In 1941, Molly's sister Elsie married Edwin McMillan, who would go on to win the Nobel Prize in Chemistry in 1951.
The development of the cyclotron.
Invention.
The invention that brought Lawrence to international fame started out as a sketch on a scrap of a paper napkin. While sitting in the library one evening in 1929, Lawrence glanced over a journal article by Rolf Widerøe, and was intrigued by one of the diagrams. This depicted a device that produced high-energy particles by means of a succession of small "pushes". The device depicted was laid out in a straight line using increasingly longer electrodes. At the time, physicists were beginning to explore the atomic nucleus. In 1919, the New Zealand physicist Ernest Rutherford had fired alpha particles into nitrogen and had succeeded in knocking protons out of some of the nuclei. But nuclei have a positive charge that repels other positively charged nuclei, and they are bound together tightly by a force that physicists were only just beginning to understand. To break them up, to disintegrate them, would require much higher energies, of the order of millions of volts.
Lawrence saw that such a particle accelerator would soon become too long and unwieldy for his university laboratory. In pondering a way to make the accelerator more compact, Lawrence decided to set a circular accelerating chamber between the poles of an electromagnet. The magnetic field would hold the charged protons in a spiral path as they were accelerated between just two semicircular electrodes connected to an alternating potential. After a hundred turns or so, the protons would impact the target as a beam of high-energy particles. Lawrence excitedly told his colleagues that he had discovered a method for obtaining particles of very high energy without the use of any high voltage. He initially worked with Niels Edlefsen. Their first cyclotron was made out of brass, wire, and sealing wax and was only four inches (10 cm) in diameter—it could literally be held in one hand, and probably cost $25 in all.
What Lawrence needed to develop the idea was capable graduate students to do the work. Edlefsen left to take up an assistant professorship in September 1930, and Lawrence replaced him with David H. Sloan and M. Stanley Livingston, who he set to work on developing Widerøe's accelerator and Edlefsen's cyclotron, respectively. Both had their own financial support. Both designs proved practical, and by May 1931, Sloan's linear accelerator was able to accelerate ions to 1 MeV. Livingston had a greater technical challenge, but when he applied 1,800 V to his 11-inch cyclotron on January 2, 1931, he got 80,000-electron volt protons spinning around. A week later, he had 1.22 MeV with 3,000 V, more than enough for his PhD thesis on its construction.
Development.
In what would become a recurring pattern, as soon as there was the first sign of success, Lawrence started planning a new, bigger machine. Lawrence and Livingston drew up a design for a cyclotron in early 1932. The magnet for the $800 11-inch cyclotron weighed 2 tons, but Lawrence found a massive 80-ton magnet rusting in a junkyard in Palo Alto for the 27-inch that had originally been built during World War I to power a transatlantic radio link. In the cyclotron, he had a powerful scientific instrument, but this did not translate into scientific discovery. In April 1932, John Cockcroft and Ernest Walton at the Cavendish Laboratory in England announced that they had bombarded lithium with protons and succeeded in transmuting it into helium. The energy required turned out to be quite low—well within the capability of the 11-inch cyclotron. On learning about it, Lawrence sent a wire to Berkeley and asked for Cockcroft and Walton's results to be verified. It took the team until September to do so, mainly due to lack of adequate detection apparatus.
Although important discoveries continued to elude Lawrence's Radiation Laboratory, mainly due to its focus on the development of the cyclotron rather than its scientific use, through his increasingly larger machines, Lawrence was able to provide crucial equipment needed for experiments in high energy physics. Around this device, he built what became the world's foremost laboratory for the new field of nuclear physics research in the 1930s. He received a patent for the cyclotron in 1934, which he assigned to the Research Corporation, a private foundation that funded much of Lawrence's early work.
In February 1936, Harvard University's president, James B. Conant, made attractive offers to Lawrence and Oppenheimer. The University of California's president, Robert Gordon Sproul, responded by improving conditions. The Radiation Laboratory became an official department of the University of California on July 1, 1936, with Lawrence formally appointed its director, with a full-time assistant director, and the University agreed to make $20,000 a year available for its research activities. Lawrence employed a simple business model: "He staffed his laboratory with graduate students and junior faculty of the physics department, with fresh Ph.D.s willing to work for anything, and with fellowship holders and wealthy guests able to serve for nothing."
Reception.
Using the new 27-inch cyclotron, the team at Berkeley discovered that every element that they bombarded with recently discovered deuterium emitted energy, and in the same range. They therefore postulated the existence of a new and hitherto unknown particle that was a possible source of limitless energy. William Laurence of "The New York Times" described Lawrence as "a new miracle worker of science". At Cockroft's invitation, Lawrence attended the 1933 Solvay Conference in Belgium. This was a regular gathering of the world's top physicists. Nearly all were from Europe, but occasionally an outstanding American scientist like Robert A. Millikan or Arthur Compton would be invited to attend. Lawrence was asked to give a presentation on the cyclotron. Lawrence's claims of limitless energy met a very different reception in Solvay. He ran into withering skepticism from the Cavendish Laboratory's James Chadwick, the physicist who had discovered the neutron in 1932, for which he had been awarded the Nobel Prize in 1935. To Chadwick, what Lawrence was doing was not Big Science but Bad Science. In a British accent that sounded condescending to American ears, Chadwick suggested that what Lawrence's team was observing was contamination of their apparatus.
When he returned to Berkeley, Lawrence mobilized his team to go painstakingly over the results to gather enough evidence to convince Chadwick. Meanwhile, at the Cavendish laboratory, Rutherford and Mark Oliphant found that deuterium fuses to form helium-3, which causes the effect that the cyclotroneers had observed. Not only was Chadwick correct in that they had been observing contamination, but they had overlooked yet another important discovery, that of nuclear fusion. Lawrence's response was to press on with the creation of still larger cyclotrons. The 27-inch cyclotron was superseded by a 37-inch cyclotron in June 1937, which in turn was superseded by a 60-inch cyclotron in May 1939. It was used to bombard iron and produced its first radioactive isotopes in June.
As it was easier to raise money for medical purposes, particularly cancer treatment, than for nuclear physics, Lawrence encouraged the use of the cyclotron for medical research. Working with his brother John and Israel Lyon Chaikoff from the University of California's Physiology Department, Lawrence supported research into the use of radioactive isotopes for therapeutic purposes. Phosphorus-32 was easily produced in the cyclotron, and John used it to cure a woman afflicted with polycythemia vera, a blood disease. John used phosphorus-32 created in the 37-inch cyclotron in 1938 in tests on mice with leukemia. He found that the radioactive phosphorus concentrated in the fast-growing cancer cells. This then led to clinical trials on human patients. A 1948 evaluation of the therapy showed that remissions occurred under certain circumstances. Lawrence also had hoped for the medical use of neutrons. The first cancer patient received neutron therapy from the 60-inch cyclotron on November 20. Chaikoff conducted trials on the use of radioactive isotopes as radioactive tracers to explore the mechanism of biochemical reactions.
Lawrence was awarded the Nobel Prize in Physics in November 1939 "for the invention and development of the cyclotron and for results obtained with it, especially with regard to artificial radioactive elements". He was the first at Berkeley as well as the first South Dakotan to become a Nobel Laureate, and the first to be so honored while at a state-supported university. The Nobel award ceremony was held on February 29, 1940, in Berkeley, California, due to World War II, in the auditorium of Wheeler Hall on the campus of the university. Lawrence received his medal from Carl E. Wallerstedt, Sweden's Consul General in San Francisco. Robert W. Wood wrote to Lawrence and presciently noted "As you are laying the foundations for the cataclysmic explosion of uranium ... I'm sure old Nobel would approve."
In March 1940, Arthur Compton, Vannevar Bush, James B. Conant, Karl T. Compton, and Alfred Lee Loomis traveled to Berkeley to discuss Lawrence's proposal for a 184-inch cyclotron with a 4,500-ton magnet that was estimated to cost $2.65 million. The Rockefeller Foundation put up $1.15 million to get the project started.
World War II and the Manhattan Project.
Radiation Laboratory.
After the outbreak of World War II in Europe, Lawrence became drawn into military projects. He helped recruit staff for the MIT Radiation Laboratory, where American physicists developed the cavity magnetron invented by Oliphant's team in Britain. The name of the new laboratory was deliberately copied from Lawrence's laboratory in Berkeley for security reasons. He also became involved in recruiting staff for underwater sound laboratories to develop techniques for detecting German submarines. Meanwhile, work continued at Berkeley with cyclotrons. In December 1940, Glenn T. Seaborg and Emilio Segré used the cyclotron to bombard uranium-238 with deuterons producing a new element, neptunium-238, which decayed by beta emission to form plutonium-238. The discovery of plutonium was kept secret until a year after the end of World War II, after the discovery that one of its isotopes, plutonium-239, could undergo nuclear fission in a way that might be useful in an atomic bomb.
Lawrence offered Segrè a job as a research assistant—a relatively lowly position for someone who had discovered an element—for US$300 a month for six months. However, when Lawrence learned that Segrè was legally trapped in California, he reduced Segrè's salary to US$116 a month. When the regents of the University of California wanted to terminate Segré's employment owing to his foreign nationality, Lawrence managed to retain Segré by hiring him as a part-time lecturer paid by the Rockefeller Foundation. Similar arrangements were made to retain his doctoral students Chien-Shiung Wu (a Chinese national) and Kenneth Ross MacKenzie (a Canadian national) when they graduated.
In September 1941, Oliphant met with Lawrence and Oppenheimer at Berkeley, where they showed him the site for the new cyclotron. Oliphant in turn took the Americans to task for not following up the recommendations of the British MAUD Committee, which advocated a program to develop an atomic bomb. Lawrence had already thought about the problem of separating the fissile isotope uranium-235 from uranium-238, a process known today as uranium enrichment. Separating uranium isotopes was difficult because the two isotopes have very nearly identical chemical properties, and could only be separated gradually using their small mass differences. Separating isotopes with a mass spectrometer was a technique Oliphant had pioneered with lithium in 1934.
Lawrence began converting his old 37-inch cyclotron into a giant mass spectrometer. On his recommendation, the director of the Manhattan Project, Brigadier General Leslie R. Groves, Jr., appointed Oppenheimer as head of its Los Alamos Laboratory in New Mexico. While the Radiation laboratory developed the electromagnetic uranium enrichment process, the Los Alamos Laboratory designed and constructed the atomic bombs. Like the Radiation Laboratory, it was run by the University of California.
Electromagnetic isotope separation used devices known as calutrons, a hybrid of two laboratory instruments, the mass spectrometer and cyclotron. The name was derived from "California university cyclotrons". On November 1943, Lawrence's team at Berkeley was bolstered by 29 British scientists, including Oliphant.
In the electromagnetic process, a magnetic field deflected charged particles according to mass. The process was neither scientifically elegant nor industrially efficient. Compared with a gaseous diffusion plant or a nuclear reactor, an electromagnetic separation plant would consume more scarce materials, require more manpower to operate, and cost more to build. Nonetheless, the process was approved because it was based on proven technology and therefore represented less risk. Moreover, it could be built in stages, and would rapidly reach industrial capacity.
Oak Ridge.
Responsibility for the design and construction of the electromagnetic separation plant at Oak Ridge, Tennessee, which came to be called Y-12, was assigned to Stone & Webster. The design called for five first-stage processing units, known as Alpha racetracks, and two units for final processing, known as Beta racetracks. In September 1943 Groves authorized construction of four more racetracks, known as Alpha II. When the plant was started up for testing on schedule in October 1943, the 14-ton vacuum tanks crept out of alignment because of the power of the magnets, and had to be fastened more securely. A more serious problem arose when the magnetic coils started shorting out. In December Groves ordered a magnet to be broken open, and handfuls of rust were found inside. Groves then ordered the racetracks to be torn down and the magnets sent back to the factory to be cleaned. A pickling plant was established on-site to clean the pipes and fittings.
Tennessee Eastman was hired to manage Y-12. Y-12 initially enriched the uranium-235 content to between 13% and 15%, and shipped the first few hundred grams of this to Los Alamos laboratory in March 1944. Only 1 part in 5,825 of the uranium feed emerged as final product. The rest was splattered over equipment in the process. Strenuous recovery efforts helped raise production to 10% of the uranium-235 feed by January 1945. In February the Alpha racetracks began receiving slightly enriched (1.4%) feed from the new S-50 thermal diffusion plant. The next month it received enhanced (5%) feed from the K-25 gaseous diffusion plant. By April 1945 K-25 was producing uranium sufficiently enriched to feed directly into the Beta tracks.
On July 16, 1945, Lawrence observed the Trinity nuclear test of the first atomic bomb with Chadwick and Charles A. Thomas. Few were more excited at its success than Lawrence. The question of how to use the now functional weapon on Japan became an issue for the scientists. While Oppenheimer favored no demonstration of the power of the new weapon to Japanese leaders, Lawrence felt strongly that a demonstration would be wise. When a uranium bomb was used without warning in the atomic bombing of Hiroshima, Lawrence felt great pride in his accomplishment.
Lawrence hoped that the Manhattan Project would develop improved calutrons and construct Alpha III racetracks, but they were judged to be uneconomical. The Alpha tracks were closed down in September 1945. Although performing better than ever, they could not compete with K-25 and the new K-27, which commenced operation in January 1946. In December, the Y-12 plant was closed, thereby cutting the Tennessee Eastman payroll from 8,600 to 1,500 and saving $2 million a month. Staff numbers at the Radiation laboratory fell from 1,086 in May 1945 to 424 by the end of the year.
Post-war career.
Big Science.
After the war, Lawrence campaigned extensively for government sponsorship of large scientific programs. He was a forceful advocate of Big Science with its requirements for big machines and big money, and in 1946 he asked the Manhattan Project for over $2 million for research at the Radiation Laboratory. Groves approved the money, but cut a number of programs, including Seaborg's proposal for a "hot" radiation laboratory in densely populated Berkeley, and John Lawrence's for production of medical isotopes, because this need could now be better met from nuclear reactors. One obstacle was the University of California, which was eager to divest its wartime military obligations. Lawrence and Groves managed to persuade Sproul to accept a contract extension. In 1946, the Manhattan Project spent $7 on physics at the University of California for every dollar spent by the University.
The 184-inch cyclotron was completed with wartime dollars from the Manhattan Project. It incorporated new ideas by Ed McMillan, and was completed as a synchrotron. It commenced operation on November 13, 1946. For the first time since 1935, Lawrence actively participated in the experiments, working unsuccessfully with Eugene Gardner in an attempt to create recently discovered pi mesons with the synchrotron. César Lattes then used the apparatus they had created to find negative pi mesons in 1948.
Responsibility for the national laboratories passed to the newly created Atomic Energy Commission (AEC) on January 1, 1947. That year, Lawrence asked for $15 million for his projects, which included a new linear accelerator and a new gigaelectronvolt synchrotron which became known as the bevatron. The University of California's contract to run the Los Alamos laboratory was due to expire on July 1, 1948, and some board members wished to divest the university of the responsibility for running a site outside California. After some negotiation, the university agreed to extend the contract for what was now the Los Alamos National Laboratory for four more years, and to appoint Norris Bradbury, who had replaced Oppenheimer as its director on October 1945, as a professor. Soon after, Lawrence received all the funds he had requested.
Notwithstanding the fact that he voted for Franklin Roosevelt, Lawrence was a Republican, who had strongly disapproved of Oppenheimer's efforts before the war to unionize the Radiation Laboratory workers, which Lawrence considered "leftwandering activities". In the chilly Cold War climate of the post-war University of California, Lawrence was forced to defend Radiation Laboratory staff members like Robert Serber who were investigated by the University's Personnel Security Board. Lawrence barred Robert Oppenheimer's brother Frank from the Radiation Laboratory, damaging his relationship with Robert. An acrimonious loyalty oath campaign drove away faculty members.
Thermonuclear weapons.
Lawrence was alarmed by the Soviet Union's first nuclear test in August 1949. The proper response, he concluded, was an all-out effort to build a bigger nuclear weapon: the hydrogen bomb. Lawrence proposed to use accelerators instead of nuclear reactors to produce the neutrons needed to create the tritium the bomb required, as well as plutonium, which was more difficult, as much higher energies would be required. He first proposed the construction of Mark I, a prototype $7 million, 25 MeV linear accelerator, codenamed Materials Test Accelerator (MTA). He was soon talking about a new, even larger MTA known as the Mark II, which could produce tritium or plutonium from depleted uranium-238. Serber and Segré attempted in vain to explain the technical problems that made it impractical, but Lawrence felt that they were being unpatriotic.
Lawrence strongly backed Edward Teller's campaign for a second nuclear weapons laboratory, which Lawrence proposed to locate with the MTA Mark I at Livermore, California. Lawrence and Teller had to argue their case not only with the Atomic Energy Commission, which did not want it, and the Los Alamos National Laboratory, which was implacably opposed, but with proponents who felt that Chicago was the more obvious site for it. The new laboratory at Livermore was finally approved on July 17, 1952, but the Mark II MTA was cancelled. By this time, the Atomic Energy Commission had spent $45 million on the Mark I, which had commenced operation, but was mainly used to produce polonium for the nuclear weapons program. Meanwhile, the Brookhaven National Laboratory's Cosmotron had generated a 1 GeV beam.
Death and legacy.
In addition to the Nobel Prize, Lawrence received the Elliott Cresson Medal and the Hughes Medal in 1937, the Comstock Prize in Physics in 1938, the Duddell Medal and Prize in 1940, the Holley Medal in 1942, the Medal for Merit in 1946, the William Procter Prize in 1951, Faraday Medal in 1952, and the Enrico Fermi Award from the Atomic Energy Commission in 1957. He was made an Officer of the Legion d'Honneur in 1948, and was the first recipient of the Sylvanus Thayer Award by the US Military Academy in 1958.
In July 1958, President Dwight D. Eisenhower asked Lawrence travel to Geneva, Switzerland, to help negotiate a proposed Partial Nuclear Test Ban Treaty with the Soviet Union. AEC Chairman Lewis Strauss had pressed for Lawrence's inclusion. The two men had argued the case for the development of the hydrogen bomb, and Strauss had helped raise funds for Lawrence's cyclotron in 1939. Strauss was keen to have Lawrence as part of the Geneva delegation because Lawrence was known to favor continued nuclear testing. Despite suffering from a serious flare-up of his chronic ulcerative colitis, Lawrence decided to go, but he became ill while in Geneva, and was rushed back to the hospital at Stanford University. Surgeons removed much of his large intestine, but found other problems, including severe atherosclerosis in one of his arteries. He died in Palo Alto Hospital on August 27, 1958. Molly did not want a public funeral, but agreed to a memorial service at the First Congregationalist Church in Berkeley. University of California President Clark Kerr delivered the eulogy.
Just 23 days after his death, the Regents of the University of California voted to rename two of the university's nuclear research sites after Lawrence: the Lawrence Livermore National Laboratory and the Lawrence Berkeley National Laboratory. The Ernest Orlando Lawrence Award was established in his memory in 1959. Chemical element number 103, discovered at the Lawrence Berkeley National Laboratory in 1961, was named lawrencium after him. In 1968 the Lawrence Hall of Science public science education center was established in his honor. His papers are in the Bancroft Library at the University of California in Berkeley. In the 1980s, Lawrence's widow petitioned the University of California Board of Regents on several occasions to remove her husband's name from the Livermore Laboratory, due to its focus on nuclear weapons Lawrence helped build, but was denied each time. She outlived her husband by more than 44 years and died in Walnut Creek at the age of 92 on January 6, 2003. George B. Kauffman wrote that: 

</doc>
<doc id="39043" url="https://en.wikipedia.org/wiki?curid=39043" title="African clawed frog">
African clawed frog

The African clawed frog ("Xenopus laevis", also known as the xenopus, African clawed toad, African claw-toed frog or the platanna) is a species of African aquatic frog of the Pipidae family. Its name is derived from the three short claws on each hind foot, which it uses to tear apart its food. The word "Xenopus" means "strange foot" and "laevis" means "smooth".
The species is found throughout much of Sub-Saharan Africa (Nigeria and Sudan to South Africa), and in isolated, introduced populations in North America, South America, and Europe. All species of the Pipidae family are tongueless, toothless and completely aquatic. They use their hands to shove food in their mouths and down their throats and a hyobranchial pump to draw or suck things in their mouth. Pipidae have powerful legs for swimming and lunging after food. They also use the claws on their feet to tear pieces of large food. They lack true ears but have lateral lines running down the length of the body and underside, which is how they can sense movements and vibrations in the water. They use their sensitive fingers, sense of smell, and lateral line system to find food. Pipidae are scavengers and will eat almost anything living, dying, or dead and any type of organic waste.
Description.
These frogs are plentiful in ponds and rivers within the south-eastern portion of Sub-Saharan Africa. They are aquatic and are often greenish-grey in color. Albino varieties are commonly sold as pets. “Wild type" African Clawed Frogs are also frequently sold as pets, and often incorrectly labeled as a Congo Frog or African Dwarf Frog because of similar colorings. They are easily distinguished from African Dwarf Frogs because African Clawed Frogs have webbing only on their hind feet while African Dwarf Frogs have webbing on all four feet. They reproduce by laying eggs (see frog reproduction). Also, the clawed frogs are the only amphibians to have actual claws used to climb and shred foods like fish or tadpoles. They lay their eggs from winter till spring. During wet rainy seasons they will travel to other ponds or puddles of water to search for food.
The average life-span of these frogs ranges from 5–15 years with some individuals recorded to have lived for 20–25 years. They shed their skin every season, and eat their own shed skin.
Although lacking a vocal sac, the males make a mating call of alternating long and short trills, by contracting the intrinsic laryngeal muscles. Females also answer vocally, signaling either acceptance (a rapping sound) or rejection (slow ticking) of the male. This frog has smooth slippery skin which is multicolored on its back with blotches of olive gray or brown. The underside is creamy white with a yellow tinge.
Male and female frogs can be easily distinguished through the following differences. Male frogs are usually about 20% smaller than females, with slim bodies and legs. Males make mating calls to attract females, sounding very much like a cricket calling underwater. Females are larger than the males, appearing far more plump with hip-like bulges above their rear legs (where their eggs are internally located).
Both males and females have a cloaca, which is a chamber through which digestive and urinary wastes pass and through which the reproductive systems also empty. The cloaca empties by way of the vent which in reptiles and amphibians is a single opening for all three systems.
In the wild.
In the wild, "Xenopus laevis" are native to wetlands, ponds, and lakes across arid/semiarid regions of Sub-Saharan Africa. "Xenopus laevis" and "Xenopus muelleri" occur along the western boundary of the Great African Rift. The people of the sub-Saharan are generally very familiar with this frog, and some cultures use it as a source of protein, an aphrodisiac, or as fertility medicine. Two historic outbreaks of priapism have been linked to consumption of frog legs from frogs that ate insects containing cantharidin. Wild "Xenopus" are much larger than their captive bred counterparts.
"Xenopus laevis" in the wild are commonly infected by various parasites, including monogeneans in the urinary bladder.
Use in research.
"Xenopus" embryos and eggs are a popular model system for a wide variety of biological studies. This animal is widely used because of its powerful combination of experimental tractability and close evolutionary relationship with humans, at least compared to many model organisms. For a more comprehensive discussion of the use of these frogs in biomedical research, see the Wikipedia entry for "Xenopus".
"Xenopus" has long been an important tool for in vivo studies in molecular, cell, and developmental biology of vertebrate animals. However, the wide breadth of "Xenopus" research stems from the additional fact that cell-free extracts made from "Xenopus" are a premier in vitro system for studies of fundamental aspects of cell and molecular biology. Thus, "Xenopus" is the only vertebrate model system that allows for high-throughput in vivo analyses of gene function and high-throughput biochemistry. Finally, "Xenopus" oocytes are a leading system for studies of ion transport and channel physiology.
Although "X. laevis" does not have the short generation time and genetic simplicity generally desired in genetic model organisms, it is an important model organism in developmental biology, cell biology, toxicology and neurobiology. "X. laevis" takes 1 to 2 years to reach sexual maturity and, like most of its genus, it is tetraploid. It does have a large and easily manipulated embryo, however. The ease of manipulation in amphibian embryos has given them an important place in historical and modern developmental biology. A related species, "Xenopus tropicalis", is now being promoted as a more viable model for genetics.
Roger Wolcott Sperry used "X. laevis" for his famous experiments describing the development of the visual system. These experiments led to the formulation of the Chemoaffinity hypothesis.
"Xenopus" oocytes provide an important expression system for molecular biology. By injecting DNA or mRNA into the oocyte or developing embryo, scientists can study the protein products in a controlled system. This allows rapid functional expression of manipulated DNAs (or mRNA). This is particularly useful in electrophysiology, where the ease of recording from the oocyte makes expression of membrane channels attractive. One challenge of oocyte work is eliminating native proteins that might confound results, such as membrane channels native to the oocyte. Translation of proteins can be blocked or splicing of pre-mRNA can be modified by injection of Morpholino antisense oligos into the oocyte (for distribution throughout the embryo) or early embryo (for distribution only into daughter cells of the injected cell).
Extracts from the eggs of "X. laevis" frogs are also commonly used for biochemical studies of DNA replication and repair, as these extracts fully support DNA replication and other related processes in a cell-free environment which allows easier manipulation.
The first vertebrate ever to be cloned was an African clawed frog, an experiment for which Sir John Gurdon was awarded the Nobel Prize in Physiology or Medicine 2012 "for the discovery that mature cells can be reprogrammed to become pluripotent".
, several African clawed frogs were present on the space shuttle Endeavour (which was launched into space on September 12, 1992) so that scientists could test whether reproduction and development could occur normally in zero gravity.
"X. laevis" is also notable for its use in the first well-documented method of pregnancy testing when it was discovered that the urine from pregnant women induced "X. laevis" oocyte production. Human chorionic gonadotropin (HCG) is a hormone found in substantial quantities in the urine of pregnant women. Today, commercially available HCG is injected into "Xenopus" males and females to induce mating behavior and to breed these frogs in captivity at any time of the year.
Amphibian frog "Xenopus laevis" also serves as an ideal model system for the study of the mechanisms of apoptosis. In fact, iodine and thyroxine stimulate the spectacular apoptosis of the cells of the larval gills, tail and fins in amphibians metamorphosis, and stimulate the evolution of their nervous system transforming the aquatic, vegetarian tadpole into the terrestrial, carnivorous frog.
Genome sequencing.
The Wallingford and Marcotte labs obtained funding from the Texas Institute for Drug and Diagnostic Development (TI3D), in conjunction with projects funded by the National Institutes of Health, to begin sequencing of the "X. laevis" genome. The project began with Scott Hunicke-Smith at the University of Texas Genome Sequencing and Analysis facility, with funding sufficient for ~20x coverage of the "X. laevis genome" using ABI SOLiD next-generation sequencing. The project rapidly expanded to include "de novo" reconstruction of "X. laevis" transcripts, in collaboration with groups around the world donating Illumina Hi-Seq RNA sequencing datasets, coordinating these efforts with genome sequencing by the Harland and Rokhsar groups at UC Berkeley and with Taira and collaborators at the University of Tokyo, Japan.
Xenbase is the Model Organism Database (MOD) with the full details and release information regarding the current "Xenopus laevis" genome (9.1).
As pets.
"Xenopus laevis" have been kept as pets and research subjects since as early as the 1950s. They are extremely hardy and long lived, having been known to live up to 20 or even 30 years in captivity.
African Clawed Frogs are frequently mislabeled as African Dwarf Frogs in pet stores. The astute pet owner will recognize the difference, however, because of the following characteristics:
As a pest.
African Clawed Frogs are voracious predators and easily adapt to many habitats. For this reason, they can easily become a harmful invasive species. They can travel short distances to other bodies of water, and some have even been documented to survive mild freezes. They have been shown to devastate native populations of frogs and other creatures by eating their young.
In 2003, "Xenopus laevis" frogs were discovered in a pond at San Francisco's Golden Gate Park. Much debate now exists in the area on how to exterminate these creatures and keep them from spreading. It is unknown if these frogs entered the San Francisco ecosystem through intentional release or escape into the wild. San Francisco officials drained Lily Pond and fenced off the area to prevent the frogs from escaping to other ponds in the hopes they starve to death.
Due to incidents in which these frogs were released and allowed to escape into the wild, African Clawed Frogs are illegal to own, transport or sell without a permit in the following US states: Arizona, California, Kentucky, Louisiana, New Jersey, North Carolina, Oregon, Virginia, Hawaii, Nevada, and Washington state. However, it is legal to own "Xenopus laevis" in New Brunswick and Ohio.
Feral colonies of "Xenopus laevis" exist in South Wales, United Kingdom.
The African clawed frog may be an important vector and the initial source of "Batrachochytrium dendrobatidis", a chytrid fungus that has been implicated in the drastic decline in amphibian populations in many parts of the world. Unlike in many other amphibian species (including the closely related western clawed frog) where this chytrid fungus causes the disease Chytridiomycosis, it does not appear to affect the African clawed frog, making it an effective carrier.

</doc>
<doc id="39044" url="https://en.wikipedia.org/wiki?curid=39044" title="Mesoderm">
Mesoderm

In all bilaterian animals, the mesoderm is one of the three primary "germ layers" in the very early "embryo". The other two layers are the "ectoderm" (outside layer) and "endoderm" (inside layer), with the mesoderm as the "middle" layer between them.
The mesoderm forms mesenchyme, mesothelium, non-epithelial blood cells and coelomocytes. Mesothelium lines coeloms. Mesoderm forms the muscles in a process known as myogenesis, septa (cross-wise partitions) and mesenteries (length-wise partitions); and forms part of the gonads (the rest being the gametes).
Myogenesis is specifically a function of Mesenchyme.
The mesoderm differentiates from the rest of the embryo through intercellular signaling, after which the mesoderm is polarized by an organizing center.
The position of the organizing center is in turn determined by the regions in which beta-catenin is protected from degradation by GSK-3. Beta-catenin acts as a co-factor that alters the activity of the transcription factor tcf-3 from repressing to activating, which initiates the synthesis of gene products critical for mesoderm differentiation and gastrulation. Furthermore, mesoderm has the capability to induce the growth of other structures, such as the neural plate, the precursor to the nervous system.
Definition.
The mesoderm is one of the three germinal layers that appears in the third week of embryonic development. It is formed through a process called gastrulation. There are three important components, the paraxial mesoderm, the intermediate mesoderm and the lateral plate mesoderm. The paraxial mesoderm forms the somitomeres, which give rise to mesenchyme of the head and organize into somites in occipital and caudal segments. Somites give rise to the myotome (muscle tissue), sclerotome (cartilage and bone), and dermatome (subcutaneous tissue of the skin). Signals for somite differentiation are derived from surroundings structures, including the notochord, neural tube and epidermis. The intermediate mesoderm connects the paraxial mesoderm with the lateral plate, eventually it differentiates into urogenital structures consisting of the kidneys, gonads, their associated ducts, and the adrenal glands. The lateral plate mesoderm give rise to the heart, blood vessels and blood cells of the circulatory system as well as to the mesodermal component of the limbs.
Some of the mesoderm derivatives include the muscle (smooth, cardiac and skeletal), the muscles of the tongue (occipital somites), the pharyngeal arches muscle (muscles of mastication, muscles of facial expressions), connective tissue, dermis and subcutaneous layer of the skin, bone and cartilage, dura mater, endothelium of blood vessels, red blood cells, white blood cells, microglia and Kupffer cells, the kidneys and the adrenal cortex.
Development of the mesodermal germ layer.
During the third week a process called gastrulation creates a mesodermal layer between the endoderm and the ectoderm. This process begins with formation of a primitive streak on the surface of the epiblast. The cells of the layers move between the epiblast and hypoblast and begin to spread laterally and cranially. The cells of the epiblast move toward the primitive streak and slip beneath it in a process called invagination. Some of the migrating cells displace the hypoblast and create the endoderm, and others migrate between the endoderm and the epiblast to create the mesoderm. The remaining cells form the ectoderm. After that, the epiblast and the hypoblast establish contact with the extraembryonic mesoderm until they cover the yolk sac and amnion. They move onto either side of the prechordal plate. The prechordal cells migrate to the midline to form the notochordal plate. The chordamesoderm is the central region of trunk mesoderm. This forms the notochord which induces the formation of the neural tube and establishes the anterior-posterior body axis. The notochord extends beneath the neural tube from the head to the tail. The mesoderm moves to the midline until it covers the notochord, when the mesoderm cells proliferate they form the paraxial mesoderm. In each side, the mesoderm remains thin and is known as the lateral plate. The intermediate mesoderm lies between the paraxial mesoderm and the lateral plate.
Between days 13 and 15, the proliferation of extraembryonic mesoderm, primitive streak and embryonic mesoderm take place. The notochord process occurs between days 15 and 17. Eventually, the development of the notochord canal and the axial canal takes place between days 17 and 19 when the first three somites are formed.
Paraxial mesoderm.
During the third week, the paraxial mesoderm is organized into segments. If they appear in the cephalic region and grow with cephalocaudal direction, they are called somitomeres. If they appear in the cephalic region but establish contact with the neural plate, they are known as neuromeres, which later will form the mesenchyme in the head. The somitomeres organize into somites which grow in pairs. In the fourth week the somites lose their organization and cover the notochord and spinal cord to form the backbone. In the fifth week, there are 4 occipital somites, 8 cervical, 12 thoracic, 5 lumbar, 5 sacral and 8 to 10 coccygeal that will form the axial skeleton. Somatic derivatives are determined by local signaling between adjacent embryonic tissues, in particular the neural tube, notochord, surface ectoderm and the somatic compartments themselves. The correct specification of the deriving tissues, skeletal, cartilage, endothelia and connective tissue is achieved by a sequence of morphogenic changes of the paraxial mesoderm, leading to the three transitory somatic compartments: dermomyotome, myotome and sclerotome. These structures are specified from dorsal to ventral and from medial to lateral. each somite will form its own sclerotome that will differentiate into the tendon cartilage and bone component. Its myotome will form the muscle component and the dermatome that will form the dermis of the back. The myotome and dermatome have a nerve component.
Molecular Regulation of Somite Differentiation.
Surrounding structures such as the notochord, neural tube, epidermis and lateral plate mesoderm send signals for somite differentiation Notochord protein accumulates in presomitic mesoderm destined to form the next somite and then decreases as that somite is established. The notochord and the neural tube activate the protein SHH which helps the somite to form its sclerotome. The cells of the sclerotome express the protein PAX1 that induces the cartilage and bone formation. The neural tube activates the protein WNT1 that expresses PAX 2 so the somite creates the myotome and dermatome. Finally, the neural tube also secretes neurotrophin 3 (NT-3), so that the somite creates the dermis. Boundaries for each somite are regulated by retinoic acid (RA) and a combination of FGF8and WNT3a. So the retinoic acid is and endogenous signal that maintains the bilateral synchrony of mesoderm segmentation and controls bilateral symmetry in vertebrates. The bilaterally symmetric body plan of vertebrate embryos is obvious in somites and their derivates such as the vertebral column. Therefore, asymmetric somite formation correlates with a left-right desynchronization of the segmentation oscillations.
Many studies with Xenopus and zebrafish have analyzed the factors of this development and how they interact in signaling and transcription. However, there are still some doubts in how the prospective mesodermal cells integrate the various signals they receive and how they regulate their morphogenic behaviours and cell-fate decisions. Human embryonic stem cells for example have the potential to produce all of the cells in the body and they are able to self-renew indefinitely so they can be used for a large-scale production of therapeutic cell lines. They are also able to remodel and contract collagen and were induced to express muscle actin. This shows that these cells are multipotent cells.
Intermediate mesoderm.
The intermediate mesoderm connects the paraxial mesoderm with the lateral plate and differentiates into urogenital structures. In upper thoracic and cervical regions this forms the nephrotomes, and in caudally regions this forms the nephrogenic cord. It also helps to develop the excretory units of the urinary system and the gonads.
Lateral plate mesoderm.
The lateral plate mesoderm splits into parietal (somatic) and visceral (splanchnic) layers. The formation of these layers starts with the appearance of intercellular cavities. The somatic layer depends on a continuous layer with mesoderm that covers the amnion. The splanchnic depends on a continuous layer that covers the yolk sac. The two layers cover the intraembryonic cavity. The parietal layer together with overlying ectoderm forms the lateral body wall folds. The visceral layer forms the walls of the gut tube. Mesoderm cells of the parietal layer form the mesothelial membranes or serous membranes which line the peritoneal, pleural and pericardial cavities.

</doc>
<doc id="39047" url="https://en.wikipedia.org/wiki?curid=39047" title="Peter Tork">
Peter Tork

Peter Tork (born Peter Halsten Thorkelson, February 13, 1942) is an American musician and actor, best known as the keyboardist and bass guitarist of the Monkees.
Early life.
Tork was born at the former Doctors Hospital, in Washington, D.C. Although he was born in the District of Columbia in 1942, many news articles incorrectly report him as born in 1944 in New York City, which was the date and place given on early Monkees press releases. He is the son of Virginia Hope (née Straus) and Halsten John Thorkelson, an economics professor at the University of Connecticut. His paternal grandfather was of Norwegian descent, while his mother was of half German Jewish and half British Isles ancestry. He began studying piano at the age of nine, showing an aptitude for music by learning to play several different instruments, including the banjo and both acoustic bass and guitars. Tork attended Windham High School in Willimantic, Connecticut, and was a member of the first graduating class at E.O. Smith High School in Storrs, Connecticut. He attended Carleton College before he moved to New York City, where he became part of the folk music scene in Greenwich Village during the first half of the 1960s. While there, he befriended other up-and-coming musicians such as Stephen Stills.
The Monkees.
Stephen Stills had auditioned for the new television series about four pop-rock musicians but was turned down because the show's producers felt his hair and teeth would not photograph well on camera. They asked Stills if he knew of someone with a similar "open, Nordic look," and Stills suggested Tork audition for the part. Tork got the job and became one of the four members of the Monkees, a fictitious pop band in the mid 1960s, created for a television comedy sitcom written about the fictitious band. Tork was the oldest member of the group.
Tork was a proficient musician, and though the group was not allowed to play their own instruments on their first two albums, he was an exception, playing what he described as "third chair guitar" on Mike Nesmith's song, "Papa Gene's Blues," from their first album. He subsequently played keyboards, bass guitar, banjo, harpsichord, and other instruments on their recordings. He also co-wrote, along with Joey Richards, the closing theme song of the second season of "The Monkees", "For Pete's Sake". On the television show, he was relegated to playing the "lovable dummy," a persona Tork had developed as a folk singer in New York's Greenwich Village.
The DVD release of the first season of the show contained commentary from the various bandmates. In it, Nesmith stated that Tork was better at playing guitar than bass. In Tork's commentary, he stated that Jones was a good drummer and had the live performance lineups been based solely on playing ability, it should have been Tork on guitar, Nesmith on bass, and Jones on drums, with Dolenz taking the fronting role, rather than as it was done (with Nesmith on guitar, Tork on bass, and Dolenz on drums). Jones filled in briefly for Tork on bass when he played keyboards.
Recording and producing as a group was Tork's major interest, and he hoped that the four members would continue working together as a band on future recordings. However, the four did not have enough in common regarding their musical interests. In commentary for the DVD release of the second season of the show, Tork said that Dolenz was "incapable of repeating a triumph". Dolenz felt that once he had accomplished something and became a success at it, there was no artistic sense in repeating a formula. 
Tork, once free from Don Kirshner's restrictions, in 1967, contributed some of the most memorable and catchy instrumental flourishes, such as the piano introduction to "Daydream Believer" and the banjo part on "You Told Me", as well as exploring occasional songwriting with the likes of "For Pete's Sake" and "Lady's Baby".
Tork was close to his grandmother, staying with her sometimes in his Greenwich Village days, and after he became a Monkee. "Grams" was one of his most ardent supporters and managed his fan club, often writing personal letters to members, and visiting music stores to make sure they carried Monkees records.
Six albums were produced with the original Monkees lineup, four of which went to No 1 on the "Billboard" chart. This success was supplemented by two years of the TV show, a series of successful concert tours both across America and abroad, and a trippy-psychedelic movie, "Head", a bit ahead of its time. However, tensions, both musical and personal, were increasing within the group. The band finished a Far East tour in December 1968 (where his copy of "Naked Lunch" was confiscated by Australian Customs) and then filmed an NBC television special, "33⅓ Revolutions Per Monkee", which rehashed many of the ideas from "Head", only with the Monkees playing a strangely second-string role.
No longer getting the group dynamic he wanted, and pleading "exhaustion" from the grueling schedule, Tork bought out the remaining four years of his contract after filming was complete on December 20, 1968, at a default of $150,000/year. In the DVD commentary for the "33⅓ Revolutions Per Monkee" TV special—originally broadcast April 14, 1969—Dolenz noted that Nesmith gave Tork a gold watch as a going-away present, engraved "From the guys down at work". Tork kept the back, but replaced the watch several times in later years.
Post-Monkees.
During a trip to London in December 1967, Tork contributed banjo to George Harrison's soundtrack to the 1968 film "Wonderwall". His playing featured in the movie, but not on the official "Wonderwall Music" soundtrack album released in November 1968. Tork's brief five-string banjo piece can be heard 16 minutes into the film, as Professor Collins is caught by his mother while spying on his neighbour Penny Lane.
Striking out on his own, he formed a group called 'Peter Tork And/Or Release' with girlfriend Reine Stewart on drums (she had played drums on part of "33⅓ Revolutions Per Monkee"), Riley "Wyldflower" Cummings (ex The Gentle Soul) on bass and – sometimes – singer/keyboard player Judy Mayhan. Tork said in April 1969, "We sometimes have four. We're thinking of having a rotating fourth. Right now, the fourth is that girl I'm promoting named Judy Mayhan." "We're like Peter's back-up band", added Stewart, "except we happen to be a group instead of a back-up band." Release hoped to have a record out immediately, and Tork has said that they did record some demos, which he may still have stored away somewhere. According to Stewart the band were supposed to go to Muscle Shoals as the backing band for Mayhan's Atlantic Records solo album "Moments" (1970) but they were ultimately replaced. They mainly played parties for their "in" friends and one of their songs was considered for the soundtrack to "Easy Rider", but the producers – who had also produced "Head" – eventually decided not to include it. Release could not secure a record contract, and by 1970 Tork was once again a solo artist, as he later recalled, "I didn't know how to stick to it. I ran out of money and told the band members, 'I can't support us as a crew any more, you'll just have to find your own way'."
Tork's record and movie production entity, the Breakthrough Influence Company (BRINCO), also failed to launch, despite such talent as future Little Feat guitarist, Lowell George. He was forced to sell his house in 1970, and he and a pregnant Reine Stewart moved into the basement of David Crosby's home. Tork was credited with co-arranging a Micky Dolenz solo single on MGM Records in 1971 ("Easy on You", b/w "Oh Someone"). An arrest and conviction for possession of hashish resulted in three months in an Oklahoma penitentiary in 1972. He moved to Fairfax in Marin County, California, in the early 1970s, where he joined the 35-voice Fairfax Street Choir and played guitar for a shuffle blues band called Osceola. Tork returned to Southern California in the mid-1970s, where he married and had a son and took a job teaching at Pacific Hills School in Santa Monica for a year and a half. He spent a total of three years as a teacher of music, social studies, math, French and history and coaching baseball at a number of schools, but enjoyed some more than others.
Peter Tork joined 'Dolenz, Jones, Boyce & Hart' onstage for a guest appearance on their concert tour on July 4, 1976 in Disneyland. Later that year he reunited with Jones and Dolenz in the studio for the recording of the single "Christmas Is My Time of The Year" b/w "White Christmas", which saw a limited release for fan club members that holiday season.
Sire Records.
A chance meeting with Sire Records executive Pat Horgan at the Bottom Line in New York City led to Tork recording a six-song demo, his first recording in many years. Recorded in summer 1980, it featured Tork, who sang, played rhythm guitar, keyboards, and banjo; it was backed by Southern rock band Cottonmouth, led by guitarist/singer/songwriter Johnny Pontiff, featuring Gerard Trahan on guitar/keyboards/vocals, Gene Pyle on bass guitar/vocals and Gary Hille on drums/percussion.
Horgan produced the six tracks (which included two Monkees covers, "Shades of Gray" and "Pleasant Valley Sunday"), with George Dispigno as engineer. The four other tracks were "Good Looker," "Since You Went Away" (which appeared on the Monkees 1987 CD "Pool It"), "Higher & Higher" and "Hi Hi Babe." Also present at the sessions were Joan Jett, Chrissie Hynde of The Pretenders, and Tommy Ramone of the Ramones. The tracks were recorded at Blue Horizon House, 165 West 74th Street, home of Sire Records, but Seymour Stein, president of Sire, rejected the demo, stating "there's nothing there." Tork recorded a second set of demos in New York City, but little is known about these (other than the fact that one track was a yet another version of "Pleasant Valley Sunday" with an unknown rock band, and featured a violin solo).
During this time Tork appeared regularly on "The Uncle Floyd Show" broadcast on U-68 out of New Jersey. He performed comedy bits and lip-synced the Sire recordings. Floyd claimed Tork was the "first real star" to appear on the show. (Later, Davy Jones, the Ramones, Shrapnel, and others would follow in his footsteps.)
In 1981 he released the 45 rpm single "(I'm Not Your) Steppin' Stone" (b/w "Higher And Higher") with "The New Monks". He also did some club performances and live television appearances, including taking part in a "Win A Date With Peter Tork" bit on "Late Night with David Letterman".
Monkees reunion.
In 1986, Tork rejoined fellow Monkees Davy Jones and Micky Dolenz for a highly successful 20th anniversary reunion tour. Three new songs were recorded by Tork and Dolenz for a greatest hits release. The three Monkees recorded "Pool It!". A decade later, all four group members recorded "Justus", the first recording with all four members since 1968. The quartet performed live in the United Kingdom in 1997, but for the next several years only the trio of Tork, Dolenz and Jones toured together. The trio of Monkees parted ways in 2001 with a public feud but reunited in 2011 for a series of 45th anniversary concerts in England and the United States.
Since 1986, Tork has intermittently toured with his former band mates and also played with his own bands The Peter Tork Project and Shoe Suede Blues. In 1991, Tork formed a band called the Dashboard Saints and played at a pizza restaurant in Guerneville, California. In 1994, he released his first album length solo project, "Stranger Things Have Happened", which featured brief appearances by Micky Dolenz and Michael Nesmith. In 1996, Tork collaborated on an album called "Two Man Band" with James Lee Stanley. The duo followed up in 2001 with a second release, "Once Again".
In 2001, Tork took time out from touring to appear in a leading role in the short film "Mixed Signals", written and directed by John Graziano.
In 2002, Tork resumed working with his band Shoe Suede Blues. The band performs original blues music, Monkees covers (blues versions of some), and covers of classic blues hits by greats such as Muddy Waters and has shared the stage with bands such as Captain Zig. The band toured extensively in 2006-7 following the release of the album "Cambria Hotel".
Tork also had an occasional roles as Topanga Lawrence's father on the sitcom "Boy Meets World", as well as a guest character on "7th Heaven". In 1995, Tork appeared as himself on the show "Wings", bidding against Crystal Bernard's character for the Monkeemobile. In 1999, he appeared as the Band Leader of a wedding band in season one episode 13, "Best Man", of "The King of Queens".
In early 2008, Tork added "advice columnist" to his extensive resume by authoring an online advice and info column called "Ask Peter Tork" at the webzine "The Daily Panic". 
In 2011, he joined Dolenz and Jones for the 2011 tour, .
In 2012, Peter joined Micky Dolenz and Michael Nesmith with a Monkees tour in honor of the album "Headquarters" 45th anniversary as well as in tribute to the late Davy Jones. The trio would tour again in 2013 and 2014.
Cancer.
On March 3, 2009, Tork reported on his website that he had been diagnosed with adenoid cystic carcinoma, a rare, slow-growing form of head and neck cancer. A preliminary biopsy discovered that the cancer had not spread beyond the initial site. "It's a bad news, good news situation," explained Tork. "It's so rare a combination (on the tongue) that there isn't a lot of experience among the medical community about this particular combination. On the other hand, the type of cancer it is, never mind the location, is somewhat well known, and the prognosis, I'm told, is good." Tork underwent radiation treatment to prevent the cancer from returning.
On March 4, 2009, Tork underwent extensive surgery in New York City, which was successful.
On June 11, 2009, a spokesman for Tork reported that his cancer had returned. Tork was reportedly "shaken but not stirred" by the news, and said that the doctors had given him an 80% chance of containing and shrinking the new tumor.
In July 2009, while undergoing radiation therapy, he was interviewed by the "Washington Post": "I recovered very quickly after my surgery, and I've been hoping that my better-than-average constitution will keep the worst effects of radiation at bay. My voice and energy still seem to be in decent shape, so maybe I can pull these gigs off after all." He continued to tour and perform while receiving his treatments.
On September 15, 2009, Tork received an "all clear" from his doctor.
Tork documented his cancer experience on Facebook and encouraged his fans to support research efforts of the Adenoid Cystic Carcinoma Research Foundation.
Personal life.
Tork currently resides in Mansfield, Connecticut. He has been married four times, and has a child each from two of the marriages and one child from a relationship:
Song list.
All songs written by Peter Tork or co-written by Tork as indicated.
Discography.
Solo:
with James Lee Stanley:
with Shoe Suede Blues:

</doc>
<doc id="39048" url="https://en.wikipedia.org/wiki?curid=39048" title="Laboratory for Atmospheric and Space Physics">
Laboratory for Atmospheric and Space Physics

The Laboratory for Atmospheric and Space Physics (LASP) is a research organization at the University of Colorado Boulder. LASP is a research institute with over one hundred research scientists ranging in fields from solar influences, to Earth's and other planetary atmospherics processes, space weather, space plasma and dusty plasma physics. LASP has advanced technical capabilities specializing in designing, building, and operating spacecraft and spacecraft instruments.
History.
Founded after World War II, the first scientific instruments built at LASP were launched into space using captured German V-2 rockets. To this day LASP continues a suborbital rocket program through periodic calibration instrument flights from White Sands Missile Range. It was originally called the Upper Air Laboratory, but changed to its current name in 1965. LASP has historical ties to Ball Aerospace Corporation and the Center for Astrophysics and Space Astronomy (CASA).
Facilities.
LASP has two facilities: offices on the main CU-Boulder campus, and the “Space Technology Building” in the University’s research park.
LASP’s new facilities allow it to handle almost every aspect of space missions, itself. Hardware facilities allow for the construction of single instruments or entire spacecraft. A Mission Operations Center allows for the control of spacecraft data collection, and a large research staff analyzes the data.
Being part of the University, LASP has heavy student involvement in every aspect of its operations, including science, hardware design / construction and mission operations.
Satellites and Instruments.
LASP supports the following spacecraft and instruments:
Upcoming Missions.
LASP is involved in upcoming missions:

</doc>
<doc id="39049" url="https://en.wikipedia.org/wiki?curid=39049" title="Robert S. Mulliken">
Robert S. Mulliken

Robert Sanderson Mulliken (June 7, 1896 – October 31, 1986) was an American physicist and chemist, primarily responsible for the early development of molecular orbital theory, i.e. the elaboration of the molecular orbital method of computing the structure of molecules. Dr. Mulliken received the Nobel Prize for chemistry in 1966. He received the Priestley Medal in 1983.
Early years.
Mulliken was born in Newburyport, Massachusetts. His father, Samuel Parsons Mulliken, was a professor of organic chemistry at the Massachusetts Institute of Technology. As a child, Robert Mulliken learned the name and botanical classification of plants and, in general, had an excellent, but selective, memory. For example, he learned German well enough to skip the course in scientific German in college, but could not remember the name of his high school German teacher. He also made the acquaintance, while still a child, of the physical chemist Arthur Amos Noyes.
Mulliken helped with some of the editorial work when his father wrote his four-volume text on organic compound identification, and thus became an expert on organic chemical nomenclature.
Education.
In high school in Newburyport, Mulliken followed a scientific curriculum. He graduated in 1913 and succeeded in getting a scholarship to MIT which had earlier been won by his father. Like his father, he majored in chemistry. Already as an undergraduate, he conducted his first publishable research: on the synthesis of organic chlorides. Because he was unsure of his future direction, he included some chemical engineering courses in his curriculum and spent a summer touring chemical plants in Massachusetts and Maine. He received his B. S. degree in chemistry from MIT in 1917.
Early career.
At this time, the United States had just entered World War I, and Mulliken took a position at American University in Washington, D.C., making poison gas under James B. Conant. After nine months, he was drafted into the Army's Chemical Warfare Service, but continued on the same task. His laboratory techniques left much to be desired, and he was out of service for months with burns. Later he got a bad case of influenza, and was still in the hospital at war's end.
After the war, he took a job investigating the effects of zinc oxide and carbon black on rubber, but quickly decided that this was not the kind of chemistry he wanted to pursue. So in 1919 he entered the Ph.D. program at the University of Chicago.
Graduate and early postdoctoral education.
He got his doctorate in 1921 based on research into the separation of isotopes of mercury by evaporation, and continued in his isotope separation by this method. While at Chicago, he took a course under the Nobel Prize-winning physicist Robert A. Millikan, which exposed him to the old quantum theory. He also became interested in strange molecules after exposure to work by Hermann I. Schlesinger on diborane.
At Chicago, he had received a grant from the National Research Council (NRC) which had paid for much of his work on isotope separation. The NRC grant was extended in 1923 for two years so he could study isotope effects on band spectra of such diatomic molecules as boron nitride (BN) (comparing molecules with B10 and B11). He went to Harvard University to learn spectrographic technique from Frederick A. Saunders and quantum theory from E. C. Kemble. At the time, he was able to associate with many future luminaries, including J. Robert Oppenheimer, John H. Van Vleck, and Harold C. Urey. He also met John C. Slater, who had worked with Niels Bohr.
In 1925 and 1927, Mulliken traveled to Europe, working with outstanding spectroscopists and quantum theorists such as Erwin Schrödinger, Paul A. M. Dirac, Werner Heisenberg, Louis de Broglie, Max Born, and Walther Bothe (all of whom eventually received Nobel Prizes) and Friedrich Hund, who was at the time Born's assistant. They all, as well as Wolfgang Pauli, were developing the new quantum mechanics that would eventually supersede the old quantum theory. Mulliken was particularly influenced by Hund, who had been working on quantum interpretation of band spectra of diatomic molecules, the same spectra which Mulliken had investigated at Harvard. In 1927 Mulliken worked with Hund and as a result developed his molecular orbital theory, in which electrons are assigned to states that extend over an entire molecule. In consequence, molecular orbital theory was also referred to as the Hund-Mulliken theory.
Early scientific career.
From 1926 to 1928, he taught in the physics department at New York University (NYU). This was his first recognition as a physicist; though his work had been considered important by chemists, it clearly was on the borderline between the two sciences and both would claim him from this point on. Then he returned to the University of Chicago as an associate professor of physics, being promoted to full professor in 1931. He would ultimately hold a position jointly in both the physics and chemistry departments. At both NYU and Chicago, he continued to refine his molecular-orbital theory.
Up to this point, the primary way to calculate the electronic structure of molecules was based on a calculation by Walter Heitler and Fritz London on the hydrogen molecule (H2) in 1927. With the conception of hybridized atomic orbitals by John C. Slater and Linus Pauling, which rationalized observed molecular geometries, the method was based on the premise that the bonds in any molecule could be described in a manner similar to the bond in H2, namely, as overlapping atomic orbitals centered on the atoms involved. Since it corresponded to chemists' ideas of localized bonds between pairs of atoms, this method (called the Valence-Bond (VB) or Heitler-London-Slater-Pauling (HLSP) method), was very popular. However, particularly in attempting to calculate the properties of excited states (molecules that have been excited by some source of energy), the VB method does not always work well. With its description of the electron wave functions in molecules as delocalized molecular orbitals that possess the same symmetry as the molecule, Hund and Mulliken's molecular-orbital method, including contributions by John Lennard-Jones, proved to be more flexible and applicable to a vast variety of types of molecules and molecular fragments, and has eclipsed the valence-bond method. As a result of this development, he received the Nobel Prize in Chemistry in 1966.
Mulliken became a member of the National Academy of Sciences in 1936, the youngest member in the organization's history, at that time. He was elected a Foreign Member of the Royal Society (ForMemRs) in 1967.
Mulliken population analysis is named after him, a method of assigning charges to atoms in a molecule.
Personal life.
On December 24, 1929, he married Mary Helen von Noé, daughter of Adolf Carl Noé, a geology professor at the University of Chicago. They had two daughters.
Later years.
In 1934, he derived a new scale for measuring the electronegativity of elements. This does not entirely correlate with the scale of Linus Pauling, but is generally in close correspondence.
In World War II, from 1942 to 1945, Mulliken directed the Information Office for the University of Chicago's Plutonium project. Afterward, he developed mathematical formulas to enable the progress of the molecular-orbital theory.
In 1952 he began to apply quantum mechanics to the analysis of the reaction between Lewis acid and base molecules. (See Acid-base reaction theories.) He became Distinguished Professor of Physics and Chemistry in 1961 and continued in his studies of molecular structure and spectra, ranging from diatomic molecules to large complex aggregates. He retired in 1985. His wife died in 1975.
At the age of 90, Mulliken died of congestive heart failure at his daughter's home in Arlington, Virginia on October 31, 1986. His body was returned to Chicago for burial.

</doc>
<doc id="39051" url="https://en.wikipedia.org/wiki?curid=39051" title="Cape Cod National Seashore">
Cape Cod National Seashore

The Cape Cod National Seashore (CCNS), created on August 7, 1961 by President John F. Kennedy, encompasses on Cape Cod, in Massachusetts. It includes ponds, woods and beachfront of the Atlantic coastal pine barrens ecoregion. The CCNS includes nearly of seashore along the Atlantic-facing eastern shore of Cape Cod, in the towns of Provincetown, Truro, Wellfleet, Eastham, Orleans and Chatham. It is administered by the National Park Service.
Places of interest.
Notable sites encompassed by the CCNS include Marconi Station, site of the first two-way transatlantic radio transmission, and the Highlands Center for the Arts, formerly the North Truro Air Force Station. Dune Shacks of Peaked Hill Bars Historic District is a 1,950-acre historic district containing dune shacks and the dune environment. The glacial erratic known as Doane Rock is also located in the park.
A former United States Coast Guard station on the ocean in Truro is now operated as a 42-bed youth hostel by Hostelling International USA.
There are several paved bike trails:
Restoration efforts.
As part of the NPS Centennial Initiative, the Herring River estuary will be restored to its natural state through removal of dikes and drains that date back to 1909.

</doc>
<doc id="39052" url="https://en.wikipedia.org/wiki?curid=39052" title="Provincetown, Massachusetts">
Provincetown, Massachusetts

Provincetown is a New England town located at the extreme tip of Cape Cod in Barnstable County, Massachusetts, in the United States. A small coastal resort town with a year-round population of just under 3,000, Provincetown has a summer population of as high as 60,000. Often called "P-town" or "P'town", the town is known for its beaches, harbor, artists, tourist industry, and its status as a vacation destination for the LGBTQ community.
History.
At the time of European encounter, the area was long settled by the historic Nauset tribe, who had a settlement known as "Meeshawn". They spoke Massachusett, a Southern New England Algonquian language dialect that they shared in common with their closely related neighbors, the Wampanoag.
On May 15, 1602, having made landfall from the west and believing it to be an island, Bartholomew Gosnold initially named this area "Shoal Hope". Later that day, after catching a "great store of codfish", he chose instead to name this outermost tip of land "Cape Cod". Notably, that name referred specifically to the area of modern-day Provincetown; it wasn't until much later that that name was reused to designate the entire region now known as Cape Cod. 
On November 9, 1620, the Pilgrims aboard the "Mayflower" sighted Cape Cod while en route to the Colony of Virginia. After two days of failed attempts to sail south against the strong winter seas, they returned to the safety of the harbor, known today as Provincetown Harbor, and set anchor. It was here that the Mayflower Compact was drawn up and signed. They agreed to settle and build a self-governing community, and came ashore in the West End.
Though the Pilgrims chose to settle across the bay in Plymouth, Cape Cod enjoyed an early reputation for its valuable fishing grounds, and for its harbor: a naturally deep, protected basin that was considered the best along the coast. In 1654, the Governor of the Plymouth Colony purchased this land from the Chief of the Nausets, for a selling price of two brass kettles, six coats, 12 hoes, 12 axes, 12 knives and a box.
That land, which spanned from East Harbor (formerly, Pilgrim Lake) – near the present-day border between Provincetown and Truro – to Long Point, was kept for the benefit of Plymouth Colony, which began leasing fishing rights to roving fishermen. The collected fees were used to defray the costs of schools and other projects throughout the colony. In 1678, the fishing grounds were opened up to allow the inclusion of fishermen from the Massachusetts Bay Colony.
In 1692, a new Royal Charter combined the Plymouth and Massachusetts Bay colonies into the Province of Massachusetts Bay. "Cape Cod" was thus officially renamed the "Province Lands". 
The first record of a municipal government with jurisdiction over the Province Lands was in 1714, with an Act that declared it the "Precinct of Cape Cod", annexed under control of Truro.
On June 14, 1727, after harboring ships for more than a century, the Precinct of Cape Cod was incorporated as a township. The name chosen by its inhabitants was "Herringtown", which was rejected by the Massachusetts General Court in favor of "Provincetown". The act of incorporation provided that inhabitants of Provincetown could be land holders, but not land owners. They received a quit claim to their property, but the Province retained the title. The land was to be used as it had been from the beginning of the colony — a place for the making of fish. All resources, including the trees, could be used for that purpose. In 1893 the Great and General Court changed the Town's charter, giving the townspeople deeds to the properties they held, while still reserving unoccupied areas.
The population of Provincetown remained small through most of the 18th century.
The town was affected by the American Revolution the same way most of Cape Cod was: the effective British blockade shut down most fish production and shipping and the town dwindled. It was, by happenstance, the location of the wreck of a British warship, "HMS Somerset" at the Peaked Hill Bars off the Atlantic Coast of Provincetown in 1778.
Following the American Revolution, Provincetown grew rapidly as a fishing and whaling center. The population was bolstered by numerous Portuguese sailors, many of whom were from the Azores, and settled in Provincetown after being hired to work on US ships.
By the 1890s, Provincetown was booming, and began to develop a resident population of writers and artists, as well as a summer tourist industry. After the 1898 Portland Gale severely damaged the town's fishing industry, members of the town's art community took over many of the abandoned buildings. By the early decades of the 20th century, the town had acquired an international reputation for its artistic and literary productions. The Provincetown Players was an important experimental theatre company formed during this period. Many of its members lived during other parts of the year in Greenwich Village in New York, and intellectual and artistic connections were woven between the places. In 1898 Charles Webster Hawthorne opened the Cape Cod School of Art, said to be the first outdoor school for figure painting, in Provincetown. Film of his class from 1916 has been preserved.
The town includes eight buildings and two historic districts on the National Register of Historic Places: Provincetown Historic District and Dune Shacks of Peaked Hill Bars Historic District.
In the mid-1960s, Provincetown saw population growth. The town's rural character appealed to the hippies of the era; property was relatively cheap and rents were correspondingly low, especially during the winter. Many of those who came stayed and raised families. Commercial Street, the town's equivalent to "Main Street", gained numerous cafés, leather shops, head shops – various hip small businesses blossomed and many flourished.
By the 1970s Provincetown had a significant gay population, especially during the summer tourist season, when restaurants, bars and small shops serving the tourist trade were open. There had been a gay presence in Provincetown as early as the start of the 20th century as the artists' colony developed, along with experimental theatre. Drag queens could be seen in performance as early as the 1940s in Provincetown. In 1978 the Provincetown Business Guild (PBG) was formed to promote gay tourism. Today more than 200 businesses belong to the PBG, and Provincetown is perhaps the best-known gay summer resort on the East Coast. The 2010 US Census revealed Provincetown to have the highest rate of same-sex couples in the country, at 163.1 per 1000 couples.
Since the 1990s, property prices have risen significantly, causing some residents economic hardship. The housing bust of 2005 - 2012 caused property values in and around town to fall by 10 percent or more in less than a year. This did not slow down the town's economy, however. Provincetown's tourist season has expanded, and the town has scheduled created festivals and week-long events throughout the year. The most established are in the summer: the Portuguese Festival, Bear Week and PBG's Carnival Week.
Geography.
Provincetown is located at the very tip of Cape Cod, encompassing a total area of − 55% of that, or , is land area, and the remaining water area. Surrounded by water in every direction except due east, the town has of coastal shoreline. Provincetown is bordered to the east by its only neighbor, the town of Truro, and by Provincetown Harbor to the southeast, Cape Cod Bay to the south and west, Massachusetts Bay to the northwest and north, and the Atlantic Ocean to the northeast.
The town is north (by road) from Barnstable, Hyannis, Massachusetts and by road to the Sagamore Bridge, which spans the Cape Cod Canal and connects Cape Cod to the mainland. Provincetown is east by southeast from Boston by air or sea, and by road.
About 4,500 acres, or about 73% of the town's land area, is owned by the National Park Service, which operates the Cape Cod National Seashore, leaving about of land under the town's jurisdiction. To the north lie the "Province Lands", the area of dunes and small ponds extending from Mount Ararat in the east to Race Point in the west, along the Massachusetts Bay shore. The Cape Cod Bay shoreline extends from Race Point to the far west, to Wood End in the south, eastward to Long Point, which in turn points inward towards the town, and provides a natural barrier for Provincetown Harbor. All three points are marked by lighthouses. The town's population center extends along the harbor, south of the Seashore's lands.
Mount Ararat was named after Noah's landing place, while Mount Gilboa, and another dune, was named for the mountain described in the book of Samuel.
Climate.
The water surrounding Provincetown has the effect of moderating temperatures, such that the entire town is included in USDA plant hardiness zone 7a, which indicates an average annual extreme minimum temperature (1976–2005) of between . The water also has the effect of delaying the onset of the seasons, by keeping spring temperatures cooler and fall temperatures warmer than the rest of the state.
Under the Köppen climate classification the climate could be described as transitional between oceanic and humid subtropical, with some continental influences. The July mean of is narrowly below the isotherm for subtropical. If the more seldom used isotherm for continental is used, its climate could be described as the latter, indicating the transitional nature of the climatic conditions on the peninsula.
Transportation.
Historic transportation.
For nearly all of Provincetown's recorded history, life has revolved around the waterfront − especially the waterfront on its southern shore − which offers a naturally deep harbor with easy and safe boat access, plus natural protection from the wind and waves. An additional element of Provincetown's geography tremendously influenced the manner in which the town evolved: the town was physically isolated, being at the hard-to-reach tip of a long, narrow peninsula.
The East Harbor, which provided the most protected mooring place in Provincetown, had a inlet from Provincetown Harbor, and effectively blocked off access to Provincetown by land. Until the late 19th century, no road led to Provincetown – the only land route connecting the village to points back toward the mainland was along a thin stretch of beach along the shore to the north (known locally as the "backshore"). A wooden bridge was erected over the East Harbor in 1854, only to be destroyed by a winter storm and ice two years later. Although the bridge was replaced the following year, any traveler who crossed it still needed to traverse several miles over sand routes, which, together with the backshore route, was occasionally washed out by storms. This made Provincetown very much like an island. Its residents relied almost entirely upon its harbor for its communication, travel, and commerce needs.
That changed in 1868, when the mouth of the East Harbor was diked to enable the laying of track for the arrival of the railroad. The railroad was completed, to great fanfare, in 1873; and the wooden bridge and sand road was finally replaced by a formal roadway in 1877. The railroad terminated at Railroad Wharf, known today as MacMillan Wharf. It provided an easy means for fishermen to offload their vessels and ship their catch to the cities by rail.
The railroad was not the only late arrival to Provincetown. Even roads "within" the town were slow to be constructed: 
The town's internal road layout reflects the historic importance of the waterfront, the key to communication and commerce with the outside world. As the town grew, it organically expanded along the harborfront. The main "thoroughfare" was the hard-packed beach, where all commerce and socializing took place. Early deeds refer to a "Town Rode", which was little more than a footpath that ran behind the houses. In 1835, County Commissioners turned that into "Front Street", now known as Commercial Street. "Back Street" ran parallel to Front Street, but was set back from the harbor − today it is known as Bradford Street. 
Modern-day transportation.
Provincetown is the eastern terminus of U.S. Route 6, both in the state and in the nation. Although the terminus is directed east officially, geographically speaking, the road, having curved around Cape Cod, is facing west-southwest at the point, and is marked only by its junction with Route 6A. The state-controlled portion ends with a "" sign as the road enters the Cape Cod National Seashore, after which the road is under federal maintenance. Route 6A passes through the town as well, mostly following Bradford Street (whereas US 6 originally followed Commercial Street before the bypass was built and Commercial Street was switched to one-way westbound), and ending just south of the Herring Cove Beach.
Provincetown is served by two seasonal ferries to Boston and one to Plymouth. They all dock at MacMillan Pier, located just east of the Town Hall in the center of town. The town has no rail service; the town's only railway operated from 1873 until the early 1960s, when it was abandoned by the New York, New Haven and Hartford Railroad. A large portion of the "road" later converted into three roads (Harry Kemp Way, Railroad Avenue and Rear Howland) plus the "Old Colony Nature Pathway", a pedestrian path and greenway.
The Cape Cod Regional Transit Authority offers flex route buses between MacMillan Pier and Harwich and a shuttle to Truro. Provincetown is at one end of the scenic "Bike Route 1" from Boston called the Claire Saltonstall Bikeway.
The Provincetown Municipal Airport is located just east of Race Point. This airport is surrounded by the Cape Cod National Seashore, and is used mostly for General Aviation, but does receive regular scheduled service to Boston or White Plains, New York (with optional car service to Manhattan) via Cape Air, which also operates code-share flights for JetBlue. The airport is a well-equipped, if small, general-aviation airport with a single runway, an ILS approach, and full lighting. The nearest national and international service is from Logan International Airport in Boston.
Demographics.
United States census information.
According to the U.S. census of 2010, there were 2,942 people residing in the town (down 14.3% since 2000). The population density was . There were 4,494 housing units (up 15.5%) at an average density of . The racial makeup of the town was 91.5% White, 4.0% African American, 0.6% Native American, 0.6% Asian, 1.6% from other races, and 1.7% from two or more races. Hispanic or Latino of any race were 4.8% of the population.
The top reported ancestries were Irish (26.7%, up 9.3% from 2010), English (17.4%, up 2.6%), Portuguese (14.6%, down 8.2%), Italian (13.5%, up 3.4%), and German (12.5%, up 3.6%).
There were 1,765 households (down 3.9%), out of which 416 (23.6%) had families, 115 (6.5%) had children under the age of 18 living within them, and 76.4% were non-families. The average household size was 1.64 persons/household, and the average family size was 2.55.
The distribution of the population, broken down by age and gender, is shown in the population pyramid. In 2010, 6.8% of the population was under the age of 18, and the median age was 52.3. There were 1,602 males and 1,340 females.
For 2011, the estimated median income for a year-round household in the town was $46,547, with a mean household income of $74,840. For families, the median income was $87,228, and the mean is $84,050. For nonfamily households, the median income was $42,375, and the mean, $71,008. Median earnings for male full-time, year-round workers was $49,688, versus $36,471 for females. The per capita income for the town was $41,488. About 2.1% of families and 15.4% of the population were below the poverty line, including 26.0% of those under age 18 and 7.5% of those age 65 or over.
Provincetown's ZIP code has the highest concentration of same-sex couple households of any ZIP code in the United States.
Demographics in a resort town.
Data from traditional demographic sources like the U.S. Census, municipal voting rolls and property records may not accurately portray the demography of resort towns. They often reveal unusual results, as in this case, where the number of housing units far exceeds the Town's total population, where that number of housing units rose 15% while the population dropped 14%, and where nearly 61% of the housing stock is vacant, with 53% designated "for seasonal, recreational, or occasional use", according to the census.
In the decade spanning the years 2000 through 2010, Provincetown's small year-round population declined 14.3% from 3,431 to 2,942, yet during the summer months, population estimates vary wildly, ranging from 19,000 to 60,000. Census figures are unable to capture these dynamic population fluctuations that are associated with seasonal tourism. Part-time residents are not counted in the census. These people may own a second home in the town or pay rent for up to six months each year. Many of them pay property and other taxes, hold jobs in the community and even own businesses.
Government.
Provincetown is governed, like most New England towns, by the Open town meeting form of government. In the Town Meeting form of government, the citizens, gathered in the town meeting, act as the legislative branch and approve the budget and amend the town's bylaws, while the popularly elected Board of Selectmen act as the executive branch and hire and oversee the Town Manager, meet regularly to determine policy and appoint members of other boards and commissions.
Provincetown is represented in the Massachusetts House of Representatives as a part of the Fourth Barnstable District, which includes (with the exception of Brewster) all the towns east and north of Harwich on the Cape. The seat is held by Democrat Sarah Peake, a former Provincetown selectman. The town is represented in the Massachusetts Senate as a part of the Cape and Islands District, which includes all of Cape Cod, Martha's Vineyard and Nantucket except the towns of Bourne, Falmouth, Sandwich and a portion of Barnstable. The Senate seat is held by Democrat Dan Wolf, President of Cape Air. Provincetown is patrolled by its own Police Department as well as the Second (Yarmouth) Barracks of Troop D of the Massachusetts State Police.
On the national level, Provincetown is a part of Massachusetts's 9th congressional district, and is currently represented by Bill Keating. Following the death of Ted Kennedy, the state's senior (Class I) member of the United States Senate was John Kerry (last re-elected in 2008) until he became Secretary of State; that seat has been occupied by Ed Markey since July 16, 2013. The other (Class II) senate seat is held by Elizabeth Warren, a Democrat, elected in the November 2012 elections and sworn in as senator in January 2013. 
Provincetown is governed by the open town meeting form of government, and is led by a town manager and a board of selectmen. The town has its own police and fire departments, both of which are stationed on Shankpainter Road. The town's post office is located along Commercial Street, near the town's Fourth Wharf. The Provincetown Public Library is a member of the Cape Libraries Automated Materials Sharing library network and is also located along Commercial Street, in the former Center Methodist Episcopal Church building since 2005.
Education.
Provincetown Schools is an International Baccalaureate World School. Verified in 2013 in the prestigious Primary Years Program and in 2014 in the Middle Years program. Provincetown Schools formally joins the IB community as an IB World School - PYP and MYP continuum program. This is a unique teaching and learning environment where students are encouraged to be global citizens, creative thinkers and open-minded learners. Provincetown Schools is now the only school in Massachusetts to offer the IB continuum, grades PK-8.
Provincetown Schools educates approximately 120 children in Grades PreK - Grade 8. The Veterans Memorial Community Center houses Provincetown Schools Early Learning Center (Wee Care and Preschool ages 3–5).
In 2010, the Provincetown school board elected to phase out the high school program Provincetown High School at the end of the 2012−2013 school year, and send students to nearby Nauset Regional High School in North Eastham, beginning with the 2013−2014 academic year. Provincetown High School's last senior class graduated on June 7, 2013. The final Senior class, consisted of eight students. There are no private schools in Provincetown; high school students from the town will now attend Cape Cod Regional Technical High School in Harwich or Nauset Regional High School in North Eastham. Prior to its closing, Provincetown High School (PHS) served students from seventh through twelfth grades (and for a time also accepted students from Truro). In 2012, Provincetown High School was recognized as one of the smallest high schools in the country with a student population of 32 students in grades 10-12.
PHS's sports teams were known as the Fishermen, and the school colors were black and orange.
Culture.
The Fine Arts Work Center is a nonprofit educational enterprise, located in Provincetown since 1968. Its stated mission is to encourage the growth and development of emerging visual artists and writers through residency programs, to propagate aesthetic values and experience, and to restore the year-round vitality of the historic art colony of Provincetown.
Provincetown Art Association and Museum (PAAM) is a nationally recognized, year-round cultural institution that celebrated its Centennial in 2014. PAAM mounts 35 art exhibitions each year, offers workshops in the fine arts for children, youth, and adults, and hosts an array of programs and events to enrich visitor experience. The PAAM Permanent Collection consists of 3,000 objects, which are displayed throughout the year in the PAAM galleries. 
Between 2004 and 2007, PAAM received four Rural Development grants and loans totaling $3 million to increase the museum's space, add climate-controlled facilities, renovate a historic sea captain's house (the Hargood House) and cover cost overruns. As the mission of the Rural Development program is "To increase economic opportunity and improve the quality of life for all rural Americans", the USDA considered Provincetown's residents in the 2000s to still be rural and to still require such federal assistance.
In 2003, Provincetown received a $1.95 million low interest loan from the Rural Development program of the U.S. Department of Agriculture to help rebuild the town's MacMillan Pier. It primarily serves the town's active fishing fleet, and also tourists and high-speed ferries.
The Atlantic House in Provincetown is considered the oldest gay bar in the US and Frommer's calls it "the nation's premier gay bar".
The Art House provides a venue for numerous entertainers and shows during the summer season, in particular Varla Jean Merman, Miss Richfield 1981, Ms. CoCo Peru, and other town favorites. In off season, the Art House remains open providing nightly entertainment that includes a Wii Bowling League, Trivia Night, and similar events.
The Provincetown International Film Festival, honors the best in independent and avante garde film. Among the honorees for 2014 were actress Patricia Clarkson and director David Cronenberg. Previous honorees include Matt Dillon, Harmony Korine, Parker Posey, Roger Corman, Vera Farmiga, Darren Aronofsky, Quentin Tarantino, Jane Lynch, Gael García Bernal, Tilda Swinton, Kathleen Turner, Jim Jarmusch, Todd Haynes, Gus Van Sant, and John Waters. Waters, a summer resident, is a major participant in the festival.
In November, 2011, the Provincetown Theater Company became the first theater company in New England to stage a live-action dramatic theatrical presentation of horror-fantasy author H.P. Lovecraft. The story was Lovecraft's 1919 classic, "The Picture in the House," and was described as "...the macabre come to life." The adaptation was produced for the 22nd Fall Playwright's Festival.

</doc>
<doc id="39056" url="https://en.wikipedia.org/wiki?curid=39056" title="Saint Anselm (disambiguation)">
Saint Anselm (disambiguation)

St, St., or Saint Anselm usually refers to the Anselm who was archbishop of Canterbury in the 11th and 12th centuries.
Saint Anselm may also refer to:

</doc>
<doc id="39057" url="https://en.wikipedia.org/wiki?curid=39057" title="Straw man">
Straw man

A straw man is a common form of argument and is an informal fallacy based on giving the impression of refuting an opponent's argument, while actually refuting an argument that was not advanced by that opponent.
The so-called typical "attacking a straw man" argument creates the illusion of having completely refuted or defeated an opponent's proposition by covertly replacing it with a different proposition (i.e. "stand up a straw man") and then to refute or defeat that false argument ("knock down a straw man") instead of the original proposition.
This technique has been used throughout history in polemical debate, particularly in arguments about highly charged emotional issues where a fiery, entertaining "battle" and the defeat of an "enemy" may be more valued than critical thinking or understanding both sides of the issue.
Allegedly, straw-man tactics were once known in some parts of the United Kingdom as an "Aunt Sally", after a pub game of the same name where patrons threw sticks or battens at a post to knock off a skittle balanced on top.
Origin.
As a fallacy, the identification and name of straw man arguments are of relatively recent date, although Aristotle makes remarks that suggest a similar concern; Douglas Walton identified "the first inclusion of it we can find in a textbook as an informal fallacy" in Stuart Chase's "Guides to Straight Thinking" from 1956 (p. 40). However, Hamblin's classic text "Fallacies" (1970) neither mentions it as a distinct type, nor even as a historical term. The idea of "men of straw" who can be knocked down by "the lightest puff, the smallest breath of truth," erected by invaders upon a field to scare away others who might join the movement, can be found in Victoria C. Woodhull's "The Scare-Crows of Sexual Slavery", written in 1873.
The origins of the term are unclear. The usage of the term in rhetoric suggests a human figure made of straw which is easily knocked down or destroyed, such as a military training dummy, scarecrow, or effigy. The rhetorical technique is sometimes called an Aunt Sally in the UK, with reference to a traditional fairground game in which objects are thrown at a fixed target. One common folk etymology is that it refers to men who stood outside courthouses with a straw in their shoe in order to indicate their willingness to be a false witness.
Structure.
The straw man fallacy occurs in the following pattern of argument:
This reasoning is a fallacy of relevance: it fails to address the proposition in question by misrepresenting the opposing position.
For example:
Examples.
Straw man arguments often arise in public debates such as a (hypothetical) prohibition debate:
The original proposal was to relax laws on beer. Person B has misconstrued/misrepresented this proposal by responding to it as if it had been something like "(we should have...) unrestricted access to intoxicants". It is a logical fallacy because Person A never advocated allowing said unrestricted access to intoxicants.
In a 1977 appeal of a U.S. bank robbery conviction, a prosecuting attorney said in his closing argument
This was a straw man designed to alarm the appeal judges; the idea that the precedent set by one case would literally make it impossible to convict "any" bank robbers is remote.
An example often given of a straw man is US President Richard Nixon's 1952 "Checkers speech". When campaigning for vice president in 1952, Nixon was accused of having illegally appropriated $18,000 in campaign funds for his personal use. In a televised response, instead of addressing the funds, he spoke about another gift, a dog he had been given by a supporter:
This was a straw man response; his critics had never criticized the dog as a gift or suggested he return it. This argument was successful at distracting many people from the funds, and portraying his critics as nitpicking and heartless. Nixon received an outpouring of public support and remained on the ticket. He and Eisenhower were elected by a landslide.
Christopher Tindale presents, as an example, the following passage from a draft of a bill (HCR 74) considered by the Louisiana State Legislature in 2001:
Tindale comments that "the portrait painted of Darwinian ideology is a caricature, one not borne out by any objective survey of the works cited. That similar misrepresentations of Darwinian thinking have been used to justify and approve racist practices is beside the point: the position that the legislation is attacking and dismissing is a Straw Man. In subsequent debate this error was recognized, and the eventual bill omitted all mention of Darwin and Darwinist ideology."
Contemporary work.
In 2006, Robert Talisse and Scott Aikin expanded the application and use of the straw man fallacy beyond that of previous rhetorical scholars, arguing that the straw man fallacy can take two forms, the original form in which the opponent's position is misrepresented, which they call the and a new form which they call the .
The selection form focuses on a partial and weaker (and easier to refute) representation of the opponent's position. Then the easier refutation of this weaker position is claimed to refute the opponent's complete position. They point out the similarity of the selection form to the fallacy of hasty generalization, in which the refutation of an opposing position that is weaker than the opponent's is claimed as a refutation of all opposing arguments. Because they have found significantly increased use of the selection form in modern political argumentation, they view its identification as an important new tool for the improvement of public discourse.
Aikin and Casey expanded on this model in 2010, introducing a third form. Referring to the "representative form" as the classic , and the "selection form" as the , a third form is called the . A hollow man argument is one that is a complete fabrication, where both the viewpoint and the opponent expressing it do not in fact exist, or at the very least the arguer has never encountered them. Such arguments frequently takes the form of vague phrasing such as "some say," "someone out there thinks" or similar weasel words, or it might attribute a non-existent argument to a broad movement in general, rather than an individual or organization.
A variation on the selection form, or "weak man" argument, that combines with an ad hominem is , a neologism coined by Kevin Drum. A portmanteau of "nut" (i.e., insane person) and cherry picking, nut picking refers to intentionally seeking out extremely fringe, non-representative statements and/or individuals from members of an opposing group and parading these around as evidence of that entire group's incompetence or irrationality.

</doc>
<doc id="39058" url="https://en.wikipedia.org/wiki?curid=39058" title="Wellfleet, Massachusetts">
Wellfleet, Massachusetts

Wellfleet is a town in Barnstable County, Massachusetts, United States, and is located halfway between the "tip" and "elbow" of Cape Cod. The town had a population of 2,750 at the 2010 census, which swells nearly sixfold during the summer. A total of 70% of the town's land area is in protection, and nearly half of it is part of the Cape Cod National Seashore. Wellfleet is famous for its eponymous oysters, which are celebrated in the annual October Wellfleet OysterFest.
History.
Wellfleet was encountered by Europeans as early as 1606, when the French explorer Samuel de Champlain explored and named it "Port Aux Huitres" (Oyster Port) for the bountiful oyster population resident to the area. Originally settled in the 1650s by the Europeans as Billingsgate (after the famous fish market in East London), Wellfleet was part of neighboring Eastham until 1763, achieving town status after nearly 30 years of petitioning.
Wellfleet's oyster beds drove the early economy, as did whaling and other fishing endeavors. The town was home to 30 whaling ships at the time of the American Revolution. However, because of the decline of whaling and the mackerel catch in the late 19th century, the fleet declined, being completely free of schooners by 1900. The oyster fleet continues to this day, however, harvesting many other types of shellfish as well.
Guglielmo Marconi built America's first transatlantic radio transmitter station on a coastal bluff in South Wellfleet in 1901–02. The first radio telegraph transmission from America to England was sent from this station on January 18, 1903, a ceremonial telegram from President Theodore Roosevelt to King Edward VII. Most of the transmitter site is gone, however, as three quarters of the land it originally encompassed has been eroded into the sea. The South Wellfleet station's first call sign was "CC" for Cape Cod.
In 1961, President John F. Kennedy created the Cape Cod National Seashore, which encompasses most of the Atlantic shoreline of Cape Cod. In Wellfleet the territory circles the town, from Jeremy Point through the marshes and "islands" along the Herring River, includes Cahoon Hollow Beach, and extends the length of the Atlantic shore of the town.
Construction of the Chequesset Inn in the late 19th century contributed to the development of a tourist economy in Wellfleet. The town has the second greatest concentration of art galleries on Cape Cod, right after Provincetown. It is also a popular retirement spot.
In 1717, the pirate "Black Sam" Bellamy was sailing near what is now Wellfleet when his ship, the "Whydah", sank off shore, together with over of gold and silver and all but two of its 145 men. The wreck was discovered in 1984, the first of only two confirmed pirate shipwrecks ever to have been discovered.
Geography.
According to the United States Census Bureau, the town has a total area of , of which is land and , or 44.11%, is water. Wellfleet is bordered by Truro to the north, the Atlantic Ocean to the east, Eastham to the south, and Cape Cod Bay to the west. Wellfleet is approximately south of Provincetown, (by road) northeast of Barnstable, from the Sagamore Bridge, and (by road) southeast of Boston.
The lands of Wellfleet wrap around Wellfleet Harbor, extending from the main portion of the Cape around the harbor to Jeremy Point. At one time, Wellfleet Harbor included an island known as Billingsgate Island, which sat at the harbor's mouth, to the south of the point. Once a flourishing small community with a lighthouse, the island was destroyed by coastal erosion and now exists as a shoal that is exposed at low tide. The Billingsgate shoals are split between Wellfleet and neighboring Eastham. Several other inlets extend inland from the harbor, at the mouth of the Herring River (also called "The Gut"), Duck Creek, Blackfish Creek and Fresh Brook (commonly known as "The Run") which leads to several brooks.
In addition to the Seashore, Wellfleet Bay Wildlife Sanctuary, run by Massachusetts Audubon, surrounds much of The Run, including part of Small Island (between The Run and Blackfish Creek). Between the sanctuary, seashore and other small parks and beaches, seventy percent of the town's area is protected.
A small whaling community was founded on the land that is now Wellfleet Bay Wildlife Sanctuary, and was originally known as Silver Spring, after Silver Spring Brook. What remains of it is a marsh that was once its harbor, known as the Silver Spring Brook Marshes. This land is now protected by the Massachusetts Audubon Society in its Wellfleet Bay Sanctuary.
Transportation.
U.S. Route 6 passes from north to south through the town. The town's commercial center lies west of the route, along the shores of the harbor. The route was straightened in the mid-20th century, and some maps still consider the "old" Route 6 to be a portion of Route 6A. The town has no rail or air service. The last train left the area in the 1930s, the train station was razed and the tracks were torn up through Provincetown. The nearest municipal airports are in Chatham and Provincetown, both about from town; the nearest national and international service can be found at Logan International Airport in Boston.
There is currently limited bus service between Wellfleet and Hyannis, and from there on to Boston and Logan Airport, on the Plymouth & Brockton Street Railway Company, a Plymouth-based bus service. The CCRTA, which runs between Hyannis and Provincetown, also makes stops in Wellfleet.
Demographics.
As of the census of 2000, there were 2,749 people, 1,301 households, and 724 families residing in the town. The population density was . There were 3,998 housing units at an average density of . The racial makeup of the town was 96.58% White, 0.95% African American, 0.29% Native American, 0.36% Asian, 0.04% Pacific Islander, 0.58% from other races, and 1.20% from two or more races. Hispanic or Latino of any race were 0.69% of the population.
There were 1,301 households out of which 20.0% had children under the age of 18 living with them, 44.8% were married couples living together, 8.2% had a female householder with no husband present, and 44.3% were non-families. 34.8% of all households were made up of individuals and 13.2% had someone living alone who was 65 years of age or older. The average household size was 2.11 and the average family size was 2.75.
In the town the population was spread out with 17.8% under the age of 18, 4.9% from 18 to 24, 23.3% from 25 to 44, 32.2% from 45 to 64, and 21.7% who were 65 years of age or older. The median age was 47 years. For every 100 females there were 89.2 males. For every 100 females age 18 and over, there were 83.5 males.
The median income for a household in the town was $43,558, and the median income for a family was $50,990. Males had a median income of $38,100 versus $35,964 for females. The per capita income for the town was $25,712. About 5.7% of families and 7.5% of the population were below the poverty line, including 9.7% of those under age 18 and 6.0% of those age 65 or over.
Government.
Wellfleet is represented in the Massachusetts House of Representatives as a part of the Fourth Barnstable district, which includes (with the exception of Brewster) all the towns east and north of Harwich on the Cape. The town is represented in the Massachusetts Senate as a part of the Cape and Islands District, which includes all of Cape Cod, Martha's Vineyard and Nantucket except the towns of Bourne, Falmouth, Sandwich and a portion of Barnstable. Wellfleet is patrolled by the Second (Yarmouth) Barracks of Troop D of the Massachusetts State Police.
On the national level, Wellfleet is a part of the 9th congressional district, currently represented by Bill Keating. The state's senior (Class I) member of the United States Senate, elected in 2012, is Elizabeth Warren. The junior (Class II) senator, elected on April 30, 2013, is Ed Markey.
Wellfleet is governed by the open town meeting form of government and a board of selectmen, who employ a town administrator to oversee day-to-day business. The town has its own police and fire departments, headquartered on Route 6 near the town center. There are two post offices, and both are also located along Route 6. The Wellfleet Public Library is located in the town center, in a former curtain and candle factory converted in 1989.
Education.
Wellfleet, Brewster, Eastham and Orleans make up the Nauset Regional School District. Each town operates its own elementary schools, with a regional middle school and high school accepting the students of all four towns. Wellfleet Elementary School is located just off Route 6 near the town center, and serves students from kindergarten to fifth grade. The Nauset Regional Middle School is located in Orleans, and the Nauset Regional High School is located in neighboring Eastham. There are no private schools in Wellfleet; high school students may, however, choose to attend Cape Cod Regional Technical High School in Harwich free of charge.
Notable people.
Notable current and former residents of Wellfleet include:

</doc>
<doc id="39059" url="https://en.wikipedia.org/wiki?curid=39059" title="European bison">
European bison

The European bison ("Bison bonasus"), also known as wisent ( or ) or the European wood bison, is a Eurasian species of bison. It is one of two extant species of bison, alongside the American bison. Three subspecies have existed in the past, but only one survives today.
European bison were hunted to extinction in the wild in the early 20th century, with the last wild animals of the "B. b. bonasus" species being shot in the Białowieża Forest (on the Poland-Belarus border) in 1921, and the "B. b. caucasus" in the northwestern Caucasus in 1927. "(B. b. hungarorum" was hunted to extinction in the mid 1800s.) They have since been reintroduced from captivity into several countries in Europe, all descendants of the Białowieża or lowland European bison. They are now forest-dwelling. They have few predators (besides humans), with only scattered reports from the 19th century of wolf and bear predation. European bison were first scientifically described by Carl Linnaeus in 1758. Some later descriptions treat the European bison as conspecific with the American bison. It is not to be confused with the aurochs, the extinct ancestor of domestic cattle.
In 1996, the International Union for Conservation of Nature (IUCN) classified the European bison as an endangered species. Its status has since been changed to being a vulnerable species. In the past, especially during the Middle Ages, it was commonly killed for its hide, and to produce drinking horns.
Description.
The European bison is the heaviest surviving wild land animal in Europe; a typical European bison is about long, not counting a tail of long, and tall. At birth, calves are quite small, weighing between . In the free-ranging population of the Białowieża Forest of Belarus and Poland, body masses among adults (aged 6 and over) are on average in the cases of males, with a range of , and of among females, with a range of . An occasional big bull European bison can weigh up to or more.
On average, it is slightly lighter in body mass and yet taller at the shoulder than the American bison ("Bison bison"). Compared to the American species, the wisent has shorter hair on the neck, head, and forequarters, but longer tail and horns.
Etymology.
The modern English word 'wisent' was borrowed in the 19th century from modern German "Wisent" [], itself from Old High German "wisunt", "wisant", related to Old English "wesend", "weosend", and Old Norse "vísundr". The Old English cognate disappeared as the bison's range shrank away from English-speaking areas by the Late Middle Ages.
The English word 'bison' was borrowed around 1611 from Latin "bisōn" (pl. "bisontes"), itself from Germanic. The root *"wis"-, also found in "weasel", originally referred to the animal's musk.
The word "bonasus" was first mentioned by Aristotle in the 4th century BC when he precisely described the animal, calling it bόνασος ("bonasus") in Greek. He also noted that the Paeonians called it mόναπος ("monapos").
History.
Historically, the lowland European bison's range encompassed all lowlands of Europe, extending from the Massif Central to the Volga River and the Caucasus. It may have once lived in the Asiatic part of what is now the Russian Federation. Its range decreased as human populations expanded cutting down forests. The last references (Oppian, Claudius Aelianus) to the animal in the transitional Mediterranean/Continental biogeographical region in the Balkans in the area of modern borderline between Greece, Macedonia and Bulgaria date to 3rd century AD. The population of Gaul was extinct in the 8th century AD. The European bison became extinct in southern Sweden in the 11th century, and Southern England in the holocene. The species survived in the Ardennes and the Vosges Mountains until the 15th century. In the Early Middle Ages, the wisent apparently still occurred in the forest steppes east of the Urals, in the Altay Mountains, and seems to have reached Lake Baikal in the east. The northern boundary in the Holocene was probably around 60°N in Finland.
European bison survived in a few natural forests in Europe, but its numbers dwindled. The last European bison in Transylvania died in 1790. In Poland, European bison in the Białowieża Forest were legally the property of the Polish kings until the third partition of Poland. Wild European bison herds also existed in the forest until the mid-17th century. Polish kings took measures to protect the bison. King Sigismund II Augustus instituted the death penalty for poaching a European bison in Białowieża in the mid-16th century. In the early 19th century, Russian czars retained old Polish laws protecting the European bison herd in Białowieża. Despite these measures and others, the European bison population continued to decline over the following century, with only Białowieża and Northern Caucasus populations surviving into the 20th century.
During World War I, occupying German troops killed 600 of the European bison in the Białowieża Forest for sport, meat, hides, and horns. A German scientist informed army officers that the European bison were facing imminent extinction, but at the very end of the war, retreating German soldiers shot all but nine animals. The last wild European bison in Poland was killed in 1921. The last wild European bison in the world was killed by poachers in 1927 in the western Caucasus. By that year, fewer than 50 remained, all held by zoos.
To help manage this captive population, Dr. Heinz Heck began the first studbook for a nondomesticated species, initially as a card index in 1923, leading to a full publication in 1932.
Genetic history.
A 2003 study of mitochondrial DNA indicated four distinct maternal lineages in the tribe Bovini:
Y chromosome analysis associated wisent and American bison. An earlier study, using amplified fragment-length polymorphism fingerprinting, showed a close association of wisent and American bison and probably with yak. It noted the interbreeding of Bovini species made determining relationships problematic. 
European bison can cross-breed with American bison. The products of a German interbreeding programme were destroyed after the Second World War. This programme was related to the impulse which created the Heck cattle. The cross-bred individuals created at other zoos were eliminated from breed books by the 1950s. A Russian back-breeding programme resulted in a wild herd of hybrid animals, which presently lives in the Caucasian Biosphere Reserve (550 animals in 1999).
Wisent-cattle hybrids also occur, similar to North America beefalo. Cattle and European bison hybridise fairly readily, but the calves cannot be born naturally (birth is not triggered correctly by the first-cross hybrid calf, and they must therefore be delivered by Caesarian section). First-generation males are infertile. In 1847, a herd of wisent-cattle hybrids named "żubroń" was created by Leopold Walicki. The animals were intended to become durable and cheap alternatives to cattle. The experiment was continued by researchers from the Polish Academy of Sciences until the late 1980s. Although the program resulted in a quite successful animal that was both hardy and could be bred in marginal grazing lands, it was eventually discontinued. Currently the only surviving żubroń herd consists of just a few animals in Białowieża Forest, Poland and Belarus.
Differences from American bison.
Although superficially similar, a number of physical and behavioural differences are seen between the European bison and the American bison. The European bison has 14 pairs of ribs, while the American bison has 15. Adult European bison are (on average) taller than American bison, and have longer legs. European bison tend to browse more, and graze less than their American relatives, due to their necks being set differently. Compared to the American bison, the nose of the European bison is set further forward than the forehead when the neck is in a neutral position.
The body of the European bison is less hairy, though its tail is hairier than that of the American species. The horns of the European bison point forward through the plane of their faces, making them more adept at fighting through the interlocking of horns in the same manner as domestic cattle, unlike the American bison, which favours charging. European bison are less tameable than the American ones, and breed with domestic cattle less readily.
Behaviour and biology.
Social structure and territorial behaviours.
The European bison is a herd animal, which lives in both mixed and solely male groups. Mixed groups consist of adult females, calves, young aged 2–3 years and young adult bulls. The average herd size is dependent on environmental factors, though on average, they number eight to 13 animals per herd. Herds consisting solely of bulls are smaller than mixed ones, containing two individuals on average. European bison herds are not family units. Different herds frequently interact, combine and quickly split after exchanging individuals.
Territory held by bulls is correlated by age, with young bulls aged between five and six tending to form larger home ranges than older males. The European bison does not defend territory, and herd ranges tend to greatly overlap. Core areas of territory are usually sited near meadows and water sources.
Reproduction.
The rutting season occurs from August through to October. Bulls aged 4–6 years, though sexually mature, are prevented from mating by older bulls. Cows usually have a gestation period of 264 days, and typically give birth to one calf at a time.
On average, male calves weigh at birth, and females . Body size in males increases proportionately to the age of 6 years. While females have a higher increase in body mass in their first year, their growth rate is comparatively slower than that of males by the age of 3–5. Bulls reach sexual maturity at the age of two, while cows do so in their third year.
European bison have lived as long as 30 years in captivity, although in the wild their lifespans are shorter. Productive breeding years are between four and 20 years of age in females, and only between six and 12 years of age in males.
Diet.
European bison feed predominantly on grasses, although they will also browse on shoots and leaves; in summer months, an adult male can consume 32 kg of food in a day. European bison in the Białowieża Forest in Poland have traditionally been fed hay in the winter for centuries, and vast herds may gather around this diet supplement. European bison need to drink every day, and in winter can be seen breaking ice with their heavy hooves. Despite their usual slow movements, European bison are surprisingly agile and can clear 3-m-wide streams or 2-m-high fences from a standing start.
Conservation.
The protection of the European bison has a long history; between the 15th and 18th centuries, those in the Forest of Białowieża were protected and their diet supplemented. Efforts to restore this species to the wild began in 1929, with the establishment of the Bison Restitution Centre at Białowieża, Poland. Subsequently, in 1948, the Bison Breeding Centre was established within the Prioksko-Terrasny Biosphere Reserve.
The modern herds are managed as two separate lines – one consisting of only "Bison bonasus bonasus" (all descended from only seven animals) and one consisting of all 12 ancestors, including the one "B. b. caucasicus" bull. The latter is generally not considered a separate sub-species because they contain DNA from both "B. b. bonasus" and "B. b. caucasicius", although some scientists classify them as a new subspecies, "B. b. montanus". Only a limited amount of inbreeding depression from the population bottleneck has been found, having a small effect on skeletal growth in cows and a small rise in calf mortality. Genetic variability continues to shrink. From five initial bulls, all current European bison bulls have one of only two remaining Y chromosomes.
Reintroduction.
Beginning in 1951, European bison have been reintroduced into the wild. Free-ranging herds are currently found in Poland, Lithuania, Belarus, Ukraine, Romania, Russia, Slovakia, Latvia, Kyrgyzstan, Germany and in forest preserves in the Western Caucasus. Białowieża Forest, an ancient woodland that straddles the border between Poland and Belarus, is now home to 800 wild bison. Herds have also been introduced in Moldova (2005), Spain (2010), Denmark (2012), Bulgaria (2012) and Czech Republic (2014). 
Numbers and distribution.
Numbers.
The total worldwide population is around 4,663 (including 2,701 free-ranging) and has been increasing.
Some local populations are estimated as follows:
Distribution.
Since 1983, a small reintroduced population lives in the Altai Mountains. This population suffers from inbreeding depression and needs the introduction of unrelated animals for "blood refreshment". In the long term, authorities hope to establish a population of about 1,000 animals in the area. One of the northernmost current populations of the European bison lives in the Vologodskaya Oblast in the Northern Dvina River valley at about 60°N. It survives without supplementary winter feeding. Another Russian population lives in the forests around the Desna River on the border between Russia and Ukraine. The north-easternmost population lives in Pleistocene Park south of Chersky in Siberia, a project to recreate the steppe ecosystem which began to be altered 10,000 years ago. Five wisents were introduced on 24 April 2011. The wisents were brought to the park from the Prioksko-Terrasny Nature Reserve near Moscow. Winter temperatures often drop below -50 °C. Four of the five bison have subsequently died due to problems acclimatizing to the low winter temperature.
In June 2012, one male and six females were moved to the Danish island of Bornholm. The plan is to release these animals into the wild after five years of adjusting to the island's environment. The plan is that the bison will aid biodiversity by naturally maintaining open grassland.
In 2011, three bison were introduced into Alladale Wilderness Reserve in Scotland. Plans to move more into the reserve were made, but the project failed due to not being "well thought through". In April 2013, eight European bison (one male, five females, and two calves) were released into the wild in the Bad Berleburg region of Germany, after 850 years of absence since the species became extinct in that region.
Plans are being made to reintroduce two herds in Germany and in the Netherlands in Oostvaardersplassen Nature Reserve in Flevoland as well as the Veluwe. In 2007, a bison pilot project in a fenced area was begun in Zuid-Kennemerland National Park in the Netherlands. Because of their limited genetic pool, they are considered highly vulnerable to illnesses such as foot-and-mouth disease. In March 2016, a herd was released in the Maashorst Nature Reserve in North Brabant. Zoos in 30 countries also have quite a few bison involved in captive breeding programs. 

</doc>
<doc id="39061" url="https://en.wikipedia.org/wiki?curid=39061" title="Argumentum ad baculum">
Argumentum ad baculum

Argumentum ad baculum (Latin for "argument to the cudgel" or "appeal to the stick"), also known as appeal to force, is an argument where force, coercion, or the "threat of force", is given as a justification. It is a specific case of the negative form of an argument to the consequences. 
The fallacious "ad baculum".
A fallacious logical argument based on "argumentum ad baculum" generally proceeds as follows:
This form of argument is an informal fallacy, because the attack on Q may not necessarily reveal anything about the truth value of the premise P. This fallacy has been identified since the Middle Ages by many philosophers. This is a special case of "argumentum ad consequentiam", or "appeal to consequences".
The non-fallacious "ad baculum".
This argument is of the form:
The fallacy in the argument lies in assuming that the truth value of "x accepts P" is related to the truth value of P itself. Whether x does actually accept P, and whether P is true can not be inferred from the available statements. However, the argument can be changed into a valid modus tollens by changing the conclusion.
Example.
Note that this argument does not assert or come to any conclusion on whether Peter actually knows Jesus (cf. the fallacious conclusion "Therefore, Peter does not know Jesus").

</doc>
<doc id="39062" url="https://en.wikipedia.org/wiki?curid=39062" title="French and Indian War">
French and Indian War

The French and Indian War (1754–1763) was the North American theater of the worldwide Seven Years' War. The war was fought between the colonies of British America and New France, with both sides supported by military units from their parent countries of Great Britain and France, as well as Native American allies. At the start of the war, the French North American colonies had a population of roughly 60,000 European settlers, compared with 2 million in the British North American colonies. The outnumbered French particularly depended on the Indians. Long in conflict, the metropole nations declared war on each other in 1756, escalating the war from a regional affair into an intercontinental conflict.
The name "French and Indian War" is used mainly in the United States and refers to the two main enemies of the British colonists: the royal French forces and the various indigenous forces allied with them. British and European historians use the term the "Seven Years' War", as do English speaking Canadians. French Canadians call it La guerre de la Conquête (War of the Conquest) or (rarely) the Fourth Intercolonial War.
The war was fought primarily along the frontiers between New France and the British colonies, from Virginia in the South to Nova Scotia in the North. It began with a dispute over control of the confluence of the Allegheny and Monongahela rivers, called the Forks of the Ohio, and the site of the French Fort Duquesne and present-day Pittsburgh, Pennsylvania. The dispute erupted into violence in the Battle of Jumonville Glen in May 1754, during which Virginia militiamen under the command of 22-year-old George Washington ambushed a French patrol.
In 1755, six colonial governors in North America met with General Edward Braddock, the newly arrived British Army commander, and planned a four-way attack on the French. None succeeded and the main effort by Braddock was a disaster; he was defeated in the Battle of the Monongahela on July 9, 1755 and died a few days later. British operations in 1755, 1756 and 1757 in the frontier areas of Pennsylvania and New York all failed, due to a combination of poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies. In 1755, the British captured Fort Beauséjour on the border separating Nova Scotia from Acadia; soon afterward they ordered the expulsion of the Acadians. Orders for the deportation were given by William Shirley, Commander-in-Chief, North America, without direction from Great Britain. The Acadians, both those captured in arms and those who had sworn the loyalty oath to His Britannic Majesty, were expelled. Native Americans were likewise driven off their land to make way for settlers from New England.
After the disastrous 1757 British campaigns (resulting in a failed expedition against Louisbourg and the Siege of Fort William Henry, which was followed by Indian torture and massacres of British victims), the British government fell. William Pitt came to power and significantly increased British military resources in the colonies at a time when France was unwilling to risk large convoys to aid the limited forces it had in New France. France concentrated its forces against Prussia and its allies in the European theatre of the war. Between 1758 and 1760, the British military launched a campaign to capture the Colony of Canada. They succeeded in capturing territory in surrounding colonies and ultimately Quebec. Though the British were later defeated at Sainte Foy in Quebec, the French ceded Canada in accordance with the 1763 treaty.
The outcome was one of the most significant developments in a century of Anglo-French conflict. France ceded its territory east of the Mississippi to Great Britain. It ceded French Louisiana west of the Mississippi River (including New Orleans) to its ally Spain, in compensation for Spain's loss to Britain of Florida (Spain had ceded this to Britain in exchange for the return of Havana, Cuba). France's colonial presence north of the Caribbean was reduced to the islands of Saint Pierre and Miquelon, confirming Britain's position as the dominant colonial power in eastern North America.
The origin of the name.
The conflict is known by multiple names. In British America, wars were often named after the sitting British monarch, such as King William's War or Queen Anne's War. As there had already been a King George's War in the 1740s, British colonists named the second war in King George's reign after their opponents, and it became known as the "French and Indian War". This traditional name continues as the standard in the United States, but it obscures the fact that Indians fought on both sides of the conflict, and that this was part of the Seven Years' War, a much larger conflict between France and Great Britain. American historians generally use the traditional name or sometimes the Seven Years' War. Other, less frequently used names for the war include the "Fourth Intercolonial War" and the "Great War for the Empire".
In Europe, the North American theater of the Seven Years' War usually is not given a separate name. The entire international conflict is known as the "Seven Years' War". "Seven Years" refers to events in Europe, from the official declaration of war in 1756 to the signing of the peace treaty in 1763. These dates do not correspond with the fighting on mainland North America, where the fighting between the two colonial powers was largely concluded in six years, from the Battle of Jumonville Glen in 1754 to the capture of Montreal in 1760.
In Canada, both French-speaking and English-speaking Canadians refer to both the European and North American conflicts as the Seven Years' War ("Guerre de Sept Ans"). French Canadians also use the term "War of Conquest" ("Guerre de la Conquête"), since it is the war in which Canada was conquered by the British and became part of the British Empire.
North America in the 1750s.
At this time, North America east of the Mississippi River was largely claimed by either Great Britain or France. Large areas had no settlements by Europeans.
The French population numbered about 75,000 and was heavily concentrated along the St. Lawrence River valley, with some also in Acadia (present-day New Brunswick and parts of Nova Scotia, including Île Royale (present-day Cape Breton Island)). Fewer lived in New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries. French fur traders and trappers traveled throughout the St. Lawrence and Mississippi watersheds, did business with local tribes, and often married Indian women. Traders married daughters of chiefs, creating high-ranking unions.
British settlers outnumbered the French 20 to 1 with a population of about 1.5 million ranged along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south. Many of the older colonies had land claims that extended arbitrarily far to the west, as the extent of the continent was unknown at the time their provincial charters were granted. While their population centers were along the coast, the settlements were growing into the interior. Nova Scotia, which had been captured from France in 1713, still had a significant French-speaking population. Britain also claimed Rupert's Land, where the Hudson's Bay Company traded for furs with local tribes.
In between the French and the British, large areas were dominated by native tribes. To the north, the Mi'kmaq and the Abenaki were engaged in Father Le Loutre's War and still held sway in parts of Nova Scotia, Acadia, and the eastern portions of the province of Canada, as well as much of present-day Maine. The Iroquois Confederation dominated much of present-day Upstate New York and the Ohio Country, although the latter also included Algonquian-speaking populations of Delaware and Shawnee, as well as Iroquoian-speaking Mingo. These tribes were formally under Iroquois rule, and were limited by them in authority to make agreements.
Further south the Southeast interior was dominated by Siouan-speaking Catawba, Muskogee-speaking Creek and Choctaw, and the Iroquoian-speaking Cherokee tribes. When war broke out, the French used their trading connections to recruit fighters from tribes in western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British), including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi. The British were supported in the war by the Iroquois Six Nations, and also by the Cherokee – until differences sparked the Anglo-Cherokee War in 1758. In 1758 the Pennsylvania government successfully negotiated the Treaty of Easton, in which a number of tribes in the Ohio Country promised neutrality in exchange for land concessions and other considerations. Most of the other northern tribes sided with the French, their primary trading partner and supplier of arms. The Creek and Cherokee were subject to diplomatic efforts by both the French and British to gain either their support or neutrality in the conflict. It was not uncommon for small bands to participate on the "other side" of the conflict from formally negotiated agreements, as most tribes were decentralized and bands made their own decisions about warfare.
By this time, in eastern North America Spain claimed only the province of Florida; it controlled Cuba and other territories in the West Indies that became military objectives in the Seven Years' War. Florida's European population was a few hundred, concentrated in St. Augustine and Pensacola.
At the start of the war, no French regular army troops were stationed in North America, and few British troops. New France was defended by about 3,000 troupes de la marine, companies of colonial regulars (some of whom had significant woodland combat experience). The colonial government recruited militia support when needed. Most British colonies mustered local militia companies, generally ill trained and available only for short periods, to deal with native threats, but did not have any standing forces.
Because of its large frontier, Virginia had several companies of British regulars. The colonial governments were used to operating independently of each other and of the government in London, a situation that complicated negotiations with Native tribes. Their territories often encompassed land claimed by multiple colonies. After the war began, the leaders of the British Army establishment tried to impose constraints and demands on the colonial administrations.
Events leading to war.
Céloron's expedition.
In June 1747, concerned about the incursion and expanding influence of British traders such as George Croghan in the Ohio Country, Roland-Michel Barrin de La Galissonière, the Governor-General of New France, ordered Pierre-Joseph Céloron to lead a military expedition through the area. Its objectives were to confirm the original French claim to the territory, determine the level of British influence, and impress the Indians with a French show of force.
Céloron's expedition force consisted of about 200 Troupes de la marine and 30 Indians. The expedition covered about between June and November 1749. It went up the St. Lawrence, continued along the northern shore of Lake Ontario, crossed the portage at Niagara, and followed the southern shore of Lake Erie. At the Chautauqua Portage (near present-day Barcelona, New York), the expedition moved inland to the Allegheny River, which it followed to the site of present-day Pittsburgh. There Céloron buried lead plates engraved with the French claim to the Ohio Country. Whenever he encountered British merchants or fur-traders, Céloron informed them of the French claims on the territory and told them to leave.
When Céloron's expedition arrived at Logstown, the Native Americans in the area informed Céloron that they owned the Ohio Country and that they would trade with the British regardless of the French. Céloron continued south until his expedition reached the confluence of the Ohio and the Miami rivers, which lay just south of the village of Pickawillany, the home of the Miami chief known as "Old Briton". Céloron threatened "Old Briton" with severe consequences if he continued to trade with the British. "Old Briton" ignored the warning. Disappointed, Céloron returned to Montreal in November 1749.
In his extensively detailed report, Céloron wrote, "All I can say is that the Natives of these localities are very badly disposed towards the French, and are entirely devoted to the English. I don't know in what way they could be brought back." Even before his return to Montreal, reports on the situation in the Ohio Country were making their way to London and Paris, each side proposing that action be taken. William Shirley, the expansionist governor of the Province of Massachusetts Bay, was particularly forceful, stating that British colonists would not be safe as long as the French were present. Conflicts between the colonies, accomplished through raiding parties that included Indian allies, had taken place for decades, leading to a brisk trade in European colonial captives from either side.
Negotiations.
In 1749 the British government gave land to the Ohio Company of Virginia for the purpose of developing trade and settlements in the Ohio Country. The grant required that it settle 100 families in the territory, and construct a fort for their protection. But, as the territory was also claimed by Pennsylvania, both colonies began pushing for action to improve their respective claims. In 1750 Christopher Gist, acting on behalf of both Virginia and the company, explored the Ohio territory and opened negotiations with the Indian tribes at Logstown. He completed the 1752 Treaty of Logstown in which the local Indians, through their "Half-King" Tanacharison and an Iroquois representative, agreed to terms that included permission to build a "strong house" at the mouth of the Monongahela River (the site of present-day Pittsburgh, Pennsylvania). By the late 17th century, the Iroquois had pushed many tribes out of the Ohio Valley, and kept it as hunting ground by right of conquest.
The War of the Austrian Succession (whose North American theater is known as King George's War) formally ended in 1748 with the signing of the Treaty of Aix-la-Chapelle. The treaty was primarily focused on resolving issues in Europe. The issues of conflicting territorial claims between British and French colonies in North America were turned over to a commission to resolve, but it reached no decision. Frontiers from between Nova Scotia and Acadia in the north, to the Ohio Country in the south, were claimed by both sides. The disputes also extended into the Atlantic Ocean, where both powers wanted access to the rich fisheries of the Grand Banks off Newfoundland.
Attack on Pickawillany.
On March 17, 1752, the Governor-General of New France, Marquis de la Jonquière, died and was temporarily replaced by Charles le Moyne de Longueuil. His permanent replacement, the Marquis Duquesne, did not arrive in New France until 1752 to take over the post. The continuing British activity in the Ohio territories prompted Longueuil to dispatch another expedition to the area under the command of Charles Michel de Langlade, an officer in the Troupes de la Marine. Langlade was given 300 men, including French-Canadians and warriors of the Ottawa. His objective was to punish the Miami people of Pickawillany for not following Céloron's orders to cease trading with the British. On June 21, the French war party attacked the trading centre at Pickawillany, capturing three traders and killing 14 people of the Miami nation, including Old Briton. He was reportedly ritually cannibalized by some aboriginal members of the expedition.
French fort construction.
In the spring of 1753, Paul Marin de la Malgue was given command of a 2,000-man force of Troupes de la Marine and Indians. His orders were to protect the King's land in the Ohio Valley from the British. Marin followed the route that Céloron had mapped out four years earlier, but where Céloron had limited the record of French claims to the burial of lead plates, Marin constructed and garrisoned forts. He first constructed Fort Presque Isle (near present-day Erie, Pennsylvania) on Lake Erie's south shore. He had a road built to the headwaters of LeBoeuf Creek. Marin constructed a second fort at Fort Le Boeuf (present-day Waterford, Pennsylvania), designed to guard the headwaters of LeBoeuf Creek. As he moved south, he drove off or captured British traders, alarming both the British and the Iroquois. Tanaghrisson, a chief of the Mingo, who were remnants of Iroquois and other tribes who had been driven west by colonial expansion. He intensely disliked the French (whom he accused of killing and eating his father). Traveling to Fort Le Boeuf, he threatened the French with military action, which Marin contemptuously dismissed.
The Iroquois sent runners to the manor of William Johnson in upstate New York. The British Superintendent for Indian Affairs in the New York region and beyond, Johnson was known to the Iroquois as "Warraghiggey", meaning "He who does great things." He spoke their languages and had become a respected honorary member of the Iroquois Confederacy in the area. In 1746, Johnson was made a colonel of the Iroquois. Later he was commissioned as a colonel of the Western New York Militia. They met at Albany, New York with Governor Clinton and officials from some of the other American colonies. Mohawk Chief Hendrick, Speaker of their tribal council, insisted that the British abide by their obligations and block French expansion. When Clinton did not respond to his satisfaction, Chief Hendrick said that the "Covenant Chain", a long-standing friendly relationship between the Iroquois Confederacy and the British Crown, was broken.
Virginia's response.
Governor Robert Dinwiddie of Virginia was an investor in the Ohio Company, which stood to lose money if the French held their claim. To counter the French military presence in Ohio, in October 1753 Dinwiddie ordered the 21-year-old Major George Washington (whose brother was another Ohio Company investor) of the Virginia Regiment to warn the French to leave Virginia territory. Washington left with a small party, picking up along the way Jacob Van Braam as an interpreter; Christopher Gist, a company surveyor working in the area; and a few Mingo led by Tanaghrisson. On December 12, Washington and his men reached Fort Le Boeuf.
Jacques Legardeur de Saint-Pierre, who succeeded Marin as commander of the French forces after the latter died on October 29, invited Washington to dine with him. Over dinner, Washington presented Saint-Pierre with the letter from Dinwiddie demanding an immediate French withdrawal from the Ohio Country. Saint-Pierre said, "As to the Summons you send me to retire, I do not think myself obliged to obey it." He told Washington that France's claim to the region was superior to that of the British, since René-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.
Leaving Fort Le Boeuf early on December 16, Washington and his party arrived in Williamsburg on January 16, 1754. In his report, Washington stated, "The French had swept south", detailing the steps they had taken to fortify the area, and their intention to fortify the confluence of the Allegheny and Monongahela rivers.
Course of the war.
Even before Washington returned, Dinwiddie had sent a company of 40 men under William Trent to that point, where in the early months of 1754 they began construction of a small stockaded fort. Governor Duquesne sent additional French forces under Claude-Pierre Pecaudy de Contrecœur to relieve Saint-Pierre during the same period, and Contrecœur led 500 men south from Fort Venango on April 5, 1754. When these forces arrived at the fort on April 16, Contrecœur generously allowed Trent's small company to withdraw. He purchased their construction tools to continue building what became Fort Duquesne.
After Washington had returned to Williamsburg, Dinwiddie ordered him to lead a larger force to assist Trent in his work. While en route, Washington learned of Trent's retreat. Since Tanaghrisson had promised support to the British, Washington continued toward Fort Duquesne and met with the Mingo leader. Learning of a French scouting party in the area, Washington, with Tanaghrisson and his party, surprised the Canadians on May 28 in what became known as the Battle of Jumonville Glen. They killed many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville, whose head was reportedly split open by Tanaghrisson with a tomahawk. The historian Fred Anderson suggests that Tanaghrisson was acting to gain the support of the British and regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships. One of Tanaghrisson's men told Contrecoeur that Jumonville had been killed by British musket fire.
Historians generally consider the Battle of Jumonville Glen as the opening battle of the French and Indian War in North America, and the start of hostilities in the Ohio valley.
Following the battle, Washington pulled back several miles and established Fort Necessity, which the Canadians attacked under the command of Jummonville's brother at the Battle of Fort Necessity on July 3rd. Washington surrendered; he negotiated a withdrawal under arms. One of Washington's men reported that the Canadian force was accompanied by Shawnee, Delaware, and Mingo native warriors—just those whom Tanaghrisson was seeking to influence.
News of the two battles reached England in August. After several months of negotiations, the government of the Duke of Newcastle decided to send an army expedition the following year to dislodge the French. They chose Major General Edward Braddock to lead the expedition. Word of the British military plans leaked to France well before Braddock's departure for North America. In response, King Louis XV dispatched six regiments to New France under the command of Baron Dieskau in 1755. The British, intending to blockade French ports, sent out their fleet in February 1755, but the French fleet had already sailed. Admiral Edward Hawke detached a fast squadron to North America in an attempt to intercept the French.
In a second British action, Admiral Edward Boscawen fired on the French ship "Alcide" on June 8, 1755, capturing her and two troop ships. The British harassed French shipping throughout 1755, seizing ships and capturing seamen. These actions contributed to the eventual formal declarations of war in spring 1756.
Albany Congress.
An early important political response to the opening of hostilities was the convening of the Albany Congress in June and July, 1754. The goal of the congress was to formalize a unified front in trade and negotiations with various Indians, since allegiance of the various tribes and nations was seen to be pivotal in the success in the war that was unfolding. The plan that the delegates agreed to was never ratified by the colonial legislatures nor approved of by the crown. Nevertheless, the format of the congress and many specifics of the plan became the prototype for confederation during the War of Independence.
British campaigns, 1755.
The British formed an aggressive plan of operations for 1755. General Braddock was to lead the expedition to Fort Duquesne. While the Massachusetts provincial governor William Shirley was given the task of fortifying Fort Oswego and attacking Fort Niagara, Sir William Johnson was to capture Fort St. Frédéric (at present-day Crown Point, New York). Lieutenant Colonel Robert Monckton was to capture Fort Beauséjour to the east, on the frontier between Nova Scotia and Acadia.
Braddock (with George Washington as one of his aides) led about 1,500 army troops and provincial militia on an expedition in June 1755 to take Fort Duquesne. The expedition was a disaster. It was attacked by French and Indian soldiers ambushing them from up in trees and behind logs. Braddock called for a retreat. He was killed. Approximately 1,000 British soldiers were killed or injured. The remaining 500 British troops, led by George Washington, retreated to Virginia. Two future opponents in the American Revolutionary War, Washington and Thomas Gage, played key roles in organizing the retreat.
The French acquired a copy of the British war plans, including the activities of Shirley and Johnson. Shirley's efforts to fortify Oswego were bogged down in logistical difficulties, exacerbated by Shirley's inexperience in managing large expeditions. In conjunction, Shirley was made aware that the French were massing for an attack on Fort Oswego in his absence when he planned to attack Fort Niagara. As a response, Shirley left garrisons at Oswego, Fort Bull, and Fort Williams (the latter two located on the Oneida Carry between the Mohawk River and Wood Creek at present-day Rome, New York). Supplies for use in the projected attack on Niagara were cached at Fort Bull.
Johnson's expedition was better organized than Shirley's, which was noticed by New France's governor, the Marquis de Vaudreuil. He had primarily been concerned about the extended supply line to the forts on the Ohio, and had sent Baron Dieskau to lead the defenses at Frontenac against Shirley's expected attack. When Johnson was seen as the larger threat, Vaudreuil sent Dieskau to Fort St. Frédéric to meet that threat. Dieskau planned to attack the British encampment at Fort Edward at the upper end of navigation on the Hudson River, but Johnson had strongly fortified it, and Dieskau's Indian support was reluctant to attack. The two forces finally met in the bloody Battle of Lake George between Fort Edward and Fort William Henry. The battle ended inconclusively, with both sides withdrawing from the field. Johnson's advance stopped at Fort William Henry, and the French withdrew to Ticonderoga Point, where they began the construction of Fort Carillon (later renamed Fort Ticonderoga after British capture in 1759).
Colonel Monckton, in the sole British success that year, captured Fort Beauséjour in June 1755, cutting the French fortress at Louisbourg off from land-based reinforcements. To cut vital supplies to Louisbourg, Nova Scotia's Governor Charles Lawrence ordered the deportation of the French-speaking Acadian population from the area. Monckton's forces, including companies of Rogers' Rangers, forcibly removed thousands of Acadians, chasing down many who resisted, and sometimes committing atrocities. More than any other factor, the cutting off of supplies to Louisbourg led to its demise. The Acadian resistance, in concert with native allies, including the Mi'kmaq, was sometimes quite stiff, with ongoing frontier raids (against Dartmouth and Lunenburg among others). Other than the campaigns to expel the Acadians (ranging around the Bay of Fundy, on the Petitcodiac and St. John rivers, and Île Saint-Jean), the only clashes of any size were at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757.
French victories, 1756–1757.
Following the death of Braddock, William Shirley assumed command of British forces in North America. At a meeting in Albany in December 1755, he laid out his plans for 1756. In addition to renewing the efforts to capture Niagara, Crown Point and Duquesne, he proposed attacks on Fort Frontenac on the north shore of Lake Ontario and an expedition through the wilderness of the Maine district and down the Chaudière River to attack the city of Quebec. Bogged down by disagreements and disputes with others, including William Johnson and New York's Governor Sir Charles Hardy, Shirley's plan had little support.
Newcastle replaced him in January 1756 with Lord Loudoun, with Major General James Abercrombie as his second in command. Neither of these men had as much campaign experience as the trio of officers France sent to North America. French regular army reinforcements arrived in New France in May 1756, led by Major General Louis-Joseph de Montcalm and seconded by the Chevalier de Lévis and Colonel François-Charles de Bourlamaque, all experienced veterans from the War of the Austrian Succession. During that time in Europe, on May 18, 1756, England formally declared war on France, which expanded the war into Europe, which was later to be known as the Seven Years' War.
Governor Vaudreuil, who harboured ambitions to become the French commander in chief (in addition to his role as governor), acted during the winter of 1756 before those reinforcements arrived. Scouts had reported the weakness of the British supply chain, so he ordered an attack against the forts Shirley had erected at the Oneida Carry. In the March Battle of Fort Bull, French forces destroyed the fort and large quantities of supplies, including 45,000 pounds of gunpowder. They set back any British hopes for campaigns on Lake Ontario, and endangered the Oswego garrison, already short on supplies. French forces in the Ohio valley also continued to intrigue with Indians throughout the area, encouraging them to raid frontier settlements. This led to ongoing alarms along the western frontiers, with streams of refugees returning east to get away from the action.
The new British command was not in place until July. When he arrived in Albany, Abercrombie refused to take any significant actions until Loudoun approved them. Montcalm took bold action against his inertia. Building on Vaudreuil's work harassing the Oswego garrison, Montcalm executed a strategic feint by moving his headquarters to Ticonderoga, as if to presage another attack along Lake George. With Abercrombie pinned down at Albany, Montcalm slipped away and led the successful attack on Oswego in August. In the aftermath, Montcalm and the Indians under his command disagreed about the disposition of prisoners' personal effects. The Europeans did not consider them prizes and prevented the Indians from stripping the prisoners of their valuables, which angered the Indians.
Loudoun, a capable administrator but a cautious field commander, planned one major operation for 1757: an attack on New France's capital, Quebec. Leaving a sizable force at Fort William Henry to distract Montcalm, he began organizing for the expedition to Quebec. He was then ordered by William Pitt, the Secretary of State responsible for the colonies, to attack Louisbourg first. Beset by delays of all kinds, the expedition was finally ready to sail from Halifax, Nova Scotia in early August. In the meantime French ships had escaped the British blockade of the French coast, and a fleet outnumbering the British one awaited Loudoun at Louisbourg. Faced with this strength, Loudoun returned to New York amid news that a massacre had occurred at Fort William Henry.
French irregular forces (Canadian scouts and Indians) harassed Fort William Henry throughout the first half of 1757. In January they ambushed British rangers near Ticonderoga. In February they launched a daring raid against the position across the frozen Lake George, destroying storehouses and buildings outside the main fortification. In early August, Montcalm and 7,000 troops besieged the fort, which capitulated with an agreement to withdraw under parole. When the withdrawal began, some of Montcalm's Indian allies, angered at the lost opportunity for loot, attacked the British column, killing and capturing several hundred men, women, children, and slaves. The aftermath of the siege may have contributed to the transmission of smallpox into remote Indian populations; as some Indians were reported to have traveled from beyond the Mississippi to participate in the campaign and returned afterward having been exposed to European carriers.
British conquest, 1758–1760.
Vaudreuil and Montcalm were minimally resupplied in 1758, as the British blockade of the French coastline limited French shipping. The situation in New France was further exacerbated by a poor harvest in 1757, a difficult winter, and the allegedly corrupt machinations of François Bigot, the intendant of the territory. His schemes to supply the colony inflated prices and were believed by Montcalm to line his pockets and those of his associates. A massive outbreak of smallpox among western tribes led many of them to stay away from trading in 1758. While many parties to the conflict blamed others (the Indians blamed the French for bringing "bad medicine" as well as denying them prizes at Fort William Henry), the disease was probably spread through the crowded conditions at William Henry after the battle. Montcalm focused his meager resources on the defense of the St. Lawrence, with primary defenses at Carillon, Quebec, and Louisbourg, while Vaudreuil argued unsuccessfully for a continuation of the raiding tactics that had worked quite effectively in previous years.
The British failures in North America, combined with other failures in the European theater, led to the fall from power of Newcastle and his principal military advisor, the Duke of Cumberland. Newcastle and Pitt joined in an uneasy coalition in which Pitt dominated the military planning. He embarked on a plan for the 1758 campaign that was largely developed by Loudoun. He had been replaced by Abercrombie as commander in chief after the failures of 1757. Pitt's plan called for three major offensive actions involving large numbers of regular troops, supported by the provincial militias, aimed at capturing the heartlands of New France. Two of the expeditions were successful, with Fort Duquesne and Louisbourg falling to sizable British forces.
1758.
The Forbes Expedition was a British campaign in September–October 1758, with 6,000 troops led by General John Forbes to drive the French out of the contested Ohio Country. After a British advance party on Fort Duquesne was repulsed on September 14, the French withdrew from Fort Duquesne, leaving the British in control of the Ohio River Valley. The great French fortress at Louisbourg in Nova Scotia was captured after a siege.
The third invasion was stopped with the improbable French victory in the Battle of Carillon, in which 3,600 Frenchmen famously and decisively defeated Abercrombie's force of 18,000 regulars, militia and Native American allies outside the fort the French called Carillon and the British called Ticonderoga. Abercrombie saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac, including caches of supplies destined for New France's western forts and furs destined for Europe. Abercrombie was recalled and replaced by Jeffery Amherst, victor at Louisbourg.
In the aftermath of generally poor French results in most theaters of the Seven Years' War in 1758, France's new foreign minister, the duc de Choiseul, decided to focus on an invasion of Britain, to draw British resources away from North America and the European mainland. The invasion failed both militarily and politically, as Pitt again planned significant campaigns against New France, and sent funds to Britain's ally on the mainland, Prussia, and the French Navy failed in the 1759 naval battles at Lagos and Quiberon Bay. In one piece of good fortune, some French supply ships managed to depart France, eluding the British blockade of the French coast.
1759–1760.
British victories continued in all theaters in the Annus Mirabilis of 1759, when they finally captured Ticonderoga, James Wolfe defeated Montcalm at Quebec (in a battle that claimed the lives of both commanders), and victory at Fort Niagara successfully cut off the French frontier forts further to the west and south. The victory was made complete in 1760 when, despite losing outside Quebec City in the Battle of Sainte-Foy, the British were able to prevent the arrival of French relief ships in the naval Battle of the Restigouche while armies marched on Montreal from three sides.
In September 1760, and before any hostilities erupted, Governor Vaudreuil negotiated from Montreal a capitulation with General Amherst. Amherst granted Vaudreuil's request that any French residents who chose to remain in the colony would be given freedom to continue worshiping in their Roman Catholic tradition, continued ownership of their property, and the right to remain undisturbed in their homes. The British provided medical treatment for the sick and wounded French soldiers and French regular troops were returned to France aboard British ships with an agreement that they were not to serve again in the present war.
End of the war.
Most of the fighting between France and Britain in continental North America ended in 1760, while the fighting in Europe continued. The notable exception was the French seizure of St. John's, Newfoundland. When General Amherst heard of this surprise action, he immediately dispatched troops under his nephew William Amherst, who regained control of Newfoundland after the Battle of Signal Hill in September 1762.
Many troops from North America were reassigned to participate in further British actions in the West Indies, including the capture of Spanish Havana when Spain belatedly entered the conflict on the side of France, and a British expedition against French Martinique in 1762, led by (the now) Major General Robert Monckton.
General Amherst also oversaw the transition of French forts in the western lands to British control. The policies he introduced in those lands disturbed large numbers of Indians, and contributed to the outbreak in 1763 of the conflict known as Pontiac's Rebellion. This series of attacks on frontier forts and settlements required the continued deployment of British troops, and was not resolved until 1766.
The war in North America officially ended with the signing of the Treaty of Paris on 10 February 1763, and war in the European theatre of the Seven Years' War was settled by the Treaty of Hubertusburg on 15 February 1763. The British offered France the choice of surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique, which had been occupied by the British. France chose to cede the former, but was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence, along with fishing rights in the area. They viewed the economic value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent. The contemporaneous French philosopher Voltaire referred to Canada disparagingly as nothing more than a few acres of snow. The British, for their part, were happy to take New France, as defence of their North American colonies would no longer be an issue and also because they already had ample places from which to obtain sugar. Spain, which traded Florida to Britain to regain Cuba, also gained Louisiana, including New Orleans, from France in compensation for its losses. Great Britain and Spain also agreed that navigation on the Mississippi River was to be open to vessels of all nations.
Consequences.
The war changed economic, political, governmental and social relations between three European powers (Britain, France, and Spain), their colonies and colonists, and the natives that inhabited the territories they claimed. France and Britain both suffered financially because of the war, with significant long-term consequences.
Britain gained control of French Canada and Acadia, colonies containing approximately 80,000 primarily French-speaking Roman Catholic residents. The deportation of Acadians beginning in 1755 resulted in land made available to migrants from Europe and the colonies further south. The British resettled many Acadians throughout its North American provinces, but many went to France, and some went to New Orleans, which they had expected to remain French. Some were sent to colonize places as diverse as French Guiana and the Falkland Islands; these latter efforts were unsuccessful. Others migrated to places like Saint-Domingue, and fled to New Orleans after the Haitian Revolution. The Louisiana population contributed to the founding of the modern Cajun population. (The French word "Acadien" evolved to "Cadien", then to "Cajun".)
Following the treaty, King George III issued the Royal Proclamation of 1763 on October 7, 1763, which outlined the division and administration of the newly conquered territory, and to some extent continues to govern relations between the government of modern Canada and the First Nations. Included in its provisions was the reservation of lands west of the Appalachian Mountains to its Indian population, a demarcation that was at best a temporary impediment to a rising tide of westward-bound settlers. The proclamation also contained provisions that prevented civic participation by the Roman Catholic Canadians. When accommodations were made in the Quebec Act in 1774 to address this and other issues, religious concerns were raised in the largely Protestant Thirteen Colonies over the advance of "popery"; the Act maintained French Civil law, including the seigneurial system, a medieval code soon to be removed from France within a generation by the French Revolution.
The Seven Years' War nearly doubled Britain's national debt. The Crown, seeking sources of revenue to pay off the debt, attempted to impose new taxes on its colonies. These attempts were met with increasingly stiff resistance, until troops were called in so that representatives of the Crown could safely perform their duties. These acts ultimately led to the start of the American Revolutionary War.
France attached comparatively little value to its North American possessions, apart from the highly profitable sugar-producing Antilles islands, which it managed to retain. Minister Choiseul considered he had made a good deal at the Treaty of Paris, and philosopher Voltaire wrote that Louis XV had lost "a few acres of snow". For France however, the military defeat and the financial burden of the war weakened the monarchy and contributed to the advent of the French Revolution in 1789.
For many native populations, the elimination of French power in North America meant the disappearance of a strong ally and counterweight to British expansion, leading to their ultimate dispossession. The Ohio Country was particularly vulnerable to legal and illegal settlement due to the construction of military roads to the area by Braddock and Forbes. Although the Spanish takeover of the Louisiana territory (which was not completed until 1769) had modest repercussions, the British takeover of Spanish Florida resulted in the westward migration of tribes that did not want to do business with the British, and a rise in tensions between the Choctaw and the Creek, historic enemies whose divisions the British at times exploited. The change of control in Florida also prompted most of its Spanish Catholic population to leave. Most went to Cuba, including the entire governmental records from St. Augustine, although some Christianized Yamasee were resettled to the coast of Mexico.
France returned to North America in 1778 with the establishment of a Franco-American alliance against Great Britain in the American War of Independence. This time France succeeded in prevailing over Great Britain, in what historian Alfred A. Cave describes as "French [...] revenge for Montcalm's death".

</doc>
<doc id="39064" url="https://en.wikipedia.org/wiki?curid=39064" title="Casimir IV Jagiellon">
Casimir IV Jagiellon

Casimir IV KG ( ; ; 30 November 1427 – 7 June 1492) of the Jagiellonian dynasty was Grand Duke of Lithuania from 1440, and King of Poland from 1447, until his death. He was one of the most active Polish rulers, under whom Poland, by defeating the Teutonic Knights in the Thirteen Years' War recovered Pomerania, and the Jagiellonian dynasty became one of the leading royal houses in Europe. He was a strong opponent of aristocracy, and helped to strengthen the importance of Parliament and the Senate.
The great triumph of his reign was the effective and final destruction of the Teutonic Order, which brought Prussia under Polish rule. The long and brilliant rule of Casimir corresponded to the age of “new monarchies” in western Europe. By the 15th century Poland had narrowed the distance separating it from western Europe and become a significant factor in international relations. The demand for raw materials and semi-finished goods stimulated trade, producing a positive balance, and contributed to the growth of crafts and mining in the entire country.
He was a recipient of the English Order of the Garter (KG), the highest order of chivalry and the most prestigious honour in England and of the United Kingdom, awarded at the Sovereign's pleasure as his or her personal gift, on recipients from the United Kingdom and other Commonwealth realms.
Youth.
Casimir Jagiellon was the third and youngest son of King Władysław II Jagiełło and his fourth wife, Sophia of Halshany. His father was already 65 at the time of Casimir’s birth, and his brother Władysław III, three years his senior, was expected to become king before his majority. Strangely, little was done for his education; he was never taught Latin, nor was he trained for the responsibilities of office, despite the fact he was the only brother of the rightful sovereign. He often relied on his instinct and feelings and had little political knowledge, but shared a great interest in the diplomacy and economic affairs of the country. Throughout Casimir's youth, Bishop Zbigniew Oleśnicki was his mentor and tutor, however, the cleric felt a strong reluctance towards him, believing that he would be an unsuccessful monarch following Władysław's death.
Grand Duke of Lithuania.
The sudden death of Sigismund Kęstutaitis left the office of the Grand Duchy of Lithuania empty. The Voivode of Trakai, Jonas Goštautas, and other magnates of Lithuania, supported Casimir as a candidate to the throne. However many Polish noblemen hoped that the thirteen-year-old boy would become a Vice-regent for the Polish King in Lithuania. Casimir was invited by the Lithuanian magnates to Lithuania, and when he arrived in Vilnius in 1440, he was proclaimed as the Grand Duke of Lithuania on 29 June 1440 by the Council of Lords, contrary to the wishes of the Polish noble lords—an act supported and coordinated by Jonas Goštautas. When the news arrived in the Kingdom of Poland concerning the proclamation of Casimir as the Grand Duke of Lithuania, it was met with hostility, even to the point of military threats against Lithuania. Since the young Grand Duke was underage, the supreme control over the Grand Duchy of Lithuania was in the hands of the Council of Lords, presided by Jonas Goštautas. Casimir had been taught Lithuanian language and the customs of Lithuania by appointed court officials.
During Casimir's rule the rights of the Lithuanian nobility—dukes, magnates and boyars (lesser nobles), irrespective of their religion and ethnicity—were put on an equal footing to those of the Polish szlachta. Additionally, Casimir promised to protect the Grand Duchy's borders and not to appoint persons from the Polish Kingdom to the offices of the Grand Duchy. He accepted that decisions on matters concerning the Grand Duchy would not be made without the Council of Lords' consent. He also granted the subject region of Samogitia the right to elect its own elder. Casimir was the first ruler of Lithuania baptised at birth, becoming the first native Roman Catholic Grand Duke.
King of Poland.
Casimir succeeded his brother Władysław III (killed at the Battle of Varna in 1444) as King of Poland after a three-year interregnum on 25 June 1447. In 1454, he married Elisabeth of Austria, daughter of the late King of the Romans Albert II of Habsburg by his late wife Elisabeth of Bohemia. Her distant relative Frederick of Habsburg became Holy Roman Emperor and reigned as Frederick III until after Casimir's own death. The marriage strengthened the ties between the house of Jagiellon and the sovereigns of Hungary-Bohemia and put Casimir at odds with the Holy Roman Emperor through internal Habsburg rivalry.
That same year, Casimir was approached by the Prussian Confederation for aid against the Teutonic Order, which he promised, by making the separatist Prussian regions a protectorate of the Polish Kingdom. However, when the insurgent cities rebelled against the Order, it resisted and the Thirteen Years' War (1454–1466) ensued. Casimir and the Prussian Confederation defeated the Teutonic Order, taking over its capital at Marienburg (Malbork Castle). In the Second Peace of Thorn (1466), the Order recognized Polish sovereignty over the seceded western Prussian regions, Royal Prussia, and the Polish crown's overlordship over the remaining Teutonic Monastic State, transformed in 1525 into a duchy, Ducal Prussia.
Elisabeth's only brother Ladislaus, king of Bohemia and Hungary, died in 1457, and after that Casimir and Elisabeth's dynastic interests were directed also towards her brother's former kingdoms.
King Casimir IV died on 7 June 1492 in the Old Hrodna Castle in the Grand Duchy of Lithuania, which was in a personal union with Poland.
Foreign policies.
The intervention of the Curia, which hitherto had been hostile to Casimir because of his steady and patriotic resistance to papal aggression, was due to the permutations of European politics. The pope was anxious to get rid of the Hussite King of Bohemia, George Podebrad, as the first step towards the formation of a league against the Turk. Casimir was to be a leading factor in this combination, and he took advantage of it to procure the election of his son Vladislaus II as King of Bohemia. But he would not commit himself too far, and his ulterior plans were frustrated by the rivalry of Matthias Corvinus, King of Hungary, who even went so far as to stimulate the Teutonic Order to rise against Casimir. The death of Matthias in 1490 was a great relief to Poland, and Casimir employed the two remaining years of his reign in consolidating his position still further.
Legacy and opinion of reign.
In domestic affairs Casimir was relatively passive but anxious to preserve the prerogatives of the crown, notably his right to nominate bishops. In the question of territories in dispute between his two states (Volhynia and Podolia) he favoured Lithuania. During the war against the Teutonic Order he was forced to grant the Polish nobility substantial concessions by the Privilege (statute) of Nieszawa (November 1454); these, however, became important only after his death, and royal power was not greatly diminished during his lifetime. The feature of Casimir's character which most impressed his contemporaries was his extraordinary simplicity and sobriety. He, one of the greatest monarchs in Europe, habitually wore plain cloth from Kraków, drank nothing but water, and kept the most austere of tables. His one passion was the chase. Yet his liberality to his ministers and servants was proverbial, and his vanquished enemies he always treated with magnificent generosity.
Casimir was neither a splendid ruler nor a good and wise administrator, but a mistrusting, cautious, and sober head of a large family who regarded Lithuania as his personal estate, however his reign was remembered as being both successful and the most peaceful in the history of Poland.
Culture.
During Casimir's rule the cultural progress was striking, with the reconstituted and enlarged University of Kraków playing a major role. Humanist trends found a promoter at Kraków in the Italian scholar Filippo de Buonacorsi, known as Callimachus. From the pen of Jan Długosz came the first major, royal history of Poland.
Curse of the Royal Tomb.
The remains of King Casimir IV and his wife Elisabeth were interred in a tomb situated in the chapel of the Wawel Castle in Kraków, Poland. With the consent of then Cardinal Karol Wojtyła (Archbishop of Kraków, who became Pope John Paul II), a team of scientists was given permission to open the tomb and examine the remains, with restoration as the ultimate objective. Casimir's tomb was opened on Friday, April 13, 1973. Twelve researchers were present. Inside the tomb they found a wooden coffin that was heavily rotted. It contained what was left of the king's decayed corpse.
Within a few days, four of the twelve scientists and researchers had died. Not long after, there were only two survivors: Dr. Bolesław Smyk, a microbiologist, and Dr. Edward Roszycki. Smyk was to suffer problems with his equilibrium for the next five years. In the course of his microbiological examinations, Dr. Smyk found traces of fungi on the royal insignia taken from the tomb. He identified three species - Aspergillus flavus, Penicillim rubrum, and Penicillim rugulosum. These fungi are known to produce aflatoxins that can be deadly when in contact with skin and inhaled into the lungs.

</doc>
<doc id="39066" url="https://en.wikipedia.org/wiki?curid=39066" title="List of religions and spiritual traditions">
List of religions and spiritual traditions

Religion is a collection of cultural systems, beliefs, and world views that establishes symbols that relate humanity to spirituality and, sometimes to moral values. While religion is hard to define, one standard model of religion, used in religious studies courses, was proposed by Clifford Geertz, who simply called it a "cultural system." A critique of Geertz's model by Talal Asad categorized religion as "an anthropological category." Many religions have narratives, symbols, traditions, and sacred histories that are intended to give meaning to life or to explain the origin of life or the universe. They tend to derive morality, ethics, religious laws, or a preferred lifestyle from their ideas about the cosmos and human nature. According to some estimates, there are roughly 4,200 religions in the world.
The word "religion" is sometimes used interchangeably with "faith" or "belief system", but religion differs from private belief in that it has a public aspect. Most religions have organized behaviors, including clerical hierarchies, a definition of what constitutes adherence or membership, congregations of laity, regular meetings or services for the purposes of veneration of a deity or for prayer, holy places (either natural or architectural), and/or religious texts. Certain religions also have a sacred language often used in liturgical services. The practice of a religion may also include sermons, commemoration of the activities of a god or gods, sacrifices, festivals, feasts, trance, initiations, funerals, marriages, meditation, music, art, dance, public service, or other aspects of human culture. Religious beliefs have also been used to explain parapsychological phenomena such as out-of-body experiences, near-death experiences and reincarnation, along with many other paranormal experiences.
Some academics studying the subject have divided religions into three broad categories: world religions, a term which refers to transcultural, international faiths; indigenous religions, which refers to smaller, culture-specific or nation-specific religious groups; and new religious movements, which refers to recently developed faiths. One modern academic theory of religion, social constructionism, says that religion is a modern concept that suggests all spiritual practice and worship follows a model similar to the Abrahamic religions as an orientation system that helps to interpret reality and define human beings, and thus religion, as a concept, has been applied inappropriately to non-Western cultures that are not based upon such systems, or in which these systems are a substantially simpler construct.
Abrahamic religions.
A group of monotheistic traditions sometimes grouped with one another for comparative purposes, because all refer to a patriarch named Abraham.
Christianity.
Other Christian.
Certain Christian groups are difficult to classify as "Eastern" or "Western."
Gnosticism.
Many Gnostic groups were closely related to early Christianity, for example, Valentinism. Irenaeus wrote polemics against them from the standpoint of the then-unified Catholic Church.
The Yazidis are a syncretic Kurdish religion with a Gnostic influence:
None of these religions are still extant.
Judaism and related religions.
Samaritans use a slightly different version of the Pentateuch as their Torah, worshiping at Mount Gerizim instead of Jerusalem, and are possibly the descendants of the lost Northern Kingdom. They are definitely of ancient Israelite origin, but their status as Jews is disputed.
Noahidism is a monotheistic ideology based on the Seven Laws of Noah, and on their traditional interpretations within Rabbinic Judaism. According to Jewish law, non-Jews are not obligated to convert to Judaism, but they are required to observe the Seven Laws of Noah.
Second Temple Judaism
Indian religions.
Indian religions are the religions that originated in the Indian subcontinent; namely Hinduism, Jainism, Buddhism and Sikhism and religions and traditions related to, and descended from, them.
African diasporic religions.
African diasporic religions are a number of related religions that developed in the Americas among African slaves and their descendants in various countries of the Caribbean Islands and Latin America, as well as parts of the southern United States. They derive from African traditional religions, especially of West and Central Africa, showing similarities to the Yoruba religion in particular.
Indigenous traditional religions.
Traditionally, these faiths have all been classified "Pagan", but scholars prefer the terms "indigenous/primal/folk/ethnic religions".

</doc>
<doc id="39068" url="https://en.wikipedia.org/wiki?curid=39068" title="Digital electronics">
Digital electronics

Digital electronics or digital (electronic) circuits are electronics that handle digital signals – discrete bands of analog levels – rather than by continuous ranges (as used in analogue electronics). All levels within a band of values represent the same numeric value. Because of this discretization, relatively small changes to the analog signal levels due to manufacturing tolerance, signal attenuation or parasitic noise do not leave the discrete envelope, and as a result are ignored by signal state sensing circuitry.
In most cases, the number of these states is two, and they are represented by two voltage bands: one near a reference value (typically termed as "ground" or zero volts), and the other a value near the supply voltage. These correspond to the "false" ("0") and "true" ("1") values of the Boolean domain respectively, named after its inventor, George Boole, yielding binary code.
Digital techniques are useful because it is easier to get an electronic device to switch into one of a number of known states than to accurately reproduce a continuous range of values.
Digital electronic circuits are usually made from large assemblies of logic gates, simple electronic representations of Boolean logic functions.
History.
The binary number system was refined by Gottfried Wilhelm Leibniz (published in 1705) and he also established that by using the binary system, the principles of arithmetic and logic could be combined. Digital logic as we know it was the brain-child of George Boole, in the mid 19th century. Boole died young, but his ideas lived on. In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits. Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as an AND logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of "Tractatus Logico-Philosophicus" (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924.
Mechanical analog computers started appearing in the first century and were later used in the medieval era for astronomical calculations. In World War II, mechanical analog computers were used for specialized military applications such as calculating torpedo aiming. During this time the first electronic digital computers were developed. Originally they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs).
The Z3 was an electromechanical computer designed by Konrad Zuse, finished in 1941. It was the world's first working programmable, fully automatic digital computer. Its operation was facilitated by the invention of the vacuum tube in 1904 by John Ambrose Fleming.
Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The bipolar junction transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers.
Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.
At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955.
While working at Texas Instruments, Jack Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. This new technique allowed for quick, low-cost fabrication of complex circuits by having a set of electronic circuits on one small plate ("chip") of semiconductor material, normally silicon.
In the early days of simple integrated circuits, the technology's large scale limited each chip to only a few transistors, and the low degree of integration meant the design process was relatively simple. Manufacturing yields were also quite low by today's standards. As the technology progressed, millions, then billions of transistors could be placed on one chip, and good designs required thorough planning, giving rise to new design methods.
Properties.
An advantage of digital circuits when compared to analog circuits is that signals represented digitally can be transmitted without degradation due to noise. For example, a continuous audio signal transmitted as a sequence of 1s and 0s, can be reconstructed without error, provided the noise picked up in transmission is not enough to prevent identification of the 1s and 0s. An hour of music can be stored on a compact disc using about 6 billion binary digits.
In a digital system, a more precise representation of a signal can be obtained by using more binary digits to represent it. While this requires more digital circuits to process the signals, each digit is handled by the same kind of hardware, resulting in an easily scalable system. In an analog system, additional resolution requires fundamental improvements in the linearity and noise characteristics of each step of the signal chain.
Computer-controlled digital systems can be controlled by software, allowing new functions to be added without changing hardware. Often this can be done outside of the factory by updating the product's software. So, the product's design errors can be corrected after the product is in a customer's hands.
Information storage can be easier in digital systems than in analog ones. The noise-immunity of digital systems permits data to be stored and retrieved without degradation. In an analog system, noise from aging and wear degrade the information stored. In a digital system, as long as the total noise is below a certain level, the information can be recovered perfectly.
Even when more significant noise is present, the use of redundancy permits the recovery of the original data provided too many errors do not occur.
In some cases, digital circuits use more energy than analog circuits to accomplish the same tasks, thus producing more heat which increases the complexity of the circuits such as the inclusion of heat sinks. In portable or battery-powered systems this can limit use of digital systems.
For example, battery-powered cellular telephones often use a low-power analog front-end to amplify and tune in the radio signals from the base station. However, a base station has grid power and can use power-hungry, but very flexible software radios. Such base stations can be easily reprogrammed to process the signals used in new cellular standards.
Digital circuits are sometimes more expensive, especially in small quantities.
Most useful digital systems must translate from continuous analog signals to discrete digital signals. This causes quantization errors. Quantization error can be reduced if the system stores enough digital data to represent the signal to the desired degree of fidelity. The Nyquist-Shannon sampling theorem provides an important guideline as to how much digital data is needed to accurately portray a given analog signal.
In some systems, if a single piece of digital data is lost or misinterpreted, the meaning of large blocks of related data can completely change. Because of the cliff effect, it can be difficult for users to tell if a particular system is right on the edge of failure, or if it can tolerate much more noise before failing.
Digital fragility can be reduced by designing a digital system for robustness. For example, a parity bit or other error management method can be inserted into the signal path. These schemes help the system detect errors, and then either correct the errors, or at least ask for a new copy of the data. In a state-machine, the state transition logic can be designed to catch unused states and trigger a reset sequence or other error recovery routine.
Digital memory and transmission systems can use techniques such as error detection and correction to use additional data to correct any errors in transmission and storage.
On the other hand, some techniques used in digital systems make those systems more vulnerable to single-bit errors. These techniques are acceptable when the underlying bits are reliable enough that such errors are highly unlikely.
A single-bit error in audio data stored directly as linear pulse code modulation (such as on a CD-ROM) causes, at worst, a single click. Instead, many people use audio compression to save storage space and download time, even though a single-bit error may corrupt the entire song.
Construction.
A digital circuit is typically constructed from small electronic circuits called logic gates that can be used to create combinational logic. Each logic gate is designed to perform a function of boolean logic when acting on logic signals. A logic gate is generally created from one or more electrically controlled switches, usually transistors but thermionic valves have seen historic use. The output of a logic gate can, in turn, control or feed into more logic gates.
Integrated circuits consist of multiple transistors on one silicon chip, and are the least expensive way to make large number of interconnected logic gates. Integrated circuits are usually designed by engineers using electronic design automation software (see below for more information) to perform some type of function.
Integrated circuits are usually interconnected on a printed circuit board which is a board which holds electrical components, and connects them together with copper traces.
Design.
Each logic symbol is represented by a different shape. The actual set of shapes was introduced in 1984 under IEEE/ANSI standard 91-1984. "The logic symbol given under this standard are being increasingly used now and have even started appearing in the literature published by manufacturers of digital integrated circuits."
Another form of digital circuit is constructed from lookup tables, (many sold as "programmable logic devices", though other kinds of PLDs exist). Lookup tables can perform the same functions as machines based on logic gates, but can be easily reprogrammed without changing the wiring. This means that a designer can often repair design errors without changing the arrangement of wires. Therefore, in small volume products, programmable logic devices are often the preferred solution. They are usually designed by engineers using electronic design automation software.
When the volumes are medium to large, and the logic can be slow, or involves complex algorithms or sequences, often a small microcontroller is programmed to make an embedded system. These are usually programmed by software engineers.
When only one digital circuit is needed, and its design is totally customized, as for a factory production line controller, the conventional solution is a programmable logic controller, or PLC. These are usually programmed by electricians, using ladder logic.
Structure of digital systems.
Engineers use many methods to minimize logic functions, in order to reduce the circuit's complexity. When the complexity is less, the circuit also has fewer errors and less electronics, and is therefore less expensive.
The most widely used simplification is a minimization algorithm like the Espresso heuristic logic minimizer within a CAD system, although historically, binary decision diagrams, an automated Quine–McCluskey algorithm, truth tables, Karnaugh maps, and Boolean algebra have been used.
Representation.
Representations are crucial to an engineer's design of digital circuits. Some analysis methods only work with particular representations.
The classical way to represent a digital circuit is with an equivalent set of logic gates. Another way, often with the least electronics, is to construct an equivalent system of electronic switches (usually transistors). One of the easiest ways is to simply have a memory containing a truth table. The inputs are fed into the address of the memory, and the data outputs of the memory become the outputs.
For automated analysis, these representations have digital file formats that can be processed by computer programs. Most digital engineers are very careful to select computer programs ("tools") with compatible file formats.
Combinational vs. Sequential.
To choose representations, engineers consider types of digital systems. Most digital systems divide into "combinational systems" and "sequential systems." A combinational system always presents the same output when given the same inputs. It is basically a representation of a set of logic functions, as already discussed.
A sequential system is a combinational system with some of the outputs fed back as inputs. This makes the digital machine perform a "sequence" of operations. The simplest sequential system is probably a flip flop, a mechanism that represents a binary digit or "bit".
Sequential systems are often designed as state machines. In this way, engineers can design a system's gross behavior, and even test it in a simulation, without considering all the details of the logic functions.
Sequential systems divide into two further subcategories. "Synchronous" sequential systems change state all at once, when a "clock" signal changes state. "Asynchronous" sequential systems propagate changes whenever inputs change. Synchronous sequential systems are made of well-characterized asynchronous circuits such as flip-flops, that change only when the clock changes, and which have carefully designed timing margins.
Synchronous systems.
The usual way to implement a synchronous sequential state machine is to divide it into a piece of combinational logic and a set of flip flops called a "state register." Each time a clock signal ticks, the state register captures the feedback generated from the previous state of the combinational logic, and feeds it back as an unchanging input to the combinational part of the state machine. The fastest rate of the clock is set by the most time-consuming logic calculation in the combinational logic.
The state register is just a representation of a binary number. If the states in the state machine are numbered (easy to arrange), the logic function is some combinational logic that produces the number of the next state.
Asynchronous systems.
As of 2014, almost all digital machines are synchronous designs because it is easier to create and verify a synchronous design. However, asynchronous logic is thought can be superior because its speed is not constrained by an arbitrary clock; instead, it runs at the maximum speed of its logic gates. Building an asynchronous system using faster parts makes the circuit faster.
Many systems need circuits that allow external unsynchronized signals to enter synchronous logic circuits. These are inherently asynchronous in their design and must be analyzed as such. Examples of widely used asynchronous circuits include synchronizer flip-flops, switch debouncers and arbiters.
Asynchronous logic components can be hard to design because all possible states, in all possible timings must be considered. The usual method is to construct a table of the minimum and maximum time that each such state can exist, and then adjust the circuit to minimize the number of such states. Then the designer must force the circuit to periodically wait for all of its parts to enter a compatible state (this is called "self-resynchronization"). Without such careful design, it is easy to accidentally produce asynchronous logic that is "unstable," that is, real electronics will have unpredictable results because of the cumulative delays caused by small variations in the values of the electronic components.
Register transfer systems.
Many digital systems are data flow machines. These are usually designed using synchronous register transfer logic, using hardware description languages such as VHDL or Verilog.
In register transfer logic, binary numbers are stored in groups of flip flops called registers. The outputs of each register are a bundle of wires called a "bus" that carries that number to other calculations. A calculation is simply a piece of combinational logic. Each calculation also has an output bus, and these may be connected to the inputs of several registers. Sometimes a register will have a multiplexer on its input, so that it can store a number from any one of several buses. Alternatively, the outputs of several items may be connected to a bus through buffers that can turn off the output of all of the devices except one. A sequential state machine controls when each register accepts new data from its input.
Asynchronous register-transfer systems (such as computers) have a general solution. In the 1980s, some researchers discovered that almost all synchronous register-transfer machines could be converted to asynchronous designs by using first-in-first-out synchronization logic. In this scheme, the digital machine is characterized as a set of data flows. In each step of the flow, an asynchronous "synchronization circuit" determines when the outputs of that step are valid, and presents a signal that says, "grab the data" to the stages that use that stage's inputs. It turns out that just a few relatively simple synchronization circuits are needed.
Computer design.
The most general-purpose register-transfer logic machine is a computer. This is basically an automatic binary abacus. The control unit of a computer is usually designed as a microprogram run by a microsequencer. A microprogram is much like a player-piano roll. Each table entry or "word" of the microprogram commands the state of every bit that controls the computer. The sequencer then counts, and the count addresses the memory or combinational logic machine that contains the microprogram. The bits from the microprogram control the arithmetic logic unit, memory and other parts of the computer, including the microsequencer itself.A "specialized computer" is usually a conventional computer with special-purpose control logic or microprogram.
In this way, the complex task of designing the controls of a computer is reduced to a simpler task of programming a collection of much simpler logic machines.
Almost all computers are synchronous. However, true asynchronous computers have also been designed. One example is the Aspida DLX core. Another was offered by ARM Holdings. Speed advantages have not materialized, because modern computer designs already run at the speed of their slowest componment, usually memory. These do use somewhat less power because a clock distribution network is not needed. An unexpected advantage is that asynchronous computers do not produce spectrally-pure radio noise, so they are used in some mobile-phone base-station controllers. They may be more secure in cryptographic applications because their electrical and radio emissions can be more difficult to decode.
Computer architecture.
Computer architecture is a specialized engineering activity that tries to arrange the registers, calculation logic, buses and other parts of the computer in the best way for some purpose. Computer architects have applied large amounts of ingenuity to computer design to reduce the cost and increase the speed and immunity to programming errors of computers. An increasingly common goal is to reduce the power used in a battery-powered computer system, such as a cell-phone. Many computer architects serve an extended apprenticeship as microprogrammers.
Design issues in digital circuits.
Digital circuits are made from analog components. The design must assure that the analog nature of the components doesn't dominate the desired digital behavior. Digital systems must manage noise and timing margins, parasitic inductances and capacitances, and filter power connections.
Bad designs have intermittent problems such as "glitches", vanishingly fast pulses that may trigger some logic but not others, "runt pulses" that do not reach valid "threshold" voltages, or unexpected ("undecoded") combinations of logic states.
Additionally, where clocked digital systems interface to analog systems or systems that are driven from a different clock, the digital system can be subject to metastability where a change to the input violates the set-up time for a digital input latch. This situation will self-resolve, but will take a random time, and while it persists can result in invalid signals being propagated within the digital system for a short time.
Since digital circuits are made from analog components, digital circuits calculate more slowly than low-precision analog circuits that use a similar amount of space and power. However, the digital circuit will calculate more repeatably, because of its high noise immunity. On the other hand, in the high-precision domain (for example, where 14 or more bits of precision are needed), analog circuits require much more power and area than digital equivalents.
Automated design tools.
To save costly engineering effort, much of the effort of designing large logic machines has been automated. The computer programs are called "electronic design automation tools" or just "EDA."
Simple truth table-style descriptions of logic are often optimized with EDA that automatically produces reduced systems of logic gates or smaller lookup tables that still produce the desired outputs. The most common example of this kind of software is the Espresso heuristic logic minimizer.
Most practical algorithms for optimizing large logic systems use algebraic manipulations or binary decision diagrams, and there are promising experiments with genetic algorithms and annealing optimizations.
To automate costly engineering processes, some EDA can take state tables that describe state machines and automatically produce a truth table or a function table for the combinational logic of a state machine. The state table is a piece of text that lists each state, together with the conditions controlling the transitions between them and the belonging output signals.
It is common for the function tables of such computer-generated state-machines to be optimized with logic-minimization software such as Minilog.
Often, real logic systems are designed as a series of sub-projects, which are combined using a "tool flow." The tool flow is usually a "script," a simplified computer language that can invoke the software design tools in the right order.
Tool flows for large logic systems such as microprocessors can be thousands of commands long, and combine the work of hundreds of engineers.
Writing and debugging tool flows is an established engineering specialty in companies that produce digital designs. The tool flow usually terminates in a detailed computer file or set of files that describe how to physically construct the logic. Often it consists of instructions to draw the transistors and wires on an integrated circuit or a printed circuit board.
Parts of tool flows are "debugged" by verifying the outputs of simulated logic against expected inputs. The test tools take computer files with sets of inputs and outputs, and highlight discrepancies between the simulated behavior and the expected behavior.
Once the input data is believed correct, the design itself must still be verified for correctness. Some tool flows verify designs by first producing a design, and then scanning the design to produce compatible input data for the tool flow. If the scanned data matches the input data, then the tool flow has probably not introduced errors.
The functional verification data are usually called "test vectors." The functional test vectors may be preserved and used in the factory to test that newly constructed logic works correctly. However, functional test patterns don't discover common fabrication faults. Production tests are often designed by software tools called "test pattern generators". These generate test vectors by examining the structure of the logic and systematically generating tests for particular faults. This way the fault coverage can closely approach 100%, provided the design is properly made testable (see next section).
Once a design exists, and is verified and testable, it often needs to be processed to be manufacturable as well. Modern integrated circuits have features smaller than the wavelength of the light used to expose the photoresist. Manufacturability software adds interference patterns to the exposure masks to eliminate open-circuits, and enhance the masks' contrast.
Design for testability.
There are several reasons for testing a logic circuit. When the circuit is first developed, it is necessary to verify that the design circuit meets the required functional and timing specifications. When multiple copies of a correctly designed circuit are being manufactured, it is essential to test each copy to ensure that the manufacturing process has not introduced any flaws.
A large logic machine (say, with more than a hundred logical variables) can have an astronomical number of possible states. Obviously, in the factory, testing every state is impractical if testing each state takes a microsecond, and there are more states than the number of microseconds since the universe began. Unfortunately, this ridiculous-sounding case is typical.
Fortunately, large logic machines are almost always designed as assemblies of smaller logic machines. To save time, the smaller sub-machines are isolated by permanently installed "design for test" circuitry, and are tested independently.
One common test scheme known as "scan design" moves test bits serially (one after another) from external test equipment through one or more serial shift registers known as "scan chains". Serial scans have only one or two wires to carry the data, and minimize the physical size and expense of the infrequently used test logic.
After all the test data bits are in place, the design is reconfigured to be in "normal mode" and one or more clock pulses are applied, to test for faults (e.g. stuck-at low or stuck-at high) and capture the test result into flip-flops and/or latches in the scan shift register(s). Finally, the result of the test is shifted out to the block boundary and compared against the predicted "good machine" result.
In a board-test environment, serial to parallel testing has been formalized with a standard called "JTAG" (named after the "Joint Test Action Group" that proposed it).
Another common testing scheme provides a test mode that forces some part of the logic machine to enter a "test cycle." The test cycle usually exercises large independent parts of the machine.
Trade-offs.
Several numbers determine the practicality of a system of digital logic: cost, reliability, fanout and speed. Engineers explored numerous electronic devices to get an ideal combination of these traits.
Cost.
The cost of a logic gate is crucial, primarily because very many gates are needed to build a computer or other advanced digital system and because the more gates can be used, the more capable and/or fast the machine can be. Since the majority of a digital computer is simply an interconnected network of logic gates, the overall cost of building a computer correlates strongly with the price per logic gate. In the 1930s, the earliest digital logic systems were constructed from telephone relays because these were inexpensive and relatively reliable. After that, engineers always used the cheapest available electronic switches that could still fulfill the requirements.
The earliest integrated circuits were a happy accident. They were constructed not to save money, but to save weight, and permit the Apollo Guidance Computer to control an inertial guidance system for a spacecraft. The first integrated circuit logic gates cost nearly $50 (in 1960 dollars, when an engineer earned $10,000/year). To everyone's surprise, by the time the circuits were mass-produced, they had become the least-expensive method of constructing digital logic. Improvements in this technology have driven all subsequent improvements in cost.
With the rise of integrated circuits, reducing the absolute number of chips used represented another way to save costs. The goal of a designer is not just to make the simplest circuit, but to keep the component count down. Sometimes this results in more complicated designs with respect to the underlying digital logic but nevertheless reduces the number of components, board size, and even power consumption. A major motive for reducing component count on printed circuit boards is to reduce the manufacturing defect rate and increase reliability, as every soldered connection is a potentially bad one, so the defect and failure rates tend to increase along with the total number of component pins.
For example, in some logic families, NAND gates are the simplest digital gate to build. All other logical operations can be implemented by NAND gates. If a circuit already required a single NAND gate, and a single chip normally carried four NAND gates, then the remaining gates could be used to implement other logical operations like logical and. This could eliminate the need for a separate chip containing those different types of gates.
Reliability.
The "reliability" of a logic gate describes its mean time between failure (MTBF). Digital machines often have millions of logic gates. Also, most digital machines are "optimized" to reduce their cost. The result is that often, the failure of a single logic gate will cause a digital machine to stop working. It is possible to design machines to be more reliable by using redundant logic which will not malfunction as a result of the failure of any single gate (or even any two, three, or four gates), but this necessarily entails using more components, which raises the financial cost and also usually increases the weight of the machine and may increase the power it consumes.
Digital machines first became useful when the MTBF for a switch got above a few hundred hours. Even so, many of these machines had complex, well-rehearsed repair procedures, and would be nonfunctional for hours because a tube burned-out, or a moth got stuck in a relay. Modern transistorized integrated circuit logic gates have MTBFs greater than 82 billion hours (8.2×1010) hours, and need them because they have so many logic gates.
Fanout.
Fanout describes how many logic inputs can be controlled by a single logic output without exceeding the electrical current ratings of the gate outputs. The minimum practical fanout is about five. Modern electronic logic gates using CMOS transistors for switches have fanouts near fifty, and can sometimes go much higher.
Speed.
The "switching speed" describes how many times per second an inverter (an electronic representation of a "logical not" function) can change from true to false and back. Faster logic can accomplish more operations in less time. Digital logic first became useful when switching speeds got above fifty hertz, because that was faster than a team of humans operating mechanical calculators. Modern electronic digital logic routinely switches at five gigahertz (5×109 hertz), and some laboratory systems switch at more than a terahertz (1×1012 hertz).
Logic families.
Design started with relays. Relay logic was relatively inexpensive and reliable, but slow. Occasionally a mechanical failure would occur. Fanouts were typically about ten, limited by the resistance of the coils and arcing on the contacts from high voltages.
Later, vacuum tubes were used. These were very fast, but generated heat, and were unreliable because the filaments would burn out. Fanouts were typically five to seven, limited by the heating from the tubes' current. In the 1950s, special "computer tubes" were developed with filaments that omitted volatile elements like silicon. These ran for hundreds of thousands of hours.
The first semiconductor logic family was resistor–transistor logic. This was a thousand times more reliable than tubes, ran cooler, and used less power, but had a very low fan-in of three. Diode–transistor logic improved the fanout up to about seven, and reduced the power. Some DTL designs used two power-supplies with alternating layers of NPN and PNP transistors to increase the fanout.
Transistor–transistor logic (TTL) was a great improvement over these. In early devices, fanout improved to ten, and later variations reliably achieved twenty. TTL was also fast, with some variations achieving switching times as low as twenty nanoseconds. TTL is still used in some designs.
Emitter coupled logic is very fast but uses a lot of power. It was extensively used for high-performance computers made up of many medium-scale components (such as the Illiac IV).
By far, the most common digital integrated circuits built today use CMOS logic, which is fast, offers high circuit density and low-power per gate. This is used even in large, fast computers, such as the IBM System z.
Recent developments.
In 2009, researchers discovered that memristors can implement a boolean state storage (similar to a flip flop, implication and logical inversion), providing a complete logic family with very small amounts of space and power, using familiar CMOS semiconductor processes.
The discovery of superconductivity has enabled the development of rapid single flux quantum (RSFQ) circuit technology, which uses Josephson junctions instead of transistors. Most recently, attempts are being made to construct purely optical computing systems capable of processing digital information using nonlinear optical elements.

</doc>
<doc id="39070" url="https://en.wikipedia.org/wiki?curid=39070" title="Maximilian I, Holy Roman Emperor">
Maximilian I, Holy Roman Emperor

Maximilian I (22 March 1459 – 12 January 1519), the son of Frederick III, Holy Roman Emperor, and Eleanor of Portugal, was King of the Romans (also known as King of the Germans) from 1486 and Holy Roman Emperor from 1508 until his death, though he was never in fact crowned by the Pope, the journey to Rome always being too risky. He had ruled jointly with his father for the last ten years of his father's reign, from c. 1483. He expanded the influence of the House of Habsburg through war and his marriage in 1477 to Mary of Burgundy, the heiress to the Duchy of Burgundy, but he also lost the Austrian territories in today's Switzerland to the Swiss Confederacy.
Through marriage of his son Philip the Handsome to eventual queen Joanna of Castile in 1498, Maximilian helped to establish the Habsburg dynasty in Spain which allowed his grandson Charles to hold the thrones of both Castile and Aragon. Since his father Philip died in 1506, Charles succeeded Maximilian as Holy Roman Emperor in 1519, and thus ruled both the Holy Roman Empire and the Spanish Empire simultaneously.
Background and childhood.
Maximilian was born at Wiener Neustadt on 22 March 1459. His father, Frederick III, named him for an obscure saint whom Frederick believed had once warned him of imminent peril in a dream. In his infancy, he and his parents were besieged in Vienna by Albert of Austria. One source relates that, during the siege's bleakest days, the young prince would wander about the castle garrison, begging the servants and men-at-arms for bits of bread .
At the time, the Dukes of Burgundy, a cadet branch of the French royal family, with their sophisticated nobility and court culture, were the rulers of substantial territories on the eastern and northern boundaries of modern-day France. The reigning duke of Burgundy, Charles the Bold, was the chief political opponent of Maximilian's father Frederick III. Frederick was concerned about Burgundy's expansive tendencies on the western border of his Holy Roman Empire and, to forestall military conflict, he attempted to secure the marriage of Charles's only daughter, Mary of Burgundy, to his son Maximilian. After the Siege of Neuss (1474–75), he was successful. The wedding between Maximilian and Mary took place on the evening of 16 August 1477.
Reign in Burgundy and The Netherlands.
Maximilian's wife had inherited the large Burgundian domains in France and the Low Countries upon her father's death in the Battle of Nancy on 5 January 1477. Already before his coronation as the King of the Romans in 1486, Maximilian decided to secure this distant and extensive Burgundian inheritance to his family, the House of Habsburg, at all costs.
The Duchy of Burgundy was also claimed by the French crown under Salic Law, with Louis XI, King of France vigorously contesting the Habsburg claim to the Burgundian inheritance by means of military force. Maximilian undertook the defence of his wife's dominions from an attack by Louis XI and defeated the French forces at Guinegate, the modern Enguinegatte, on 7 August 1479.
The wedding contract between Maximilian and Mary stipulated that only the children of bride and groom had a right to inherit from each, not the surviving parent. Mary tried to bypass this rule with a promise to transfer territories as a gift in case of her death, but her plans were confounded. After Mary's death in a riding accident on 27 March 1482 near the Wijnendale Castle, Maximilian's aim was now to secure the inheritance to one of his and Mary's children, Philip the Handsome.
Some of the Netherlander provinces were hostile to Maximilian, and they signed a treaty with Louis XI in 1482 that forced Maximilian to give up Franche-Comté and Artois to the French crown and openly rebelled twice in the period 1482–1492, in an attempt to regain the autonomy they had enjoined under Mary. Flemish rebels managed to capture Philip and even Maximilian himself, but were defeated when Frederick III intervened. Maximilian continued to govern Mary's remaining inheritance in the name of Philip the Handsome. After the regency ended, Maximilian and Charles VIII of France exchanged these two territories for Burgundy and Picardy in the Treaty of Senlis (1493). Thus a large part of the Netherlands (known as the Seventeen Provinces) stayed in the Habsburg patrimony.
Reign in the Holy Roman Empire.
Elected King of the Romans 16 February 1486 in Frankfurt-am-Main at his father's initiative and crowned on 9 April 1486 in Aachen, Maximilian also stood at the head of the Holy Roman Empire upon his father's death in 1493. During his first year as an Emperor, much of Austria was under Hungarian rule as they had occupied the territory under the reign of Frederick. In 1490, Maximilian finally reconquered it and entered Vienna.
Italian and Swiss wars.
As the Treaty of Senlis had resolved French differences with the Holy Roman Empire, King Louis XII of France had his borders secured in the north and turned his attention to Italy, where he made claims for the Duchy of Milan. In 1499/1500 he conquered it and drove the Sforza regent Lodovico il Moro into exile. This brought him into a potential conflict with Maximilian, who on 16 March 1494 had married Bianca Maria Sforza, a daughter of Galeazzo Maria Sforza, duke of Milan. However, Maximilian was unable to hinder the French from taking over Milan. The prolonged Italian Wars resulted in Maximilian joining the Holy League to counter the French. In 1513, with Henry VIII of England, Maximilian won an important victory at the battle of the Spurs against the French, stopping their advance in northern France. His campaigns in Italy were not as successful and there, his progress was quickly checked.
In the late 15th century two states, Tyrol and Bavaria, went to war. Bavaria demanded money back from Tyrol that had been loaned on the collateral of Tyrolean lands. In 1490, the two nations demanded that Maximilian I step in to mediate the dispute. In response, he assumed the control of Tyrol and its debt. Because Tyrol had no law code at this time, the nobility freely expropriated money from the populace, which caused the royal palace in Innsbruck to fester with corruption. After taking control, Maximilian instituted immediate financial reform. In order to symbolize his new wealth and power, he built the Golden Roof, a canopy overlooking the town center of Innsbruck, from which to watch the festivities celebrating his assumption of rule over Tyrol. It is made entirely from golden shingles. Gaining theoretical control of Tyrol for the Habsburgs was of strategic importance because it linked the Swiss Confederacy to the Habsburg-controlled Austrian lands, which facilitated some imperial geographic continuity.
The situation in Italy was not the only problem Maximilian had at the time. The Swiss won a decisive victory against the Empire in the Battle of Dornach on 22 July 1499. Maximilian had no choice but to agree to a peace treaty signed on 22 September 1499 in Basel that granted the Swiss Confederacy independence from the Holy Roman Empire.
Reforms.
Within the Holy Roman Empire, Maximilian faced pressure from local rulers who believed that the King's continued wars with the French to increase the power of his own house were not in their best interests. There was also a consensus that in order to preserve the unity of the Empire, deep reforms were needed. The reforms, which had been delayed for a long time, were launched in the 1495 Reichstag at Worms. A new organ, the "Reichskammergericht" was introduced, and it was to be largely independent from the Emperor. To finance it, a new tax, the "Gemeine Pfennig" was launched. However, its collection was never fully successful. The local rulers wanted more independence from the Emperor and a strengthening of their own territorial rule. This led to Maximilian agreeing to establish an organ called the "Reichsregiment", which would meet in Nuremberg and consist of the deputies of the Emperor, local rulers, commoners, and the prince-electors of the Holy Roman Empire. The new organ proved itself politically weak and its power returned to Maximilian in 1502.
Due to the difficult external and internal situation he faced, Maximilian also felt it necessary to introduce reforms in the historic territories of the House of Habsburg in order to finance his army. Using Burgundian institutions as a model, he attempted to create a unified state. This was not very successful, but one of the lasting results was the creation of three different subdivisions of the Austrian lands: Lower Austria, Upper Austria, and Vorderösterreich.
Maximilian was always troubled by financial shortcomings; his income never seemed to be enough to sustain his large-scale goals and policies. For this reason he was forced to take substantial credits from Upper German banker families, especially from the families of Baumgarten, Fugger and Welser. Jörg Baumgarten even served as Maximilian's financial advisor. The Fuggers, who dominated the copper and silver mining business in Tyrol, provided a credit of almost 1 million gulden for the purpose of bribing the prince-electors to choose Maximilian's grandson Charles V as the new Emperor. At the end of Maximilian's rule, the Habsburgs' mountain of debt totalled 6 million gulden; this corresponded to a decade's worth of tax revenues from their inherited lands. It took until the end of the 16th century for this debt to be repaid.
In 1508, Maximilian, with the assent of Pope Julius II, took the title "Erwählter Römischer Kaiser" ("Elected Roman Emperor"), thus ending the centuries-old custom that the Holy Roman Emperor had to be crowned by the pope.
"Tu felix Austria nube".
As part of the Treaty of Arras, Maximilian betrothed his three-year-old daughter Margaret to the Dauphin of France (later Charles VIII), son of his adversary Louis XI. Under the terms of Margaret's betrothal, she was sent to Louis to be brought up under his guardianship. Despite Louis's death in 1483, shortly after Margaret arrived in France, she remained at the French court. The Dauphin, now Charles VIII, was still a minor, and his regent until 1491 was his sister Anne.
Dying shortly after signing the Treaty of Le Verger, Francis II, Duke of Brittany, left his realm to his daughter Anne. In her search of alliances to protect her domain from neighboring interests, she betrothed Maximilian I in 1490. About a year later, they married by proxy.
However, Charles and his sister wanted her inheritance for France. So, when the former came of age in 1491, and taking advantage of Maximilian and his father's interest in the succession of their adversary Mathias Corvinus, King of Hungary, Charles repudiated his betrothal to Margaret, invaded Brittany, forced Anne of Brittany to repudiate her unconsummated marriage to Maximilian, and married Anne of Brittany himself.
Margaret then remained in France as a hostage of sorts until 1493, when she was finally returned to her father with the signing of the Treaty of Senlis.
In the same year, as the hostilities of the lengthy Italian Wars with France were in preparation, Maximilian contracted another marriage for himself, this time to Bianca Maria Sforza, daughter of Galeazzo Maria Sforza, Duke of Milan, with the intercession of his brother, Ludovico Sforza, then regent of the duchy after the former's death.
Years later, in order to reduce the growing pressures on the Empire brought about by treaties between the rulers of France, Poland, Hungary, Bohemia, and Russia, as well as to secure Bohemia and Hungary for the Habsburgs, Maximilian met with the Jagiellonian kings Ladislaus II of Hungary and Bohemia and Sigismund I of Poland at the First Congress of Vienna in 1515. There they arranged for Maximilian's granddaughter Mary to marry Louis, the son of Ladislaus, and for Anne (the sister of Louis) to marry Maximilian's grandson Ferdinand (both grandchildren being the children of Philip the Handsome, Maximilian's son, and Joanna of Castile). The marriages arranged there brought Habsburg kingship over Hungary and Bohemia in 1526. Both Anne and Louis were adopted by Maximilian following the death of Ladislaus.
Thus Maximilian through his own marriages and those of his descendants (attempted unsuccessfully and successfully alike) sought, as was current practice for dynastic states at the time, to extend his sphere of influence. The marriages he arranged for both of his children more successfully fulfilled the specific goal of thwarting French interests, and after the turn of the sixteenth century, his matchmaking focused on his grandchildren, for whom he looked away from France towards the east.
These political marriages were summed up in the following Latin elegiac couplet: "Bella gerant aliī, tū fēlix Austria nūbe/ Nam quae Mars aliīs, dat tibi regna Venus", "Let others wage war, but thou, O happy Austria, marry; for those kingdoms which Mars gives to others, Venus gives to thee."
Succession.
After it became clear that Maximilian's policies in Italy had been unsuccessful, and after 1517 Venice reconquered the last pieces of their territory from Maximilian, the emperor now started to focus entirely on the question of his succession. His goal was to secure the throne for a member of his house and prevent Francis I of France from gaining the throne; the resulting "election campaign" was unprecedented due to the massive use of bribery. The Fugger family provided Maximilian a credit of 1 million gulden, which was used to bribe the prince-electors. At first, this policy seemed successful, and Maximilian managed to secure the votes from Mainz, Cologne, Brandenburg and Bohemia for his grandson Charles V. The death of Maximilian in 1519 seemed to put the succession at risk, but in a few months the election of Charles V was secured.
Death and legacy.
In 1501, Maximilian fell from his horse, an accident that badly injured his leg and caused him pain for the rest of his life. Some historians have suggested that Maximilian was "morbidly" depressed: From 1514, he travelled everywhere with his coffin.
Maximilian died in Wels, Upper Austria, and was succeeded as Emperor by his grandson Charles V, his son Philip the Handsome having died in 1506. For penitential reasons, he gave very specific instructions for the treatment of his body after death. After death he wanted his hair to be cut off and his teeth knocked out. The body should be whipped and covered with lime and ash, wrapped in linen and "publicly displayed to show the perishableness of all earthly glory". Although he is buried in the Castle Chapel at Wiener Neustadt, an extremely elaborate cenotaph tomb for Maximilian is in the Hofkirche, Innsbruck, where the tomb is surrounded by statues of heroes from the past. Much of the work was done in his lifetime, but it was not finally completed until decades later.
Maximilian was a keen supporter of the arts and sciences, and he surrounded himself with scholars such as Joachim Vadian and Andreas Stoberl (Stiborius), promoting them to important court posts. Many of them were commissioned to assist him complete a series of projects, in different art forms, intended to glorify for posterity his life and deeds and those of his Habsburg ancestors. He referred to these projects as "Gedechtnus" ("memorial"), and included a series of stylised autobiographical works: the epic poems "Theuerdank" and "Freydal", and the chivalric novel "Weisskunig", both published in editions lavishly illustrated with woodcuts. In this vein, he commissioned a series of three monumental woodblock prints – "The Triumphal Arch" (1512–18, 192 woodcut panels, 295 cm wide and 357 cm high – approximately 9'8" by 11'8½"), and a "Triumphal Procession" (1516–18, 137 woodcut panels, 54 m long) which is led by a "Large Triumphal Carriage" (1522, 8 woodcut panels, 1½' high and 8' long), created by artists including Albrecht Dürer, Albrecht Altdorfer and Hans Burgkmair.
Maximilian had a great passion for armour, not only as equipment for battle or tournaments, but as an art form. The style of armour that became popular during the second half of his reign featured elaborate fluting and metalworking, and became known as Maximilian armour. It emphasized the details in the shaping of the metal itself, rather than the etched or gilded designs popular in the Milanese style. Maximilian also gave a bizarre jousting helmet as a gift to King Henry VIII – the helmet's visor featured a human face, with eyes, nose and a grinning mouth, and was modelled after the appearance of Maximilian himself. It also sported a pair of curled ram's horns, brass spectacles, and even etched beard stubble.
Maximilian had appointed his daughter Margaret as both Regent of the Netherlands and the guardian and educator of his grandsons Charles and Ferdinand (their father, Philip, having predeceased Maximilian), and she fulfilled this task well. Through wars and marriages he extended the Habsburg influence in every direction: to the Netherlands, Spain, Bohemia, Hungary, Poland, and Italy. This influence would last for centuries and shape much of European history.
Official style.
"Maximilian I, by the grace of God elected Holy Roman Emperor, forever August, King of Germany, of Hungary, Dalmatia, Croatia, etc. Archduke of Austria, Duke of Burgundy, Brabant, Lorraine, Styria, Carinthia, Carniola, Limburg, Luxembourg, Gelderland, Landgrave of Alsace, Prince of Swabia, Count Palatine of Burgundy, Princely Count of Habsburg, Hainaut, Flanders, Tyrol, Gorizia, Artois, Holland, Seeland, Ferrette, Kyburg, Namur, Zutphen, Margrave of the Holy Roman Empire, the Enns, Burgau, Lord of Frisia, the Wendish March, Pordenone, Salins, Mechelen, etc. etc."
Chivalric order.
Maximilian I was a member of the Order of the Garter, nominated by King Henry VII of England in 1489. His Garter stall plate survives in St George's Chapel, Windsor Castle.
Marriages and offspring.
Maximilian was married three times. However, only the first marriage produced offspring:

</doc>
